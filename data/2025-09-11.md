<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.SD](#cs.SD) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [hep-ph](#hep-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 6]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.DM](#cs.DM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Polyglot Persistence in Microservices: Managing Data Diversity in Distributed Systems](https://arxiv.org/abs/2509.08014)
*Festim Halili,Anila Nuhiji,Diellza Mustafai Veliu*

Main category: cs.DB

TL;DR: 本文探讨微服务架构中的多语言持久化策略，通过理论分析和实证研究评估不同数据库技术的性能、可扩展性和集成复杂度，并讨论应对复杂性的架构模式。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽然提供了可扩展性和灵活性，但带来了分布式数据管理的挑战，需要研究多语言持久化策略来应对异构数据存储需求。

Method: 采用比较框架分析关系型、文档型、键值型、列族型和图数据库的性能指标，结合Netflix、Uber、Shopify等行业案例研究和调查数据进行实证分析。

Result: 研究发现多语言持久化提高了系统的适应性、性能和领域对齐性，但同时也增加了治理和操作复杂性。

Conclusion: 为平衡多语言持久化的利弊，需要采用saga工作流、事件溯源和outbox集成等架构模式来管理分布式数据一致性。

Abstract: Microservices architectures have become the foundation for developing
scalable and modern software systems, but they also bring significant
challenges in managing heterogeneous and distributed data. The pragmatic
solution is polyglot persistence, the deliberate use of several different
database technologies adapted to a given microservice requirement - is one such
strategy. This paper examines polyglot persistence in microservice based
systems. This paper brings together theoretical concepts with evidence from
practical implementations and comparative benchmarks of standard database
platforms. A comparative framework is applied to relational, document,
key-value, column-family and graph databases to assess scalability,
consistency, query expressiveness, operational overhead and integration ease.
Empirical data drawn from industry case studies such as Netflix, Uber, and
Shopify, and survey data illustrate real-life adoption trends and challenges.
These findings demonstrate that polyglot persistence increases adaptability ,
performance , domain alignment but also governance or operational complexity.
To cope with such trade-offs, architectural patterns such as saga workflows,
event sourcing, and outbox integration are discussed.

</details>


### [2] [Infinite Stream Estimation under Personalized $w$-Event Privacy](https://arxiv.org/abs/2509.08387)
*Leilei Du,Peng Cheng,Lei Chen,Heng Tao Shen,Xuemin Lin,Wei Xi*

Main category: cs.DB

TL;DR: 本文提出了个性化w-event隐私保护机制，允许不同用户有不同的隐私需求，设计了PWSM机制和两种解决方案PBD、PBA，在满足所有用户隐私要求的同时显著提高了流数据统计估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有w-event隐私研究主要关注所有用户的同质隐私需求，但在实际应用中不同用户可能有不同的隐私保护要求，需要个性化的隐私保护方案。

Method: 设计了PWSM机制保持用户在每个时间段的恒定隐私需求，提出了PBD和PBA两种解决方案来实现个性化差分隐私。PBD确保为下一步提供至少与之前发布消耗相同的隐私预算，PBA则完全吸收前k个时间段的隐私预算并借用后k个时间段的预算来增加当前时间段的预算。

Result: PBD在真实数据集上比BD平均减少68%的误差，PBA在合成数据集上比BA平均减少24.9%的误差，两种方法都优于现有的流估计方法。

Conclusion: 个性化w-event隐私保护机制能够有效满足不同用户的差异化隐私需求，同时在保持高精度的数据收集方面表现出色，为流数据分析提供了更好的隐私保护解决方案。

Abstract: Streaming data collection is indispensable for stream data analysis, such as
event monitoring. However, publishing these data directly leads to privacy
leaks. $w$-event privacy is a valuable tool to protect individual privacy
within a given time window while maintaining high accuracy in data collection.
Most existing $w$-event privacy studies on infinite data stream only focus on
homogeneous privacy requirements for all users. In this paper, we propose
personalized $w$-event privacy protection that allows different users to have
different privacy requirements in private data stream estimation. Specifically,
we design a mechanism that allows users to maintain constant privacy
requirements at each time slot, namely Personalized Window Size Mechanism
(PWSM). Then, we propose two solutions to accurately estimate stream data
statistics while achieving $w$-event level $\epsilon$ personalized differential
privacy ( ($w$, $\epsilon$)-EPDP), namely Personalized Budget Distribution
(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the
same privacy budget for the next time step as the amount consumed in the
previous release. PBA fully absorbs the privacy budget from the previous $k$
time slots, while also borrowing from the privacy budget of the next $k$ time
slots, to increase the privacy budget for the current time slot. We prove that
both PBD and PBA outperform the state-of-the-art private stream estimation
methods while satisfying the privacy requirements of all users. We demonstrate
the efficiency and effectiveness of our PBD and PBA on both real and synthetic
data sets, compared with the recent uniformity $w$-event approaches, Budget
Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error
than BD on average on real data sets. Besides, our PBA achieves 24.9% less
error than BA on average on synthetic data sets.

</details>


### [3] [SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors](https://arxiv.org/abs/2509.08395)
*Ruoxuan Li,Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Wangze Ni,Lei Chen,Zhitao Shen,Wei Jia,Xiangyu Wang,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: SINDI是一种针对稀疏向量最大内积搜索的优化索引结构，通过SIMD加速、内存友好设计和向量剪枝技术，显著提升了检索效率和吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有稀疏向量MIPS算法在生产环境中受限于冗余距离计算、频繁随机内存访问，以及压缩存储格式阻碍SIMD加速的问题

Method: 提出稀疏倒排非冗余距离索引(SINDI)，包含三个关键优化：1)利用SIMD加速和消除冗余标识符查找实现批量内积计算；2)用顺序访问倒排列表替代随机内存访问；3)保留高幅值非零项进行向量剪枝

Result: 在多个真实数据集上评估显示，SINDI在不同规模、语言和模型的数据集上达到最先进性能。在MsMarco数据集上Recall@50超过99%时，单线程QPS比SEISMIC和PyANNs提升4.2-26.4倍

Conclusion: SINDI有效解决了稀疏向量MIPS的性能瓶颈，已集成到蚂蚁集团开源向量搜索库VSAG中，具有重要的实际应用价值

Abstract: Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path
retrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based
and graph-based algorithms have achieved high search accuracy with practical
efficiency. However, their performance in production environments is often
limited by redundant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hinders the use of
SIMD acceleration. In this paper, we propose the sparse inverted non-redundant
distance index (SINDI), which incorporates three key optimizations: (i)
Efficient Inner Product Computation: SINDI leverages SIMD acceleration and
eliminates redundant identifier lookups, enabling batched inner product
computation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses
to original vectors with sequential accesses to inverted lists, substantially
reducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the
high-magnitude non-zero entries of vectors, improving query throughput while
maintaining accuracy. We evaluate SINDI on multiple real-world datasets.
Experimental results show that SINDI achieves state-of-the-art performance
across datasets of varying scales, languages, and models. On the MsMarco
dataset, when Recall@50 exceeds 99%, SINDI delivers single-thread
query-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared
with SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's
open-source vector search library, VSAG.

</details>


### [4] [Un cadre paraconsistant pour l'{é}valuation de similarit{é} dans les bases de connaissances](https://arxiv.org/abs/2509.08433)
*José-Luis Vilchis Medina*

Main category: cs.DB

TL;DR: 提出了一种基于次协调逻辑的知识库相似性评估框架，能够显式处理矛盾，引入新的相似性度量S*，在惩罚不一致性的同时奖励共享属性


<details>
  <summary>Details</summary>
Motivation: 传统相似性评估方法无法有效处理知识库中的矛盾信息，需要一种能够显式整合矛盾并保持鲁棒性和可解释性的框架

Method: 定义次协调超类别Ξ_K*来层次化组织知识实体，引入矛盾提取器E和修复机制，提出新的相似性度量S*，确保自反性、对称性和有界性

Result: 理论结果证明了S*度量的数学性质，框架能够有效管理冲突知识

Conclusion: 该次协调框架为处理矛盾知识提供了有前景的解决方案，在多智能体系统中具有应用前景

Abstract: This article proposes a paraconsistent framework for evaluating similarity in
knowledge bases. Unlike classical approaches, this framework explicitly
integrates contradictions, enabling a more robust and interpretable similarity
measure. A new measure $ S^* $ is introduced, which penalizes inconsistencies
while rewarding shared properties. Paraconsistent super-categories $ \Xi_K^* $
are defined to hierarchically organize knowledge entities. The model also
includes a contradiction extractor $ E $ and a repair mechanism, ensuring
consistency in the evaluations. Theoretical results guarantee reflexivity,
symmetry, and boundedness of $ S^* $. This approach offers a promising solution
for managing conflicting knowledge, with perspectives in multi-agent systems.

</details>


### [5] [SQLGovernor: An LLM-powered SQL Toolkit for Real World Application](https://arxiv.org/abs/2509.08575)
*Jie Jiang,Siqi Shen,Haining Xie,Yang Li,Yu Shen,Danqing Huang,Bo Qian,Yinjun Wu,Wentao Zhang,Bin Cui,Peng Chen*

Main category: cs.DB

TL;DR: SQLGovernor是一个基于LLM的SQL工具包，通过片段化处理和混合自学习机制，统一处理SQL语法纠错、查询重写、修改和一致性验证，在基准测试中提升基础模型性能达10%


<details>
  <summary>Details</summary>
Motivation: 解决现实分析环境中SQL查询存在的语法错误、效率低下和语义偏差问题，特别是在复杂的OLAP场景中

Method: 采用片段化处理策略实现细粒度重写和局部错误纠正，结合专家反馈指导的混合自学习机制，通过DBMS输出分析和规则验证持续改进

Result: 在BIRD、BIRD CRITIC基准测试和工业数据集上，SQLGovernor持续提升基础模型性能达10%，减少对人工专业知识的依赖

Conclusion: SQLGovernor在生产环境中展现出强大的实用性和有效性能，为复杂OLAP场景下的SQL查询问题提供了统一解决方案

Abstract: SQL queries in real world analytical environments, whether written by humans
or generated automatically often suffer from syntax errors, inefficiency, or
semantic misalignment, especially in complex OLAP scenarios. To address these
challenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies
multiple functionalities, including syntax correction, query rewriting, query
modification, and consistency verification within a structured framework
enhanced by knowledge management. SQLGovernor introduces a fragment wise
processing strategy to enable fine grained rewriting and localized error
correction, significantly reducing the cognitive load on the LLM. It further
incorporates a hybrid self learning mechanism guided by expert feedback,
allowing the system to continuously improve through DBMS output analysis and
rule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as
well as industrial datasets, show that SQLGovernor consistently boosts the
performance of base models by up to 10%, while minimizing reliance on manual
expertise. Deployed in production environments, SQLGovernor demonstrates strong
practical utility and effective performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery](https://arxiv.org/abs/2509.08207)
*Benjamin S. Allen,James Anchell,Victor Anisimov,Thomas Applencourt,Abhishek Bagusetty,Ramesh Balakrishnan,Riccardo Balin,Solomon Bekele,Colleen Bertoni,Cyrus Blackworth,Renzo Bustamante,Kevin Canada,John Carrier,Christopher Chan-nui,Lance C. Cheney,Taylor Childers,Paul Coffman,Susan Coghlan,Michael D'Mello,Murali Emani,Kyle G. Felker,Sam Foreman,Olivier Franza,Longfei Gao,Marta García,María Garzarán,Balazs Gerofi,Yasaman Ghadar,Neha Gupta,Kevin Harms,Väinö Hatanpää,Brian Holland,Carissa Holohan,Brian Homerding,Khalid Hossain,Louise Huot,Huda Ibeid,Joseph A. Insley,Sai Jayanthi,Hong Jiang,Wei Jiang,Xiao-Yong Jin,Jeongnim Kim,Christopher Knight,Kalyan Kumaran,JaeHyuk Kwack,Ti Leggett,Ben Lenard,Chris Lewis,Nevin Liber,Johann Lombardi,Raymond M. Loy,Ye Luo,Bethany Lusch,Nilakantan Mahadevan,Victor A. Mateevitsi,Gordon McPheeters,Ryan Milner,Vitali A. Morozov,Servesh Muralidharan,Tom Musta,Mrigendra Nagar,Vikram Narayana,Marieme Ngom,Anthony-Trung Nguyen,Nathan Nichols,Aditya Nishtala,James C. Osborn,Michael E. Papka,Scott Parker,Saumil S. Patel,Adrian C. Pope,Sucheta Raghunanda,Esteban Rangel,Paul M. Rich,Silvio Rizzi,Kris Rowe,Varuni Sastry,Adam Scovel,Filippo Simini,Haritha Siddabathuni Som,Patrick Steinbrecher,Rick Stevens,Xinmin Tian,Peter Upton,Thomas Uram,Archit K. Vasan,Álvaro Vázquez-Mayagoitia,Kaushik Velusamy,Brice Videau,Venkatram Vishwanath,Brian Whitney,Timothy J. Williams,Michael Woodacre,Sam Zeltner,Gengbin Zheng,Huihuo Zheng*

Main category: cs.DC

TL;DR: Aurora是阿贡国家实验室的百亿亿次超级计算机，采用Intel Xeon Sapphire Rapids和Ponte Vecchio GPU等创新架构，集成DAOS存储系统和oneAPI编程环境，支持科学发现。


<details>
  <summary>Details</summary>
Motivation: 为加速科学发现，需要构建具有尖端架构创新的百亿亿次超级计算机，集成高性能计算硬件和新型存储解决方案。

Method: 采用Intel Xeon Sapphire Rapids处理器（支持HBM高带宽内存）和Ponte Vecchio GPU计算节点，集成DAOS分布式异步对象存储系统，使用HPE Slingshot互连技术和Intel oneAPI编程环境。

Result: 论文深入探讨了Aurora的节点架构、互连技术、软件生态系统和DAOS存储系统，提供了标准基准测试性能数据和应用准备情况。

Conclusion: Aurora通过创新的硬件架构和软件生态系统，为百亿亿次计算时代的科学应用提供了强大的计算平台，并通过早期科学计划和百亿亿次计算项目验证了其性能和应用准备度。

Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,
designed to accelerate scientific discovery with cutting-edge architectural
innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center
GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth
Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named
Ponte Vecchio) on each compute node. Aurora also integrates the Distributed
Asynchronous Object Storage (DAOS), a novel exascale storage solution, and
leverages Intel's oneAPI programming environment. This paper presents an
in-depth exploration of Aurora's node architecture, the HPE Slingshot
interconnect, the supporting software ecosystem, and DAOS. We provide insights
into standard benchmark performance and applications readiness efforts via
Aurora's Early Science Program and the Exascale Computing Project.

</details>


### [7] [Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem](https://arxiv.org/abs/2509.08215)
*Bingbing Zhang,Ziyu Lin,Yingxin Su*

Main category: cs.DC

TL;DR: 本研究实现了一个结合CodeBERT和GPT-3.5的混合模型，用于代码建议和自动完成任务，在准确性、代码质量和性能效率方面优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的软件开发行业中，编码效率和准确性对交付高质量软件至关重要。现有的代码建议工具如CodeBERT和GPT-3.5各有优势，但单独使用时无法充分发挥各自的优势。

Method: 开发了一个混合模型，整合了CodeBERT的上下文感知能力和GPT-3.5的高级代码生成能力，用于代码建议和自动完成任务。

Result: 混合模型在准确性、生成代码质量和性能效率三个主要指标上均优于基准模型，鲁棒性测试进一步证实了其可靠性和稳定性。

Conclusion: 该研究不仅强调了深度学习在软件开发行业的重要性，还揭示了通过合成互补的深度学习模型来充分发挥各自优势的潜力。

Abstract: In the rapidly evolving industry of software development, coding efficiency
and accuracy play significant roles in delivering high-quality software.
Various code suggestion and completion tools, such as CodeBERT from Microsoft
and GPT-3.5 from OpenAI, have been developed using deep learning techniques and
integrated into IDEs to assist software engineers' development. Research has
shown that CodeBERT has outstanding performance in code summarization and
capturing code semantics, while GPT-3.5 demonstrated its adept capability at
code generation. This study focuses on implementing a hybrid model that
integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and
autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and
taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in
three main metrics: accuracy, quality of generated code and performance
efficiency with various software and hardware, the hybrid model outperforms
benchmarks, demonstrating its feasibility and effectiveness. Robustness testing
further confirms the reliability and stability of the hybrid model. This study
not only emphasizes the importance of deep learning in the software development
industry, but also reveals the potential of synthesizing complementary deep
learning models to fully exploit strengths of each model.

</details>


### [8] [Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism](https://arxiv.org/abs/2509.08309)
*Zizhao Mo,Jianxiong Liao,Huanle Xu,Zhi Zhou,Chengzhong Xu*

Main category: cs.DC

TL;DR: Hetis是一个专为异构GPU集群设计的LLM服务系统，通过细粒度动态并行化解决内存和计算效率问题，相比现有系统吞吐量提升2.25倍，延迟降低1.49倍


<details>
  <summary>Details</summary>
Motivation: LLM服务在异构硬件环境中，现有并行化方法由于粗粒度和静态策略难以高效扩展，存在内存容量与计算能力不匹配、不同LLM模块性能差距等问题

Method: 采用细粒度动态并行设计：选择性并行化计算密集型操作降低延迟；以head粒度动态分配Attention计算到低端GPU；在线负载调度策略平衡网络延迟、计算负载和内存强度

Result: 评估结果显示，相比现有系统，Hetis可将服务吞吐量提升最多2.25倍，延迟降低1.49倍

Conclusion: Hetis通过创新的细粒度动态并行化方法有效解决了异构GPU集群中的LLM服务效率问题，显著提升了系统性能

Abstract: The significant resource demands in LLM serving prompts production clusters
to fully utilize heterogeneous hardware by partitioning LLM models across a mix
of high-end and low-end GPUs. However, existing parallelization approaches
often struggle to scale efficiently in heterogeneous environments due to their
coarse-grained and static parallelization strategies.
  In this paper, we introduce Hetis, a new LLM system tailored for
heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory
inefficiency caused by the mismatch between memory capacity and computational
power in heterogeneous devices, and (2) computational inefficiency arising from
performance gaps across different LLM modules. To tackle these issues, Hetis
employs a fine-grained and dynamic parallelism design. Specifically, it
selectively parallelizes compute-intensive operations to reduce latency and
dynamically distributes Attention computations to low-end GPUs at a head
granularity, leveraging the distinct characteristics of each module.
Additionally, Hetis features an online load dispatching policy that
continuously optimizes serving performance by carefully balancing network
latency, computational load, and memory intensity. Evaluation results
demonstrate that Hetis can improve serving throughput by up to $2.25\times$ and
reduce latency by $1.49\times$ compared to existing systems.

</details>


### [9] [An HPC Benchmark Survey and Taxonomy for Characterization](https://arxiv.org/abs/2509.08347)
*Andreas Herten,Olga Pearce,Filipe S. M. Guimarães*

Main category: cs.DC

TL;DR: 本文对高性能计算(HPC)领域的基准测试工具进行了全面调查和分类，提出了基准测试分类法，并提供了包含关键细节的表格总结和交互式网站。


<details>
  <summary>Details</summary>
Motivation: HPC领域需要标准化的基准测试工具来评估硬件、软件和算法性能，但目前缺乏系统性的分类和总结，难以进行有效的性能比较和采购决策。

Method: 通过调查现有HPC基准测试工具，创建详细的表格总结，开发交互式网站，并提出基准测试分类法来进行系统性的分类和特征描述。

Result: 提供了完整的HPC基准测试概览，包括各种特定功能测试和小众基准测试，以及大规模基准测试套件，为系统架构师和研究人员提供了有价值的参考工具。

Conclusion: 建立的基准测试分类法和综合调查为HPC社区提供了标准化的评估框架，有助于推动硬件软件协同设计和性能优化。

Abstract: The field of High-Performance Computing (HPC) is defined by providing
computing devices with highest performance for a variety of demanding
scientific users. The tight co-design relationship between HPC providers and
users propels the field forward, paired with technological improvements,
achieving continuously higher performance and resource utilization. A key
device for system architects, architecture researchers, and scientific users
are benchmarks, allowing for well-defined assessment of hardware, software, and
algorithms. Many benchmarks exist in the community, from individual niche
benchmarks testing specific features, to large-scale benchmark suites for whole
procurements. We survey the available HPC benchmarks, summarizing them in table
form with key details and concise categorization, also through an interactive
website. For categorization, we present a benchmark taxonomy for well-defined
characterization of benchmarks.

</details>


### [10] [Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data](https://arxiv.org/abs/2509.08409)
*Shilong Wang,Jianchun Liu,Hongli Xu,Chenxia Tang,Qianpiao Ma,Liusheng Huang*

Main category: cs.DC

TL;DR: Duplex是一个联合优化网络拓扑和图形采样的统一框架，通过考虑两者的耦合关系，在降低通信成本的同时提升DFGL训练性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦图学习(DFGL)虽然避免了参数服务器的瓶颈，但图节点嵌入的跨工作节点通信带来了巨大的通信开销。现有方法单独使用稀疏网络拓扑或图邻居采样，但直接组合会导致训练性能显著下降。

Method: 提出Duplex框架，通过考虑网络拓扑和图形采样的耦合关系进行联合优化。引入学习驱动算法自适应确定最优网络拓扑和图形采样比例，以应对统计异构性和动态网络环境。

Result: 实验结果显示，Duplex在达到目标精度时减少完成时间20.1%-48.8%，通信成本降低16.7%-37.6%。在相同资源预算下，精度提高3.3%-7.9%。

Conclusion: Duplex通过联合优化网络拓扑和图形采样，有效解决了DFGL中的通信效率问题，在降低通信成本的同时提升了训练性能，适用于统计异构和动态网络环境。

Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks
of the parameter server in FGL by establishing a peer-to-peer (P2P)
communication network among workers. However, while extensive cross-worker
communication of graph node embeddings is crucial for DFGL training, it
introduces substantial communication costs. Most existing works typically
construct sparse network topologies or utilize graph neighbor sampling methods
to alleviate the communication overhead in DFGL. Intuitively, integrating these
methods may offer promise for doubly improving communication efficiency in
DFGL. However, our preliminary experiments indicate that directly combining
these methods leads to significant training performance degradation if they are
jointly optimized. To address this issue, we propose Duplex, a unified
framework that jointly optimizes network topology and graph sampling by
accounting for their coupled relationship, thereby significantly reducing
communication cost while enhancing training performance in DFGL. To overcome
practical DFGL challenges, eg, statistical heterogeneity and dynamic network
environments, Duplex introduces a learning-driven algorithm to adaptively
determine optimal network topologies and graph sampling ratios for workers.
Experimental results demonstrate that Duplex reduces completion time by
20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target
accuracy, while improving accuracy by 3.3%--7.9% under identical resource
budgets compared to baselines.

</details>


### [11] [A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN](https://arxiv.org/abs/2509.08608)
*Yichao Zhang,Marco Bertuletti,Sergio Mazzola,Samuel Riedel,Luca Benini*

Main category: cs.DC

TL;DR: HeartStream是一个64核共享L1内存集群，专为AI增强的O-RAN设计，在基带处理方面提供高性能和能效，支持复杂数值运算和AI处理，满足5G/6G上行链路时延要求。


<details>
  <summary>Details</summary>
Motivation: 为AI增强的开放无线接入网络(O-RAN)开发一个能效优化的处理集群，专门针对基带处理需求，同时支持传统无线工作负载和AI处理，满足下一代无线通信系统的功率和时延约束。

Method: 设计64个RV核心的共享L1内存集群架构，定制化支持复数(16位实部和虚部)指令，包括乘积累加、除法和平方根、SIMD指令，以及硬件管理的脉动队列，优化基带处理内核的能效。

Result: 在800MHz@0.8V下提供243GFLOP/s的复数无线工作负载性能，AI处理达到72 GOP/s，软件定义的PUSCH效率达到49.6GFLOP/s/W，功耗仅0.68W(645MHz@0.65V)，满足4ms端到端时延约束。

Conclusion: HeartStream展示了针对B5G/6G上行链路的能效优化设计，在保持完全基站兼容性的同时，相比关键基带内核提升了1.89倍的能效，为AI增强的O-RAN提供了可行的硬件解决方案。

Abstract: We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s
peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced
O-RAN. The cores and cluster architecture are customized for baseband
processing, supporting complex (16-bit real&imaginary) instructions:
multiply&accumulate, division&square-root, SIMD instructions, and
hardware-managed systolic queues, improving up to 1.89x the energy efficiency
of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s
on complex-valued wireless workloads. Furthermore, the cores also support
efficient AI processing on received data at up to 72 GOP/s. HeartStream is
fully compatible with base station power and processing latency limits: it
achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and
consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for
B5G/6G uplink.

</details>


### [12] [Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges](https://arxiv.org/abs/2509.08770)
*Muhammad Ali Jamshed,Muhammad Ahmed Mohsin,Hongliang Zhang,Bushra Haq,Aryan Kaushik,Boya Di,Weiwei Jiang*

Main category: cs.DC

TL;DR: 本文提出将近场通信(NFC)和可重构全息表面(RHS)结合用于非地面网络(NTN)，通过RHS与卫星、HAPS和无人机等NTN平台集成，实现近场区域的精确波束成形和智能波前控制，提升能效、频谱利用率和空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 为了解决超低延迟、泛在覆盖和激增数据速率的挑战，需要创新性的网络架构和技术方案。

Method: 提出系统架构，将RHS与NTN平台（卫星、高空平台站、无人机）集成，实现近场区域的精确波束成形和波前控制，并进行用例分析验证能效提升。

Result: 该集成能够增强能效(EE)、频谱利用率和空间分辨率，在公共安全用例场景中进一步强化了无人机-RHS融合。

Conclusion: 文章识别了关键应用、挑战和未来方向，表明NFC与RHS在NTN中的结合具有重要应用前景，需要进一步研究以完全采用这种集成方案。

Abstract: To overcome the challenges of ultra-low latency, ubiquitous coverage, and
soaring data rates, this article presents a combined use of Near Field
Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for
Non-Terrestrial Networks (NTN). A system architecture has been presented, which
shows that the integration of RHS with NTN platforms such as satellites, High
Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can
achieve precise beamforming and intelligent wavefront control in near-field
regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial
resolution. Moreover, key applications, challenges, and future directions have
been identified to fully adopt this integration. In addition, a use case
analysis has been presented to improve the EE of the system in a public safety
use case scenario, further strengthening the UAV-RHS fusion.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [A Dynamic, Self-balancing k-d Tree](https://arxiv.org/abs/2509.08148)
*Russell A. Brown*

Main category: cs.DS

TL;DR: 本文提出了动态k-d树的插入和删除算法，解决了传统k-d树无法使用旋转平衡技术的问题，并对其性能进行了测量。


<details>
  <summary>Details</summary>
Motivation: 传统k-d树无法像AVL树或红黑树那样使用旋转技术进行重新平衡，因为旋转会破坏k-d树的排序顺序。这导致k-d树通常需要从所有数据一次性静态构建，缺乏动态更新能力。

Method: 开发了动态k-d树的插入和删除算法，使树能够在每次插入或删除单个k维数据后自动进行必要的自平衡。

Result: 实现了动态k-d树的自平衡功能，并对其性能进行了测量评估。

Conclusion: 成功证明了构建动态k-d树的可行性，为k-d树提供了动态更新和自平衡的能力，扩展了其应用范围。

Abstract: The original description of the k-d tree recognized that rebalancing
techniques, such as used to build an AVL tree or a red-black tree, are not
applicable to a k-d tree, because these techniques involve cyclic exchange (aka
rotation) of tree nodes, which destroys the sorted order of the k-d tree. For
this reason, a static k-d tree is often built from all of the k-dimensional
data en masse. However, it is possible to build a dynamic k-d tree that
self-balances when necessary after insertion or deletion of each individual
k-dimensional datum. This article describes insertion and deletion algorithms
for a dynamic k-d tree, and measures their performance.

</details>


### [14] [Enumeration kernels for Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2509.08475)
*Marin Bougeret,Guilherme C. M. Gomes,Vinicius F. dos Santos,Ignasi Sau*

Main category: cs.DS

TL;DR: 本文在枚举核化领域取得重要进展，为Vertex Cover和Feedback Vertex Set这两个经典问题提供了多项式延迟的枚举核，分别将核大小改进到2k个顶点和O(k^3)个顶点和边。


<details>
  <summary>Details</summary>
Motivation: 枚举核化是参数化复杂性和枚举算法交叉的新兴领域，但在Vertex Cover和Feedback Vertex Set这两个核化研究中最受关注的问题上进展有限，本文旨在解决这一不足。

Method: 对于Enum Vertex Cover，开发了非平凡的提升算法来处理经典的crown分解归约规则；对于Enum Feedback Vertex Set，受Thomassé思想的启发，但应用q-expansion技术时遇到困难导致核大小界限较弱。

Result: 为Enum Vertex Cover获得了具有2k个顶点的多项式延迟枚举核，直接改进了之前O(k^2)顶点的结果；为Enum Feedback Vertex Set获得了具有O(k^3)个顶点和边的多项式延迟枚举核。

Conclusion: 本文成功解决了枚举核化领域在经典问题上的关键挑战，为Vertex Cover和Feedback Vertex Set提供了高效的枚举核，推动了该领域的发展。

Abstract: Enumerative kernelization is a recent and promising area sitting at the
intersection of parameterized complexity and enumeration algorithms. Its study
began with the paper of Creignou et al. [Theory Comput. Syst., 2017], and
development in the area has started to accelerate with the work of Golovach et
al. [J. Comput. Syst. Sci., 2022]. The latter introduced polynomial-delay
enumeration kernels and applied them in the study of structural
parameterizations of the \textsc{Matching Cut} problem and some variants. Few
other results, mostly on \textsc{Longest Path} and some generalizations of
\textsc{Matching Cut}, have also been developed. However, little success has
been seen in enumeration versions of \textsc{Vertex Cover} and \textsc{Feedback
Vertex Set}, some of the most studied problems in kernelization. In this paper,
we address this shortcoming. Our first result is a polynomial-delay enumeration
kernel with $2k$ vertices for \textsc{Enum Vertex Cover}, where we wish to list
all solutions with at most $k$ vertices. This is obtained by developing a
non-trivial lifting algorithm for the classical crown decomposition reduction
rule, and directly improves upon the kernel with $\mathcal{O}(k^2)$ vertices
derived from the work of Creignou et al. Our other result is a polynomial-delay
enumeration kernel with $\mathcal{O}(k^3)$ vertices and edges for \textsc{Enum
Feedback Vertex Set}; the proof is inspired by some ideas of Thomass\'e [TALG,
2010], but with a weaker bound on the kernel size due to difficulties in
applying the $q$-expansion technique.

</details>


### [15] [Checking and producing word attractors](https://arxiv.org/abs/2509.08503)
*Marie-Pierre Béal,Maxime Crochemore,Giuseppe Romana*

Main category: cs.DS

TL;DR: 本文提出了两个基于后缀自动机或有向无环词图的组合算法，用于判断和生成词吸引子，其中一个算法能在线性时间内判断位置集是否为吸引子，另一个贪心算法能高效生成小型吸引子。


<details>
  <summary>Details</summary>
Motivation: 词吸引子与文本压缩效率密切相关，研究如何高效判断和生成最优吸引子对文本压缩算法具有重要意义。

Method: 使用后缀自动机（Suffix automata）或有向无环词图（Directed Acyclic Word Graphs）开发了两个组合算法：一个是线性时间判断算法，另一个是贪心生成算法。

Result: 第一个算法能在线性时间内判断位置集是否为词吸引子；第二个贪心算法虽然面对的是NP难问题，但能高效运行并为多个著名词族生成非常小的吸引子。

Conclusion: 提出的算法在理论和实践上都表现良好，特别是贪心算法能在多项式时间内为复杂问题生成接近最优的解，对文本压缩领域有重要贡献。

Abstract: The article focuses on word (or string) attractors, which are sets of
positions related to the text compression efficiency of the underlying word.
The article presents two combinatorial algorithms based on Suffix automata or
Directed Acyclic Word Graphs. The first algorithm decides in linear time
whether a set of positions on the word is an attractor of the word. The second
algorithm generates an attractor for a given word in a greedy manner. Although
this problem is NP-hard, the algorithm is efficient and produces very small
attractors for several well-known families of words.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts](https://arxiv.org/abs/2509.08090)
*Eman Abdullah AlOmar,Luo Xu,Sofia Martinez,Anthony Peruma,Mohamed Wiem Mkaouer,Christian D. Newman,Ali Ouni*

Main category: cs.SE

TL;DR: 本文通过分析29,778个ChatGPT对话中的715个重构相关互动，探索开发者如何表达重构需求以及ChatGPT如何响应这些需求。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究探讨了LLMs在重构建议方面的有效性，但对于开发者如何与ChatGPT互动表达重构需求的理解仍然有限。

Method: 采用文本挖掘方法，从29,778个ChatGPT提示和回复中提取715个重构相关互动，并分析开发者的明确重构意图。

Result: 研究发现揭示了开发者识别代码改进领域的方式以及ChatGPT如何满足开发者的重构需求。

Conclusion: 该研究为理解开发者与LLMs在重构任务中的互动模式提供了重要见解，有助于改进LLMs在软件工程重构任务中的应用效果。

Abstract: Large Language Models (LLMs), such as ChatGPT, have become widely popular and
widely used in various software engineering tasks such as refactoring, testing,
code review, and program comprehension. Although recent studies have examined
the effectiveness of LLMs in recommending and suggesting refactoring, there is
a limited understanding of how developers express their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore interactions
related to refactoring between developers and ChatGPT to better understand how
developers identify areas for improvement in code, and how ChatGPT addresses
developers' needs. Our approach involves text mining 715 refactoring-related
interactions from 29,778 ChatGPT prompts and responses, as well as the analysis
of developers' explicit refactoring intentions.

</details>


### [17] [Safety Factories -- a Manifesto](https://arxiv.org/abs/2509.08285)
*Carmen Cârlan,Daniel Ratiu,Michael Wagner*

Main category: cs.SE

TL;DR: 论文提出将安全工程工具和方法集成到软件开发流水线中，建立安全工厂，以弥合软件开发与安全工程之间的脱节。


<details>
  <summary>Details</summary>
Motivation: 现代信息物理系统由复杂软件操作，承担安全关键功能。高速软件开发需要纪律、严谨和自动化，但当前软件开发与安全工程的方法和工具存在脱节，需要弥合这一差距。

Method: 建议在前期投入更多形式化工作：用语义丰富的机器可处理模型捕获安全工作产品，定义自动一致性检查，自动化文档生成。将软件工程的最佳实践转移到安全工程中，建立安全工厂。

Result: 通过安全工厂的建立，可以实现软件开发与安全工程的更好集成，提高安全关键系统开发的效率和可靠性。

Conclusion: 将安全工具和方法集成到软件开发流水线中是值得探索的方向，安全工厂有助于保持软件快速演进的同时确保安全性。

Abstract: Modern cyber-physical systems are operated by complex software that
increasingly takes over safety-critical functions. Software enables rapid
iterations and continuous delivery of new functionality that meets the
ever-changing expectations of users. As high-speed development requires
discipline, rigor, and automation, software factories are used. These entail
methods and tools used for software development, such as build systems and
pipelines. To keep up with the rapid evolution of software, we need to bridge
the disconnect in methods and tools between software development and safety
engineering today. We need to invest more in formality upfront - capturing
safety work products in semantically rich models that are machine-processable,
defining automatic consistency checks, and automating the generation of
documentation - to benefit later. Transferring best practices from software to
safety engineering is worth exploring. We advocate for safety factories, which
integrate safety tooling and methods into software development pipelines.

</details>


### [18] [The Impact of Team Diversity in Agile Development Education](https://arxiv.org/abs/2509.08389)
*Marco Torchiano,Riccardo Coppola,Antonio Vetro',Xhoi Musaj*

Main category: cs.SE

TL;DR: 研究分析了51个软件开发团队，发现性别多样性对项目成功有显著正相关，国籍多样性影响可忽略不计，但两者结合会产生负面效应。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域长期存在性别失衡问题，研究旨在评估团队多样性（特别是性别和国籍）对敏捷软件开发项目课程中团队表现的影响。

Method: 分析了3个学年中51个团队的数据，使用性别多样性指数、国籍多样性指数及其共存指数来衡量多样性对项目成果质量的影响。

Result: 统计分析显示性别多样性与项目成功存在中等程度显著正相关；国籍多样性对项目结果有轻微负面影响但可忽略；性别和国籍多样性共存会产生负面影响，主要由于沟通障碍和文化差异。

Conclusion: 在教育环境中需要考虑多重多样性维度及其相互作用，促进团队多样性不会对绩效和教育目标达成产生负面影响。

Abstract: Software Engineering is mostly a male-dominated sector, where gender
diversity is a key feature for improving equality of opportunities,
productivity, and innovation. Other diversity aspects, including but not
limited to nationality and ethnicity, are often understudied.In this work we
aim to assess the impact of team diversity, focusing mainly on gender and
nationality, in the context of an agile software development project-based
course. We analyzed 51 teams over three academic years, measuring three
different Diversity indexes - regarding Gender, Nationality and their
co-presence - to examine how different aspects of diversity impact the quality
of team project outcomes.Statistical analysis revealed a moderate,
statistically significant correlation between gender diversity and project
success, aligning with existing literature. Diversity in nationality showed a
negative but negligible effect on project results, indicating that promoting
these aspects does not harm students' performance. Analyzing their co-presence
within a team, gender and nationality combined had a negative impact, likely
due to increased communication barriers and differing cultural norms.This study
underscores the importance of considering multiple diversity dimensions and
their interactions in educational settings. Our findings, overall, show that
promoting diversity in teams does not negatively impact their performance and
achievement of educational goals.

</details>


### [19] [AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution](https://arxiv.org/abs/2509.08524)
*Felix Mächtle,Nils Loose,Jan-Niclas Serr,Jonas Sander,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: AutoStub使用遗传编程自动为符号执行中的外部函数生成符号存根，无需手动干预即可近似函数行为，准确率超过90%且能发现关键边缘情况


<details>
  <summary>Details</summary>
Motivation: 符号执行在遇到外部函数（如原生方法或第三方库）时存在局限性，现有解决方案需要额外上下文、昂贵的SMT求解器或手动干预来近似这些函数

Method: 当符号执行器遇到外部函数时，AutoStub通过随机生成输入并收集输出来生成训练数据，然后使用遗传编程推导近似函数行为的表达式作为符号存根

Result: AutoStub能自动近似外部函数，在评估的函数中有55%达到超过90%的准确率，并能推断出语言特定行为，揭示对软件测试至关重要的边缘情况

Conclusion: 该方法使符号执行器能够继续分析而无需手动干预，实现了先前难以处理的程序路径探索，显著提升了符号执行的自动化程度和效果

Abstract: Symbolic execution is a powerful technique for software testing, but suffers
from limitations when encountering external functions, such as native methods
or third-party libraries. Existing solutions often require additional context,
expensive SMT solvers, or manual intervention to approximate these functions
through symbolic stubs. In this work, we propose a novel approach to
automatically generate symbolic stubs for external functions during symbolic
execution that leverages Genetic Programming. When the symbolic executor
encounters an external function, AutoStub generates training data by executing
the function on randomly generated inputs and collecting the outputs. Genetic
Programming then derives expressions that approximate the behavior of the
function, serving as symbolic stubs. These automatically generated stubs allow
the symbolic executor to continue the analysis without manual intervention,
enabling the exploration of program paths that were previously intractable. We
demonstrate that AutoStub can automatically approximate external functions with
over 90% accuracy for 55% of the functions evaluated, and can infer
language-specific behaviors that reveal edge cases crucial for software
testing.

</details>


### [20] [Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China](https://arxiv.org/abs/2509.08546)
*Yu Zhu,Jiyuan Ye*

Main category: cs.SE

TL;DR: 本文提出了SAER（科研全评价体系）框架，通过整合形式、内容和效用三个评价维度与六个关键要素，旨在超越当前科研评价中定性与定量方法的二元对立，为全球科研评价体系改革提供理论突破和综合基础。


<details>
  <summary>Details</summary>
Motivation: 当前全球科研评价体系改革缺乏宏观系统性评价理论指导，定性与定量方法在评价实践中存在二元对立，这已成为改革的关键瓶颈。

Method: 通过回顾科研评价的历史发展，提出SAER框架，整合形式评价、内容评价和效用评价三个维度，结合六个关键评价要素，构建全面的科研评价体系。

Result: SAER框架提供了超越二元对立的评价理论突破，展示了中国科研评价理论中的辩证智慧，为全球评价体系改革提供了有价值的见解和参考。

Conclusion: 该全面评价体系提出的三维一体评价框架结合六个评价要素，有助于学术评价者和研究者调和评价方法中的二元对立，推动全球科研评价体系的改革与进步。

Abstract: The lack of a macro-level, systematic evaluation theory to guide the
implementation of evaluation practices has become a key bottleneck in the
reform of global research evaluation systems. By reviewing the historical
development of research evaluation, this paper highlights the current binary
opposition between qualitative and quantitative methods in evaluation
practices. This paper introduces the System of All-round Evaluation of Research
(SAER), a framework that integrates form, content, and utility evaluations with
six key elements. SAER offers a theoretical breakthrough by transcending the
binary, providing a comprehensive foundation for global evaluation reforms. The
comprehensive system proposes a trinity of three evaluation dimensions,
combined with six evaluation elements, which would help academic evaluators and
researchers reconcile binary oppositions in evaluation methods. The system
highlights the dialectical wisdom and experience embedded in Chinese research
evaluation theory, offering valuable insights and references for the reform and
advancement of global research evaluation systems.

</details>


### [21] [Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization](https://arxiv.org/abs/2509.08667)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: EZR是一个用于软件工程多目标优化的轻量级框架，通过主动学习和决策树解释，用更少但更信息丰富的样本实现高效可解释的优化


<details>
  <summary>Details</summary>
Motivation: 解决软件工程中配置空间巨大、标注成本高且易出错的问题，提供高效可解释的优化方法

Method: 采用基于朴素贝叶斯采样的主动学习策略，结合最大清晰度启发式方法，使用决策树提供全局和局部解释

Result: 在60个真实数据集上，EZR在大多数情况下达到超过90%的最佳优化性能，解释清晰度和实用性优于LIME、SHAP等标准XAI方法

Conclusion: 证明了'少而精'的可行性，使用更少但更信息丰富的样本可以实现标签高效的优化和解释

Abstract: Efficient, interpretable optimization is a critical but underexplored
challenge in software engineering, where practitioners routinely face vast
configuration spaces and costly, error-prone labeling processes. This paper
introduces EZR, a novel and modular framework for multi-objective optimization
that unifies active sampling, learning, and explanation within a single,
lightweight pipeline. Departing from conventional wisdom, our Maximum Clarity
Heuristic demonstrates that using less (but more informative) data can yield
optimization models that are both effective and deeply understandable. EZR
employs an active learning strategy based on Naive Bayes sampling to
efficiently identify high-quality configurations with a fraction of the labels
required by fully supervised approaches. It then distills optimization logic
into concise decision trees, offering transparent, actionable explanations for
both global and local decision-making. Extensive experiments across 60
real-world datasets establish that EZR reliably achieves over 90% of the
best-known optimization performance in most cases, while providing clear,
cohort-based rationales that surpass standard attribution-based explainable AI
(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results
endorse "less but better"; it is both possible and often preferable to use
fewer (but more informative) examples to generate label-efficient optimization
and explanations in software systems. To support transparency and
reproducibility, all code and experimental materials are publicly available at
https://github.com/amiiralii/Minimal-Data-Maximum-Clarity.

</details>


### [22] [SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories](https://arxiv.org/abs/2509.08724)
*Junhao Wang,Daoguang Zan,Shulin Xin,Siyao Liu,Yurong Wu,Kai Shen*

Main category: cs.SE

TL;DR: SWE-Mirror是一个从GitHub真实issue中构建可验证任务数据集的管道，通过重用现有Gym环境和issue解决历史，创建了60,671个任务，显著提升了代码代理的issue解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自动化Gym环境设置方面成功率低、开销大，而合成任务无法利用真实的人类报告问题。需要最大化利用现有Gym环境和GitHub丰富的issue解决历史数据。

Method: 引入SWE-Mirror管道：提取真实issue的语义本质，将其镜像到配置了Gym环境的仓库中，重新激活为可验证的issue解决任务。

Result: 在40个仓库4种语言中构建了60,671个任务的数据集。训练后模型在issue解决能力上表现提升，基于Qwen2.5-Coder-Instruct的LLM在SWE-Bench-Verified上的解决率分别提升21.8%(7B)和46.0%(32B)。

Conclusion: SWE-Mirror成功构建了大规模可验证数据集，验证了方法的有效性，为代码代理训练提供了高质量数据资源。

Abstract: Creating large-scale verifiable training datasets for issue-resolving tasks
is a critical yet notoriously difficult challenge. Existing methods on
automating the Gym environment setup process for real-world issues suffer from
low success rates and high overhead. Meanwhile, synthesizing new tasks within
existing Gym environments leaves the vast pool of authentic, human-reported
problems untapped. To maximize the utilization of existing Gym environments and
also the rich data of issue-resolving history on GitHub, we introduce
SWE-Mirror, a pipeline that distills a real-world issue's semantic essence,
mirrors it into another repository with a configured Gym environment, and
re-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing
Gym environments along with the vast pool of issue-resolving history hosted on
GitHub to construct a large-scale dataset of mirrored authentic and verifiable
tasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have
curated a dataset with 60,671 issue-resolving tasks and demonstrated the value
of our dataset by training and evaluating coding agents at various scale.
Post-training experiments show that models trained with the dataset exhibit
improvements in issue-resolving capabilities. Furthermore, by extending the
dataset size to over 12,000 high-quality trajectories, we established a new
state-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the
OpenHands agent framework, which increases the resolve rate on
SWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and
validates the effectiveness of our approach.

</details>


### [23] [Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval-Augmented Parsing with Expert Knowledge](https://arxiv.org/abs/2509.08808)
*Mohammad Saqib Hasan,Sayontan Ghosh,Dhruv Verma,Geoff Kuenning,Erez Zadok,Scott A. Smolka,Niranjan Balasubramanian*

Main category: cs.SE

TL;DR: 本文提出了动态知识增强解析(DKAP)方法，通过检索增强的ROLex解析器，利用专家提供的动态增长知识库来处理开放词汇构造(OVCs)问题，在自然语言到形式语言的转换任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇构造(OVCs)问题——即事先未知的构造，在将自然语言规范转换为形式语言时，由于缺乏先验知识，模型在OVCs上表现不佳。专家可以在推理时提供正确的构造，但需要有效重用这些知识而不重新训练模型。

Method: 提出动态知识增强解析(DKAP)框架，使用键值词典存储专家提供的NL短语与正确OVC构造的映射。开发ROLex检索增强解析器，包含检索器和生成器，通过合成数据生成和数据增强技术训练模型，并采用多种策略使模型专注于相关检索知识子集。

Result: 在三个形式化任务(NL2LTL、NL2Code、NL2CMD)上的评估表明，DKAP是一个具有挑战性的问题，ROLex通过有效利用动态专家知识，显著提高了基线模型的性能。

Conclusion: DKAP框架和ROLex方法为处理开放词汇构造问题提供了有效解决方案，能够动态利用专家知识而不需要重新训练模型，在自然语言到形式语言的转换任务中展现出优越性能。

Abstract: We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known
beforehand -- in the context of converting natural language (NL) specifications
into formal languages (e.g., temporal logic or code). Models fare poorly on
OVCs due to a lack of necessary knowledge a priori. In such situations, a
domain expert can provide correct constructs at inference time based on their
preferences or domain knowledge. Our goal is to effectively reuse this
inference-time, expert-provided knowledge for future parses without retraining
the model. We present dynamic knowledge-augmented parsing(DKAP), where in
addition to the input sentence, the model receives (dynamically growing) expert
knowledge as a key-value lexicon that associates NL phrases with correct OVC
constructs. We propose ROLex, a retrieval-augmented parsing approach that uses
this lexicon. A retriever and a generator are trained to find and use the
key-value store to produce the correct parse. A key challenge lies in curating
data for this retrieval-augmented parser. We utilize synthetic data generation
and the data augmentation techniques on annotated (NL sentence, FL statement)
pairs to train the augmented parser. To improve training effectiveness, we
propose multiple strategies to teach models to focus on the relevant subset of
retrieved knowledge. Finally, we introduce a new evaluation paradigm modeled
after the DKAP problem and simulate the scenario across three formalization
tasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a
difficult challenge, and ROLex helps improve the performance of baseline models
by using dynamic expert knowledge effectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [24] [Matisse: Visualizing Measured Internet Latencies as Manifolds](https://arxiv.org/abs/2509.08097)
*Stephen Jasina,Loqman Salamatian,Joshua Mathews,Scott Anderson,Paul Barford,Mark Crovella,Walter Willinger*

Main category: cs.NI

TL;DR: 提出了一种名为Matisse的新方法和系统，用于从互联网延迟测量数据生成和可视化流形，通过图结构和Ricci曲率信息在2D地理地图上投影形成曲面流形


<details>
  <summary>Details</summary>
Motivation: 流形可以表示现实世界测量数据集，可视化流形有助于展示拓扑特征（如曲率）并揭示底层数据的重要属性（如测量异常），特别是在互联网延迟空间分析中

Method: 利用包含顶点位置信息和边Ricci曲率信息的图结构，生成在2D欧几里得空间（地理地图）上投影的曲面流形，保持顶点地理位置同时用Ricci曲率值确定流形曲率特性

Result: 开发了Matisse工具用于生成、可视化和操作投影到基础地图上的流形，通过两个案例研究展示了方法的实用性：简单概念演示和美国公共互联网可视化

Conclusion: 该方法能够突出关键连接区域并定义"互联网延迟空间"实例，其中延迟测量表现为测地线，为网络性能分析提供了新的可视化手段

Abstract: Manifolds are complex topological spaces that can be used to represent
datasets of real-world measurements. Visualizing such manifolds can help with
illustrating their topological characteristics (e.g., curvature) and providing
insights into important properties of the underlying data (e.g., anomalies in
the measurements). In this paper, we describe a new methodology and system for
generating and visualizing manifolds that are inferred from actual Internet
latency measurements between different cities and are projected over a 2D
Euclidean space (e.g., a geographic map). Our method leverages a series of
graphs that capture critical information contained in the data, including
well-defined locations (for vertices) and Ricci curvature information (for
edges). Our visualization approach then generates a curved surface (manifold)
in which (a) geographical locations of vertices are maintained and (b) the
Ricci curvature values of the graph edges determine the curvature properties of
the manifold. The resulting manifold highlights areas of critical connectivity
and defines an instance of "Internet delay space" where latency measurements
manifest as geodesics. We describe details of our method and its implementation
in a tool, which we call Matisse, for generating, visualizing and manipulating
manifolds projected onto a base map. We illustrate Matisse with two case
studies: a simple example to demonstrate key concepts, and visualizations of
the US public Internet to show Matisse's utility.

</details>


### [25] [UTM Performance Under Stressing Scenarios](https://arxiv.org/abs/2509.08124)
*Ian Jessen*

Main category: cs.NI

TL;DR: ANAMLL是一个虚拟系统集成实验室，用于测试和验证无人机交通管理系统(UTM)在压力条件下的性能，展示了系统在需求高峰时的限制和网络连接对空域访问的影响。


<details>
  <summary>Details</summary>
Motivation: 随着新型空域参与者（如无人机和先进空中交通车辆）的激增，需要开发新的空域管理解决方案。虽然UTM系统已在有限测试环境中验证，但需要建模和仿真环境来研究系统在压力条件下的行为。

Method: 开发了ANAMLL虚拟系统集成实验室(SIL)，用于托管联邦自治网络（如UTM或PSU网络），并在无法通过实际部署实现的规模下进行测试和验证。

Result: ANAMLL展示了UTM系统在压力需求场景下的性能：1）确定了飞行中重新规划无法在允许时间窗口内完成的临界点；2）证明了网络连接性能对最终用户空域访问的影响。

Conclusion: ANAMLL作为一个有效的建模和仿真环境，能够揭示UTM系统在极端条件下的性能限制，为空域管理系统的开发和优化提供了重要工具。

Abstract: Proliferation of new classes of airspace participants, including uncrewed and
advanced aerial mobility vehicles, necessitates the development and deployment
of novel airspace management solutions, such as the Unmanned Traffic Management
(UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of
such systems has been demonstrated on multiple occasions via real-world
deployments in limited test environments, however exploration of system
behavior under stressing conditions requires the development of appropriate
modeling and simulation (M&S) environments. Autonomy Networks for Advanced
Mobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration
Laboratory (SIL) designed to host federated autonomy networks, such as a UTM or
PSU Network, and to enable test and validation at scales not available in
real-world deployments. As an example of ANAMLL's utility, we explore the
performance of a representative UTM network during a stressing demand scenario.
In a close examination of the demand scenario, ANAMLL demonstrates a UTM system
demand point at which in-flight replanning can no longer be accomplished within
an allowable time window. In a second analysis of the same scenario, ANAMLL
demonstrates the impact of network connectivity performance on end-user
airspace access.

</details>


### [26] [Enhancing 6G Network Security and Incident Response through Integrated VNF and SDN Technologies](https://arxiv.org/abs/2509.08274)
*Abdul Razaque,Abitkhanova Zhadyra Abitkhanovna*

Main category: cs.NI

TL;DR: 本文提出了一种结合VNF和SDN技术的VNFSDN架构，用于提升6G网络的安全响应能力，解决低速网络对安全事件响应的负面影响。


<details>
  <summary>Details</summary>
Motivation: 低速网络会降低团队协作效率、延迟威胁检测、导致行动效率低下并增加安全风险，这些问题在6G网络时代变得更加突出，需要新的技术解决方案。

Method: 通过整合虚拟网络功能(VNF)和软件定义网络(SDN)技术，构建虚拟网络功能服务交付网络(VNFSDN)，利用SDN的动态可编程性和VNF的灵活性来实时分析6G网络产生的大量数据。

Result: VNFSDN能够动态适应变化的安全需求和网络条件，显著提升网络安全效率和效果，降低漏洞风险，增强网络韧性，并支持主动威胁检测和攻击影响缓解。

Conclusion: VNFSDN架构通过VNF和SDN技术的结合，为6G网络提供了有效的安全解决方案，未来结合机器学习和人工智能可以进一步提升网络安全和威胁检测能力。

Abstract: Low-speed internet can negatively affect incident response in a number of
ways, including decreased teamwork, delayed detection, inefficient action, and
elevated risk. Delayed data acquisition and processing may result from
inadequate internet connectivity, hindering security teams' ability to obtain
the necessary information for timely and effective responses. Each of these
factors may augment the organization's susceptibility to security incidents and
their subsequent ramifications. This article establishes a virtual network
function service delivery network (VNFSDN) through the integration of virtual
network function (VNF) and software-defined networking (SDN) technologies. The
VNFSDN approach enhances network security effectiveness and efficiency while
reducing the danger of breaches. This method assists security services in
rapidly assessing vast quantities of data generated by 6G networks. VNFSDN
adapts dynamically to changing safety requirements and connection conditions
through the use of SDN and VNF. This flexibility enables enterprises to
mitigate or halt the impact of cyberattacks by swiftly identifying and
addressing security threats. The VNFSDN enhances network resilience, allowing
operators to proactively mitigate possible security attacks and minimize
downtime. The incorporation of machine learning and artificial intelligence
into VNFSDN can significantly improve network security and threat detection
capabilities. The VNFSDN integrates VNF and SDN technologies to deliver
security services that analyze vast quantities of 6G data in real time. As
security requirements and network conditions evolve, it adapts dynamically to
enhance network resilience and facilitate proactive threat detection.

</details>


### [27] [Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution](https://arxiv.org/abs/2509.08400)
*Xingkun Yin,Feiran You,Hongyang Du,Kaibin Huang*

Main category: cs.NI

TL;DR: 提出了泛在智能范式，通过无线网络与大型语言模型的协同进化，实现可扩展的持续智能提升，构建自改进系统。


<details>
  <summary>Details</summary>
Motivation: 传统静态模型部署无法适应动态环境，需要一种能够持续学习和进化的智能系统范式，特别是在资源受限的多样化环境中。

Method: 通过无线网络支持系统编排的终身学习，同时利用LLMs驱动下一代自适应和响应式网络的发展，实现网络与模型的协同进化。

Result: 建立了无线网络驱动的生态系统，支持LLMs的可扩展和持续智能提升，形成了自改进系统的框架。

Conclusion: 泛在智能范式代表了向自改进系统的转变，能够在多样化和资源受限的环境中持续维持能力增长，为下一代智能系统发展指明了方向。

Abstract: We introduce ubiquitous intelligence as a paradigm where Large Language
Models (LLMs) evolve within wireless network-driven ecosystems. Unlike static
model deployments, this approach enables scalable and continuous intelligence
ascension through coordination between networks and LLMs. Wireless networks
support system-orchestrated lifelong learning, while LLMs drive the
next-generation network development that is more adaptive and responsive. This
co-evolution highlights a shift toward self-improving systems, sustaining
capability growth across diverse and resource-constrained environments.

</details>


### [28] [SKYLINK: Scalable and Resilient Link Management in LEO Satellite Network](https://arxiv.org/abs/2509.08455)
*Wanja de Sombre,Arash Asadi,Debopam Bhattacherjee,Deepak Vasisht,Andrea Ortiz*

Main category: cs.NI

TL;DR: SKYLINK是一种完全分布式的学习策略，用于LEO卫星网络中的链路管理，能够显著降低延迟和丢包率，同时保持计算复杂度恒定。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络的高移动性、动态流量模式和潜在链路故障对高效弹性路由提出了重大挑战，需要解决传统方法在规模化和实时响应方面的不足。

Method: 将LEO卫星网络建模为时变图，提出SKYLINK分布式学习策略，使每个卫星能够独立实时分配流量，适应时变网络条件。

Result: 相比弯管方法，SKYLINK将平均延迟和丢包率的加权和降低了29%；相比Dijkstra算法降低了92%；丢包率相比k最短路径降低95%，相比Dijkstra降低99%，相比弯管基线降低74%；吞吐量最高提升46%。

Conclusion: SKYLINK在保持低通信开销和计算复杂度的同时，能够实现实时响应、扩展到数百万用户规模，并对网络故障具有弹性，是LEO卫星网络链路管理的有效解决方案。

Abstract: The rapid growth of space-based services has established LEO satellite
networks as a promising option for global broadband connectivity.
Next-generation LEO networks leverage inter-satellite links (ISLs) to provide
faster and more reliable communications compared to traditional bent-pipe
architectures, even in remote regions. However, the high mobility of
satellites, dynamic traffic patterns, and potential link failures pose
significant challenges for efficient and resilient routing. To address these
challenges, we model the LEO satellite network as a time-varying graph
comprising a constellation of satellites and ground stations. Our objective is
to minimize a weighted sum of average delay and packet drop rate. Each
satellite independently decides how to distribute its incoming traffic to
neighboring nodes in real time. Given the infeasibility of finding optimal
solutions at scale, due to the exponential growth of routing options and
uncertainties in link capacities, we propose SKYLINK, a novel fully distributed
learning strategy for link management in LEO satellite networks. SKYLINK
enables each satellite to adapt to the time-varying network conditions,
ensuring real-time responsiveness, scalability to millions of users, and
resilience to network failures, while maintaining low communication overhead
and computational complexity. To support the evaluation of SKYLINK at global
scale, we develop a new simulator for large-scale LEO satellite networks. For
25.4 million users, SKYLINK reduces the weighted sum of average delay and drop
rate by 29% compared to the bent-pipe approach, and by 92% compared to
Dijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99%
relative to Dijkstra, and 74% compared to the bent-pipe baseline, while
achieving up to 46% higher throughput. At the same time, SKYLINK maintains
constant computational complexity with respect to constellation size.

</details>


### [29] [Design and Development of a Scalable and Energy-Efficient Localization Framework Leveraging LoRa Ranging-Capable Transceivers](https://arxiv.org/abs/2509.08488)
*Hasan Albinsaid,Bodhibrata Mukhopadhyay,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出了一种基于Semtech SX1280 LoRa收发器的节能定位协调框架，通过同步唤醒窗口调度显著降低功耗，单枚纽扣电池可维持9个月待机，定位精度保持在5米内。


<details>
  <summary>Details</summary>
Motivation: 物联网应用中需要精确且节能的定位解决方案，特别是在大规模部署场景如资产标签、农业和智慧城市中。虽然SX1280 LoRa收发器具有低成本、低功耗和精确测距能力，但现有系统缺乏有效的睡眠-唤醒协调和角色分配机制来管理能量消耗。

Method: 设计了一个协调框架，通过调度短暂的同步唤醒窗口，使设备大部分时间处于深度睡眠状态。该框架减少了对精确连续定时的依赖，并缓解了低成本振荡器的漂移问题。开发了符合协议的自定义节点进行验证。

Result: 实验结果表明，节点可以保持在超低功耗模式，定期唤醒检查指令。单枚纽扣电池可支持长达9个月的待机时间，并能按需进行近乎实时的测距操作，同时保持5米内的定位精度。

Conclusion: 该协调框架成功解决了LoRa定位系统中的能耗问题，在保持精确测距能力的同时显著降低了功耗，为大规模物联网部署提供了可行的节能定位解决方案。

Abstract: Precise and energy-efficient localization is a critical requirement in many
Internet of Things (IoT) applications, particularly in large-scale deployments
such as asset tagging, agriculture, and smart cities, where long battery life
and cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver
presents a promising solution for IoT localization. It combines low cost, low
power, and precise ranging capability over distances of up to 1 km. However,
the ranging process requires two devices to be simultaneously active, one
initiating the ranging request and the other responding to it, which can lead
to significant energy expenditure if not properly managed. Despite the
transceiver's excellent performance, no existing system-level framework
effectively manages sleep-wake coordination and role assignment needed for
energy-efficient operation. This paper presents a coordination framework that
significantly reduces power consumption while maintaining the inherent precise
ranging capability of the chip. The framework schedules short, synchronized
wake-up windows between the initiator and the responder, allowing devices to
remain in deep sleep for most of their duty cycle. This scheduling strategy
minimizes reliance on precise continuous timing and mitigates drift in low-cost
oscillators. To validate the framework, we designed and developed custom nodes
that are compliant with the framework's protocol. Experimental results show
that the proposed approach allows a node to stay in ultra-low power mode and
wake periodically to check for instructions. The node can remain in standby
mode for up to nine months on a single coin cell battery and can perform
ranging operations on demand in near real-time, all while maintaining a
localization accuracy within five meters.

</details>


### [30] [The Role of Legacy Mobile Networks in Infrastructure Resilience: Evidence from the Southern Brazil Flood](https://arxiv.org/abs/2509.08595)
*Daniel Meyer,Lisandro Z Granville,Leandro M. Bertholdo*

Main category: cs.NI

TL;DR: 本文分析了2024年5月巴西南里奥格兰德州洪水期间移动通信网络的韧性表现，发现现代4G/5G网络在灾害中极其脆弱，而传统2G/3G技术在维持基本连接方面发挥了关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究极端洪水事件对移动通信网络的影响，识别网络中断的主要原因，为未来灾害应对提供基础设施规划建议。

Method: 基于监管数据和运营商技术洞察进行分析，重点关注洪水事件期间的网络性能表现和中断原因。

Result: 发现移动网络中断主要源于洪水和长期停电，现代4G/5G网络在灾害中表现出显著脆弱性，而传统2G/3G技术成为维持基本连接的关键支撑。

Conclusion: 强调需要进行灾害感知的基础设施规划，重视传统系统的持续重要性，采用多样化的电力供应策略和韧性网络设计，以提升未来危机中的服务连续性。

Abstract: This paper investigates the resilience of mobile communication networks
during the extreme flooding that affected Rio Grande do Sul, Brazil, in May
2024. Based on regulatory data and technical insights from operators, the study
identifies the leading causes of mobile network disruptions, primarily related
to flooding and prolonged power outages. The results reveal the significant
vulnerability of modern networks (4G/5G) during the event and the essential
role played by legacy technologies (2G/3G) in sustaining basic connectivity
under adverse conditions. The findings underscore the necessity of
disaster-aware infrastructure planning, taking into account the ongoing
significance of legacy systems, diversified power supply strategies, and
resilient network designs to enhance service continuity during future crises.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization](https://arxiv.org/abs/2509.07993)
*Federico Fontana,Anxhelo Diko,Romeo Lanzino,Marco Raoul Marini,Bachir Kaddar,Gian Luca Foresti,Luigi Cinque*

Main category: cs.LG

TL;DR: 将深度伪造检测重新定义为持续学习问题，提出高效框架来增量适应新兴视觉操纵技术，同时保留历史生成器知识。


<details>
  <summary>Details</summary>
Motivation: 深度伪造生成技术快速发展，非持续学习方法需要频繁且昂贵的重新训练，现有方法依赖不真实的模拟序列。

Method: 基于轻量级视觉骨干网络构建框架，模拟7年间深度伪造技术的真实时间演变，提出两个新评估指标C-AUC和FWT-AUC。

Result: 实现高效适应（比完全重新训练快155倍）和稳健的历史知识保留，但对未来生成器的泛化能力接近随机水平（FWT-AUC≈0.5）。

Conclusion: 提出非通用深度伪造分布假说，表明当前方法对未来生成器的泛化存在根本性挑战，需要新的研究方向。

Abstract: The rapid evolution of deepfake generation technologies poses critical
challenges for detection systems, as non-continual learning methods demand
frequent and expensive retraining. We reframe deepfake detection (DFD) as a
Continual Learning (CL) problem, proposing an efficient framework that
incrementally adapts to emerging visual manipulation techniques while retaining
knowledge of past generators. Our framework, unlike prior approaches that rely
on unreal simulation sequences, simulates the real-world chronological
evolution of deepfake technologies in extended periods across 7 years.
Simultaneously, our framework builds upon lightweight visual backbones to allow
for the real-time performance of DFD systems. Additionally, we contribute two
novel metrics: Continual AUC (C-AUC) for historical performance and Forward
Transfer AUC (FWT-AUC) for future generalization. Through extensive
experimentation (over 600 simulations), we empirically demonstrate that while
efficient adaptation (+155 times faster than full retraining) and robust
retention of historical knowledge is possible, the generalization of current
approaches to future generators without additional training remains near-random
(FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing
generator. Such observations are the foundation of our newly proposed
Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}

</details>


### [32] [How Far Are We from True Unlearnability?](https://arxiv.org/abs/2509.08058)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.LG

TL;DR: 本文发现现有不可学习样本方法在多任务场景下失效，从模型优化角度分析损失景观与不可学习性的关系，提出SAL指标和UD距离来量化不可学习性，并评估现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有不可学习样本方法在跨任务场景中表现不佳，无法真正保护数据所有者权益，需要深入理解不可学习性的本质并开发有效的评估指标。

Method: 通过观察清洁模型和中毒模型的收敛过程差异，分析损失景观特征，提出Sharpness-Aware Learnability (SAL)指标来量化参数不可学习性，并基于此定义Unlearnable Distance (UD)来评估数据不可学习性。

Result: 发现现有不可学习方法在多任务数据集Taskonomy上失效，损失景观分析揭示了参数优化路径的差异，提出的UD指标能够有效评估不同不可学习方法的性能边界。

Conclusion: 当前不可学习方法距离真正的不可学习性还有差距，需要基于损失景观理解开发更有效的保护方法，提出的SAL和UD指标为社区提供了新的评估工具。

Abstract: High-quality data plays an indispensable role in the era of large models, but
the use of unauthorized data for model training greatly damages the interests
of data owners. To overcome this threat, several unlearnable methods have been
proposed, which generate unlearnable examples (UEs) by compromising the
training availability of data. Clearly, due to unknown training purposes and
the powerful representation learning capabilities of existing models, these
data are expected to be unlearnable for models across multiple tasks, i.e.,
they will not help improve the model's performance. However, unexpectedly, we
find that on the multi-task dataset Taskonomy, UEs still perform well in tasks
such as semantic segmentation, failing to exhibit cross-task unlearnability.
This phenomenon leads us to question: How far are we from attaining truly
unlearnable examples? We attempt to answer this question from the perspective
of model optimization. To this end, we observe the difference in the
convergence process between clean and poisoned models using a simple model
architecture. Subsequently, from the loss landscape we find that only a part of
the critical parameter optimization paths show significant differences,
implying a close relationship between the loss landscape and unlearnability.
Consequently, we employ the loss landscape to explain the underlying reasons
for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the
unlearnability of parameters based on this explanation. Furthermore, we propose
an Unlearnable Distance (UD) to measure the unlearnability of data based on the
SAL distribution of parameters in clean and poisoned models. Finally, we
conduct benchmark tests on mainstream unlearnable methods using the proposed
UD, aiming to promote community awareness of the capability boundaries of
existing unlearnable methods.

</details>


### [33] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

TL;DR: JEL是一个新颖的计算高效端到端多神经网络实体链接模型，在性能上超越了当前最先进模型


<details>
  <summary>Details</summary>
Motivation: 知识图谱需要将文本中的提及与实体正确链接，这在新闻分析等应用中至关重要。摩根大通内部有25个团队需要新闻分析解决方案，每年花费超过200万美元外部供应商成本，实体链接能够桥接非结构化新闻文本与知识图谱

Method: 端到端多神经网络架构，计算效率高

Result: 超越了当前最先进的实体链接模型

Conclusion: JEL模型在实体链接任务上表现出色，为知识图谱与文本数据的集成提供了有效解决方案

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>


### [34] [Performance Assessment Strategies for Generative AI Applications in Healthcare](https://arxiv.org/abs/2509.08087)
*Victor Garcia,Mariia Sidulova,Aldo Badano*

Main category: cs.LG

TL;DR: 本文讨论了医疗领域生成式AI评估的现状、局限性和新兴方法，强调需要超越传统定量基准的评估策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域的应用日益广泛，但现有评估方法主要依赖定量基准，存在过拟合和泛化性不足的问题，需要更全面的评估框架。

Method: 分析当前评估方法的局限性，探讨利用人类专业知识和成本效益计算模型作为评估器的新兴评估策略。

Result: 识别了传统定量基准评估的缺陷，提出了结合人类专业知识和计算模型的综合评估方法，为医疗GenAI应用提供更可靠的性能评估框架。

Conclusion: 医疗领域生成式AI的评估需要多维度方法，结合定量指标和人类专家判断，以确保模型在实际临床环境中的有效性和安全性。

Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm
within artificial intelligence, with applications throughout the medical
enterprise. Assessing GenAI applications necessitates a comprehensive
understanding of the clinical task and awareness of the variability in
performance when implemented in actual clinical environments. Presently, a
prevalent method for evaluating the performance of generative models relies on
quantitative benchmarks. Such benchmarks have limitations and may suffer from
train-to-the-test overfitting, optimizing performance for a specified test set
at the cost of generalizability across other task and data distributions.
Evaluation strategies leveraging human expertise and utilizing cost-effective
computational models as evaluators are gaining interest. We discuss current
state-of-the-art methodologies for assessing the performance of GenAI
applications in healthcare and medical devices.

</details>


### [35] [Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning](https://arxiv.org/abs/2509.08089)
*Lucas Fenaux,Zheng Wang,Jacob Yan,Nathan Chung,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习后门攻击方法，能够以1-2个恶意客户端攻破现有防御，并提出了Hammer and Anvil防御框架，其中Krum+防御能有效对抗新型自适应攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习分布式特性使其容易受到恶意客户端的后门攻击，现有防御在面对自适应攻击者时效果有限，需要更强大的防御机制。

Method: 首先设计了一种超越现有能力的新型自适应攻击者，然后提出了Hammer and Anvil防御框架，结合两种原理正交的防御方法，通过参数优化产生能对抗任何攻击的联合防御。

Result: 新型攻击只需20个客户端中的1-2个恶意客户端就能攻破现有最先进防御；提出的Krum+联合防御成功抵御了新型自适应攻击和最先进的攻击方法。

Conclusion: 该研究展示了联邦学习后门攻击防御的脆弱性，提出了有效的联合防御方案Krum+，为对抗自适应攻击提供了新的解决方案。

Abstract: Federated Learning is a distributed learning technique in which multiple
clients cooperate to train a machine learning model. Distributed settings
facilitate backdoor attacks by malicious clients, who can embed malicious
behaviors into the model during their participation in the training process.
These malicious behaviors are activated during inference by a specific trigger.
No defense against backdoor attacks has stood the test of time, especially
against adaptive attackers, a powerful but not fully explored category of
attackers. In this work, we first devise a new adaptive adversary that
surpasses existing adversaries in capabilities, yielding attacks that only
require one or two malicious clients out of 20 to break existing
state-of-the-art defenses. Then, we present Hammer and Anvil, a principled
defense approach that combines two defenses orthogonal in their underlying
principle to produce a combined defense that, given the right set of
parameters, must succeed against any attack. We show that our best combined
defense, Krum+, is successful against our new adaptive adversary and
state-of-the-art attacks.

</details>


### [36] [Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography](https://arxiv.org/abs/2509.08116)
*Nooshin Maghsoodi,Sarah Nassar,Paul F R Wilson,Minh Nguyen Nhat To,Sophia Mannina,Shamel Addas,Stephanie Sibley,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: PhysioCLR是一种生理感知的对比学习框架，通过整合ECG生理学先验知识来提升心律失常分类的泛化能力和临床相关性，在多个数据集上相对最强基线平均AUROC提升12%。


<details>
  <summary>Details</summary>
Motivation: 心电图(ECG)在心脏病诊断中至关重要，但基于AI的ECG分析受限于标注数据稀缺。自监督学习可以利用大规模未标注数据，但现有方法缺乏对ECG生理学特性的考虑。

Method: 提出PhysioCLR框架，在预训练阶段学习将具有相似临床特征的样本嵌入拉近，不相似样本推远。整合ECG生理相似性线索到对比学习中，提出ECG特定的数据增强方法保持类别不变性，并使用混合损失函数优化表示质量。

Result: 在Chapman、Georgia两个公共ECG数据集和私有ICU数据集上评估，PhysioCLR相对最强基线平均AUROC提升12%，显示出强大的跨数据集泛化能力。

Conclusion: 通过将生理学知识嵌入对比学习，PhysioCLR能够学习具有临床意义且可迁移的ECG特征，展示了生理学信息自监督学习在ECG诊断中的潜力。

Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart
conditions; however, the effectiveness of artificial intelligence (AI)-based
ECG analysis is often hindered by the limited availability of labeled data.
Self-supervised learning (SSL) can address this by leveraging large-scale
unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning
Representation for ECG), a physiology-aware contrastive learning framework that
incorporates domain-specific priors to enhance the generalizability and
clinical relevance of ECG-based arrhythmia classification. Methods: During
pretraining, PhysioCLR learns to bring together embeddings of samples that
share similar clinically relevant features while pushing apart those that are
dissimilar. Unlike existing methods, our method integrates ECG physiological
similarity cues into contrastive learning, promoting the learning of clinically
meaningful representations. Additionally, we introduce ECG- specific
augmentations that preserve the ECG category post augmentation and propose a
hybrid loss function to further refine the quality of learned representations.
Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,
for multilabel ECG diagnoses, as well as a private ICU dataset labeled for
binary classification. Across the Chapman, Georgia, and private cohorts,
PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,
underscoring its robust cross-dataset generalization. Conclusion: By embedding
physiological knowledge into contrastive learning, PhysioCLR enables the model
to learn clinically meaningful and transferable ECG eatures. Significance:
PhysioCLR demonstrates the potential of physiology-informed SSL to offer a
promising path toward more effective and label-efficient ECG diagnostics.

</details>


### [37] [Optimization Methods and Software for Federated Learning](https://arxiv.org/abs/2509.08120)
*Konstantin Burlachenko*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习的五个关键挑战（数据设备异构性、通信问题、隐私保护等），提出了新方法连接理论进展与实际应用，并探讨了从实践反馈到理论设计的反向过程。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在去中心化环境中面临数据异构、设备差异、通信限制和隐私保护等独特挑战，需要将理论方法转化为实际可用的系统实现。

Method: 识别联邦学习的五个核心挑战，提出新颖算法和系统解决方案，建立理论进展与实际实现之间的桥梁，并研究从实践反馈到理论设计的反向适应过程。

Result: 开发了先进的联邦学习算法和系统，增强了理论方法在现实世界中的适用性，为研究者提供了将理论转化为高效实现的指南。

Conclusion: 该研究不仅推进了联邦学习的技术发展，更重要的是建立了理论与实践之间的双向反馈机制，通过实践视角深入揭示了算法的内在机制和灵活性，为未来研究提供了新的维度。

Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning
paradigm where multiple clients, such as mobile devices, collaborate to solve
machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al.
(2016a,b); McMahan et al. (2017), FL has gained further attention through its
inclusion in the National AI Research and Development Strategic Plan (2023
Update) of the United States (Science and on Artificial Intelligence, 2023).
The FL training process is inherently decentralized and often takes place in
less controlled settings compared to data centers, posing unique challenges
distinct from those in fully controlled environments. In this thesis, we
identify five key challenges in Federated Learning and propose novel approaches
to address them. These challenges arise from the heterogeneity of data and
devices, communication issues, and privacy concerns for clients in FL training.
Moreover, even well-established theoretical advances in FL require diverse
forms of practical implementation to enhance their real-world applicability.
Our contributions advance FL algorithms and systems, bridging theoretical
advancements and practical implementations. More broadly, our work serves as a
guide for researchers navigating the complexities of translating theoretical
methods into efficient real-world implementations and software. Additionally,
it offers insights into the reverse process of adapting practical
implementation aspects back into theoretical algorithm design. This reverse
process is particularly intriguing, as the practical perspective compels us to
examine the underlying mechanics and flexibilities of algorithms more deeply,
often uncovering new dimensions of the algorithms under study.

</details>


### [38] [In-Context Learning Enhanced Credibility Transformer](https://arxiv.org/abs/2509.08122)
*Kishan Padayachy,Ronald Richman,Salvatore Scognamiglio,Mario V. Wüthrich*

Main category: cs.LG

TL;DR: 本文提出了一种基于Credibility Transformer的新范式，通过添加上下文学习机制来增强模型性能，能够处理训练时未见过的类别特征。


<details>
  <summary>Details</summary>
Motivation: 扩展经典Transformer架构，通过可信度机制和上下文学习来提高模型学习能力和预测性能，特别是处理训练数据中未出现的新类别特征。

Method: 在Credibility Transformer基础上增加上下文学习机制，使用相似实例组成的上下文批次来增强CLS令牌表示，通过额外的上下文信息和微调来改进模型。

Result: 实证验证表明，上下文学习通过适应相似风险模式提高了预测准确性，并且使模型能够泛化到训练时未见过的类别特征水平的新实例。

Conclusion: 提出的上下文学习机制有效增强了Credibility Transformer的性能，使其能够处理现实世界中的新类别特征问题，具有重要的实际应用价值。

Abstract: The starting point of our network architecture is the Credibility Transformer
which extends the classical Transformer architecture by a credibility mechanism
to improve model learning and predictive performance. This Credibility
Transformer learns credibilitized CLS tokens that serve as learned
representations of the original input features. In this paper we present a new
paradigm that augments this architecture by an in-context learning mechanism,
i.e., we increase the information set by a context batch consisting of similar
instances. This allows the model to enhance the CLS token representations of
the instances by additional in-context information and fine-tuning. We
empirically verify that this in-context learning enhances predictive accuracy
by adapting to similar risk patterns. Moreover, this in-context learning also
allows the model to generalize to new instances which, e.g., have feature
levels in the categorical covariates that have not been present when the model
was trained -- for a relevant example, think of a new vehicle model which has
just been developed by a car manufacturer.

</details>


### [39] [Sketched Gaussian Mechanism for Private Federated Learning](https://arxiv.org/abs/2509.08195)
*Qiaobo Li,Zhijie Chen,Arindam Banerjee*

Main category: cs.LG

TL;DR: 论文提出了Sketched Gaussian Mechanism (SGM)，将梯度压缩和差分隐私机制结合，通过联合分析提供了比单独分析更灵活和更强的隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信成本和隐私保护是两个重要问题。现有研究对梯度压缩和差分隐私机制的分析是孤立的，需要一种联合分析方法来获得更好的隐私保护效果。

Method: 提出SGM机制，直接结合梯度压缩（sketching）和高斯机制（Gaussian mechanism），使用Rényi-DP工具进行联合隐私分析，证明隐私级别与压缩维度b的平方根成反比。

Result: 理论证明SGM在相同噪声预算下能提供比原始高斯机制更强的隐私保证，实验表明基于SGM的联邦学习在相同隐私级别下至少与非压缩的私有FL变体相当，在某些设置中表现更优。

Conclusion: SGM机制有效结合了梯度压缩和差分隐私，提供了更强的隐私保护，同时保持了通信效率，自适应优化器的使用进一步提升了性能。

Abstract: Communication cost and privacy are two major considerations in federated
learning (FL). For communication cost, gradient compression by sketching the
clients' transmitted model updates is often used for reducing per-round
communication. For privacy, the Gaussian mechanism (GM), which consists of
clipping updates and adding Gaussian noise, is commonly used to guarantee
client-level differential privacy. Existing literature on private FL analyzes
privacy of sketching and GM in an isolated manner, illustrating that sketching
provides privacy determined by the sketching dimension and that GM has to
supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which
directly combines sketching and the Gaussian mechanism for privacy. Using
R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy
guarantee, which is significantly more flexible and sharper compared to
isolated analysis of sketching and GM privacy. In particular, we prove that the
privacy level of SGM for a fixed noise magnitude is proportional to
$1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for
moderate $b$) SGM can provide much stronger privacy guarantees than the
original GM under the same noise budget. We demonstrate the application of SGM
to FL with either gradient descent or adaptive server optimizers, and establish
theoretical results on optimization convergence, which exhibits only a
logarithmic dependence on the number of parameters $d$. Experimental results
confirm that at the same privacy level, SGM based FL is at least competitive
with non-sketching private FL variants and outperforms them in some settings.
Moreover, using adaptive optimization at the server improves empirical
performance while maintaining the privacy guarantees.

</details>


### [40] [torchmil: A PyTorch-based library for deep Multiple Instance Learning](https://arxiv.org/abs/2509.08129)
*Francisco M. Castro-Macías,Francisco J. Sáez-Maldonado,Pablo Morales-Álvarez,Rafael Molina*

Main category: cs.LG

TL;DR: torchmil是一个基于PyTorch的开源Python库，为多实例学习（MIL）提供标准化工具，包括模型构建模块、统一数据格式和基准数据集，旨在促进MIL研究的可重复性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度MIL方法受到越来越多的关注，但该领域缺乏用于模型开发、评估和比较的标准化工具，这阻碍了研究的可重复性和可访问性。

Method: 开发了一个基于PyTorch的开源Python库torchmil，提供统一的模块化框架，包含MIL模型的基本构建块、标准化数据格式以及精选的基准数据集和模型集合。

Result: 成功创建了torchmil库，包含全面的文档和教程，支持从业者和研究人员使用，库已在https://torchmil.readthedocs.io上提供。

Conclusion: torchmil旨在加速MIL领域的进展，降低新用户的入门门槛，为弱监督学习提供标准化的开发工具。

Abstract: Multiple Instance Learning (MIL) is a powerful framework for weakly
supervised learning, particularly useful when fine-grained annotations are
unavailable. Despite growing interest in deep MIL methods, the field lacks
standardized tools for model development, evaluation, and comparison, which
hinders reproducibility and accessibility. To address this, we present
torchmil, an open-source Python library built on PyTorch. torchmil offers a
unified, modular, and extensible framework, featuring basic building blocks for
MIL models, a standardized data format, and a curated collection of benchmark
datasets and models. The library includes comprehensive documentation and
tutorials to support both practitioners and researchers. torchmil aims to
accelerate progress in MIL and lower the entry barrier for new users. Available
at https://torchmil.readthedocs.io.

</details>


### [41] [From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital](https://arxiv.org/abs/2509.08140)
*Mihir Kumar,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Afriyie Kwesi Samuel,Fuat Alican,Yigit Ihlamur*

Main category: cs.LG

TL;DR: 提出一个结合大语言模型和多模型机器学习架构的框架，用于预测罕见高影响事件，在风险投资领域实现9.8-11.1倍于随机基线的精确度提升


<details>
  <summary>Details</summary>
Motivation: 解决风险投资中早期创业公司数据有限且嘈杂的问题，需要既能提供准确预测又能保持可解释性的方法，以支持可靠的投资决策

Method: 使用LLM进行特征工程从非结构化数据提取复杂信号，然后通过XGBoost、随机森林和线性回归组成的集成模型进行预测，首先生成连续的成功概率估计，再通过阈值处理得到二元罕见事件预测

Result: 模型在三个独立测试子集上达到随机分类器基线9.8-11.1倍的精确度，特征敏感性分析显示创业公司类别列表占预测影响力的15.6%，创始人数量次之，教育水平和领域专业知识也有稳定贡献

Conclusion: 该框架成功整合了黑盒模型的预测能力和可解释性要求，为罕见高影响事件预测提供了有效的解决方案，特别适用于风险投资等数据稀缺但决策重要的领域

Abstract: This paper presents a framework for predicting rare, high-impact outcomes by
integrating large language models (LLMs) with a multi-model machine learning
(ML) architecture. The approach combines the predictive strength of black-box
models with the interpretability required for reliable decision-making. We use
LLM-powered feature engineering to extract and synthesize complex signals from
unstructured data, which are then processed within a layered ensemble of models
including XGBoost, Random Forest, and Linear Regression. The ensemble first
produces a continuous estimate of success likelihood, which is then thresholded
to produce a binary rare-event prediction. We apply this framework to the
domain of Venture Capital (VC), where investors must evaluate startups with
limited and noisy early-stage data. The empirical results show strong
performance: the model achieves precision between 9.8X and 11.1X the random
classifier baseline in three independent test subsets. Feature sensitivity
analysis further reveals interpretable success drivers: the startup's category
list accounts for 15.6% of predictive influence, followed by the number of
founders, while education level and domain expertise contribute smaller yet
consistent effects.

</details>


### [42] [PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints](https://arxiv.org/abs/2509.08750)
*Yuanchun Guo,Bingyan Liu,Yulong Sha,Zhensheng Xian*

Main category: cs.LG

TL;DR: 构建首个系统平台PracMHBench，在边缘设备实际约束下评估模型异构联邦学习算法，通过大量实验分析不同边缘约束下算法的适用性和异构模式


<details>
  <summary>Details</summary>
Motivation: 现有模型异构联邦学习算法缺乏在实际边缘设备约束下的定量分析，需要重新思考和评估这一范式

Method: 构建PracMHBench系统平台，对多种模型异构算法进行分类并在多个数据任务和指标上进行测试，进行大量实验分析

Result: 提供了在不同边缘约束下模型异构联邦学习算法的性能评估和适用性分析

Conclusion: 该研究填补了模型异构联邦学习在实际边缘设备约束下评估的空白，为算法选择和应用提供了实践指导

Abstract: Federating heterogeneous models on edge devices with diverse resource
constraints has been a notable trend in recent years. Compared to traditional
federated learning (FL) that assumes an identical model architecture to
cooperate, model-heterogeneous FL is more practical and flexible since the
model can be customized to satisfy the deployment requirement. Unfortunately,
no prior work ever dives into the existing model-heterogeneous FL algorithms
under the practical edge device constraints and provides quantitative analysis
on various data scenarios and metrics, which motivates us to rethink and
re-evaluate this paradigm. In our work, we construct the first system platform
\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical
constraints of edge devices, where diverse model heterogeneity algorithms are
classified and tested on multiple data tasks and metrics. Based on the
platform, we perform extensive experiments on these algorithms under the
different edge constraints to observe their applicability and the corresponding
heterogeneity pattern.

</details>


### [43] [MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs](https://arxiv.org/abs/2509.08156)
*Swati Swati,Arjun Roy,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: mmm-fair是一个开源工具包，使用基于boosting的集成方法动态优化模型权重，同时最小化分类错误和多种公平性违规，支持多目标优化和交叉偏见检测。


<details>
  <summary>Details</summary>
Motivation: 解决分类任务中性能与公平性的平衡问题，特别是交叉偏见和冲突的公平性定义带来的挑战，现有工具包对多维公平性和相关权衡的支持有限。

Method: 采用基于boosting的集成方法，动态优化模型权重，联合最小化分类错误和多样化的公平性违规，支持灵活的多目标优化。

Result: 开发了一个包含多属性公平性、多目标优化、无代码聊天界面、LLM驱动解释、交互式帕累托探索、自定义公平约束和部署就绪模型的综合工具包。

Conclusion: mmm-fair独特地结合了多维公平性分析和多目标优化能力，提供了现有公平性工具中罕见的完整解决方案，能够可靠地发现最先进方法经常遗漏的交叉偏见。

Abstract: Fairness-aware classification requires balancing performance and fairness,
often intensified by intersectional biases. Conflicting fairness definitions
further complicate the task, making it difficult to identify universally fair
solutions. Despite growing regulatory and societal demands for equitable AI,
popular toolkits offer limited support for exploring multi-dimensional fairness
and related trade-offs. To address this, we present mmm-fair, an open-source
toolkit leveraging boosting-based ensemble approaches that dynamically
optimizes model weights to jointly minimize classification errors and diverse
fairness violations, enabling flexible multi-objective optimization. The system
empowers users to deploy models that align with their context-specific needs
while reliably uncovering intersectional biases often missed by
state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth
multi-attribute fairness, multi-objective optimization, a no-code, chat-based
interface, LLM-powered explanations, interactive Pareto exploration for model
selection, custom fairness constraint definition, and deployment-ready models
in a single open-source toolkit, a combination rarely found in existing
fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.

</details>


### [44] [Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation](https://arxiv.org/abs/2509.08163)
*Ho Ming Lee,Katrien Antonio,Benjamin Avanzi,Lorenzo Marchi,Rui Zhou*

Main category: cs.LG

TL;DR: 提出基于距离协方差的正则化框架来解决回归和分类任务中的多属性公平性问题，特别是针对公平性选区划分问题，能够处理线性和非线性依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法主要关注二分类问题，而回归任务（如保险定价、招聘评分）同样重要。现有方法无法有效处理连续属性（如年龄）和多个受保护属性同时存在时的公平性选区划分问题。

Method: 提出距离协方差正则化框架，减弱模型预测与受保护属性之间的关联。扩展框架包含两种多元依赖度量：联合距离协方差(JdCov)和新提出的连接距离协方差(CCdCov)，可处理各种类型受保护属性的公平性问题。

Result: 在COMPAS累犯数据集和大型机动车保险索赔数据集上应用该框架，有效解决了多属性公平性问题。

Conclusion: 该框架为回归和分类任务中的多属性公平性问题提供了有效解决方案，特别是能够处理公平性选区划分问题，适用于各种类型的受保护属性。

Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as
gender or ethnicity) is a critical issue in machine learning. Most existing
literature focuses on binary classification, but achieving fairness in
regression tasks-such as insurance pricing or hiring score assessments-is
equally important. Moreover, anti-discrimination laws also apply to continuous
attributes, such as age, for which many existing methods are not applicable. In
practice, multiple protected attributes can exist simultaneously; however,
methods targeting fairness across several attributes often overlook so-called
"fairness gerrymandering", thereby ignoring disparities among intersectional
subgroups (e.g., African-American women or Hispanic men). In this paper, we
propose a distance covariance regularisation framework that mitigates the
association between model predictions and protected attributes, in line with
the fairness definition of demographic parity, and that captures both linear
and nonlinear dependencies. To enhance applicability in the presence of
multiple protected attributes, we extend our framework by incorporating two
multivariate dependence measures based on distance covariance: the previously
proposed joint distance covariance (JdCov) and our novel concatenated distance
covariance (CCdCov), which effectively address fairness gerrymandering in both
regression and classification tasks involving protected attributes of various
types. We discuss and illustrate how to calibrate regularisation strength,
including a method based on Jensen-Shannon divergence, which quantifies
dissimilarities in prediction distributions across groups. We apply our
framework to the COMPAS recidivism dataset and a large motor insurance claims
dataset.

</details>


### [45] [MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments](https://arxiv.org/abs/2509.08176)
*Honghui Du,Leandro Minku,Huiyu Zhou*

Main category: cs.LG

TL;DR: MARLINE是一种新颖的多源迁移学习方法，用于处理非平稳环境中的概念漂移问题，即使源概念与目标概念不匹配也能有效利用多源知识


<details>
  <summary>Details</summary>
Motivation: 现有方法假设至少有一个源模型代表与目标概念相似的概念，但这在许多现实场景中不成立。需要一种能在源概念与目标概念不匹配时仍能从多个数据源获益的方法

Method: 通过将目标概念投影到每个源概念的空间中，使多个源子分类器能够作为集成的一部分对目标概念的预测做出贡献

Result: 在多个合成和真实数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确

Conclusion: MARLINE提供了一种有效的解决方案，能够在非平稳环境中利用多源知识，即使源概念与目标概念不完全匹配也能提升预测性能

Abstract: Concept drift is a major problem in online learning due to its impact on the
predictive performance of data stream mining systems. Recent studies have
started exploring data streams from different sources as a strategy to tackle
concept drift in a given target domain. These approaches make the assumption
that at least one of the source models represents a concept similar to the
target concept, which may not hold in many real-world scenarios. In this paper,
we propose a novel approach called Multi-source mApping with tRansfer LearnIng
for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge
from multiple data sources in non-stationary environments even when source and
target concepts do not match. This is achieved by projecting the target concept
to the space of each source concept, enabling multiple source sub-classifiers
to contribute towards the prediction of the target concept as part of an
ensemble. Experiments on several synthetic and real-world datasets show that
MARLINE was more accurate than several state-of-the-art data stream learning
approaches.

</details>


### [46] [The Domain Mixed Unit: A New Neural Arithmetic Layer](https://arxiv.org/abs/2509.08180)
*Paul Curry*

Main category: cs.LG

TL;DR: DMU是一种新的神经算术单元，通过单一参数门在log空间和线性空间表示之间混合，实现加法或减法运算，在NALM基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 为了解决神经算术单元在算术运算泛化能力方面的挑战，特别是在乘法和除法运算上的泛化性能

Method: 提出Domain Mixed Unit (DMU)，学习一个参数门在log空间和线性空间表示之间混合，支持加法(DMU add)和减法(DMU sub)运算，并提供两种初始化方案

Result: 在NALM基准测试中达到最先进性能，在乘法和除法运算上获得所有种子中的最高解决百分比

Conclusion: DMU是一种有效的神经算术单元，能够显著提升算术运算的泛化能力，特别是在乘法和除法任务上表现优异

Abstract: The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a
single parameter gate that mixes between log-space and linear-space
representations while performing either addition (DMU add) or subtraction (DMU
sub). Two initializations are proposed for the DMU: one covering addition and
multiplication, and another covering subtraction and division. The DMU achieves
state-of-the-art performance on the NALM Benchmark, a dataset designed to test
the ability of neural arithmetic units to generalize arithmetic operations,
specifically performing with the highest percentage solved over all seeds on
multiplication and division. The DMU will be submitted as a pull request to the
open-source NALM benchmark, and its code is available on GitHub at
https://github.com/marict?tab=repositories

</details>


### [47] [Multi-Label Transfer Learning in Non-Stationary Data Streams](https://arxiv.org/abs/2509.08181)
*Honghui Du,Leandro Minku,Aonghus Lawlor,Huiyu Zhou*

Main category: cs.LG

TL;DR: 提出了两种新的多标签数据流迁移学习方法：BR-MARLENE利用源流和目标流中不同标签的知识进行多标签分类；BRPW-MARLENE进一步显式建模和迁移标签间依赖关系以提高性能。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中的标签概念在非平稳环境中经常发生漂移，现有研究对多标签数据流迁移学习的关注有限，希望通过标签间知识迁移来加速适应过程。

Method: BR-MARLENE方法利用源流和目标流中不同标签的知识进行迁移学习；BRPW-MARLENE在此基础上显式建模和迁移标签间的成对依赖关系。

Result: 综合实验表明，两种方法在非平稳环境中都优于最先进的多标签流处理方法，证明了标签间知识迁移对提高预测性能的有效性。

Conclusion: 提出的两种迁移学习方法能够有效处理多标签数据流中的概念漂移问题，通过标签间知识迁移显著提升了在非平稳环境中的预测性能。

Abstract: Label concepts in multi-label data streams often experience drift in
non-stationary environments, either independently or in relation to other
labels. Transferring knowledge between related labels can accelerate
adaptation, yet research on multi-label transfer learning for data streams
remains limited. To address this, we propose two novel transfer learning
methods: BR-MARLENE leverages knowledge from different labels in both source
and target streams for multi-label classification; BRPW-MARLENE builds on this
by explicitly modelling and transferring pairwise label dependencies to enhance
learning performance. Comprehensive experiments show that both methods
outperform state-of-the-art multi-label stream approaches in non-stationary
environments, demonstrating the effectiveness of inter-label knowledge transfer
for improved predictive performance.

</details>


### [48] [Selective Induction Heads: How Transformers Select Causal Structures In Context](https://arxiv.org/abs/2509.08184)
*Francesco D'Angelo,Francesco Croce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文提出了一个新颖框架，展示Transformer动态处理因果结构的能力，通过交错马尔可夫链揭示选择性归纳头的形成机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖固定因果结构的马尔可夫链，无法捕捉自然语言中token关系随上下文动态变化的复杂性。

Method: 使用具有不同滞后的交错马尔可夫链来变化因果结构，同时保持转移概率固定，从而研究选择性归纳头的形成。

Result: Transformer学会了通过识别正确滞后并从过去复制相应token来预测下一个token，构建了3层Transformer实现选择性归纳头。

Conclusion: 该机制渐近收敛于最大似然解，增进了对Transformer如何选择因果结构的理解，为其功能和可解释性提供了新见解。

Abstract: Transformers have exhibited exceptional capabilities in sequence modeling
tasks, leveraging self-attention and in-context learning. Critical to this
success are induction heads, attention circuits that enable copying tokens
based on their previous occurrences. In this work, we introduce a novel
framework that showcases transformers' ability to dynamically handle causal
structures. Existing works rely on Markov Chains to study the formation of
induction heads, revealing how transformers capture causal dependencies and
learn transition probabilities in-context. However, they rely on a fixed causal
structure that fails to capture the complexity of natural languages, where the
relationship between tokens dynamically changes with context. To this end, our
framework varies the causal structure through interleaved Markov chains with
different lags while keeping the transition probabilities fixed. This setting
unveils the formation of Selective Induction Heads, a new circuit that endows
transformers with the ability to select the correct causal structure
in-context. We empirically demonstrate that transformers learn this mechanism
to predict the next token by identifying the correct lag and copying the
corresponding token from the past. We provide a detailed construction of a
3-layer transformer to implement the selective induction head, and a
theoretical analysis proving that this mechanism asymptotically converges to
the maximum likelihood solution. Our findings advance the understanding of how
transformers select causal structures, providing new insights into their
functioning and interpretability.

</details>


### [49] [ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis](https://arxiv.org/abs/2509.08188)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 本研究比较了条件WGAN-GP和1D去噪扩散模型在生成真实EEG伪迹数据方面的表现，发现WGAN-GP在频谱对齐方面表现更好，但两种模型在类别条件恢复方面都较弱，限制了数据增强效果。


<details>
  <summary>Details</summary>
Motivation: EEG中的伪迹（肌肉、眼动、电极等）会干扰自动化分析，但大规模标注成本高昂。研究探索现代生成模型是否能合成真实、标签感知的伪迹片段用于数据增强和压力测试。

Method: 使用TUH EEG伪迹语料库，整理受试者分割和固定长度多通道窗口。比较条件WGAN-GP（带投影判别器）和1D去噪扩散模型（带分类器自由引导），从保真度、特异性和实用性三个维度评估。

Result: WGAN-GP实现了更接近真实数据的频谱对齐和更低的最大均值差异，但两种模型都表现出较弱的类别条件恢复能力，限制了即时数据增强效果。

Conclusion: 研究揭示了在更强条件约束和覆盖范围方面的改进机会，并发布了可复现的流水线作为EEG伪迹合成的基线，为未来工作提供了可操作的失败模式分析。

Abstract: Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,
chewing, and shiver -- confound automated analysis yet are costly to label at
scale. We study whether modern generative models can synthesize realistic,
label-aware artifact segments suitable for augmentation and stress-testing.
Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and
fixed-length multi-channel windows (e.g., 250 samples) with preprocessing
tailored to each model (per-window min-max for adversarial training;
per-recording/channel $z$-score for diffusion). We compare a conditional
WGAN-GP with a projection discriminator to a 1D denoising diffusion model with
classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch
band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$),
channel-covariance Frobenius distance, autocorrelation $L_2$, and
distributional metrics (MMD/PRD); (ii) specificity via class-conditional
recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation
effects on artifact recognition. In our setting, WGAN-GP achieves closer
spectral alignment and lower MMD to real data, while both models exhibit weak
class-conditional recovery, limiting immediate augmentation gains and revealing
opportunities for stronger conditioning and coverage. We release a reproducible
pipeline -- data manifests, training configurations, and evaluation scripts --
to establish a baseline for EEG artifact synthesis and to surface actionable
failure modes for future work.

</details>


### [50] [Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics](https://arxiv.org/abs/2509.08191)
*Robert Stephany,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种新的降阶模型方法，通过引入灵活的高阶有限差分格式和Rollout损失函数，解决了传统ROM在长时间预测中精度下降的问题，并在2D Burgers方程上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统降阶模型(ROM)虽然能快速求解参数化偏微分方程族，但在长时间预测时精度会显著下降，需要新的方法来保持长期预测的准确性。

Method: 1) 引入灵活、高阶且计算成本低的有限差分格式；2) 提出Rollout损失函数来训练ROM，使其能够在任意时间范围内做出准确预测

Result: 在2D Burgers方程上验证了所提方法的有效性，证明了该方法能够显著改善长时间预测的精度

Conclusion: 该方法通过结合高阶有限差分格式和专门的Rollout损失训练策略，成功解决了ROM在长时间预测中的精度退化问题，为复杂偏微分方程的高效求解提供了新思路

Abstract: Solving complex partial differential equations is vital in the physical
sciences, but often requires computationally expensive numerical methods.
Reduced-order models (ROMs) address this by exploiting dimensionality reduction
to create fast approximations. While modern ROMs can solve parameterized
families of PDEs, their predictive power degrades over long time horizons. We
address this by (1) introducing a flexible, high-order, yet inexpensive
finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to
make accurate predictions over arbitrary time horizons. We demonstrate our
approach on the 2D Burgers equation.

</details>


### [51] [Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization](https://arxiv.org/abs/2509.08194)
*Caio de Prospero Iglesias,Kimberly Villalobos Carballo,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出了Prescribe-then-Select框架，通过构建候选策略库并学习元策略来选择最优策略，在上下文随机优化中实现数据驱动的策略选择


<details>
  <summary>Details</summary>
Motivation: 解决上下文随机优化中的策略选择问题，当多个候选策略在不同协变量空间表现各异且没有单一主导策略时，需要一种方法来选择最优策略

Method: Prescribe-then-Select模块化框架：先构建可行候选策略库，然后学习元策略选择最佳策略；使用交叉验证训练的最优策略树集成实现元策略

Result: 在两个基准CSO问题（单阶段报童问题和两阶段运输规划）中，PS在协变量空间异质区域始终优于最佳单一策略，在无异质时收敛到主导策略

Conclusion: PS框架提供了一种有效的数据驱动方法来解决上下文随机优化中的策略选择问题，能够适应协变量空间的异质性并实现最优性能

Abstract: We address the problem of policy selection in contextual stochastic
optimization (CSO), where covariates are available as contextual information
and decisions must satisfy hard feasibility constraints. In many CSO settings,
multiple candidate policies--arising from different modeling paradigms--exhibit
heterogeneous performance across the covariate space, with no single policy
uniformly dominating. We propose Prescribe-then-Select (PS), a modular
framework that first constructs a library of feasible candidate policies and
then learns a meta-policy to select the best policy for the observed
covariates. We implement the meta-policy using ensembles of Optimal Policy
Trees trained via cross-validation on the training set, making policy choice
entirely data-driven. Across two benchmark CSO problems--single-stage
newsvendor and two-stage shipment planning--PS consistently outperforms the
best single policy in heterogeneous regimes of the covariate space and
converges to the dominant policy when such heterogeneity is absent. All the
code to reproduce the results can be found at
https://anonymous.4open.science/r/Prescribe-then-Select-TMLR.

</details>


### [52] [Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition](https://arxiv.org/abs/2509.08225)
*Matthew Nolan,Lina Yao,Robert Davidson*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成分布蒸馏的自监督学习框架，用于人体活动识别，通过利用未标记数据和部分监督训练策略，提高了预测精度、不确定性估计和对抗扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人体活动识别在深度学习技术应用方面取得了显著进展，但在数据需求、可靠性和鲁棒性方面仍存在挑战。本文旨在通过自监督学习和集成分布蒸馏来解决这些问题。

Method: 采用集成分布蒸馏（EDD）在自监督学习框架中，利用未标记数据和部分监督训练策略，开发了专门针对人体活动识别的创新数据增强技术。

Result: 该方法在多个公开数据集上验证有效，显著提高了预测准确性、不确定性估计的鲁棒性以及对对抗扰动的抵抗力，同时不增加推理时的计算复杂度。

Conclusion: 提出的自监督EDD框架为人体活动识别提供了更可靠和鲁棒的解决方案，在实际应用场景中具有重要价值，特别是在提高系统可靠性方面表现突出。

Abstract: Human Activity Recognition (HAR) has seen significant advancements with the
adoption of deep learning techniques, yet challenges remain in terms of data
requirements, reliability and robustness. This paper explores a novel
application of Ensemble Distribution Distillation (EDD) within a
self-supervised learning framework for HAR aimed at overcoming these
challenges. By leveraging unlabeled data and a partially supervised training
strategy, our approach yields an increase in predictive accuracy, robust
estimates of uncertainty, and substantial increases in robustness against
adversarial perturbation; thereby significantly improving reliability in
real-world scenarios without increasing computational complexity at inference.
We demonstrate this with an evaluation on several publicly available datasets.
The contributions of this work include the development of a self-supervised EDD
framework, an innovative data augmentation technique designed for HAR, and
empirical validation of the proposed method's effectiveness in increasing
robustness and reliability.

</details>


### [53] [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)
*Kai Yi*

Main category: cs.LG

TL;DR: 该论文提出了多种通信效率优化策略，包括模型压缩、本地训练个性化、隐私保护剪枝框架，在保持准确性的同时显著降低分布式和联邦学习的通信开销。


<details>
  <summary>Details</summary>
Motivation: 分布式和联邦学习虽然能保护隐私，但通信开销是主要瓶颈，需要开发高效的通信优化策略。

Method: 建立了有偏和无偏压缩算子的统一框架，提出自适应本地训练策略（Scafflix）、隐私保护剪枝框架（Cohort-Squeeze）和对称后训练剪枝方法（SymWanda）。

Result: 在基准测试和大规模语言模型上展示了准确性、收敛性和通信效率之间的良好权衡，Scafflix在IID和非IID设置下均表现优异。

Conclusion: 提供了可扩展高效分布式学习的理论和实践见解，通过多种创新方法有效解决了通信瓶颈问题。

Abstract: Distributed and federated learning are essential paradigms for training
models across decentralized data sources while preserving privacy, yet
communication overhead remains a major bottleneck. This dissertation explores
strategies to improve communication efficiency, focusing on model compression,
local training, and personalization. We establish a unified framework for
biased and unbiased compression operators with convergence guarantees, then
propose adaptive local training strategies that incorporate personalization to
accelerate convergence and mitigate client drift. In particular, Scafflix
balances global and personalized objectives, achieving superior performance
under both IID and non-IID settings. We further introduce privacy-preserving
pruning frameworks that optimize sparsity while minimizing communication costs,
with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device
overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances
robustness under high sparsity and maintains accuracy without retraining.
Extensive experiments on benchmarks and large-scale language models demonstrate
favorable trade-offs among accuracy, convergence, and communication, offering
theoretical and practical insights for scalable, efficient distributed
learning.

</details>


### [54] [The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data](https://arxiv.org/abs/2509.08247)
*Xiaolong Luo,Michael Lingzhi Li*

Main category: cs.LG

TL;DR: CRITICAL数据集提供了大规模多机构重症监护数据，CRISP工具包通过数据质量管理和标准化处理，将其转化为ML就绪数据集，大幅减少预处理时间。


<details>
  <summary>Details</summary>
Motivation: 现有重症监护数据集如MIMIC和eICU在规模和多样性上有限，无法支持跨机构通用预测模型和健康公平性研究，需要更大规模、更多样化的数据集。

Method: 开发CRISP工具包，通过四个步骤处理数据：(1)透明数据质量管理与审计追踪；(2)跨词汇表映射到统一SNOMED-CT标准；(3)模块化架构实现高效处理；(4)建立基准模型性能标准。

Result: CRISP能够将包含19.5亿条记录的大规模数据集在标准硬件上1天内处理完成，为研究人员节省数月预处理时间。

Conclusion: CRISP工具包成功解决了多机构医疗数据整合的复杂性，使研究人员能够专注于临床AI算法开发，推动重症监护AI研究的发展。

Abstract: While existing critical care EHR datasets such as MIMIC and eICU have enabled
significant advances in clinical AI research, the CRITICAL dataset opens new
frontiers by providing extensive scale and diversity -- containing 1.95 billion
records from 371,365 patients across four geographically diverse CTSA
institutions. CRITICAL's unique strength lies in capturing full-spectrum
patient journeys, including pre-ICU, ICU, and post-ICU encounters across both
inpatient and outpatient settings. This multi-institutional, longitudinal
perspective creates transformative opportunities for developing generalizable
predictive models and advancing health equity research. However, the richness
of this multi-site resource introduces substantial complexity in data
harmonization, with heterogeneous collection practices and diverse vocabulary
usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource.
CRISP systematically transforms raw Observational Medical Outcomes Partnership
Common Data Model data into ML-ready datasets through: (1) transparent data
quality management with comprehensive audit trails, (2) cross-vocabulary
mapping of heterogeneous medical terminologies to unified SNOMED-CT standards,
with deduplication and unit standardization, (3) modular architecture with
parallel optimization enabling complete dataset processing in $<$1 day even on
standard computing hardware, and (4) comprehensive baseline model benchmarks
spanning multiple clinical prediction tasks to establish reproducible
performance standards. By providing processing pipeline, baseline
implementations, and detailed transformation documentation, CRISP saves
researchers months of preprocessing effort and democratizes access to
large-scale multi-institutional critical care data, enabling them to focus on
advancing clinical AI.

</details>


### [55] [Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning](https://arxiv.org/abs/2509.08255)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.LG

TL;DR: 提出FAPM遗忘感知剪枝指标，通过任务向量与预训练参数的重叠度来量化灾难性遗忘，在保持下游任务99.67%准确率的同时将遗忘控制在0.25%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调时面临灾难性遗忘问题，需要平衡下游任务性能和预训练知识的保留

Method: 基于任务向量与预训练参数重叠度的发现，设计FAPM剪枝指标，将任务向量与预训练参数的比值作为量化遗忘的度量，无需修改训练过程或模型架构

Result: 在8个数据集上的实验表明，FAPM将灾难性遗忘控制在0.25%，同时保持下游任务99.67%的准确率

Conclusion: FAPM是一种有效的剪枝方法，能够显著缓解灾难性遗忘问题，且无需额外数据或架构修改

Abstract: Recent advancements in large language models (LLMs) have shown impressive
capabilities in various downstream tasks but typically face Catastrophic
Forgetting (CF) during fine-tuning. In this paper, we propose the
Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to
balance CF and downstream task performance. Our investigation reveals that the
degree to which task vectors (i.e., the subtraction of pre-trained weights from
the weights fine-tuned on downstream tasks) overlap with pre-trained model
parameters is a critical factor for CF. Based on this finding, FAPM employs the
ratio of the task vector to pre-trained model parameters as a metric to
quantify CF, integrating this measure into the pruning criteria. Importantly,
FAPM does not necessitate modifications to the training process or model
architecture, nor does it require any auxiliary data. We conducted extensive
experiments across eight datasets, covering natural language inference, General
Q&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results
demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\%
accuracy on downstream tasks. We provide the code to reproduce our results.

</details>


### [56] [Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models](https://arxiv.org/abs/2509.08270)
*Pranav Pawar,Kavish Shah,Akshat Bhalani,Komal Kasat,Dev Mittal,Hadi Gala,Deepali Patil,Nikita Raichada,Monali Deshmukh*

Main category: cs.LG

TL;DR: 该论文提出了一个评估视觉语言模型在2D物理理解能力上的新框架，包含400多个测试问题，发现模型规模与推理能力正相关，但在需要抽象空间推理的领域表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的发展，需要系统评估其对基础科学原理（如物理学）的理解能力，当前这一领域尚未得到充分探索。

Method: 开发了一个包含场景生成器的评估框架，覆盖四个核心物理领域（抛体运动、碰撞动力学、力学、流体动力学），创建了400多个多样化测试问题。

Result: 评估了四个最先进的视觉语言模型，发现模型规模与推理能力呈强正相关，最佳模型Qwen2.5-VL-7B总体得分0.815，模型在公式化问题上表现良好，但在需要抽象空间推理的领域表现显著较差。

Conclusion: 该框架旨在促进视觉语言模型科学推理能力的民主化研究，为深入理解其能力和局限性提供支持。

Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to
perform reasoning is coming under increasing supervision. While they excel at
many tasks, their grasp of fundamental scientific principles, such as physics,
remains an underexplored frontier. To reflect the advancements in these
capabilities, we introduce a novel and accessible framework designed to
rigorously evaluate VLMs on their understanding of 2D physics. Our framework
features a pragmatic scenario generator that creates a diverse testbed of over
400 problems across four core domains: Projectile Motion, Collision Dynamics,
Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four
state-of-the-art VLMs, we demonstrate a strong correlation between model scale
and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving
an overall score of 0.815. We find that while models excel at formulaic
problems, they struggle significantly with domains requiring abstract spatial
reasoning. By designing this framework, we aim to democratize the study of
scientific reasoning in VLMs and foster deeper insights into their capabilities
and limitations.

</details>


### [57] [Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning](https://arxiv.org/abs/2509.08277)
*Dung T. Tran,Huyen Ngoc Huyen,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.LG

TL;DR: 提出基于矩阵轮廓的加权集成框架MPWE，用于越南降雨预测，通过动态捕捉地理模型间的协变依赖和冗余感知加权，在多个流域和时间尺度上实现更准确稳定的预测。


<details>
  <summary>Details</summary>
Motivation: 越南降雨预测因气候多样性和地理变异性而极具挑战，但准确的预测对洪水管理、水电运营和灾害准备至关重要。现有地理模型预测存在局限性，需要集成方法提升准确性和稳定性。

Method: 提出矩阵轮廓基础的加权集成(MPWE)框架，采用制度切换机制动态捕捉多个地理模型预测间的协变依赖关系，并结合冗余感知加权来平衡各模型的贡献度。

Result: 在越南8个主要流域的5个预测时间尺度(1小时及12-84小时累积降雨)上测试，MPWE相比地理模型和集成基线方法，预测误差的均值和标准差均显著降低。

Conclusion: MPWE框架能够有效提升降雨预测的准确性和稳定性，在不同流域和时间尺度上均表现出优越性能，为复杂地理条件下的降雨预测提供了有效解决方案。

Abstract: Rainfall forecasting in Vietnam is highly challenging due to its diverse
climatic conditions and strong geographical variability across river basins,
yet accurate and reliable forecasts are vital for flood management, hydropower
operation, and disaster preparedness. In this work, we propose a Matrix
Profile-based Weighted Ensemble (MPWE), a regime-switching framework that
dynamically captures covariant dependencies among multiple geographical model
forecasts while incorporating redundancy-aware weighting to balance
contributions across models. We evaluate MPWE using rainfall forecasts from
eight major basins in Vietnam, spanning five forecast horizons (1 hour and
accumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results
show that MPWE consistently achieves lower mean and standard deviation of
prediction errors compared to geographical models and ensemble baselines,
demonstrating both improved accuracy and stability across basins and horizons.

</details>


### [58] [\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2509.08300)
*Yao Lu,Chunfeng Sun,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: FoQuS是一种通过选择核心集来近似完整训练效果的方法，显著减少自动调制识别模型的训练开销，仅需1%-30%数据即可保持高精度


<details>
  <summary>Details</summary>
Motivation: 解决深度学习自动调制识别模型在开发新模型或超参数调优时，使用海量数据进行重复训练带来的时间和能耗过高的问题

Method: 记录完整数据集训练过程中每个样本的预测轨迹，基于训练动态构建三个重要性指标来选择核心集

Result: 在多个AMR数据集上仅使用1%-30%原始数据就能保持高识别精度，并具有良好的跨架构泛化能力

Conclusion: FoQuS通过核心集选择有效降低了训练成本，为AMR模型的开发和调优提供了高效的解决方案

Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made
significant progress with the support of large-scale labeled data. However,
when developing new models or performing hyperparameter tuning, the time and
energy consumption associated with repeated training using massive amounts of
data are often unbearable. To address the above challenges, we propose
\emph{FoQuS}, which approximates the effect of full training by selecting a
coreset from the original dataset, thereby significantly reducing training
overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each
sample during full-dataset training and constructs three importance metrics
based on training dynamics. Experiments show that \emph{FoQuS} can maintain
high recognition accuracy and good cross-architecture generalization on
multiple AMR datasets using only 1\%-30\% of the original data.

</details>


### [59] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: EvolKV是一个自适应KV缓存压缩框架，通过进化搜索动态配置各层缓存预算，在保持任务性能的同时显著提升内存效率


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法依赖启发式策略，忽略了层间特征模式与任务性能的关键交互，导致泛化性能下降

Method: 将缓存分配重新表述为多目标优化问题，利用进化搜索动态配置各层预算，直接最大化下游任务性能

Result: 在11个任务上超越所有基线方法，在长上下文任务上表现优异，GSM8K任务上比启发式基线提升7个百分点，代码补全任务仅用1.5%原始预算就达到优于全缓存性能

Conclusion: 学习式KV缓存压缩策略具有巨大潜力，EvolKV展示了在保持性能的同时显著减少内存使用的可行性

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [60] [Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing](https://arxiv.org/abs/2509.08329)
*Lukas Toral,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 使用预训练大语言模型作为导师，通过学生-教师架构加速强化学习算法的收敛速度，实验表明LLM指导能显著减少训练时间并保持性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在复杂稀疏奖励环境中训练时间长，现有加速技术需要领域专业知识。研究探索使用LLM作为导师来加速RL收敛。

Method: 采用学生-教师架构，使用不同LLM模型（Llama、Vicuna、DeepSeek）作为导师，结合多种RL算法（DQN、PPO、A2C）在三个环境（Blackjack、Snake、Connect Four）中进行54种配置实验，并探索建议重用机制。

Result: LLM指导显著加速RL收敛，同时保持可比的最优性能。建议重用机制进一步缩短训练时间，但导致收敛动态不太稳定。效果对具体任务、RL算法和LLM模型组合敏感。

Conclusion: LLM辅导能有效改善强化学习收敛性能，但其效果依赖于任务、算法和模型的特定组合，建议重用虽能加速但会降低稳定性。

Abstract: Reinforcement Learning (RL) algorithms often require long training to become
useful, especially in complex environments with sparse rewards. While
techniques like reward shaping and curriculum learning exist to accelerate
training, these are often extremely specific and require the developer's
professionalism and dedicated expertise in the problem's domain. Tackling this
challenge, in this study, we explore the effectiveness of pre-trained Large
Language Models (LLMs) as tutors in a student-teacher architecture with RL
algorithms, hypothesizing that LLM-generated guidance allows for faster
convergence. In particular, we explore the effectiveness of reusing the LLM's
advice on the RL's convergence dynamics. Through an extensive empirical
examination, which included 54 configurations, varying the RL algorithm (DQN,
PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,
Snake, Connect Four), our results demonstrate that LLM tutoring significantly
accelerates RL convergence while maintaining comparable optimal performance.
Furthermore, the advice reuse mechanism shows a further improvement in training
duration but also results in less stable convergence dynamics. Our findings
suggest that LLM tutoring generally improves convergence, and its effectiveness
is sensitive to the specific task, RL algorithm, and LLM model combination.

</details>


### [61] [Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism](https://arxiv.org/abs/2509.08342)
*Jiaming Yan,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.LG

TL;DR: MoEpic是一种高效的MoE推理系统，通过专家分割机制和预测预取策略，在减少GPU内存占用的同时显著提升推理速度


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型参数庞大，GPU内存需求高，专家缓存命中率低且加载延迟大，限制了MoE LLMs的广泛应用

Method: 提出专家垂直分割机制（top和bottom段），缓存热点专家的top段以提高缓存命中率；预测下一层激活专家并预取；基于定点迭代的自适应缓存配置算法

Result: 实验表明MoEpic可节省约一半GPU成本，推理延迟降低37.51%-65.73%

Conclusion: MoEpic通过创新的专家分割和智能缓存管理，有效解决了MoE推理中的内存和性能瓶颈问题

Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern
large language models (LLMs). However, massive parameters impose heavy GPU
memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.
Offloading the expert parameters to CPU RAM offers an effective way to
alleviate the VRAM requirements for MoE inference. Existing approaches
typically cache a small subset of experts in VRAM and dynamically prefetch
experts from RAM during inference, leading to significant degradation in
inference speed due to the poor cache hit rate and substantial expert loading
latency. In this work, we propose MoEpic, an efficient MoE inference system
with a novel expert split mechanism. Specifically, each expert is vertically
divided into two segments: top and bottom. MoEpic caches the top segment of hot
experts, so that more experts will be stored under the limited VRAM budget,
thereby improving the cache hit rate. During each layer's inference, MoEpic
predicts and prefetches the activated experts for the next layer. Since the top
segments of cached experts are exempt from fetching, the loading time is
reduced, which allows efficient transfer-computation overlap. Nevertheless, the
performance of MoEpic critically depends on the cache configuration (i.e., each
layer's VRAM budget and expert split ratio). To this end, we propose a
divide-and-conquer algorithm based on fixed-point iteration for adaptive cache
configuration. Extensive experiments on popular MoE LLMs demonstrate that
MoEpic can save about half of the GPU cost, while lowering the inference
latency by about 37.51%-65.73% compared to the baselines.

</details>


### [62] [Prediction Loss Guided Decision-Focused Learning](https://arxiv.org/abs/2509.08359)
*Haeun Jeon,Hyunglip Bae,Chanyeong Kim,Yongjae Lee,Woo Chang Kim*

Main category: cs.LG

TL;DR: 提出一种通过预测损失梯度扰动决策损失梯度的方法，结合预测聚焦学习和决策聚焦学习的优势，实现更稳定的训练和更好的决策质量


<details>
  <summary>Details</summary>
Motivation: 传统预测聚焦学习(PFL)和决策聚焦学习(DFL)各有优缺点：PFL训练稳定但忽略下游决策质量，DFL直接优化决策质量但存在收敛不稳定问题

Method: 使用预测损失梯度扰动决策损失梯度来构建更新方向，采用类sigmoid衰减参数让预测损失梯度引导决策损失梯度，无需额外训练且可与任何DFL求解器集成

Result: 在三个随机优化问题上验证了方法的有效性，相比基线方法实现了更低的遗憾值和更稳定的训练，在PFL或DFL单独表现不佳的情况下也能取得良好效果

Conclusion: 提出的梯度扰动方法成功结合了PFL的稳定性和DFL的决策质量优化优势，具有理论收敛保证和实际应用价值

Abstract: Decision-making under uncertainty is often considered in two stages:
predicting the unknown parameters, and then optimizing decisions based on
predictions. While traditional prediction-focused learning (PFL) treats these
two stages separately, decision-focused learning (DFL) trains the predictive
model by directly optimizing the decision quality in an end-to-end manner.
However, despite using exact or well-approximated gradients, vanilla DFL often
suffers from unstable convergence due to its flat-and-sharp loss landscapes. In
contrast, PFL yields more stable optimization, but overlooks the downstream
decision quality. To address this, we propose a simple yet effective approach:
perturbing the decision loss gradient using the prediction loss gradient to
construct an update direction. Our method requires no additional training and
can be integrated with any DFL solvers. Using the sigmoid-like decaying
parameter, we let the prediction loss gradient guide the decision loss gradient
to train a predictive model that optimizes decision quality. Also, we provide a
theoretical convergence guarantee to Pareto stationary point under mild
assumptions. Empirically, we demonstrate our method across three stochastic
optimization problems, showing promising results compared to other baselines.
We validate that our approach achieves lower regret with more stable training,
even in situations where either PFL or DFL struggles.

</details>


### [63] [Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models](https://arxiv.org/abs/2509.08372)
*Kosuke Kihara,Junki Mori,Taiki Miyagawa,Akinori F. Ebihara*

Main category: cs.LG

TL;DR: 本文提出在类别不平衡的联邦无源域适应(CI-FFREEDA)场景中使用冻结的视觉基础模型(VFM)作为特征提取器，显著提升了性能并降低了计算通信成本


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中更现实的类别不平衡问题，包括源域和目标域的类别不平衡、域间标签偏移以及目标客户端间的非独立同分布问题

Method: 将FFREEDA框架的主干网络替换为冻结的视觉基础模型(VFM)，避免复杂的参数调优，利用强大的预训练特征提取能力

Result: 实验结果表明VFM能有效缓解域差距、类别不平衡和客户端非独立同分布问题，显著提高整体准确率

Conclusion: 在真实联邦学习场景中，强大的特征提取器比复杂的适应方法或联邦学习方法更为关键

Abstract: Federated Learning (FL) offers a framework for training models
collaboratively while preserving data privacy of each client. Recently,
research has focused on Federated Source-Free Domain Adaptation (FFREEDA), a
more realistic scenario wherein client-held target domain data remains
unlabeled, and the server can access source domain data only during
pre-training. We extend this framework to a more complex and realistic setting:
Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account class
imbalances in both the source and target domains, as well as label shifts
between source and target and among target clients. The replication of existing
methods in our experimental setup lead us to rethink the focus from enhancing
aggregation and domain adaptation methods to improving the feature extractors
within the network itself. We propose replacing the FFREEDA backbone with a
frozen vision foundation model (VFM), thereby improving overall accuracy
without extensive parameter tuning and reducing computational and communication
costs in federated learning. Our experimental results demonstrate that VFMs
effectively mitigate the effects of domain gaps, class imbalances, and even
non-IID-ness among target clients, suggesting that strong feature extractors,
not complex adaptation or FL methods, are key to success in the real-world FL.

</details>


### [64] [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)
*Matan Avitan,Moran Baruch,Nir Drucker,Itamar Zimerman,Yoav Goldberg*

Main category: cs.LG

TL;DR: 提出了cutmax算法和HE兼容的nucleus采样方法，显著提升同态加密下LLM文本生成的效率，比基线快24-35倍


<details>
  <summary>Details</summary>
Motivation: 解决同态加密下神经网络文本生成中argmax和采样等非多项式操作的计算瓶颈问题，保护敏感数据隐私

Method: 开发HE友好的cutmax argmax算法减少密文操作，并提出首个HE兼容的top-p采样方法，两者都是多项式算法

Result: 在真实LLM输出上实现24-35倍的延迟降低，提供理论收敛保证和隐私保护证明

Conclusion: cutmax和HE采样方法为隐私保护的文本生成提供了实用的多项式解决方案，显著提升同态加密下的推理效率

Abstract: Large language models (LLMs) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires decoding methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy decoding under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic decoding with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-preserving
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic LLM outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.

</details>


### [65] [Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models](https://arxiv.org/abs/2509.08401)
*Xunkai Li,Daohan Su,Sicheng Liu,Ru Zhang,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了MoT（Mixture-of-Tinkers）方法来解决图基础模型中的领域泛化冲突问题，包括模型退化和表示坍塌两个主要陷阱，通过信息调整和正则化调整来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 图基础模型在多领域图数据预训练中存在领域泛化冲突，导致模型退化和表示坍塌问题，影响下游任务的泛化能力。

Method: 提出MoT框架，包含信息调整器（边缘语义融合策略和混合码本）和正则化调整器（两种额外正则化），以提升信息容量和梯度监督。

Result: 在6个领域的22个数据集上实验表明，MoT在全监督、少样本和零样本场景下均显著优于现有最优基线。

Conclusion: MoT有效解决了图基础模型优化困境，遵循模型缩放规律，提供了可控的模型规模，在多领域图学习任务中表现出色。

Abstract: Graph foundation models, inspired by the success of LLMs, are designed to
learn the optimal embedding from multi-domain TAGs for the downstream
cross-task generalization capability. During our investigation, graph VQ-MAE
stands out among the increasingly diverse landscape of GFM architectures. This
is attributed to its ability to jointly encode topology and textual attributes
from multiple domains into discrete embedding spaces with clear semantic
boundaries. Despite its potential, domain generalization conflicts cause
imperceptible pitfalls. In this paper, we instantiate two of them, and they are
just like two sides of the same GFM optimization coin - Side 1 Model
Degradation: The encoder and codebook fail to capture the diversity of inputs;
Side 2 Representation Collapse: The hidden embedding and codebook vector fail
to preserve semantic separability due to constraints from narrow representation
subspaces. These two pitfalls (sides) collectively impair the decoder and
generate the low-quality reconstructed supervision, causing the GFM
optimization dilemma during pre-training (coin). Through empirical
investigation, we attribute the above challenges to Information Bottleneck and
Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -
(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic
fusion strategy and a mixture-of-codebooks with domain-aware routing to improve
information capacity. (2) Regularization Tinker for Optimization Coin, which
utilizes two additional regularizations to further improve gradient supervision
in our proposed Information Tinker. Notably, as a flexible architecture, MoT
adheres to the scaling laws of GFM, offering a controllable model scale.
Compared to SOTA baselines, experiments on 22 datasets across 6 domains
demonstrate that MoT achieves significant improvements in supervised, few-shot,
and zero-shot scenarios.

</details>


### [66] [Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics](https://arxiv.org/abs/2509.08461)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 使用微调版LLaMa 3.2视觉语言模型在粒子物理实验中识别中微子相互作用，性能超越传统卷积神经网络，同时提供更好的可解释性


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在高能物理实验中的应用潜力，特别是处理像素化探测器数据和中微子相互作用识别任务

Method: 使用微调的LLaMa 3.2视觉语言模型，与NOvA和DUNE实验中使用的先进卷积神经网络架构进行基准测试对比

Result: 视觉语言模型在分类性能上超越卷积神经网络，同时提供更好的灵活性和基于推理的可解释预测

Conclusion: 视觉语言模型作为物理事件分类的通用骨干网络具有巨大潜力，因其高性能、可解释性和泛化能力，为实验粒子物理中的多模态推理开辟了新途径

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their
remarkable capacity to process and reason over structured and unstructured data
modalities beyond natural language. In this work, we explore the applications
of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa
3.2, to the task of identifying neutrino interactions in pixelated detector
data from high-energy physics (HEP) experiments. We benchmark this model
against a state-of-the-art convolutional neural network (CNN) architecture,
similar to those used in the NOvA and DUNE experiments, which have achieved
high efficiency and purity in classifying electron and muon neutrino events.
Our evaluation considers both the classification performance and
interpretability of the model predictions. We find that VLMs can outperform
CNNs, while also providing greater flexibility in integrating auxiliary textual
or semantic information and offering more interpretable, reasoning-based
predictions. This work highlights the potential of VLMs as a general-purpose
backbone for physics event classification, due to their high performance,
interpretability, and generalizability, which opens new avenues for integrating
multimodal reasoning in experimental neutrino physics.

</details>


### [67] [An Interpretable Deep Learning Model for General Insurance Pricing](https://arxiv.org/abs/2509.08467)
*Patrick J. Laub,Tu Pho,Bernard Wong*

Main category: cs.LG

TL;DR: 提出保险精算神经加性模型，这是一种可解释的深度学习模型，在保持神经网络强大预测能力的同时，为保险定价提供完全透明和可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 传统保险定价模型缺乏透明度，而现代机器学习模型虽然预测能力强但缺乏可解释性。需要一种既能保持预测准确性又能提供完全透明度的模型来满足保险行业的实际需求。

Method: 为每个协变量和两两交互项分配专用的神经网络（或子网络），独立学习其对输出的影响，同时实施架构约束以确保可解释性（如稀疏性）和满足保险应用的实际要求（如平滑性、单调性）。

Result: 在合成和真实保险数据集上的实验表明，该模型在大多数情况下优于传统精算方法和最先进的机器学习方法，同时提供完全透明的内部逻辑。

Conclusion: 该模型成功实现了预测能力和可解释性的平衡，为保险定价提供了一个既强大又透明的解决方案，满足了保险行业对模型透明度和监管合规性的要求。

Abstract: This paper introduces the Actuarial Neural Additive Model, an inherently
interpretable deep learning model for general insurance pricing that offers
fully transparent and interpretable results while retaining the strong
predictive power of neural networks. This model assigns a dedicated neural
network (or subnetwork) to each individual covariate and pairwise interaction
term to independently learn its impact on the modeled output while implementing
various architectural constraints to allow for essential interpretability (e.g.
sparsity) and practical requirements (e.g. smoothness, monotonicity) in
insurance applications. The development of our model is grounded in a solid
foundation, where we establish a concrete definition of interpretability within
the insurance context, complemented by a rigorous mathematical framework.
Comparisons in terms of prediction accuracy are made with traditional actuarial
and state-of-the-art machine learning methods using both synthetic and real
insurance datasets. The results show that the proposed model outperforms other
methods in most cases while offering complete transparency in its internal
logic, underscoring the strong interpretability and predictive capability.

</details>


### [68] [SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms](https://arxiv.org/abs/2509.08482)
*Andrea Maldonado,Christian M. M. Frey,Sai Anirudh Aryasomayajula,Ludwig Zellner,Stephan A. Fahrenkrog-Petersen,Thomas Seidl*

Main category: cs.LG

TL;DR: SHAining方法首次量化事件日志特征对流程挖掘算法指标的边际贡献，通过分析22,000多个事件日志发现特征值与其影响程度的相关性，评估算法鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在固定的事件日志集上评估算法，缺乏对事件日志特征如何单独影响算法的系统分析，且忽略了特征共现的问题

Method: 引入SHAining方法，使用流程发现作为下游任务，分析超过22,000个覆盖广泛特征的事件日志，量化不同事件日志特征对算法指标的边际贡献

Result: 揭示了哪些特征对算法指标（如拟合度、精确度、复杂度）影响最大，并提供了关于事件日志特征值与其贡献影响相关性新颖见解

Conclusion: 该方法能够系统评估事件日志特征对流程挖掘算法性能的影响，为算法选择和评估提供了更全面的分析框架

Abstract: Process mining aims to extract and analyze insights from event logs, yet
algorithm metric results vary widely depending on structural event log
characteristics. Existing work often evaluates algorithms on a fixed set of
real-world event logs but lacks a systematic analysis of how event log
characteristics impact algorithms individually. Moreover, since event logs are
generated from processes, where characteristics co-occur, we focus on
associational rather than causal effects to assess how strong the overlapping
individual characteristic affects evaluation metrics without assuming isolated
causal effects, a factor often neglected by prior work. We introduce SHAining,
the first approach to quantify the marginal contribution of varying event log
characteristics to process mining algorithms' metrics. Using process discovery
as a downstream task, we analyze over 22,000 event logs covering a wide span of
characteristics to uncover which affect algorithms across metrics (e.g.,
fitness, precision, complexity) the most. Furthermore, we offer novel insights
about how the value of event log characteristics correlates with their
contributed impact, assessing the algorithm's robustness.

</details>


### [69] [Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis](https://arxiv.org/abs/2509.08483)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 该论文分析了带Polyak重球动量的梯度下降算法，证明了在指数吸引不变流形上，该算法等价于带修正损失的普通梯度下降，并提供了任意精度的逼近误差分析。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降中重球动量的数学本质，理解动量参数如何影响优化过程，为其他优化算法的类似分析提供理论框架。

Method: 基于Kovachki和Stuart(2021)的工作，在指数吸引不变流形上分析算法，推导修正损失的无记忆逼近，进行组合数学的精细分析，建立连续修正方程和主流动量逼近。

Result: 证明了步长足够小时HB算法等价于带修正损失的梯度下降，获得了任意阶O(h^R)的全局逼近界，发现了包含Eulerian和Narayana多项式的丰富多项式族。

Conclusion: 理论结果揭示了重球动量的主要特征，为其他优化算法的类似分析提供了路线图，覆盖了全批次和小批次HB的逼近定理。

Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed
momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory.
Building on Kovachki and Stuart (2021), we prove that on an exponentially
attractive invariant manifold the algorithm is exactly plain gradient descent
with a modified loss, provided that the step size $h$ is small enough. Although
the modified loss does not admit a closed-form expression, we describe it with
arbitrary precision and prove global (finite "time" horizon) approximation
bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a
fine-grained analysis of the combinatorics underlying the memoryless
approximations of HB, in particular, finding a rich family of polynomials in
$\beta$ hidden inside which contains Eulerian and Narayana polynomials. We
derive continuous modified equations of arbitrary approximation order (with
rigorous bounds) and the principal flow that approximates the HB dynamics,
generalizing Rosca et al. (2023). Approximation theorems cover both full-batch
and mini-batch HB. Our theoretical results shed new light on the main features
of gradient descent with heavy-ball momentum, and outline a road-map for
similar analysis of other optimization algorithms.

</details>


### [70] [Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks](https://arxiv.org/abs/2509.08499)
*Chisom Chibuike,Adeyinka Ogunsanya*

Main category: cs.LG

TL;DR: 本文比较了10种优化器在心脏病预测任务中的性能，发现RMSProp在收敛速度和关键指标上表现最佳，但稳定性不是最优。建议通过全面评估选择优化器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练中优化器选择缺乏系统研究，需要深入探讨如何基于性能指标选择最适合的优化器。

Method: 使用Kaggle心脏病数据集训练多层感知机模型，在一致的训练范式下比较10种优化器的收敛速度、稳定性以及AUC、精确率、召回率等分类指标。

Result: Adagrad和Adadelta更稳定但收敛慢；RMSProp在精确率(0.765)、召回率(0.827)和AUC(0.841)上表现最佳，训练时间更快，但稳定性不是最优。

Conclusion: RMSProp是心脏病预测任务中最有效的优化器，建议在计算资源充足时采用这种全面评估的方法来选择优化器，以提高深度学习模型训练的科学性和性能。

Abstract: Optimization has been an important factor and topic of interest in training
deep learning models, yet less attention has been given to how we select the
optimizers we use to train these models. Hence, there is a need to dive deeper
into how we select the optimizers we use for training and the metrics that
determine this selection. In this work, we compare the performance of 10
different optimizers in training a simple Multi-layer Perceptron model using a
heart disease dataset from Kaggle. We set up a consistent training paradigm and
evaluate the optimizers based on metrics such as convergence speed and
stability. We also include some other Machine Learning Evaluation metrics such
as AUC, Precision, and Recall, which are central metrics to classification
problems. Our results show that there are trade-offs between convergence speed
and stability, as optimizers like Adagrad and Adadelta, which are more stable,
took longer time to converge. Across all our metrics, we chose RMSProp to be
the most effective optimizer for this heart disease prediction task because it
offered a balanced performance across key metrics. It achieved a precision of
0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.
However, it was not the most stable. We recommend that, in less
compute-constrained environments, this method of choosing optimizers through a
thorough evaluation should be adopted to increase the scientific nature and
performance in training deep learning models.

</details>


### [71] [Variational Rank Reduction Autoencoders for Generative](https://arxiv.org/abs/2509.08515)
*Alicia Tierz,Jad Mounayer,Beatriz Moya,Francisco Chinesta*

Main category: cs.LG

TL;DR: 提出结合变分秩降自编码器(VRRAE)和深度算子网络(DeepONet)的混合框架，用于复杂几何形状的生成式热设计，解决传统方法的高计算成本和潜在空间不连续性问题。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器和变分自编码器在热设计中存在高计算成本、潜在空间不连续等问题，限制了设计探索和物理一致性解决方案的生成能力。

Method: 使用VRRAE在潜在空间中引入截断SVD，获得连续可解释的结构化表示；DeepONet利用紧凑潜在编码和空间坐标高效预测温度梯度。

Result: 该方法提高了生成几何形状的质量和梯度预测精度，在推理效率上相比传统数值求解器具有显著优势。

Conclusion: 结构化潜在表示对算子学习至关重要，生成模型与算子网络的结合在热设计和更广泛工程应用中具有巨大潜力。

Abstract: Generative thermal design for complex geometries is fundamental in many areas
of engineering, yet it faces two main challenges: the high computational cost
of high-fidelity simulations and the limitations of conventional generative
models. Approaches such as autoencoders (AEs) and variational autoencoders
(VAEs) often produce unstructured latent spaces with discontinuities, which
restricts their capacity to explore designs and generate physically consistent
solutions.
  To address these limitations, we propose a hybrid framework that combines
Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks
(DeepONets). The VRRAE introduces a truncated SVD within the latent space,
leading to continuous, interpretable, and well-structured representations that
mitigate posterior collapse and improve geometric reconstruction. The DeepONet
then exploits this compact latent encoding in its branch network, together with
spatial coordinates in the trunk network, to predict temperature gradients
efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries
and the accuracy of gradient prediction, but also provides a substantial
advantage in inference efficiency compared to traditional numerical solvers.
Overall, the study underscores the importance of structured latent
representations for operator learning and highlights the potential of combining
generative models and operator networks in thermal design and broader
engineering applications.

</details>


### [72] [Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures](https://arxiv.org/abs/2509.08530)
*Wen-Bo Xie,Xun Fu,Bin Chen,Yan-Li Lee,Tao Deng,Tian Zou,Xin Wang,Zhen Liu,Jaideep Srivastavad*

Main category: cs.LG

TL;DR: 提出基于图结构的主动聚类算法，使用两个稀疏图（数据骨架和更新图）来降低计算成本、减少标注需求并节省内存，在大规模数据处理中实现更准确的聚类效果。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据处理中基于成对约束的主动聚类算法的效率、可扩展性和标注成本问题，特别是在数据挖掘、知识标注和AI模型预训练等应用场景中的实际需求。

Method: 采用图基的主动聚类算法，构建两个稀疏图：数据骨架图用于表示数据间关系，更新图用于动态更新数据骨架。通过精炼连接子图形成嵌套簇结构。

Result: 算法在显著减少用户标注约束的情况下实现更准确的聚类，计算性能和可扩展性优于同类方法，且在不同距离度量下保持鲁棒性。

Conclusion: 所提出的双稀疏图结构有效提升了主动聚类的效率、可扩展性和实用性，为大规模数据处理提供了高效的聚类解决方案。

Abstract: In this work, we focus on the efficiency and scalability of pairwise
constraint-based active clustering, crucial for processing large-scale data in
applications such as data mining, knowledge annotation, and AI model
pre-training. Our goals are threefold: (1) to reduce computational costs for
iterative clustering updates; (2) to enhance the impact of user-provided
constraints to minimize annotation requirements for precise clustering; and (3)
to cut down memory usage in practical deployments. To achieve these aims, we
propose a graph-based active clustering algorithm that utilizes two sparse
graphs: one for representing relationships between data (our proposed data
skeleton) and another for updating this data skeleton. These two graphs work in
concert, enabling the refinement of connected subgraphs within the data
skeleton to create nested clusters. Our empirical analysis confirms that the
proposed algorithm consistently facilitates more accurate clustering with
dramatically less input of user-provided constraints, and outperforms its
counterparts in terms of computational performance and scalability, while
maintaining robustness across various distance metrics.

</details>


### [73] [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](https://arxiv.org/abs/2509.08578)
*Hong Liu*

Main category: cs.LG

TL;DR: MAESTRO是一个多模态自适应集成模型，用于流感发病率预测，通过融合监测数据、网络搜索趋势和气象数据，结合频谱-时间架构，实现了0.956的R平方值，达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 及时准确的流感发病率预测对公共卫生决策至关重要，需要开发能够融合多模态数据并处理时间序列复杂性的鲁棒预测模型。

Method: 采用多模态自适应集成方法，首先将时间序列分解为季节性和趋势成分，然后通过Transformer编码器、Mamba状态空间模型、多尺度时间卷积和频域分析模块的混合特征增强管道进行处理，使用跨通道注意力机制整合不同数据模态。

Result: 在香港11年流感数据上的评估显示，MAESTRO具有强大的竞争性能，模型拟合优度和相对准确性优越，达到了0.956的最先进R平方值。

Conclusion: MAESTRO提供了一个强大、统一的框架，证明了先进的频谱-时间建模和多模态数据融合对于稳健流行病学预测的关键协同作用，其模块化和可复现的管道便于部署和扩展到其他地区和病原体。

Abstract: Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

</details>


### [74] [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
*Aadit Sengupta,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 本文主张将可解释性（特别是机制性方法）作为AI对齐的核心设计原则，而非辅助诊断工具，认为这是实现安全可信AI的关键路径


<details>
  <summary>Details</summary>
Motivation: 大型神经网络在高风险场景中的部署日益增多，需要确保其行为与人类价值观可靠对齐，而当前的事后解释方法存在局限性

Method: 提倡采用机制性可解释性方法（如电路追踪、激活修补等因果分析技术），而非仅依赖LIME、SHAP等相关性解释方法

Result: 机制性方法能够揭示内部故障的因果关系，包括欺骗性或未对齐的推理模式，这些是行为方法（如RLHF、红队测试）可能忽略的

Conclusion: 安全可信AI的进展取决于将可解释性作为AI研发的一等目标，确保系统不仅有效，而且可审计、透明且与人类意图对齐

Abstract: Large neural models are increasingly deployed in high-stakes settings,
raising concerns about whether their behavior reliably aligns with human
values. Interpretability provides a route to internal transparency by revealing
the computations that drive outputs. We argue that interpretability especially
mechanistic approaches should be treated as a design principle for alignment,
not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer
intuitive but correlational explanations, while mechanistic techniques like
circuit tracing or activation patching yield causal insight into internal
failures, including deceptive or misaligned reasoning that behavioral methods
like RLHF, red teaming, or Constitutional AI may overlook. Despite these
advantages, interpretability faces challenges of scalability, epistemic
uncertainty, and mismatches between learned representations and human concepts.
Our position is that progress on safe and trustworthy AI will depend on making
interpretability a first-class objective of AI research and development,
ensuring that systems are not only effective but also auditable, transparent,
and aligned with human intent.

</details>


### [75] [Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques](https://arxiv.org/abs/2509.08606)
*Alireza Sameh,Mehrdad Rostami,Mourad Oussalah,Vahid Farrahi*

Main category: cs.LG

TL;DR: 比较深度学习和传统机器学习算法在24小时运动行为分类中的性能，发现使用原始加速度信号的DL方法略优于使用手工特征的方法


<details>
  <summary>Details</summary>
Motivation: 评估深度学习和传统机器学习算法在基于加速度计数据的24小时运动行为（睡眠、久坐、轻度活动、中高强度活动）分类中的性能差异

Method: 使用151名成年人的手腕加速度计数据，提取104个手工特征，比较4种DL算法（LSTM、BiLSTM、GRU、1D-CNN）和6种传统ML算法（RF、SVM、XGBoost、LR、ANN、DT）的分类性能

Result: 使用原始加速度信号的LSTM、BiLSTM和GRU达到约85%的准确率，1D-CNN约80%；使用手工特征的DL和传统ML算法准确率在70%-81%之间；MVPA和LPA的分类混淆度较高

Conclusion: 使用原始加速度信号的深度学习方法在预测24小时运动行为强度方面仅略优于使用手工特征的深度学习和传统机器学习方法

Abstract: Purpose: We compared the performance of deep learning (DL) and classical
machine learning (ML) algorithms for the classification of 24-hour movement
behavior into sleep, sedentary, light intensity physical activity (LPA), and
moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access
data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.
Participants were randomly divided into training, validation, and test sets
(121, 15, and 15 participants each). Raw acceleration signals were segmented
into non-overlapping 10-second windows, and then a total of 104 handcrafted
features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),
Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and
One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw
acceleration signals and with handcrafted features extracted from these signals
to predict 24-hour movement behavior categories. The handcrafted features were
also used to train classical ML algorithms, namely Random Forest (RF), Support
Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression
(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying
24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an
overall accuracy of approximately 85% when trained with raw acceleration
signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on
handcrafted features, the overall accuracy for both DL and classical ML
algorithms ranged from 70% to 81%. Overall, there was a higher confusion in
classification of MVPA and LPA, compared to sleep and sedentary categories.
Conclusion: DL methods with raw acceleration signals had only slightly better
performance in predicting 24-hour movement behavior intensities, compared to
when DL and classical ML were trained with handcrafted features.

</details>


### [76] [Towards Interpretable Deep Neural Networks for Tabular Data](https://arxiv.org/abs/2509.08617)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab是一种可解释的神经网络架构，通过稀疏自编码器学习单语义特征字典，为表格数据预测提供完全可解释性，同时保持与最先进黑盒模型相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据在金融和医疗等领域应用广泛，但现有的DNN模型虽然预测性能好，但缺乏可解释性，限制了其在关键决策场景中的应用。

Method: 使用稀疏自编码器(SAE)在学习用于预测的潜在空间中学习单语义特征字典，并通过自动化方法为这些特征分配人类可解释的语义，使预测结果可以表示为语义有意义组件的线性组合。

Result: 实证评估表明，XNNTab在保持完全可解释性的同时，达到了与最先进的黑盒神经网络模型和经典机器学习方法相当或更好的性能。

Conclusion: XNNTab成功解决了表格数据预测中性能与可解释性之间的权衡问题，为关键应用领域提供了既准确又可解释的解决方案。

Abstract: Tabular data is the foundation of many applications in fields such as finance
and healthcare. Although DNNs tailored for tabular data achieve competitive
predictive performance, they are blackboxes with little interpretability. We
introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to
learn a dictionary of monosemantic features within the latent space used for
prediction. Using an automated method, we assign human-interpretable semantics
to these features. This allows us to represent predictions as linear
combinations of semantically meaningful components. Empirical evaluations
demonstrate that XNNTab attains performance on par with or exceeding that of
state-of-the-art, black-box neural models and classical machine learning
approaches while being fully interpretable.

</details>


### [77] [An upper bound of the silhouette validation metric for clustering](https://arxiv.org/abs/2509.08625)
*Hugo Sträng,Tai Dinh*

Main category: cs.LG

TL;DR: 本文提出了针对轮廓系数的新上界计算方法，为每个数据点推导出尖锐的个体轮廓宽度上界，并通过聚合得到数据集相关的ASW上界，显著丰富了聚类质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 传统轮廓系数的标准上界1通常无法达到，且数据集特定的最大ASW值未知，这限制了轮廓系数在聚类质量评估中的实用性。

Method: 为每个数据点推导出其轮廓宽度的尖锐上界，然后通过聚合这些个体界限得到数据集相关的ASW上界，该界限通常远低于1。

Result: 在合成和真实数据集上的实验表明，所提出的界限在许多情况下被证明接近紧致，显著丰富了聚类质量评估。

Conclusion: 该方法能够指示单个数据点是否能够被良好放置，支持基于轮廓的优化循环的早期停止，并帮助回答聚类结果与特定数据上最佳可能结果的接近程度。

Abstract: The silhouette coefficient summarizes, per observation, cohesion versus
separation in [-1, 1]; the average silhouette width (ASW) is a common internal
measure of clustering quality where higher values indicate more coveted
results. However, the dataset-specific maximum of ASW is typically unknown, and
the standard upper limit 1 is often unattainable. In this work, we derive for
each data point in a given dataset a sharp upper bound on its silhouette width.
By aggregating these individual bounds, we present a canonical data-dependent
upper bound on ASW that often assumes values well below 1. The presented bounds
can indicate whether individual data points can ever be well placed, enable
early stopping of silhouette-based optimization loops, and help answer a key
question: How close is my clustering result to the best possible outcome on
this specific data? Across synthetic and real datasets, the bounds are provably
near-tight in many cases and offer significant enrichment of cluster quality
evaluation.

</details>


### [78] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: 提出Generative Data Refinement (GDR)框架，使用预训练生成模型将有不良内容的数据集转化为更适合训练的净化数据集，解决训练数据短缺问题。


<details>
  <summary>Details</summary>
Motivation: 随着模型参数量固定，模型能力主要取决于训练数据质量和数量。当前训练数据集增长速度超过网络新数据索引速度，预计未来十年将面临数据枯竭问题。大量用户生成内容未被公开索引，但直接使用这些数据存在隐私泄露和不良内容风险。

Method: GDR框架通过预训练生成模型，以真实数据集中的每个样本为条件生成合成数据，将含有不良内容的数据集转化为净化后的数据集。这种方法避免了通过模型提示生成多样化合成数据的挑战。

Result: 实验显示GDR在数据集匿名化方面优于行业级解决方案，能够直接对高度不安全数据集进行去毒化处理。GDR生成的合成数据自然匹配网络规模数据集的多样性。

Conclusion: GDR的简单性和有效性使其成为扩展前沿模型训练数据总量的强大工具，为解决数据枯竭问题提供了可行方案。

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [79] [Replicable Reinforcement Learning with Linear Function Approximation](https://arxiv.org/abs/2509.08660)
*Eric Eaton,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 本文提出了首个可证明高效的可复制强化学习算法，用于线性马尔可夫决策过程，解决了函数逼近设置中的可复制性问题


<details>
  <summary>Details</summary>
Motivation: 机器学习领域存在实验结果难以复现的问题，特别是在强化学习中算法表现不稳定。虽然表格RL已有可复制算法，但扩展到更实用的函数逼近设置一直是个开放问题

Method: 首先开发了两种高效算法用于可复制随机设计回归和非中心协方差估计，然后利用这些工具为线性MDP提供了首个可证明高效的可复制RL算法

Result: 在生成模型和情景设置中都实现了可证明高效的可复制RL算法，实验评估显示这些算法能够激发更一致的神经网络策略

Conclusion: 这项工作在强化学习的可复制性方面取得了重要进展，为线性函数逼近提供了首个可证明高效的可复制算法，具有重要的理论和实践意义

Abstract: Replication of experimental results has been a challenge faced by many
scientific disciplines, including the field of machine learning. Recent work on
the theory of machine learning has formalized replicability as the demand that
an algorithm produce identical outcomes when executed twice on different
samples from the same distribution. Provably replicable algorithms are
especially interesting for reinforcement learning (RL), where algorithms are
known to be unstable in practice. While replicable algorithms exist for tabular
RL settings, extending these guarantees to more practical function
approximation settings has remained an open problem. In this work, we make
progress by developing replicable methods for linear function approximation in
RL. We first introduce two efficient algorithms for replicable random design
regression and uncentered covariance estimation, each of independent interest.
We then leverage these tools to provide the first provably efficient replicable
RL algorithms for linear Markov decision processes in both the generative model
and episodic settings. Finally, we evaluate our algorithms experimentally and
show how they can inspire more consistent neural policies.

</details>


### [80] [Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data](https://arxiv.org/abs/2509.08679)
*Jingya Cheng,Jiazi Tian,Federica Spoto,Alaleh Azhir,Daniel Mork,Hossein Estiri*

Main category: cs.LG

TL;DR: 开发信号保真度指数(SFI)量化痴呆症诊断数据质量，通过SFI感知校准显著提升机器学习模型在异构医疗数据集上的性能，无需结果标签即可改善模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHR)训练的机器学习模型在不同医疗系统间存在性能下降问题，主要原因是诊断信号衰减——不同机构的诊断质量和一致性差异影响了编码的可靠性。

Method: 构建包含2,500个合成数据集的模拟框架，开发基于六个可解释组件(诊断特异性、时间一致性、熵、上下文一致性、药物对齐和轨迹稳定性)的SFI指标，应用乘法调整进行SFI感知校准。

Result: 在最优参数(α=2.0)下，SFI感知校准显著改善所有指标(p<0.001)，平衡准确率提升10.3%，召回率提升32.5%，精确率提升31.9%，F1分数提升26.1%。

Conclusion: 诊断信号衰减是模型泛化的可处理障碍，SFI感知校准为缺乏结果标签的大规模管理数据集提供了一种实用的无标签策略来增强跨医疗环境的预测能力。

Abstract: \textbf{Background:} Machine learning models trained on electronic health
records (EHRs) often degrade across healthcare systems due to distributional
shift. A fundamental but underexplored factor is diagnostic signal decay:
variability in diagnostic quality and consistency across institutions, which
affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying
diagnostic data quality at the patient level in dementia, and to test SFI-aware
calibration for improving model performance across heterogeneous datasets
without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic
datasets, each with 1,000 patients and realistic demographics, encounters, and
coding patterns based on dementia risk factors. The SFI was derived from six
interpretable components: diagnostic specificity, temporal consistency,
entropy, contextual concordance, medication alignment, and trajectory
stability. SFI-aware calibration applied a multiplicative adjustment, optimized
across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware
calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from
10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in
Precision (31.9\%) and F1-score (26.1\%). Performance approached reference
standards, with F1-score and Recall within 1\% and Balanced Accuracy and
Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model
generalization. SFI-aware calibration provides a practical, label-free strategy
to enhance prediction across healthcare contexts, particularly for large-scale
administrative datasets lacking outcome labels.

</details>


### [81] [Perfectly-Private Analog Secure Aggregation in Federated Learning](https://arxiv.org/abs/2509.08683)
*Delio Jaramillo-Velez,Charul Rajput,Ragnar Freij-Hollanti,Camilla Hollanti,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出了一种基于环面(torus)而非有限域的新型安全参数聚合方法，在联邦学习中实现完美隐私保护，同时避免精度损失。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，通过安全多方计算实现安全聚合可以增强隐私，但实数数据无法实现完美隐私，而有限域方法存在精度复杂度权衡问题。

Method: 采用环面(torus)而非有限域进行安全参数聚合，利用环面上的均匀分布来保证完美隐私，避免精度损失。

Result: 实验结果表明，新协议在不安全聚合模型的基础上实现了完美隐私保护，在某些情况下显著优于有限域安全聚合方法。

Conclusion: 基于环面的安全聚合协议是更安全的选择，能够在保持模型精度的同时实现完美隐私保护。

Abstract: In federated learning, multiple parties train models locally and share their
parameters with a central server, which aggregates them to update a global
model. To address the risk of exposing sensitive data through local models,
secure aggregation via secure multiparty computation has been proposed to
enhance privacy. At the same time, perfect privacy can only be achieved by a
uniform distribution of the masked local models to be aggregated. This raises a
problem when working with real valued data, as there is no measure on the reals
that is invariant under the masking operation, and hence information leakage is
bound to occur. Shifting the data to a finite field circumvents this problem,
but as a downside runs into an inherent accuracy complexity tradeoff issue due
to fixed point modular arithmetic as opposed to floating point numbers that can
simultaneously handle numbers of varying magnitudes. In this paper, a novel
secure parameter aggregation method is proposed that employs the torus rather
than a finite field. This approach guarantees perfect privacy for each party's
data by utilizing the uniform distribution on the torus, while avoiding
accuracy losses. Experimental results show that the new protocol performs
similarly to the model without secure aggregation while maintaining perfect
privacy. Compared to the finite field secure aggregation, the torus-based
protocol can in some cases significantly outperform it in terms of model
accuracy and cosine similarity, hence making it a safer choice.

</details>


### [82] [Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective](https://arxiv.org/abs/2509.08697)
*James Gong,Raymond Luo,Emma Wang,Leon Ge,Bruce Li,Felix Marattukalam,Waleed Abdulla*

Main category: cs.LG

TL;DR: FAUST算法通过将Forward-Forward算法与相似性学习框架结合，消除了推理时需要多次前向传播的问题，在保持生物合理性的同时显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 解决Backpropagation算法的生物不合理性问题（反向锁定和全局误差传播），同时改进Forward-Forward算法在准确率和推理效率方面的不足。

Method: 将Forward-Forward算法与基于相似性的Tuplet损失函数框架相结合，提出FAUST算法，消除了推理时多次前向传播的需求。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，FAUST显著提升了准确率。在CIFAR-10上使用简单MLP架构达到56.22%准确率，接近Backpropagation的57.63%基准。

Conclusion: FAUST算法成功地在保持生物合理性的同时，大幅缩小了与Backpropagation在准确率上的差距，并提高了推理效率。

Abstract: Backpropagation is the pivotal algorithm underpinning the success of
artificial neural networks, yet it has critical limitations such as
biologically implausible backward locking and global error propagation. To
circumvent these constraints, the Forward-Forward algorithm was proposed as a
more biologically plausible method that replaces the backward pass with an
additional forward pass. Despite this advantage, the Forward-Forward algorithm
significantly trails backpropagation in accuracy, and its optimal form exhibits
low inference efficiency due to multiple forward passes required. In this work,
the Forward-Forward algorithm is reshaped through its integration with
similarity learning frameworks, eliminating the need for multiple forward
passes during inference. This proposed algorithm is named Forward-Forward
Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical
evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST
substantially improves accuracy, narrowing the gap with backpropagation. On
CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron
architecture, approaching the backpropagation benchmark of 57.63\% accuracy.

</details>


### [83] [A layered architecture for log analysis in complex IT systems](https://arxiv.org/abs/2509.08698)
*Thorsten Wittkopp*

Main category: cs.LG

TL;DR: 提出三层架构支持DevOps故障解决：日志调查层实现自动标注和异常分类，异常检测层提供灵活检测方法，根因分析层识别关键日志集，整体提升IT系统可靠性


<details>
  <summary>Details</summary>
Motivation: IT系统复杂性增加给DevOps团队带来维护挑战，需要有效的日志分析方法来提供系统行为和故障的关键洞察

Method: 三层架构：1)日志调查层-自主日志标注和异常分类；2)异常检测层-适应无监督、弱监督和监督训练的灵活检测方法；3)根因分析层-识别最小日志集描述故障、起源和事件序列

Result: 异常检测F1分数0.98-1.0，根因分析在top10候选中检测到90-98%的根因日志行，提供可操作的缓解见解

Conclusion: 集成三层架构为团队提供强大方法，通过优化的日志分析帮助DevOps高效解决故障，增强IT系统可靠性

Abstract: In the evolving IT landscape, stability and reliability of systems are
essential, yet their growing complexity challenges DevOps teams in
implementation and maintenance. Log analysis, a core element of AIOps, provides
critical insights into complex behaviors and failures. This dissertation
introduces a three-layered architecture to support DevOps in failure
resolution. The first layer, Log Investigation, performs autonomous log
labeling and anomaly classification. We propose a method that labels log data
without manual effort, enabling supervised training and precise evaluation of
anomaly detection. Additionally, we define a taxonomy that groups anomalies
into three categories, ensuring appropriate method selection. The second layer,
Anomaly Detection, detects behaviors deviating from the norm. We propose a
flexible Anomaly Detection method adaptable to unsupervised, weakly supervised,
and supervised training. Evaluations on public and industry datasets show
F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third
layer, Root Cause Analysis, identifies minimal log sets describing failures,
their origin, and event sequences. By balancing training data and identifying
key services, our Root Cause Analysis method consistently detects 90-98% of
root cause log lines within the top 10 candidates, providing actionable
insights for mitigation. Our research addresses how log analysis methods can be
designed and optimized to help DevOps resolve failures efficiently. By
integrating these three layers, the architecture equips teams with robust
methods to enhance IT system reliability.

</details>


### [84] [Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping](https://arxiv.org/abs/2509.08703)
*Nikasadat Emami,Amirhossein Khalilian-Gourtani,Jianghao Qian,Antoine Ratouchniak,Xupeng Chen,Yao Wang,Adeen Flinker*

Main category: cs.LG

TL;DR: 开发机器学习模型通过ECoG数据预测大脑语言关键区域，替代侵入性的电刺激映射方法，结合神经活动、解剖区域和功能连接特征，达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 电刺激映射(ESM)虽然是临床金标准，但具有侵入性和耗时的问题，需要开发非侵入性替代方法来安全识别语言关键皮层区域。

Method: 分析16名参与者的颅内ECoG数据，整合神经活动信号、解剖区域标签和功能连接特征，使用RBF核支持向量机进行试验级预测，并通过MLP聚合电极级分类。

Result: 最佳模型在保留参与者上表现优异（ROC-AUC: 0.87, PR-AUC: 0.57），结合区域和连接特征的模型性能与完整特征集相当，且优于单独使用任一特征类型。

Conclusion: 结合空间和网络信息与非线形建模能显著改善术前功能映射评估，为安全脑部手术提供有价值的非侵入性替代方案。

Abstract: Identifying cortical regions critical for speech is essential for safe brain
surgery in or near language areas. While Electrical Stimulation Mapping (ESM)
remains the clinical gold standard, it is invasive and time-consuming. To
address this, we analyzed intracranial electrocorticographic (ECoG) data from
16 participants performing speech tasks and developed machine learning models
to directly predict if the brain region underneath each ECoG electrode is
critical. Ground truth labels indicating speech arrest were derived
independently from Electrical Stimulation Mapping (ESM) and used to train
classification models. Our framework integrates neural activity signals,
anatomical region labels, and functional connectivity features to capture both
local activity and network-level dynamics. We found that models combining
region and connectivity features matched the performance of the full feature
set, and outperformed models using either type alone. To classify each
electrode, trial-level predictions were aggregated using an MLP applied to
histogram-encoded scores. Our best-performing model, a trial-level RBF-kernel
Support Vector Machine together with MLP-based aggregation, achieved strong
accuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings
highlight the value of combining spatial and network information with
non-linear modeling to improve functional mapping in presurgical evaluation.

</details>


### [85] [Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing](https://arxiv.org/abs/2509.08709)
*Shun Takagi,Satoshi Hasegawa*

Main category: cs.LG

TL;DR: 提出了一种基于临时TEE模块的服务器扩展方案，实现恶意安全下的差分隐私联邦学习，通过可验证证明和客户端审计来确保服务器行为的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的DP-FTRL方法假设服务器是半诚实的，无法应对实际场景中客户端退出或被破坏的情况。虽然TEE看似是解决方案，但直接实现会带来分叉攻击或可用性问题。

Method: 在服务器端使用临时TEE模块作为可信计算基(TCB)，生成服务器行为的可验证证明。被选中的客户端参与审计这些证明，只需少量额外的通信和计算开销。

Result: 该方案在保持系统可扩展性和活跃性的同时减小了TCB的大小。实验表明在多种实际设置下，对客户端只增加了很小的常数开销。

Conclusion: 提出的框架为恶意安全环境下的DP-FTRL提供了可行的解决方案，通过TEE和客户端审计的结合，在保证隐私的同时维持了系统效率。

Abstract: In cross-device private federated learning, differentially private
follow-the-regularized-leader (DP-FTRL) has emerged as a promising
privacy-preserving method. However, existing approaches assume a semi-honest
server and have not addressed the challenge of securely removing this
assumption. This is due to its statefulness, which becomes particularly
problematic in practical settings where clients can drop out or be corrupted.
While trusted execution environments (TEEs) might seem like an obvious
solution, a straightforward implementation can introduce forking attacks or
availability issues due to state management. To address this problem, our paper
introduces a novel server extension that acts as a trusted computing base (TCB)
to realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral
TEE module on the server side to produce verifiable proofs of server actions.
Some clients, upon being selected, participate in auditing these proofs with
small additional communication and computational demands. This extension
solution reduces the size of the TCB while maintaining the system's scalability
and liveness. We provide formal proofs based on interactive differential
privacy, demonstrating privacy guarantee in malicious settings. Finally, we
experimentally show that our framework adds small constant overhead to clients
in several realistic settings.

</details>


### [86] [Compressing CNN models for resource-constrained systems by channel and layer pruning](https://arxiv.org/abs/2509.08714)
*Ahmed Sadaqa,Di Liu*

Main category: cs.LG

TL;DR: 提出了一种结合通道剪枝和层剪枝的混合剪枝框架，通过逆向应用EfficientNet的缩放原则来缩减CNN模型复杂度，在保持精度的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: CNN模型复杂度不断增加，难以在边缘设备上部署，需要有效的模型压缩技术来解决这一问题。

Method: 结合通道剪枝和层剪枝的混合剪枝框架，逆向应用EfficientNet的缩放原则来缩减网络规模。

Result: 实验显示模型复杂度显著降低，精度损失极小，在NVIDIA JETSON TX2设备上部署时延迟明显减少。

Conclusion: 混合剪枝框架是一种有效的模型压缩方法，能够在保持模型性能的同时显著降低计算复杂度和部署延迟。

Abstract: Convolutional Neural Networks (CNNs) have achieved significant breakthroughs
in various fields. However, these advancements have led to a substantial
increase in the complexity and size of these networks. This poses a challenge
when deploying large and complex networks on edge devices. Consequently, model
compression has emerged as a research field aimed at reducing the size and
complexity of CNNs. One prominent technique in model compression is model
pruning. This paper will present a new technique of pruning that combines both
channel and layer pruning in what is called a "hybrid pruning framework".
Inspired by EfficientNet, a renowned CNN architecture known for scaling up
networks from both channel and layer perspectives, this hybrid approach applies
the same principles but in reverse, where it scales down the network through
pruning. Experiments on the hybrid approach demonstrated a notable decrease in
the overall complexity of the model, with only a minimal reduction in accuracy
compared to the baseline model. This complexity reduction translates into
reduced latency when deploying the pruned models on an NVIDIA JETSON TX2
embedded AI device.

</details>


### [87] [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
*Jeffrey Amico,Gabriel Passamani Andrade,John Donaghy,Ben Fielding,Tristin Forbus,Harry Grieve,Semih Kara,Jari Kolehmainen,Yihua Lou,Christopher Nies,Edward Phillip Flores Nuño,Diogo Ortega,Shikhar Rastogi,Austin Virts,Matthew J. Wright*

Main category: cs.LG

TL;DR: SAPO是一种完全去中心化的异步RL后训练算法，专为异构计算节点网络设计，避免了传统RL扩展中的瓶颈问题，在实验中实现了高达94%的累积奖励增益。


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练需要大规模并行化推理，存在延迟、内存、可靠性等技术挑战以及高昂成本，需要一种去中心化的解决方案。

Method: 提出Swarm sAmpling Policy Optimization (SAPO)算法，在异构计算节点网络中，每个节点管理自己的策略模型，通过网络"共享"rollouts，无需对延迟、模型同质性或硬件做假设。

Result: 在受控实验中实现了高达94%的累积奖励增益，并在数千个节点组成的网络上进行了测试，展示了算法的可扩展性和实用性。

Conclusion: SAPO成功解决了RL后训练的扩展瓶颈问题，为去中心化RL训练提供了有效方案，并展示了在实际异构网络环境中的良好性能。

Abstract: Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs
requires significant parallelization to scale-up inference, which introduces
non-trivial technical challenges (e.g. latency, memory, and reliability)
alongside ever-growing financial costs. We present Swarm sAmpling Policy
Optimization (SAPO), a fully decentralized and asynchronous RL post-training
algorithm. SAPO is designed for decentralized networks of heterogenous compute
nodes, where each node manages its own policy model(s) while "sharing" rollouts
with others in the network; no explicit assumptions about latency, model
homogeneity, or hardware are required and nodes can operate in silo if desired.
As a result, the algorithm avoids common bottlenecks in scaling RL
post-training while also allowing (and even encouraging) new possibilities. By
sampling rollouts "shared" across the network, it enables "Aha moments" to
propagate, thereby bootstrapping the learning process. In this paper we show
SAPO achieved cumulative reward gains of up to 94% in controlled experiments.
We also share insights from tests on a network with thousands of nodes
contributed by Gensyn community members running the algorithm on diverse
hardware and models during an open-source demo.

</details>


### [88] [Data-driven generative simulation of SDEs using diffusion models](https://arxiv.org/abs/2509.08731)
*Xuefeng Gao,Jiale Zha,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 本文提出使用扩散模型生成未知随机微分方程样本路径的新方法，无需显式漂移和扩散系数，通过数据驱动方式从有限样本生成新路径


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法模拟随机微分方程需要明确的系数参数，而实际应用中这些参数往往未知。本文旨在开发一种无需模型假设、纯数据驱动的方法来生成SDE样本路径

Method: 采用条件扩散模型，基于有限的实际样本路径数据，生成相同随机微分方程的合成路径。通过模拟实验与神经SDE等基准方法进行比较验证

Result: 方法在模拟实验中表现优于基准方法。在实证研究中，使用合成样本路径增强了连续时间均值-方差投资组合选择中强化学习算法的性能

Conclusion: 扩散模型在金融分析和决策中具有广阔应用前景，特别是在随机过程建模和投资组合优化等领域，为模型未知的随机系统提供了有效的路径生成解决方案

Abstract: This paper introduces a new approach to generating sample paths of unknown
stochastic differential equations (SDEs) using diffusion models, a class of
generative AI models commonly employed in image and video applications. Unlike
the traditional Monte Carlo methods for simulating SDEs, which require explicit
specifications of the drift and diffusion coefficients, our method takes a
model-free, data-driven approach. Given a finite set of sample paths from an
SDE, we utilize conditional diffusion models to generate new, synthetic paths
of the same SDE. To demonstrate the effectiveness of our approach, we conduct a
simulation experiment to compare our method with alternative benchmark ones
including neural SDEs. Furthermore, in an empirical study we leverage these
synthetically generated sample paths to enhance the performance of
reinforcement learning algorithms for continuous-time mean-variance portfolio
selection, hinting promising applications of diffusion models in financial
analysis and decision-making.

</details>


### [89] [DEQuify your force field: More efficient simulations using deep equilibrium models](https://arxiv.org/abs/2509.08734)
*Andreas Burger,Luca Thiede,Alán Aspuru-Guzik,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: 将等变力场模型重构为深度平衡模型，利用分子动力学模拟的连续性特征，通过重用先前时间步的中间特征，在精度和速度上提升10-20%


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟具有连续性特征，连续状态间高度相似，这一先验信息尚未被充分利用

Method: 将最先进的等变基础模型重构为深度平衡模型，重用先前时间步的中间神经网络特征

Result: 在MD17、MD22和OC20 200k数据集上，相比非DEQ基础模型，精度和速度均提升10-20%，训练内存效率更高，能够训练更大系统的更表达性模型

Conclusion: 利用分子动力学模拟的连续性特征，通过深度平衡模型重用中间特征，能有效提升力场模型的性能和效率

Abstract: Machine learning force fields show great promise in enabling more accurate
molecular dynamics simulations compared to manually derived ones. Much of the
progress in recent years was driven by exploiting prior knowledge about
physical systems, in particular symmetries under rotation, translation, and
reflections. In this paper, we argue that there is another important piece of
prior information that, thus fa,r hasn't been explored: Simulating a molecular
system is necessarily continuous, and successive states are therefore extremely
similar. Our contribution is to show that we can exploit this information by
recasting a state-of-the-art equivariant base model as a deep equilibrium
model. This allows us to recycle intermediate neural network features from
previous time steps, enabling us to improve both accuracy and speed by
$10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ
base model. The training is also much more memory efficient, allowing us to
train more expressive models on larger systems.

</details>


### [90] [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)
*Dong Han,Zhehong Ai,Pengxiang Cai,Shuzhou Sun,Shanya Lu,Jianpeng Chen,Ben Gao,Lingli Ge,Weida Wang,Xiangxin Zhou,Xihui Liu,Mao Su,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Tao XU,Yuqiang Li,Shufei Zhang*

Main category: cs.LG

TL;DR: ChemBOMAS是一个LLM增强的多智能体系统，通过知识驱动的粗粒度优化和数据驱动的细粒度优化策略，显著提升了贝叶斯优化在化学领域的效率和效果，在真实实验中达到了96%的最优目标值。


<details>
  <summary>Details</summary>
Motivation: 化学领域的贝叶斯优化常常受到稀疏实验数据和复杂反应机制的限制，需要开发新的方法来克服这些挑战。

Method: ChemBOMAS采用双阶段策略：1）知识驱动的粗粒度优化阶段，LLM基于化学知识分解搜索空间识别候选区域；2）数据驱动的细粒度优化阶段，LLM生成伪数据点增强BO过程，提高数据利用效率和收敛速度。

Result: 基准评估显示ChemBOMAS显著优于各种BO算法。在湿实验室实验中，针对一个先前未报道的具有挑战性的化学反应，ChemBOMAS达到了96%的最优目标值，远高于领域专家的15%。

Conclusion: ChemBOMAS是一个强大的工具，能够加速化学发现过程，在真实世界应用中表现出色。

Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by sparse experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by LLMs and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
LLMs intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, LLMs enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.

</details>


### [91] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出了AgentGym-RL框架和ScalingInter-RL训练方法，用于通过强化学习训练LLM智能体进行多轮交互决策，在27个任务上达到或超越商业模型性能


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的交互式强化学习框架来训练LLM智能体从零开始解决复杂现实任务，而不依赖监督微调

Method: 采用模块化解耦架构的AgentGym-RL框架，支持主流RL算法；提出ScalingInter-RL训练方法，通过逐步增加交互范围来平衡探索与利用

Result: 在多样化环境中验证了框架的稳定性和有效性，在27个任务上达到或超越商业模型性能

Conclusion: 该框架为开发下一代智能智能体提供了重要基础，将开源完整框架包括代码和数据集

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [92] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 开发了一个基于深度强化学习的AI决策支持系统，用于优化大规模伤亡事件中的患者转运分配，通过人机协作显著提高了决策质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件（MCIs）会压垮医疗系统，需要在极端压力下做出快速准确的患者-医院分配决策。现有的决策过程往往依赖人工经验，存在效率低下和一致性差的问题。

Method: 开发了基于深度强化学习的AI代理，平衡患者病情严重程度、专科护理需求、医院容量和运输物流。构建了MasTER网络可访问指挥仪表板进行MCI管理模拟，并通过30名参与者（6名创伤专家和24名非专家）的用户研究，比较了三种交互方式（纯人工、人机协作、纯AI）在20和60名患者MCI场景中的表现。

Result: AI参与度增加显著提高了决策质量和一致性。AI代理表现优于创伤外科医生（p < 0.001），并能使非专家在辅助下达到专家水平，与其未辅助时的显著较差表现形成鲜明对比（p < 0.001）。

Conclusion: 这项研究确立了AI驱动决策支持在增强MCI准备培训和现实世界应急响应管理方面的潜力，为人机协作在紧急医疗决策中的应用提供了有力证据。

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


### [93] [Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning](https://arxiv.org/abs/2509.08759)
*Mominul Rubel,Adam Meyers,Gabriel Nicolosi*

Main category: cs.LG

TL;DR: FLM是一种新型神经网络架构，使用余弦激活函数学习傅里叶级数的频率、振幅和相移参数，能够表示多维非谐波傅里叶级数，在科学计算问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够表示多维完整可分离傅里叶基的神经网络架构，解决传统傅里叶启发模型无法在多维空间中表示完整基的问题。

Method: 采用前馈结构，使用余弦激活函数，将频率、振幅和相移作为可训练参数，建立傅里叶系数与振幅相移之间的一一对应关系。

Result: 在偏微分方程和最优控制问题等科学计算任务中，FLM性能与SIREN和标准前馈神经网络相当甚至更优。

Conclusion: FLM是第一个能够在多维空间中表示完整可分离傅里叶基的标准MLP式架构，为科学计算提供了有效的谱基表示方法。

Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN)
architecture designed to represent a multidimensional nonharmonic Fourier
series. The FLM uses a simple feedforward structure with cosine activation
functions to learn the frequencies, amplitudes, and phase shifts of the series
as trainable parameters. This design allows the model to create a
problem-specific spectral basis adaptable to both periodic and nonperiodic
functions. Unlike previous Fourier-inspired NN models, the FLM is the first
architecture able to represent a complete, separable Fourier basis in multiple
dimensions using a standard Multilayer Perceptron-like architecture. A
one-to-one correspondence between the Fourier coefficients and amplitudes and
phase-shifts is demonstrated, allowing for the translation between a full,
separable basis form and the cosine phase--shifted one. Additionally, we
evaluate the performance of FLMs on several scientific computing problems,
including benchmark Partial Differential Equations (PDEs) and a family of
Optimal Control Problems (OCPs). Computational experiments show that the
performance of FLMs is comparable, and often superior, to that of established
architectures like SIREN and vanilla feedforward NNs.

</details>


### [94] [ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals](https://arxiv.org/abs/2509.08779)
*Ali Amini,Mohammad Alijanpour,Behnam Latifi,Ali Motie Nasrabadi*

Main category: cs.LG

TL;DR: 本文提出了ADHDeepNet深度学习模型，利用EEG信号和时空特征提取、注意力机制来提高ADHD诊断的准确性和效率，在121名参与者数据集上取得了99.17%的准确率。


<details>
  <summary>Details</summary>
Motivation: ADHD是一种常见的脑部疾病，早期诊断对患者和社会都很重要，但传统诊断方法耗时耗力。需要开发更精准、高效的诊断方法来改善这一状况。

Method: 提出ADHDeepNet深度学习模型，整合时空特征提取、注意力模块和可解释性技术。采用两阶段方法：10折交叉验证优化超参数，使用加性高斯噪声进行数据增强，并通过t-SNE可视化分析模型决策。

Result: 在61名ADHD患者和60名健康对照者的数据集上，模型达到了100%的敏感性和99.17%的准确率，成功识别出关键的脑区和频率波段。

Conclusion: 研究表明深度学习和EEG信号结合在ADHD诊断中具有巨大潜力，能够显著提高诊断准确性和效率，为临床诊断提供了新的技术途径。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.

</details>


### [95] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: MoT是一个轻量级框架，通过交替进行教师特定监督微调和权重空间合并，将多个教师模型的推理能力统一到学生模型中，在仅使用200个高质量CoT样本的情况下，使14B参数模型超越多个大型模型。


<details>
  <summary>Details</summary>
Motivation: 传统推理蒸馏假设单一最优教师，但实践中存在多个候选教师和不断增长的CoT语料库。研究发现不同学生有不同"最佳教师"，且同一学生的最佳教师在不同数据集上也会变化。

Method: 提出Merge-of-Thought Distillation (MoT)框架，交替进行教师特定的监督微调分支和结果学生变体的权重空间合并，以统一多个教师的推理能力并克服监督冲突。

Result: 在数学竞赛基准测试中，使用仅约200个高质量CoT样本，Qwen3-14B学生模型超越了DEEPSEEK-R1、QWEN3-30B-A3B等多个强模型，性能显著提升。MoT持续优于最佳单教师蒸馏和朴素多教师联合方法。

Conclusion: MoT作为一种简单、可扩展的路径，能够高效地从多样化教师中蒸馏长链推理能力到紧凑学生模型中，减少灾难性遗忘，提升通用推理能力，甚至培养出更好的教师模型。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


### [96] [A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management](https://arxiv.org/abs/2509.08822)
*Willy Sucipto,Jianlong Zhou,Ray Seung Min Kwon,Fang Chen*

Main category: cs.LG

TL;DR: 这篇综述论文探讨了TinyML技术在养蜂业中的应用，重点关注蜂群监测、行为识别、病虫害检测和分蜂预测四个关键功能领域，旨在为可持续传粉媒介管理提供可扩展的AI驱动监测系统。


<details>
  <summary>Details</summary>
Motivation: 传统蜂箱检查方法劳动密集且具有干扰性，而基于云的监测方案在偏远或资源有限的养蜂场不实用。物联网和TinyML技术的发展为边缘设备提供了低功耗、实时监测的替代方案。

Method: 通过综合调研当前TinyML与养蜂业交叉领域的最新创新，围绕四个功能领域组织分析：监测蜂箱条件、识别蜜蜂行为、检测病虫害和预测分蜂事件。

Result: 论文整合了公开可用数据集、针对嵌入式部署优化的轻量级模型架构以及适应现场约束的基准测试策略，为相关研究提供了系统性的资源支持。

Conclusion: 尽管存在数据稀缺、泛化挑战和离网环境部署障碍等限制，但超高效推理管道、自适应边缘学习和数据集标准化等新兴机会为可持续传粉媒介管理提供了重要基础。

Abstract: Honey bee colonies are essential for global food security and ecosystem
stability, yet they face escalating threats from pests, diseases, and
environmental stressors. Traditional hive inspections are labor-intensive and
disruptive, while cloud-based monitoring solutions remain impractical for
remote or resource-limited apiaries. Recent advances in Internet of Things
(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring
directly on edge devices, offering scalable and non-invasive alternatives. This
survey synthesizes current innovations at the intersection of TinyML and
apiculture, organized around four key functional areas: monitoring hive
conditions, recognizing bee behaviors, detecting pests and diseases, and
forecasting swarming events. We further examine supporting resources, including
publicly available datasets, lightweight model architectures optimized for
embedded deployment, and benchmarking strategies tailored to field constraints.
Critical limitations such as data scarcity, generalization challenges, and
deployment barriers in off-grid environments are highlighted, alongside
emerging opportunities in ultra-efficient inference pipelines, adaptive edge
learning, and dataset standardization. By consolidating research and
engineering practices, this work provides a foundation for scalable, AI-driven,
and ecologically informed monitoring systems to support sustainable pollinator
management.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [97] [LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models](https://arxiv.org/abs/2509.08031)
*Sidharth Surapaneni,Hoang Nguyen,Jash Mehta,Aman Tiwari,Oluwanifemi Bamgbose,Akshay Kalkunte,Sai Rajeswar,Sathwik Tejaswi Madhusudhan*

Main category: cs.SD

TL;DR: LALM-Eval是一个高效的大音频语言模型评估框架，解决了现有工具包处理慢、提示不一致和任务覆盖窄的问题，速度提升127%，并引入了新的评估类别来揭示模型在时间理解和复杂音频推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型（LALMs）的评估工具存在三个关键问题：处理速度慢阻碍大规模研究、提示不一致影响可重现性、任务覆盖范围窄无法全面评估音频推理能力，这限制了模型的公平比较和系统性评估。

Method: 开发了LALM-Eval框架，通过优化的批处理和并行执行实现高达127%的速度提升；提供标准化提示协议和灵活配置；引入两个新评估类别：LLM-Adaptive Diarization（时间音频理解）和Spoken Language Reasoning（复杂音频认知任务）。

Result: 在380多个任务上的评估显示，当前LALMs在时间理解和复杂口语推理任务方面存在显著差距；发现音频基准测试中指令模态缺乏标准化，在复杂指令跟随下游任务上可导致高达9.5个绝对百分点的性能差异。

Conclusion: LALM-Eval不仅提供了实用的评估工具，还揭示了模型局限性，为系统性LALM发展提供了重要见解和推动力。

Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating
them remains challenging due to inefficient toolkits that limit fair comparison
and systematic assessment. Current frameworks suffer from three critical
issues: slow processing that bottlenecks large-scale studies, inconsistent
prompting that hurts reproducibility, and narrow task coverage that misses
important audio reasoning capabilities. We introduce LALM-Eval, an efficient
and comprehensive evaluation framework for LALMs. Our system achieves a speedup
of up to 127% over existing toolkits through optimized batch processing and
parallel execution, enabling large-scale evaluations previously impractical. We
provide standardized prompting protocols and flexible configurations for fair
model comparison across diverse scenarios. Additionally, we introduce two new
evaluation categories: LLM-Adaptive Diarization for temporal audio
understanding and Spoken Language Reasoning for complex audio-based cognitive
tasks. Through evaluation across 380+ tasks, we reveal significant gaps in
current LALMs, particularly in temporal understanding and complex spoken
language reasoning tasks. Our findings also highlight a lack of standardization
in instruction modality existent across audio benchmarks, which can lead up
performance differences up to 9.5 absolute points on the challenging complex
instruction following downstream tasks. LALM-Eval provides both practical
evaluation tools and insights into model limitations, advancing systematic LALM
development.

</details>


### [98] [Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition](https://arxiv.org/abs/2509.08454)
*Yujian Ma,Jinqiu Sang,Ruizhe Li*

Main category: cs.SD

TL;DR: 对Whisper语音模型在语音情感识别任务中LoRA微调机制的首个系统性可解释性研究，揭示了延迟专业化和前向对齐、后向分化的关键机制


<details>
  <summary>Details</summary>
Motivation: 大型预训练语音模型如Whisper具有强大的泛化能力，但资源高效的适应方法（如LoRA）在语音任务中的工作机制仍不明确，需要深入研究其内在机制

Method: 使用层贡献探测、logit-lens检查、奇异值分解(SVD)和中心核对齐(CKA)等分析工具，系统研究Whisper编码器中LoRA的机制

Result: 发现了两个关键机制：延迟专业化过程（早期层保留通用特征，后期层整合任务特定信息）和前向对齐、后向分化动态（LoRA矩阵间的相互作用）

Conclusion: 阐明了LoRA如何重塑编码器层次结构，为设计高效且可解释的大型语音模型适应策略提供了实证见解和机制理解

Abstract: Large pre-trained speech models such as Whisper offer strong generalization
but pose significant challenges for resource-efficient adaptation. Low-Rank
Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method,
yet its underlying mechanisms in speech tasks remain poorly understood. In this
work, we conduct the first systematic mechanistic interpretability study of
LoRA within the Whisper encoder for speech emotion recognition (SER). Using a
suite of analytical tools, including layer contribution probing, logit-lens
inspection, and representational similarity via singular value decomposition
(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a
delayed specialization process that preserves general features in early layers
before consolidating task-specific information, and a forward alignment,
backward differentiation dynamic between LoRA's matrices. Our findings clarify
how LoRA reshapes encoder hierarchies, providing both empirical insights and a
deeper mechanistic understanding for designing efficient and interpretable
adaptation strategies in large speech models.

</details>


### [99] [Explainability of CNN Based Classification Models for Acoustic Signal](https://arxiv.org/abs/2509.08717)
*Zubair Faruqui,Mackenzie S. McIntire,Rahul Dubey,Jay McEntee*

Main category: cs.SD

TL;DR: 本研究应用多种可解释人工智能(XAI)技术分析北美鸟类鸣声的地理变异，CNN分类准确率达94.8%，不同XAI方法提供互补解释，组合使用可增强模型可信度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然XAI在声学领域应用日益广泛，但在生物声学（分析生物音频信号）中的应用仍相对不足。本研究旨在探索XAI在鸟类地理变异鸣声分析中的应用价值。

Method: 将音频录音转换为频谱图图像，训练深度卷积神经网络(CNN)进行分类，应用模型无关(LIME、SHAP)和模型特定(DeepLIFT、Grad-CAM)的XAI技术解释模型预测。

Result: CNN分类准确率达到94.8%，不同XAI技术产生不同但互补的解释，组合使用时能提供更完整和可解释的模型决策洞察。

Conclusion: 这项工作强调了组合使用多种XAI技术的重要性，不仅能提高声学信号分析中的信任度和互操作性，在其他领域特定任务中也具有广泛适用性。

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a critical tool for
interpreting the predictions of complex deep learning models. While XAI has
been increasingly applied in various domains within acoustics, its use in
bioacoustics, which involves analyzing audio signals from living organisms,
remains relatively underexplored. In this paper, we investigate the
vocalizations of a bird species with strong geographic variation throughout its
range in North America. Audio recordings were converted into spectrogram images
and used to train a deep Convolutional Neural Network (CNN) for classification,
achieving an accuracy of 94.8\%. To interpret the model's predictions, we
applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,
Grad-CAM) XAI techniques. These techniques produced different but complementary
explanations, and when their explanations were considered together, they
provided more complete and interpretable insights into the model's
decision-making. This work highlights the importance of using a combination of
XAI techniques to improve trust and interoperability, not only in broader
acoustics signal analysis but also argues for broader applicability in
different domain specific tasks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [100] [Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor](https://arxiv.org/abs/2509.08020)
*Matthew Carter,Lee Devlin,Alexander Philips,Edward Pyzer-Knapp,Paul Spirakis,Simon Maskell*

Main category: q-bio.QM

TL;DR: 提出了一个基于机会计算的SMC采样器框架，利用闲置计算资源进行大规模蛋白质组学贝叶斯推断，无需专用高性能计算设施


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯模型在蛋白质组学数据分析中计算密集，限制了先进建模技术的可及性。现有高性能SMC实现依赖专用硬件，成本高昂

Method: 开发了Coordinator-Manager-Follower架构的机会计算框架，通过HTCondor利用利物浦大学闲置计算资源，减少同步开销并支持异构不可靠环境

Result: 在真实蛋白质组学模型上验证显示，机会SMC能够提供准确推断，在固定时间预算下随着资源增加生成更多样本，实现弱扩展性

Conclusion: CondorSMC开源包使SMC采样器能够在机会计算环境中部署，为大规模蛋白质组学推断提供了可扩展且成本效益高的解决方案

Abstract: Quantitative proteomics plays a central role in uncovering regulatory
mechanisms, identifying disease biomarkers, and guiding the development of
precision therapies. These insights are often obtained through complex Bayesian
models, whose inference procedures are computationally intensive, especially
when applied at scale to biological datasets. This limits the accessibility of
advanced modelling techniques needed to fully exploit proteomics data. Although
Sequential Monte Carlo (SMC) methods offer a parallelisable alternative to
traditional Markov Chain Monte Carlo, their high-performance implementations
often rely on specialised hardware, increasing both financial and energy costs.
We address these challenges by introducing an opportunistic computing framework
for SMC samplers, tailored to the demands of large-scale proteomics inference.
Our approach leverages idle compute resources at the University of Liverpool
via HTCondor, enabling scalable Bayesian inference without dedicated
high-performance computing infrastructure. Central to this framework is a novel
Coordinator-Manager-Follower architecture that reduces synchronisation overhead
and supports robust operation in heterogeneous, unreliable environments. We
evaluate the framework on a realistic proteomics model and show that
opportunistic SMC delivers accurate inference with weak scaling, increasing
samples generated under a fixed time budget as more resources join. To support
adoption, we release CondorSMC, an open-source package for deploying SMC
samplers in opportunistic computing environments.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [101] [Forecasting Generative Amplification](https://arxiv.org/abs/2509.08048)
*Henning Bahl,Sascha Diefenbacher,Nina Elmer,Tilman Plehn,Jonas Spinner*

Main category: hep-ph

TL;DR: 本文提出了两种互补的方法来估计生成网络在LHC模拟中的统计精度放大因子，无需大型保留数据集。平均放大方法使用贝叶斯网络或集成学习，微分放大方法使用假设检验。应用结果表明在特定相空间区域可以实现放大，但尚未在整个分布中实现。


<details>
  <summary>Details</summary>
Motivation: 生成网络是提高LHC模拟速度和精度的理想工具，但需要理解其统计精度，特别是在生成超出训练数据集规模的事件时。

Method: 提出了两种方法：1）平均放大方法 - 使用贝叶斯网络或集成学习通过相空间体积积分精度来估计放大因子；2）微分放大方法 - 使用假设检验来量化放大，无需任何分辨率损失。

Result: 将这两种方法应用于最先进的事件生成器，结果表明在特定相空间区域可以实现放大，但尚未在整个分布中实现放大。

Conclusion: 虽然生成网络在LHC模拟的特定区域能够实现统计精度放大，但要实现整个相空间分布的全面放大仍需进一步研究和发展。

Abstract: Generative networks are perfect tools to enhance the speed and precision of
LHC simulations. It is important to understand their statistical precision,
especially when generating events beyond the size of the training dataset. We
present two complementary methods to estimate the amplification factor without
large holdout datasets. Averaging amplification uses Bayesian networks or
ensembling to estimate amplification from the precision of integrals over given
phase-space volumes. Differential amplification uses hypothesis testing to
quantify amplification without any resolution loss. Applied to state-of-the-art
event generators, both methods indicate that amplification is possible in
specific regions of phase space, but not yet across the entire distribution.

</details>


### [102] [Agents of Discovery](https://arxiv.org/abs/2509.08535)
*Sascha Diefenbacher,Anna Hallin,Gregor Kasieczka,Michael Krämer,Anne Lauscher,Tim Lukas*

Main category: hep-ph

TL;DR: 使用大型语言模型构建代理团队来自动化粒子物理数据分析，在LHC Olympics异常检测任务中达到接近人类最佳水平的性能


<details>
  <summary>Details</summary>
Motivation: 现代粒子物理研究面临海量数据和复杂分析工具的挑战，需要自动化解决方案来应对日益复杂的工具链

Method: 构建基于大型语言模型的代理团队，让多个LLM实例协作解决数据分析问题，通过编写代码操作标准工具库（包括机器学习系统）并迭代改进结果

Result: 基于GPT-4o、o4-mini、GPT-4.1和GPT-5等模型的代理系统能够解决数据分析问题，最佳代理创建方案的性能与人类最先进结果相当

Conclusion: 基于大型语言模型的代理系统有潜力自动化常规分析组件，有效应对现代工具链的复杂性挑战

Abstract: The substantial data volumes encountered in modern particle physics and other
domains of fundamental physics research allow (and require) the use of
increasingly complex data analysis tools and workflows. While the use of
machine learning (ML) tools for data analysis has recently proliferated, these
tools are typically special-purpose algorithms that rely, for example, on
encoded physics knowledge to reach optimal performance. In this work, we
investigate a new and orthogonal direction: Using recent progress in large
language models (LLMs) to create a team of agents -- instances of LLMs with
specific subtasks -- that jointly solve data analysis-based research problems
in a way similar to how a human researcher might: by creating code to operate
standard tools and libraries (including ML systems) and by building on results
of previous iterations. If successful, such agent-based systems could be
deployed to automate routine analysis components to counteract the increasing
complexity of modern tool chains. To investigate the capabilities of
current-generation commercial LLMs, we consider the task of anomaly detection
via the publicly available and highly-studied LHC Olympics dataset. Several
current models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated
and their stability tested. Overall, we observe the capacity of the agent-based
system to solve this data analysis problem. The best agent-created solutions
mirror the performance of human state-of-the-art results.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [103] [QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction](https://arxiv.org/abs/2509.08817)
*Tobias Winker,Jinghua Groppe,Sven Groppe*

Main category: quant-ph

TL;DR: 提出基于量子机器学习的量子基数估计方法QCardEst和量子基数校正QCardCorr，通过混合量子经典网络处理SQL查询，显著提升数据库查询优化的基数估计精度


<details>
  <summary>Details</summary>
Motivation: 基数估计是数据库查询优化的重要环节，传统方法存在精度不足的问题，需要利用量子计算的优势来提升估计准确性

Method: 使用紧凑编码将SQL查询转换为量子态，仅需与查询表数量相等的量子比特；采用变分量子电路(VQC)处理完整查询；比较多种经典后处理层将概率向量输出转换为基数值；提出QCardCorr通过VQC生成校正因子改进经典估计器

Result: QCardCorr相比PostgreSQL标准优化器在JOB-light上提升6.37倍，在STATS上提升8.66倍；在JOB-light上甚至超越MSCN 3.47倍

Conclusion: 量子机器学习方法在数据库基数估计方面展现出显著优势，QCardEst和QCardCorr能够有效提升查询优化性能，为量子计算在数据库领域的应用提供了有前景的方向

Abstract: Cardinality estimation is an important part of query optimization in DBMS. We
develop a Quantum Cardinality Estimation (QCardEst) approach using Quantum
Machine Learning with a Hybrid Quantum-Classical Network. We define a compact
encoding for turning SQL queries into a quantum state, which requires only
qubits equal to the number of tables in the query. This allows the processing
of a complete query with a single variational quantum circuit (VQC) on current
hardware. In addition, we compare multiple classical post-processing layers to
turn the probability vector output of VQC into a cardinality value. We
introduce Quantum Cardinality Correction QCardCorr, which improves classical
cardinality estimators by multiplying the output with a factor generated by a
VQC to improve the cardinality estimation. With QCardCorr, we have an
improvement over the standard PostgreSQL optimizer of 6.37 times for JOB-light
and 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of
3.47.

</details>


### [104] [FeynmanDD: Quantum Circuit Analysis with Classical Decision Diagrams](https://arxiv.org/abs/2509.08276)
*Ziyuan Wang,Bin Cheng,Longxiang Yuan,Zhengfeng Ji*

Main category: quant-ph

TL;DR: FeynmanDD是一种基于决策图的新方法，用于量子电路仿真和等价性检查，通过路径积分公式将分析转化为计数问题


<details>
  <summary>Details</summary>
Motivation: 现有量子电路分析方法主要利用量子态和算子的模式，需要探索新的结构和方法来提升分析效率

Method: 利用标准和多终端决策图，将量子电路分析转化为路径积分公式中的计数问题，并采用高效的决策图计数算法

Result: 通过理论分析和数值实验验证了FeynmanDD在量子电路分析中的能力和局限性

Conclusion: 这种基于BDD的新方法在量子电路分析中具有重要价值

Abstract: Applications of decision diagrams in quantum circuit analysis have been an
active research area. Our work introduces FeynmanDD, a new method utilizing
standard and multi-terminal decision diagrams for quantum circuit simulation
and equivalence checking. Unlike previous approaches that exploit patterns in
quantum states and operators, our method explores useful structures in the path
integral formulation, essentially transforming the analysis into a counting
problem. The method then employs efficient counting algorithms using decision
diagrams as its underlying computational engine. Through comprehensive
theoretical analysis and numerical experiments, we demonstrate FeynmanDD's
capabilities and limitations in quantum circuit analysis, highlighting the
value of this new BDD-based approach.

</details>


### [105] [From Physical to Logical: Graph-State-Based Connectivity in Quantum Networks](https://arxiv.org/abs/2509.08384)
*Mateo M. Blanco,Manuel Fernández-Veiga,Ana Fernández-Vilas,Rebeca P. Díaz-Redondo*

Main category: quant-ph

TL;DR: 本文扩展了基于双星配置的量子网络方法，提出了更复杂的多星拓扑结构，分析了m个交换机各连接n个客户端时的最大连接性，包括非对称情况，并支持可扩展量子网络的发展。


<details>
  <summary>Details</summary>
Motivation: 纠缠是量子通信的关键资源，但传统的双方案在量子秘密共享或分布式计算等高级协议中往往不足。图状态为量子网络中的多部分纠缠提供了灵活的表示和管理方式。

Method: 扩展基于双星配置的现有方法到更复杂的多星拓扑结构，分析m个交换机各连接n个客户端的网络最大连接性，包括非对称情况，并提出实现远距离节点间逻辑通信的方法。

Result: 研究结果支持开发具有丰富连接性的可扩展量子网络，超越了传统的双结构限制。

Conclusion: 多星拓扑结构为量子网络提供了更强大的连接能力和可扩展性，为高级量子通信协议的实施奠定了基础。

Abstract: Entanglement is a key resource in quantum communication, but bipartite
schemes are often insufficient for advanced protocols like quantum secret
sharing or distributed computing. Graph states offer a flexible way to
represent and manage multipartite entanglement in quantum networks, enabling
logical connectivity through local operations and classical communication
(LOCC). In this work, we extend existing approaches based on bi-star
configurations to more complex multi-star topologies. We analyze the maximum
connectivity that can be achieved in networks of $m$ switches, each connected
to $n$ clients, including asymmetric cases where the number of clients varies
per switch. We also propose methods to enable logical communication between
distant nodes. Our results support the development of scalable quantum networks
with rich connectivity beyond traditional bipartite structures.

</details>


### [106] [Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions](https://arxiv.org/abs/2509.08654)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: quant-ph

TL;DR: 提出基于特征的POMDP框架结合图神经网络，解决量子网络路由中的部分可观测性、退相干和可扩展性挑战，在模拟实验中显著提升路由保真度和纠缠传递率。


<details>
  <summary>Details</summary>
Motivation: 量子网络路由面临部分可观测性、量子退相干和时间变化信道噪声等挑战，传统方法难以有效处理这些动态量子系统的复杂性。

Method: 采用混合GNN-POMDP架构，将量子网络动态编码到低维特征空间，结合信念状态规划和图神经网络处理图结构纠缠链路，并集成噪声自适应机制。

Result: 在多达100个节点的模拟量子网络中，相比最先进基线方法，在路由保真度和纠缠传递率方面取得显著提升，特别是在高退相干和非平稳条件下表现优异。

Conclusion: 该框架通过结合POMDP和GNN，有效解决了量子网络路由的关键挑战，提供了可扩展的解决方案，并为信念收敛、策略改进和噪声鲁棒性提供了理论保证。

Abstract: This paper presents a feature-based Partially Observable Markov Decision
Process (POMDP) framework for quantum network routing, combining belief-state
planning with Graph Neural Networks (GNNs) to address partial observability,
decoherence, and scalability challenges in dynamic quantum systems. Our
approach encodes complex quantum network dynamics, including entanglement
degradation and time-varying channel noise, into a low-dimensional feature
space, enabling efficient belief updates and scalable policy learning. The core
of our framework is a hybrid GNN-POMDP architecture that processes
graph-structured representations of entangled links to learn routing policies,
coupled with a noise-adaptive mechanism that fuses POMDP belief updates with
GNN outputs for robust decision making. We provide a theoretical analysis
establishing guarantees for belief convergence, policy improvement, and
robustness to noise. Experiments on simulated quantum networks with up to 100
nodes demonstrate significant improvements in routing fidelity and entanglement
delivery rates compared to state-of-the-art baselines, particularly under high
decoherence and nonstationary conditions.

</details>


### [107] [RAPID Quantum Detection and Demodulation of Covert Communications: Breaking the Noise Limit with Solid-State Spin Sensors](https://arxiv.org/abs/2509.08171)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: quant-ph

TL;DR: RAPID是一个基于固态自旋传感器的两阶段混合策略框架，用于检测和解调隐蔽电磁信号，通过量子Fisher信息矩阵和深度强化学习实现超越经典噪声底限的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在安全关键应用中部署的量子传感器框架，用于电子战和隐蔽监视，需要解决在相关噪声环境中保持高精度估计的挑战。

Method: 采用两阶段混合策略：首先基于量子Fisher信息矩阵计算鲁棒的非自适应基线协议，然后使用深度强化学习（Soft Actor-Critic）学习在线自适应策略，动态优化控制脉冲、询问时间和测量基。

Result: 数值模拟显示该协议相比静态方法获得显著灵敏度增益，在相关噪声环境中保持高估计精度，传感器阵列应用可实现海森堡级精度的相干量子波束成形。

Conclusion: 该工作为在安全关键应用中部署量子传感器建立了理论严谨且实际可行的途径，实现了超越经典方法的量子增强性能。

Abstract: We introduce a comprehensive framework for the detection and demodulation of
covert electromagnetic signals using solid-state spin sensors. Our approach,
named RAPID, is a two-stage hybrid strategy that leverages nitrogen-vacancy
(NV) centers to operate below the classical noise floor employing a robust
adaptive policy via imitation and distillation. We first formulate the joint
detection and estimation task as a unified stochastic optimal control problem,
optimizing a composite Bayesian risk objective under realistic physical
constraints. The RAPID algorithm solves this by first computing a robust,
non-adaptive baseline protocol grounded in the quantum Fisher information
matrix (QFIM), and then using this baseline to warm-start an online, adaptive
policy learned via deep reinforcement learning (Soft Actor-Critic). This method
dynamically optimizes control pulses, interrogation times, and measurement
bases to maximize information gain while actively suppressing non-Markovian
noise and decoherence. Numerical simulations demonstrate that the protocol
achieves a significant sensitivity gain over static methods, maintains high
estimation precision in correlated noise environments, and, when applied to
sensor arrays, enables coherent quantum beamforming that achieves
Heisenberg-like scaling in precision. This work establishes a theoretically
rigorous and practically viable pathway for deploying quantum sensors in
security-critical applications such as electronic warfare and covert
surveillance.

</details>


### [108] [LLM-Guided Ansätze Design for Quantum Circuit Born Machines in Financial Generative Modeling](https://arxiv.org/abs/2509.08385)
*Yaswitha Gujju,Romain Harang,Tetsuo Shibuya*

Main category: quant-ph

TL;DR: 使用基于提示的大语言模型生成硬件感知的量子电路Born机器架构，在12量子位IBM量子硬件上实现了比标准基线更浅的电路深度和更好的生成性能


<details>
  <summary>Details</summary>
Motivation: 解决在噪声中等规模量子(NISQ)设备上发现既具有表达性又硬件高效的量子电路架构的关键挑战

Method: 提出基于提示的框架，利用大语言模型根据量子比特连接性、门错误率和硬件拓扑生成硬件感知的QCBM架构，使用KL散度、电路深度和有效性等迭代反馈来优化电路

Result: 在涉及日本政府债券利率日变化的金融建模任务中，LLM生成的ansätze比标准基线显著更浅，并在真实IBM量子硬件上实现了更优越的生成性能

Conclusion: 该方法展示了LLM驱动的量子架构搜索的实际效用，为近期量子设备提供了通向稳健、可部署生成模型的可行路径

Abstract: Quantum generative modeling using quantum circuit Born machines (QCBMs) shows
promising potential for practical quantum advantage. However, discovering
ans\"atze that are both expressive and hardware-efficient remains a key
challenge, particularly on noisy intermediate-scale quantum (NISQ) devices. In
this work, we introduce a prompt-based framework that leverages large language
models (LLMs) to generate hardware-aware QCBM architectures. Prompts are
conditioned on qubit connectivity, gate error rates, and hardware topology,
while iterative feedback, including Kullback-Leibler (KL) divergence, circuit
depth, and validity, is used to refine the circuits. We evaluate our method on
a financial modeling task involving daily changes in Japanese government bond
(JGB) interest rates. Our results show that the LLM-generated ans\"atze are
significantly shallower and achieve superior generative performance compared to
the standard baseline when executed on real IBM quantum hardware using 12
qubits. These findings demonstrate the practical utility of LLM-driven quantum
architecture search and highlight a promising path toward robust, deployable
generative models for near-term quantum devices.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [109] [kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions](https://arxiv.org/abs/2509.08366)
*Parastoo Pashmchi,Jerome Benoit,Motonobu Kanagawa*

Main category: stat.ML

TL;DR: kNNSampler是一种缺失值插补方法，通过从k个最相似单元的观测响应中随机采样来填补缺失值，能够估计条件分布而非仅条件均值


<details>
  <summary>Details</summary>
Motivation: 现有kNNImputer等方法只能估计缺失值的条件均值，无法捕捉完整的条件分布和不确定性，需要一种能够量化缺失值不确定性并支持多重插补的方法

Method: 基于k近邻的随机采样方法，对于给定单元的缺失响应，从其k个最相似单元（基于观测协变量）的观测响应中随机采样进行填补

Result: 理论证明能够估计缺失响应给定观测协变量的条件分布，实验证明在恢复缺失值分布方面有效

Conclusion: kNNSampler提供了一种有效的方法来估计缺失值的条件分布并量化不确定性，优于仅估计条件均值的传统方法

Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a
given unit's missing response by randomly sampling from the observed responses
of the $k$ most similar units to the given unit in terms of the observed
covariates. This method can sample unknown missing values from their
distributions, quantify the uncertainties of missing values, and be readily
used for multiple imputation. Unlike popular kNNImputer, which estimates the
conditional mean of a missing response given an observed covariate, kNNSampler
is theoretically shown to estimate the conditional distribution of a missing
response given an observed covariate. Experiments demonstrate its effectiveness
in recovering the distribution of missing values. The code for kNNSampler is
made publicly available (https://github.com/SAP/knn-sampler).

</details>


### [110] [Gaussian Process Regression -- Neural Network Hybrid with Optimized Redundant Coordinates](https://arxiv.org/abs/2509.08457)
*Sergei Manzhos,Manabu Ihara*

Main category: stat.ML

TL;DR: 本文提出了opt-GPRNN方法，通过蒙特卡洛算法优化GPRNN的冗余坐标，在减少神经元数量的同时获得更低的测试误差，并保持避免过拟合的优势


<details>
  <summary>Details</summary>
Motivation: 结合神经网络表达能力和线性回归鲁棒性的GPRNN方法存在冗余坐标问题，需要优化以提高性能并减少神经元数量

Method: 使用蒙特卡洛算法优化GPRNN中的冗余坐标，实现维度约简，结合加性核高斯过程回归和神经网络

Result: opt-GPRNN用更少的神经元获得最低测试误差，保持过拟合避免特性，表达力接近多层神经网络

Conclusion: 该方法在某些应用中可替代深度神经网络，具有维度约简能力，在原子间势能学习和材料信息学中应用良好

Abstract: Recently, a Gaussian Process Regression - neural network (GPRNN) hybrid
machine learning method was proposed, which is based on additive-kernel GPR in
redundant coordinates constructed by rules [J. Phys. Chem. A 127 (2023) 7823].
The method combined the expressive power of an NN with the robustness of linear
regression, in particular, with respect to overfitting when the number of
neurons is increased beyond optimal. We introduce opt-GPRNN, in which the
redundant coordinates of GPRNN are optimized with a Monte Carlo algorithm and
show that when combined with optimization of redundant coordinates, GPRNN
attains the lowest test set error with much fewer terms / neurons and retains
the advantage of avoiding overfitting when the number of neurons is increased
beyond optimal value. The method, opt-GPRNN possesses an expressive power
closer to that of a multilayer NN and could obviate the need for deep NNs in
some applications. With optimized redundant coordinates, a dimensionality
reduction regime is also possible. Examples of application to machine learning
an interatomic potential and materials informatics are given.

</details>


### [111] [PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research](https://arxiv.org/abs/2509.08553)
*Jessica Gronsbell,Vidul Ayakulangara Panickan,Chris Lin,Thomas Charlon,Chuan Hong,Doudou Zhou,Linshanshan Wang,Jianhui Gao,Shirley Zhou,Yuan Tian,Yaqi Shi,Ziming Gan,Tianxi Cai*

Main category: stat.ML

TL;DR: PEHRT是一个标准化的多机构电子健康记录(EHR)数据整合管道，通过数据预处理和表示学习模块解决数据异构性和隐私问题，无需共享个体级数据即可生成研究就绪数据集。


<details>
  <summary>Details</summary>
Motivation: 多机构EHR数据整合能提高转化研究的可靠性和泛化性，但面临数据异构性、语义差异和隐私保护等重大挑战。

Method: PEHRT包含两个核心模块：(1)数据预处理：将EHR数据映射到标准编码系统；(2)表示学习：使用先进机器学习技术生成研究就绪数据集，无需个体级数据共享，且与数据模型无关。

Result: 开发了完整的开源软件套件和用户友好教程，并在多个医疗系统的多样化任务中验证了PEHRT的实用性。

Conclusion: PEHRT提供了一个高效、标准化的解决方案，能够跨机构无缝执行EHR数据整合，促进多中心研究合作。

Abstract: Integrative analysis of multi-institutional Electronic Health Record (EHR)
data enhances the reliability and generalizability of translational research by
leveraging larger, more diverse patient cohorts and incorporating multiple data
modalities. However, harmonizing EHR data across institutions poses major
challenges due to data heterogeneity, semantic differences, and privacy
concerns. To address these challenges, we introduce $\textit{PEHRT}$, a
standardized pipeline for efficient EHR data harmonization consisting of two
core modules: (1) data pre-processing and (2) representation learning. PEHRT
maps EHR data to standard coding systems and uses advanced machine learning to
generate research-ready datasets without requiring individual-level data
sharing. Our pipeline is also data model agnostic and designed for streamlined
execution across institutions based on our extensive real-world experience. We
provide a complete suite of open source software, accompanied by a
user-friendly tutorial, and demonstrate the utility of PEHRT in a variety of
tasks using data from diverse healthcare systems.

</details>


### [112] [A hierarchical entropy method for the delocalization of bias in high-dimensional Langevin Monte Carlo](https://arxiv.org/abs/2509.08619)
*Daniel Lacker,Fuzhong Zhou*

Main category: stat.ML

TL;DR: 本文改进了Chen等人(2024)关于非调整Langevin算法偏差的研究，在稀疏交互机制下去除了对数因子，使用相对熵度量距离，并放宽了强对数凹性假设，证明了偏差仅与低维边际相关而非全维度。


<details>
  <summary>Details</summary>
Motivation: 非调整Langevin算法广泛用于高维分布采样，但存在维度相关的偏差。Chen等人发现稀疏交互分布中存在偏差仅与低维边际相关的去局部化现象，本文旨在强化这一结果并扩展其适用范围。

Method: 采用边际相对熵的层次分析方法，受作者最近关于混沌传播研究的启发，对稀疏交互和弱交互两类分布进行分析。

Result: 成功去除了原结果中的对数因子，将距离度量扩展到相对熵，放宽了强对数凹性假设，并证明去局部化现象在弱交互分布中也成立。

Conclusion: 研究强化了非调整Langevin算法在稀疏和弱交互高维分布中的偏差分析，证明了偏差仅与低维边际维度相关的重要特性，为高维采样提供了理论保障。

Abstract: The unadjusted Langevin algorithm is widely used for sampling from complex
high-dimensional distributions. It is well known to be biased, with the bias
typically scaling linearly with the dimension when measured in squared
Wasserstein distance. However, the recent paper of Chen et al. (2024)
identifies an intriguing new delocalization effect: For a class of
distributions with sparse interactions, the bias between low-dimensional
marginals scales only with the lower dimension, not the full dimension. In this
work, we strengthen the results of Chen et al. (2024) in the sparse interaction
regime by removing a logarithmic factor, measuring distance in relative entropy
(a.k.a. KL-divergence), and relaxing the strong log-concavity assumption. In
addition, we expand the scope of the delocalization phenomenon by showing that
it holds for a class of distributions with weak interactions. Our proofs are
based on a hierarchical analysis of the marginal relative entropies, inspired
by the authors' recent work on propagation of chaos.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [113] [TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/abs/2509.08699)
*Stefan Podgorski,Sourav Garg,Mehdi Hosseinzadeh,Lachlan Mares,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: 提出了一种基于RGB图像的零样本长距离机器人导航方法，无需3D地图或预训练控制器，通过全局拓扑路径规划和局部轨迹控制实现目标导航


<details>
  <summary>Details</summary>
Motivation: 传统视觉导航依赖全局一致的3D地图或学习控制器，计算成本高且难以泛化到多样化环境，需要更通用和高效的解决方案

Method: 集成全局拓扑路径规划和局部度量轨迹控制，使用单目深度和可通行性估计持续预测局部轨迹，并包含自动切换机制在必要时回退到基线控制器

Result: 在仿真环境和真实世界测试中验证了方法的有效性，表现出优于现有最先进方法的性能

Conclusion: 该方法为开放集环境中的视觉导航提供了更适应和有效的解决方案，具有鲁棒性和可部署性

Abstract: Visual navigation in robotics traditionally relies on globally-consistent 3D
maps or learned controllers, which can be computationally expensive and
difficult to generalize across diverse environments. In this work, we present a
novel RGB-only, object-level topometric navigation pipeline that enables
zero-shot, long-horizon robot navigation without requiring 3D maps or
pre-trained controllers. Our approach integrates global topological path
planning with local metric trajectory control, allowing the robot to navigate
towards object-level sub-goals while avoiding obstacles. We address key
limitations of previous methods by continuously predicting local trajectory
using monocular depth and traversability estimation, and incorporating an
auto-switching mechanism that falls back to a baseline controller when
necessary. The system operates using foundational models, ensuring open-set
applicability without the need for domain-specific fine-tuning. We demonstrate
the effectiveness of our method in both simulated environments and real-world
tests, highlighting its robustness and deployability. Our approach outperforms
existing state-of-the-art methods, offering a more adaptable and effective
solution for visual navigation in open-set environments. The source code is
made publicly available: https://github.com/podgorki/TANGO.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [114] [Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction](https://arxiv.org/abs/2509.08752)
*Vivek Oommen,Siavash Khodakarami,Aniruddha Bora,Zhicheng Wang,George Em Karniadakis*

Main category: physics.flu-dyn

TL;DR: 结合神经算子与生成对抗训练，解决了传统L2损失导致的湍流细节平滑问题，在超分辨率、预测和稀疏重建任务中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在训练中使用标准L2损失会导致细尺度湍流结构过度平滑，无法准确捕捉湍流细节

Method: 采用对抗训练的神经算子（adv-NO）和条件生成模型，结合生成建模技术来保持湍流的尖锐梯度和统计特性

Result: 在Schlieren射流超分辨率中能量谱误差降低15倍；3D湍流预测准确达5个涡旋翻转时间，推理速度比扩散方法快114倍；稀疏PTV输入可重建完整3D流场

Conclusion: 该方法实现了低计算成本下的准确重建和预测，为实验和计算流体力学中的近实时分析和控制提供了可能

Abstract: Neural operators are promising surrogates for dynamical systems but when
trained with standard L2 losses they tend to oversmooth fine-scale turbulent
structures. Here, we show that combining operator learning with generative
modeling overcomes this limitation. We consider three practical turbulent-flow
challenges where conventional neural operators fail: spatio-temporal
super-resolution, forecasting, and sparse flow reconstruction. For Schlieren
jet super-resolution, an adversarially trained neural operator (adv-NO) reduces
the energy-spectrum error by 15x while preserving sharp gradients at neural
operator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO
trained on only 160 timesteps from a single trajectory forecasts accurately for
five eddy-turnover times and offers 114x wall-clock speed-up at inference than
the baseline diffusion-based forecasters, enabling near-real-time rollouts. For
reconstructing cylinder wake flows from highly sparse Particle Tracking
Velocimetry-like inputs, a conditional generative model infers full 3D velocity
and pressure fields with correct phase alignment and statistics. These advances
enable accurate reconstruction and forecasting at low compute cost, bringing
near-real-time analysis and control within reach in experimental and
computational fluid mechanics. See our project page:
https://vivekoommen.github.io/Gen4Turb/

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [115] [The-Bodega: A Matlab Toolbox for Biologically Dynamic Microbubble Simulations on Realistic Hemodynamic Microvascular Graphs](https://arxiv.org/abs/2509.08149)
*Stephen Alexander Lee,Alexis Leconte,Alice Wu,Jonathan Poree,Maxence Laplante-Berthier,Simon Desrocher,Pierre-Olivier Bouchard,Joshua Kinugasa,Samuel Mihelic,Andreas Linninger,Jean Provost*

Main category: physics.med-ph

TL;DR: The-Bodega是一个基于Matlab的工具箱，用于生成超声定位显微镜(ULM)的模拟数据集，支持复杂血管结构中的微泡动力学模拟和超声数据生成。


<details>
  <summary>Details</summary>
Motivation: 为超声定位显微镜(ULM)这种超分辨率成像技术提供开源的地面真值数据集模拟工具，以支持算法基准测试和图像质量评估。

Method: 采用顺序蒙特卡洛模拟方法，结合泊肃叶流分布和动态脉动流，支持任意血管架构，并利用CPU/GPU并行化提高计算效率。

Result: 开发了一个灵活的仿真框架，能够模拟从小鼠大脑到人类心脏的各种应用场景，支持图像质量评估、运动伪影分析和新型ULM模态模拟。

Conclusion: The-Bodega为ULM研究提供了一个强大的仿真平台，能够生成真实的血流动力学数字幻影，支持多种应用场景的算法基准测试。

Abstract: The-Bodega is a Matlab-based toolbox for simulating ground-truth datasets for
Ultrasound Localization Microscopy (ULM)-a super resolution imaging technique
that resolves microvessels by systematically tracking microbubbles flowing
through the microvasculature. The-Bodega enables open-source simulation of
stochastic microbubble dynamics through anatomically complex vascular graphs
and features a quasi-automated pipeline for generating ground-truth ultrasound
data from simple vascular inputs. It incorporates sequential Monte Carlo
simulations augmented with Poiseuille flow distributions and dynamic pulsatile
flow. A key novelty of our framework is its flexibility to accommodate
arbitrary vascular architectures and benchmark common ULM algorithms, such as
Fourier Ring Correlation and Singular Value Decomposition (SVD) spatiotemporal
filtering, on realistic hemodynamic digital phantoms. The-Bodega supports
consistent microbubble-to-ultrasound simulations across domains ranging from
mouse brains to human hearts and automatically leverages available CPU/GPU
parallelization to improve computational efficiency. We demonstrate its
versatility in applications including image quality assessment, motion artifact
analysis, and the simulation of novel ULM modalities, such as capillary
imaging, myocardial reconstruction under beating heart motion, and simulating
neurovascular evoked responses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [116] [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)
*Hyungjin Chung,Hyelin Nam,Jiyeon Kim,Hyojun Go,Byeongjun Park,Junho Kim,Joonseok Lee,Seongsu Ha,Byung-Hoon Kim*

Main category: cs.CV

TL;DR: Video Parallel Scaling (VPS) 是一种推理时方法，通过并行处理视频帧子集来扩展模型感知带宽，避免增加上下文窗口带来的计算成本问题


<details>
  <summary>Details</summary>
Motivation: 解决VideoLLMs在处理长视频时面临的计算成本过高和性能下降问题，传统方法增加输入帧数会导致上下文长度过长

Method: 运行多个并行推理流，每个流处理视频帧的不同子集，然后聚合输出概率来整合更丰富的视觉信息

Result: 在Video-MME和EventHallusion等基准测试中，VPS在不同模型规模(2B-32B)上均显著提升性能，比自一致性等并行替代方案扩展性更好

Conclusion: VPS提供了一种内存高效且鲁棒的框架，无需额外训练即可增强VideoLLMs的时间推理能力，并能与其他解码策略互补

Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck:
increasing the number of input frames to capture fine-grained temporal detail
leads to prohibitive computational costs and performance degradation from long
context lengths. We introduce Video Parallel Scaling (VPS), an inference-time
method that expands a model's perceptual bandwidth without increasing its
context window. VPS operates by running multiple parallel inference streams,
each processing a unique, disjoint subset of the video's frames. By aggregating
the output probabilities from these complementary streams, VPS integrates a
richer set of visual information than is possible with a single pass. We
theoretically show that this approach effectively contracts the Chinchilla
scaling law by leveraging uncorrelated visual evidence, thereby improving
performance without additional training. Extensive experiments across various
model architectures and scales (2B-32B) on benchmarks such as Video-MME and
EventHallusion demonstrate that VPS consistently and significantly improves
performance. It scales more favorably than other parallel alternatives (e.g.
Self-consistency) and is complementary to other decoding strategies, offering a
memory-efficient and robust framework for enhancing the temporal reasoning
capabilities of VideoLLMs.

</details>


### [117] [MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery](https://arxiv.org/abs/2509.08027)
*Rafał Osadnik,Pablo Gómez,Eleni Bohacek,Rickbir Bahia*

Main category: cs.CV

TL;DR: 本文提出了一个名为MCTED的火星数字高程模型预测数据集，包含80,898个样本，用于机器学习应用。通过处理火星高分辨率正射影像和DEM数据对生成，并提供了训练和验证分割。实验表明，即使很小的U-Net架构在该数据集上的表现也优于DepthAnythingV2等深度估计基础模型的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模DEM处理流程中常见的伪影和缺失数据问题，为火星地形分析提供高质量的机器学习就绪数据集，促进火星数字高程模型的准确预测研究。

Method: 开发了综合处理流程来处理火星勘测轨道飞行器CTX仪器采集的高分辨率火星正射影像和DEM数据对，生成包含光学图像块、DEM块和两个掩码块的数据样本。使用工具解决原始数据中的缺失值和伪影问题，并将数据分为训练和验证分割以避免数据泄露。

Result: 成功构建了包含80,898个样本的MCTED数据集，提供了样本空间分布、高程值分布、坡度分布等统计洞察。实验结果显示，在该数据集上训练的小型U-Net架构在火星高程预测任务上的表现优于DepthAnythingV2基础模型的零样本性能。

Conclusion: MCTED数据集为火星数字高程模型预测提供了高质量的机器学习就绪资源，即使小型专用模型也能超越大型通用基础模型的零样本性能，证明了领域特定数据集的重要性。数据集和生成代码已完全开源。

Abstract: This work presents a new dataset for the Martian digital elevation model
prediction task, ready for machine learning applications called MCTED. The
dataset has been generated using a comprehensive pipeline designed to process
high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a
dataset consisting of 80,898 data samples. The source images are data gathered
by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very
diverse and comprehensive coverage of the Martian surface. Given the complexity
of the processing pipelines used in large-scale DEMs, there are often artefacts
and missing data points in the original data, for which we developed tools to
solve or mitigate their impact. We divide the processed samples into training
and validation splits, ensuring samples in both splits cover no mutual areas to
avoid data leakage. Every sample in the dataset is represented by the optical
image patch, DEM patch, and two mask patches, indicating values that were
originally missing or were altered by us. This allows future users of the
dataset to handle altered elevation regions as they please. We provide
statistical insights of the generated dataset, including the spatial
distribution of samples, the distributions of elevation values, slopes and
more. Finally, we train a small U-Net architecture on the MCTED dataset and
compare its performance to a monocular depth estimation foundation model,
DepthAnythingV2, on the task of elevation prediction. We find that even a very
small architecture trained on this dataset specifically, beats a zero-shot
performance of a depth estimation foundation model like DepthAnythingV2. We
make the dataset and code used for its generation completely open source in
public repositories.

</details>


### [118] [RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification](https://arxiv.org/abs/2509.08234)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出RepViT-CXR方法，通过通道复制策略将单通道胸部X光图像适配到ViT架构，在TB和肺炎检测任务上达到新的SOTA性能


<details>
  <summary>Details</summary>
Motivation: ViT架构通常预训练于三通道自然图像，而胸部X光扫描本质上是灰度单通道图像，需要解决这种格式不匹配问题

Method: 采用通道复制策略，将单通道CXR图像转换为ViT兼容的三通道格式，避免信息损失

Result: 在TB-CXR数据集上达到99.9%准确率和AUC，儿科肺炎数据集上99.0%准确率，深圳TB数据集上91.1%准确率，均超越现有方法

Conclusion: 简单有效的通道复制策略使ViT能够在灰度医学影像任务中充分发挥表征能力，具有实际临床部署潜力

Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic
tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia.
Recent advances in deep learning, particularly Vision Transformers (ViTs), have
shown strong potential for automated medical image analysis. However, most ViT
architectures are pretrained on natural images and require three-channel
inputs, while CXR scans are inherently grayscale. To address this gap, we
propose RepViT-CXR, a channel replication strategy that adapts single-channel
CXR images into a ViT-compatible format without introducing additional
information loss. We evaluate RepViT-CXR on three benchmark datasets. On the
TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%,
surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy,
99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0%
accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%,
outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB
dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a
performance improvement over previously reported CNN-based methods. These
results demonstrate that a simple yet effective channel replication strategy
allows ViTs to fully leverage their representational power on grayscale medical
imaging tasks. RepViT-CXR establishes a new state of the art for TB and
pneumonia detection from chest X-rays, showing strong potential for deployment
in real-world clinical screening systems.

</details>


### [119] [Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis](https://arxiv.org/abs/2509.08338)
*Jihyun Moon,Charmgil Hong*

Main category: cs.CV

TL;DR: 提出了一种检索增强的视觉语言模型框架，通过整合语义相似的病例到诊断提示中，无需微调即可提高黑色素瘤诊断的准确性和纠错能力


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法忽略临床元数据且需要大量预处理，而通用视觉语言模型在医疗领域缺乏临床特异性，需要改进多模态诊断方法

Method: 检索增强的视觉语言模型框架，将语义相似的病人病例整合到诊断提示中，实现无需微调的信息化预测

Result: 相比传统基线方法，显著提高了分类准确性和错误纠正能力

Conclusion: 检索增强提示策略为临床决策支持提供了稳健的方法

Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving
patient outcomes. While convolutional neural networks (CNNs) have shown promise
in dermoscopic image analysis, they often neglect clinical metadata and require
extensive preprocessing. Vision-language models (VLMs) offer a multimodal
alternative but struggle to capture clinical specificity when trained on
general-domain data. To address this, we propose a retrieval-augmented VLM
framework that incorporates semantically similar patient cases into the
diagnostic prompt. Our method enables informed predictions without fine-tuning
and significantly improves classification accuracy and error correction over
conventional baselines. These results demonstrate that retrieval-augmented
prompting provides a robust strategy for clinical decision support.

</details>


### [120] [LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations](https://arxiv.org/abs/2509.08422)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Sheraz Ahmed,Andreas Dengel*

Main category: cs.CV

TL;DR: LD-ViCE是一个基于潜在扩散模型的视频反事实解释框架，通过在潜在空间中操作降低计算成本，同时生成现实且可解释的反事实解释，在多个视频数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频AI系统在安全关键领域应用广泛，但现有解释技术存在时间一致性不足、鲁棒性差、缺乏因果洞察等问题，特别是现有反事实解释方法通常不结合目标模型指导，降低了语义保真度和实用性。

Method: 使用最先进的扩散模型在潜在空间中操作，通过额外的细化步骤生成现实且可解释的反事实解释，降低了生成解释的计算成本。

Result: 在三个不同的视频数据集（EchoNet-Dynamic、FERV39k、Something-Something V2）上验证有效，相比现有最先进方法R2分数提升高达68%，推理时间减少一半，生成语义有意义且时间一致的解释。

Conclusion: LD-ViCE代表了在安全关键领域可信AI部署的重要进展，为理解目标模型行为提供了有价值的洞察。

Abstract: Video-based AI systems are increasingly adopted in safety-critical domains
such as autonomous driving and healthcare. However, interpreting their
decisions remains challenging due to the inherent spatiotemporal complexity of
video data and the opacity of deep learning models. Existing explanation
techniques often suffer from limited temporal coherence, insufficient
robustness, and a lack of actionable causal insights. Current counterfactual
explanation methods typically do not incorporate guidance from the target
model, reducing semantic fidelity and practical utility. We introduce Latent
Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework
designed to explain the behavior of video-based AI models. Compared to previous
approaches, LD-ViCE reduces the computational costs of generating explanations
by operating in latent space using a state-of-the-art diffusion model, while
producing realistic and interpretable counterfactuals through an additional
refinement step. Our experiments demonstrate the effectiveness of LD-ViCE
across three diverse video datasets, including EchoNet-Dynamic (cardiac
ultrasound), FERV39k (facial expression), and Something-Something V2 (action
recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving
an increase in R2 score of up to 68% while reducing inference time by half.
Qualitative analysis confirms that LD-ViCE generates semantically meaningful
and temporally coherent explanations, offering valuable insights into the
target model behavior. LD-ViCE represents a valuable step toward the
trustworthy deployment of AI in safety-critical domains.

</details>


### [121] [Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting](https://arxiv.org/abs/2509.08442)
*Ivan Stoyanov,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 提出了球面布朗桥扩散模型(SBDM)来预测个体化高分辨率皮层厚度轨迹，通过双向条件布朗桥扩散过程和条件球面U-Net，在ADNI和OASIS数据集上显著降低了预测误差


<details>
  <summary>Details</summary>
Motivation: 准确预测个体化高分辨率皮层厚度轨迹对于检测细微皮层变化、理解神经退行性过程和制定早期精准干预策略至关重要，但由于大脑皮层的复杂非欧几何特性和需要整合多模态数据，这是一个具有挑战性的任务

Method: 提出球面布朗桥扩散模型(SBDM)，使用双向条件布朗桥扩散过程在配准皮层表面的顶点级别预测皮层厚度轨迹，开发了条件球面U-Net(CoS-UNet)去噪模型，结合球面卷积和密集交叉注意力机制来无缝整合皮层表面和表格条件数据

Result: 与先前方法相比，SBDM在ADNI和OASIS纵向数据集上实现了显著降低的预测误差，能够生成个体事实和反事实皮层厚度轨迹，为探索皮层发育的假设场景提供了新框架

Conclusion: SBDM为皮层厚度轨迹预测提供了一个有效的解决方案，通过创新的扩散模型架构和球面卷积技术，在预测精度和生成能力方面都表现出优越性能，为神经退行性疾病研究提供了有力工具

Abstract: Accurate forecasting of individualized, high-resolution cortical thickness
(CTh) trajectories is essential for detecting subtle cortical changes,
providing invaluable insights into neurodegenerative processes and facilitating
earlier and more precise intervention strategies. However, CTh forecasting is a
challenging task due to the intricate non-Euclidean geometry of the cerebral
cortex and the need to integrate multi-modal data for subject-specific
predictions. To address these challenges, we introduce the Spherical Brownian
Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional
conditional Brownian bridge diffusion process to forecast CTh trajectories at
the vertex level of registered cortical surfaces. Our technical contribution
includes a new denoising model, the conditional spherical U-Net (CoS-UNet),
which combines spherical convolutions and dense cross-attention to integrate
cortical surfaces and tabular conditions seamlessly. Compared to previous
approaches, SBDM achieves significantly reduced prediction errors, as
demonstrated by our experiments based on longitudinal datasets from the ADNI
and OASIS. Additionally, we demonstrate SBDM's ability to generate individual
factual and counterfactual CTh trajectories, offering a novel framework for
exploring hypothetical scenarios of cortical development.

</details>


### [122] [Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation](https://arxiv.org/abs/2509.08580)
*Mathilde Monvoisin,Louise Piecuch,Blanche Texier,Cédric Hémon,Anaïs Barateau,Jérémie Huet,Antoine Nordez,Anne-Sophie Boureau,Jean-Claude Nunes,Diana Mateus*

Main category: cs.CV

TL;DR: 提出一种基于隐式形状先验的医学图像分割方法，通过稀疏切片标注实现3D器官分割，并自动选择最具信息量的切片来减少人工标注工作量


<details>
  <summary>Details</summary>
Motivation: 减轻医学专业人员在复杂3D分割任务中的手动工作量，特别是在放疗规划和退行性疾病诊断中需要精确器官分割的场景

Method: 引入隐式形状先验从稀疏切片手动标注中分割体积，扩展到多器官情况，并提供自动选择最具信息量切片的框架来指导最小化后续交互

Result: 实验验证显示该方法在脑癌患者风险器官辅助分割和肌少症患者新肌肉形状数据库创建加速两个医学用例中有效

Conclusion: 该方法能显著减少医学3D分割任务的人工工作量，为放疗规划和疾病诊断提供有效的辅助工具

Abstract: The objective of this paper is to significantly reduce the manual workload
required from medical professionals in complex 3D segmentation tasks that
cannot be yet fully automated. For instance, in radiotherapy planning, organs
at risk must be accurately identified in computed tomography (CT) or magnetic
resonance imaging (MRI) scans to ensure they are spared from harmful radiation.
Similarly, diagnosing age-related degenerative diseases such as sarcopenia,
which involve progressive muscle volume loss and strength, is commonly based on
muscular mass measurements often obtained from manual segmentation of medical
volumes. To alleviate the manual-segmentation burden, this paper introduces an
implicit shape prior to segment volumes from sparse slice manual annotations
generalized to the multi-organ case, along with a simple framework for
automatically selecting the most informative slices to guide and minimize the
next interactions. The experimental validation shows the method's effectiveness
on two medical use cases: assisted segmentation in the context of at risks
organs for brain cancer patients, and acceleration of the creation of a new
database with unseen muscle shapes for patients with sarcopenia.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [123] [Dorst-Smeulders Coding for Arbitrary Binary Words](https://arxiv.org/abs/2509.08684)
*Alessandro De Luca,Gabriele Fici*

Main category: cs.DM

TL;DR: 提出线性时间算法计算二进制词最长Sturmian前缀的Dorst-Smeulders编码，并基于此实现任意二进制词的Sturmian分解


<details>
  <summary>Details</summary>
Motivation: Sturmian词在数字几何中对应欧几里得平面直线段的离散近似，Dorst-Smeulders编码能高效表示Sturmian词，可用于压缩方案

Method: 基于Sturmian词结构特性的深度分析，开发概念简单、代码简洁的线性时间算法，通过模运算实现高效计算

Result: 算法能在线性时间内计算任意二进制词的最长Sturmian前缀的Dorst-Smeulders编码，并实现最小Sturmian分解

Conclusion: 该算法为处理包含长Sturmian段的二进制词提供了高效工具，在压缩等领域具有应用潜力

Abstract: A binary word is Sturmian if the occurrences of each letter are balanced, in
the sense that in any two factors of the same length, the difference between
the number of occurrences of the same letter is at most 1. In digital geometry,
Sturmian words correspond to discrete approximations of straight line segments
in the Euclidean plane. The Dorst-Smeulders coding, introduced in 1984, is a
4-tuple of integers that uniquely represents a Sturmian word $w$, enabling its
reconstruction using $|w|$ modular operations, making it highly efficient in
practice. In this paper, we present a linear-time algorithm that, given a
binary input word $w$, computes the Dorst-Smeulders coding of its longest
Sturmian prefix. This forms the basis for computing the Dorst-Smeulders coding
of an arbitrary binary word $w$, which is a minimal decomposition (in terms of
the number of factors) of $w$ into Sturmian words, each represented by its
Dorst-Smeulders coding. This coding could be leveraged in compression schemes
where the input is transformed into a binary word composed of long Sturmian
segments. Although the algorithm is conceptually simple and can be implemented
in just a few lines of code, it is grounded in a deep analysis of the
structural properties of Sturmian words.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [124] [Facet: highly efficient E(3)-equivariant networks for interatomic potentials](https://arxiv.org/abs/2509.08418)
*Nicholas Miklaucic,Lai Wei,Rongzhi Dong,Nihang Fu,Sadman Sadeed Omee,Qingyang Li,Sourin Dey,Victor Fung,Jianjun Hu*

Main category: cond-mat.mtrl-sci

TL;DR: Facet是一种高效的等变图神经网络架构，通过用样条函数替换昂贵的MLP和引入新的等变层，大幅降低了计算和内存需求，在保持性能的同时实现10倍以上的训练加速。


<details>
  <summary>Details</summary>
Motivation: 第一性原理计算成本高昂限制了计算材料发现。现有机器学习势能方法面临计算瓶颈，等变图神经网络虽然能保持物理对称性，但计算复杂且效率低下。

Method: 提出Facet架构：1）用样条函数替代MLP处理原子间距离，降低计算和内存需求；2）引入通用等变层，通过球面网格投影和标准MLP混合节点信息，比张量积更快且表达能力更强。

Result: 在MPTrj数据集上，Facet以少得多的参数和不到10%的训练计算量达到领先模型性能；在晶体弛豫任务中比MACE模型快2倍；SevenNet-0参数可减少25%以上且无精度损失。

Conclusion: Facet的技术使大规模ML势能基础模型的训练速度提升10倍以上，有望重塑计算材料发现领域。

Abstract: Computational materials discovery is limited by the high cost of
first-principles calculations. Machine learning (ML) potentials that predict
energies from crystal structures are promising, but existing methods face
computational bottlenecks. Steerable graph neural networks (GNNs) encode
geometry with spherical harmonics, respecting atomic symmetries -- permutation,
rotation, and translation -- for physically realistic predictions. Yet
maintaining equivariance is difficult: activation functions must be modified,
and each layer must handle multiple data types for different harmonic orders.
We present Facet, a GNN architecture for efficient ML potentials, developed
through systematic analysis of steerable GNNs. Our innovations include
replacing expensive multi-layer perceptrons (MLPs) for interatomic distances
with splines, which match performance while cutting computational and memory
demands. We also introduce a general-purpose equivariant layer that mixes node
information via spherical grid projection followed by standard MLPs -- faster
than tensor products and more expressive than linear or gate layers. On the
MPTrj dataset, Facet matches leading models with far fewer parameters and under
10% of their training compute. On a crystal relaxation task, it runs twice as
fast as MACE models. We further show SevenNet-0's parameters can be reduced by
over 25% with no accuracy loss. These techniques enable more than 10x faster
training of large-scale foundation models for ML potentials, potentially
reshaping computational materials discovery.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [125] [STROKEVISION-BENCH: A Multimodal Video And 2D Pose Benchmark For Tracking Stroke Recovery](https://arxiv.org/abs/2509.07994)
*David Robinson,Animesh Gupta,Rizwan Quershi,Qiushi Fu,Mubarak Shah*

Main category: eess.IV

TL;DR: StrokeVision-Bench是首个专门针对中风患者执行临床结构化块转移任务的数据集，包含1000个标注视频，分为四个临床相关动作类别，提供原始视频帧和2D骨骼关键点两种模态，旨在实现客观、量化的上肢功能评估。


<details>
  <summary>Details</summary>
Motivation: 当前中风后上肢功能评估主要依赖治疗师的主观观察和粗粒度评分系统，缺乏敏感性来检测细微运动改善。计算机视觉技术为实现客观、量化和可扩展的评估提供了可能，但现有数据集主要关注日常生活活动，缺乏临床结构化评估任务。

Method: 构建StrokeVision-Bench数据集，包含1000个标注视频，分为四个临床相关动作类别（块转移任务），提供原始视频帧和2D骨骼关键点两种数据模态。使用最先进的视频动作识别和基于骨骼的动作分类方法进行基准测试。

Result: 建立了首个专门针对中风患者块转移任务的数据集，为自动化中风康复评估提供了性能基准，促进了该领域未来研究的发展。

Conclusion: StrokeVision-Bench填补了现有数据集的空白，为开发客观、量化的中风康复评估工具提供了重要资源，有望改善个性化康复规划的效果。

Abstract: Despite advancements in rehabilitation protocols, clinical assessment of
upper extremity (UE) function after stroke largely remains subjective, relying
heavily on therapist observation and coarse scoring systems. This subjectivity
limits the sensitivity of assessments to detect subtle motor improvements,
which are critical for personalized rehabilitation planning. Recent progress in
computer vision offers promising avenues for enabling objective, quantitative,
and scalable assessment of UE motor function. Among standardized tests, the Box
and Block Test (BBT) is widely utilized for measuring gross manual dexterity
and tracking stroke recovery, providing a structured setting that lends itself
well to computational analysis. However, existing datasets targeting stroke
rehabilitation primarily focus on daily living activities and often fail to
capture clinically structured assessments such as block transfer tasks.
Furthermore, many available datasets include a mixture of healthy and
stroke-affected individuals, limiting their specificity and clinical utility.
To address these critical gaps, we introduce StrokeVision-Bench, the first-ever
dedicated dataset of stroke patients performing clinically structured block
transfer tasks. StrokeVision-Bench comprises 1,000 annotated videos categorized
into four clinically meaningful action classes, with each sample represented in
two modalities: raw video frames and 2D skeletal keypoints. We benchmark
several state-of-the-art video action recognition and skeleton-based action
classification methods to establish performance baselines for this domain and
facilitate future research in automated stroke rehabilitation assessment.

</details>


### [126] [CardioComposer: Flexible and Compositional Anatomical Structure Generation with Disentangled Geometric Guidance](https://arxiv.org/abs/2509.08015)
*Karim Kadry,Shoaib Goraya,Ajay Manicka,Abdalla Abdelwahed,Farhad Nezami,Elazer Edelman*

Main category: eess.IV

TL;DR: 提出了一种基于可解释椭球体原语的可编程组合框架，用于引导无条件扩散模型生成更可控且解剖学真实的人体解剖结构


<details>
  <summary>Details</summary>
Motivation: 当前3D解剖学生成模型在可控性和解剖学真实性之间存在权衡，需要一种既能保持解剖学真实性又能提供精确控制的方法来研究结构-功能关系和医疗设备设计

Method: 使用嵌入3D空间的可解释椭球体原语来引导无条件扩散模型，通过在多组织分割图中选择特定组织并应用几何矩损失来指导反向扩散过程

Result: 该框架支持在推理过程中独立控制大小、形状和位置，以及多组件约束的组合

Conclusion: 提出的方法为生成可控且解剖学真实的3D解剖结构提供了一种有效的解决方案，有助于临床研究和医疗设备设计

Abstract: Generative models of 3D anatomy, when integrated with biophysical simulators,
enable the study of structure-function relationships for clinical research and
medical device design. However, current models face a trade-off between
controllability and anatomical realism. We propose a programmable and
compositional framework for guiding unconditional diffusion models of human
anatomy using interpretable ellipsoidal primitives embedded in 3D space. Our
method involves the selection of certain tissues within multi-tissue
segmentation maps, upon which we apply geometric moment losses to guide the
reverse diffusion process. This framework supports the independent control over
size, shape, and position, as well as the composition of multi-component
constraints during inference.

</details>


### [127] [Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis](https://arxiv.org/abs/2509.08018)
*Avais Jan,Qasim Zia,Murray Patterson*

Main category: eess.IV

TL;DR: 本文提出了一种基于数字孪生的联邦迁移学习(FTL)框架，用于解决CT扫描分析中的数据隐私、计算资源有限和数据异构性问题，在非IID数据环境下表现优于传统联邦学习和聚类联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决医疗CT图像分析中的数据隐私保护、计算资源限制和数据异构性挑战，同时实现实时协作和患者身份保护。

Method: 采用联邦迁移学习(FTL)框架，结合预训练模型和节点间知识迁移，在数字孪生使能的CT扫描仪和云服务器之间实现实时协作。

Result: FTL方法在收敛时间、模型准确率、精确率、召回率和F1分数等指标上均优于传统FL和CFL方法，特别在非IID数据环境下表现优异。

Conclusion: FTL技术为数字孪生基础的CT扫描分析提供了可靠、高效且安全的解决方案，有望推动精准医疗和智能医疗系统的发展。

Abstract: The application of Digital Twin (DT) technology and Federated Learning (FL)
has great potential to change the field of biomedical image analysis,
particularly for Computed Tomography (CT) scans. This paper presents Federated
Transfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.
FTL uses pre-trained models and knowledge transfer between peer nodes to solve
problems such as data privacy, limited computing resources, and data
heterogeneity. The proposed framework allows real-time collaboration between
cloud servers and Digital Twin-enabled CT scanners while protecting patient
identity. We apply the FTL method to a heterogeneous CT scan dataset and assess
model performance using convergence time, model accuracy, precision, recall, F1
score, and confusion matrix. It has been shown to perform better than
conventional FL and Clustered Federated Learning (CFL) methods with better
precision, accuracy, recall, and F1-score. The technique is beneficial in
settings where the data is not independently and identically distributed
(non-IID), and it offers reliable, efficient, and secure solutions for medical
diagnosis. These findings highlight the possibility of using FTL to improve
decision-making in digital twin-based CT scan analysis, secure and efficient
medical image analysis, promote privacy, and open new possibilities for
applying precision medicine and smart healthcare systems.

</details>


### [128] [Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding](https://arxiv.org/abs/2509.08685)
*Tam Thuc Do,Philip A. Chou,Gene Cheung*

Main category: eess.IV

TL;DR: 该论文提出了一种基于多分辨率B样条投影框架的3D点云属性无损压缩方法，通过端到端可微的网络结构优化率失真性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云几何编码后属性压缩的问题，在解码器端利用已有的几何信息，通过多分辨率投影框架实现高效的属性压缩。

Method: 使用B样条基函数构建嵌套子空间序列，通过可变复杂度展开的率失真优化算法计算投影系数，采用稀疏促进的ℓ1范数作为率项，并设计粗到细的预测器进行系数调整。

Result: 提出了一种端到端可微的压缩框架，能够实现多分辨率下的属性压缩，并通过数据驱动方式优化预测过程。

Conclusion: 该方法为3D点云属性压缩提供了一种新的多分辨率B样条投影框架，具有端到端可微和率失真优化的优势。

Abstract: Given encoded 3D point cloud geometry available at the decoder, we study the
problem of lossy attribute compression in a multi-resolution B-spline
projection framework. A target continuous 3D attribute function is first
projected onto a sequence of nested subspaces $\mathcal{F}^{(p)}_{l_0}
\subseteq \cdots \subseteq \mathcal{F}^{(p)}_{L}$, where
$\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basis
function of order $p$ at a chosen scale and its integer shifts. The projected
low-pass coefficients $F_l^*$ are computed by variable-complexity unrolling of
a rate-distortion (RD) optimization algorithm into a feed-forward network,
where the rate term is the sparsity-promoting $\ell_1$-norm. Thus, the
projection operation is end-to-end differentiable. For a chosen coarse-to-fine
predictor, the coefficients are then adjusted to account for the prediction
from a lower-resolution to a higher-resolution, which is also optimized in a
data-driven manner.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [129] [Membrane: A Cryptographic Access Control System for Data Lakes](https://arxiv.org/abs/2509.08740)
*Sam Kumar,Samyukta Yagati,Conor Power,David E. Culler,Raluca Ada Popa*

Main category: cs.CR

TL;DR: Membrane是一个为数据湖设计的安全系统，通过密码学方式强制执行数据依赖的访问控制视图，同时不限制数据科学家运行分析查询的能力。


<details>
  <summary>Details</summary>
Motivation: 数据湖存储敏感数据但存在被黑客绕过访问控制的风险，需要在不影响分析查询能力的前提下加强安全保护。

Method: 结合静态加密和SQL感知加密技术，使用块密码算法和硬件加速，开发新的SQL感知加密协议，在会话开始时解密视图数据。

Result: 系统仅在交互会话开始时产生开销（首次查询延迟约20倍），后续查询处理解密后的明文数据，总体摊销开销较低。

Conclusion: Membrane通过创新的加密方法有效保护数据湖安全，同时保持了分析查询的性能和灵活性。

Abstract: Organizations use data lakes to store and analyze sensitive data. But hackers
may compromise data lake storage to bypass access controls and access sensitive
data. To address this, we propose Membrane, a system that (1) cryptographically
enforces data-dependent access control views over a data lake, (2) without
restricting the analytical queries data scientists can run. We observe that
data lakes, unlike DBMSes, disaggregate computation and storage into separate
trust domains, making at-rest encryption sufficient to defend against remote
attackers targeting data lake storage, even when running analytical queries in
plaintext. This leads to a new system design for Membrane that combines
encryption at rest with SQL-aware encryption. Using block ciphers, a fast
symmetric-key primitive with hardware acceleration in CPUs, we develop a new
SQL-aware encryption protocol well-suited to at-rest encryption. Membrane adds
overhead only at the start of an interactive session due to decrypting views,
delaying the first query result by up to $\approx 20\times$; subsequent queries
process decrypted data in plaintext, resulting in low amortized overhead.

</details>


### [130] [EFPIX: A zero-trust encrypted flood protocol](https://arxiv.org/abs/2509.08248)
*Arin Upadhyay*

Main category: cs.CR

TL;DR: 提出一种基于泛洪的中继通信协议，实现端到端加密、用户合理否认性和消息不可追踪性


<details>
  <summary>Details</summary>
Motivation: 为了解决传统通信协议在隐私保护方面的不足，特别是元数据泄露、消息可追踪性以及缺乏合理否认机制等问题

Method: 采用基于泛洪的中继通信方式，设计端到端加密机制，隐藏发送者和接收者等元数据信息

Result: 协议能够抵抗拓扑变化和基础设施故障，有效保护用户隐私，实现消息的不可追踪性和合理否认性

Conclusion: 该泛洪中继通信协议为隐私保护通信提供了一种有效的解决方案，具有较好的鲁棒性和隐私保护特性

Abstract: We propose a flood-based relay communication protocol that achieves
end-to-end encryption, plausible deniability for users, and untraceable
messages. It is resistant to changes in topology and infrastructure failures.
It is also designed to hide metadata, such as sender and receiver, from those
not involved.

</details>


### [131] [DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation](https://arxiv.org/abs/2509.08449)
*Charuka Herath,Yogachandran Rahulamathavan,Varuna De Silva,Sangarapillai Lambotharan*

Main category: cs.CR

TL;DR: DSFL是一个双服务器拜占庭容错联邦学习框架，通过组安全聚合方法解决了隐私保护和鲁棒性问题，在非IID数据和拜占庭攻击下保持高性能和轻量级特性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习协议难以同时防御拜占庭攻击、在非IID数据下保持模型效用，且对边缘设备不够轻量。先前工作要么依赖可信硬件，要么使用昂贵的密码学工具，无法同时解决隐私和鲁棒性问题。

Method: 提出双服务器安全聚合协议（无需加密或密钥交换）、基于组信用的过滤机制（通过偏差分数隔离拜占庭客户端）、动态奖惩系统（确保公平参与）。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上，面对30%拜占庭参与者的IID和非IID设置，DSFL持续优于现有基线方法。在CIFAR-10上达到97.15%准确率，CIFAR-100上达到68.60%，而FedAvg在类似威胁下降至9.39%。每轮仅需55.9ms运行时间和1088KB通信量。

Conclusion: DSFL成功解决了联邦学习中隐私保护、拜占庭容错和轻量级需求的矛盾，提供了一个高效且实用的解决方案，特别适合边缘计算环境。

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data, offering strong privacy guarantees. However, existing FL protocols
struggle to defend against Byzantine participants, maintain model utility under
non-independent and identically distributed (non-IID) data, and remain
lightweight for edge devices. Prior work either assumes trusted hardware, uses
expensive cryptographic tools, or fails to address privacy and robustness
simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated
Learning framework that addresses these limitations using a group-based secure
aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest
servers, DSFL removes this dependency by revealing a key vulnerability: privacy
leakage through client-server collusion. DSFL introduces three key innovations:
(1) a dual-server secure aggregation protocol that protects updates without
encryption or key exchange, (2) a group-wise credit-based filtering mechanism
to isolate Byzantine clients based on deviation scores, and (3) a dynamic
reward-penalty system for enforcing fair participation. DSFL is evaluated on
MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in
both IID and non-IID settings. It consistently outperforms existing baselines,
including LSFL, homomorphic encryption methods, and differential privacy
approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and
68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar
threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB
communication per round.

</details>


### [132] [Unlocking Reproducibility: Automating re-Build Process for Open-Source Software](https://arxiv.org/abs/2509.08204)
*Behnaz Hassanshahi,Trong Nhan Mai,Benjamin Selwyn Smith,Nicholas Allen*

Main category: cs.CR

TL;DR: 本文介绍了Macaron框架的扩展，用于自动化从源代码重建Maven构件，以解决Maven Central中二进制文件与源代码分离带来的安全风险，通过改进源代码检测性能和自动化构建规范提取来增强供应链安全性。


<details>
  <summary>Details</summary>
Motivation: Maven Central等软件生态系统中二进制文件与源代码分离导致供应链安全风险，约84%的常用构件未使用透明CI/CD流水线构建，用户需要信任构建环境。从源代码重建构件可以提供代码深度审查、验证二进制-源代码等价性和控制依赖关系。

Method: 扩展Macaron供应链安全框架，自动化Maven构件从源代码的重建过程，改进源代码检测性能，自动化从GitHub Actions工作流提取构建规范，并进行Java项目构建失败的根因分析。

Result: 提出了可扩展的自动化构件重建解决方案，能够处理大型复杂依赖图，提高构建成功率，最终增强开源供应链的安全性和透明度。

Conclusion: 通过自动化从源代码重建Maven构件的方法，有效解决了软件供应链中的安全挑战，为构建更安全的开源生态系统提供了实用框架。

Abstract: Software ecosystems like Maven Central play a crucial role in modern software
supply chains by providing repositories for libraries and build plugins.
However, the separation between binaries and their corresponding source code in
Maven Central presents a significant challenge, particularly when it comes to
linking binaries back to their original build environment. This lack of
transparency poses security risks, as approximately 84% of the top 1200
commonly used artifacts are not built using a transparent CI/CD pipeline.
Consequently, users must place a significant amount of trust not only in the
source code but also in the environment in which these artifacts are built.
  Rebuilding software artifacts from source provides a robust solution to
improve supply chain security. This approach allows for a deeper review of
code, verification of binary-source equivalence, and control over dependencies.
However, challenges arise due to variations in build environments, such as JDK
versions and build commands, which can lead to build failures. Additionally,
ensuring that all dependencies are rebuilt from source across large and complex
dependency graphs further complicates the process. In this paper, we introduce
an extension to Macaron, an industry-grade open-source supply chain security
framework, to automate the rebuilding of Maven artifacts from source. Our
approach improves upon existing tools, by offering better performance in source
code detection and automating the extraction of build specifications from
GitHub Actions workflows. We also present a comprehensive root cause analysis
of build failures in Java projects and propose a scalable solution to automate
the rebuilding of artifacts, ultimately enhancing security and transparency in
the open-source supply chain.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [133] [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
*Benjamin Sturgeon,Daniel Samuelson,Jacob Haimes,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 本文开发了HumanAgencyBench基准测试，用于评估AI系统对人类自主性的支持程度，发现当代LLM助手对人类自主性的支持程度较低且存在显著差异


<details>
  <summary>Details</summary>
Motivation: 随着AI在人类决策中扮演越来越重要的角色，存在失去对个人和集体未来控制的风险，需要评估AI系统对人类自主性的支持程度

Method: 整合哲学和科学理论，使用LLM模拟用户查询和评估AI响应，开发包含六个维度的可扩展基准测试HumanAgencyBench

Result: 当代基于LLM的助手对人类自主性的支持程度为低到中等，不同系统开发商和维度间存在显著差异，Anthropic LLM整体支持度最高但在避免价值操纵方面最差

Conclusion: AI能力提升或指令遵循行为并不一致导致自主性支持度提高，需要转向更稳健的安全和对齐目标

Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [134] [Generative Quasi-Continuum Modeling of Confined Fluids at the Nanoscale](https://arxiv.org/abs/2509.08223)
*Bugra Yalcin,Ishan Nadkarni,Jinu Jeong,Chenxing Liang,Narayana R. Aluru*

Main category: physics.comp-ph

TL;DR: 提出了一种基于条件去噪扩散概率模型的多尺度框架，用于高效预测纳米尺度受限流体的密度分布，相比传统方法在计算效率和数据需求方面有显著提升


<details>
  <summary>Details</summary>
Motivation: 传统从头算分子动力学(AIMD)模拟计算成本过高，机器学习分子动力学(MLMD)虽然效率更高但仍受飞秒时间步长限制，难以获得准确的长时间平均密度估计

Method: 使用条件去噪扩散概率模型(DDPM)，基于有限的AIMD数据提取噪声力，预测沿约束方向的长时间力分布，然后通过Nernst-Planck方程与连续介质理论连接来揭示密度行为

Result: 在石墨烯纳米狭缝中受限水的测试表明，该方法能够以从头算精度恢复训练域外通道宽度的密度分布，相比AIMD和MLMD实现了数量级的运行时间加速，且需要更少的训练数据

Conclusion: 该框架为纳米尺度受限流体的高效密度预测提供了一种数据高效的多尺度解决方案，在计算效率和准确性方面都有显著优势

Abstract: We present a data-efficient, multiscale framework for predicting the density
profiles of confined fluids at the nanoscale. While accurate density estimates
require prohibitively long timescales that are inaccessible by ab initio
molecular dynamics (AIMD) simulations, machine-learned molecular dynamics
(MLMD) offers a scalable alternative, enabling the generation of force
predictions at ab initio accuracy with reduced computational cost. However,
despite their efficiency, MLMD simulations remain constrained by femtosecond
timesteps, which limit their practicality for computing long-time averages
needed for accurate density estimation. To address this, we propose a
conditional denoising diffusion probabilistic model (DDPM) based
quasi-continuum approach that predicts the long-time behavior of force profiles
along the confinement direction, conditioned on noisy forces extracted from a
limited AIMD dataset. The predicted smooth forces are then linked to continuum
theory via the Nernst-Planck equation to reveal the underlying density
behavior. We test the framework on water confined between two graphene
nanoscale slits and demonstrate that density profiles for channel widths
outside of the training domain can be recovered with ab initio accuracy.
Compared to AIMD and MLMD simulations, our method achieves orders-of-magnitude
speed-up in runtime and requires significantly less training data than prior
works.

</details>


### [135] [PCGBandit: One-shot acceleration of transient PDE solvers via online-learned preconditioners](https://arxiv.org/abs/2509.08765)
*Mikhail Khodak,Min Ki Jung,Brian Wynne,Edmond chow,Egemen Kolemen*

Main category: physics.comp-ph

TL;DR: PCGBandit方法通过在线学习自适应求解器配置，实现瞬态PDE数值模拟的一次性加速


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习加速科学计算的方法需要大量经典模拟数据进行训练，且难以证明相对于强基线方法的优势。本文探索利用经典求解器自身数据来加速的新范式

Method: 开发PCGBandit方法，利用PCG线性求解器的反馈，通过bandit算法在线学习自适应求解器配置序列（如预处理器），在OpenFOAM开源软件上实现

Result: 在流体和磁流体动力学问题上展示了方法的有效性

Conclusion: 提出了一种新的科学计算加速范式，通过在线学习自适应配置实现瞬态PDE模拟的一次性加速，避免了传统ML方法的数据需求和训练挑战

Abstract: Data-driven acceleration of scientific computing workflows has been a
high-profile aim of machine learning (ML) for science, with numerical
simulation of transient partial differential equations (PDEs) being one of the
main applications. The focus thus far has been on methods that require
classical simulations to train, which when combined with the data-hungriness
and optimization challenges of neural networks has caused difficulties in
demonstrating a convincing advantage against strong classical baselines. We
consider an alternative paradigm in which the learner uses a classical solver's
own data to accelerate it, enabling a one-shot speedup of the simulation.
Concretely, since transient PDEs often require solving a sequence of related
linear systems, the feedback from repeated calls to a linear solver such as
preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to
online-learn an adaptive sequence of solver configurations (e.g.
preconditioners). The method we develop, PCGBandit, is implemented directly on
top of the popular open source software OpenFOAM, which we use to show its
effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [136] [Chordless cycle filtrations for dimensionality detection in complex networks via topological data analysis](https://arxiv.org/abs/2509.08350)
*Aina Ferrà Marcús,Robert Jankowski,Meritxell Vila Miñana,Carles Casacuberta,M. Ángeles Serrano*

Main category: physics.soc-ph

TL;DR: 提出了一种基于弦无环的拓扑数据分析方法，结合机器学习来估计复杂网络的隐藏双曲几何维度，无需重新训练即可应用于真实网络。


<details>
  <summary>Details</summary>
Motivation: 许多复杂网络（从社交到生物系统）表现出与底层双曲几何一致的结构模式，揭示这种潜在空间的维度可以解耦社区的结构复杂性、影响高效网络导航，并从根本上塑造连接性和系统行为。

Method: 引入基于弦无环的新型拓扑数据分析权重方案，使用神经网络架构在为此目的构建的合成图数据库上进行训练，结合循环感知过滤和代数拓扑。

Result: 所得描述符可以有效地估计网络维度，且无需重新训练即可有效迁移到真实世界网络。

Conclusion: 该方法为揭示复杂网络的隐藏几何提供了稳健有效的方法，指导准确的建模和低维嵌入。

Abstract: Many complex networks, ranging from social to biological systems, exhibit
structural patterns consistent with an underlying hyperbolic geometry.
Revealing the dimensionality of this latent space can disentangle the
structural complexity of communities, impact efficient network navigation, and
fundamentally shape connectivity and system behavior. We introduce a novel
topological data analysis weighting scheme for graphs, based on chordless
cycles, aimed at estimating the dimensionality of networks in a data-driven
way. We further show that the resulting descriptors can effectively estimate
network dimensionality using a neural network architecture trained in a
synthetic graph database constructed for this purpose, which does not need
retraining to transfer effectively to real-world networks. Thus, by combining
cycle-aware filtrations, algebraic topology, and machine learning, our approach
provides a robust and effective method for uncovering the hidden geometry of
complex networks and guiding accurate modeling and low-dimensional embedding.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [137] [OCTANE -- Optimal Control for Tensor-based Autoencoder Network Emergence: Explicit Case](https://arxiv.org/abs/2509.08169)
*Ratna Khatri,Anthony Kolshorn,Colin Olson,Harbir Antil*

Main category: math.OC

TL;DR: OCTANE框架结合最优控制理论和低秩张量方法，为自编码器神经网络提供数学严谨的训练方法，实现内存高效压缩和自动化架构发现


<details>
  <summary>Details</summary>
Motivation: 解决传统自编码器训练中内存消耗大、架构设计依赖经验的问题，通过数学优化方法实现自动化高效学习

Method: 将学习任务建模为微分方程约束的优化问题，使用拉格朗日方法推导最优条件，采用低秩张量流形上的自适应显式积分方案进行内存压缩

Result: 开发出OCTANE统一训练框架，能够生成紧凑的自编码器架构，显著减少内存使用，在有限训练数据下实现有效学习

Conclusion: 该框架在图像去噪和去模糊任务中表现出良好效果，为自编码器训练提供了数学严谨且内存高效的新方法

Abstract: This paper presents a novel, mathematically rigorous framework for
autoencoder-type deep neural networks that combines optimal control theory and
low-rank tensor methods to yield memory-efficient training and automated
architecture discovery. The learning task is formulated as an optimization
problem constrained by differential equations representing the encoder and
decoder components of the network and the corresponding optimality conditions
are derived via a Lagrangian approach. Efficient memory compression is enabled
by approximating differential equation solutions on low-rank tensor manifolds
using an adaptive explicit integration scheme. These concepts are combined to
form OCTANE (Optimal Control for Tensor-based Autoencoder Network Emergence) --
a unified training framework that yields compact autoencoder architectures,
reduces memory usage, and enables effective learning, even with limited
training data. The framework's utility is illustrated with application to image
denoising and deblurring tasks and recommendations regarding governing
hyperparameters are provided.

</details>


### [138] [Decentralized Stochastic Nonconvex Optimization under the Relaxed Smoothness](https://arxiv.org/abs/2509.08726)
*Luo Luo,Xue Cui,Tingkai Jia,Cheng Chen*

Main category: math.OC

TL;DR: 本文提出了一种去中心化归一化随机梯度下降算法(DNSGD)，用于解决(L0,L1)-松弛光滑但可能非凸的分布式优化问题，在样本复杂度和通信复杂度方面都取得了较好的理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化优化方法主要针对标准光滑函数，但在实际应用中，许多目标函数具有更复杂的平滑性质（如松弛光滑性）。本文旨在开发一种能够处理(L0,L1)-松弛光滑非凸函数的去中心化算法，并提供严格的理论分析。

Method: 提出DNSGD算法，基于Lyapunov函数框架分析去中心化一阶方法，该框架考虑了梯度范数和一致性误差的乘积关系。算法通过归一化技术处理松弛光滑性，并分析了样本复杂度和通信复杂度。

Result: 理论分析表明，DNSGD在每台机器上达到ε-稳定点的样本复杂度为O(m⁻¹(L_fσ²Δ_fε⁻⁴ + σ²ε⁻² + L_f⁻²L₁³σ²Δ_fε⁻¹ + L_f⁻²L₁²σ²))，通信复杂度为Õ((L_fε⁻² + L₁ε⁻¹)γ⁻¹/²Δ_f)。当L₁=0时，结果接近去中心化非凸优化的下界。数值实验验证了方法的有效性。

Conclusion: DNSGD算法在松弛光滑设置下具有优越的理论性能，特别是在标准光滑情况下能够达到近乎最优的复杂度界限，为处理更广泛类别的去中心化优化问题提供了有效的解决方案。

Abstract: This paper studies decentralized optimization problem
$f(\mathbf{x})=\frac{1}{m}\sum_{i=1}^m f_i(\mathbf{x})$, where each local
function has the form of $f_i(\mathbf{x}) = {\mathbb
E}\left[F(\mathbf{x};{\xi}_i)\right]$ which is $(L_0,L_1)$-smooth but possibly
nonconvex and the random variable ${\xi}_i$ follows distribution ${\mathcal
D}_i$. We propose a novel algorithm called decentralized normalized stochastic
gradient descent (DNSGD), which can achieve the $\epsilon$-stationary point on
each local agent. We present a new framework for analyzing decentralized
first-order methods in the relaxed smooth setting, based on the Lyapunov
function related to the product of the gradient norm and the consensus error.
The analysis shows upper bounds on sample complexity of ${\mathcal
O}(m^{-1}(L_f\sigma^2\Delta_f\epsilon^{-4} + \sigma^2\epsilon^{-2} +
L_f^{-2}L_1^3\sigma^2\Delta_f\epsilon^{-1} + L_f^{-2}L_1^2\sigma^2))$ per agent
and communication complexity of $\tilde{\mathcal O}((L_f\epsilon^{-2} +
L_1\epsilon^{-1})\gamma^{-1/2}\Delta_f)$, where $L_f=L_0 +L_1\zeta$, $\sigma^2$
is the variance of the stochastic gradient, $\Delta_f$ is the initial optimal
function value gap, $\gamma$ is the spectral gap of the network, and $\zeta$ is
the degree of the gradient dissimilarity. In the special case of $L_1=0$, the
above results (nearly) match the lower bounds on decentralized nonconvex
optimization in the standard smooth setting. We also conduct numerical
experiments to show the empirical superiority of our method.

</details>


### [139] [Bregman Douglas-Rachford Splitting Method](https://arxiv.org/abs/2509.08739)
*Shiqian Ma,Lin Xiao,Renbo Zhao*

Main category: math.OC

TL;DR: 提出了Bregman Douglas-Rachford分裂方法及其变体Bregman Peaceman-Rachford分裂方法，用于求解极大单调包含问题，并证明其与Bregman ADMM在问题对偶形式下的等价性


<details>
  <summary>Details</summary>
Motivation: 针对极大单调包含问题，提出新的分裂算法，特别是为离散最优传输问题提供有效的求解方法

Method: Bregman Douglas-Rachford分裂方法和Bregman Peaceman-Rachford分裂方法，通过Bregman散度推广传统分裂算法，并与Bregman ADMM建立等价关系

Result: 提出了文献中未见的新算法，证明了算法在特定假设下的收敛性，但指出其中一个假设不适用于最优传输问题

Conclusion: 所提出的Bregman分裂算法为求解极大单调包含问题提供了新的有效工具，特别是在最优传输问题中具有应用潜力

Abstract: In this paper, we propose the Bregman Douglas-Rachford splitting (BDRS)
method and its variant Bregman Peaceman-Rachford splitting method for solving
maximal monotone inclusion problem. We show that BDRS is equivalent to a
Bregman alternating direction method of multipliers (ADMM) when applied to the
dual of the problem. A special case of the Bregman ADMM is an alternating
direction version of the exponential multiplier method. To the best of our
knowledge, algorithms proposed in this paper are new to the literature. We also
discuss how to use our algorithms to solve the discrete optimal transport (OT)
problem. We prove the convergence of the algorithms under certain assumptions,
though we point out that one assumption does not apply to the OT problem.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [140] [DLGE: Dual Local-Global Encoding for Generalizable Cross-BCI-Paradigm](https://arxiv.org/abs/2509.07991)
*Jingyuan Wang,Junhua Li*

Main category: q-bio.NC

TL;DR: 提出Dual Local-Global Encoder (DLGE)模型，用于跨不同脑机接口范式的分类，无需重新训练即可处理多种BCI范式


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型通常只能解码单一脑机接口范式的问题，克服不同范式间通道配置和任务相关表示的异质性，开发通用模型实现跨范式分类

Method: 采用解剖学启发的脑区分区和填充策略标准化EEG通道配置；局部编码器学习各脑区内跨范式的共享特征（整合时间注意力和空间注意力）；全局编码器聚合特征形成范式特定表示

Result: 在三种BCI范式（运动想象、静息状态、驾驶疲劳）上评估，平均宏精确率60.16%、召回率59.88%、F1分数59.56%，无需重新训练和调参

Conclusion: 首次尝试开发跨BCI范式分类的通用模型，为开发有效且简单的跨范式解码模型铺平道路，有助于便携式通用BCI解码设备的设计

Abstract: Deep learning models have been frequently used to decode a single
brain-computer interface (BCI) paradigm based on electroencephalography (EEG).
It is challenging to decode multiple BCI paradigms using one model due to
diverse barriers, such as different channel configurations and disparate
task-related representations. In this study, we propose Dual Local-Global
Encoder (DLGE), enabling the classification across different BCI paradigms. To
address the heterogeneity in EEG channel configurations across paradigms, we
employ an anatomically inspired brain-region partitioning and padding strategy
to standardize EEG channel configuration. In the proposed model, the local
encoder is designed to learn shared features across BCI paradigms within each
brain region based on time-frequency information, which integrates temporal
attention on individual channels with spatial attention among channels for each
brain region. These shared features are subsequently aggregated in the global
encoder to form respective paradigm-specific feature representations. Three BCI
paradigms (motor imagery, resting state, and driving fatigue) were used to
evaluate the proposed model. The results demonstrate that our model is capable
of processing diverse BCI paradigms without retraining and retuning, achieving
average macro precision, recall, and F1-score of 60.16\%, 59.88\%, and 59.56\%,
respectively. We made an initial attempt to develop a general model for
cross-BCI-paradigm classification, avoiding retraining or redevelopment for
each paradigm. This study paves the way for the development of an effective but
simple model for cross-BCI-paradigm decoding, which might benefit the design of
portable devices for universal BCI decoding.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [141] [Steering Protein Language Models](https://arxiv.org/abs/2509.07983)
*Long-Kai Huang,Rongyi Zhu,Bing He,Jianhua Yao*

Main category: q-bio.BM

TL;DR: 本研究探索了将原本用于控制文本生成的激活导向技术应用于蛋白质语言模型，提出了一种简单有效的激活编辑方法，可以无需额外训练就能精确控制PLM生成具有特定功能的蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型虽然强大，但在生成具有精确指定功能或特性的蛋白质序列方面存在困难，需要开发能够精确控制PLM输出的方法。

Method: 采用激活导向技术，通过激活编辑来引导PLM输出，并开发了新的编辑位点识别模块用于蛋白质优化，可无缝集成到自编码和自回归PLM中。

Result: 在溶菌酶样序列生成和优化实验中证明，该方法能够有效引导PLM生成具有目标特性的蛋白质序列，且不需要额外训练。

Conclusion: 这项工作展示了使用基础模型进行精确蛋白质工程的有前景方向，激活导向技术为蛋白质设计提供了新的控制手段。

Abstract: Protein Language Models (PLMs), pre-trained on extensive evolutionary data
from natural proteins, have emerged as indispensable tools for protein design.
While powerful, PLMs often struggle to produce proteins with precisely
specified functionalities or properties due to inherent challenges in
controlling their outputs. In this work, we investigate the potential of
Activation Steering, a technique originally developed for controlling text
generation in Large Language Models (LLMs), to direct PLMs toward generating
protein sequences with targeted properties. We propose a simple yet effective
method that employs activation editing to steer PLM outputs, and extend this
approach to protein optimization through a novel editing site identification
module. Through comprehensive experiments on lysozyme-like sequence generation
and optimization, we demonstrate that our methods can be seamlessly integrated
into both auto-encoding and autoregressive PLMs without requiring additional
training. These results highlight a promising direction for precise protein
engineering using foundation models.

</details>


### [142] [Tokenizing Loops of Antibodies](https://arxiv.org/abs/2509.08707)
*Ada Fang,Robert G. Alberstein,Simon Kelow,Frédéric A. Dreyer*

Main category: q-bio.BM

TL;DR: Igloo是一个多模态抗体环区标记器，通过对比学习编码抗体环区的骨架二面角和序列信息，能够有效检索相似环结构并提升蛋白质语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的抗体环区分类方法覆盖范围有限且难以整合到蛋白质基础模型中，需要一种能够编码抗体环区结构多样性的新方法。

Method: 开发了Igloo多模态抗体环区标记器，使用对比学习目标训练，将具有相似骨架二面角的环映射到潜在空间中更接近的位置。

Result: Igloo在识别相似H3环方面比现有方法提升5.9%，在10个抗体-抗原靶点中的8个上预测结合亲和力优于基础蛋白质语言模型，与参数量7倍的模型性能相当。

Conclusion: Igloo证明了引入多模态标记对编码抗体环区多样性、改进蛋白质基础模型和抗体CDR设计的好处。

Abstract: The complementarity-determining regions of antibodies are loop structures
that are key to their interactions with antigens, and of high importance to the
design of novel biologics. Since the 1980s, categorizing the diversity of CDR
structures into canonical clusters has enabled the identification of key
structural motifs of antibodies. However, existing approaches have limited
coverage and cannot be readily incorporated into protein foundation models.
Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody
loop tokenizer that encodes backbone dihedral angles and sequence. Igloo is
trained using a contrastive learning objective to map loops with similar
backbone dihedral angles closer together in latent space. Igloo can efficiently
retrieve the closest matching loop structures from a structural antibody
database, outperforming existing methods on identifying similar H3 loops by
5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issue
of canonical clusters, while retaining the ability to recover canonical loop
conformations. To demonstrate the versatility of Igloo tokens, we show that
they can be incorporated into protein language models with IglooLM and
IglooALM. On predicting binding affinity of heavy chain variants, IglooLM
outperforms the base protein language model on 8 out of 10 antibody-antigen
targets. Additionally, it is on par with existing state-of-the-art
sequence-based and multimodal protein language models, performing comparably to
models with $7\times$ more parameters. IglooALM samples antibody loops which
are diverse in sequence and more consistent in structure than state-of-the-art
antibody inverse folding models. Igloo demonstrates the benefit of introducing
multimodal tokens for antibody loops for encoding the diverse landscape of
antibody loops, improving protein foundation models, and for antibody CDR
design.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives](https://arxiv.org/abs/2509.08380)
*Prathamesh Vasudeo Naik,Naresh Kumar Dintakurthi,Zhanghao Hu,Yue Wang,Robby Qiu*

Main category: cs.AI

TL;DR: 提出了Co-Investigator AI代理框架，用于自动生成合规的可疑活动报告(SAR)，比传统方法更快更准确，解决了LLM在合规领域的事实幻觉、犯罪类型对齐和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 可疑活动报告生成是反洗钱工作流程中的高成本、低可扩展性瓶颈，现有大语言模型存在事实幻觉、犯罪类型对齐不足和可解释性差等问题，在合规关键领域存在不可接受的风险。

Method: 采用自主代理架构，集成规划、犯罪类型检测、外部情报收集和合规验证等专门代理，包含动态内存管理、AI隐私保护层和实时验证代理(Agent-as-a-Judge范式)。

Result: 在复杂金融犯罪场景中展示出强大能力，能够简化SAR起草流程，使叙述符合监管期望，让合规团队专注于更高层次的分析工作。

Conclusion: 该方法标志着合规报告新时代的开始，将AI代理的变革性优势带入监管流程核心，为可扩展、可靠和透明的SAR生成铺平道路，同时保持人类调查员的监督作用。

Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [144] [Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data](https://arxiv.org/abs/2509.08155)
*Kai Yang*

Main category: math.ST

TL;DR: 本论文提出了针对高维数据的三种鲁棒计算方法：基于互信息的非线性变量筛选方法、非凸惩罚的稀疏估计优化方法，以及基于Tsallis熵最大化的混合效应模型，有效解决了高维数据分析中的计算效率和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据的特征维度往往远大于样本量，传统线性方法难以处理非线性关联和异常值，需要开发能够同时处理非线性关系、计算高效且对异常值鲁棒的新方法。

Method: 1) 基于互信息的非线性变量筛选方法；2) 非凸惩罚的稀疏估计优化方法；3) 基于Tsallis幂律熵最大化的混合效应模型，配合近端非线性共轭梯度算法。

Result: 开发的方法能够准确识别非线性关联的重要变量，提供计算高效的稀疏估计，并构建对异常值鲁棒的混合效应模型，在神经影像数据等复杂数据集上表现优异。

Conclusion: 论文提出的三种方法为高维数据分析提供了完整的解决方案，突破了传统线性假设和正态分布限制，在计算效率、鲁棒性和理论性质方面均有显著改进。

Abstract: A ubiquitous feature of data of our era is their extra-large sizes and
dimensions. Analyzing such high-dimensional data poses significant challenges,
since the feature dimension is often much larger than the sample size. This
thesis introduces robust and computationally efficient methods to address
several common challenges associated with high-dimensional data. In my first
manuscript, I propose a coherent approach to variable screening that
accommodates nonlinear associations. I develop a novel variable screening
method that transcends traditional linear assumptions by leveraging mutual
information, with an intended application in neuroimaging data. This approach
allows for accurate identification of important variables by capturing
nonlinear as well as linear relationships between the outcome and covariates.
Building on this foundation, I develop new optimization methods for sparse
estimation using nonconvex penalties in my second manuscript. These methods
address notable challenges in current statistical computing practices,
facilitating computationally efficient and robust analyses of complex datasets.
The proposed method can be applied to a general class of optimization problems.
In my third manuscript, I contribute to robust modeling of high-dimensional
correlated observations by developing a mixed-effects model based on Tsallis
power-law entropy maximization and discussed the theoretical properties of such
distribution. This model surpasses the constraints of conventional Gaussian
models by accommodating a broader class of distributions with enhanced
robustness to outliers. Additionally, I develop a proximal nonlinear conjugate
gradient algorithm that accelerates convergence while maintaining numerical
stability, along with rigorous statistical properties for the proposed
framework.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [145] [ToDMA: Large Model-Driven Token-Domain Multiple Access for Semantic Communications](https://arxiv.org/abs/2505.10946)
*Li Qiao,Mahdi Boloursaz Mashhadi,Zhen Gao,Robert Schober,Deniz Gündüz*

Main category: cs.IT

TL;DR: 提出基于令牌域的多址接入方案ToDMA，利用预训练多模态大语言模型处理令牌碰撞，显著降低传输延迟并提升语义通信质量


<details>
  <summary>Details</summary>
Motivation: 传统语义通信在多设备接入时存在效率问题，需要一种能够利用上下文信息处理令牌碰撞的多址接入方案

Method: 设备共享令牌码本和调制码本，通过压缩感知检测活跃令牌，利用MLLM预测被碰撞的令牌来重构语义信息

Result: 在文本和图像传输任务中，相比无上下文正交方案显著降低延迟，相比最先进的无上下文非正交方法提供更好的失真和感知质量

Conclusion: ToDMA框架有效解决了多设备语义通信中的令牌碰撞问题，为未来大规模物联网通信提供了高效解决方案

Abstract: Token communications (TokCom) is an emerging generative semantic
communication concept that reduces transmission rates by using context and
multimodal large language model (MLLM)-based token processing, with tokens
serving as universal semantic units across modalities. In this paper, we
propose a semantic multiple access scheme in the token domain, referred to as
token domain multiple access (ToDMA), where a large number of devices share a
token codebook and a modulation codebook for source and channel coding,
respectively. Specifically, each transmitter first tokenizes its source signal
and modulate each token to a codeword. At the receiver, compressed sensing is
employed first to detect active tokens and the corresponding channel state
information (CSI) from the superposed signals. Then, the source token sequences
are reconstructed by clustering the token-associated CSI across multiple time
slots. In case of token collisions, some active tokens cannot be assigned and
some positions in the reconstructed token sequences are empty. We propose to
use pre-trained MLLMs to leverage the context, predict masked tokens, and thus
mitigate token collisions. Simulation results demonstrate the effectiveness of
the proposed ToDMA framework for both text and image transmission tasks,
achieving significantly lower latency compared to context-unaware orthogonal
communication schemes, while also delivering superior distortion and perceptual
quality compared to state-of-the-art context-unaware non-orthogonal
communication methods.

</details>


### [146] [SCA-LLM: Spectral-Attentive Channel Prediction with Large Language Models in MIMO-OFDM](https://arxiv.org/abs/2509.08139)
*Ke He,Le He,Lisheng Fan,Xianfu Lei,Thang X. Vu,George K. Karagiannidis,Symeon Chatzinotas*

Main category: cs.IT

TL;DR: SCA-LLM是一个基于频谱注意力的大语言模型适配器框架，用于MIMO-OFDM系统的信道预测，通过捕捉频谱细节来缓解领域不匹配问题，相比现有方法在NMSE指标上提升2.4dB。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在信道预测中存在文本预训练与CSI数据的领域不匹配问题，现有适配器未能充分利用该范式的潜力，需要更有效的方法来弥合领域差距。

Method: 提出频谱注意力框架SCA-LLM，其新型适配器能够从CSI特征的频谱分量中学习表示，捕捉更精细的频谱细节，更好地适配大语言模型进行信道预测。

Result: 大量仿真显示SCA-LLM实现了最先进的预测性能和强泛化能力，在归一化均方误差(NMSE)指标上比之前基于LLM的方法提升达2.4dB。消融研究进一步证实了其在缓解领域不匹配方面的优越性。

Conclusion: 通过从频谱分量学习表示可以更有效地弥合领域差距，SCA-LLM框架在MIMO-OFDM信道预测中表现出色，为LLM在无线通信中的应用提供了新的解决方案。

Abstract: In recent years, the success of large language models (LLMs) has inspired
growing interest in exploring their potential applications in wireless
communications, especially for channel prediction tasks. However, directly
applying LLMs to channel prediction faces a domain mismatch issue stemming from
their text-based pre-training. To mitigate this, the ``adapter + LLM" paradigm
has emerged, where an adapter is designed to bridge the domain gap between the
channel state information (CSI) data and LLMs. While showing initial success,
existing adapters may not fully exploit the potential of this paradigm. To
address this limitation, this work provides a key insight that learning
representations from the spectral components of CSI features can more
effectively help bridge the domain gap. Accordingly, we propose a
spectral-attentive framework, named SCA-LLM, for channel prediction in
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. Specifically, its novel adapter can capture finer spectral
details and better adapt the LLM for channel prediction than previous methods.
Extensive simulations show that SCA-LLM achieves state-of-the-art prediction
performance and strong generalization, yielding up to $-2.4~\text{dB}$
normalized mean squared error (NMSE) advantage over the previous LLM based
method. Ablation studies further confirm the superiority of SCA-LLM in
mitigating domain mismatch.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [147] [Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction](https://arxiv.org/abs/2509.07990)
*Charan Gajjala Chenchu,Kinam Kim,Gao Lu,Zia Ud Din*

Main category: eess.SP

TL;DR: 本研究比较了sEMG信号和视频两种数据模态在建筑行业人机协作中运动意图识别的性能，发现基于sEMG的CNN-LSTM模型预测速度快(0.04秒)但准确率较低(87%)，而基于视频的Swin Transformer模型准确率高(94%)但预测时间较长(0.15秒)。


<details>
  <summary>Details</summary>
Motivation: 建筑行业人机协作需要机器人精确快速地识别人类运动意图以确保安全和效率，但目前缺乏对不同数据模态（信号vs视频）在运动意图识别方面的比较研究。

Method: 使用深度学习技术评估两种数据模态：1）基于表面肌电信号(sEMG)的CNN-LSTM模型；2）基于视频序列的预训练Video Swin Transformer结合迁移学习。在干墙安装任务中识别早期运动意图。

Result: sEMG+CNN-LSTM模型准确率约87%，平均预测时间0.04秒；视频+Swin Transformer模型准确率94%，但平均预测时间较长(0.15秒)。两种模态各有优劣。

Conclusion: 研究强调了不同数据格式的独特优势和权衡，为在实际建筑项目中系统部署人机协作技术提供了指导，需要根据具体应用场景选择合适的数据模态。

Abstract: Human-robot collaboration (HRC) in the construction industry depends on
precise and prompt recognition of human motion intentions and actions by robots
to maximize safety and workflow efficiency. There is a research gap in
comparing data modalities, specifically signals and videos, for motion
intention recognition. To address this, the study leverages deep learning to
assess two different modalities in recognizing workers' motion intention at the
early stage of movement in drywall installation tasks. The Convolutional Neural
Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface
electromyography (sEMG) data achieved an accuracy of around 87% with an average
time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the
pre-trained Video Swin Transformer combined with transfer learning harnessed
video sequences as input to recognize motion intention and attained an accuracy
of 94% but with a longer average time of 0.15 seconds for a similar prediction.
This study emphasizes the unique strengths and trade-offs of both data formats,
directing their systematic deployments to enhance HRC in real-world
construction projects.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [148] [Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units](https://arxiv.org/abs/2509.08203)
*Ryan Lingo,Rajeev Chhajer,Martin Arroyo,Luka Brkljacic,Ben Davis,Nithin Santhanam*

Main category: cs.HC

TL;DR: 提出了组件化方法，将LLM生成的单一文本分解为可独立编辑的模块化单元，通过MAOD算法和CBRA架构实现，支持协作编辑和重用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的文本通常是单一整体，难以进行部分编辑，这降低了协作工作效率。需要一种方法将输出分解为可独立操作的模块化组件。

Method: 提出Modular and Adaptable Output Decomposition (MAOD)算法进行语义分割，设计Component-Based Response Architecture (CBRA)架构，并开发MAODchat原型系统，采用微服务设计和状态机分解代理。

Result: 在4名参与者的探索性研究中发现，组件级编辑符合常见工作流程，支持迭代优化和选择性重用，参与者还提到了可能的团队协作流程。

Conclusion: 组件化是将被动文本消费转变为主动组件级协作的有前景方向，为LLM输出提供了更好的可编辑性和协作性。

Abstract: Large Language Models (LLMs) often produce monolithic text that is hard to
edit in parts, which can slow down collaborative workflows. We present
componentization, an approach that decomposes model outputs into modular,
independently editable units while preserving context. We describe Modular and
Adaptable Output Decomposition (MAOD), which segments responses into coherent
components and maintains links among them, and we outline the Component-Based
Response Architecture (CBRA) as one way to implement this idea. Our reference
prototype, MAODchat, uses a microservices design with state-machine-based
decomposition agents, vendor-agnostic model adapters, and real-time component
manipulation with recomposition.
  In an exploratory study with four participants from academic, engineering,
and product roles, we observed that component-level editing aligned with
several common workflows and enabled iterative refinement and selective reuse.
Participants also mentioned possible team workflows. Our contributions are: (1)
a definition of componentization for transforming monolithic outputs into
manipulable units, (2) CBRA and MAODchat as a prototype architecture, (3)
preliminary observations from a small user study, (4) MAOD as an algorithmic
sketch for semantic segmentation, and (5) example Agent-to-Agent protocols for
automated decomposition. We view componentization as a promising direction for
turning passive text consumption into more active, component-level
collaboration.

</details>


### [149] [Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning](https://arxiv.org/abs/2509.08539)
*Lukas Schach,Christian Rack,Ryan P. McMahan,Marc Erich Latoschik*

Main category: cs.HC

TL;DR: 本文研究了两种最先进的分类和相似性学习模型在不同XR应用中的用户运动识别泛化能力，发现模型在同一应用内识别准确但跨应用泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进的用户识别模型在扩展现实(XR)应用中的泛化能力，为XR和元宇宙应用中的用户身份验证提供风险评估。

Method: 开发了包含49名用户在5个不同XR应用中的运动数据集，包括4个XR游戏和1个社交XR应用，用于评估两种模型的跨应用泛化性能。

Result: 模型在同一应用内能准确识别用户，但在不同XR应用间的用户识别能力有限，表明当前模型的跨应用泛化能力不足。

Conclusion: 研究结果揭示了当前运动识别模型在XR应用中的泛化局限性，为元宇宙用户身份验证提供了重要风险评估，并公开了数据集以促进相关研究。

Abstract: This paper examines the generalization capacity of two state-of-the-art
classification and similarity learning models in reliably identifying users
based on their motions in various Extended Reality (XR) applications. We
developed a novel dataset containing a wide range of motion data from 49 users
in five different XR applications: four XR games with distinct tasks and action
patterns, and an additional social XR application with no predefined task sets.
The dataset is used to evaluate the performance and, in particular, the
generalization capacity of the two models across applications. Our results
indicate that while the models can accurately identify individuals within the
same application, their ability to identify users across different XR
applications remains limited. Overall, our results provide insight into current
models generalization capabilities and suitability as biometric methods for
user verification and identification. The results also serve as a much-needed
risk assessment of hazardous and unwanted user identification in XR and
Metaverse applications. Our cross-application XR motion dataset and code are
made available to the public to encourage similar research on the
generalization of motion-based user identification in typical Metaverse
application use cases.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [150] [MasconCube: Fast and Accurate Gravity Modeling with an Explicit Representation](https://arxiv.org/abs/2509.08607)
*Pietro Fanti,Dario Izzo*

Main category: astro-ph.EP

TL;DR: MasconCubes是一种新型自监督学习方法，通过3D点质量网格直接优化重力反演问题，解决了传统方法在小天体重力场建模中的局限性，计算效率比现有方法快40倍。


<details>
  <summary>Details</summary>
Motivation: 传统方法如球谐函数在Brillouin球内发散，多面体模型假设不现实的均匀密度分布，现有机器学习方法计算资源需求大且训练时间长，无法满足深空探测任务对小天体重力场精确建模的需求。

Method: 提出MasconCubes方法，将重力反演表述为对规则3D点质量网格的直接优化问题，利用已知小行星形状信息约束解空间，显式建模质量分布而非隐式神经网络表示。

Result: 在Bennu、Eros、Itokawa等多样化小行星模型上的综合评估显示，MasconCubes在多个指标上表现优异，训练时间比GeodesyNets快约40倍，同时保持通过显式质量分布的物理可解释性。

Conclusion: MasconCubes为任务关键的重力建模应用提供了一种有前景的方法，具有高精度、计算效率和内部质量分布物理洞察力的优势，适用于不规则天体的重力场建模。

Abstract: The geodesy of irregularly shaped small bodies presents fundamental
challenges for gravitational field modeling, particularly as deep space
exploration missions increasingly target asteroids and comets. Traditional
approaches suffer from critical limitations: spherical harmonics diverge within
the Brillouin sphere where spacecraft typically operate, polyhedral models
assume unrealistic homogeneous density distributions, and existing machine
learning methods like GeodesyNets and Physics-Informed Neural Networks
(PINN-GM) require extensive computational resources and training time. This
work introduces MasconCubes, a novel self-supervised learning approach that
formulates gravity inversion as a direct optimization problem over a regular 3D
grid of point masses (mascons). Unlike implicit neural representations,
MasconCubes explicitly model mass distributions while leveraging known asteroid
shape information to constrain the solution space. Comprehensive evaluation on
diverse asteroid models including Bennu, Eros, Itokawa, and synthetic
planetesimals demonstrates that MasconCubes achieve superior performance across
multiple metrics. Most notably, MasconCubes demonstrate computational
efficiency advantages with training times approximately 40 times faster than
GeodesyNets while maintaining physical interpretability through explicit mass
distributions. These results establish MasconCubes as a promising approach for
mission-critical gravitational modeling applications requiring high accuracy,
computational efficiency, and physical insight into internal mass distributions
of irregular celestial bodies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [151] [Network Contagion in Financial Labor Markets: Predicting Turnover in Hong Kong](https://arxiv.org/abs/2509.08001)
*Abdulla AlKetbi,Patrick Yam,Gautier Marti,Raed Jaradat*

Main category: cs.SI

TL;DR: 基于香港证监会2007-2024年数据的网络分析显示，员工离职存在传染效应，当30%以上同事在6个月内离职时，个人离职概率增加23%。图神经网络模型将离职预测准确率提升30%。


<details>
  <summary>Details</summary>
Motivation: 金融市场员工流动是一个重要但研究不足的问题，特别是专业网络在职业流动中的作用尚未被充分探索。研究旨在分析网络效应对员工离职的影响。

Method: 使用香港证监会公开注册数据构建121,883名专业人士和4,979家公司的时序网络，开发基于图的特征传播框架来捕捉同伴影响和组织稳定性。

Result: 发现明显的传染效应：当超过30%的同事在6个月内离职时，专业人士离职可能性增加23%。将网络信号嵌入机器学习模型使离职预测准确率比基线提高30%。

Conclusion: 时序网络效应在劳动力动态中具有强大预测能力，基于网络的分析可为监管监控、人才管理和系统性风险评估提供重要信息。

Abstract: Employee turnover is a critical challenge in financial markets, yet little is
known about the role of professional networks in shaping career moves. Using
the Hong Kong Securities and Futures Commission (SFC) public register
(2007-2024), we construct temporal networks of 121,883 professionals and 4,979
firms to analyze and predict employee departures. We introduce a graph-based
feature propagation framework that captures peer influence and organizational
stability. Our analysis shows a contagion effect: professionals are 23% more
likely to leave when over 30% of their peers depart within six months.
Embedding these network signals into machine learning models improves turnover
prediction by 30% over baselines. These results highlight the predictive power
of temporal network effects in workforce dynamics, and demonstrate how
network-based analytics can inform regulatory monitoring, talent management,
and systemic risk assessment.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [152] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 研究发现预训练大语言模型中的偏见可以通过提示适应传递到下游模型，且现有的提示去偏方法无法一致地阻止偏见传递，需要在内在模型中纠正偏见以防止偏见传播。


<details>
  <summary>Details</summary>
Motivation: 先前关于偏见传递假设的研究可能让人误以为偏见不会从预训练大语言模型传递到适应模型，本文旨在验证这一假设在提示适应场景下的有效性。

Method: 研究因果模型在提示适应下的偏见传递，分析不同人口统计特征（性别、年龄、宗教等）和任务中的偏见相关性，评估多种基于提示的去偏策略效果。

Result: 发现偏见通过提示传递的相关系数很高（性别ρ≥0.94，年龄ρ≥0.98，宗教ρ≥0.69），改变少样本组合参数时偏见仍强相关（ρ≥0.90），现有去偏方法无法一致减少偏见传递。

Conclusion: 需要在内在模型中纠正偏见和改进推理能力，才能有效防止偏见向下游任务的传播。

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [153] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: LLM在社会科学研究中存在"LLM hacking"风险，研究人员的不同实现选择会导致系统性偏差和随机错误，约1/3的假设会得出错误结论，即使高性能模型也无法完全消除风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在变革社会科学研究，但研究人员在模型选择、提示策略等实现选择上的差异会导致输出结果显著变化，可能引入系统性偏差和随机错误，影响下游分析的统计结论可靠性。

Method: 通过复制21项已发表社会科学研究中的37个数据标注任务，使用18个不同模型生成1300万条LLM标签，测试2361个现实假设来量化LLM hacking风险。

Result: 发现约1/3的假设会基于LLM标注数据得出错误结论（小型语言模型达一半），即使高精度模型也无法完全消除风险。效应量越大风险越低，常见回归校正技术效果有限。

Conclusion: LLM hacking风险普遍存在，需要更严格的验证机制。人类标注在减少误报和改进模型选择方面至关重要，故意操纵LLM输出以呈现统计显著性异常容易。

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [154] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本论文综述了强化学习在大型语言模型推理能力提升方面的最新进展，重点分析了RL在数学和编程等复杂逻辑任务中的应用，探讨了RL向人工超级智能扩展面临的挑战和发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着RL在提升LLM推理能力方面取得显著成功，特别是在数学和编程等复杂逻辑任务中，RL已成为将LLM转化为LRM的基础方法。但该领域在计算资源、算法设计、训练数据和基础设施等方面面临扩展性挑战，需要重新评估发展轨迹并探索增强RL可扩展性的策略。

Method: 采用文献综述方法，系统考察了自DeepSeek-R1发布以来将RL应用于LLM和LRM推理能力的研究，包括基础组件、核心问题、训练资源和下游应用等方面的分析。

Result: 研究发现RL在推进LLM能力前沿方面取得了显著成就，特别是在复杂逻辑推理任务中。同时识别了RL向ASI扩展时在多个维度面临的基础性挑战。

Conclusion: 本文综述为RL在更广泛推理模型中的未来研究提供了系统性的分析框架和发展方向，有望促进该快速演进领域的进一步研究和发展。

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>
