<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.CY](#cs.CY) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.CE](#cs.CE) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 21]
- [stat.OT](#stat.OT) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Revisiting Graph Analytics Benchmark](https://arxiv.org/abs/2506.21811)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Xue Li,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.DB

TL;DR: 提出了一种新的图分析基准测试方法，解决了现有基准测试在核心算法选择、数据生成和API可用性评估方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图分析基准测试在核心算法选择、数据生成和API可用性评估方面存在不足，无法全面评估平台性能。

Method: 1. 通过广泛调研选择八种核心算法；2. 设计高效灵活的数据生成器并生成八种新合成数据集；3. 引入基于大语言模型的多层次API可用性评估框架。

Result: 实验结果表明，提出的基准测试方法优于现有平台（如GraphX、PowerGraph等）。

Conclusion: 新基准测试方法在算法选择、数据生成和API可用性评估方面表现优越，为图分析平台提供了更全面的评估工具。

Abstract: The rise of graph analytics platforms has led to the development of various
benchmarks for evaluating and comparing platform performance. However, existing
benchmarks often fall short of fully assessing performance due to limitations
in core algorithm selection, data generation processes (and the corresponding
synthetic datasets), as well as the neglect of API usability evaluation. To
address these shortcomings, we propose a novel graph analytics benchmark.
First, we select eight core algorithms by extensively reviewing both academic
and industrial settings. Second, we design an efficient and flexible data
generator and produce eight new synthetic datasets as the default datasets for
our benchmark. Lastly, we introduce a multi-level large language model
(LLM)-based framework for API usability evaluation-the first of its kind in
graph analytics benchmarks. We conduct comprehensive experimental evaluations
on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and
G-thinker). The experimental results demonstrate the superiority of our
proposed benchmark.

</details>


### [2] [A Survey of LLM Inference Systems](https://arxiv.org/abs/2506.21901)
*James Pan,Guoliang Li*

Main category: cs.DB

TL;DR: 本文综述了大型语言模型（LLM）推理系统中的关键技术，包括请求处理、模型优化、内存管理等，并探讨了如何将这些技术结合以构建单副本和多副本推理系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，其独特的自回归特性对推理系统设计提出了新挑战，需要高效技术以支持高负载和高速度的工作流。

Method: 通过分析请求处理算法、模型优化技术（如内核设计、批处理和调度）以及内存管理方法（如分页内存、量化和缓存持久化），探讨了这些技术的依赖关系。

Result: 研究表明，这些技术依赖于负载预测、自适应机制和成本降低，以克服自回归生成的挑战。

Conclusion: 文章总结了构建高效LLM推理系统的关键技术，并讨论了剩余挑战，如资源分配和共享基础设施部署。

Abstract: The past few years has witnessed specialized large language model (LLM)
inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside
rapid LLM adoption via services like ChatGPT. Driving these system design
efforts is the unique autoregressive nature of LLM request processing,
motivating new techniques for achieving high performance while preserving high
inference quality over high-volume and high-velocity workloads. While many of
these techniques are discussed across the literature, they have not been
analyzed under the framework of a complete inference system, nor have the
systems themselves been analyzed and compared.
  In this survey, we review these techniques, starting from operators and
algorithms for request processing, then moving on to techniques for model
optimization and execution, including kernel design, batching, and scheduling,
before ending with techniques for memory management, including paged memory,
eviction and offloading techniques, quantization, and cache persistence.
Through these discussions, we show that these techniques fundamentally rely on
load prediction, adaptive mechanisms, and cost reduction in order to overcome
the challenges introduced by autoregressive generation and achieve the goals of
the system. We then discuss how these techniques can be combined to form
single-replica and multi-replica inference systems, including disaggregated
inference systems that offer more control over resource allocation and
serverless systems that can be deployed over shared hardware infrastructure. We
end with a discussion of remaining challenges.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference](https://arxiv.org/abs/2506.22033)
*Yongchao He,Bohan Zhao,Zheng Cao*

Main category: cs.DC

TL;DR: SiPipe是一种异构流水线设计，通过利用未充分利用的CPU资源来卸载辅助计算和通信，显著提高了大型语言模型（LLM）的推理吞吐量和效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理工作负载的增加，流水线并行（PP）在多GPU部署中广泛使用，但其存在执行效率低下的问题，如负载不平衡和阶段间/阶段内气泡，限制了流水线饱和。

Method: SiPipe采用三种关键技术：CPU采样、令牌安全执行模型和结构感知传输，以减轻流水线气泡并提高执行效率。

Result: 实验表明，SiPipe在相同PP配置下，比现有最佳方案vLLM实现了高达2.1倍的吞吐量提升、43%的每令牌延迟降低和23%的平均GPU利用率提升。

Conclusion: SiPipe通过优化异构资源利用，显著提升了LLM推理的性能和效率，适用于多种LLM和部署场景。

Abstract: As inference workloads for large language models (LLMs) scale to meet growing
user demand, pipeline parallelism (PP) has become a widely adopted strategy for
multi-GPU deployment, particularly in cross-node setups, to improve key-value
(KV) cache capacity and inference throughput. However, PP suffers from inherent
inefficiencies caused by three types of execution bubbles-load-imbalance,
intra-stage, and inter-stage-which limit pipeline saturation. We present
SiPipe, a heterogeneous pipeline design that improves throughput by leveraging
underutilized CPU resources to offload auxiliary computation and communication.
SiPipe incorporates three key techniques-CPU sampling, a token-safe execution
model, and structure-aware transmission-to mitigate pipeline bubbles and
improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1
times higher throughput, 43% lower per-token latency, and up to 23% higher
average GPU utilization compared to the state-of-the-art vLLM under the same PP
configuration, demonstrating its generality across LLMs and deployment
scenarios.

</details>


### [4] [SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap](https://arxiv.org/abs/2506.22035)
*Qiqi GU,Chenpeng Wu,Heng Shi,Jianguo Yao*

Main category: cs.DC

TL;DR: SPTCStencil利用稀疏张量核心优化模板计算，消除冗余零填充，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前在张量核心加速器上优化模板计算的方法因冗余零填充导致高开销，需改进。

Method: 提出稀疏计算范式，将模板计算高效转换为稀疏矩阵乘法，并设计专用GPU内核。

Result: SPTCStencil平均性能提升5.46倍，优于传统张量核心方法2倍。

Conclusion: SPTCStencil首次将稀疏张量核心应用于非深度学习领域，显著提升模板计算效率。

Abstract: Stencil computation, a pivotal numerical method in science and engineering,
iteratively updates grid points using weighted neighbor contributions and
exhibits strong parallelism for multi-core processors. Current optimization
techniques targeting conducting stencil computation on tensor core accelerators
incur substantial overheads due to redundant zero-padding during the
transformation to matrix multiplication. To address this, we introduce a sparse
computation paradigm that eliminates inefficiencies by exploiting specialized
hardware units.
  This paper exploits the sparsity in these matrices as a feature and presents
SPTCStencil, a high-performance stencil computation system accelerated by
Sparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for
acceleration beyond deep learning domains. First, Our approach generalizes an
efficient transformation of stencil computation into matrix multiplications and
specializes this conversion for SpTC compatibility through a novel
sparsification strategy. Furthermore, SPTCStencil incorporates a
high-performance GPU kernel with systematic optimizations designed to maximize
efficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil
5.46$\times$ and Tensor Core-based approaches by 2.00$\times$ on average.

</details>


### [5] [MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators](https://arxiv.org/abs/2506.22169)
*Zheng Zhang,Donglin Yang,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: MCFuser是一个创新框架，用于高效融合内存密集型计算密集型（MBCI）算子链，通过高级分块表达式和DAG分析优化内核，显著提升性能并减少调优时间。


<details>
  <summary>Details</summary>
Motivation: 解决多计算密集型算子融合因计算吞吐饱和而失败的问题，以及动态张量维度导致的内存限制和调优效率低下的挑战。

Method: 利用高级分块表达式定义搜索空间，结合DAG分析消除冗余内存访问，通过剪枝搜索空间和启发式搜索加速调优。

Result: 在NVIDIA A100和RTX3080 GPU上，MCFuser比Ansor等编译器性能提升5.9倍，调优时间减少70倍以上。

Conclusion: MCFuser通过高效的内核融合和优化策略，显著提升了内存密集型计算密集型算子的性能，同时大幅降低了调优时间。

Abstract: Operator fusion, a key technique to improve data locality and alleviate GPU
memory bandwidth pressure, often fails to extend to the fusion of multiple
compute-intensive operators due to saturated computation throughput. However,
the dynamicity of tensor dimension sizes could potentially lead to these
operators becoming memory-bound, necessitating the generation of fused kernels,
a task hindered by limited search spaces for fusion strategies, redundant
memory access, and prolonged tuning time, leading to sub-optimal performance
and inefficient deployment.
  We introduce MCFuser, a pioneering framework designed to overcome these
obstacles by generating high-performance fused kernels for what we define as
memory-bound compute-intensive (MBCI) operator chains. Leveraging high-level
tiling expressions to delineate a comprehensive search space, coupled with
Directed Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,
MCFuser streamlines kernel optimization. By implementing guidelines to prune
the search space and incorporating an analytical performance model with a
heuristic search, MCFuser not only significantly accelerates the tuning process
but also demonstrates superior performance. Benchmarked against leading
compilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a
5.9x speedup in kernel performance and outpaces other baselines while reducing
tuning time by over 70-fold, showcasing its agility.

</details>


### [6] [Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance](https://arxiv.org/abs/2506.22171)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.DC

TL;DR: 论文提出了一种名为Proof-of-Behavior (PoB)的共识模型，通过行为评分和动态权重调整来提升区块链的安全性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有区块链协议（如PoW和PoS）无法衡量验证者的可信度，导致在DeFi环境中存在潜在危害的微妙不当行为。

Method: PoB通过分层行为评分（动机和结果）、动态调整验证者权重以及去中心化验证与比例惩罚机制来实现共识。

Result: 模拟实验显示，PoB将欺诈接受率降低90%以上，两轮内降级恶意验证者，并提升提案公平性，吞吐量开销不超过5%。

Conclusion: PoB通过将共识影响力与可验证的可信行为关联，为金融应用中的区块链治理提供了可扩展且合规的基础。

Abstract: Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure
the ledger yet cannot measure validator trustworthiness, allowing subtle
misconduct that is especially damaging in decentralized-finance (DeFi)
settings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)
gives each action a layered utility score -- covering motivation and outcome,
(ii) adapts validator weights using recent scores, and (iii) applies
decentralized verification with proportional slashing. The reward design is
incentive-compatible, yielding a Nash equilibrium in which honest behavior
maximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,
reputation-weighted validation) show that PoB cuts fraud acceptance by more
than 90%, demotes malicious validators within two rounds, and improves proposer
fairness versus standard PoS, all with no more than a 5% throughput overhead.
By linking consensus influence to verifiably trustworthy conduct, PoB offers a
scalable, regulation-friendly foundation for secure and fair blockchain
governance in financial applications.

</details>


### [7] [MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism](https://arxiv.org/abs/2506.22175)
*Zheng Zhang,Donglin Yang,Yaqi Xia,Liang Ding,Dacheng Tao,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: MPipeMoE是一个高性能库，通过自适应和内存高效的流水线并行加速MoE训练，实现了2.8倍的速度提升和47%的内存占用减少。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE技术在扩展预训练模型方面表现出色，但其通信和内存消耗的低效性仍是主要挑战。

Method: 设计了自适应流水线并行和内存重用策略，结合硬件和模型特性动态优化。

Result: 在8台NVIDIA DGX A100服务器上测试，MPipeMoE比现有方法快2.8倍，内存占用减少47%。

Conclusion: MPipeMoE有效解决了MoE训练中的效率和内存问题，为大规模模型训练提供了实用解决方案。

Abstract: Recently, Mixture-of-Experts (MoE) has become one of the most popular
techniques to scale pre-trained models to extraordinarily large sizes. Dynamic
activation of experts allows for conditional computation, increasing the number
of parameters of neural networks, which is critical for absorbing the vast
amounts of knowledge available in many deep learning areas. However, despite
the existing system and algorithm optimizations, there are significant
challenges to be tackled when it comes to the inefficiencies of communication
and memory consumption.
  In this paper, we present the design and implementation of MPipeMoE, a
high-performance library that accelerates MoE training with adaptive and
memory-efficient pipeline parallelism. Inspired by that the MoE training
procedure can be divided into multiple independent sub-stages, we design
adaptive pipeline parallelism with an online algorithm to configure the
granularity of the pipelining. Further, we analyze the memory footprint
breakdown of MoE training and identify that activations and temporary buffers
are the primary contributors to the overall memory footprint. Toward memory
efficiency, we propose memory reusing strategies to reduce memory requirements
by eliminating memory redundancies, and develop an adaptive selection component
to determine the optimal strategy that considers both hardware capacities and
model characteristics at runtime. We implement MPipeMoE upon PyTorch and
evaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA
DGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up
to 2.8x speedup and reduces memory footprint by up to 47% in training large
models.

</details>


### [8] [Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need](https://arxiv.org/abs/2506.22267)
*Junaid Ahmed Khan,Hiari Pizzini Cavagna,Andrea Proia,Andrea Bartolini*

Main category: cs.DC

TL;DR: 提出了一种基于虚拟知识图谱（VKG）和大型语言模型（LLM）的端到端ODA聊天机器人系统，显著提高了查询效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能和数据中心的快速发展，计算效率变得至关重要。传统方法在查询无模式的NoSQL数据库时效率低下，需要一种更高效的方法来支持实时数据分析。

Method: 利用VKG生成运行时查询特定的图谱，结合LLM生成SPARQL查询，优化了VKG构建和LLM推理过程。

Result: 系统查询准确率达到92.5%，查询延迟从20.36秒降至3.03秒，VKG大小控制在179 MiB以内。

Conclusion: 该方法显著提升了ODA系统的实时交互能力，适合实际部署。

Abstract: With generative artificial intelligence challenging computational scientific
computing, data centers are experiencing unprecedented growth in both scale and
volume. As a result, computing efficiency has become more critical than ever.
Operational Data Analytics (ODA) relies on the collection of data center
telemetry to improve efficiency, but so far has been focusing on real-time
telemetry data visualization and post-mortem analysis. However, with NoSQL
databases now serving as the default storage backend to support scalability,
querying this data is challenging due to its schema-less nature, which requires
domain knowledge to traverse relationships between data sources. Ontologies and
Knowledge Graphs (KGs) can capture these relationships, but traditional KGs are
costly to scale and have not been widely applied to multivariate timeseries.
Virtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating
query-specific graphs at runtime. In this work, we present a full end-to-end
ODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL
queries, utilizing VKG for data retrieval. This approach achieves 92.5%
accuracy compared to 25% with direct NoSQL queries. The proposed methodology
optimizes VKG construction and LLM inference, cutting previous work average
query latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179
MiB. This performance makes the tool suitable for deployment and real-time
interaction with ODA end-users.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [INTACT: Compact Storage of Data Streams in Mobile Devices to Unlock User Privacy at the Edge](https://arxiv.org/abs/2506.21998)
*Rémy Raes,Olivier Ruas,Adrien Luxey-Bitri,Romain Rouvoy*

Main category: cs.DS

TL;DR: 论文提出了一种名为FLI的技术，通过分段线性近似方法在移动设备上捕获数据流的紧凑表示，并引入了Divide & Stay隐私保护技术，最终在Android和iOS上部署为INTACT框架。


<details>
  <summary>Details</summary>
Motivation: 移动设备产生的数据流具有高价值，但集中处理可能暴露敏感信息。现有隐私保护机制未在实际移动设备中部署，且设备资源受限难以直接处理数据流。

Method: 采用FLI技术进行数据流紧凑表示，结合Divide & Stay隐私保护技术执行POI推断。

Result: 成功在Android和iOS上部署INTACT框架，为边缘隐私保护提供了实际解决方案。

Conclusion: FLI和Divide & Stay技术为移动设备上的隐私保护提供了可行方案，INTACT框架迈出了实际部署的重要一步。

Abstract: Data streams produced by mobile devices, such as smartphones, offer highly
valuable sources of information to build ubiquitous services. Such data streams
are generally uploaded and centralized to be processed by third parties,
potentially exposing sensitive personal information. In this context, existing
protection mechanisms, such as Location Privacy Protection Mechanisms (LPPMs),
have been investigated. Alas, none of them have actually been implemented, nor
deployed in real-life, in mobile devices to enforce user privacy at the edge.
Moreover, the diversity of embedded sensors and the resulting data deluge makes
it impractical to provision such services directly on mobiles, due to their
constrained storage capacity, communication bandwidth and processing power.
This article reports on the FLI technique, which leverages a piece-wise linear
approximation technique to capture compact representations of data streams in
mobile devices. Beyond the FLI storage layer, we introduce Divide \& Stay, a
new privacy preservation technique to execute Points of Interest (POIs)
inference. Finally, we deploy both of them on Android and iOS as the INTACT
framework, making a concrete step towards enforcing privacy and trust in
ubiquitous computing systems.

</details>


### [10] [Fault-Tolerant Matroid Bases](https://arxiv.org/abs/2506.22010)
*Matthias Bentert,Fedor V. Fomin,Petr A. Golovach,Laure Morelle*

Main category: cs.DS

TL;DR: 研究在拟阵中构建容错基的问题，提出了一种固定参数可解（FPT）算法，参数化为k和拟阵的秩r。


<details>
  <summary>Details</summary>
Motivation: 拟阵能够推广线性独立性到向量空间、图等结构，因此研究其容错基问题可以统一和扩展先前研究中的多个容错概念。

Method: 提出了一种固定参数可解（FPT）算法，参数化为k和拟阵的秩r。

Result: 问题在k=1时已经是NP难，而对于r≥3是Para-NP难，对于r≤2则是多项式时间可解。

Conclusion: 该算法在参数k和r下是紧的，为拟阵容错基问题提供了有效的解决方案。

Abstract: We investigate the problem of constructing fault-tolerant bases in matroids.
Given a matroid M and a redundancy parameter k, a k-fault-tolerant basis is a
minimum-size set of elements such that, even after the removal of any k
elements, the remaining subset still spans the entire ground set. Since
matroids generalize linear independence across structures such as vector
spaces, graphs, and set systems, this problem unifies and extends several
fault-tolerant concepts appearing in prior research.
  Our main contribution is a fixed-parameter tractable (FPT) algorithm for the
k-fault-tolerant basis problem, parameterized by both k and the rank r of the
matroid. This two-variable parameterization by k + r is shown to be tight in
the following sense. On the one hand, the problem is already NP-hard for k=1.
On the other hand, it is Para-NP-hard for r \geq 3 and polynomial-time solvable
for r \leq 2.

</details>


### [11] [Parameterized Complexity of Directed Traveling Salesman Problem](https://arxiv.org/abs/2506.22127)
*Václav Blažej,Andreas Emil Feldmann,Foivos Fioravantes,Paweł Rzążewski,Ondřej Suchý*

Main category: cs.DS

TL;DR: 本文研究了有向旅行商问题（DTSP）及其变体有向路径路由问题（DWRP）的参数化复杂性，展示了DWRP在多种结构参数下的固定参数可解性（FPT）和XP复杂性，并证明了其在某些参数下的W[1]-困难性。


<details>
  <summary>Details</summary>
Motivation: 尽管DTSP及其变体已被广泛研究，但关于其参数化复杂性的结果较少。本文旨在系统性地研究这些问题的复杂性，填补这一空白。

Method: 通过分析不同的结构参数（如解的大小、反馈边数、顶点完整性等），研究了DWRP的参数化复杂性。

Result: 证明了DWRP在解的大小、反馈边数和顶点完整性参数下是FPT的，在树宽参数下是XP的，而在恒定树深距离参数下是W[1]-困难的。

Conclusion: 本文为DTSP及其变体的参数化复杂性研究提供了系统性的结果，为进一步研究奠定了基础。

Abstract: The Directed Traveling Salesman Problem (DTSP) is a variant of the classical
Traveling Salesman Problem in which the edges in the graph are directed and a
vertex and edge can be visited multiple times. The goal is to find a directed
closed walk of minimum length (or total weight) that visits every vertex of the
given graph at least once. In a yet more general version, Directed Waypoint
Routing Problem (DWRP), some vertices are marked as terminals and we are only
required to visit all terminals. Furthermore, each edge has its capacity
bounding the number of times this edge can be used by a solution.
  While both problems (and many other variants of TSP) were extensively
investigated, mostly from the approximation point of view, there are
surprisingly few results concerning the parameterized complexity. Our starting
point is the result of Marx et al. [APPROX/RANDOM 2016] who proved that DTSP is
W[1]-hard parameterized by distance to pathwidth 3. In this paper we aim to
initiate the systematic complexity study of variants of DTSP with respect to
various, mostly structural, parameters.
  We show that DWRP is FPT parameterized by the solution size, the feedback
edge number, and the vertex integrity of the underlying undirected graph.
Furthermore, the problem is XP parameterized by treewidth. On the complexity
side, we show that the problem is W[1]-hard parameterized by the distance to
constant treedepth.

</details>


### [12] [Shortest Paths in Multimode Graphs](https://arxiv.org/abs/2506.22261)
*Yael Kirkpatrick,Virginia Vassilevska Williams*

Main category: cs.DS

TL;DR: 本文研究了多模式图中的最短路径问题，提出了一系列近似算法和条件精细下界，证明了某些算法的紧性。


<details>
  <summary>Details</summary>
Motivation: 研究多模式图中最短路径问题，以解决现实世界中不同交通模式不可组合的场景。

Method: 提出线性时间3-近似算法用于2模式直径，并扩展为通用子程序；开发了3-近似k模式半径的通用方案；针对有向图设计了线性时间算法。

Result: 证明了算法的紧性，并提出了新的ℓ-Hitting Set假设以支持参数化下界。

Conclusion: 本文在多模式图参数近似算法和复杂性理论方面取得了重要进展。

Abstract: In this work we study shortest path problems in multimode graphs, a
generalization of the min-distance measure introduced by Abboud, Vassilevska W.
and Wang in [SODA'16]. A multimode shortest path is the shortest path using one
of multiple `modes' of transportation that cannot be combined. This represents
real-world scenarios where different modes are not combinable, such as flights
operated by different airlines. More precisely, a $k$-multimode graph is a
collection of $k$ graphs on the same vertex set and the $k$-mode distance
between two vertices is defined as the minimum among the distances computed in
each individual graph.
  We focus on approximating fundamental graph parameters on these graphs,
specifically diameter and radius. In undirected multimode graphs we first show
an elegant linear time 3-approximation algorithm for 2-mode diameter. We then
extend this idea into a general subroutine that can be used as a part of any
$\alpha$-approximation, and use it to construct a 2 and 2.5 approximation
algorithm for 2-mode diameter. For undirected radius, we introduce a general
scheme that can compute a 3-approximation of the $k$-mode radius for any $k$.
In the directed case we develop novel techniques to construct a linear time
algorithm to determine whether the diameter is finite.
  We also develop many conditional fine-grained lower bounds for various
multimode diameter and radius approximation problems. We are able to show that
many of our algorithms are tight under popular fine-grained complexity
hypotheses, including our linear time 3-approximation for $3$-mode undirected
diameter and radius. As part of this effort we propose the first extension to
the Hitting Set Hypothesis [SODA'16], which we call the $\ell$-Hitting Set
Hypothesis. We use this hypothesis to prove the first parameterized lower bound
tradeoff for radius approximation algorithms.

</details>


### [13] [Faster exponential algorithms for cut problems via geometric data structures](https://arxiv.org/abs/2506.22281)
*László Kozma,Junqi Tan*

Main category: cs.DS

TL;DR: 论文提出了针对几个经典图问题的指数级更快算法，运行时间为$O(1.9999977^n)$，结合了分列技术和计算几何工具。


<details>
  <summary>Details</summary>
Motivation: 解决传统算法运行时间$2^n \cdot n^{O(1)}$的问题，推动精确指数算法领域的发展。

Method: 结合分列技术（split and list）和正交范围搜索（computational geometry工具），适用于决策、优化和计数问题。

Result: 实现了$O(1.9999977^n)$的运行时间，解决了$d$-Cut、Internal Partition和($\alpha,\beta$)-Domination问题。

Conclusion: 算法简单且通用，适用于多种变体和约束条件，填补了部分问题的研究空白。

Abstract: For many hard computational problems, simple algorithms that run in time $2^n
\cdot n^{O(1)}$ arise, say, from enumerating all subsets of a size-$n$ set.
Finding (exponentially) faster algorithms is a natural goal that has driven
much of the field of exact exponential algorithms (e.g., see Fomin and Kratsch,
2010). In this paper we obtain algorithms with running time $O(1.9999977^n)$ on
input graphs with $n$ vertices, for the following well-studied problems:
  - $d$-Cut: find a proper cut in which no vertex has more than $d$ neighbors
on the other side of the cut;
  - Internal Partition: find a proper cut in which every vertex has at least as
many neighbors on its side of the cut as on the other side; and
  - ($\alpha,\beta$)-Domination: given intervals $\alpha,\beta \subseteq
[0,n]$, find a subset $S$ of the vertices, so that for every vertex $v \in S$
the number of neighbors of $v$ in $S$ is from $\alpha$ and for every vertex $v
\notin S$, the number of neighbors of $v$ in $S$ is from $\beta$.
  Our algorithms are exceedingly simple, combining the split and list technique
(Horowitz and Sahni, 1974; Williams, 2005) with a tool from computational
geometry: orthogonal range searching in the moderate dimensional regime (Chan,
2017). Our technique is applicable to the decision, optimization and counting
versions of these problems and easily extends to various generalizations with
more fine-grained, vertex-specific constraints, as well as to directed,
balanced, and other variants. Algorithms with running times of the form $c^n$,
for $c<2$, were known for the first problem only for constant $d$, and for the
third problem for certain special cases of $\alpha$ and $\beta$; for the second
problem we are not aware of such results.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [14] [How (Not) To Write a Software Engineering Abstract](https://arxiv.org/abs/2506.21634)
*Lutz Prechelt,Lloyd Montgomery,Julian Frattini,Franz Zieris*

Main category: cs.SE

TL;DR: 研究发现高质量软件工程会议摘要中仅有29%完整，结构化摘要表现更好，建议改进摘要写作指南。


<details>
  <summary>Details</summary>
Motivation: 分析高质量软件工程会议摘要的结构，发现其不足并提出改进建议。

Method: 采用定性开放编码和定量内容分析，分析362篇摘要的结构和问题。

Result: 仅29%摘要完整，结构化摘要表现更优，4%摘要完全符合标准。

Conclusion: 多数摘要不理想，结构化摘要更优，需改进结论部分并推广新格式。

Abstract: Background: Abstracts are a particularly valuable element in a software
engineering research article. However, not all abstracts are as informative as
they could be. Objective: Characterize the structure of abstracts in
high-quality software engineering venues. Observe and quantify deficiencies.
Suggest guidelines for writing informative abstracts. Methods: Use qualitative
open coding to derive concepts that explain relevant properties of abstracts.
Identify the archetypical structure of abstracts. Use quantitative content
analysis to objectively characterize abstract structure of a sample of 362
abstracts from five presumably high-quality venues. Use exploratory data
analysis to find recurring issues in abstracts. Compare the archetypical
structure to actual structures. Infer guidelines for producing informative
abstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,
provide background, objective, method, result, and conclusion information. For
structured abstracts, the ratio is twice as big. Only 4% of the abstracts are
proper, i.e., they also have good readability (Flesch-Kincaid score) and have
no informativeness gaps, understandability gaps, nor highly ambiguous
sentences. Conclusions: (1) Even in top venues, a large majority of abstracts
are far from ideal. (2) Structured abstracts tend to be better than
unstructured ones. (3) Artifact-centric works need a different structured
format. (4) The community should start requiring conclusions that generalize,
which currently are often missing in abstracts.

</details>


### [15] [Experience converting a large mathematical software package written in C++ to C++20 modules](https://arxiv.org/abs/2506.21654)
*Wolfgang Bangerth*

Main category: cs.SE

TL;DR: 论文探讨了如何将大型数学软件包从传统的C++头文件接口转换为C++20的模块系统，以deal.II有限元库为例，展示了转换的可行性和效果。


<details>
  <summary>Details</summary>
Motivation: 传统的C++头文件接口方式存在笨重、不可靠和速度慢的问题，C++20引入的模块系统提供了更高效的替代方案。

Method: 提出了一种方法，允许在同一代码库中同时提供基于头文件和模块的接口，并讨论了转换过程中的挑战和实际应用中的技术及人为因素。

Result: 转换到模块系统是可行的，能减少库自身的编译时间，但对下游项目的编译时间影响不明显。

Conclusion: 论文总结了转换的可行性和效果，并提出了未来几年或几十年内将整个数学软件生态系统转换为模块系统的长期策略。

Abstract: Mathematical software has traditionally been built in the form of "packages"
that build on each other. A substantial fraction of these packages is written
in C++ and, as a consequence, the interface of a package is described in the
form of header files that downstream packages and applications can then
#include. C++ has inherited this approach towards exporting interfaces from C,
but the approach is clunky, unreliable, and slow. As a consequence, C++20 has
introduced a "module" system in which packages explicitly export declarations
and code that compilers then store in machine-readable form and that downstream
users can "import" -- a system in line with what many other programming
languages have used for decades.
  Herein, I explore how one can convert large mathematical software packages
written in C++ to this system, using the deal.II finite element library with
its around 800,000 lines of code as an example. I describe an approach that
allows providing both header-based and module-based interfaces from the same
code base, discuss the challenges one encounters, and how modules actually work
in practice in a variety of technical and human metrics. The results show that
with a non-trivial, but also not prohibitive effort, the conversion to modules
is possible, resulting in a reduction in compile time for the converted library
itself; on the other hand, for downstream projects, compile times show no clear
trend. I end with thoughts about long-term strategies for converting the entire
ecosystem of mathematical software over the coming years or decades.

</details>


### [16] [The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation](https://arxiv.org/abs/2506.21693)
*Ali Nouri,Beatriz Cabrero-Daniel,Fredrik Törner,Christian Berger*

Main category: cs.SE

TL;DR: 本文通过系统文献综述，探讨了DevOps在自动驾驶开发中的应用，总结了挑战与解决方案，并指出仍需解决的安全问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统开发复杂且需确保安全可靠，DevOps方法因其支持快速响应和持续开发而显得有前景。

Method: 进行了系统文献综述，识别、分析并综合了DevOps在自动驾驶开发中的相关文献。

Result: 提供了挑战与解决方案的结构化概述，揭示了安全相关AI功能中DevOps应用的问题。

Conclusion: 研究指出，实现安全的DevOps以开发安全自动驾驶系统仍需解决多个开放性问题。

Abstract: Developing autonomous driving (AD) systems is challenging due to the
complexity of the systems and the need to assure their safe and reliable
operation. The widely adopted approach of DevOps seems promising to support the
continuous technological progress in AI and the demand for fast reaction to
incidents, which necessitate continuous development, deployment, and
monitoring. We present a systematic literature review meant to identify,
analyse, and synthesise a broad range of existing literature related to usage
of DevOps in autonomous driving development. Our results provide a structured
overview of challenges and solutions, arising from applying DevOps to
safety-related AI-enabled functions. Our results indicate that there are still
several open topics to be addressed to enable safe DevOps for the development
of safe AD.

</details>


### [17] [Using Generative AI in Software Design Education: An Experience Report](https://arxiv.org/abs/2506.21703)
*Victoria Jackson,Susannah Liu,Andre van der Hoek*

Main category: cs.SE

TL;DR: 论文探讨了在本科软件设计课程中引入生成式AI（如ChatGPT）的经验，分析了学生使用AI完成团队作业的效果及其对学习的影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的快速普及，软件工程教育者需要探索如何将其有效融入课堂，尤其是在软件设计等非编码领域。

Method: 学生在团队作业中使用ChatGPT辅助设计，收集对话日志和反思，并进行定性分析。

Result: 学生发现ChatGPT在设计过程中提供了帮助，但也意识到需批判性评估其回答；同时，研究总结了教育者有效部署AI的关键经验。

Conclusion: 生成式AI在软件设计教育中具有潜力，既能辅助设计，又能帮助学生了解AI的优势与局限。

Abstract: With the rapid adoption of Generative AI (GenAI) tools, software engineering
educators have grappled with how best to incorporate them into the classroom.
While some research discusses the use of GenAI in the context of learning to
code, there is little research that explores the use of GenAI in the classroom
for other areas of software development. This paper provides an experience
report on introducing GenAI into an undergraduate software design class.
Students were required to use GenAI (in the form of ChatGPT) to help complete a
team-based assignment. The data collected consisted of the ChatGPT conversation
logs and students' reflections on using ChatGPT for the assignment.
Subsequently, qualitative analysis was undertaken on the data. Students
identified numerous ways ChatGPT helped them in their design process while
recognizing the need to critique the response before incorporating it into
their design. At the same time, we identified several key lessons for educators
in how to deploy GenAI in a software design class effectively. Based on our
experience, we believe students can benefit from using GenAI in software design
education as it helps them design and learn about the strengths and weaknesses
of GenAI.

</details>


### [18] [KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering](https://arxiv.org/abs/2506.22037)
*Jiawei Li,Zan Liang,Guoxin Wang,Jinzhi Lu,Yan Yan,Shouxuan Wu,Hao Wang*

Main category: cs.SE

TL;DR: 提出了一种基于KARMA语言和自然语言处理的模型重构方法，用于支持系统开发过程模型的管理和优化。


<details>
  <summary>Details</summary>
Motivation: 解决系统开发过程中需求变更管理不足和模型重构困难的问题。

Method: 利用KARMA语言统一形式化过程模型，引入模型重构框架，结合自然语言处理技术分析需求文本并提取约束信息。

Result: 通过飞机机载维护系统的案例验证，显著提高了开发过程的设计效率。

Conclusion: 该方法有效支持了开发过程模型的动态重构和优化。

Abstract: Model reconstruction is a method used to drive the development of complex
system development processes in model-based systems engineering. Currently,
during the iterative design process of a system, there is a lack of an
effective method to manage changes in development requirements, such as
development cycle requirements and cost requirements, and to realize the
reconstruction of the system development process model. To address these
issues, this paper proposes a model reconstruction method to support the
development process model. Firstly, the KARMA language, based on the GOPPRR-E
metamodeling method, is utilized to uniformly formalize the process models
constructed based on different modeling languages. Secondly, a model
reconstruction framework is introduced. This framework takes a structured
development requirements based natural language as input, employs natural
language processing techniques to analyze the development requirements text,
and extracts structural and optimization constraint information. Then, after
structural reorganization and algorithm optimization, a development process
model that meets the development requirements is obtained. Finally, as a case
study, the development process of the aircraft onboard maintenance system is
reconstructed. The results demonstrate that this method can significantly
enhance the design efficiency of the development process.

</details>


### [19] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 提出基于MAPE-K和代理AI的框架，用于微服务的自主异常检测与修复，提升系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 微服务的去中心化特性带来安全和管理的挑战，威胁系统稳定性。

Method: 基于MAPE-K框架，利用代理AI实现自主异常检测与修复。

Result: 提供实用、行业化的解决方案，增强系统稳定性、减少停机时间，并监控性能、弹性等质量属性。

Conclusion: 该框架可定制，适用于研究和实践，提升微服务系统的整体质量。

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


### [20] [Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny](https://arxiv.org/abs/2506.22370)
*Carolina Carreira,Álvaro Silva,Alexandre Abreu,Alexandra Mendes*

Main category: cs.SE

TL;DR: 论文研究了学生在使用大型语言模型（如ChatGPT）解决形式化验证任务时的表现和策略，发现使用ChatGPT的学生表现更好，但效果与提示质量相关。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在支持认知密集型任务（如程序验证）中的作用，填补现有研究的空白。

Method: 采用混合方法研究，让硕士生在形式化方法课程中完成Dafny验证问题，一组使用ChatGPT，另一组不使用，并记录互动日志。

Result: 使用ChatGPT的学生表现显著更好，但表现提升与提示质量密切相关。

Conclusion: 建议在形式化方法课程中更有效地整合大型语言模型，设计能促进学习而非替代的LLM感知挑战。

Abstract: Students in computing education increasingly use large language models (LLMs)
such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding
tasks, like deductive program verification, remains poorly understood. This
paper investigates how students interact with an LLM when solving formal
verification exercises in Dafny, a language that supports functional
correctness, by allowing programmers to write formal specifications and
automatically verifying that the implementation satisfies the specification. We
conducted a mixed-methods study with master's students enrolled in a formal
methods course. Each participant completed two verification problems, one with
access to a custom ChatGPT interface, that logged all interactions, and the
other without. We identified strategies used by successful students and
assessed the level of trust students place in LLMs. %\todo{Our findings show
that something here} Our findings show that students perform significantly
better when using ChatGPT; however, performance gains are tied to prompt
quality. We conclude with practical recommendations for integrating LLMs into
formal methods courses more effectively, including designing LLM-aware
challenges that promote learning rather than substitution.

</details>


### [21] [What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub](https://arxiv.org/abs/2506.22390)
*Ramtin Ehsani,Sakshi Pathak,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 分析了686个开发者与ChatGPT的对话，发现62%的对话对解决问题有帮助。ChatGPT在代码生成和工具推荐上表现较好，但在代码解释上较差。有效的对话通常更短、更易读且语义对齐。


<details>
  <summary>Details</summary>
Motivation: 研究开发者与ChatGPT对话的有效性，以优化问题解决效率。

Method: 分析686个GitHub问题线程中的对话，分类任务类型，评估对话、项目和问题相关指标。

Result: 62%的对话有帮助，ChatGPT擅长代码生成和工具推荐，但对代码解释表现不佳。有效对话更短、更易读。

Conclusion: 研究结果可指导开发者优化交互策略，改进提示设计，并为LLM微调提供参考。

Abstract: Conversational large-language models are extensively used for issue
resolution tasks. However, not all developer-LLM conversations are useful for
effective issue resolution. In this paper, we analyze 686 developer-ChatGPT
conversations shared within GitHub issue threads to identify characteristics
that make these conversations effective for issue resolution. First, we analyze
the conversations and their corresponding issues to distinguish helpful from
unhelpful conversations. We begin by categorizing the types of tasks developers
seek help with to better understand the scenarios in which ChatGPT is most
effective. Next, we examine a wide range of conversational, project, and
issue-related metrics to uncover factors associated with helpful conversations.
Finally, we identify common deficiencies in unhelpful ChatGPT responses to
highlight areas that could inform the design of more effective developer-facing
tools. We found that only 62% of the ChatGPT conversations were helpful for
successful issue resolution. ChatGPT is most effective for code generation and
tools/libraries/APIs recommendations, but struggles with code explanations.
Helpful conversations tend to be shorter, more readable, and exhibit stronger
semantic and linguistic alignment. Larger, more popular projects and more
experienced developers benefit more from ChatGPT. At the issue level, ChatGPT
performs best on simpler problems with limited developer activity and faster
resolution, typically well-scoped tasks like compilation errors. The most
common deficiencies in unhelpful ChatGPT responses include incorrect
information and lack of comprehensiveness. Our findings have wide implications
including guiding developers on effective interaction strategies for issue
resolution, informing the development of tools or frameworks to support optimal
prompt design, and providing insights on fine-tuning LLMs for issue resolution
tasks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion](https://arxiv.org/abs/2506.21933)
*Yifan Xue,Ruihuai Liang,Bo Yang,Xuelin Cao,Zhiwen Yu,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于图注意力扩散的解决方案生成器（GADSG），用于低空经济网络中的任务卸载和资源分配问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展，对实时和智能任务调度的需求增加，但现有系统面临节点异构性、通信链路不稳定和任务动态变化等挑战。

Method: 构建了三层异构MEC系统架构，提出GADSG方法，结合图注意力网络的上下文感知和扩散模型的解分布学习能力。

Result: 实验表明，GADSG在优化性能、鲁棒性和泛化性上显著优于基线方法。

Conclusion: GADSG在动态复杂的低空经济网络环境中展现出高效任务调度的潜力。

Abstract: With the rapid development of the low-altitude economy, air-ground integrated
multi-access edge computing (MEC) systems are facing increasing demands for
real-time and intelligent task scheduling. In such systems, task offloading and
resource allocation encounter multiple challenges, including node
heterogeneity, unstable communication links, and dynamic task variations. To
address these issues, this paper constructs a three-layer heterogeneous MEC
system architecture for low-altitude economic networks, encompassing aerial and
ground users as well as edge servers. The system is systematically modeled from
the perspectives of communication channels, computational costs, and constraint
conditions, and the joint optimization problem of offloading decisions and
resource allocation is uniformly abstracted into a graph-structured modeling
task. On this basis, we propose a graph attention diffusion-based solution
generator (GADSG). This method integrates the contextual awareness of graph
attention networks with the solution distribution learning capability of
diffusion models, enabling joint modeling and optimization of discrete
offloading variables and continuous resource allocation variables within a
high-dimensional latent space. We construct multiple simulation datasets with
varying scales and topologies. Extensive experiments demonstrate that the
proposed GADSG model significantly outperforms existing baseline methods in
terms of optimization performance, robustness, and generalization across task
structures, showing strong potential for efficient task scheduling in dynamic
and complex low-altitude economic network environments.

</details>


### [23] [Resilient Communication For Avalanche Response in Infrastructure-Limited Environments](https://arxiv.org/abs/2506.22148)
*Joshua Goulton,Milena Radenkovic*

Main category: cs.NI

TL;DR: 本文研究了利用瑞士铁路网络作为数据骨干网传播关键雪崩警报的可行性，通过模拟实验验证了DTN协议在密集城市和偏远山区的有效性。


<details>
  <summary>Details</summary>
Motivation: 在基础设施有限的自然灾害环境中，延迟容忍网络（DTN）为维持通信提供了新思路。本文旨在探索利用现有国家运输系统（瑞士铁路网络）作为数据骨干网的潜力。

Method: 使用The Opportunistic Network Environment (ONE)模拟器，对瑞士铁路网络进行建模，并比较两种DTN路由协议（Epidemic和PROPHET）在密集城市和偏远山区的表现。

Result: 实验结果表明，铁路网络在两种环境中均能提供稳健的通信连接，验证了DTN在偏远场景中的应用潜力。

Conclusion: 瑞士铁路网络可作为DTN的有效骨干网，支持关键警报的传播，尤其在基础设施有限的偏远地区。

Abstract: Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining
communication in infrastructure limited environments, such as those encountered
during natural disasters. This paper investigates the viability of leveraging
an existing national transport system - the Swiss rail network - as a data mule
backbone for disseminating critical avalanche alerts. Using The Opportunistic
Network Environment (ONE) simulator, we model the entire Swiss rail network and
conduct a rigorous comparative analysis of two seminal DTN routing protocols:
Epidemic and PROPHET. Experiments are performed in two distinct scenarios:
alerts originating from dense urban centres and from sparse, remote mountainous
regions. Our results demonstrate that the rail network provides robust
connectivity for opportunistic communication in both environments thus
validating the integration of DTN principles in remote scenarios.

</details>


### [24] [V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles](https://arxiv.org/abs/2506.22223)
*Felipe Valle Quiroz,Johan Elfing,Joel Pålsson,Elena Haller,Oscar Amador Molina*

Main category: cs.NI

TL;DR: 提出了一种新型的电动助力自行车意图共享机制，通过椭圆地理区域表示和最小二乘法优化轨迹预测，提升了V2X通信中的网络可靠性和传输频率。


<details>
  <summary>Details</summary>
Motivation: 提升电动助力自行车在V2X通信中的意图共享效率，增强弱势道路使用者的安全性。

Method: 采用二次多项式拟合和最小二乘法，将离散轨迹预测点替换为紧凑的椭圆地理区域表示，固定数据负载大小。

Result: 仿真显示在受限通信条件下，相比标准ETSI VAM协议，具有更优的包间隔性能；物理实验验证了嵌入式系统的实时部署可行性。

Conclusion: 该方法支持可扩展、低延迟的意图共享，为联网和自动化出行生态系统中的弱势道路用户提供了更高的安全性，并探讨了最小二乘法的适用性。

Abstract: This paper introduces a novel intention-sharing mechanism for Electrically
Power-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing
the ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete
predicted trajectory points with a compact elliptical geographical area
representation derived via quadratic polynomial fitting and Least Squares
Method (LSM). This approach encodes trajectory predictions with fixed-size data
payloads, independent of the number of forecasted points, enabling
higher-frequency transmissions and improved network reliability. Simulation
results demonstrate superior inter-packet gap (IPG) performance compared to
standard ETSI VAMs, particularly under constrained communication conditions. A
physical experiment validates the feasibility of real-time deployment on
embedded systems. The method supports scalable, low-latency intention sharing,
contributing to cooperative perception and enhanced safety for vulnerable road
users in connected and automated mobility ecosystems. Finally, we discuss the
viability of LSM and open the door to other methods for prediction.

</details>


### [25] [Design and Evaluation of IEEE 802.11ax Uplink Orthogonal Frequency Division Multiple Random Access in ns-3](https://arxiv.org/abs/2506.22260)
*Douglas Dziedzorm Agbeve,Andrey Belogaev,Jeroen Famaey*

Main category: cs.NI

TL;DR: 论文提出了一种完全符合标准的开源UORA实现，解决了现有ns-3模拟器中UORA实现的关键限制，提升了资源分配的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着Wi-Fi网络密度增加和新兴应用对低延迟、高可靠性的需求，EDCA机制的局限性日益明显，而现有UORA研究多依赖不公开的模拟器，限制了结果的可复现性和验证。

Method: 开发了一个完全符合标准且开源的UORA实现，解决了ns-3模拟器中现有UORA实现的关键限制，如资源调度和配置信号问题。

Result: 实现了更高效和灵活的UORA资源分配，为未来Wi-Fi资源分配策略研究提供了更准确的评估工具。

Conclusion: 开源UORA实现填补了现有研究的空白，推动了Wi-Fi资源分配策略的进一步研究。

Abstract: Wi-Fi networks have long relied on the Enhanced Distributed Channel Access
(EDCA) mechanism, allowing stations to compete for transmission opportunities.
However, as networks become denser and emerging applications demand lower
latency and higher reliability, the limitations of EDCA such as overhead due to
contention and collisions have become more pronounced. To address these
challenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been
introduced in Wi-Fi, enabling more efficient channel utilization through
scheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal
Frequency Division Multiple Random Access (UORA), a hybrid mechanism that
combines both scheduled and random access, balancing efficiency and
responsiveness in resource allocation. Despite significant research on UORA,
most studies rely on custom simulators that are not publicly available,
limiting reproducibility and preventing validation of the presented results.
The only known open-source UORA implementation in the ns-3 simulator exhibits
key limitations, such as usage of the same trigger frame (TF) to schedule
resources for buffer status reports and data transmissions, and lack of
signaling for UORA configuration. In this paper, we present a fully
standard-compliant and open source UORA implementation that is compatible with
ns-3 version 3.38, addressing these limitations to improve resource allocation
efficiency and adaptability. This implementation enables more accurate and
flexible evaluation of UORA, fostering future research on Wi-Fi resource
allocation strategies.

</details>


### [26] [Concept-Level AI for Telecom: Moving Beyond Large Language Models](https://arxiv.org/abs/2506.22359)
*Viswanath Kumarskandpriya,Abdulhalim Dandoush,Abbas Bradai,Ali Belgacem*

Main category: cs.NI

TL;DR: 论文探讨了在电信和网络领域，大型概念模型（LCMs）比大型语言模型（LLMs）更适合解决复杂、多层次的电信问题。


<details>
  <summary>Details</summary>
Motivation: 电信领域面临复杂、多层次和多管理域系统的挑战，现有LLMs因处理方式受限无法满足需求。

Method: 提出使用LCMs，通过语义概念抽象和双曲潜在空间表示来解决电信问题。

Result: LCMs在内存效率、跨层关联和多模态集成方面优于LLMs。

Conclusion: 采用LCMs是实现稳健且高效AI驱动电信管理的必要进化步骤。

Abstract: The telecommunications and networking domain stands at the precipice of a
transformative era, driven by the necessity to manage increasingly complex,
hierarchical, multi administrative domains (i.e., several operators on the same
path) and multilingual systems. Recent research has demonstrated that Large
Language Models (LLMs), with their exceptional general-purpose text analysis
and code generation capabilities, can be effectively applied to certain telecom
problems (e.g., auto-configuration of data plan to meet certain application
requirements). However, due to their inherent token-by-token processing and
limited capacity for maintaining extended context, LLMs struggle to fulfill
telecom-specific requirements such as cross-layer dependency cascades (i.e.,
over OSI), temporal-spatial fault correlation, and real-time distributed
coordination. In contrast, Large Concept Models (LCMs), which reason at the
abstraction level of semantic concepts rather than individual lexical tokens,
offer a fundamentally superior approach for addressing these telecom
challenges. By employing hyperbolic latent spaces for hierarchical
representation and encapsulating complex multi-layered network interactions
within concise concept embeddings, LCMs overcome critical shortcomings of LLMs
in terms of memory efficiency, cross-layer correlation, and native multimodal
integration. This paper argues that adopting LCMs is not simply an incremental
step, but a necessary evolutionary leap toward achieving robust and effective
AI-driven telecom management.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: REDELEX框架评估了不同复杂度的RDL模型在70多个RDB上的表现，发现RDL整体优于传统方法，并分析了性能影响因素。


<details>
  <summary>Details</summary>
Motivation: 研究RDL模型性能与底层RDB特性之间的关系，填补现有研究的空白。

Method: 提出REDELEX框架，评估多种RDL模型在70多个RDB上的表现，并与传统方法对比。

Result: RDL性能普遍优于传统方法，模型复杂度、数据库大小和结构特性是主要影响因素。

Conclusion: REDELEX为RDL模型评估提供了全面框架，揭示了性能关键因素，推动了该领域发展。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [28] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: 论文提出Asymmetric Policy Optimization (APO)方法，结合DADS和STCR技术，解决了MLLMs中复杂推理和过思考问题，显著提升了推理能力且不影响通用任务表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在复杂推理上表现不佳，而强化学习（RL）的应用又容易导致性能下降和过思考问题。

Method: 提出APO方法，将样本分为正负两组：正样本采用DADS动态调整KL散度权重；负样本采用STCR惩罚过长响应。

Result: View-R1-3B模型在推理任务上平均提升7%，优于更大规模的MLLMs，且通用任务表现稳定。

Conclusion: DADS和STCR技术有效提升了MLLMs的复杂推理能力，并具有广泛适用性。

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [29] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: 提出了一种Q学习算法，用于解决风险厌恶总奖励MDP问题，适用于ERM和EVaR目标，具有强收敛性和性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的算法需要完全访问转移概率，限制了在小问题中的应用。

Method: 提出了一种Q学习算法，利用ERM的动态一致性和可激发性。

Result: 数值实验表明，该算法在表格域中快速可靠地收敛到最优风险厌恶值函数。

Conclusion: 该算法为风险厌恶总奖励MDP问题提供了一种有效的无模型解决方案。

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [30] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: 本文揭示了密度聚类方法中核心点邻域半径与簇数之间的单峰关系，并提出基于三分搜索的高效参数调优策略，适用于高维大规模数据。


<details>
  <summary>Details</summary>
Motivation: 密度聚类方法在处理噪声或复杂分布数据时优于基于质心的方法，但参数调优计算成本高，需更高效策略。

Method: 通过理论和实证分析揭示邻域半径与簇数的单峰关系，并基于三分搜索算法设计高效参数调优方法。

Result: 在多个高维大规模NLP、音频和计算机视觉任务中验证了方法的有效性和鲁棒性。

Conclusion: 本研究不仅改进了密度聚类的参数控制，还深化了对参数间关系的理解。

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [31] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 论文提出了一种动态控制质量-复杂度权衡的方法，通过调整时间步长和神经网络长度，实现了高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法在采样过程中需要多次迭代求解ODE，计算复杂度高，因此需要探索更高效的采样方式。

Method: 通过重新连接Transformer架构中的块来求解内部离散ODE，并在训练中引入时间和长度一致性项。

Result: 在CelebA-HQ和ImageNet上的实验显示，延迟减少了3倍，FID分数提高了3.5分。

Conclusion: 该方法在时间和长度维度上均具有灵活性，显著提升了采样效率和生成质量。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [32] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: 论文提出了一种基于文本到文本回归的通用方法，用于预测复杂系统数据中的指标结果，相比传统表格回归方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时效果不佳，需要一种更通用且可扩展的替代方案。

Method: 采用60M参数的编码器-解码器模型，从随机初始化开始训练，进行文本到文本回归。

Result: 在Google的Borg集群调度系统中，模型实现了接近完美的0.99等级相关性（平均0.9），MSE比表格方法低100倍，并能轻松适应新任务。

Conclusion: 该方法为复杂系统结果的通用模拟器提供了可能，强调了编码器、序列长度和不确定性量化的重要性。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [33] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: 提出了一种名为FedIRT的新框架，将联邦学习与IRT结合，实现分布式估计，保护隐私且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 传统IRT需要集中所有数据，存在隐私问题；联邦学习能保护隐私并支持分布式计算。

Method: 结合联邦学习与IRT，提出FedIRT框架，支持分布式估计。

Result: 实验证明FedIRT与传统IRT准确性相当，同时保护隐私并降低通信成本。

Conclusion: FedIRT扩展了IRT的适用性，适用于分布式场景，如多校评估，且开源实现支持实际应用。

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [34] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: 提出了一种基于梯度的神经可塑性适应方法，同时优化神经模糊网络的参数和结构，解决了现有方法中参数与结构分离优化的问题，并在视觉任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经模糊网络（NFNs）具有透明性和符号化表达的优势，但其系统化设计过程仍具挑战性。现有方法通常将参数和结构优化分离，导致架构脆弱且性能不佳。

Method: 提出了一种独立于应用的梯度基神经可塑性适应方法，同时优化NFNs的参数和结构。

Result: 该方法使得NFNs能够应用于之前难以处理的场景，如基于视觉的在线强化学习任务，并在DOOM游戏中验证了其有效性。

Conclusion: 同时优化参数和结构的方法显著提升了NFNs的性能和适用性，为复杂任务提供了新的解决方案。

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [35] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: M3PO是一种基于模型的强化学习框架，通过结合隐式世界模型和混合探索策略，解决了单任务样本效率低和多任务泛化差的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于模型的方法（如DreamerV3）忽略控制中心表示，以及无模型方法（如PPO）样本复杂度高和探索能力弱的问题。

Method: 整合隐式世界模型（预测任务结果而非观测重建）和混合探索策略（结合基于模型的规划和无模型不确定性奖励），利用基于模型与无模型价值估计的差异指导探索，并通过信任区域优化器稳定策略更新。

Result: 在多个基准测试中达到最先进性能。

Conclusion: M3PO为基于模型的策略优化提供了高效且稳健的替代方案。

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [36] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多任务并行化的图基础模型方法，通过GPU加速在超级计算机上高效扩展，解决了多源多保真度数据的预训练挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多源、多保真度数据在预训练中的处理挑战，并提升模型在未探索化学区域的迁移能力。

Method: 采用多任务学习方法，共享消息传递层处理输入原子结构，并通过多个解码头预测数据特定输出；提出多任务并行化方法，利用GPU加速在超级计算机上实现高效扩展。

Result: 在超过2400万个结构的数据集上训练，并在三种高度异构的超级计算机架构上展示了高效扩展性。

Conclusion: 该方法在预训练稳定性和模型迁移能力方面表现优异，同时展示了在超级计算机上的高效扩展潜力。

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [37] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，解释离散符号结构如何从连续神经网络训练动态中自然涌现。通过将神经参数提升到测度空间并建模训练为Wasserstein梯度流，揭示了在几何约束下参数测度的两种现象：梯度流解耦和自由度收缩。


<details>
  <summary>Details</summary>
Motivation: 探索连续神经网络训练如何自然形成离散符号结构，为神经符号系统的设计和理解提供理论基础。

Method: 将神经参数提升到测度空间，建模训练为Wasserstein梯度流，分析几何约束下的参数测度变化。

Result: 训练过程中，网络从高维探索过渡到符合代数运算的低自由度组合表示，并建立了数据缩放定律。

Conclusion: 该框架为整合连续学习与离散代数推理的神经符号系统提供了理论基础。

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [38] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: 该论文比较了前向模式自动微分（FmAD）和零阶优化（ZO）与反向传播（BP）及其内存高效变体（如激活检查点）的性能，发现BP在准确性、收敛速度和计算效率上显著优于FmAD和ZO。


<details>
  <summary>Details</summary>
Motivation: 研究FmAD和ZO作为BP的替代方法在实际应用中的表现，尤其是在内存受限的环境中，填补了现有研究中缺乏与内存高效BP变体比较和统一理论分析的空白。

Method: 通过理论和实验分析，比较BP、FmAD和ZO在内存使用、准确性、收敛速度和计算效率上的表现。

Result: 实验表明，BP在内存使用相当的情况下，准确性比FmAD和ZO高31.1%，收敛速度快34.8%，计算量少3.8倍。

Conclusion: BP结合检查点是最有效的内存受限训练策略，而FmAD和ZO存在显著的性能缺陷。

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [39] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: 论文探讨了在随机系统中使用Koopman算子理论处理部分观测的效果，强调了状态空间与函数空间的区分，并展示了延迟嵌入技术的优势。


<details>
  <summary>Details</summary>
Motivation: 在随机系统中，部分观测是常见的，但缺乏理论框架。Mori-Zwanzig形式主义和Koopman算子理论的结合为解决这一问题提供了可能。

Method: 采用Koopman算子理论分析随机系统中的部分观测，结合数值实验验证延迟嵌入技术的有效性。

Result: 数值实验显示，加性噪声的幅度与精度之间存在幂律行为，并探讨了幂律指数与部分观测效果的关系。

Conclusion: 在随机系统中，区分状态空间与函数空间至关重要，延迟嵌入技术对部分观测有效，幂律行为揭示了噪声与精度的关系。

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [40] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: 该论文综述了持续强化学习（CRL）的核心概念、挑战和方法，提出了新的分类法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）在动态和真实环境中的泛化能力有限，持续学习（CL）为解决这一问题提供了可能。CRL旨在通过持续学习和知识保留来提升RL的适用性。

Method: 论文首先回顾了现有工作，分析了其指标、任务、基准和场景设置；其次提出了基于知识存储和转移的CRL方法分类法。

Result: 通过分类法和分析，论文总结了CRL的独特挑战，并为未来研究提供了实用见解。

Conclusion: CRL是一个有前景的研究方向，未来需要进一步解决其挑战以提升RL在动态环境中的表现。

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [41] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: 本文综述了持续强化学习（CRL）的关键概念、挑战和方法，重点关注其在机器人领域的应用，并讨论了未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究持续强化学习的动机在于使RL代理能够动态学习和保留知识，适应多样化和动态的任务需求。

Method: 综述了CRL的基本概念、挑战和新兴方法，特别关注机器人领域的最新进展和评估环境。

Result: 总结了CRL的现状，包括其优势和局限性，并提供了对未来研究的建议。

Conclusion: 持续强化学习是一个充满潜力的领域，但仍需解决挑战以推动其在实际应用中的发展。

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [42] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST是一个面向6G网络的语义感知通信框架，通过动态任务平衡、参数高效微调和噪声恢复技术，显著提升了多任务优化性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要从比特传输转向语义感知通信，以强调任务相关信息，解决动态无线环境中的多任务优化挑战。

Method: TOAST结合了深度强化学习的任务平衡、基于Swin Transformer的低秩适应机制和潜在空间扩散模型，以优化性能和适应不同信道条件。

Result: 实验表明，TOAST在低信噪比条件下显著提升了分类准确性和重建质量，并在所有测试场景中表现稳健。

Conclusion: TOAST为6G语义通信提供了一个高效、自适应的解决方案，具有广泛的应用潜力。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [43] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: HQCM-EBTC是一种混合量子-经典模型，用于MRI图像的自动脑肿瘤分类，显著优于经典基线模型。


<details>
  <summary>Details</summary>
Motivation: 提高脑肿瘤分类的准确性和可解释性，探索量子增强模型在医学影像中的潜力。

Method: 结合5量子比特、深度为2的量子层与5个并行电路，使用AdamW优化器和混合损失函数（交叉熵和注意力一致性）。

Result: 在7,576张扫描图像上达到96.48%的准确率，优于经典模型的86.72%，尤其在胶质瘤检测中表现更优。

Conclusion: 量子增强模型在医学影像中具有潜力，可提升诊断准确性和可解释性。

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [44] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: GuiderNet通过元学习框架优化量子电路的几何条件，显著提升训练效果和测试准确率。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法（VQAs）在近期量子优势中具有潜力，但面临梯度消失和优化条件差的问题。

Method: 引入GuiderNet，一个基于元学习的框架，通过数据依赖的参数调整优化量子电路的几何条件。

Result: 在糖尿病分类任务中，GuiderNet将测试准确率从75.3%提升至98.6%，并显著改善训练稳定性。

Conclusion: 几何元条件化可缓解量子机器学习中的梯度消失和优化问题，提升可训练性和泛化能力。

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [45] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的DAS神经网络范式，无需真实事件数据进行训练，通过物理建模生成数据，并在去噪和事件识别中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决DAS应用中真实事件数据有限的问题，同时提升去噪和事件识别的性能。

Method: 通过物理建模生成DAS事件数据，训练生成网络和去背景网络，应用于事件识别和故障监测。

Result: 在公开数据集和实际应用中表现优于或媲美基于真实数据的模型，故障诊断准确率达91.8%。

Conclusion: 该范式为解决DAS数据获取和噪声问题提供了潜在解决方案，拓展了DAS的应用潜力。

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [46] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种改进的决策变换器（R* DT），用于自动竞价任务，通过数据增强和优化RTG值，提升了竞价系统的性能。


<details>
  <summary>Details</summary>
Motivation: 在线广告竞价中，传统决策变换器（DT）存在RTG值预设和训练数据质量不均的问题，限制了自动化竞价的效果。

Method: 提出R* DT，分三步：R DT存储状态和RTG值；R^ DT预测最优RTG值；R* DT通过生成高奖励轨迹增强训练数据。

Result: 在公开竞价数据集上验证了R* DT的有效性，尤其在处理混合质量轨迹时表现更优。

Conclusion: R* DT通过优化RTG值和数据增强，显著提升了自动竞价的性能。

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [47] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: 提出CitySim愿景，通过SceneDiffuser++实现端到端的城市规模交通模拟，整合场景生成、行为建模等技术，验证其在长时模拟中的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决交通模拟中手动驾驶数据有限的问题，通过生成式模拟城市环境，支持自动驾驶软件的测试与验证。

Method: 提出SceneDiffuser++，首个基于单一损失函数的端到端生成世界模型，整合场景生成、动态代理行为建模等技术。

Result: 在扩展版Waymo Open Motion Dataset上验证了SceneDiffuser++的城市规模模拟能力及长时模拟的优越性。

Conclusion: SceneDiffuser++为城市规模交通模拟提供了高效、真实的解决方案，填补了动态场景生成等技术的空白。

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [48] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 提出了一种新的概率半参数模型，利用数据分箱降低核密度估计的计算成本，通过稀疏张量和限制父节点数量解决维度灾难，实验表明其性能与未分箱模型相当但速度更快。


<details>
  <summary>Details</summary>
Motivation: 解决非参数分布中核密度估计的高计算成本问题，同时应对分箱模型中的维度灾难。

Method: 开发了两种新的条件概率分布（稀疏分箱核密度估计和傅里叶核密度估计），利用稀疏张量和限制父节点数量优化计算。

Result: 分箱半参数贝叶斯网络在结构学习和对数似然估计上与未分箱模型无显著差异，但速度显著提升。

Conclusion: 分箱半参数贝叶斯网络是一种更高效且可靠的替代方案。

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [49] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种基于图的状态空间模型，用于图时间序列数据，结合了图结构和时间动态，通过最大似然和深度学习进行参数学习和状态跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决图时间序列数据中图-时间模式的联合建模问题，适用于城市水网、经济学和网络神经科学等领域。

Method: 使用图诱导的参数量化模型构建状态和观测方程，状态方程基于图上的随机偏微分方程，观测模型为图滤波采样版本。

Result: 提出了理论可追踪的最大似然方法和可扩展的深度学习架构，用于参数学习和状态跟踪。

Conclusion: 模型在部分观测数据下有效，适用于预测和插补等下游任务。

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [50] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: 论文提出了一种名为TROFI的离线逆强化学习方法，无需预定义奖励函数即可有效学习策略，并在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，通常需要预定义的奖励函数来标记数据集，但在实际应用中（如视频游戏开发）奖励函数可能不可用。

Method: TROFI通过从人类偏好中学习奖励函数，并用其标记原始数据集，从而训练策略。该方法无需最优轨迹。

Result: 在D4RL基准测试中，TROFI表现优于基线方法，并与使用真实奖励函数学习策略的效果相当。在3D游戏环境中也验证了其有效性。

Conclusion: 研究表明，奖励函数的设计对策略学习至关重要，良好的奖励函数能确保价值函数与实际未来奖励对齐。

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [51] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了联邦多模态知识图谱补全任务（FedMKGC），并提出了MMFeD3-HidE框架以解决多模态不确定性和客户端异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着多模态知识私有化需求的增加，分散的多模态知识图谱缺乏有效的协作系统，需要更强的推理能力和传输安全保障。

Method: 提出了HidE模型用于客户端内恢复完整多模态分布，以及MMFeD3方法用于客户端间知识互传。

Result: 实验验证了MMFeD3-HidE的有效性、语义一致性和收敛鲁棒性。

Conclusion: MMFeD3-HidE为联邦多模态知识图谱补全提供了有效的解决方案。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [52] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: UniCA框架通过将异构协变量转化为同构表示，并利用注意力机制融合，扩展了时间序列基础模型（TSFMs）的适用性，支持多模态数据预测。


<details>
  <summary>Details</summary>
Motivation: 现有TSFMs主要针对实值序列，难以处理包含异构协变量（如分类变量、图像、文本）的预测任务。UniCA旨在填补这一空白。

Method: UniCA通过协变量同质化将异构数据转化为同构序列表示，并采用统一的注意力融合机制。

Result: 在单模态和多模态协变量预测基准测试中，UniCA表现优异。

Conclusion: UniCA展示了协变量感知TSFM适应在现实预测场景中的潜力。

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [53] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GPAS的技术，用于解决Pre-LN Transformer中激活方差指数增长的问题，通过缩放中间激活值但保持梯度不变，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: Pre-LN Transformer在预训练中稳定且可扩展，但存在激活方差指数增长的问题，导致深层学习能力受限。

Method: 提出GPAS技术，通过缩放中间激活值但保持梯度不变，避免梯度消失问题。

Result: 在71M到1B不同规模的模型上实验，GPAS均带来性能提升，并适用于其他架构如Sandwich-LN和DeepNorm。

Conclusion: GPAS是一种简单有效的技术，能够提升Pre-LN及其他架构的训练动态和性能。

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [54] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: 提出了一种结合LSTM和XGBoost的混合深度学习与机器学习模型，用于加密货币价格预测，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场的波动性和复杂性对价格预测提出了独特挑战，需要更高效的预测方法。

Method: 整合LSTM（捕捉时间依赖性）和XGBoost（建模非线性关系），结合历史价格数据和辅助特征（如情感分数和宏观经济指标）。

Result: 在比特币、以太坊等加密货币数据集上，LSTM+XGBoost混合模型在MAPE和MinMax RMSE指标上优于单一模型和传统方法。

Conclusion: 混合架构在金融预测中具有潜力，并展示了跨不同加密货币和市场环境的适应性。

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [55] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 论文揭示了Transformer架构与图神经网络（GNN）之间的联系，指出Transformer可视为在完全连接图上操作的消息传递GNN，并探讨了其在硬件效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer与GNN之间的理论联系，揭示Transformer在硬件效率上的优势。

Method: 通过将Transformer的自注意力机制解释为消息传递GNN，分析其在完全连接图上的操作。

Result: Transformer是一种表达能力强的集合处理网络，能够高效学习输入元素间的关系。

Conclusion: Transformer是当前硬件环境下高效的GNN实现，展现了硬件优势对算法选择的影响。

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [56] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: 论文提出两种神经方法解决多目标多图路由问题，验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 多图环境下的多目标路由问题具有实际意义，但现有研究较少。

Method: 第一种方法直接在多图上自回归选择边；第二种方法先剪枝为简单图再构建路径。

Result: 两种方法在TSP和CVRP等问题上表现优异。

Conclusion: 提出的神经方法在多目标多图路由中有效且性能优越。

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [57] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的模型，简化重金属污染评估过程，通过迁移学习解决数据稀缺问题，并在澳大利亚六个港口验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统PLI评估方法繁琐且数据收集困难，亟需一种高效、低成本的重金属污染评估方法。

Method: 利用迁移学习开发深度学习模型，跨领域转移学习特征，预测PLI。

Result: 模型在六个港口的测试中表现出色，MAE和MAPE分别为0.5和0.03，性能优于基线模型。

Conclusion: 该模型为水质预测提供了创新、易用且经济的方法，对环境保护和工业监测具有重要意义。

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [58] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: 该论文研究了地震后建筑结构损坏等级的多分类预测，通过SMOTE解决类别不平衡问题，并比较了多种机器学习和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 地震后准确评估建筑损坏等级对救援和资源分配至关重要，但现有方法存在类别不平衡问题，影响预测效果。

Method: 使用SMOTE处理类别不平衡，结合多种机器学习和深度学习方法（如XGBoost和集成方法）进行多分类预测，并通过特征实验和混淆矩阵评估性能。

Result: 研究发现SMOTE能有效改善类别不平衡问题，并识别出影响地震脆弱性的关键因素。

Conclusion: 该研究为地震后建筑损坏预测提供了更准确的方法，有助于优化救援资源分配。

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [59] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: 提出了一种基于再生核希尔伯特空间的参数化方法，用于控制律学习，并设计了数据驱动的主动学习控制方法。


<details>
  <summary>Details</summary>
Motivation: Thompson采样（TS）依赖于有限的参数表示，限制了其在更一般的控制设计空间中的应用。

Method: 将控制律视为函数空间的元素，利用再生核希尔伯特空间进行参数化，并设计TS框架探索最优控制律。

Result: 理论分析表明，该方法以指数速率学习控制律与闭环性能指标的关系，并推导了控制遗憾的上界。数值实验验证了有效性。

Conclusion: 该方法能够在不限制系统结构或控制器形式的情况下设计控制律，具有广泛适用性和高效性。

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [60] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: 研究探讨了基于LLM的智能体系统在药物发现中的模块化问题，比较了不同LLM和工具调用与代码生成智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLM和智能体系统在药物发现中的潜力，特别是模块化（如LLM的可替换性）这一未被充分研究的领域。

Method: 比较不同LLM（如Claude-3.5-Sonnet、GPT-4o等）的性能，以及工具调用与代码生成智能体的效果，使用LLM-as-a-judge评分。

Result: Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o表现优于其他模型；代码生成智能体平均表现更好，但结果高度依赖问题和模型。

Conclusion: 模块化需结合提示词重新设计，未来需进一步研究以实现稳定、可扩展的智能体系统解决方案。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [61] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: dreaMLearning框架通过压缩数据直接学习，显著减少计算和存储需求，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习对大量标记数据和计算资源的高需求问题。

Method: 基于EntroGeDe的无损压缩方法，直接从压缩数据中学习，支持多种数据类型和任务。

Result: 训练速度提升8.8倍，内存使用减少10倍，存储节省42%，性能影响极小。

Conclusion: dreaMLearning为资源受限场景下的高效学习提供了新可能。

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [62] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: EFRame框架通过探索、过滤和回放机制增强GRPO，提升强化学习在复杂推理任务中的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: GRPO在强化学习中存在探索不足、样本效率低和不稳定的问题，限制了其在复杂推理任务中的表现。

Method: EFRame通过额外探索高质量轨迹、在线过滤低质量样本和利用经验回放，构建完整稳定的学习循环。

Result: 实验表明EFRame提高了训练效率和鲁棒性，并解锁了GRPO无法实现的更深层次推理能力。

Conclusion: EFRame不仅优化了GRPO的性能，还提供了对训练样本更细粒度的分析，推动了强化学习的发展。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [63] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: 论文提出了一种新的随机多臂老虎机优化问题设置，旨在同时最大化预期奖励和最小化风险（通过均值-方差准则衡量），并设计了一个统一的元算法框架，适用于固定置信度和固定预算两种场景。


<details>
  <summary>Details</summary>
Motivation: 传统老虎机问题仅关注预期回报，而忽略了风险。本文旨在解决这一局限，通过联合优化预期奖励和风险，为不确定环境下的决策提供更全面的解决方案。

Method: 提出了一种统一的元算法框架，通过自适应设计的置信区间和相同的样本探索策略，适用于固定置信度和固定预算两种场景。

Result: 理论分析证明了算法在两种场景下的正确性，实验结果表明该方法在准确性和样本效率上优于现有方法。

Conclusion: 该方法为风险感知决策任务提供了一种高效且通用的解决方案，适用于不确定环境中的广泛应用。

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [64] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: 论文提出了一种名为“投影压缩”的新模型压缩技术，通过投影模块减少模型权重，同时保持原始模型参数，最终生成一个更小的标准Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模的增大，推理时间和计算需求也随之增加，因此需要研究模型压缩方法以减少这些开销。

Method: 训练额外的可训练投影权重，保留原始模型参数，并将这些投影合并为低维乘积矩阵，从而减少模型大小。

Result: 实验表明，投影压缩在高质量模型上优于硬剪枝和再训练方法，且性能随token数量增加而提升。

Conclusion: 投影压缩是一种有效的模型压缩方法，能够在不增加额外计算开销的情况下减少模型大小并保持性能。

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [65] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了一种基于分数匹配的模型，用于低秩张量分解，无需预定义结构或分布假设，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法依赖预定义的结构假设（如CP或Tucker分解），但在实际场景中，这些假设往往不适用，且优化过程复杂，导致精度损失。

Method: 设计了一个神经网络来学习能量函数，通过分数匹配优化，捕捉张量条目和共享因子的联合对数概率梯度，并结合块坐标下降算法（BCD）和平滑正则化。

Result: 实验结果表明，该方法在稀疏张量、连续时间张量和视觉数据等多种张量类型上均表现出显著性能提升。

Conclusion: 该方法通过消除预定义假设，实现了更灵活的张量分解，适用于更广泛的实际应用场景。

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [66] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: CoATA是一个双通道GNN框架，通过联合增强拓扑和属性，解决了现实图中噪声和不完整性问题，并通过对比学习优化性能。


<details>
  <summary>Details</summary>
Motivation: 现实图中的噪声和不完整性严重影响了GNN的性能，现有方法仅关注单一维度的增强，忽略了拓扑和属性之间的深层交互。

Method: CoATA通过双通道设计，先传播结构信号增强节点属性，再通过节点-属性二分图优化结构，并引入对比学习进行相互校正。

Result: 在七个基准数据集上的实验表明，CoATA优于十一种现有方法，有效捕捉了拓扑与属性的协同关系。

Conclusion: CoATA通过联合增强拓扑和属性，显著提升了GNN在噪声和不完整图中的性能。

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [67] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出了一种弱监督领域自适应方法，利用目标域的类别比例信息，通过比例约束伪标记提升性能，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 解决医学领域中因数据分布差异导致的模型性能下降问题，特别是当源域和目标域类别比例不同时。

Method: 基于目标域类别比例信息，对未标记目标数据分配伪标签（比例约束伪标记）。

Result: 在两个内窥镜数据集上表现优于半监督领域自适应方法，即使目标域仅有5%标记数据；对噪声比例标签具有鲁棒性。

Conclusion: 该方法在现实应用场景中有效且鲁棒。

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [68] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: Koopman-enhanced Conditional Flow Matching (CFM) 通过结合Koopman算子理论，加速CFM采样并提供生成过程的可解释性，实现高效且结构化的生成建模。


<details>
  <summary>Details</summary>
Motivation: 传统CFM采样依赖非线性ODE数值解，计算昂贵且难以解释；现有方法虽提升速度但未能揭示生成过程的结构。

Method: 提出解码器自由的Koopman-CFM架构，学习线性动态嵌入空间，通过矩阵指数实现一步闭式采样。

Result: 在2D数据集和MNIST等基准上显著加速采样，Koopman生成器的谱特性提供分析工具（如时间缩放、模式稳定性）。

Conclusion: Koopman增强的CFM结合采样效率与解析结构，为快速可解释生成建模迈出重要一步。

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [69] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: LGES是GES的改进版本，通过更针对性的搜索策略，提高了计算速度和准确性，同时支持先验假设和数据修正。


<details>
  <summary>Details</summary>
Motivation: 解决GES算法在计算成本和有限样本准确性方面的不足。

Method: 修改贪婪步骤，避免在条件独立变量间插入边，并支持先验假设和数据修正。

Result: LGES在速度和准确性上优于GES，且能处理错误先验假设。

Conclusion: LGES在理论和实验中均表现出色，适用于观测和干预数据。

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [70] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度学习和流行病模型的框架，用于流行病预测和传播机制建模，同时整合了多种数据集（包括差分隐私保护的数据），并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多样化的数据集在流行病学和公共卫生分析中具有重要价值，但部分数据敏感，需要隐私保护。差分隐私（DP）因其强保障成为标准。本文旨在开发一个框架，整合深度学习与流行病模型，同时利用多数据集（包括DP保护的数据）进行分析。

Method: 开发了一个结合深度学习和流行病模型的框架，用于流行病预测和传播机制建模，整合了多种数据集（包括差分隐私保护的数据）。

Result: 通过合成金融数据集（带DP保护）验证了框架的有效性，证明该数据集在预测和建模中具有显著价值。

Conclusion: 该框架成功整合了深度学习和流行病模型，利用多数据集（包括DP保护的数据）进行流行病分析，为敏感数据的使用提供了可行方案。

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [71] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: 论文提出了一种符号化方法PiPRL，将物理先验知识融入强化学习，通过分层模块化设计提升样本效率和泛化能力，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前将物理先验知识融入强化学习的方法需要大量人工和领域知识，限制了其通用性。

Method: 提出PiPRL框架，结合符号化编程和神经网络，通过分层模块化设计将物理先验知识转化为可执行的策略。

Result: PiPRL在室内导航任务中表现优于纯符号或神经策略，训练时间减少26%。

Conclusion: PiPRL通过符号化方法有效整合物理先验知识，提升了强化学习的效率和性能。

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [72] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出Sheaf-DMFL框架，利用层理论解决多模态数据在联邦学习中的协作问题，并通过Sheaf-DMFL-Att增强学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习算法仅适用于单模态数据，无法充分利用多模态信息，限制了在现实场景中的应用。

Method: 提出Sheaf-DMFL框架，通过层理论建模客户端任务层间的相关性；Sheaf-DMFL-Att引入注意力机制捕捉多模态相关性。

Result: 在链路阻塞预测和毫米波波束成形等场景中验证了算法的优越性。

Conclusion: Sheaf-DMFL框架在多模态异构通信系统中表现出色，具有理论和实际应用价值。

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [73] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出了一个概率框架，用于形式化推理时扩展的最优性，并开发了动态确定最优样本数的算法OptScale，显著减少了采样开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式并行采样策略，缺乏理论基础，无法高效部署大型语言模型进行复杂推理。

Method: 提出概率框架，假设并行样本独立同分布，推导理论下限，开发动态确定样本数的算法OptScale。

Result: 在数学推理基准测试中，OptScale显著减少采样开销，性能优于或与现有方法相当。

Conclusion: 为推理时扩展提供了理论基础和实用解决方案，填补了高效部署大型语言模型的空白。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [74] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: 论文提出了一种分布式神经架构（DNA），通过动态路由和模块化设计，在视觉和语言任务中实现高效计算和参数共享。


<details>
  <summary>Details</summary>
Motivation: 旨在通过动态路由和模块化设计，提升模型的灵活性和效率，同时探索计算和参数分配的优化。

Method: 使用包含多种模块（如Transformer、MLP、注意力等）的原型架构，通过端到端训练学习动态路由和计算模式。

Result: DNA在性能上与密集基线相当，并能学习计算效率和参数共享；路径分布符合幂律，模块表现出专业化。

Conclusion: DNA展示了动态计算和参数分配的潜力，为高效模型设计提供了新思路。

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [75] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: 提出一种基于多视图对比学习的新框架，用于医学时间序列数据的跨领域适应，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列数据具有复杂的时序依赖性和动态分布变化，现有方法难以充分捕捉这些特性。

Method: 采用多视图对比学习，整合时序模式、导数动态和频域特征，通过独立编码器和分层融合机制学习跨领域可迁移的特征不变表示。

Result: 在EEG、ECG和EMG等多样医学数据集上，性能显著优于现有方法。

Conclusion: 该框架提升了模型的鲁棒性和泛化能力，为医疗AI系统的实际部署提供了可行方案。

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [76] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文提出了一种新的价值激励演员-评论家（VAC）方法，通过原始-对偶优化视角解释乐观原则，整合探索与开发，并提供了理论性能保证。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习中平衡探索与开发是一个长期挑战，缺乏高效且理论支持的方法。

Method: 基于乐观正则化的最新发展，提出VAC方法，通过单一目标优化整合探索与开发。

Result: VAC方法在有限和无限时间范围的线性MDP中具有接近最优的遗憾保证，并可推广到一般函数逼近。

Conclusion: VAC方法为探索与开发的平衡提供了理论和实践上的有效解决方案。

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [77] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: ARMOR是一种针对无人机传感器攻击的鲁棒强化学习控制器，通过两阶段训练框架学习鲁棒状态表示，确保无人机安全。


<details>
  <summary>Details</summary>
Motivation: 无人机传感器易受物理攻击（如GPS欺骗），现有安全强化学习方法对此无效，需开发更鲁棒的控制器。

Method: ARMOR采用两阶段训练：教师编码器生成攻击感知的潜在状态，学生编码器通过监督学习近似教师状态，仅使用历史传感器数据。

Result: 实验表明ARMOR优于传统方法，确保无人机安全，提高对未知攻击的泛化能力，并降低训练成本。

Conclusion: ARMOR为无人机在对抗性传感器攻击下提供了鲁棒且高效的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [78] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: CLoVE是一种用于聚类联邦学习（CFL）的新算法，通过客户损失向量嵌入识别客户群，无需初始化最优模型，适用于监督和无监督任务。


<details>
  <summary>Details</summary>
Motivation: 在CFL中，客户基于数据分布自然分组，但识别这些群组具有挑战性。CLoVE旨在通过损失模式差异准确区分客户群。

Method: CLoVE利用客户数据上的模型损失生成嵌入，通过迭代分离不同群组的客户，并优化群组特定模型。

Result: 理论证明CLoVE能高概率准确恢复群组，并在线性设置中指数级快速收敛。实验显示其在多种非独立同分布设置下表现优异。

Conclusion: CLoVE在群组恢复和模型准确性上优于现有CFL和PFL算法，适用于实际应用。

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [79] [Ark: An Open-source Python-based Framework for Robot Learning](https://arxiv.org/abs/2506.21628)
*Magnus Dierking,Christopher E. Mower,Sarthak Das,Huang Helong,Jiacheng Qiu,Cody Reading,Wei Chen,Huidong Liang,Huang Guowei,Jan Peters,Quan Xingyue,Jun Wang,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: ARK是一个开源的、以Python为中心的机器人框架，旨在简化机器人软件开发，提供类似Gym的接口，支持仿真与物理机器人切换，并集成ROS。


<details>
  <summary>Details</summary>
Motivation: 当前机器人软件栈学习曲线陡峭、工具分散且硬件集成复杂，与AI领域的Python生态系统形成鲜明对比，ARK旨在填补这一差距。

Method: ARK提供Gym风格的环境接口，支持数据收集、预处理和策略训练，采用轻量级客户端-服务器架构，并附带可重用模块和ROS互操作性。

Result: ARK实现了快速原型设计、硬件无缝切换和端到端流程，降低了机器人开发的入门门槛。

Conclusion: ARK通过统一Python生态系统下的机器人和AI实践，加速了自主机器人的研究和商业部署。

Abstract: Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.

</details>


### [80] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.RO

TL;DR: 论文提出了一种针对非结构化户外环境中可通行路径检测的解决方案，包括新数据集TOMD和多模态数据融合模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和模型主要针对城市或宽阔的越野场景，无法满足狭窄小径类越野环境的需求。

Method: 引入TOMD数据集，并提出动态多尺度数据融合模型，分析不同融合策略在不同光照条件下的性能。

Result: 结果表明该方法有效，且光照对分割性能有显著影响。

Conclusion: TOMD数据集和提出的模型为未来研究提供了支持。

Abstract: Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.

</details>


### [81] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Main category: cs.RO

TL;DR: 提出了一种新的结构化学习方法，用于视觉导航，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决滑移转向车辆在动态操作中缺乏准确分析模型的问题。

Method: 采用端到端学习方法（如模仿学习和深度强化学习），并进行软件模拟、硬件评估和消融研究。

Result: 在当代文献中表现出显著改进的性能。

Conclusion: 该方法为视觉导航提供了一种有效的结构化解决方案。

Abstract: Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.

</details>


### [82] [ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research](https://arxiv.org/abs/2506.22174)
*Bavo Lesy,Siemen Herremans,Robin Kerstens,Jan Steckel,Walter Daems,Siegfried Mercelis,Ali Anwar*

Main category: cs.RO

TL;DR: ASVSim是一个开源仿真框架，专为内河和港口环境中的自主航运研究设计，结合了船舶动力学和海洋传感器模拟能力。


<details>
  <summary>Details</summary>
Motivation: 解决无人水面车辆（USVs）研究中缺乏开源、高保真仿真框架和数据集的问题，以支持自主航运的发展。

Method: 基于Cosys-AirSim开发ASVSim，结合船舶动力学和传感器模拟（如雷达和摄像头），支持生成合成数据集。

Result: ASVSim为自主导航算法开发和合成数据集生成提供了全面平台，支持传统控制方法和深度学习研究。

Conclusion: ASVSim作为开源项目，有助于推动海洋工程领域的自主导航研究。

Abstract: The transport industry has recently shown significant interest in unmanned
surface vehicles (USVs), specifically for port and inland waterway transport.
These systems can improve operational efficiency and safety, which is
especially relevant in the European Union, where initiatives such as the Green
Deal are driving a shift towards increased use of inland waterways. At the same
time, a shortage of qualified personnel is accelerating the adoption of
autonomous solutions. However, there is a notable lack of open-source,
high-fidelity simulation frameworks and datasets for developing and evaluating
such solutions. To address these challenges, we introduce AirSim For Surface
Vehicles (ASVSim), an open-source simulation framework specifically designed
for autonomous shipping research in inland and port environments. The framework
combines simulated vessel dynamics with marine sensor simulation capabilities,
including radar and camera systems and supports the generation of synthetic
datasets for training computer vision models and reinforcement learning agents.
Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for
developing autonomous navigation algorithms and generating synthetic datasets.
The simulator supports research of both traditional control methods and deep
learning-based approaches. Through limited experiments, we demonstrate the
potential of the simulator in these research areas. ASVSim is provided as an
open-source project under the MIT license, making autonomous navigation
research accessible to a larger part of the ocean engineering community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: HRM是一种新型递归架构，通过分层处理实现高效推理，无需大量数据或显式监督，性能优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLMs在推理任务中的脆弱性、高数据需求和延迟问题，受人类大脑分层处理启发。

Method: 提出HRM，包含高低级递归模块，分别处理抽象规划和详细计算，单次前向传播完成推理。

Result: 仅用27M参数和1000样本，HRM在复杂推理任务（如数独、迷宫路径）和ARC基准上表现优异。

Conclusion: HRM为通用计算和推理系统提供了潜在突破性进展。

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [84] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: Active-MoSH是一个交互式框架，用于高风险决策中多目标优化问题，结合软硬边界和偏好学习，提升决策者信任和效率。


<details>
  <summary>Details</summary>
Motivation: 解决高后果决策中多目标优化的挑战，如资源密集评估和决策者信任问题。

Method: 结合局部（软硬边界与偏好学习）和全局（多目标敏感性分析）组件，通过主动采样优化探索与利用。

Result: 在合成和实际应用中表现优异，用户研究验证了其在收敛性、信任和偏好表达上的优势。

Conclusion: Active-MoSH有效支持决策者，提升决策质量和信任度。

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [85] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: 论文探讨了知识图谱补全（KGC）模型中的秩瓶颈问题，并提出了一种基于混合的输出层（KGE-MoS）来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有KGC模型在实体数量远大于嵌入维度时，由于线性输出层的秩瓶颈限制了模型表达能力，影响了排名准确性和分数分布保真度。

Method: 提出KGE-MoS，一种基于混合的输出层，以打破秩瓶颈。

Result: 在四个数据集上的实验表明，KGE-MoS以较低参数成本提升了KGC模型的性能和概率拟合度。

Conclusion: KGE-MoS有效解决了秩瓶颈问题，提升了KGC模型的表达能力和性能。

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [86] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: 论文提出了一种基于形式概念分析（FCA）的方法FAT-CAT，用于改进主题建模的可解释性和可视化效果。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模方法难以提供可解释的表示，限制了数据结构和内容的深入理解。

Method: 采用形式概念分析（FCA）构建概念格，实现主题的层次化表示和可视化。

Result: 在ETYNTKE数据集上的实验表明，FAT-CAT比现有方法提供更具意义和可解释的主题表示。

Conclusion: FAT-CAT通过FCA提升了主题建模的可解释性，为数据集分析提供了更深入的见解。

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [87] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 论文介绍了自动化LLM速度运行基准测试，评估AI代理在科学重现中的能力，发现前沿LLM即使有详细提示也难以重现已知创新。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在科学研究中重现结果的能力，以推动科学进步。

Method: 利用NanoGPT速度运行竞赛的数据，设计19个任务，提供不同格式的提示，测试LLM重现代码改进的能力。

Result: 前沿LLM即使结合先进框架，也难以重现已知创新。

Conclusion: 该基准测试为衡量LLM自动化科学重现能力提供了简单且未饱和的标准，是自主研究代理的必要技能。

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [88] [PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications](https://arxiv.org/abs/2506.21593)
*Abu Hanif Muhammad Syarubany,Chang Dong Yoo*

Main category: cs.IR

TL;DR: PentaRAG是一个五层模块，通过缓存、记忆召回和传统检索增强层优化LLM的查询处理，显著提升速度、效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 企业部署LLM需要低延迟、可预测的GPU成本，传统RAG无法完全满足需求。

Method: PentaRAG采用五层路由策略，包括固定键值缓存、语义缓存、记忆召回模式、自适应会话内存和传统检索增强层。

Result: 在TriviaQA领域，PentaRAG提升答案相似度8%和事实正确性16%，平均GPU时间减少50%，延迟降至亚秒级。

Conclusion: 分层路由策略可在生产级RAG系统中同时实现新鲜度、速度和效率。

Abstract: Enterprise deployments of large-language model (LLM) demand continuously
changing document collections with sub-second latency and predictable GPU cost
requirements that classical Retrieval-Augmented Generation (RAG) pipelines only
partially satisfy. We present PentaRAG, a five-layer module that routes each
query through two instant caches (fixed key-value and semantic), a
memory-recall mode that exploits the LLM's own weights, an adaptive session
memory, and a conventional retrieval-augmentation layer. Implemented with
Mistral-8B, Milvus and vLLM, the system can answer most repeated or
semantically similar questions from low-latency caches while retaining full
retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined
with the memory-recall layer raises answer similarity by approximately 8% and
factual correctness by approximately 16% over the base model. Under a
nine-session runtime simulation, cache warming reduces mean latency from
several seconds to well below one second and shifts traffic toward the fast
paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to
0.248 seconds per query, roughly half that of a naive RAG baseline, and
sustains an aggregate throughput of approximately 100,000 queries per second on
our setup. These results demonstrate that a layered routing strategy can
deliver freshness, speed, and efficiency simultaneously in production-grade RAG
systems.

</details>


### [89] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.IR

TL;DR: 论文指出评估基准的特性可能扭曲检索模型中领域适应的真实效益，导致误导性评估。通过环境监管文件检索的案例研究，发现不同语义结构的基准对领域适应效果的评价差异显著。


<details>
  <summary>Details</summary>
Motivation: 揭示评估基准的选择如何影响领域适应效果的感知，特别是在专业领域中，避免因基准特性导致的误导性部署决策。

Method: 使用ColBERTv2模型在环境监管文件（EIS）上进行微调，并在两个语义结构不同的基准上评估性能。通过主题多样性指标比较基准差异。

Result: 在主题边界清晰的基准上，领域适应仅带来0.61%的NDCG增益；而在语义重叠的基准上，增益高达2.22%，差异达3.6倍。高增益基准的主题多样性更高。

Conclusion: 评估基准的选择显著影响检索系统在专业领域中的效果评估。语义重叠的基准更能反映真实世界的复杂性，对领域适应的评估更准确。

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [90] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Main category: cs.IR

TL;DR: 论文提出Refine-POI框架，通过强化微调解决LLM在POI推荐中SFT方法的局限性，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的POI推荐方法中，SFT方法因数据与任务不匹配而受限，需要改进。

Method: 提出Refine-POI框架，结合强化学习和推荐驱动奖励，利用单一真实POI生成top-k推荐列表。

Result: 实验表明Refine-POI在真实数据集上达到最优的top-k推荐性能。

Conclusion: Refine-POI通过强化微调有效解决了SFT方法的局限性，提升了推荐效果。

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [91] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 提出了一种系统化、量化的基准框架，用于评估多模态生成AI的可信度，特别是在企业文档智能中的VisualRAG系统。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态生成AI评估框架难以建立可信度，阻碍了企业采用，而可靠性对企业至关重要。

Method: 通过量化技术指标与用户信任度之间的关系，提出了一种多模态输入（文本、图像、标题、OCR）的加权方法。

Result: 最优模态权重（30%文本、15%图像、25%标题、30%OCR）比纯文本基线性能提升57.3%，同时保持计算效率。

Conclusion: 该研究为企业AI的可信度提供了严格的量化框架，推动了负责任AI的部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [92] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Main category: cs.IR

TL;DR: DCN^2是对DCNv2的改进版本，通过三种算法优化解决了DCNv2的关键限制，并在实际推荐系统中表现出色。


<details>
  <summary>Details</summary>
Motivation: DCNv2虽然高效且广泛使用，但仍存在信息丢失和碰撞管理等问题，需要进一步优化。

Method: 引入三种算法改进：解决Cross层信息丢失、可学习查找级权重管理碰撞、自定义层模拟FFM行为。

Result: DCN^2在离线、在线测试及公开数据集上均优于DCNv2，处理能力达每秒5亿预测。

Conclusion: DCN^2显著提升了推荐系统的性能和效率，成为更优的解决方案。

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [93] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Main category: cs.IR

TL;DR: 论文提出了一种统一的排序基础模型IRanker，通过强化学习和迭代解码解决排序任务缺乏明确监督标签的问题，并在多个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 排序任务广泛存在，但缺乏统一的模型，且监督信号不明确，因此需要开发一种通用的排序基础模型。

Method: 提出IRanker框架，利用强化学习和迭代解码逐步剔除候选池中最差选项，减少输出组合空间。

Result: IRanker-3B在多个数据集上表现优异，甚至超越更大模型，同时在零样本任务中也有显著提升。

Conclusion: IRanker通过强化学习和迭代解码有效解决了排序任务中的挑战，展现了良好的泛化能力和性能提升。

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [94] [Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling](https://arxiv.org/abs/2506.21946)
*Till Wenke*

Main category: cs.CY

TL;DR: 本文分析了最大的搭便车数据集（63,000+条目），揭示了其时空特征、季节性模式及用户行为，同时指出了数据集的局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 搭便车作为一种非正式的出行方式，缺乏系统性研究。本文旨在通过分析大规模数据集，填补这一空白。

Method: 利用hitchwiki.org和hitchmap.com平台收集的众包数据，进行时空和策略分析。

Result: 数据集揭示了欧洲中心的分布、季节性模式及活跃贡献者的作用，同时发现了等待时间和用户行为的特点。

Conclusion: 尽管数据集存在偏差，但仍为研究搭便车提供了宝贵资源，未来可进一步丰富数据并深化研究。

Abstract: Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded
systematic study due to its informal nature. This paper presents and analyzes
the largest known structured dataset of hitchhiking rides, comprising over
63,000 entries collected over nearly two decades through platforms associated
with hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced
contributions, the dataset captures key spatiotemporal and strategic aspects of
hitchhiking. This work documents the dataset's origins, evolution, and
community-driven maintenance, highlighting its Europe-centric distribution,
seasonal patterns, and reliance on a small number of highly active
contributors. Through exploratory analyses, I examine waiting times, user
behavior, and comment metadata, shedding light on the lived realities of
hitchhikers. While the dataset has inherent biases and limitations - such as
demographic skew and unverifiable entries it offers a rare and valuable window
into an alternative form of mobility. I conclude by outlining future directions
for enriching the dataset and advancing research on hitchhiking as both a
transportation practice and cultural phenomenon.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [95] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: 论文提出了一种基于扩散模型的超表面逆向设计方法，解决了传统方法计算量大、易陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 超表面的逆向设计因其复杂的非线性结构-光学关系而具有挑战性，传统方法需要专家调参且计算成本高。

Method: 通过RCWA模拟器生成训练数据，训练条件扩散模型以根据目标空间功率分布预测超表面几何结构。

Result: 模型能快速生成低误差超表面，如均匀强度分束器和偏振分束器，设计时间少于30分钟。

Conclusion: 扩散模型为超表面设计提供了高效的数据驱动方法，代码和数据集已公开以支持进一步研究。

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered
sub-wavelength structures that enable precise control of light. Their inverse
design - determining a geometry that yields a desired optical response - is
challenging due to the complex, nonlinear relationship between structure and
optical properties. This often requires expert tuning, is prone to local
minima, and involves significant computational overhead. In this work, we
address these challenges by integrating the generative capabilities of
diffusion models into computational design workflows. Using an RCWA simulator,
we generate training data consisting of metasurface geometries and their
corresponding far-field scattering patterns. We then train a conditional
diffusion model to predict meta-atom geometry and height from a target spatial
power distribution at a specified wavelength, sampled from a continuous
supported band. Once trained, the model can generate metasurfaces with low
error, either directly using RCWA-guided posterior sampling or by serving as an
initializer for traditional optimization methods. We demonstrate our approach
on the design of a spatially uniform intensity splitter and a polarization beam
splitter, both produced with low error in under 30 minutes. To support further
research in data-driven metasurface design, we publicly release our code and
datasets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [96] [Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19](https://arxiv.org/abs/2506.21739)
*Felipe Rogério Pimentel,Rafael Gustavo Alves*

Main category: stat.ML

TL;DR: 论文提出了一种改进的FIR滤波器算法，用于预测COVID-19感染和康复人数，并在巴西米纳斯吉拉斯州的数据中验证了其优于原算法的表现。


<details>
  <summary>Details</summary>
Motivation: 在疫苗尚未普及的疫情初期，通过改进算法提高对感染和康复人数的预测准确性。

Method: 采用FIR线性系统滤波方法，结合时间依赖离散SIR模型，通过修改Chen等人的算法（调整滤波器阶数和正则化参数）来优化预测。

Result: 改进算法在某些模拟中表现出比原算法更低的近似误差。

Conclusion: 改进的算法在特定场景下能更准确地预测疫情数据，为疫情防控提供了更有效的工具。

Abstract: Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use
the Finite Impulse Response (FIR) linear system filtering method to track and
predict the number of people infected and recovered from COVID-19, in a
pandemic context in which there was still no vaccine and the only way to avoid
contagion was isolation. To estimate the coefficients of these FIR filters,
Chen et al. used machine learning methods through a classical optimization
problem with regularization (ridge regression). These estimated coefficients
are called ridge coefficients. The epidemic mathematical model adopted by these
researchers to formulate the FIR filters is the time-dependent discrete SIR. In
this paper, we propose a small modification to the algorithm of Chen et al. to
obtain the ridge coefficients. We then used this modified algorithm to track
and predict the number of people infected and recovered from COVID-19 in the
state of Minas Gerais/Brazil, within a prediction window, during the initial
period of the pandemic. We also compare the predicted data with the respective
real data to check how good the approximation is. In the modified algorithm, we
set values for the FIR filter orders and for the regularization parameters,
both different from the respective values defined by Chen et al. in their
algorithm. In this context, the numerical results obtained by the modified
algorithm in some simulations present better approximation errors compared to
the respective approximation errors presented by the algorithm of Chen et al.

</details>


### [97] [Critically-Damped Higher-Order Langevin Dynamics](https://arxiv.org/abs/2506.21741)
*Benjamin Sterling,Chad Gueli,Mónica F. Bugallo*

Main category: stat.ML

TL;DR: 论文提出了一种将临界阻尼引入高阶Langevin动力学（HOLD）的新方法，扩展了生成AI的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 探索临界阻尼在任意阶动力学中的应用，以改进现有的扩散方法。

Method: 将临界阻尼概念从系统分析引入高阶Langevin动力学（HOLD）。

Result: 提出了一种新的扩散方法，扩展了HOLD的能力。

Conclusion: 临界阻尼的应用为高阶扩散模型提供了新的研究方向。

Abstract: Denoising Diffusion Probabilistic Models represent an entirely new class of
generative AI methods that have yet to be fully explored. Critical damping has
been successfully introduced in Critically-Damped Langevin Dynamics (CLD) and
Critically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been
applied to dynamics of arbitrary order. The proposed line of work generalizes
Higher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion
method, by introducing the concept of critical damping from systems analysis.

</details>


### [98] [TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics](https://arxiv.org/abs/2506.21757)
*Tianrong Chen,Huangjie Zheng,David Berthelot,Jiatao Gu,Josh Susskind,Shuangfei Zhai*

Main category: stat.ML

TL;DR: 本文提出了一种新的采样方法，比现有最快采样方法快186%，且无需额外训练，通过高维初始噪声和ODE求解器实现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高保真图像效率低，现有方法在采样速度上仍有提升空间。

Method: 使用高维初始噪声和ODE求解器，通过动量动力学控制细节水平。

Result: 在ImageNet512上，采样速度提升186%，且适用于多种预训练扩散模型。

Conclusion: 新方法显著提升采样效率，适用于多种扩散模型，且能灵活控制细节。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images but typically suffer from inefficient sampling. Many
solver designs and noise scheduling strategies have been proposed to
dramatically improve sampling speeds. In this paper, we introduce a new
sampling method that is up to $186\%$ faster than the current state of the art
solver for comparative FID on ImageNet512. This new sampling method is
training-free and uses an ordinary differential equation (ODE) solver. The key
to our method resides in using higher-dimensional initial noise, allowing to
produce more detailed samples with less function evaluations from existing
pretrained diffusion models. In addition, by design our solver allows to
control the level of detail through a simple hyper-parameter at no extra
computational cost. We present how our approach leverages momentum dynamics by
establishing a fundamental equivalence between momentum diffusion models and
conventional diffusion models with respect to their training paradigms.
Moreover, we observe the use of higher-dimensional noise naturally exhibits
characteristics similar to stochastic differential equations (SDEs). Finally,
we demonstrate strong performances on a set of representative pretrained
diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover
models in both pixel and latent spaces, as well as class and text conditional
settings. The code is available at https://github.com/apple/ml-tada.

</details>


### [99] [Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction](https://arxiv.org/abs/2506.21802)
*Johan Hallberg Szabadváry,Tuwe Löfström,Ulf Johansson,Cecilia Sönströd,Ernst Ahlberg,Lars Carlsson*

Main category: stat.ML

TL;DR: 论文提出了一种基于共形预测（CP）的二元分类拒绝选项方法，通过理论保证和数值示例展示了错误率与拒绝率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在预测不可靠时仍强制输出结果的问题，提供一种可信任的拒绝选项机制。

Method: 利用共形预测框架生成预测集，通过仅接受单例预测将其转化为带拒绝选项的二元分类器。

Result: 理论证明了错误率，并通过数值示例展示了不同CP设置下的错误率与拒绝率关系。

Conclusion: 共形预测为二元分类提供了一种有效的拒绝选项方法，错误-拒绝曲线帮助用户在实际中设置可接受的参数。

Abstract: Machine learning (ML) models always make a prediction, even when they are
likely to be wrong. This causes problems in practical applications, as we do
not know if we should trust a prediction. ML with reject option addresses this
issue by abstaining from making a prediction if it is likely to be incorrect.
In this work, we formalise the approach to ML with reject option in binary
classification, deriving theoretical guarantees on the resulting error rate.
This is achieved through conformal prediction (CP), which produce prediction
sets with distribution-free validity guarantees. In binary classification, CP
can output prediction sets containing exactly one, two or no labels. By
accepting only the singleton predictions, we turn CP into a binary classifier
with reject option.
  Here, CP is formally put in the framework of predicting with reject option.
We state and prove the resulting error rate, and give finite sample estimates.
Numerical examples provide illustrations of derived error rate through several
different conformal prediction settings, ranging from full conformal prediction
to offline batch inductive conformal prediction. The former has a direct link
to sharp validity guarantees, whereas the latter is more fuzzy in terms of
validity guarantees but can be used in practice. Error-reject curves illustrate
the trade-off between error rate and reject rate, and can serve to aid a user
to set an acceptable error rate or reject rate in practice.

</details>


### [100] [Thompson Sampling in Function Spaces via Neural Operators](https://arxiv.org/abs/2506.21894)
*Rafael Oliveira,Xuesong Wang,Kian Ming A. Chai,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出了一种基于Thompson采样的函数空间优化方法，利用神经算子替代高成本操作，无需显式不确定性量化，并提供了理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 解决函数空间中优化问题，其中目标函数是未知算子输出的已知泛函，且算子查询成本高。

Method: 采用样本后优化策略，使用神经算子替代高斯过程样本，避免显式不确定性量化。

Result: 在偏微分方程等非线性算子驱动任务中，表现出更高的样本效率和竞争力。

Conclusion: 该方法在函数优化任务中具有高效性和理论保证，优于现有基线。

Abstract: We propose an extension of Thompson sampling to optimization problems over
function spaces where the objective is a known functional of an unknown
operator's output. We assume that functional evaluations are inexpensive, while
queries to the operator (such as running a high-fidelity simulator) are costly.
Our algorithm employs a sample-then-optimize approach using neural operator
surrogates. This strategy avoids explicit uncertainty quantification by
treating trained neural operators as approximate samples from a Gaussian
process. We provide novel theoretical convergence guarantees, based on Gaussian
processes in the infinite-dimensional setting, under minimal assumptions. We
benchmark our method against existing baselines on functional optimization
tasks involving partial differential equations and other nonlinear
operator-driven phenomena, demonstrating improved sample efficiency and
competitive performance.

</details>


### [101] [Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport](https://arxiv.org/abs/2506.22204)
*Gurjeet Sangra Singh,Maciej Falkiewicz,Alexandros Kalousis*

Main category: stat.ML

TL;DR: 论文提出了一种结合深度灰箱建模与最优传输（OT）方法的混合生成模型，用于补全不完整物理模型，解决真实数据与模型模拟之间的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统常因物理模型不完整而与真实数据生成过程（DGP）存在分布差异，需结合理论驱动和数据驱动方法解决这一问题。

Method: 采用深度灰箱建模与OT方法结合的混合生成模型，在数据空间实现OT映射，同时最小化源分布失真。

Result: 实验表明，该方法在生成任务和模型透明度上表现优异，能准确学习系统动力学并保持可解释性。

Conclusion: 该方法有效解决了未配对数据问题，同时利用物理先验知识提升了模型性能和可解释性。

Abstract: Physics phenomena are often described by ordinary and/or partial differential
equations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,
many real-world systems are described only approximately with missing or
unknown terms in the equations. This makes the distribution of the physics
model differ from the true data-generating process (DGP). Using limited and
unpaired data between DGP observations and the imperfect model simulations, we
investigate this particular setting by completing the known-physics model,
combining theory-driven models and data-driven to describe the shifted
distribution involved in the DGP. We present a novel hybrid generative model
approach combining deep grey-box modelling with Optimal Transport (OT) methods
to enhance incomplete physics models. Our method implements OT maps in data
space while maintaining minimal source distribution distortion, demonstrating
superior performance in resolving the unpaired problem and ensuring correct
usage of physics parameters. Unlike black-box alternatives, our approach
leverages physics-based inductive biases to accurately learn system dynamics
while preserving interpretability through its domain knowledge foundation.
Experimental results validate our method's effectiveness in both generation
tasks and model transparency, offering detailed insights into learned physics
dynamics.

</details>


### [102] [Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings](https://arxiv.org/abs/2506.22228)
*Rong Ma,Xi Li,Jingyuan Hu,Bin Yu*

Main category: stat.ML

TL;DR: 论文提出了一种名为NESS的新方法，用于改进单细胞数据的低维嵌入表示，解决了现有方法在捕捉连续细胞状态转换时的失真问题。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序技术虽能详细研究细胞状态转换，但现有邻域嵌入算法（如t-SNE和UMAP）常引入失真，且评估方法多关注离散细胞类型而非连续转换。

Method: 基于PCS框架，通过实证分析、模拟和理论评估现有NE算法，并提出NESS方法，利用算法稳定性改进表示并推断平滑生物结构。

Result: NESS在六种单细胞数据集中一致提供了有用的生物学见解，如识别过渡和稳定细胞状态及量化发育中的转录动态。

Conclusion: NESS是一种可靠且可解释的方法，能有效揭示单细胞数据中的连续生物结构。

Abstract: Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.

</details>


### [103] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li,Garrett Wen,Weiqing He,Jiayuan Wu,Qi Long,Weijie J. Su*

Main category: stat.ML

TL;DR: 该论文研究了混合来源文本中水印比例的最优估计问题，提出了基于关键统计量的高效估计方法，并在理论和实验上验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中常存在混合来源文本（人类书写与LLM生成内容混合），现有研究多关注整篇文本的水印检测，而缺乏对混合文本中水印比例的估计方法。

Method: 将问题建模为基于关键统计量的混合模型比例参数估计，针对连续关键统计量的水印方法提出高效估计器，并推导极小极大下界。

Result: 实验表明，所提估计器在合成数据和开源模型生成的混合文本中均能实现高精度估计。

Conclusion: 对于使用连续关键统计量的水印方法，水印比例参数是可识别的，且所提估计器在理论和实践中均表现优异。

Abstract: Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


### [104] [Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks](https://arxiv.org/abs/2506.22429)
*David Holzmüller,Max Schölpple*

Main category: stat.ML

TL;DR: 论文分析了典型激活函数（如SELU、ELU、LeakyReLU）在神经正切核（NTK）和神经网络高斯过程核（NNGP）中的RKHS特性，并探讨了不同网络深度和多项式激活函数的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习理论主要局限于ReLU激活函数，对其他激活函数的理解不足。

Method: 通过分析典型激活函数的RKHS特性，研究其在NTK和NNGP中的表现，并扩展到特殊情况（如无偏置、两层网络、多项式激活）。

Result: 发现一类非无限平滑的激活函数在不同网络深度下生成等效的RKHS，而多项式激活函数生成非等效的RKHS。同时，研究了NNGP样本路径的平滑性。

Conclusion: 研究为更广泛激活函数的理论分析提供了基础，揭示了不同激活函数对网络行为的影响。

Abstract: While the theory of deep learning has made some progress in recent years,
much of it is limited to the ReLU activation function. In particular, while the
neural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)
have given theoreticians tractable limiting cases of fully connected neural
networks, their properties for most activation functions except for powers of
the ReLU function are poorly understood. Our main contribution is to provide a
more general characterization of the RKHS of these kernels for typical
activation functions whose only non-smoothness is at zero, such as SELU, ELU,
or LeakyReLU. Our analysis also covers a broad set of special cases such as
missing biases, two-layer networks, or polynomial activations. Our results show
that a broad class of not infinitely smooth activations generate equivalent
RKHSs at different network depths, while polynomial activations generate
non-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP
sample paths, characterizing the smoothness of infinitely wide neural networks
at initialization.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [105] [Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification](https://arxiv.org/abs/2506.21828)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Robert Galinsky,Gari D. Clifford,Faezeh Marzbanrad*

Main category: q-bio.NC

TL;DR: 综述了胎儿睡眠的生理特征、发育规律及调控机制，比较了人类与大型动物模型的睡眠模式，探讨了侵入性和非侵入性监测技术，并分析了宫内环境对胎儿睡眠的影响。


<details>
  <summary>Details</summary>
Motivation: 胎儿睡眠对早期神经发育至关重要，了解其模式有助于发现缺氧或生长受限等神经问题。

Method: 综合了80多年的研究，比较人类与动物模型的睡眠模式，分析侵入性和非侵入性技术，以及计算方法（如深度学习和规则分类）。

Result: 揭示了物种间睡眠模式的差异及宫内环境（如缺氧）对胎儿睡眠的干扰。

Conclusion: 为开发非侵入性胎儿睡眠监测技术提供了基础，支持早期诊断和干预。

Abstract: Fetal sleep is a relatively underexplored yet vital aspect of prenatal
neurodevelopment. Understanding fetal sleep patterns could provide insights
into early brain maturation and help clinicians detect signs of neurological
compromise that arise due to fetal hypoxia or fetal growth restriction. This
review synthesizes over eight decades of research on the physiological
characteristics, ontogeny, and regulation of fetal sleep. We compare
sleep-state patterns in humans and large animal models, highlighting
species-specific differences and the presence of sleep-state analogs. We review
both invasive techniques in animals and non-invasive modalities in humans.
Computational methods for sleep-state classification are also examined,
including rule-based approaches (with and without clustering-based
preprocessing) and state-of-the-art deep learning techniques. Finally, we
discuss how intrauterine conditions such as hypoxia and fetal growth
restriction can disrupt fetal sleep. This review provides a comprehensive
foundation for the development of objective, multimodal, and non-invasive fetal
sleep monitoring technologies to support early diagnosis and intervention in
prenatal care.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [106] [Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study](https://arxiv.org/abs/2506.22180)
*Önder Gürcan*

Main category: cs.CR

TL;DR: 论文提出了一种评估智能合约执行安全性的模型，并通过物联网能源案例研究验证了Execute-Order-Validate架构在可靠性和安全性上的优势。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统复杂性和互联性的增加，可靠的安全解决方案需求日益迫切，智能合约因其能够满足合同条件、避免恶意错误等特点成为潜在解决方案。

Method: 开发了评估智能合约执行安全性的模型，并设计了一个基于智能合约的物联网能源案例研究，通过模拟评估多种安全漏洞。

Result: 结果表明，Execute-Order-Validate架构在可靠性和安全性方面表现更优。

Conclusion: 智能合约在自主系统安全中具有潜力，尤其是Execute-Order-Validate架构更适用于高安全需求场景。

Abstract: The industrial market continuously needs reliable solutions to secure
autonomous systems. Especially as these systems become more complex and
interconnected, reliable security solutions are becoming increasingly
important. One promising solution to tackle this challenge is using smart
contracts designed to meet contractual conditions, avoid malicious errors,
secure exchanges, and minimize the need for reliable intermediaries. However,
smart contracts are immutable. Moreover, there are different smart contract
execution architectures (namely Order-Execute and Execute-Order-Validate) that
have different throughputs. In this study, we developed an evaluation model for
assessing the security of reliable smart contract execution. We then developed
a realistic smart contract enabled IoT energy case study. Finally, we simulate
the developed case study to evaluate several smart contract security
vulnerabilities reported in the literature. Our results show that the
Execute-Order-Validate architecture is more promising regarding reliability and
security.

</details>


### [107] [Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America](https://arxiv.org/abs/2506.22323)
*Alessio Di Santo*

Main category: cs.CR

TL;DR: 针对拉丁美洲（尤其是巴西）的复杂恶意邮件活动，通过钓鱼邮件诱使用户执行恶意MSI文件，利用DLL侧加载绕过安全防御，部署BlotchyQuasar（QuasarRAT变种）窃取敏感信息。


<details>
  <summary>Details</summary>
Motivation: 揭示针对拉丁美洲的恶意攻击活动，分析其技术手段和潜在危害。

Method: 利用钓鱼邮件和DLL侧加载技术，部署BlotchyQuasar恶意软件，窃取浏览器凭证和银行信息。

Result: 恶意软件成功窃取数据并建立持久性，但因开发仓促存在缺陷。

Conclusion: 该活动威胁严重，需加强网络安全防御。

Abstract: A sophisticated malspam campaign was recently uncovered targeting Latin
American countries, with a particular focus on Brazil. This operation utilizes
a highly deceptive phishing email to trick users into executing a malicious MSI
file, initiating a multi-stage infection. The core of the attack leverages DLL
side-loading, where a legitimate executable from Valve Corporation is used to
load a trojanized DLL, thereby bypassing standard security defenses.
  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is
capable of a wide range of malicious activities. It is designed to steal
sensitive browser-stored credentials and banking information, the latter
through fake login windows mimicking well-known Brazilian banks. The threat
establishes persistence by modifying the Windows registry , captures user
keystrokes through keylogging , and exfiltrates stolen data to a
Command-and-Control (C2) server using encrypted payloads. Despite its advanced
capabilities, the malware code exhibits signs of rushed development, with
inefficiencies and poor error handling that suggest the threat actors
prioritized rapid deployment over meticulous design. Nonetheless, the campaign
extensive reach and sophisticated mechanisms pose a serious and immediate
threat to the targeted regions, underscoring the need for robust cybersecurity
defenses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 论文提出ADL和AAM方法解决人脸对齐中的误差偏差问题，通过ADNet实现端到端训练，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 研究人脸对齐中误差分布偏差问题，发现误差沿切线方向分布，与模糊标注任务相关。

Method: 提出ADL（各向异性方向损失）和AAM（各向异性注意力模块），分别用于坐标和热图回归。

Result: ADNet在300W、WFLW和COFW数据集上取得最优结果。

Conclusion: ADL和AAM互补学习人脸结构和纹理细节，ADNet表现出高效性和鲁棒性。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [109] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: 提出了一种通过稀疏地标数据集（如300W和WFLW）增强地标密度的框架，实现了高精度的密集人脸对齐。


<details>
  <summary>Details</summary>
Motivation: 密集人脸地标在美容医学和面部美化等场景中需求高，但现有工作多关注稀疏对齐。

Method: 利用稀疏地标学习细化能力，并通过设计多个操作符实现密集地标生成。

Result: 在密集300W测试集及原始稀疏测试集上均达到最优精度。

Conclusion: 该方法无需额外成本即可提升地标密度和精度。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [110] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT是一种生成全景图像的新方法，通过统一扩散模型生成切线平面图像，解决了现有模型在全景生成中的几何失真和一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在全景图像生成中存在几何失真和一致性不足的问题。

Method: 提出TanDiT方法，利用统一扩散模型生成切线平面图像，并设计后处理步骤增强全局一致性。

Result: 实验表明TanDiT能有效泛化，处理复杂文本提示，并生成高质量全景图像。

Conclusion: TanDiT为全景图像生成提供了一种高效且通用的解决方案。

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [111] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: 提出了一种基于EfficientNet-B0的深度学习流程，用于从视网膜眼底图像中检测青光眼，通过多数据集训练提高泛化能力，结果表明简单预处理效果更优。


<details>
  <summary>Details</summary>
Motivation: 青光眼是导致不可逆失明的主要原因，早期检测对治疗效果至关重要，但传统方法通常具有侵入性且需要专业设备。

Method: 使用EfficientNet-B0架构，通过ACRIMA、ORIGA和RIM-ONE数据集进行顺序训练和微调，以增强泛化能力。

Result: 实验表明，简单预处理比复杂增强方法具有更高的AUC-ROC，模型在未见数据集上表现出强判别性能。

Conclusion: 该流程为青光眼早期检测提供了可重复且可扩展的方法，具有潜在的临床应用价值。

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [112] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 该论文介绍了首个白内障手术场景图数据集（CAT-SG），通过结构化标注工具-组织交互和时序依赖，为手术工作流提供全面视角，并提出新模型CatSGG，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅关注手术分析的孤立方面（如工具检测或阶段分割），缺乏对实体间语义关系的全面捕捉。

Method: 引入CAT-SG数据集，标注工具-组织交互和时序依赖，并提出场景图生成模型CatSGG。

Result: CAT-SG提供了手术工作流的全面视图，CatSGG模型在生成结构化手术表示方面优于现有方法。

Conclusion: CAT-SG数据集和CatSGG模型为AI驱动的临床实践（如手术培训和实时决策支持）提供了更智能的解决方案。

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [113] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉基础模型和参数高效微调的少样本历史地图分割方法，在多个数据集上表现优异，显著减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 历史地图作为丰富的历史资源，其多样化的视觉表现和有限的标注数据给自动化处理带来挑战。

Method: 结合大型视觉基础模型的语义嵌入和参数高效微调，实现少样本分割。

Result: 在Siegfried数据集上，10-shot和5-shot场景下分别提升5%和13%的mIoU；在ICDAR 2021数据集上达到67.3%的PQ。

Conclusion: 该方法在极低数据量下仍保持高性能，显著减少人工标注需求，推动历史地图的自动化处理。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [114] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: SPADE是一个基础模型，整合了组织病理学和空间转录组学数据，通过对比学习在多任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 数字病理学和自监督深度学习的快速发展为跨疾病病理任务的基础模型开发提供了可能，但WSI与ST数据的全面整合仍存在空白。

Method: SPADE采用混合数据专家技术，通过两阶段特征空间聚类和对比学习，整合WSI和基因表达数据。

Result: 在14个下游任务中，SPADE表现出显著优于基线模型的少样本性能。

Conclusion: SPADE证明了将形态学和分子信息整合到统一潜在空间的价值。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [115] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: TPM改进ADNet，通过绑定原型位置和多原型扩展，提升医学图像少样本分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决ADNet在医学图像少样本分割中的局限性，如单原型依赖、二分类问题和固定阈值。

Method: 提出Tied Prototype Model (TPM)，绑定前景和背景分布的原型位置，支持多原型和多类分割，并利用类先验定义自适应阈值。

Result: TPM显著提升了分割准确性，尤其在多原型和多类分割任务中。

Conclusion: TPM为医学图像少样本分割提供了新的原型建模视角，代码已开源。

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [116] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 论文提出通过增强视觉输入的低级空间结构和文本提示，显著提升视觉语言模型在视觉推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视觉推理中存在绑定问题，难以可靠地将感知特征与视觉对象关联，导致在计数、视觉搜索等任务中表现不佳。

Method: 通过添加低级空间结构（如水平线）并结合文本提示，引导模型进行空间感知的序列解析。

Result: 方法显著提升了性能：视觉搜索准确率提高25.00%，计数准确率提高26.83%，场景描述编辑距离误差减少0.32，空间关系任务性能提升9.50%。

Conclusion: 低级的视觉结构调整是提升视觉语言模型在空间任务中性能的有效且未被充分探索的方向。

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [117] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: 论文研究了量子门扰动作为数据增强技术对经典机器学习的影响，展示了其在图像分类任务中的性能提升，并探讨了其隐私计算局限性。


<details>
  <summary>Details</summary>
Motivation: 探索量子扰动在量子机器学习中的潜在优势，尤其是如何将其作为数据增强技术应用于经典机器学习。

Method: 使用随机Bloch球旋转作为量子启发的数据增强技术，并在ImageNet数据集上测试其效果。

Result: 量子增强方法显著提升了图像分类性能（Top-1准确率提高3%，Top-5准确率提高2.5%，F1分数从8%提升至12%）。

Conclusion: 量子扰动增强技术对性能提升有效，但在隐私计算方面无显著优势。

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [118] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 研究比较了CNN（ResNet34）和ViT（ViT B16）在事件相机数据上的性能，发现ResNet34在分类精度上略优，但ViT B16在噪声环境下更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 事件相机适用于动态环境（如无人机和自动驾驶车辆），但缺乏对其适用的深度学习架构的系统评估。

Method: 在GEN1事件数据集上微调ResNet34和ViT B16，并在标准条件和模拟噪声下评估性能。

Result: ResNet34和ViT B16在干净数据上的准确率分别为88%和86%，ViT B16在噪声下表现更稳健。

Conclusion: 研究为无人机等动态环境中的事件视觉系统提供了方法参考，ViT B16的鲁棒性尤其值得关注。

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [119] [Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting](https://arxiv.org/abs/2506.21743)
*Jinpai Zhao,Albert Cerrone,Eirik Valseth,Leendert Westerink,Clint Dawson*

Main category: cs.CE

TL;DR: 提出了一种将非结构化水位数据转换为RGB图像表示的方法，结合ConvLSTM网络和物理驱动因素，提升了风暴潮预测的时空分辨率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在风暴潮预测中存在空间分辨率低、依赖海岸站点数据、泛化能力差等问题，且与现代深度学习架构不兼容。

Method: 将非结构化水位数据投影为RGB图像表示，结合ConvLSTM网络，并整合风场和地形数据作为动态和静态输入。

Result: 在墨西哥湾合成风暴数据集上验证，模型在48小时预测中表现稳健，并具有空间扩展性。

Conclusion: 通过结构化表示、物理驱动和可扩展深度学习，提升了风暴潮预测的实用性、适应性和可解释性。

Abstract: Storm surge forecasting plays a crucial role in coastal disaster
preparedness, yet existing machine learning approaches often suffer from
limited spatial resolution, reliance on coastal station data, and poor
generalization. Moreover, many prior models operate directly on unstructured
spatial data, making them incompatible with modern deep learning architectures.
In this work, we introduce a novel approach that projects unstructured water
elevation fields onto structured Red Green Blue (RGB)-encoded image
representations, enabling the application of Convolutional Long Short Term
Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our
model further integrates ground-truth wind fields as dynamic conditioning
signals and topo-bathymetry as a static input, capturing physically meaningful
drivers of surge evolution. Evaluated on a large-scale dataset of synthetic
storms in the Gulf of Mexico, our method demonstrates robust 48-hour
forecasting performance across multiple regions along the Texas coast and
exhibits strong spatial extensibility to other coastal areas. By combining
structured representation, physically grounded forcings, and scalable deep
learning, this study advances the frontier of storm surge forecasting in
usability, adaptability, and interpretability.

</details>


### [120] [Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning](https://arxiv.org/abs/2506.21815)
*Augustine Twumasi,Prokash Chandra Roy,Zixun Li,Soumya Shouvik Bhattacharjee,Zhengtao Gan*

Main category: cs.CE

TL;DR: 提出了一种结合物理模型和机器学习的方法，优化激光粉末床熔融（L-PBF）的扫描路径，以实现目标微观结构，并显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: L-PBF技术中复杂微观结构的形成影响产品质量，传统方法计算成本高且依赖试错。

Method: 结合相场法（PFM）和3D U-Net卷积神经网络构建代理模型，利用深度强化学习（DRL）优化扫描路径。

Result: 代理模型实现了两个数量级的加速，DRL方法在三种案例中表现出色，优于传统锯齿形扫描。

Conclusion: 机器学习方法在L-PBF优化中展现出高效控制微观结构和计算效率的潜力。

Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing
technology for producing intricate metal components with exceptional accuracy.
A key challenge in L-PBF is the formation of complex microstructures affecting
product quality. We propose a physics-guided, machine-learning approach to
optimize scan paths for desired microstructure outcomes, such as equiaxed
grains. We utilized a phase-field method (PFM) to model crystalline grain
structure evolution. To reduce computational costs, we trained a surrogate
machine learning model, a 3D U-Net convolutional neural network, using
single-track phase-field simulations with various laser powers to predict
crystalline grain orientations based on initial microstructure and thermal
history. We investigated three scanning strategies across various hatch
spacings within a square domain, achieving a two-orders-of-magnitude speedup
using the surrogate model. To reduce trial and error in designing laser scan
toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan
paths for target microstructure. Results from three cases demonstrate the DRL
approach's effectiveness. We integrated the surrogate 3D U-Net model into our
DRL environment to accelerate the reinforcement learning training process. The
reward function minimizes both aspect ratio and grain volume of the predicted
microstructure from the agent's scan path. The reinforcement learning algorithm
was benchmarked against conventional zigzag approach for smaller and larger
domains, showing machine learning methods' potential to enhance microstructure
control and computational efficiency in L-PBF optimization.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [121] [Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search](https://arxiv.org/abs/2506.21772)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli,Stéphanie Gourdin*

Main category: eess.SP

TL;DR: 论文提出了一种基于蒙特卡洛树搜索的神经架构搜索方法，用于设计低计算复杂度的神经网络，以提升雷达目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在雷达目标检测中表现优异，但其高计算复杂度限制了在嵌入式雷达系统中的广泛应用。

Method: 采用基于蒙特卡洛树搜索的神经架构搜索方法，寻找满足检测性能且计算复杂度较低的神经网络。

Result: 在雷达信号上评估搜索到的架构，提出了一种新网络，其检测性能满足要求且显著轻量化。

Conclusion: 该方法成功设计出高性能且低复杂度的神经网络，为嵌入式雷达系统提供了可行的解决方案。

Abstract: Recent research works establish deep neural networks as high performing tools
for radar target detection, especially on challenging environments (presence of
clutter or interferences, multi-target scenarii...). However, the usually large
computational complexity of these networks is one of the factors preventing
them from being widely implemented in embedded radar systems. We propose to
investigate novel neural architecture search (NAS) methods, based on
Monte-Carlo Tree Search (MCTS), for finding neural networks achieving the
required detection performance and striving towards a lower computational
complexity. We evaluate the searched architectures on endoclutter radar
signals, in order to compare their respective performance metrics and
generalization properties. A novel network satisfying the required detection
probability while being significantly lighter than the expert-designed baseline
is proposed.

</details>


### [122] [From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining](https://arxiv.org/abs/2506.21803)
*Fuying Wang,Jiacheng Xu,Lequan Yu*

Main category: eess.SP

TL;DR: MELP是一种新型的多尺度ECG-语言预训练模型，通过分层监督从ECG-文本对中提取信息，显著优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统ECG分析方法依赖大量人工标注，耗时耗力；自监督学习虽能提取ECG表征，但未能捕捉其多尺度特性。

Method: MELP结合心脏学专用语言模型和三层次跨模态监督（token、beat、rhythm），对齐ECG信号与文本报告。

Result: 在多个ECG数据集和任务中，MELP表现优于现有方法，展示了其高效性和适应性。

Conclusion: MELP通过多尺度监督显著提升ECG表征学习，为临床ECG分析提供了高效解决方案。

Abstract: Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and
diagnosing heart diseases. However, traditional deep learning approaches for
ECG analysis rely heavily on large-scale manual annotations, which are both
time-consuming and resource-intensive to obtain. To overcome this limitation,
self-supervised learning (SSL) has emerged as a promising alternative, enabling
the extraction of robust ECG representations that can be efficiently
transferred to various downstream tasks. While previous studies have explored
SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail
to capture the multi-scale nature of ECG signals. As a result, these methods
struggle to learn generalized representations due to their inability to model
the hierarchical structure of ECG data. To address this gap, we introduce MELP,
a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages
hierarchical supervision from ECG-text pairs. MELP first pretrains a
cardiology-specific language model to enhance its understanding of clinical
text. It then applies three levels of cross-modal supervision-at the token,
beat, and rhythm levels-to align ECG signals with textual reports, capturing
structured information across different time scales. We evaluate MELP on three
public ECG datasets across multiple tasks, including zero-shot ECG
classification, linear probing, and transfer learning. Experimental results
demonstrate that MELP outperforms existing SSL methods, underscoring its
effectiveness and adaptability across diverse clinical applications. Our code
is available at https://github.com/HKU-MedAI/MELP.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [123] [Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses](https://arxiv.org/abs/2506.21842)
*Archisman Ghosh,Satwik Kundu,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 量子机器学习（QML）结合量子计算与经典机器学习，但面临NISQ时代的独特安全挑战。本章探讨QML系统的对抗性威胁及防御机制。


<details>
  <summary>Details</summary>
Motivation: QML快速发展引发安全担忧，尤其在云部署和混合架构中，需研究其独特漏洞及防御方法。

Method: 分析QML的对抗威胁（如模型窃取、数据投毒）及防御机制（如噪声水印、硬件感知混淆）。

Result: 提出防御策略（如量子对抗训练、差分隐私），但需解决噪声平衡等开放性问题。

Conclusion: 为构建鲁棒QML系统提供研究路线图，强调对抗性威胁与防御的平衡。

Abstract: Quantum Machine Learning (QML) integrates quantum computing with classical
machine learning, primarily to solve classification, regression and generative
tasks. However, its rapid development raises critical security challenges in
the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines
adversarial threats unique to QML systems, focusing on vulnerabilities in
cloud-based deployments, hybrid architectures, and quantum generative models.
Key attack vectors include model stealing via transpilation or output
extraction, data poisoning through quantum-specific perturbations, reverse
engineering of proprietary variational quantum circuits, and backdoor attacks.
Adversaries exploit noise-prone quantum hardware and insufficiently secured
QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,
and functionality. Defense mechanisms leverage quantum properties to counter
these threats. Noise signatures from training hardware act as non-invasive
watermarks, while hardware-aware obfuscation techniques and ensemble strategies
disrupt cloning attempts. Emerging solutions also adapt classical adversarial
training and differential privacy to quantum settings, addressing
vulnerabilities in quantum neural networks and generative architectures.
However, securing QML requires addressing open challenges such as balancing
noise levels for reliability and security, mitigating cross-platform attacks,
and developing quantum-classical trust frameworks. This chapter summarizes
recent advances in attacks and defenses, offering a roadmap for researchers and
practitioners to build robust, trustworthy QML systems resilient to evolving
adversarial landscapes.

</details>


### [124] [Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability](https://arxiv.org/abs/2506.22335)
*Osama Ahmed,Felix Tennie,Luca Magri*

Main category: quant-ph

TL;DR: 论文提出量子储备计算器（QRCs）及其无递归架构（RF-QRCs）是学习混沌动力学的有效工具，并证明了其鲁棒性和设计方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子储备计算器在混沌时间序列预测中的潜力，并探索其鲁棒性和设计准则。

Method: 将QRCs建模为耦合动力系统，推导其Jacobian矩阵，提出GS=ESP设计准则，并通过数值模拟验证。

Result: QRCs能学习混沌动力学及其不变性质，噪声增强了鲁棒性，RF-QRCs满足GS=ESP准则。

Conclusion: 该研究为近量子硬件上的混沌时间序列预测提供了鲁棒性设计方法。

Abstract: We show that recurrent quantum reservoir computers (QRCs) and their
recurrence-free architectures (RF-QRCs) are robust tools for learning and
forecasting chaotic dynamics from time-series data. First, we formulate and
interpret quantum reservoir computers as coupled dynamical systems, where the
reservoir acts as a response system driven by training data; in other words,
quantum reservoir computers are generalized-synchronization (GS) systems.
Second, we show that quantum reservoir computers can learn chaotic dynamics and
their invariant properties, such as Lyapunov spectra, attractor dimensions, and
geometric properties such as the covariant Lyapunov vectors. This analysis is
enabled by deriving the Jacobian of the quantum reservoir update. Third, by
leveraging tools from generalized synchronization, we provide a method for
designing robust quantum reservoir computers. We propose the criterion
$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We
analytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we
analyze the effect of simulated noise. We find that dissipation from noise
enhances the robustness of quantum reservoir computers. Numerical verifications
on systems of different dimensions support our conclusions. This work opens
opportunities for designing robust quantum machines for chaotic time series
forecasting on near-term quantum hardware.

</details>


### [125] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 论文提出了一种基于Kolmogorov Arnold Networks (KANs)的量子机器学习架构，结合经典和量子组件，展示了其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 探索KANs在量子机器学习中的潜力，利用其表达复杂函数的优势。

Method: 实现混合和全量子形式的KAN架构，使用Quantum Circuit Born Machine (QCBM)和预训练残差函数。

Result: 展示了Quantum KAN (QuKAN)架构的可行性、可解释性和性能。

Conclusion: QuKAN架构为量子机器学习提供了新的可能性，结合了KANs和量子计算的优势。

Abstract: Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold
representation theorem (KAR), have demonstrated promising capabilities in
expressing complex functions with fewer neurons. This is achieved by
implementing learnable parameters on the edges instead of on the nodes, unlike
traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs
potential in quantum machine learning has not yet been well explored. In this
work, we present an implementation of these KAN architectures in both hybrid
and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt
the KAN transfer using pre-trained residual functions, thereby exploiting the
representational power of parametrized quantum circuits. In the hybrid model we
combine classical KAN components with quantum subroutines, while the fully
quantum version the entire architecture of the residual function is translated
to a quantum model. We demonstrate the feasibility, interpretability and
performance of the proposed Quantum KAN (QuKAN) architecture.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [126] [Computing Maximum Cliques in Unit Disk Graphs](https://arxiv.org/abs/2506.21926)
*Anastasiia Tkachenko,Haitao Wang*

Main category: cs.CG

TL;DR: 本文提出了一种更高效的算法，用于计算平面点集的单位圆图中的最大团，时间复杂度优于现有方法，特别是在最大团规模较小时。对于凸包点集，还提供了随机化算法和确定性算法。


<details>
  <summary>Details</summary>
Motivation: 解决单位圆图中最大团计算问题的高效算法需求，尤其是在最大团规模较小时和点集为凸包时的特殊情况。

Method: 提出了一种时间复杂度为O(n log n + n K^{4/3+o(1)})的算法，适用于一般点集；对于凸包点集，设计了随机化算法和确定性算法。

Result: 新算法在最大团规模较小时优于现有方法；凸包点集的随机化算法时间复杂度为O(n^{2.143})，确定性算法为O(n^2 log n)。

Conclusion: 本文提出的算法显著提升了单位圆图中最大团计算的效率，特别是在特定条件下，为相关问题提供了更优解决方案。

Abstract: Given a set $P$ of $n$ points in the plane, the unit-disk graph $G(P)$ is a
graph with $P$ as its vertex set such that two points of $P$ have an edge if
their Euclidean distance is at most $1$. We consider the problem of computing a
maximum clique in $G(P)$. The previously best algorithm for the problem runs in
$O(n^{7/3+o(1)})$ time. We show that the problem can be solved in $O(n \log n +
n K^{4/3+o(1)})$ time, where $K$ is the maximum clique size. The algorithm is
faster than the previous one when $K=o(n)$. In addition, if $P$ is in convex
position, we give a randomized algorithm that runs in $O(n^{15/7+o(1)})=
O(n^{2.143})$ worst-case time and the algorithm can compute a maximum clique
with high probability. For points in convex position, one special case we solve
is when a point in the maximum clique is given; we present an $O(n^2\log n)$
time (deterministic) algorithm for this special case.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [127] [Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles](https://arxiv.org/abs/2506.22052)
*Nico Ostendorf,Keno Garlichs,Lars Wolf*

Main category: cs.ET

TL;DR: 研究评估了冗余缓解（RM）对V2X通信中自行车手感知消息（VAM）的效果，发现标准化RM机制可能导致信息过时，提出改进方法以平衡信道负载和安全性。


<details>
  <summary>Details</summary>
Motivation: V2X通信对提升道路安全至关重要，但设备增多导致信道负载增加，需评估RM对VAM的效果以减少冗余信息。

Method: 基于德国汉诺威交通数据的高密度自行车场景模拟，分析RM对信道负载（CBR）和安全性（VPR）的影响，并比较位置、速度和航向的实际差异。

Result: RM减少信道拥堵但降低VPR；标准化RM使用过时信息，改进方法显著降低CBR且VPR下降较少，提升安全性。

Conclusion: 需进一步优化RM技术，确保V2X通信效率同时不牺牲VRU安全，改进方法展示更好平衡。

Abstract: V2X communication has become crucial for enhancing road safety, especially
for Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the
increasing number of devices communicating on the same channels will lead to
significant channel load. To address this issue this study evaluates the
effectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),
focusing specifically on cyclists. The objective of RM is to minimize the
transmission of redundant information. We conducted a simulation study using a
urban scenario with a high bicycle density based on traffic data from Hannover,
Germany. This study assessed the impact of RM on channel load, measured by
Channel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in
simulation. To evaluate the accuracy and reliability of the RM mechanisms, we
analyzed the actual differences in position, speed, and heading between the ego
VRU and the VRU, which was assumed to be redundant. Our findings indicate that
while RM can reduce channel congestion, it also leads to a decrease in VPR. The
analysis of actual differences revealed that the RM mechanism standardized by
ETSI often uses outdated information, leading to significant discrepancies in
position, speed, and heading, which could result in dangerous situations. To
address these limitations, we propose an adapted RM mechanism that improves the
balance between reducing channel load and maintaining VRU awareness. The
adapted approach shows a significant reduction in maximum CBR and a less
significant decrease in VPR compared to the standardized RM. Moreover, it
demonstrates better performance in the actual differences in position, speed,
and heading, thereby enhancing overall safety. Our results highlight the need
for further research to optimize RM techniques and ensure they effectively
enhance V2X communication without compromising the safety of VRUs.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [128] [CaloHadronic: a diffusion model for the generation of hadronic showers](https://arxiv.org/abs/2506.21720)
*Thorsten Buss,Frank Gaede,Gregor Kasieczka,Anatolii Korol,Katja Krüger,Peter McKeown,Martina Mozzanica*

Main category: physics.ins-det

TL;DR: 论文提出了一种基于Transformer的生成模型，用于高效模拟高粒度量能器中电磁和强子簇射，首次实现了跨两种量能器的整体生成。


<details>
  <summary>Details</summary>
Motivation: 传统模拟方法计算成本高，生成式机器学习模型可提升模拟的准确性和速度，缓解计算限制。

Method: 采用基于扩散的生成方法，结合Transformer架构的注意力机制，生成几何无关的点云数据。

Result: 模型能够高效生成具有复杂子结构的强子簇射，适用于电磁和强子量能器。

Conclusion: 该方法首次实现了跨电磁和强子量能器的整体簇射生成，为粒子物理模拟提供了新工具。

Abstract: Simulating showers of particles in highly-granular calorimeters is a key
frontier in the application of machine learning to particle physics. Achieving
high accuracy and speed with generative machine learning models can enable them
to augment traditional simulations and alleviate a major computing constraint.
Recent developments have shown how diffusion based generative shower simulation
approaches that do not rely on a fixed structure, but instead generate
geometry-independent point clouds, are very efficient. We present a
transformer-based extension to previous architectures which were developed for
simulating electromagnetic showers in the highly granular electromagnetic
calorimeter of the International Large Detector, ILD. The attention mechanism
now allows us to generate complex hadronic showers with more pronounced
substructure across both the electromagnetic and hadronic calorimeters. This is
the first time that machine learning methods are used to holistically generate
showers across the electromagnetic and hadronic calorimeter in highly granular
imaging calorimeter systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [129] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: UnMix-NeRF结合光谱分解与NeRF，实现高光谱新视角合成和无监督材料分割，提升材料感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF分割方法依赖RGB数据，缺乏材料属性，限制了在机器人、AR等应用中的准确性。

Method: 通过建模光谱反射的漫反射和镜面反射成分，学习全局端元字典和点级丰度分布，实现无监督材料聚类和场景编辑。

Result: 实验表明，UnMix-NeRF在光谱重建和材料分割上优于现有方法。

Conclusion: UnMix-NeRF为材料感知提供了新框架，支持灵活的场景编辑。

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [130] [DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding](https://arxiv.org/abs/2506.22362)
*Yang Yang,Yunpeng Li,George Sung,Shao-Fu Shih,Craig Dooley,Alessio Centazzo,Ramanan Rajeswaran*

Main category: eess.AS

TL;DR: DiffSoundStream通过减少语义和声学令牌的冗余以及利用潜在扩散模型，提高了非流式场景中语音令牌化的效率。


<details>
  <summary>Details</summary>
Motivation: 语音生成中令牌化方法的推理速度受限于令牌率，需要更高效的解决方案。

Method: 1. 通过语义令牌调节神经编解码器以减少冗余；2. 使用潜在扩散模型从语义和粗粒度声学令牌合成高质量波形。

Result: 在50令牌/秒下，DiffSoundStream的语音质量与标准SoundStream模型在100令牌/秒时相当，且仅需4步扩散采样即可实现步长蒸馏。

Conclusion: DiffSoundStream显著提高了语音令牌化的效率，同时保持了高质量的语音生成。

Abstract: Token-based language modeling is a prominent approach for speech generation,
where tokens are obtained by quantizing features from self-supervised learning
(SSL) models and extracting codes from neural speech codecs, generally referred
to as semantic tokens and acoustic tokens. These tokens are often modeled
autoregressively, with the inference speed being constrained by the token rate.
In this work, we propose DiffSoundStream, a solution that improves the
efficiency of speech tokenization in non-streaming scenarios through two
techniques: (1) conditioning the neural codec on semantic tokens to minimize
redundancy between semantic and acoustic tokens, and (2) leveraging latent
diffusion models to synthesize high-quality waveforms from semantic and
coarse-level acoustic tokens. Experiments show that at 50 tokens per second,
DiffSoundStream achieves speech quality on par with a standard SoundStream
model operating at twice the token rate. Additionally, we achieve step-size
distillation using just four diffusion sampling steps with only a minor quality
loss.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [131] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: 论文提出了MMLU-CF，一个无污染且更具挑战性的多选题基准，用于更可靠地评估大语言模型的世界知识理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多选题数据集（如MMLU）因开源性和训练数据来源广泛，导致基准污染，评估结果不可靠。

Method: 通过从更广泛的领域获取数据并设计三条去污染规则，避免无意数据泄漏；将基准分为验证集和测试集以防止恶意数据泄漏。

Result: 主流大语言模型在测试集上的表现显著下降（如GPT-4o的5-shot得分为73.4%），验证了方法的有效性。

Conclusion: MMLU-CF提供了一个更严格且无污染的评估标准，提升了评估的可靠性。

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [132] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 论文提出数据效能（Data Efficacy）概念，通过优化训练数据的组织提升语言模型性能，并提出了DELT范式，包含数据评分、选择和排序。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据效率（如数据筛选和采样），而数据组织优化（数据效能）尚未充分探索。本文旨在填补这一空白。

Method: 提出DELT范式，包含数据评分（如LQS）、数据选择和数据排序（如FO）。LQS从梯度一致性角度评估数据样本的学习性和质量，FO解决模型遗忘和数据分布偏差问题。

Result: 实验表明，DELT能不同程度提升模型性能，且不增加数据规模或模型大小。LQS与FO组合效果最佳，数据效能与数据效率可同时实现。

Conclusion: 数据效能是语言模型训练中一个潜力巨大的基础研究方向。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [133] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 论文提出了一个名为BTF的“过去预测”基准，用于评估LLM的预测能力，通过已知结果的问题和大量相关网页数据模拟真实预测环境。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个现实、封闭且可重复的LLM预测评估环境，BTF旨在填补这一空白。

Method: BTF使用已知结果的数百个高质量问题及大量相关网页数据，模拟真实预测场景，并测试不同LLM的预测能力。

Result: 实验表明，BTF能产生与实时互联网预测相似的结果，并能追踪LLM预测能力的持续进步。

Conclusion: BTF是一个动态基准，将持续更新问题以适配训练数据的变化，为研究者提供实用工具。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [134] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: 本研究提出了一种结合日本传统沟通方式的多智能体推理框架（KCS+IBC），用于情感分析中的偏见缓解、可解释性提升和概率预测。


<details>
  <summary>Details</summary>
Motivation: 受日本传统沟通方式（如回覧板和井端会话）启发，旨在通过多智能体框架实现更平衡和多样化的情感分析预测。

Method: 整合多个大型语言模型（LLMs），在推理过程中加入非正式对话环节，并引入概率情感预测。

Result: KCS与单一LLM的准确率相当，而KCS+IBC在推理后期表现出熵的持续降低和方差的逐渐增加，表明其能平衡预测的聚合与多样性。

Conclusion: 未来工作将量化这些特性对偏见修正的影响，并开发更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [135] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: 研究探讨了反向翻译（BT）在英语-古吉拉特语低资源机器翻译中的效果，发现合成数据并未提升性能，甚至略有下降。


<details>
  <summary>Details</summary>
Motivation: 反向翻译在低资源机器翻译中广泛使用，但其在高质量低资源环境中的效果尚不明确。

Method: 使用MBART50模型，基于5万句对的高质量平行语料库训练基线系统，并加入反向翻译生成的合成数据。

Result: 合成数据未提升翻译性能，部分情况下略有下降，通过多种指标验证。

Conclusion: 反向翻译在某些低资源场景中可能收益递减，需进一步研究。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [136] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: 该论文介绍了BioPars，一种用于评估大型语言模型（LLMs）在生物信息学任务中能力的工具，并展示了其在波斯医学问答中的首次应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在生物信息学领域的潜力，尤其是在波斯医学问答中的表现。

Method: 方法包括引入BIOPARS-BENCH数据集和BioParsQA评估工具，并比较ChatGPT、Llama和Galactica在生物信息学任务中的表现。

Result: 结果显示BioPars在ROUGE-L、BERTScore、MoverScore和BLEURT等指标上优于其他模型，如GPT-4 1.0。

Conclusion: 结论指出LLMs在生物信息学任务中仍有改进空间，BioPars为波斯医学问答提供了新的解决方案。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [137] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 分析预训练语言模型在低数据量时间序列预测中的有效性，探讨设计选择对验证损失的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地将预训练语言模型迁移到时间序列预测任务中，尤其是在低数据量情况下。

Method: 通过分析上游后训练、时间序列分词器和语言模型主干大小等设计选择，评估其对验证损失的影响。

Result: 发现验证损失在语言模型中持续下降，而随机初始化模型已收敛，形成非零迁移差距。

Conclusion: 这些发现有助于理解高效计算训练在时间序列中的应用，并为研究模态无关的数据分布特性提供方向。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [138] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: 提出了一种结合黑盒和白盒模型优势的新框架，通过语义相似性约束优化大语言模型的指令，显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 黑盒模型成本高，白盒模型资源需求大且表达能力有限，需要一种高效结合两者的方法。

Method: 融合黑盒模型的高质量初始化指令和白盒模型的细粒度可解释性，通过语义相似性约束实现优化。

Result: 在复杂推理和跨语言泛化等任务中表现优于现有基线。

Conclusion: 该框架为下一代大语言模型应用提供了高效、可扩展的解决方案。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [139] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 比较了两种基于LLM的方法（直接提取和间接提取）从食品产品页面提取结构化信息的性能，间接方法在准确率略低的情况下显著提升了效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 探索利用生成式AI和LLM自动化从网页提取结构化信息的潜力，特别是在食品产品页面上。

Method: 对比了直接提取和通过生成函数间接提取两种LLM方法，评估了准确性、效率和成本。

Result: 间接提取方法准确率略低（96.48%），但减少了95.82%的LLM调用，显著提升了效率和成本效益。

Conclusion: 间接提取方法为基于模板网页的大规模信息提取提供了可扩展且经济高效的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [140] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: 提出了一种基于表示一致性（RC）的测试时扩展方法，通过聚合LLM生成的多个候选答案，结合内部激活一致性提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要复杂的提示和采样策略修改，RC旨在简化并提升答案聚合效果。

Method: 利用模型内部激活（密集或稀疏）的一致性，对候选答案进行加权聚合。

Result: 在四个开源LLM和四个推理数据集上验证，性能提升达4%。

Conclusion: RC方法无需额外查询，仅需轻量计算即可显著提升推理性能。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [141] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: 论文探讨了自动化作文评分（AES）系统的人本操作化，比较了机器学习与大型语言模型（LLMs）的优劣，重点关注偏差、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在超越准确性，探索AES系统在实际应用中的人本需求，如公平性和透明度。

Method: 比较了机器学习与LLMs在AES中的表现，分析其在不同维度（如偏差、鲁棒性、可解释性）的差异。

Result: 机器学习模型在准确性上优于LLMs，但可解释性较差；LLMs提供更丰富的解释，但两者在偏差和边缘分数鲁棒性上均表现不佳。

Conclusion: 研究揭示了不同AES方法的挑战与权衡，为开发更可靠、可信的系统提供了方向。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [142] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: CORE-KG是一个模块化框架，用于从法律文本构建可解释的知识图谱，通过类型感知的共指消解和领域引导的实体关系提取，显著减少节点重复和噪声。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱方法在处理法律文本时存在共指消解不足和噪声问题，CORE-KG旨在解决这些问题。

Method: 采用两步流程：类型感知的共指消解和领域引导的实体关系提取，基于改进的GraphRAG框架。

Result: 相比基线，CORE-KG减少节点重复33.28%，噪声38.37%，生成更清晰的知识图谱。

Conclusion: CORE-KG为分析复杂犯罪网络提供了有效工具。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [143] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 研究探讨了在多模态时间序列预测中，文本信息的作用及其适用条件，发现其效果因模型和数据特性而异。


<details>
  <summary>Details</summary>
Motivation: 探索在多模态时间序列预测中，文本信息是否及在何种条件下能带来一致的性能提升。

Method: 系统评估了14个预测任务中的两种多模态方法：基于对齐的方法和基于提示的方法。

Result: 多模态方法的效果并非普遍适用，其性能提升依赖于模型架构和数据特性。

Conclusion: 提供了多模态预测中文本信息适用的具体条件，指导实际应用。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [144] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: 论文提出结合人类经验训练的算法和合成数据生成方法，以提升文本分类性能，尤其是消费者投诉中的语义差异识别。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言中细微语义差异和上下文变化的捕捉问题，特别是在消费者投诉领域。

Method: 结合人类经验训练的算法和专家评估的生成对抗网络（GAN）生成合成数据，并通过专家标注优化。

Result: 预期显著提升分类器性能，降低数据集获取成本，并改善评估指标和鲁棒性。

Conclusion: 通过结合专家训练和高质量合成数据，能够有效提升文本分类任务的性能和经济性。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [145] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: ANUBHUTI是一个包含2000句孟加拉语方言的标注数据集，用于情感分析，填补了低资源方言的空白。


<details>
  <summary>Details</summary>
Motivation: 由于语言多样性和标注数据有限，孟加拉语方言的情感分析研究不足。

Method: 数据集包含四种方言的句子，采用双重标注方案：主题分类和情感标注，并由专家翻译和标注。

Result: 数据集质量高，通过一致性检验，适用于低资源方言的情感分析。

Conclusion: ANUBHUTI为孟加拉语方言的情感分析提供了重要资源，提升了自然语言处理的准确性。

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [146] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: 本文提出了一种基于XLNet的自动检测社交媒体上冒犯性语言的模型，并与BERT进行了性能比较。实验表明，XLNet在检测冒犯性内容和分类冒犯类型上优于BERT，而BERT在识别冒犯目标上稍优。此外，过采样和欠采样策略能有效解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上文本交流的普及导致冒犯性内容增加，手动审核不切实际，需要自动化系统。

Method: 使用XLNet和BERT模型，基于OLID数据集进行性能比较，并采用过采样和欠采样策略解决类别不平衡。

Result: XLNet在检测冒犯性内容和分类冒犯类型上优于BERT，BERT在识别冒犯目标上表现稍好。过采样和欠采样策略有效。

Conclusion: XLNet和迁移学习架构在构建冒犯性语言检测系统方面具有潜力。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [147] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）输出置信度的估计方法，提出了基于生成一致性的假设（一致性假设），并通过统计测试和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在需要高用户信任的实际应用中，估计LLM输出的置信度至关重要。黑盒不确定性量化（UQ）方法因其仅需模型API访问的实用性而受到关注。

Method: 作者提出了三个数学陈述及统计测试，形式化了一致性假设，并在8个基准数据集和3个任务（问答、文本摘要和文本到SQL）上进行了实证研究。

Result: 研究发现一致性假设在不同设置下普遍成立，其中“Sim-Any”假设最具可操作性。基于此，作者提出了无需数据的黑盒UQ方法，其性能优于基线方法。

Conclusion: 一致性假设的实证观察具有实际价值，提出的方法为LLM输出置信度估计提供了有效工具。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [148] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: 研究指出当前工具集成LLM代理的评估多关注端到端工具使用，而忽视其稳定性。实验发现代理在各阶段易出错，开源模型代理更脆弱，增大模型规模未必提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有评估忽视代理稳定性，影响其实际应用，因此研究代理在工具调用全过程中的脆弱性。

Method: 通过实验分析代理在读取工具文档、选择工具与生成参数、处理工具响应等阶段的错误易发性。

Result: 代理在各阶段均易出错，开源模型代理更脆弱；增大模型规模未显著提升稳定性，反而可能增加攻击风险。

Conclusion: 强调评估代理稳定性的重要性，为未来LLM开发与评估提供洞见。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [149] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: 论文提出两种混合攻击方法（GCG + PAIR和GCG + WordGame），结合令牌级和提示级技术，显著提高了对预训练语言模型的攻击成功率，并突破了现有防御措施。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型（PTLMs）和大型语言模型（LLMs）广泛应用，但其安全性仍存在漏洞，现有攻击方法（令牌级和提示级）各有局限性。

Method: 提出两种混合攻击方法：GCG + PAIR和GCG + WordGame，结合令牌级和提示级技术，评估了它们在Vicuna和Llama模型上的表现。

Result: GCG + PAIR在Llama-3上的攻击成功率（ASR）达到91.6%，显著高于PAIR的58.4%；GCG + WordGame在高严格评估下仍保持80%以上的ASR。两种方法均能突破高级防御。

Conclusion: 研究揭示了现有安全措施的漏洞，强调了在攻击成功率和防御鲁棒性之间的权衡，并呼吁开发更全面的防护措施。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [150] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: 本文研究了如何通过微调和标准化方案提升Whisper模型在驾驶舱对话转录中的准确性，将词错误率从68.49%降至26.26%。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练模型在通用领域表现优异，但在驾驶舱等特定领域的转录任务中表现不佳，需要改进。

Method: 收集驾驶舱模拟器和飞行员访谈录音，手动标注数据；提出多种标准化方案；采用LoRA进行高效微调。

Result: 微调后的Whisper Large模型结合标准化方案，词错误率从68.49%降至26.26%。

Conclusion: 通过数据标准化和微调，显著提升了Whisper模型在驾驶舱对话转录中的性能。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [151] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: 研究通过技术手段从GPT-2 Small中分离出负责主谓一致的子网络，并验证其性能。


<details>
  <summary>Details</summary>
Motivation: 探索GPT-2 Small中主谓一致任务的内部机制，理解模型如何实现这一功能。

Method: 使用性能验证、自动电路发现（直接路径修补）和直接对数归因技术，分离出候选电路。

Result: 仅需少量网络组件即可完成基础任务，但复杂任务需要更多组件。

Conclusion: 主谓一致功能由特定子网络实现，且其复杂性随任务难度增加。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [152] [A Plea for History and Philosophy of Statistics and Machine Learning](https://arxiv.org/abs/2506.22236)
*Hanti Lin*

Main category: stat.OT

TL;DR: 该论文呼吁将统计学与机器学习的历史和哲学进行双重整合，并通过案例研究揭示了一个基础假设——achievableism。


<details>
  <summary>Details</summary>
Motivation: 当前统计学与机器学习的界限模糊，需要整合其历史与哲学，以促进领域发展。

Method: 通过案例研究，追溯哲学思想到Neyman和Pearson的早期工作，并揭示基础假设。

Result: 提出了achievableism这一基础假设，并展示了方法论层面的整合。

Conclusion: 双重整合对统计学和机器学习的未来发展至关重要。

Abstract: The integration of the history and philosophy of statistics was initiated at
least by Hacking (1965) and advanced by Mayo (1996), but it has not received
sustained follow-up. Yet such integration is more urgent than ever, as the
recent success of artificial intelligence has been driven largely by machine
learning -- a field historically developed alongside statistics. Today, the
boundary between statistics and machine learning is increasingly blurred. What
we now need is integration, twice over: of history and philosophy, and of the
field they engage -- statistics and machine learning. I present a case study of
a philosophical idea in machine learning (and in formal epistemology) whose
root can be traced back to an often under-appreciated insight in Neyman and
Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the
articulation of a foundational assumption -- largely implicit in, but shared
by, the practices of frequentist statistics and machine learning -- which I
call achievabilism. Another integration also emerges at the level of
methodology, combining two ends of the philosophy of science spectrum: history
and philosophy of science on the one hand, and formal epistemology on the other
hand.

</details>
