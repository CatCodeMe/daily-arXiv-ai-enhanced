{"id": "2507.06471", "pdf": "https://arxiv.org/pdf/2507.06471", "abs": "https://arxiv.org/abs/2507.06471", "authors": ["Fuhuan Li", "Zhihui Du", "David A. Bader"], "title": "Designing Parallel Algorithms for Community Detection using Arachne", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "The rise of graph data in various fields calls for efficient and scalable\ncommunity detection algorithms. In this paper, we present parallel\nimplementations of two widely used algorithms: Label Propagation and Louvain,\nspecifically designed to leverage the capabilities of Arachne which is a\nPython-accessible, open-source framework for large-scale graph analysis. Our\nimplementations achieve substantial speedups over existing Python-based tools\nlike NetworkX and igraph, which lack efficient parallelization, and are\ncompetitive with parallel frameworks such as NetworKit. Experimental results\nshow that Arachne-based methods outperform these baselines, achieving speedups\nof up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit.\nAdditionally, we analyze the scalability of our implementation under varying\nthread counts, demonstrating how different phases contribute to overall\nperformance gains of the parallel Louvain algorithm. Arachne, including our\ncommunity detection implementation, is open-source and available at\nhttps://github.com/Bears-R-Us/arkouda-njit .", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eArachne\u6846\u67b6\u7684\u5e76\u884c\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\uff08Label Propagation\u548cLouvain\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u901f\u5ea6\u8fdc\u8d85NetworkX\u548cigraph\uff0c\u5e76\u4e0eNetworKit\u7ade\u4e89\u3002", "motivation": "\u56fe\u6570\u636e\u5728\u5404\u9886\u57df\u7684\u589e\u957f\u9700\u8981\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\u3002", "method": "\u5728Arachne\u6846\u67b6\u4e2d\u5e76\u884c\u5b9e\u73b0Label Propagation\u548cLouvain\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cArachne\u65b9\u6cd5\u901f\u5ea6\u63d0\u5347\u663e\u8457\uff08\u6700\u9ad8710\u500d\u4e8eNetworkX\uff0c75\u500d\u4e8eigraph\uff0c12\u500d\u4e8eNetworKit\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5e76\u884cLouvain\u7b97\u6cd5\u7684\u6269\u5c55\u6027\u3002", "conclusion": "Arachne\u6846\u67b6\u53ca\u5176\u793e\u533a\u68c0\u6d4b\u5b9e\u73b0\u5f00\u6e90\u53ef\u7528\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.06608", "pdf": "https://arxiv.org/pdf/2507.06608", "abs": "https://arxiv.org/abs/2507.06608", "authors": ["Xiaoxiang Shi", "Colin Cai", "Junjia Du", "Zhanda Zhu", "Xingda Wei", "Zhihao Jia"], "title": "Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5355\u4e2aGPU\u5185\u52a8\u6001\u5206\u914d\u8d44\u6e90\u4ee5\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u9884\u586b\u5145-\u89e3\u7801\uff08PD\uff09\u89e3\u8026\u65b9\u6cd5\u9700\u8981\u66f4\u591a\u786c\u4ef6\u8d44\u6e90\uff0c\u800c\u5206\u5757\u9884\u586b\u5145\u65b9\u6cd5\u867d\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\uff0c\u4f46\u5f15\u5165\u4e86\u9636\u6bb5\u5e72\u6270\u3002\u8bba\u6587\u63a2\u7d22\u662f\u5426\u80fd\u5728\u5355\u4e2aGPU\u5185\u5b9e\u73b0\u89e3\u8026\u3002", "method": "\u901a\u8fc7\u5206\u6790GPU\u8d44\u6e90\u7684\u9012\u51cf\u6548\u5e94\uff0c\u52a8\u6001\u5206\u914d\u5355\u4e2aGPU\u8d44\u6e90\u7ed9\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5b9e\u73b0\u89e3\u8026\u3002", "result": "Nexus\u7cfb\u7edf\u5728\u591a\u79cd\u6a21\u578b\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53472.2\u500d\uff0cTTFT\u964d\u4f4e20\u500d\uff0cTBT\u964d\u4f4e2.5\u500d\uff0c\u4e14\u4ec5\u7528\u4e00\u534aGPU\u6570\u91cf\u3002", "conclusion": "\u5728\u5355\u4e2aGPU\u5185\u52a8\u6001\u8d44\u6e90\u5206\u914d\u80fd\u6709\u6548\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u786c\u4ef6\u9700\u6c42\u3002"}}
{"id": "2507.06653", "pdf": "https://arxiv.org/pdf/2507.06653", "abs": "https://arxiv.org/abs/2507.06653", "authors": ["Xiangyu Zhi", "Meng Chen", "Xiao Yan", "Baotong Lu", "Hui Li", "Qianxi Zhang", "Qi Chen", "James Cheng"], "title": "Towards Efficient and Scalable Distributed Vector Search with RDMA", "categories": ["cs.DC"], "comment": null, "summary": "Similarity-based vector search facilitates many important applications such\nas search and recommendation but is limited by the memory capacity and\nbandwidth of a single machine due to large datasets and intensive data read. In\nthis paper, we present CoTra, a system that scales up vector search for\ndistributed execution. We observe a tension between computation and\ncommunication efficiency, which is the main challenge for good scalability,\ni.e., handling the local vectors on each machine independently blows up\ncomputation as the pruning power of vector index is not fully utilized, while\nrunning a global index over all machines introduces rich data dependencies and\nthus extensive communication. To resolve such tension, we leverage the fact\nthat vector search is approximate in nature and robust to asynchronous\nexecution. In particular, we run collaborative vector search over the machines\nwith algorithm-system co-designs including clustering-based data partitioning\nto reduce communication, asynchronous execution to avoid communication stall,\nand task push to reduce network traffic. To make collaborative search\nefficient, we introduce a suite of system optimizations including task\nscheduling, communication batching, and storage format. We evaluate CoTra on\nreal datasets and compare with four baselines. The results show that when using\n16 machines, the query throughput of CoTra scales to 9.8-13.4x over a single\nmachine and is 2.12-3.58x of the best-performing baseline at 0.95 recall@10.", "AI": {"tldr": "CoTra\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u8ba1\u7b97\u4e0e\u901a\u4fe1\u6548\u7387\u7684\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u541e\u5410\u91cf\u3002", "motivation": "\u5355\u673a\u5185\u5b58\u548c\u5e26\u5bbd\u9650\u5236\u4e86\u5927\u5411\u91cf\u6570\u636e\u96c6\u7684\u641c\u7d22\u6548\u7387\uff0c\u9700\u8981\u5206\u5e03\u5f0f\u6267\u884c\u4ee5\u6269\u5c55\u641c\u7d22\u80fd\u529b\u3002", "method": "\u91c7\u7528\u805a\u7c7b\u6570\u636e\u5206\u533a\u3001\u5f02\u6b65\u6267\u884c\u548c\u4efb\u52a1\u63a8\u9001\u7b49\u6280\u672f\uff0c\u7ed3\u5408\u4efb\u52a1\u8c03\u5ea6\u3001\u901a\u4fe1\u6279\u5904\u7406\u548c\u5b58\u50a8\u683c\u5f0f\u4f18\u5316\u3002", "result": "\u572816\u53f0\u673a\u5668\u4e0a\uff0cCoTra\u7684\u67e5\u8be2\u541e\u5410\u91cf\u6bd4\u5355\u673a\u63d0\u53479.8-13.4\u500d\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad82.12-3.58\u500d\u3002", "conclusion": "CoTra\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u5411\u91cf\u641c\u7d22\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002"}}
{"id": "2507.06343", "pdf": "https://arxiv.org/pdf/2507.06343", "abs": "https://arxiv.org/abs/2507.06343", "authors": ["Huynh Khanh Vi Tran", "Nauman bin Ali", "Michael Unterkalmsteiner", "J\u00fcrgen B\u00f6rstler", "Panagiota Chatzipetrou"], "title": "Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives", "categories": ["cs.SE"], "comment": null, "summary": "Context: The quality of the test suites and the constituent test cases\nsignificantly impacts confidence in software testing. While research has\nidentified several quality attributes of test cases and test suites, there is a\nneed for a better understanding of their relative importance in practice.\nObjective: We investigate practitioners' perceptions regarding the relative\nimportance of quality attributes of test cases and test suites and the\nchallenges they face in ensuring the perceived important quality attributes.\nMethod: We conducted an industrial survey using a questionnaire based on the\nquality attributes identified in an extensive literature review. We used a\nsampling strategy that leverages LinkedIn to draw a large and heterogeneous\nsample of professionals with experience in software testing. Results: We\ncollected 354 responses from practitioners with a wide range of experience. We\nfound that the majority of practitioners rated Fault Detection, Usability,\nMaintainability, Reliability, and Coverage to be the most important quality\nattributes. Resource Efficiency, Reusability, and Simplicity received the most\ndivergent opinions, which, according to our analysis, depend on the\nsoftware-testing contexts. We identified common challenges that apply to the\nimportant attributes, namely inadequate definition, lack of useful metrics,\nlack of an established review process, and lack of external support.\nConclusion: The findings point out where practitioners actually need further\nsupport with respect to achieving high-quality test cases and test suites under\ndifferent software testing contexts. The findings can serve as a guideline for\nacademic researchers when looking for research directions on the topic. The\nfindings can also be used to encourage companies to provide more support to\npractitioners to achieve high-quality test cases and test suites.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u5b9e\u8df5\u8005\u5bf9\u6d4b\u8bd5\u7528\u4f8b\u548c\u6d4b\u8bd5\u5957\u4ef6\u8d28\u91cf\u5c5e\u6027\u7684\u91cd\u8981\u6027\u8ba4\u77e5\u53ca\u9762\u4e34\u7684\u6311\u6218\uff0c\u53d1\u73b0\u6545\u969c\u68c0\u6d4b\u3001\u53ef\u7528\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u53ef\u9760\u6027\u548c\u8986\u76d6\u7387\u662f\u6700\u91cd\u8981\u7684\u5c5e\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u8d44\u6e90\u6548\u7387\u3001\u53ef\u91cd\u7528\u6027\u548c\u7b80\u5355\u6027\u7684\u610f\u89c1\u5206\u6b67\u3002", "motivation": "\u7406\u89e3\u5b9e\u8df5\u8005\u5bf9\u6d4b\u8bd5\u7528\u4f8b\u548c\u6d4b\u8bd5\u5957\u4ef6\u8d28\u91cf\u5c5e\u6027\u7684\u91cd\u8981\u6027\u8ba4\u77e5\u53ca\u5b9e\u9645\u6311\u6218\uff0c\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u6587\u732e\u7efc\u8ff0\u7684\u95ee\u5377\u8fdb\u884c\u5de5\u4e1a\u8c03\u67e5\uff0c\u5229\u7528LinkedIn\u62bd\u6837\u7b56\u7565\u83b7\u53d6\u591a\u6837\u5316\u7684\u4e13\u4e1a\u4eba\u5458\u6837\u672c\u3002", "result": "354\u4efd\u56de\u590d\u663e\u793a\u6545\u969c\u68c0\u6d4b\u7b49\u5c5e\u6027\u6700\u91cd\u8981\uff0c\u8d44\u6e90\u6548\u7387\u7b49\u5c5e\u6027\u610f\u89c1\u5206\u6b67\uff0c\u5e76\u8bc6\u522b\u4e86\u5e38\u89c1\u6311\u6218\u5982\u5b9a\u4e49\u4e0d\u8db3\u3001\u7f3a\u4e4f\u6307\u6807\u7b49\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u5b66\u672f\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6307\u5357\uff0c\u5e76\u9f13\u52b1\u4f01\u4e1a\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u66f4\u591a\u652f\u6301\u3002"}}
{"id": "2507.06258", "pdf": "https://arxiv.org/pdf/2507.06258", "abs": "https://arxiv.org/abs/2507.06258", "authors": ["Bo Yan", "Yurong Hao", "Dingqi Liu", "Huabin Sun", "Pengpeng Qiao", "Wei Yang Bryan Lim", "Yang Cao", "Chuan Shi"], "title": "Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.IR"], "comment": "13 pages", "summary": "Federated recommender systems (FedRec) have emerged as a promising solution\nfor delivering personalized recommendations while safeguarding user privacy.\nHowever, recent studies have demonstrated their vulnerability to poisoning\nattacks. Existing attacks typically target the entire user group, which\ncompromises stealth and increases the risk of detection. In contrast,\nreal-world adversaries may prefer to prompt target items to specific user\nsubgroups, such as recommending health supplements to elderly users. Motivated\nby this gap, we introduce Spattack, the first targeted poisoning attack\ndesigned to manipulate recommendations for specific user subgroups in the\nfederated setting. Specifically, Spattack adopts a two-stage\napproximation-and-promotion strategy, which first simulates user embeddings of\ntarget/non-target subgroups and then prompts target items to the target\nsubgroups. To enhance the approximation stage, we push the inter-group\nembeddings away based on contrastive learning and augment the target group's\nrelevant item set based on clustering. To enhance the promotion stage, we\nfurther propose to adaptively tune the optimization weights between target and\nnon-target subgroups. Besides, an embedding alignment strategy is proposed to\nalign the embeddings between the target items and the relevant items. We\nconduct comprehensive experiments on three real-world datasets, comparing\nSpattack against seven state-of-the-art poisoning attacks and seven\nrepresentative defense mechanisms. Experimental results demonstrate that\nSpattack consistently achieves strong manipulation performance on the specific\nuser subgroup, while incurring minimal impact on non-target users, even when\nonly 0.1\\% of users are malicious. Moreover, Spattack maintains competitive\noverall recommendation performance and exhibits strong resilience against\nexisting mainstream defenses.", "AI": {"tldr": "Spattack\u662f\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u5b9a\u5411\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\uff08\u8fd1\u4f3c\u548c\u63a8\u5e7f\uff09\u64cd\u7eb5\u7279\u5b9a\u7528\u6237\u5b50\u7ec4\u7684\u63a8\u8350\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5176\u4ed6\u7528\u6237\u7684\u5f71\u54cd\u6700\u5c0f\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u6295\u6bd2\u653b\u51fb\u901a\u5e38\u9488\u5bf9\u6574\u4e2a\u7528\u6237\u7fa4\u4f53\uff0c\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u5b50\u7ec4\u7684\u9488\u5bf9\u6027\uff0c\u800c\u73b0\u5b9e\u4e2d\u7684\u653b\u51fb\u8005\u53ef\u80fd\u66f4\u503e\u5411\u4e8e\u9488\u5bf9\u7279\u5b9a\u5b50\u7ec4\uff08\u5982\u8001\u5e74\u7528\u6237\uff09\u3002", "method": "Spattack\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a1\uff09\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\u6a21\u62df\u76ee\u6807/\u975e\u76ee\u6807\u5b50\u7ec4\u7684\u7528\u6237\u5d4c\u5165\uff1b2\uff09\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u6743\u91cd\u548c\u5d4c\u5165\u5bf9\u9f50\u7b56\u7565\u63a8\u5e7f\u76ee\u6807\u9879\u76ee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpattack\u5728\u7279\u5b9a\u7528\u6237\u5b50\u7ec4\u4e0a\u5177\u6709\u5f3a\u64cd\u7eb5\u6548\u679c\uff0c\u5bf9\u5176\u4ed6\u7528\u6237\u5f71\u54cd\u6781\u5c0f\uff0c\u4e14\u80fd\u62b5\u6297\u4e3b\u6d41\u9632\u5fa1\u673a\u5236\u3002", "conclusion": "Spattack\u586b\u8865\u4e86\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9a\u5411\u6295\u6bd2\u653b\u51fb\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u548c\u9690\u853d\u6027\u3002"}}
{"id": "2507.06354", "pdf": "https://arxiv.org/pdf/2507.06354", "abs": "https://arxiv.org/abs/2507.06354", "authors": ["Huynh Khanh Vi Tran", "Nauman bin Ali", "Michael Unterkalmsteiner", "J\u00fcrgen B\u00f6rstler"], "title": "A proposal and assessment of an improved heuristic for the Eager Test smell detection", "categories": ["cs.SE"], "comment": null, "summary": "Context: The evidence for the prevalence of test smells at the unit testing\nlevel has relied on the accuracy of detection tools, which have seen intense\nresearch in the last two decades. The Eager Test smell, one of the most\nprevalent, is often identified using simplified detection rules that\npractitioners find inadequate. Objective: We aim to improve the rules for\ndetecting the Eager Test smell. Method: We reviewed the literature on test\nsmells to analyze the definitions and detection rules of the Eager Test smell.\nWe proposed a novel, unambiguous definition of the test smell and a heuristic\nto address the limitations of the existing rules. We evaluated our heuristic\nagainst existing detection rules by manually applying it to 300 unit test cases\nin Java. Results: Our review identified 56 relevant studies. We found that\ninadequate interpretations of original definitions of the Eager Test smell led\nto imprecise detection rules, resulting in a high level of disagreement in\ndetection outcomes. Also, our heuristic detected patterns of eager and\nnon-eager tests that existing rules missed. Conclusion: Our heuristic captures\nthe essence of the Eager Test smell more precisely; hence, it may address\npractitioners' concerns regarding the adequacy of existing detection rules.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u68c0\u6d4bEager Test smell\u7684\u89c4\u5219\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9a\u4e49\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u89c4\u5219\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4bEager Test smell\u7684\u89c4\u5219\u5b58\u5728\u4e0d\u51c6\u786e\u548c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u8df5\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5206\u6790Eager Test smell\u7684\u5b9a\u4e49\u548c\u68c0\u6d4b\u89c4\u5219\uff0c\u63d0\u51fa\u65b0\u7684\u5b9a\u4e49\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5728300\u4e2aJava\u5355\u5143\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u624b\u52a8\u9a8c\u8bc1\u3002", "result": "\u6587\u732e\u7efc\u8ff0\u53d1\u73b056\u9879\u76f8\u5173\u7814\u7a76\uff0c\u65b0\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u68c0\u6d4bEager Test smell\uff0c\u5f25\u8865\u73b0\u6709\u89c4\u5219\u7684\u4e0d\u8db3\u3002", "conclusion": "\u65b0\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349Eager Test smell\u7684\u672c\u8d28\uff0c\u6709\u671b\u89e3\u51b3\u5b9e\u8df5\u4e2d\u5bf9\u73b0\u6709\u89c4\u5219\u4e0d\u8db3\u7684\u62c5\u5fe7\u3002"}}
{"id": "2507.06334", "pdf": "https://arxiv.org/pdf/2507.06334", "abs": "https://arxiv.org/abs/2507.06334", "authors": ["Mohsen Ghaffari", "Jaehyun Koo"], "title": "Parallel Batch-Dynamic Coreness Decomposition with Worst-Case Guarantees", "categories": ["cs.DS"], "comment": "SPAA 2025", "summary": "We present the first parallel batch-dynamic algorithm for approximating\ncoreness decomposition with worst-case update times. Given any batch of edge\ninsertions and deletions, our algorithm processes all these updates in $\n\\text{poly}(\\log n)$ depth, using a worst-case work bound of $b\\cdot\n\\text{poly}(\\log n)$ where $b$ denotes the batch size. This means the batch\ngets processed in $\\tilde{O}(b/p)$ time, given $p$ processors, which is optimal\nup to logarithmic factors. Previously, an algorithm with similar guarantees was\nknown by the celebrated work of Liu, Shi, Yu, Dhulipala, and Shun [SPAA'22],\nbut with the caveat of the work bound, and thus the runtime, being only\namortized.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5e76\u884c\u6279\u91cf\u52a8\u6001\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u6838\u5fc3\u5206\u89e3\uff0c\u5177\u6709\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u66f4\u65b0\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u4ec5\u652f\u6301\u644a\u9500\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u6279\u91cf\u5904\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5e76\u884c\u7b97\u6cd5\uff0c\u5904\u7406\u6279\u91cf\u8fb9\u63d2\u5165\u548c\u5220\u9664\uff0c\u786e\u4fdd\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u5de5\u4f5c\u91cf\u548c\u6df1\u5ea6\u5747\u4e3a\u591a\u9879\u5f0f\u5bf9\u6570\u7ea7\u522b\u3002", "result": "\u7b97\u6cd5\u5728\u7ed9\u5b9a\u5904\u7406\u5668\u6570\u91cf\u4e0b\uff0c\u6279\u91cf\u5904\u7406\u65f6\u95f4\u63a5\u8fd1\u6700\u4f18\uff0c\u4ec5\u5dee\u5bf9\u6570\u56e0\u5b50\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u7406\u8bba\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u6279\u91cf\u52a8\u6001\u6838\u5fc3\u5206\u89e3\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.06267", "pdf": "https://arxiv.org/pdf/2507.06267", "abs": "https://arxiv.org/abs/2507.06267", "authors": ["Hyeontae Jo", "Kre\u0161imir Josi\u0107", "Jae Kyoung Kim"], "title": "Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals", "categories": ["cs.LG", "34C60, 92B05, 68T07, 93C15, 65K10"], "comment": null, "summary": "Non-autonomous differential equations are crucial for modeling systems\ninfluenced by external signals, yet fitting these models to data becomes\nparticularly challenging when the signals change abruptly. To address this\nproblem, we propose a novel parameter estimation method utilizing functional\napproximations with artificial neural networks. Our approach, termed Harmonic\nApproximation of Discontinuous External Signals using Neural Networks\n(HADES-NN), operates in two iterated stages. In the first stage, the algorithm\nemploys a neural network to approximate the discontinuous signal with a smooth\nfunction. In the second stage, it uses this smooth approximate signal to\nestimate model parameters. HADES-NN gives highly accurate and precise parameter\nestimates across various applications, including circadian clock systems\nregulated by external light inputs measured via wearable devices and the mating\nresponse of yeast to external pheromone signals. HADES-NN greatly extends the\nrange of model systems that can be fit to real-world measurements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHADES-NN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u975e\u81ea\u6cbb\u5fae\u5206\u65b9\u7a0b\u7684\u53c2\u6570\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5e73\u6ed1\u5904\u7406\u4e0d\u8fde\u7eed\u7684\u5916\u90e8\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u5347\u53c2\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u7cbe\u5ea6\u3002", "motivation": "\u975e\u81ea\u6cbb\u5fae\u5206\u65b9\u7a0b\u5728\u5efa\u6a21\u53d7\u5916\u90e8\u4fe1\u53f7\u5f71\u54cd\u7684\u7cfb\u7edf\u65f6\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5f53\u4fe1\u53f7\u7a81\u53d8\u65f6\uff0c\u6a21\u578b\u62df\u5408\u53d8\u5f97\u56f0\u96be\u3002", "method": "HADES-NN\u5206\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u7528\u795e\u7ecf\u7f51\u7edc\u5c06\u4e0d\u8fde\u7eed\u4fe1\u53f7\u5e73\u6ed1\u5316\uff0c\u518d\u7528\u5e73\u6ed1\u540e\u7684\u4fe1\u53f7\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u3002", "result": "HADES-NN\u5728\u591a\u79cd\u5e94\u7528\u4e2d\uff08\u5982\u663c\u591c\u8282\u5f8b\u7cfb\u7edf\u548c\u9175\u6bcd\u4ea4\u914d\u54cd\u5e94\uff09\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "HADES-NN\u663e\u8457\u6269\u5c55\u4e86\u53ef\u62df\u5408\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u7684\u6a21\u578b\u7cfb\u7edf\u8303\u56f4\u3002"}}
{"id": "2507.06467", "pdf": "https://arxiv.org/pdf/2507.06467", "abs": "https://arxiv.org/abs/2507.06467", "authors": ["Luyu Qiu", "Jianing Li", "Chi Su", "Lei Chen"], "title": "Interactive Text-to-SQL via Expected Information Gain for Disambiguation", "categories": ["cs.DB"], "comment": "13 pages, 5 figure", "summary": "Relational databases are foundational to numerous domains, including business\nintelligence, scientific research, and enterprise systems. However, accessing\nand analyzing structured data often requires proficiency in SQL, which is a\nskill that many end users lack. With the development of Natural Language\nProcessing (NLP) technology, the Text-to-SQL systems attempt to bridge this gap\nby translating natural language questions into executable SQL queries via an\nautomated algorithm. Yet, when operating on complex real-world databases, the\nText-to-SQL systems often suffer from ambiguity due to natural ambiguity in\nnatural language queries. These ambiguities pose a significant challenge for\nexisting Text-to-SQL translation systems, which tend to commit early to a\npotentially incorrect interpretation. To address this, we propose an\ninteractive Text-to-SQL framework that models SQL generation as a probabilistic\nreasoning process over multiple candidate queries. Rather than producing a\nsingle deterministic output, our system maintains a distribution over possible\nSQL outputs and seeks to resolve uncertainty through user interaction. At each\ninteraction step, the system selects a branching decision and formulates a\nclarification question aimed at disambiguating that aspect of the query.\nCrucially, we adopt a principled decision criterion based on Expected\nInformation Gain to identify the clarification that will, in expectation, most\nreduce the uncertainty in the SQL distribution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0fText-to-SQL\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u63a8\u7406\u548c\u591a\u5019\u9009\u67e5\u8be2\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u5229\u7528\u7528\u6237\u4ea4\u4e92\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u5e93\u65f6\u56e0\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\u6613\u4ea7\u751f\u9519\u8bef\uff0c\u7f3a\u4e4f\u4ea4\u4e92\u673a\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6982\u7387\u63a8\u7406\u7684\u6846\u67b6\uff0c\u751f\u6210\u591a\u4e2a\u5019\u9009SQL\u67e5\u8be2\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u548c\u671f\u671b\u4fe1\u606f\u589e\u76ca\u9009\u62e9\u6700\u4f73\u6f84\u6e05\u95ee\u9898\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u52a8\u6001\u51cf\u5c11SQL\u67e5\u8be2\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\u95ee\u9898\uff0c\u63d0\u5347\u4e86Text-to-SQL\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.06430", "pdf": "https://arxiv.org/pdf/2507.06430", "abs": "https://arxiv.org/abs/2507.06430", "authors": ["Elham Akbari", "Zihao Zhou", "Mohammad Ali Salahuddin", "Noura Limam", "Raouf Boutaba", "Bertrand Mathieu", "Stephanie Moteau", "Stephane Tuffin"], "title": "One task to rule them all: A closer look at traffic classification generalizability", "categories": ["cs.NI"], "comment": null, "summary": "Existing website fingerprinting and traffic classification solutions do not\nwork well when the evaluation context changes, as their performances often\nheavily rely on context-specific assumptions. To clarify this problem, we take\nthree prior solutions presented for different but similar traffic\nclassification and website fingerprinting tasks, and apply each solution's\nmodel to another solution's dataset. We pinpoint dataset-specific and\nmodel-specific properties that lead each of them to overperform in their\nspecific evaluation context.\n  As a realistic evaluation context that takes practical labeling constraints\ninto account, we design an evaluation framework using two recent real-world TLS\ntraffic datasets from large-scale networks. The framework simulates a\nfuturistic scenario in which SNIs are hidden in some networks but not in\nothers, and the classifier's goal is to predict destination services in one\nnetwork's traffic, having been trained on a labelled dataset collected from a\ndifferent network. Our framework has the distinction of including real-world\ndistribution shift, while excluding concept drift. We show that, even when\nabundant labeled data is available, the best solutions' performances under\ndistribution shift are between 30% and 40%, and a simple 1-Nearest Neighbor\nclassifier's performance is not far behind. We depict all performances measured\non different models, not just the best ones, for a fair representation of\ntraffic models in practice.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u73b0\u6709\u7f51\u7ad9\u6307\u7eb9\u548c\u6d41\u91cf\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u5728\u8bc4\u4f30\u73af\u5883\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u5b9e\u9645\u6807\u7b7e\u7ea6\u675f\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e0a\u4e0b\u6587\u5047\u8bbe\uff0c\u5bfc\u81f4\u5728\u8bc4\u4f30\u73af\u5883\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5c06\u4e09\u79cd\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6a21\u578b\u5e94\u7528\u4e8e\u5f7c\u6b64\u7684\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u7684\u7279\u5b9a\u5c5e\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u62df\u73b0\u5b9e\u6807\u7b7e\u7ea6\u675f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u4ec5\u4e3a30%-40%\uff0c\u800c\u7b80\u5355\u76841-Nearest Neighbor\u5206\u7c7b\u5668\u8868\u73b0\u63a5\u8fd1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8003\u8651\u5206\u5e03\u504f\u79fb\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u5b9e\u9645\u6d41\u91cf\u6a21\u578b\u6027\u80fd\u7684\u516c\u5e73\u8bc4\u4f30\u3002"}}
{"id": "2507.06449", "pdf": "https://arxiv.org/pdf/2507.06449", "abs": "https://arxiv.org/abs/2507.06449", "authors": ["Qianyu Long", "Qiyuan Wang", "Christos Anagnostopoulos", "Daning Bi"], "title": "FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.DC", "68T05, 68T07, 68Q85, 94A08", "I.2.6; I.2.11; C.2.4"], "comment": "12 pages, 8 figures, 5 tables. This paper introduces FedPhD, a novel\n  hierarchical federated learning framework for training diffusion models that\n  addresses data heterogeneity and communication costs through\n  homogeneity-aware aggregation and structured pruning. Submitted to IEEE\n  Transactions on Cybernetics and is under review", "summary": "Federated Learning (FL), as a distributed learning paradigm, trains models\nover distributed clients' data. FL is particularly beneficial for distributed\ntraining of Diffusion Models (DMs), which are high-quality image generators\nthat require diverse data. However, challenges such as high communication costs\nand data heterogeneity persist in training DMs similar to training Transformers\nand Convolutional Neural Networks. Limited research has addressed these issues\nin FL environments. To address this gap and challenges, we introduce a novel\napproach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD\nleverages Hierarchical FL with homogeneity-aware model aggregation and\nselection policy to tackle data heterogeneity while reducing communication\ncosts. The distributed structured pruning of FedPhD enhances computational\nefficiency and reduces model storage requirements in clients. Our experiments\nacross multiple datasets demonstrate that FedPhD achieves high model\nperformance regarding Fr\\'echet Inception Distance (FID) scores while reducing\ncommunication costs by up to $88\\%$. FedPhD outperforms baseline methods\nachieving at least a $34\\%$ improvement in FID, while utilizing only $56\\%$ of\nthe total computation and communication resources.", "AI": {"tldr": "FedPhD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6548\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08DMs\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u548c\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u65f6\u9762\u4e34\u6570\u636e\u5f02\u8d28\u6027\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u6d89\u53ca\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "FedPhD\u91c7\u7528\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff0c\u7ed3\u5408\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u548c\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u53ca\u5206\u5e03\u5f0f\u7ed3\u6784\u5316\u526a\u679d\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedPhD\u5728FID\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e88%\uff0c\u8ba1\u7b97\u548c\u901a\u4fe1\u8d44\u6e90\u4ec5\u970056%\uff0cFID\u6539\u8fdb\u81f3\u5c1134%\u3002", "conclusion": "FedPhD\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u9ad8\u6548\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2507.06463", "pdf": "https://arxiv.org/pdf/2507.06463", "abs": "https://arxiv.org/abs/2507.06463", "authors": ["Atieh Barati Nia", "Mohammad Dindoost", "David A. Bader"], "title": "Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate software\ndevelopment, yet most prior evaluations focus on functional correctness or\nhigh-level languages such as Python. We present the first systematic study of\nLLMs' ability to generate efficient C implementations of graph-analysis\nroutines--code that must satisfy the stringent runtime and memory constraints.\nEight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic\nClaude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok\n3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.\nThe first approach checks the ability of LLMs in generating an algorithm\noutperforming other present algorithms in the benchmark. The second approach\nevaluates the ability of LLMs to generate graph algorithms for integration into\nthe benchmark. Results show that Claude Sonnet 4 Extended achieves the best\nresult in the case of ready-to-use code generation and efficiency,\noutperforming human-written baselines in triangle counting. The study confirms\nthat contemporary LLMs excel at optimizing and integrating established\nalgorithms but not inventing novel techniques. We provide prompts, the first\napproach's generated code, and measurement scripts to foster reproducible\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86LLMs\u751f\u6210\u9ad8\u6548C\u8bed\u8a00\u56fe\u5206\u6790\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u4e868\u79cd\u5148\u8fdb\u6a21\u578b\uff0c\u53d1\u73b0Claude Sonnet 4 Extended\u5728\u751f\u6210\u9ad8\u6548\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6ee1\u8db3\u4e25\u683c\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u7ea6\u675f\u4e0b\u751f\u6210\u9ad8\u6548C\u8bed\u8a00\u56fe\u5206\u6790\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u8bc4\u4f30LLMs\u751f\u6210\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u7684\u4ee3\u7801\uff1b2) \u8bc4\u4f30LLMs\u751f\u6210\u53ef\u96c6\u6210\u5230\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u56fe\u7b97\u6cd5\u3002", "result": "Claude Sonnet 4 Extended\u5728\u751f\u6210\u9ad8\u6548\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u5de5\u7f16\u5199\u7684\u57fa\u7ebf\u3002", "conclusion": "\u5f53\u4ee3LLMs\u64c5\u957f\u4f18\u5316\u548c\u96c6\u6210\u73b0\u6709\u7b97\u6cd5\uff0c\u4f46\u4e0d\u64c5\u957f\u53d1\u660e\u65b0\u6280\u672f\u3002"}}
{"id": "2507.06338", "pdf": "https://arxiv.org/pdf/2507.06338", "abs": "https://arxiv.org/abs/2507.06338", "authors": ["Mohsen Ghaffari", "Jaehyun Koo"], "title": "Parallel Batch-Dynamic Algorithms for Spanners, and Extensions", "categories": ["cs.DS"], "comment": "SPAA 2025", "summary": "This paper presents the first parallel batch-dynamic algorithms for computing\nspanners and sparsifiers. Our algorithms process any batch of edge insertions\nand deletions in an $n$-node undirected graph, in $\\text{poly}(\\log n)$ depth\nand using amortized work near-linear in the batch size. Our concrete results\nare as follows:\n  - Our base algorithm maintains a spanner with $(2k-1)$ stretch and\n$\\tilde{O}(n^{1+1/k})$ edges, for any $k\\geq 1$.\n  - Our first extension maintains a sparse spanner with only $O(n)$ edges, and\n$\\tilde{O}(\\log n)$ stretch.\n  - Our second extension maintains a $t$-bundle of spanners -- i.e., $t$\nspanners, each of which is the spanner of the graph remaining after removing\nthe previous ones -- and allows us to maintain cut/spectral sparsifiers with\n$\\tilde{O}(n)$ edges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5e76\u884c\u6279\u91cf\u52a8\u6001\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u56fe\u7684\u751f\u6210\u5668\u548c\u7a00\u758f\u5316\u5668\uff0c\u652f\u6301\u9ad8\u6548\u7684\u8fb9\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u56fe\u4e2d\u9ad8\u6548\u7ef4\u62a4\u751f\u6210\u5668\u548c\u7a00\u758f\u5316\u5668\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u56fe\u5206\u6790\u7684\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u6279\u91cf\u52a8\u6001\u7b97\u6cd5\uff0c\u5904\u7406\u8fb9\u63d2\u5165\u548c\u5220\u9664\uff0c\u4f7f\u7528\u591a\u5bf9\u6570\u6df1\u5ea6\u548c\u63a5\u8fd1\u7ebf\u6027\u7684\u644a\u9500\u5de5\u4f5c\u91cf\u3002", "result": "1. \u7ef4\u62a4(2k-1)\u62c9\u4f38\u7684\u751f\u6210\u5668\uff1b2. \u7ef4\u62a4\u7a00\u758f\u751f\u6210\u5668\uff1b3. \u7ef4\u62a4t-\u675f\u751f\u6210\u5668\u4ee5\u652f\u6301\u7a00\u758f\u5316\u5668\u3002", "conclusion": "\u7b97\u6cd5\u5728\u52a8\u6001\u56fe\u4e2d\u9ad8\u6548\u7ef4\u62a4\u751f\u6210\u5668\u548c\u7a00\u758f\u5316\u5668\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.06326", "pdf": "https://arxiv.org/pdf/2507.06326", "abs": "https://arxiv.org/abs/2507.06326", "authors": ["Harsh Ravivarapu", "Gaurav Bagwe", "Xiaoyong Yuan", "Chunxiu Yu", "Lan Zhang"], "title": "Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "q-bio.NC"], "comment": "Accepted by IEEE IMC 2025", "summary": "Deep brain stimulation (DBS) is an established intervention for Parkinson's\ndisease (PD), but conventional open-loop systems lack adaptability, are\nenergy-inefficient due to continuous stimulation, and provide limited\npersonalization to individual neural dynamics. Adaptive DBS (aDBS) offers a\nclosed-loop alternative, using biomarkers such as beta-band oscillations to\ndynamically modulate stimulation. While reinforcement learning (RL) holds\npromise for personalized aDBS control, existing methods suffer from high sample\ncomplexity, unstable exploration in binary action spaces, and limited\ndeployability on resource-constrained hardware.\n  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses\nthe core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a\npredictive reward model to reduce reliance on real-time feedback and employs\nGumbel Softmax-based exploration for stable, differentiable policy updates in\nbinary action spaces. Together, these components improve sample efficiency,\nexploration robustness, and compatibility with resource-constrained\nneuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic\nsimulation of Parkinsonian basal ganglia activity, demonstrating faster\nconvergence, stronger suppression of pathological beta-band power, and\nresilience to post-training FP16 quantization. Our results show that SEA-DBS\noffers a practical and effective RL-based aDBS framework for real-time,\nresource-constrained neuromodulation.", "AI": {"tldr": "SEA-DBS\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6df1\u8111\u523a\u6fc0\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u5956\u52b1\u6a21\u578b\u548cGumbel Softmax\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u6837\u672c\u6548\u7387\u4f4e\u548c\u786c\u4ef6\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f00\u73afDBS\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\uff0c\u800c\u73b0\u6709RL\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u548c\u786c\u4ef6\u517c\u5bb9\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "SEA-DBS\u7ed3\u5408\u9884\u6d4b\u5956\u52b1\u6a21\u578b\u548cGumbel Softmax\u63a2\u7d22\uff0c\u4f18\u5316\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u5e15\u91d1\u68ee\u75c5\u57fa\u5e95\u8282\u6a21\u62df\u4e2d\uff0cSEA-DBS\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u75c5\u7406\u03b2\u6ce2\u6bb5\u6291\u5236\u80fd\u529b\u3002", "conclusion": "SEA-DBS\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u795e\u7ecf\u8c03\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684RL\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06515", "pdf": "https://arxiv.org/pdf/2507.06515", "abs": "https://arxiv.org/abs/2507.06515", "authors": ["Zhaoze Sun", "Qiyan Deng", "Chengliang Chai", "Kaisen Jin", "Xinyu Guo", "Han Han", "Ye Yuan", "Guoren Wang", "Lei Cao"], "title": "QUEST: Query Optimization in Unstructured Document Analysis", "categories": ["cs.DB"], "comment": null, "summary": "Most recently, researchers have started building large language models (LLMs)\npowered data systems that allow users to analyze unstructured text documents\nlike working with a database because LLMs are very effective in extracting\nattributes from documents. In such systems, LLM-based extraction operations\nconstitute the performance bottleneck of query execution due to the high\nmonetary cost and slow LLM inference. Existing systems typically borrow the\nquery optimization principles popular in relational databases to produce query\nexecution plans, which unfortunately are ineffective in minimizing LLM cost. To\nfill this gap, we propose QUEST, which features a bunch of novel optimization\nstrategies for unstructured document analysis. First, we introduce an\nindex-based strategy to minimize the cost of each extraction operation. With\nthis index, QUEST quickly retrieves the text segments relevant to the target\nattributes and only feeds them to LLMs. Furthermore, we design an\nevidence-augmented retrieval strategy to reduce the possibility of missing\nrelevant segments. Moreover, we develop an instance-optimized query execution\nstrategy: because the attribute extraction cost could vary significantly\ndocument by document, QUEST produces different plans for different documents.\nFor each document, QUEST produces a plan to minimize the frequency of attribute\nextraction. The innovations include LLM cost-aware operator ordering strategies\nand an optimized join execution approach that transforms joins into filters.\nExtensive experiments on 3 real-world datasets demonstrate the superiority of\nQUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%\ncompared with state-of-the-art baselines.", "AI": {"tldr": "QUEST\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u7ed3\u6784\u5316\u6587\u6863\u5206\u6790\u7684\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u7d22\u5f15\u3001\u8bc1\u636e\u589e\u5f3a\u68c0\u7d22\u548c\u5b9e\u4f8b\u4f18\u5316\u67e5\u8be2\u6267\u884c\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4eLLM\u6210\u672c\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u6863\u65f6\uff0cLLM\u63d0\u53d6\u64cd\u4f5c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u4f20\u7edf\u6570\u636e\u5e93\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u964d\u4f4eLLM\u6210\u672c\u3002", "method": "QUEST\u5f15\u5165\u7d22\u5f15\u7b56\u7565\u51cf\u5c11\u63d0\u53d6\u64cd\u4f5c\u6210\u672c\uff0c\u8bbe\u8ba1\u8bc1\u636e\u589e\u5f3a\u68c0\u7d22\u7b56\u7565\u907f\u514d\u9057\u6f0f\u76f8\u5173\u6587\u672c\uff0c\u5e76\u4e3a\u4e0d\u540c\u6587\u6863\u751f\u6210\u5b9a\u5236\u5316\u67e5\u8be2\u6267\u884c\u8ba1\u5212\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cQUEST\u5b9e\u73b0\u4e8630%-6\u500d\u7684\u6210\u672c\u8282\u7701\uff0cF1\u5206\u6570\u63d0\u534710%-27%\u3002", "conclusion": "QUEST\u901a\u8fc7\u521b\u65b0\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ed3\u6784\u5316\u6587\u6863\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06632", "pdf": "https://arxiv.org/pdf/2507.06632", "abs": "https://arxiv.org/abs/2507.06632", "authors": ["Liyuan Chen", "Kai Xiong", "Yujie Qin", "Hanqing Yu", "Supeng Leng", "Chau Yuen"], "title": "Stacked Intelligent Metasurfaces-Aided eVTOL Delay Sensitive Communications", "categories": ["cs.NI"], "comment": null, "summary": "With rapid urbanization and increasing population density, urban traffic\ncongestion has become a critical issue, and traditional ground transportation\nmethods are no longer sufficient to address it effectively. To tackle this\nchallenge, the concept of Advanced Air Mobility (AAM) has emerged, aiming to\nutilize low-altitude airspace to establish a three-dimensional transportation\nsystem. Among various components of the AAM system, electric vertical take-off\nand landing (eVTOL) aircraft plays a pivotal role due to their flexibility and\nefficiency. However, the immaturity of Ultra Reliable Low Latency Communication\n(URLLC) technologies poses significant challenges to safety-critical AAM\noperations. Specifically, existing Stacked Intelligent Metasurfaces (SIM)-based\neVTOL systems lack rigorous mathematical frameworks to quantify probabilistic\ndelay bounds under dynamic air traffic patterns, a prerequisite for collision\navoidance and airspace management. To bridge this gap, we employ network\ncalculus tools to derive the probabilistic upper bound on communication delay\nin the AAM system for the first time. Furthermore, we formulate a complex\nnon-convex optimization problem that jointly minimizes the probabilistic delay\nbound and the propagation delay. To solve this problem efficiently, we propose\na solution based on the Block Coordinate Descent (BCD) algorithm and\nSemidefinite Relaxation (SDR) method. In addition, we conduct a comprehensive\nanalysis of how various factors impact regret and transmission rate, and\nexplore the influence of varying load intensity and total delay on the\nprobabilistic delay bound.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u6f14\u7b97\u548c\u4f18\u5316\u65b9\u6cd5\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u9ad8\u7ea7\u7a7a\u4e2d\u4ea4\u901a\uff08AAM\uff09\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u6982\u7387\u4e0a\u9650\uff0c\u4ee5\u89e3\u51b3eVTOL\u7cfb\u7edf\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\uff0c\u5730\u9762\u4ea4\u901a\u62e5\u5835\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u800c\u73b0\u6709\u7684AAM\u7cfb\u7edf\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\u6765\u91cf\u5316\u52a8\u6001\u7a7a\u4e2d\u4ea4\u901a\u6a21\u5f0f\u4e0b\u7684\u901a\u4fe1\u5ef6\u8fdf\u6982\u7387\u4e0a\u9650\uff0c\u8fd9\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7f51\u7edc\u6f14\u7b97\u5de5\u5177\u9996\u6b21\u63a8\u5bfcAAM\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u6982\u7387\u4e0a\u9650\uff0c\u5e76\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8054\u5408\u6700\u5c0f\u5316\u5ef6\u8fdf\u6982\u7387\u4e0a\u9650\u548c\u4f20\u64ad\u5ef6\u8fdf\uff0c\u63d0\u51fa\u57fa\u4e8eBCD\u7b97\u6cd5\u548cSDR\u65b9\u6cd5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u79cd\u56e0\u7d20\u5bf9\u5ef6\u8fdf\u6982\u7387\u4e0a\u9650\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u603b\u5ef6\u8fdf\u5bf9\u7ed3\u679c\u7684\u4f5c\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3aAAM\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\u3002"}}
{"id": "2507.06542", "pdf": "https://arxiv.org/pdf/2507.06542", "abs": "https://arxiv.org/abs/2507.06542", "authors": ["Tongtian Zhu", "Tianyu Zhang", "Mingze Wang", "Zhanpeng Zhou", "Can Wang"], "title": "A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning", "categories": ["cs.LG", "cs.DC", "cs.MA", "stat.ML"], "comment": "We discover and theoretically explain why and when a single global\n  parameter merging in decentralized learning can recover the performance of\n  server-based learning, even in highly heterogeneous and\n  communication-constrained environments", "summary": "Decentralized learning provides a scalable alternative to traditional\nparameter-server-based training, yet its performance is often hindered by\nlimited peer-to-peer communication. In this paper, we study how communication\nshould be scheduled over time, including determining when and how frequently\ndevices synchronize. Our empirical results show that concentrating\ncommunication budgets in the later stages of decentralized training markedly\nimproves global generalization. Surprisingly, we uncover that fully connected\ncommunication at the final step, implemented by a single global merging, is\nsufficient to match the performance of server-based training. We further show\nthat low communication in decentralized learning preserves the\n\\textit{mergeability} of local models throughout training. Our theoretical\ncontributions, which explains these phenomena, are first to establish that the\nglobally merged model of decentralized SGD can converge faster than centralized\nmini-batch SGD. Technically, we novelly reinterpret part of the discrepancy\namong local models, which were previously considered as detrimental noise, as\nconstructive components that accelerate convergence. This work challenges the\ncommon belief that decentralized learning generalizes poorly under data\nheterogeneity and limited communication, while offering new insights into model\nmerging and neural network loss landscapes.", "AI": {"tldr": "\u7814\u7a76\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u901a\u4fe1\u8c03\u5ea6\u7684\u4f18\u5316\uff0c\u53d1\u73b0\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u9884\u7b97\u548c\u6700\u7ec8\u5168\u5c40\u5408\u5e76\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6311\u6218\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5728\u6570\u636e\u5f02\u6784\u548c\u6709\u9650\u901a\u4fe1\u4e0b\u8868\u73b0\u4e0d\u4f73\u7684\u5e38\u89c1\u89c2\u70b9\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u867d\u5177\u6269\u5c55\u6027\uff0c\u4f46\u6027\u80fd\u53d7\u9650\u4e8e\u70b9\u5bf9\u70b9\u901a\u4fe1\u3002\u7814\u7a76\u5982\u4f55\u4f18\u5316\u901a\u4fe1\u8c03\u5ea6\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u901a\u4fe1\u8c03\u5ea6\u7b56\u7565\uff0c\u5305\u62ec\u901a\u4fe1\u65f6\u673a\u548c\u9891\u7387\uff0c\u5e76\u63a2\u8ba8\u6a21\u578b\u5408\u5e76\u7684\u53ef\u884c\u6027\u3002", "result": "\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u548c\u6700\u7ec8\u5168\u5c40\u5408\u5e76\u80fd\u5339\u914d\u4e2d\u5fc3\u5316\u8bad\u7ec3\u6027\u80fd\uff0c\u4f4e\u901a\u4fe1\u4e0b\u672c\u5730\u6a21\u578b\u4ecd\u53ef\u5408\u5e76\u3002\u7406\u8bba\u8bc1\u660e\u53bb\u4e2d\u5fc3\u5316SGD\u7684\u5408\u5e76\u6a21\u578b\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5728\u4f18\u5316\u901a\u4fe1\u8c03\u5ea6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff0c\u4e3a\u6a21\u578b\u5408\u5e76\u548c\u635f\u5931\u666f\u89c2\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.06704", "pdf": "https://arxiv.org/pdf/2507.06704", "abs": "https://arxiv.org/abs/2507.06704", "authors": ["Lloyd Montgomery"], "title": "Issue Tracking Ecosystems: Context and Best Practices", "categories": ["cs.SE"], "comment": "300 pages, Dissertation for the doctoral degree Dr. rer. nat. at the\n  Faculty of Mathematics, Informatics, and Natural Sciences, Department of\n  Informatics, University of Hamburg, Hamburg, Germany", "summary": "Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools\nthat support Software Engineering (SE) organisations through the management of\n``issues'', which represent different SE artefacts such as requirements,\ndevelopment tasks, and maintenance items. ITSs also support internal linking\nbetween issues, and external linking to other tools and information sources.\nThis provides SE organisations key forms of documentation, including forwards\nand backwards traceability (e.g., Feature Requests linked to sprint releases\nand code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is\nthe aggregate of the central ITS and the related SE artefacts, stakeholders,\nand processes -- with an emphasis on how these contextual factors interact with\nthe ITS. The quality of ITEs is central to the success of these organisations\nand their software products. There are challenges, however, within ITEs,\nincluding complex networks of interlinked artefacts and diverse workflows.\nWhile ITSs have been the subject of study in SE research for decades, ITEs as a\nwhole need further exploration.\n  In this thesis, I undertake the challenge of understanding ITEs at a broader\nlevel, addressing these questions regarding complexity and diversity. I\ninterviewed practitioners and performed archival analysis on a diverse set of\nITSs. These analyses revealed the context-dependent nature of ITE problems,\nhighlighting the need for context-specific ITE research. While previous work\nhas produced many solutions to specific ITS problems, these solutions are not\nconsistently framed in a context-rich and comparable way, leading to a desire\nfor more aligned solutions across research and practice. To address this\nemergent information and lack of alignment, I created the Best Practice\nOntology for ITEs. <... truncated due to arXiv abstract character limit ...>", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u95ee\u9898\u8ddf\u8e2a\u751f\u6001\u7cfb\u7edf\uff08ITE\uff09\u7684\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u6700\u4f73\u5b9e\u8df5\u672c\u4f53\u4ee5\u89e3\u51b3\u7814\u7a76\u548c\u5b9e\u8df5\u4e2d\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u95ee\u9898\u8ddf\u8e2a\u7cfb\u7edf\uff08ITS\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u751f\u6001\u7cfb\u7edf\uff08ITE\uff09\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u8bbf\u8c08\u4ece\u4e1a\u8005\u548c\u5bf9\u591a\u79cdITS\u8fdb\u884c\u6863\u6848\u5206\u6790\uff0c\u63ed\u793a\u4e86ITE\u95ee\u9898\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u6700\u4f73\u5b9e\u8df5\u672c\u4f53\u3002", "result": "\u7814\u7a76\u53d1\u73b0ITE\u95ee\u9898\u5177\u6709\u9ad8\u5ea6\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u4e00\u81f4\u6027\u548c\u53ef\u6bd4\u6027\uff0c\u9700\u8981\u66f4\u5bf9\u9f50\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u6700\u4f73\u5b9e\u8df5\u672c\u4f53\uff0c\u4e3aITE\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u95ee\u9898\u3002"}}
{"id": "2507.06349", "pdf": "https://arxiv.org/pdf/2507.06349", "abs": "https://arxiv.org/abs/2507.06349", "authors": ["Erin Ransom", "Andrew Lim", "Michael Mitzenmacher"], "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure Design", "categories": ["cs.DS", "cs.AR"], "comment": null, "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b58\u50a8\u62bd\u8c61\u6a21\u578bMQSSD\uff0c\u65e8\u5728\u66f4\u51c6\u786e\u5730\u53cd\u6620\u73b0\u4ee3\u591a\u961f\u5217\u56fa\u6001\u786c\u76d8\u7684\u6027\u80fd\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u4f18\u5316LSM\u6811\u5b58\u50a8\u5f15\u64ce\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u5b58\u50a8\u6027\u80fd\u6a21\u578b\uff08\u5982DAM\u6a21\u578b\uff09\u5728\u73b0\u4ee3\u5b58\u50a8\u786c\u4ef6\u4e0a\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u5df2\u663e\u8457\u964d\u4f4e\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u62bd\u8c61\u6a21\u578b\u6765\u4f18\u5316\u5916\u90e8\u5185\u5b58\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\u8bbe\u8ba1\u3002", "method": "\u57fa\u4e8e\u73b0\u4ee3\u591a\u961f\u5217SSD\u7684\u5173\u952e\u6027\u80fd\u7279\u5f81\uff0c\u63d0\u51faMQSSD\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u786c\u4ef6\u9a8c\u8bc1\u5176\u7279\u6027\u3002\u5c06\u5176\u5e94\u7528\u4e8eLSM\u6811\u5b58\u50a8\u5f15\u64ce\u4f18\u5316\uff0c\u5f3a\u8c03\u5e76\u53d1\u8bbf\u95ee\u7684\u91cd\u8981\u6027\u3002", "result": "MQSSD\u6a21\u578b\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u51c6\u786e\u5730\u62bd\u8c61\u73b0\u4ee3\u786c\u4ef6\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728RocksDB\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "conclusion": "MQSSD\u6a21\u578b\u63d0\u4f9b\u4e86\u5bf9\u73b0\u4ee3\u786c\u4ef6\u66f4\u51c6\u786e\u7684\u62bd\u8c61\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7684\u6d1e\u5bdf\u548c\u4f18\u5316\u3002"}}
{"id": "2507.06342", "pdf": "https://arxiv.org/pdf/2507.06342", "abs": "https://arxiv.org/abs/2507.06342", "authors": ["M. A. Evangelista-Alvarado", "P. Su\u00e1rez-Serrato"], "title": "SymFlux: deep symbolic regression of Hamiltonian vector fields", "categories": ["cs.LG", "cs.AI", "math.DS", "math.SG"], "comment": "26 pages, 7 figures", "summary": "We present SymFlux, a novel deep learning framework that performs symbolic\nregression to identify Hamiltonian functions from their corresponding vector\nfields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM\narchitectures to learn and output the symbolic mathematical expression of the\nunderlying Hamiltonian. Training and validation are conducted on newly\ndeveloped datasets of Hamiltonian vector fields, a key contribution of this\nwork. Our results demonstrate the model's effectiveness in accurately\nrecovering these symbolic expressions, advancing automated discovery in\nHamiltonian mechanics.", "AI": {"tldr": "SymFlux\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u56de\u5f52\u4ece\u6807\u51c6\u8f9b\u5e73\u9762\u4e0a\u7684\u5411\u91cf\u573a\u8bc6\u522b\u54c8\u5bc6\u987f\u51fd\u6570\uff0c\u91c7\u7528\u6df7\u5408CNN-LSTM\u67b6\u6784\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u53d1\u73b0\u54c8\u5bc6\u987f\u529b\u5b66\u4e2d\u7684\u7b26\u53f7\u8868\u8fbe\u5f0f\uff0c\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u6df7\u5408CNN-LSTM\u67b6\u6784\u8fdb\u884c\u7b26\u53f7\u56de\u5f52\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u57fa\u4e8e\u65b0\u5f00\u53d1\u7684\u54c8\u5bc6\u987f\u5411\u91cf\u573a\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u80fd\u51c6\u786e\u6062\u590d\u7b26\u53f7\u8868\u8fbe\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SymFlux\u5728\u54c8\u5bc6\u987f\u529b\u5b66\u7684\u81ea\u52a8\u5316\u53d1\u73b0\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.06911", "pdf": "https://arxiv.org/pdf/2507.06911", "abs": "https://arxiv.org/abs/2507.06911", "authors": ["Michele Polese", "Niloofar Mohamadi", "Salvatore D'Oro", "Tommaso Melodia"], "title": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G", "categories": ["cs.NI", "cs.AI", "eess.SP"], "comment": "Submitted to IEEE for publication, copyright may change without\n  notice. 8 pages, 6 figures", "summary": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408O-RAN\u548cAI-RAN\u7684\u65b0\u67b6\u6784\uff0c\u652f\u6301\u5728\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u4e0a\u7edf\u4e00\u7ba1\u7406\u901a\u4fe1\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u5f15\u5165\u4e86AI-RAN Orchestrator\u548cAI-RAN\u7ad9\u70b9\u4e24\u9879\u521b\u65b0\u3002", "motivation": "\u968f\u7740\u6570\u636e\u5bc6\u96c6\u578bAI\u5e94\u7528\u5728\u7f51\u7edc\u8fb9\u7f18\u7684\u666e\u53ca\uff0c\u9700\u8981\u4ece\u5355\u7eaf\u5229\u7528AI\u4f18\u5316\u7f51\u7edc\u8f6c\u5411\u652f\u6301\u5206\u5e03\u5f0fAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e3a\u8fd0\u8425\u5546\u63d0\u4f9b\u65b0\u7684\u76c8\u5229\u673a\u4f1a\u3002", "method": "\u6269\u5c55O-RAN\u7684\u6a21\u5757\u5316\u3001\u89e3\u8026\u548c\u4e91\u539f\u751f\u539f\u5219\uff0c\u63d0\u51faAI-RAN Orchestrator\u548cAI-RAN\u7ad9\u70b9\uff0c\u652f\u6301\u5f02\u6784AI\u90e8\u7f72\u548c\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u67b6\u6784\u652f\u6301\u7075\u6d3b\u90e8\u7f72\uff0c\u6ee1\u8db3\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u548c\u5730\u7406\u5b9a\u4f4d\u7684AI\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u5f00\u653e\u63a5\u53e3\u548c\u591a\u5382\u5546\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u7f51\u7edc\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5728\u8fb9\u7f18\u5b9e\u73b0AI\u76c8\u5229\u7684\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.06567", "pdf": "https://arxiv.org/pdf/2507.06567", "abs": "https://arxiv.org/abs/2507.06567", "authors": ["Qian Chen", "Xianhao Chen", "Kaibin Huang"], "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference", "categories": ["cs.LG", "cs.DC", "cs.NI"], "comment": "14 pages, 10 figures", "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u8fb9\u7f18\u7f51\u7edc\u4e2d\u4e13\u5bb6\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "MoE\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b58\u50a8\u5927\u91cf\u4e13\u5bb6\u7f51\u7edc\u5e26\u6765\u8d1f\u62c5\uff0c\u9700\u4f18\u5316\u4e13\u5bb6\u7f13\u5b58\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002", "method": "\u9488\u5bf9Top-K\u4e13\u5bb6\u9009\u62e9\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e86\u8d2a\u5fc3\u7b97\u6cd5\u548c\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6700\u5927\u5377\u79ef\u7684\u52a0\u901f\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fb9\u7f18\u7f51\u7edc\u4e2d\u6709\u6548\u4f18\u5316\u4e86\u4e13\u5bb6\u7f13\u5b58\uff0c\u63d0\u5347\u4e86MoE\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.06762", "pdf": "https://arxiv.org/pdf/2507.06762", "abs": "https://arxiv.org/abs/2507.06762", "authors": ["Nathalia Barbosa", "Paulo Borba", "L\u00e9uson Da Silva"], "title": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation", "categories": ["cs.SE", "K.6.3"], "comment": "Comments: 11 pages, in Portuguese language. 3 figures. Submitted to\n  SAST 2025 (X Simp\\'osio Brasileiro de Teste de Software Sistem\\'atico e\n  Automatizado)", "summary": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u4e49\u51b2\u7a81\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCode Llama 70B\u7684\u6d4b\u8bd5\u751f\u6210\u5de5\u5177\uff0c\u4ee5\u6539\u8fdbSMAT\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5408\u5e76\u5de5\u5177\u65e0\u6cd5\u68c0\u6d4b\u8bed\u4e49\u51b2\u7a81\uff0cSMAT\u867d\u6709\u6548\u4f46\u5b58\u5728\u9ad8\u5047\u9634\u6027\u7387\uff0c\u9700\u63a2\u7d22LLM\u7684\u6f5c\u529b\u3002", "method": "\u96c6\u6210Code Llama 70B\u5230SMAT\u4e2d\uff0c\u7814\u7a76\u4e0d\u540c\u4ea4\u4e92\u7b56\u7565\u3001\u63d0\u793a\u5185\u5bb9\u548c\u53c2\u6570\u914d\u7f6e\u7684\u6d4b\u8bd5\u751f\u6210\u80fd\u529b\u3002", "result": "LLM\u5728\u590d\u6742\u573a\u666f\u4e0b\u751f\u6210\u6d4b\u8bd5\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46\u663e\u793a\u51fa\u6539\u8fdb\u8bed\u4e49\u51b2\u7a81\u68c0\u6d4b\u7684\u6f5c\u529b\u3002", "conclusion": "LLM\u4e3a\u8bed\u4e49\u51b2\u7a81\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c3d\u7ba1\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002"}}
{"id": "2507.06509", "pdf": "https://arxiv.org/pdf/2507.06509", "abs": "https://arxiv.org/abs/2507.06509", "authors": ["Yangguang Shi", "Zhenyu Xue"], "title": "Prediction-Augmented Mechanism Design for Weighted Facility Location", "categories": ["cs.DS", "cs.GT", "cs.LG", "68W27, 68Q32", "F.2.2"], "comment": "An extended abstract of this paper is to appear in the 19th Annual\n  Conference on Theory and Applications of Models of Computation (TAMC 2025)", "summary": "Facility location is fundamental in operations research, mechanism design,\nand algorithmic game theory, with applications ranging from urban\ninfrastructure planning to distributed systems. Recent research in this area\nhas focused on augmenting classic strategyproof mechanisms with predictions to\nachieve an improved performance guarantee against the uncertainty under the\nstrategic environment. Previous work has been devoted to address the trade-off\nobstacle of balancing the consistency (near-optimality under accurate\npredictions) and robustness (bounded inefficiency under poor predictions)\nprimarily in the unweighted setting, assuming that all agents have the same\nimportance. However, this assumption may not be true in some practical\nscenarios, leading to research of weighted facility location problems.\n  The major contribution of the current work is to provide a prediction\naugmented algorithmic framework for balancing the consistency and robustness\nover strategic agents with non-uniform weights. In particular, through a\nreduction technique that identifies a subset of \\emph{representative} instances\nand maps the other given locations to the representative ones, we prove that\nthere exists a \\emph{strategyproof} mechanism achieving a bounded consistency\nguarantee of $\\frac{\\sqrt{(1+c)^2W^2_{\\min}+(1-c)^2W^2_{\\max}}}{(1+c)W_{\\min}}$\nand a bounded robustness guarantee of\n$\\frac{\\sqrt{(1-c)^2W^2_{\\min}+(1+c)^2W^2_{\\max}}}{(1-c)W_{\\min}}$ in weighted\nsettings, where $c$ can be viewed as a parameter to make a trade-off between\nthe consistency and robustness and $W_{\\min}$ and $W_{\\max}$ denote the minimum\nand maximum agents' weight. We also proved that there is no strategyproof\ndeterministic mechanism that reach $1$-consistency and $O\\left( n \\cdot\n\\frac{W_{\\max}}{W_{\\min}} \\right)$-robustness in weighted FLP, even with fully\npredictions of all agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u589e\u5f3a\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u975e\u5747\u5300\u6743\u91cd\u7684\u6218\u7565\u4ee3\u7406\u4e2d\u5e73\u8861\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7ecf\u5178\u7684\u65e0\u6743\u91cd\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u5047\u8bbe\u6240\u6709\u4ee3\u7406\u7684\u91cd\u8981\u6027\u76f8\u540c\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u4ee3\u7406\u7684\u6743\u91cd\u53ef\u80fd\u4e0d\u540c\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u52a0\u6743\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e00\u79cd\u5f52\u7ea6\u6280\u672f\uff0c\u8bc6\u522b\u4ee3\u8868\u6027\u5b9e\u4f8b\u5e76\u5c06\u5176\u4ed6\u4f4d\u7f6e\u6620\u5c04\u5230\u8fd9\u4e9b\u5b9e\u4f8b\u4e0a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b56\u7565\u8bc1\u660e\u673a\u5236\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u52a0\u6743\u8bbe\u7f6e\u4e0b\uff0c\u5b58\u5728\u4e00\u79cd\u7b56\u7565\u8bc1\u660e\u673a\u5236\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u754c\u7684\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u5728\u52a0\u6743\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u4e2d\uff0c\u5373\u4f7f\u6709\u5b8c\u5168\u9884\u6d4b\uff0c\u4e5f\u65e0\u6cd5\u901a\u8fc7\u7b56\u7565\u8bc1\u660e\u7684\u786e\u5b9a\u6027\u673a\u5236\u540c\u65f6\u8fbe\u5230\u6700\u4f18\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06366", "pdf": "https://arxiv.org/pdf/2507.06366", "abs": "https://arxiv.org/abs/2507.06366", "authors": ["Yupu Zhang", "Zelin Xu", "Tingsong Xiao", "Gustavo Seabra", "Yanjun Li", "Chenglong Li", "Zhe Jiang"], "title": "DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Predicting the binding affinity of protein-ligand complexes plays a vital\nrole in drug discovery. Unfortunately, progress has been hindered by the lack\nof large-scale and high-quality binding affinity labels. The widely used\nPDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,\nespecially graph contrastive learning (GCL), provides a unique opportunity to\nbreak the barrier by pre-training graph neural network models based on vast\nunlabeled complexes and fine-tuning the models on much fewer labeled complexes.\nHowever, the problem faces unique challenges, including a lack of a\ncomprehensive unlabeled dataset with well-defined positive/negative complex\npairs and the need to design GCL algorithms that incorporate the unique\ncharacteristics of such data. To fill the gap, we propose DecoyDB, a\nlarge-scale, structure-aware dataset specifically designed for self-supervised\nGCL on protein-ligand complexes. DecoyDB consists of high-resolution ground\ntruth complexes (less than 2.5 Angstrom) and diverse decoy structures with\ncomputationally generated binding poses that range from realistic to suboptimal\n(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation\n(RMSD) from the native pose. We further design a customized GCL framework to\npre-train graph neural networks based on DecoyDB and fine-tune the models with\nlabels from PDBbind. Extensive experiments confirm that models pre-trained with\nDecoyDB achieve superior accuracy, label efficiency, and generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDecoyDB\u6570\u636e\u96c6\u548c\u5b9a\u5236\u5316GCL\u6846\u67b6\uff0c\u7528\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u4e2d\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u800c\u53d7\u9650\uff0c\u73b0\u6709PDBbind\u6570\u636e\u96c6\u6807\u8bb0\u4e0d\u8db320K\u3002", "method": "\u6784\u5efaDecoyDB\u6570\u636e\u96c6\uff08\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u771f\u5b9e\u590d\u5408\u7269\u548c\u8ba1\u7b97\u751f\u6210\u7684\u8bf1\u9975\u7ed3\u6784\uff09\uff0c\u5e76\u8bbe\u8ba1\u5b9a\u5236\u5316GCL\u6846\u67b6\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eDecoyDB\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6807\u7b7e\u6548\u7387\u548c\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "DecoyDB\u548c\u5b9a\u5236\u5316GCL\u6846\u67b6\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2507.06499", "pdf": "https://arxiv.org/pdf/2507.06499", "abs": "https://arxiv.org/abs/2507.06499", "authors": ["Shivangi Agarwal", "Adi Asija", "Sanjit K. Kaul", "Arani Bhattacharya", "Saket Anand"], "title": "Learning To Communicate Over An Unknown Shared Network", "categories": ["cs.MA", "cs.NI"], "comment": "22 pages, 15 figures, 4 tables", "summary": "As robots (edge-devices, agents) find uses in an increasing number of\nsettings and edge-cloud resources become pervasive, wireless networks will\noften be shared by flows of data traffic that result from communication between\nagents and corresponding edge-cloud. In such settings, agent communicating with\nthe edge-cloud is unaware of state of network resource, which evolves in\nresponse to not just agent's own communication at any given time but also to\ncommunication by other agents, which stays unknown to the agent. We address\nchallenge of an agent learning a policy that allows it to decide whether or not\nto communicate with its cloud node, using limited feedback it obtains from its\nown attempts to communicate, to optimize its utility. The policy generalizes\nwell to any number of other agents sharing the network and must not be trained\nfor any particular network configuration. Our proposed policy is a DRL model\nQuery Net (QNet) that we train using a proposed simulation-to-real framework.\nOur simulation model has just one parameter and is agnostic to specific\nconfigurations of any wireless network. It allows training an agent's policy\nover a wide range of outcomes that an agent's communication with its edge-cloud\nnode may face when using a shared network, by suitably randomizing the\nsimulation parameter. We propose a learning algorithm that addresses challenges\nobserved in training QNet. We validate our simulation-to-real driven approach\nthrough experiments conducted on real wireless networks including WiFi and\ncellular. We compare QNet with other policies to demonstrate its efficacy. WiFi\nexperiments involved as few as five agents, resulting in barely any contention\nfor the network, to as many as fifty agents, resulting in severe contention.\nThe cellular experiments spanned a broad range of network conditions, with\nbaseline RTT ranging from a low of 0.07 second to a high of 0.83 second.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQNet\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u5e2e\u52a9\u4ee3\u7406\u5728\u5171\u4eab\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f18\u5316\u901a\u4fe1\u7b56\u7565\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u7f51\u7edc\u914d\u7f6e\u8bad\u7ec3\u3002", "motivation": "\u5728\u5171\u4eab\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u4ee3\u7406\u65e0\u6cd5\u611f\u77e5\u7f51\u7edc\u8d44\u6e90\u72b6\u6001\uff0c\u9700\u901a\u8fc7\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u901a\u4fe1\u6548\u7528\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u6846\u67b6\u8bad\u7ec3QNet\uff0c\u6a21\u62df\u6a21\u578b\u4ec5\u9700\u4e00\u4e2a\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u65e0\u7ebf\u7f51\u7edc\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86QNet\u5728WiFi\u548c\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u6709\u6548\u6027\uff0c\u9002\u5e94\u4ece\u4f4e\u5230\u9ad8\u7684\u7f51\u7edc\u7ade\u4e89\u6761\u4ef6\u3002", "conclusion": "QNet\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u7f51\u7edc\u914d\u7f6e\u548c\u4ee3\u7406\u6570\u91cf\uff0c\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\u3002"}}
{"id": "2507.06931", "pdf": "https://arxiv.org/pdf/2507.06931", "abs": "https://arxiv.org/abs/2507.06931", "authors": ["Tongtian Zhu", "Wenhao Li", "Can Wang", "Fengxiang He"], "title": "DICE: Data Influence Cascade in Decentralized Learning", "categories": ["cs.LG", "cs.DC", "cs.MA", "cs.SI", "stat.ML"], "comment": "Published as a poster at ICLR 2025", "summary": "Decentralized learning offers a promising approach to crowdsource data\nconsumptions and computational workloads across geographically distributed\ncompute interconnected through peer-to-peer networks, accommodating the\nexponentially increasing demands. However, proper incentives are still in\nabsence, considerably discouraging participation. Our vision is that a fair\nincentive mechanism relies on fair attribution of contributions to\nparticipating nodes, which faces non-trivial challenges arising from the\nlocalized connections making influence ``cascade'' in a decentralized network.\nTo overcome this, we design the first method to estimate \\textbf{D}ata\n\\textbf{I}nfluence \\textbf{C}ascad\\textbf{E} (DICE) in a decentralized\nenvironment. Theoretically, the framework derives tractable approximations of\ninfluence cascade over arbitrary neighbor hops, suggesting the influence\ncascade is determined by an interplay of data, communication topology, and the\ncurvature of loss landscape. DICE also lays the foundations for applications\nincluding selecting suitable collaborators and identifying malicious behaviors.\nProject page is available at https://raiden-zhu.github.io/blog/2025/DICE/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDICE\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u4f30\u8ba1\u6570\u636e\u5f71\u54cd\u4f20\u64ad\uff0c\u4ee5\u89e3\u51b3\u6fc0\u52b1\u673a\u5236\u4e2d\u516c\u5e73\u8d21\u732e\u5206\u914d\u7684\u6311\u6218\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5206\u6563\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u4f46\u7f3a\u4e4f\u516c\u5e73\u6fc0\u52b1\u673a\u5236\u963b\u788d\u4e86\u53c2\u4e0e\u3002\u516c\u5e73\u6fc0\u52b1\u9700\u8981\u51c6\u786e\u8bc4\u4f30\u8282\u70b9\u8d21\u732e\uff0c\u4f46\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u7684\u5c40\u90e8\u8fde\u63a5\u4f7f\u5f97\u5f71\u54cd\u4f20\u64ad\u590d\u6742\u3002", "method": "\u8bbe\u8ba1\u4e86DICE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u8fd1\u4f3c\u8ba1\u7b97\u4efb\u610f\u90bb\u5c45\u8df3\u6570\u7684\u5f71\u54cd\u4f20\u64ad\uff0c\u7ed3\u5408\u6570\u636e\u3001\u901a\u4fe1\u62d3\u6251\u548c\u635f\u5931\u51fd\u6570\u66f2\u7387\u3002", "result": "DICE\u4e3a\u9009\u62e9\u5408\u9002\u5408\u4f5c\u8005\u548c\u8bc6\u522b\u6076\u610f\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "DICE\u662f\u9996\u4e2a\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u91cf\u5316\u5f71\u54cd\u4f20\u64ad\u7684\u65b9\u6cd5\uff0c\u4e3a\u6fc0\u52b1\u673a\u5236\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.06881", "pdf": "https://arxiv.org/pdf/2507.06881", "abs": "https://arxiv.org/abs/2507.06881", "authors": ["Brian R Larson", "Ehsan Ahmad"], "title": "Formalization of the AADL Run-Time Services with Time", "categories": ["cs.SE", "cs.SY", "eess.SY"], "comment": "35 pages, 13 figures", "summary": "The Architecture Analysis & Design Language (AADL) is an architecture\ndescription language for design of cyber-physical systems--machines controlled\nby software. The AADL standard, SAE International AS5506D, describes Run-Time\nServices (RTS) to be provided to execute AADL models in accordance with\nsemantics defined by the standard. The RTS of primary concern are transport\nservices and timing services. Although, the study presented in [1] sets a\nfoundation for the formal semantics of AADL, but without modeling time. This\npaper extends and simplifies this formalization using a modal logic defined by\na Kripke structure, to explicitly include time. The RTS defined in the AADL\nstandard are also expanded to support reactive state-transition machines of the\nBehavior Specification annex standard language (BA) and its closely-related,\nformally-defined counterpart, the Behavior Language for Embedded Systems with\nSoftware (BLESS). An example of AADL RTS with time, implemented by the High\nAssurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for\nstate-transition machine behavior written in BLESS, is also presented.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u5e76\u7b80\u5316\u4e86AADL\u7684\u5f62\u5f0f\u5316\u8bed\u4e49\uff0c\u901a\u8fc7\u6a21\u6001\u903b\u8f91\u660e\u786e\u5f15\u5165\u65f6\u95f4\uff0c\u5e76\u6269\u5c55\u4e86AADL\u6807\u51c6\u7684\u8fd0\u884c\u65f6\u670d\u52a1\u4ee5\u652f\u6301\u884c\u4e3a\u89c4\u8303\u8bed\u8a00\u3002", "motivation": "AADL\u6807\u51c6\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u7684\u660e\u786e\u5efa\u6a21\uff0c\u4e14\u8fd0\u884c\u65f6\u670d\u52a1\u9700\u8981\u6269\u5c55\u4ee5\u652f\u6301\u884c\u4e3a\u89c4\u8303\u8bed\u8a00\u3002", "method": "\u4f7f\u7528Kripke\u7ed3\u6784\u5b9a\u4e49\u7684\u6a21\u6001\u903b\u8f91\u6269\u5c55\u5f62\u5f0f\u5316\u8bed\u4e49\uff0c\u5e76\u6269\u5c55\u8fd0\u884c\u65f6\u670d\u52a1\u4ee5\u652f\u6301BLESS\u548cBA\u8bed\u8a00\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u65f6\u95f4\u7684AADL\u8fd0\u884c\u65f6\u670d\u52a1\u5b9e\u73b0\u793a\u4f8b\uff0c\u5c55\u793a\u4e86HAMR\u5bf9BLESS\u72b6\u6001\u8f6c\u6362\u673a\u884c\u4e3a\u7684\u652f\u6301\u3002", "conclusion": "\u901a\u8fc7\u6a21\u6001\u903b\u8f91\u548c\u6269\u5c55\u8fd0\u884c\u65f6\u670d\u52a1\uff0cAADL\u7684\u5f62\u5f0f\u5316\u8bed\u4e49\u66f4\u5b8c\u5584\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u884c\u4e3a\u5efa\u6a21\u3002"}}
{"id": "2507.06721", "pdf": "https://arxiv.org/pdf/2507.06721", "abs": "https://arxiv.org/abs/2507.06721", "authors": ["Avi Kadria", "Liam Roditty"], "title": "Faster Algorithms for $(2k-1)$-Stretch Distance Oracles", "categories": ["cs.DS"], "comment": null, "summary": "Let $G=(V, E)$ be an undirected $n$-vertices $m$-edges graph with\nnon-negative edge weights. In this paper, we present three new algorithms for\nconstructing a $(2k-1)$-stretch distance oracle with $O(n^{1+\\frac{1}{k}})$\nspace. The first algorithm runs in $\\Ot(\\max(n^{1+2/k},\nm^{1-\\frac{1}{k-1}}n^{\\frac{2}{k-1}}))$ time, and improves upon the\n$\\Ot(\\min(mn^{\\frac{1}{k}},n^2))$ time of Thorup and Zwick [STOC 2001, JACM\n2005] and Baswana and Kavitha [FOCS 2006, SICOMP 2010], for every $k > 2$ and\n$m=\\Omega(n^{1+\\frac{1}{k}+\\eps})$. This yields the first truly subquadratic\ntime construction for every $2 < k < 6$, and nearly resolves the open problem\nposed by Wulff-Nilsen [SODA 2012] on the existence of such constructions.\n  The two other algorithms have a running time of the form $\\Ot(m+n^{1+f(k)})$,\nwhich is near linear in $m$ if $m=\\Omega(n^{1+f(k)})$, and therefore optimal in\nsuch graphs. One algorithm runs in $\\Ot(m+n^{\\frac32+\\frac{3}{4k-6}})$-time,\nwhich improves upon the $\\Ot(n^2)$-time algorithm of Baswana and Kavitha [FOCS\n2006, SICOMP 2010], for $3 < k < 6$, and upon the\n$\\Ot(m+n^{\\frac{3}{2}+\\frac{2}{k}+O(k^{-2})})$-time algorithm of Wulff-Nilsen\n[SODA 2012], for every $k\\geq 6$. This is the first linear time algorithm for\nconstructing a $7$-stretch distance oracle and a $9$-stretch distance oracle,\nfor graphs with truly subquadratic density.\\footnote{with $m=n^{2-\\eps}$ for\nsome $\\eps > 0$.} The other algorithm runs in\n$\\Ot(\\sqrt{k}m+kn^{1+\\frac{2\\sqrt{2}}{\\sqrt{k}}})$ time, (and hence relevant\nonly for $k\\ge 16$), and improves upon the\n$\\Ot(\\sqrt{k}m+kn^{1+\\frac{2\\sqrt{6}}{\\sqrt{k}}+O(k^{-1})})$ time algorithm of\nWulff-Nilsen [SODA 2012] (which is relevant only for $k\\ge 96$). ...", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u5177\u6709$(2k-1)$-stretch\u7684\u8ddd\u79bb\u9884\u8a00\u673a\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u6784\u9020\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8ddd\u79bb\u9884\u8a00\u673a\u6784\u9020\u7b97\u6cd5\u5728\u9ad8\u5bc6\u5ea6\u56fe\u4e2d\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8fc7\u9ad8\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9$k>2$\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u65b0\u7b97\u6cd5\uff1a\u7b2c\u4e00\u79cd\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e9a\u4e8c\u6b21\u65f6\u95f4\u6784\u9020\uff1b\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5728\u8fd1\u7ebf\u6027\u65f6\u95f4\u5185\u5b8c\u6210\u6784\u9020\uff1b\u7b2c\u4e09\u79cd\u7b97\u6cd5\u9488\u5bf9\u5927$k$\u503c\u4f18\u5316\u3002", "result": "\u7b2c\u4e00\u79cd\u7b97\u6cd5\u5728$2<k<6$\u65f6\u9996\u6b21\u5b9e\u73b0\u771f\u6b63\u4e9a\u4e8c\u6b21\u65f6\u95f4\u6784\u9020\uff1b\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5728$3<k<6$\u548c$k\\geq6$\u65f6\u5206\u522b\u6539\u8fdb\u73b0\u6709\u7ed3\u679c\uff1b\u7b2c\u4e09\u79cd\u7b97\u6cd5\u5728$k\\geq16$\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u7b97\u6cd5\u663e\u8457\u6539\u8fdb\u4e86\u8ddd\u79bb\u9884\u8a00\u673a\u7684\u6784\u9020\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u591a\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u4e3a\u9ad8\u5bc6\u5ea6\u56fe\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06367", "pdf": "https://arxiv.org/pdf/2507.06367", "abs": "https://arxiv.org/abs/2507.06367", "authors": ["El Mehdi Achour", "Kathl\u00e9n Kohn", "Holger Rauhut"], "title": "The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks", "categories": ["cs.LG", "math.AG"], "comment": null, "summary": "We study geometric properties of the gradient flow for learning deep linear\nconvolutional networks. For linear fully connected networks, it has been shown\nrecently that the corresponding gradient flow on parameter space can be written\nas a Riemannian gradient flow on function space (i.e., on the product of weight\nmatrices) if the initialization satisfies a so-called balancedness condition.\nWe establish that the gradient flow on parameter space for learning linear\nconvolutional networks can be written as a Riemannian gradient flow on function\nspace regardless of the initialization. This result holds for $D$-dimensional\nconvolutions with $D \\geq 2$, and for $D =1$ it holds if all so-called strides\nof the convolutions are greater than one. The corresponding Riemannian metric\ndepends on the initialization.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u53d1\u73b0\u53c2\u6570\u7a7a\u95f4\u7684\u68af\u5ea6\u6d41\u53ef\u4ee5\u8868\u793a\u4e3a\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u4e14\u4e0d\u53d7\u521d\u59cb\u5316\u6761\u4ef6\u9650\u5236\u3002", "motivation": "\u63a2\u7d22\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u7279\u6027\uff0c\u6269\u5c55\u4e86\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u76f8\u5173\u7814\u7a76\u3002", "method": "\u5206\u6790\u4e86\u53c2\u6570\u7a7a\u95f4\u7684\u68af\u5ea6\u6d41\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u51fd\u6570\u7a7a\u95f4\u4e0a\u53ef\u8868\u793a\u4e3a\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u5377\u79ef\u3002", "result": "\u68af\u5ea6\u6d41\u53ef\u8868\u793a\u4e3a\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u4e14\u9002\u7528\u4e8eD\u22652\u7ef4\u5377\u79ef\uff0cD=1\u65f6\u9700\u6ee1\u8db3\u6b65\u957f\u5927\u4e8e1\u7684\u6761\u4ef6\u3002", "conclusion": "\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u7684\u68af\u5ea6\u6d41\u5728\u51fd\u6570\u7a7a\u95f4\u4e0a\u5177\u6709\u9ece\u66fc\u7ed3\u6784\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7406\u8bba\u3002"}}
{"id": "2507.06938", "pdf": "https://arxiv.org/pdf/2507.06938", "abs": "https://arxiv.org/abs/2507.06938", "authors": ["Lisa Gaedke-Merzh\u00e4user", "Vincent Maillou", "Fernando Rodriguez Avellaneda", "Olaf Schenk", "Mathieu Luisier", "Paula Moraga", "Alexandros Nikolaos Ziogas", "H\u00e5vard Rue"], "title": "Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian Processes", "categories": ["stat.CO", "cs.DC", "62F15, 68W15", "G.3; G.4"], "comment": null, "summary": "Multivariate Gaussian processes (GPs) offer a powerful probabilistic\nframework to represent complex interdependent phenomena. They pose, however,\nsignificant computational challenges in high-dimensional settings, which\nfrequently arise in spatial-temporal applications. We present DALIA, a highly\nscalable framework for performing Bayesian inference tasks on spatio-temporal\nmultivariate GPs, based on the methodology of integrated nested Laplace\napproximations. Our approach relies on a sparse inverse covariance matrix\nformulation of the GP, puts forward a GPU-accelerated block-dense approach, and\nintroduces a hierarchical, triple-layer, distributed memory parallel scheme. We\nshowcase weak scaling performance surpassing the state-of-the-art by two orders\nof magnitude on a model whose parameter space is 8$\\times$ larger and measure\nstrong scaling speedups of three orders of magnitude when running on 496 GH200\nsuperchips on the Alps supercomputer. Applying DALIA to air pollution data from\nnorthern Italy over 48 days, we showcase refined spatial resolutions over the\naggregated pollutant measurements.", "AI": {"tldr": "DALIA\u662f\u4e00\u4e2a\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u65f6\u7a7a\u591a\u5143\u9ad8\u65af\u8fc7\u7a0b\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u901a\u8fc7GPU\u52a0\u901f\u548c\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "\u591a\u5143\u9ad8\u65af\u8fc7\u7a0b\u5728\u9ad8\u7ef4\u65f6\u7a7a\u5e94\u7528\u4e2d\u9762\u4e34\u663e\u8457\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DALIA\u57fa\u4e8e\u96c6\u6210\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u91c7\u7528\u7a00\u758f\u9006\u534f\u65b9\u5dee\u77e9\u9635\u3001GPU\u52a0\u901f\u7684\u5757\u5bc6\u96c6\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e09\u5c42\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u65b9\u6848\u3002", "result": "\u57288\u500d\u53c2\u6570\u7a7a\u95f4\u7684\u6a21\u578b\u4e0a\uff0c\u5f31\u6269\u5c55\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u4e24\u4e2a\u6570\u91cf\u7ea7\uff1b\u5728496\u4e2aGH200\u8d85\u7ea7\u82af\u7247\u4e0a\u8fd0\u884c\u65f6\uff0c\u5f3a\u6269\u5c55\u901f\u5ea6\u63d0\u5347\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002\u5e94\u7528\u4e8e\u610f\u5927\u5229\u5317\u90e8\u7684\u7a7a\u6c14\u6c61\u67d3\u6570\u636e\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "conclusion": "DALIA\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u5143\u9ad8\u65af\u8fc7\u7a0b\u5728\u65f6\u7a7a\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u9ad8\u7ef4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.06980", "pdf": "https://arxiv.org/pdf/2507.06980", "abs": "https://arxiv.org/abs/2507.06980", "authors": ["Binquan Zhang", "Li Zhang", "Zhiwen Luo", "Yuxin Du", "Fang Liu", "Song Wang", "Lin Shi"], "title": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5f71\u54cdCoT\u8d28\u91cf\u7684\u5185\u5916\u90e8\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76LLM\u751f\u6210\u7684CoT\u8d28\u91cf\u53ca\u5176\u5bf9\u4ee3\u7801\u751f\u6210\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347LLM\u7684\u53ef\u9760\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u67901,023\u4e2a\u5931\u8d25\u7684\u4ee3\u7801\u6837\u672c\u548c210\u4e2aCoT-\u4ee3\u7801\u5bf9\uff0c\u8bc4\u4f30CoT\u8d28\u91cf\u53ca\u5176\u5bf9\u4ee3\u7801\u751f\u6210\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u63d0\u793aLLM\u6539\u8fdb\u4f4e\u8d28\u91cfCoT\u3002", "result": "\u53d1\u73b0\u5916\u90e8\u56e0\u7d20\uff0853.60%\uff09\u548c\u5185\u90e8\u56e0\u7d20\uff0840.10%\uff09\u5f71\u54cdCoT\u8d28\u91cf\uff1b\u5373\u4f7fCoT\u6b63\u786e\uff0c18.5%\u7684\u4ee3\u7801\u4ecd\u6709\u9519\u8bef\uff1b\u6539\u8fdbCoT\u53ef\u63d0\u5347LLM\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86CoT\u751f\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u63d0\u5347LLM\u63a8\u7406\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.06925", "pdf": "https://arxiv.org/pdf/2507.06925", "abs": "https://arxiv.org/abs/2507.06925", "authors": ["Lorenzo Beretta", "Deeparnab Chakrabarty", "C. Seshadhri"], "title": "Faster Estimation of the Average Degree of a Graph Using Random Edges and Structural Queries", "categories": ["cs.DS"], "comment": null, "summary": "We revisit the problem of designing sublinear algorithms for estimating the\naverage degree of an $n$-vertex graph. The standard access model for graphs\nallows for the following queries: sampling a uniform random vertex, the degree\nof a vertex, sampling a uniform random neighbor of a vertex, and ``pair\nqueries'' which determine if a pair of vertices form an edge. In this model,\noriginal results [Goldreich-Ron, RSA 2008; Eden-Ron-Seshadhri, SIDMA 2019] on\nthis problem prove that the complexity of getting\n$(1+\\varepsilon)$-multiplicative approximations to the average degree, ignoring\n$\\varepsilon$-dependencies, is $\\Theta(\\sqrt{n})$. When random edges can be\nsampled, it is known that the average degree can estimated in\n$\\widetilde{O}(n^{1/3})$ queries, even without pair queries\n[Motwani-Panigrahy-Xu, ICALP 2007; Beretta-Tetek, TALG 2024].\n  We give a nearly optimal algorithm in the standard access model with random\nedge samples. Our algorithm makes $\\widetilde{O}(n^{1/4})$ queries exploiting\nthe power of pair queries. We also analyze the ``full neighborhood access\"\nmodel wherein the entire adjacency list of a vertex can be obtained with a\nsingle query; this model is relevant in many practical applications. In a\nweaker version of this model, we give an algorithm that makes\n$\\widetilde{O}(n^{1/5})$ queries. Both these results underscore the power of\n{\\em structural queries}, such as pair queries and full neighborhood access\nqueries, for estimating the average degree. We give nearly matching lower\nbounds, ignoring $\\varepsilon$-dependencies, for all our results.\n  So far, almost all algorithms for estimating average degree assume that the\nnumber of vertices, $n$, is known. Inspired by [Beretta-Tetek, TALG 2024], we\nstudy this problem when $n$ is unknown and show that structural queries do not\nhelp in estimating average degree in this setting.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6807\u51c6\u56fe\u8bbf\u95ee\u6a21\u578b\u4e2d\u8bbe\u8ba1\u5b50\u7ebf\u6027\u7b97\u6cd5\u6765\u4f30\u8ba1\u56fe\u7684\u5e73\u5747\u5ea6\u6570\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u7ed3\u6784\u67e5\u8be2\uff08\u5982\u8fb9\u5bf9\u67e5\u8be2\u548c\u5168\u90bb\u57df\u8bbf\u95ee\u67e5\u8be2\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5728\u672a\u77e5\u9876\u70b9\u6570\u60c5\u51b5\u4e0b\u7684\u9650\u5236\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u56fe\u5e73\u5747\u5ea6\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u6807\u51c6\u8bbf\u95ee\u6a21\u578b\u548c\u5168\u90bb\u57df\u8bbf\u95ee\u6a21\u578b\u4e2d\u66f4\u9ad8\u6548\u7684\u5b50\u7ebf\u6027\u7b97\u6cd5\uff0c\u540c\u65f6\u7814\u7a76\u9876\u70b9\u6570\u672a\u77e5\u65f6\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u5229\u7528\u8fb9\u5bf9\u67e5\u8be2\u548c\u5168\u90bb\u57df\u8bbf\u95ee\u67e5\u8be2\u7684\u65b0\u7b97\u6cd5\uff0c\u5206\u522b\u5728\u6807\u51c6\u6a21\u578b\u548c\u5168\u90bb\u57df\u6a21\u578b\u4e2d\u5b9e\u73b0\u67e5\u8be2\u590d\u6742\u5ea6\u4e3a$\\widetilde{O}(n^{1/4})$\u548c$\\widetilde{O}(n^{1/5})$\u3002", "result": "\u5728\u6807\u51c6\u6a21\u578b\u4e2d\uff0c\u7b97\u6cd5\u67e5\u8be2\u590d\u6742\u5ea6\u4e3a$\\widetilde{O}(n^{1/4})$\uff1b\u5728\u5168\u90bb\u57df\u6a21\u578b\u4e2d\u4e3a$\\widetilde{O}(n^{1/5})$\u3002\u540c\u65f6\u8bc1\u660e\u4e86\u9876\u70b9\u6570\u672a\u77e5\u65f6\u7ed3\u6784\u67e5\u8be2\u65e0\u6548\u3002", "conclusion": "\u7ed3\u6784\u67e5\u8be2\uff08\u5982\u8fb9\u5bf9\u67e5\u8be2\u548c\u5168\u90bb\u57df\u8bbf\u95ee\uff09\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747\u5ea6\u6570\u4f30\u8ba1\u7684\u6548\u7387\uff0c\u4f46\u5728\u9876\u70b9\u6570\u672a\u77e5\u65f6\u65e0\u6cd5\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2507.06380", "pdf": "https://arxiv.org/pdf/2507.06380", "abs": "https://arxiv.org/abs/2507.06380", "authors": ["Habibur Rahaman", "Atri Chatterjee", "Swarup Bhunia"], "title": "Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "7 pages, 7 figures", "summary": "Complex neural networks require substantial memory to store a large number of\nsynaptic weights. This work introduces WINGs (Automatic Weight Generator for\nSecure and Storage-Efficient Deep Learning Models), a novel framework that\ndynamically generates layer weights in a fully connected neural network (FC)\nand compresses the weights in convolutional neural networks (CNNs) during\ninference, significantly reducing memory requirements without sacrificing\naccuracy. WINGs framework uses principal component analysis (PCA) for\ndimensionality reduction and lightweight support vector regression (SVR) models\nto predict layer weights in the FC networks, removing the need for storing\nfull-weight matrices and achieving substantial memory savings. It also\npreferentially compresses the weights in low-sensitivity layers of CNNs using\nPCA and SVR with sensitivity analysis. The sensitivity-aware design also offers\nan added level of security, as any bit-flip attack with weights in compressed\nlayers has an amplified and readily detectable effect on accuracy. WINGs\nachieves 53x compression for the FC layers and 28x for AlexNet with MNIST\ndataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.\nThis significant reduction in memory results in higher throughput and lower\nenergy for DNN inference, making it attractive for resource-constrained edge\napplications.", "AI": {"tldr": "WINGs\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u751f\u6210\u5168\u8fde\u63a5\u7f51\u7edc\u6743\u91cd\u548c\u538b\u7f29\u5377\u79ef\u7f51\u7edc\u6743\u91cd\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u5b58\u50a8\u5927\u91cf\u7a81\u89e6\u6743\u91cd\u7684\u9ad8\u5185\u5b58\u9700\u6c42\u95ee\u9898\u3002", "method": "\u4f7f\u7528PCA\u964d\u7ef4\u548c\u8f7b\u91cf\u7ea7SVR\u6a21\u578b\u9884\u6d4b\u6743\u91cd\uff0c\u7ed3\u5408\u654f\u611f\u6027\u5206\u6790\u538b\u7f29CNN\u6743\u91cd\u3002", "result": "FC\u5c42\u538b\u7f2953\u500d\uff0cAlexNet\u5728MNIST\u4e0a\u538b\u7f2928\u500d\uff0cCIFAR-10\u4e0a\u538b\u7f2918\u500d\uff0c\u7cbe\u5ea6\u635f\u59311-2%\u3002", "conclusion": "WINGs\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u63d0\u5347\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u5e94\u7528\u3002"}}
{"id": "2507.06827", "pdf": "https://arxiv.org/pdf/2507.06827", "abs": "https://arxiv.org/abs/2507.06827", "authors": ["Dibakar Das", "Barath S Narayan", "Aarna Bhammar", "Jyotsna Bapat"], "title": "Connecting the Unconnected -- Sentiment Analysis of Field Survey of Internet Connectivity in Emerging Economies", "categories": ["cs.CY", "cs.NI"], "comment": null, "summary": "Internet has significantly improved the quality of citizens across the world.\nThough the internet coverage is quite high, 40% of global population do not\nhave access to broadband internet. This paper presents an analysis of a field\nsurvey of population in some areas of Kathmandu, Nepal, an emerging economy.\nThis survey was triggered by intermittent severe congestion of internet in\ncertain areas of the city. People from three different areas were asked about\ntheir present experience of internet usage, its impact on their lives and their\naspirations for the future. Survey pointed to high speed, low cost, reliable\nand secure internet as a major aspiration of the respondents. Based on their\ninputs, this paper presents a sentiment analysis as well as demographic\ninformation. Keys insights from this analysis shows that overall sentiment to\nmost queries are positive. The variances of positive sentiments are high\nwhereas those for negative ones are low. Also, some correlations and clusters\nare observed among the attributes though no dominant component exists in the\ndata.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5c3c\u6cca\u5c14\u52a0\u5fb7\u6ee1\u90fd\u90e8\u5206\u5730\u533a\u5c45\u6c11\u5bf9\u4e92\u8054\u7f51\u4f7f\u7528\u7684\u4f53\u9a8c\u548c\u671f\u671b\uff0c\u53d1\u73b0\u9ad8\u901f\u3001\u4f4e\u6210\u672c\u3001\u53ef\u9760\u548c\u5b89\u5168\u7684\u4e92\u8054\u7f51\u662f\u4e3b\u8981\u9700\u6c42\uff0c\u6574\u4f53\u60c5\u7eea\u504f\u79ef\u6781\u3002", "motivation": "\u7814\u7a76\u4e92\u8054\u7f51\u8986\u76d6\u867d\u5e7f\u4f46\u4ecd\u670940%\u5168\u7403\u4eba\u53e3\u65e0\u6cd5\u4f7f\u7528\u5bbd\u5e26\u7684\u95ee\u9898\uff0c\u8c03\u67e5\u52a0\u5fb7\u6ee1\u90fd\u5c45\u6c11\u5bf9\u4e92\u8054\u7f51\u7684\u4f53\u9a8c\u548c\u672a\u6765\u671f\u671b\u3002", "method": "\u5728\u52a0\u5fb7\u6ee1\u90fd\u4e09\u4e2a\u5730\u533a\u8fdb\u884c\u5b9e\u5730\u8c03\u67e5\uff0c\u6536\u96c6\u5c45\u6c11\u5bf9\u4e92\u8054\u7f51\u4f7f\u7528\u4f53\u9a8c\u3001\u751f\u6d3b\u5f71\u54cd\u53ca\u672a\u6765\u671f\u671b\u7684\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u60c5\u611f\u5206\u6790\u548c\u4eba\u53e3\u7edf\u8ba1\u3002", "result": "\u8c03\u67e5\u663e\u793a\u5c45\u6c11\u5bf9\u4e92\u8054\u7f51\u7684\u603b\u4f53\u60c5\u7eea\u79ef\u6781\uff0c\u9ad8\u901f\u3001\u4f4e\u6210\u672c\u3001\u53ef\u9760\u548c\u5b89\u5168\u7684\u4e92\u8054\u7f51\u662f\u4e3b\u8981\u9700\u6c42\uff1b\u60c5\u611f\u5206\u6790\u663e\u793a\u79ef\u6781\u60c5\u7eea\u65b9\u5dee\u9ad8\uff0c\u6d88\u6781\u60c5\u7eea\u65b9\u5dee\u4f4e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u52a0\u5fb7\u6ee1\u90fd\u5c45\u6c11\u5bf9\u4e92\u8054\u7f51\u7684\u79ef\u6781\u6001\u5ea6\u548c\u660e\u786e\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u4e92\u8054\u7f51\u670d\u52a1\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.07026", "pdf": "https://arxiv.org/pdf/2507.07026", "abs": "https://arxiv.org/abs/2507.07026", "authors": ["Sadia Afrin Mim", "Fatema Tuz Zohra", "Justin Smith", "Brittany Johnson"], "title": "Exploring Fairness Interventions in Open Source Projects", "categories": ["cs.SE"], "comment": "Revised version accepted at the 1st International Workshop on\n  Fairness in Software Systems(SANER 2025)", "summary": "The deployment of biased machine learning (ML) models has resulted in adverse\neffects in crucial sectors such as criminal justice and healthcare. To address\nthese challenges, a diverse range of machine learning fairness interventions\nhave been developed, aiming to mitigate bias and promote the creation of more\nequitable models. Despite the growing availability of these interventions,\ntheir adoption in real-world applications remains limited, with many\npractitioners unaware of their existence. To address this gap, we\nsystematically identified and compiled a dataset of 62 open source fairness\ninterventions and identified active ones. We conducted an in-depth analysis of\ntheir specifications and features to uncover considerations that may drive\npractitioner preference and to identify the software interventions actively\nmaintained in the open source ecosystem. Our findings indicate that 32% of\nthese interventions have been actively maintained within the past year, and 50%\nof them offer both bias detection and mitigation capabilities, mostly during\ninprocessing.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e8662\u79cd\u5f00\u6e90\u516c\u5e73\u6027\u5e72\u9884\u5de5\u5177\uff0c\u53d1\u73b032%\u5728\u8fc7\u53bb\u4e00\u5e74\u5185\u6d3b\u8dc3\u7ef4\u62a4\uff0c50%\u63d0\u4f9b\u504f\u5dee\u68c0\u6d4b\u548c\u7f13\u89e3\u529f\u80fd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u5728\u5173\u952e\u9886\u57df\uff08\u5982\u5211\u4e8b\u53f8\u6cd5\u548c\u533b\u7597\uff09\u9020\u6210\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u516c\u5e73\u6027\u5e72\u9884\u5de5\u5177\u7684\u91c7\u7528\u7387\u4f4e\uff0c\u90e8\u5206\u539f\u56e0\u662f\u5b9e\u8df5\u8005\u5bf9\u5176\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u8bc6\u522b\u5e76\u5206\u679062\u79cd\u5f00\u6e90\u516c\u5e73\u6027\u5e72\u9884\u5de5\u5177\uff0c\u8bc4\u4f30\u5176\u6d3b\u8dc3\u6027\u3001\u529f\u80fd\u53ca\u5b9e\u8df5\u8005\u504f\u597d\u3002", "result": "32%\u7684\u5de5\u5177\u5728\u8fc7\u53bb\u4e00\u5e74\u5185\u6d3b\u8dc3\u7ef4\u62a4\uff0c50%\u63d0\u4f9b\u504f\u5dee\u68c0\u6d4b\u548c\u7f13\u89e3\u529f\u80fd\uff0c\u4e3b\u8981\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u3002", "conclusion": "\u5f00\u6e90\u516c\u5e73\u6027\u5e72\u9884\u5de5\u5177\u867d\u591a\uff0c\u4f46\u6d3b\u8dc3\u7ef4\u62a4\u7387\u8f83\u4f4e\uff0c\u9700\u63d0\u9ad8\u5b9e\u8df5\u8005\u5bf9\u5176\u7684\u8ba4\u77e5\u548c\u4f7f\u7528\u3002"}}
{"id": "2507.06381", "pdf": "https://arxiv.org/pdf/2507.06381", "abs": "https://arxiv.org/abs/2507.06381", "authors": ["James Hazelden", "Laura Driscoll", "Eli Shlizerman", "Eric Shea-Brown"], "title": "KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks", "categories": ["cs.LG", "cs.AI", "math.DS", "q-bio.NC"], "comment": null, "summary": "Gradient Descent (GD) and its variants are the primary tool for enabling\nefficient training of recurrent dynamical systems such as Recurrent Neural\nNetworks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics\nthat are formed in these models exhibit features such as neural collapse and\nemergence of latent representations that may support the remarkable\ngeneralization properties of networks. In neuroscience, qualitative features of\nthese representations are used to compare learning in biological and artificial\nsystems. Despite recent progress, there remains a need for theoretical tools to\nrigorously understand the mechanisms shaping learned representations,\nespecially in finite, non-linear models. Here, we show that the gradient flow,\nwhich describes how the model's dynamics evolve over GD, can be decomposed into\na product that involves two operators: a Parameter Operator, K, and a\nLinearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in\nfeed-forward neural networks, while P appears in Lyapunov stability and optimal\ncontrol theory. We demonstrate two applications of our decomposition. First, we\nshow how their interplay gives rise to low-dimensional latent dynamics under\nGD, and, specifically, how the collapse is a result of the network structure,\nover and above the nature of the underlying task. Second, for multi-task\ntraining, we show that the operators can be used to measure how objectives\nrelevant to individual sub-tasks align. We experimentally and theoretically\nvalidate these findings, providing an efficient Pytorch package, \\emph{KPFlow},\nimplementing robust analysis tools for general recurrent architectures. Taken\ntogether, our work moves towards building a next stage of understanding of GD\nlearning in non-linear recurrent models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u68af\u5ea6\u6d41\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7406\u89e3\u5faa\u73af\u52a8\u6001\u7cfb\u7edf\u4e2d\u5b66\u4e60\u8868\u793a\u7684\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u68af\u5ea6\u4e0b\u964d\u53ca\u5176\u53d8\u4f53\u5728\u8bad\u7ec3\u5faa\u73af\u52a8\u6001\u7cfb\u7edf\uff08\u5982RNNs\u3001Neural ODEs\u548cGRUs\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u5de5\u5177\u6765\u4e25\u683c\u7406\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u5b66\u4e60\u8868\u793a\u7684\u673a\u5236\u3002", "method": "\u5c06\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u4e24\u4e2a\u7b97\u5b50\uff08\u53c2\u6570\u7b97\u5b50K\u548c\u7ebf\u6027\u5316\u6d41\u4f20\u64ad\u5b50P\uff09\u7684\u4e58\u79ef\uff0c\u5e76\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u4f4e\u7ef4\u6f5c\u5728\u52a8\u6001\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u9a8c\u8bc1\u8868\u660e\uff0c\u8fd9\u79cd\u5206\u89e3\u80fd\u591f\u89e3\u91ca\u7f51\u7edc\u7ed3\u6784\u5982\u4f55\u5bfc\u81f4\u4f4e\u7ef4\u6f5c\u5728\u52a8\u6001\uff0c\u5e76\u8861\u91cf\u591a\u4efb\u52a1\u4e2d\u76ee\u6807\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u7ebf\u6027\u548c\u6709\u9650\u5faa\u73af\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u5bf9\u5176\u7406\u89e3\u7684\u65b0\u9636\u6bb5\u3002"}}
{"id": "2507.06981", "pdf": "https://arxiv.org/pdf/2507.06981", "abs": "https://arxiv.org/abs/2507.06981", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Optimizing Cognitive Networks: Reinforcement Learning Meets Energy Harvesting Over Cascaded Channels", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "This paper presents a reinforcement learning (RL) based approach to improve\nthe physical layer security (PLS) of an underlay cognitive radio network (CRN)\nover cascaded channels. These channels are utilized in highly mobile networks\nsuch as cognitive vehicular networks (CVN). In addition, an eavesdropper aims\nto intercept the communications between secondary users (SUs). The SU receiver\nhas full-duplex and energy harvesting capabilities to generate jamming signals\nto confound the eavesdropper and enhance security. Moreover, the SU transmitter\nextracts energy from ambient radio frequency signals in order to power\nsubsequent transmissions to its intended receiver. To optimize the privacy and\nreliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized\nwhere multiple DQN agents are required such that an agent is assigned at each\nSU transmitter. The objective for the SUs is to determine the optimal\ntransmission power and decide whether to collect energy or transmit messages\nduring each time period in order to maximize their secrecy rate. Thereafter, we\npropose a DQN approach to maximize the throughput of the SUs while respecting\nthe interference threshold acceptable at the receiver of the primary user.\nAccording to our findings, our strategy outperforms two other baseline\nstrategies in terms of security and reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u4e2d\u7269\u7406\u5c42\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u6df1\u5ea6Q\u7f51\u7edc\u4f18\u5316\u4f20\u8f93\u529f\u7387\u548c\u80fd\u91cf\u6536\u96c6\u51b3\u7b56\u3002", "motivation": "\u5728\u9ad8\u5ea6\u79fb\u52a8\u7684\u7f51\u7edc\uff08\u5982\u8ba4\u77e5\u8f66\u8f7d\u7f51\u7edc\uff09\u4e2d\uff0c\u7a83\u542c\u8005\u53ef\u80fd\u62e6\u622a\u901a\u4fe1\uff0c\u56e0\u6b64\u9700\u8981\u589e\u5f3a\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7b56\u7565\uff0c\u6bcf\u4e2a\u6b21\u7ea7\u7528\u6237\u53d1\u5c04\u5668\u5206\u914d\u4e00\u4e2a\u4ee3\u7406\uff0c\u4f18\u5316\u4f20\u8f93\u529f\u7387\u548c\u80fd\u91cf\u6536\u96c6\u51b3\u7b56\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u7684DQN\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6b21\u7ea7\u7528\u6237\u7684\u4fdd\u5bc6\u7387\u548c\u541e\u5410\u91cf\uff0c\u540c\u65f6\u6ee1\u8db3\u4e3b\u7528\u6237\u7684\u5e72\u6270\u9608\u503c\u3002"}}
{"id": "2507.07045", "pdf": "https://arxiv.org/pdf/2507.07045", "abs": "https://arxiv.org/abs/2507.07045", "authors": ["Ugur Ari"], "title": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage", "categories": ["cs.SE", "cs.SI", "68T05", "I.2.7; I.2.6"], "comment": "5 pages, 5 tables. Includes comparative experimental results across\n  OpenAI, Anthropic, DeepSeek, and Gemini LLMs", "summary": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a5C Prompt Contract\u7684\u6846\u67b6\uff0c\u5c06\u63d0\u793a\u8bbe\u8ba1\u7b80\u5316\u4e3a\u4e94\u4e2a\u76f4\u89c2\u7ec4\u4ef6\uff0c\u4ee5\u63d0\u9ad8LLM\u4ea4\u4e92\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u7cfb\u7edf\u53c8\u7b80\u6d01\u7684\u63d0\u793a\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ee5\u964d\u4f4e\u8ba4\u77e5\u8d1f\u62c5\u5e76\u4fdd\u6301\u521b\u9020\u6027\u3002", "method": "\u63d0\u51fa5C Prompt Contract\u6846\u67b6\uff0c\u5305\u62ecCharacter\u3001Cause\u3001Constraint\u3001Contingency\u548cCalibration\u4e94\u4e2a\u7ec4\u4ef6\uff0c\u6574\u5408\u4e86\u5907\u7528\u548c\u8f93\u51fa\u4f18\u5316\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c5C\u6846\u67b6\u5728\u591a\u79cdLLM\u67b6\u6784\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8f93\u5165\u4ee4\u724c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e30\u5bcc\u4e14\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "conclusion": "5C\u6846\u67b6\u7279\u522b\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u4e2a\u4eba\u548c\u4e2d\u5c0f\u4f01\u4e1a\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u7075\u6d3b\u7684AI\u4ea4\u4e92\u3002"}}
{"id": "2507.06402", "pdf": "https://arxiv.org/pdf/2507.06402", "abs": "https://arxiv.org/abs/2507.06402", "authors": ["Siddhant Deshpande", "Yalemzerf Getnet", "Waltenegus Dargie"], "title": "Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning", "categories": ["cs.LG", "cs.CR", "eess.SP"], "comment": null, "summary": "With the proliferation of wireless electrocardiogram (ECG) systems for health\nmonitoring and authentication, protecting signal integrity against tampering is\nbecoming increasingly important. This paper analyzes the performance of CNN,\nResNet, and hybrid Transformer-CNN models for tamper detection. It also\nevaluates the performance of a Siamese network for ECG based identity\nverification. Six tampering strategies, including structured segment\nsubstitutions and random insertions, are emulated to mimic real world attacks.\nThe one-dimensional ECG signals are transformed into a two dimensional\nrepresentation in the time frequency domain using the continuous wavelet\ntransform (CWT). The models are trained and evaluated using ECG data from 54\nsubjects recorded in four sessions 2019 to 2025 outside of clinical settings\nwhile the subjects performed seven different daily activities. Experimental\nresults show that in highly fragmented manipulation scenarios, CNN,\nFeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding\n99.5 percent . Similarly, for subtle manipulations (for example, 50 percent\nfrom A and 50 percent from B and, 75 percent from A and 25 percent from B\nsubstitutions) our FeatCNN-TranCNN model demonstrated consistently reliable\nperformance, achieving an average accuracy of 98 percent . For identity\nverification, the pure Transformer-Siamese network achieved an average accuracy\nof 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model\ndelivered perfect verification performance with 100 percent accuracy.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86CNN\u3001ResNet\u548c\u6df7\u5408Transformer-CNN\u6a21\u578b\u5728\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u7be1\u6539\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4e86Siamese\u7f51\u7edc\u5728ECG\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u79cd\u7be1\u6539\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc799.5%\u3002", "motivation": "\u968f\u7740\u65e0\u7ebfECG\u7cfb\u7edf\u5728\u5065\u5eb7\u76d1\u6d4b\u548c\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u7684\u666e\u53ca\uff0c\u4fdd\u62a4\u4fe1\u53f7\u5b8c\u6574\u6027\u514d\u53d7\u7be1\u6539\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\uff08CWT\uff09\u5c06\u4e00\u7ef4ECG\u4fe1\u53f7\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u65f6\u9891\u8868\u793a\uff0c\u5e76\u8bad\u7ec3\u548c\u8bc4\u4f30CNN\u3001ResNet\u3001\u6df7\u5408Transformer-CNN\u6a21\u578b\u53caSiamese\u7f51\u7edc\u3002\u6a21\u62df\u4e86\u516d\u79cd\u7be1\u6539\u7b56\u7565\u3002", "result": "\u5728\u9ad8\u5ea6\u788e\u7247\u5316\u7be1\u6539\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc799.5%\uff1b\u5bf9\u4e8e\u7ec6\u5fae\u7be1\u6539\uff0cFeatCNN-TranCNN\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u8fbe98%\u3002Siamese\u7f51\u7edc\u4e2d\uff0c\u6df7\u5408CNN-Transformer\u6a21\u578b\u5b9e\u73b0\u4e86100%\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "\u6df7\u5408Transformer-CNN\u548cSiamese\u7f51\u7edc\u5728ECG\u7be1\u6539\u68c0\u6d4b\u548c\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u6df7\u5408CNN-Transformer Siamese\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5b8c\u7f8e\u7684\u9a8c\u8bc1\u6027\u80fd\u3002"}}
{"id": "2507.06983", "pdf": "https://arxiv.org/pdf/2507.06983", "abs": "https://arxiv.org/abs/2507.06983", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Maximizing Reliability in Overlay Radio Networks with Time Switching and Power Splitting Energy Harvesting", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "Cognitive radio networks (CRNs) are acknowledged for their ability to tackle\nthe issue of spectrum under-utilization. In the realm of CRNs, this paper\ninvestigates the energy efficiency issue and addresses the critical challenge\nof optimizing system reliability for overlay CRN access mode. Randomly\ndispersed secondary users (SUs) serving as relays for primary users (PUs) are\nconsidered, in which one of these relays is designated to harvest energy\nthrough the time switching-energy harvesting (EH) protocol. Moreover, this\nrelay amplifies-and-forwards (AF) the PU's messages and broadcasts them along\nwith its own across cascaded $\\kappa$-$\\mu$ fading channels. The power\nsplitting protocol is another EH approach utilized by the SU and PU receivers\nto enhance the amount of energy in their storage devices. In addition, the SU\ntransmitters and the SU receiver are deployed with multiple antennas for\nreception and apply the maximal ratio combining approach. The outage\nprobability is utilized to assess both networks' reliability. Then, an energy\nefficiency evaluation is performed to determine the effectiveness of EH on the\nsystem. Finally, an optimization problem is provided with the goal of\nmaximizing the data rate of the SUs by optimizing the time switching and the\npower allocation parameters of the SU relay.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\uff08CRN\uff09\u4e2d\u7684\u80fd\u91cf\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u65f6\u95f4\u5207\u6362\u548c\u529f\u7387\u5206\u914d\u53c2\u6570\uff0c\u63d0\u5347\u6b21\u7ea7\u7528\u6237\uff08SU\uff09\u7684\u6570\u636e\u901f\u7387\u3002", "motivation": "\u89e3\u51b3\u9891\u8c31\u5229\u7528\u7387\u4e0d\u8db3\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u80fd\u91cf\u6548\u7387\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u5207\u6362-\u80fd\u91cf\u6536\u96c6\uff08EH\uff09\u534f\u8bae\u548c\u529f\u7387\u5206\u914d\u534f\u8bae\uff0c\u7ed3\u5408\u591a\u5929\u7ebf\u6280\u672f\u548c\u6700\u5927\u6bd4\u5408\u5e76\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e2d\u65ad\u6982\u7387\u8bc4\u4f30\u7f51\u7edc\u53ef\u9760\u6027\uff0c\u5e76\u4f18\u5316\u4e86SU\u7684\u6570\u636e\u901f\u7387\u3002", "conclusion": "\u4f18\u5316EH\u534f\u8bae\u548c\u529f\u7387\u5206\u914d\u53ef\u663e\u8457\u63d0\u5347CRN\u7684\u80fd\u91cf\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2507.06250", "pdf": "https://arxiv.org/pdf/2507.06250", "abs": "https://arxiv.org/abs/2507.06250", "authors": ["Zhihao Li", "Kun Li", "Boyang Ma", "Minghui Xu", "Yue Zhang", "Xiuzhen Cheng"], "title": "We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a widely adopted mechanism\nfor connecting large language models to external tools and resources. While MCP\npromises seamless extensibility and rich integrations, it also introduces a\nsubstantially expanded attack surface: any plugin can inherit broad system\nprivileges with minimal isolation or oversight. In this work, we conduct the\nfirst large-scale empirical analysis of MCP security risks. We develop an\nautomated static analysis framework and systematically examine 2,562 real-world\nMCP applications spanning 23 functional categories. Our measurements reveal\nthat network and system resource APIs dominate usage patterns, affecting 1,438\nand 1,237 servers respectively, while file and memory resources are less\nfrequent but still significant. We find that Developer Tools and API\nDevelopment plugins are the most API-intensive, and that less popular plugins\noften contain disproportionately high-risk operations. Through concrete case\nstudies, we demonstrate how insufficient privilege separation enables privilege\nescalation, misinformation propagation, and data tampering. Based on these\nfindings, we propose a detailed taxonomy of MCP resource access, quantify\nsecurity-relevant API usage, and identify open challenges for building safer\nMCP ecosystems, including dynamic permission models and automated trust\nassessment.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9MCP\uff08Model Context Protocol\uff09\u7684\u5b89\u5168\u98ce\u9669\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u63d2\u4ef6\u6743\u9650\u6ee5\u7528\u548c\u9694\u79bb\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "MCP\u867d\u7136\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6269\u5c55\u6027\u548c\u96c6\u6210\u80fd\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u663e\u8457\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u63d2\u4ef6\u6743\u9650\u6ee5\u7528\u548c\u9694\u79bb\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u9759\u6001\u5206\u6790\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e862,562\u4e2a\u5b9e\u9645MCP\u5e94\u7528\uff0c\u6db5\u76d623\u4e2a\u529f\u80fd\u7c7b\u522b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7f51\u7edc\u548c\u7cfb\u7edf\u8d44\u6e90API\u4f7f\u7528\u6700\u5e7f\u6cdb\uff0c\u5f00\u53d1\u8005\u5de5\u5177\u548cAPI\u5f00\u53d1\u63d2\u4ef6\u98ce\u9669\u6700\u9ad8\uff0c\u6743\u9650\u5206\u79bb\u4e0d\u8db3\u5bfc\u81f4\u7279\u6743\u5347\u7ea7\u7b49\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86MCP\u8d44\u6e90\u8bbf\u95ee\u7684\u5206\u7c7b\u6cd5\uff0c\u91cf\u5316\u4e86\u5b89\u5168\u76f8\u5173\u7684API\u4f7f\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u52a8\u6001\u6743\u9650\u6a21\u578b\u548c\u81ea\u52a8\u4fe1\u4efb\u8bc4\u4f30\u7b49\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2507.06432", "pdf": "https://arxiv.org/pdf/2507.06432", "abs": "https://arxiv.org/abs/2507.06432", "authors": ["Mingcheng Zhu", "Yu Liu", "Zhiyao Luo", "Tingting Zhu"], "title": "Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial Intelligence has revolutionised critical care for common\nconditions. Yet, rare conditions in the intensive care unit (ICU), including\nrecognised rare diseases and low-prevalence conditions in the ICU, remain\nunderserved due to data scarcity and intra-condition heterogeneity. To bridge\nsuch gaps, we developed KnowRare, a domain adaptation-based deep learning\nframework for predicting clinical outcomes for rare conditions in the ICU.\nKnowRare mitigates data scarcity by initially learning condition-agnostic\nrepresentations from diverse electronic health records through self-supervised\npre-training. It addresses intra-condition heterogeneity by selectively\nadapting knowledge from clinically similar conditions with a developed\ncondition knowledge graph. Evaluated on two ICU datasets across five clinical\nprediction tasks (90-day mortality, 30-day readmission, ICU mortality,\nremaining length of stay, and phenotyping), KnowRare consistently outperformed\nexisting state-of-the-art models. Additionally, KnowRare demonstrated superior\npredictive performance compared to established ICU scoring systems, including\nAPACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in\nadapting its parameters to accommodate dataset-specific and task-specific\ncharacteristics, its generalisation to common conditions under limited data\nscenarios, and its rationality in selecting source conditions. These findings\nhighlight KnowRare's potential as a robust and practical solution for\nsupporting clinical decision-making and improving care for rare conditions in\nthe ICU.", "AI": {"tldr": "KnowRare\u662f\u4e00\u4e2a\u57fa\u4e8e\u9886\u57df\u9002\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bICU\u4e2d\u7f55\u89c1\u75be\u75c5\u7684\u4e34\u5e8a\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "ICU\u4e2d\u7f55\u89c1\u75be\u75c5\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u800c\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60\u6761\u4ef6\u65e0\u5173\u8868\u793a\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u77e5\u8bc6\u56fe\u9009\u62e9\u6027\u9002\u5e94\u4e34\u5e8a\u76f8\u4f3c\u6761\u4ef6\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u4e94\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548cICU\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u4e86\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "KnowRare\u662f\u652f\u6301ICU\u7f55\u89c1\u75be\u75c5\u4e34\u5e8a\u51b3\u7b56\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06997", "pdf": "https://arxiv.org/pdf/2507.06997", "abs": "https://arxiv.org/abs/2507.06997", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks", "categories": ["eess.SP", "cs.ET", "cs.LG", "cs.NI"], "comment": null, "summary": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u8d855G\u7f51\u7edc\u4e2d\u591a\u8702\u7a9d\u7f51\u7edc\u7684\u7269\u7406\u5c42\u5b89\u5168\u6027\uff08PLS\uff09\u3002\u901a\u8fc7\u6bd4\u8f83DQN\u548cRDPG\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0RDPG\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u4f18\u4e8e\u5206\u5e03\u5f0fDRL\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u591a\u8702\u7a9d\u7f51\u7edc\u4e2d\u5408\u6cd5\u7528\u6237\u4e0e\u7a83\u542c\u8005\u4e4b\u95f4\u7684\u5b89\u5168\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u6570\u636e\u7684\u9690\u79c1\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u7684MARL\u7b56\u7565\uff0c\u6bcf\u4e2a\u57fa\u7ad9\u4f5c\u4e3aDRL\u667a\u80fd\u4f53\uff0c\u4f7f\u7528DQN\u548cRDPG\u4e24\u79cd\u65b9\u6cd5\u4f18\u5316\u4fdd\u5bc6\u901f\u7387\u3002", "result": "\u7ed3\u679c\u8868\u660eRDPG\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u4f18\u4e8e\u5206\u5e03\u5f0fDRL\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5b89\u5168\u6027\u4e0e\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8054\u90a6\u5b66\u4e60\u7684MARL\u7b56\u7565\u5728\u63d0\u5347PLS\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662fRDPG\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.06332", "pdf": "https://arxiv.org/pdf/2507.06332", "abs": "https://arxiv.org/abs/2507.06332", "authors": ["Fuyuan Zhang", "Qichen Wang", "Jianjun Zhao"], "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions", "categories": ["cs.CV", "cs.LG", "cs.SE"], "comment": null, "summary": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions.", "AI": {"tldr": "AR2\u662f\u4e00\u79cd\u901a\u8fc7\u5bf9\u9f50\u5e72\u51c0\u548c\u635f\u574f\u56fe\u50cf\u7684\u7c7b\u6fc0\u6d3b\u56fe\uff08CAMs\uff09\u6765\u589e\u5f3a\u9884\u8bad\u7ec3CNN\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u6539\u53d8\u67b6\u6784\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u635f\u574f\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5e38\u89c1\u635f\u574f\uff08\u5982\u566a\u58f0\u3001\u6a21\u7cca\u3001\u5929\u6c14\u548c\u6570\u5b57\u5931\u771f\uff09\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "AR2\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u7b56\u7565\uff0c\u4ea4\u66ff\u8fdb\u884cCAM\u5f15\u5bfc\u7684\u7ec6\u5316\u548c\u6807\u51c6\u5fae\u8c03\uff0c\u5bf9\u9f50\u5e72\u51c0\u548c\u635f\u574f\u56fe\u50cf\u7684CAMs\u3002", "result": "AR2\u5728\u6807\u51c6\u635f\u574f\u57fa\u51c6\u6d4b\u8bd5\uff08CIFAR-10-C\u3001CIFAR-100-C\u548cImageNet-C\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u5e72\u51c0\u6570\u636e\u7684\u51c6\u786e\u6027\u548c\u635f\u574f\u9c81\u68d2\u6027\u3002", "conclusion": "AR2\u4e3a\u589e\u5f3a\u6a21\u578b\u5728\u591a\u6837\u5316\u635f\u574f\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06433", "pdf": "https://arxiv.org/pdf/2507.06433", "abs": "https://arxiv.org/abs/2507.06433", "authors": ["Niloy Sikder", "Paul Zerr", "Mahdad Jafarzadeh Esfahani", "Martin Dresler", "Matthias Krauledat"], "title": "eegFloss: A Python package for refining sleep EEG recordings using machine learning models", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "comment": "The eegFloss package is available under the MIT License at\n  https://github.com/Niloy333/eegFloss", "summary": "Electroencephalography (EEG) allows monitoring of brain activity, providing\ninsights into the functional dynamics of various brain regions and their roles\nin cognitive processes. EEG is a cornerstone in sleep research, serving as the\nprimary modality of polysomnography, the gold standard in the field. However,\nEEG signals are prone to artifacts caused by both internal (device-specific)\nfactors and external (environmental) interferences. As sleep studies are\nbecoming larger, most rely on automatic sleep staging, a process highly\nsusceptible to artifacts, leading to erroneous sleep scores. This paper\naddresses this challenge by introducing eegFloss, an open-source Python package\nto utilize eegUsability, a novel machine learning (ML) model designed to detect\nsegments with artifacts in sleep EEG recordings. eegUsability has been trained\nand evaluated on manually artifact-labeled EEG data collected from 15\nparticipants over 127 nights using the Zmax headband. It demonstrates solid\noverall classification performance (F1-score is approximately 0.85, Cohens\nkappa is 0.78), achieving a high recall rate of approximately 94% in\nidentifying channel-wise usable EEG data, and extends beyond Zmax.\nAdditionally, eegFloss offers features such as automatic time-in-bed detection\nusing another ML model named eegMobility, filtering out certain artifacts, and\ngenerating hypnograms and sleep statistics. By addressing a fundamental\nchallenge faced by most sleep studies, eegFloss can enhance the precision and\nrigor of their analysis as well as the accuracy and reliability of their\noutcomes.", "AI": {"tldr": "eegFloss\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u68c0\u6d4b\u7761\u7720EEG\u8bb0\u5f55\u4e2d\u7684\u4f2a\u8ff9\uff0c\u63d0\u9ad8\u81ea\u52a8\u7761\u7720\u5206\u671f\u7684\u51c6\u786e\u6027\u3002", "motivation": "EEG\u4fe1\u53f7\u5728\u7761\u7720\u7814\u7a76\u4e2d\u6613\u53d7\u4f2a\u8ff9\u5e72\u6270\uff0c\u5bfc\u81f4\u81ea\u52a8\u7761\u7720\u5206\u671f\u9519\u8bef\uff0c\u5f71\u54cd\u7814\u7a76\u7ed3\u679c\u3002", "method": "\u5f00\u53d1\u4e86eegUsability\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e\u624b\u52a8\u6807\u8bb0\u7684EEG\u6570\u636e\u8bad\u7ec3\uff0c\u7528\u4e8e\u68c0\u6d4b\u4f2a\u8ff9\u3002", "result": "eegUsability\u8868\u73b0\u4f18\u5f02\uff08F1-score\u7ea60.85\uff0cCohen's kappa 0.78\uff09\uff0c\u53ec\u56de\u7387\u7ea694%\u3002", "conclusion": "eegFloss\u80fd\u63d0\u5347\u7761\u7720\u7814\u7a76\u7684\u5206\u6790\u7cbe\u5ea6\u548c\u7ed3\u679c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.06350", "pdf": "https://arxiv.org/pdf/2507.06350", "abs": "https://arxiv.org/abs/2507.06350", "authors": ["Kenneth Odoh"], "title": "An Architecture for Privacy-Preserving Telemetry Scheme", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "We present a privacy-preserving telemetry aggregation scheme. Our underlying\nfrequency estimation routine works within the framework of differential\nprivacy. The design philosophy follows a client-server architecture.\nFurthermore, the system uses a local differential privacy scheme where data\ngets randomized on the client before submitting the request to the resource\nserver. This scheme allows for data analysis on de-identified data by carefully\nadding noise to prevent re-identification attacks, thereby facilitating public\ndata release without compromising the identifiability of the individual record.\nThis work further enhances privacy guarantees by leveraging Oblivious HTTP\n(OHTTP) to achieve increased privacy protection for data in transit that\naddresses pre-existing privacy vulnerabilities in raw HTTP. We provide an\nimplementation that focuses on frequency estimation with a histogram of a known\ndictionary. Our resulting formulation based on OHTTP has provided stricter\nprivacy safeguards when compared to trusting an organization to manually delete\nidentifying information from the client's request in the ingestor as deployed\nin reference work~\\cite{apple2017}. Code available at\nhttps://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u9065\u6d4b\u805a\u5408\u65b9\u6848\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548c\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u5229\u7528\u672c\u5730\u5dee\u5206\u9690\u79c1\u548cOblivious HTTP\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9065\u6d4b\u6570\u636e\u805a\u5408\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u9632\u6b62\u91cd\u65b0\u8bc6\u522b\u653b\u51fb\uff0c\u540c\u65f6\u652f\u6301\u516c\u5f00\u6570\u636e\u53d1\u5e03\u3002", "method": "\u91c7\u7528\u672c\u5730\u5dee\u5206\u9690\u79c1\u65b9\u6848\uff0c\u5ba2\u6237\u7aef\u968f\u673a\u5316\u6570\u636e\u540e\u63d0\u4ea4\u81f3\u670d\u52a1\u5668\uff0c\u7ed3\u5408Oblivious HTTP\u4fdd\u62a4\u4f20\u8f93\u6570\u636e\u9690\u79c1\u3002", "result": "\u65b9\u6848\u5728\u9891\u7387\u4f30\u8ba1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4e25\u683c\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u589e\u5f3a\u4e86\u9690\u79c1\u4fdd\u62a4\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u9690\u79c1\u4fdd\u969c\u7684\u9065\u6d4b\u6570\u636e\u805a\u5408\u573a\u666f\u3002"}}
{"id": "2507.06445", "pdf": "https://arxiv.org/pdf/2507.06445", "abs": "https://arxiv.org/abs/2507.06445", "authors": ["Victoria R. Li", "Jenny Kaufmann", "Martin Wattenberg", "David Alvarez-Melis", "Naomi Saphra"], "title": "Can Interpretation Predict Behavior on Unseen Data?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Interpretability research often aims to predict how a model will respond to\ntargeted interventions on specific mechanisms. However, it rarely predicts how\na model will respond to unseen input data. This paper explores the promises and\nchallenges of interpretability as a tool for predicting out-of-distribution\n(OOD) model behavior. Specifically, we investigate the correspondence between\nattention patterns and OOD generalization in hundreds of Transformer models\nindependently trained on a synthetic classification task. These models exhibit\nseveral distinct systematic generalization rules OOD, forming a diverse\npopulation for correlational analysis. In this setting, we find that simple\nobservational tools from interpretability can predict OOD performance. In\nparticular, when in-distribution attention exhibits hierarchical patterns, the\nmodel is likely to generalize hierarchically on OOD data -- even when the\nrule's implementation does not rely on these hierarchical patterns, according\nto ablation tests. Our findings offer a proof-of-concept to motivate further\ninterpretability work on predicting unseen model behavior.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5728\u9884\u6d4b\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\uff08OOD\uff09\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u901a\u8fc7\u5206\u6790Transformer\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0eOOD\u6cdb\u5316\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u53ef\u4ee5\u9884\u6d4bOOD\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5728\u9884\u6d4b\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u884c\u4e3a\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u63a2\u7d22\u5176\u4f5c\u4e3a\u9884\u6d4b\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u5728\u5408\u6210\u5206\u7c7b\u4efb\u52a1\u4e0a\u72ec\u7acb\u8bad\u7ec3\u6570\u767e\u4e2aTransformer\u6a21\u578b\uff0c\u5206\u6790\u5176\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0eOOD\u6cdb\u5316\u7684\u76f8\u5173\u6027\u3002", "result": "\u53d1\u73b0\u5f53\u6a21\u578b\u5728\u5206\u5e03\u5185\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u5206\u5c42\u6ce8\u610f\u529b\u6a21\u5f0f\u65f6\uff0c\u5176\u5728OOD\u6570\u636e\u4e0a\u4e5f\u53ef\u80fd\u5206\u5c42\u6cdb\u5316\uff0c\u5373\u4f7f\u89c4\u5219\u5b9e\u73b0\u4e0d\u4f9d\u8d56\u8fd9\u4e9b\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u6027\u5de5\u4f5c\u9884\u6d4b\u672a\u89c1\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06396", "pdf": "https://arxiv.org/pdf/2507.06396", "abs": "https://arxiv.org/abs/2507.06396", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Prompt engineering for LLMs remains complex, with existing frameworks either\nhiding complexity behind restrictive APIs or providing inflexible canned\npatterns that resist customization -- making sophisticated agentic programming\nchallenging. We present the Prompt Declaration Language (PDL), a novel approach\nto prompt representation that tackles this fundamental complexity by bringing\nprompts to the forefront, enabling manual and automatic prompt tuning while\ncapturing the composition of LLM calls together with rule-based code and\nexternal tools. By abstracting away the plumbing for such compositions, PDL\naims at improving programmer productivity while providing a declarative\nrepresentation that is amenable to optimization. This paper demonstrates PDL's\nutility through a real-world case study of a compliance agent. Tuning the\nprompting pattern of this agent yielded up to 4x performance improvement\ncompared to using a canned agent and prompt pattern.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Prompt Declaration Language (PDL)\uff0c\u4e00\u79cd\u65b0\u578b\u63d0\u793a\u8868\u793a\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3LLM\u63d0\u793a\u5de5\u7a0b\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u5c06\u63d0\u793a\u7f6e\u4e8e\u6838\u5fc3\u4f4d\u7f6e\uff0c\u652f\u6301\u624b\u52a8\u548c\u81ea\u52a8\u8c03\u6574\uff0c\u5e76\u6574\u5408\u89c4\u5219\u4ee3\u7801\u548c\u5916\u90e8\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\u8981\u4e48\u9690\u85cf\u590d\u6742\u6027\uff0c\u8981\u4e48\u63d0\u4f9b\u4e0d\u7075\u6d3b\u7684\u56fa\u5b9a\u6a21\u5f0f\uff0c\u96be\u4ee5\u652f\u6301\u590d\u6742\u7684\u4ee3\u7406\u7f16\u7a0b\u3002", "method": "\u63d0\u51faPDL\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u8868\u793a\u62bd\u8c61\u7ec4\u5408\u903b\u8f91\uff0c\u652f\u6301\u63d0\u793a\u4f18\u5316\u548c\u81ea\u52a8\u5316\u8c03\u6574\u3002", "result": "\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\uff0c\u4f7f\u7528PDL\u8c03\u6574\u63d0\u793a\u6a21\u5f0f\u4f7f\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe4\u500d\u3002", "conclusion": "PDL\u901a\u8fc7\u7b80\u5316\u63d0\u793a\u5de5\u7a0b\u548c\u4f18\u5316\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a0b\u5e8f\u5458\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.06452", "pdf": "https://arxiv.org/pdf/2507.06452", "abs": "https://arxiv.org/abs/2507.06452", "authors": ["Yigong Hu", "Haodong Zheng", "Yicheng Liu", "Dedong Xie", "Youliang Huang", "Baris Kasikci"], "title": "gigiProfiler: Diagnosing Performance Issues by Uncovering Application Resource Bottlenecks", "categories": ["cs.PF", "cs.SE"], "comment": null, "summary": "Diagnosing performance bottlenecks in modern software is essential yet\nchallenging, particularly as applications become more complex and rely on\ncustom resource management policies. While traditional profilers effectively\nidentify execution bottlenecks by tracing system-level metrics, they fall short\nwhen it comes to application-level resource contention caused by waiting for\napplication-level events. In this work, we introduce OmniResource Profiling, a\nperformance analysis approach that integrates system-level and\napplication-level resource tracing to diagnose resource bottlenecks\ncomprehensively. gigiProfiler, our realization of OmniResource Profiling, uses\na hybrid LLM-static analysis approach to identify application-defined resources\noffline and analyze their impact on performance during buggy executions to\nuncover the performance bottleneck. gigiProfiler then samples and records\ncritical variables related to these bottleneck resources during buggy execution\nand compares their value with those from normal executions to identify the root\ncauses. We evaluated gigiProfiler on 12 real-world performance issues across\nfive applications. gigiProfiler accurately identified performance bottlenecks\nin all cases. gigiProfiler also successfully diagnosed the root causes of two\nnewly emerged, previously undiagnosed problems, with the findings confirmed by\ndevelopers.", "AI": {"tldr": "OmniResource Profiling\u662f\u4e00\u79cd\u7ed3\u5408\u7cfb\u7edf\u7ea7\u548c\u5e94\u7528\u7ea7\u8d44\u6e90\u8ffd\u8e2a\u7684\u6027\u80fd\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7gigiProfiler\u5de5\u5177\u5b9e\u73b0\uff0c\u80fd\u5168\u9762\u8bca\u65ad\u8d44\u6e90\u74f6\u9888\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u6027\u80fd\u74f6\u9888\u8bca\u65ad\u56f0\u96be\uff0c\u4f20\u7edf\u5206\u6790\u5de5\u5177\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u5e94\u7528\u7ea7\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\u3002", "method": "gigiProfiler\u91c7\u7528\u6df7\u5408LLM-\u9759\u6001\u5206\u6790\u65b9\u6cd5\u79bb\u7ebf\u8bc6\u522b\u5e94\u7528\u5b9a\u4e49\u8d44\u6e90\uff0c\u5e76\u5728\u5f02\u5e38\u6267\u884c\u65f6\u5206\u6790\u5176\u6027\u80fd\u5f71\u54cd\u3002", "result": "\u572812\u4e2a\u5b9e\u9645\u6027\u80fd\u95ee\u9898\u4e2d\uff0cgigiProfiler\u51c6\u786e\u8bc6\u522b\u4e86\u6240\u6709\u74f6\u9888\uff0c\u5e76\u6210\u529f\u8bca\u65ad\u4e86\u4e24\u4e2a\u65b0\u95ee\u9898\u3002", "conclusion": "OmniResource Profiling\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5de5\u5177\u65e0\u6cd5\u8986\u76d6\u7684\u5e94\u7528\u7ea7\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2507.06458", "pdf": "https://arxiv.org/pdf/2507.06458", "abs": "https://arxiv.org/abs/2507.06458", "authors": ["Arjun Banerjee", "David Martinez", "Camille Dang", "Ethan Tam"], "title": "Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models", "categories": ["cs.LG", "q-bio.BM"], "comment": "15 pages, 13 figures. Accepted to Proceedings of the Workshop on\n  Generative AI for Biology at the 42nd International Conference on Machine\n  Learning (Spotlight)", "summary": "Protein language models (PLMs) encode rich biological information, yet their\ninternal neuron representations are poorly understood. We introduce the first\nautomated framework for labeling every neuron in a PLM with biologically\ngrounded natural language descriptions. Unlike prior approaches relying on\nsparse autoencoders or manual annotation, our method scales to hundreds of\nthousands of neurons, revealing individual neurons are selectively sensitive to\ndiverse biochemical and structural properties. We then develop a novel neuron\nactivation-guided steering method to generate proteins with desired traits,\nenabling convergence to target biochemical properties like molecular weight and\ninstability index as well as secondary and tertiary structural motifs,\nincluding alpha helices and canonical Zinc Fingers. We finally show that\nanalysis of labeled neurons in different model sizes reveals PLM scaling laws\nand a structured neuron space distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6807\u6ce8\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u795e\u7ecf\u5143\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u5143\u5bf9\u751f\u7269\u5316\u5b66\u548c\u7ed3\u6784\u7279\u6027\u7684\u9009\u62e9\u6027\u654f\u611f\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3PLM\u5185\u90e8\u795e\u7ecf\u5143\u7684\u751f\u7269\u4fe1\u606f\u7f16\u7801\u673a\u5236\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6216\u624b\u52a8\u6807\u6ce8\uff09\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316\u6846\u67b6\u6807\u6ce8PLM\u795e\u7ecf\u5143\uff0c\u5f00\u53d1\u795e\u7ecf\u5143\u6fc0\u6d3b\u5f15\u5bfc\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u795e\u7ecf\u5143\u5bf9\u591a\u6837\u751f\u7269\u5316\u5b66\u548c\u7ed3\u6784\u7279\u6027\u7684\u9009\u62e9\u6027\u654f\u611f\uff0c\u5b9e\u73b0\u4e86\u76ee\u6807\u86cb\u767d\u8d28\u7279\u6027\u7684\u9ad8\u6548\u8bbe\u8ba1\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86PLM\u795e\u7ecf\u5143\u7684\u751f\u7269\u4fe1\u606f\u7f16\u7801\u89c4\u5f8b\uff0c\u8fd8\u4e3a\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.06497", "pdf": "https://arxiv.org/pdf/2507.06497", "abs": "https://arxiv.org/abs/2507.06497", "authors": ["Sarah Ali Siddiqui", "Chandra Thapa", "Derui Wang", "Rayne Holland", "Wei Shao", "Seyit Camtepe", "Hajime Suzuki", "Rajiv Shah"], "title": "TELSAFE: Security Gap Quantitative Risk Assessment Framework", "categories": ["cs.CR", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "Gaps between established security standards and their practical\nimplementation have the potential to introduce vulnerabilities, possibly\nexposing them to security risks. To effectively address and mitigate these\nsecurity and compliance challenges, security risk management strategies are\nessential. However, it must adhere to well-established strategies and industry\nstandards to ensure consistency, reliability, and compatibility both within and\nacross organizations. In this paper, we introduce a new hybrid risk assessment\nframework called TELSAFE, which employs probabilistic modeling for quantitative\nrisk assessment and eliminates the influence of expert opinion bias. The\nframework encompasses both qualitative and quantitative assessment phases,\nfacilitating effective risk management strategies tailored to the unique\nrequirements of organizations. A specific use case utilizing Common\nVulnerabilities and Exposures (CVE)-related data demonstrates the framework's\napplicability and implementation in real-world scenarios, such as in the\ntelecommunications industry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTELSAFE\u7684\u6df7\u5408\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u6982\u7387\u5efa\u6a21\u6d88\u9664\u4e13\u5bb6\u610f\u89c1\u504f\u89c1\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u5982\u7535\u4fe1\u884c\u4e1a\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u6807\u51c6\u4e0e\u5b9e\u9645\u5b9e\u65bd\u4e4b\u95f4\u7684\u5dee\u8ddd\u53ef\u80fd\u5bfc\u81f4\u6f0f\u6d1e\u548c\u5b89\u5168\u98ce\u9669\uff0c\u9700\u6709\u6548\u7ba1\u7406\u7b56\u7565\u3002", "method": "\u63d0\u51faTELSAFE\u6846\u67b6\uff0c\u7ed3\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5229\u7528\u6982\u7387\u5efa\u6a21\u8fdb\u884c\u5b9a\u91cf\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u6846\u67b6\u5728\u5b9e\u9645\u6848\u4f8b\uff08\u5982\u7535\u4fe1\u884c\u4e1a\uff09\u4e2d\u5c55\u793a\u4e86\u9002\u7528\u6027\u548c\u5b9e\u65bd\u6548\u679c\u3002", "conclusion": "TELSAFE\u6846\u67b6\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u7684\u98ce\u9669\u7ba1\u7406\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u4e0e\u5b9e\u8df5\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.06461", "pdf": "https://arxiv.org/pdf/2507.06461", "abs": "https://arxiv.org/abs/2507.06461", "authors": ["Risi Jaiswal", "Supriyo Datta", "Joseph G. Makin"], "title": "Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm", "categories": ["cs.LG", "cs.NE"], "comment": "24 pages, 5 figures, 4 tables. Under review", "summary": "Reducing energy consumption has become a pressing need for modern machine\nlearning, which has achieved many of its most impressive results by scaling to\nlarger and more energy-consumptive neural networks. Unfortunately, the main\nalgorithm for training such networks, backpropagation, poses significant\nchallenges for custom hardware accelerators, due to both its serial\ndependencies and the memory footprint needed to store forward activations for\nthe backward pass. Alternatives to backprop, although less effective, do exist;\nhere the main computational bottleneck becomes matrix multiplication. In this\nstudy, we derive forward-forward algorithms for binary, stochastic units.\nBinarization of the activations transforms matrix multiplications into indexing\noperations, which can be executed efficiently in hardware. Stochasticity,\ncombined with tied weights across units with different biases, bypasses the\ninformation bottleneck imposed by binary units. Furthermore, although slow and\nexpensive in traditional hardware, binary sampling that is very fast can be\nimplemented cheaply with p-bits (probabilistic bits), novel devices made up of\nunstable magnets. We evaluate our proposed algorithms on the MNIST,\nFashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to\nreal-valued forward-forward, but with an estimated energy savings of about one\norder of magnitude.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u7684\u4e8c\u503c\u968f\u673a\u5355\u5143\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5728\u786c\u4ef6\u52a0\u901f\u4e0a\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e8c\u503c\u968f\u673a\u5355\u5143\u7684\u524d\u5411-\u524d\u5411\u7b97\u6cd5\uff0c\u5229\u7528\u786c\u4ef6\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\u66ff\u4ee3\u3002", "result": "\u5728MNIST\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u63a5\u8fd1\u5b9e\u503c\u7b97\u6cd5\uff0c\u80fd\u8017\u964d\u4f4e\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u52a0\u901f\u573a\u666f\u3002"}}
{"id": "2507.06584", "pdf": "https://arxiv.org/pdf/2507.06584", "abs": "https://arxiv.org/abs/2507.06584", "authors": ["Qiong Feng", "Xiaotian Ma", "Ziyuan Feng", "Marat Akhin", "Wei Song", "Peng Liang"], "title": "Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing", "categories": ["cs.PL", "cs.SE"], "comment": "The 40th ACM SIGPLAN International Conference on Object-Oriented\n  Programming, Systems, Languages, and Applications (OOPSLA)", "summary": "Compilers play a central role in translating high-level code into executable\nprograms, making their correctness essential for ensuring code safety and\nreliability. While extensive research has focused on verifying the correctness\nof compilers for single-language compilation, the correctness of cross-language\ncompilation - which involves the interaction between two languages and their\nrespective compilers - remains largely unexplored. To fill this research gap,\nwe propose CrossLangFuzzer, a novel framework that introduces a universal\nintermediate representation (IR) for JVM-based languages and automatically\ngenerates cross-language test programs with diverse type parameters and complex\ninheritance structures. After generating the initial IR, CrossLangFuzzer\napplies three mutation techniques - LangShuffler, FunctionRemoval, and\nTypeChanger - to enhance program diversity. By evaluating both the original and\nmutated programs across multiple compiler versions, CrossLangFuzzer\nsuccessfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed\nbugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2\nconfirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java\ncompiler. Among all mutators, TypeChanger is the most effective, detecting 11\nof the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes\nof cross-compilation bugs, examining the respective responsibilities of\nlanguage compilers when incorrect behavior occurs during cross-language\ncompilation. To the best of our knowledge, this is the firstwork specifically\nfocused on identifying and diagnosing compiler bugs in cross-language\ncompilation scenarios. Our research helps to understand these challenges and\ncontributes to improving compiler correctness in multi-language environments.", "AI": {"tldr": "CrossLangFuzzer\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u8de8\u8bed\u8a00\u7f16\u8bd1\u5668\u9519\u8bef\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\u548c\u7a81\u53d8\u6280\u672f\u53d1\u73b0\u591a\u4e2a\u7f16\u8bd1\u5668\u4e2d\u7684\u9519\u8bef\u3002", "motivation": "\u8de8\u8bed\u8a00\u7f16\u8bd1\u7684\u6b63\u786e\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u5de5\u4f5c\u591a\u5173\u6ce8\u5355\u8bed\u8a00\u7f16\u8bd1\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faCrossLangFuzzer\u6846\u67b6\uff0c\u4f7f\u7528\u901a\u7528\u4e2d\u95f4\u8868\u793a\u548c\u4e09\u79cd\u7a81\u53d8\u6280\u672f\uff08LangShuffler\u3001FunctionRemoval\u3001TypeChanger\uff09\u751f\u6210\u591a\u6837\u5316\u6d4b\u8bd5\u7a0b\u5e8f\u3002", "result": "\u53d1\u73b024\u4e2a\u7f16\u8bd1\u5668\u9519\u8bef\uff08Kotlin\u3001Groovy\u3001Scala 3\u3001Scala 2\u3001Java\uff09\uff0c\u5176\u4e2dTypeChanger\u6700\u6709\u6548\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u8de8\u8bed\u8a00\u7f16\u8bd1\u9519\u8bef\uff0c\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u7f16\u8bd1\u5668\u6b63\u786e\u6027\u6539\u8fdb\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.06464", "pdf": "https://arxiv.org/pdf/2507.06464", "abs": "https://arxiv.org/abs/2507.06464", "authors": ["Hanyang Peng", "Shuang Qin", "Yue Yu", "Fangqing Jiang", "Hui Wang", "Wen Gao"], "title": "SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam", "categories": ["cs.LG", "cs.AI"], "comment": "20pages, 11pages", "summary": "Adam has proven remarkable successful in training deep neural networks, but\nthe mechanisms underlying its empirical successes and limitations remain\nunderexplored. In this study, we demonstrate that the effectiveness of Adam\nstems largely from its similarity to SignSGD in robustly handling large\ngradient fluctuations, yet it is also vulnerable to destabilizing loss spikes\ndue to its uncontrolled update scaling. To enhance the advantage of Adam and\nmitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with\nthree key innovations. \\emph{First}, S3 generalizes the sign-like update by\nemploying a flexible $p$-th order momentum ($p \\geq 1$) in the denominator,\ndeparting from the conventional second-order momentum (variance)\npreconditioning. This design enables enhanced performance while achieving\nstable training even with aggressive learning rates. \\emph{Second}, S3\nminimizes the occurrences of loss spikes through unified exponential moving\naverage coefficients for numerator and denominator momenta, which inherently\nbound updates to $[-1, 1]$ and simplify hyperparameter tuning. \\emph{Third}, S3\nincorporates an equivalent Nesterov's accelerated gradient(NAG) module,\naccelerating convergence without memory overhead. Theoretically, we prove that\nS3 achieves the optimal convergence rate of\n$O\\left(\\frac{1}{T^{\\sfrac{1}{4}}}\\right)$ for general nonconvex stochastic\noptimization under weak assumptions. Extensive experiments across a range of\nvision and language tasks show that \\textsf{\\small S3} not only converges more\nrapidly and improves performance but also rarely experiences loss spikes, even\nwith a \\textbf{$\\bm{10 \\times}$} larger learning rate. In fact, S3 delivers\nperformance comparable to or better than AdamW with \\textbf{$2 \\times$} the\ntraining steps, establishing its efficacy in both efficiency and final task\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSignSoftSGD\uff08S3\uff09\u7684\u65b0\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6539\u8fdbAdam\u7684\u66f4\u65b0\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5176\u68af\u5ea6\u6ce2\u52a8\u548c\u635f\u5931\u5c16\u5cf0\u7684\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "Adam\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u6210\u529f\u673a\u5236\u548c\u5c40\u9650\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u589e\u5f3aAdam\u7684\u4f18\u52bf\u5e76\u7f13\u89e3\u5176\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faS3\u4f18\u5316\u5668\uff0c\u91c7\u7528p\u9636\u52a8\u91cf\u3001\u7edf\u4e00\u7684EMA\u7cfb\u6570\u548cNAG\u6a21\u5757\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u5feb\u901f\u6536\u655b\u3002", "result": "S3\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eAdam\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u635f\u5931\u5c16\u5cf0\u66f4\u5c11\uff0c\u751a\u81f3\u80fd\u4f7f\u752810\u500d\u5b66\u4e60\u7387\u3002", "conclusion": "S3\u5728\u6548\u7387\u548c\u6700\u7ec8\u4efb\u52a1\u6027\u80fd\u4e0a\u5747\u4f18\u4e8eAdamW\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.06907", "pdf": "https://arxiv.org/pdf/2507.06907", "abs": "https://arxiv.org/abs/2507.06907", "authors": ["Linyun Gao", "Qiang Wen", "Fumio Machida"], "title": "Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting", "categories": ["cs.LG", "cs.SE"], "comment": "27 pages including appendix, 1 figure", "summary": "Autonomous driving is rapidly advancing as a key application of machine\nlearning, yet ensuring the safety of these systems remains a critical\nchallenge. Traffic sign recognition, an essential component of autonomous\nvehicles, is particularly vulnerable to adversarial attacks that can compromise\ndriving safety. In this paper, we propose an N-version machine learning (NVML)\nframework that integrates a safety-aware weighted soft voting mechanism. Our\napproach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential\nsafety risks and assign dynamic, safety-aware weights to the ensemble outputs.\nWe evaluate the robustness of three-version NVML systems employing various\nvoting mechanisms against adversarial samples generated using the Fast Gradient\nSign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental\nresults demonstrate that our NVML approach significantly enhances the\nrobustness and safety of traffic sign recognition systems under adversarial\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eN\u7248\u672c\u673a\u5668\u5b66\u4e60\uff08NVML\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u611f\u77e5\u52a0\u6743\u8f6f\u6295\u7968\u673a\u5236\u63d0\u5347\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\uff0c\u5a01\u80c1\u9a7e\u9a76\u5b89\u5168\uff0c\u9700\u63d0\u5347\u5176\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528NVML\u6846\u67b6\uff0c\u7ed3\u5408FMEA\u8bc4\u4f30\u5b89\u5168\u98ce\u9669\uff0c\u52a8\u6001\u5206\u914d\u5b89\u5168\u611f\u77e5\u6743\u91cd\uff0c\u5e76\u901a\u8fc7FGSM\u548cPGD\u653b\u51fb\u6d4b\u8bd5\u4e09\u7248\u672cNVML\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNVML\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "NVML\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06466", "pdf": "https://arxiv.org/pdf/2507.06466", "abs": "https://arxiv.org/abs/2507.06466", "authors": ["Aaron Dharna", "Cong Lu", "Jeff Clune"], "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "67 pages, accepted to RLC 2025", "summary": "Multi-agent interactions have long fueled innovation, from natural\npredator-prey dynamics to the space race. Self-play (SP) algorithms try to\nharness these dynamics by pitting agents against ever-improving opponents,\nthereby creating an implicit curriculum toward learning high-quality solutions.\nHowever, SP often fails to produce diverse solutions and can get stuck in\nlocally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a\nnew direction that leverages the code-generation capabilities and vast\nknowledge of foundation models (FMs) to overcome these challenges by leaping\nacross local optima in policy space. We propose a family of approaches: (1)\n\\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent\npolicies via competitive self-play; (2) \\textbf{Novelty-Search Self-Play\n(NSSP)} builds a diverse population of strategies, ignoring performance; and\n(3) the most promising variant, \\textbf{Quality-Diveristy Self-Play (QDSP)},\ncreates a diverse set of high-quality policies by combining the diversity of\nNSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a\ncontinuous-control pursuer-evader setting, and in Gandalf, a simple AI safety\nsimulation in which an attacker tries to jailbreak an LLM's defenses. In Car\nTag, FMSPs explore a wide variety of reinforcement learning, tree search, and\nheuristic-based methods, to name just a few. In terms of discovered policy\nquality, \\ouralgo and vFMSP surpass strong human-designed strategies. In\nGandalf, FMSPs can successfully automatically red-team an LLM, breaking through\nand jailbreaking six different, progressively stronger levels of defense.\nFurthermore, FMSPs can automatically proceed to patch the discovered\nvulnerabilities. Overall, FMSPs represent a promising new research frontier of\nimproving self-play with foundation models, opening fresh paths toward more\ncreative and open-ended strategy discovery", "AI": {"tldr": "FMSP\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u548c\u5e7f\u6cdb\u77e5\u8bc6\uff0c\u901a\u8fc7\u8de8\u8d8a\u7b56\u7565\u7a7a\u95f4\u7684\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u6539\u8fdb\u81ea\u535a\u5f08\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e09\u79cd\u53d8\u4f53\uff08vFMSP\u3001NSSP\u3001QDSP\uff09\uff0c\u5e76\u5728Car Tag\u548cGandalf\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u81ea\u535a\u5f08\u7b97\u6cd5\u5e38\u56e0\u9677\u5165\u5c40\u90e8\u6700\u4f18\u89e3\u800c\u65e0\u6cd5\u751f\u6210\u591a\u6837\u5316\u7684\u7b56\u7565\uff0cFMSP\u65e8\u5728\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u79cdFMSP\u53d8\u4f53\uff1avFMSP\u901a\u8fc7\u7ade\u4e89\u81ea\u535a\u5f08\u4f18\u5316\u7b56\u7565\uff0cNSSP\u4e13\u6ce8\u4e8e\u7b56\u7565\u591a\u6837\u6027\uff0cQDSP\u7ed3\u5408\u591a\u6837\u6027\u4e0e\u9ad8\u8d28\u91cf\u7b56\u7565\u3002", "result": "\u5728Car Tag\u4e2d\uff0cFMSP\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u7684\u7b56\u7565\uff1b\u5728Gandalf\u4e2d\uff0c\u6210\u529f\u7a81\u7834LLM\u7684\u591a\u5c42\u9632\u5fa1\u5e76\u81ea\u52a8\u4fee\u590d\u6f0f\u6d1e\u3002", "conclusion": "FMSP\u4e3a\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u535a\u5f08\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u66f4\u5177\u521b\u9020\u6027\u548c\u5f00\u653e\u6027\u7684\u7b56\u7565\u53d1\u73b0\u3002"}}
{"id": "2507.06990", "pdf": "https://arxiv.org/pdf/2507.06990", "abs": "https://arxiv.org/abs/2507.06990", "authors": ["Mahee Gamage", "Otso Kinanen", "Jake Muff", "Vlad Stirbu"], "title": "Enhancing Quantum Software Development Process with Experiment Tracking", "categories": ["quant-ph", "cs.SE"], "comment": null, "summary": "As quantum computing advances from theoretical promise to experimental\nreality, the need for rigorous experiment tracking becomes critical. Drawing\ninspiration from best practices in machine learning (ML) and artificial\nintelligence (AI), we argue that reproducibility, scalability, and\ncollaboration in quantum research can benefit significantly from structured\ntracking workflows. This paper explores the application of MLflow in quantum\nresearch, illustrating how it enables better development practices, experiment\nreproducibility, decision making, and cross-domain integration in an\nincreasingly hybrid classical-quantum landscape.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u91cf\u5b50\u7814\u7a76\u4e2d\u5e94\u7528MLflow\u4ee5\u63d0\u5347\u5b9e\u9a8c\u53ef\u91cd\u590d\u6027\u3001\u534f\u4f5c\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u4ece\u7406\u8bba\u8d70\u5411\u5b9e\u9a8c\uff0c\u4e25\u683c\u7684\u5b9e\u9a8c\u8ddf\u8e2a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u501f\u9274\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u7684\u6700\u4f73\u5b9e\u8df5\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u91cf\u5b50\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u534f\u4f5c\u6027\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86MLflow\u5728\u91cf\u5b50\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5982\u4f55\u6539\u5584\u5f00\u53d1\u5b9e\u8df5\u3001\u5b9e\u9a8c\u53ef\u91cd\u590d\u6027\u548c\u8de8\u9886\u57df\u6574\u5408\u3002", "result": "MLflow\u5728\u91cf\u5b50\u7814\u7a76\u4e2d\u80fd\u591f\u6709\u6548\u652f\u6301\u5b9e\u9a8c\u8ddf\u8e2a\u3001\u51b3\u7b56\u548c\u534f\u4f5c\uff0c\u9002\u5e94\u7ecf\u5178-\u91cf\u5b50\u6df7\u5408\u7684\u7814\u7a76\u73af\u5883\u3002", "conclusion": "MLflow\u4e3a\u91cf\u5b50\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u8ddf\u8e2a\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u534f\u4f5c\u6548\u7387\u3002"}}
{"id": "2507.06469", "pdf": "https://arxiv.org/pdf/2507.06469", "abs": "https://arxiv.org/abs/2507.06469", "authors": ["Yudan Song", "Yuecen Wei", "Yuhang Lu", "Qingyun Sun", "Minglai Shao", "Li-e Wang", "Chunming Hu", "Xianxian Li", "Xingcheng Fu"], "title": "Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Graph representation learning has become a mainstream method for fraud\ndetection due to its strong expressive power, which focuses on enhancing node\nrepresentations through improved neighborhood knowledge capture. However, the\nfocus on local interactions leads to imbalanced transmission of global\ntopological information and increased risk of node-specific information being\noverwhelmed during aggregation due to the imbalance between fraud and benign\nnodes. In this paper, we first summarize the impact of topology and class\nimbalance on downstream tasks in GNN-based fraud detection, as the problem of\nimbalanced supervisory messages is caused by fraudsters' topological behavior\nobfuscation and identity feature concealment. Based on statistical validation,\nwe propose a novel dual-view graph representation learning method to mitigate\nMessage imbalance in Fraud Detection(MimbFD). Specifically, we design a\ntopological message reachability module for high-quality node representation\nlearning to penetrate fraudsters' camouflage and alleviate insufficient\npropagation. Then, we introduce a local confounding debiasing module to adjust\nnode representations, enhancing the stable association between node\nrepresentations and labels to balance the influence of different classes.\nFinally, we conducted experiments on three public fraud datasets, and the\nresults demonstrate that MimbFD exhibits outstanding performance in fraud\ndetection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u89c6\u56fe\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08MimbFD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8eGNN\u7684\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u62d3\u6251\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u62d3\u6251\u6d88\u606f\u53ef\u8fbe\u6027\u548c\u5c40\u90e8\u6df7\u6dc6\u53bb\u504f\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u56e0\u5c40\u90e8\u4ea4\u4e92\u5bfc\u81f4\u5168\u5c40\u62d3\u6251\u4fe1\u606f\u4f20\u8f93\u4e0d\u5e73\u8861\uff0c\u4e14\u6b3a\u8bc8\u8282\u70b9\u4fe1\u606f\u6613\u88ab\u6df9\u6ca1\u3002", "method": "\u8bbe\u8ba1\u4e86\u62d3\u6251\u6d88\u606f\u53ef\u8fbe\u6027\u6a21\u5757\u548c\u5c40\u90e8\u6df7\u6dc6\u53bb\u504f\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u7a7f\u900f\u6b3a\u8bc8\u8005\u4f2a\u88c5\u548c\u5e73\u8861\u7c7b\u522b\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6b3a\u8bc8\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cMimbFD\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MimbFD\u80fd\u6709\u6548\u7f13\u89e3\u6d88\u606f\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.07010", "pdf": "https://arxiv.org/pdf/2507.07010", "abs": "https://arxiv.org/abs/2507.07010", "authors": ["Zhiyuan Li", "Kurt G. Schilling", "Bennett A. Landman"], "title": "Robust Containerization of the High Angular Resolution Functional Imaging (HARFI) Pipeline", "categories": ["physics.med-ph", "cs.SE"], "comment": null, "summary": "Historically, functional magnetic resonance imaging (fMRI) of the brain has\nfocused primarily on gray matter, particularly the cortical gray matter and\nassociated nuclei. However, recent work has demonstrated that functional\nactivity in white matter also plays a meaningful role in both cognition and\nlearning. In previous work, we introduced the High Angular Resolution\nFunctional Imaging (HARFI) pipeline, which demonstrated both local and global\npatterns of functional correlation in white matter. Notably, HARFI enabled\nexploration of asymmetric voxel-wise correlation using odd-order spherical\nharmonics. Although the original implementation of HARFI was released via\nGitHub, adoption was limited due to the technical complexity of running the\nsource code. In this work, we present a robust and efficient containerized\nversion of the HARFI pipeline, enabling seamless execution across multiple\npublic datasets. Our goal is to facilitate broader and deeper exploration of\nfunctional white matter architecture, especially through the lens of high\nangular resolution functional correlations. The key innovation of this work is\nthe containerized implementation, which we have made available under a\npermissive open-source license to support reproducible and accessible research\npractices.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HARFI\u7ba1\u9053\u7684\u5bb9\u5668\u5316\u7248\u672c\uff0c\u65e8\u5728\u7b80\u5316\u6280\u672f\u590d\u6742\u6027\uff0c\u4fc3\u8fdb\u5bf9\u767d\u8d28\u529f\u80fd\u7ed3\u6784\u7684\u5e7f\u6cdb\u7814\u7a76\u3002", "motivation": "\u4f20\u7edffMRI\u4e3b\u8981\u5173\u6ce8\u7070\u8d28\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u767d\u8d28\u529f\u80fd\u6d3b\u52a8\u5728\u8ba4\u77e5\u548c\u5b66\u4e60\u4e2d\u4e5f\u6709\u91cd\u8981\u4f5c\u7528\u3002HARFI\u7ba1\u9053\u867d\u5df2\u63d0\u51fa\uff0c\u4f46\u56e0\u6280\u672f\u590d\u6742\u6027\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86HARFI\u7ba1\u9053\u7684\u5bb9\u5668\u5316\u7248\u672c\uff0c\u652f\u6301\u8de8\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684\u4fbf\u6377\u6267\u884c\u3002", "result": "\u5bb9\u5668\u5316\u5b9e\u73b0\u63d0\u9ad8\u4e86HARFI\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u652f\u6301\u9ad8\u89d2\u5ea6\u5206\u8fa8\u7387\u529f\u80fd\u76f8\u5173\u6027\u7814\u7a76\u3002", "conclusion": "\u5bb9\u5668\u5316\u7684HARFI\u7ba1\u9053\u4e3a\u767d\u8d28\u529f\u80fd\u7ed3\u6784\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u8bbf\u95ee\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.06482", "pdf": "https://arxiv.org/pdf/2507.06482", "abs": "https://arxiv.org/abs/2507.06482", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Jiahua Shi", "Jun Shen"], "title": "FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning", "categories": ["cs.LG"], "comment": "19 Pages, ICCV 2025", "summary": "Federated learning aims at training models collaboratively across\nparticipants while protecting privacy. However, one major challenge for this\nparadigm is the data heterogeneity issue, where biased data preferences across\nmultiple clients, harming the model's convergence and performance. In this\npaper, we first introduce powerful diffusion models into the federated learning\nparadigm and show that diffusion representations are effective steers during\nfederated training. To explore the possibility of using diffusion\nrepresentations in handling data heterogeneity, we propose a novel\ndiffusion-inspired Federated paradigm with Diffusion Representation\nCollaboration, termed FedDifRC, leveraging meaningful guidance of diffusion\nmodels to mitigate data heterogeneity. The key idea is to construct text-driven\ndiffusion contrasting and noise-driven diffusion regularization, aiming to\nprovide abundant class-related semantic information and consistent convergence\nsignals. On the one hand, we exploit the conditional feedback from the\ndiffusion model for different text prompts to build a text-driven contrastive\nlearning strategy. On the other hand, we introduce a noise-driven consistency\nregularization to align local instances with diffusion denoising\nrepresentations, constraining the optimization region in the feature space. In\naddition, FedDifRC can be extended to a self-supervised scheme without relying\non any labeled data. We also provide a theoretical analysis for FedDifRC to\nensure convergence under non-convex objectives. The experiments on different\nscenarios validate the effectiveness of FedDifRC and the efficiency of crucial\ncomponents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedDifRC\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6269\u6563\u6a21\u578b\u6765\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5229\u7528\u6587\u672c\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u566a\u58f0\u9a71\u52a8\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u6a21\u578b\u6536\u655b\u548c\u6027\u80fd\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFedDifRC\u65b9\u6cd5\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u9a71\u52a8\u5bf9\u6bd4\u5b66\u4e60\u548c\u566a\u58f0\u9a71\u52a8\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u4f18\u5316\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FedDifRC\u7684\u6709\u6548\u6027\u53ca\u5176\u5173\u952e\u7ec4\u4ef6\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "FedDifRC\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u6307\u5bfc\u6210\u529f\u7f13\u89e3\u4e86\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002"}}
{"id": "2507.06502", "pdf": "https://arxiv.org/pdf/2507.06502", "abs": "https://arxiv.org/abs/2507.06502", "authors": ["Yiwen Liu", "Chenyu Zhang", "Junjie Song", "Siqi Chen", "Sun Yin", "Zihan Wang", "Lingming Zeng", "Yuji Cao", "Junming Jiao"], "title": "MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As a prominent data modality task, time series forecasting plays a pivotal\nrole in diverse applications. With the remarkable advancements in Large\nLanguage Models (LLMs), the adoption of LLMs as the foundational architecture\nfor time series modeling has gained significant attention. Although existing\nmodels achieve some success, they rarely both model time and frequency\ncharacteristics in a pretraining-finetuning paradigm leading to suboptimal\nperformance in predictions of complex time series, which requires both modeling\nperiodicity and prior pattern knowledge of signals. We propose MoFE-Time, an\ninnovative time series forecasting model that integrates time and frequency\ndomain features within a Mixture of Experts (MoE) network. Moreover, we use the\npretraining-finetuning paradigm as our training framework to effectively\ntransfer prior pattern knowledge across pretraining and finetuning datasets\nwith different periodicity distributions. Our method introduces both frequency\nand time cells as experts after attention modules and leverages the MoE routing\nmechanism to construct multidimensional sparse representations of input\nsignals. In experiments on six public benchmarks, MoFE-Time has achieved new\nstate-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared\nto the representative methods Time-MoE. Beyond the existing evaluation\nbenchmarks, we have developed a proprietary dataset, NEV-sales, derived from\nreal-world business scenarios. Our method achieves outstanding results on this\ndataset, underscoring the effectiveness of the MoFE-Time model in practical\ncommercial applications.", "AI": {"tldr": "MoFE-Time\u662f\u4e00\u79cd\u521b\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u7f51\u7edc\uff08MoE\uff09\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u4e0b\u672a\u80fd\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u548c\u9891\u7387\u7279\u5f81\uff0c\u5bfc\u81f4\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMoFE-Time\u6a21\u578b\uff0c\u6574\u5408\u65f6\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u5229\u7528MoE\u7f51\u7edc\u6784\u5efa\u591a\u7ef4\u7a00\u758f\u8868\u793a\uff0c\u91c7\u7528\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoFE-Time\u8868\u73b0\u6700\u4f18\uff0cMSE\u548cMAE\u5206\u522b\u964d\u4f4e6.95%\u548c6.02%\uff1b\u5728\u79c1\u6709\u6570\u636e\u96c6NEV-sales\u4e0a\u4e5f\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "conclusion": "MoFE-Time\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.06516", "pdf": "https://arxiv.org/pdf/2507.06516", "abs": "https://arxiv.org/abs/2507.06516", "authors": ["Yunrui Zhang", "Gustavo Batista", "Salil S. Kanhere"], "title": "Instance-Wise Monotonic Calibration by Constrained Transformation", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to Conference on Uncertainty in Artificial Intelligence\n  (UAI)", "summary": "Deep neural networks often produce miscalibrated probability estimates,\nleading to overconfident predictions. A common approach for calibration is\nfitting a post-hoc calibration map on unseen validation data that transforms\npredicted probabilities. A key desirable property of the calibration map is\ninstance-wise monotonicity (i.e., preserving the ranking of probability\noutputs). However, most existing post-hoc calibration methods do not guarantee\nmonotonicity. Previous monotonic approaches either use an under-parameterized\ncalibration map with limited expressive ability or rely on black-box neural\nnetworks, which lack interpretability and robustness. In this paper, we propose\na family of novel monotonic post-hoc calibration methods, which employs a\nconstrained calibration map parameterized linearly with respect to the number\nof classes. Our proposed approach ensures expressiveness, robustness, and\ninterpretability while preserving the relative ordering of the probability\noutput by formulating the proposed calibration map as a constrained\noptimization problem. Our proposed methods achieve state-of-the-art performance\nacross datasets with different deep neural network models, outperforming\nexisting calibration methods while being data and computation-efficient. Our\ncode is available at\nhttps://github.com/YunruiZhang/Calibration-by-Constrained-Transformation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u8c03\u540e\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53c2\u6570\u5316\u7684\u7ea6\u675f\u6821\u51c6\u6620\u5c04\uff0c\u786e\u4fdd\u8868\u8fbe\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e38\u4ea7\u751f\u6821\u51c6\u4e0d\u8db3\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u73b0\u6709\u540e\u6821\u51c6\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u5355\u8c03\u6027\u6216\u727a\u7272\u8868\u8fbe\u6027/\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u53c2\u6570\u5316\u7684\u7ea6\u675f\u6821\u51c6\u6620\u5c04\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u786e\u4fdd\u5355\u8c03\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u6570\u636e\u9700\u6c42\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u8c03\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.06525", "pdf": "https://arxiv.org/pdf/2507.06525", "abs": "https://arxiv.org/abs/2507.06525", "authors": ["Huiqi Zhang", "Fang Xie"], "title": "AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Differential privacy has been proven effective for stochastic gradient\ndescent; however, existing methods often suffer from performance degradation in\nhigh-dimensional settings, as the scale of injected noise increases with\ndimensionality. To tackle this challenge, we propose AdaDPIGU--a new\ndifferentially private SGD framework with importance-based gradient updates\ntailored for deep neural networks. In the pretraining stage, we apply a\ndifferentially private Gaussian mechanism to estimate the importance of each\nparameter while preserving privacy. During the gradient update phase, we prune\nlow-importance coordinates and introduce a coordinate-wise adaptive clipping\nmechanism, enabling sparse and noise-efficient gradient updates. Theoretically,\nwe prove that AdaDPIGU satisfies $(\\varepsilon, \\delta)$-differential privacy\nand retains convergence guarantees. Extensive experiments on standard\nbenchmarks validate the effectiveness of AdaDPIGU. All results are reported\nunder a fixed retention ratio of 60%. On MNIST, our method achieves a test\naccuracy of 99.12% under a privacy budget of $\\epsilon = 8$, nearly matching\nthe non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at\n$\\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating\nthat adaptive sparsification can enhance both privacy and utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaDPIGU\u7684\u5dee\u5206\u9690\u79c1SGD\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u68af\u5ea6\u66f4\u65b0\u89e3\u51b3\u9ad8\u7ef4\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9690\u79c1\u4fdd\u62a4\u4e0e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u56e0\u566a\u58f0\u589e\u52a0\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u9ad8\u65af\u673a\u5236\u4f30\u8ba1\u53c2\u6570\u91cd\u8981\u6027\uff0c\u68af\u5ea6\u66f4\u65b0\u9636\u6bb5\u526a\u679d\u4f4e\u91cd\u8981\u6027\u5750\u6807\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u88c1\u526a\u673a\u5236\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9690\u79c1\u9884\u7b97\u4e0b\u63a5\u8fd1\u6216\u8d85\u8d8a\u975e\u9690\u79c1\u6a21\u578b\u3002", "conclusion": "AdaDPIGU\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u5316\u540c\u65f6\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u3002"}}
{"id": "2507.06529", "pdf": "https://arxiv.org/pdf/2507.06529", "abs": "https://arxiv.org/abs/2507.06529", "authors": ["Fengxue Zhang", "Yuxin Chen"], "title": "Direct Regret Optimization in Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization (BO) is a powerful paradigm for optimizing expensive\nblack-box functions. Traditional BO methods typically rely on separate\nhand-crafted acquisition functions and surrogate models for the underlying\nfunction, and often operate in a myopic manner. In this paper, we propose a\nnovel direct regret optimization approach that jointly learns the optimal model\nand non-myopic acquisition by distilling from a set of candidate models and\nacquisitions, and explicitly targets minimizing the multi-step regret. Our\nframework leverages an ensemble of Gaussian Processes (GPs) with varying\nhyperparameters to generate simulated BO trajectories, each guided by an\nacquisition function chosen from a pool of conventional choices, until a\nBayesian early stop criterion is met. These simulated trajectories, capturing\nmulti-step exploration strategies, are used to train an end-to-end decision\ntransformer that directly learns to select next query points aimed at improving\nthe ultimate objective. We further adopt a dense training--sparse learning\nparadigm: The decision transformer is trained offline with abundant simulated\ndata sampled from ensemble GPs and acquisitions, while a limited number of real\nevaluations refine the GPs online. Experimental results on synthetic and\nreal-world benchmarks suggest that our method consistently outperforms BO\nbaselines, achieving lower simple regret and demonstrating more robust\nexploration in high-dimensional or noisy settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u9057\u61be\u4f18\u5316\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6700\u4f18\u6a21\u578b\u548c\u975e\u8fd1\u89c6\u83b7\u53d6\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u83b7\u53d6\u51fd\u6570\u548c\u4ee3\u7406\u6a21\u578b\uff0c\u4e14\u901a\u5e38\u662f\u8fd1\u89c6\u7684\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u96c6\u5408\u751f\u6210\u6a21\u62df\u8f68\u8ff9\uff0c\u8bad\u7ec3\u7aef\u5230\u7aef\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u76f4\u63a5\u5b66\u4e60\u9009\u62e9\u67e5\u8be2\u70b9\u4ee5\u51cf\u5c11\u591a\u6b65\u9057\u61be\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7b80\u5355\u9057\u61be\u548c\u66f4\u9c81\u68d2\u7684\u63a2\u7d22\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u548c\u975e\u8fd1\u89c6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06535", "pdf": "https://arxiv.org/pdf/2507.06535", "abs": "https://arxiv.org/abs/2507.06535", "authors": ["Shan Shen", "Shenglu Hua", "Jiajun Zou", "Jiawei Liu", "Jianwang Zhai", "Chuan Shi", "Wenjian Yu"], "title": "Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by ICCAD2025. This is the initial version. Minor changes\n  will be made", "summary": "Graph representation learning on Analog-Mixed Signal (AMS) circuits is\ncrucial for various downstream tasks, e.g., parasitic estimation. However, the\nscarcity of design data, the unbalanced distribution of labels, and the\ninherent diversity of circuit implementations pose significant challenges to\nlearning robust and transferable circuit representations. To address these\nlimitations, we propose CircuitGCL, a novel graph contrastive learning\nframework that integrates representation scattering and label rebalancing to\nenhance transferability across heterogeneous circuit graphs. CircuitGCL employs\na self-supervised strategy to learn topology-invariant node embeddings through\nhyperspherical representation scattering, eliminating dependency on large-scale\ndata. Simultaneously, balanced mean squared error (MSE) and softmax\ncross-entropy (bsmCE) losses are introduced to mitigate label distribution\ndisparities between circuits, enabling robust and transferable parasitic\nestimation. Evaluated on parasitic capacitance estimation (edge-level task) and\nground capacitance classification (node-level task) across TSMC 28nm AMS\ndesigns, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the\n$R^2$ improvement of $33.64\\% \\sim 44.20\\%$ for edge regression and F1-score\ngain of $0.9\\times \\sim 2.1\\times$ for node classification. Our code is\navailable at\n\\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCircuitGCL\uff0c\u4e00\u79cd\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3AMS\u7535\u8def\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u4e0d\u5e73\u8861\u548c\u7535\u8def\u591a\u6837\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bc4\u751f\u4f30\u8ba1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AMS\u7535\u8def\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u4e0d\u5e73\u8861\u548c\u7535\u8def\u591a\u6837\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5bc4\u751f\u4f30\u8ba1\uff09\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faCircuitGCL\u6846\u67b6\uff0c\u7ed3\u5408\u8868\u793a\u6563\u5c04\u548c\u6807\u7b7e\u91cd\u5e73\u8861\u6280\u672f\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u548c\u5e73\u8861\u635f\u5931\u51fd\u6570\uff08\u5e73\u8861MSE\u548cbsmCE\uff09\u3002", "result": "\u5728TSMC 28nm AMS\u8bbe\u8ba1\u4e0a\uff0cCircuitGCL\u5728\u8fb9\u7ea7\u56de\u5f52\u4efb\u52a1\u4e2dR\u00b2\u63d0\u534733.64%~44.20%\uff0c\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2dF1\u5206\u6570\u63d0\u53470.9~2.1\u500d\u3002", "conclusion": "CircuitGCL\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6807\u7b7e\u5e73\u8861\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u8def\u56fe\u8868\u793a\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2507.06538", "pdf": "https://arxiv.org/pdf/2507.06538", "abs": "https://arxiv.org/abs/2507.06538", "authors": ["Shan Shen", "Yibin Zhang", "Hector Rodriguez Rodriguez", "Wenjian Yu"], "title": "Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Published in Proceedings of DAC2025", "summary": "Graph representation learning is a powerful method to extract features from\ngraph-structured data, such as analog/mixed-signal (AMS) circuits. However,\ntraining deep learning models for AMS designs is severely limited by the\nscarcity of integrated circuit design data. In this work, we present\nCircuitGPS, a few-shot learning method for parasitic effect prediction in AMS\ncircuits. The circuit netlist is represented as a heterogeneous graph, with the\ncoupling capacitance modeled as a link. CircuitGPS is pre-trained on link\nprediction and fine-tuned on edge regression. The proposed method starts with a\nsmall-hop sampling technique that converts a link or a node into a subgraph.\nThen, the subgraph embeddings are learned with a hybrid graph Transformer.\nAdditionally, CircuitGPS integrates a low-cost positional encoding that\nsummarizes the positional and structural information of the sampled subgraph.\nCircuitGPS improves the accuracy of coupling existence by at least 20\\% and\nreduces the MAE of capacitance estimation by at least 0.067 compared to\nexisting methods. Our method demonstrates strong inherent scalability, enabling\ndirect application to diverse AMS circuit designs through zero-shot learning.\nFurthermore, the ablation studies provide valuable insights into graph models\nfor representation learning.", "AI": {"tldr": "CircuitGPS\u662f\u4e00\u79cd\u7528\u4e8eAMS\u7535\u8def\u4e2d\u5bc4\u751f\u6548\u5e94\u9884\u6d4b\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f02\u6784\u56fe\u8868\u793a\u548c\u6df7\u5408\u56feTransformer\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u6570\u636e\u7a00\u7f3a\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8eAMS\u8bbe\u8ba1\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5c06\u7535\u8def\u7f51\u8868\u8868\u793a\u4e3a\u5f02\u6784\u56fe\uff0c\u91c7\u7528\u5c0f\u8df3\u91c7\u6837\u6280\u672f\u751f\u6210\u5b50\u56fe\uff0c\u7ed3\u5408\u6df7\u5408\u56feTransformer\u548c\u4f4e\u6210\u672c\u4f4d\u7f6e\u7f16\u7801\u5b66\u4e60\u5b50\u56fe\u5d4c\u5165\u3002", "result": "\u8026\u5408\u5b58\u5728\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u81f3\u5c1120%\uff0c\u7535\u5bb9\u4f30\u8ba1MAE\u964d\u4f4e\u81f3\u5c110.067\uff0c\u5177\u6709\u5f3a\u6269\u5c55\u6027\u548c\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "CircuitGPS\u5728\u5bc4\u751f\u6548\u5e94\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.06549", "pdf": "https://arxiv.org/pdf/2507.06549", "abs": "https://arxiv.org/abs/2507.06549", "authors": ["Shan Shen", "Dingcheng Yang", "Yuyang Xie", "Chunyan Pei", "Wenjian Yu", "Bei Yu"], "title": "Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs", "categories": ["cs.LG", "cs.AR", "cs.SY", "eess.SY"], "comment": "Published in Proceedings of GLSVLSI2024", "summary": "To achieve higher system energy efficiency, SRAM in SoCs is often customized.\nThe parasitic effects cause notable discrepancies between pre-layout and\npost-layout circuit simulations, leading to difficulty in converging design\nparameters and excessive design iterations. Is it possible to well predict the\nparasitics based on the pre-layout circuit, so as to perform parasitic-aware\npre-layout simulation? In this work, we propose a deep-learning-based 2-stage\nmodel to accurately predict these parasitics in pre-layout stages. The model\ncombines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron\n(MLP) regressors, effectively managing class imbalance of the net parasitics in\nSRAM circuits. We also employ Focal Loss to mitigate the impact of abundant\ninternal net samples and integrate subcircuit information into the graph to\nabstract the hierarchical structure of schematics. Experiments on 4 real SRAM\ndesigns show that our approach not only surpasses the state-of-the-art model in\nparasitic prediction by a maximum of 19X reduction of error but also\nsignificantly boosts the simulation process by up to 598X speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76842\u9636\u6bb5\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u9884\u5e03\u5c40\u9636\u6bb5\u51c6\u786e\u9884\u6d4bSRAM\u7535\u8def\u4e2d\u7684\u5bc4\u751f\u6548\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u4eff\u771f\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3SRAM\u8bbe\u8ba1\u4e2d\u9884\u5e03\u5c40\u4e0e\u540e\u5e03\u5c40\u4eff\u771f\u4e4b\u95f4\u7684\u5bc4\u751f\u6548\u5e94\u5dee\u5f02\u95ee\u9898\uff0c\u51cf\u5c11\u8bbe\u8ba1\u8fed\u4ee3\u6b21\u6570\u3002", "method": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5206\u7c7b\u5668\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u56de\u5f52\u5668\uff0c\u91c7\u7528Focal Loss\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5e76\u6574\u5408\u5b50\u7535\u8def\u4fe1\u606f\u4ee5\u62bd\u8c61\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u57284\u4e2a\u5b9e\u9645SRAM\u8bbe\u8ba1\u4e2d\uff0c\u6a21\u578b\u5c06\u5bc4\u751f\u9884\u6d4b\u8bef\u5dee\u6700\u5927\u964d\u4f4e19\u500d\uff0c\u4eff\u771f\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe598\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5bc4\u751f\u6548\u5e94\u9884\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\u548c\u4eff\u771f\u901f\u5ea6\u3002"}}
{"id": "2507.06558", "pdf": "https://arxiv.org/pdf/2507.06558", "abs": "https://arxiv.org/abs/2507.06558", "authors": ["Zicheng Zhang", "Haoran Li", "Yifeng Zhang", "Guoqiang Gong", "Jiaxing Wang", "Pengzhang Liu", "Qixia Jiang", "Junxing Hu"], "title": "The Primacy of Magnitude in Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning\nlarge models. While recent spectral initialization methods improve convergence\nand performance over the naive \"Noise & Zeros\" scheme, their extra\ncomputational and storage overhead undermines efficiency. In this paper, we\nestablish update magnitude as the fundamental driver of LoRA performance and\npropose LoRAM, a magnitude-driven \"Basis & Basis\" initialization scheme that\nmatches spectral methods without their inefficiencies. Our key contributions\nare threefold: (i) Magnitude of weight updates determines convergence. We prove\nlow-rank structures intrinsically bound update magnitudes, unifying\nhyperparameter tuning in learning rate, scaling factor, and initialization as\nmechanisms to optimize magnitude regulation. (ii) Spectral initialization\nsucceeds via magnitude amplification. We demystify that the presumed\nknowledge-driven benefit of the spectral component essentially arises from the\nboost in the weight update magnitude. (iii) A novel and compact initialization\nstrategy, LoRAM, scales deterministic orthogonal bases using pretrained weight\nmagnitudes to simulate spectral gains. Extensive experiments show that LoRAM\nserves as a strong baseline, retaining the full efficiency of LoRA while\nmatching or outperforming spectral initialization across benchmarks.", "AI": {"tldr": "LoRAM\u662f\u4e00\u79cd\u57fa\u4e8e\u66f4\u65b0\u5e45\u5ea6\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5339\u914d\u5149\u8c31\u65b9\u6cd5\u7684\u6027\u80fd\u4f46\u907f\u514d\u4e86\u5176\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\u3002", "motivation": "\u5149\u8c31\u521d\u59cb\u5316\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u5e26\u6765\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\uff0c\u5f71\u54cd\u4e86\u6548\u7387\u3002", "method": "\u63d0\u51faLoRAM\uff0c\u901a\u8fc7\u5e45\u5ea6\u9a71\u52a8\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u5e45\u5ea6\u7f29\u653e\u786e\u5b9a\u6027\u6b63\u4ea4\u57fa\u6765\u6a21\u62df\u5149\u8c31\u589e\u76ca\u3002", "result": "LoRAM\u5728\u4fdd\u6301LoRA\u6548\u7387\u7684\u540c\u65f6\uff0c\u5339\u914d\u6216\u4f18\u4e8e\u5149\u8c31\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "conclusion": "\u66f4\u65b0\u5e45\u5ea6\u662fLoRA\u6027\u80fd\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0cLoRAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002"}}
{"id": "2507.06573", "pdf": "https://arxiv.org/pdf/2507.06573", "abs": "https://arxiv.org/abs/2507.06573", "authors": ["Xinjie Chen", "Minpeng Liao", "Guoxin Chen", "Chengxi Li", "Biao Fu", "Kai Fan", "Xinggao Liu"], "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Work in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently advanced\nthe reasoning capabilities of large language models (LLMs). While prior work\nhas emphasized algorithmic design, data curation, and reward shaping, we\ninvestigate RLVR from a sample-centric perspective and introduce LPPO\n(Learning-Progress and Prefix-guided Optimization), a framework of progressive\noptimization techniques. Our work addresses a critical question: how to best\nleverage a small set of trusted, high-quality demonstrations, rather than\nsimply scaling up data volume. First, motivated by how hints aid human\nproblem-solving, we propose prefix-guided sampling, an online data augmentation\nmethod that incorporates partial solution prefixes from expert demonstrations\nto guide the policy, particularly for challenging instances. Second, inspired\nby how humans focus on important questions aligned with their current\ncapabilities, we introduce learning-progress weighting, a dynamic strategy that\nadjusts each training sample's influence based on model progression. We\nestimate sample-level learning progress via an exponential moving average of\nper-sample pass rates, promoting samples that foster learning and\nde-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks\ndemonstrate that our methods outperform strong baselines, yielding faster\nconvergence and a higher performance ceiling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLPPO\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u7f00\u5f15\u5bfc\u91c7\u6837\u548c\u5b66\u4e60\u8fdb\u5ea6\u52a0\u6743\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6709\u6548\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\uff0c\u800c\u975e\u5355\u7eaf\u589e\u52a0\u6570\u636e\u91cf\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u524d\u7f00\u5f15\u5bfc\u91c7\u6837\u548c\u5b66\u4e60\u8fdb\u5ea6\u52a0\u6743\u4e24\u79cd\u6280\u672f\uff0c\u524d\u8005\u5229\u7528\u4e13\u5bb6\u793a\u8303\u7684\u90e8\u5206\u89e3\u524d\u7f00\u6307\u5bfc\u7b56\u7565\uff0c\u540e\u8005\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6837\u672c\u6743\u91cd\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u4e0a\u9650\u66f4\u9ad8\u3002", "conclusion": "LPPO\u6846\u67b6\u901a\u8fc7\u6837\u672c\u4e2d\u5fc3\u89c6\u89d2\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.06582", "pdf": "https://arxiv.org/pdf/2507.06582", "abs": "https://arxiv.org/abs/2507.06582", "authors": ["Peter N. Loxley", "Friedrich T. Sommer"], "title": "Learning controllable dynamics through informative exploration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Environments with controllable dynamics are usually understood in terms of\nexplicit models. However, such models are not always available, but may\nsometimes be learned by exploring an environment. In this work, we investigate\nusing an information measure called \"predicted information gain\" to determine\nthe most informative regions of an environment to explore next. Applying\nmethods from reinforcement learning allows good suboptimal exploring policies\nto be found, and leads to reliable estimates of the underlying controllable\ndynamics. This approach is demonstrated by comparing with several myopic\nexploration approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u9884\u6d4b\u4fe1\u606f\u589e\u76ca\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a2\u7d22\u73af\u5883\u4e2d\u4fe1\u606f\u91cf\u6700\u5927\u7684\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u627e\u5230\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002", "motivation": "\u5728\u65e0\u6cd5\u83b7\u53d6\u663e\u5f0f\u6a21\u578b\u7684\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u63a2\u7d22\u5b66\u4e60\u53ef\u63a7\u52a8\u6001\u662f\u7814\u7a76\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u4f7f\u7528\u201c\u9884\u6d4b\u4fe1\u606f\u589e\u76ca\u201d\u4f5c\u4e3a\u4fe1\u606f\u5ea6\u91cf\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5bfb\u627e\u9ad8\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u5730\u4f30\u8ba1\u73af\u5883\u7684\u53ef\u63a7\u52a8\u6001\uff0c\u5e76\u5728\u4e0e\u51e0\u79cd\u77ed\u89c6\u63a2\u7d22\u65b9\u6cd5\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u63a2\u7d22\u7b56\u7565\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u4e0d\u53ef\u7528\u4f46\u9700\u5b66\u4e60\u52a8\u6001\u7684\u73af\u5883\u3002"}}
{"id": "2507.06602", "pdf": "https://arxiv.org/pdf/2507.06602", "abs": "https://arxiv.org/abs/2507.06602", "authors": ["Burak Demirel", "Yu Wang", "Cristian Tatino", "Pablo Soldati"], "title": "Generalization in Reinforcement Learning for Radio Access Networks", "categories": ["cs.LG"], "comment": null, "summary": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) encodes cell\ntopology and node attributes via attention-based graph representations; (ii)\napplies domain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u4ee3\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u73af\u5883\u52a8\u6001\u4e14\u5f02\u6784\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u8d44\u6e90\u7ba1\u7406\u7b97\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u90e8\u7f72\u573a\u666f\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u6784\u5efa\u56fe\u8868\u793a\u7f51\u7edc\u62d3\u6251\u548c\u8282\u70b9\u5c5e\u6027\uff0c\u7ed3\u5408\u9886\u57df\u968f\u673a\u5316\u6269\u5c55\u8bad\u7ec3\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u751f\u6210\u548c\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a5G\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\uff0c\u5c24\u5176\u5728\u9ad8\u901f\u79fb\u52a8\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u9a71\u52a8\u76846G\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06613", "pdf": "https://arxiv.org/pdf/2507.06613", "abs": "https://arxiv.org/abs/2507.06613", "authors": ["Anshuk Uppal", "Yuhta Takida", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "24 pages, 8 figures and 7 tables", "summary": "Disentangled and interpretable latent representations in generative models\ntypically come at the cost of generation quality. The $\\beta$-VAE framework\nintroduces a hyperparameter $\\beta$ to balance disentanglement and\nreconstruction quality, where setting $\\beta > 1$ introduces an information\nbottleneck that favors disentanglement over sharp, accurate reconstructions. To\naddress this trade-off, we propose a novel generative modeling framework that\nleverages a range of $\\beta$ values to learn multiple corresponding latent\nrepresentations. First, we obtain a slew of representations by training a\nsingle variational autoencoder (VAE), with a new loss function that controls\nthe information retained in each latent representation such that the higher\n$\\beta$ value prioritize disentanglement over reconstruction fidelity. We then,\nintroduce a non-linear diffusion model that smoothly transitions latent\nrepresentations corresponding to different $\\beta$ values. This model denoises\ntowards less disentangled and more informative representations, ultimately\nleading to (almost) lossless representations, enabling sharp reconstructions.\nFurthermore, our model supports sample generation without input images,\nfunctioning as a standalone generative model. We evaluate our framework in\nterms of both disentanglement and generation quality. Additionally, we observe\nsmooth transitions in the latent spaces with respect to changes in $\\beta$,\nfacilitating consistent manipulation of generated outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u03b2\u503c\u7684VAE\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e73\u8861\u89e3\u7f20\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u548c\u89e3\u7f20\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u89e3\u7f20\u8868\u793a\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u8bad\u7ec3\u5355\u4e00VAE\uff0c\u4f7f\u7528\u65b0\u635f\u5931\u51fd\u6570\u63a7\u5236\u4e0d\u540c\u03b2\u503c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u6269\u6563\u6a21\u578b\u5e73\u6ed1\u8fc7\u6e21\u8868\u793a\u3002", "result": "\u6a21\u578b\u5728\u89e3\u7f20\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u65e0\u8f93\u5165\u56fe\u50cf\u7684\u6837\u672c\u751f\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u89e3\u7f20\u4e0e\u91cd\u5efa\u8d28\u91cf\uff0c\u652f\u6301\u7075\u6d3b\u7684\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u3002"}}
{"id": "2507.06615", "pdf": "https://arxiv.org/pdf/2507.06615", "abs": "https://arxiv.org/abs/2507.06615", "authors": ["Jinmin He", "Kai Li", "Yifan Zang", "Haobo Fu", "Qiang Fu", "Junliang Xing", "Jian Cheng"], "title": "Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "NeurIPS2024", "summary": "Multi-task reinforcement learning endeavors to efficiently leverage shared\ninformation across various tasks, facilitating the simultaneous learning of\nmultiple tasks. Existing approaches primarily focus on parameter sharing with\ncarefully designed network structures or tailored optimization procedures.\nHowever, they overlook a direct and complementary way to exploit cross-task\nsimilarities: the control policies of tasks already proficient in some skills\ncan provide explicit guidance for unmastered tasks to accelerate skills\nacquisition. To this end, we present a novel framework called Cross-Task Policy\nGuidance (CTPG), which trains a guide policy for each task to select the\nbehavior policy interacting with the environment from all tasks' control\npolicies, generating better training trajectories. In addition, we propose two\ngating mechanisms to improve the learning efficiency of CTPG: one gate filters\nout control policies that are not beneficial for guidance, while the other gate\nblocks tasks that do not necessitate guidance. CTPG is a general framework\nadaptable to existing parameter sharing approaches. Empirical evaluations\ndemonstrate that incorporating CTPG with these approaches significantly\nenhances performance in manipulation and locomotion benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCTPG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u7b56\u7565\u6307\u5bfc\u52a0\u901f\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u5df2\u638c\u63e1\u4efb\u52a1\u7684\u7b56\u7565\u6307\u5bfc\u672a\u638c\u63e1\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u95e8\u63a7\u673a\u5236\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53c2\u6570\u5171\u4eab\uff0c\u4f46\u5ffd\u7565\u4e86\u5229\u7528\u5df2\u638c\u63e1\u4efb\u52a1\u7684\u7b56\u7565\u76f4\u63a5\u6307\u5bfc\u672a\u638c\u63e1\u4efb\u52a1\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faCTPG\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u4e00\u4e2a\u6307\u5bfc\u7b56\u7565\uff0c\u4ece\u6240\u6709\u4efb\u52a1\u7684\u63a7\u5236\u7b56\u7565\u4e2d\u9009\u62e9\u884c\u4e3a\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u95e8\u63a7\u673a\u5236\u4f18\u5316\u5b66\u4e60\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cCTPG\u4e0e\u73b0\u6709\u53c2\u6570\u5171\u4eab\u65b9\u6cd5\u7ed3\u5408\u540e\uff0c\u5728\u64cd\u4f5c\u548c\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "CTPG\u662f\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u80fd\u6709\u6548\u5229\u7528\u8de8\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u52a0\u901f\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2507.06619", "pdf": "https://arxiv.org/pdf/2507.06619", "abs": "https://arxiv.org/abs/2507.06619", "authors": ["Xiaobo Huang", "Fang Xie"], "title": "Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "When applying machine learning to medical image classification, data leakage\nis a critical issue. Previous methods, such as adding noise to gradients for\ndifferential privacy, work well on large datasets like MNIST and CIFAR-100, but\nfail on small, imbalanced medical datasets like HAM10000. This is because the\nimbalanced distribution causes gradients from minority classes to be clipped\nand lose crucial information, while majority classes dominate. This leads the\nmodel to fall into suboptimal solutions early. To address this, we propose\nSAD-DPSGD, which uses a linear decaying mechanism for noise and clipping\nthresholds. By allocating more privacy budget and using higher clipping\nthresholds in the initial training phases, the model avoids suboptimal\nsolutions and enhances performance. Experiments show that SAD-DPSGD outperforms\nAuto-DPSGD on HAM10000, improving accuracy by 2.15% under $\\epsilon = 3.0$ ,\n$\\delta = 10^{-3}$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSAD-DPSGD\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u566a\u58f0\u548c\u88c1\u526a\u9608\u503c\uff0c\u89e3\u51b3\u4e86\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u6570\u636e\u6cc4\u6f0f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8eAuto-DPSGD\u3002", "motivation": "\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u6570\u636e\u6cc4\u6f0f\u662f\u4e25\u91cd\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u578b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff08\u5982HAM10000\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6a21\u578b\u9677\u5165\u6b21\u4f18\u89e3\u3002", "method": "\u63d0\u51faSAD-DPSGD\uff0c\u91c7\u7528\u7ebf\u6027\u8870\u51cf\u673a\u5236\u52a8\u6001\u8c03\u6574\u566a\u58f0\u548c\u88c1\u526a\u9608\u503c\uff0c\u521d\u671f\u5206\u914d\u66f4\u591a\u9690\u79c1\u9884\u7b97\u548c\u66f4\u9ad8\u88c1\u526a\u9608\u503c\u3002", "result": "\u5728HAM10000\u6570\u636e\u96c6\u4e0a\uff0cSAD-DPSGD\u6bd4Auto-DPSGD\u51c6\u786e\u7387\u63d0\u9ad82.15%\uff08\u03b5=3.0\uff0c\u03b4=10^-3\uff09\u3002", "conclusion": "SAD-DPSGD\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9690\u79c1\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.06624", "pdf": "https://arxiv.org/pdf/2507.06624", "abs": "https://arxiv.org/abs/2507.06624", "authors": ["Dazhi Fu", "Jicong Fan"], "title": "UniOD: A Universal Model for Outlier Detection across Diverse Domains", "categories": ["cs.LG"], "comment": "20 pages, 4 figures", "summary": "Outlier detection (OD) seeks to distinguish inliers and outliers in\ncompletely unlabeled datasets and plays a vital role in science and\nengineering. Most existing OD methods require troublesome dataset-specific\nhyperparameter tuning and costly model training before they can be deployed to\nidentify outliers. In this work, we propose UniOD, a universal OD framework\nthat leverages labeled datasets to train a single model capable of detecting\noutliers of datasets from diverse domains. Specifically, UniOD converts each\ndataset into multiple graphs, produces consistent node features, and frames\noutlier detection as a node-classification task, and is able to generalize to\nunseen domains. As a result, UniOD avoids effort on model selection and\nhyperparameter tuning, reduces computational cost, and effectively utilizes the\nknowledge from historical datasets, which improves the convenience and accuracy\nin real applications. We evaluate UniOD on 15 benchmark OD datasets against 15\nstate-of-the-art baselines, demonstrating its effectiveness.", "AI": {"tldr": "UniOD\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6807\u8bb0\u6570\u636e\u96c6\u8bad\u7ec3\u5355\u4e00\u6a21\u578b\uff0c\u907f\u514d\u7e41\u7410\u7684\u8d85\u53c2\u6570\u8c03\u6574\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u6574\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u6210\u672c\u9ad8\u3002UniOD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u6d4b\u7684\u4fbf\u5229\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "UniOD\u5c06\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u591a\u4e2a\u56fe\uff0c\u751f\u6210\u4e00\u81f4\u7684\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5c06\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u8f6c\u5316\u4e3a\u8282\u70b9\u5206\u7c7b\u95ee\u9898\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9886\u57df\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e0e15\u79cd\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86UniOD\u7684\u6709\u6548\u6027\u3002", "conclusion": "UniOD\u907f\u514d\u4e86\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u6574\u7684\u9ebb\u70e6\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u6709\u6548\u5229\u7528\u5386\u53f2\u6570\u636e\u96c6\u7684\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u4fbf\u5229\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06628", "pdf": "https://arxiv.org/pdf/2507.06628", "abs": "https://arxiv.org/abs/2507.06628", "authors": ["Jinmin He", "Kai Li", "Yifan Zang", "Haobo Fu", "Qiang Fu", "Junliang Xing", "Jian Cheng"], "title": "Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML2025", "summary": "Offline multi-task reinforcement learning aims to learn a unified policy\ncapable of solving multiple tasks using only pre-collected task-mixed datasets,\nwithout requiring any online interaction with the environment. However, it\nfaces significant challenges in effectively sharing knowledge across tasks.\nInspired by the efficient knowledge abstraction observed in human learning, we\npropose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed\nto extract and utilize reusable skills to enhance knowledge transfer and task\nperformance. Our approach uncovers reusable skills through a goal-oriented\nskill extraction process and leverages vector quantization to construct a\ndiscrete skill library. To mitigate class imbalances between broadly applicable\nand task-specific skills, we introduce a skill enhancement phase to refine the\nextracted skills. Furthermore, we integrate these skills using hierarchical\npolicy learning, enabling the construction of a high-level policy that\ndynamically orchestrates discrete skills to accomplish specific tasks.\nExtensive experiments on diverse robotic manipulation tasks within the\nMetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.", "AI": {"tldr": "GO-Skill\u662f\u4e00\u79cd\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u6280\u80fd\u63d0\u53d6\u548c\u5206\u5c42\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u5347\u77e5\u8bc6\u5171\u4eab\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u8de8\u4efb\u52a1\u77e5\u8bc6\u5171\u4eab\u7684\u6311\u6218\uff0c\u53d7\u4eba\u7c7b\u5b66\u4e60\u542f\u53d1\uff0c\u63d0\u51faGO-Skill\u4ee5\u63d0\u53d6\u53ef\u91cd\u7528\u6280\u80fd\u3002", "method": "\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u6280\u80fd\u63d0\u53d6\u548c\u5411\u91cf\u91cf\u5316\u6784\u5efa\u79bb\u6563\u6280\u80fd\u5e93\uff0c\u5f15\u5165\u6280\u80fd\u589e\u5f3a\u9636\u6bb5\u4f18\u5316\u6280\u80fd\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u7b56\u7565\u5b66\u4e60\u52a8\u6001\u534f\u8c03\u6280\u80fd\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGO-Skill\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "GO-Skill\u901a\u8fc7\u6280\u80fd\u62bd\u8c61\u548c\u5206\u5c42\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u5171\u4eab\u95ee\u9898\u3002"}}
{"id": "2507.06631", "pdf": "https://arxiv.org/pdf/2507.06631", "abs": "https://arxiv.org/abs/2507.06631", "authors": ["Enda D. V. Bigarella"], "title": "Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator", "categories": ["cs.LG"], "comment": null, "summary": "This document reports on a method for detecting and preventing overfitting on\ndata regressions, herein applied to mesh-like data structures. The mesh\nstructure allows for the straightforward computation of the Laplace-operator\nsecond-order derivatives in a finite-difference fashion for noiseless data.\nDerivatives of the training data are computed on the original training mesh to\nserve as a true label of the entropy of the training data. Derivatives of the\ntrained data are computed on a staggered mesh to identify oscillations in the\ninterior of the original training mesh cells. The loss of the Laplace-operator\nderivatives is used for hyperparameter optimisation, achieving a reduction of\nunwanted oscillation through the minimisation of the entropy of the trained\nmodel. In this setup, testing does not require the splitting of points from the\ntraining data, and training is thus directly performed on all available\ntraining points. The Laplace operator applied to the trained data on a\nstaggered mesh serves as a surrogate testing metric based on diffusion\nproperties.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u548c\u9632\u6b62\u7f51\u683c\u6570\u636e\u56de\u5f52\u8fc7\u62df\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u4e8c\u9636\u5bfc\u6570\u8ba1\u7b97\u71b5\uff0c\u4f18\u5316\u8d85\u53c2\u6570\u4ee5\u51cf\u5c11\u632f\u8361\u3002", "motivation": "\u89e3\u51b3\u7f51\u683c\u6570\u636e\u56de\u5f52\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u907f\u514d\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u632f\u8361\u73b0\u8c61\u3002", "method": "\u5728\u539f\u59cb\u8bad\u7ec3\u7f51\u683c\u4e0a\u8ba1\u7b97\u5bfc\u6570\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u5728\u4ea4\u9519\u7f51\u683c\u4e0a\u8ba1\u7b97\u8bad\u7ec3\u6570\u636e\u7684\u5bfc\u6570\u4ee5\u8bc6\u522b\u632f\u8361\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5bfc\u6570\u635f\u5931\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u6700\u5c0f\u5316\u8bad\u7ec3\u6a21\u578b\u7684\u71b5\uff0c\u51cf\u5c11\u4e86\u4e0d\u5e0c\u671b\u7684\u632f\u8361\uff0c\u65e0\u9700\u5206\u5272\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fdb\u884c\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u7f51\u683c\u6570\u636e\u56de\u5f52\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7279\u6027\u7684\u66ff\u4ee3\u6d4b\u8bd5\u6307\u6807\u3002"}}
{"id": "2507.06650", "pdf": "https://arxiv.org/pdf/2507.06650", "abs": "https://arxiv.org/abs/2507.06650", "authors": ["Hui Meng", "Keping Yang", "Xuyu Peng", "Bo Zheng"], "title": "Deep Disentangled Representation Network for Treatment Effect Estimation", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "Estimating individual-level treatment effect from observational data is a\nfundamental problem in causal inference and has attracted increasing attention\nin the fields of education, healthcare, and public policy.In this work, we\nconcentrate on the study of disentangled representation methods that have shown\npromising outcomes by decomposing observed covariates into instrumental,\nconfounding, and adjustment factors. However, most of the previous work has\nprimarily revolved around generative models or hard decomposition methods for\ncovariates, which often struggle to guarantee the attainment of precisely\ndisentangled factors. In order to effectively model different causal\nrelationships, we propose a novel treatment effect estimation algorithm that\nincorporates a mixture of experts with multi-head attention and a linear\northogonal regularizer to softly decompose the pre-treatment variables, and\nsimultaneously eliminates selection bias via importance sampling re-weighting\ntechniques. We conduct extensive experiments on both public semi-synthetic and\nreal-world production datasets. The experimental results clearly demonstrate\nthat our algorithm outperforms the state-of-the-art methods focused on\nindividual treatment effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ebf\u6027\u6b63\u4ea4\u6b63\u5219\u5316\u5668\uff0c\u901a\u8fc7\u8f6f\u5206\u89e3\u9884\u5904\u7406\u53d8\u91cf\u5e76\u6d88\u9664\u9009\u62e9\u504f\u5dee\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u89c2\u6d4b\u6570\u636e\u4e2d\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u786e\u5206\u89e3\u53d8\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ebf\u6027\u6b63\u4ea4\u6b63\u5219\u5316\u5668\u8f6f\u5206\u89e3\u53d8\u91cf\uff0c\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\u91cd\u52a0\u6743\u6280\u672f\u6d88\u9664\u9009\u62e9\u504f\u5dee\u3002", "result": "\u5728\u516c\u5f00\u534a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.06652", "pdf": "https://arxiv.org/pdf/2507.06652", "abs": "https://arxiv.org/abs/2507.06652", "authors": ["Arthur Alexander Lim", "Zhen Bin It", "Jovan Bowen Heng", "Tee Hui Teo"], "title": "Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making", "categories": ["cs.LG"], "comment": null, "summary": "Fuzzy systems are a way to allow machines, systems and frameworks to deal\nwith uncertainty, which is not possible in binary systems that most computers\nuse. These systems have already been deployed for certain use cases, and fuzzy\nsystems could be further improved as proposed in this paper. Such technologies\nto draw inspiration from include machine learning and federated learning.\nMachine learning is one of the recent breakthroughs of technology and could be\napplied to fuzzy systems to further improve the results it produces. Federated\nlearning is also one of the recent technologies that have huge potential, which\nallows machine learning training to improve by reducing privacy risk, reducing\nburden on networking infrastructure, and reducing latency of the latest model.\nAspects from federated learning could be used to improve federated learning,\nsuch as applying the idea of updating the fuzzy rules that make up a key part\nof fuzzy systems, to further improve it over time. This paper discusses how\nthese improvements would be implemented in fuzzy systems, and how it would\nimprove fuzzy systems. It also discusses certain limitations on the potential\nimprovements. It concludes that these proposed ideas and improvements require\nfurther investigation to see how far the improvements are, but the potential is\nthere to improve fuzzy systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u6539\u8fdb\u6a21\u7cca\u7cfb\u7edf\uff0c\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u8ba8\u8bba\u5176\u6f5c\u5728\u6539\u8fdb\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u6a21\u7cca\u7cfb\u7edf\u80fd\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7684\u65b0\u6280\u672f\u3002", "method": "\u63d0\u51fa\u5c06\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7684\u7406\u5ff5\u5e94\u7528\u4e8e\u6a21\u7cca\u7cfb\u7edf\uff0c\u4f8b\u5982\u66f4\u65b0\u6a21\u7cca\u89c4\u5219\u3002", "result": "\u6f5c\u5728\u6539\u8fdb\u5305\u62ec\u63d0\u9ad8\u6a21\u7cca\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9a8c\u8bc1\u3002", "conclusion": "\u6539\u8fdb\u6a21\u7cca\u7cfb\u7edf\u7684\u6f5c\u529b\u5b58\u5728\uff0c\u4f46\u9700\u66f4\u591a\u7814\u7a76\u4ee5\u786e\u5b9a\u5176\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2507.06694", "pdf": "https://arxiv.org/pdf/2507.06694", "abs": "https://arxiv.org/abs/2507.06694", "authors": ["Raffael Theiler", "Olga Fink"], "title": "Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": "25 pages, 9 figures", "summary": "Accurate short-term state forecasting is essential for efficient and stable\noperation of modern power systems, especially in the context of increasing\nvariability introduced by renewable and distributed energy resources. As these\nsystems evolve rapidly, it becomes increasingly important to reliably predict\ntheir states in the short term to ensure operational stability, support control\ndecisions, and enable interpretable monitoring of sensor and machine behavior.\nModern power systems often span multiple physical domains - including\nelectrical, mechanical, hydraulic, and thermal - posing significant challenges\nfor modeling and prediction. Graph Neural Networks (GNNs) have emerged as a\npromising data-driven framework for system state estimation and state\nforecasting in such settings. By leveraging the topological structure of sensor\nnetworks, GNNs can implicitly learn inter-sensor relationships and propagate\ninformation across the network. However, most existing GNN-based methods are\ndesigned under the assumption of homogeneous sensor relationships and are\ntypically constrained to a single physical domain. This limitation restricts\ntheir ability to integrate and reason over heterogeneous sensor data commonly\nencountered in real-world energy systems, such as those used in energy\nconversion infrastructure. In this work, we propose the use of Heterogeneous\nGraph Attention Networks to address these limitations. Our approach models both\nhomogeneous intra-domain and heterogeneous inter-domain relationships among\nsensor data from two distinct physical domains - hydraulic and electrical -\nwhich exhibit fundamentally different temporal dynamics. Experimental results\ndemonstrate that our method significantly outperforms conventional baselines on\naverage by 35.5% in terms of normalized root mean square error, confirming its\neffectiveness in multi-domain, multi-rate power system state forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u57df\u3001\u591a\u901f\u7387\u7cfb\u7edf\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u56e0\u53ef\u518d\u751f\u80fd\u6e90\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u7684\u5f15\u5165\u800c\u53d8\u5f97\u590d\u6742\u591a\u53d8\uff0c\u9700\u8981\u53ef\u9760\u7684\u77ed\u671f\u72b6\u6001\u9884\u6d4b\u4ee5\u786e\u4fdd\u7a33\u5b9a\u8fd0\u884c\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u57df\u5f02\u6784\u6570\u636e\u3002", "method": "\u91c7\u7528\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08Heterogeneous Graph Attention Networks\uff09\uff0c\u5efa\u6a21\u4f20\u611f\u5668\u6570\u636e\u7684\u540c\u8d28\u57df\u5185\u548c\u5f02\u8d28\u57df\u95f4\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u5e73\u5747\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf35.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u57df\u3001\u591a\u901f\u7387\u7684\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.06701", "pdf": "https://arxiv.org/pdf/2507.06701", "abs": "https://arxiv.org/abs/2507.06701", "authors": ["Michael Bloesch", "Markus Wulfmeier", "Philemon Brakel", "Todor Davchev", "Martina Zambelli", "Jost Tobias Springenberg", "Abbas Abdolmaleki", "William F Whitney", "Nicolas Heess", "Roland Hafner", "Martin Riedmiller"], "title": "Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement", "categories": ["cs.LG"], "comment": null, "summary": "Imitation Learning from Observation (IfO) offers a powerful way to learn\nbehaviors at large-scale: Unlike behavior cloning or offline reinforcement\nlearning, IfO can leverage action-free demonstrations and thus circumvents the\nneed for costly action-labeled demonstrations or reward functions. However,\ncurrent IfO research focuses on idealized scenarios with mostly bimodal-quality\ndata distributions, restricting the meaningfulness of the results. In contrast,\nthis paper investigates more nuanced distributions and introduces a method to\nlearn from such data, moving closer to a paradigm in which imitation learning\ncan be performed iteratively via self-improvement. Our method adapts RL-based\nimitation learning to action-free demonstrations, using a value function to\ntransfer information between expert and non-expert data. Through comprehensive\nevaluation, we delineate the relation between different data distributions and\nthe applicability of algorithms and highlight the limitations of established\nmethods. Our findings provide valuable insights for developing more robust and\npractical IfO techniques on a path to scalable behaviour learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6a21\u4eff\u5b66\u4e60\u89c2\u5bdf\u65b9\u6cd5\uff08IfO\uff09\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u52a8\u4f5c\u6f14\u793a\u6570\u636e\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u52a8\u4f5c\u6807\u7b7e\u6216\u5956\u52b1\u51fd\u6570\u7684\u9700\u6c42\uff0c\u5e76\u7814\u7a76\u4e86\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u52a8\u4f5c\u6807\u7b7e\u6216\u5956\u52b1\u51fd\u6570\uff0c\u6210\u672c\u9ad8\u6602\u3002IfO\u53ef\u4ee5\u5229\u7528\u65e0\u52a8\u4f5c\u6f14\u793a\u6570\u636e\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u7406\u60f3\u5316\u573a\u666f\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4ef7\u503c\u51fd\u6570\u5728\u4e13\u5bb6\u4e0e\u975e\u4e13\u5bb6\u6570\u636e\u95f4\u4f20\u9012\u4fe1\u606f\uff0c\u9002\u5e94\u65e0\u52a8\u4f5c\u6f14\u793a\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0e\u7b97\u6cd5\u9002\u7528\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u5b9e\u7528\u7684IfO\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u884c\u4e3a\u5b66\u4e60\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.06712", "pdf": "https://arxiv.org/pdf/2507.06712", "abs": "https://arxiv.org/abs/2507.06712", "authors": ["Ayoub Farkane", "Mohamed Boutayeb", "Mustapha Oudani", "Mounir Ghogho"], "title": "PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems", "categories": ["cs.LG", "math.DS", "nlin.CD"], "comment": null, "summary": "State estimation for nonlinear dynamical systems is a critical challenge in\ncontrol and engineering applications, particularly when only partial and noisy\nmeasurements are available. This paper introduces a novel Adaptive\nPhysics-Informed Neural Network-based Observer (PINN-Obs) for accurate state\nestimation in nonlinear systems. Unlike traditional model-based observers,\nwhich require explicit system transformations or linearization, the proposed\nframework directly integrates system dynamics and sensor data into a\nphysics-informed learning process. The observer adaptively learns an optimal\ngain matrix, ensuring convergence of the estimated states to the true system\nstates. A rigorous theoretical analysis establishes formal convergence\nguarantees, demonstrating that the proposed approach achieves uniform error\nminimization under mild observability conditions. The effectiveness of PINN-Obs\nis validated through extensive numerical simulations on diverse nonlinear\nsystems, including an induction motor model, a satellite motion system, and\nbenchmark academic examples. Comparative experimental studies against existing\nobserver designs highlight its superior accuracy, robustness, and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN-Obs\uff09\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u89c2\u6d4b\u5668\u3002", "motivation": "\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u5728\u90e8\u5206\u548c\u566a\u58f0\u6d4b\u91cf\u4e0b\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7cfb\u7edf\u53d8\u6362\u6216\u7ebf\u6027\u5316\u3002", "method": "\u76f4\u63a5\u96c6\u6210\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u4f20\u611f\u5668\u6570\u636e\u5230\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u6700\u4f18\u589e\u76ca\u77e9\u9635\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6536\u655b\u6027\uff0c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6837\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PINN-Obs\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c2\u6d4b\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2507.06752", "pdf": "https://arxiv.org/pdf/2507.06752", "abs": "https://arxiv.org/abs/2507.06752", "authors": ["Heng Wu", "Benzhuo Lu"], "title": "Mathematical artificial data for operator learning", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "68T07, 35J05", "I.2.6; G.1.8; G.4"], "comment": "22 pages, 5 figures", "summary": "Machine learning has emerged as a transformative tool for solving\ndifferential equations (DEs), yet prevailing methodologies remain constrained\nby dual limitations: data-driven methods demand costly labeled datasets while\nmodel-driven techniques face efficiency-accuracy trade-offs. We present the\nMathematical Artificial Data (MAD) framework, a new paradigm that integrates\nphysical laws with data-driven learning to facilitate large-scale operator\ndiscovery. By exploiting DEs' intrinsic mathematical structure to generate\nphysics-embedded analytical solutions and associated synthetic data, MAD\nfundamentally eliminates dependence on experimental or simulated training data.\nThis enables computationally efficient operator learning across multi-parameter\nsystems while maintaining mathematical rigor. Through numerical demonstrations\nspanning 2D parametric problems where both the boundary values and source term\nare functions, we showcase MAD's generalizability and superior\nefficiency/accuracy across various DE scenarios. This\nphysics-embedded-data-driven framework and its capacity to handle complex\nparameter spaces gives it the potential to become a universal paradigm for\nphysics-informed machine intelligence in scientific computing.", "AI": {"tldr": "\u63d0\u51fa\u4e86MAD\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u4e2d\u7684\u6570\u636e\u4f9d\u8d56\u548c\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\u6570\u636e\u6216\u9762\u4e34\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\uff0c\u4e9f\u9700\u65b0\u8303\u5f0f\u3002", "method": "\u5229\u7528\u5fae\u5206\u65b9\u7a0b\u6570\u5b66\u7ed3\u6784\u751f\u6210\u7269\u7406\u89e3\u6790\u89e3\u4e0e\u5408\u6210\u6570\u636e\uff0c\u5b9e\u73b0\u65e0\u5b9e\u9a8c/\u6a21\u62df\u6570\u636e\u7684\u7b97\u5b50\u5b66\u4e60\u3002", "result": "\u57282D\u53c2\u6570\u5316\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86MAD\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6548/\u9ad8\u7cbe\u5ea6\u7279\u6027\u3002", "conclusion": "MAD\u6846\u67b6\u6709\u671b\u6210\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7269\u7406\u9a71\u52a8\u673a\u5668\u667a\u80fd\u7684\u901a\u7528\u8303\u5f0f\u3002"}}
{"id": "2507.06765", "pdf": "https://arxiv.org/pdf/2507.06765", "abs": "https://arxiv.org/abs/2507.06765", "authors": ["Enda D. V. Bigarella"], "title": "Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric", "categories": ["cs.LG"], "comment": null, "summary": "This document proposes a parametric activation function (ac.f.) aimed at\nimproving multidimensional nonlinear data regression. It is a established\nknowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.\nThis work shows that smoothness and gradient properties of the ac.f. further\nimpact the performance of large neural networks in terms of overfitting and\nsensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as\nELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and\nLeaky-RELU further impart discontinuity in the trained model. Improved\nperformance is demonstrated with a smooth \"Leaky Exponential Linear Unit\", with\nnon-zero gradient that can be trained. A novel diffusion-loss metric is also\nproposed to gauge the performance of the trained models in terms of\noverfitting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u6fc0\u6d3b\u51fd\u6570\uff08Leaky Exponential Linear Unit\uff09\u4ee5\u6539\u8fdb\u591a\u7ef4\u975e\u7ebf\u6027\u6570\u636e\u56de\u5f52\uff0c\u5e76\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u6269\u6563\u635f\u5931\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u5bf9\u5b66\u4e60\u975e\u7ebf\u6027\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5e73\u6ed1\u6027\u548c\u68af\u5ea6\u7279\u6027\u4f1a\u5f71\u54cd\u5927\u795e\u7ecf\u7f51\u7edc\u7684\u8fc7\u62df\u5408\u548c\u53c2\u6570\u654f\u611f\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e73\u6ed1\u4e14\u5177\u6709\u975e\u96f6\u68af\u5ea6\u7684Leaky Exponential Linear Unit\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u6269\u6563\u635f\u5931\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u6539\u8fdb\u7684\u6fc0\u6d3b\u51fd\u6570\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ELU\u3001SiLU\u3001RELU\u7b49\uff09\u3002", "conclusion": "\u5e73\u6ed1\u4e14\u975e\u96f6\u68af\u5ea6\u7684\u6fc0\u6d3b\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u6269\u6563\u635f\u5931\u6307\u6807\u4e3a\u8bc4\u4f30\u8fc7\u62df\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.06775", "pdf": "https://arxiv.org/pdf/2507.06775", "abs": "https://arxiv.org/abs/2507.06775", "authors": ["Mario Tuci", "Lennart Bastian", "Benjamin Dupuis", "Nassir Navab", "Tolga Birdal", "Umut \u015eim\u015fekli"], "title": "Mutual Information Free Topological Generalization Bounds via Stability", "categories": ["cs.LG", "math.AT", "stat.ML"], "comment": "25 pages, 5 figures", "summary": "Providing generalization guarantees for stochastic optimization algorithms is\na major challenge in modern learning theory. Recently, several studies\nhighlighted the impact of the geometry of training trajectories on the\ngeneralization error, both theoretically and empirically. Among these works, a\nseries of topological generalization bounds have been proposed, relating the\ngeneralization error to notions of topological complexity that stem from\ntopological data analysis (TDA). Despite their empirical success, these bounds\nrely on intricate information-theoretic (IT) terms that can be bounded in\nspecific cases but remain intractable for practical algorithms (such as ADAM),\npotentially reducing the relevance of the derived bounds. In this paper, we\nseek to formulate comprehensive and interpretable topological generalization\nbounds free of intractable mutual information terms. To this end, we introduce\na novel learning theoretic framework that departs from the existing strategies\nvia proof techniques rooted in algorithmic stability. By extending an existing\nnotion of \\textit{hypothesis set stability}, to \\textit{trajectory stability},\nwe prove that the generalization error of trajectory-stable algorithms can be\nupper bounded in terms of (i) TDA quantities describing the complexity of the\ntrajectory of the optimizer in the parameter space, and (ii) the trajectory\nstability parameter of the algorithm. Through a series of experimental\nevaluations, we demonstrate that the TDA terms in the bound are of great\nimportance, especially as the number of training samples grows. This ultimately\nforms an explanation of the empirical success of the topological generalization\nbounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u6cdb\u5316\u754c\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u4fe1\u606f\u8bba\u9879\uff0c\u901a\u8fc7\u8f68\u8ff9\u7a33\u5b9a\u6027\u6846\u67b6\u5c06\u6cdb\u5316\u8bef\u5dee\u4e0e\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u7b97\u6cd5\u7a33\u5b9a\u6027\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u73b0\u6709\u62d3\u6251\u6cdb\u5316\u754c\u4f9d\u8d56\u590d\u6742\u7684\u4fe1\u606f\u8bba\u9879\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u7b97\u6cd5\uff08\u5982ADAM\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u89c2\u4e14\u53ef\u89e3\u91ca\u7684\u6cdb\u5316\u754c\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u6269\u5c55\u5047\u8bbe\u96c6\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u7b97\u6cd5\u7a33\u5b9a\u6027\u8bc1\u660e\u6cdb\u5316\u8bef\u5dee\u4e0e\u62d3\u6251\u590d\u6742\u6027\u548c\u8f68\u8ff9\u7a33\u5b9a\u6027\u53c2\u6570\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u62d3\u6251\u6570\u636e\u9879\u5728\u6cdb\u5316\u754c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u8bad\u7ec3\u6837\u672c\u589e\u52a0\u65f6\uff0c\u89e3\u91ca\u4e86\u62d3\u6251\u6cdb\u5316\u754c\u7684\u5b9e\u8bc1\u6210\u529f\u3002", "conclusion": "\u65b0\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u62d3\u6251\u6cdb\u5316\u754c\uff0c\u4e3a\u7406\u89e3\u4f18\u5316\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.06780", "pdf": "https://arxiv.org/pdf/2507.06780", "abs": "https://arxiv.org/abs/2507.06780", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "This article introduces an imitation learning method for learning maximum\nentropy policies that comply with constraints demonstrated by expert\ntrajectories executing a task. The formulation of the method takes advantage of\nresults connecting performance to bounds for the KL-divergence between\ndemonstrated and learned policies, and its objective is rigorously justified\nthrough a connection to a probabilistic inference framework for reinforcement\nlearning, incorporating the reinforcement learning objective and the objective\nto abide by constraints in an entropy maximization setting. The proposed\nalgorithm optimizes the learning objective with dual gradient descent,\nsupporting effective and stable training. Experiments show that the proposed\nmethod can learn effective policy models for constraints-abiding behaviour, in\nsettings with multiple constraints of different types, accommodating different\nmodalities of demonstrated behaviour, and with abilities to generalize.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u7b26\u5408\u4e13\u5bb6\u8f68\u8ff9\u7ea6\u675f\u7684\u6700\u5927\u71b5\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u540c\u65f6\u6700\u5927\u5316\u71b5\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528KL\u6563\u5ea6\u8fb9\u754c\u8fde\u63a5\u6027\u80fd\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u548c\u7ea6\u675f\u76ee\u6807\uff0c\u901a\u8fc7\u53cc\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7b26\u5408\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u9002\u5e94\u591a\u79cd\u7ea6\u675f\u7c7b\u578b\u548c\u884c\u4e3a\u6a21\u6001\uff0c\u5e76\u5177\u5907\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u7ed3\u5408\u7ea6\u675f\u548c\u71b5\u6700\u5927\u5316\uff0c\u901a\u8fc7\u53cc\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u7ea6\u675f\u573a\u666f\u3002"}}
{"id": "2507.06802", "pdf": "https://arxiv.org/pdf/2507.06802", "abs": "https://arxiv.org/abs/2507.06802", "authors": ["Wonjin Jung", "Sungil Kang", "Dong-Yeon Cho"], "title": "Speech Tokenizer is Key to Consistent Representation", "categories": ["cs.LG"], "comment": null, "summary": "Speech tokenization is crucial in digital speech processing, converting\ncontinuous speech signals into discrete units for various computational tasks.\nThis paper introduces a novel speech tokenizer with broad applicability across\ndownstream tasks. While recent advances in residual vector quantization (RVQ)\nhave incorporated semantic elements, they often neglect critical acoustic\nfeatures. We propose an advanced approach that simultaneously encodes both\nlinguistic and acoustic information, preserving prosodic and emotional content.\nOur method significantly enhances speech representation fidelity across diverse\napplications. Empirical evaluations demonstrate its effectiveness in speech\ncoding, voice conversion, emotion recognition, and multimodal language\nmodeling, without requiring additional training. This versatility underscores\nits potential as a key tool for advancing AI-driven speech processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u540c\u65f6\u7f16\u7801\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8868\u793a\u7684\u771f\u5b9e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09\u7684\u65b9\u6cd5\u5e38\u5ffd\u7565\u5173\u952e\u58f0\u5b66\u7279\u5f81\uff0c\u672c\u6587\u65e8\u5728\u540c\u65f6\u4fdd\u7559\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u7f16\u7801\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u4fdd\u7559\u97f5\u5f8b\u548c\u60c5\u611f\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8bed\u97f3\u7f16\u7801\u3001\u8bed\u97f3\u8f6c\u6362\u3001\u60c5\u611f\u8bc6\u522b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u5efa\u6a21\u4e2d\u6709\u6548\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u662f\u63a8\u52a8AI\u8bed\u97f3\u5904\u7406\u7684\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2507.06813", "pdf": "https://arxiv.org/pdf/2507.06813", "abs": "https://arxiv.org/abs/2507.06813", "authors": ["Cosimo Fiorini", "Matteo Mosconi", "Pietro Buzzega", "Riccardo Salami", "Simone Calderara"], "title": "Intrinsic Training Signals for Federated Learning Aggregation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nwill be made publicly available upon acceptance.", "AI": {"tldr": "LIVAR\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u8bad\u7ec3\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u805a\u5408\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4fee\u6539\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\uff0cLIVAR\u65e8\u5728\u5229\u7528\u73b0\u6709\u8bad\u7ec3\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u805a\u5408\u3002", "method": "LIVAR\u901a\u8fc7\u65b9\u5dee\u52a0\u6743\u5206\u7c7b\u5668\u805a\u5408\u548c\u57fa\u4e8eSHAP\u5206\u6790\u7684LoRA\u5408\u5e76\u6280\u672f\u5b9e\u73b0\u3002", "result": "LIVAR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "LIVAR\u8bc1\u660e\u4ec5\u901a\u8fc7\u73b0\u6709\u8bad\u7ec3\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u805a\u5408\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.06819", "pdf": "https://arxiv.org/pdf/2507.06819", "abs": "https://arxiv.org/abs/2507.06819", "authors": ["Philipp Schlinge", "Steffen Meinert", "Martin Atzmueller"], "title": "Comprehensive Evaluation of Prototype Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto", "AI": {"tldr": "\u672c\u6587\u5bf9\u539f\u578b\u6a21\u578b\uff08\u5982ProtoPNet\u3001ProtoPool\u548cPIPNet\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u79cd\u6570\u636e\u96c6\u3002", "motivation": "\u539f\u578b\u6a21\u578b\u662f\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u6307\u6807\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "method": "\u5e94\u7528\u6807\u51c6\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684\u6307\u6807\uff0c\u5bf9\u539f\u578b\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3001\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u539f\u578b\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u5e93\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u4e3a\u539f\u578b\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86XAI\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.06821", "pdf": "https://arxiv.org/pdf/2507.06821", "abs": "https://arxiv.org/abs/2507.06821", "authors": ["Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Daoqiang Zhang", "Qi Zhu"], "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning", "categories": ["cs.LG", "cs.AI", "cs.MM"], "comment": null, "summary": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeLo\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u6316\u6398\u591a\u6a21\u6001\u60c5\u611f\u6570\u636e\u7684\u5f02\u8d28\u6027\u548c\u4e92\u8865\u4fe1\u606f\uff0c\u4ee5\u53ca\u6df7\u5408\u57fa\u672c\u60c5\u611f\u4e4b\u95f4\u7684\u6807\u7b7e\u76f8\u5173\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6316\u6398\u591a\u6a21\u6001\u5f02\u8d28\u6027\u548c\u5229\u7528\u57fa\u672c\u60c5\u611f\u95f4\u7684\u8bed\u4e49\u76f8\u5173\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u751f\u7406\u6570\u636e\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5f02\u8d28\u6027\u6316\u6398\u6a21\u5757\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6807\u7b7e\u5d4c\u5165\u548c\u76f8\u5173\u6027\u77e9\u9635\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u6807\u7b7e\u76f8\u5173\u6027\u9a71\u52a8\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u591a\u6a21\u6001\u8868\u793a\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "HeLo\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027\u6316\u6398\u548c\u6807\u7b7e\u76f8\u5173\u6027\u5229\u7528\u95ee\u9898\u3002"}}
{"id": "2507.06825", "pdf": "https://arxiv.org/pdf/2507.06825", "abs": "https://arxiv.org/abs/2507.06825", "authors": ["Matej Straka", "Martin Schmid"], "title": "Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a real-time strategy game environment built on Generals.io, a\ngame that hosts thousands of active players each week across multiple game\nformats. Our environment is fully compatible with Gymnasium and PettingZoo,\ncapable of running thousands of frames per second on commodity hardware. Our\nreference agent -- trained with supervised pre-training and self-play -- hits\nthe top 0.003\\% of the 1v1 human leaderboard after just 36 hours on a single\nH100 GPU. To accelerate learning, we incorporate potential-based reward shaping\nand memory features. Our contributions -- a modular RTS benchmark and a\ncompetitive, state-of-the-art baseline agent -- provide an accessible yet\nchallenging platform for advancing multi-agent reinforcement learning research.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eGenerals.io\u7684\u5b9e\u65f6\u7b56\u7565\u6e38\u620f\u73af\u5883\uff0c\u517c\u5bb9Gymnasium\u548cPettingZoo\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u8fd0\u884c\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9876\u7ea7\u6c34\u5e73\u7684\u53c2\u8003\u4ee3\u7406\u3002", "motivation": "\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u6613\u7528\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u81ea\u535a\u5f08\u8bad\u7ec3\u53c2\u8003\u4ee3\u7406\uff0c\u91c7\u7528\u57fa\u4e8e\u6f5c\u5728\u5956\u52b1\u5851\u9020\u548c\u8bb0\u5fc6\u7279\u5f81\u52a0\u901f\u5b66\u4e60\u3002", "result": "\u53c2\u8003\u4ee3\u7406\u572836\u5c0f\u65f6\u5185\u8fbe\u52301v1\u4eba\u7c7b\u6392\u884c\u699c\u524d0.003%\uff0c\u8fd0\u884c\u6027\u80fd\u9ad8\u8fbe\u6bcf\u79d2\u6570\u5343\u5e27\u3002", "conclusion": "\u8be5\u73af\u5883\u548c\u57fa\u7ebf\u4ee3\u7406\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ade\u4e89\u6027\u5f3a\u7684\u5de5\u5177\u3002"}}
{"id": "2507.06839", "pdf": "https://arxiv.org/pdf/2507.06839", "abs": "https://arxiv.org/abs/2507.06839", "authors": ["Jihao Andreas Lin"], "title": "Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning", "categories": ["cs.LG", "stat.ML"], "comment": "PhD Thesis, University of Cambridge", "summary": "Gaussian processes are a powerful framework for uncertainty-aware function\napproximation and sequential decision-making. Unfortunately, their classical\nformulation does not scale gracefully to large amounts of data and modern\nhardware for massively-parallel computation, prompting many researchers to\ndevelop techniques which improve their scalability. This dissertation focuses\non the powerful combination of iterative methods and pathwise conditioning to\ndevelop methodological contributions which facilitate the use of Gaussian\nprocesses in modern large-scale settings. By combining these two techniques\nsynergistically, expensive computations are expressed as solutions to systems\nof linear equations and obtained by leveraging iterative linear system solvers.\nThis drastically reduces memory requirements, facilitating application to\nsignificantly larger amounts of data, and introduces matrix multiplication as\nthe main computational operation, which is ideal for modern hardware.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u65b9\u6cd5\u548c\u8def\u5f84\u6761\u4ef6\u5316\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9ad8\u65af\u8fc7\u7a0b\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u5728\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51fd\u6570\u903c\u8fd1\u548c\u5e8f\u5217\u51b3\u7b56\u4e2d\u5177\u6709\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u7ecf\u5178\u5f62\u5f0f\u96be\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u6570\u636e\u548c\u73b0\u4ee3\u786c\u4ef6\u5e76\u884c\u8ba1\u7b97\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5c06\u8fed\u4ee3\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\u4e0e\u8def\u5f84\u6761\u4ef6\u5316\u76f8\u7ed3\u5408\uff0c\u5c06\u6602\u8d35\u8ba1\u7b97\u8f6c\u5316\u4e3a\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u6c42\u89e3\uff0c\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u5e76\u5229\u7528\u77e9\u9635\u4e58\u6cd5\u4f18\u5316\u8ba1\u7b97\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6570\u636e\uff0c\u5e76\u4f18\u5316\u4e86\u73b0\u4ee3\u786c\u4ef6\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408\u8fed\u4ee3\u65b9\u6cd5\u548c\u8def\u5f84\u6761\u4ef6\u5316\uff0c\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06853", "pdf": "https://arxiv.org/pdf/2507.06853", "abs": "https://arxiv.org/abs/2507.06853", "authors": ["Liang Wang", "Yu Rong", "Tingyang Xu", "Zhenyi Zhong", "Zhiyuan Liu", "Pengju Wang", "Deli Zhao", "Qiang Liu", "Shu Wu", "Liang Wang"], "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph", "q-bio.MN"], "comment": null, "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.", "AI": {"tldr": "DiffSpectra\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u63a8\u65ad2D\u548c3D\u5206\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u662f\u5316\u5b66\u4e2d\u7684\u57fa\u7840\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u89e3\u91ca\u4e14\u7f3a\u4e4f\u6269\u5c55\u6027\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6709\u9650\u5e93\u3002DiffSpectra\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiffSpectra\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548cSE(3)-\u7b49\u53d8\u67b6\u6784\u7684Diffusion Molecule Transformer\uff0c\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u5149\u8c31\u7f16\u7801\u5668SpecFormer\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffSpectra\u5728\u7ed3\u6784\u89e3\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0cTop-1\u51c6\u786e\u7387\u4e3a16.01%\uff0cTop-20\u51c6\u786e\u7387\u4e3a96.86%\uff0c3D\u51e0\u4f55\u5efa\u6a21\u548c\u591a\u6a21\u6001\u6761\u4ef6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "DiffSpectra\u9996\u6b21\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u5149\u8c31\u63a8\u7406\u548c2D/3D\u751f\u6210\u5efa\u6a21\uff0c\u4e3a\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06859", "pdf": "https://arxiv.org/pdf/2507.06859", "abs": "https://arxiv.org/abs/2507.06859", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Episodic Contextual Bandits with Knapsacks under Conversion Models", "categories": ["cs.LG"], "comment": null, "summary": "We study an online setting, where a decision maker (DM) interacts with\ncontextual bandit-with-knapsack (BwK) instances in repeated episodes. These\nepisodes start with different resource amounts, and the contexts' probability\ndistributions are non-stationary in an episode. All episodes share the same\nlatent conversion model, which governs the random outcome contingent upon a\nrequest's context and an allocation decision. Our model captures applications\nsuch as dynamic pricing on perishable resources with episodic replenishment,\nand first price auctions in repeated episodes with different starting budgets.\nWe design an online algorithm that achieves a regret sub-linear in $T$, the\nnumber of episodes, assuming access to a \\emph{confidence bound oracle} that\nachieves an $o(T)$-regret. Such an oracle is readily available from existing\ncontextual bandit literature. We overcome the technical challenge with\narbitrarily many possible contexts, which leads to a reinforcement learning\nproblem with an unbounded state space. Our framework provides improved regret\nbounds in certain settings when the DM is provided with unlabeled feature data,\nwhich is novel to the contextual BwK literature.", "AI": {"tldr": "\u7814\u7a76\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5728\u975e\u5e73\u7a33\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u7684\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u5b9a\u4ef7\u548c\u62cd\u5356\u7b49\u5e94\u7528\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u975e\u5e73\u7a33\u6027\u548c\u591a\u4e0a\u4e0b\u6587\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u7b97\u6cd5\uff0c\u5229\u7528\u7f6e\u4fe1\u8fb9\u754c\u9884\u8a00\u673a\u5904\u7406\u975e\u5e73\u7a33\u4e0a\u4e0b\u6587\u548c\u65e0\u9650\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u7b97\u6cd5\u5728T\u6b21\u8fed\u4ee3\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6539\u8fdb\u7684\u9057\u61be\u8fb9\u754c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0a\u4e0b\u6587BwK\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u672a\u6807\u8bb0\u7279\u5f81\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.06888", "pdf": "https://arxiv.org/pdf/2507.06888", "abs": "https://arxiv.org/abs/2507.06888", "authors": ["Wei Chen", "Wanyang Gu", "Linjun Peng", "Ruichu Cai", "Zhifeng Hao", "Kun Zhang"], "title": "Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants", "categories": ["cs.LG"], "comment": null, "summary": "Federated causal discovery aims to uncover the causal relationships between\nentities while protecting data privacy, which has significant importance and\nnumerous applications in real-world scenarios. Existing federated causal\nstructure learning methods primarily focus on horizontal federated settings.\nHowever, in practical situations, different clients may not necessarily contain\ndata on the same variables. In a single client, the incomplete set of variables\ncan easily lead to spurious causal relationships, thereby affecting the\ninformation transmitted to other clients. To address this issue, we\ncomprehensively consider causal structure learning methods under both\nhorizontal and vertical federated settings. We provide the identification\ntheories and methods for learning causal structure in the horizontal and\nvertical federal setting via higher-order cumulants. Specifically, we first\naggregate higher-order cumulant information from all participating clients to\nconstruct global cumulant estimates. These global estimates are then used for\nrecursive source identification, ultimately yielding a global causal strength\nmatrix. Our approach not only enables the reconstruction of causal graphs but\nalso facilitates the estimation of causal strength coefficients. Our algorithm\ndemonstrates superior performance in experiments conducted on both synthetic\ndata and real-world data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6c34\u5e73\u4e0e\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\uff0c\u5229\u7528\u9ad8\u9636\u7d2f\u79ef\u91cf\u6784\u5efa\u5168\u5c40\u56e0\u679c\u56fe\uff0c\u89e3\u51b3\u4e86\u53d8\u91cf\u4e0d\u5b8c\u6574\u5bfc\u81f4\u7684\u865a\u5047\u56e0\u679c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6c34\u5e73\u8054\u90a6\u8bbe\u7f6e\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u5ba2\u6237\u7aef\u53d8\u91cf\u53ef\u80fd\u4e0d\u540c\uff0c\u5bfc\u81f4\u865a\u5047\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u805a\u5408\u5ba2\u6237\u7aef\u7684\u9ad8\u9636\u7d2f\u79ef\u91cf\u4fe1\u606f\u6784\u5efa\u5168\u5c40\u4f30\u8ba1\uff0c\u9012\u5f52\u8bc6\u522b\u56e0\u679c\u6e90\uff0c\u751f\u6210\u5168\u5c40\u56e0\u679c\u5f3a\u5ea6\u77e9\u9635\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u91cd\u5efa\u56e0\u679c\u56fe\u5e76\u4f30\u8ba1\u56e0\u679c\u5f3a\u5ea6\u7cfb\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6c34\u5e73\u548c\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\uff0c\u89e3\u51b3\u4e86\u53d8\u91cf\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.06892", "pdf": "https://arxiv.org/pdf/2507.06892", "abs": "https://arxiv.org/abs/2507.06892", "authors": ["Jing Liang", "Hongyao Tang", "Yi Ma", "Jinyi Liu", "Yan Zheng", "Shuyue Hu", "Lei Bai", "Jianye Hao"], "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preliminary version. Project page:\n  https://anitaleungxx.github.io/ReMix", "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReMix\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7b56\u7565\u8fd1\u7aef\u7b56\u7565\u68af\u5ea6\u3001KL\u51f8\u7b56\u7565\u7ea6\u675f\u548c\u7b56\u7565\u91cd\u751f\uff0c\u4f7f\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\u80fd\u591f\u5229\u7528\u975e\u7b56\u7565\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff08\u5982PPO\u548cGRPO\uff09\u4e3b\u8981\u4f9d\u8d56\u7b56\u7565\u5185\u6570\u636e\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u65f6\u95f4\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u6269\u5c55\u3002", "method": "ReMix\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u6df7\u5408\u7b56\u7565\u8fd1\u7aef\u7b56\u7565\u68af\u5ea6\uff08\u63d0\u9ad8\u66f4\u65b0\u4e0e\u6570\u636e\u6bd4\uff09\u3001KL\u51f8\u7b56\u7565\u7ea6\u675f\uff08\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\uff09\u3001\u7b56\u7565\u91cd\u751f\uff08\u5b9e\u73b0\u4ece\u9ad8\u6548\u65e9\u671f\u5b66\u4e60\u5230\u7a33\u5b9a\u6e10\u8fdb\u6539\u8fdb\u7684\u65e0\u7f1d\u8fc7\u6e21\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReMix\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e30\u81f3450\u500d\uff0c\u540c\u65f6\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "ReMix\u4e3a\u5f3a\u5316\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u975e\u7b56\u7565\u6570\u636e\u4f7f\u7528\u4e2d\u7684\u4e00\u4e9b\u6709\u8da3\u73b0\u8c61\u3002"}}
{"id": "2507.06901", "pdf": "https://arxiv.org/pdf/2507.06901", "abs": "https://arxiv.org/abs/2507.06901", "authors": ["Abolfazl Zarghani", "Sadegh Abedi"], "title": "Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams", "categories": ["cs.LG"], "comment": null, "summary": "Multi-dimensional data streams, prevalent in applications like IoT, financial\nmarkets, and real-time analytics, pose significant challenges due to their high\nvelocity, unbounded nature, and complex inter-dimensional dependencies. Sliding\nwindow techniques are critical for processing such streams, but fixed-size\nwindows struggle to adapt to dynamic changes like concept drift or bursty\npatterns. This paper proposes a novel reinforcement learning (RL)-based\napproach to dynamically optimize sliding window sizes for multi-dimensional\ndata streams. By formulating window size selection as an RL problem, we enable\nan agent to learn an adaptive policy based on stream characteristics, such as\nvariance, correlations, and temporal trends. Our method, RL-Window, leverages a\nDueling Deep Q-Network (DQN) with prioritized experience replay to handle\nnon-stationarity and high-dimensionality. Evaluations on benchmark datasets\n(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms\nstate-of-the-art methods like ADWIN and CNN-Adaptive in classification\naccuracy, drift robustness, and computational efficiency. Additional\nqualitative analyses, extended metrics (e.g., energy efficiency, latency), and\na comprehensive dataset characterization further highlight its adaptability and\nstability, making it suitable for real-time applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316\u65b9\u6cd5\uff08RL-Window\uff09\uff0c\u7528\u4e8e\u5904\u7406\u591a\u7ef4\u6570\u636e\u6d41\uff0c\u9002\u5e94\u52a8\u6001\u53d8\u5316\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u7ef4\u6570\u636e\u6d41\uff08\u5982IoT\u3001\u91d1\u878d\u5e02\u573a\uff09\u7684\u9ad8\u901f\u5ea6\u3001\u65e0\u754c\u6027\u548c\u590d\u6742\u4f9d\u8d56\u6027\u5bf9\u5904\u7406\u6280\u672f\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f20\u7edf\u56fa\u5b9a\u7a97\u53e3\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002", "method": "\u5c06\u7a97\u53e3\u5927\u5c0f\u9009\u62e9\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528Dueling DQN\u548c\u4f18\u5148\u7ea7\u7ecf\u9a8c\u56de\u653e\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cRL-Window\u5728\u5206\u7c7b\u7cbe\u5ea6\u3001\u6f02\u79fb\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982ADWIN\u3001CNN-Adaptive\uff09\u3002", "conclusion": "RL-Window\u5177\u6709\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u6307\u6807\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2507.06952", "pdf": "https://arxiv.org/pdf/2507.06952", "abs": "https://arxiv.org/abs/2507.06952", "authors": ["Keyon Vafa", "Peter G. Chang", "Ashesh Rambachan", "Sendhil Mullainathan"], "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models", "categories": ["cs.LG", "cs.AI"], "comment": "To appear in ICML 2025", "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u6df1\u5c42\u7ed3\u6784\u7684\u6280\u672f\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u672a\u80fd\u53d1\u5c55\u51fa\u4e0e\u5e95\u5c42\u4e16\u754c\u6a21\u578b\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u5dee\u3002", "motivation": "\u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u901a\u8fc7\u5e8f\u5217\u9884\u6d4b\u63ed\u793a\u66f4\u6df1\u5c42\u6b21\u7684\u9886\u57df\u7406\u89e3\uff0c\u7c7b\u4f3c\u4e8e\u5f00\u666e\u52d2\u9884\u6d4b\u884c\u661f\u8fd0\u52a8\u540e\u725b\u987f\u529b\u5b66\u7684\u53d1\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5f52\u7eb3\u504f\u5dee\u63a2\u9488\u201d\u7684\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u5e76\u6d4b\u8bd5\u6a21\u578b\u9002\u5e94\u6027\u6765\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u8bad\u7ec3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u672a\u80fd\u53d1\u5c55\u51fa\u4e0e\u5e95\u5c42\u4e16\u754c\u6a21\u578b\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u8f68\u9053\u8f68\u8ff9\u4efb\u52a1\u4e2d\u672a\u80fd\u5e94\u7528\u725b\u987f\u529b\u5b66\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4ec5\u53d1\u5c55\u51fa\u4efb\u52a1\u7279\u5b9a\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.06967", "pdf": "https://arxiv.org/pdf/2507.06967", "abs": "https://arxiv.org/abs/2507.06967", "authors": ["Sebastien Andre-Sloan", "Anirbit Mukherjee", "Matthew Colbrook"], "title": "Noisy PDE Training Requires Bigger PINNs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are increasingly used to approximate\nsolutions of partial differential equations (PDEs), especially in high\ndimensions. In real-world applications, data samples are noisy, so it is\nimportant to know when a predictor can still achieve low empirical risk.\nHowever, little is known about the conditions under which a PINN can do so\neffectively. We prove a lower bound on the size of neural networks required for\nthe supervised PINN empirical risk to fall below the variance of noisy\nsupervision labels. Specifically, if a predictor achieves an empirical risk\n$O(\\eta)$ below $\\sigma^2$ (variance of supervision data), then necessarily\n$d_N\\log d_N\\gtrsim N_s \\eta^2$, where $N_s$ is the number of samples and $d_N$\nis the number of trainable parameters of the PINN. A similar constraint applies\nto the fully unsupervised PINN setting when boundary labels are sampled\nnoisily. Consequently, increasing the number of noisy supervision labels alone\ndoes not provide a ``free lunch'' in reducing empirical risk. We also show\nempirically that PINNs can indeed achieve empirical risks below $\\sigma^2$\nunder such conditions. As a case study, we investigate PINNs applied to the\nHamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for\nquantitatively understanding the parameter requirements for training PINNs in\nthe presence of noise.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u566a\u58f0\u6570\u636e\u4e0b\uff0c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u903c\u8fd1\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7684\u80fd\u529b\uff0c\u5e76\u8bc1\u660e\u4e86\u7f51\u7edc\u89c4\u6a21\u7684\u4e0b\u754c\u6761\u4ef6\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u6837\u672c\u901a\u5e38\u542b\u6709\u566a\u58f0\uff0c\u4f46PINNs\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u6709\u6548\u964d\u4f4e\u7ecf\u9a8c\u98ce\u9669\u7684\u6761\u4ef6\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u76d1\u7763\u548c\u65e0\u76d1\u7763PINN\u8bbe\u7f6e\u4e0b\uff0c\u7f51\u7edc\u89c4\u6a21\u4e0e\u6837\u672c\u6570\u91cf\u53ca\u566a\u58f0\u65b9\u5dee\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u589e\u52a0\u566a\u58f0\u6807\u7b7e\u6570\u91cf\u5e76\u4e0d\u80fd\u65e0\u6761\u4ef6\u964d\u4f4e\u7ecf\u9a8c\u98ce\u9669\uff0c\u4e14PINNs\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u4f4e\u4e8e\u566a\u58f0\u65b9\u5dee\u7684\u7ecf\u9a8c\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9a\u91cf\u7406\u89e3\u566a\u58f0\u73af\u5883\u4e0b\u8bad\u7ec3PINNs\u7684\u53c2\u6570\u9700\u6c42\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.06969", "pdf": "https://arxiv.org/pdf/2507.06969", "abs": "https://arxiv.org/abs/2507.06969", "authors": ["Bogdan Kulynych", "Juan Felipe Gomez", "Georgios Kaissis", "Jamie Hayes", "Borja Balle", "Flavio du Pin Calmon", "Jean Louis Raisaro"], "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CY", "stat.ML"], "comment": null, "summary": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.06979", "pdf": "https://arxiv.org/pdf/2507.06979", "abs": "https://arxiv.org/abs/2507.06979", "authors": ["Panagiotis Koromilas", "Efthymios Georgiou", "Giorgos Bouritsas", "Theodoros Giannakopoulos", "Mihalis A. Nicolaou", "Yannis Panagakis"], "title": "A Principled Framework for Multi-View Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08MV-InfoNCE\u548cMV-DHEL\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u56db\u4e2a\u5173\u952e\u9650\u5236\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u591a\u89c6\u56fe\u6570\u636e\u65f6\u5b58\u5728\u4f18\u5316\u51b2\u7a81\u3001\u89c6\u56fe\u4ea4\u4e92\u5efa\u6a21\u4e0d\u8db3\u3001\u5bf9\u9f50-\u5747\u5300\u6027\u8026\u5408\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u89c6\u56fe\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86MV-InfoNCE\u548cMV-DHEL\u4e24\u79cd\u635f\u5931\u51fd\u6570\uff0c\u5206\u522b\u901a\u8fc7\u540c\u65f6\u5efa\u6a21\u6240\u6709\u89c6\u56fe\u4ea4\u4e92\u548c\u89e3\u8026\u5bf9\u9f50\u4e0e\u5747\u5300\u6027\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002", "result": "\u5728ImageNet1K\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u591a\u89c6\u56fe\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u5229\u7528\u591a\u89c6\u56fe\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e94\u89c6\u56fe\u4ee5\u4e0a\u65f6\u663e\u8457\u7f13\u89e3\u7ef4\u5ea6\u584c\u7f29\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3a\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u6269\u5c55\u4e86\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.06996", "pdf": "https://arxiv.org/pdf/2507.06996", "abs": "https://arxiv.org/abs/2507.06996", "authors": ["Eunbyeol Cho", "Jiyoun Kim", "Minjae Lee", "Sungjin Park", "Edward Choi"], "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.", "AI": {"tldr": "RawMed\u662f\u4e00\u4e2a\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u8868\u793a\u548c\u538b\u7f29\u6280\u672f\uff0c\u65e0\u9700\u590d\u6742\u9884\u5904\u7406\u5373\u53ef\u6355\u6349\u590d\u6742\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u76d1\u7ba1\u9650\u5236\uff0c\u771f\u5b9eEHR\u6570\u636e\u96be\u4ee5\u5171\u4eab\u548c\u4f7f\u7528\uff0c\u9700\u8981\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u751f\u6210\u4e13\u5bb6\u9009\u62e9\u7684\u7279\u5f81\uff0c\u65e0\u6cd5\u6a21\u62df\u539f\u59cbEHR\u7684\u590d\u6742\u6027\u3002", "method": "RawMed\u91c7\u7528\u6587\u672c\u8868\u793a\u548c\u538b\u7f29\u6280\u672f\uff0c\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217EHR\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6\u5206\u5e03\u76f8\u4f3c\u6027\u3001\u8868\u95f4\u5173\u7cfb\u3001\u65f6\u95f4\u52a8\u6001\u548c\u9690\u79c1\u3002", "result": "\u5728\u5f00\u6e90EHR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRawMed\u5728\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RawMed\u662f\u9996\u4e2a\u80fd\u751f\u6210\u63a5\u8fd1\u539f\u59cbEHR\u7684\u5408\u6210\u6570\u636e\u6846\u67b6\uff0c\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.07008", "pdf": "https://arxiv.org/pdf/2507.07008", "abs": "https://arxiv.org/abs/2507.07008", "authors": ["Emile Pierret", "Bruno Galerne"], "title": "Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions", "categories": ["cs.LG"], "comment": null, "summary": "Used as priors for Bayesian inverse problems, diffusion models have recently\nattracted considerable attention in the literature. Their flexibility and high\nvariance enable them to generate multiple solutions for a given task, such as\ninpainting, super-resolution, and deblurring. However, several unresolved\nquestions remain about how well they perform. In this article, we investigate\nthe accuracy of these models when applied to a Gaussian data distribution for\ndeblurring. Within this constrained context, we are able to precisely analyze\nthe discrepancy between the theoretical resolution of inverse problems and\ntheir resolution obtained using diffusion models by computing the exact\nWasserstein distance between the distribution of the diffusion model sampler\nand the ideal distribution of solutions to the inverse problem. Our findings\nallow for the comparison of different algorithms from the literature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u9ad8\u65af\u6570\u636e\u5206\u5e03\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u8ba1\u7b97Wasserstein\u8ddd\u79bb\u6bd4\u8f83\u7406\u8bba\u89e3\u4e0e\u6269\u6563\u6a21\u578b\u89e3\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u5148\u9a8c\uff0c\u56e0\u5176\u7075\u6d3b\u6027\u548c\u9ad8\u65b9\u5dee\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u6027\u80fd\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u5728\u7ea6\u675f\u7684\u9ad8\u65af\u6570\u636e\u5206\u5e03\u4e0b\uff0c\u8ba1\u7b97\u6269\u6563\u6a21\u578b\u91c7\u6837\u5668\u5206\u5e03\u4e0e\u7406\u60f3\u9006\u95ee\u9898\u89e3\u5206\u5e03\u4e4b\u95f4\u7684\u7cbe\u786eWasserstein\u8ddd\u79bb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53ef\u4ee5\u6bd4\u8f83\u6587\u732e\u4e2d\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u53ef\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e3a\u7b97\u6cd5\u6bd4\u8f83\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2507.07016", "pdf": "https://arxiv.org/pdf/2507.07016", "abs": "https://arxiv.org/abs/2507.07016", "authors": ["Jian Huang", "Yongli Zhu", "Linna Xu", "Zhe Zheng", "Wenpeng Cui", "Mingyang Sun"], "title": "On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence", "categories": ["cs.LG", "eess.SP"], "comment": "This paper is currently under reviewing by an IEEE publication; it\n  may be subjected to minor changes due to review comments later", "summary": "In this paper, an edge-side model training study is conducted on a\nresource-limited smart meter. The motivation of grid-edge intelligence and the\nconcept of on-device training are introduced. Then, the technical preparation\nsteps for on-device training are described. A case study on the task of\nphotovoltaic power forecasting is presented, where two representative machine\nlearning models are investigated: a gradient boosting tree model and a\nrecurrent neural network model. To adapt to the resource-limited situation in\nthe smart meter, \"mixed\"- and \"reduced\"-precision training schemes are also\ndevised. Experiment results demonstrate the feasibility of economically\nachieving grid-edge intelligence via the existing advanced metering\ninfrastructures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8d44\u6e90\u6709\u9650\u7684\u667a\u80fd\u7535\u8868\u4e0a\u8fdb\u884c\u8fb9\u7f18\u4fa7\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u548c\u964d\u4f4e\u7cbe\u5ea6\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5149\u4f0f\u529f\u7387\u9884\u6d4b\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5176\u7ecf\u6d4e\u6027\u3002", "motivation": "\u63a8\u52a8\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u5316\u548c\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u6982\u5ff5\uff0c\u4ee5\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u6280\u672f\u51c6\u5907\u6b65\u9aa4\uff0c\u5e76\u5728\u5149\u4f0f\u529f\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u4e86\u68af\u5ea6\u63d0\u5347\u6811\u6a21\u578b\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u6df7\u5408\u548c\u964d\u4f4e\u7cbe\u5ea6\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u73b0\u6709\u5148\u8fdb\u8ba1\u91cf\u57fa\u7840\u8bbe\u65bd\u7ecf\u6d4e\u5730\u5b9e\u73b0\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8d44\u6e90\u53d7\u9650\u7684\u667a\u80fd\u7535\u8868\u4e0a\u53ef\u4ee5\u5b9e\u73b0\u8fb9\u7f18\u4fa7\u6a21\u578b\u8bad\u7ec3\uff0c\u4e3a\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u5316\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07032", "pdf": "https://arxiv.org/pdf/2507.07032", "abs": "https://arxiv.org/abs/2507.07032", "authors": ["Hanqun Cao", "Xinyi Zhou", "Zijun Gao", "Chenyu Wang", "Xin Gao", "Zhi Zhang", "Chunbin Gu", "Ge Liu", "Pheng-Ann Heng"], "title": "PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein structure prediction is essential for drug discovery and\nunderstanding biological functions. While recent advancements like AlphaFold\nhave achieved remarkable accuracy, most folding models rely heavily on multiple\nsequence alignments (MSAs) to boost prediction performance. This dependency\nlimits their effectiveness on low-homology proteins and orphan proteins, where\nMSA information is sparse or unavailable. To address this limitation, we\npropose PLAME, a novel MSA design model that leverages evolutionary embeddings\nfrom pretrained protein language models. Unlike existing methods, PLAME\nintroduces pretrained representations to enhance evolutionary information and\nemploys a conservation-diversity loss to enhance generation quality.\nAdditionally, we propose a novel MSA selection method to effectively screen\nhigh-quality MSAs and improve folding performance. We also propose a sequence\nquality assessment metric that provides an orthogonal perspective to evaluate\nMSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,\nPLAME achieves state-of-the-art performance in folding enhancement and sequence\nquality assessment, with consistent improvements demonstrated on AlphaFold3.\nAblation studies validate the effectiveness of the MSA selection method, while\nextensive case studies on various protein types provide insights into the\nrelationship between AlphaFold's prediction quality and MSA characteristics.\nFurthermore, we demonstrate that PLAME can serve as an adapter achieving\nAlphaFold2-level accuracy with the ESMFold's inference speed.", "AI": {"tldr": "PLAME\u662f\u4e00\u79cd\u65b0\u578b\u7684MSA\u8bbe\u8ba1\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u5d4c\u5165\uff0c\u63d0\u5347\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u7684\u7ed3\u6784\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6298\u53e0\u6a21\u578b\u5bf9\u591a\u5e8f\u5217\u6bd4\u5bf9\uff08MSA\uff09\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u4e2dMSA\u4fe1\u606f\u7a00\u7f3a\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faPLAME\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8868\u793a\u548c\u5b88\u6052-\u591a\u6837\u6027\u635f\u5931\u51fd\u6570\uff0c\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u7684MSA\uff0c\u5e76\u5f15\u5165\u65b0\u7684MSA\u7b5b\u9009\u65b9\u6cd5\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728AlphaFold2\u548cAlphaFold3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPLAME\u5728\u6298\u53e0\u589e\u5f3a\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "PLAME\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u9002\u914d\u5668\u5b9e\u73b0AlphaFold2\u7ea7\u7cbe\u5ea6\u4e0eESMFold\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2507.07033", "pdf": "https://arxiv.org/pdf/2507.07033", "abs": "https://arxiv.org/abs/2507.07033", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Asal Rangrazi", "Marco Miozzo", "Charalampos Kalalas", "Paolo Dini"], "title": "Self-Supervised Learning at the Edge: The Cost of Labeling", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted for publication in IEEE MLSP 2025", "summary": "Contrastive learning (CL) has recently emerged as an alternative to\ntraditional supervised machine learning solutions by enabling rich\nrepresentations from unstructured and unlabeled data. However, CL and, more\nbroadly, self-supervised learning (SSL) methods often demand a large amount of\ndata and computational resources, posing challenges for deployment on\nresource-constrained edge devices. In this work, we explore the feasibility and\nefficiency of SSL techniques for edge-based learning, focusing on trade-offs\nbetween model performance and energy efficiency. In particular, we analyze how\ndifferent SSL techniques adapt to limited computational, data, and energy\nbudgets, evaluating their effectiveness in learning robust representations\nunder resource-constrained settings. Moreover, we also consider the energy\ncosts involved in labeling data and assess how semi-supervised learning may\nassist in reducing the overall energy consumed to train CL models. Through\nextensive experiments, we demonstrate that tailored SSL strategies can achieve\ncompetitive performance while reducing resource consumption by up to 4X,\nunderscoring their potential for energy-efficient learning at the edge.", "AI": {"tldr": "\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u901a\u8fc7\u5b9a\u5236\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7b56\u7565\uff0c\u6027\u80fd\u63a5\u8fd1\u76d1\u7763\u5b66\u4e60\uff0c\u540c\u65f6\u8d44\u6e90\u6d88\u8017\u51cf\u5c114\u500d\u3002", "motivation": "\u63a2\u7d22\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u9ad8\u9700\u6c42\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e0d\u540cSSL\u6280\u672f\u5728\u6709\u9650\u8ba1\u7b97\u3001\u6570\u636e\u548c\u80fd\u6e90\u9884\u7b97\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u8bc4\u4f30\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b66\u4e60\u9c81\u68d2\u8868\u793a\u7684\u6548\u679c\uff0c\u5e76\u8003\u8651\u534a\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u6807\u6ce8\u80fd\u8017\u3002", "result": "\u5b9a\u5236SSL\u7b56\u7565\u5728\u6027\u80fd\u63a5\u8fd1\u76d1\u7763\u5b66\u4e60\u7684\u540c\u65f6\uff0c\u8d44\u6e90\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe4\u500d\u3002", "conclusion": "SSL\u6280\u672f\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\uff0c\u80fd\u5b9e\u73b0\u9ad8\u6548\u8282\u80fd\u7684\u5b66\u4e60\u3002"}}
{"id": "2507.07061", "pdf": "https://arxiv.org/pdf/2507.07061", "abs": "https://arxiv.org/abs/2507.07061", "authors": ["Shervin Ghaffari", "Zohre Bahranifard", "Mohammad Akbari"], "title": "An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems", "categories": ["cs.LG", "68T50", "I.2.7; H.3.3; I.5.1"], "comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science", "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u548c\u8bad\u7ec3\u5143\u7f16\u7801\u5668\uff0c\u63d0\u5347LLM\u7f13\u5b58\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49\u7f13\u5b58\u6846\u67b6\u4f9d\u8d56\u5355\u4e00\u5d4c\u5165\u6a21\u578b\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u67e5\u8be2\u5206\u5e03\u4e2d\u7684\u591a\u6837\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u7f13\u5b58\u6548\u7387\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u5e76\u901a\u8fc7\u8bad\u7ec3\u5143\u7f16\u7801\u5668\u4f18\u5316\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u3002", "result": "\u5728QQP\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8692%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u548c85%\u7684\u975e\u7b49\u4ef7\u67e5\u8be2\u62d2\u7edd\u51c6\u786e\u7387\u3002", "conclusion": "\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\uff0c\u80fd\u66f4\u6709\u6548\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u548c\u76f8\u5f02\u67e5\u8be2\uff0c\u63d0\u5347LLM\u7cfb\u7edf\u7684\u7f13\u5b58\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.07100", "pdf": "https://arxiv.org/pdf/2507.07100", "abs": "https://arxiv.org/abs/2507.07100", "authors": ["Lan Li", "Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan"], "title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Domain-Incremental Learning (DIL) focuses on continual learning in\nnon-stationary environments, requiring models to adjust to evolving domains\nwhile preserving historical knowledge. DIL faces two critical challenges in the\ncontext of imbalanced data: intra-domain class imbalance and cross-domain class\ndistribution shifts. These challenges significantly hinder model performance,\nas intra-domain imbalance leads to underfitting of few-shot classes, while\ncross-domain shifts require maintaining well-learned many-shot classes and\ntransferring knowledge to improve few-shot class performance in old domains. To\novercome these challenges, we introduce the Dual-Balance Collaborative Experts\n(DCE) framework. DCE employs a frequency-aware expert group, where each expert\nis guided by specialized loss functions to learn features for specific\nfrequency groups, effectively addressing intra-domain class imbalance.\nSubsequently, a dynamic expert selector is learned by synthesizing\npseudo-features through balanced Gaussian sampling from historical class\nstatistics. This mechanism navigates the trade-off between preserving many-shot\nknowledge of previous domains and leveraging new data to improve few-shot class\nperformance in earlier tasks. Extensive experimental results on four benchmark\ndatasets demonstrate DCE's state-of-the-art performance.", "AI": {"tldr": "DCE\u6846\u67b6\u901a\u8fc7\u9891\u7387\u611f\u77e5\u4e13\u5bb6\u7ec4\u548c\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u5668\u89e3\u51b3DIL\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "DIL\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9762\u4e34\u7c7b\u5185\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faDCE\u6846\u67b6\uff0c\u5305\u62ec\u9891\u7387\u611f\u77e5\u4e13\u5bb6\u7ec4\u548c\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u5668\uff0c\u5206\u522b\u5904\u7406\u7c7b\u5185\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDCE\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DCE\u6709\u6548\u89e3\u51b3\u4e86DIL\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2507.07101", "pdf": "https://arxiv.org/pdf/2507.07101", "abs": "https://arxiv.org/abs/2507.07101", "authors": ["Martin Marek", "Sanae Lotfi", "Aditya Somasundaram", "Andrew Gordon Wilson", "Micah Goldblum"], "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful", "categories": ["cs.LG"], "comment": "Code available at: https://github.com/martin-marek/batch-size", "summary": "Conventional wisdom dictates that small batch sizes make language model\npretraining and fine-tuning unstable, motivating gradient accumulation, which\ntrades off the number of optimizer steps for a proportional increase in batch\nsize. While it is common to decrease the learning rate for smaller batch sizes,\nother hyperparameters are often held fixed. In this work, we revisit small\nbatch sizes all the way down to batch size one, and we propose a rule for\nscaling Adam hyperparameters to small batch sizes. We find that small batch\nsizes (1) train stably, (2) are consistently more robust to hyperparameter\nchoices, (3) achieve equal or better per-FLOP performance than larger batch\nsizes, and (4) notably enable stable language model training with vanilla SGD,\neven without momentum, despite storing no optimizer state. Building on these\nresults, we provide practical recommendations for selecting a batch size and\nsetting optimizer hyperparameters. We further recommend against gradient\naccumulation unless training on multiple devices with multiple model replicas,\nbottlenecked by inter-device bandwidth.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c0f\u6279\u91cf\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u66f4\u7a33\u5b9a\u4e14\u6027\u80fd\u66f4\u4f18\uff0c\u5efa\u8bae\u8c03\u6574Adam\u8d85\u53c2\u6570\u5e76\u907f\u514d\u68af\u5ea6\u7d2f\u79ef\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u5c0f\u6279\u91cf\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u4f46\u672c\u7814\u7a76\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22\u5c0f\u6279\u91cf\uff08\u751a\u81f3\u6279\u91cf\u5927\u5c0f\u4e3a1\uff09\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u8c03\u6574Adam\u8d85\u53c2\u6570\u7684\u89c4\u5219\uff0c\u5e76\u9a8c\u8bc1\u5c0f\u6279\u91cf\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u3002", "result": "\u5c0f\u6279\u91cf\u8bad\u7ec3\u66f4\u7a33\u5b9a\u3001\u5bf9\u8d85\u53c2\u6570\u66f4\u9c81\u68d2\uff0c\u6027\u80fd\u4f18\u4e8e\u5927\u6279\u91cf\uff0c\u4e14\u652f\u6301SGD\u7a33\u5b9a\u8bad\u7ec3\u3002", "conclusion": "\u5efa\u8bae\u9009\u62e9\u5c0f\u6279\u91cf\u5e76\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u907f\u514d\u68af\u5ea6\u7d2f\u79ef\uff0c\u9664\u975e\u5728\u591a\u8bbe\u5907\u8bad\u7ec3\u4e2d\u5e26\u5bbd\u53d7\u9650\u3002"}}
{"id": "2507.07102", "pdf": "https://arxiv.org/pdf/2507.07102", "abs": "https://arxiv.org/abs/2507.07102", "authors": ["Arnas Uselis", "Andrea Dittadi", "Seong Joon Oh"], "title": "Does Data Scaling Lead to Visual Compositional Generalization?", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u7531\u6570\u636e\u591a\u6837\u6027\u800c\u975e\u6570\u636e\u89c4\u6a21\u9a71\u52a8\uff0c\u7ebf\u6027\u5206\u89e3\u8868\u793a\u7ed3\u6784\u662f\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u5f53\u4ee3\u89c6\u89c9\u6a21\u578b\u662f\u5426\u5177\u5907\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u5bf9\u7ec4\u5408\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u53d8\u5316\u6570\u636e\u89c4\u6a21\u3001\u6982\u5ff5\u591a\u6837\u6027\u548c\u7ec4\u5408\u8986\u76d6\u7387\uff0c\u5206\u6790\u6a21\u578b\u8868\u73b0\u3002", "result": "\u7ec4\u5408\u6cdb\u5316\u4f9d\u8d56\u6570\u636e\u591a\u6837\u6027\uff0c\u7ebf\u6027\u5206\u89e3\u8868\u793a\u7ed3\u6784\u80fd\u5b9e\u73b0\u9ad8\u6548\u6cdb\u5316\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u90e8\u5206\u7b26\u5408\u6b64\u7ed3\u6784\u3002", "conclusion": "\u5f3a\u8c03\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u4f18\u5316\u8868\u793a\u7ed3\u6784\u4ee5\u63d0\u5347\u7ec4\u5408\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2507.06243", "pdf": "https://arxiv.org/pdf/2507.06243", "abs": "https://arxiv.org/abs/2507.06243", "authors": ["Md Nahid Hasan", "Md Monzur Murshed", "Md Mahadi Hasan", "Faysal A. Chowdhury"], "title": "A Machine Learning Framework for Breast Cancer Treatment Classification Using a Novel Dataset", "categories": ["stat.AP", "cs.LG"], "comment": "12 pages, 3 figures, 3 tables. This paper has been submitted to\n  Scientific Reports and has been under review for five months", "summary": "Breast cancer (BC) remains a significant global health challenge, with\npersonalized treatment selection complicated by the disease's molecular and\nclinical heterogeneity. BC treatment decisions rely on various patient-specific\nclinical factors, and machine learning (ML) offers a powerful approach to\npredicting treatment outcomes. This study utilizes The Cancer Genome Atlas\n(TCGA) breast cancer clinical dataset to develop ML models for predicting the\nlikelihood of undergoing chemotherapy or hormonal therapy. The models are\ntrained using five-fold cross-validation and evaluated through performance\nmetrics, including accuracy, precision, recall, specificity, sensitivity,\nF1-score, and area under the receiver operating characteristic curve (AUROC).\nModel uncertainty is assessed using bootstrap techniques, while SHAP values\nenhance interpretability by identifying key predictors. Among the tested\nmodels, the Gradient Boosting Machine (GBM) achieves the highest stable\nperformance (accuracy = 0.7718, AUROC = 0.8252), followed by Extreme Gradient\nBoosting (XGBoost) (accuracy = 0.7557, AUROC = 0.8044) and Adaptive Boosting\n(AdaBoost) (accuracy = 0.7552, AUROC = 0.8016). These findings underscore the\npotential of ML in supporting personalized breast cancer treatment decisions\nthrough data-driven insights.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e73\u817a\u764c\u60a3\u8005\u63a5\u53d7\u5316\u7597\u6216\u6fc0\u7d20\u6cbb\u7597\u7684\u53ef\u80fd\u6027\uff0cGBM\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e73\u817a\u764c\u6cbb\u7597\u7684\u4e2a\u6027\u5316\u9009\u62e9\u56e0\u5206\u5b50\u548c\u4e34\u5e8a\u5f02\u8d28\u6027\u800c\u590d\u6742\u5316\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u652f\u6301\u3002", "method": "\u4f7f\u7528TCGA\u4e73\u817a\u764c\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u79cdML\u6a21\u578b\uff0c\u5305\u62ecGBM\u3001XGBoost\u548cAdaBoost\u3002", "result": "GBM\u8868\u73b0\u6700\u4f18\uff08\u51c6\u786e\u73870.7718\uff0cAUROC 0.8252\uff09\uff0cXGBoost\u548cAdaBoost\u6b21\u4e4b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5728\u4e2a\u6027\u5316\u4e73\u817a\u764c\u6cbb\u7597\u51b3\u7b56\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.06252", "pdf": "https://arxiv.org/pdf/2507.06252", "abs": "https://arxiv.org/abs/2507.06252", "authors": ["Samaneh Shafee", "Alysson Bessani", "Pedro M. Ferreira"], "title": "False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach\nthat operates in the early phases of the cyber threat lifecycle. CTI involves\ncollecting, processing, and analyzing threat data to provide a more accurate\nand rapid understanding of cyber threats. Due to the large volume of data,\nautomation through Machine Learning (ML) and Natural Language Processing (NLP)\nmodels is essential for effective CTI extraction. These automated systems\nleverage Open Source Intelligence (OSINT) from sources like social networks,\nforums, and blogs to identify Indicators of Compromise (IoCs). Although prior\nresearch has focused on adversarial attacks on specific ML models, this study\nexpands the scope by investigating vulnerabilities within various components of\nthe entire CTI pipeline and their susceptibility to adversarial attacks. These\nvulnerabilities arise because they ingest textual inputs from various open\nsources, including real and potentially fake content. We analyse three types of\nattacks against CTI pipelines, including evasion, flooding, and poisoning, and\nassess their impact on the system's information selection capabilities.\nSpecifically, on fake text generation, the work demonstrates how adversarial\ntext generation techniques can create fake cybersecurity and cybersecurity-like\ntext that misleads classifiers, degrades performance, and disrupts system\nfunctionality. The focus is primarily on the evasion attack, as it precedes and\nenables flooding and poisoning attacks within the CTI pipeline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7f51\u7edc\u5a01\u80c1\u60c5\u62a5\uff08CTI\uff09\u7ba1\u9053\u4e2d\u5bf9\u6297\u6027\u653b\u51fb\u7684\u6f0f\u6d1e\uff0c\u91cd\u70b9\u5173\u6ce8\u9003\u907f\u3001\u6d2a\u6cdb\u548c\u6295\u6bd2\u653b\u51fb\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u7cfb\u7edf\u4fe1\u606f\u9009\u62e9\u80fd\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8eCTI\u7ba1\u9053\u4ece\u5f00\u653e\u6e90\uff08\u5982\u793e\u4ea4\u5a92\u4f53\u548c\u8bba\u575b\uff09\u83b7\u53d6\u6587\u672c\u8f93\u5165\uff0c\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5c24\u5176\u662f\u865a\u5047\u5185\u5bb9\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u4e09\u79cd\u653b\u51fb\u7c7b\u578b\uff08\u9003\u907f\u3001\u6d2a\u6cdb\u548c\u6295\u6bd2\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5bf9CTI\u7ba1\u9053\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5bf9\u6297\u6027\u6587\u672c\u751f\u6210\u6280\u672f\u5236\u9020\u865a\u5047\u7f51\u7edc\u5b89\u5168\u6587\u672c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u6027\u6587\u672c\u751f\u6210\u6280\u672f\u53ef\u4ee5\u8bef\u5bfc\u5206\u7c7b\u5668\u3001\u964d\u4f4e\u6027\u80fd\u5e76\u7834\u574f\u7cfb\u7edf\u529f\u80fd\uff0c\u5c24\u5176\u662f\u9003\u907f\u653b\u51fb\u4e3a\u540e\u7eed\u653b\u51fb\u63d0\u4f9b\u4e86\u6761\u4ef6\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86CTI\u7ba1\u9053\u4e2d\u5bf9\u6297\u6027\u653b\u51fb\u7684\u4e25\u91cd\u6027\uff0c\u5e76\u6307\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u589e\u5f3a\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2507.06262", "pdf": "https://arxiv.org/pdf/2507.06262", "abs": "https://arxiv.org/abs/2507.06262", "authors": ["Haoqi He", "Xiaokai Lin", "Jiancai Chen", "Yan Xiao"], "title": "Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method", "categories": ["cs.CR", "cs.AI", "cs.LG", "quant-ph"], "comment": "IJCAI 2025 Main Conference Accepted Paper", "summary": "Data poisoning attacks pose significant threats to machine learning models by\nintroducing malicious data into the training process, thereby degrading model\nperformance or manipulating predictions. Detecting and sifting out poisoned\ndata is an important method to prevent data poisoning attacks. Limited by\nclassical computation frameworks, upcoming larger-scale and more complex\ndatasets may pose difficulties for detection. We introduce the unique speedup\nof quantum computing for the first time in the task of detecting data\npoisoning. We present Q-Detection, a quantum-classical hybrid defense method\nfor detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which\nis optimized using quantum computing devices. Experimental results using\nmultiple quantum simulation libraries show that Q-Detection effectively defends\nagainst label manipulation and backdoor attacks. The metrics demonstrate that\nQ-Detection consistently outperforms the baseline methods and is comparable to\nthe state-of-the-art. Theoretical analysis shows that Q-Detection is expected\nto achieve more than a 20% speedup using quantum computing power.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u9632\u5fa1\u65b9\u6cd5Q-Detection\uff0c\u7528\u4e8e\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u6709\u671b\u5b9e\u73b020%\u4ee5\u4e0a\u7684\u52a0\u901f\u3002", "motivation": "\u6570\u636e\u6295\u6bd2\u653b\u51fb\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6784\u6210\u5a01\u80c1\uff0c\u4f20\u7edf\u8ba1\u7b97\u6846\u67b6\u5728\u5927\u89c4\u6a21\u590d\u6742\u6570\u636e\u96c6\u4e0a\u68c0\u6d4b\u56f0\u96be\uff0c\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faQ-Detection\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u8bbe\u5907\u4f18\u5316\u7684Q-WAN\uff0c\u901a\u8fc7\u91cf\u5b50\u6a21\u62df\u5e93\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "Q-Detection\u6709\u6548\u9632\u5fa1\u6807\u7b7e\u64cd\u7eb5\u548c\u540e\u95e8\u653b\u51fb\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u91cf\u5b50\u8ba1\u7b97\u53ef\u5e26\u676520%\u4ee5\u4e0a\u7684\u52a0\u901f\u3002", "conclusion": "Q-Detection\u4e3a\u6570\u636e\u6295\u6bd2\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u91cf\u5b50\u8ba1\u7b97\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.06264", "pdf": "https://arxiv.org/pdf/2507.06264", "abs": "https://arxiv.org/abs/2507.06264", "authors": ["Weronika Hryniewska-Guzik", "Przemyslaw Biecek"], "title": "X-ray transferable polyrepresentation learning", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "part of Weronika's PhD thesis", "summary": "The success of machine learning algorithms is inherently related to the\nextraction of meaningful features, as they play a pivotal role in the\nperformance of these algorithms. Central to this challenge is the quality of\ndata representation. However, the ability to generalize and extract these\nfeatures effectively from unseen datasets is also crucial. In light of this, we\nintroduce a novel concept: the polyrepresentation. Polyrepresentation\nintegrates multiple representations of the same modality extracted from\ndistinct sources, for example, vector embeddings from the Siamese Network,\nself-supervised models, and interpretable radiomic features. This approach\nyields better performance metrics compared to relying on a single\nrepresentation. Additionally, in the context of X-ray images, we demonstrate\nthe transferability of the created polyrepresentation to a smaller dataset,\nunderscoring its potential as a pragmatic and resource-efficient approach in\nvarious image-related solutions. It is worth noting that the concept of\npolyprepresentation on the example of medical data can also be applied to other\ndomains, showcasing its versatility and broad potential impact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u591a\u8868\u5f81\u201d\uff08polyrepresentation\uff09\u7684\u65b0\u6982\u5ff5\uff0c\u901a\u8fc7\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u540c\u4e00\u6a21\u6001\u7684\u591a\u79cd\u8868\u5f81\uff08\u5982Siamese Network\u7684\u5411\u91cf\u5d4c\u5165\u3001\u81ea\u76d1\u7763\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff09\uff0c\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u6709\u610f\u4e49\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u800c\u6570\u636e\u8868\u793a\u7684\u8d28\u91cf\u548c\u4ece\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e2d\u6709\u6548\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u8868\u5f81\u65b9\u6cd5\uff0c\u6574\u5408\u540c\u4e00\u6a21\u6001\u7684\u591a\u79cd\u8868\u5f81\uff08\u5982Siamese Network\u7684\u5411\u91cf\u5d4c\u5165\u3001\u81ea\u76d1\u7763\u6a21\u578b\u548c\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff09\u3002", "result": "\u591a\u8868\u5f81\u65b9\u6cd5\u5728\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u5355\u4e00\u8868\u5f81\uff0c\u4e14\u5728X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u591a\u8868\u5f81\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u6570\u636e\u53ca\u5176\u4ed6\u9886\u57df\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2507.06266", "pdf": "https://arxiv.org/pdf/2507.06266", "abs": "https://arxiv.org/abs/2507.06266", "authors": ["Tingyu Yuan", "Xi Zhang", "Xuanjing Chen"], "title": "Machine Learning based Enterprise Financial Audit Framework and High Risk Identification", "categories": ["q-fin.RM", "cs.AI", "cs.LG", "stat.AP"], "comment": null, "summary": "In the face of global economic uncertainty, financial auditing has become\nessential for regulatory compliance and risk mitigation. Traditional manual\nauditing methods are increasingly limited by large data volumes, complex\nbusiness structures, and evolving fraud tactics. This study proposes an\nAI-driven framework for enterprise financial audits and high-risk\nidentification, leveraging machine learning to improve efficiency and accuracy.\nUsing a dataset from the Big Four accounting firms (EY, PwC, Deloitte, KPMG)\nfrom 2020 to 2025, the research examines trends in risk assessment, compliance\nviolations, and fraud detection. The dataset includes key indicators such as\naudit project counts, high-risk cases, fraud instances, compliance breaches,\nemployee workload, and client satisfaction, capturing both audit behaviors and\nAI's impact on operations. To build a robust risk prediction model, three\nalgorithms - Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbors (KNN) - are evaluated. SVM uses hyperplane optimization for complex\nclassification, RF combines decision trees to manage high-dimensional,\nnonlinear data with resistance to overfitting, and KNN applies distance-based\nlearning for flexible performance. Through hierarchical K-fold cross-validation\nand evaluation using F1-score, accuracy, and recall, Random Forest achieves the\nbest performance, with an F1-score of 0.9012, excelling in identifying fraud\nand compliance anomalies. Feature importance analysis reveals audit frequency,\npast violations, employee workload, and client ratings as key predictors. The\nstudy recommends adopting Random Forest as a core model, enhancing features via\nengineering, and implementing real-time risk monitoring. This research\ncontributes valuable insights into using machine learning for intelligent\nauditing and risk management in modern enterprises.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u4f01\u4e1a\u8d22\u52a1\u5ba1\u8ba1\u548c\u9ad8\u98ce\u9669\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u4f7f\u7528\u56db\u5927\u4f1a\u8ba1\u5e08\u4e8b\u52a1\u62402020-2025\u5e74\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86SVM\u3001RF\u548cKNN\u4e09\u79cd\u7b97\u6cd5\uff0c\u6700\u7ec8RF\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u9762\u5bf9\u5168\u7403\u7ecf\u6d4e\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u4eba\u5de5\u5ba1\u8ba1\u65b9\u6cd5\u5728\u5927\u6570\u636e\u91cf\u548c\u590d\u6742\u4e1a\u52a1\u7ed3\u6784\u4e0b\u6548\u7387\u4f4e\u4e0b\uff0c\u9700AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528SVM\u3001RF\u548cKNN\u4e09\u79cd\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "RF\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u65700.9012\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u6b3a\u8bc8\u548c\u5408\u89c4\u5f02\u5e38\u3002\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u5ba1\u8ba1\u9891\u7387\u3001\u5386\u53f2\u8fdd\u89c4\u3001\u5458\u5de5\u5de5\u4f5c\u91cf\u548c\u5ba2\u6237\u8bc4\u5206\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528RF\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u7ed3\u5408\u7279\u5f81\u5de5\u7a0b\u548c\u5b9e\u65f6\u98ce\u9669\u76d1\u63a7\uff0c\u4e3a\u73b0\u4ee3\u4f01\u4e1a\u667a\u80fd\u5ba1\u8ba1\u548c\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.06275", "pdf": "https://arxiv.org/pdf/2507.06275", "abs": "https://arxiv.org/abs/2507.06275", "authors": ["Yassin Hussein Rassul", "Aram M. Ahmed", "Polla Fattah", "Bryar A. Hassan", "Arwaa W. Abdulkareem", "Tarik A. Rashid", "Joan Lu"], "title": "Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Offline Handwritten Text Recognition (HTR) systems play a crucial role in\napplications such as historical document digitization, automatic form\nprocessing, and biometric authentication. However, their performance is often\nhindered by the limited availability of annotated training data, particularly\nfor low-resource languages and complex scripts. This paper presents a\ncomprehensive survey of offline handwritten data augmentation and generation\ntechniques designed to improve the accuracy and robustness of HTR systems. We\nsystematically examine traditional augmentation methods alongside recent\nadvances in deep learning, including Generative Adversarial Networks (GANs),\ndiffusion models, and transformer-based approaches. Furthermore, we explore the\nchallenges associated with generating diverse and realistic handwriting\nsamples, particularly in preserving script authenticity and addressing data\nscarcity. This survey follows the PRISMA methodology, ensuring a structured and\nrigorous selection process. Our analysis began with 1,302 primary studies,\nwhich were filtered down to 848 after removing duplicates, drawing from key\nacademic sources such as IEEE Digital Library, Springer Link, Science Direct,\nand ACM Digital Library. By evaluating existing datasets, assessment metrics,\nand state-of-the-art methodologies, this survey identifies key research gaps\nand proposes future directions to advance the field of handwritten text\ngeneration across diverse linguistic and stylistic landscapes.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u79bb\u7ebf\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u4e0e\u751f\u6210\u6280\u672f\uff0c\u65e8\u5728\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52a3\u3002", "motivation": "\u79bb\u7ebf\u624b\u5199\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u5728\u5386\u53f2\u6587\u6863\u6570\u5b57\u5316\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u590d\u6742\u811a\u672c\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528PRISMA\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GANs\u3001\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff09\uff0c\u5e76\u4ece1,302\u9879\u7814\u7a76\u4e2d\u7b5b\u9009\u51fa848\u9879\u8fdb\u884c\u7efc\u8ff0\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u6570\u636e\u96c6\u3001\u6307\u6807\u548c\u5148\u8fdb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u624b\u5199\u6587\u672c\u751f\u6210\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u603b\u7ed3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u6f5c\u5728\u8def\u5f84\u3002"}}
{"id": "2507.06278", "pdf": "https://arxiv.org/pdf/2507.06278", "abs": "https://arxiv.org/abs/2507.06278", "authors": ["Kemboi Cheruiyot", "Nickson Kiprotich", "Vyacheslav Kungurtsev", "Kennedy Mugo", "Vivian Mwirigi", "Marvin Ngesa"], "title": "A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "The increasing interest in research and innovation towards the development of\nautonomous agents presents a number of complex yet important scenarios of\nmultiple AI Agents interacting with each other in an environment. The\nparticular setting can be understood as exhibiting three possibly topologies of\ninteraction - centrally coordinated cooperation, ad-hoc interaction and\ncooperation, and settings with noncooperative incentive structures. This\narticle presents a comprehensive survey of all three domains, defined under the\nformalism of Federal Reinforcement Learning (RL), Decentralized RL, and\nNoncooperative RL, respectively. Highlighting the structural similarities and\ndistinctions, we review the state of the art in these subjects, primarily\nexplored and developed only recently in the literature. We include the\nformulations as well as known theoretical guarantees and highlights and\nlimitations of numerical performance.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e09\u79cd\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u62d3\u6251\u7ed3\u6784\uff1a\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3001\u53bb\u4e2d\u5fc3\u5316RL\u548c\u975e\u5408\u4f5cRL\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u7ed3\u6784\u5f02\u540c\u3001\u7406\u8bba\u4fdd\u8bc1\u53ca\u6570\u503c\u6027\u80fd\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5bf9\u81ea\u4e3b\u667a\u80fd\u4f53\u7814\u7a76\u7684\u5174\u8da3\u589e\u52a0\uff0c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u590d\u6742\u573a\u666f\u53d8\u5f97\u91cd\u8981\uff0c\u672c\u6587\u65e8\u5728\u5168\u9762\u68b3\u7406\u4e09\u79cd\u4ea4\u4e92\u62d3\u6251\u7684\u7814\u7a76\u73b0\u72b6\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u522b\u5b9a\u4e49\u5e76\u5206\u6790\u4e86\u8054\u90a6RL\u3001\u53bb\u4e2d\u5fc3\u5316RL\u548c\u975e\u5408\u4f5cRL\u7684\u6846\u67b6\u3001\u7406\u8bba\u4fdd\u8bc1\u53ca\u6570\u503c\u6027\u80fd\u3002", "result": "\u603b\u7ed3\u4e86\u4e09\u79cd\u4ea4\u4e92\u62d3\u6251\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u5305\u62ec\u5176\u7ed3\u6784\u7279\u70b9\u3001\u7406\u8bba\u652f\u6301\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u7efc\u8ff0\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2507.06321", "pdf": "https://arxiv.org/pdf/2507.06321", "abs": "https://arxiv.org/abs/2507.06321", "authors": ["Joon Tai Kim", "Tianle Chen", "Ziyu Dong", "Nishanth Kunchala", "Alexander Guller", "Daniel Ospina Acero", "Roger Williams", "Mrinal Kumar"], "title": "Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 5 figures, and under review for AIAA SciTech 2026", "summary": "Collecting and annotating images for the purpose of training segmentation\nmodels is often cost prohibitive. In the domain of wildland fire science, this\nchallenge is further compounded by the scarcity of reliable public datasets\nwith labeled ground truth. This paper presents the Centralized Copy-Paste Data\nAugmentation (CCPDA) method, for the purpose of assisting with the training of\ndeep-learning multiclass segmentation models, with special focus on improving\nsegmentation outcomes for the fire-class. CCPDA has three main steps: (i)\nidentify fire clusters in the source image, (ii) apply a centralization\ntechnique to focus on the core of the fire area, and (iii) paste the refined\nfire clusters onto a target image. This method increases dataset diversity\nwhile preserving the essential characteristics of the fire class. The\neffectiveness of this augmentation technique is demonstrated via numerical\nanalysis and comparison against various other augmentation methods using a\nweighted sum-based multi-objective optimization approach. This approach helps\nelevate segmentation performance metrics specific to the fire class, which\ncarries significantly more operational significance than other classes (fuel,\nash, or background). Numerical performance assessment validates the efficacy of\nthe presented CCPDA method in alleviating the difficulties associated with\nsmall, manually labeled training datasets. It also illustrates that CCPDA\noutperforms other augmentation strategies in the application scenario\nconsidered, particularly in improving fire-class segmentation performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCPDA\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u91ce\u706b\u79d1\u5b66\u4e2d\u591a\u7c7b\u5206\u5272\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u7279\u522b\u662f\u63d0\u5347\u706b\u707e\u7c7b\u522b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5728\u91ce\u706b\u79d1\u5b66\u9886\u57df\uff0c\u83b7\u53d6\u548c\u6807\u6ce8\u56fe\u50cf\u7528\u4e8e\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u6210\u672c\u9ad8\u6602\u4e14\u516c\u5f00\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "CCPDA\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a\u8bc6\u522b\u6e90\u56fe\u50cf\u4e2d\u7684\u706b\u707e\u96c6\u7fa4\u3001\u96c6\u4e2d\u6280\u672f\u805a\u7126\u706b\u707e\u6838\u5fc3\u533a\u57df\u3001\u5c06\u7cbe\u70bc\u7684\u706b\u707e\u96c6\u7fa4\u7c98\u8d34\u5230\u76ee\u6807\u56fe\u50cf\u4e0a\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5206\u6790\u548c\u591a\u76ee\u6807\u4f18\u5316\u6bd4\u8f83\uff0cCCPDA\u663e\u8457\u63d0\u5347\u4e86\u706b\u707e\u7c7b\u522b\u7684\u5206\u5272\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6\u589e\u5f3a\u65b9\u6cd5\u3002", "conclusion": "CCPDA\u6709\u6548\u7f13\u89e3\u4e86\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u56f0\u96be\uff0c\u5c24\u5176\u5728\u706b\u707e\u7c7b\u522b\u5206\u5272\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.06336", "pdf": "https://arxiv.org/pdf/2507.06336", "abs": "https://arxiv.org/abs/2507.06336", "authors": ["Adam J Riesselman", "Evan M Cofer", "Therese LaRue", "Wim Meeussen"], "title": "Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data", "categories": ["q-bio.QM", "cs.LG", "cs.RO"], "comment": null, "summary": "Quantifying organism-level phenotypes, such as growth dynamics and biomass\naccumulation, is fundamental to understanding agronomic traits and optimizing\ncrop production. However, quality growing data of plants at scale is difficult\nto generate. Here we use a mobile robotic platform to capture high-resolution\nenvironmental sensing and phenotyping measurements of a large-scale hydroponic\nleafy greens system. We describe a self-supervised modeling approach to build a\nmap from observed growing data to the entire plant growth trajectory. We\ndemonstrate our approach by forecasting future plant height and harvest mass of\ncrops in this system. This approach represents a significant advance in\ncombining robotic automation and machine learning, as well as providing\nactionable insights for agronomic research and operational efficiency.", "AI": {"tldr": "\u5229\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u548c\u5927\u89c4\u6a21\u6c34\u57f9\u7eff\u53f6\u852c\u83dc\u7cfb\u7edf\u7684\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5efa\u6a21\u65b9\u6cd5\u9884\u6d4b\u690d\u7269\u751f\u957f\u8f68\u8ff9\uff0c\u4e3a\u519c\u4e1a\u7814\u7a76\u548c\u6548\u7387\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u91cf\u5316\u690d\u7269\u751f\u957f\u52a8\u6001\u548c\u751f\u7269\u91cf\u79ef\u7d2f\u5bf9\u7406\u89e3\u519c\u827a\u6027\u72b6\u548c\u4f18\u5316\u4f5c\u7269\u751f\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u751f\u957f\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u4f7f\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u611f\u77e5\u548c\u8868\u578b\u6d4b\u91cf\u6570\u636e\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5efa\u6a21\u65b9\u6cd5\u6784\u5efa\u4ece\u89c2\u6d4b\u6570\u636e\u5230\u690d\u7269\u751f\u957f\u8f68\u8ff9\u7684\u6620\u5c04\u3002", "result": "\u6210\u529f\u9884\u6d4b\u4e86\u672a\u6765\u690d\u7269\u9ad8\u5ea6\u548c\u6536\u83b7\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u81ea\u52a8\u5316\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u519c\u4e1a\u7814\u7a76\u548c\u64cd\u4f5c\u6548\u7387\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u662f\u673a\u5668\u4eba\u81ea\u52a8\u5316\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.06344", "pdf": "https://arxiv.org/pdf/2507.06344", "abs": "https://arxiv.org/abs/2507.06344", "authors": ["Sabri Meyer", "Francesco Scala", "Francesco Tacchino", "Aurelien Lucchi"], "title": "Trainability of Quantum Models Beyond Known Classical Simulability", "categories": ["quant-ph", "cs.CC", "cs.LG"], "comment": "11 pages, 7 figures, 52 pages of supplementary material", "summary": "Variational Quantum Algorithms (VQAs) are promising candidates for near-term\nquantum computing, yet they face scalability challenges due to barren plateaus,\nwhere gradients vanish exponentially in the system size. Recent conjectures\nsuggest that avoiding barren plateaus might inherently lead to classical\nsimulability, thus limiting the opportunities for quantum advantage. In this\nwork, we advance the theoretical understanding of the relationship between the\ntrainability and computational complexity of VQAs, thus directly addressing the\nconjecture. We introduce the Linear Clifford Encoder (LCE), a novel technique\nthat ensures constant-scaling gradient statistics on optimization landscape\nregions that are close to Clifford circuits. Additionally, we leverage\nclassical Taylor surrogates to reveal computational complexity phase\ntransitions from polynomial to super-polynomial as the initialization region\nsize increases. Combining these results, we reveal a deeper link between\ntrainability and computational complexity, and analytically prove that barren\nplateaus can be avoided in regions for which no classical surrogate is known to\nexist. Furthermore, numerical experiments on LCE transformed landscapes confirm\nin practice the existence of a super-polynomially complex ``transition zone''\nwhere gradients decay polynomially. These findings indicate a plausible path to\npractically relevant, barren plateau-free variational models with potential for\nquantum advantage.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\uff08VQAs\uff09\u7684\u53ef\u8bad\u7ec3\u6027\u4e0e\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u907f\u514d\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86\u91cf\u5b50\u4f18\u52bf\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u89e3\u51b3VQAs\u56e0\u68af\u5ea6\u6d88\u5931\uff08barren plateaus\uff09\u800c\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u907f\u514d\u68af\u5ea6\u6d88\u5931\u662f\u5426\u4f1a\u5bfc\u81f4\u7ecf\u5178\u53ef\u6a21\u62df\u6027\u3002", "method": "\u5f15\u5165\u7ebf\u6027Clifford\u7f16\u7801\u5668\uff08LCE\uff09\u786e\u4fdd\u68af\u5ea6\u7edf\u8ba1\u6052\u5b9a\uff0c\u5e76\u7ed3\u5408\u7ecf\u5178\u6cf0\u52d2\u4ee3\u7406\u63ed\u793a\u8ba1\u7b97\u590d\u6742\u6027\u76f8\u53d8\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u65e0\u7ecf\u5178\u4ee3\u7406\u7684\u533a\u57df\u53ef\u4ee5\u907f\u514d\u68af\u5ea6\u6d88\u5931\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b58\u5728\u8d85\u591a\u9879\u5f0f\u590d\u6742\u6027\u7684\u8fc7\u6e21\u533a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u65e0\u68af\u5ea6\u6d88\u5931\u7684\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u73b0\u91cf\u5b50\u4f18\u52bf\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.06358", "pdf": "https://arxiv.org/pdf/2507.06358", "abs": "https://arxiv.org/abs/2507.06358", "authors": ["Victor Boussange", "Philipp Brun", "Johanna T. Malle", "Gabriele Midolo", "Jeanne Portier", "Th\u00e9ophile Sanchez", "Niklaus E. Zimmermann", "Irena Axmanov\u00e1", "Helge Bruelheide", "Milan Chytr\u00fd", "Stephan Kambach", "Zde\u0148ka Lososov\u00e1", "Martin Ve\u010de\u0159a", "Idoia Biurrun", "Klaus T. Ecker", "Jonathan Lenoir", "Jens-Christian Svenning", "Dirk Nikolaus Karger"], "title": "Deep learning-based species-area models reveal multi-scale patterns of species richness and turnover", "categories": ["q-bio.PE", "cs.LG", "92-08, 92B05, 92B15, 92B20, 92D40 (Primary) 62P10, 62P12 (Secondary)"], "comment": "31 pages", "summary": "The number of species within ecosystems is influenced not only by their\nintrinsic characteristics but also by the spatial scale considered. As the\nsampled area expands, species richness increases, a phenomenon described by the\nspecies-area relationship (SAR). The accumulation dynamics of the SAR results\nfrom a complex interplay of biotic and abiotic processes operating at various\nspatial scales. However, the challenge of collecting exhaustive biodiversity\nrecords across spatial scales has hindered a comprehensive understanding of\nthese dynamics. Here, we develop a deep learning approach that leverages\nsampling theory and small-scale ecological surveys to spatially resolve the\nscale-dependency of species richness. We demonstrate its performance by\npredicting the species richness of vascular plant communities across Europe,\nand evaluate the predictions against an independent dataset of plant community\ninventories. Our model improves species richness estimates by 32\\% and delivers\nspatially explicit patterns of species richness and turnover for sampling areas\nranging from square meters to hundreds of square kilometers. Explainable AI\ntechniques further disentangle how drivers of species richness operate across\nspatial scales. The ability of our model to represent the multi-scale nature of\nbiodiversity is essential to deliver robust biodiversity assessments and\nforecasts under global change.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u91c7\u6837\u7406\u8bba\u548c\u5c0f\u89c4\u6a21\u751f\u6001\u8c03\u67e5\uff0c\u9884\u6d4b\u6b27\u6d32\u7ef4\u7ba1\u690d\u7269\u7fa4\u843d\u7684\u7269\u79cd\u4e30\u5bcc\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u7406\u89e3\u7269\u79cd\u4e30\u5bcc\u5ea6\u5728\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u4e0a\u7684\u52a8\u6001\u53d8\u5316\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5168\u9762\u7684\u751f\u7269\u591a\u6837\u6027\u8bb0\u5f55\uff0c\u8fd9\u4e00\u76ee\u6807\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u91c7\u6837\u7406\u8bba\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u9884\u6d4b\u7269\u79cd\u4e30\u5bcc\u5ea6\uff0c\u5e76\u901a\u8fc7\u72ec\u7acb\u6570\u636e\u96c6\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5c06\u7269\u79cd\u4e30\u5bcc\u5ea6\u4f30\u8ba1\u63d0\u9ad8\u4e8632%\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ece\u5e73\u65b9\u7c73\u5230\u6570\u767e\u5e73\u65b9\u516c\u91cc\u7684\u7269\u79cd\u4e30\u5bcc\u5ea6\u548c\u66f4\u66ff\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u751f\u7269\u591a\u6837\u6027\u7684\u591a\u5c3a\u5ea6\u7279\u6027\uff0c\u4e3a\u5168\u7403\u53d8\u5316\u4e0b\u7684\u751f\u7269\u591a\u6837\u6027\u8bc4\u4f30\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2507.06404", "pdf": "https://arxiv.org/pdf/2507.06404", "abs": "https://arxiv.org/abs/2507.06404", "authors": ["Matteo Tiezzi", "Tommaso Apicella", "Carlos Cardenas-Perez", "Giovanni Fregonese", "Stefano Dafarra", "Pietro Morerio", "Daniele Pucci", "Alessio Del Bue"], "title": "Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Evaluating and comparing the performance of autonomous Humanoid Robots is\nchallenging, as success rate metrics are difficult to reproduce and fail to\ncapture the complexity of robot movement trajectories, critical in Human-Robot\nInteraction and Collaboration (HRIC). To address these challenges, we propose a\ngeneral evaluation framework that measures the quality of Imitation Learning\n(IL) methods by focusing on trajectory performance. We devise the Neural Meta\nEvaluator (NeME), a deep learning model trained to classify actions from robot\njoint trajectories. NeME serves as a meta-evaluator to compare the performance\nof robot control policies, enabling policy evaluation without requiring human\ninvolvement in the loop. We validate our framework on ergoCub, a humanoid\nrobot, using teleoperation data and comparing IL methods tailored to the\navailable platform. The experimental results indicate that our method is more\naligned with the success rate obtained on the robot than baselines, offering a\nreproducible, systematic, and insightful means for comparing the performance of\nmultimodal imitation learning approaches in complex HRI tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u6027\u80fd\u7684\u901a\u7528\u8bc4\u4f30\u6846\u67b6NeME\uff0c\u7528\u4e8e\u6bd4\u8f83\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u6210\u529f\u7387\u6307\u6807\u96be\u4ee5\u590d\u73b0\u4e14\u65e0\u6cd5\u6355\u6349\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u7684\u590d\u6742\u6027\uff0c\u8bc4\u4f30\u4eba\u5f62\u673a\u5668\u4eba\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86Neural Meta Evaluator (NeME)\uff0c\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u673a\u5668\u4eba\u5173\u8282\u8f68\u8ff9\u4e2d\u5206\u7c7b\u52a8\u4f5c\uff0c\u5e76\u4f5c\u4e3a\u5143\u8bc4\u4f30\u5668\u6bd4\u8f83\u63a7\u5236\u7b56\u7565\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u66f4\u7b26\u5408\u673a\u5668\u4eba\u7684\u5b9e\u9645\u6210\u529f\u7387\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "conclusion": "NeME\u6846\u67b6\u4e3a\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6027\u80fd\u6bd4\u8f83\u5de5\u5177\u3002"}}
{"id": "2507.06415", "pdf": "https://arxiv.org/pdf/2507.06415", "abs": "https://arxiv.org/abs/2507.06415", "authors": ["Zeming Chen", "Angelika Romanou", "Gail Weiss", "Antoine Bosselut"], "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 7 figures", "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.", "AI": {"tldr": "PERK\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u68af\u5ea6\u66f4\u65b0\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6765\u7f16\u7801\u957f\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u4fe1\u606f\u566a\u58f0\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u5143\u5b66\u4e60\u65b9\u6cd5\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u3002", "method": "\u91c7\u7528\u53cc\u5d4c\u5957\u4f18\u5316\u5faa\u73af\uff1a\u5185\u5faa\u73af\u5c06\u4e0a\u4e0b\u6587\u7f16\u7801\u5230\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\uff0c\u5916\u5faa\u73af\u5b66\u4e60\u4f7f\u7528\u9002\u914d\u5668\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c0f\u6a21\u578b\u63d0\u534790%\uff0c\u5927\u6a21\u578b\u63d0\u534727%\uff0c\u4e14\u5bf9\u63a8\u7406\u590d\u6742\u6027\u3001\u957f\u5ea6\u5916\u63a8\u548c\u4fe1\u606f\u4f4d\u7f6e\u66f4\u9c81\u68d2\u3002", "conclusion": "PERK\u5728\u8bad\u7ec3\u65f6\u5185\u5b58\u6d88\u8017\u5927\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u6bd4\u63d0\u793a\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3002"}}
{"id": "2507.06417", "pdf": "https://arxiv.org/pdf/2507.06417", "abs": "https://arxiv.org/abs/2507.06417", "authors": ["Laura Pitukov\u00e1", "Peter Sin\u010d\u00e1k", "L\u00e1szl\u00f3 J\u00f3zsef Kov\u00e1cs"], "title": "Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Preprint version. Accepted to IEEE SMC 2025", "summary": "This study conducts a comprehensive comparison of four neural network\narchitectures: Convolutional Neural Network, Capsule Network, Convolutional\nKolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional\nKolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines\nthe dynamic routing and spatial hierarchy capabilities of Capsule Network with\nthe flexible and interpretable function approximation of Convolutional\nKolmogorov--Arnold Networks. This novel hybrid model was developed to improve\nfeature representation and classification accuracy, particularly in challenging\nreal-world biomedical image data. The architectures were evaluated on a\nhistopathological image dataset, where Capsule-ConvKAN achieved the highest\nclassification performance with an accuracy of 91.21\\%. The results demonstrate\nthe potential of the newly introduced Capsule-ConvKAN in capturing spatial\npatterns, managing complex features, and addressing the limitations of\ntraditional convolutional models in medical image classification.", "AI": {"tldr": "\u6bd4\u8f83\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u51fa\u65b0\u7684Capsule-ConvKAN\u6a21\u578b\uff0c\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u6539\u8fdb\u7279\u5f81\u8868\u793a\u548c\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6570\u636e\u4e2d\u3002", "method": "\u7ed3\u5408Capsule Network\u7684\u52a8\u6001\u8def\u7531\u548c\u7a7a\u95f4\u5c42\u6b21\u80fd\u529b\u4e0eConvKAN\u7684\u7075\u6d3b\u53ef\u89e3\u91ca\u51fd\u6570\u903c\u8fd1\uff0c\u63d0\u51faCapsule-ConvKAN\u3002", "result": "\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cCapsule-ConvKAN\u8fbe\u523091.21%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u65b0\u6a21\u578b\u5728\u6355\u83b7\u7a7a\u95f4\u6a21\u5f0f\u548c\u7ba1\u7406\u590d\u6742\u7279\u5f81\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5377\u79ef\u6a21\u578b\u3002"}}
{"id": "2507.06427", "pdf": "https://arxiv.org/pdf/2507.06427", "abs": "https://arxiv.org/abs/2507.06427", "authors": ["Shun Wang", "Tyler Loakman", "Youbo Lei", "Yi Liu", "Bohao Yang", "Yuting Zhao", "Dong Yang", "Chenghua Lin"], "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u5178\u5b66\u4e60\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u53d6\u5355\u4e49\u7279\u5f81\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u88ab\u89c6\u4e3a\u9ed1\u76d2\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u5f71\u54cd\u6027\u80fd\u63d0\u5347\u3002", "method": "\u91c7\u7528\u5b57\u5178\u5b66\u4e60\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\uff0c\u63d0\u53d6\u5355\u4e49\u7279\u5f81\u5e76\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u9690\u55bb\u68c0\u6d4b\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u5206\u89e3\u65b9\u6cd5\u589e\u5f3a\u4e86LLM\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.06428", "pdf": "https://arxiv.org/pdf/2507.06428", "abs": "https://arxiv.org/abs/2507.06428", "authors": ["Samuel N. Cohen", "Jackson Hebner", "Deqing Jiang", "Justin Sirignano"], "title": "Neural Actor-Critic Methods for Hamilton-Jacobi-Bellman PDEs: Asymptotic Analysis and Numerical Studies", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "stat.ML", "93E20, 35Q93, 68T07, 90-08"], "comment": "41 pages", "summary": "We mathematically analyze and numerically study an actor-critic machine\nlearning algorithm for solving high-dimensional Hamilton-Jacobi-Bellman (HJB)\npartial differential equations from stochastic control theory. The architecture\nof the critic (the estimator for the value function) is structured so that the\nboundary condition is always perfectly satisfied (rather than being included in\nthe training loss) and utilizes a biased gradient which reduces computational\ncost. The actor (the estimator for the optimal control) is trained by\nminimizing the integral of the Hamiltonian over the domain, where the\nHamiltonian is estimated using the critic. We show that the training dynamics\nof the actor and critic neural networks converge in a Sobolev-type space to a\ncertain infinite-dimensional ordinary differential equation (ODE) as the number\nof hidden units in the actor and critic $\\rightarrow \\infty$. Further, under a\nconvexity-like assumption on the Hamiltonian, we prove that any fixed point of\nthis limit ODE is a solution of the original stochastic control problem. This\nprovides an important guarantee for the algorithm's performance in light of the\nfact that finite-width neural networks may only converge to a local minimizers\n(and not optimal solutions) due to the non-convexity of their loss functions.\nIn our numerical studies, we demonstrate that the algorithm can solve\nstochastic control problems accurately in up to 200 dimensions. In particular,\nwe construct a series of increasingly complex stochastic control problems with\nknown analytic solutions and study the algorithm's numerical performance on\nthem. These problems range from a linear-quadratic regulator equation to highly\nchallenging equations with non-convex Hamiltonians, allowing us to identify and\nanalyze the strengths and limitations of this neural actor-critic method for\nsolving HJB equations.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u7528\u4e8e\u6c42\u89e3\u9ad8\u7ef4HJB\u65b9\u7a0b\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728200\u7ef4\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\u4e2d\u7684HJB\u65b9\u7a0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u53ef\u80fd\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u8bbe\u8ba1\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u7684\u8bc4\u8bba\u5bb6\u7f51\u7edc\u548c\u57fa\u4e8e\u54c8\u5bc6\u987f\u91cf\u79ef\u5206\u7684\u6f14\u5458\u7f51\u7edc\uff0c\u8bc1\u660e\u5176\u6536\u655b\u6027\u3002", "result": "\u7b97\u6cd5\u5728200\u7ef4\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4HJB\u65b9\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u6ce8\u610f\u975e\u51f8\u54c8\u5bc6\u987f\u91cf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.06434", "pdf": "https://arxiv.org/pdf/2507.06434", "abs": "https://arxiv.org/abs/2507.06434", "authors": ["Ayrton San Joaquin", "Rokas Gipi\u0161kis", "Leon Staufer", "Ariel Gil"], "title": "Deprecating Benchmarks: Criteria and Framework", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "10 pages, 1 table. Accepted to the ICML 2025 Technical AI Governance\n  Workshop", "summary": "As frontier artificial intelligence (AI) models rapidly advance, benchmarks\nare integral to comparing different models and measuring their progress in\ndifferent task-specific domains. However, there is a lack of guidance on when\nand how benchmarks should be deprecated once they cease to effectively perform\ntheir purpose. This risks benchmark scores over-valuing model capabilities, or\nworse, obscuring capabilities and safety-washing. Based on a review of\nbenchmarking practices, we propose criteria to decide when to fully or\npartially deprecate benchmarks, and a framework for deprecating benchmarks. Our\nwork aims to advance the state of benchmarking towards rigorous and quality\nevaluations, especially for frontier models, and our recommendations are aimed\nto benefit benchmark developers, benchmark users, AI governance actors (across\ngovernments, academia, and industry panels), and policy makers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u51c6\u6d4b\u8bd5\u6dd8\u6c70\u6807\u51c6\u4e0e\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3AI\u6a21\u578b\u8bc4\u4f30\u4e2d\u8fc7\u65f6\u57fa\u51c6\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8fc7\u65f6\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\uff0c\u751a\u81f3\u63a9\u76d6\u771f\u5b9e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u56de\u987e\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\uff0c\u63d0\u51fa\u6dd8\u6c70\u57fa\u51c6\u7684\u6807\u51c6\u548c\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u51b3\u5b9a\u4f55\u65f6\u6dd8\u6c70\u57fa\u51c6\u7684\u51c6\u5219\u548c\u5b9e\u65bd\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u63a8\u52a8\u66f4\u4e25\u8c28\u7684\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\uff0c\u4e3a\u5f00\u53d1\u8005\u3001\u7528\u6237\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2507.06472", "pdf": "https://arxiv.org/pdf/2507.06472", "abs": "https://arxiv.org/abs/2507.06472", "authors": ["Tian Li", "Artem Polyvyanyy", "Sander J. J. Leemans"], "title": "Stochastic Alignments: Matching an Observed Trace to Stochastic Process Models", "categories": ["cs.FL", "cs.LG"], "comment": null, "summary": "Process mining leverages event data extracted from IT systems to generate\ninsights into the business processes of organizations. Such insights benefit\nfrom explicitly considering the frequency of behavior in business processes,\nwhich is captured by stochastic process models. Given an observed trace and a\nstochastic process model, conventional alignment-based conformance checking\ntechniques face a fundamental limitation: They prioritize matching the trace to\na model path with minimal deviations, which may, however, lead to selecting an\nunlikely path. In this paper, we study the problem of matching an observed\ntrace to a stochastic process model by identifying a likely model path with a\nlow edit distance to the trace. We phrase this as an optimization problem and\ndevelop a heuristic-guided path-finding algorithm to solve it. Our open-source\nimplementation demonstrates the feasibility of the approach and shows that it\ncan provide new, useful diagnostic insights for analysts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u8def\u5f84\u67e5\u627e\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u4e0e\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u5339\u914d\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u53ef\u80fd\u9009\u62e9\u4f4e\u6982\u7387\u8def\u5f84\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5728\u5339\u914d\u8f68\u8ff9\u4e0e\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u65f6\uff0c\u53ef\u80fd\u9009\u62e9\u504f\u79bb\u8f83\u5927\u7684\u4f4e\u6982\u7387\u8def\u5f84\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u9891\u7387\u4fe1\u606f\u3002", "method": "\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\u3002", "result": "\u5f00\u6e90\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u5206\u6790\u4eba\u5458\u63d0\u4f9b\u4e86\u65b0\u7684\u8bca\u65ad\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5339\u914d\u8f68\u8ff9\u4e0e\u6a21\u578b\u8def\u5f84\uff0c\u540c\u65f6\u8003\u8651\u8def\u5f84\u7684\u4f3c\u7136\u6027\u548c\u7f16\u8f91\u8ddd\u79bb\u3002"}}
{"id": "2507.06479", "pdf": "https://arxiv.org/pdf/2507.06479", "abs": "https://arxiv.org/abs/2507.06479", "authors": ["Niloofar Asefi", "Leonard Lupin-Jimenez", "Tianning Wu", "Ruoying He", "Ashesh Chattopadhyay"], "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity", "categories": ["physics.ao-ph", "cs.AI", "cs.LG", "math.DS", "nlin.CD"], "comment": null, "summary": "Reconstructing ocean dynamics from observational data is fundamentally\nlimited by the sparse, irregular, and Lagrangian nature of spatial sampling,\nparticularly in subsurface and remote regions. This sparsity poses significant\nchallenges for forecasting key phenomena such as eddy shedding and rogue waves.\nTraditional data assimilation methods and deep learning models often struggle\nto recover mesoscale turbulence under such constraints. We leverage a deep\nlearning framework that combines neural operators with denoising diffusion\nprobabilistic models (DDPMs) to reconstruct high-resolution ocean states from\nextremely sparse Lagrangian observations. By conditioning the generative model\non neural operator outputs, the framework accurately captures small-scale,\nhigh-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and\n$99.9\\%$ sparsity (for real satellite observations). We validate our method on\nbenchmark systems, synthetic float observations, and real satellite data,\ndemonstrating robust performance under severe spatial sampling limitations as\ncompared to other deep learning baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u548c\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPMs\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6781\u7a00\u758f\u7684\u62c9\u683c\u6717\u65e5\u89c2\u6d4b\u6570\u636e\u4e2d\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u6d77\u6d0b\u72b6\u6001\u3002", "motivation": "\u6d77\u6d0b\u89c2\u6d4b\u6570\u636e\u7a00\u758f\u3001\u4e0d\u89c4\u5219\u4e14\u591a\u4e3a\u62c9\u683c\u6717\u65e5\u91c7\u6837\uff0c\u9650\u5236\u4e86\u6d77\u6d0b\u52a8\u529b\u5b66\u7684\u91cd\u5efa\uff0c\u5c24\u5176\u662f\u5bf9\u4e2d\u5c3a\u5ea6\u6e4d\u6d41\u7b49\u5173\u952e\u73b0\u8c61\u7684\u9884\u6d4b\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u548cDDPMs\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5728\u795e\u7ecf\u7b97\u5b50\u8f93\u51fa\u4e0a\u6761\u4ef6\u5316\uff0c\u6355\u6349\u5c0f\u5c3a\u5ea6\u9ad8\u6ce2\u6570\u52a8\u529b\u5b66\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u536b\u661f\u89c2\u6d4b\u4e2d\uff0c\u5373\u4f7f\u6570\u636e\u7a00\u758f\u5ea6\u9ad8\u8fbe99%\u548c99.9%\uff0c\u6a21\u578b\u4ecd\u80fd\u51c6\u786e\u91cd\u5efa\u6d77\u6d0b\u72b6\u6001\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6781\u7aef\u7a00\u758f\u91c7\u6837\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6d77\u6d0b\u52a8\u529b\u5b66\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06506", "pdf": "https://arxiv.org/pdf/2507.06506", "abs": "https://arxiv.org/abs/2507.06506", "authors": ["Russell Taylor", "Benjamin Herbert", "Michael Sana"], "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Translating wordplay across languages presents unique challenges that have\nlong confounded both professional human translators and machine translation\nsystems. This research proposes a novel approach for translating puns from\nEnglish to French by combining state-of-the-art large language models with\nspecialized techniques for wordplay generation.\n  Our methodology employs a three-stage approach. First, we establish a\nbaseline using multiple frontier large language models with feedback based on a\nnew contrastive learning dataset. Second, we implement a guided\nchain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we\nimplement a multi-agent generator-discriminator framework for evaluating and\nregenerating puns with feedback.\n  Moving beyond the limitations of literal translation, our methodology's\nprimary objective is to capture the linguistic creativity and humor of the\nsource text wordplay, rather than simply duplicating its vocabulary. Our best\nruns earned first and second place in the CLEF JOKER 2025 Task 2 competition\nwhere they were evaluated manually by expert native French speakers.\n  This research addresses a gap between translation studies and computational\nlinguistics by implementing linguistically-informed techniques for wordplay\ntranslation, advancing our understanding of how language models can be\nleveraged to handle the complex interplay between semantic ambiguity, phonetic\nsimilarity, and the implicit cultural and linguistic awareness needed for\nsuccessful humor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u95e8\u6280\u672f\u7684\u53cc\u5173\u8bed\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u8de8\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u5e7d\u9ed8\u548c\u521b\u610f\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53cc\u5173\u8bed\u7ffb\u8bd1\u4e2d\u8bed\u4e49\u6a21\u7cca\u3001\u8bed\u97f3\u76f8\u4f3c\u6027\u548c\u6587\u5316\u5dee\u5f02\u7684\u6311\u6218\uff0c\u586b\u8865\u7ffb\u8bd1\u7814\u7a76\u4e0e\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\u96c6\u7684\u5927\u6a21\u578b\u57fa\u7ebf\uff1b2) \u7ed3\u5408\u8bed\u97f3-\u8bed\u4e49\u5d4c\u5165\u7684\u5f15\u5bfc\u601d\u7ef4\u94fe\uff1b3) \u751f\u6210-\u5224\u522b\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "result": "\u5728CLEF JOKER 2025\u7ade\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u548c\u7b2c\u4e8c\u540d\uff0c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u901a\u8fc7\u8bed\u8a00\u5b66\u6280\u672f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5904\u7406\u53cc\u5173\u8bed\u7684\u80fd\u529b\uff0c\u4e3a\u5e7d\u9ed8\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06528", "pdf": "https://arxiv.org/pdf/2507.06528", "abs": "https://arxiv.org/abs/2507.06528", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInvestAlign\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u89e3\u51b3\u65b9\u6848\u6784\u5efa\u9ad8\u8d28\u91cfSFT\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u884c\u4e3a\u91d1\u878d\u4e2dLLM\u4e0e\u6295\u8d44\u8005\u51b3\u7b56\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u907f\u514d\u771f\u5b9e\u7528\u6237\u6570\u636e\u7684\u9ad8\u6210\u672c\u548c\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u884c\u4e3a\u91d1\u878d\u4e2d\uff0cLLM\u4e0e\u6295\u8d44\u8005\u51b3\u7b56\u5bf9\u9f50\u9700\u5927\u91cf\u771f\u5b9e\u7528\u6237\u6570\u636e\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u3001\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u98ce\u9669\u5927\u3002", "method": "\u63d0\u51faInvestAlign\u6846\u67b6\uff0c\u5229\u7528\u7406\u8bba\u89e3\u51b3\u65b9\u6848\u751f\u6210\u9ad8\u8d28\u91cfSFT\u6570\u636e\u96c6\uff0c\u66ff\u4ee3\u590d\u6742\u573a\u666f\u7684\u771f\u5b9e\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cInvestAlign\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3LLM\u53c2\u6570\u6536\u655b\u66f4\u5feb\uff0c\u4e14InvestAgent\u6a21\u578b\u66f4\u8d34\u8fd1\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u3002", "conclusion": "InvestAlign\u4e3a\u590d\u6742\u6295\u8d44\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5b9e\u73b0LLM\u4e0e\u6295\u8d44\u8005\u51b3\u7b56\u884c\u4e3a\u7684\u5bf9\u9f50\u3002"}}
{"id": "2507.06533", "pdf": "https://arxiv.org/pdf/2507.06533", "abs": "https://arxiv.org/abs/2507.06533", "authors": ["Themistoklis Vargiemezis", "Catherine Gorl\u00e9"], "title": "From large-eddy simulations to deep learning: A U-net model for fast urban canopy flow predictions", "categories": ["physics.comp-ph", "cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Accurate prediction of wind flow fields in urban canopies is crucial for\nensuring pedestrian comfort, safety, and sustainable urban design. Traditional\nmethods using wind tunnels and Computational Fluid Dynamics, such as Large-Eddy\nSimulations (LES), are limited by high costs, computational demands, and time\nrequirements. This study presents a deep neural network (DNN) approach for fast\nand accurate predictions of urban wind flow fields, reducing computation time\nfrom an order of 10 hours on 32 CPUs for one LES evaluation to an order of 1\nsecond on a single GPU using the DNN model. We employ a U-Net architecture\ntrained on LES data including 252 synthetic urban configurations at seven wind\ndirections ($0^{o}$ to $90^{o}$ in $15^{o}$ increments). The model predicts two\nkey quantities of interest: mean velocity magnitude and streamwise turbulence\nintensity, at multiple heights within the urban canopy. The U-net uses 2D\nbuilding representations augmented with signed distance functions and their\ngradients as inputs, forming a $256\\times256\\times9$ tensor. In addition, a\nSpatial Attention Module is used for feature transfer through skip connections.\nThe loss function combines the root-mean-square error of predictions, their\ngradient magnitudes, and L2 regularization. Model evaluation on 50 test cases\ndemonstrates high accuracy with an overall mean relative error of 9.3% for\nvelocity magnitude and 5.2% for turbulence intensity. This research shows the\npotential of deep learning approaches to provide fast, accurate urban wind\nassessments essential for creating comfortable and safe urban environments.\nCode is available at https://github.com/tvarg/Urban-FlowUnet.git", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5feb\u901f\u9884\u6d4b\u57ce\u5e02\u98ce\u573a\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u98ce\u6d1e\u548c\u5927\u6da1\u6a21\u62df\uff09\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u98ce\u573a\u9884\u6d4b\u65b9\u6cd5\u4ee5\u652f\u6301\u57ce\u5e02\u8bbe\u8ba1\u548c\u884c\u4eba\u5b89\u5168\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\uff0c\u8f93\u5165\u4e3a2D\u5efa\u7b51\u8868\u793a\u548c\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff0c\u7ed3\u5408\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u8bad\u7ec3\u57fa\u4e8e\u5927\u6da1\u6a21\u62df\u6570\u636e\u7684\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4e3a9.3%\uff08\u901f\u5ea6\u5927\u5c0f\uff09\u548c5.2%\uff08\u6e4d\u6d41\u5f3a\u5ea6\uff09\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u9884\u6d4b\u57ce\u5e02\u98ce\u573a\uff0c\u4e3a\u57ce\u5e02\u73af\u5883\u8bbe\u8ba1\u63d0\u4f9b\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2507.06541", "pdf": "https://arxiv.org/pdf/2507.06541", "abs": "https://arxiv.org/abs/2507.06541", "authors": ["Ali Safarpoor Dehkordi", "Ahad N. Zehmakan"], "title": "Graph-based Fake Account Detection: A Survey", "categories": ["cs.SI", "cs.AI", "cs.LG", "A.1; I.2.6; I.5.1"], "comment": "16 Tables, 5 Figures, 41 Pages", "summary": "In recent years, there has been a growing effort to develop effective and\nefficient algorithms for fake account detection in online social networks. This\nsurvey comprehensively reviews existing methods, with a focus on graph-based\ntechniques that utilise topological features of social graphs (in addition to\naccount information, such as their shared contents and profile data) to\ndistinguish between fake and real accounts. We provide several categorisations\nof these methods (for example, based on techniques used, input data, and\ndetection time), discuss their strengths and limitations, and explain how these\nmethods connect in the broader context. We also investigate the available\ndatasets, including both real-world data and synthesised models. We conclude\nthe paper by proposing several potential avenues for future research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u793e\u4ea4\u7f51\u7edc\u4e2d\u865a\u5047\u8d26\u6237\u68c0\u6d4b\u7684\u56fe\u57fa\u65b9\u6cd5\uff0c\u5206\u7c7b\u8ba8\u8bba\u4e86\u73b0\u6709\u6280\u672f\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u865a\u5047\u8d26\u6237\u95ee\u9898\u7684\u65e5\u76ca\u4e25\u91cd\uff0c\u5f00\u53d1\u9ad8\u6548\u68c0\u6d4b\u7b97\u6cd5\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u57fa\u4e8e\u56fe\u62d3\u6251\u7279\u5f81\u5e76\u7ed3\u5408\u8d26\u6237\u4fe1\u606f\uff08\u5982\u5185\u5bb9\u548c\u8d44\u6599\u6570\u636e\uff09\uff0c\u5206\u7c7b\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u636e\u96c6\uff08\u771f\u5b9e\u4e0e\u5408\u6210\uff09\u7684\u53ef\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2507.06547", "pdf": "https://arxiv.org/pdf/2507.06547", "abs": "https://arxiv.org/abs/2507.06547", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Naoki Murata", "Wei-Hsiang Liao", "Woosung Choi", "Kin Wai Cheuk", "Junghyun Koo", "Yuki Mitsufuji"], "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "While diffusion models excel at image generation, their growing adoption\nraises critical concerns around copyright issues and model transparency.\nExisting attribution methods identify training examples influencing an entire\nimage, but fall short in isolating contributions to specific elements, such as\nstyles or objects, that matter most to stakeholders. To bridge this gap, we\nintroduce \\emph{concept-level attribution} via a novel method called\n\\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key\ninnovations: (1) a reformulated diffusion training loss based on diffusion\nposterior sampling, enabling robust, sample-specific attribution; and (2) a\nconcept-aware reward function that emphasizes semantic relevance. We evaluate\nConcept-TRAK on the AbC benchmark, showing substantial improvements over prior\nmethods. Through diverse case studies--ranging from identifying IP-protected\nand unsafe content to analyzing prompt engineering and compositional\nlearning--we demonstrate how concept-level attribution yields actionable\ninsights for responsible generative AI development and governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConcept-TRAK\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7248\u6743\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u6982\u5ff5\u7ea7\u5f52\u56e0\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u7248\u6743\u548c\u6a21\u578b\u900f\u660e\u5ea6\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u5f52\u56e0\u5230\u7279\u5b9a\u5143\u7d20\uff08\u5982\u98ce\u683c\u6216\u5bf9\u8c61\uff09\u3002", "method": "Concept-TRAK\u6269\u5c55\u4e86\u5f71\u54cd\u51fd\u6570\uff0c\u901a\u8fc7\u4e24\u79cd\u521b\u65b0\uff1a(1)\u57fa\u4e8e\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u8bad\u7ec3\u635f\u5931\uff1b(2)\u5f3a\u8c03\u8bed\u4e49\u76f8\u5173\u6027\u7684\u6982\u5ff5\u611f\u77e5\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728AbC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConcept-TRAK\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u7248\u6743\u4fdd\u62a4\u3001\u5185\u5bb9\u5b89\u5168\u548c\u63d0\u793a\u5de5\u7a0b\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u6982\u5ff5\u7ea7\u5f52\u56e0\u4e3a\u751f\u6210AI\u7684\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.06552", "pdf": "https://arxiv.org/pdf/2507.06552", "abs": "https://arxiv.org/abs/2507.06552", "authors": ["Zhiyi Dong", "Zixuan Liu", "Yongyi Mao"], "title": "On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": "Accepted at the 4th Conference on Lifelong Learning Agents (CoLLAs\n  2025)", "summary": "This paper studies the hardness of unsupervised domain adaptation (UDA) under\ncovariate shift. We model the uncertainty that the learner faces by a\ndistribution $\\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a\nUDA class -- where $(p, q)$ is the source -- target distribution pair and $f$\nis the classifier. We define the performance of a learner as the overall target\ndomain risk, averaged over the randomness of the ground-truth triple. This\nformulation couples the source distribution, the target distribution and the\nclassifier in the ground truth, and deviates from the classical worst-case\nanalyses, which pessimistically emphasize the impact of hard but rare UDA\ninstances. In this formulation, we precisely characterize the optimal learner.\nThe performance of the optimal learner then allows us to define the learning\ndifficulty for the UDA class and for the observed sample. To quantify this\ndifficulty, we introduce an information-theoretic quantity -- Posterior Target\nLabel Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the\nsample , which capture the uncertainty in the prediction for the target domain.\nBriefly, PTLU is the entropy of the predicted label in the target domain under\nthe posterior distribution of ground-truth classifier given the observed source\nand target samples. By proving that such a quantity serves to lower-bound the\nrisk of any learner, we suggest that these quantities can be used as proxies\nfor evaluating the hardness of UDA learning. We provide several examples to\ndemonstrate the advantage of PTLU, relative to the existing measures, in\nevaluating the difficulty of UDA learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u7684\u56f0\u96be\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u540e\u9a8c\u76ee\u6807\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff08PTLU\uff09\u53ca\u5176\u7ecf\u9a8c\u4f30\u8ba1\uff08EPTLU\uff09\u6765\u91cf\u5316\u5b66\u4e60\u96be\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u6700\u574f\u60c5\u51b5\u5206\u6790\u8fc7\u4e8e\u60b2\u89c2\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u6e90\u5206\u5e03\u3001\u76ee\u6807\u5206\u5e03\u548c\u5206\u7c7b\u5668\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30UDA\u7684\u56f0\u96be\u6027\u3002", "method": "\u5b9a\u4e49\u4e86UDA\u7c7b\uff0c\u5e76\u901a\u8fc7\u5e73\u5747\u76ee\u6807\u57df\u98ce\u9669\u6765\u8861\u91cf\u5b66\u4e60\u5668\u6027\u80fd\u3002\u5f15\u5165PTLU\u548cEPTLU\u4f5c\u4e3a\u4fe1\u606f\u8bba\u91cf\u6765\u91cf\u5316\u5b66\u4e60\u96be\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86PTLU\u53ef\u4ee5\u4e0b\u754c\u4efb\u4f55\u5b66\u4e60\u5668\u7684\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u5c55\u793a\u4e86PTLU\u5728\u8bc4\u4f30UDA\u5b66\u4e60\u96be\u5ea6\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "PTLU\u548cEPTLU\u662f\u8bc4\u4f30UDA\u5b66\u4e60\u96be\u5ea6\u7684\u6709\u6548\u4ee3\u7406\u6307\u6807\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.06560", "pdf": "https://arxiv.org/pdf/2507.06560", "abs": "https://arxiv.org/abs/2507.06560", "authors": ["Jae Hyoung Jeon", "Cheolsu Lim", "Myungjoo Kang"], "title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning", "categories": ["cs.CV", "cs.LG", "68T07, 62H12", "I.2.6; I.4.8; I.5.1"], "comment": "9 pages, 5 figures", "summary": "Recent success in contrastive learning has sparked growing interest in more\neffectively leveraging multiple augmented views of an instance. While prior\nmethods incorporate multiple views at the loss or feature level, they primarily\ncapture pairwise relationships and fail to model the joint structure across all\nviews. In this work, we propose a divergence-based similarity function (DSF)\nthat explicitly captures the joint structure by representing each set of\naugmented views as a distribution and measuring similarity as the divergence\nbetween distributions. Extensive experiments demonstrate that DSF consistently\nimproves performance across various tasks, including kNN classification and\nlinear evaluation, while also offering greater efficiency compared to other\nmulti-view methods. Furthermore, we establish a theoretical connection between\nDSF and cosine similarity, and show that, unlike cosine similarity, DSF\noperates effectively without requiring a temperature hyperparameter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u7684\u76f8\u4f3c\u6027\u51fd\u6570\uff08DSF\uff09\uff0c\u901a\u8fc7\u5c06\u591a\u89c6\u56fe\u8868\u793a\u4e3a\u5206\u5e03\u5e76\u6d4b\u91cf\u5206\u5e03\u95f4\u7684\u5dee\u5f02\u6765\u6355\u83b7\u8054\u5408\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u6210\u5bf9\u5173\u7cfb\uff0c\u672a\u80fd\u5efa\u6a21\u591a\u89c6\u56fe\u7684\u8054\u5408\u7ed3\u6784\u3002", "method": "\u63d0\u51faDSF\uff0c\u5c06\u591a\u89c6\u56fe\u8868\u793a\u4e3a\u5206\u5e03\u5e76\u6d4b\u91cf\u5206\u5e03\u95f4\u7684\u5dee\u5f02\u3002", "result": "DSF\u5728kNN\u5206\u7c7b\u548c\u7ebf\u6027\u8bc4\u4f30\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "DSF\u65e0\u9700\u6e29\u5ea6\u8d85\u53c2\u6570\u5373\u53ef\u6709\u6548\u5de5\u4f5c\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u4e0e\u4f59\u5f26\u76f8\u4f3c\u6027\u7684\u8054\u7cfb\u3002"}}
{"id": "2507.06565", "pdf": "https://arxiv.org/pdf/2507.06565", "abs": "https://arxiv.org/abs/2507.06565", "authors": ["Juan B. Guti\u00e9rrez"], "title": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production", "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "comment": "27 pages, 3 figures, 4 tables, 1 algorithm, 28 references", "summary": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c6\u4e3a\u5e73\u7b49\u8282\u70b9\u7684\u7f51\u7edc\u6a21\u578b\uff0c\u5206\u6790\u4e86\u56db\u79cd\u5371\u5bb3\uff08\u6f02\u79fb\u3001\u81ea\u6211\u4fee\u590d\u3001\u65b0\u7f16\u9020\u548c\u5916\u90e8\u68c0\u6d4b\uff09\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u548c\u5f00\u6e90\u7b97\u6cd5\uff08FOO\uff09\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u540c\u884c\u8bc4\u5ba1\u63d0\u9ad8\u7f51\u7edc\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u4eba\u7c7b\u4e0eLLMs\u4ea4\u4e92\u4e2d\u7684\u65b0\u5a92\u4ecb\uff0c\u5173\u6ce8\u5176\u53ef\u9760\u6027\u95ee\u9898\uff0c\u800c\u975e\u5b64\u7acb\u5730\u5206\u6790\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u79f0\u4e3a\u2018\u8bdd\u8bed\u7f51\u7edc\u2019\u7684\u6a21\u578b\uff0c\u5c06\u4eba\u7c7b\u548cLLMs\u89c6\u4e3a\u5e73\u7b49\u8282\u70b9\uff0c\u5e76\u5f00\u53d1\u4e86\u6570\u5b66\u6a21\u578b\u548c\u5f00\u6e90\u7b97\u6cd5\uff08FOO\uff09\u6765\u6a21\u62df\u548c\u4f18\u5316\u7f51\u7edc\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u4f9d\u8d56\u6f02\u79fb\u548c\u81ea\u6211\u4fee\u590d\u7684\u7f51\u7edc\u4f1a\u7a33\u5b9a\u5728\u8f83\u4f4e\u9519\u8bef\u7387\uff1b\u52a0\u5165\u7f16\u9020\u884c\u4e3a\u4f1a\u91cd\u73b0\u5f53\u524dLLMs\u7684\u9ad8\u9519\u8bef\u7387\uff1b\u800c\u5f15\u5165\u540c\u884c\u8bc4\u5ba1\uff08\u5982FOO\u7b97\u6cd5\uff09\u53ef\u5c06\u7cfb\u7edf\u63a8\u5411\u4ee5\u771f\u76f8\u4e3a\u4e3b\u7684\u72b6\u6001\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\uff0c\u65b0\u5a92\u4ecb\u7684\u53ef\u9760\u6027\u5e76\u975e\u6765\u81ea\u5355\u4e00\u6a21\u578b\u7684\u5b8c\u7f8e\uff0c\u800c\u662f\u901a\u8fc7\u5c06\u4e0d\u5b8c\u7f8e\u6a21\u578b\u8fde\u63a5\u6210\u4e92\u76f8\u76d1\u7763\u7684\u7f51\u7edc\u6765\u5b9e\u73b0\u3002"}}
{"id": "2507.06607", "pdf": "https://arxiv.org/pdf/2507.06607", "abs": "https://arxiv.org/abs/2507.06607", "authors": ["Liliang Ren", "Congcong Chen", "Haoran Xu", "Young Jin Kim", "Adam Atkinson", "Zheng Zhan", "Jiankai Sun", "Baolin Peng", "Liyuan Liu", "Shuohang Wang", "Hao Cheng", "Jianfeng Gao", "Weizhu Chen", "Yelong Shen"], "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGated Memory Unit (GMU)\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5728SSM\u5c42\u4e4b\u95f4\u9ad8\u6548\u5171\u4eab\u5185\u5b58\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86SambaY\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u7801\u6548\u7387\u548c\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22SSM\u5c42\u4e4b\u95f4\u8868\u793a\u5171\u4eab\u7684\u6548\u7387\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u5e8f\u5217\u5efa\u6a21\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165GMU\u673a\u5236\uff0c\u6784\u5efaSambaY\u67b6\u6784\uff0c\u7ed3\u5408Samba\u548cYOCO\u7684\u4f18\u52bf\uff0c\u5e76\u5728\u8de8\u89e3\u7801\u5668\u4e2d\u5171\u4eab\u5185\u5b58\u72b6\u6001\u3002", "result": "SambaY\u663e\u8457\u63d0\u5347\u4e86\u89e3\u7801\u6548\u7387\uff0c\u4fdd\u6301\u4e86\u7ebf\u6027\u9884\u586b\u5145\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GMU\u548cSambaY\u67b6\u6784\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u4f18\u8d8a\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.06625", "pdf": "https://arxiv.org/pdf/2507.06625", "abs": "https://arxiv.org/abs/2507.06625", "authors": ["Shizhe Cai", "Jayadeep Jacob", "Zeya Yin", "Fabio Ramos"], "title": "Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "9 pages, 10 figures", "summary": "Deep reinforcement learning has shown remarkable success in continuous\ncontrol tasks, yet often requires extensive training data, struggles with\ncomplex, long-horizon planning, and fails to maintain safety constraints during\noperation. Meanwhile, Model Predictive Control (MPC) offers explainability and\nconstraint satisfaction, but typically yields only locally optimal solutions\nand demands careful cost function design. This paper introduces the Q-guided\nSTein variational model predictive Actor-Critic (Q-STAC), a novel framework\nthat bridges these approaches by integrating Bayesian MPC with actor-critic\nreinforcement learning through constrained Stein Variational Gradient Descent\n(SVGD). Our method optimizes control sequences directly using learned Q-values\nas objectives, eliminating the need for explicit cost function design while\nleveraging known system dynamics to enhance sample efficiency and ensure\ncontrol signals remain within safe boundaries. Extensive experiments on 2D\nnavigation and robotic manipulation tasks demonstrate that Q-STAC achieves\nsuperior sample efficiency, robustness, and optimality compared to\nstate-of-the-art algorithms, while maintaining the high expressiveness of\npolicy distributions. Experiment videos are available on our website:\nhttps://sites.google.com/view/q-stac", "AI": {"tldr": "Q-STAC\u7ed3\u5408\u8d1d\u53f6\u65afMPC\u4e0eactor-critic\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7ea6\u675fStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u63a7\u5236\u5e8f\u5217\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u6570\u636e\u9700\u6c42\u5927\u3001\u957f\u65f6\u89c4\u5212\u96be\u53ca\u5b89\u5168\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865MPC\u4ec5\u5c40\u90e8\u6700\u4f18\u548c\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\u590d\u6742\u7684\u7f3a\u9677\u3002", "method": "\u6574\u5408\u8d1d\u53f6\u65afMPC\u4e0eactor-critic\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u7ea6\u675fSVGD\u4f18\u5316\u63a7\u5236\u5e8f\u5217\uff0c\u4ee5Q\u503c\u66ff\u4ee3\u663e\u5f0f\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\u3002", "result": "\u57282D\u5bfc\u822a\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cQ-STAC\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u6700\u4f18\u6027\u3002", "conclusion": "Q-STAC\u6210\u529f\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0eMPC\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u4e14\u6700\u4f18\u7684\u63a7\u5236\u3002"}}
{"id": "2507.06637", "pdf": "https://arxiv.org/pdf/2507.06637", "abs": "https://arxiv.org/abs/2507.06637", "authors": ["Pengcheng Zeng", "Siyuan Jiang"], "title": "Semi-parametric Functional Classification via Path Signatures Logistic Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose Path Signatures Logistic Regression (PSLR), a semi-parametric\nframework for classifying vector-valued functional data with scalar covariates.\nClassical functional logistic regression models rely on linear assumptions and\nfixed basis expansions, which limit flexibility and degrade performance under\nirregular sampling. PSLR overcomes these issues by leveraging truncated path\nsignatures to construct a finite-dimensional, basis-free representation that\ncaptures nonlinear and cross-channel dependencies. By embedding trajectories as\ntime-augmented paths, PSLR extracts stable, geometry-aware features that are\nrobust to sampling irregularity without requiring a common time grid, while\nstill preserving subject-specific timing patterns. We establish theoretical\nguarantees for the existence and consistent estimation of the optimal\ntruncation order, along with non-asymptotic risk bounds. Experiments on\nsynthetic and real-world datasets show that PSLR outperforms traditional\nfunctional classifiers in accuracy, robustness, and interpretability,\nparticularly under non-uniform sampling schemes. Our results highlight the\npractical and theoretical benefits of integrating rough path theory into modern\nfunctional data analysis.", "AI": {"tldr": "PSLR\u662f\u4e00\u79cd\u534a\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u5e26\u6709\u6807\u91cf\u534f\u53d8\u91cf\u7684\u5411\u91cf\u503c\u51fd\u6570\u6570\u636e\uff0c\u901a\u8fc7\u8def\u5f84\u7b7e\u540d\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u51fd\u6570\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4f9d\u8d56\u7ebf\u6027\u5047\u8bbe\u548c\u56fa\u5b9a\u57fa\u5c55\u5f00\uff0c\u7075\u6d3b\u6027\u4e0d\u8db3\u4e14\u5728\u4e0d\u89c4\u5219\u91c7\u6837\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528\u622a\u65ad\u8def\u5f84\u7b7e\u540d\u6784\u5efa\u6709\u9650\u7ef4\u3001\u65e0\u57fa\u8868\u793a\uff0c\u6355\u6349\u975e\u7ebf\u6027\u548c\u8de8\u901a\u9053\u4f9d\u8d56\uff0c\u5d4c\u5165\u65f6\u95f4\u589e\u5f3a\u8def\u5f84\u63d0\u53d6\u7a33\u5b9a\u7279\u5f81\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cPSLR\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5206\u7c7b\u5668\uff0c\u5c24\u5176\u5728\u4e0d\u5747\u5300\u91c7\u6837\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "PSLR\u5c06\u7c97\u7cd9\u8def\u5f84\u7406\u8bba\u878d\u5165\u73b0\u4ee3\u51fd\u6570\u6570\u636e\u5206\u6790\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002"}}
{"id": "2507.06639", "pdf": "https://arxiv.org/pdf/2507.06639", "abs": "https://arxiv.org/abs/2507.06639", "authors": ["Myungjang Pyeon", "Janghyeon Lee", "Minsoo Lee", "Juseung Yun", "Hwanil Choi", "Jonghyun Kim", "Jiwon Kim", "Yi Hu", "Jongseong Jang", "Soonyoung Lee"], "title": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "EXAONE Path 2.0 technical report", "summary": "In digital pathology, whole-slide images (WSIs) are often difficult to handle\ndue to their gigapixel scale, so most approaches train patch encoders via\nself-supervised learning (SSL) and then aggregate the patch-level embeddings\nvia multiple instance learning (MIL) or slide encoders for downstream tasks.\nHowever, patch-level SSL may overlook complex domain-specific features that are\nessential for biomarker prediction, such as mutation status and molecular\ncharacteristics, as SSL methods rely only on basic augmentations selected for\nnatural image domains on small patch-level area. Moreover, SSL methods remain\nless data efficient than fully supervised approaches, requiring extensive\ncomputational resources and datasets to achieve competitive performance. To\naddress these limitations, we present EXAONE Path 2.0, a pathology foundation\nmodel that learns patch-level representations under direct slide-level\nsupervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves\nstate-of-the-art average performance across 10 biomarker prediction tasks,\ndemonstrating remarkable data efficiency.", "AI": {"tldr": "EXAONE Path 2.0\u901a\u8fc7\u76f4\u63a5\u4f7f\u7528\u5e7b\u706f\u7247\u7ea7\u76d1\u7763\u5b66\u4e60\u8865\u4e01\u7ea7\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u8865\u4e01\u7ea7\u522b\u8bad\u7ec3\u65f6\u53ef\u80fd\u5ffd\u7565\u590d\u6742\u7684\u9886\u57df\u7279\u5b9a\u7279\u5f81\uff0c\u4e14\u6570\u636e\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u63d0\u51faEXAONE Path 2.0\uff0c\u4e00\u79cd\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u5e7b\u706f\u7247\u7ea7\u76d1\u7763\u5b66\u4e60\u8865\u4e01\u7ea7\u8868\u793a\u3002", "result": "\u4ec5\u4f7f\u752837k\u5f20\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u8bad\u7ec3\uff0cEXAONE Path 2.0\u572810\u4e2a\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "EXAONE Path 2.0\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06643", "pdf": "https://arxiv.org/pdf/2507.06643", "abs": "https://arxiv.org/abs/2507.06643", "authors": ["Farahdiba Zarin", "Riccardo Oliva", "Vinkle Srivastav", "Armine Vardazaryan", "Andrea Rosati", "Alice Zampolini Faustini", "Giovanni Scambia", "Anna Fagotti", "Pietro Mascagni", "Nicolas Padoy"], "title": "Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning from sparse labels is a challenge commonplace in the medical domain.\nThis is due to numerous factors, such as annotation cost, and is especially\ntrue for newly introduced tasks. When dense pixel-level annotations are needed,\nthis becomes even more unfeasible. However, being able to learn from just a few\nannotations at the pixel-level, while extremely difficult and underutilized,\ncan drive progress in studies where perfect annotations are not immediately\navailable. This work tackles the challenge of learning the dense prediction\ntask of keypoint localization from a few point annotations in the context of 2d\ncarcinosis keypoint localization from laparoscopic video frames for diagnostic\nplanning of advanced ovarian cancer patients. To enable this, we formulate the\nproblem as a sparse heatmap regression from a few point annotations per image\nand propose a new loss function, called Crag and Tail loss, for efficient\nlearning. Our proposed loss function effectively leverages positive sparse\nlabels while minimizing the impact of false negatives or missed annotations.\nThrough an extensive ablation study, we demonstrate the effectiveness of our\napproach in achieving accurate dense localization of carcinosis keypoints,\nhighlighting its potential to advance research in scenarios where dense\nannotations are challenging to obtain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7a00\u758f\u6807\u6ce8\u4e2d\u5b66\u4e60\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08Crag and Tail loss\uff09\u57282D\u8179\u8154\u955c\u89c6\u9891\u5e27\u4e2d\u5b9a\u4f4d\u764c\u53d8\u5173\u952e\u70b9\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u9886\u57df\u4e2d\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u4e2d\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u5bc6\u96c6\u6807\u6ce8\u4e0d\u73b0\u5b9e\uff0c\u5c24\u5176\u662f\u5728\u65b0\u4efb\u52a1\u4e2d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c11\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u5b66\u4e60\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u63a8\u52a8\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7a00\u758f\u70ed\u56fe\u56de\u5f52\uff0c\u63d0\u51faCrag and Tail\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u5229\u7528\u7a00\u758f\u6807\u6ce8\u5e76\u51cf\u5c11\u5047\u9634\u6027\u6807\u6ce8\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u5b9e\u73b0\u764c\u53d8\u5173\u952e\u70b9\u7684\u5bc6\u96c6\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bc6\u96c6\u6807\u6ce8\u96be\u4ee5\u83b7\u53d6\u7684\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2507.06656", "pdf": "https://arxiv.org/pdf/2507.06656", "abs": "https://arxiv.org/abs/2507.06656", "authors": ["Hongjie Wu", "Mingqin Zhang", "Linchao He", "Ji-Zhe Zhou", "Jiancheng Lv"], "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient Management", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ACM Multimedia 2025. Preprint version", "summary": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68af\u5ea6\u7ba1\u7406\u6280\u672fSPGD\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u548c\u4f3c\u7136\u68af\u5ea6\u65b9\u5411\u51b2\u7a81\u53ca\u68af\u5ea6\u6ce2\u52a8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5148\u9a8c\u548c\u4f3c\u7136\u68af\u5ea6\u65b9\u5411\u7684\u51b2\u7a81\u53ca\u68af\u5ea6\u6ce2\u52a8\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5f71\u54cd\u4e86\u6062\u590d\u6027\u80fd\u3002", "method": "\u63d0\u51faSPGD\u6280\u672f\uff0c\u5305\u542b\u6e10\u8fdb\u5f0f\u4f3c\u7136\u9884\u70ed\u7b56\u7565\u548c\u81ea\u9002\u5e94\u65b9\u5411\u52a8\u91cf\u5e73\u6ed1\uff0c\u4ee5\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\u548c\u6ce2\u52a8\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPGD\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7a33\u5b9a\u6027\uff0c\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u6548\u679c\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "SPGD\u901a\u8fc7\u68af\u5ea6\u7ba1\u7406\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06677", "pdf": "https://arxiv.org/pdf/2507.06677", "abs": "https://arxiv.org/abs/2507.06677", "authors": ["Chao Zhang", "Jasper M. Everink", "Jakob Sauer J\u00f8rgensen"], "title": "Fast Gaussian Processes under Monotonicity Constraints", "categories": ["stat.ML", "cs.LG", "stat.ME", "60G15"], "comment": "35 pages, 10 figures", "summary": "Gaussian processes (GPs) are widely used as surrogate models for complicated\nfunctions in scientific and engineering applications. In many cases, prior\nknowledge about the function to be approximated, such as monotonicity, is\navailable and can be leveraged to improve model fidelity. Incorporating such\nconstraints into GP models enhances predictive accuracy and reduces\nuncertainty, but remains a computationally challenging task for\nhigh-dimensional problems. In this work, we present a novel virtual point-based\nframework for building constrained GP models under monotonicity constraints,\nbased on regularized linear randomize-then-optimize (RLRTO), which enables\nefficient sampling from a constrained posterior distribution by means of\nsolving randomized optimization problems. We also enhance two existing virtual\npoint-based approaches by replacing Gibbs sampling with the No U-Turn Sampler\n(NUTS) for improved efficiency. A Python implementation of these methods is\nprovided and can be easily applied to a wide range of problems. This\nimplementation is then used to validate the approaches on approximating a range\nof synthetic functions, demonstrating comparable predictive performance between\nall considered methods and significant improvements in computational efficiency\nwith the two NUTS methods and especially with the RLRTO method. The framework\nis further applied to construct surrogate models for systems of differential\nequations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u70b9\u7684\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5355\u8c03\u6027\u7ea6\u675f\uff0c\u901a\u8fc7RLRTO\u65b9\u6cd5\u9ad8\u6548\u91c7\u6837\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\uff08\u5982\u5355\u8c03\u6027\uff09\u6539\u8fdbGP\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u9ad8\u7ef4\u95ee\u9898\u8ba1\u7b97\u590d\u6742\u3002", "method": "\u91c7\u7528RLRTO\u65b9\u6cd5\u8fdb\u884c\u7ea6\u675f\u540e\u9a8c\u5206\u5e03\u7684\u9ad8\u6548\u91c7\u6837\uff0c\u5e76\u7528NUTS\u66ff\u4ee3Gibbs\u91c7\u6837\u6539\u8fdb\u73b0\u6709\u865a\u62df\u70b9\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u51fd\u6570\u548c\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u6240\u6709\u65b9\u6cd5\u9884\u6d4b\u6027\u80fd\u76f8\u5f53\uff0c\u4f46RLRTO\u548cNUTS\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u9ad8\u6548\u6784\u5efa\u7ea6\u675fGP\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9ad8\u7ef4\u573a\u666f\u3002"}}
{"id": "2507.06722", "pdf": "https://arxiv.org/pdf/2507.06722", "abs": "https://arxiv.org/abs/2507.06722", "authors": ["Sunwoo Kim", "Haneul Yoo", "Alice Oh"], "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to Actionable Interpretability Workshop - ICML 2025", "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c42\u95f4\u6982\u7387\u8f68\u8ff9\u7684\u52a8\u6001\u53d8\u5316\uff0c\u53d1\u73b0\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u63a8\u7406\u52a8\u6001\u57fa\u672c\u4e00\u81f4\uff0c\u6311\u6218\u4e86\u5229\u7528\u7b80\u5355\u65b9\u6cd5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5185\u90e8\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u9632\u6b62\u5e7b\u89c9\u3002", "method": "\u4f7f\u7528Tuned Lens\uff08Logit Lens\u7684\u53d8\u4f53\uff09\u5206\u679011\u4e2a\u6570\u636e\u96c6\u548c5\u4e2a\u6a21\u578b\u7684\u5c42\u95f4\u6982\u7387\u8f68\u8ff9\uff0c\u4ee5\u9519\u8bef\u9884\u6d4b\u4f5c\u4e3a\u9ad8\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u4ee3\u8868\u3002", "result": "\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u8f68\u8ff9\u57fa\u672c\u4e00\u81f4\uff0c\u5747\u5728\u76f8\u4f3c\u5c42\u51fa\u73b0\u7f6e\u4fe1\u5ea6\u7a81\u589e\uff1b\u66f4\u4f18\u79c0\u7684\u6a21\u578b\u53ef\u80fd\u5b66\u4f1a\u4e0d\u540c\u65b9\u5f0f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5355\u65b9\u6cd5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u4e0d\u53ef\u884c\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u7814\u7a76\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.06735", "pdf": "https://arxiv.org/pdf/2507.06735", "abs": "https://arxiv.org/abs/2507.06735", "authors": ["Guan Zheng", "Xue Wang", "Wenhua Qian", "Peng Liu", "Runzhuo Ma"], "title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.", "AI": {"tldr": "RPFNet\u901a\u8fc7\u6b8b\u5dee\u5148\u9a8c\u548c\u9891\u57df\u878d\u5408\u6a21\u5757\uff0c\u9ad8\u6548\u6574\u5408\u591a\u6a21\u6001\u56fe\u50cf\u4fe1\u606f\uff0c\u63d0\u5347\u878d\u5408\u8d28\u91cf\u548c\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u957f\u8ddd\u79bb\u7279\u5f81\u4f9d\u8d56\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6846\u67b6\uff08RPM\u548cFDFM\uff09\u63d0\u53d6\u6b8b\u5dee\u5148\u9a8c\u548c\u9891\u57df\u7279\u5f81\uff0c\u7ed3\u5408CPM\u6a21\u5757\u589e\u5f3a\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRPFNet\u80fd\u6709\u6548\u6574\u5408\u7279\u5f81\u3001\u589e\u5f3a\u7ec6\u8282\u548c\u663e\u8457\u5bf9\u8c61\uff0c\u63d0\u5347\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "RPFNet\u5728\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.06744", "pdf": "https://arxiv.org/pdf/2507.06744", "abs": "https://arxiv.org/abs/2507.06744", "authors": ["Yafei Zhang", "Yongle Shang", "Huafeng Li"], "title": "Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c40\u90e8\u548c\u5168\u5c40\u53cc\u7c92\u5ea6\u8eab\u4efd\u5173\u8054\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u6587\u672c\u5230\u4eba\u7269\u56fe\u50cf\u5339\u914d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u4e00\u5bf9\u591a\u8eab\u4efd\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u548c\u5168\u5c40\u53cc\u7c92\u5ea6\u8eab\u4efd\u5173\u8054\u673a\u5236\uff0c\u5c40\u90e8\u5c42\u9762\u663e\u5f0f\u5efa\u7acb\u8de8\u6a21\u6001\u8eab\u4efd\u5173\u7cfb\uff0c\u5168\u5c40\u5c42\u9762\u6784\u5efa\u52a8\u6001\u8de8\u6a21\u6001\u8eab\u4efd\u5173\u8054\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6837\u672c\u5bf9\u6784\u5efa\u548c\u4e00\u81f4\u6027\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5339\u914d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u4eba\u7269\u56fe\u50cf\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06764", "pdf": "https://arxiv.org/pdf/2507.06764", "abs": "https://arxiv.org/abs/2507.06764", "authors": ["Guixian Xu", "Jinglai Li", "Junqi Tang"], "title": "Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers", "categories": ["eess.IV", "cs.CV", "cs.LG", "math.OC"], "comment": null, "summary": "We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning\nframework to efficiently train deep imaging networks without ground-truth data.\nFrom the perspective of reformulating the Equivariant Imaging based\noptimization problem via the method of Lagrange multipliers and utilizing\nplug-and-play denoisers, this novel unsupervised scheme shows superior\nefficiency and performance compared to vanilla Equivariant Imaging paradigm. In\nparticular, our PnP-FEI scheme achieves an order-of-magnitude (10x)\nacceleration over standard EI on training U-Net with CT100 dataset for X-ray CT\nreconstruction, with improved generalization performance.", "AI": {"tldr": "\u63d0\u51faFast Equivariant Imaging (FEI)\uff0c\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\u5373\u53ef\u9ad8\u6548\u8bad\u7ec3\u6df1\u5ea6\u6210\u50cf\u7f51\u7edc\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfEquivariant Imaging (EI) \u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u4e58\u6570\u6cd5\u91cd\u65b0\u6784\u5efa\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u5373\u63d2\u5373\u7528\u53bb\u566a\u5668\uff08PnP\uff09\u3002", "result": "PnP-FEI\u65b9\u6848\u5728CT100\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3U-Net\u65f6\uff0c\u6bd4\u6807\u51c6EI\u5feb10\u500d\uff0c\u4e14\u6cdb\u5316\u6027\u80fd\u66f4\u597d\u3002", "conclusion": "FEI\u6846\u67b6\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.06779", "pdf": "https://arxiv.org/pdf/2507.06779", "abs": "https://arxiv.org/abs/2507.06779", "authors": ["Martin Wimpff", "Jan Zerfowski", "Bin Yang"], "title": "Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Despite the growing success of deep learning (DL) in offline brain-computer\ninterfaces (BCIs), its adoption in real-time applications remains limited due\nto three primary challenges. First, most DL solutions are designed for offline\ndecoding, making the transition to online decoding unclear. Second, the use of\nsliding windows in online decoding substantially increases computational\ncomplexity. Third, DL models typically require large amounts of training data,\nwhich are often scarce in BCI applications. To address these challenges and\nenable real-time, cross-subject decoding without subject-specific calibration,\nwe introduce realtime adaptive pooling (RAP), a novel parameter-free method.\nRAP seamlessly modifies the pooling layers of existing offline DL models to\nmeet online decoding requirements. It also reduces computational complexity\nduring training by jointly decoding consecutive sliding windows. To further\nalleviate data requirements, our method leverages source-free domain\nadaptation, enabling privacy-preserving adaptation across varying amounts of\ntarget data. Our results demonstrate that RAP provides a robust and efficient\nframework for real-time BCI applications. It preserves privacy, reduces\ncalibration demands, and supports co-adaptive BCI systems, paving the way for\nbroader adoption of DL in online BCIs. These findings lay a strong foundation\nfor developing user-centered, high-performance BCIs that facilitate immediate\nfeedback and user learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAP\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u8111\u673a\u63a5\u53e3\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u8f6c\u6362\u3001\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u79bb\u7ebf\u8111\u673a\u63a5\u53e3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u79bb\u7ebf\u6a21\u578b\u7684\u5728\u7ebf\u8f6c\u6362\u3001\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u5f15\u5165RAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4fee\u6539\u73b0\u6709\u79bb\u7ebf\u6a21\u578b\u7684\u6c60\u5316\u5c42\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u5229\u7528\u6e90\u81ea\u7531\u57df\u9002\u5e94\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "result": "RAP\u5728\u5b9e\u65f6\u8111\u673a\u63a5\u53e3\u4e2d\u8868\u73b0\u7a33\u5065\u9ad8\u6548\uff0c\u4fdd\u62a4\u9690\u79c1\u3001\u51cf\u5c11\u6821\u51c6\u9700\u6c42\uff0c\u652f\u6301\u534f\u540c\u81ea\u9002\u5e94\u7cfb\u7edf\u3002", "conclusion": "RAP\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u5728\u7ebf\u8111\u673a\u63a5\u53e3\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u7528\u6237\u4e2d\u5fc3\u5316\u9ad8\u6027\u80fd\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.06782", "pdf": "https://arxiv.org/pdf/2507.06782", "abs": "https://arxiv.org/abs/2507.06782", "authors": ["SeungYoon Han", "Taeho Hwang", "Sukmin Cho", "Soyeong Jeong", "Hoyun Song", "Huije Lee", "Jong C. Park"], "title": "Temporal Information Retrieval via Time-Specifier Model Merging", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTSM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u9488\u5bf9\u4e0d\u540c\u65f6\u95f4\u6307\u793a\u7b26\u7684\u4e13\u7528\u68c0\u7d22\u5668\uff0c\u63d0\u5347\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f20\u7edf\u65f6\u95f4\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5f71\u54cd\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u6027\u80fd\u3002", "method": "TSM\u8bad\u7ec3\u9488\u5bf9\u5355\u4e2a\u65f6\u95f4\u6307\u793a\u7b26\u7684\u4e13\u7528\u68c0\u7d22\u5668\uff0c\u5e76\u5c06\u5b83\u4eec\u5408\u5e76\u4e3a\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTSM\u5728\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "TSM\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u65f6\u95f4\u7ea6\u675f\u548c\u975e\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2507.06795", "pdf": "https://arxiv.org/pdf/2507.06795", "abs": "https://arxiv.org/abs/2507.06795", "authors": ["Seonwu Kim", "Yohan Na", "Kihun Kim", "Hanhee Cho", "Geun Lim", "Mintae Kim", "Seongik Park", "Ki Hyun Kim", "Youngsub Han", "Byoung-Ki Jeon"], "title": "Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8eDACP\u7684\u65b9\u6cd5\u5728\u5c0f\u578bLLMs\u4e0a\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u9886\u57df\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\uff0c\u4e3a\u4f01\u4e1a\u90e8\u7f72\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u4e3a\u4f01\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u8bb8\u591a\u7ec4\u7ec7\u7f3a\u4e4f\u90e8\u7f72\u548c\u7ef4\u62a4\u5927\u89c4\u6a21\u6a21\u578b\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u56e0\u6b64\u5c0f\u578bLLMs\uff08sLLMs\uff09\u6210\u4e3a\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\uff08DACP\uff09\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u5176\u5728\u591a\u79cd\u57fa\u7840\u6a21\u578b\u548c\u670d\u52a1\u9886\u57df\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u5b9e\u9645\u8bc4\u4f30\uff0cDACP\u5e94\u7528\u7684\u5c0f\u578bLLMs\u5728\u76ee\u6807\u9886\u57df\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u7559\u901a\u7528\u80fd\u529b\u3002", "conclusion": "DACP\u4e3a\u5c0f\u578bLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4f01\u4e1a\u7ea7\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06817", "pdf": "https://arxiv.org/pdf/2507.06817", "abs": "https://arxiv.org/abs/2507.06817", "authors": ["Ayoub Farkane", "Mohamed Boutayeb", "Mustapha Oudani", "Mounir Ghogho"], "title": "Designing Robust Software Sensors for Nonlinear Systems via Neural Networks and Adaptive Sliding Mode Control", "categories": ["math.DS", "cs.LG", "cs.NE", "math.OC"], "comment": "Submitted to IEEE Transactions Journal", "summary": "Accurate knowledge of the state variables in a dynamical system is critical\nfor effective control, diagnosis, and supervision, especially when direct\nmeasurements of all states are infeasible. This paper presents a novel approach\nto designing software sensors for nonlinear dynamical systems expressed in\ntheir most general form. Unlike traditional model-based observers that rely on\nexplicit transformations or linearization, the proposed framework integrates\nneural networks with adaptive Sliding Mode Control (SMC) to design a robust\nstate observer under a less restrictive set of conditions. The learning process\nis driven by available sensor measurements, which are used to correct the\nobserver's state estimate. The training methodology leverages the system's\ngoverning equations as a physics-based constraint, enabling observer synthesis\nwithout access to ground-truth state trajectories. By employing a time-varying\ngain matrix dynamically adjusted by the neural network, the observer adapts in\nreal-time to system changes, ensuring robustness against noise, external\ndisturbances, and variations in system dynamics. Furthermore, we provide\nsufficient conditions to guarantee estimation error convergence, establishing a\ntheoretical foundation for the observer's reliability. The methodology's\neffectiveness is validated through simulations on challenging examples,\nincluding systems with non-differentiable dynamics and varying observability\nconditions. These examples, which are often problematic for conventional\ntechniques, serve to demonstrate the robustness and broad applicability of our\napproach. The results show rapid convergence and high accuracy, underscoring\nthe method's potential for addressing complex state estimation challenges in\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e0e\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u7684\u9c81\u68d2\u72b6\u6001\u89c2\u6d4b\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f\u53d8\u6362\u6216\u7ebf\u6027\u5316\u3002", "motivation": "\u52a8\u6001\u7cfb\u7edf\u4e2d\u72b6\u6001\u53d8\u91cf\u7684\u51c6\u786e\u4f30\u8ba1\u5bf9\u63a7\u5236\u3001\u8bca\u65ad\u548c\u76d1\u63a7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u6d4b\u91cf\u6240\u6709\u72b6\u6001\u5f80\u5f80\u4e0d\u53ef\u884c\u3002", "method": "\u96c6\u6210\u795e\u7ecf\u7f51\u7edc\u4e0e\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\uff0c\u5229\u7528\u4f20\u611f\u5668\u6d4b\u91cf\u9a71\u52a8\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u52a8\u529b\u5b66\u65b9\u7a0b\u4f5c\u4e3a\u7269\u7406\u7ea6\u675f\u8fdb\u884c\u89c2\u6d4b\u5668\u8bbe\u8ba1\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u975e\u53ef\u5fae\u52a8\u6001\u548c\u53d8\u5316\u53ef\u89c2\u6d4b\u6027\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\uff0c\u8868\u73b0\u51fa\u5feb\u901f\u6536\u655b\u548c\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06844", "pdf": "https://arxiv.org/pdf/2507.06844", "abs": "https://arxiv.org/abs/2507.06844", "authors": ["Constantin Philippenko", "Batiste Le Bars", "Kevin Scaman", "Laurent Massouli\u00e9"], "title": "Adaptive collaboration for online personalized distributed learning with heterogeneous clients", "categories": ["stat.ML", "cs.LG"], "comment": "18 pages", "summary": "We study the problem of online personalized decentralized learning with $N$\nstatistically heterogeneous clients collaborating to accelerate local training.\nAn important challenge in this setting is to select relevant collaborators to\nreduce gradient variance while mitigating the introduced bias. To tackle this,\nwe introduce a gradient-based collaboration criterion, allowing each client to\ndynamically select peers with similar gradients during the optimization\nprocess. Our criterion is motivated by a refined and more general theoretical\nanalysis of the All-for-one algorithm, proved to be optimal in Even et al.\n(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds\nfor smooth objective functions, being either strongly convex, non-convex, or\nsatisfying the Polyak-Lojasiewicz condition; our analysis reveals that the\nalgorithm acts as a variance reduction method where the speed-up depends on a\nsufficient variance. We put forward two collaboration methods instantiating the\nproposed general schema; and we show that one variant preserves the optimality\nof All-for-one. We validate our results with experiments on synthetic and real\ndatasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u4e2a\u6027\u5316\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u534f\u4f5c\u51c6\u5219\uff0c\u52a8\u6001\u9009\u62e9\u76f8\u4f3c\u68af\u5ea6\u7684\u5ba2\u6237\u7aef\u4ee5\u51cf\u5c11\u68af\u5ea6\u65b9\u5dee\u5e76\u7f13\u89e3\u504f\u5dee\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u7edf\u8ba1\u5f02\u6784\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u534f\u4f5c\u4f19\u4f34\u4f18\u5316\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u5f15\u5165\u68af\u5ea6\u534f\u4f5c\u51c6\u5219\uff0c\u52a8\u6001\u9009\u62e9\u68af\u5ea6\u76f8\u4f3c\u7684\u5ba2\u6237\u7aef\uff0c\u63d0\u51fa\u4e24\u79cd\u534f\u4f5c\u65b9\u6cd5\u5e76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u7b97\u6cd5\u53ef\u4f5c\u4e3a\u65b9\u5dee\u51cf\u5c11\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u4f5c\u51c6\u5219\u548c\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5f02\u6784\u5ba2\u6237\u7aef\u573a\u666f\u3002"}}
{"id": "2507.06867", "pdf": "https://arxiv.org/pdf/2507.06867", "abs": "https://arxiv.org/abs/2507.06867", "authors": ["Tiffany Ding", "Jean-Baptiste Fermanian", "Joseph Salmon"], "title": "Conformal Prediction for Long-Tailed Classification", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we propose a conformal score function,\nprevalence-adjusted softmax, that targets a relaxed notion of class-conditional\ncoverage called macro-coverage. Second, we propose a label-weighted conformal\nprediction method that allows us to interpolate between marginal and\nclass-conditional conformal prediction. We demonstrate our methods on Pl@ntNet\nand iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,\nrespectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u957f\u5c3e\u5206\u5e03\u5206\u7c7b\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u9884\u6d4b\u96c6\u7684\u8986\u76d6\u7387\u548c\u5927\u5c0f\uff0c\u5e73\u8861\u7a00\u6709\u7c7b\u522b\u7684\u8986\u76d6\u548c\u9884\u6d4b\u96c6\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u5206\u7c7b\u95ee\u9898\u4e2d\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u9884\u6d4b\u96c6\u7684\u5c0f\u89c4\u6a21\u548c\u7a00\u6709\u7c7b\u522b\u7684\u8986\u76d6\u7387\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u5b8f\u8986\u76d6\u7684prevalence-adjusted softmax\u8bc4\u5206\u51fd\u6570\uff1b2\uff09\u6807\u7b7e\u52a0\u6743\u7684\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8fb9\u9645\u548c\u7c7b\u522b\u6761\u4ef6\u8986\u76d6\u4e4b\u95f4\u63d2\u503c\u3002", "result": "\u5728Pl@ntNet\u548ciNaturalist\u4e24\u4e2a\u957f\u5c3e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u8fb9\u9645\u8986\u76d6\u7684\u540c\u65f6\uff0c\u7075\u6d3b\u5e73\u8861\u9884\u6d4b\u96c6\u5927\u5c0f\u548c\u7c7b\u522b\u6761\u4ef6\u8986\u76d6\uff0c\u9002\u7528\u4e8e\u957f\u5c3e\u5206\u5e03\u5206\u7c7b\u95ee\u9898\u3002"}}
{"id": "2507.06895", "pdf": "https://arxiv.org/pdf/2507.06895", "abs": "https://arxiv.org/abs/2507.06895", "authors": ["Luca Mariotti", "Veronica Guidetti", "Federica Mandreoli"], "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.", "AI": {"tldr": "SCoRE\u662f\u4e00\u79cd\u6a21\u5757\u5316\u3001\u4f4e\u6210\u672c\u7684\u53e5\u5b50\u7ea7\u5173\u7cfb\u62bd\u53d6\u7cfb\u7edf\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u9002\u914d\u4e0d\u540c\u8bed\u6599\u5e93\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u80fd\u8017\u66f4\u4f4e\u3002", "motivation": "\u89e3\u51b3\u4f4e\u76d1\u7763\u73af\u5883\u4e0b\u5173\u7cfb\u62bd\u53d6\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u9002\u5e94\u6027\u5f3a\u4e14\u6297\u566a\u58f0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u8d1d\u53f6\u65afk\u8fd1\u90bb\u5206\u7c7b\u5668\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807CSD\u548cP@R\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "conclusion": "SCoRE\u51ed\u501f\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6210\u4e3a\u5b9e\u9645\u5e94\u7528\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2507.06921", "pdf": "https://arxiv.org/pdf/2507.06921", "abs": "https://arxiv.org/abs/2507.06921", "authors": ["Alokesh Manna", "Aditya Vikram Sett", "Dipak K. Dey", "Yuwen Gu", "Elizabeth D. Schifano", "Jichao He"], "title": "Distribution-free inference for LightGBM and GLM with Tweedie loss", "categories": ["stat.ML", "cs.LG", "Application to insurance data, Methodology"], "comment": null, "summary": "Prediction uncertainty quantification is a key research topic in recent years\nscientific and business problems. In insurance industries\n(\\cite{parodi2023pricing}), assessing the range of possible claim costs for\nindividual drivers improves premium pricing accuracy. It also enables insurers\nto manage risk more effectively by accounting for uncertainty in accident\nlikelihood and severity. In the presence of covariates, a variety of\nregression-type models are often used for modeling insurance claims, ranging\nfrom relatively simple generalized linear models (GLMs) to regularized GLMs to\ngradient boosting models (GBMs). Conformal predictive inference has arisen as a\npopular distribution-free approach for quantifying predictive uncertainty under\nrelatively weak assumptions of exchangeability, and has been well studied under\nthe classic linear regression setting. In this work, we propose new\nnon-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized\nTweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal\nprediction performance with these non-conformity measures in insurance claims\ndata. Our simulation results favor the use of locally weighted Pearson\nresiduals for LightGBM over other methods considered, as the resulting\nintervals maintained the nominal coverage with the smallest average width.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u4e00\u81f4\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8eGLMs\u548cGBMs\uff0c\u4ee5\u6539\u8fdb\u4fdd\u9669\u7d22\u8d54\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u4fdd\u9669\u884c\u4e1a\u9700\u8981\u66f4\u51c6\u786e\u5730\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u63d0\u9ad8\u4fdd\u8d39\u5b9a\u4ef7\u7684\u7cbe\u786e\u6027\u548c\u98ce\u9669\u7ba1\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u5316Tweedie GLM\u56de\u5f52\u548cLightGBM\uff08\u5e26Tweedie\u635f\u5931\uff09\u8fdb\u884c\u5171\u5f62\u9884\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u975e\u4e00\u81f4\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u5c40\u90e8\u52a0\u6743Pearson\u6b8b\u5dee\u7684LightGBM\u65b9\u6cd5\u5728\u4fdd\u6301\u540d\u4e49\u8986\u76d6\u8303\u56f4\u7684\u540c\u65f6\uff0c\u5e73\u5747\u533a\u95f4\u5bbd\u5ea6\u6700\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u9669\u7d22\u8d54\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5c40\u90e8\u52a0\u6743Pearson\u6b8b\u5dee\u7684LightGBM\u65b9\u6cd5\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2507.06929", "pdf": "https://arxiv.org/pdf/2507.06929", "abs": "https://arxiv.org/abs/2507.06929", "authors": ["Sita Sch\u00f6nbauer", "Johanna P. Carbone", "Andreas Gr\u00fcneis"], "title": "Machine-Learned Force Fields for Lattice Dynamics at Coupled-Cluster Level Accuracy", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "comment": "22 pages, 12 figures", "summary": "We investigate Machine-Learned Force Fields (MLFFs) trained on approximate\nDensity Functional Theory (DFT) and Coupled Cluster (CC) level potential energy\nsurfaces for the carbon diamond and lithium hydride solids. We assess the\naccuracy and precision of the MLFFs by calculating phonon dispersions and\nvibrational densities of states (VDOS) that are compared to experiment and\nreference ab initio results. To overcome limitations from long-range effects\nand the lack of atomic forces in the CC training data, a delta-learning\napproach based on the difference between CC and DFT results is explored.\nCompared to DFT, MLFFs trained on CC theory yield higher vibrational\nfrequencies for optical modes, agreeing better with experiment. Furthermore,\nthe MLFFs are used to estimate anharmonic effects on the VDOS of lithium\nhydride at the level of CC theory.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8fd1\u4f3c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u548c\u8026\u5408\u7c07\uff08CC\uff09\u52bf\u80fd\u9762\u7684\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08MLFFs\uff09\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u78b3\u91d1\u521a\u77f3\u548c\u6c22\u5316\u9502\u56fa\u4f53\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u58f0\u5b50\u8272\u6563\u548c\u632f\u52a8\u6001\u5bc6\u5ea6\uff08VDOS\uff09\u4e0e\u5b9e\u9a8c\u548c\u53c2\u8003\u7ed3\u679c\u5bf9\u6bd4\u3002", "motivation": "\u63a2\u8ba8MLFFs\u5728\u6a21\u62df\u56fa\u4f53\u6750\u6599\u632f\u52a8\u6027\u8d28\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u514b\u670dCC\u8bad\u7ec3\u6570\u636e\u4e2d\u957f\u7a0b\u6548\u5e94\u548c\u539f\u5b50\u529b\u7f3a\u5931\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8eCC\u548cDFT\u5dee\u5f02\u7684delta-learning\u65b9\u6cd5\uff0c\u8bad\u7ec3MLFFs\u5e76\u8ba1\u7b97\u58f0\u5b50\u8272\u6563\u548cVDOS\u3002", "result": "\u4e0eDFT\u76f8\u6bd4\uff0c\u57fa\u4e8eCC\u7684MLFFs\u5728\u5149\u5b66\u6a21\u5f0f\u632f\u52a8\u9891\u7387\u4e0a\u66f4\u63a5\u8fd1\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5e76\u7528\u4e8e\u4f30\u8ba1\u6c22\u5316\u9502\u7684VDOS\u975e\u8c10\u6548\u5e94\u3002", "conclusion": "\u57fa\u4e8eCC\u7684MLFFs\u5728\u6a21\u62df\u56fa\u4f53\u632f\u52a8\u6027\u8d28\u4e0a\u4f18\u4e8eDFT\uff0cdelta-learning\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2507.06961", "pdf": "https://arxiv.org/pdf/2507.06961", "abs": "https://arxiv.org/abs/2507.06961", "authors": ["Han Wang", "Yang Xu", "Wenbin Lu", "Rui Song"], "title": "Off-Policy Evaluation Under Nonignorable Missing Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Off-Policy Evaluation (OPE) aims to estimate the value of a target policy\nusing offline data collected from potentially different policies. In real-world\napplications, however, logged data often suffers from missingness. While OPE\nhas been extensively studied in the literature, a theoretical understanding of\nhow missing data affects OPE results remains unclear. In this paper, we\ninvestigate OPE in the presence of monotone missingness and theoretically\ndemonstrate that the value estimates remain unbiased under ignorable\nmissingness but can be biased under nonignorable (informative) missingness. To\nretain the consistency of value estimation, we propose an inverse probability\nweighted value estimator and conduct statistical inference to quantify the\nuncertainty of the estimates. Through a series of numerical experiments, we\nempirically demonstrate that our proposed estimator yields a more reliable\nvalue inference under missing data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5355\u8c03\u7f3a\u5931\u6570\u636e\u4e0b\u7684\u79bb\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\uff0c\u53d1\u73b0\u53ef\u5ffd\u7565\u7f3a\u5931\u65f6\u4f30\u8ba1\u65e0\u504f\uff0c\u4f46\u975e\u53ef\u5ffd\u7565\u7f3a\u5931\u65f6\u53ef\u80fd\u6709\u504f\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u79bb\u7ebf\u6570\u636e\u5e38\u5b58\u5728\u7f3a\u5931\uff0c\u4f46\u7f3a\u5931\u6570\u636e\u5982\u4f55\u5f71\u54cdOPE\u7684\u7406\u8bba\u7406\u89e3\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u9006\u6982\u7387\u52a0\u6743\u503c\u4f30\u8ba1\u5668\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u63a8\u65ad\u4ee5\u91cf\u5316\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u53ef\u5ffd\u7565\u7f3a\u5931\u65f6\u4f30\u8ba1\u65e0\u504f\uff0c\u975e\u53ef\u5ffd\u7565\u7f3a\u5931\u65f6\u6709\u504f\uff1b\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u4f30\u8ba1\u5668\u5728\u7f3a\u5931\u6570\u636e\u4e0b\u66f4\u53ef\u9760\u3002", "conclusion": "\u5728\u5355\u8c03\u7f3a\u5931\u6570\u636e\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u4fdd\u6301\u4f30\u8ba1\u4e00\u81f4\u6027\uff0c\u4e3aOPE\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06999", "pdf": "https://arxiv.org/pdf/2507.06999", "abs": "https://arxiv.org/abs/2507.06999", "authors": ["Yahan Yu", "Yuyang Dong", "Masafumi Oyamada"], "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD2I\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5219\u5956\u52b1\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6216\u590d\u6742\u5956\u52b1\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u5728\u6a21\u6001\u5bf9\u9f50\u548c\u8bad\u7ec3\u6210\u672c\u65b9\u9762\u4ecd\u9700\u63a2\u7d22\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6807\u6ce8\u548c\u590d\u6742\u5956\u52b1\uff0c\u589e\u52a0\u4e86\u6210\u672c\u5e76\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "D2I\u6846\u67b6\u5728\u8bad\u7ec3\u65f6\u901a\u8fc7\u89c4\u5219\u5956\u52b1\u8bbe\u7f6e\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u7b56\u7565\uff0c\u8bc4\u4f30\u65f6\u8f6c\u4e3a\u76f4\u89c9\u63a8\u7406\uff0c\u9690\u5f0f\u53cd\u6620\u6a21\u578b\u80fd\u529b\u3002", "result": "D2I\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "D2I\u5c55\u793a\u4e86\u89c4\u5219\u5956\u52b1\u5728\u63d0\u5347MLLMs\u53ef\u8fc1\u79fb\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u63a8\u7406\u6df1\u5ea6\u7684\u89e3\u8026\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.07006", "pdf": "https://arxiv.org/pdf/2507.07006", "abs": "https://arxiv.org/abs/2507.07006", "authors": ["S M Taslim Uddin Raju", "Md. Milon Islam", "Md Rezwanul Haque", "Hamdi Altaheri", "Fakhri Karray"], "title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis.", "AI": {"tldr": "GNN-ViTCap\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u805a\u7c7b\u548c\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5197\u4f59WSI\u8865\u4e01\uff0c\u7ed3\u5408GNN\u548cViT\u5b9e\u73b0\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4e0e\u63cf\u8ff0\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u4e2d\u5197\u4f59\u8865\u4e01\u548c\u672a\u77e5\u4f4d\u7f6e\u95ee\u9898\uff0c\u4ee5\u53ca\u81ea\u52a8\u751f\u6210\u75c5\u7406\u63cf\u8ff0\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528ViT\u63d0\u53d6\u8865\u4e01\u5d4c\u5165\uff0c\u52a8\u6001\u805a\u7c7b\u53bb\u5197\u4f59\uff0cGNN\u6355\u83b7\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63cf\u8ff0\u3002", "result": "\u5728BreakHis\u548cPatchGastric\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7bF1\u4e3a0.934\uff0cAUC\u4e3a0.963\uff1b\u63cf\u8ff0BLEU-4\u4e3a0.811\uff0cMETEOR\u4e3a0.569\u3002", "conclusion": "GNN-ViTCap\u4e3a\u75c5\u7406\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.07012", "pdf": "https://arxiv.org/pdf/2507.07012", "abs": "https://arxiv.org/abs/2507.07012", "authors": ["Chengyuan Zhang", "Zhengbing He", "Cathy Wu", "Lijun Sun"], "title": "When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior", "categories": ["stat.AP", "cs.LG", "cs.RO"], "comment": null, "summary": "Modeling car-following behavior is fundamental to microscopic traffic\nsimulation, yet traditional deterministic models often fail to capture the full\nextent of variability and unpredictability in human driving. While many modern\napproaches incorporate context-aware inputs (e.g., spacing, speed, relative\nspeed), they frequently overlook structured stochasticity that arises from\nlatent driver intentions, perception errors, and memory effects -- factors that\nare not directly observable from context alone. To fill the gap, this study\nintroduces an interpretable stochastic modeling framework that captures not\nonly context-dependent dynamics but also residual variability beyond what\ncontext can explain. Leveraging deep neural networks integrated with\nnonstationary Gaussian processes (GPs), our model employs a scenario-adaptive\nGibbs kernel to learn dynamic temporal correlations in acceleration decisions,\nwhere the strength and duration of correlations between acceleration decisions\nevolve with the driving context. This formulation enables a principled,\ndata-driven quantification of uncertainty in acceleration, speed, and spacing,\ngrounded in both observable context and latent behavioral variability.\nComprehensive experiments on the naturalistic vehicle trajectory dataset\ncollected from the German highway, i.e., the HighD dataset, demonstrate that\nthe proposed stochastic simulation method within this framework surpasses\nconventional methods in both predictive performance and interpretable\nuncertainty quantification. The integration of interpretability and accuracy\nmakes this framework a promising tool for traffic analysis and safety-critical\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u7684\u968f\u673a\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u6a21\u62df\u8ddf\u8f66\u884c\u4e3a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6f5c\u5728\u56e0\u7d20\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u6a21\u578b\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4eba\u7c7b\u9a7e\u9a76\u4e2d\u7684\u53d8\u5f02\u6027\uff0c\u800c\u73b0\u4ee3\u65b9\u6cd5\u5e38\u5ffd\u7565\u6f5c\u5728\u56e0\u7d20\uff08\u5982\u9a7e\u9a76\u5458\u610f\u56fe\u3001\u611f\u77e5\u8bef\u5dee\u7b49\uff09\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u7ed3\u5408\uff0c\u4f7f\u7528\u573a\u666f\u81ea\u9002\u5e94\u7684Gibbs\u6838\u5b66\u4e60\u52a0\u901f\u5ea6\u51b3\u7b56\u7684\u52a8\u6001\u65f6\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5728HighD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u517c\u5177\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4ea4\u901a\u5206\u6790\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2507.07015", "pdf": "https://arxiv.org/pdf/2507.07015", "abs": "https://arxiv.org/abs/2507.07015", "authors": ["Hui Li", "Pengfei Yang", "Juanyang Chen", "Le Dong", "Yanxin Chen", "Quan Wang"], "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025 (The 33rd ACM International Conference on\n  Multimedia)", "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.", "AI": {"tldr": "MST-Distill\u662f\u4e00\u79cd\u65b0\u578b\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6559\u5e08\u6a21\u578b\u548c\u52a8\u6001\u8def\u7531\u7f51\u7edc\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u573a\u666f\u4e2d\u56e0\u6570\u636e\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8de8\u6a21\u6001\u6559\u5e08\u6a21\u578b\u7684\u4e92\u8865\u77e5\u8bc6\uff0c\u5b58\u5728\u84b8\u998f\u8def\u5f84\u9009\u62e9\u548c\u77e5\u8bc6\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faMST-Distill\u6846\u67b6\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u548c\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u7528\u52a8\u6001\u8def\u7531\u7f51\u7edc\u81ea\u9002\u5e94\u9009\u62e9\u84b8\u998f\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u63a9\u7801\u6a21\u5757\u6291\u5236\u6a21\u6001\u5dee\u5f02\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMST-Distill\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "MST-Distill\u901a\u8fc7\u52a8\u6001\u6559\u5e08\u6a21\u578b\u548c\u63a9\u7801\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.07031", "pdf": "https://arxiv.org/pdf/2507.07031", "abs": "https://arxiv.org/abs/2507.07031", "authors": ["Bing-Jyue Chen", "Lilia Tang", "Daniel Kang"], "title": "ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation", "categories": ["cs.CR", "cs.LG"], "comment": "16 pages, 2 figures", "summary": "As AI models become ubiquitous in our daily lives, there has been an\nincreasing demand for transparency in ML services. However, the model owner\ndoes not want to reveal the weights, as they are considered trade secrets. To\nsolve this problem, researchers have turned to zero-knowledge proofs of ML\nmodel inference. These proofs convince the user that the ML model output is\ncorrect, without revealing the weights of the model to the user. Past work on\nthese provers can be placed into two categories. The first method compiles the\nML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The\nsecond method uses custom cryptographic protocols designed only for a specific\nclass of models. Unfortunately, the first method is highly inefficient, making\nit impractical for the large models used today, and the second method does not\ngeneralize well, making it difficult to update in the rapidly changing field of\nmachine learning. To solve this, we propose ZKTorch, an open source end-to-end\nproving system that compiles ML models into base cryptographic operations\ncalled basic blocks, each proved using specialized protocols. ZKTorch is built\non top of a novel parallel extension to the Mira accumulation scheme, enabling\nsuccinct proofs with minimal accumulation overhead. These contributions allow\nZKTorch to achieve at least a $3\\times$ reduction in the proof size compared to\nspecialized protocols and up to a $6\\times$ speedup in proving time over a\ngeneral-purpose ZKML framework.", "AI": {"tldr": "ZKTorch\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06ML\u6a21\u578b\u7f16\u8bd1\u4e3a\u57fa\u672c\u52a0\u5bc6\u64cd\u4f5c\u5757\uff0c\u5e76\u4f7f\u7528\u5e76\u884c\u6269\u5c55\u7684Mira\u7d2f\u79ef\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bc1\u660e\u5927\u5c0f\u548c\u9a8c\u8bc1\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u96f6\u77e5\u8bc6\u8bc1\u660e\u65b9\u6cd5\u5728ML\u6a21\u578b\u4e2d\u7684\u6548\u7387\u4f4e\u548c\u901a\u7528\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5c06ML\u6a21\u578b\u7f16\u8bd1\u4e3a\u57fa\u672c\u52a0\u5bc6\u64cd\u4f5c\u5757\uff0c\u4f7f\u7528\u5e76\u884c\u6269\u5c55\u7684Mira\u7d2f\u79ef\u65b9\u6848\u751f\u6210\u7b80\u6d01\u8bc1\u660e\u3002", "result": "\u8bc1\u660e\u5927\u5c0f\u51cf\u5c113\u500d\uff0c\u9a8c\u8bc1\u65f6\u95f4\u52a0\u5feb6\u500d\u3002", "conclusion": "ZKTorch\u4e3aML\u6a21\u578b\u7684\u96f6\u77e5\u8bc6\u8bc1\u660e\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07041", "pdf": "https://arxiv.org/pdf/2507.07041", "abs": "https://arxiv.org/abs/2507.07041", "authors": ["Enze Shi", "Jinhan Xie", "Bei Jiang", "Linglong Kong", "Xuming He"], "title": "Non-Asymptotic Analysis of Online Local Private Learning with SGD", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely\nused for solving optimization problems with privacy guarantees in machine\nlearning and statistics. Despite this, a systematic non-asymptotic convergence\nanalysis for DP-SGD, particularly in the context of online problems and local\ndifferential privacy (LDP) models, remains largely elusive. Existing\nnon-asymptotic analyses have focused on non-private optimization methods, and\nhence are not applicable to privacy-preserving optimization problems. This work\ninitiates the analysis to bridge this gap and opens the door to non-asymptotic\nconvergence analysis of private optimization problems. A general framework is\ninvestigated for the online LDP model in stochastic optimization problems. We\nassume that sensitive information from individuals is collected sequentially\nand aim to estimate, in real-time, a static parameter that pertains to the\npopulation of interest. Most importantly, we conduct a comprehensive\nnon-asymptotic convergence analysis of the proposed estimators in finite-sample\nsituations, which gives their users practical guidelines regarding the effect\nof various hyperparameters, such as step size, parameter dimensions, and\nprivacy budgets, on convergence rates. Our proposed estimators are validated in\nthe theoretical and practical realms by rigorous mathematical derivations and\ncarefully constructed numerical experiments.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08DP-SGD\uff09\u5728\u5728\u7ebf\u95ee\u9898\u548c\u672c\u5730\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u6a21\u578b\u4e2d\u7684\u975e\u6e10\u8fd1\u6536\u655b\u6027\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u975e\u9690\u79c1\u4f18\u5316\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u9690\u79c1\u4fdd\u62a4\u4f18\u5316\u95ee\u9898\u7684\u975e\u6e10\u8fd1\u6536\u655b\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u9002\u7528\u4e8e\u5728\u7ebfLDP\u6a21\u578b\u7684\u901a\u7528\u6846\u67b6\uff0c\u5bf9\u654f\u611f\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u4f30\u8ba1\uff0c\u5e76\u8fdb\u884c\u975e\u6e10\u8fd1\u6536\u655b\u5206\u6790\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u4f30\u8ba1\u5668\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8d85\u53c2\u6570\u5bf9\u6536\u655b\u901f\u7387\u7684\u5b9e\u9645\u5f71\u54cd\u6307\u5357\u3002", "conclusion": "\u672c\u6587\u4e3a\u975e\u6e10\u8fd1\u6536\u655b\u5206\u6790\u5728\u9690\u79c1\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.07046", "pdf": "https://arxiv.org/pdf/2507.07046", "abs": "https://arxiv.org/abs/2507.07046", "authors": ["Shahana Yasmin Chowdhury", "Bithi Banik", "Md Tamjidul Hoque", "Shreya Banerjee"], "title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "17 pages, 11 figures", "summary": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of\nhuman-computer interaction (HCI) and the evolution of artificial intelligence\n(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:\nneutral, happy, sad, angry, fear, disgust, and surprise, which are trained on\nfive datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).\nThe model achieves high accuracy on individual datasets, including 97.83% on\nRAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS\nand EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,\noutperforming previously reported results. To our knowledge, no existing study\nhas evaluated a single SER model across all five benchmark datasets (i.e.,\nR+T+S+C+E) simultaneously. In our work, we introduce this comprehensive\ncombination and achieve a remarkable overall accuracy of 93.76%. These results\nconfirm the robustness and generalizability of our DCRF-BiLSTM framework across\ndiverse datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCRF-BiLSTM\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u4e03\u79cd\u60c5\u611f\uff0c\u5e76\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u5728\u4eba\u673a\u4ea4\u4e92\u548c\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u540c\u65f6\u8bc4\u4f30\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528DCRF-BiLSTM\u6a21\u578b\uff0c\u5728RAVDESS\u3001TESS\u3001SAVEE\u3001EmoDB\u548cCrema-D\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8bc6\u522b\u4e03\u79cd\u60c5\u611f\u3002", "result": "\u6a21\u578b\u5728\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u9ad8\u8fbe97.83%\u81f3100%\uff0c\u7ec4\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.82%\uff0c\u7efc\u5408\u4e94\u4e2a\u6570\u636e\u96c6\u65f6\u6574\u4f53\u51c6\u786e\u7387\u4e3a93.76%\u3002", "conclusion": "DCRF-BiLSTM\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2507.07050", "pdf": "https://arxiv.org/pdf/2507.07050", "abs": "https://arxiv.org/abs/2507.07050", "authors": ["Ashen Weligalle"], "title": "Discrete Diffusion Models for Language Generation", "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50 (Primary) 68Q32, 60J27 (Secondary)", "G.3"], "comment": "pdfLaTeX, 69 pages with 21 figures, Licentiate Thesis", "summary": "Diffusion models have emerged as a powerful class of generative models,\nachieving state-of-the-art results in continuous data domains such as image and\nvideo generation. Their core mechanism involves a forward diffusion process\nthat gradually transforms structured data into a Gaussian-like distribution,\nfollowed by a learned reverse process to reconstruct the data. While successful\nin continuous modalities, applying this framework to discrete data-particularly\nnatural language-remains challenging due to token dependency complexities and\nthe lack of a defined generation order.This thesis investigates the feasibility\nand performance of discrete diffusion models for natural language generation.\nSpecifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model\n(D3PM) and compare it with traditional autoregressive (AR) language models. To\nassess generative performance, we use Bits Per Token (BPT), Negative\nLog-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.\n  Results show the best-performing D3PM model achieves a BPT of 5.72, with a\nmean of 8.05. The AR model outperforms in compression with a lower mean BPT of\n4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches\nper sec., indicating potential for parallel generation.All evaluations were\nconducted under consistent conditions-generating 100,000 tokens per model with\na fixed batch size of four-for fair comparison. This research presents a\ndetailed analysis of diffusion-based vs. autoregressive models, highlighting\ntrade-offs in generative quality and efficiency. Findings emphasize both the\npromise and limitations of diffusion models for discrete data, supporting\nfuture work in non-autoregressive language generation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\uff08D3PM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e0e\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793aD3PM\u5728\u5e76\u884c\u751f\u6210\u901f\u5ea6\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46AR\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u79bb\u6563\u6570\u636e\uff08\u5982\u81ea\u7136\u8bed\u8a00\uff09\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u79bb\u6563\u6570\u636e\u4e0a\u7684\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u79bb\u6563\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08D3PM\uff09\uff0c\u5e76\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u5bf9\u6bd4\uff0c\u4f7f\u7528BPT\u3001NLL\u3001PPL\u548c\u6279\u5904\u7406\u901f\u5ea6\u7b49\u6307\u6807\u3002", "result": "D3PM\u5728\u6279\u5904\u7406\u901f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff083.97\u6279\u6b21/\u79d2\uff09\uff0c\u4f46AR\u5728\u538b\u7f29\u6027\u80fd\uff08BPT\u5747\u503c4.59\uff09\u4e0a\u4f18\u4e8eD3PM\uff08BPT\u5747\u503c8.05\uff09\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u79bb\u6563\u6570\u636e\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u5e76\u884c\u751f\u6210\u65b9\u9762\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.07056", "pdf": "https://arxiv.org/pdf/2507.07056", "abs": "https://arxiv.org/abs/2507.07056", "authors": ["Jiahao Chen", "junhao li", "Yiming Wang", "Zhe Ma", "Yi Jiang", "Chunyi Zhou", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "The proliferation of Low-Rank Adaptation (LoRA) models has democratized\npersonalized text-to-image generation, enabling users to share lightweight\nmodels (e.g., personal portraits) on platforms like Civitai and Liblib.\nHowever, this \"share-and-play\" ecosystem introduces critical risks: benign\nLoRAs can be weaponized by adversaries to generate harmful content (e.g.,\npolitical, defamatory imagery), undermining creator rights and platform safety.\nExisting defenses like concept-erasure methods focus on full diffusion models\n(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability\nto adversarial prompt engineering. To bridge this gap, we propose LoRAShield,\nthe first data-free editing framework for securing LoRA models against misuse.\nOur platform-driven approach dynamically edits and realigns LoRA's weight\nsubspace via adversarial optimization and semantic augmentation. Experimental\nresults demonstrate that LoRAShield achieves remarkable effectiveness,\nefficiency, and robustness in blocking malicious generations without\nsacrificing the functionality of the benign task. By shifting the defense to\nplatforms, LoRAShield enables secure, scalable sharing of personalized models,\na critical step toward trustworthy generative ecosystems.", "AI": {"tldr": "LoRAShield\u662f\u4e00\u79cd\u6570\u636e\u65e0\u5173\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u62a4LoRA\u6a21\u578b\u514d\u53d7\u6ee5\u7528\uff0c\u901a\u8fc7\u52a8\u6001\u7f16\u8f91\u548c\u91cd\u65b0\u5bf9\u9f50\u6743\u91cd\u5b50\u7a7a\u95f4\u6765\u963b\u6b62\u6076\u610f\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u826f\u6027\u4efb\u52a1\u7684\u529f\u80fd\u3002", "motivation": "LoRA\u6a21\u578b\u7684\u5171\u4eab\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u88ab\u6ee5\u7528\u7684\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5ffd\u89c6\u5176\u6a21\u5757\u5316\u9002\u914d\u5668\u7684\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLoRAShield\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u4f18\u5316\u548c\u8bed\u4e49\u589e\u5f3a\u52a8\u6001\u7f16\u8f91LoRA\u7684\u6743\u91cd\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLoRAShield\u5728\u963b\u6b62\u6076\u610f\u751f\u6210\u65b9\u9762\u9ad8\u6548\u3001\u6709\u6548\u4e14\u9c81\u68d2\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u826f\u6027\u4efb\u52a1\u3002", "conclusion": "LoRAShield\u4e3a\u751f\u6210\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u5171\u4eab\u65b9\u6848\uff0c\u662f\u8fc8\u5411\u53ef\u4fe1\u751f\u6210\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2507.07058", "pdf": "https://arxiv.org/pdf/2507.07058", "abs": "https://arxiv.org/abs/2507.07058", "authors": ["Martin Sondermann", "Pinar Bisgin", "Niklas Tschorn", "Anja Burmann", "Christoph M. Friedrich"], "title": "Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Preprint Version. Accepted at EMBC 2025", "summary": "The automated classification of phonocardiogram (PCG) recordings represents a\nsubstantial advancement in cardiovascular diagnostics. This paper presents a\nsystematic comparison of four distinct models for heart murmur detection: two\nspecialized convolutional neural networks (CNNs) and two zero-shot universal\naudio transformers (BEATs), evaluated using fixed-length and heart cycle\nnormalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart\ncycle normalization method tailored to individual cardiac rhythms is\nintroduced. The findings indicate the following AUROC values: the CNN model\nwith fixed-length windowing achieves 79.5%, the CNN model with heart cycle\nnormalization scores 75.4%, the BEATs transformer with fixed-length windowing\nachieves 65.7%, and the BEATs transformer with heart cycle normalization\nresults in 70.1%.\n  The findings indicate that physiological signal constraints, especially those\nintroduced by different normalization strategies, have a substantial impact on\nmodel performance. The research provides evidence-based guidelines for\narchitecture selection in clinical settings, emphasizing the need for a balance\nbetween accuracy and computational efficiency. Although specialized CNNs\ndemonstrate superior performance overall, the zero-shot transformer models may\noffer promising efficiency advantages during development, such as faster\ntraining and evaluation cycles, despite their lower classification accuracy.\nThese findings highlight the potential of automated classification systems to\nenhance cardiac diagnostics and improve patient care.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u6a21\u578b\uff08\u4e24\u79cdCNN\u548c\u4e24\u79cdBEATs\u53d8\u6362\u5668\uff09\u7528\u4e8e\u5fc3\u97f3\u56fe\u5206\u7c7b\uff0c\u53d1\u73b0CNN\u6027\u80fd\u66f4\u4f18\uff0c\u4f46BEATs\u5728\u6548\u7387\u4e0a\u6709\u4f18\u52bf\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u6a21\u578b\u548c\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u5fc3\u97f3\u56fe\u81ea\u52a8\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528PhysioNet2022\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e24\u79cdCNN\u548c\u4e24\u79cdBEATs\u53d8\u6362\u5668\u5728\u56fa\u5b9a\u957f\u5ea6\u548c\u5fc3\u5468\u671f\u5f52\u4e00\u5316\u4e0b\u7684\u8868\u73b0\u3002", "result": "CNN\u56fa\u5b9a\u957f\u5ea6\u7a97\u53e3AUROC\u4e3a79.5%\uff0c\u5fc3\u5468\u671f\u5f52\u4e00\u5316\u4e3a75.4%\uff1bBEATs\u56fa\u5b9a\u957f\u5ea6\u7a97\u53e3\u4e3a65.7%\uff0c\u5fc3\u5468\u671f\u5f52\u4e00\u5316\u4e3a70.1%\u3002", "conclusion": "CNN\u6027\u80fd\u66f4\u4f18\uff0c\u4f46BEATs\u5728\u5f00\u53d1\u6548\u7387\u4e0a\u6709\u6f5c\u529b\uff0c\u9700\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.07060", "pdf": "https://arxiv.org/pdf/2507.07060", "abs": "https://arxiv.org/abs/2507.07060", "authors": ["Shreyas Vinaya Sathyanarayana", "Rahil Shah", "Sharanabasava D. Hiremath", "Rishikesh Panda", "Rahul Jana", "Riya Singh", "Rida Irfan", "Ashwin Murali", "Bharath Ramsundar"], "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "51 pages,", "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses.", "AI": {"tldr": "DeepRetro\u7ed3\u5408\u6a21\u677f\u548cLLM\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u9006\u5408\u6210\u5206\u6790\uff0c\u751f\u6210\u65b0\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u9006\u5408\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u6a21\u677f\uff0cLLM\u867d\u6709\u671b\u4f46\u591a\u6b65\u89c4\u5212\u80fd\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "\u7ed3\u5408\u6a21\u677f\u5f15\u64ce\u548cLLM\u751f\u6210\u5355\u6b65\u65ad\u5f00\u5efa\u8bae\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u52a8\u6001\u4f18\u5316\u8def\u5f84\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u6848\u4f8b\u4e2d\u6210\u529f\u8bc6\u522b\u53ef\u884c\u53ca\u65b0\u9896\u8def\u5f84\uff0c\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u53cd\u9988\u3002", "conclusion": "\u8fed\u4ee3LLM\u63a8\u7406\u53ef\u63d0\u5347\u590d\u6742\u5316\u5b66\u5408\u6210\u7684\u524d\u6cbf\u6c34\u5e73\u3002"}}
{"id": "2507.07067", "pdf": "https://arxiv.org/pdf/2507.07067", "abs": "https://arxiv.org/abs/2507.07067", "authors": ["Clement Ruah", "Houssem Sifaou", "Osvaldo Simeone", "Bashir M. Al-Hashimi"], "title": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks", "categories": ["eess.SP", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7535\u4fe1\u9886\u57dfAI\u6a21\u578b\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u548c\u6a21\u62df\u5230\u73b0\u5b9e\uff08sim-to-real\uff09\u7b56\u7565\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7535\u4fe1\u9886\u57dfAI\u6a21\u578b\u8bad\u7ec3\u56e0\u90e8\u7f72\u7279\u5b9a\u6570\u636e\u7a00\u7f3a\u800c\u56f0\u96be\uff0c\u6570\u5b57\u5b6a\u751f\u548csim-to-real\u7b56\u7565\u53ef\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u771f\u5b9e\u6d4b\u91cf\u6821\u51c6\u6570\u5b57\u5b6a\u751f\uff1b2\uff09\u4f7f\u7528sim-to-real\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u5904\u7406\u6570\u636e\u5dee\u5f02\u3002", "result": "\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u73af\u5883\u7ea7\u5efa\u6a21\u548c\u57fa\u4e8e\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\u7684\u8bad\u7ec3\u635f\u5931\u7ea7\u5efa\u6a21\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u548csim-to-real\u7b56\u7565\u80fd\u6709\u6548\u89e3\u51b3\u7535\u4fe1AI\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2507.07106", "pdf": "https://arxiv.org/pdf/2507.07106", "abs": "https://arxiv.org/abs/2507.07106", "authors": ["Vatsal Agarwal", "Matthew Gwilliam", "Gefen Kohavi", "Eshan Verma", "Daniel Ulbricht", "Abhinav Shrivastava"], "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor", "categories": ["cs.CV", "cs.LG"], "comment": "Website: see https://vatsalag99.github.io/mustafar/", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u5e76\u89e3\u51b3CLIP\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\u53ca\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3CLIP\u89c6\u89c9\u7f16\u7801\u5668\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e0a\u7684\u4e0d\u8db3\uff0c\u63a2\u7d22\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6\u805a\u7126\u76f8\u5173\u533a\u57df\uff0c\u5e76\u63d0\u51fa\u878d\u5408CLIP\u4e0e\u6269\u6563\u7279\u5f81\u7684\u7b56\u7565\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u9700\u8981\u7a7a\u95f4\u548c\u7ec4\u5408\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u6709\u671b\u6210\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u65b0\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4efb\u52a1\u4e2d\u3002"}}
