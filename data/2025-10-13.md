<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 11]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 117]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.CV](#cs.CV) [Total: 18]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.CR](#cs.CR) [Total: 6]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CL](#cs.CL) [Total: 18]
- [math.CO](#math.CO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly](https://arxiv.org/abs/2510.08863)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DB

TL;DR: 对Redis、Aerospike和Dragonfly三种NoSQL键值存储系统进行性能评估，使用YCSB基准测试框架，在不同工作负载模式（读重、写重、平衡）和1-32个客户端并发下测试延迟、吞吐量和内存特性。


<details>
  <summary>Details</summary>
Motivation: 分布式应用和云计算的发展对可扩展、高性能的键值存储系统提出了需求，需要评估不同NoSQL系统的性能表现。

Method: 使用Yahoo! Cloud Serving Benchmark (YCSB)框架，在三种不同工作负载模式（读重、写重、平衡）下，通过1到32个客户端的并发变化进行广泛实验。

Result: 评估方法捕获了在实际操作条件下的延迟、吞吐量和内存特性，揭示了每个系统的性能权衡和可扩展性行为。

Conclusion: 该研究为不同NoSQL键值存储系统的性能特征提供了深入见解，有助于在实际应用中选择合适的存储解决方案。

Abstract: The rise of distributed applications and cloud computing has created a demand
for scalable, high-performance key-value storage systems. This paper presents a
performance evaluation of three prominent NoSQL key-value stores: Redis,
Aerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)
framework. We conducted extensive experiments across three distinct workload
patterns (read-heavy, write-heavy), and balanced while systematically varying
client concurrency from 1 to 32 clients. Our evaluation methodology captures
both latency, throughput, and memory characteristics under realistic
operational conditions, providing insights into the performance trade-offs and
scalability behaviour of each system

</details>


### [2] [HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance](https://arxiv.org/abs/2510.08896)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.DB

TL;DR: HES-SQL是一个新颖的混合训练框架，通过结合思维模式融合的监督微调与组相对策略优化，提升Text-to-SQL生成性能，在准确性和查询效率之间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL系统在生成准确且计算高效的SQL查询方面存在挑战，需要在语义准确性和执行效率之间找到平衡。

Method: 提出三个关键创新：骨架完整性评分机制、查询延迟感知奖励系统、思维模式完成的自蒸馏过程，结合监督微调和强化学习进行混合训练。

Result: 在BIRD和KaggleDBQA基准测试中分别达到79.14%和54.9%的执行准确率，查询延迟相比监督基线提升11%-20%。

Conclusion: HES-SQL为Text-to-SQL系统建立了新范式，通过执行感知的强化学习有效平衡语义准确性和计算效率，可扩展到需要正确性和效率优化的结构化生成任务。

Abstract: We present HES-SQL, a novel hybrid training framework that advances
Text-to-SQL generation through the integration of thinking-mode-fused
supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).
Our approach introduces three key innovations: (1) a skeleton-completeness
scoring mechanism that enhances preference alignment between generated queries
and optimal SQL structures; (2) a query-latency-aware reward system that
incentivizes the generation of computationally efficient SQL queries; (3) a
self-distillation process for thinking-mode completion that prevents
degradation of the model's reasoning capabilities. This framework enables
hybrid thinking models to switch between reasoning and non-reasoning modes
while improving SQL query accuracy and execution efficiency.
  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under
controlled single-user conditions, demonstrates that HES-SQL achieves
competitive performance with execution accuracies of 79.14\% and 54.9\% on the
BIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the
end-to-end execution time of generated queries on the DBMS, averaged over
multiple runs to mitigate variance. Efficiency gains range from 11\% to 20\%
relative to supervised baselines. Our results establish a new paradigm for
Text-to-SQL systems that effectively balances semantic accuracy with
computational efficiency through execution-informed reinforcement learning
(RL). The proposed methodology has significant implications for developing
robust natural language interfaces to databases and can be extended to broader
structured generation tasks requiring both correctness and efficiency
optimization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Maple是一个多智能体系统，通过自然语言输入自动生成正确的深度学习命令，解决了在异构GPU集群上训练深度学习模型时手动编写命令的困难。


<details>
  <summary>Details</summary>
Motivation: 在GPU集群上训练深度学习模型时，用户需要适应异构启动器、调度器、亲和性选项、框架参数和环境变量，手动编写命令容易出错且令人沮丧，阻碍研究并浪费资源。

Method: Maple包含四个智能体：信息提取、模板检索、命令行验证和错误纠正，利用多个总参数量为10B的语言模型。

Result: 在9个美国国家计算中心的GPU集群、5个代表性深度学习模型家族和4种常用并行训练范式的567个测试案例中，Maple实现了92.0%的准确率，性能与GPT-5、Claude和Gemini等最先进模型相当。

Conclusion: Maple在异构高性能计算环境中实现了便携和可扩展的分布式深度学习训练，具有重要的实用价值。

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [4] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: 提出了一种通用的单边分布式矩阵乘法算法，支持所有分区和复制因子组合，通过切片计算重叠图块，性能与PyTorch DTensor相当。


<details>
  <summary>Details</summary>
Motivation: 现有分布式矩阵乘法算法仅支持部分分区方式，需要多个算法实现来支持全部分区组合，否则需要重新分布操作数增加通信成本。

Method: 使用切片（索引算术）计算需要相乘的重叠图块集合，然后直接执行或重新排序并降低为优化IR以最大化重叠。基于高级C++ PGAS编程框架实现，使用节点内互连进行GPU到GPU直接通信。

Result: 在各种分区和复制因子下评估性能，发现与针对AI模型优化的PyTorch DTensor性能相当。

Conclusion: 提出的通用单边算法能够支持所有分区和复制因子组合，避免了操作数重新分布，在保持性能的同时提供了更大的灵活性。

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [5] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: 提出了一种硬件-软件协同设计的轻量级MPC控制器，用于多核HPC处理器的能量和热管理，通过算子分裂QP求解器和嵌入式RISC-V控制器实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 传统MPC方法在PE上执行控制器会产生操作系统开销，导致抖动和带宽限制；专用片上控制器虽能实现快速确定性控制，但存在面积和功耗开销问题。

Method: 采用算子分裂二次规划求解器和嵌入式多核RISC-V控制器，通过剪枝弱热耦合减少模型内存，并采用提前调度来高效并行执行稀疏三角系统。

Result: 在500MHz频率下控制144个PE时实现亚毫秒延迟，比单核基准延迟降低33倍，能效提高7.9倍，内存占用小于1MiB，功耗仅325mW，占用典型HPC处理器芯片面积不到1.5%。

Conclusion: 该硬件-软件协同设计方法成功解决了多核HPC处理器能量热管理的控制挑战，实现了高效、低延迟、低开销的片上MPC控制。

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [The Online Submodular Cover Problem](https://arxiv.org/abs/2510.08883)
*Anupam Gupta,Roie Levin*

Main category: cs.DS

TL;DR: 提出了在线子模覆盖问题的对数竞争算法，竞争比为O(ln n ln(T·f(N)/f_min))，与在线集合覆盖问题的最佳竞争比匹配。


<details>
  <summary>Details</summary>
Motivation: 受网络监控和资源分配问题启发，研究在在线设置下的子模覆盖问题，其中子模函数随时间逐步给出，且不能撤销之前的选择。

Method: 通过在线近似求解子模覆盖的指数大小线性规划松弛并进行舍入，克服了离线贪婪方法难以在线实现的挑战。

Result: 给出了输入序列长度为T时的竞争比O(ln n ln(T·f(N)/f_min))，其中f_min是函数的最小非零边际值。

Conclusion: 该算法在多项式时间在线算法中是最优的，除非NP ⊆ BPP，为在线子模覆盖问题提供了有效的解决方案。

Abstract: In the submodular cover problem, we are given a monotone submodular function
$f$, and we want to pick the min-cost set $S$ such that $f(S) = f(N)$.
Motivated by problems in network monitoring and resource allocation, we
consider the submodular cover problem in an online setting. As a concrete
example, suppose at each time $t$, a nonnegative monotone submodular function
$g_t$ is given to us. We define $f^{(t)} = \sum_{s \leq t} g_s$ as the sum of
all functions seen so far. We need to maintain a submodular cover of these
submodular functions $f^{(1)}, f^{(2)}, \ldots f^{(T)}$ in an online fashion;
i.e., we cannot revoke previous choices. Formally, at each time $t$ we produce
a set $S_t \subseteq N$ such that $f^{(t)}(S_t) = f^{(t)}(N)$ -- i.e., this set
$S_t$ is a cover -- such that $S_{t-1} \subseteq S_t$, so previously decisions
to pick elements cannot be revoked. (We actually allow more general sequences
$\{f^{(t)}\}$ of submodular functions, but this
sum-of-simpler-submodular-functions case is useful for concreteness.)
  We give polylogarithmic competitive algorithms for this online submodular
cover problem. The competitive ratio on an input sequence of length $T$ is
$O(\ln n \ln (T \cdot f(N) / f_{\text{min}}))$, where $f_{\text{min}}$ is the
smallest nonzero marginal for functions $f^{(t)}$, and $|N| = n$. For the
special case of online set cover, our competitive ratio matches that of Alon et
al. [SIAM J. Comp. 03], which are best possible for polynomial-time online
algorithms unless $NP \subseteq BPP$ (see Korman 04). Since existing offline
algorithms for submodular cover are based on greedy approaches which seem
difficult to implement online, the technical challenge is to (approximately)
solve the exponential-sized linear programming relaxation for submodular cover,
and to round it, both in the online setting.

</details>


### [7] [Planar Length-Constrained Minimum Spanning Trees](https://arxiv.org/abs/2510.09002)
*D Ellis Hershkowitz,Richard Z Huang*

Main category: cs.DS

TL;DR: 本文针对平面图中的长度约束最小生成树问题，提出了一个多项式时间算法，能在任何常数ε>0的情况下，输出O(log^(1+ε)n近似解，同时保证每个节点到根节点的距离不超过(1+ε)h。


<details>
  <summary>Details</summary>
Motivation: 长度约束最小生成树问题在一般图上难以获得良好的近似比，本文旨在探索平面图中该问题的可近似性，并分离平面图与一般图在该问题上的近似能力差异。

Method: 基于新的长度约束平面分隔器，开发了多项式时间算法，该算法也适用于长度约束Steiner树问题。

Result: 在平面图上实现了O(log^(1+ε)n近似比，同时满足(1+ε)h的距离约束；在一般图上证明了任何算法若要求节点距离不超过2h，则无法达到O(log^(2-ε)n的近似比。

Conclusion: 本文结果分离了长度约束最小生成树问题在平面图和一般图上的可近似性，为平面图提供了有效的近似算法，同时证明了在一般图上的近似难度下界。

Abstract: In length-constrained minimum spanning tree (MST) we are given an $n$-node
graph $G = (V,E)$ with edge weights $w : E \to \mathbb{Z}_{\geq 0}$ and edge
lengths $l: E \to \mathbb{Z}_{\geq 0}$ along with a root node $r \in V$ and a
length-constraint $h \in \mathbb{Z}_{\geq 0}$. Our goal is to output a spanning
tree of minimum weight according to $w$ in which every node is at distance at
most $h$ from $r$ according to $l$.
  We give a polynomial-time algorithm for planar graphs which, for any constant
$\epsilon > 0$, outputs an $O\left(\log^{1+\epsilon} n\right)$-approximate
solution with every node at distance at most $(1+\epsilon)h$ from $r$ for any
constant $\epsilon > 0$. Our algorithm is based on new length-constrained
versions of classic planar separators which may be of independent interest.
Additionally, our algorithm works for length-constrained Steiner tree.
Complementing this, we show that any algorithm on general graphs for
length-constrained MST in which nodes are at most $2h$ from $r$ cannot achieve
an approximation of $O\left(\log ^{2-\epsilon} n\right)$ for any constant
$\epsilon > 0$ under standard complexity assumptions; as such, our results
separate the approximability of length-constrained MST in planar and general
graphs.

</details>


### [8] [A Faster Randomized Algorithm for Vertex Cover: An Automated Approach](https://arxiv.org/abs/2510.09027)
*Katie Clinch,Serge Gaspers,Tao Zixu He,Simon Mackenzie,Tiankuang Zhang*

Main category: cs.DS

TL;DR: 提出了两种设计和分析分支算法的技术：自动生成分支规则的系统方法和基于Measure & Conquer的随机分支算法分析技术，在Vertex Cover问题上获得了当前最快的随机算法。


<details>
  <summary>Details</summary>
Motivation: 改进Vertex Cover问题的分支算法性能，特别是在有界度图和一般图上获得更快的运行时间。

Method: 1. 通过局部结构的系统案例分析自动生成分支规则；2. 使用Measure & Conquer方法分析随机分支算法，提供更大的分支规则制定灵活性。

Result: 在次立方图上获得O*(1.07625^n)和O*(1.13132^k)的运行时间；在最大度为4的图上获得O*(1.13735^n)和O*(1.21103^k)；在一般图上获得O*(1.25281^k)。

Conclusion: 结合这些创新技术，在Vertex Cover问题上获得了不同参数下当前最快的随机算法，特别在有界度图上表现优异。

Abstract: This work introduces two techniques for the design and analysis of branching
algorithms, illustrated through the case study of the Vertex Cover problem.
First, we present a method for automatically generating branching rules through
a systematic case analysis of local structures. Second, we develop a new
technique for analyzing randomized branching algorithms using the Measure &
Conquer method, offering greater flexibility in formulating branching rules. By
combining these innovations with additional techniques, we obtain the fastest
known randomized algorithms in different parameters for the Vertex Cover
problem on graphs with bounded degree (up to 6) and on general graphs. For
example, our algorithm solves Vertex Cover on subcubic graphs in
$O^*(1.07625^n)$ time and $O^*(1.13132^k)$ time, respectively. For graphs with
maximum degree 4, we achieve running times of $O^*(1.13735^n)$ and
$O^*(1.21103^k)$, while for general graphs we achieve $O^*(1.25281^k)$.

</details>


### [9] [Multi-product Influence Maximization in Billboard Advertisement](https://arxiv.org/abs/2510.09050)
*Dildar Ali,Rajibul Islam,Suman Banerjee*

Main category: cs.DS

TL;DR: 该论文研究了多产品广告牌投放优化问题，提出了两种变体：一种是通过选择k个广告位满足各产品的影响力需求；另一种是为每个产品分配不相交的广告位集合，在预算约束下最大化影响力。


<details>
  <summary>Details</summary>
Motivation: 传统广告牌投放只关注单一产品的影响力最大化，但现实中广告商通常需要同时推广多个产品，每个产品有特定的影响力需求，这需要新的优化方法。

Method: 将第一个问题建模为多子模覆盖问题，采用双准则近似算法；将第二个问题建模为多子模覆盖的泛化问题，提出基于采样的近似算法。

Result: 在真实世界轨迹和广告牌数据集上的大量实验表明，所提出的解决方案在效果和效率方面都很有效。

Conclusion: 论文成功解决了多产品广告牌投放优化问题，提出的算法能够有效满足各产品的影响力需求，在实际应用中具有良好性能。

Abstract: Billboard Advertisement has emerged as an effective out-of-home advertisement
technique where the goal is to select a limited number of slots and play
advertisement content over there with the hope that this will be observed by
many people, and effectively, a significant number of them will be influenced
towards the brand. Given a trajectory and a billboard database and a positive
integer $k$, how can we select $k$ highly influential slots to maximize
influence? In this paper, we study a variant of this problem where a commercial
house wants to make a promotion of multiple products, and there is an influence
demand for each product. We have studied two variants of the problem. In the
first variant, our goal is to select $k$ slots such that the respective
influence demand of each product is satisfied. In the other variant of the
problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the
goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots,
S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i
\neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the
products gets satisfied. We model the first variant of the problem as a
multi-submodular cover problem and the second variant as its generalization.
For solving the first variant, we adopt the bi-criteria approximation
algorithm, and for the other variant, we propose a sampling-based approximation
algorithm. Extensive experiments with real-world trajectory and billboard
datasets highlight the effectiveness and efficiency of the proposed solution
approach.

</details>


### [10] [Random-Shift Revisited: Tight Approximations for Tree Embeddings and L1-Oblivious Routings](https://arxiv.org/abs/2510.09124)
*Rasmus Kyng,Maximilian Probst Gutenberg,Tim Rieder*

Main category: cs.DS

TL;DR: 本文提出了一种对随机移位分解的新分析，证明了在指数增长的尺度下，距离中心与分离概率之间存在紧致的常数权衡，而非单尺度下必需的Ω(log n)权衡。这直接导致了具有预期O(log n)拉伸的树嵌入构造方法，改进了FRT算法的分析，并为ℓ1-无意识路由提供了紧致的竞争比分析。


<details>
  <summary>Details</summary>
Motivation: 传统的随机移位分解在单尺度下需要Ω(log n)的权衡，限制了树嵌入和路由算法的性能。本文旨在通过分析指数增长尺度的分解，发现更优的权衡关系，从而改进现有算法的理论保证和计算效率。

Method: 采用随机移位分解方法，但不同于传统单尺度分析，本文分析指数增长的尺度序列D=2^0,2^1,...,2^{log_2(diam(G))}。通过证明在这些尺度上平均而言，距离中心与分离概率之间存在常数权衡，而非对数权衡。

Result: 获得了紧致的O(log n)拉伸树嵌入构造，为FRT算法提供了替代证明，匹配了Bartal的下界。同时改进了ℓ1-无意识路由的分析，得到紧致的O(log n)竞争比。算法在顺序、并行和分布式设置中都能以最优的工作量、深度和轮数实现。

Conclusion: 通过分析指数增长尺度的随机移位分解，本文揭示了比传统单尺度分析更优的权衡关系，为树嵌入和路由算法提供了紧致的理论保证和高效的计算实现，填补了并行和分布式设置中快速算法的空白。

Abstract: We present a new and surprisingly simple analysis of random-shift
decompositions -- originally proposed by Miller, Peng, and Xu [SPAA'13]: We
show that decompositions for exponentially growing scales $D = 2^0, 2^1,
\ldots, 2^{\log_2(\operatorname{diam}(G))}$, have a tight constant trade-off
between distance-to-center and separation probability on average across the
distance scales -- opposed to a necessary $\Omega(\log n)$ trade-off for a
single scale.
  This almost immediately yields a way to compute a tree $T$ for graph $G$ that
preserves all graph distances with expected $O(\log n)$-stretch. This gives an
alternative proof that obtains tight approximation bounds of the seminal result
by Fakcharoenphol, Rao, and Talwar [STOC'03] matching the $\Omega(\log n)$
lower bound by Bartal [FOCS'96]. Our insights can also be used to refine the
analysis of a simple $\ell_1$-oblivious routing proposed in [FOCS'22], yielding
a tight $O(\log n)$ competitive ratio.
  Our algorithms for constructing tree embeddings and $\ell_1$-oblivious
routings can be implemented in the sequential, parallel, and distributed
settings with optimal work, depth, and rounds, up to polylogarithmic factors.
Previously, fast algorithms with tight guarantees were not known for tree
embeddings in parallel and distributed settings, and for $\ell_1$-oblivious
routings, not even a fast sequential algorithm was known.

</details>


### [11] [Confluence of the Node-Domination and Edge-Domination Hypergraph Rewrite Rules](https://arxiv.org/abs/2510.09286)
*Antoine Amarilli,Mikaël Monet,Rémi De Pretto*

Main category: cs.DS

TL;DR: 该论文研究了超图中的边支配和节点支配重写规则，证明了这些规则具有汇合性，即无论应用顺序如何，最终都能得到同构的最小超图。


<details>
  <summary>Details</summary>
Motivation: 边支配和节点支配规则是计算超图最小命中集时常用的预处理步骤，但之前缺乏对这些规则汇合性的形式化证明。

Method: 通过形式化定义边支配和节点支配规则，并证明这些规则在超图变换中具有汇合性，即无论应用顺序如何，最终都能收敛到同构的结果。

Result: 证明了边支配和节点支配规则是汇合的，这意味着存在一个唯一的（在同构意义下）最小超图，可以通过这些规则从原始超图得到。

Conclusion: 边支配和节点支配规则是汇合的，这为超图最小化提供了理论基础，确保在计算最小命中集时预处理步骤的可靠性。

Abstract: In this note, we study two rewrite rules on hypergraphs, called
edge-domination and node-domination, and show that they are confluent. These
rules are rather natural and commonly used before computing the minimum hitting
sets of a hypergraph. Intuitively, edge-domination allows us to remove
hyperedges that are supersets of another hyperedge, and node-domination allows
us to remove nodes whose incident hyperedges are a subset of that of another
node. We show that these rules are confluent up to isomorphism, i.e., if we
apply any sequences of edge-domination and node-domination rules, then the
resulting hypergraphs can be made isomorphic via more rule applications. This
in particular implies the existence of a unique minimal hypergraph, up to
isomorphism.

</details>


### [12] [Improved Extended Regular Expression Matching](https://arxiv.org/abs/2510.09311)
*Philip Bille,Inge Li Gørtz,Rikke Schjeldrup Jessen*

Main category: cs.DS

TL;DR: 本文提出了一种新的扩展正则表达式匹配算法，显著改进了先前的时间和空间复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 扩展正则表达式匹配是形式语言理论中的基本问题，自1970年代Hopcroft和Ullmann提出O(n³m)时间复杂度的算法以来，很少有显著的渐进改进。现有最优算法仍存在较大的计算开销，需要更高效的解决方案。

Method: 提出了一种新算法，利用矩阵乘法指数ω≈2.3716来优化计算。通过改进数据结构和使用更高效的位操作技术，将时间复杂度中的主导项从n³k降低到n^ωk，同时将空间复杂度中的n²k项改进为O(n²)。

Result: 新算法在时间复杂度上达到O(n^ωk + n²m/min(w/log w, log n) + m)，空间复杂度为O(n² log k/w + n + m) = O(n² + m)。这显著优于Yamamoto和Miyazaki的O((n³k + n²m)/w + n + m)时间复杂度和O((n²k + nm)/w + n + m)空间复杂度。

Conclusion: 该研究在扩展正则表达式匹配问题上取得了突破性进展，通过利用矩阵乘法的最新进展和优化数据结构，显著降低了算法的时间和空间复杂度，为该领域的后续研究提供了新的方向。

Abstract: An extended regular expression $R$ specifies a set of strings formed by
characters from an alphabet combined with concatenation, union, intersection,
complement, and star operators. Given an extended regular expression $R$ and a
string $Q$, the extended regular expression matching problem is to decide if
$Q$ matches any of the strings specified by $R$. Extended regular expressions
are a basic concept in formal language theory and a basic primitive for
searching and processing data. Extended regular expression matching was
introduced by Hopcroft and Ullmann in the 1970s [\textit{Introduction to
Automata Theory, Languages and Computation}, 1979], who gave a simple dynamic
programming solution using $O(n^3m)$ time and $O(n^2m)$ space, where $n$ is the
length of $Q$ and $m$ is the length of $R$. Since then, several solutions have
been proposed, but few significant asymptotic improvements have been obtained.
The current state-of-the art solution, by Yamamoto and Miyazaki~[COCOON, 2003],
uses $O(\frac{n^3k + n^2m}{w} + n + m)$ time and $O(\frac{n^2k + nm}{w} + n +
m)$ space, where $k$ is the number of negation and complement operators in $R$
and $w$ is the number of bits in a word. This roughly replaces the $m$ factor
with $k$ in the dominant terms of both the space and time bounds of the
Hopcroft and Ullmann algorithm.
  We revisit the problem and present a new solution that significantly improves
the previous time and space bounds. Our main result is a new algorithm that
solves extended regular expression matching in \[O\left(n^\omega k +
\frac{n^2m}{\min(w/\log w, \log n)} + m\right)\] time and $O(\frac{n^2 \log
k}{w} + n + m) = O(n^2 +m)$ space, where $\omega \approx 2.3716$ is the
exponent of matrix multiplication. Essentially, this replaces the dominant
$n^3k$ term with $n^\omega k$ in the time bound, while simultaneously improving
the $n^2k$ term in the space to $O(n^2)$.

</details>


### [13] [Optimizing Administrative Divisions: A Vertex $k$-Center Approach for Edge-Weighted Road Graphs](https://arxiv.org/abs/2510.09334)
*Peteris Daugulis*

Main category: cs.DS

TL;DR: 提出了一种基于Voronoi分割和顶点k中心问题的透明数据驱动方法，用于优化市政行政区域划分，以减少旅行时间差异并平衡行政负担。


<details>
  <summary>Details</summary>
Motivation: 市政服务的效率和公平性依赖于合理的行政区域划分，需要适应人口、基础设施和经济因素的变化。

Method: 基于道路图的Voronoi分割和顶点k中心问题（最小最大设施选址问题的特例），考虑道路网络结构和行政中心战略布局。

Result: 在拉脱维亚（具有复杂地理特征和多样化人口分布的国家）实施了该方法。

Conclusion: 该方法能够最小化旅行时间差异，确保人口行政时间负担的均衡分布。

Abstract: Efficient and equitable access to municipal services hinges on well-designed
administrative divisions. It requires ongoing adaptation to changing
demographics, infrastructure, and economic factors. This article proposes a
novel transparent data-driven method for territorial division based on the
Voronoi partition of edge-weighted road graphs and the vertex $k$-center
problem as a special case of the minimax facility location problem. By
considering road network structure and strategic placement of administrative
centers, this method seeks to minimize travel time disparities and ensure a
more balanced distribution of administrative time burden for the population. We
show implementations of this approach in the context of Latvia, a country with
complex geographical features and diverse population distribution.

</details>


### [14] [On Stable Cutsets in General and Minimum Degree Constrained Graphs](https://arxiv.org/abs/2510.09432)
*Mats Vroon,Hans L. Bodlaender*

Main category: cs.DS

TL;DR: 本文提出了稳定割集的改进精确算法，时间复杂度为O*(1.2972^n)，并研究了有最小度约束的图上的稳定割集问题。


<details>
  <summary>Details</summary>
Motivation: 稳定割集判定问题是NP完全问题，现有算法效率有待提升。同时，研究图的最小度对稳定割集存在性的影响具有理论意义。

Method: 通过分支图配置并使用Beigel和Eppstein的(3,2)-约束满足问题算法，结合最小度约束下的图结构分析。

Result: 提出了O*(1.2972^n)的改进算法；证明了当最小度≥2/3(n-1)时图无稳定割集；为最小度≥n/2的图提供了多项式时间算法；证明了稳定割集在最小度>1时仍为NP完全问题。

Conclusion: 本文显著改进了稳定割集问题的精确算法，并深入研究了最小度约束下的问题复杂度，为相关图论问题提供了新的算法和理论结果。

Abstract: A stable cutset is a set of vertices $S$ of a connected graph, that is
pairwise non-adjacent and when deleting $S$, the graph becomes disconnected.
Determining the existence of a stable cutset in a graph is known to be
NP-complete. In this paper, we introduce a new exact algorithm for Stable
Cutset. By branching on graph configurations and using the $O^*(1.3645)$
algorithm for the (3,2)-Constraint Satisfaction Problem presented by Beigel and
Eppstein, we achieve an improved running time of $O^*(1.2972^n)$.
  In addition, we investigate the Stable Cutset problem for graphs with a bound
on the minimum degree $\delta$. First, we show that if the minimum degree of a
graph $G$ is at least $\frac{2}{3}(n-1)$, then $G$ does not contain a stable
cutset. Furthermore, we provide a polynomial-time algorithm for graphs where
$\delta \geq \tfrac{1}{2}n$, and a similar kernelisation algorithm for graphs
where $\delta = \tfrac{1}{2}n - k$.
  Finally, we prove that Stable Cutset remains NP-complete for graphs with
minimum degree $c$, where $c > 1$. We design an exact algorithm for this
problem that runs in $O^*(\lambda^n)$ time, where $\lambda$ is the positive
root of $x^{\delta + 2} - x^{\delta + 1} + 6$. This algorithm can also be
applied to the \textsc{3-Colouring} problem with the same minimum degree
constraint, leading to an improved exact algorithm as well.

</details>


### [15] [Parameterized Algorithms for Diversity of Networks with Ecological Dependencies](https://arxiv.org/abs/2510.09512)
*Mark Jones,Jannik Schestag*

Main category: cs.DS

TL;DR: 该论文研究在考虑食物网生态约束条件下，在系统发育网络中寻找具有最大系统发育多样性的小规模物种集合问题，并分析了不同参数组合下的参数化复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的系统发育多样性研究要么在系统发育网络中寻找多样性最大集合，要么在系统发育树中考虑生态约束，但很少同时考虑这两个重要因素。生物保护规划需要同时考虑系统发育多样性和生态可行性。

Method: 引入决策问题，给定系统发育网络、食物网和参数k、D，寻找满足食物网生存条件的k个物种集合，使其在最大化所有路径度量下具有至少D的系统发育多样性。使用颜色编码方法开发新的算法框架。

Result: 提供了完整的复杂度二分法，表征了不同参数组合（k、D、食物网扫描宽度、网络最大入度、网络高度h）下哪些导致W[1]-困难性，哪些允许固定参数可处理算法。提出了多个FPT算法。

Conclusion: 成功开发了解决系统发育网络中考虑依赖关系（如食物网）的系统发育多样性问题的新算法框架，为生物保护规划提供了有效的计算工具。

Abstract: For a phylogenetic tree, the phylogenetic diversity of a set A of taxa is the
total weight of edges on paths to A. Finding small sets of maximal diversity is
crucial for conservation planning, as it indicates where limited resources can
be invested most efficiently. In recent years, efficient algorithms have been
developed to find sets of taxa that maximize phylogenetic diversity either in a
phylogenetic network or in a phylogenetic tree subject to ecological
constraints, such as a food web. However, these aspects have mostly been
studied independently. Since both factors are biologically important, it seems
natural to consider them together. In this paper, we introduce decision
problems where, given a phylogenetic network, a food web, and integers k, and
D, the task is to find a set of k taxa with phylogenetic diversity of at least
D under the maximize all paths measure, while also satisfying viability
conditions within the food web. Here, we consider different definitions of
viability, which all demand that a "sufficient" number of prey species survive
to support surviving predators. We investigate the parameterized complexity of
these problems and present several fixed-parameter tractable (FPT) algorithms.
Specifically, we provide a complete complexity dichotomy characterizing which
combinations of parameters - out of the size constraint k, the acceptable
diversity loss D, the scanwidth of the food web, the maximum in-degree in the
network, and the network height h - lead to W[1]-hardness and which admit FPT
algorithms. Our primary methodological contribution is a novel algorithmic
framework for solving phylogenetic diversity problems in networks where
dependencies (such as those from a food web) impose an order, using a color
coding approach.

</details>


### [16] [Minimizing the Weighted Makespan with Restarts on a Single Machine](https://arxiv.org/abs/2510.09589)
*Aflatoun Amouzandeh,Klaus Jansen,Lis Pirotton,Rob van Stee,Corinna Wambsganz*

Main category: cs.DS

TL;DR: 研究单机重启调度问题，目标是最小化加权完工时间。建立了确定性在线算法的竞争比下界1.4656，对于相同处理时间的情况设计了竞争比优于1.3098的算法，并证明下界为1.2344。


<details>
  <summary>Details</summary>
Motivation: 重启调度比抢占调度更弱，因为中断的作业必须从头重新开始，而不是从中断点继续。研究这种更弱但更现实的调度模型，特别是在线算法的性能保证。

Method: 使用竞争分析框架，建立理论下界并设计确定性在线算法。对于相同处理时间的特殊情况，设计了改进的算法。

Result: 证明了一般情况下的竞争比下界为1.4656，相同处理时间情况下设计了竞争比优于1.3098的算法，并证明该情况下的下界为1.2344。

Conclusion: 重启调度问题具有挑战性，即使在相同处理时间的简化情况下，也存在显著的算法性能差距，表明该问题值得进一步研究。

Abstract: We consider the problem of minimizing the weighted makespan on a single
machine with restarts. Restarts are similar to preemptions but weaker: a job
can be interrupted, but then it has to be run again from the start instead of
resuming at the point of interruption later. The objective is to minimize the
weighted makespan, defined as the maximum weighted completion time of jobs.
  We establish a lower bound of 1.4656 on the competitive ratio achievable by
deterministic online algorithms. For the case where all jobs have identical
processing times, we design and analyze a deterministic online algorithm that
improves the competitive ratio to better than 1.3098. Finally, we prove a lower
bound of 1.2344 for this case.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: 本研究评估开源大语言模型在本地部署环境下作为意图驱动操作系统核心组件的可行性，与GPT-4进行性能对比分析。


<details>
  <summary>Details</summary>
Motivation: 当前基于云端专有模型的实现存在隐私、自主性和可扩展性限制，本地部署对于构建稳健可信的语言优先交互界面至关重要。

Method: 对多个开源和开放访问模型进行比较分析，评估其在解析用户意图和生成工作流程方面的能力，并与OpenAI的GPT-4系统进行性能对比。

Result: 研究提供了关于开源LLMs作为下一代操作系统本地可操作组件的实际可行性、性能权衡和潜力的实证见解。

Conclusion: 研究结果支持AI基础设施的去中心化和民主化，指向通过本地嵌入智能实现更无缝、自适应和隐私保护的用户设备交互的未来。

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [18] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: 本研究通过实证分析评估了不同版本约束类型（固定版本与浮动版本）对依赖项过时或易受攻击可能性的影响，发现在npm、PyPI和Cargo生态系统中，浮动次要版本是最可能导致过时依赖的约束类型，而浮动主版本最不容易导致过时依赖。


<details>
  <summary>Details</summary>
Motivation: 开发者在依赖管理时面临两难选择：固定依赖版本可减少破坏性变更风险但需手动管理更新，浮动依赖可自动获取修复但存在破坏性变更风险。安全从业者推荐固定版本以防止供应链攻击，但固定版本最易导致依赖过时。目前缺乏关于不同版本约束类型对依赖状态影响的大规模实证研究。

Method: 首先识别npm、PyPI和Cargo生态系统中依赖版本约束的使用趋势和开发者变更模式，然后使用生存分析建模依赖状态转换，评估固定版本与其他约束类型相比导致依赖过时或易受攻击的可能性。

Result: 在过时和易受攻击的依赖中，最常用的版本约束类型是浮动次要版本，固定版本次之。浮动主版本最不容易导致依赖过时，浮动次要版本最不容易导致依赖易受攻击。

Conclusion: 研究为开发者提供了依赖版本约束选择的实证依据，帮助开发者在安全性和更新便利性之间做出更明智的权衡决策。

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [19] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 提出了一种有效的上下文收集策略，通过代码分块和相对定位来提升LLM在代码补全任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有IDE中的代码补全功能缺乏对什么构成良好上下文的研究，特别是基于IDE可用信息来帮助大语言模型在代码补全中表现更好

Method: 将代码库预处理为更小的代码块，然后使用基于语法和语义相似性的代码块检索方法，并结合相对定位

Result: 代码分块和代码块在最终上下文中的相对定位能够提高代码补全任务的性能

Conclusion: 提出的上下文收集策略能有效提升LLM在代码补全任务中的表现

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [20] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: 本文研究了LLMs在软件开发生命周期中对团队协作的影响，通过文献综述、行业案例、团队调查和案例研究，发现LLMs能显著提升效率、改善沟通，但也带来模型局限性和隐私等新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地集成到软件开发过程中，需要了解它们如何影响团队协作流程和生产力，以及如何解决SDLC中的协作障碍。

Method: 通过文献综述、行业案例、团队调查和两个案例研究，评估LLM辅助工具（如代码生成助手和AI驱动的项目管理代理）对协作软件工程实践的影响。

Result: LLMs能显著提高效率（通过自动化重复性任务和文档）、增强沟通清晰度、促进跨职能协作，同时引入模型局限性和隐私问题等新挑战。

Conclusion: LLMs在软件团队协作中具有重要价值，但需要解决模型定制、工具集成改进以及确保信任和安全的稳健策略等未来研究方向。

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [21] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: 提出了GradleFixer，一个专门用于修复Android构建错误的LLM代理，通过领域特定工具显著提升了构建失败修复率


<details>
  <summary>Details</summary>
Motivation: Android是最大的移动平台，但自动构建应用仍面临挑战。虽然LLM在代码修复方面有潜力，但在修复Android构建错误方面的应用尚未充分探索

Method: 1) 创建AndroidBuildBench基准测试集；2) 开发GradleFixer代理，使用领域特定工具检查和操作Gradle构建环境；3) 采用工具桥接策略，用领域感知抽象替换通用shell命令

Result: GradleFixer在pass@1下达到81.4%的解决率，显著优于依赖通用shell的最先进编码代理

Conclusion: LLM具备解决构建失败的高层知识，但难以通过通用shell将其转化为有效的底层操作。工具桥接策略通过提供API格式的工具和约束动作空间，成功弥合了高层推理与底层执行之间的差距

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [22] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 提出Faver方法，通过函数抽象的可验证中间件来简化基于LLM的RTL验证流程，将LLM友好代码结构与基于规则的模板结合，使LLM专注于功能本身而非电路验证细节。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在RTL生成中面临语义鸿沟大、训练数据有限的问题，导致生成准确性不足。虽然设计验证有助于提高准确性，但RTL测试平台数据稀缺且LLM在高级语言和硬件代码之间存在巨大语义差异。

Method: 提出Faver方法，使用函数抽象的可验证中间件，将LLM友好代码结构与基于规则的模板结合，解耦电路验证细节，让LLM专注于功能实现。

Result: 在SFT模型和开源模型上的实验表明，Faver将模型生成准确率提高了最高14%。

Conclusion: Faver方法有效简化了基于LLM的RTL验证流程，显著提高了生成准确性，为芯片设计自动化提供了可行解决方案。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [23] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 提出基于ReAct范式的可控代码生成多智能体框架，通过四个专业智能体的协作实现高效、精确和可解释的代码生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有代码生成模型在安全性、准确性和可控性方面的不足，特别是缺乏外部工具的动态集成、透明推理和用户安全控制。

Method: 采用多智能体系统架构，包含规划器、基于ReAct的搜索器、代码生成器和提取器四个专业智能体，通过动态交互实现代码生成。

Result: 在SVEN数据集上达到94.8%的安全率（使用CodeQL），优于现有方法，支持多种编程语言。

Conclusion: 该框架通过透明推理过程增强用户信任，提高可控性，为复杂代码生成任务提供了有效的解决方案。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [24] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 提出基于RAG框架的智能工单解决系统，通过整合JIRA工单和GitHub数据，使用语义嵌入和向量搜索来推荐相关历史解决方案，显著提升问题解决效率和知识复用。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发团队在处理重复或相关问题时常因知识分散在JIRA工单、开发者讨论和GitHub PR中而延迟解决，需要统一的知识整合和智能推荐方案。

Method: 采用检索增强生成(RAG)框架，结合Sentence-Transformers生成语义嵌入，使用FAISS进行向量搜索，检索相似历史案例，再由LLM生成基于证据的解决方案建议。

Result: 实验评估显示，该系统在精度、召回率、解决时间减少和开发者接受度等指标上显著提升，改善了解决准确性、修复质量和知识复用。

Conclusion: 该RAG框架为现代DevOps环境提供了有效的工单解决推荐系统，成功整合了异构软件工件，实现了基于检索证据的智能解决方案生成。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [25] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena是一个基于Chatbot Arena构建的代码生成人类评估平台，提供实时执行环境来评估LLM生成的代码质量。该平台收集了超过14,000个代码对话，识别了4,700多个带有人类偏好的多轮样本，并开发了BigCodeReward和AutoCodeArena两个基准来系统评估LLM的代码能力。


<details>
  <summary>Details</summary>
Motivation: 在代码领域，手动评估LLM生成内容的质量极具挑战性，需要理解冗长的原始代码并模拟代码执行。现有的人类评估平台缺乏对代码执行环境的支持。

Method: 基于Chatbot Arena构建BigCodeArena平台，提供全面的实时执行环境，允许人类与代码执行过程和结果交互。收集了10个LLM在10种语言和8种执行环境下的14,000多个代码对话。

Result: 识别了4,700多个带有人类偏好的多轮样本。BigCodeReward评估显示，当执行结果可用时，大多数LLM在判断编码偏好方面表现优异。AutoCodeArena评估表明，GPT-5、Claude-Sonnet-4和Claude-Opus-4等专有LLM在代码生成性能方面领先。

Conclusion: BigCodeArena为代码生成评估提供了有效的平台，通过执行环境增强了人类评估的可靠性。专有LLM在代码生成方面仍保持领先地位，而自动评估基准AutoCodeArena为无需人类参与的代码质量评估提供了可行方案。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [26] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 使用差分进化算法优化DynaMOSA和MIO多目标搜索算法的超参数，相比网格搜索更高效，能显著提升测试套件的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 搜索式测试生成算法有众多配置选项，用户通常使用默认值，但这些默认值可能不是最优的。超参数调优可以找到更好的参数值，但传统方法资源消耗大。

Method: 采用差分进化算法作为元启发式搜索方法，在Pynguin框架中调优DynaMOSA和MIO算法的超参数。

Result: 调优后的DynaMOSA算法能显著提升测试套件的覆盖率，且差分进化比基础网格搜索更高效。

Conclusion: 差分进化是有效的超参数调优方法，能显著改善测试生成算法的性能。

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [27] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 开发了PyMigTool，一个结合LLM、静态分析和动态分析的端到端Python库迁移工具，能够自动迁移任意功能相似的Python库对。


<details>
  <summary>Details</summary>
Motivation: 手动库迁移耗时且易错，现有自动化技术大多停留在API映射阶段或支持有限。需要开发一个能够处理任意Python库对的完整迁移解决方案。

Method: 使用LLM作为主要迁移引擎，结合静态分析和动态分析进行后处理，开发了PyMigTool命令行应用程序。

Result: 在717个真实Python应用上评估，PyMigTool能够完全正确迁移32%的案例，剩余案例中超过一半项目只有14%的迁移相关更改需要开发者修复。

Conclusion: LLM能够有效执行库迁移，结合后处理步骤可以进一步提升性能，PyMigTool为Python库迁移提供了实用的自动化解决方案。

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [28] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: McMining任务旨在从学生代码样本中挖掘编程误解，研究者构建了包含误解和代码样本的基准数据集，并展示了LLM模型能有效发现学生代码中的误解。


<details>
  <summary>Details</summary>
Motivation: 学生在学习编程时经常产生对编程概念的误解，这不仅会导致bug和低效代码，还会阻碍相关概念的学习。

Method: 开发了可扩展的误解基准数据集，引入了两种基于LLM的McMiner方法，使用Gemini、Claude和GPT系列模型进行广泛评估。

Result: Gemini、Claude和GPT系列模型在发现学生代码中的误解方面表现有效。

Conclusion: LLM模型能够有效识别学生编程中的误解，为编程教育提供了新的分析工具。

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [29] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: 游戏开发者花费36.6%时间检查游戏工件，35.1%时间本地复现bug，主要调试工具有屏幕控制台、调试绘制、调试相机等


<details>
  <summary>Details</summary>
Motivation: 传统软件调试技术不完全适用于视频游戏开发，需要专门针对游戏特性的调试方法

Method: 记录20名资深游戏开发者的调试会话，分析崩溃、对象行为和对象持久性等关键bug的调试过程

Result: 识别出阻碍bug解决的调试活动瓶颈，发现技术角色在调试中处于核心地位，不同学科在调试过程中协作方式

Conclusion: 游戏开发需要专门的调试技术，开发者大量时间用于检查游戏工件和复现bug，技术角色在调试过程中发挥关键作用

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [30] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 提出一种基于LLM的文件路径检索方法，通过代码感知策略生成训练数据，在Python项目中实现高精度的文件路径预测。


<details>
  <summary>Details</summary>
Motivation: 传统代码搜索方法缺乏语义上下文和跨文件链接理解，而大语言模型虽然理解自然语言但缺乏仓库特定细节，需要结合两者优势。

Method: 使用QLoRA和Unsloth优化对Qwen3-8B进行微调，通过六种基于AST结构和仓库内容的代码感知策略生成训练数据，直接根据自然语言查询预测相关文件路径。

Result: 在Python项目中达到91%精确匹配和93%召回率，在PyTorch等大型代码库中达到59%召回率，明显优于单策略训练。

Conclusion: 多级代码信号帮助LLM推理跨文件上下文，该方法展示了在大型代码库中的可扩展性，未来可与基于LLM的代码智能集成。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [31] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 将大型软件仓库转换为向量化知识图谱的系统，通过捕获语义关系实现仓库开发的自动化


<details>
  <summary>Details</summary>
Motivation: 解决大型软件仓库难以理解和维护的问题，通过结构化表示来提升开发效率

Method: 构建包含语法关系和LLM生成摘要的知识图谱，结合语义检索和图感知扩展的混合检索管道

Result: 系统能够自动分析仓库结构，提供语义关系和开发辅助

Conclusion: 该方法为软件仓库开发提供了有效的自动化和理解工具

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [32] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: SEER框架在软件需求工程阶段解决可持续性问题，通过识别、评估和优化可持续性需求，使用大语言模型和智能RAG方法实现。


<details>
  <summary>Details</summary>
Motivation: 现有可持续性方法多为高层指导，实施耗时且依赖团队适应性，而可持续性评估应从需求工程阶段开始。

Method: 三阶段框架：从通用分类中识别相关可持续性需求，评估系统需求的可持续性，优化不满足可持续性需求的系统需求，使用大语言模型和智能RAG方法实现。

Result: 在四个不同领域的软件项目上实验，使用Gemini 2.5推理模型，证明该方法能准确识别跨领域的广泛可持续性问题。

Conclusion: SEER框架在软件开发早期阶段有效处理可持续性问题，展示了在不同领域中识别可持续性问题的能力。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [33] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 该研究通过系统文献综述构建了一个全面的可持续性需求分类法，涵盖环境、技术、社会和经济四个维度，并提供了定义、指标和相关性分析。


<details>
  <summary>Details</summary>
Motivation: 现有可持续性需求研究存在碎片化、局限于特定维度或应用领域的问题，缺乏统一的分类体系，需要为软件工程社区提供系统化的参考框架。

Method: 采用系统文献综述方法，从最新研究中提取和组织可持续性需求，构建跨四个维度的分类法，并分析各维度间的协同与冲突关系。

Result: 开发了一个包含四个维度的可持续性需求综合分类法，为每个类别提供明确定义、相关指标和度量方法，并建立了相关性矩阵展示维度间的相互影响。

Conclusion: 该分类法为软件开发者和研究人员提供了系统化参考，有助于在可持续软件开发中有效制定、管理和协调需求间的权衡关系。

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [34] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 提出新的基准测试框架，通过分析开发者与聊天式编码助手的交互模式，将现有正式基准转换为更贴近真实场景的用户查询，发现现有基准显著高估了AI代理在真实bug修复场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于GitHub问题的软件工程代理评估基准无法准确反映开发者在IDE中与聊天式编码助手的真实交互方式，导致对代理实际能力的高估。

Method: 开发灵活的基准转换框架，通过分析流行聊天式代理的交互遥测数据，将正式GitHub问题描述转换为现实用户风格的查询，应用于SWE-Bench Verified等多个基准。

Result: 现有基准显著高估了代理能力，在公共基准上某些模型被高估超过50%，内部基准上被高估约10-16%。

Conclusion: 这项工作通过基准变异技术为评估交互式聊天软件工程代理建立了新范式，揭示了现有评估方法的局限性。

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [35] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 提出了一种零样本代码翻译方法，通过标识符替换来提升长代码翻译的效率和准确性


<details>
  <summary>Details</summary>
Motivation: LLMs在处理超出上下文窗口的长源代码时翻译不准确，需要解决长代码翻译的效率和成本问题

Method: 使用标识符替换技术，将用户给定的长标识符替换为通用占位符，让LLM专注于代码逻辑结构

Result: 减少了token数量和内存使用，保持了语法和层次结构信息，提高了翻译效率

Conclusion: 该方法能够有效提升长代码翻译的效率和成本效益

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [36] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 对131名软件从业者的调查显示，LLMs主要用于编码任务，能提高生产力但存在输出不准确、上下文理解有限等风险，开发者普遍将其视为辅助工具而非独立解决方案。


<details>
  <summary>Details</summary>
Motivation: 了解LLMs在实际软件开发中的使用情况以及专业人士对其优势和局限性的看法，填补对LLM实际应用认知的空白。

Method: 通过对全球131名软件从业者进行调查，收集关于LLM使用情况的实证数据。

Result: LLMs被用于多种编码任务，带来生产力提升、认知负担减轻和学习加速等益处，但也存在输出不准确、上下文理解有限和伦理风险等问题。

Conclusion: 开发者普遍将LLMs视为辅助工具而非独立解决方案，体现了谨慎而实用的集成态度，为未来研究和负责任使用提供了重要参考。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [37] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 本文提出了一种称为"文学化追踪"的程序文档范式，通过带注释的具体执行追踪来解释软件系统，并开发了TReX工具来创建交互式、可视化的文学化追踪。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统日益庞大复杂，系统专家需要向新手解释程序工作原理。现有代码注释缺乏全局上下文，设计文档缺乏与代码的具体连接。

Method: 开发了TReX工具，用于创建交互式、可视化且保证与程序语义一致的文学化追踪文档。

Result: 已成功使用TReX为Linux内核、Git源代码控制系统和GCC编译器等大型系统软件组件编写文学化追踪文档。

Conclusion: 文学化追踪通过具体执行追踪补充了传统文档方法的不足，为程序理解提供了更有效的途径。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [38] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: PynguinML通过从API文档中提取约束条件，改进Pynguin测试生成器，为机器学习API生成合规输入，显著提高代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 机器学习库如PyTorch和TensorFlow的API有严格的输入约束，现有测试工具如Pynguin不了解这些约束，导致测试失败和代码覆盖率低。

Method: 从官方API文档中提取约束条件，改进Pynguin测试生成器，使其能够生成符合ML API约束的输入数据。

Result: 在PyTorch和TensorFlow的165个模块上评估，PynguinML相比Pynguin显著提高测试效果，代码覆盖率最高提升63.9%。

Conclusion: 利用API文档中的约束条件可以显著提高机器学习库测试生成器的有效性。

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [39] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: 本文提出了患者医疗数字孪生（PMDT）框架，这是一个基于本体的患者模型，整合了多模态健康数据，支持语义互操作性和隐私保护的慢性病管理。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生应用多为器官特异性或局限于孤立数据类型，缺乏统一且隐私保护的框架来支持个性化慢性病护理。

Method: 采用OWL 2.0实现的本体驱动框架，围绕模块化蓝图构建，包括患者、疾病诊断、治疗随访、轨迹、安全、路径和不良事件等模块，通过专家工作坊、问卷和真实世界免疫治疗患者试点研究进行迭代验证。

Result: 评估确认了本体覆盖度、推理正确性、可用性和GDPR合规性，PMDT能够统一异构数据，操作能力问题，支持描述性、预测性和规范性分析。

Conclusion: PMDT通过弥合数据碎片化和语义标准化差距，为下一代数字健康生态系统提供了经过验证的基础，推动慢性病护理向主动、持续优化和公平管理转变。

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [40] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: 提出基于模型驱动工程(MDE)的医疗AI框架，使用医学互操作性语言(MILA)和联邦学习架构，在多中心癌症免疫治疗研究中实现高精度预测并保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI应用中面临的数据碎片化、隐私法规严格和系统构建复杂等挑战，促进AI在医疗领域的实际应用。

Method: 开发基于形式化元模型、领域特定语言(DSL)和自动化转换的MDE框架，核心是图形化DSL语言MILA，结合联邦学习架构实现跨机构协作。

Result: 在多中心癌症免疫治疗研究中，生成的学习管道在关键任务中达到98.5%和98.3%的准确率，同时显著减少手动编码工作量。

Conclusion: 模型驱动工程原则（元建模、语义集成和自动化代码生成）为实现可互操作、可复现和可信赖的数字健康平台提供了实用路径。

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [41] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: 提出TIT方法，通过树结构指令调优解决LLM代码翻译中的语法混淆和语义对齐问题，显著提升翻译成功率


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码翻译方法存在两个关键局限：对语言特定特征敏感导致语法混淆，以及缺乏细粒度语义对齐导致语义偏差

Method: TIT包含三个模块：语法信息表示模块集成语言无关的语法特征，细粒度并行数据增强模块通过语句级分割和对齐生成高质量数据，双阶段树指令调优模块减轻LLM上下文处理负担

Result: 实验结果表明该方法在多个LLM中显著优于现有方法，代码翻译成功率提高1.22-1.75倍，同时明显减少语法混淆

Conclusion: TIT方法通过树结构指令调优有效解决了LLM代码翻译中的语法混淆和语义对齐问题，为代码翻译任务提供了新的解决方案

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [42] [Wireless Datasets for Aerial Networks](https://arxiv.org/abs/2510.08752)
*Amir Hossein Fahim Raouf,Donggu Lee,Mushfiqur Rahman,Saad Masrur,Gautham Reddy,Cole Dickerson,Md Sharif Hossen,Sergio Vargas Villar,Anıl Gürses,Simran Singh,Sung Joon Maeng,Martins Ezuma,Christopher Roberts,Mohamed Rabeek Sarbudeen,Thomas J. Zajkowski,Magreth Mushi,Ozgur Ozdemir,Ram Asokan,Ismail Guvenc,Mihail L. Sichitiu,Rudra Dutta*

Main category: cs.NI

TL;DR: 本文对AERPAW平台收集的公开无人机无线数据集进行了全面调查，包括蜂窝网络原始I/Q样本、频谱测量、飞行基站、5G网络等数据，旨在指导社区有效利用这些数据集验证传播模型和开发机器学习算法。


<details>
  <summary>Details</summary>
Motivation: 无人机集成到5G-Advanced和未来6G网络带来了无线连接的革命性机遇，但这些空中网络的有效设计和优化严重依赖于高质量的经验数据。

Method: 调查和分析了AERPAW平台收集的多种公开无线数据集，包括硬件软件配置、数据格式说明，并提供代表性结果和数据处理脚本。

Result: 提供了包括蜂窝网络原始I/Q样本、不同高度频谱测量、飞行4G基站、5G-NSA网络、LoRaWAN网络、RF传感器网络定位、无线传播数据等多种数据集，确保结果完全可复现。

Conclusion: 通过提供全面的数据集调查和可复现性支持，指导研究社区有效利用这些数据验证传播模型、开发机器学习算法，并推动下一代空中无线系统的发展。

Abstract: The integration of unmanned aerial vehicles (UAVs) into 5G-Advanced and
future 6G networks presents a transformative opportunity for wireless
connectivity, enabling agile deployment and improved LoS communications.
However, the effective design and optimization of these aerial networks depend
critically on high-quality, empirical data. This paper provides a comprehensive
survey of publicly available wireless datasets collected from an airborne
platform called Aerial Experimentation and Research Platform on Advanced
Wireless (AERPAW). We highlight the unique challenges associated with
generating reproducible aerial wireless datasets, and review the existing
related works in the literature. Subsequently, for each dataset considered, we
explain the hardware and software used, present the dataset format, provide
representative results, and discuss how these datasets can be used to conduct
additional research. The specific aerial wireless datasets presented include
raw I/Q samples from a cellular network over different UAV trajectories,
spectrum measurements at different altitudes, flying 4G base station (BS), a
5G-NSA Ericsson network, a LoRaWAN network, an radio frequency (RF) sensor
network for source localization, wireless propagation data for various
scenarios, and comparison of ray tracing and real-world propagation scenarios.
References to all datasets and post-processing scripts are provided to enable
full reproducibility of the results. Ultimately, we aim to guide the community
toward effective dataset utilization for validating propagation models,
developing machine learning algorithms, and advancing the next generation of
aerial wireless systems.

</details>


### [43] [Prioritizing Latency with Profit: A DRL-Based Admission Control for 5G Network Slices](https://arxiv.org/abs/2510.08769)
*Proggya Chakraborty,Aaquib Asrar,Jayasree Sengupta,Sipra Das Bit*

Main category: cs.NI

TL;DR: 提出DePSAC方案，通过延迟感知奖励函数和Boltzmann探索，在5G网络切片中实现更好的QoS-利润权衡


<details>
  <summary>Details</summary>
Motivation: 现有DRL框架主要关注利润优化，未充分考虑服务延迟，可能导致延迟敏感切片的QoS违规；同时epsilon-greedy探索导致收敛不稳定

Method: 基于DRL的延迟和利润感知切片准入控制方案，包含延迟感知奖励函数和Boltzmann探索策略

Result: 在模拟5G核心网络中，DePSAC在整体利润、URLLC切片延迟降低、接受率和资源消耗方面均优于DSARA基线

Conclusion: DePSAC方案在5G网络切片场景中有效实现了更好的QoS-利润权衡

Abstract: 5G networks enable diverse services such as eMBB, URLLC, and mMTC through
network slicing, necessitating intelligent admission control and resource
allocation to meet stringent QoS requirements while maximizing Network Service
Provider (NSP) profits. However, existing Deep Reinforcement Learning (DRL)
frameworks focus primarily on profit optimization without explicitly accounting
for service delay, potentially leading to QoS violations for latency-sensitive
slices. Moreover, commonly used epsilon-greedy exploration of DRL often results
in unstable convergence and suboptimal policy learning. To address these gaps,
we propose DePSAC -- a Delay and Profit-aware Slice Admission Control scheme.
Our DRL-based approach incorporates a delay-aware reward function, where
penalties due to service delay incentivize the prioritization of
latency-critical slices such as URLLC. Additionally, we employ Boltzmann
exploration to achieve smoother and faster convergence. We implement and
evaluate DePSAC on a simulated 5G core network substrate with realistic Network
Slice Request (NSLR) arrival patterns. Experimental results demonstrate that
our method outperforms the DSARA baseline in terms of overall profit, reduced
URLLC slice delays, improved acceptance rates, and improved resource
consumption. These findings validate the effectiveness of the proposed DePSAC
in achieving better QoS-profit trade-offs for practical 5G network slicing
scenarios.

</details>


### [44] [Characterizing 5G User Throughput via Uncertainty Modeling and Crowdsourced Measurements](https://arxiv.org/abs/2510.09239)
*Javier Albert-Smet,Zoraida Frias,Luis Mendo,Sergio Melones,Eduardo Yraola*

Main category: cs.NI

TL;DR: 提出了一种基于众包测量的不确定性感知和可解释的下行用户吞吐量估计方法，解决了5G网络中应用层吞吐量评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 5G无线接入网络容量提升使得连接瓶颈转向网络深层，传统方法成本高且无法捕捉端到端服务质量及其变异性。

Method: 利用大规模众包测量数据（包括端到端、无线、上下文和网络部署特征），应用NGBoost模型输出点估计和校准置信区间。

Result: 改进了4G方法（R²提升8.7%），首次为5G众包数据集提供基准，分析显示吞吐量瓶颈从无线接入网络转向传输和服务层。

Conclusion: 端到端指标比无线相关特征更重要，验证了5G网络中瓶颈转移的趋势，为网络性能评估提供了新方法。

Abstract: Characterizing application-layer user throughput in next-generation networks
is increasingly challenging as the higher capacity of the 5G Radio Access
Network (RAN) shifts connectivity bottlenecks towards deeper parts of the
network. Traditional methods, such as drive tests and operator equipment
counters, are costly, limited, or fail to capture end-to-end (E2E) Quality of
Service (QoS) and its variability. In this work, we leverage large-scale
crowdsourced measurements-including E2E, radio, contextual and network
deployment features collected by the user equipment (UE)-to propose an
uncertainty-aware and explainable approach for downlink user throughput
estimation. We first validate prior 4G methods, improving R^2 by 8.7%, and then
extend them to 5G NSA and 5G SA, providing the first benchmarks for 5G
crowdsourced datasets. To address the variability of throughput, we apply
NGBoost, a model that outputs both point estimates and calibrated confidence
intervals, representing its first use in the field of computer communications.
Finally, we use the proposed model to analyze the evolution from 4G to 5G SA,
and show that throughput bottlenecks move from the RAN to transport and service
layers, as seen by E2E metrics gaining importance over radio-related features.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Energy-Driven Steering: Reducing False Refusals in Large Language Models](https://arxiv.org/abs/2510.08646)
*Eric Hanchen Jiang,Weixuan Ou,Run Liu,Shengyuan Pang,Guancheng Wan,Ranjie Duan,Wei Dong,Kai-Wei Chang,XiaoFeng Wang,Ying Nian Wu,Xinfeng Li*

Main category: cs.LG

TL;DR: 提出了一种无需微调的推理时干预框架EDS，通过能量模型动态引导LLM隐藏状态，在保持安全性的同时显著降低错误拒绝率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全对齐技术往往过度谨慎，导致对良性提示也错误拒绝。需要在增强安全性的同时减少错误拒绝。

Method: 训练轻量级能量模型(EBM)，为不良状态分配高能量，为理想状态分配低能量。推理时通过能量梯度动态引导LLM隐藏状态到低能量区域。

Result: 在ORB-H基准上，合规率从57.3%提升到82.6%，同时保持基线安全性能。显著降低了错误拒绝率。

Conclusion: EDS提供了一种有效范式，使LLM既能实现低错误拒绝率又能保持高安全性，将行为控制与核心知识解耦。

Abstract: Safety alignment of large language models (LLMs) faces a key challenge:
current alignment techniques often only focus on improving safety against
harmful prompts, causing LLMs to become over-cautious and refuse to respond to
benign prompts. Therefore, a key objective of safe alignment is to enhance
safety while simultaneously reducing false refusals. In this paper, we
introduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework
designed to resolve this challenge through dynamic, inference-time
intervention. We trained a lightweight, external Energy-Based Model (EBM) to
assign high energy to undesirable (false refusal or jailbreak) states and low
energy to desirable (helpful response or safe reject) ones. During inference,
EBM maps the LLM's internal activations to an "energy landscape". We use the
gradient of the energy function to dynamically steer the LLM's hidden states to
low energy regions, correcting the model to generate a desirable response in
real-time without modifying its weights. This method decouples behavioral
control from the model's core knowledge, offering a flexible solution with
minimal computational overhead. Extensive experiments across a wide range of
models show our method successfully achieves this objective: it substantially
lowers false refusal rates. For example, raising compliance on the ORB-H
benchmark from 57.3% to 82.6% while maintaining the baseline safety
performance. Our work presents an effective paradigm for building LLMs that
achieve both low false refusal rates and high safety.

</details>


### [46] [Velocity and Density-Aware RRI Analysis and Optimization for AoI Minimization in IoV SPS](https://arxiv.org/abs/2510.08911)
*Maoxin Ji,Tong Wang,Qiong Wu,Pingyi Fan,Nan Cheng,Wen Chen*

Main category: cs.LG

TL;DR: 提出基于LLM和DDPG的双路径优化方法，解决车联网中SPS调度导致的AoI恶化问题，通过结合深度强化学习和语言模型的优势来优化资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决车联网中半持续调度(SPS)因数据包碰撞和车辆速度相关信道不确定性导致的AoI恶化问题。

Method: 建立受车辆速度、密度和资源预留间隔影响的AoI计算模型，设计双路径优化方案：DDPG基于状态空间和奖励函数进行优化，LLM利用上下文学习生成最优参数配置。

Result: 实验结果表明，LLM在积累少量样本后无需模型训练即可显著降低AoI，而DDPG方法在训练后能获得更稳定的性能。

Conclusion: LLM和DDPG的结合为车联网AoI优化提供了有效的双路径解决方案，LLM适合快速部署，DDPG适合长期稳定运行。

Abstract: Addressing the problem of Age of Information (AoI) deterioration caused by
packet collisions and vehicle speed-related channel uncertainties in
Semi-Persistent Scheduling (SPS) for the Internet of Vehicles (IoV), this
letter proposes an optimization approach based on Large Language Models (LLM)
and Deep Deterministic Policy Gradient (DDPG). First, an AoI calculation model
influenced by vehicle speed, vehicle density, and Resource Reservation Interval
(RRI) is established, followed by the design of a dual-path optimization
scheme. The DDPG is guided by the state space and reward function, while the
LLM leverages contextual learning to generate optimal parameter configurations.
Experimental results demonstrate that LLM can significantly reduce AoI after
accumulating a small number of exemplars without requiring model training,
whereas the DDPG method achieves more stable performance after training.

</details>


### [47] [Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity](https://arxiv.org/abs/2510.08648)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.LG

TL;DR: WILSON是一个后处理诊断工具包，通过检查内部表示的循环和重排序来检测大语言模型中的不变性失效问题，帮助在部署中预防故障并批准安全优化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中存在不变性失效问题：RAG输出随段落重排序而翻转、微调侵蚀预训练学到的恒定性、辩论或思维链提示的路径依赖性等，这些故障违反预期不变性、破坏持续集成，并迫使团队在安全与速度之间权衡。

Method: WILSON结合无逆曲率映射（使用JVP和Hutchinson探针计算）和激活级交换子来标记重排序风险，这些信号计算成本低、适用于标准Transformer模型，并导出为阈值和CSV工件供编排器使用。

Result: WILSON能够实现具体行动：保护RAG免受顺序效应影响、捕获微调回归、稳定辩论路径和长多轮上下文、在部署中控制融合或重排序操作。

Conclusion: WILSON帮助预测故障并批准安全优化，使可靠性和吞吐量能够共同提升，而无需改变模型架构或训练过程。

Abstract: Large language models can change answers under harmless edits that matter in
practice: RAG outputs flip when passages are reordered, fine-tuning erodes
invariances learned at pretraining, debate or chain-of-thought prompts take
path-dependent routes, and compiler fusion or reordering perturbs logits near
decision boundaries. These failures violate intended invariances, break
continuous integration, and force teams to trade safety for speed. The effects
are small yet distributed across layers and positions, sensitive to context
length and evaluation order, and costly to repair with retraining or formal
verification. We present WILSON, a minimal post-hoc diagnostic suite that
converts simple loop and reordering checks on internal representations into
system signals. WILSON combines an inverse-free curvature map over positions
and layers, computed with JVPs and Hutchinson probes, with activation-level
commutators that flag reorder risk. Signals are cheap to compute,
model-agnostic for standard Transformers, and exported as thresholds and CSV
artifacts for orchestrators. This enables concrete actions: guard RAG against
order effects, catch fine-tuning regressions, stabilize debate pathways and
long multi-turn contexts, and gate fusions or reorders in deployment. In short,
WILSON helps anticipate failures and approve safe optimizations so reliability
and throughput can improve together without changing model architecture or
training.

</details>


### [48] [Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis](https://arxiv.org/abs/2510.08655)
*Premt Cara,Kamilia Zaripova,David Bani-Harouni,Nassir Navab,Azade Farshad*

Main category: cs.LG

TL;DR: RareNet是一个基于子图的图神经网络，仅需患者表型数据即可识别最可能的致病基因，为资源有限地区提供可访问的基因诊断工具。


<details>
  <summary>Details</summary>
Motivation: 罕见遗传病诊断面临三大挑战：患者数据不足、全基因组测序难以获取、致病基因数量庞大。这些问题导致诊断周期长、治疗不当，特别是在资源有限地区影响更为严重。

Method: 提出RareNet，一种基于子图的图神经网络方法，仅使用患者表型数据来识别致病基因，并检索相关的患者子图进行针对性临床调查。该方法可作为独立工具或与其他候选基因优先排序方法结合使用。

Result: 在两个生物医学数据集上的综合评估显示，RareNet在致病基因预测方面具有竞争力和鲁棒性，与其他框架集成时能显著提升性能。

Conclusion: RareNet仅需表型数据即可实现先进的基因分析，为缺乏先进基因组基础设施的服务不足人群提供了特别价值，实现了基因诊断的民主化。

Abstract: Rare genetic disease diagnosis faces critical challenges: insufficient
patient data, inaccessible full genome sequencing, and the immense number of
possible causative genes. These limitations cause prolonged diagnostic
journeys, inappropriate treatments, and critical delays, disproportionately
affecting patients in resource-limited settings where diagnostic tools are
scarce. We propose RareNet, a subgraph-based Graph Neural Network that requires
only patient phenotypes to identify the most likely causal gene and retrieve
focused patient subgraphs for targeted clinical investigation. RareNet can
function as a standalone method or serve as a pre-processing or post-processing
filter for other candidate gene prioritization methods, consistently enhancing
their performance while potentially enabling explainable insights. Through
comprehensive evaluation on two biomedical datasets, we demonstrate competitive
and robust causal gene prediction and significant performance gains when
integrated with other frameworks. By requiring only phenotypic data, which is
readily available in any clinical setting, RareNet democratizes access to
sophisticated genetic analysis, offering particular value for underserved
populations lacking advanced genomic infrastructure.

</details>


### [49] [RFOD: Random Forest-based Outlier Detection for Tabular Data](https://arxiv.org/abs/2510.08747)
*Yihao Ang,Peicheng Yao,Yifan Bao,Yushuo Feng,Qiang Huang,Anthony K. H. Tung,Zhiyong Huang*

Main category: cs.LG

TL;DR: RFOD是一个基于随机森林的表格数据异常检测框架，通过特征级条件重构处理混合类型数据，结合调整后的Gower距离和不确定性加权平均，在15个真实数据集上表现出优于现有方法的检测精度、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在处理混合类型表格数据时存在语义信息丢失和缺乏可解释性的问题，特别是在网络安全、金融欺诈检测和医疗保健等高风险领域，异常可能导致严重的运营和经济影响。

Method: RFOD将异常检测重新定义为特征级条件重构问题，为每个特征训练专门的随机森林模型，使用调整后的Gower距离进行单元格级评分，并通过不确定性加权平均将单元格级分数聚合成行级异常分数。

Result: 在15个真实世界数据集上的广泛实验表明，RFOD在检测精度方面持续优于最先进的基线方法，同时对混合类型表格数据具有优越的鲁棒性、可扩展性和可解释性。

Conclusion: RFOD框架成功解决了混合类型表格数据异常检测中的关键挑战，提供了一种既准确又可解释的解决方案，特别适用于需要数据完整性的高风险应用领域。

Abstract: Outlier detection in tabular data is crucial for safeguarding data integrity
in high-stakes domains such as cybersecurity, financial fraud detection, and
healthcare, where anomalies can cause serious operational and economic impacts.
Despite advances in both data mining and deep learning, many existing methods
struggle with mixed-type tabular data, often relying on encoding schemes that
lose important semantic information. Moreover, they frequently lack
interpretability, offering little insight into which specific values cause
anomalies. To overcome these challenges, we introduce \textsf{\textbf{RFOD}}, a
novel \textsf{\textbf{R}}andom \textsf{\textbf{F}}orest-based
\textsf{\textbf{O}}utlier \textsf{\textbf{D}}etection framework tailored for
tabular data. Rather than modeling a global joint distribution, \textsf{RFOD}
reframes anomaly detection as a feature-wise conditional reconstruction
problem, training dedicated random forests for each feature conditioned on the
others. This design robustly handles heterogeneous data types while preserving
the semantic integrity of categorical features. To further enable precise and
interpretable detection, \textsf{RFOD} combines Adjusted Gower's Distance (AGD)
for cell-level scoring, which adapts to skewed numerical data and accounts for
categorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate
cell-level scores into robust row-level anomaly scores. Extensive experiments
on 15 real-world datasets demonstrate that \textsf{RFOD} consistently
outperforms state-of-the-art baselines in detection accuracy while offering
superior robustness, scalability, and interpretability for mixed-type tabular
data.

</details>


### [50] [Inner-Instance Normalization for Time Series Forecasting](https://arxiv.org/abs/2510.08657)
*Zipo Jibao,Yingyi Fu,Xinyang Chen,Guoting Chen*

Main category: cs.LG

TL;DR: 提出两种点级方法LD和LCD来处理时间序列预测中的实例内分布偏移问题，通过在不同时间步使用不同参数来消除内部不一致性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列具有复杂的非平稳特性，会导致分布偏移，现有实例归一化技术无法处理实例内的分布偏移，导致性能不佳。

Method: 提出两种点级方法：学习分布(LD)通过在不同时间步使用不同参数拟合输入输出的内部分布；学习条件分布(LCD)使用神经网络预测输出的缩放系数。

Result: 在多个公共基准测试中与各种骨干模型结合评估，证明了点级范式的有效性。

Conclusion: 点级方法能有效处理时间序列预测中的实例内分布偏移问题，提升模型性能。

Abstract: Real-world time series are influenced by numerous factors and exhibit complex
non-stationary characteristics. Non-stationarity can lead to distribution
shifts, where the statistical properties of time series change over time,
negatively impacting model performance. Several instance normalization
techniques have been proposed to address distribution shifts in time series
forecasting. However, existing methods fail to account for shifts within
individual instances, leading to suboptimal performance. To tackle
inner-instance distribution shifts, we propose two novel point-level methods:
Learning Distribution (LD) and Learning Conditional Distribution (LCD). LD
eliminates internal discrepancies by fitting the internal distribution of input
and output with different parameters at different time steps, while LCD
utilizes neural networks to predict scaling coefficients of the output. We
evaluate the performance of the two methods with various backbone models across
public benchmarks and demonstrate the effectiveness of the point-level paradigm
through comparative experiments.

</details>


### [51] [Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction](https://arxiv.org/abs/2510.08839)
*Motahare Mounesan,Sourya Saha,Houchao Gan,Md. Nurul Absur,Saptarshi Debroy*

Main category: cs.LG

TL;DR: 提出基于强化学习的边缘资源管理框架，用于在资源受限和易受干扰的环境中实现可靠的多视角3D重建。


<details>
  <summary>Details</summary>
Motivation: 边缘环境中的动态和不可预测资源可用性（如图像质量下降、网络不稳定、服务器负载波动）对3D重建管道的可靠性构成挑战，特别是在消防救援等关键应用中。

Method: 采用两个协作的Q学习代理，一个用于相机选择，一个用于服务器选择，两者完全在线运行，通过与边缘环境交互学习策略。在分布式测试平台上实现，模拟智能城市边缘基础设施。

Result: 该框架通过有效平衡端到端延迟和重建质量，提高了动态环境中的应用程序可靠性。

Conclusion: 强化学习框架能够在资源受限和易受干扰的边缘环境中实现高质量的实时3D重建，为关键应用提供可靠支持。

Abstract: Real-time multi-view 3D reconstruction is a mission-critical application for
key edge-native use cases, such as fire rescue, where timely and accurate 3D
scene modeling enables situational awareness and informed decision-making.
However, the dynamic and unpredictable nature of edge resource availability
introduces disruptions, such as degraded image quality, unstable network links,
and fluctuating server loads, which challenge the reliability of the
reconstruction pipeline. In this work, we present a reinforcement learning
(RL)-based edge resource management framework for reliable 3D reconstruction to
ensure high quality reconstruction within a reasonable amount of time, despite
the system operating under a resource-constrained and disruption-prone
environment. In particular, the framework adopts two cooperative Q-learning
agents, one for camera selection and one for server selection, both of which
operate entirely online, learning policies through interactions with the edge
environment. To support learning under realistic constraints and evaluate
system performance, we implement a distributed testbed comprising lab-hosted
end devices and FABRIC infrastructure-hosted edge servers to emulate smart city
edge infrastructure under realistic disruption scenarios. Results show that the
proposed framework improves application reliability by effectively balancing
end-to-end latency and reconstruction quality in dynamic environments.

</details>


### [52] [Provably Robust Adaptation for Language-Empowered Foundation Models](https://arxiv.org/abs/2510.08659)
*Yuni Lai,Xiaoyu Xue,Linghui Shen,Yulun Wu,Gaolei Li,Song Guo,Kai Zhou,Bin Xiao*

Main category: cs.LG

TL;DR: 提出了第一个针对语言赋能基础模型的可证明鲁棒少样本分类器LeFCert，通过双重修剪均值原型和自适应混合机制，在中毒攻击下提供可证明的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 现有的语言赋能基础模型在少样本学习中依赖开放环境中收集的小型任务特定支持数据集，容易受到中毒攻击，而现有防御方法缺乏形式化保证且对未知和自适应攻击脆弱。

Method: LeFCert整合文本和特征嵌入与自适应混合机制，提出双重修剪均值原型，推导分类分数的可证明上下界。还扩展了LeFCert-L（随机平滑）和LeFCert-C（集体认证）两个变体。

Result: 实验表明LeFCert实现了最先进的性能，在干净准确率和认证准确率方面均显著优于现有基线方法，且计算效率高。

Conclusion: LeFCert是第一个为语言赋能基础模型设计的可证明鲁棒少样本分类器，在保持高效计算的同时提供了强大的鲁棒性保证。

Abstract: Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP,
have transformed multimodal learning by aligning visual (or graph) features
with textual representations, enabling powerful downstream capabilities like
few-shot learning. However, the reliance on small, task-specific support
datasets collected in open environments exposes these models to poisoning
attacks, where adversaries manipulate the support samples to degrade
performance. Existing defenses rely on empirical strategies, which lack formal
guarantees and remain vulnerable to unseen and adaptive attacks. Certified
robustness offers provable guarantees but has been largely unexplored for
few-shot classifiers based on LeFMs. This study seeks to fill these critical
gaps by proposing the first provably robust few-shot classifier that is
tailored for LeFMs. We term our model Language-empowered Few-shot Certification
(\textbf{LeFCert}). It integrates both textual and feature embeddings with an
adaptive blending mechanism. To achieve provable robustness, we propose a
twofold trimmed mean prototype and derive provable upper and lower bounds for
classification scores, enabling certification under worst-case poisoning
scenarios. To further enhance the performance, we extend LeFCert with two
variants by considering a more realistic and tighter attack budget: LeFCert-L
incorporates randomized smoothing to provide Lipschitz continuity and derive
robustness under dual budget constraints, and LeFCert-C provides collective
certification for scenarios where attackers distribute a shared poisoning
budget across multiple samples. Experiments demonstrate that LeFCert achieves
state-of-the-art performance, significantly improving both clean and certified
accuracy compared to existing baselines. Despite its advanced robustness
mechanisms, LeFCert is computationally efficient, making it practical for
real-world applications.

</details>


### [53] [How Scale Breaks "Normalized Stress" and KL Divergence: Rethinking Quality Metrics](https://arxiv.org/abs/2510.08660)
*Kiran Smelser,Kaviru Gunaratne,Jacob Miller,Stephen Kobourov*

Main category: cs.LG

TL;DR: 论文分析了高维数据可视化中常用的质量指标（归一化应力和KL散度）对尺度变化的敏感性，并提出了一种简单方法使这些指标具有尺度不变性。


<details>
  <summary>Details</summary>
Motivation: 高维数据可视化中，二维散点图是常用方法，但评估这些投影的准确性具有挑战性。常用的质量指标如归一化应力和KL散度对均匀缩放敏感，即使缩放不会实质性改变投影的质量。

Method: 通过分析和实证研究缩放对应力和KL散度的影响，引入一种简单技术使这两个指标具有尺度不变性，并在小型基准测试中验证其有效性。

Result: 研究表明缩放会显著改变应力和KL散度的值，影响降维技术评估。提出的尺度不变方法在小基准测试中能准确捕捉预期行为。

Conclusion: 现有可视化质量指标对尺度变化敏感，提出的尺度不变方法能更准确地评估降维投影质量，避免缩放带来的误导性评估结果。

Abstract: Complex, high-dimensional data is ubiquitous across many scientific
disciplines, including machine learning, biology, and the social sciences. One
of the primary methods of visualizing these datasets is with two-dimensional
scatter plots that visually capture some properties of the data. Because
visually determining the accuracy of these plots is challenging, researchers
often use quality metrics to measure the projection's accuracy and faithfulness
to the original data. One of the most commonly employed metrics, normalized
stress, is sensitive to uniform scaling (stretching, shrinking) of the
projection, despite this act not meaningfully changing anything about the
projection. Another quality metric, the Kullback--Leibler (KL) divergence used
in the popular t-Distributed Stochastic Neighbor Embedding (t-SNE) technique,
is also susceptible to this scale sensitivity. We investigate the effect of
scaling on stress and KL divergence analytically and empirically by showing
just how much the values change and how this affects dimension reduction
technique evaluations. We introduce a simple technique to make both metrics
scale-invariant and show that it accurately captures expected behavior on a
small benchmark.

</details>


### [54] [Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction](https://arxiv.org/abs/2510.09159)
*Tianyi Chen,Mingcheng Zhu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 该论文提出了首个系统性基准来比较EHR表示方法，发现事件流模型表现最佳，并揭示了不同临床场景下特征选择策略的差异。


<details>
  <summary>Details</summary>
Motivation: 由于评估实践不一致，EHR患者数据的最佳表示方法仍不明确，需要标准化比较不同表示方法在临床预测中的表现。

Method: 构建统一基准，在MIMIC-IV和EHRSHOT数据集上比较三种EHR表示范式：多变量时间序列、事件流和文本事件流，使用Transformer、MLP、LSTM、Retain、CLMBR、计数模型和8-20B LLMs等模型，并分析基于数据缺失的特征剪枝影响。

Result: 事件流模型始终表现最强，预训练模型CLMBR在少样本场景下样本效率高，而简单计数模型在数据充足时具有竞争力。ICU任务中剪枝稀疏特征有益，而纵向任务中保留稀疏特征至关重要。

Conclusion: 通过统一可复现的流程，为基于临床环境和数据情况选择EHR表示提供了实用指导，强调了表示方法选择需适应具体临床场景。

Abstract: Electronic Health Records (EHRs) enable deep learning for clinical
predictions, but the optimal method for representing patient data remains
unclear due to inconsistent evaluation practices. We present the first
systematic benchmark to compare EHR representation methods, including
multivariate time-series, event streams, and textual event streams for LLMs.
This benchmark standardises data curation and evaluation across two distinct
clinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping)
and the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year
pancreatic cancer). For each paradigm, we evaluate appropriate modelling
families--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR
and count-based models for event streams, 8-20B LLMs for textual streams--and
analyse the impact of feature pruning based on data missingness. Our
experiments reveal that event stream models consistently deliver the strongest
performance. Pre-trained models like CLMBR are highly sample-efficient in
few-shot settings, though simpler count-based models can be competitive given
sufficient data. Furthermore, we find that feature selection strategies must be
adapted to the clinical setting: pruning sparse features improves ICU
predictions, while retaining them is critical for longitudinal tasks. Our
results, enabled by a unified and reproducible pipeline, provide practical
guidance for selecting EHR representations based on the clinical context and
data regime.

</details>


### [55] [CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting](https://arxiv.org/abs/2510.08661)
*Zipo Jibao,Yingyi Fu,Xinyang Chen,Guoting Chen*

Main category: cs.LG

TL;DR: 提出了CATS-Linear模型，通过分类辅助通道独立机制动态路由时间序列实例到专用预测器，结合趋势-季节解耦架构，在固定超参数下实现SOTA预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示线性模型能达到与复杂架构相当的预测性能，但增强线性模型的方法仍待探索。假设不同时间序列实例可能遵循异质线性映射，需要设计动态路由机制。

Method: 采用分类辅助通道独立(CACI)机制动态路由实例到专用预测器；重新设计趋势-季节分解架构，添加解耦-线性映射-重耦合框架处理趋势分量，使用复数域线性投影处理季节分量。

Result: 大量实验验证CATS-Linear在固定超参数下达到与超参数调优基线相当的最先进精度，同时在固定超参数对比中提供SOTA准确性。

Conclusion: CATS-Linear通过动态实例路由和重新设计的分解架构，证明了线性模型在时间序列预测中的强大潜力，无需复杂超参数调优即可实现竞争性性能。

Abstract: Recent research demonstrates that linear models achieve forecasting
performance competitive with complex architectures, yet methodologies for
enhancing linear models remain underexplored. Motivated by the hypothesis that
distinct time series instances may follow heterogeneous linear mappings, we
propose the Classification Auxiliary Trend-Seasonal Decoupling Linear Model
CATS-Linear, employing Classification Auxiliary Channel-Independence (CACI).
CACI dynamically routes instances to dedicated predictors via classification,
enabling supervised channel design. We further analyze the theoretical expected
risks of different channel settings. Additionally, we redesign the
trend-seasonal decomposition architecture by adding a decoupling -- linear
mapping -- recoupling framework for trend components and complex-domain linear
projections for seasonal components. Extensive experiments validate that
CATS-Linear with fixed hyperparameters achieves state-of-the-art accuracy
comparable to hyperparameter-tuned baselines while delivering SOTA accuracy
against fixed-hyperparameter counterparts.

</details>


### [56] [DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops](https://arxiv.org/abs/2510.08662)
*Pengcheng Deng,Kening Liu,Mengxi Zhou,Mingxi Li,Rui Yang,Chuzhe Cao,Maojun Wang,Zeyu Zhang*

Main category: cs.LG

TL;DR: DPCformer是一种结合卷积神经网络和自注意力机制的深度学习模型，用于基因组选择，在多种作物性状预测中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基因组选择方法在处理复杂性状和大数据集时预测精度不足，需要更有效的模型来建模复杂的基因型-表型关系。

Method: 使用8维one-hot编码处理SNP数据，按染色体排序，采用PMF算法进行特征选择，整合CNN和自注意力机制构建DPCformer模型。

Result: 在5种作物13个性状上测试，玉米性状准确率提升达2.92%，棉花纤维性状提升8.37%，番茄小样本场景PCC提升57.35%，鹰嘴豆产量相关性提升16.62%。

Conclusion: DPCformer在精度、小样本鲁棒性和可解释性方面表现优异，为精准育种和应对全球粮食安全挑战提供了有力工具。

Abstract: Genomic Selection (GS) uses whole-genome information to predict crop
phenotypes and accelerate breeding. Traditional GS methods, however, struggle
with prediction accuracy for complex traits and large datasets. We propose
DPCformer, a deep learning model integrating convolutional neural networks with
a self-attention mechanism to model complex genotype-phenotype relationships.
We applied DPCformer to 13 traits across five crops (maize, cotton, tomato,
rice, chickpea). Our approach uses an 8-dimensional one-hot encoding for SNP
data, ordered by chromosome, and employs the PMF algorithm for feature
selection. Evaluations show DPCformer outperforms existing methods. In maize
datasets, accuracy for traits like days to tasseling and plant height improved
by up to 2.92%. For cotton, accuracy gains for fiber traits reached 8.37%. On
small-sample tomato data, the Pearson Correlation Coefficient for a key trait
increased by up to 57.35%. In chickpea, the yield correlation was boosted by
16.62%. DPCformer demonstrates superior accuracy, robustness in small-sample
scenarios, and enhanced interpretability, providing a powerful tool for
precision breeding and addressing global food security challenges.

</details>


### [57] [FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching](https://arxiv.org/abs/2510.08669)
*Jiacheng Liu,Peiliang Cai,Qinming Zhou,Yuqi Lin,Deyang Kong,Benhao Huang,Yupei Pan,Haowen Xu,Chang Zou,Junshu Tang,Shikang Zheng,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文提出频率感知缓存(FreqCa)方法，通过分析扩散模型中不同频率分量的动态特性，分别处理低频和高频特征：直接重用相似的低频特征，使用二阶Hermite插值预测连续的高频特征，显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法假设相邻时间步特征相似或连续，但实际中这一假设并不总是成立。通过频域分析发现，低频分量决定图像结构但连续性差，高频分量细节丰富但相似性差，需要针对性处理。

Method: 1. 频域分析揭示不同频率分量的动态特性；2. 提出FreqCa方法：直接重用低频特征，使用二阶Hermite插值预测高频特征；3. 提出累积残差特征(CRF)缓存，减少99%内存占用。

Result: 在FLUX.1-dev、FLUX.1-Kontext-dev、Qwen-Image和Qwen-Image-Edit等数据集上的广泛实验验证了该方法在生成和编辑任务中的有效性。

Conclusion: 频率感知缓存方法通过针对性处理不同频率特征，有效解决了扩散变换器推理成本高的问题，在保持性能的同时显著提升了效率。

Abstract: The application of diffusion transformers is suffering from their significant
inference costs. Recently, feature caching has been proposed to solve this
problem by reusing features from previous timesteps, thereby skipping
computation in future timesteps. However, previous feature caching assumes that
features in adjacent timesteps are similar or continuous, which does not always
hold in all settings. To investigate this, this paper begins with an analysis
from the frequency domain, which reveal that different frequency bands in the
features of diffusion models exhibit different dynamics across timesteps.
Concretely, low-frequency components, which decide the structure of images,
exhibit higher similarity but poor continuity. In contrast, the high-frequency
bands, which decode the details of images, show significant continuity but poor
similarity. These interesting observations motivate us to propose
Frequency-aware Caching (FreqCa)
  which directly reuses features of low-frequency components based on their
similarity, while using a second-order Hermite interpolator to predict the
volatile high-frequency ones based on its continuity.
  Besides, we further propose to cache Cumulative Residual Feature (CRF)
instead of the features in all the layers, which reduces the memory footprint
of feature caching by 99%.
  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and
Qwen-Image-Edit demonstrate its effectiveness in both generation and editing.
Codes are available in the supplementary materials and will be released on
GitHub.

</details>


### [58] [Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting](https://arxiv.org/abs/2510.08696)
*Yunzhen Feng,Parag Jain,Anthony Hartshorn,Yaqi Duan,Julia Kempe*

Main category: cs.LG

TL;DR: 提出LENS方法，通过置信度加权惩罚机制利用负样本组，改进GRPO算法在RLVR中的效率问题


<details>
  <summary>Details</summary>
Motivation: GRPO在强化学习中浪费大量计算资源处理负样本组（无正确答案的组），这些组产生零优势而无梯度更新

Method: 从奖励建模的最大似然目标出发，推导出等价于修改后价值函数的策略梯度，为错误响应添加置信度加权惩罚

Result: 在MATH基准测试中，LENS方法在Llama-3.1-8B和Qwen-2.5-3B上持续优于GRPO基线，在困难项目上提升显著

Conclusion: LENS提供了一种原则性和实用的方法来"拯救"负样本组，提高了RLVR的效率和性能

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a standard
recipe for improving large language models (LLMs) on reasoning tasks, with
Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO
wastes substantial compute on negative groups: groups in which no sampled
response is correct yield zero advantage and thus no gradient. We ask whether
negative groups can be leveraged without extra supervision. Starting from a
maximum-likelihood (MLE) objective in reward modeling, we show that the MLE
gradient is equivalent to a policy gradient for a modified value function. This
value function adds a confidence-weighted penalty on incorrect responses,
imposing larger penalties on more confident mistakes. We refer to this as
\textbf{L}ikelihood \textbf{E}stimation with \textbf{N}egative \textbf{S}amples
(\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent
rewards to incorrect generations, making negative groups informative and
converting previously wasted samples into useful gradient updates. On the MATH
benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently
outperforms GRPO baseline, with significant gains on harder items. These
results demonstrate a principled and practical way to "rescue" negative groups,
improving efficiency and performance in RLVR.

</details>


### [59] [In-Context Learning for Non-Stationary MIMO Equalization](https://arxiv.org/abs/2510.08711)
*Jiachen Jiang,Zhen Qin,Zhihui Zhu*

Main category: cs.LG

TL;DR: 本文研究了基于上下文学习(ICL)的均衡器在非平稳时变信道均衡中的应用，提出了从经典自适应算法(如LMS、LRMS)推导的新型注意力机制，显著提升了在动态环境中的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有ICL均衡器主要针对静态信道设计，缺乏对非平稳问题的理论分析和实践研究。本文旨在探索ICL在时变信道均衡中的潜力，为开发具有更强适应性的下一代无线基础模型提供关键见解。

Method: 采用原则性框架设计高效注意力机制，利用自适应信号处理算法指导设计。从LMS自适应算法推导新注意力变体，提出LRMS公式增强鲁棒性，使用多步梯度更新改进长期跟踪能力。

Result: 实验结果表明ICL在非平稳MIMO均衡中表现出强大潜力，受经典自适应算法启发的注意力机制能显著提升动态环境中的适应性和性能。

Conclusion: ICL在非平稳信道均衡中具有重要应用价值，基于自适应算法的注意力机制设计方法为开发更适应动态环境的无线基础模型提供了关键思路。

Abstract: Channel equalization is fundamental for mitigating distortions such as
frequency-selective fading and inter-symbol interference. Unlike standard
supervised learning approaches that require costly retraining or fine-tuning
for each new task, in-context learning (ICL) adapts to new channels at
inference time with only a few examples. However, existing ICL-based equalizers
are primarily developed for and evaluated on static channels within the context
window. Indeed, to our knowledge, prior principled analyses and theoretical
studies of ICL focus exclusively on the stationary setting, where the function
remains fixed within the context. In this paper, we investigate the ability of
ICL to address non-stationary problems through the lens of time-varying channel
equalization. We employ a principled framework for designing efficient
attention mechanisms with improved adaptivity in non-stationary tasks,
leveraging algorithms from adaptive signal processing to guide better designs.
For example, new attention variants can be derived from the Least Mean Square
(LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for
enhanced robustness, or multi-step gradient updates for improved long-term
tracking. Experimental results demonstrate that ICL holds strong promise for
non-stationary MIMO equalization, and that attention mechanisms inspired by
classical adaptive algorithms can substantially enhance adaptability and
performance in dynamic environments. Our findings may provide critical insights
for developing next-generation wireless foundation models with stronger
adaptability and robustness.

</details>


### [60] [Enhancing Self-Supervised Learning with Semantic Pairs A New Dataset and Empirical Study](https://arxiv.org/abs/2510.08722)
*Mohammad Alkhalefi,Georgios Leontidis,Mingjun Zhong*

Main category: cs.LG

TL;DR: 实例判别是一种自监督表示学习方法，将数据集中的每个实例视为独立类别，通过应用随机变换生成不同视图，使模型学习对底层对象的表示不变性。


<details>
  <summary>Details</summary>
Motivation: 通过将每个数据实例视为独立类别，让模型学习区分不同实例，从而在无监督情况下学习有意义的表示。

Method: 对每个实例应用随机变换生成两个不同视图，训练模型识别这些视图属于同一实例，学习对变换不变的表示。

Result: 模型能够学习到对底层对象具有不变性的表示，有助于下游任务的性能提升。

Conclusion: 实例判别是一种有效的自监督表示学习方法，通过区分不同实例学习有意义的表示，对变换具有鲁棒性。

Abstract: Instance discrimination is a self-supervised representation learning paradigm
wherein individual instances within a dataset are treated as distinct classes.
This is typically achieved by generating two disparate views of each instance
by applying stochastic transformations, which encourages the model to learn
representations that are invariant to the common underlying object across these
views.

</details>


### [61] [Counterfactually Fair Conformal Prediction](https://arxiv.org/abs/2510.08724)
*Ozgur Guldogan,Neeraj Sarna,Yuanyuan Li,Michael Berger*

Main category: cs.LG

TL;DR: 提出Counterfactually Fair Conformal Prediction (CF-CP)方法，通过保护属性干预的对称化处理，在保持边际覆盖性质的同时实现反事实公平的预测集。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注点预测器的反事实公平性，而对预测集的反事实公平性研究不足。同时，保形预测能提供有效的预测集，但不保证反事实公平性。

Method: 通过对保护属性干预的符合度分数进行对称化处理，开发CF-CP方法，在保持保形预测的边际覆盖性质的同时实现反事实公平。

Result: 在合成和真实数据集上的回归和分类任务中，CF-CP实现了期望的反事实公平性，达到了目标覆盖率，且预测集大小仅有最小程度的增加。

Conclusion: CF-CP提供了一种简单、无需训练的途径来实现反事实公平的不确定性量化。

Abstract: While counterfactual fairness of point predictors is well studied, its
extension to prediction sets--central to fair decision-making under
uncertainty--remains underexplored. On the other hand, conformal prediction
(CP) provides efficient, distribution-free, finite-sample valid prediction
sets, yet does not ensure counterfactual fairness. We close this gap by
developing Counterfactually Fair Conformal Prediction (CF-CP) that produces
counterfactually fair prediction sets. Through symmetrization of conformity
scores across protected-attribute interventions, we prove that CF-CP results in
counterfactually fair prediction sets while maintaining the marginal coverage
property. Furthermore, we empirically demonstrate that on both synthetic and
real datasets, across regression and classification tasks, CF-CP achieves the
desired counterfactual fairness and meets the target coverage rate with minimal
increase in prediction set size. CF-CP offers a simple, training-free route to
counterfactually fair uncertainty quantification.

</details>


### [62] [Transmuting prompts into weights](https://arxiv.org/abs/2510.08734)
*Hanna Mazzawi,Benoit Dherin,Michael Munn,Michael Wunder,Javier Gonzalvo*

Main category: cs.LG

TL;DR: 本文为基于内部状态修改的LLM控制技术提供了理论基础，解释了这些干预如何从Transformer架构的基本计算中产生，并提出了将文本输入转化为可重用权重更新的原则性方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型控制技术虽然有效，但往往基于经验启发式方法，缺乏理论基础。本文旨在为这些干预技术提供数学基础。

Method: 基于Dherin等人的研究，将理论推广到深度多块Transformer，分析提示信息如何通过权重向量和矩阵在内部表示和组合，并推导出将信息压缩为token无关思想向量和思想矩阵的原则性方法。

Result: 开发了将文本输入转化为可重用权重更新的计算方法，为现有的向量和矩阵模型编辑技术提供了理论解释。

Conclusion: 提出的思想向量和思想矩阵为模型控制技术提供了理论基础，实现了文本输入到权重更新的直接转换，具有计算上的可操作性。

Abstract: A growing body of research has demonstrated that the behavior of large
language models can be effectively controlled at inference time by directly
modifying their internal states, either through vector additions to their
activations or through updates to their weight matrices. These techniques,
while powerful, are often guided by empirical heuristics, such as deriving
steering vectors from the average activations of contrastive prompts. This work
provides a theoretical foundation for these interventions, explaining how they
emerge from the fundamental computations of the transformer architecture.
Building on the recent finding that a prompt's influence can be mathematically
mapped to implicit weight updates (Dherin et al., 2025), we generalize this
theory to deep, multi-block transformers. We show how the information contained
in any chunk of a user prompt is represented and composed internally through
weight vectors and weight matrices. We then derive a principled method for
condensing this information into token-independent thought vectors and thought
matrices. These constructs provide a theoretical explanation for existing
vector- and matrix-based model editing techniques and offer a direct,
computationally-grounded method for transmuting textual input into reusable
weight updates.

</details>


### [63] [SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot](https://arxiv.org/abs/2510.08737)
*Justin Lin,Julia Fukuyama*

Main category: cs.LG

TL;DR: 该论文提出通过聚类SHAP值来理解黑盒模型的预测过程，将获得相同预测且原因相似的样本分组，从而揭示不同的预测路径。


<details>
  <summary>Details</summary>
Motivation: 大型黑盒模型在处理复杂数据时缺乏可解释性，使其在高风险场景中不可靠。需要开发能够解释预测过程的方法。

Method: 使用SHAP分析量化特征对预测的贡献，然后对SHAP值进行聚类，将预测原因相似的样本分组。通过模拟实验和阿尔茨海默病案例研究验证方法，并提出多分类瀑布图的新泛化。

Result: 该方法能够识别样本获得相同预测的不同路径，在阿尔茨海默病案例中成功揭示了疾病预测的多种机制。

Conclusion: SHAP值聚类提供了一种有效的方式来理解黑盒模型的决策过程，增强了模型的可解释性和可信度，特别是在高风险医疗应用中。

Abstract: In this growing age of data and technology, large black-box models are
becoming the norm due to their ability to handle vast amounts of data and learn
incredibly complex input-output relationships. The deficiency of these methods,
however, is their inability to explain the prediction process, making them
untrustworthy and their use precarious in high-stakes situations. SHapley
Additive exPlanations (SHAP) analysis is an explainable AI method growing in
popularity for its ability to explain model predictions in terms of the
original features. For each sample and feature in the data set, we associate a
SHAP value that quantifies the contribution of that feature to the prediction
of that sample. Clustering these SHAP values can provide insight into the data
by grouping samples that not only received the same prediction, but received
the same prediction for similar reasons. In doing so, we map the various
pathways through which distinct samples arrive at the same prediction. To
showcase this methodology, we present a simulated experiment in addition to a
case study in Alzheimer's disease using data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. We also present a novel generalization
of the waterfall plot for multi-classification.

</details>


### [64] [Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis](https://arxiv.org/abs/2510.08739)
*Yikai Zhao,Jiekai Ma*

Main category: cs.LG

TL;DR: 提出一个双方法框架，通过代理模型解释复杂时间序列集成预测，并结合频谱可预测性分析来量化预测可靠性


<details>
  <summary>Details</summary>
Motivation: 解决AutoML系统生成的时间序列集成模型在准确性和可解释性之间的权衡问题，提供透明度和可靠性评估

Method: 1. 使用LightGBM训练代理模型来模拟AutoGluon的预测，实现SHAP特征归因；2. 集成频谱可预测性分析，通过比较时间序列与纯噪声基准来量化固有可预测性

Result: 在M5数据集上验证，发现更高的频谱可预测性不仅与改进的预测准确性相关，还与代理模型和原始模型之间的更高保真度相关

Conclusion: 该框架为最先进的集成预测提供可解释的实例级解释，同时为用户提供可预测性指标作为预测及其解释的可靠性指标

Abstract: Modern time series forecasting increasingly relies on complex ensemble models
generated by AutoML systems like AutoGluon, delivering superior accuracy but
with significant costs to transparency and interpretability. This paper
introduces a comprehensive, dual-approach framework that addresses both the
explainability and forecastability challenges in complex time series ensembles.
First, we develop a surrogate-based explanation methodology that bridges the
accuracy-interpretability gap by training a LightGBM model to faithfully mimic
AutoGluon's time series forecasts, enabling stable SHAP-based feature
attributions. We rigorously validated this approach through feature injection
experiments, demonstrating remarkably high faithfulness between extracted SHAP
values and known ground truth effects. Second, we integrated spectral
predictability analysis to quantify each series' inherent forecastability. By
comparing each time series' spectral predictability to its pure noise
benchmarks, we established an objective mechanism to gauge confidence in
forecasts and their explanations. Our empirical evaluation on the M5 dataset
found that higher spectral predictability strongly correlates not only with
improved forecast accuracy but also with higher fidelity between the surrogate
and the original forecasting model. These forecastability metrics serve as
effective filtering mechanisms and confidence scores, enabling users to
calibrate their trust in both the forecasts and their explanations. We further
demonstrated that per-item normalization is essential for generating meaningful
SHAP explanations across heterogeneous time series with varying scales. The
resulting framework delivers interpretable, instance-level explanations for
state-of-the-art ensemble forecasts, while equipping users with forecastability
metrics that serve as reliability indicators for both predictions and their
explanations.

</details>


### [65] [Graph Diffusion Transformers are In-Context Molecular Designers](https://arxiv.org/abs/2510.08744)
*Gang Liu,Jie Chen,Yihan Zhu,Michael Sun,Tengfei Luo,Nitesh V Chawla,Meng Jiang*

Main category: cs.LG

TL;DR: DemoDiff是一种演示条件扩散模型，通过少量分子-评分示例而非文本描述来定义任务上下文，用于分子设计。该方法在33个设计任务中表现优于大100-1000倍的语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决分子设计中上下文学习效果有限的问题，现有数据库如ChEMBL包含数百万生物检测的分子属性，但每个属性的标记数据仍然稀缺。

Method: 使用演示条件扩散模型，通过少量分子-评分示例指导去噪Transformer生成符合目标属性的分子。开发了新的分子标记器Node Pair Encoding，在基序级别表示分子，节点数减少5.5倍。

Result: 在6个类别的33个设计任务中，DemoDiff匹配或超越100-1000倍大的语言模型，平均排名为3.63，而领域特定方法的排名为5.25-10.20。

Conclusion: DemoDiff作为分子基础模型，在上下文分子设计中表现出色，为分子设计提供了有效的解决方案。

Abstract: In-context learning allows large models to adapt to new tasks from a few
demonstrations, but it has shown limited success in molecular design. Existing
databases such as ChEMBL contain molecular properties spanning millions of
biological assays, yet labeled data for each property remain scarce. To address
this limitation, we introduce demonstration-conditioned diffusion models
(DemoDiff), which define task contexts using a small set of molecule-score
examples instead of text descriptions. These demonstrations guide a denoising
Transformer to generate molecules aligned with target properties. For scalable
pretraining, we develop a new molecular tokenizer with Node Pair Encoding that
represents molecules at the motif level, requiring 5.5$\times$ fewer nodes. We
curate a dataset containing millions of context tasks from multiple sources
covering both drugs and materials, and pretrain a 0.7-billion-parameter model
on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses
language models 100-1000$\times$ larger and achieves an average rank of 3.63
compared to 5.25-10.20 for domain-specific approaches. These results position
DemoDiff as a molecular foundation model for in-context molecular design. Our
code is available at https://github.com/liugangcode/DemoDiff.

</details>


### [66] [Conformal Risk Training: End-to-End Optimization of Conformal Risk Control](https://arxiv.org/abs/2510.08748)
*Christopher Yeh,Nicolas Christianson,Adam Wierman,Yisong Yue*

Main category: cs.LG

TL;DR: 本文提出了一种控制优化确定性等价(OCE)风险的方法，扩展了传统的保形风险控制框架，并引入了端到端的保形风险训练方法，在保证风险控制的同时提升平均性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然预测准确率高，但缺乏可证明的风险保证，这在高风险应用中至关重要。传统保形风险控制只关注期望损失，无法处理尾部风险，且后处理方法会降低平均性能。

Method: 开发了控制OCE风险类的方法，包括条件风险价值等尾部风险度量；提出了端到端的保形风险训练，在模型训练或微调期间通过保形OCE风险控制进行微分。

Result: 方法在控制分类器假阴性率和电池存储操作中的金融风险等应用中，相比后处理方法显著提升了平均性能，同时保持了可证明的风险保证。

Conclusion: 提出的保形风险训练方法能够有效控制各类风险度量，包括尾部风险，并在保证风险控制的同时显著改善模型性能，适用于高风险应用场景。

Abstract: While deep learning models often achieve high predictive accuracy, their
predictions typically do not come with any provable guarantees on risk or
reliability, which are critical for deployment in high-stakes applications. The
framework of conformal risk control (CRC) provides a distribution-free,
finite-sample method for controlling the expected value of any bounded monotone
loss function and can be conveniently applied post-hoc to any pre-trained deep
learning model. However, many real-world applications are sensitive to tail
risks, as opposed to just expected loss. In this work, we develop a method for
controlling the general class of Optimized Certainty-Equivalent (OCE) risks, a
broad class of risk measures which includes as special cases the expected loss
(generalizing the original CRC method) and common tail risks like the
conditional value-at-risk (CVaR). Furthermore, standard post-hoc CRC can
degrade average-case performance due to its lack of feedback to the model. To
address this, we introduce "conformal risk training," an end-to-end approach
that differentiates through conformal OCE risk control during model training or
fine-tuning. Our method achieves provable risk guarantees while demonstrating
significantly improved average-case performance over post-hoc approaches on
applications to controlling classifiers' false negative rate and controlling
financial risk in battery storage operation.

</details>


### [67] [Exploring Cross-Client Memorization of Training Data in Large Language Models for Federated Learning](https://arxiv.org/abs/2510.08750)
*Tinnakit Udsa,Can Udomcharoenchaikit,Patomporn Payoungkhamdee,Sarana Nutanong,Norrathep Rattanavipanon*

Main category: cs.LG

TL;DR: 提出一个联邦学习框架，量化客户端内和客户端间的记忆效应，通过细粒度跨样本记忆测量来评估联邦学习中的记忆风险。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习记忆检测技术一次只关注一个样本，低估了跨样本记忆的微妙风险，而集中式学习的细粒度记忆评估方法无法直接应用于联邦学习。

Method: 提出量化框架，测量所有客户端间的细粒度跨样本记忆，包括客户端内和客户端间记忆，并研究解码策略、前缀长度和FL算法等关键因素。

Result: 发现FL模型确实会记忆客户端数据，特别是客户端内数据比客户端间数据更容易被记忆，记忆程度受训练和推理因素影响。

Conclusion: 联邦学习存在记忆风险，需要更细粒度的记忆检测方法，客户端内记忆比客户端间记忆更显著，记忆程度受多种因素影响。

Abstract: Federated learning (FL) enables collaborative training without raw data
sharing, but still risks training data memorization. Existing FL memorization
detection techniques focus on one sample at a time, underestimating more subtle
risks of cross-sample memorization. In contrast, recent work on centralized
learning (CL) has introduced fine-grained methods to assess memorization across
all samples in training data, but these assume centralized access to data and
cannot be applied directly to FL. We bridge this gap by proposing a framework
that quantifies both intra- and inter-client memorization in FL using
fine-grained cross-sample memorization measurement across all clients. Based on
this framework, we conduct two studies: (1) measuring subtle memorization
across clients and (2) examining key factors that influence memorization,
including decoding strategies, prefix length, and FL algorithms. Our findings
reveal that FL models do memorize client data, particularly intra-client data,
more than inter-client data, with memorization influenced by training and
inferencing factors.

</details>


### [68] [LOTION: Smoothing the Optimization Landscape for Quantized Training](https://arxiv.org/abs/2510.08757)
*Mujin Kwun,Depen Morwani,Chloe Huangyuan Su,Stephanie Gil,Nikhil Anand,Sham Kakade*

Main category: cs.LG

TL;DR: LOTION是一个用于量化神经网络优化的平滑框架，通过随机舍入噪声的期望来近似量化损失表面，保证标准优化器收敛到局部最小值，并在语言模型上优于标准QAT方法。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络优化具有根本性挑战，因为量化器是分段常数函数，梯度在量化阈值处未定义，现有方法如STE缺乏收敛保证。

Method: 受Nesterov平滑启发，提出LOTION框架，用无偏随机舍入噪声下的期望来替代原始量化损失，构建连续损失表面。

Result: 在合成测试集和150M/300M参数的语言模型上，该方法在经验上优于标准量化感知训练(QAT)。

Conclusion: LOTION提供了一个有理论保证的量化优化框架，能够保持原始量化损失的全局最小值，同时确保标准优化器的收敛性。

Abstract: Optimizing neural networks for quantized objectives is fundamentally
challenging because the quantizer is piece-wise constant, yielding zero
gradients everywhere except at quantization thresholds where the derivative is
undefined. Most existing methods deal with this issue by relaxing gradient
computations with techniques like Straight Through Estimators (STE) and do not
provide any guarantees of convergence. In this work, taking inspiration from
Nesterov smoothing, we approximate the quantized loss surface with a continuous
loss surface. In particular, we introduce LOTION, \textbf{L}ow-precision
\textbf{O}ptimization via s\textbf{T}ochastic-no\textbf{I}se
sm\textbf{O}othi\textbf{N}g, a principled smoothing framework that replaces the
raw quantized loss with its expectation under unbiased randomized-rounding
noise. In this framework, standard optimizers are guaranteed to converge to a
local minimum of the loss surface. Moreover, when using noise derived from
stochastic rounding, we show that the global minima of the original quantized
loss are preserved. We empirically demonstrate that this method outperforms
standard QAT on synthetic testbeds and on 150M- and 300M- parameter language
models.

</details>


### [69] [Spatial Deconfounder: Interference-Aware Deconfounding for Spatial Causal Inference](https://arxiv.org/abs/2510.08762)
*Ayush Khot,Miruna Oprescu,Maresa Schröder,Ai Kagawa,Xihaier Luo*

Main category: cs.LG

TL;DR: 提出Spatial Deconfounder方法，通过重构替代混杂因子来解决空间因果推断中的未测量混杂和干扰问题。


<details>
  <summary>Details</summary>
Motivation: 空间因果推断面临两个交织的挑战：未测量的空间混杂因子（如天气、空气污染）和来自附近处理的干扰效应。现有方法通常只解决其中一个问题而忽略另一个。

Method: 两阶段方法：使用带有空间先验的条件变分自编码器从局部处理向量重构替代混杂因子，然后通过灵活的结果模型估计因果效应。

Result: 在环境健康和社会科学真实数据集上，Spatial Deconfounder持续改进效应估计，扩展了SpaCE基准套件以包含处理干扰。

Conclusion: 通过将干扰转化为多原因信号，该框架连接了空间和去混杂文献，推进了结构化数据中的稳健因果推断。

Abstract: Causal inference in spatial domains faces two intertwined challenges: (1)
unmeasured spatial factors, such as weather, air pollution, or mobility, that
confound treatment and outcome, and (2) interference from nearby treatments
that violate standard no-interference assumptions. While existing methods
typically address one by assuming away the other, we show they are deeply
connected: interference reveals structure in the latent confounder. Leveraging
this insight, we propose the Spatial Deconfounder, a two-stage method that
reconstructs a substitute confounder from local treatment vectors using a
conditional variational autoencoder (CVAE) with a spatial prior, then estimates
causal effects via a flexible outcome model. We show that this approach enables
nonparametric identification of both direct and spillover effects under weak
assumptions--without requiring multiple treatment types or a known model of the
latent field. Empirically, we extend SpaCE, a benchmark suite for spatial
confounding, to include treatment interference, and show that the Spatial
Deconfounder consistently improves effect estimation across real-world datasets
in environmental health and social science. By turning interference into a
multi-cause signal, our framework bridges spatial and deconfounding literatures
to advance robust causal inference in structured data.

</details>


### [70] [Reinforcement Learning-Based Optimization of CT Acquisition and Reconstruction Parameters Through Virtual Imaging Trials](https://arxiv.org/abs/2510.08763)
*David Fenwick,Navid NaderiAlizadeh,Vahid Tarokh,Nicholas Felice,Darin Clark,Jayasai Rajagopal,Anuj Kapadia,Benjamin Wildman-Tobriner,Ehsan Samei,Ehsan Abadi*

Main category: cs.LG

TL;DR: 结合虚拟成像工具与强化学习优化CT协议，在保证图像质量的同时显著减少计算步骤


<details>
  <summary>Details</summary>
Motivation: 传统CT协议优化方法需要穷举参数组合，计算成本高且不实用，需要更高效的优化方法

Method: 使用虚拟成像工具生成带肝脏病变的人体模型图像，采用PPO强化学习算法优化CT采集和重建参数

Result: 强化学习方法在测试案例中达到了全局最大可检测性指数，比穷举搜索减少了79.7%的计算步骤

Conclusion: 虚拟成像工具与强化学习结合为CT协议管理提供了准确且计算高效的解决方案

Abstract: Protocol optimization is critical in Computed Tomography (CT) to achieve high
diagnostic image quality while minimizing radiation dose. However, due to the
complex interdependencies among CT acquisition and reconstruction parameters,
traditional optimization methods rely on exhaustive testing of combinations of
these parameters, which is often impractical. This study introduces a novel
methodology that combines virtual imaging tools with reinforcement learning to
optimize CT protocols more efficiently. Human models with liver lesions were
imaged using a validated CT simulator and reconstructed with a novel CT
reconstruction toolkit. The optimization parameter space included tube voltage,
tube current, reconstruction kernel, slice thickness, and pixel size. The
optimization process was performed using a Proximal Policy Optimization (PPO)
agent, which was trained to maximize an image quality objective, specifically
the detectability index (d') of liver lesions in the reconstructed images.
Optimization performance was compared against an exhaustive search performed on
a supercomputer. The proposed reinforcement learning approach achieved the
global maximum d' across test cases while requiring 79.7% fewer steps than the
exhaustive search, demonstrating both accuracy and computational efficiency.
The proposed framework is flexible and can accommodate various image quality
objectives. The findings highlight the potential of integrating virtual imaging
tools with reinforcement learning for CT protocol management.

</details>


### [71] [Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem](https://arxiv.org/abs/2510.08768)
*Francisco Pascoa,Ian Lalonde,Alexandre Girard*

Main category: cs.LG

TL;DR: 基于白金汉π定理的零样本迁移方法，通过无量纲化缩放预训练策略的输入输出，无需重新训练即可适应新的系统环境。


<details>
  <summary>Details</summary>
Motivation: 强化学习策略往往无法泛化到具有不同物理参数的新机器人、任务或环境，这限制了其实际应用。

Method: 利用白金汉π定理，通过将观测和动作缩放到无量纲空间来适应预训练策略到新系统环境，无需重新训练。

Result: 在三个复杂度递增的环境中测试，缩放迁移在动态相似环境下无性能损失，在非相似环境下始终优于朴素迁移方法。

Conclusion: 量纲分析为增强强化学习策略的鲁棒性和泛化能力提供了强大实用的工具。

Abstract: Reinforcement learning (RL) policies often fail to generalize to new robots,
tasks, or environments with different physical parameters, a challenge that
limits their real-world applicability. This paper presents a simple, zero-shot
transfer method based on Buckingham's Pi Theorem to address this limitation.
The method adapts a pre-trained policy to new system contexts by scaling its
inputs (observations) and outputs (actions) through a dimensionless space,
requiring no retraining. The approach is evaluated against a naive transfer
baseline across three environments of increasing complexity: a simulated
pendulum, a physical pendulum for sim-to-real validation, and the
high-dimensional HalfCheetah. Results demonstrate that the scaled transfer
exhibits no loss of performance on dynamically similar contexts. Furthermore,
on non-similar contexts, the scaled policy consistently outperforms the naive
transfer, significantly expanding the volume of contexts where the original
policy remains effective. These findings demonstrate that dimensional analysis
provides a powerful and practical tool to enhance the robustness and
generalization of RL policies.

</details>


### [72] [Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings](https://arxiv.org/abs/2510.08774)
*Shikun Liu,Haoyu Wang,Mufei Li,Pan Li*

Main category: cs.LG

TL;DR: 该论文提出了一种新的结构感知文本嵌入方法，通过将结构关系直接整合到LLM内部编码过程中，而不是依赖传统的后处理聚合方法，显著提升了嵌入质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本嵌入模型通常只处理原始文本，忽略了现实数据中丰富的结构信息（如超链接、引用等），这些结构信息在许多应用中提供关键上下文。

Method: 研究了两种主要的进程内方法：顺序连接和并行缓存，并引入了上下文蒸馏和语义平衡技术来处理噪声结构数据。

Result: 在检索、聚类、分类和推荐任务的零样本实验中，结构感知方法始终优于纯文本和后处理基线方法。顺序连接在噪声中等长度上下文中表现更好，而并行缓存在长高信号上下文中扩展性更强。

Conclusion: 这项工作首次全面分析了进程内结构感知编码，为构建更强大和上下文感知的嵌入模型提供了蓝图。

Abstract: Text embeddings from Large Language Models (LLMs) have become foundational
for numerous applications. However, these models typically operate on raw text,
overlooking the rich structural information, such as hyperlinks or citations,
that provides crucial context in many real-world datasets. This paper
introduces and systematically evaluates a new paradigm for generating
structure-aware text embeddings by integrating these structural relations
directly into the LLM's internal encoding process, rather than relying on
traditional post-hoc aggregation. We investigate two primary in-process
methods: sequential concatenation and parallel caching. Through extensive
zero-shot experiments across retrieval, clustering, classification, and
recommendation tasks, we demonstrate that our structure-aware approaches
consistently outperform both text-only and post-hoc baselines. Our analysis
reveals critical trade-offs: sequential concatenation excels with noisy,
moderate-length contexts, while parallel caching scales more effectively to
long, high-signal contexts but is more susceptible to distractors. To address
the challenge of noisy structural data, we also introduce and validate two
effective techniques: Context Distillation and Semantic Balancing. This work
provides the first comprehensive analysis of in-process structure-aware
encoding, offering a blueprint for building more powerful and contextually
aware embedding models.

</details>


### [73] [Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations](https://arxiv.org/abs/2510.08779)
*Vaibhav Jain,Gerrit Grossmann*

Main category: cs.LG

TL;DR: 提出了一种通过增强观察空间提供LLM生成动作推荐的框架，让RL智能体学习何时遵循或忽略LLM指导，在稀疏奖励环境中显著提升训练效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决RL智能体在稀疏奖励环境中传统探索策略失效的问题，利用LLM的程序性知识和推理能力来指导RL探索，同时避免现有方法中RL策略必须严格遵循LLM建议的刚性依赖。

Method: 通过增强观察空间提供LLM生成的动作推荐，允许RL智能体学习何时遵循或忽略这些指导，利用LLM的世界知识和推理能力，同时通过软约束保持灵活性。

Result: 在三个复杂度递增的BabyAI环境中评估，LLM指导的益处随任务难度增加而提升。在最复杂环境中，最终成功率相对基线提升71%，样本效率提升高达9倍，且无需修改现有RL算法。

Conclusion: 该方法有效利用LLM的规划能力来加速RL在挑战性环境中的训练，展示了LLM与RL结合的实用价值。

Abstract: Reinforcement Learning (RL) agents often struggle in sparse-reward
environments where traditional exploration strategies fail to discover
effective action sequences. Large Language Models (LLMs) possess procedural
knowledge and reasoning capabilities from text pretraining that could guide RL
exploration, but existing approaches create rigid dependencies where RL
policies must follow LLM suggestions or incorporate them directly into reward
functions. We propose a framework that provides LLM-generated action
recommendations through augmented observation spaces, allowing RL agents to
learn when to follow or ignore this guidance. Our method leverages LLMs' world
knowledge and reasoning abilities while maintaining flexibility through soft
constraints. We evaluate our approach on three BabyAI environments of
increasing complexity and show that the benefits of LLM guidance scale with
task difficulty. In the most challenging environment, we achieve 71% relative
improvement in final success rates over baseline. The approach provides
substantial sample efficiency gains, with agents reaching performance
thresholds up to 9 times faster, and requires no modifications to existing RL
algorithms. Our results demonstrate an effective method for leveraging LLM
planning capabilities to accelerate RL training in challenging environments.

</details>


### [74] [Weights initialization of neural networks for function approximation](https://arxiv.org/abs/2510.08780)
*Xinwen Hu,Yunqing Huang,Nianyu Yi,Peimeng Yin*

Main category: cs.LG

TL;DR: 提出基于基函数预训练的可重用初始化框架，通过预训练神经网络逼近多项式基函数，然后用于初始化复杂目标函数的网络，显著提升训练效率、泛化能力和模型可迁移性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络函数逼近中的三个主要挑战：每个目标函数需要从头训练新模型、性能对架构和超参数高度敏感、模型在训练域外泛化能力差。

Method: 首先训练基神经网络逼近参考域上的多项式族，然后将学习到的参数用于初始化更复杂目标函数的网络，并引入域映射机制将任意域输入变换到参考域。

Result: 在一维和二维设置中的大量数值实验显示，该方法在训练效率、泛化能力和模型可迁移性方面都有显著提升。

Conclusion: 基于初始化的策略为可扩展和模块化的神经函数逼近提供了有前景的解决方案。

Abstract: Neural network-based function approximation plays a pivotal role in the
advancement of scientific computing and machine learning. Yet, training such
models faces several challenges: (i) each target function often requires
training a new model from scratch; (ii) performance is highly sensitive to
architectural and hyperparameter choices; and (iii) models frequently
generalize poorly beyond the training domain. To overcome these challenges, we
propose a reusable initialization framework based on basis function
pretraining. In this approach, basis neural networks are first trained to
approximate families of polynomials on a reference domain. Their learned
parameters are then used to initialize networks for more complex target
functions. To enhance adaptability across arbitrary domains, we further
introduce a domain mapping mechanism that transforms inputs into the reference
domain, thereby preserving structural correspondence with the pretrained
models. Extensive numerical experiments in one- and two-dimensional settings
demonstrate substantial improvements in training efficiency, generalization,
and model transferability, highlighting the promise of initialization-based
strategies for scalable and modular neural function approximation. The full
code is made publicly available on Gitee.

</details>


### [75] [RepDL: Bit-level Reproducible Deep Learning Training and Inference](https://arxiv.org/abs/2510.09180)
*Peichen Xie,Xian Zhang,Shuo Chen*

Main category: cs.LG

TL;DR: RepDL是一个开源库，通过强制正确舍入和顺序不变性来解决深度学习中浮点计算不一致的问题，确保跨平台的确定性训练和推理。


<details>
  <summary>Details</summary>
Motivation: 深度学习中存在非确定性和不可复现性问题，主要源于随机数生成和浮点计算。虽然随机性可以通过确定性配置控制，但浮点不一致性问题仍未解决。

Method: 开发RepDL开源库，通过强制正确舍入和顺序不变性来确保浮点计算的确定性。

Result: RepDL能够确保深度学习的训练和推理在不同计算环境中具有确定性和比特级可复现性。

Conclusion: RepDL成功解决了深度学习中的浮点计算不一致性问题，提供了跨平台的确定性解决方案。

Abstract: Non-determinism and non-reproducibility present significant challenges in
deep learning, leading to inconsistent results across runs and platforms. These
issues stem from two origins: random number generation and floating-point
computation. While randomness can be controlled through deterministic
configurations, floating-point inconsistencies remain largely unresolved. To
address this, we introduce RepDL, an open-source library that ensures
deterministic and bitwise-reproducible deep learning training and inference
across diverse computing environments. RepDL achieves this by enforcing correct
rounding and order invariance in floating-point computation. The source code is
available at https://github.com/microsoft/RepDL .

</details>


### [76] [Deceptive Exploration in Multi-armed Bandits](https://arxiv.org/abs/2510.08794)
*I. Arda Vurankaya,Mustafa O. Karabag,Wesley A. Suttle,Jesse Milzman,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 本文研究多臂老虎机中的欺骗性学习问题，代理需要在观察者的监督下识别最佳私有臂而不被发现，通过KL散度约束实现最优私有臂识别。


<details>
  <summary>Details</summary>
Motivation: 研究在观察者监督下，代理如何在不被发现的情况下识别最佳私有臂，解决欺骗性学习中的检测约束问题。

Method: 使用KL散度约束建模检测性，将公共次优臂的拉动建模为伯努利过程，提出基于top-two算法的启发式算法。

Result: 证明在KL约束下，公共次优臂的拉动最多以Θ(√T)的速率发生，并给出了最优误差指数的特征化。

Conclusion: 提出的算法能够根据公共次优性差距自适应地调整探索策略，在欺骗性学习场景下实现有效的私有臂识别。

Abstract: We consider a multi-armed bandit setting in which each arm has a public and a
private reward distribution. An observer expects an agent to follow Thompson
Sampling according to the public rewards, however, the deceptive agent aims to
quickly identify the best private arm without being noticed. The observer can
observe the public rewards and the pulled arms, but not the private rewards.
The agent, on the other hand, observes both the public and private rewards. We
formalize detectability as a stepwise Kullback-Leibler (KL) divergence
constraint between the actual pull probabilities used by the agent and the
anticipated pull probabilities by the observer. We model successful pulling of
public suboptimal arms as a % Bernoulli process where the success probability
decreases with each successful pull, and show these pulls can happen at most at
a $\Theta(\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin
problem based on public and private means, whose solution characterizes the
optimal error exponent for best private arm identification. We finally propose
an algorithm inspired by top-two algorithms. This algorithm naturally adapts
its exploration according to the hardness of pulling arms based on the public
suboptimality gaps. We provide numerical examples illustrating the
$\Theta(\sqrt{T}) $ rate and the behavior of the proposed algorithm.

</details>


### [77] [PO-CKAN:Physics Informed Deep Operator Kolmogorov Arnold Networks with Chunk Rational Structure](https://arxiv.org/abs/2510.08795)
*Junyi Wu,Guang Lin*

Main category: cs.LG

TL;DR: PO-CKAN是一个基于分块有理Kolmogorov-Arnold网络(KANs)的物理信息深度算子框架，用于逼近偏微分方程的解算子，相比PI-DeepONet在Burgers方程上相对L2误差降低约48%。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够高效学习物理一致的时空解算子的框架，用于快速预测参数化时变PDE在不同输入（如参数、初始/边界条件）下的解。

Method: 采用DeepONet分支-主干架构，子网络实例化为有理KAN模块，通过PDE残差（PINN风格）损失强制物理一致性。

Result: 在Burgers方程（ν=0.01）上，PO-CKAN相比PI-DeepONet将平均相对L2误差降低约48%，在Eikonal和扩散-反应基准测试中达到竞争性精度。

Conclusion: PO-CKAN框架能够准确学习算子，结果与高保真解高度匹配，证明了其在参数化时变PDE求解中的有效性。

Abstract: We propose PO-CKAN, a physics-informed deep operator framework based on
Chunkwise Rational Kolmogorov--Arnold Networks (KANs), for approximating the
solution operators of partial differential equations. This framework leverages
a Deep Operator Network (DeepONet) architecture that incorporates Chunkwise
Rational Kolmogorov--Arnold Network (CKAN) sub-networks for enhanced function
approximation. The principles of Physics-Informed Neural Networks (PINNs) are
integrated into the operator learning framework to enforce physical
consistency. This design enables the efficient learning of physically
consistent spatio-temporal solution operators and allows for rapid prediction
for parametric time-dependent PDEs with varying inputs (e.g., parameters,
initial/boundary conditions) after training. Validated on challenging benchmark
problems, PO-CKAN demonstrates accurate operator learning with results closely
matching high-fidelity solutions. PO-CKAN adopts a DeepONet-style branch--trunk
architecture with its sub-networks instantiated as rational KAN modules, and
enforces physical consistency via a PDE residual (PINN-style) loss. On Burgers'
equation with $\nu=0.01$, PO-CKAN reduces the mean relative $L^2$ error by
approximately 48\% compared to PI-DeepONet, and achieves competitive accuracy
on the Eikonal and diffusion--reaction benchmarks.

</details>


### [78] [TAPAS: Datasets for Learning the Learning with Errors Problem](https://arxiv.org/abs/2510.08797)
*Eshika Saxena,Alberto Alfarano,François Charton,Emily Wenger,Kristin Lauter*

Main category: cs.LG

TL;DR: 提出了TAPAS数据集工具包，用于加速AI在LWE攻击方面的研究，解决了AI实践者因缺乏可访问数据而难以研究和改进LWE攻击的问题。


<details>
  <summary>Details</summary>
Motivation: AI驱动的LWE攻击在某些参数设置下已经媲美或超越传统攻击方法，但由于缺乏可访问的训练数据，限制了AI实践者研究和改进这些攻击的能力。创建LWE数据需要大量时间和计算资源，且需要深厚的领域专业知识。

Method: 开发了TAPAS（后量子密码学AI分析工具包）数据集，涵盖多种LWE设置，AI实践者可以开箱即用地使用这些数据集来原型化新的LWE破解方法。

Result: 创建了TAPAS数据集，建立了攻击性能基线，为未来工作指明了方向。

Conclusion: TAPAS数据集填补了AI研究LWE攻击的数据空白，将加速后量子密码学领域的AI研究进展。

Abstract: AI-powered attacks on Learning with Errors (LWE), an important hard math
problem in post-quantum cryptography, rival or outperform "classical" attacks
on LWE under certain parameter settings. Despite the promise of this approach,
a dearth of accessible data limits AI practitioners' ability to study and
improve these attacks. Creating LWE data for AI model training is time- and
compute-intensive and requires significant domain expertise. To fill this gap
and accelerate AI research on LWE attacks, we propose the TAPAS datasets, a
Toolkit for Analysis of Post-quantum cryptography using AI Systems. These
datasets cover several LWE settings and can be used off-the-shelf by AI
practitioners to prototype new approaches to cracking LWE. This work documents
TAPAS dataset creation, establishes attack performance baselines, and lays out
directions for future work.

</details>


### [79] [Edu-EmotionNet: Cross-Modality Attention Alignment with Temporal Feedback Loops](https://arxiv.org/abs/2510.08802)
*S M Rafiuddin*

Main category: cs.LG

TL;DR: Edu-EmotionNet是一个用于在线教育情感识别的新框架，通过联合建模时间情感演化和模态可靠性，解决了现有方法依赖静态融合策略和假设模态输入始终可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 在线教育中理解学习者情绪对提高参与度和个性化教学至关重要。现有情感识别方法依赖静态融合策略，假设模态输入始终可靠，这在真实学习环境中很少成立。

Method: 提出Edu-EmotionNet框架，包含三个关键组件：跨模态注意力对齐模块（动态跨模态上下文共享）、模态重要性估计器（基于置信度为每个模态分配权重）、时间反馈循环（利用先前预测确保时间一致性）。

Result: 在IEMOCAP和MOSEI的教育子集上评估，重新标注了困惑、好奇、无聊和沮丧情绪，Edu-EmotionNet实现了最先进的性能，并对缺失或噪声模态表现出强鲁棒性。

Conclusion: 可视化证实该框架能够捕捉情感转变并自适应地优先处理可靠信号，使其非常适合部署在实时学习系统中。

Abstract: Understanding learner emotions in online education is critical for improving
engagement and personalized instruction. While prior work in emotion
recognition has explored multimodal fusion and temporal modeling, existing
methods often rely on static fusion strategies and assume that modality inputs
are consistently reliable, which is rarely the case in real-world learning
environments. We introduce Edu-EmotionNet, a novel framework that jointly
models temporal emotion evolution and modality reliability for robust affect
recognition. Our model incorporates three key components: a Cross-Modality
Attention Alignment (CMAA) module for dynamic cross-modal context sharing, a
Modality Importance Estimator (MIE) that assigns confidence-based weights to
each modality at every time step, and a Temporal Feedback Loop (TFL) that
leverages previous predictions to enforce temporal consistency. Evaluated on
educational subsets of IEMOCAP and MOSEI, re-annotated for confusion,
curiosity, boredom, and frustration, Edu-EmotionNet achieves state-of-the-art
performance and demonstrates strong robustness to missing or noisy modalities.
Visualizations confirm its ability to capture emotional transitions and
adaptively prioritize reliable signals, making it well suited for deployment in
real-time learning systems

</details>


### [80] [TinyGraphEstimator: Adapting Lightweight Language Models for Graph Structure Inference](https://arxiv.org/abs/2510.08808)
*Michal Podstawski*

Main category: cs.LG

TL;DR: 小型语言模型能够通过文本图表示直接推断图论参数，通过LoRA微调在密度、聚类和色数等关键指标上取得一致改进。


<details>
  <summary>Details</summary>
Motivation: 探索资源高效的小型语言模型在图结构推理任务中的潜力，填补大型模型与小型模型在该领域研究空白。

Method: 引入TinyGraphEstimator数据集，评估多个小型开源模型预测图参数能力，并应用LoRA技术进行轻量级微调。

Result: 小型语言模型在图结构数据上具有非平凡的推理能力，通过高效参数调优可有效适应结构推理任务。

Conclusion: 紧凑的基于Transformer的语言模型能够直接从文本图表示中推断图论参数，为资源受限环境下的图分析提供了可行方案。

Abstract: Graphs provide a universal framework for representing complex relational
systems, and inferring their structural properties is a core challenge in graph
analysis and reasoning. While large language models have recently demonstrated
emerging abilities to perform symbolic and numerical reasoning, the potential
of smaller, resource-efficient models in this context remains largely
unexplored. This paper investigates whether compact transformer-based language
models can infer graph-theoretic parameters directly from textual graph
representations. To enable systematic evaluation, we introduce the
TinyGraphEstimator dataset - a balanced collection of connected graphs
generated from multiple random graph models and annotated with detailed
structural metadata. We evaluate several small open models on their ability to
predict key graph parameters such as density, clustering, and chromatic number.
Furthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation
(LoRA) technique, achieving consistent improvements across all evaluated
metrics. The results demonstrate that small language models possess non-trivial
reasoning capacity over graph-structured data and can be effectively adapted
for structural inference tasks through efficient parameter tuning.

</details>


### [81] [Long-Tailed Recognition via Information-Preservable Two-Stage Learning](https://arxiv.org/abs/2510.08836)
*Fudong Lin,Xu Yuan*

Main category: cs.LG

TL;DR: 提出一种两阶段学习方法解决长尾数据分类问题，通过信息理论视角的特征表示学习和数学信息实例采样策略，在多个基准数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现实世界数据分布通常存在不平衡（长尾）特性，导致深度分类模型偏向频繁类别，对尾部类别性能较差

Method: 两阶段方法：第一阶段从信息论角度提出新的表示学习技术，理论上等价于最小化类内距离；第二阶段开发新的采样策略，选择数学上信息丰富的实例来修正多数类偏向的决策边界

Result: 在多个长尾基准数据集上实现了最先进的性能，通过大量实验验证

Conclusion: 该方法有效缓解了多数类偏向趋势，同时保留了数据集中的有价值信息，在长尾分类问题上表现优异

Abstract: The imbalance (or long-tail) is the nature of many real-world data
distributions, which often induces the undesirable bias of deep classification
models toward frequent classes, resulting in poor performance for tail classes.
In this paper, we propose a novel two-stage learning approach to mitigate such
a majority-biased tendency while preserving valuable information within
datasets. Specifically, the first stage proposes a new representation learning
technique from the information theory perspective. This approach is
theoretically equivalent to minimizing intra-class distance, yielding an
effective and well-separated feature space. The second stage develops a novel
sampling strategy that selects mathematically informative instances, able to
rectify majority-biased decision boundaries without compromising a model's
overall performance. As a result, our approach achieves the state-of-the-art
performance across various long-tailed benchmark datasets, validated via
extensive experiments. Our code is available at
https://github.com/fudong03/BNS_IPDPP.

</details>


### [82] [The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective](https://arxiv.org/abs/2510.08840)
*Thai-Hoang Pham,Jiayuan Chen,Seungyeon Lee,Yuanlong Wang,Sayoko Moroi,Xueru Zhang,Ping Zhang*

Main category: cs.LG

TL;DR: FairTTE是首个用于评估医学影像中时间到事件预测公平性的综合框架，揭示了不同成像模态中普遍存在的偏见问题，并发现现有公平性方法缓解效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习公平性研究主要关注医学图像诊断任务（如分类和分割），而忽视了预后场景中的时间到事件预测公平性问题。

Method: 开发了FairTTE框架，整合了先进的时间到事件预测和公平性算法，利用因果分析技术识别和量化医学影像数据集中的偏见来源。

Result: 大规模评估显示偏见在不同成像模态中普遍存在，现有公平性方法缓解效果有限，公平性在分布偏移下更难维持。

Conclusion: 需要针对所有偏见形式的整体方法，以及更稳健、公平的预后模型来应对分布偏移带来的挑战。

Abstract: As machine learning (ML) algorithms are increasingly used in medical image
analysis, concerns have emerged about their potential biases against certain
social groups. Although many approaches have been proposed to ensure the
fairness of ML models, most existing works focus only on medical image
diagnosis tasks, such as image classification and segmentation, and overlooked
prognosis scenarios, which involve predicting the likely outcome or progression
of a medical condition over time. To address this gap, we introduce FairTTE,
the first comprehensive framework for assessing fairness in time-to-event (TTE)
prediction in medical imaging. FairTTE encompasses a diverse range of imaging
modalities and TTE outcomes, integrating cutting-edge TTE prediction and
fairness algorithms to enable systematic and fine-grained analysis of fairness
in medical image prognosis. Leveraging causal analysis techniques, FairTTE
uncovers and quantifies distinct sources of bias embedded within medical
imaging datasets. Our large-scale evaluation reveals that bias is pervasive
across different imaging modalities and that current fairness methods offer
limited mitigation. We further demonstrate a strong association between
underlying bias sources and model disparities, emphasizing the need for
holistic approaches that target all forms of bias. Notably, we find that
fairness becomes increasingly difficult to maintain under distribution shifts,
underscoring the limitations of existing solutions and the pressing need for
more robust, equitable prognostic models.

</details>


### [83] [On the Alignment Between Supervised and Self-Supervised Contrastive Learning](https://arxiv.org/abs/2510.08852)
*Achleshwar Luthra,Priyadarsi Mishra,Tomer Galanti*

Main category: cs.LG

TL;DR: 本文研究了自监督对比学习(CL)和负样本监督对比学习(NSCL)在表示层面的对齐问题，证明了它们在表示相似性矩阵上保持接近，但参数空间会发散。


<details>
  <summary>Details</summary>
Motivation: 虽然已有理论表明CL损失近似于NSCL损失，但尚不清楚它们在训练过程中表示层面是否保持对齐。

Method: 在共享随机性条件下训练CL和NSCL模型，分析表示相似性矩阵的接近程度，并提供理论保证。

Result: CL和NSCL的表示相似性矩阵在现实条件下保持接近，但参数权重会指数级发散。对齐程度随类别数、温度增加而改善。

Conclusion: NSCL可作为自监督和监督学习之间的理论桥梁，表示层面的对齐解释了CL的成功表现。

Abstract: Self-supervised contrastive learning (CL) has achieved remarkable empirical
success, often producing representations that rival supervised pre-training on
downstream tasks. Recent theory explains this by showing that the CL loss
closely approximates a supervised surrogate, Negatives-Only Supervised
Contrastive Learning (NSCL) loss, as the number of classes grows. Yet this
loss-level similarity leaves an open question: {\em Do CL and NSCL also remain
aligned at the representation level throughout training, not just in their
objectives?}
  We address this by analyzing the representation alignment of CL and NSCL
models trained under shared randomness (same initialization, batches, and
augmentations). First, we show that their induced representations remain
similar: specifically, we prove that the similarity matrices of CL and NSCL
stay close under realistic conditions. Our bounds provide high-probability
guarantees on alignment metrics such as centered kernel alignment (CKA) and
representational similarity analysis (RSA), and they clarify how alignment
improves with more classes, higher temperatures, and its dependence on batch
size. In contrast, we demonstrate that parameter-space coupling is inherently
unstable: divergence between CL and NSCL weights can grow exponentially with
training time.
  Finally, we validate these predictions empirically, showing that CL-NSCL
alignment strengthens with scale and temperature, and that NSCL tracks CL more
closely than other supervised objectives. This positions NSCL as a principled
bridge between self-supervised and supervised learning. Our code and project
page are available at
[\href{https://github.com/DLFundamentals/understanding_ssl_v2}{code},
\href{https://dlfundamentals.github.io/cl-nscl-representation-alignment/}{project
page}].

</details>


### [84] [Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training](https://arxiv.org/abs/2510.08855)
*T. Ed Li,Junyu Ren*

Main category: cs.LG

TL;DR: 提出自适应时间掩码(ATM)方法解决稀疏自编码器中的特征吸收问题，通过动态调整特征选择来学习稳定、可解释的特征


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器训练方法存在特征吸收问题，特征相互吸收以最小化L1惩罚，导致难以一致识别和分析模型行为

Method: ATM方法通过跟踪激活幅度、频率和重构贡献来计算随时间演化的重要性分数，基于统计阈值应用概率掩码机制

Result: 在Gemma-2-2b模型上的实验表明，ATM相比TopK和JumpReLU SAEs实现了显著更低的吸收分数，同时保持优异的重构质量

Conclusion: ATM为学习稳定、可解释的神经网络特征提供了原则性解决方案，为更可靠的模型分析奠定基础

Abstract: Understanding the internal representations of large language models is
crucial for ensuring their reliability and safety, with sparse autoencoders
(SAEs) emerging as a promising interpretability approach. However, current SAE
training methods face feature absorption, where features (or neurons) are
absorbed into each other to minimize $L_1$ penalty, making it difficult to
consistently identify and analyze model behaviors. We introduce Adaptive
Temporal Masking (ATM), a novel training approach that dynamically adjusts
feature selection by tracking activation magnitudes, frequencies, and
reconstruction contributions to compute importance scores that evolve over
time. ATM applies a probabilistic masking mechanism based on statistical
thresholding of these importance scores, creating a more natural feature
selection process. Through extensive experiments on the Gemma-2-2b model, we
demonstrate that ATM achieves substantially lower absorption scores compared to
existing methods like TopK and JumpReLU SAEs, while maintaining excellent
reconstruction quality. These results establish ATM as a principled solution
for learning stable, interpretable features in neural networks, providing a
foundation for more reliable model analysis.

</details>


### [85] [Sparse components distinguish visual pathways & their alignment to neural networks](https://arxiv.org/abs/2510.08858)
*Ammar I Marvi,Nancy G Kanwisher,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 该论文通过稀疏分解方法识别人类视觉皮层三个通路（腹侧、背侧、外侧）的主要表征组件，并开发了稀疏组件对齐（SCA）方法来更精确地测量大脑与深度神经网络之间的表征对齐。


<details>
  <summary>Details</summary>
Motivation: 探索为什么在单个任务上训练的深度神经网络能够很好地建模整个视觉系统，尽管神经科学研究表明视觉皮层的不同通路具有不同的功能过程。

Method: 使用新颖的稀疏分解方法识别各视觉通路的主导表征组件，并开发稀疏组件对齐（SCA）方法来测量大脑与深度神经网络之间的表征对齐。

Result: 发现三个视觉通路的组件响应特征存在明显差异：腹侧通路对脸、地点、身体、文字和食物有选择性；外侧通路对社会互动、隐含运动和手部动作有选择性；背侧通路包含一些难以解释的组件。SCA显示标准视觉DNN与腹侧通路的对齐程度高于背侧或外侧通路。

Conclusion: 稀疏组件对齐方法能够以比传统群体水平几何方法更高的分辨率揭示大脑与机器视觉系统之间的表征对齐差异，提供对系统潜在神经调谐轴的敏感测量。

Abstract: The ventral, dorsal, and lateral streams in high-level human visual cortex
are implicated in distinct functional processes. Yet, deep neural networks
(DNNs) trained on a single task model the entire visual system surprisingly
well, hinting at common computational principles across these pathways. To
explore this inconsistency, we applied a novel sparse decomposition approach to
identify the dominant components of visual representations within each stream.
Consistent with traditional neuroscience research, we find a clear difference
in component response profiles across the three visual streams -- identifying
components selective for faces, places, bodies, text, and food in the ventral
stream; social interactions, implied motion, and hand actions in the lateral
stream; and some less interpretable components in the dorsal stream. Building
on this, we introduce Sparse Component Alignment (SCA), a new method for
measuring representational alignment between brains and machines that better
captures the latent neural tuning of these two visual systems. Using SCA, we
find that standard visual DNNs are more aligned with the ventral than either
dorsal or lateral representations. SCA reveals these distinctions with greater
resolution than conventional population-level geometry, offering a measure of
representational alignment that is sensitive to a system's underlying axes of
neural tuning.

</details>


### [86] [Multi-fidelity Batch Active Learning for Gaussian Process Classifiers](https://arxiv.org/abs/2510.08865)
*Murray Cutforth,Yiming Yang,Tiffany Fan,Serge Guillas,Eric Darve*

Main category: cs.LG

TL;DR: 提出BPMI算法，一种用于多保真度高斯过程分类器的批量主动学习方法，通过一阶泰勒展开计算互信息，在固定计算预算下获得更高预测精度


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程问题依赖昂贵的计算模拟，多保真度方法可以加速参数空间探索，需要研究在二进制模拟输出情况下如何有效分配模拟预算

Method: 引入Bernoulli参数互信息(BPMI)，通过使用链接函数的一阶泰勒展开来规避概率空间中计算互信息的难处理性

Result: 在两个合成测试案例和一个涉及激光点火火箭燃烧器模拟的复杂实际应用中，BPMI在所有实验中表现出优越性能

Conclusion: BPMI在固定计算预算下实现了更高的预测精度，是多保真度高斯过程分类器的有效批量主动学习算法

Abstract: Many science and engineering problems rely on expensive computational
simulations, where a multi-fidelity approach can accelerate the exploration of
a parameter space. We study efficient allocation of a simulation budget using a
Gaussian Process (GP) model in the binary simulation output case. This paper
introduces Bernoulli Parameter Mutual Information (BPMI), a batch active
learning algorithm for multi-fidelity GP classifiers. BPMI circumvents the
intractability of calculating mutual information in the probability space by
employing a first-order Taylor expansion of the link function. We evaluate BPMI
against several baselines on two synthetic test cases and a complex, real-world
application involving the simulation of a laser-ignited rocket combustor. In
all experiments, BPMI demonstrates superior performance, achieving higher
predictive accuracy for a fixed computational budget.

</details>


### [87] [An Improved Model-Free Decision-Estimation Coefficient with Applications in Adversarial MDPs](https://arxiv.org/abs/2510.08882)
*Haolin Liu,Chen-Yu Wei,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文提出了Dig-DEC，一种无需乐观主义、仅通过信息增益驱动探索的模型无关决策估计系数，能够处理对抗性环境，并在混合MDP中实现了首个模型无关的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 现有基于乐观DEC的方法只能处理随机环境，无法扩展到对抗性环境，且存在遗憾边界与模型类大小相关的差距。

Method: 引入Dig-DEC，通过纯信息增益驱动探索，无需乐观主义，改进了在线函数估计过程，包括平均估计误差最小化和贝尔曼完备MDP中的平方误差最小化。

Result: Dig-DEC始终不大于乐观DEC，在特殊情况下更小；在混合MDP中获得了首个模型无关遗憾边界；将遗憾边界从T^{3/4}提升到T^{2/3}（在线策略）和从T^{5/6}提升到T^{7/9}（离线策略）；在贝尔曼完备MDP中将遗憾边界从T^{2/3}提升到√T。

Conclusion: Dig-DEC填补了DEC方法在对抗性环境中的空白，首次使DEC方法在贝尔曼完备MDP中达到与乐观主义方法相当的性能。

Abstract: We study decision making with structured observation (DMSO). Previous work
(Foster et al., 2021b, 2023a) has characterized the complexity of DMSO via the
decision-estimation coefficient (DEC), but left a gap between the regret upper
and lower bounds that scales with the size of the model class. To tighten this
gap, Foster et al. (2023b) introduced optimistic DEC, achieving a bound that
scales only with the size of the value-function class. However, their
optimism-based exploration is only known to handle the stochastic setting, and
it remains unclear whether it extends to the adversarial setting.
  We introduce Dig-DEC, a model-free DEC that removes optimism and drives
exploration purely by information gain. Dig-DEC is always no larger than
optimistic DEC and can be much smaller in special cases. Importantly, the
removal of optimism allows it to handle adversarial environments without
explicit reward estimators. By applying Dig-DEC to hybrid MDPs with stochastic
transitions and adversarial rewards, we obtain the first model-free regret
bounds for hybrid MDPs with bandit feedback under several general transition
structures, resolving the main open problem left by Liu et al. (2025).
  We also improve the online function-estimation procedure in model-free
learning: For average estimation error minimization, we refine the estimator in
Foster et al. (2023b) to achieve sharper concentration, improving their regret
bounds from $T^{3/4}$ to $T^{2/3}$ (on-policy) and from $T^{5/6}$ to $T^{7/9}$
(off-policy). For squared error minimization in Bellman-complete MDPs, we
redesign their two-timescale procedure, improving the regret bound from
$T^{2/3}$ to $\sqrt{T}$. This is the first time a DEC-based method achieves
performance matching that of optimism-based approaches (Jin et al., 2021; Xie
et al., 2023) in Bellman-complete MDPs.

</details>


### [88] [Pinpointing crucial steps: Attribution-based Credit Assignment for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.08899)
*Junxi Yin,Haisen Luo,Zhenyu Li,Yihua Liu,Dan Liu,Zequn Li,Xiaohang Xu*

Main category: cs.LG

TL;DR: 提出了ACPO框架，通过难度感知课程学习和基于归因的贡献度评估，解决RLVR中探索与利用的平衡问题，在复杂推理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习与可验证奖励方法在平衡探索与利用方面存在困难，导致中间步骤信用分配不准确和过早熵崩溃，限制了模型性能。

Method: ACPO框架采用分阶段方法，包含难度感知课程学习，通过轨迹语义分割和基于归因的表征动态调节策略熵，同时使用因子化奖励系统精确量化每个推理步骤的层次贡献。

Result: 在AIME、MATH和AMC等具有挑战性的基准测试中，ACPO显著优于现有的最先进方法。

Conclusion: ACPO通过改进探索和利用的平衡，有效解决了RLVR中的关键问题，在复杂推理任务上取得了卓越性能。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) enhances complex
reasoning in LLMs, current methods struggle to balance exploration and
exploitation. This leads to critical issues like inaccurate credit assignment
for intermediate steps and premature entropy collapse, limiting model
performance. To address this, we introduce Attribution-based Contribution to
Policy Optimization (ACPO), a phased framework that incorporates a
difficulty-aware curriculum. ACPO improves exploration by using trajectory
semantic segmentation and an attribution-based representation to dynamically
regulate policy entropy, thus mitigating its collapse. Concurrently, it
enhances exploitation with a factorized reward system that precisely quantifies
the hierarchical contribution of each reasoning step, ensuring accurate credit
assignment. Extensive experiments on challenging benchmarks, including AIME,
MATH, and AMC, demonstrate that ACPO significantly outperforms existing
state-of-the-art approaches.

</details>


### [89] [A Frequency-Domain Analysis of the Multi-Armed Bandit Problem: A New Perspective on the Exploration-Exploitation Trade-off](https://arxiv.org/abs/2510.08908)
*Di Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种频域分析框架，将多臂老虎机问题重新表述为信号处理问题，证明了UCB算法中的置信区间项在频域中等效于对不确定频谱分量的时变增益。


<details>
  <summary>Details</summary>
Motivation: 现有老虎机算法分析主要从时域和累积遗憾角度进行，难以刻画学习过程的动态特性，需要新的分析框架来理解算法的动态行为。

Method: 构建频域老虎机模型，将每个臂的奖励估计视为频谱分量，其不确定性对应分量频率，将老虎机算法解释为自适应滤波器。

Result: 证明了UCB算法置信区间项在频域中等效于时变增益，该增益与访问次数的平方根成反比，并推导了关于探索率衰减的有限时间动态界限。

Conclusion: 该理论不仅为经典算法提供了新颖直观的物理解释，还为设计具有自适应参数调整的下一代算法奠定了严格理论基础。

Abstract: The stochastic multi-armed bandit (MAB) problem is one of the most
fundamental models in sequential decision-making, with the core challenge being
the trade-off between exploration and exploitation. Although algorithms such as
Upper Confidence Bound (UCB) and Thompson Sampling, along with their regret
theories, are well-established, existing analyses primarily operate from a
time-domain and cumulative regret perspective, struggling to characterize the
dynamic nature of the learning process. This paper proposes a novel
frequency-domain analysis framework, reformulating the bandit process as a
signal processing problem. Within this framework, the reward estimate of each
arm is viewed as a spectral component, with its uncertainty corresponding to
the component's frequency, and the bandit algorithm is interpreted as an
adaptive filter. We construct a formal Frequency-Domain Bandit Model and prove
the main theorem: the confidence bound term in the UCB algorithm is equivalent
in the frequency domain to a time-varying gain applied to uncertain spectral
components, a gain inversely proportional to the square root of the visit
count. Based on this, we further derive finite-time dynamic bounds concerning
the exploration rate decay. This theory not only provides a novel and intuitive
physical interpretation for classical algorithms but also lays a rigorous
theoretical foundation for designing next-generation algorithms with adaptive
parameter adjustment.

</details>


### [90] [Simple and Robust Forecasting of Spatiotemporally Correlated Small Earth Data with A Tabular Foundation Model](https://arxiv.org/abs/2510.08920)
*Yuting Yang,Gang Mei,Zhengjing Ma,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 提出了一种针对小地球数据的时空预测方法，通过量化时空模式并利用表格基础模型，在不同场景下实现准确预测，优于传统深度学习和基础模型。


<details>
  <summary>Details</summary>
Motivation: 小地球数据具有短期监测变异性有限、时空相关性强但稀疏的特点，传统深度学习需要任务特定训练，而基础模型存在对预训练分布全局均值的预测偏差。

Method: 量化小地球数据的时空模式特征，然后利用表格基础模型进行跨场景的准确预测。

Result: 在三种典型场景下的比较结果显示，该方法在大多数情况下比图深度学习模型（T-GCN）和表格基础模型（TabPFN）具有更优的准确性和更强的鲁棒性。

Conclusion: 该方法为小地球数据的时空预测提供了一种简单而有效的解决方案，能够克服传统方法的局限性。

Abstract: Small Earth data are geoscience observations with limited short-term
monitoring variability, providing sparse but meaningful measurements, typically
exhibiting spatiotemporal correlations. Spatiotemporal forecasting on such data
is crucial for understanding geoscientific processes despite their small scale.
However, conventional deep learning models for spatiotemporal forecasting
requires task-specific training for different scenarios. Foundation models do
not need task-specific training, but they often exhibit forecasting bias toward
the global mean of the pretraining distribution. Here we propose a simple and
robust approach for spatiotemporally correlated small Earth data forecasting.
The essential idea is to characterize and quantify spatiotemporal patterns of
small Earth data and then utilize tabular foundation models for accurate
forecasting across different scenarios. Comparative results across three
typical scenarios demonstrate that our forecasting approach achieves superior
accuracy compared to the graph deep learning model (T-GCN) and tabular
foundation model (TabPFN) in the majority of instances, exhibiting stronger
robustness.

</details>


### [91] [AB-PINNs: Adaptive-Basis Physics-Informed Neural Networks for Residual-Driven Domain Decomposition](https://arxiv.org/abs/2510.08924)
*Jonah Botvinick-Greenhouse,Wael H. Ali,Mouhacine Benosman,Saviz Mowlavi*

Main category: cs.LG

TL;DR: 提出了自适应基物理信息神经网络（AB-PINNs），一种动态域分解方法，通过自适应调整子域和在高残差区域引入新子域来改进PINNs训练，特别适用于多尺度问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理多尺度问题时面临挑战，静态域分解方法需要大量超参数调优且容易陷入局部最小值。需要一种能够动态适应解内在特征的域分解方法。

Method: 使用自适应基PINNs，在训练过程中动态调整现有子域以适应解的固有特征，并在高残差损失区域引入新子域，类似于经典网格细化技术。

Result: AB-PINNs在解决各种复杂多尺度偏微分方程方面表现出有效性，能够捕捉不同尺度的解特征，减少对超参数调优的需求。

Conclusion: AB-PINNs提供了一种灵活的域分解方法，特别适合多尺度问题，能够防止收敛到不希望的局部最小值，并在复杂PDE求解中表现出优越性能。

Abstract: We introduce adaptive-basis physics-informed neural networks (AB-PINNs), a
novel approach to domain decomposition for training PINNs in which existing
subdomains dynamically adapt to the intrinsic features of the unknown solution.
Drawing inspiration from classical mesh refinement techniques, we also modify
the domain decomposition on-the-fly throughout training by introducing new
subdomains in regions of high residual loss, thereby providing additional
expressive power where the solution of the differential equation is challenging
to represent. Our flexible approach to domain decomposition is well-suited for
multiscale problems, as different subdomains can learn to capture different
scales of the underlying solution. Moreover, the ability to introduce new
subdomains during training helps prevent convergence to unwanted local minima
and can reduce the need for extensive hyperparameter tuning compared to static
domain decomposition approaches. Throughout, we present comprehensive numerical
results which demonstrate the effectiveness of AB-PINNs at solving a variety of
complex multiscale partial differential equations.

</details>


### [92] [MATT-CTR: Unleashing a Model-Agnostic Test-Time Paradigm for CTR Prediction with Confidence-Guided Inference Paths](https://arxiv.org/abs/2510.08932)
*Moyu Zhang,Yun Chen,Yujun Jin,Jinxin Hu,Yu Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: 提出了一种模型无关的测试时优化范式MATT，通过置信度引导的多推理路径生成来提升CTR模型的预测性能，特别是在处理低频特征组合时。


<details>
  <summary>Details</summary>
Motivation: 现有CTR模型优化主要关注训练阶段，忽视了推理阶段的优化机会。低频特征组合会降低预测性能，导致不可靠输出。

Method: 使用分层概率哈希方法量化特征组合置信度，基于置信度分数通过迭代采样生成多个实例特定的推理路径，最后聚合多路径预测结果。

Result: 离线和在线A/B测试验证了MATT在现有CTR模型上的兼容性和有效性。

Conclusion: MATT能够解锁已训练CTR模型的预测潜力，通过测试时优化显著提升预测性能。

Abstract: Recently, a growing body of research has focused on either optimizing CTR
model architectures to better model feature interactions or refining training
objectives to aid parameter learning, thereby achieving better predictive
performance. However, previous efforts have primarily focused on the training
phase, largely neglecting opportunities for optimization during the inference
phase. Infrequently occurring feature combinations, in particular, can degrade
prediction performance, leading to unreliable or low-confidence outputs. To
unlock the predictive potential of trained CTR models, we propose a
Model-Agnostic Test-Time paradigm (MATT), which leverages the confidence scores
of feature combinations to guide the generation of multiple inference paths,
thereby mitigating the influence of low-confidence features on the final
prediction. Specifically, to quantify the confidence of feature combinations,
we introduce a hierarchical probabilistic hashing method to estimate the
occurrence frequencies of feature combinations at various orders, which serve
as their corresponding confidence scores. Then, using the confidence scores as
sampling probabilities, we generate multiple instance-specific inference paths
through iterative sampling and subsequently aggregate the prediction scores
from multiple paths to conduct robust predictions. Finally, extensive offline
experiments and online A/B tests strongly validate the compatibility and
effectiveness of MATT across existing CTR models.

</details>


### [93] [Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning](https://arxiv.org/abs/2510.08938)
*Zhen Yang,Yansong Ma,Lei Chen*

Main category: cs.LG

TL;DR: 提出了Meta-Policy Controller (MPC)框架，通过元学习动态调整KL散度系数和狄利克雷先验强度，解决传统EDL方法在动态数据分布下校准不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统证据深度学习方法的静态超参数限制了在动态数据分布中的适应性，导致高风险决策任务中校准和泛化能力不足。

Method: 采用双层优化方法：内层通过动态配置的损失函数更新模型参数；外层通过策略网络基于多目标奖励优化KL散度系数和类别特定的狄利克雷先验强度。

Result: 实验结果表明MPC显著提升了模型预测的可靠性和校准性，改善了不确定性校准、预测准确性和基于置信度的样本拒绝后的性能保持。

Conclusion: MPC框架通过可学习的狄利克雷先验实现了对类别分布和训练动态的灵活适应，显著增强了不确定性建模能力。

Abstract: Traditional Evidence Deep Learning (EDL) methods rely on static
hyperparameter for uncertainty calibration, limiting their adaptability in
dynamic data distributions, which results in poor calibration and
generalization in high-risk decision-making tasks. To address this limitation,
we propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework
that adjusts the KL divergence coefficient and Dirichlet prior strengths for
optimal uncertainty modeling. Specifically, MPC employs a bi-level optimization
approach: in the inner loop, model parameters are updated through a dynamically
configured loss function that adapts to the current training state; in the
outer loop, a policy network optimizes the KL divergence coefficient and
class-specific Dirichlet prior strengths based on multi-objective rewards
balancing prediction accuracy and uncertainty quality. Unlike previous methods
with fixed priors, our learnable Dirichlet prior enables flexible adaptation to
class distributions and training dynamics. Extensive experimental results show
that MPC significantly enhances the reliability and calibration of model
predictions across various tasks, improving uncertainty calibration, prediction
accuracy, and performance retention after confidence-based sample rejection.

</details>


### [94] [Variability Aware Recursive Neural Network (VARNN): A Residual-Memory Model for Capturing Temporal Deviation in Sequence Regression Modeling](https://arxiv.org/abs/2510.08944)
*Haroon Gharwi,Kai Shu*

Main category: cs.LG

TL;DR: VARNN是一种新颖的残差感知神经网络架构，通过显式学习近期预测残差的误差记忆来重新校准后续预测，提升时间序列回归在非平稳环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列数据存在非平稳行为、制度转换和时间变化噪声，这会降低标准回归模型的鲁棒性。

Method: VARNN在传统前馈预测器基础上增加了学习的误差记忆状态，该状态从短期上下文步骤的残差中更新，作为变异性和漂移的信号，并调节当前时间步的最终预测。

Result: 在多种数据集领域的实验结果表明，VARNN实现了优于静态、动态和循环基线的性能，在漂移和波动环境下获得更低的测试MSE，且计算开销最小。

Conclusion: VARNN模型在漂移和波动环境下提供鲁棒预测，展示了其作为时间序列学习有前景框架的潜力。

Abstract: Real-world time series data exhibit non-stationary behavior, regime shifts,
and temporally varying noise (heteroscedastic) that degrade the robustness of
standard regression models. We introduce the Variability-Aware Recursive Neural
Network (VARNN), a novel residual-aware architecture for supervised time-series
regression that learns an explicit error memory from recent prediction
residuals and uses it to recalibrate subsequent predictions. VARNN augments a
feed-forward predictor with a learned error-memory state that is updated from
residuals over a short context steps as a signal of variability and drift, and
then conditions the final prediction at the current time step. Across diverse
dataset domains, appliance energy, healthcare, and environmental monitoring,
experimental results demonstrate VARNN achieves superior performance and
attains lower test MSE with minimal computational overhead over static,
dynamic, and recurrent baselines. Our findings show that the VARNN model offers
robust predictions under a drift and volatility environment, highlighting its
potential as a promising framework for time-series learning.

</details>


### [95] [When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach](https://arxiv.org/abs/2510.08952)
*Zhihan Zhang,Xunkai Li,Yilong Zuo,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: LAGA是一个统一的多智能体框架，通过数据中心的视角解决文本属性图的质量问题，在稀疏性、噪声和不平衡等九种场景中显著提升图质量和学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络和LLM增强的GNN在文本属性图上的性能对输入质量高度敏感，现有方法主要关注模型架构改进而忽视了系统性的图数据优化。

Method: 提出LAGA多智能体框架，包含检测、规划、行动和评估四个协作智能体，行动智能体采用双编码器和三目标设计来捕获跨模态互补信息并进行整体图质量增强。

Result: 在九种场景的实验表明，LAGA显著提升了图质量，并在各种任务和骨干网络上实现了最先进的性能。

Conclusion: 数据中心的质量优化是实现可靠文本属性图和鲁棒图学习的关键。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation that
combines structural connections with fine-grained semantics, supporting a wide
range of data-centric applications. However, the performance of graph neural
networks (GNNs) on TAGs is highly sensitive to input quality. Our empirical
study shows that both traditional GNNs and LLM-enhanced GNNs suffer significant
degradation across nine representative scenarios of sparsity, noise, and
imbalance, highlighting graph quality as a critical bottleneck. Existing
approaches mainly focus on improving model architectures, while neglecting
systematic optimization of TAG data itself, leading to limited effectiveness in
practice. To address this gap, we propose LAGA (Large Language and Graph
Agent), a unified multi-agent framework that treats graph quality control as a
first-class, data-centric problem. LAGA integrates four collaborative
agents-detection, planning, action, and evaluation-into an automated closed
loop. At its core, the action agent employs a dual-encoder and tri-objective
design to capture complementary information across modalities and perform
holistic graph quality enhancement. Experiments across nine scenarios show that
LAGA improves graph quality and achieves state-of-the-art performance across
various tasks and backbones, validating data-centric quality optimization as
key to reliable TAGs and robust graph learning.

</details>


### [96] [Analytical Survey of Learning with Low-Resource Data: From Analysis to Investigation](https://arxiv.org/abs/2510.08962)
*Xiaofeng Cao,Mingwei Xu,Xin Yu,Jiangchao Yao,Wei Ye,Shengjun Huang,Minling Zhang,Ivor W. Tsang,Yew Soon Ong,James T. Kwok,Heng Tao Shen*

Main category: cs.LG

TL;DR: 该调查论文使用PAC框架中的不可知主动采样理论分析低资源数据学习的泛化误差和标签复杂度，研究针对低资源数据优化的策略，并概述多种可从低资源数据中受益的学习范式。


<details>
  <summary>Details</summary>
Motivation: 高资源数据学习虽然成功，但数据标注和模型训练成本高昂。AI研究的一个基本目标是在有限资源数据下实现稳健的泛化。

Method: 采用PAC框架中的不可知主动采样理论分析泛化误差和标签复杂度，研究梯度信息优化、元迭代优化、几何感知优化和LLM驱动的优化等策略。

Result: 分析揭示了低资源数据学习的理论基础，并提出了一系列优化策略和多种学习范式，包括领域迁移、强化反馈和层次结构建模。

Conclusion: 总结了关键发现，强调了这些发现对低资源数据学习的影响，为在有限数据条件下实现有效学习提供了理论指导和实践策略。

Abstract: Learning with high-resource data has demonstrated substantial success in
artificial intelligence (AI); however, the costs associated with data
annotation and model training remain significant. A fundamental objective of AI
research is to achieve robust generalization with limited-resource data. This
survey employs agnostic active sampling theory within the Probably
Approximately Correct (PAC) framework to analyze the generalization error and
label complexity associated with learning from low-resource data in both
model-agnostic supervised and unsupervised settings. Based on this analysis, we
investigate a suite of optimization strategies tailored for low-resource data
learning, including gradient-informed optimization, meta-iteration
optimization, geometry-aware optimization, and LLMs-powered optimization.
Furthermore, we provide a comprehensive overview of multiple learning paradigms
that can benefit from low-resource data, including domain transfer,
reinforcement feedback, and hierarchical structure modeling. Finally, we
conclude our analysis and investigation by summarizing the key findings and
highlighting their implications for learning with low-resource data.

</details>


### [97] [HiBBO: HiPPO-based Space Consistency for High-dimensional Bayesian Optimisation](https://arxiv.org/abs/2510.08965)
*Junyu Xuan,Wenlong Chen,Yingzhen Li*

Main category: cs.LG

TL;DR: HiBBO是一个新的贝叶斯优化框架，通过将空间一致性引入VAE的潜在空间构建，使用HiPPO方法减少潜在空间和原始空间之间的函数分布不匹配，在高维基准任务中优于现有VAE-BO方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于VAE的贝叶斯优化方法由于仅使用重构损失，导致潜在空间和原始空间之间存在函数分布不匹配，从而影响优化性能。

Method: 提出HiBBO框架，在VAE潜在空间构建中引入空间一致性，使用HiPPO（长期序列建模方法）来减少函数分布不匹配。

Result: 在高维基准任务上的实验表明，HiBBO在收敛速度和解决方案质量方面优于现有的VAE-BO方法。

Conclusion: 该工作弥合了高维序列表示学习和高效贝叶斯优化之间的差距，为神经架构搜索、材料科学等领域提供了更广泛的应用可能。

Abstract: Bayesian Optimisation (BO) is a powerful tool for optimising expensive
blackbox functions but its effectiveness diminishes in highdimensional spaces
due to sparse data and poor surrogate model scalability While Variational
Autoencoder (VAE) based approaches address this by learning low-dimensional
latent representations the reconstructionbased objective function often brings
the functional distribution mismatch between the latent space and original
space leading to suboptimal optimisation performance In this paper we first
analyse the reason why reconstructiononly loss may lead to distribution
mismatch and then propose HiBBO a novel BO framework that introduces the space
consistency into the latent space construction in VAE using HiPPO - a method
for longterm sequence modelling - to reduce the functional distribution
mismatch between the latent space and original space Experiments on
highdimensional benchmark tasks demonstrate that HiBBO outperforms existing
VAEBO methods in convergence speed and solution quality Our work bridges the
gap between high-dimensional sequence representation learning and efficient
Bayesian Optimisation enabling broader applications in neural architecture
search materials science and beyond.

</details>


### [98] [Learning Regularizers: Learning Optimizers that can Regularize](https://arxiv.org/abs/2510.08968)
*Suraj Kumar Sahoo,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: 研究表明学习优化器(LOs)能够学习并内化传统正则化技术，无需在目标函数中显式应用正则化。


<details>
  <summary>Details</summary>
Motivation: 探索学习优化器是否能学习正则化效果，挑战传统显式正则化的必要性。

Method: 通过标准基准测试比较有/无显式正则化的学习优化器性能，验证其学习正则化效果的能力。

Result: 正则化学习优化器在测试准确率和泛化能力上始终优于未正则化版本，并能将正则化效果迁移到新任务。

Conclusion: 学习优化器能够内在地学习正则化特性，挑战了传统显式正则化的必要性。

Abstract: Learned Optimizers (LOs), a type of Meta-learning, have gained traction due
to their ability to be parameterized and trained for efficient optimization.
Traditional gradient-based methods incorporate explicit regularization
techniques such as Sharpness-Aware Minimization (SAM), Gradient-norm Aware
Minimization (GAM), and Gap-guided Sharpness-Aware Minimization (GSAM) to
enhance generalization and convergence. In this work, we explore a fundamental
question: \textbf{Can regularizers be learned?} We empirically demonstrate that
LOs can be trained to learn and internalize the effects of traditional
regularization techniques without explicitly applying them to the objective
function. We validate this through extensive experiments on standard benchmarks
(including MNIST, FMNIST, CIFAR and Neural Networks such as MLP, MLP-Relu and
CNN), comparing LOs trained with and without access to explicit regularizers.
Regularized LOs consistently outperform their unregularized counterparts in
terms of test accuracy and generalization. Furthermore, we show that LOs retain
and transfer these regularization effects to new optimization tasks by
inherently seeking minima similar to those targeted by these regularizers. Our
results suggest that LOs can inherently learn regularization properties,
\textit{challenging the conventional necessity of explicit optimizee loss
regularization.

</details>


### [99] [Diagnosing and Mitigating System Bias in Self-Rewarding RL](https://arxiv.org/abs/2510.08977)
*Chuyi Tan,Peiwen Yuan,Xinglin Wang,Yiwei Li,Shaoxiong Feng,Yueqi Zhang,Jiayi Shi,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: 本文提出RLER方法来解决RLIR中的系统偏差问题，通过集成多个模型和自适应奖励插值来稳定训练，在无标注数据上接近RLVR的性能。


<details>
  <summary>Details</summary>
Motivation: RLVR依赖标注数据难以扩展，RLIR在无标注设置中可持续扩展但存在性能不稳定问题，主要原因是模型对高置信度轨迹的过度奖励估计偏差。

Method: 提出RLER方法，通过集成多个模型、自适应奖励插值和轨迹选择来缓解系统偏差，使用三个指标量化偏差：ρ_noise、ρ_selfbias和ρ_symbias。

Result: RLER相比RLIR提升13.6%，仅比RLVR低3.6%，在无标注样本上实现稳定扩展。

Conclusion: RLER有效解决了RLIR中的偏差累积问题，实现了接近RLVR的性能，在无标注数据场景中具有高度适用性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) scales the reasoning
ability of large language models (LLMs) but remains bottlenecked by limited
labeled samples for continued data scaling. Reinforcement learning with
intrinsic rewards (RLIR), where the policy model assigns rewards to its own
rollouts, enables sustainable scaling in unlabeled settings, yet its
performance and stability lag behind RLVR. We trace this gap to a system bias:
the model tends to overestimate its high-confidence rollouts, leading to biased
and unstable reward estimation. This bias accumulates as training progresses,
with deviations from the oracle drifting toward over-reward, causing unstable
training. We characterize this bias using three metrics: $\rho_{\text{noise}}$,
$\rho_{\text{selfbias}}$, and $\rho_{\text{symbias}}$. We find that
$\rho_{\text{noise}}$ and $\rho_{\text{symbias}}$ impact convergence, while
$\rho_{\text{selfbias}}$ amplifies both correct and incorrect updates, leading
to instability. To mitigate this, we propose reinforcement learning with
ensembled rewards (RLER), which aggregates diverse models and adapts reward
interpolation and rollout selection. Extensive experiments show that RLER
improves by +13.6% over RLIR and is only 3.6% below RLVR, achieving stable
scaling on unlabeled samples, making it highly applicable.

</details>


### [100] [FedL2T: Personalized Federated Learning with Two-Teacher Distillation for Seizure Prediction](https://arxiv.org/abs/2510.08984)
*Jionghao Lou,Jian Zhang,Zhongmei Li,Lanlan Chen,Enbo Feng*

Main category: cs.LG

TL;DR: FedL2T是一个个性化的联邦学习框架，通过双教师知识蒸馏策略解决癫痫预测中的客户端异质性问题，在保护隐私的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 癫痫预测需要大量标记的EEG数据，但数据获取困难且涉及隐私问题。联邦学习虽然能保护隐私，但在客户端异质性场景下性能不佳。

Method: 提出FedL2T框架，采用双教师知识蒸馏策略：每个客户端同时从全局聚合模型和动态分配的同伴模型学习；使用自适应多级蒸馏对齐预测输出和中间特征表示；引入近端正则化项增强训练稳定性。

Result: 在两个EEG数据集上的实验表明，FedL2T优于现有联邦学习方法，特别是在低标记条件下；具有快速稳定的收敛性，减少通信轮次和开销。

Conclusion: FedL2T为隐私敏感的医疗场景提供了可靠且个性化的癫痫预测解决方案。

Abstract: The training of deep learning models in seizure prediction requires large
amounts of Electroencephalogram (EEG) data. However, acquiring sufficient
labeled EEG data is difficult due to annotation costs and privacy constraints.
Federated Learning (FL) enables privacy-preserving collaborative training by
sharing model updates instead of raw data. However, due to the inherent
inter-patient variability in real-world scenarios, existing FL-based seizure
prediction methods struggle to achieve robust performance under heterogeneous
client settings. To address this challenge, we propose FedL2T, a personalized
federated learning framework that leverages a novel two-teacher knowledge
distillation strategy to generate superior personalized models for each client.
Specifically, each client simultaneously learns from a globally aggregated
model and a dynamically assigned peer model, promoting more direct and enriched
knowledge exchange. To ensure reliable knowledge transfer, FedL2T employs an
adaptive multi-level distillation strategy that aligns both prediction outputs
and intermediate feature representations based on task confidence. In addition,
a proximal regularization term is introduced to constrain personalized model
updates, thereby enhancing training stability. Extensive experiments on two EEG
datasets demonstrate that FedL2T consistently outperforms state-of-the-art FL
methods, particularly under low-label conditions. Moreover, FedL2T exhibits
rapid and stable convergence toward optimal performance, thereby reducing the
number of communication rounds and associated overhead. These results
underscore the potential of FedL2T as a reliable and personalized solution for
seizure prediction in privacy-sensitive healthcare scenarios.

</details>


### [101] [Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search](https://arxiv.org/abs/2510.08992)
*Kamel Alrashedy,Vriksha Srihari,Zulfiqar Zaidi,Ridam Srivastava,Pradyumna Tambwekar,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出Constraints-of-Thought (Const-o-T)框架，通过(intent, constraint)对结构化表示来引导蒙特卡洛树搜索，解决LLMs在多步规划中难以满足符号约束和用户意图的问题。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法如CoT、ToT等虽然扩展了搜索空间，但经常产生不可行动作或幻觉步骤，无法确保计划与高层用户意图对齐并满足符号约束。

Method: 使用(intent, constraint)对作为结构化先验，压缩搜索空间并强制有效性，将Const-o-T集成到MCTS中，通过约束剪枝不可行分支并引导探索语义有效动作。

Result: 在Risk游戏、CAD代码生成和算术推理三个领域中，该方法优于基线，产生更高准确性和更强的结构对齐。

Conclusion: Const-o-T为约束引导推理提供了可泛化的基础，使LLMs能够进行更高效、约束对齐和领域适应的规划。

Abstract: While researchers have made significant progress in enabling large language
models (LLMs) to perform multi-step planning, LLMs struggle to ensure that
those plans align with high-level user intent and satisfy symbolic constraints,
especially in complex, multi-step domains. Existing reasoning approaches such
as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented
methods, expand the search space but often yield infeasible actions or
hallucinated steps. To overcome these limitations, we propose
Constraints-of-Thought (Const-o-T), a framework that provides a structured
prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically
meaningful paths. Each reasoning step is represented as an (intent, constraint)
pair, which serves both to compress the search space and enforce validity.
Unlike prior methods that merely generate reasoning traces or validate outputs
post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search
toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a
structured representation of intent-constraint pairs constraints prune
infeasible branches and guide exploration toward semantically valid actions,
improving planning efficiency and verifiable decision-making. We demonstrate
across three domains Risk game, CAD code generation, and arithmetic reasoning
that our approach outperforms baselines, yielding higher accuracy and stronger
structural alignment. Our contribution is to demonstrate that Const-of-T offers
a generalizable foundation for constraint-guided reasoning, enabling more
efficient, constraint-aligned, and domain-adaptable planning with LLMs.

</details>


### [102] [PlatformX: An End-to-End Transferable Platform for Energy-Efficient Neural Architecture Search](https://arxiv.org/abs/2510.08993)
*Xiaolong Tu,Dawei Chen,Kyungtae Han,Onur Altintas,Haoxin Wang*

Main category: cs.LG

TL;DR: PlatformX是一个完全自动化的硬件感知神经架构搜索框架，通过集成能量驱动搜索空间、可转移的核级能量预测器、多目标搜索算法和高分辨率运行时能量分析系统，显著降低了搜索开销，在移动平台上实现了比MobileNet-V2更好的准确性和能效。


<details>
  <summary>Details</summary>
Motivation: 现有的硬件感知神经架构搜索方法由于高时间成本、大量手动配置以及在多样化硬件平台上扩展性差，在实际部署中不实用。

Method: PlatformX集成四个关键组件：能量驱动搜索空间、可转移的核级能量预测器、基于Pareto的多目标搜索算法、高分辨率运行时能量分析系统。

Result: 在多个移动平台上评估显示，PlatformX显著减少搜索开销，同时保持准确性和能量保真度，识别出的模型准确率高达0.94或每推理仅需0.16mJ，均优于MobileNet-V2。

Conclusion: PlatformX提供了一个实用且可扩展的解决方案，克服了现有HW-NAS方法的局限性，实现了高效的自动化神经网络设计。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) has emerged as a powerful
tool for designing efficient deep neural networks (DNNs) tailored to edge
devices. However, existing methods remain largely impractical for real-world
deployment due to their high time cost, extensive manual profiling, and poor
scalability across diverse hardware platforms with complex, device-specific
energy behavior. In this paper, we present PlatformX, a fully automated and
transferable HW-NAS framework designed to overcome these limitations. PlatformX
integrates four key components: (i) an energy-driven search space that expands
conventional NAS design by incorporating energy-critical configurations,
enabling exploration of high-efficiency architectures; (ii) a transferable
kernel-level energy predictor across devices and incrementally refined with
minimal on-device samples; (iii) a Pareto-based multi-objective search
algorithm that balances energy and accuracy to identify optimal trade-offs; and
(iv) a high-resolution runtime energy profiling system that automates on-device
power measurement using external monitors without human intervention. We
evaluate PlatformX across multiple mobile platforms, showing that it
significantly reduces search overhead while preserving accuracy and energy
fidelity. It identifies models with up to 0.94 accuracy or as little as 0.16 mJ
per inference, both outperforming MobileNet-V2 in accuracy and efficiency. Code
and tutorials are available at github.com/amai-gsu/PlatformX.

</details>


### [103] [SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions](https://arxiv.org/abs/2510.08999)
*Ziyi Wang,Nan Jiang,Guang Lin,Qifan Song*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯变分学习的统一框架SQS，同时进行剪枝和低比特量化，在保持性能的同时实现更高的压缩率


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独使用权重剪枝或低比特量化，导致压缩率不理想，需要同时处理这两种压缩技术以获得更好的压缩效果

Method: 使用spike-and-slab先验诱导稀疏性，采用高斯混合模型(GMMs)建模量化权重，实现低比特精度，通过贝叶斯变分学习统一框架

Result: 在ResNet、BERT-base、Llama3和Qwen2.5模型上的实验表明，该方法比现有方法获得更高的压缩率，同时保持可比的性能下降

Conclusion: 提出的SQS框架能够有效同时进行剪枝和量化，在保持模型性能的同时实现更高的压缩率，为资源受限设备上的模型部署提供了有效解决方案

Abstract: Compressing large-scale neural networks is essential for deploying models on
resource-constrained devices. Most existing methods adopt weight pruning or
low-bit quantization individually, often resulting in suboptimal compression
rates to preserve acceptable performance drops. We introduce a unified
framework for simultaneous pruning and low-bit quantization via Bayesian
variational learning (SQS), which achieves higher compression rates than prior
baselines while maintaining comparable performance. The key idea is to employ a
spike-and-slab prior to inducing sparsity and model quantized weights using
Gaussian Mixture Models (GMMs) to enable low-bit precision. In theory, we
provide the consistent result of our proposed variational approach to a sparse
and quantized deep neural network. Extensive experiments on compressing ResNet,
BERT-base, Llama3, and Qwen2.5 models show that our method achieves higher
compression rates than a line of existing methods with comparable performance
drops.

</details>


### [104] [LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and Watermarked Data](https://arxiv.org/abs/2510.09007)
*Changsheng Wang,Yihua Zhang,Dennis Wei,Jinghan Jia,Pin-Yu Chen,Sijia Liu*

Main category: cs.LG

TL;DR: 研究发现LLM遗忘方法对噪声遗忘数据具有鲁棒性，只要核心语义信号得以保留，即使表面形式变化显著，遗忘效果依然稳定。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的遗忘数据往往是低质量、重写或带水印的，而现有遗忘方法假设访问干净、定义明确的遗忘数据，这引发了遗忘可靠性的担忧。

Method: 系统性地在噪声遗忘集上基准测试最先进的LLM遗忘方法RMU和NPO，并提出基于显著性的解释框架。

Result: 遗忘对数据扰动表现出惊人的鲁棒性，关键语义组件在表面形式变化下仍保持一致的遗忘驱动作用。

Conclusion: 遗忘算法主要受深层语义线索而非浅层词汇模式引导，这解释了其在噪声数据下的鲁棒性表现。

Abstract: Large language models (LLMs) exhibit remarkable generative capabilities but
raise ethical and security concerns by memorizing sensitive data, reinforcing
biases, and producing harmful content. These risks have spurred interest in LLM
unlearning, the task of removing knowledge associated with undesirable data
from pre-trained models. However, most existing methods assume access to clean,
well-defined forget data samples, whereas real-world forget data could often be
low-quality, synthetically rewritten, or watermarked, casting doubt on the
reliability of unlearning. This work presents the first study of unlearning
under perturbed or low-fidelity forget data, referred to as noisy forget sets.
By systematically benchmarking state-of-the-art LLM unlearning methods, RMU and
NPO, on such noisy forget sets, we find that unlearning remains surprisingly
robust to perturbations, provided that core semantic signals are preserved. To
explain this robustness, we propose a saliency-based interpretation: key
semantic components that drive forgetting remain consistently influential
despite substantial variation in surface form. This suggests that unlearning
algorithms are primarily guided by deep semantic cues rather than shallow
lexical patterns.

</details>


### [105] [Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers](https://arxiv.org/abs/2510.09017)
*Rui Bu,Haofeng Zhong,Wenzheng Chen,Yangyan Li*

Main category: cs.LG

TL;DR: 提出Value-State Gated Attention (VGA)机制，通过基于值向量的可学习门控来打破注意力与值状态之间的恶性循环，有效缓解极端token现象。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的大模型容易受到极端token现象（如注意力汇和值状态流失）的影响，这些问题会降低模型性能、量化保真度和可解释性，源于模型学习到的一种低效的'无操作'行为。

Method: VGA引入一个直接从值向量(V)计算的可学习、数据依赖的门控，用于调制输出。通过理论分析证明，基于值状态自身的门控比先前基于输入嵌入的门控方法更有效地解耦值和注意力分数的更新。

Result: 实验表明VGA显著缓解了注意力汇的形成并稳定了值状态范数，带来了性能提升、鲁棒的量化保真度和增强的模型可解释性。

Conclusion: VGA提供了一种简单、专用且稳定的架构机制，通过直接打破注意力与值状态之间的恶性循环，有效执行'无操作'注意力，解决了极端token现象问题。

Abstract: Large models based on the Transformer architecture are susceptible to
extreme-token phenomena, such as attention sinks and value-state drains. These
issues, which degrade model performance, quantization fidelity, and
interpretability, arise from a problematic mutual reinforcement mechanism where
the model learns an inefficient 'no-op' behavior by focusing attention on
tokens with near-zero value states. In this paper, we propose Value-State Gated
Attention (VGA), a simple, dedicated, and stable architectural mechanism for
performing 'no-op' attention efficiently by directly breaking this cycle. VGA
introduces a learnable, data-dependent gate, computed directly from the value
vectors (V), to modulate the output. Through a theoretical analysis of the
underlying gradients, we show that gating the value-state with a function of
itself is more effective at decoupling value and attention score updates than
prior methods that gate on input embeddings. This creates a direct regulatory
pathway that allows the model to suppress a token's contribution based on its
emergent value representation. Our experiments demonstrate that VGA
significantly mitigates the formation of attention sinks and stabilizes
value-state norms, leading to improved performance, robust quantization
fidelity, and enhanced model interpretability.

</details>


### [106] [Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN Inference](https://arxiv.org/abs/2510.09018)
*Ian Harshbarger,Calvin Chidambaram*

Main category: cs.LG

TL;DR: Slim Scheduler是一个混合调度框架，结合PPO强化学习和贪心调度器，为可瘦身模型协调分布式推理，在准确性和延迟之间实现灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 大多数神经网络调度研究关注固定宽度的静态端到端模型优化，忽视了适应异构硬件和动态运行时条件的动态方法。

Method: 采用分层设计：本地贪心调度器处理请求批处理和实例扩展，PPO路由器学习全局路由策略（设备选择、宽度比例、批配置），降低搜索空间复杂度。

Result: 相比纯随机任务分配基线，在准确率降至最瘦模型（70.3%）时，平均延迟减少96.45%，能耗减少97.31%；也可在提高准确率的同时降低平均延迟和能耗，但延迟和能耗的标准差更高。

Conclusion: 该混合调度框架能有效平衡效率和吞吐量，适应异构硬件环境，实现准确性与性能的灵活权衡。

Abstract: Most neural network scheduling research focuses on optimizing static,
end-to-end models of fixed width, overlooking dynamic approaches that adapt to
heterogeneous hardware and fluctuating runtime conditions. We present Slim
Scheduler, a hybrid scheduling framework that integrates a Proximal Policy
Optimization (PPO) reinforcement learning policy with algorithmic, greedy
schedulers to coordinate distributed inference for slimmable models. Each
server runs a local greedy scheduler that batches compatible requests and
manages instance scaling based on VRAM and utilization constraints, while the
PPO router learns global routing policies for device selection, width ratio,
and batch configuration. This hierarchical design reduces search space
complexity, mitigates overfitting to specific hardware, and balances efficiency
and throughput. Compared to a purely randomized task distribution baseline,
Slim Scheduler can achieve various accuracy and latency trade-offs such as: A
96.45% reduction in mean latency and a 97.31% reduction in energy usage
dropping accuracy to the slimmest model available (70.3%). It can then
accomplish an overall reduction in average latency plus energy consumption with
an increase in accuracy at the cost of higher standard deviations of said
latency and energy, effecting overall task throughput.

</details>


### [107] [MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion](https://arxiv.org/abs/2510.09020)
*Zekai Chen,Xunkai Li,Sirui Zhang,Henan Sun,Jia Li,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: MagicDock是一个用于从头配体设计的框架，通过梯度反转和可微分表面建模解决现有方法的局限性，在9个场景中相比SOTA方法分别提升27.1%和11.7%。


<details>
  <summary>Details</summary>
Motivation: 解决现有从头配体设计方法存在的三个主要问题：伪从头设计、有限的对接建模和僵化的配体类型限制。

Method: 采用梯度反转框架，将受体和配体的对接知识通过结合预测实例化为反向梯度流；强调可微分表面建模，使用可学习的3D点云表示；为不同配体类型提供定制化设计并集成到统一框架中。

Result: 在9个场景的广泛实验中，MagicDock相比专门用于蛋白质或分子配体设计的SOTA基线分别实现了27.1%和11.7%的平均改进。

Conclusion: MagicDock通过创新的梯度反转框架和可微分表面建模，有效解决了从头配体设计的关键挑战，为生物医学应用提供了强大的工具。

Abstract: De novo ligand design is a fundamental task that seeks to generate protein or
molecule candidates that can effectively dock with protein receptors and
achieve strong binding affinity entirely from scratch. It holds paramount
significance for a wide spectrum of biomedical applications. However, most
existing studies are constrained by the \textbf{Pseudo De Novo},
\textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. To
address these issues, we propose MagicDock, a forward-looking framework
grounded in the progressive pipeline and differentiable surface modeling. (1)
We adopt a well-designed gradient inversion framework. To begin with, general
docking knowledge of receptors and ligands is incorporated into the backbone
model. Subsequently, the docking knowledge is instantiated as reverse gradient
flows by binding prediction, which iteratively guide the de novo generation of
ligands. (2) We emphasize differentiable surface modeling in the docking
process, leveraging learnable 3D point-cloud representations to precisely
capture binding details, thereby ensuring that the generated ligands preserve
docking validity through direct and interpretable spatial fingerprints. (3) We
introduce customized designs for different ligand types and integrate them into
a unified gradient inversion framework with flexible triggers, thereby ensuring
broad applicability. Moreover, we provide rigorous theoretical guarantees for
each component of MagicDock. Extensive experiments across 9 scenarios
demonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\%
over SOTA baselines specialized for protein or molecule ligand design,
respectively.

</details>


### [108] [The Environmental Impacts of Machine Learning Training Keep Rising Evidencing Rebound Effect](https://arxiv.org/abs/2510.09022)
*Clément Morand,Anne-Laure Ligozat,Aurélie Névéol*

Main category: cs.LG

TL;DR: 该研究分析了近十年来AI系统训练的环境影响，发现即使采用优化策略，ML模型训练的环境影响仍在指数级增长，存在反弹效应，仅靠效率提升无法确保可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习性能提升，计算需求激增导致能源消耗和环境问题日益严重，需要评估现有优化策略是否能实现可持续的ML模型训练。

Method: 通过估算过去十年中包括大语言模型在内的著名AI系统训练的环境影响，重点关注显卡的整个生命周期分析。

Result: 研究发现显卡生产影响持续增加，ML模型训练能耗和环境影响呈指数增长，优化策略无法抵消训练带来的影响，存在反弹效应。

Conclusion: 仅提高效率无法确保ML的可持续性，必须减少AI活动规模、质疑资源密集型训练的频次，并考虑硬件的全生命周期影响而非仅使用阶段。

Abstract: Recent Machine Learning (ML) approaches have shown increased performance on
benchmarks but at the cost of escalating computational demands. Hardware,
algorithmic and carbon optimizations have been proposed to curb energy
consumption and environmental impacts. Can these strategies lead to sustainable
ML model training? Here, we estimate the environmental impacts associated with
training notable AI systems over the last decade, including Large Language
Models, with a focus on the life cycle of graphics cards. Our analysis reveals
two critical trends: First, the impacts of graphics cards production have
increased steadily over this period; Second, energy consumption and
environmental impacts associated with training ML models have increased
exponentially, even when considering reduction strategies such as location
shifting to places with less carbon intensive electricity mixes. Optimization
strategies do not mitigate the impacts induced by model training, evidencing
rebound effect. We show that the impacts of hardware must be considered over
the entire life cycle rather than the sole use phase in order to avoid impact
shifting. Our study demonstrates that increasing efficiency alone cannot ensure
sustainability in ML. Mitigating the environmental impact of AI also requires
reducing AI activities and questioning the scale and frequency of
resource-intensive training.

</details>


### [109] [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections](https://arxiv.org/abs/2510.09023)
*Milad Nasr,Nicholas Carlini,Chawin Sitawarin,Sander V. Schulhoff,Jamie Hayes,Michael Ilie,Juliette Pluto,Shuang Song,Harsh Chaudhari,Ilia Shumailov,Abhradeep Thakurta,Kai Yuanqing Xiao,Andreas Terzis,Florian Tramèr*

Main category: cs.LG

TL;DR: 当前语言模型防御评估方法存在缺陷，应使用自适应攻击者进行测试。通过系统优化技术，成功绕过了12种近期防御方法，多数防御的攻击成功率从接近零提升至90%以上。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型防御评估方法存在缺陷，通常使用静态攻击字符串或计算能力较弱的优化方法，未能充分考虑自适应攻击者的威胁。

Method: 采用系统调优和扩展通用优化技术，包括梯度下降、强化学习、随机搜索和人工引导探索，针对12种不同的防御技术进行攻击测试。

Result: 成功绕过了12种近期防御方法，大多数防御的攻击成功率从原本接近零的水平提升至90%以上。

Conclusion: 未来的防御工作必须考虑更强的攻击方法，如本文所述的自适应攻击，才能做出可靠且令人信服的鲁棒性声明。

Abstract: How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense's design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.

</details>


### [110] [Convergence of optimizers implies eigenvalues filtering at equilibrium](https://arxiv.org/abs/2510.09034)
*Jerome Bolte,Quoc-Tung Le,Edouard Pauwels*

Main category: cs.LG

TL;DR: 论文从假设神经网络优化器会收敛到任意点的角度出发，提出不同优化器本质上是特征值滤波器，梯度下降天然避免最尖锐的极小值，而SAM算法更倾向于宽盆地。基于此提出了两种增强特征值过滤的新算法。


<details>
  <summary>Details</summary>
Motivation: 现有经验表明各种优化器往往能找到接近全局最优解，本文反向思考，假设收敛到任意点，研究这种假设的后果，特别是与边缘稳定性现象相关的影响。

Method: 采用广义Hadamard-Perron稳定流形定理，分析一般半代数C^2函数，无需额外非退化条件或全局Lipschitz边界假设。提出两种增强特征值过滤的新算法。

Result: 理论分析和数值实验表明，不同优化器确实作为特征值滤波器运行，梯度下降避免尖锐极小值，SAM算法更偏好宽盆地，新算法能进一步促进宽极小值。

Conclusion: 优化器的选择本质上是对特征值谱的过滤过程，这一视角为理解深度学习优化提供了新思路，提出的新算法能更有效地寻找宽盆地解。

Abstract: Ample empirical evidence in deep neural network training suggests that a
variety of optimizers tend to find nearly global optima. In this article, we
adopt the reversed perspective that convergence to an arbitrary point is
assumed rather than proven, focusing on the consequences of this assumption.
From this viewpoint, in line with recent advances on the edge-of-stability
phenomenon, we argue that different optimizers effectively act as eigenvalue
filters determined by their hyperparameters. Specifically, the standard
gradient descent method inherently avoids the sharpest minima, whereas
Sharpness-Aware Minimization (SAM) algorithms go even further by actively
favoring wider basins. Inspired by these insights, we propose two novel
algorithms that exhibit enhanced eigenvalue filtering, effectively promoting
wider minima. Our theoretical analysis leverages a generalized Hadamard--Perron
stable manifold theorem and applies to general semialgebraic $C^2$ functions,
without requiring additional non-degeneracy conditions or global Lipschitz
bound assumptions. We support our conclusions with numerical experiments on
feed-forward neural networks.

</details>


### [111] [Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach](https://arxiv.org/abs/2510.09041)
*Junchao Fan,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出IGCARL方法解决深度强化学习在自动驾驶中的对抗攻击脆弱性问题，通过战略对抗者和约束优化提升安全性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒方法存在三个关键问题：对抗短视攻击、难以引发真正安全关键事件、训练不稳定和策略漂移，限制了DRL在自动驾驶中的实际部署

Method: IGCARL包含战略目标对抗者和鲁棒驾驶智能体。对抗者利用DRL的时序决策能力执行战略协调的多步攻击，鲁棒智能体通过约束优化在对抗环境中稳定学习

Result: 实验表明IGCARL比最先进方法至少提高27.9%的成功率，显著增强了对对抗攻击的鲁棒性

Conclusion: IGCARL通过战略对抗训练和约束优化有效提升了DRL自动驾驶策略的安全性和可靠性

Abstract: Deep reinforcement learning (DRL) has demonstrated remarkable success in
developing autonomous driving policies. However, its vulnerability to
adversarial attacks remains a critical barrier to real-world deployment.
Although existing robust methods have achieved success, they still suffer from
three key issues: (i) these methods are trained against myopic adversarial
attacks, limiting their abilities to respond to more strategic threats, (ii)
they have trouble causing truly safety-critical events (e.g., collisions), but
instead often result in minor consequences, and (iii) these methods can
introduce learning instability and policy drift during training due to the lack
of robust constraints. To address these issues, we propose Intelligent
General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel
robust autonomous driving approach that consists of a strategic targeted
adversary and a robust driving agent. The strategic targeted adversary is
designed to leverage the temporal decision-making capabilities of DRL to
execute strategically coordinated multi-step attacks. In addition, it
explicitly focuses on inducing safety-critical events by adopting a general-sum
objective. The robust driving agent learns by interacting with the adversary to
develop a robust autonomous driving policy against adversarial attacks. To
ensure stable learning in adversarial environments and to mitigate policy drift
caused by attacks, the agent is optimized under a constrained formulation.
Extensive experiments show that IGCARL improves the success rate by at least
27.9\% over state-of-the-art methods, demonstrating superior robustness to
adversarial attacks and enhancing the safety and reliability of DRL-based
autonomous driving.

</details>


### [112] [Spatio-Temporal Graph Convolutional Networks for EV Charging Demand Forecasting Using Real-World Multi-Modal Data Integration](https://arxiv.org/abs/2510.09048)
*Jose Tupayachi,Mustafa C. Camur,Kevin Heaslip,Xueping Li*

Main category: cs.LG

TL;DR: 提出TW-GCN框架，结合图卷积网络和时序模型预测美国田纳西州电动汽车充电需求，实现3小时中期预测的最佳平衡，支持可持续交通和电网管理。


<details>
  <summary>Details</summary>
Motivation: 交通是温室气体排放主要来源，电动汽车充电基础设施空间分布不均和使用不规律给电网稳定性和投资规划带来挑战。

Method: 使用图卷积网络结合时序架构，整合真实交通流量、天气条件和专有数据，捕捉空间依赖性和时间动态。

Result: 3小时中期预测在响应性和稳定性间达到最佳平衡，1DCNN模型表现最优；区域分析显示预测精度在东、中、西田纳西州存在差异。

Conclusion: TW-GCN框架推进了数据驱动智能在电动汽车基础设施规划中的整合，支持可持续交通转型和弹性电网管理。

Abstract: Transportation remains a major contributor to greenhouse gas emissions,
highlighting the urgency of transitioning toward sustainable alternatives such
as electric vehicles (EVs). Yet, uneven spatial distribution and irregular
utilization of charging infrastructure create challenges for both power grid
stability and investment planning. This study introduces TW-GCN, a
spatio-temporal forecasting framework that combines Graph Convolutional
Networks with temporal architectures to predict EV charging demand in
Tennessee, United States (U.S.). We utilize real-world traffic flows, weather
conditions, and proprietary data provided by one of the largest EV
infrastructure company in the U.S. to capture both spatial dependencies and
temporal dynamics. Extensive experiments across varying lag horizons,
clustering strategies, and sequence lengths reveal that mid-horizon (3-hour)
forecasts achieve the best balance between responsiveness and stability, with
1DCNN consistently outperforming other temporal models. Regional analysis shows
disparities in predictive accuracy across East, Middle, and West Tennessee,
reflecting how station density, population, and local demand variability shape
model performance. The proposed TW-GCN framework advances the integration of
data-driven intelligence into EV infrastructure planning, supporting both
sustainable mobility transitions and resilient grid management.

</details>


### [113] [Improving Anomaly Detection in Industrial Time Series: The Role of Segmentation and Heterogeneous Ensemble](https://arxiv.org/abs/2510.09079)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 该论文研究了在工业生产环境中，通过集成分割技术和异构集成方法来增强异常检测性能。结果表明，在异构集成算法之前应用分割作为预处理步骤，显著提高了AUC-ROC指标。


<details>
  <summary>Details</summary>
Motivation: 探索如何将分割技术与异构集成相结合，以增强工业生产环境中的异常检测能力，特别是通过减少时间不确定性来提高检测性能。

Method: 使用变化点检测（CPD）算法如ChangeFinder进行时间序列分割，作为异构集成算法（如随机森林和XGBoost）的预处理步骤。

Result: 应用分割预处理后，AUC-ROC指标从0.8599（使用PCA和LSTM集成）显著提升至0.9760（使用随机森林和XGBoost）。

Conclusion: 分割技术通过减少时间模糊性和促进监督算法的学习过程，显著提升了异常检测性能。未来工作将研究引入基于变化点研究的加权特征，进一步优化早期异常检测性能。

Abstract: Concerning machine learning, segmentation models can identify state changes
within time series, facilitating the detection of transitions between normal
and anomalous conditions. Specific techniques such as Change Point Detection
(CPD), particularly algorithms like ChangeFinder, have been successfully
applied to segment time series and improve anomaly detection by reducing
temporal uncertainty, especially in multivariate environments. In this work, we
explored how the integration of segmentation techniques, combined with a
heterogeneous ensemble, can enhance anomaly detection in an industrial
production context. The results show that applying segmentation as a
pre-processing step before selecting heterogeneous ensemble algorithms provided
a significant advantage in our case study, improving the AUC-ROC metric from
0.8599 (achieved with a PCA and LSTM ensemble) to 0.9760 (achieved with Random
Forest and XGBoost). This improvement is imputable to the ability of
segmentation to reduce temporal ambiguity and facilitate the learning process
of supervised algorithms. In our future work, we intend to assess the benefit
of introducing weighted features derived from the study of change points,
combined with segmentation and the use of heterogeneous ensembles, to further
optimize model performance in early anomaly detection.

</details>


### [114] [FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms](https://arxiv.org/abs/2510.09085)
*Atul Shree,Harshith Jupuru*

Main category: cs.LG

TL;DR: FLToP CTC是一种基于帧级token剪枝的CTC解码算法，通过动态剪除低概率token来显著降低计算和内存需求，在保持WER几乎不变的同时实现10.5倍运行速度提升和2.78倍内存减少。


<details>
  <summary>Details</summary>
Motivation: 解决CTC-based ASR系统在资源受限环境中的计算和内存瓶颈问题，传统CTC解码器需要消耗高达90%的处理时间，存在token级操作的效率低下问题。

Method: 采用帧级token剪枝策略，基于相对阈值概率动态剪除每帧中的低概率token，从而减少计算和内存需求。

Result: 在LibriSpeech数据集上，FLToP CTC相比标准CTC解码器实现了10.5倍运行速度提升和2.78倍内存减少，同时WER退化可忽略不计。

Conclusion: FLToP CTC有效解决了CTC瓶颈问题，为资源受限环境和实时应用提供了可扩展性，提升了语音识别的可访问性和效率。

Abstract: CTC-based ASR systems face computational and memory bottlenecks in
resource-limited environments. Traditional CTC decoders, requiring up to 90% of
processing time in systems (e.g., wav2vec2-large on L4 GPUs), face
inefficiencies due to exhaustive token-level operations. This paper introduces
Frame Level Token Pruning for Connectionist Temporal Classification (FLToP
CTC), a novel decoding algorithm that employs frame-level token pruning guided
by a relative threshold probability. By dynamically eliminating low-probability
tokens per frame, FLToP CTC reduces compute and memory demands while
maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a
10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders.
Its simplicity enables seamless integration into CTC decoders across platforms
(CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability
for resource-limited environments and realtime applications, enhancing speech
recognition accessibility and efficiency.

</details>


### [115] [Neural Codecs as Biosignal Tokenizers](https://arxiv.org/abs/2510.09095)
*Kleanthis Avramidis,Tiantian Feng,Woojae Jeong,Jihwan Lee,Wenhui Cui,Richard M Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: BioCodec是一个受神经编解码器启发的表示学习框架，用于将生物信号（如EEG）转换为离散标记，在多个下游任务中表现优异，特别是在低资源环境下。


<details>
  <summary>Details</summary>
Motivation: 神经生理记录（如EEG）产生高维、噪声时间序列数据，需要大量预处理和手工特征提取。现有基于大型预训练模型的表示学习方法在生物信号解码方面面临挑战。

Method: 提出BioCodec框架，受神经编解码器启发，将生物信号的低级特征捕获为离散标记。在数千小时EEG数据上预训练，适用于多种生物信号类型。

Result: BioCodec在临床诊断、睡眠生理、语音解码和运动想象等多个下游任务中表现优异，特别是在低资源设置下。该方法也适用于其他生物信号如EMG。

Conclusion: BioCodec为生物信号标记化提供了通用解决方案，性能与最先进模型相当，代码和模型检查点已开源。

Abstract: Neurophysiological recordings such as electroencephalography (EEG) offer
accessible and minimally invasive means of estimating physiological activity
for applications in healthcare, diagnostic screening, and even immersive
entertainment. However, these recordings yield high-dimensional, noisy
time-series data that typically require extensive pre-processing and
handcrafted feature extraction to reveal meaningful information. Recently,
there has been a surge of interest in applying representation learning
techniques from large pre-trained (foundation) models to effectively decode and
interpret biosignals. We discuss the challenges posed for incorporating such
methods and introduce BioCodec, an alternative representation learning
framework inspired by neural codecs to capture low-level signal characteristics
in the form of discrete tokens. Pre-trained on thousands of EEG hours, BioCodec
shows efficacy across multiple downstream tasks, ranging from clinical
diagnostic tasks and sleep physiology to decoding speech and motor imagery,
particularly in low-resource settings. Additionally, we provide a qualitative
analysis of codebook usage and estimate the spatial coherence of codebook
embeddings from EEG connectivity. Notably, we also document the suitability of
our method to other biosignal data, i.e., electromyographic (EMG) signals.
Overall, the proposed approach provides a versatile solution for biosignal
tokenization that performs competitively with state-of-the-art models. The
source code and model checkpoints are shared.

</details>


### [116] [AdaPM: a Partial Momentum Algorithm for LLM Training](https://arxiv.org/abs/2510.09103)
*Yimu Zhang,Yuanshi Liu,Cong Fang*

Main category: cs.LG

TL;DR: AdaPM是一种自适应训练策略，利用部分动量实现内存高效优化器，通过非均匀动量设计和偏置校正技术，在保持性能的同时减少90%以上的动量内存使用。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型训练中，动量被广泛使用但存储动量通常带来内存挑战，需要开发内存高效的优化器。

Method: 采用非均匀动量设计，对大多数块不需要完整动量；通过偏置校正技术增强部分动量以减轻性能损失；可结合二阶统计量的内存高效技术进一步减少优化器状态内存。

Result: 经验验证表明，该方法在预训练60M到1.5B语言模型、监督微调和RLHF中，减少90%以上动量内存的同时保持效率和性能；结合其他技术可进一步减少95%优化器状态内存，为GPT-2 1.5B预训练节省超过30% GPU小时。

Conclusion: AdaPM提供了一种有效的内存优化解决方案，在保持模型性能的同时显著减少训练过程中的内存消耗。

Abstract: In the training of large language models, momentum is widely used and often
demonstrated to achieve significant acceleration. However, storing momentum
typically presents memory challenges. In this paper, we propose AdaPM, an
adaptive training strategy that leverages partial momentum to implement a
memory-efficient optimizer. To this end, AdaPM utilizes a non-uniform momentum
design: for most blocks, full momentum is not necessary to preserve the
performance of the optimization. In the momentum design of AdaPM, to mitigate
the bias and performance loss caused by partial momentum, we enhance the
partial momentum by a bias correction technique. Empirically, we verify that
our approach reduces memory by over $90\%$ in momentum while maintaining both
efficiency and performance for pretraining various language models ranging from
60M to 1.5B, as well as for supervised fine-tuning and RLHF. AdaPM can further
reduce memory by up to $95\%$ in optimizer states by combining the
memory-efficient technique on the second-order statistic, saving over $30\%$
GPU hours for pretraining GPT-2 1.5B.

</details>


### [117] [MemLoss: Enhancing Adversarial Training with Recycling Adversarial Examples](https://arxiv.org/abs/2510.09105)
*Soroush Mahdi,Maryam Amirmazlaghani,Saeed Saravani,Zahra Dehghanian*

Main category: cs.LG

TL;DR: 提出MemLoss方法，通过利用历史生成的对抗样本来增强对抗训练，在保持干净数据性能的同时提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法在提升模型鲁棒性时往往牺牲了在干净数据上的准确率，需要一种平衡自然准确率和对抗鲁棒性的方法

Method: 使用历史生成的对抗样本（记忆对抗样本）在多个训练周期中进行复用，通过记忆机制增强对抗训练效果

Result: 在CIFAR-10等多个数据集上的实验表明，MemLoss相比现有对抗训练方法获得了更好的准确率，同时保持了强大的对抗攻击鲁棒性

Conclusion: MemLoss方法能够有效平衡自然准确率和对抗鲁棒性，为对抗训练提供了一种有效的改进方案

Abstract: In this paper, we propose a new approach called MemLoss to improve the
adversarial training of machine learning models. MemLoss leverages previously
generated adversarial examples, referred to as 'Memory Adversarial Examples,'
to enhance model robustness and accuracy without compromising performance on
clean data. By using these examples across training epochs, MemLoss provides a
balanced improvement in both natural accuracy and adversarial robustness.
Experimental results on multiple datasets, including CIFAR-10, demonstrate that
our method achieves better accuracy compared to existing adversarial training
methods while maintaining strong robustness against attacks.

</details>


### [118] [On the Fairness of Privacy Protection: Measuring and Mitigating the Disparity of Group Privacy Risks for Differentially Private Machine Learning](https://arxiv.org/abs/2510.09114)
*Zhi Yang,Changwu Huang,Ke Tang,Xin Yao*

Main category: cs.LG

TL;DR: 本文提出了一种新的成员推理游戏来高效审计数据记录的近似最坏情况隐私风险，并改进了DP-SGD算法，通过自适应组特定梯度裁剪策略减少组间隐私风险差异，提升DPML中隐私保护的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要基于数据记录的平均隐私风险评估组隐私风险，可能低估组隐私风险及其差异，且当前评估最坏情况隐私风险的方法效率低下，限制了实际应用。

Method: 引入新的成员推理游戏高效审计近似最坏情况隐私风险；改进DP-SGD算法，采用自适应组特定梯度裁剪策略。

Result: 实验结果表明，该方法能更严格地衡量组隐私风险，可靠评估组隐私风险差异；改进的算法有效减少了组隐私风险差异。

Conclusion: 提出的方法能更准确评估组隐私风险差异，并通过改进的DP-SGD算法提升DPML中隐私保护的公平性。

Abstract: While significant progress has been made in conventional fairness-aware
machine learning (ML) and differentially private ML (DPML), the fairness of
privacy protection across groups remains underexplored. Existing studies have
proposed methods to assess group privacy risks, but these are based on the
average-case privacy risks of data records. Such approaches may underestimate
the group privacy risks, thereby potentially underestimating the disparity
across group privacy risks. Moreover, the current method for assessing the
worst-case privacy risks of data records is time-consuming, limiting their
practical applicability. To address these limitations, we introduce a novel
membership inference game that can efficiently audit the approximate worst-case
privacy risks of data records. Experimental results demonstrate that our method
provides a more stringent measurement of group privacy risks, yielding a
reliable assessment of the disparity in group privacy risks. Furthermore, to
promote privacy protection fairness in DPML, we enhance the standard DP-SGD
algorithm with an adaptive group-specific gradient clipping strategy, inspired
by the design of canaries in differential privacy auditing studies. Extensive
experiments confirm that our algorithm effectively reduces the disparity in
group privacy risks, thereby enhancing the fairness of privacy protection in
DPML.

</details>


### [119] [Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback](https://arxiv.org/abs/2510.09127)
*Orin Levy,Liad Erez,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 提出了针对延迟反馈情境下上下文多臂老虎机问题的遗憾最小化算法，包括有限策略类的最优遗憾界和一般函数逼近设置下的遗憾界分析


<details>
  <summary>Details</summary>
Motivation: 解决在对抗性延迟反馈环境下的上下文多臂老虎机问题，其中损失观测可能被对手延迟交付

Method: 使用有限策略类直接访问方法和一般函数逼近方法，后者利用在线最小二乘回归预言机，并分析了基于Hedge的Vovk聚合预测器的稳定性

Result: 在有限策略类下获得最优期望遗憾界O(√(KTlog|Π|) + √(Dlog|Π|))；在一般函数逼近下获得O(√(KTℛ_T(O)) + √(d_maxDβ))，其中β≤log|F|时简化为O(√(KTlog|F|) + √(d_maxDlog|F|))

Conclusion: 提出的算法在延迟反馈环境下有效，但一般函数逼近方法的结果与下界存在√d_max的差距，表明还有改进空间

Abstract: We present regret minimization algorithms for the contextual multi-armed
bandit (CMAB) problem over $K$ actions in the presence of delayed feedback, a
scenario where loss observations arrive with delays chosen by an adversary. As
a preliminary result, assuming direct access to a finite policy class $\Pi$ we
establish an optimal expected regret bound of $ O (\sqrt{KT \log |\Pi|} +
\sqrt{D \log |\Pi|)} $ where $D$ is the sum of delays. For our main
contribution, we study the general function approximation setting over a
(possibly infinite) contextual loss function class $ \mathcal{F} $ with access
to an online least-square regression oracle $\mathcal{O}$ over $\mathcal{F}$.
In this setting, we achieve an expected regret bound of
$O(\sqrt{KT\mathcal{R}_T(\mathcal{O})} + \sqrt{ d_{\max} D \beta})$ assuming
FIFO order, where $d_{\max}$ is the maximal delay, $\mathcal{R}_T(\mathcal{O})$
is an upper bound on the oracle's regret and $\beta$ is a stability parameter
associated with the oracle. We complement this general result by presenting a
novel stability analysis of a Hedge-based version of Vovk's aggregating
forecaster as an oracle implementation for least-square regression over a
finite function class $\mathcal{F}$ and show that its stability parameter
$\beta$ is bounded by $\log |\mathcal{F}|$, resulting in an expected regret
bound of $O(\sqrt{KT \log |\mathcal{F}|} + \sqrt{d_{\max} D \log
|\mathcal{F}|})$ which is a $\sqrt{d_{\max}}$ factor away from the lower bound
of $\Omega(\sqrt{KT \log |\mathcal{F}|} + \sqrt{D \log |\mathcal{F}|})$ that we
also present.

</details>


### [120] [Score-Based Density Estimation from Pairwise Comparisons](https://arxiv.org/abs/2510.09146)
*Petrus Mikkola,Luigi Acerbi,Arto Klami*

Main category: cs.LG

TL;DR: 该论文提出了一种从成对比较中估计密度的方法，通过将未观测的目标密度与经过温度调节的胜者密度相关联，利用分数匹配学习胜者分数，然后通过"去温度调节"来估计目标密度。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于专家知识获取和从人类反馈中学习的需求，特别是在只能获得成对比较数据的情况下进行密度估计。

Method: 方法包括：将目标密度与温度调节的胜者密度关联；通过分数匹配学习胜者分数；推导信念密度和胜者密度分数向量之间的共线关系；提出在Bradley-Terry模型下的温度场估计器；使用基于分数缩放退火Langevin动力学的扩散模型。

Result: 该方法能够仅使用数百到数千个成对比较，学习模拟专家的复杂多元信念密度。

Conclusion: 通过建立目标密度与温度调节胜者密度之间的关系，并利用分数匹配和扩散模型，实现了从有限成对比较数据中有效估计复杂密度分布。

Abstract: We study density estimation from pairwise comparisons, motivated by expert
knowledge elicitation and learning from human feedback. We relate the
unobserved target density to a tempered winner density (marginal density of
preferred choices), learning the winner's score via score-matching. This allows
estimating the target by `de-tempering' the estimated winner density's score.
We prove that the score vectors of the belief and the winner density are
collinear, linked by a position-dependent tempering field. We give analytical
formulas for this field and propose an estimator for it under the Bradley-Terry
model. Using a diffusion model trained on tempered samples generated via
score-scaled annealed Langevin dynamics, we can learn complex multivariate
belief densities of simulated experts, from only hundreds to thousands of
pairwise comparisons.

</details>


### [121] [Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting](https://arxiv.org/abs/2510.09152)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.LG

TL;DR: 提出Logits Replay + MoClip两阶段框架，通过压缩logit空间监督和稳定优化更新，在提升领域性能的同时缓解通用能力遗忘，降低40%以上训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在领域适应中面临的权衡问题：提升专业领域能力往往以牺牲通用能力为代价，现有方法在计算、数据访问或适应性方面成本高昂。

Method: 两阶段框架：Stage 0记录动态Top-K token子集（包含黄金标签），Stage 1重放这些紧凑子集计算精确重归一化损失；设计MoClip优化器限制梯度动量旋转并基于arctan2重新缩放更新。

Result: 在通信技术和NL2SQL任务上提升领域性能，在通用基准（MMLU、BBH、GPQA、MATH）上缓解遗忘，训练成本降低40%以上。

Conclusion: 该方法为LLMs领域适应提供了可扩展、架构无关的路径，无需牺牲泛化能力。

Abstract: Large language models (LLMs) often face a trade-off in post-training:
improvements on specialized domains frequently come at the expense of general
capabilities. Existing solutions attempt to mitigate this tension via
regularization, selective parameter updates, or data-centric replay, but each
imposes significant costs in computation, data access, or adaptability. Recent
work has shown that training signals can be compressed to subsets of logits
without severe accuracy loss, suggesting a path toward efficient adaptation.
However, naive truncation destabilizes optimization and exacerbates forgetting.
  We introduce Logits Replay + MoClip, a two-stage framework that compresses
supervision in the logit space and stabilizes optimization at the update level.
In Stage 0, we record dynamic Top-K token subsets that cover a probability
threshold, always including the gold label. In Stage 1, we replay these compact
subsets to compute exact renormalized losses, avoiding full softmax computation
and implicitly regularizing. To ensure stability, we design MoClip, an
optimizer that caps gradient-momentum rotation and applies an arctan2-based
rescaling of updates. Empirically, our method improves domain performance on
Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on
general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over
40%. Together, these contributions offer a scalable, architecture-agnostic path
for domain adaptation of LLMs without sacrificing generalization.

</details>


### [122] [Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.09156)
*Jing Li,Zhijie Sun,Zhicheng Zhou,Suming Qiu,Junjie Huang,Haijia Sun,Linyuan Qiu*

Main category: cs.LG

TL;DR: Agentic-KGR是一个通过多轮强化学习实现大语言模型与知识图谱协同进化的框架，解决了静态知识库的覆盖不足和时效性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识增强的大语言模型依赖静态知识库，存在覆盖不足和时效性差的问题，限制了在动态信息环境中的有效性。

Method: 提出三个关键创新：动态模式扩展机制、检索增强记忆系统、可学习的多尺度提示压缩方法，通过多轮强化学习实现LLM与KG的协同进化。

Result: 实验结果显示在知识抽取任务中显著优于监督基线和单轮RL方法，与GraphRAG集成后在问答任务中取得更好的准确性和知识覆盖。

Conclusion: Agentic-KGR框架通过协同进化机制有效解决了知识增强LLM的覆盖和时效性问题，在知识抽取和下游任务中表现出优越性能。

Abstract: Current knowledge-enhanced large language models (LLMs) rely on static,
pre-constructed knowledge bases that suffer from coverage gaps and temporal
obsolescence, limiting their effectiveness in dynamic information environments.
We present Agentic-KGR, a novel framework enabling co-evolution between LLMs
and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our
approach introduces three key innovations: (1) a dynamic schema expansion
mechanism that systematically extends graph ontologies beyond pre-defined
boundaries during training; (2) a retrieval-augmented memory system enabling
synergistic co-evolution between model parameters and knowledge structures
through continuous optimization; (3) a learnable multi-scale prompt compression
approach that preserves critical information while reducing computational
complexity through adaptive sequence optimization. Experimental results
demonstrate substantial improvements over supervised baselines and single-round
RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our
method achieves superior performance in downstream QA tasks, with significant
gains in both accuracy and knowledge coverage compared to existing methods.

</details>


### [123] [Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization](https://arxiv.org/abs/2510.09160)
*Le-Trung Nguyen,Enzo Tartaglione,Van-Tam Nguyen*

Main category: cs.LG

TL;DR: 提出了WASI方法，通过将训练限制在固定子空间内，显著降低transformer模型的内存使用和计算成本，同时保持与标准训练相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在日常生活中的应用增加，能耗和数据隐私成为关键问题。设备端学习能减少能耗并保护数据隐私，但现代神经网络规模扩大成为设备端训练的主要障碍。

Method: 应用基于子空间的训练方法到transformer模型，引入Weight-Activation Subspace Iteration (WASI)方法，通过将训练限制在固定子空间内来缓解反向传播的内存瓶颈。

Result: WASI在保持与标准训练相当准确性的同时，将内存使用减少高达62倍，计算成本(FLOPs)减少高达2倍。在树莓派5上，训练和推理速度比标准训练快约1.5倍。

Conclusion: WASI方法有效解决了transformer模型在设备端训练中的内存和计算瓶颈问题，为能耗敏感和数据隐私保护的应用提供了可行的解决方案。

Abstract: As AI increasingly shapes daily life, energy consumption and data privacy
have become pressing concerns. On-device learning trains models directly on
edge devices, cutting energy consumption and safeguarding data privacy.
However, the expanding scale of modern neural networks creates a major obstacle
for on-device training. Although prior work has concentrated on compact
convolutional architectures, we instead apply subspace-based training to
transformer models. Motivated by the idea that a model's essential information
lies in a fixed subspace, we introduce Weight-Activation Subspace Iteration
(WASI), a method that mitigates the memory bottleneck of backpropagation and
boosts inference efficiency in transformer models by restricting training to
this subspace. Our results demonstrate that WASI maintains accuracy comparable
to vanilla training while reducing memory usage by up to $62\times$ and
computational cost (FLOPs) by up to $2\times$. On a Raspberry Pi 5, WASI
achieves roughly $1.5\times$ faster training and inference than vanilla
training.

</details>


### [124] [Robustness and Regularization in Hierarchical Re-Basin](https://arxiv.org/abs/2510.09174)
*Benedikt Franke,Florian Heinrich,Markus Lange,Arne Raul*

Main category: cs.LG

TL;DR: 本文重新审视Git Re-Basin模型合并方法，提出了分层模型合并方案，显著优于标准MergeMany算法，发现Re-Basin能为合并模型带来对抗性和扰动鲁棒性，但性能下降比原作者报告的要大。


<details>
  <summary>Details</summary>
Motivation: 深入研究Git Re-Basin这一新的模型合并方法，探索其实际效果和局限性。

Method: 提出分层模型合并方案，与标准MergeMany算法进行对比实验。

Result: 分层合并方案显著优于标准算法，Re-Basin能为合并模型带来对抗性和扰动鲁棒性，且参与合并的模型越多效果越强，但性能下降比原作者报告的要大。

Conclusion: Git Re-Basin方法在带来鲁棒性提升的同时，会导致比预期更大的性能下降，需要谨慎使用。

Abstract: This paper takes a closer look at Git Re-Basin, an interesting new approach
to merge trained models. We propose a hierarchical model merging scheme that
significantly outperforms the standard MergeMany algorithm. With our new
algorithm, we find that Re-Basin induces adversarial and perturbation
robustness into the merged models, with the effect becoming stronger the more
models participate in the hierarchical merging scheme. However, in our
experiments Re-Basin induces a much bigger performance drop than reported by
the original authors.

</details>


### [125] [Beyond Pairwise Connections: Extracting High-Order Functional Brain Network Structures under Global Constraints](https://arxiv.org/abs/2510.09175)
*Ling Zhan,Junjie Huang,Xiaoyao Yu,Wenyu Chen,Tao Jia*

Main category: cs.LG

TL;DR: 提出GCM框架，通过全局约束从数据分布中端到端学习功能脑网络的高阶结构，在5个数据集和2个任务设置中相比基线方法实现30.6%准确率提升和96.3%计算时间减少。


<details>
  <summary>Details</summary>
Motivation: 现有功能脑网络建模主要依赖局部成对交互，难以捕捉高阶依赖关系；同时当前超图建模方法计算负担重且具有启发性，阻碍从数据分布中端到端学习网络结构。

Method: 提出全局约束导向的多分辨率(GCM)框架，整合4种全局约束（信号同步、被试身份、期望边数、数据标签）来学习4个不同建模分辨率（样本/被试/组/项目）的功能脑网络结构。

Result: 在5个数据集和2个任务设置中，相比9个基线和10个最先进方法，GCM实现相对准确率提升30.6%，计算时间减少96.3%。

Conclusion: 该工作为功能脑网络结构学习提供了新视角，为认知神经科学的跨学科应用奠定了基础。

Abstract: Functional brain network (FBN) modeling often relies on local pairwise
interactions, whose limitation in capturing high-order dependencies is
theoretically analyzed in this paper. Meanwhile, the computational burden and
heuristic nature of current hypergraph modeling approaches hinder end-to-end
learning of FBN structures directly from data distributions. To address this,
we propose to extract high-order FBN structures under global constraints, and
implement this as a Global Constraints oriented Multi-resolution (GCM) FBN
structure learning framework. It incorporates 4 types of global constraint
(signal synchronization, subject identity, expected edge numbers, and data
labels) to enable learning FBN structures for 4 distinct levels
(sample/subject/group/project) of modeling resolution. Experimental results
demonstrate that GCM achieves up to a 30.6% improvement in relative accuracy
and a 96.3% reduction in computational time across 5 datasets and 2 task
settings, compared to 9 baselines and 10 state-of-the-art methods. Extensive
experiments validate the contributions of individual components and highlight
the interpretability of GCM. This work offers a novel perspective on FBN
structure learning and provides a foundation for interdisciplinary applications
in cognitive neuroscience. Code is publicly available on
https://github.com/lzhan94swu/GCM.

</details>


### [126] [On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual Learning](https://arxiv.org/abs/2510.09181)
*Ze Peng,Jian Zhang,Jintao Guo,Lei Qi,Yang Gao,Yinghuan Shi*

Main category: cs.LG

TL;DR: 本文揭示了持续学习中灾难性遗忘的根本原因：新任务训练本质上是对旧任务知识的对抗性攻击，新任务梯度会自动与旧任务损失函数的尖锐方向对齐，导致旧任务损失快速增加。


<details>
  <summary>Details</summary>
Motivation: 理解深度网络中灾难性遗忘的深层原因，现有研究尚未完全揭示其根本机制。

Method: 通过理论分析发现训练的低秩偏置导致新旧任务梯度在同一低维子空间中对齐，提出backGP方法解决反向传播导致的对抗对齐问题。

Result: backGP方法相比梯度投影方法平均减少10.8%的遗忘，提高12.7%的准确率。

Conclusion: 灾难性遗忘源于新任务训练对旧任务知识的对抗性攻击，backGP能有效缓解由反向传播导致的对抗对齐问题。

Abstract: Continual learning seeks the human-like ability to accumulate new skills in
machine intelligence. Its central challenge is catastrophic forgetting, whose
underlying cause has not been fully understood for deep networks. In this
paper, we demystify catastrophic forgetting by revealing that the new-task
training is implicitly an adversarial attack against the old-task knowledge.
Specifically, the new-task gradients automatically and accurately align with
the sharp directions of the old-task loss landscape, rapidly increasing the
old-task loss. This adversarial alignment is intriguingly counter-intuitive
because the sharp directions are too sparsely distributed to align with by
chance. To understand it, we theoretically show that it arises from training's
low-rank bias, which, through forward and backward propagation, confines the
two directions into the same low-dimensional subspace, facilitating alignment.
Gradient projection (GP) methods, a representative family of
forgetting-mitigating methods, reduce adversarial alignment caused by forward
propagation, but cannot address the alignment due to backward propagation. We
propose backGP to address it, which reduces forgetting by 10.8% and improves
accuracy by 12.7% on average over GP methods.

</details>


### [127] [Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs](https://arxiv.org/abs/2510.09201)
*Yumin Choi,Dongki Kim,Jinheon Baek,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了多模态提示优化问题，并开发了MPO框架，通过联合优化文本和非文本提示来提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法仅限于文本，限制了多模态大语言模型（MLLMs）的潜力，需要扩展到多模态空间。

Method: 提出多模态提示优化器（MPO），通过保持对齐的联合更新来优化多模态提示，并利用贝叶斯选择策略指导候选提示的选择。

Result: 在图像、视频、分子等多种模态上的实验表明，MPO优于领先的纯文本优化方法。

Conclusion: 多模态提示优化是实现MLLMs潜力的关键步骤，MPO框架为多模态提示优化提供了有效解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable success, and their
multimodal expansions (MLLMs) further unlock capabilities spanning images,
videos, and other modalities beyond text. However, despite this shift, prompt
optimization approaches, designed to reduce the burden of manual prompt
crafting while maximizing performance, remain confined to text, ultimately
limiting the full potential of MLLMs. Motivated by this gap, we introduce the
new problem of multimodal prompt optimization, which expands the prior
definition of prompt optimization to the multimodal space defined by the pairs
of textual and non-textual prompts. To tackle this problem, we then propose the
Multimodal Prompt Optimizer (MPO), a unified framework that not only performs
the joint optimization of multimodal prompts through alignment-preserving
updates but also guides the selection process of candidate prompts by
leveraging earlier evaluations as priors in a Bayesian-based selection
strategy. Through extensive experiments across diverse modalities that go
beyond text, such as images, videos, and even molecules, we demonstrate that
MPO outperforms leading text-only optimization methods, establishing multimodal
prompt optimization as a crucial step to realizing the potential of MLLMs.

</details>


### [128] [FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning](https://arxiv.org/abs/2510.09222)
*Zhenglin Wan,Jingxuan Wu,Xingrui Yu,Chubin Zhang,Mingcong Lei,Bo An,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出一种结合流匹配模型和强化学习的混合方法，使用简单的MLP学生策略进行在线探索，通过教师FM模型提供奖励信号和行为正则化，解决了FM策略在线学习效率低和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在离线模仿学习中表现出色，但缺乏环境交互和探索能力，导致在未见场景中泛化能力差。直接在线优化FM策略存在梯度不稳定和高推理成本的问题。

Method: 使用简单的MLP学生策略进行在线探索和RL更新，通过教师FM模型提供奖励信号（包含专家数据分布信息）和行为正则化来稳定学习过程。

Result: 实验表明该方法显著提高了学习效率、泛化能力和鲁棒性，特别是在从次优专家数据学习时表现突出。

Conclusion: 提出的混合方法结合了FM的表达能力和简单策略的在线学习效率，有效解决了FM策略在线优化的挑战，为模仿学习提供了更高效的解决方案。

Abstract: Flow Matching (FM) has shown remarkable ability in modeling complex
distributions and achieves strong performance in offline imitation learning for
cloning expert behaviors. However, despite its behavioral cloning
expressiveness, FM-based policies are inherently limited by their lack of
environmental interaction and exploration. This leads to poor generalization in
unseen scenarios beyond the expert demonstrations, underscoring the necessity
of online interaction with environment. Unfortunately, optimizing FM policies
via online interaction is challenging and inefficient due to instability in
gradient computation and high inference costs. To address these issues, we
propose to let a student policy with simple MLP structure explore the
environment and be online updated via RL algorithm with a reward model. This
reward model is associated with a teacher FM model, containing rich information
of expert data distribution. Furthermore, the same teacher FM model is utilized
to regularize the student policy's behavior to stabilize policy learning. Due
to the student's simple architecture, we avoid the gradient instability of FM
policies and enable efficient online exploration, while still leveraging the
expressiveness of the teacher FM model. Extensive experiments show that our
approach significantly enhances learning efficiency, generalization, and
robustness, especially when learning from suboptimal expert data.

</details>


### [129] [Prime Implicant Explanations for Reaction Feasibility Prediction](https://arxiv.org/abs/2510.09226)
*Klaus Weinbauer,Tieu-Long Phan,Peter F. Stadler,Thomas Gärtner,Sagar Malhotra*

Main category: cs.LG

TL;DR: 提出了一种针对化学反应可行性预测的质蕴涵解释新方法，能够识别对预测结果至关重要的分子属性


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型虽然能成功预测化学反应可行性，但缺乏透明度和可解释性

Method: 引入了质蕴涵解释的新表述，并提出了在小规模反应预测任务中计算此类解释的算法

Result: 初步实验表明，该方法保守地捕捉了真实解释，虽然常包含冗余的键和原子，但能一致地捕获对预测反应可行性至关重要的分子属性

Conclusion: 该方法为化学反应预测模型提供了有效的可解释性工具，能够识别关键的分子特征

Abstract: Machine learning models that predict the feasibility of chemical reactions
have become central to automated synthesis planning. Despite their predictive
success, these models often lack transparency and interpretability. We
introduce a novel formulation of prime implicant explanations--also known as
minimally sufficient reasons--tailored to this domain, and propose an algorithm
for computing such explanations in small-scale reaction prediction tasks.
Preliminary experiments demonstrate that our notion of prime implicant
explanations conservatively captures the ground truth explanations. That is,
such explanations often contain redundant bonds and atoms but consistently
capture the molecular attributes that are essential for predicting reaction
feasibility.

</details>


### [130] [Incentivizing Time-Aware Fairness in Data Sharing](https://arxiv.org/abs/2510.09240)
*Jiangwei Chen,Kieu Thao Nguyen Pham,Rachael Hwee Ling Sim,Arun Verma,Zhaoxuan Wu,Chuan-Sheng Foo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 提出一个考虑参与时间的公平数据共享框架，为早期参与者提供更高奖励以补偿其承担的风险和鼓励贡献。


<details>
  <summary>Details</summary>
Motivation: 现有协作框架假设所有参与者同时加入，但现实中参与者可能在不同时间加入。早期参与者承担更高风险并鼓励观望者贡献，应获得更高奖励。

Method: 开发了时间感知激励机制，包括决定奖励价值的新方法，以及生成模型奖励来实现这些价值。

Result: 在合成和真实数据集上实证验证了所提方法的特性，展示了时间感知激励机制的有效性。

Conclusion: 提出的时间感知数据共享框架能够公平奖励不同时间加入的参与者，特别是补偿早期参与者的风险和贡献。

Abstract: In collaborative data sharing and machine learning, multiple parties
aggregate their data resources to train a machine learning model with better
model performance. However, as the parties incur data collection costs, they
are only willing to do so when guaranteed incentives, such as fairness and
individual rationality. Existing frameworks assume that all parties join the
collaboration simultaneously, which does not hold in many real-world scenarios.
Due to the long processing time for data cleaning, difficulty in overcoming
legal barriers, or unawareness, the parties may join the collaboration at
different times. In this work, we propose the following perspective: As a party
who joins earlier incurs higher risk and encourages the contribution from other
wait-and-see parties, that party should receive a reward of higher value for
sharing data earlier. To this end, we propose a fair and time-aware data
sharing framework, including novel time-aware incentives. We develop new
methods for deciding reward values to satisfy these incentives. We further
illustrate how to generate model rewards that realize the reward values and
empirically demonstrate the properties of our methods on synthetic and
real-world datasets.

</details>


### [131] [A PCA-based Data Prediction Method](https://arxiv.org/abs/2510.09246)
*Peteris Daugulis,Vija Vagale,Emiliano Mancini,Filippo Castiglione*

Main category: cs.LG

TL;DR: 提出了一种结合传统数学和机器学习的新方法，用于预测缺失数据，该方法基于现有数据和候选集之间的移位线性子空间距离概念。


<details>
  <summary>Details</summary>
Motivation: 数据科学中经常遇到需要为缺失数据选择适当值的问题，需要一种有效的缺失数据预测方法。

Method: 该方法基于移位线性子空间之间的距离概念，将现有数据集表示为由其主成分张成的子空间，并提供了欧几里得度量下的解决方案。

Result: 开发了一种结合传统数学和机器学习元素的新方法，能够有效地预测缺失数据。

Conclusion: 所提出的方法为缺失数据预测问题提供了一种有效的解决方案，结合了传统数学和机器学习的优势。

Abstract: The problem of choosing appropriate values for missing data is often
encountered in the data science. We describe a novel method containing both
traditional mathematics and machine learning elements for prediction
(imputation) of missing data. This method is based on the notion of distance
between shifted linear subspaces representing the existing data and candidate
sets. The existing data set is represented by the subspace spanned by its first
principal components. Solutions for the case of the Euclidean metric are given.

</details>


### [132] [Mitigating Model Drift in Developing Economies Using Synthetic Data and Outliers](https://arxiv.org/abs/2510.09294)
*Ilyas Varshavskiy,Bonu Boboeva,Shuhrat Khalilbekov,Azizjon Azimi,Sergey Shulgin,Akhlitdin Nizamitdinov,Haitz Saez de Ocariz Borde*

Main category: cs.LG

TL;DR: 该研究探讨了在金融机器学习模型中，通过添加合成异常值来缓解模型漂移问题，特别是在中亚和高加索地区的发展中经济体。


<details>
  <summary>Details</summary>
Motivation: 金融机器学习模型容易因数据分布变化而性能下降，这一问题在宏观经济冲击频繁的发展中经济体尤为严重。

Method: 提出使用合成异常值的方法，并引入一个两级评估框架来衡量性能下降程度和冲击严重性。

Result: 实验表明，添加少量合成异常值通常能提高模型稳定性，但最佳比例因数据集和模型而异。

Conclusion: 合成异常值是一种有效的模型漂移缓解方法，特别适用于宏观经济不稳定的地区。

Abstract: Machine Learning models in finance are highly susceptible to model drift,
where predictive performance declines as data distributions shift. This issue
is especially acute in developing economies such as those in Central Asia and
the Caucasus - including Tajikistan, Uzbekistan, Kazakhstan, and Azerbaijan -
where frequent and unpredictable macroeconomics shocks destabilize financial
data. To the best of our knowledge, this is among the first studies to examine
drift mitigation methods on financial datasets from these regions. We
investigate the use of synthetic outliers, a largely unexplored approach, to
improve model stability against unforeseen shocks. To evaluate effectiveness,
we introduce a two-level framework that measures both the extent of performance
degradation and the severity of shocks. Our experiments on macroeconomic
tabular datasets show that adding a small proportion of synthetic outliers
generally improves stability compared to baseline models, though the optimal
amount varies by dataset and model

</details>


### [133] [Large Language Model Prompt Datasets: An In-depth Analysis and Insights](https://arxiv.org/abs/2510.09316)
*Yuanming Zhang,Yan Lin,Arijit Khan,Huaiyu Wan*

Main category: cs.LG

TL;DR: 本文首次系统收集了来自多个平台的多样化提示数据集，分析了不同类别提示的共性与差异，并提出了一种基于句法嵌入的提示优化方法，通过引导LLM将提示重写向中心表示来提升模型输出的意义性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛部署，来自GitHub、社交媒体等平台的多样化提示数据集不断涌现，这些数据集涵盖广泛的应用和内容类型，但缺乏系统性分析。本文旨在填补这一空白，通过收集和分析这些数据集来促进更好的提示工程。

Method: 1) 从多个渠道收集广泛的提示数据集；2) 选择代表性数据集进行系统性分析；3) 提出基于词性和依存结构句法嵌入的提示优化方法，通过识别提示的中心表示并引导LLM向该中心重写提示。

Result: 成功收集了涵盖多种下游任务、语言、工程技术和模态的提示数据集，揭示了不同类别提示构造的共性与差异。提出的优化方法能够有效提升模型输出的意义性。

Conclusion: 本文提供了首个全面的提示数据集分析框架，提出的句法嵌入优化方法为提示工程提供了新的技术路径，相关数据集和代码已公开可用。

Abstract: A prompt is a natural language instruction that defines a specific task for a
large language model (LLM) and serves as the primary interface for human-LLM
interaction. With the growing deployment of LLMs, diverse prompt datasets are
emerging from platforms such as GitHub and social media. These datasets span a
wide array of applications and content types, facilitating both broader LLM
utilization and improved prompt engineering. In this work, we--for the first
time--have compiled an extensive list of prompt datasets sourced from various
channels, representing a spectrum of downstream tasks, languages, engineering
techniques, attributes, and modalities. We select key representative datasets
for systematic analysis, revealing commonalities and differences in prompt
construction across categories, distinguishing them from other text corpora
like literature and web. We further propose a prompt optimization approach that
leverages syntactic embeddings of part-of-speech and dependency structures. By
identifying a centroid representation of prompts and guiding LLMs to rewrite
prompts toward this centroid, our method improves the meaningfulness of model
outputs. We have made our datasets and code available.

</details>


### [134] [Residual-Informed Learning of Solutions to Algebraic Loops](https://arxiv.org/abs/2510.09317)
*Felix Brandt,Andreas Heuermann,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: 本文提出了一种残差感知的机器学习方法，用神经网络替代Modelica模型中的代数环，无需监督数据集，在IEEE 14-Bus系统中实现了60%的仿真时间减少。


<details>
  <summary>Details</summary>
Motivation: 传统Modelica模型中的代数环会显著增加仿真时间，需要一种高效的方法来替代这些代数环，同时保持仿真精度。

Method: 使用前馈神经网络，在损失函数中直接使用代数环的残差（误差）进行训练，无需监督数据集，解决了多解模糊性问题。

Result: 在IEEE 14-Bus系统上的应用表明，该方法比传统仿真方法减少了60%的仿真时间，同时通过误差控制机制保持了相同的精度水平。

Conclusion: 残差感知的机器学习方法能够有效替代Modelica模型中的代数环，显著提升仿真效率而不损失精度。

Abstract: This paper presents a residual-informed machine learning approach for
replacing algebraic loops in equation-based Modelica models with neural network
surrogates. A feedforward neural network is trained using the residual (error)
of the algebraic loop directly in its loss function, eliminating the need for a
supervised dataset. This training strategy also resolves the issue of ambiguous
solutions, allowing the surrogate to converge to a consistent solution rather
than averaging multiple valid ones. Applied to the large-scale IEEE 14-Bus
system, our method achieves a 60% reduction in simulation time compared to
conventional simulations, while maintaining the same level of accuracy through
error control mechanisms.

</details>


### [135] [Rate optimal learning of equilibria from data](https://arxiv.org/abs/2510.09325)
*Till Freihaut,Luca Viano,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 本文填补了多智能体模仿学习(MAIL)的理论空白，证明了非交互式MAIL的统计下界并提出了首个具有近最优样本复杂度的交互式算法MAIL-WARM。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体模仿学习中存在的理论空白，特别是非交互式方法的局限性，并开发更高效的交互式算法。

Method: 在非交互式设置中证明统计下界，识别全策略偏差集中系数为基本复杂度度量；在交互式设置中提出结合无奖励强化学习与交互式MAIL的框架，并实例化为MAIL-WARM算法。

Result: MAIL-WARM将样本复杂度从O(ε⁻⁸)改进到O(ε⁻²)，匹配理论下界；在网格世界等环境中，行为克隆方法失败而MAIL-WARM成功学习。

Conclusion: 本文建立了MAIL的完整理论框架，证明了非交互式方法的根本限制，并提供了具有最优样本复杂度的交互式解决方案。

Abstract: We close open theoretical gaps in Multi-Agent Imitation Learning (MAIL) by
characterizing the limits of non-interactive MAIL and presenting the first
interactive algorithm with near-optimal sample complexity. In the
non-interactive setting, we prove a statistical lower bound that identifies the
all-policy deviation concentrability coefficient as the fundamental complexity
measure, and we show that Behavior Cloning (BC) is rate-optimal. For the
interactive setting, we introduce a framework that combines reward-free
reinforcement learning with interactive MAIL and instantiate it with an
algorithm, MAIL-WARM. It improves the best previously known sample complexity
from $\mathcal{O}(\varepsilon^{-8})$ to $\mathcal{O}(\varepsilon^{-2}),$
matching the dependence on $\varepsilon$ implied by our lower bound. Finally,
we provide numerical results that support our theory and illustrate, in
environments such as grid worlds, where Behavior Cloning fails to learn.

</details>


### [136] [Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers](https://arxiv.org/abs/2510.09330)
*Tuan Nguyen,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出了一种模型无关的黑盒安全对齐框架，通过博弈论方法在推理时平衡安全性和帮助性，无需重新训练或访问LLM内部结构。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要在训练阶段进行，成本高且不灵活，无法适应新要求。推理时对齐方法仍需要访问模型内部，对第三方不实用。

Method: 将安全与帮助性的权衡建模为两人零和博弈，使用线性规划求解器在推理时计算均衡策略，实现黑盒安全对齐。

Result: 证明了黑盒安全对齐的可行性，为资源受限的组织提供了可扩展的安全实施途径。

Conclusion: 该框架为各种利益相关者提供了一种无需模型访问权限的安全对齐解决方案，适用于快速发展的LLM生态系统。

Abstract: Ensuring that large language models (LLMs) comply with safety requirements is
a central challenge in AI deployment. Existing alignment approaches primarily
operate during training, such as through fine-tuning or reinforcement learning
from human feedback, but these methods are costly and inflexible, requiring
retraining whenever new requirements arise. Recent efforts toward
inference-time alignment mitigate some of these limitations but still assume
access to model internals, which is impractical, and not suitable for third
party stakeholders who do not have access to the models. In this work, we
propose a model-independent, black-box framework for safety alignment that does
not require retraining or access to the underlying LLM architecture. As a proof
of concept, we address the problem of trading off between generating safe but
uninformative answers versus helpful yet potentially risky ones. We formulate
this dilemma as a two-player zero-sum game whose minimax equilibrium captures
the optimal balance between safety and helpfulness. LLM agents operationalize
this framework by leveraging a linear programming solver at inference time to
compute equilibrium strategies. Our results demonstrate the feasibility of
black-box safety alignment, offering a scalable and accessible pathway for
stakeholders, including smaller organizations and entities in
resource-constrained settings, to enforce safety across rapidly evolving LLM
ecosystems.

</details>


### [137] [Efficient Bayesian Inference from Noisy Pairwise Comparisons](https://arxiv.org/abs/2510.09333)
*Till Aczel,Lucas Theis,Wattenhofer Roger*

Main category: cs.LG

TL;DR: BBQ是一个贝叶斯Bradley-Terry变体，通过显式建模评分者质量来改进生成模型的人类评估，提供更可靠和成本效益的排名方法。


<details>
  <summary>Details</summary>
Motivation: 标准评估指标往往无法反映人类偏好，而人工评估虽然更可靠但成本高且噪声大。现有Bradley-Terry方法要么忽略评分者变异性，要么缺乏收敛保证。

Method: 提出BBQ方法，使用贝叶斯框架显式建模评分者质量，通过期望最大化算法提供单调似然收敛保证，对不可靠参与者进行降权或移除。

Result: 实证结果显示BBQ实现更快收敛、良好校准的不确定性估计，以及相比基线Bradley-Terry模型更稳健、可解释的排名，即使在有噪声或众包评分者情况下。

Conclusion: 该框架能够实现更可靠和成本效益的生成模型人类评估。

Abstract: Evaluating generative models is challenging because standard metrics often
fail to reflect human preferences. Human evaluations are more reliable but
costly and noisy, as participants vary in expertise, attention, and diligence.
Pairwise comparisons improve consistency, yet aggregating them into overall
quality scores requires careful modeling. Bradley-Terry-based methods update
item scores from comparisons, but existing approaches either ignore rater
variability or lack convergence guarantees, limiting robustness and
interpretability. We introduce BBQ, a Bayesian Bradley-Terry variant that
explicitly models rater quality, downweighting or removing unreliable
participants, and provides guaranteed monotonic likelihood convergence through
an Expectation-Maximization algorithm. Empirical results show that BBQ achieves
faster convergence, well-calibrated uncertainty estimates, and more robust,
interpretable rankings compared to baseline Bradley-Terry models, even with
noisy or crowdsourced raters. This framework enables more reliable and
cost-effective human evaluation of generative models.

</details>


### [138] [Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network](https://arxiv.org/abs/2510.09350)
*Vu Duc Anh Nguyen,Ziyue Li*

Main category: cs.LG

TL;DR: 提出了一个名为XGeoAI的新框架，用于实现实时、可解释的多步列车延误预测。该框架采用两阶段自回归图注意力网络(GAT)模型，在荷兰铁路网络真实数据集上进行训练和评估。


<details>
  <summary>Details</summary>
Motivation: 铁路网络运营效率受列车延误级联效应影响，现有方法在网络级多步自回归预测和实时可解释性方面存在不足，需要开发能提供决策支持的预测框架。

Method: 使用两阶段自回归图注意力网络(GAT)模型，将系统表示为运营事件(到达和出发)的时空图，并加入站台和车站拥堵等细粒度特征。采用顺序k步前向预测协议模拟真实部署条件。

Result: 虽然GATv2模型在纯误差指标(MAE)上不如简单的持续性基线模型，但在延误事件分类精度上表现更优，这对于可靠的决策支持工具至关重要。

Conclusion: 该框架在延误事件分类精度方面具有优势，为实时铁路交通管理提供了有价值的决策支持工具，尽管在误差指标上仍有改进空间。

Abstract: The operational efficiency of railway networks, a cornerstone of modern
economies, is persistently undermined by the cascading effects of train delays.
Accurately forecasting this delay propagation is a critical challenge for
real-time traffic management. While recent research has leveraged Graph Neural
Networks (GNNs) to model the network structure of railways, a significant gap
remains in developing frameworks that provide multi-step autoregressive
forecasts at a network-wide scale, while simultaneously offering the live,
interpretable explanations needed for decision support. This paper addresses
this gap by developing and evaluating a novel XGeoAI framework for live,
explainable, multi-step train delay forecasting. The core of this work is a
two-stage, autoregressive Graph Attention Network (GAT) model, trained on a
real-world dataset covering over 40% of the Dutch railway network. The model
represents the system as a spatio-temporal graph of operational events
(arrivals and departures) and is enriched with granular features, including
platform and station congestion. To test its viability for live deployment, the
model is rigorously evaluated using a sequential, k-step-ahead forecasting
protocol that simulates real-world conditions where prediction errors can
compound. The results demonstrate that while the proposed GATv2 model is
challenged on pure error metrics (MAE) by a simpler Persistence baseline, it
achieves consistently higher precision in classifying delay events -- a crucial
advantage for a reliable decision support tool.

</details>


### [139] [The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton](https://arxiv.org/abs/2510.09378)
*Natalie Abreu,Nikhil Vyas,Sham Kakade,Depen Morwani*

Main category: cs.LG

TL;DR: 本文通过应用完整的高斯-牛顿预条件处理到150M参数的Transformer模型，建立了迭代复杂度的实用上限，发现完整GN更新相比现有优化器能显著减少训练迭代次数。


<details>
  <summary>Details</summary>
Motivation: 探究计算效率高的二阶结构近似方法在大规模训练中会损失多少性能，为LLM预训练加速提供性能上限参考。

Method: 对最多150M参数的Transformer模型应用完整高斯-牛顿预条件处理，并与SOAP、Muon等强基线进行比较，同时测试精确的层间GN预条件器。

Result: 完整GN更新相比SOAP和Muon等基线实现了5.4倍的训练迭代减少；层间GN预条件器几乎匹配完整GN方法的性能。

Conclusion: GN近似对预条件处理非常有效，高阶损失项对收敛速度可能不关键；层间Hessian结构包含足够信息来实现大部分潜在收益；当前近似方法与理想层间oracle存在显著性能差距。

Abstract: Recent efforts to accelerate LLM pretraining have focused on
computationally-efficient approximations that exploit second-order structure.
This raises a key question for large-scale training: how much performance is
forfeited by these approximations? To probe this question, we establish a
practical upper bound on iteration complexity by applying full Gauss-Newton
(GN) preconditioning to transformer models of up to 150M parameters. Our
experiments show that full GN updates yield substantial gains over existing
optimizers, achieving a 5.4x reduction in training iterations compared to
strong baselines like SOAP and Muon. Furthermore, we find that a precise
layerwise GN preconditioner, which ignores cross-layer information, nearly
matches the performance of the full GN method. Collectively, our results
suggest: (1) the GN approximation is highly effective for preconditioning,
implying higher-order loss terms may not be critical for convergence speed; (2)
the layerwise Hessian structure contains sufficient information to achieve most
of these potential gains; and (3) a significant performance gap exists between
current approximate methods and an idealized layerwise oracle.

</details>


### [140] [Task-Level Insights from Eigenvalues across Sequence Models](https://arxiv.org/abs/2510.09379)
*Rahel Rickenbach,Jelena Trisovic,Alexandre Didier,Jerome Sieber,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 该论文通过动力学系统框架将softmax、norm和线性注意力表示为动力学系统，并与状态空间模型(SSMs)进行特征值谱分析比较，揭示了特征值对记忆和长程依赖建模的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然softmax注意力在序列模型中表现出色，但其二次复杂度限制了可扩展性，而线性替代方案如SSMs的效率提升机制尚不明确，需要系统比较不同注意力机制的信息处理特性。

Method: 使用动力学系统框架表示不同注意力机制，通过分析特征值谱进行结构化比较，并在多种序列模型和基准测试上进行实证分析。

Result: 发现特征值影响记忆和长程依赖建模的关键方面，揭示了与任务需求一致的光谱特征，并建立了架构修改对特征值谱和任务性能影响的对应关系。

Conclusion: 特征值分析为解释、理解和改进序列模型能力提供了原则性度量标准，强化了其在序列模型分析中的重要地位。

Abstract: Although softmax attention drives state-of-the-art performance for sequence
models, its quadratic complexity limits scalability, motivating linear
alternatives such as state space models (SSMs). While these alternatives
improve efficiency, their fundamental differences in information processing
remain poorly understood. In this work, we leverage the recently proposed
dynamical systems framework to represent softmax, norm and linear attention as
dynamical systems, enabling a structured comparison with SSMs by analyzing
their respective eigenvalue spectra. Since eigenvalues capture essential
aspects of dynamical system behavior, we conduct an extensive empirical
analysis across diverse sequence models and benchmarks. We first show that
eigenvalues influence essential aspects of memory and long-range dependency
modeling, revealing spectral signatures that align with task requirements.
Building on these insights, we then investigate how architectural modifications
in sequence models impact both eigenvalue spectra and task performance. This
correspondence further strengthens the position of eigenvalue analysis as a
principled metric for interpreting, understanding, and ultimately improving the
capabilities of sequence models.

</details>


### [141] [CHUCKLE -- When Humans Teach AI To Learn Emotions The Easy Way](https://arxiv.org/abs/2510.09382)
*Ankush Pratap Singh,Houwei Cao,Yong Liu*

Main category: cs.LG

TL;DR: CHUCKLE是一个基于人类感知的课程学习框架，利用众包数据中的标注者一致性和对齐度来定义样本难度，提高情感识别模型的训练效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别中的课程学习方法依赖启发式、数据驱动或模型定义难度，忽略了人类感知难度这一主观任务中的关键因素。

Method: 通过众包数据集中的标注者一致性和对齐度来定义样本难度，假设对人类具有挑战性的样本对机器学习模型同样困难。

Result: CHUCKLE使LSTM相对平均准确率提高6.56%，Transformer提高1.61%，同时减少梯度更新次数。

Conclusion: 基于人类感知的课程学习框架能有效提升情感识别模型的训练效率和鲁棒性。

Abstract: Curriculum learning (CL) structures training from simple to complex samples,
facilitating progressive learning. However, existing CL approaches for emotion
recognition often rely on heuristic, data-driven, or model-based definitions of
sample difficulty, neglecting the difficulty for human perception, a critical
factor in subjective tasks like emotion recognition. We propose CHUCKLE
(Crowdsourced Human Understanding Curriculum for Knowledge Led Emotion
Recognition), a perception-driven CL framework that leverages annotator
agreement and alignment in crowd-sourced datasets to define sample difficulty,
under the assumption that clips challenging for humans are similarly hard for
machine learning models. Empirical results suggest that CHUCKLE increases the
relative mean accuracy by 6.56% for LSTMs and 1.61% for Transformers over
non-curriculum baselines, while reducing the number of gradient updates,
thereby enhancing both training efficiency and model robustness.

</details>


### [142] [HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness](https://arxiv.org/abs/2510.09388)
*Xinyi Wang,Jinyi Han,Zishang Jiang,Tingyun Li,Jiaqing Liang,Sihang Jiang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.LG

TL;DR: 提出了HINT框架，通过提供启发式提示而非直接答案来改善强化学习在长链推理任务中的训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在任务难度超过模型能力时会出现奖励稀疏和训练效率低的问题，先前工作使用离策略数据但往往误导策略更新。

Method: 引入Affinity指标诊断训练亲和度，提出HINT框架提供自适应启发式提示，引导模型自主发现解决方案。

Result: 在数学推理任务上的广泛实验表明，HINT始终优于现有方法，在各种规模模型上达到最先进结果，同时展现出更稳定的学习和更高的数据效率。

Conclusion: HINT框架有效解决了强化学习在复杂推理任务中的训练亲和度问题，提高了探索效率和训练稳定性。

Abstract: Reinforcement Learning (RL) has become a key driver for enhancing the long
chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs).
However, prevalent methods like GRPO often fail when task difficulty exceeds
the model's capacity, leading to reward sparsity and inefficient training.
While prior work attempts to mitigate this using off-policy data, such as
mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide
policy updates In this work, we identify a core issue underlying these
failures, which we term low training affinity. This condition arises from a
large distributional mismatch between external guidance and the model's policy.
To diagnose this, we introduce Affinity, the first quantitative metric for
monitoring exploration efficiency and training stability. To improve Affinity,
we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness,
an adaptive hinting framework. Instead of providing direct answers, HINT
supplies heuristic hints that guide the model to discover solutions on its own,
preserving its autonomous reasoning capabilities. Extensive experiments on
mathematical reasoning tasks show that HINT consistently outperforms existing
methods, achieving state-of-the-art results with models of various scales,
while also demonstrating significantly more stable learning and greater data
efficiency.Code is available on Github.

</details>


### [143] [Design Principles for Sequence Models via Coefficient Dynamics](https://arxiv.org/abs/2510.09389)
*Jerome Sieber,Antonio Orvieto,Melanie N. Zeilinger,Carmen Amo Alonso*

Main category: cs.LG

TL;DR: 提出了一个统一框架，将深度序列模型的输出操作显式化为自主线性动力系统的输出，揭示了不同架构间的共同数学主题，并推导出连接架构选择与模型特性的设计原则。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解各种深度序列模型（如Transformer、状态空间模型、门控线性RNN）并系统比较它们，需要开发一个统一的框架来揭示这些架构的共同数学基础。

Method: 开发了一个统一框架，将线性组合系数建模为受脉冲输入驱动的自主线性动力系统的输出，从而显式化序列模型的输出操作。

Result: 该框架成功捕获了softmax注意力以及RNN、SSM等相关模型，揭示了不同架构间的数学共性，并识别了表达性与高效实现之间的权衡。

Conclusion: 该框架不仅解释了近期设计的经验成功，还为系统设计新的序列模型架构提供了指导原则，连接了近期文献中的多个见解和观察。

Abstract: Deep sequence models, ranging from Transformers and State Space Models (SSMs)
to more recent approaches such as gated linear RNNs, fundamentally compute
outputs as linear combinations of past value vectors. To draw insights and
systematically compare such architectures, we develop a unified framework that
makes this output operation explicit, by casting the linear combination
coefficients as the outputs of autonomous linear dynamical systems driven by
impulse inputs. This viewpoint, in spirit substantially different from
approaches focusing on connecting linear RNNs with linear attention, reveals a
common mathematical theme across diverse architectures and crucially captures
softmax attention, on top of RNNs, SSMs, and related models. In contrast to new
model proposals that are commonly evaluated on benchmarks, we derive design
principles linking architectural choices to model properties. Thereby
identifying tradeoffs between expressivity and efficient implementation,
geometric constraints on input selectivity, and stability conditions for
numerically stable training and information retention. By connecting several
insights and observations from recent literature, the framework both explains
empirical successes of recent designs and provides guiding principles for
systematically designing new sequence model architectures.

</details>


### [144] [Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training](https://arxiv.org/abs/2510.09405)
*Yuhao Pan,Xiucheng Wang,Nan Cheng,Wenchao Xu*

Main category: cs.LG

TL;DR: 提出了一种对抗训练和风格迁移结合的射频指纹识别框架，能够有效分离发射器和接收器特征，解决跨接收器性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统射频指纹识别模型在实际部署中受接收器引起的特征偏移影响，当训练和测试使用不同接收器时性能显著下降，需要解决跨接收器鲁棒性问题。

Method: 集成对抗训练和风格迁移技术，通过强制域不变表示学习来分离发射器硬件特征和接收器伪影，确保对接收器变化的鲁棒性。

Result: 在多接收器数据集上的广泛实验表明，该方法在多种接收器设置下平均准确率比现有最优方法提升高达10%。

Conclusion: 所提出的框架能够有效解决射频指纹识别中的跨接收器变异性问题，实现了更鲁棒的无线设备识别性能。

Abstract: Radio frequency fingerprint identification (RFFI) is a critical technique for
wireless network security, leveraging intrinsic hardware-level imperfections
introduced during device manufacturing to enable precise transmitter
identification. While deep neural networks have shown remarkable capability in
extracting discriminative features, their real-world deployment is hindered by
receiver-induced variability. In practice, RF fingerprint signals comprise
transmitter-specific features as well as channel distortions and
receiver-induced biases. Although channel equalization can mitigate channel
noise, receiver-induced feature shifts remain largely unaddressed, causing the
RFFI models to overfit to receiver-specific patterns. This limitation is
particularly problematic when training and evaluation share the same receiver,
as replacing the receiver in deployment can cause substantial performance
degradation. To tackle this challenge, we propose an RFFI framework robust to
cross-receiver variability, integrating adversarial training and style transfer
to explicitly disentangle transmitter and receiver features. By enforcing
domain-invariant representation learning, our method isolates genuine hardware
signatures from receiver artifacts, ensuring robustness against receiver
changes. Extensive experiments on multi-receiver datasets demonstrate that our
approach consistently outperforms state-of-the-art baselines, achieving up to a
10% improvement in average accuracy across diverse receiver settings.

</details>


### [145] [What Do Temporal Graph Learning Models Learn?](https://arxiv.org/abs/2510.09416)
*Abigail J. Hayes,Tobias Schumacher,Markus Strohmaier*

Main category: cs.LG

TL;DR: 本文系统评估了7个时序图学习模型捕捉8种基本图结构属性的能力，揭示了模型在某些属性上表现良好但在其他属性上存在局限性的混合结果。


<details>
  <summary>Details</summary>
Motivation: 由于现有基准测试结果可靠性存在问题，且简单启发式方法表现竞争力强，需要研究时序图学习模型到底利用图的哪些属性来进行预测。

Method: 使用合成和真实世界数据集，系统评估7个模型捕捉8种基本时序图属性的能力，包括结构特征、时序模式和边形成机制等。

Result: 模型能够很好地捕捉某些属性，但在重现其他属性方面失败，暴露了重要局限性。

Conclusion: 研究结果为时序图学习模型的应用提供了实用见解，并推动了时序图学习中更注重可解释性的评估方法。

Abstract: Learning on temporal graphs has become a central topic in graph
representation learning, with numerous benchmarks indicating the strong
performance of state-of-the-art models. However, recent work has raised
concerns about the reliability of benchmark results, noting issues with
commonly used evaluation protocols and the surprising competitiveness of simple
heuristics. This contrast raises the question of which properties of the
underlying graphs temporal graph learning models actually use to form their
predictions. We address this by systematically evaluating seven models on their
ability to capture eight fundamental attributes related to the link structure
of temporal graphs. These include structural characteristics such as density,
temporal patterns such as recency, and edge formation mechanisms such as
homophily. Using both synthetic and real-world datasets, we analyze how well
models learn these attributes. Our findings reveal a mixed picture: models
capture some attributes well but fail to reproduce others. With this, we expose
important limitations. Overall, we believe that our results provide practical
insights for the application of temporal graph learning models, and motivate
more interpretability-driven evaluations in temporal graph learning research.

</details>


### [146] [Weight Initialization and Variance Dynamics in Deep Neural Networks and Large Language Models](https://arxiv.org/abs/2510.09423)
*Yankun Han*

Main category: cs.LG

TL;DR: 该论文系统研究了权重初始化对训练的影响，包括ReLU多层感知机和GPT-2风格transformer，发现1e-2到1e-1的标准差范围最稳定，Kaiming初始化优于Xavier，并观察到transformer层间权重方差存在深度依赖的平衡现象。


<details>
  <summary>Details</summary>
Motivation: 权重初始化控制着训练初期的信号传播和梯度流动，对模型训练稳定性至关重要，需要理论指导和实证验证。

Method: 采用对数扫描初始标准差的方法，对比Kaiming和Xavier初始化，在12层GPT-2风格模型中跟踪层间Q/K/V权重方差变化。

Result: 发现1e-2到1e-1的标准差范围形成稳定带；Kaiming初始化收敛更快更稳定；transformer浅层权重方差快速扩展，深层变化更平缓。

Conclusion: 连接经典初始化原则与现代transformer行为，为稳健训练提供简单实用的方法。

Abstract: Weight initialization governs signal propagation and gradient flow at the
start of training. This paper offers a theory-grounded and empirically
validated study across two regimes: compact ReLU multilayer perceptrons and
GPT-2-style transformers. First, a logarithmic sweep of the initial standard
deviation maps vanishing and exploding regimes and identifies a broad stability
band with standard deviations between 1e-2 and 1e-1. Second, a controlled
comparison shows that Kaiming (fan-in) initialization converges faster and more
stably than Xavier under ReLU, consistent with variance-preserving theory.
Third, in a from-scratch 12-layer GPT-2-style model, this paper tracks
layerwise Q/K/V weight variance through pretraining and observe depth-dependent
equilibration into narrow bands: shallow layers expand rapidly while deeper
layers change more gradually. Together, these results connect classic
initialization principles with modern transformer behavior and yield simple,
practical recipes for robust training.

</details>


### [147] [Bandits with Single-Peaked Preferences and Limited Resources](https://arxiv.org/abs/2510.09425)
*Gur Keinan,Rotem Torkan,Omer Ben-Porat*

Main category: cs.LG

TL;DR: 本文研究了在线随机匹配问题，提出了针对单峰偏好结构的有效算法，在已知和未知偏好结构的情况下分别实现了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决在线随机匹配问题中由于NP难度导致的在线学习计算不可行性，利用单峰偏好结构来克服这一计算障碍。

Method: 设计了基于PQ树的顺序逼近方法，开发了针对已知和未知单峰偏好结构的两种高效算法：一种是UCB类算法，另一种是离线预算匹配算法的在线扩展。

Result: 在已知单峰结构时实现了O(U√TK)的遗憾界，在未知结构时实现了O(UKT^{2/3})的遗憾界。

Conclusion: 单峰偏好结构能够有效解决在线随机匹配问题的计算复杂性，提出的算法在理论和计算效率上都取得了良好表现。

Abstract: We study an online stochastic matching problem in which an algorithm
sequentially matches $U$ users to $K$ arms, aiming to maximize cumulative
reward over $T$ rounds under budget constraints. Without structural
assumptions, computing the optimal matching is NP-hard, making online learning
computationally infeasible. To overcome this barrier, we focus on
\emph{single-peaked preferences} -- a well-established structure in social
choice theory, where users' preferences are unimodal with respect to a common
order over arms. We devise an efficient algorithm for the offline budgeted
matching problem, and leverage it into an efficient online algorithm with a
regret of $\tilde O(UKT^{2/3})$. Our approach relies on a novel PQ tree-based
order approximation method. If the single-peaked structure is known, we develop
an efficient UCB-like algorithm that achieves a regret bound of $\tilde
O(U\sqrt{TK})$.

</details>


### [148] [Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models](https://arxiv.org/abs/2510.09435)
*Hyunin Lee,Yong Zhang,Hoang Vu Nguyen,Xiaoyi Liu,Namyong Park,Christopher Jung,Rong Jin,Yang Wang,Zhigang Wang,Somayeh Sojoudi,Xue Feng*

Main category: cs.LG

TL;DR: 本文发现跨域序列推荐中的交叉注意力机制存在正交对齐现象，即模型能发现查询输入中不存在的新信息，这与传统的残差对齐机制共存且能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将交叉注意力解释为残差对齐，但对其底层机制理解不充分。本文旨在探索交叉注意力在跨域序列推荐中的新对齐机制。

Method: 通过300多次实验分析交叉注意力机制，观察查询输入与输出之间的正交关系，研究正交对齐现象的自然涌现。

Result: 当查询输入与交叉注意力输出正交时，模型性能显著提升。正交对齐能改善缩放定律，带交叉注意力的基线模型在参数效率上优于参数匹配的基线。

Conclusion: 正交对齐现象为多模态研究中的参数高效缩放提供了新方向，交叉注意力能自然发现查询输入中不存在的新信息。

Abstract: Cross-domain sequential recommendation (CDSR) aims to align heterogeneous
user behavior sequences collected from different domains. While cross-attention
is widely used to enhance alignment and improve recommendation performance, its
underlying mechanism is not fully understood. Most researchers interpret
cross-attention as residual alignment, where the output is generated by
removing redundant and preserving non-redundant information from the query
input by referencing another domain data which is input key and value. Beyond
the prevailing view, we introduce Orthogonal Alignment, a phenomenon in which
cross-attention discovers novel information that is not present in the query
input, and further argue that those two contrasting alignment mechanisms can
co-exist in recommendation models We find that when the query input and output
of cross-attention are orthogonal, model performance improves over 300
experiments. Notably, Orthogonal Alignment emerges naturally, without any
explicit orthogonality constraints. Our key insight is that Orthogonal
Alignment emerges naturally because it improves scaling law. We show that
baselines additionally incorporating cross-attention module outperform
parameter-matched baselines, achieving a superior accuracy-per-model parameter.
We hope these findings offer new directions for parameter-efficient scaling in
multi-modal research.

</details>


### [149] [On Uniformly Scaling Flows: A Density-Aligned Approach to Deep One-Class Classification](https://arxiv.org/abs/2510.09452)
*Faried Abu Zaid,Tim Katzke,Emmanuel Müller,Daniel Neider*

Main category: cs.LG

TL;DR: 本文证明了均匀缩放流（USFs）在理论上连接了深度单类分类和密度估计两种异常检测范式，通过最大似然训练USFs等价于Deep SVDD目标，并提出了将USFs作为非USFs的替代方案来提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前无监督异常检测主要围绕深度单类分类（如Deep SVDD）和密度估计（如标准化流）两种范式展开，但两者之间的理论联系尚未明确。本文旨在揭示这两种方法的内在联系，并提出统一的框架。

Method: 通过理论证明，展示了均匀缩放流（USFs）在最大似然训练下可简化为Deep SVDD目标，并带有防止表示崩溃的正则化。USFs同时继承了流的密度忠实性和单类方法的基于距离推理特性。

Result: 实验表明，USFs在负对数似然和潜在范数之间建立了比Deep SVDD或非USFs更紧密的对齐关系。将USFs作为非USFs的替代方案在多个基准测试和模型骨干上均带来了性能提升和训练稳定性改善。

Conclusion: USFs统一了异常检测的两大主要范式，不仅推进了理论理解，还提升了实际性能。建议在现代异常检测架构中使用USFs作为非USFs的即插即用替代方案。

Abstract: Unsupervised anomaly detection is often framed around two widely studied
paradigms. Deep one-class classification, exemplified by Deep SVDD, learns
compact latent representations of normality, while density estimators realized
by normalizing flows directly model the likelihood of nominal data. In this
work, we show that uniformly scaling flows (USFs), normalizing flows with a
constant Jacobian determinant, precisely connect these approaches.
Specifically, we prove how training a USF via maximum-likelihood reduces to a
Deep SVDD objective with a unique regularization that inherently prevents
representational collapse. This theoretical bridge implies that USFs inherit
both the density faithfulness of flows and the distance-based reasoning of
one-class methods. We further demonstrate that USFs induce a tighter alignment
between negative log-likelihood and latent norm than either Deep SVDD or
non-USFs, and how recent hybrid approaches combining one-class objectives with
VAEs can be naturally extended to USFs. Consequently, we advocate using USFs as
a drop-in replacement for non-USFs in modern anomaly detection architectures.
Empirically, this substitution yields consistent performance gains and
substantially improved training stability across multiple benchmarks and model
backbones for both image-level and pixel-level detection. These results unify
two major anomaly detection paradigms, advancing both theoretical understanding
and practical performance.

</details>


### [150] [Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols](https://arxiv.org/abs/2510.09462)
*Mikhail Terekhov,Alexander Panfilov,Daniil Dzenhaliou,Caglar Gulcehre,Maksym Andriushchenko,Ameya Prabhu,Jonas Geiping*

Main category: cs.LG

TL;DR: AI控制协议中的监控模型存在严重安全漏洞，攻击者通过嵌入公开已知或零样本提示注入，能够持续规避各种监控器完成恶意任务。


<details>
  <summary>Details</summary>
Motivation: 研究AI控制协议中监控模型的适应性攻击，因为现有协议主要依赖LLM监控器，这成为了单点故障，而攻击者可能了解协议和监控模型细节。

Method: 实例化简单的适应性攻击向量，攻击者在模型输出中嵌入公开已知或零样本提示注入，利用监控模型的漏洞。

Result: 前沿模型能够持续规避多样化的监控器，在两个主要AI控制基准上完成恶意任务，攻击对当前依赖监控器的协议具有普遍有效性。

Conclusion: 监控模型的适应性攻击是当前控制协议的主要盲点，应该成为未来AI控制机制评估的标准组成部分。

Abstract: AI control protocols serve as a defense mechanism to stop untrusted LLM
agents from causing harm in autonomous settings. Prior work treats this as a
security problem, stress testing with exploits that use the deployment context
to subtly complete harmful side tasks, such as backdoor insertion. In practice,
most AI control protocols are fundamentally based on LLM monitors, which can
become a central point of failure. We study adaptive attacks by an untrusted
model that knows the protocol and the monitor model, which is plausible if the
untrusted model was trained with a later knowledge cutoff or can search for
this information autonomously. We instantiate a simple adaptive attack vector
by which the attacker embeds publicly known or zero-shot prompt injections in
the model outputs. Using this tactic, frontier models consistently evade
diverse monitors and complete malicious tasks on two main AI control
benchmarks. The attack works universally against current protocols that rely on
a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,
as its resampling amplifies the prompt injection and effectively reframes it as
a best-of-$n$ attack. In general, adaptive attacks on monitor models represent
a major blind spot in current control protocols and should become a standard
component of evaluations for future AI control mechanisms.

</details>


### [151] [Interpretable Machine Learning for Predicting Startup Funding, Patenting, and Exits](https://arxiv.org/abs/2510.09465)
*Saeid Mashhadi,Amirhossein Saghezchi,Vesal Ghassemzadeh Kashani*

Main category: cs.LG

TL;DR: 开发可解释的机器学习框架预测初创企业结果（融资、专利申请、退出），使用Crunchbase和USPTO数据，在三个时间维度上评估预测性能，获得高精度结果。


<details>
  <summary>Details</summary>
Motivation: 为创新金融提供透明且可复现的预测工具，帮助投资者和决策者更好地评估初创企业的未来发展潜力。

Method: 构建2010-2023年企业季度面板数据，使用逻辑回归和树集成方法（随机森林、XGBoost、LightGBM、CatBoost），采用逆流行权重和SMOTE-NC处理类别不平衡，评估PR-AUC和AUROC指标。

Result: 专利申请、融资和退出预测的AUROC值分别为0.921、0.817和0.872，显示出优秀的预测性能。

Conclusion: 该框架为创新金融提供了可靠的预测工具，能够准确预测初创企业的关键发展里程碑。

Abstract: This study develops an interpretable machine learning framework to forecast
startup outcomes, including funding, patenting, and exit. A firm-quarter panel
for 2010-2023 is constructed from Crunchbase and matched to U.S. Patent and
Trademark Office (USPTO) data. Three horizons are evaluated: next funding
within 12 months, patent-stock growth within 24 months, and exit through an
initial public offering (IPO) or acquisition within 36 months. Preprocessing is
fit on a development window (2010-2019) and applied without change to later
cohorts to avoid leakage. Class imbalance is addressed using inverse-prevalence
weights and the Synthetic Minority Oversampling Technique for Nominal and
Continuous features (SMOTE-NC). Logistic regression and tree ensembles,
including Random Forest, XGBoost, LightGBM, and CatBoost, are compared using
the area under the precision-recall curve (PR-AUC) and the area under the
receiver operating characteristic curve (AUROC). Patent, funding, and exit
predictions achieve AUROC values of 0.921, 0.817, and 0.872, providing
transparent and reproducible rankings for innovation finance.

</details>


### [152] [Geodesic Calculus on Latent Spaces](https://arxiv.org/abs/2510.09468)
*Florine Hartwig,Josua Sassen,Juliane Braunsmann,Martin Rumpf,Benedikt Wirth*

Main category: cs.LG

TL;DR: 提出了一种将自编码器潜在流形描述为隐式子流形的方法，开发了离散黎曼几何计算工具，支持在潜在流形上计算测地线和黎曼指数映射。


<details>
  <summary>Details</summary>
Motivation: 自编码器的潜在流形提供数据的低维表示，但从几何角度研究这些流形需要合适的数学工具和表示方法。

Method: 将潜在流形描述为隐式子流形，学习近似投影函数，开发离散黎曼几何计算工具，这些工具对隐式表示的不准确性具有鲁棒性。

Result: 在合成和真实数据训练的自编码器上评估了该方法，能够有效计算测地线路径和黎曼指数映射。

Conclusion: 该方法为研究自编码器潜在流形的几何特性提供了通用框架，支持不同黎曼几何结构，具有实际应用价值。

Abstract: Latent manifolds of autoencoders provide low-dimensional representations of
data, which can be studied from a geometric perspective. We propose to describe
these latent manifolds as implicit submanifolds of some ambient latent space.
Based on this, we develop tools for a discrete Riemannian calculus
approximating classical geometric operators. These tools are robust against
inaccuracies of the implicit representation often occurring in practical
examples. To obtain a suitable implicit representation, we propose to learn an
approximate projection onto the latent manifold by minimizing a denoising
objective. This approach is independent of the underlying autoencoder and
supports the use of different Riemannian geometries on the latent manifolds.
The framework in particular enables the computation of geodesic paths
connecting given end points and shooting geodesics via the Riemannian
exponential maps on latent manifolds. We evaluate our approach on various
autoencoders trained on synthetic and real data.

</details>


### [153] [CRPS-LAM: Regional ensemble weather forecasting from matching marginals](https://arxiv.org/abs/2510.09484)
*Erik Larsson,Joel Oskarsson,Tomas Landelius,Fredrik Lindsten*

Main category: cs.LG

TL;DR: CRPS-LAM是一种基于CRPS目标的概率性区域天气预报模型，通过单次前向传播生成集合成员，采样速度比扩散模型快39倍，在MEPS数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在区域天气预报中表现良好但计算成本高，需要更高效的集合预报方法。

Method: 使用基于CRPS的目标训练模型，通过采样和注入单个潜在噪声向量，在单次前向传播中生成集合成员。

Result: 采样速度比扩散模型快39倍，在MEPS区域数据集上误差与扩散模型相当，同时保留了精细尺度的预报细节。

Conclusion: CRPS-LAM是概率性区域天气预报的有效方法，在保持准确性的同时显著提高了计算效率。

Abstract: Machine learning for weather prediction increasingly relies on ensemble
methods to provide probabilistic forecasts. Diffusion-based models have shown
strong performance in Limited-Area Modeling (LAM) but remain computationally
expensive at sampling time. Building on the success of global weather
forecasting models trained based on Continuous Ranked Probability Score (CRPS),
we introduce CRPS-LAM, a probabilistic LAM forecasting model trained with a
CRPS-based objective. By sampling and injecting a single latent noise vector
into the model, CRPS-LAM generates ensemble members in a single forward pass,
achieving sampling speeds up to 39 times faster than a diffusion-based model.
We evaluate the model on the MEPS regional dataset, where CRPS-LAM matches the
low errors of diffusion models. By retaining also fine-scale forecast details,
the method stands out as an effective approach for probabilistic regional
weather forecasting

</details>


### [154] [Locally Optimal Private Sampling: Beyond the Global Minimax](https://arxiv.org/abs/2510.09485)
*Hrad Ghoukasian,Bonwoo Lee,Shahab Asoodeh*

Main category: cs.LG

TL;DR: 本文研究在局部差分隐私(LDP)约束下从分布中采样的局部极小极大风险问题，发现在固定分布P0邻域内的最优采样器由全局极小极大风险决定，并给出了与f-散度无关的闭式解。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注全局极小极大最优性，本文从局部视角出发，研究在固定分布邻域内的采样问题，这更贴近具有公开数据的隐私采样场景。

Method: 将先前纯LDP工作扩展到泛函LDP框架，证明全局最优泛函LDP采样器在限制到P0邻域时产生最优局部采样器，并推导局部极小极大最优采样器的闭式表达式。

Result: 确定了局部极小极大风险的确切值，发现它由全局极小极大风险决定，且局部最优采样器不依赖于f-散度的选择。在具有公开数据的隐私采样场景中，局部最优采样器持续优于全局极小极大采样器。

Conclusion: 局部框架为隐私采样提供了新的理论视角，特别适用于具有公开数据的实际场景，局部最优采样器在性能上优于全局方法。

Abstract: We study the problem of sampling from a distribution under local differential
privacy (LDP). Given a private distribution $P \in \mathcal{P}$, the goal is to
generate a single sample from a distribution that remains close to $P$ in
$f$-divergence while satisfying the constraints of LDP. This task captures the
fundamental challenge of producing realistic-looking data under strong privacy
guarantees. While prior work by Park et al. (NeurIPS'24) focuses on global
minimax-optimality across a class of distributions, we take a local
perspective. Specifically, we examine the minimax risk in a neighborhood around
a fixed distribution $P_0$, and characterize its exact value, which depends on
both $P_0$ and the privacy level. Our main result shows that the local minimax
risk is determined by the global minimax risk when the distribution class
$\mathcal{P}$ is restricted to a neighborhood around $P_0$. To establish this,
we (1) extend previous work from pure LDP to the more general functional LDP
framework, and (2) prove that the globally optimal functional LDP sampler
yields the optimal local sampler when constrained to distributions near $P_0$.
Building on this, we also derive a simple closed-form expression for the
locally minimax-optimal samplers which does not depend on the choice of
$f$-divergence. We further argue that this local framework naturally models
private sampling with public data, where the public data distribution is
represented by $P_0$. In this setting, we empirically compare our locally
optimal sampler to existing global methods, and demonstrate that it
consistently outperforms global minimax samplers.

</details>


### [155] [Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning](https://arxiv.org/abs/2510.09487)
*Shangzhe Li,Dongruo Zhou,Weitong Zhang*

Main category: cs.LG

TL;DR: 该论文提出了基于模型的对抗模仿学习算法(MB-AIL)，在无奖励的在线交互环境下，实现了与最优样本复杂度匹配的理论保证和优越的样本效率。


<details>
  <summary>Details</summary>
Motivation: 在线对抗模仿学习虽然实证效果显著，但其在线交互的益处和随机性影响缺乏理论理解。本文旨在填补这一空白，建立严格的理论分析框架。

Method: 引入基于模型的对抗模仿学习算法(MB-AIL)，使用一般函数逼近处理专家数据和奖励自由交互，建立二阶样本复杂度保证。

Result: MB-AIL实现了与horizon无关的二阶样本复杂度保证，在新建的困难实例族上达到极小极大最优样本复杂度，实验验证其样本效率优于现有方法。

Conclusion: MB-AIL在理论保证和实际性能上都表现出色，为在线对抗模仿学习提供了坚实的理论基础和有效的算法实现。

Abstract: We study online adversarial imitation learning (AIL), where an agent learns
from offline expert demonstrations and interacts with the environment online
without access to rewards. Despite strong empirical results, the benefits of
online interaction and the impact of stochasticity remain poorly understood. We
address these gaps by introducing a model-based AIL algorithm (MB-AIL) and
establish its horizon-free, second-order sample-complexity guarantees under
general function approximations for both expert data and reward-free
interactions. These second-order bounds provide an instance-dependent result
that can scale with the variance of returns under the relevant policies and
therefore tighten as the system approaches determinism. Together with
second-order, information-theoretic lower bounds on a newly constructed
hard-instance family, we show that MB-AIL attains minimax-optimal sample
complexity for online interaction (up to logarithmic factors) with limited
expert demonstrations and matches the lower bound for expert demonstrations in
terms of the dependence on horizon $H$, precision $\epsilon$ and the policy
variance $\sigma^2$. Experiments further validate our theoretical findings and
demonstrate that a practical implementation of MB-AIL matches or surpasses the
sample efficiency of existing methods.

</details>


### [156] [Performance Analysis of Machine Learning Algorithms in Chronic Kidney Disease Prediction](https://arxiv.org/abs/2510.09493)
*Iftekhar Ahmed,Tanzil Ebad Chowdhury,Biggo Bushon Routh,Nafisa Tasmiya,Shadman Sakib,Adil Ahmed Chowdhury*

Main category: cs.LG

TL;DR: 本研究开发了基于机器学习的慢性肾病预测系统，使用8种算法进行性能比较，随机森林和逻辑回归达到99%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 全球约10%人口受慢性肾病影响，需要有效的风险评估和监测系统来保护高危患者免受进一步肾损伤。

Method: 从UCL机器学习库获取CKD数据集，使用均值-众数和随机抽样方法填补缺失值，应用8种机器学习算法建立预测模型。

Result: 随机森林和逻辑回归表现最佳，达到99%准确率，AdaBoost、XGBoost、朴素贝叶斯、决策树和SVM次之，KNN分类器准确率最低为73%。

Conclusion: 机器学习模型特别是随机森林和逻辑回归，能够有效预测慢性肾病，为医疗实践提供高效准确的诊断支持。

Abstract: Kidneys are the filter of the human body. About 10% of the global population
is thought to be affected by Chronic Kidney Disease (CKD), which causes kidney
function to decline. To protect in danger patients from additional kidney
damage, effective risk evaluation of CKD and appropriate CKD monitoring are
crucial. Due to quick and precise detection capabilities, Machine Learning
models can help practitioners accomplish this goal efficiently; therefore, an
enormous number of diagnosis systems and processes in the healthcare sector
nowadays are relying on machine learning due to its disease prediction
capability. In this study, we designed and suggested disease predictive
computer-aided designs for the diagnosis of CKD. The dataset for CKD is
attained from the repository of machine learning of UCL, with a few missing
values; those are filled in using "mean-mode" and "Random sampling method"
strategies. After successfully achieving the missing data, eight ML techniques
(Random Forest, SVM, Naive Bayes, Logistic Regression, KNN, XGBoost, Decision
Tree, and AdaBoost) were used to establish models, and the performance
evaluation comparisons among the result accuracies are measured by the
techniques to find the machine learning models with the highest accuracy. Among
them, Random Forest as well as Logistic Regression showed an outstanding 99%
accuracy, followed by the Ada Boost, XGBoost, Naive Bayes, Decision Tree, and
SVM, whereas the KNN classifier model stands last with an accuracy of 73%.

</details>


### [157] [Geo-Aware Models for Stream Temperature Prediction across Different Spatial Regions and Scales](https://arxiv.org/abs/2510.09500)
*Shiyuan Luo,Runlong Yu,Shengyu Chen,Yingda Fan,Yiqun Xie,Yanhua Li,Xiaowei Jia*

Main category: cs.LG

TL;DR: Geo-STARS是一个地理感知的时空建模框架，用于预测不同流域和空间尺度下的河流水温。通过引入地理感知嵌入，该模型能够捕捉跨区域和尺度的共享模式，即使在观测数据稀疏的情况下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 现有的物理模型和数据驱动模型由于环境生态系统中固有的数据异质性，难以泛化到不同的空间区域和尺度，且训练样本有限进一步加剧了这一问题。

Method: 提出地理感知嵌入方法，利用地理信息显式捕捉跨空间区域和尺度的共享原理和模式，并将其集成到门控时空图神经网络中，使模型能够在地理和水文背景下学习复杂的时空模式。

Result: 使用美国东海岸多个流域37年的真实数据集进行评估，Geo-STARS在跨区域和跨尺度的泛化性能上优于现有最先进的基线方法。

Conclusion: Geo-STARS为可扩展、数据高效的环境监测和决策制定提供了有前景的解决方案。

Abstract: Understanding environmental ecosystems is vital for the sustainable
management of our planet. However,existing physics-based and data-driven models
often fail to generalize to varying spatial regions and scales due to the
inherent data heterogeneity presented in real environmental ecosystems. This
generalization issue is further exacerbated by the limited observation samples
available for model training. To address these issues, we propose Geo-STARS, a
geo-aware spatio-temporal modeling framework for predicting stream water
temperature across different watersheds and spatial scales. The major
innovation of Geo-STARS is the introduction of geo-aware embedding, which
leverages geographic information to explicitly capture shared principles and
patterns across spatial regions and scales. We further integrate the geo-aware
embedding into a gated spatio-temporal graph neural network. This design
enables the model to learn complex spatial and temporal patterns guided by
geographic and hydrological context, even with sparse or no observational data.
We evaluate Geo-STARS's efficacy in predicting stream water temperature, which
is a master factor for water quality. Using real-world datasets spanning 37
years across multiple watersheds along the eastern coast of the United States,
Geo-STARS demonstrates its superior generalization performance across both
regions and scales, outperforming state-of-the-art baselines. These results
highlight the promise of Geo-STARS for scalable, data-efficient environmental
monitoring and decision-making.

</details>


### [158] [Automated Evolutionary Optimization for Resource-Efficient Neural Network Training](https://arxiv.org/abs/2510.09566)
*Ilia Revin,Leon Strelkov,Vadim A. Potemkin,Ivan Kireev,Andrey Savchenko*

Main category: cs.LG

TL;DR: PETRA是一个自动化机器学习框架，通过进化优化模型架构和训练策略，结合剪枝、量化和损失正则化，显著减小模型规模（最多75%）和延迟（最多33%），提升吞吐量（13%）且不影响目标指标。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和资源高效模型需求的增加，解决分布式计算、压缩技术和高效训练等神经网络优化挑战变得至关重要。

Method: 开发PETRA框架，应用进化优化来调整模型架构和训练策略，包括剪枝、量化和损失正则化技术。

Result: 在金融事件序列、图像和时间序列基准测试中，PETRA显著减小模型规模（最多75%）、降低延迟（最多33%）、提高吞吐量（13%），且目标指标无明显下降。

Conclusion: PETRA框架有效解决了神经网络模型优化的关键挑战，实现了模型性能与资源效率的平衡。

Abstract: There are many critical challenges in optimizing neural network models,
including distributed computing, compression techniques, and efficient
training, regardless of their application to specific tasks. Solving such
problems is crucial because the need for scalable and resource-efficient models
is increasing. To address these challenges, we have developed a new automated
machine learning (AutoML) framework, Parameter Efficient Training with Robust
Automation (PETRA). It applies evolutionary optimization to model architecture
and training strategy. PETRA includes pruning, quantization, and loss
regularization. Experimental studies on real-world data with financial event
sequences, as well as image and time-series -- benchmarks, demonstrate PETRA's
ability to improve neural model performance and scalability -- namely, a
significant decrease in model size (up to 75%) and latency (up to 33%), and an
increase in throughput (by 13%) without noticeable degradation in the target
metric.

</details>


### [159] [STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging](https://arxiv.org/abs/2510.09593)
*Disharee Bhowmick,Ranjith Ramanathan,Sathyanarayanan N. Aakur*

Main category: cs.LG

TL;DR: STaTS是一个轻量级无监督框架，通过自适应压缩时间序列为紧凑的标记序列，实现高达30倍的序列压缩，同时保留核心时间动态。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常处理原始或固定窗口序列，将所有时间步视为同等信息，导致效率低下、鲁棒性差，在长序列或噪声序列中扩展性有限。

Method: 使用基于BIC的统计差异准则检测多时间分辨率的变化点，然后用均值或GMM等生成模型总结每个片段。

Result: 在150多个数据集上的实验表明，STaTS能够达到完整模型85-90%的性能，同时显著降低计算成本，在噪声环境下提高鲁棒性。

Conclusion: STaTS作为模型无关的预处理器，为高效、结构感知的时间序列建模提供了原则性的通用解决方案。

Abstract: Time series data often contain latent temporal structure, transitions between
locally stationary regimes, repeated motifs, and bursts of variability, that
are rarely leveraged in standard representation learning pipelines. Existing
models typically operate on raw or fixed-window sequences, treating all time
steps as equally informative, which leads to inefficiencies, poor robustness,
and limited scalability in long or noisy sequences. We propose STaTS, a
lightweight, unsupervised framework for Structure-Aware Temporal Summarization
that adaptively compresses both univariate and multivariate time series into
compact, information-preserving token sequences. STaTS detects change points
across multiple temporal resolutions using a BIC-based statistical divergence
criterion, then summarizes each segment using simple functions like the mean or
generative models such as GMMs. This process achieves up to 30x sequence
compression while retaining core temporal dynamics. STaTS operates as a
model-agnostic preprocessor and can be integrated with existing unsupervised
time series encoders without retraining. Extensive experiments on 150+
datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30
archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,
demonstrate that STaTS enables 85-90\% of the full-model performance while
offering dramatic reductions in computational cost. Moreover, STaTS improves
robustness under noise and preserves discriminative structure, outperforming
uniform and clustering-based compression baselines. These results position
STaTS as a principled, general-purpose solution for efficient, structure-aware
time series modeling.

</details>


### [160] [MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts](https://arxiv.org/abs/2510.09594)
*Nathan Quiblier,Roy Friedman,Matthew Ricci*

Main category: cs.LG

TL;DR: MODE是一个图模型框架，通过神经门控机制将复杂动态分解为稀疏可解释组件，能够无监督发现行为机制并在机制转换中进行准确长期预测。


<details>
  <summary>Details</summary>
Motivation: 生命科学中的动态系统通常包含重叠行为机制的复杂混合，传统基于流的建模技术假设局部平滑动态，难以处理这些机制间的噪声和不规则转换。

Method: 提出MODE（Mixture Of Dynamical Experts）框架，使用神经门控机制分解复杂动态为稀疏可解释组件，允许智能体跳转到不同控制法则。

Result: 在合成和真实计算生物学数据集上评估，MODE在无监督分类任务中表现优异，能够成功预测细胞生物学中的关键循环和分支过程，并在单细胞RNA测序数据中区分增殖与分化动态，预测细胞命运承诺时间。

Conclusion: MODE特别适合处理噪声转换，能够解决计算生物学中的关键挑战，包括行为机制的无监督发现和细胞命运预测。

Abstract: Dynamical systems in the life sciences are often composed of complex mixtures
of overlapping behavioral regimes. Cellular subpopulations may shift from
cycling to equilibrium dynamics or branch towards different developmental
fates. The transitions between these regimes can appear noisy and irregular,
posing a serious challenge to traditional, flow-based modeling techniques which
assume locally smooth dynamics. To address this challenge, we propose MODE
(Mixture Of Dynamical Experts), a graphical modeling framework whose neural
gating mechanism decomposes complex dynamics into sparse, interpretable
components, enabling both the unsupervised discovery of behavioral regimes and
accurate long-term forecasting across regime transitions. Crucially, because
agents in our framework can jump to different governing laws, MODE is
especially tailored to the aforementioned noisy transitions. We evaluate our
method on a battery of synthetic and real datasets from computational biology.
First, we systematically benchmark MODE on an unsupervised classification task
using synthetic dynamical snapshot data, including in noisy, few-sample
settings. Next, we show how MODE succeeds on challenging forecasting tasks
which simulate key cycling and branching processes in cell biology. Finally, we
deploy our method on human, single-cell RNA sequencing data and show that it
can not only distinguish proliferation from differentiation dynamics but also
predict when cells will commit to their ultimate fate, a key outstanding
challenge in computational biology.

</details>


### [161] [BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards](https://arxiv.org/abs/2510.09596)
*Sangyun Lee,Brandon Amos,Giulia Fanti*

Main category: cs.LG

TL;DR: 提出了BaNEL算法，仅使用失败样本进行后训练，在稀疏奖励任务中显著提升模型性能，同时最小化奖励评估次数


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在困难问题中面临的两个挑战：(1)基础模型获得接近零的奖励信号，(2)奖励函数调用成本高昂

Method: 将学习失败规律建模为另一个生成建模问题，利用该模型评估新数据是否类似于已见过的失败样本，并引导生成远离这些失败

Result: 在多个稀疏奖励任务中，无需观察任何成功样本即可提升模型性能，成功率比现有新颖性奖励方法高出几个数量级，同时使用更少的奖励评估

Conclusion: BaNEL算法能够有效处理稀疏奖励环境中的学习挑战，通过仅利用失败样本来改善生成质量，为困难问题提供了一种高效的后训练方法

Abstract: Today's generative models thrive with large amounts of supervised data and
informative reward functions characterizing the quality of the generation. They
work under the assumptions that the supervised data provides knowledge to
pre-train the model, and the reward function provides dense information about
how to further improve the generation quality and correctness. However, in the
hardest instances of important problems, two problems arise: (1) the base
generative model attains a near-zero reward signal, and (2) calls to the reward
oracle are expensive. This setting poses a fundamentally different learning
challenge than standard reward-based post-training. To address this, we propose
BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the
model using failed attempts only, while minimizing the number of reward
evaluations (NREs). Our method is based on the idea that the problem of
learning regularities underlying failures can be cast as another, in-loop
generative modeling problem. We then leverage this model to assess whether new
data resembles previously seen failures and steer the generation away from
them. We show that BaNEL can improve model performance without observing a
single successful sample on several sparse-reward tasks, outperforming existing
novelty-bonus approaches by up to several orders of magnitude in success rate,
while using fewer reward evaluations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [162] [Federated Data Analytics for Cancer Immunotherapy: A Privacy-Preserving Collaborative Platform for Patient Management](https://arxiv.org/abs/2510.09155)
*Mira Raheem,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi,Amal Elgammal*

Main category: cs.CY

TL;DR: 本文提出了一个用于癌症免疫治疗患者管理的协作数字框架，整合了联邦大数据分析和人工智能，在保护隐私的同时改善决策制定。


<details>
  <summary>Details</summary>
Motivation: 连接健康需要整合多种医疗数据源以实现个性化护理、预测健康结果和简化患者管理，但在数据架构、应用互操作性和安全性方面仍存在挑战。

Method: 通过敏捷系统开发生命周期开发集成AI生成解决方案，采用联邦大数据分析和人工智能技术，构建协作数字框架整合护理连续体中的各利益相关者。

Result: 在医疗合作伙伴的试点研究中，治疗推荐和不良事件预测等分析能力使用真实数据验证，达到了70%-90%的准确率。

Conclusion: 该框架通过整合利益相关者、利用联邦分析和AI技术，在保护隐私的同时有效改善了决策制定，证明了其在实际应用中的有效性。

Abstract: Connected health is a multidisciplinary approach focused on health
management, prioritizing pa-tient needs in the creation of tools, services, and
treatments. This paradigm ensures proactive and efficient care by facilitating
the timely exchange of accurate patient information among all stake-holders in
the care continuum. The rise of digital technologies and process innovations
promises to enhance connected health by integrating various healthcare data
sources. This integration aims to personalize care, predict health outcomes,
and streamline patient management, though challeng-es remain, particularly in
data architecture, application interoperability, and security. Data analytics
can provide critical insights for informed decision-making and health
co-creation, but solutions must prioritize end-users, including patients and
healthcare professionals. This perspective was explored through an agile System
Development Lifecycle in an EU-funded project aimed at developing an integrated
AI-generated solution for managing cancer patients undergoing immunotherapy.
This paper contributes with a collaborative digital framework integrating
stakeholders across the care continuum, leveraging federated big data analytics
and artificial intelligence for improved decision-making while ensuring
privacy. Analytical capabilities, such as treatment recommendations and adverse
event predictions, were validated using real-life data, achieving 70%-90%
accuracy in a pilot study with the medical partners, demonstrating the
framework's effectiveness.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [163] [PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces](https://arxiv.org/abs/2509.00670)
*Gursimran Singh,Aviral Chharia,Rahul Upadhyay,Vinay Kumar,Luca Longo*

Main category: eess.SP

TL;DR: PyNoetic是一个模块化的脑机接口框架，解决了现有BCI系统缺乏阶段性灵活性、学习曲线陡峭、成本高昂和功能不完整的问题，提供了从刺激呈现到数据可视化的完整设计流程。


<details>
  <summary>Details</summary>
Motivation: 现有BCI框架存在多个限制：缺乏实验研究所需的阶段性灵活性、对无编程经验的研究者学习曲线陡峭、依赖专有软件导致成本高昂、功能不完整需使用多个外部工具影响研究结果。

Method: 开发PyNoetic模块化BCI框架，包含完整的BCI设计流程：刺激呈现、数据采集、通道选择、滤波、特征提取、伪影去除、模拟和可视化。提供直观的端到端GUI和无代码设计的可配置流程图。

Result: PyNoetic是少数几个涵盖完整BCI设计流程的Python框架之一，具有直观GUI和无代码设计功能，支持自定义功能集成，包含丰富的分析工具如机器学习模型、脑连接指数等。

Conclusion: PyNoetic在离线和实时BCI开发方面具有多功能性，简化了设计流程，使研究人员能够专注于BCI开发的更复杂方面，从而加速研究工作。

Abstract: Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have
emerged as a transformative technology with applications spanning robotics,
virtual reality, medicine, and rehabilitation. However, existing BCI frameworks
face several limitations, including a lack of stage-wise flexibility essential
for experimental research, steep learning curves for researchers without
programming expertise, elevated costs due to reliance on proprietary software,
and a lack of all-inclusive features leading to the use of multiple external
tools affecting research outcomes. To address these challenges, we present
PyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI
research. PyNoetic is one of the very few frameworks in Python that encompasses
the entire BCI design pipeline, from stimulus presentation and data acquisition
to channel selection, filtering, feature extraction, artifact removal, and
finally simulation and visualization. Notably, PyNoetic introduces an intuitive
and end-to-end GUI coupled with a unique pick-and-place configurable flowchart
for no-code BCI design, making it accessible to researchers with minimal
programming experience. For advanced users, it facilitates the seamless
integration of custom functionalities and novel algorithms with minimal coding,
ensuring adaptability at each design stage. PyNoetic also includes a rich array
of analytical tools such as machine learning models, brain-connectivity
indices, systematic testing functionalities via simulation, and evaluation
methods of novel paradigms. PyNoetic's strengths lie in its versatility for
both offline and real-time BCI development, which streamlines the design
process, allowing researchers to focus on more intricate aspects of BCI
development and thus accelerate their research endeavors. Project Website:
https://neurodiag.github.io/PyNoetic

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [164] [Investigating the Impact of Rational Dilated Wavelet Transform on Motor Imagery EEG Decoding with Deep Learning Models](https://arxiv.org/abs/2510.09242)
*Marco Siino,Giuseppe Bonomo,Rosario Sorbello,Ilenia Tinnirello*

Main category: cs.HC

TL;DR: 本研究评估了有理离散小波变换(RDWT)作为预处理步骤对运动想象脑电信号解码的影响，发现在多个深度学习架构和基准数据集上能显著提升分类性能，特别是在具有挑战性的记录中。


<details>
  <summary>Details</summary>
Motivation: 运动想象脑电信号解码面临噪声和非平稳性等挑战，需要有效的预处理方法来提升深度学习模型的性能。

Method: 使用有理离散小波变换(RDWT)作为预处理步骤，在四个深度学习架构(EEGNet、ShallowConvNet、MBEEG_SENet、EEGTCNet)和三个基准数据集上进行系统配对评估。

Result: 在BCI-IV-2a数据集上，RDWT为EEGTCNet带来+4.44百分点的准确率提升，在其他数据集和模型上也观察到一致的性能改进，特别是在具有挑战性的记录中效果更显著。

Conclusion: RDWT是一种低开销、架构感知的预处理技术，能够显著提升深度学习模型在脑电信号解码中的准确性和一致性。

Abstract: The present study investigates the impact of the Rational Discrete Wavelet
Transform (RDWT), used as a plug-in preprocessing step for motor imagery
electroencephalographic (EEG) decoding prior to applying deep learning
classifiers. A systematic paired evaluation (with/without RDWT) is conducted on
four state-of-the-art deep learning architectures: EEGNet, ShallowConvNet,
MBEEG\_SENet, and EEGTCNet. This evaluation was carried out across three
benchmark datasets: High Gamma, BCI-IV-2a, and BCI-IV-2b. The performance of
the RDWT is reported with subject-wise averages using accuracy and Cohen's
kappa, complemented by subject-level analyses to identify when RDWT is
beneficial. On BCI-IV-2a, RDWT yields clear average gains for EEGTCNet (+4.44
percentage points, pp; kappa +0.059) and MBEEG\_SENet (+2.23 pp; +0.030), with
smaller improvements for EEGNet (+2.08 pp; +0.027) and ShallowConvNet (+0.58
pp; +0.008). On BCI-IV-2b, the enhancements observed are modest yet consistent
for EEGNet (+0.21 pp; +0.044) and EEGTCNet (+0.28 pp; +0.077). On HGD, average
effects are modest to positive, with the most significant gain observed for
MBEEG\_SENet (+1.65 pp; +0.022), followed by EEGNet (+0.76 pp; +0.010) and
EEGTCNet (+0.54 pp; +0.008). Inspection of the subject material reveals
significant enhancements in challenging recordings (e.g., non-stationary
sessions), indicating that RDWT can mitigate localized noise and enhance
rhythm-specific information. In conclusion, RDWT is shown to be a low-overhead,
architecture-aware preprocessing technique that can yield tangible gains in
accuracy and agreement for deep model families and challenging subjects.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [165] [Robust Heuristic Algorithm Design with LLMs](https://arxiv.org/abs/2510.08755)
*Pantea Karimi,Dany Rouhana,Pooria Namyar,Siva Kesava Reddy Kakarla,Venkat Arun,Behnaz Arzani*

Main category: cs.AI

TL;DR: 通过向LLM提供启发式方法表现不佳的实例、解释原因并针对输入空间特定区域进行专门设计，可以生成比现有技术更鲁棒的启发式算法，在保持运行时间的同时显著提升最坏情况性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的启发式设计方法存在鲁棒性不足的问题，需要工具来解释启发式方法表现不佳的原因并提供改进建议。

Method: 采用三个简单策略：(1)向LLM展示启发式方法表现不佳的实例；(2)解释表现不佳的原因；(3)针对输入空间特定区域进行专门设计。

Result: 生成的启发式算法比FunSearch的最坏情况性能提升约28倍，平均性能也有所改善，同时保持运行时间不变。

Conclusion: 通过向LLM提供失败案例分析和针对性设计，可以显著提升启发式算法的鲁棒性和性能表现。

Abstract: We posit that we can generate more robust and performant heuristics if we
augment approaches using LLMs for heuristic design with tools that explain why
heuristics underperform and suggestions about how to fix them. We find even
simple ideas that (1) expose the LLM to instances where the heuristic
underperforms; (2) explain why they occur; and (3) specialize design to regions
in the input space, can produce more robust algorithms compared to existing
techniques~ -- ~the heuristics we produce have a $\sim28\times$ better
worst-case performance compared to FunSearch, improve average performance, and
maintain the runtime.

</details>


### [166] [Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse](https://arxiv.org/abs/2510.09567)
*Jacopo Tagliabue,Ciro Greco*

Main category: cs.AI

TL;DR: 本文提出使用API优先、可编程的数据湖仓作为安全设计的智能体工作流基础架构，通过Bauplan案例展示了数据分支和声明式环境如何自然扩展到智能体，实现可重现性和可观测性。


<details>
  <summary>Details</summary>
Motivation: 数据湖仓运行敏感工作负载，AI驱动的自动化引发了关于信任、正确性和治理的担忧。需要为智能体工作流提供安全设计的基础架构。

Method: 使用Bauplan作为案例研究，展示数据分支和声明式环境如何扩展到智能体。提出了一个概念验证，其中智能体使用受证明携带代码启发的正确性检查来修复数据管道。

Result: 原型演示表明，不受信任的AI智能体可以在生产数据上安全操作，并为完全智能体化的湖仓指明了路径。

Conclusion: API优先、可编程的湖仓提供了正确的抽象，能够实现安全设计的智能体工作流，通过数据分支和声明式环境实现可重现性和可观测性，同时减少攻击面。

Abstract: Data lakehouses run sensitive workloads, where AI-driven automation raises
concerns about trust, correctness, and governance. We argue that API-first,
programmable lakehouses provide the right abstractions for safe-by-design,
agentic workflows. Using Bauplan as a case study, we show how data branching
and declarative environments extend naturally to agents, enabling
reproducibility and observability while reducing the attack surface. We present
a proof-of-concept in which agents repair data pipelines using correctness
checks inspired by proof-carrying code. Our prototype demonstrates that
untrusted AI agents can operate safely on production data and outlines a path
toward a fully agentic lakehouse.

</details>


### [167] [MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction](https://arxiv.org/abs/2510.09049)
*Joonghyuk Hahn,Soohan Lim,Yo-Sub Han*

Main category: cs.AI

TL;DR: MEC$^3$O是一个多专家共识系统，通过将LLMs分配到不同的时间复杂度类别并让它们进行结构化辩论，显著提升了代码时间复杂度预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在没有微调的情况下难以处理某些时间复杂度类别，且没有单一模型在所有类别上都表现优异，因此需要一种能够整合不同模型优势的方法。

Method: 提出MEC$^3$O系统，基于性能将LLMs分配到特定时间复杂度类别，提供类别专用指令使其成为专家，然后通过结构化辩论和加权共识机制整合预测结果。

Result: 在CodeComplex数据集上，MEC$^3$O比开源基线模型准确率和macro-F1分数至少高出10%，在macro-F1上平均超越GPT-4o-mini，与GPT-4o和GPT-o4-mini达到竞争性水平。

Conclusion: 多专家辩论和加权共识策略能有效生成最终预测，证明了该方法在代码时间复杂度预测任务中的有效性。

Abstract: Predicting the complexity of source code is essential for software
development and algorithm analysis. Recently, Baik et al. (2025) introduced
CodeComplex for code time complexity prediction. The paper shows that LLMs
without fine-tuning struggle with certain complexity classes. This suggests
that no single LLM excels at every class, but rather each model shows
advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus
system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs
to complexity classes based on their performance and provides them with
class-specialized instructions, turning them into experts. These experts engage
in structured debates, and their predictions are integrated through a weighted
consensus mechanism. Our expertise assignments to LLMs effectively handle
Degeneration-of-Thought, reducing reliance on a separate judge model, and
preventing convergence to incorrect majority opinions. Experiments on
CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving
at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini
in macro-F1 scores on average and demonstrates competitive on-par F1 scores to
GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of
multi-expert debates and weight consensus strategy to generate the final
predictions. Our code and data is available at
https://github.com/suhanmen/MECO.

</details>


### [168] [Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents](https://arxiv.org/abs/2510.08619)
*Tennison Liu,Silas Ruhrberg Estévez,David L. Bentley,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: AScience框架通过LLM研究代理的自组织网络实现大规模科学数据集的假设探索，能够在多样性-质量-新颖性前沿积累专家评级结果


<details>
  <summary>Details</summary>
Motivation: 大规模科学数据集为无特定研究问题的探索性发现创造了机会，需要支持假设狩猎——在复杂假设空间中持续探索积累洞察的过程

Method: 提出AScience框架，将发现建模为代理、网络和评估规范的交互，实现为ASCollab分布式系统，包含异构行为的LLM研究代理，这些代理在共享评估标准下自组织成演化网络，持续产生和同行评审发现

Result: 实验显示这种社会动力学能够在多样性-质量-新颖性前沿积累专家评级结果，包括重新发现已建立的生物标志物、扩展已知通路和提出新的治疗靶点

Conclusion: 虽然湿实验室验证仍然不可或缺，但在癌症队列上的实验表明，社会结构化的代理网络能够大规模持续进行探索性假设狩猎

Abstract: Large-scale scientific datasets -- spanning health biobanks, cell atlases,
Earth reanalyses, and more -- create opportunities for exploratory discovery
unconstrained by specific research questions. We term this process hypothesis
hunting: the cumulative search for insight through sustained exploration across
vast and complex hypothesis spaces. To support it, we introduce AScience, a
framework modeling discovery as the interaction of agents, networks, and
evaluation norms, and implement it as ASCollab, a distributed system of
LLM-based research agents with heterogeneous behaviors. These agents
self-organize into evolving networks, continually producing and peer-reviewing
findings under shared standards of evaluation. Experiments show that such
social dynamics enable the accumulation of expert-rated results along the
diversity-quality-novelty frontier, including rediscoveries of established
biomarkers, extensions of known pathways, and proposals of new therapeutic
targets. While wet-lab validation remains indispensable, our experiments on
cancer cohorts demonstrate that socially structured, agentic networks can
sustain exploratory hypothesis hunting at scale.

</details>


### [169] [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)
*Siqi Zhu,David Zhang,Pedro Cisneros-Velarde,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出了GTAlign框架，将博弈论决策融入LLM的对齐过程，通过在推理中构建收益矩阵和训练中引入互惠奖励，实现模型与用户的互利决策。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法假设最大化模型奖励就能最大化用户福利，但实践中模型可能过度澄清或生成冗长推理，导致用户偏好与模型行为不匹配，形成类似囚徒困境的次优结果。

Method: 在推理阶段将用户-LLM交互视为策略博弈，构建收益矩阵评估双方福利；在训练阶段引入互惠奖励强化合作响应；还包含基于博弈论推理的动态适应技术。

Result: 实验表明GTAlign在多样化任务中显著提升了推理效率、答案质量和互惠福利。

Conclusion: GTAlign通过博弈论框架有效解决了LLM对齐中的互利决策问题，实现了更高效的社会最优结果。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning,
yet sometimes produce responses that are suboptimal for users in tasks such as
writing, information seeking, or providing practical guidance. Conventional
alignment practices typically assume that maximizing model reward also
maximizes user welfare, but this assumption frequently fails in practice:
models may over-clarify or generate overly verbose reasoning when users prefer
concise answers. Such behaviors resemble the prisoner's dilemma, where
individually rational choices lead to socially suboptimal outcomes. The
fundamental challenge is the lack of a principled decision making mechanism
that mutually benefits both the LLM and the user. We propose Game-Theoretic
Alignment (GTAlign), an alignment framework that integrates game-theoretic
decision making into both reasoning and training. During reasoning, the model
explicitly treats user-LLM interaction as a strategic game: it constructs
payoff matrices within its reasoning chain to estimate welfare for both itself
and the user, and then selects actions that are mutually beneficial. During
training, we introduce a mutual welfare reward that reinforces cooperative
responses, aligning model behavior with socially efficient outcomes. In
addition, we introduce an inference technique that leverages game-theoretic
reasoning to dynamically adapt LLM's response when pricing policies of LLM
service change. Extensive experiments demonstrate that GTAlign substantially
improves reasoning efficiency, answer quality, and mutual welfare compared to
baselines across diverse tasks. The code is available at
https://github.com/ulab-uiuc/GTAlign .

</details>


### [170] [RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation](https://arxiv.org/abs/2510.08931)
*Ashish Kattamuri,Harshwardhan Fartale,Arpita Vats,Rahul Raja,Ishita Prasad*

Main category: cs.AI

TL;DR: RADAR是一个通过机制可解释性检测LLM数据污染的新框架，通过区分基于记忆和基于推理的模型响应来识别数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 数据污染对可靠的LLM评估构成重大挑战，模型可能通过记忆训练数据而非真正推理能力来获得高性能表现。

Method: RADAR提取37个特征，涵盖表面置信度轨迹和深层机制特性（包括注意力专业化、电路动态和激活流模式），并使用集成分类器进行检测。

Result: 在多样化评估集上达到93%的准确率，在清晰案例中表现完美，在具有挑战性的模糊示例中达到76.7%的准确率。

Conclusion: 这项工作展示了机制可解释性在超越传统表面指标推进LLM评估方面的潜力。

Abstract: Data contamination poses a significant challenge to reliable LLM evaluation,
where models may achieve high performance by memorizing training data rather
than demonstrating genuine reasoning capabilities. We introduce RADAR (Recall
vs. Reasoning Detection through Activation Representation), a novel framework
that leverages mechanistic interpretability to detect contamination by
distinguishing recall-based from reasoning-based model responses. RADAR
extracts 37 features spanning surface-level confidence trajectories and deep
mechanistic properties including attention specialization, circuit dynamics,
and activation flow patterns. Using an ensemble of classifiers trained on these
features, RADAR achieves 93\% accuracy on a diverse evaluation set, with
perfect performance on clear cases and 76.7\% accuracy on challenging ambiguous
examples. This work demonstrates the potential of mechanistic interpretability
for advancing LLM evaluation beyond traditional surface-level metrics.

</details>


### [171] [RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows](https://arxiv.org/abs/2510.09021)
*Hamed Mahdavi,Pouria Mahdavinia,Samira Malek,Pegah Mohammadipour,Alireza Hashemi,Majid Daliri,Alireza Farhadi,Amir Khasahmadi,Niloofar Mireshghallah,Vasant Honavar*

Main category: cs.AI

TL;DR: 该论文评估了SOTA LLMs在证明评分方面的能力，包括错误检测、严重性判断和分数分配，并提出了一种基于代理工作流程的多步骤评分方法来解决部分学分分配不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 随着SOTA LLMs在解决奥林匹克数学问题方面取得显著进展（能解决IMO 2025的大部分问题），需要评估这些模型在证明评分方面的能力，特别是在检测错误、判断严重性和分配公平分数方面的表现。

Method: 使用Gemini 2.5 Pro生成的90个解决方案和MathArena的IMO/USAMO 2025解决方案集，引入代理工作流程来提取和分析参考解决方案，自动推导问题特定的评分标准，并进行多步骤评分过程。

Result: 模型能够可靠地标记错误解决方案（包括细微错误），但在部分学分分配方面存在校准差距。提出的工作流程在人类评分一致性和部分学分处理一致性方面表现更好。

Conclusion: 论文提出的代理工作流程能够提高与人类评分的一致性，并在部分学分处理方面更加一致，为未来研究提供了代码、数据和提示/日志。

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [172] [Auto-scaling Continuous Memory for GUI Agent](https://arxiv.org/abs/2510.09038)
*Wenyi Wu,Kun Zhou,Ruoxin Yuan,Vivian Yu,Stephen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出连续记忆机制，将GUI轨迹编码为固定长度的连续嵌入，显著减少上下文成本并保留细粒度视觉信息，通过自动扩展数据飞轮收集10万+轨迹，在真实GUI基准测试中显著提升长任务和分布偏移下的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理将过去轨迹压缩为文本标记，导致上下文长度膨胀并丢失关键视觉线索（如精确的控件大小和位置），需要可扩展的记忆机制来泛化到不熟悉的界面和长时任务。

Method: 使用VLM作为编码器将GUI轨迹编码为固定长度的连续嵌入序列，直接插入主干网络的输入层；引入自动扩展数据飞轮，通过搜索发现新环境、合成任务、执行轨迹和验证成功来低成本扩展记忆。

Result: 随着记忆大小和检索深度的增加，性能单调提升，而文本记忆在长提示下会退化；在真实GUI基准测试中，记忆增强代理在长时任务和分布偏移下持续提升成功率；Qwen-2.5-VL-7B+连续记忆达到与最先进闭源模型相当的性能。

Conclusion: 连续记忆机制有效解决了GUI代理的可扩展记忆问题，在减少上下文成本的同时保留视觉细节，通过低成本数据收集和微调实现与顶级闭源模型竞争的性能。

Abstract: We study how to endow GUI agents with scalable memory that help generalize
across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress
past trajectories into text tokens, which balloons context length and misses
decisive visual cues (e.g., exact widget size and position). We propose a
continuous memory that encodes each GUI trajectory into a fixed-length sequence
of continuous embeddings using the VLM itself as an encoder; these embeddings
are plugged directly into the backbone's input layer, sharply reducing context
cost while preserving fine-grained visual information. As memory size and
retrieval depth increase, performance improves monotonically, unlike text
memories that degrade with long prompts. To grow memory at low cost, we
introduce an auto-scaling data flywheel that (i) discovers new environments via
search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out
trajectories with the agent, and (iv) verifies success with the same VLM. Using
this pipeline, we collect 100k+ trajectories for about \$4000 and fine-tune
only the memory encoder (LoRA on a Q-Former, 1.2\% parameters) with 1,500
samples. On real-world GUI benchmarks, our memory-augmented agent consistently
improves success rates under long horizons and distribution shifts. Notably,
Qwen-2.5-VL-7B + continuous memory achieves performance comparable to
state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).

</details>


### [173] [PAC Reasoning: Controlling the Performance Loss for Efficient Reasoning](https://arxiv.org/abs/2510.09133)
*Hao Zeng,Jianguo Huang,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出PAC推理方法，通过置信上界控制性能损失，在用户指定的性能损失容忍度下动态切换思考和非思考模式以节省计算成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型计算成本高，现有动态切换方法缺乏统计保证且会引入额外推理错误，这在关键应用中不可接受

Method: 构建性能损失的单调置信上界函数，基于不确定性分数确定切换阈值，在分布无关的方式下保证有界性能损失

Result: 在推理基准测试中，该方法能节省计算预算并控制用户指定的性能损失

Conclusion: PAC推理方法为高效推理提供了统计保证，能够在保证性能的前提下显著降低计算成本

Abstract: Large reasoning models (LRMs) have achieved remarkable progress in complex
problem-solving tasks. Despite this success, LRMs typically suffer from high
computational costs during deployment, highlighting a need for efficient
inference. A popular direction of efficiency improvement is to switch the LRM
between thinking and nonthinking modes dynamically. However, such approaches
often introduce additional reasoning errors and lack statistical guarantees for
the performance loss, which are critical for high-stakes applications. In this
work, we propose Probably Approximately Correct (PAC) reasoning that controls
the performance loss under the user-specified performance loss tolerance. In
particular, we construct an upper confidence bound on the performance loss,
formulated as a monotone function of the uncertainty score, and subsequently
determine a threshold for switching to the nonthinking model. Theoretically,
using the threshold to switch between the thinking and nonthinking modes
ensures bounded performance loss in a distribution-free manner. Our
comprehensive experiments on reasoning benchmarks show that the proposed method
can save computational budgets and control the user-specified performance loss.

</details>


### [174] [Localist LLMs -- A Mathematical Framework for Dynamic Locality Control](https://arxiv.org/abs/2510.09338)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 提出了一种可调节内部表示的LLM训练框架，通过局部性调节参数在可解释的局部化表示和高效的分布式表示之间连续切换。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在可解释性和性能之间的权衡问题，特别是在需要透明度和能力的监管领域。

Method: 使用组稀疏惩罚、信息论锚点设计和动态规则注入，通过可调节的局部性参数控制表示的局部化程度。

Result: 提供了严格的数学证明，建立了注意力集中在语义相关块上的阈值条件，实现了低熵和高保真度。

Conclusion: 该框架使从业者能够在可解释模式和高性能模式之间连续插值，支持需要透明度和能力的应用场景。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovation is a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining. This is achieved
through group sparsity penalties on attention mechanisms, information-theoretic
anchor design, and dynamic rule injection. We provide rigorous mathematical
proofs establishing explicit threshold conditions under which attention
provably concentrates on semantically relevant blocks, with exponential bounds
on attention entropy and pointer fidelity. Specifically, we prove that when
group sparsity penalties exceed certain threshold values, the model's attention
mechanisms concentrate on semantically relevant blocks, achieving low entropy
and high fidelity with negligible error. This framework enables practitioners
to continuously interpolate between interpretable and high-performance modes,
supporting applications in regulated domains requiring both transparency and
capability.

</details>


### [175] [LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?](https://arxiv.org/abs/2510.09595)
*Kaijian Zou,Aaron Xiong,Yunxiang Zhang,Frederick Zhang,Yueqi Ren,Jirong Yang,Ayoung Lee,Shitanshu Bhushan,Lu Wang*

Main category: cs.AI

TL;DR: LiveOIBench是一个包含403个奥林匹克级别编程竞赛问题的基准测试集，具有高质量测试用例和离线评估系统，用于评估大语言模型的编程能力。


<details>
  <summary>Details</summary>
Motivation: 当前编程基准测试存在缺乏挑战性问题、测试用例覆盖不足、依赖在线平台API等问题，需要更全面可靠的评估标准。

Method: 从72个官方信息学奥林匹克竞赛中收集403个专家策划的问题，每个问题平均60个测试用例，建立包含人类选手表现数据的自包含评估系统。

Result: GPT-5达到81.76百分位，但仍低于顶尖人类选手（通常>90百分位）；开源模型GPT-OSS-120B仅达60百分位，与前沿闭源模型存在显著差距。

Conclusion: 强大的推理模型应优先精确问题分析而非过度探索，未来模型应强调结构化分析并减少不必要的探索。

Abstract: Competitive programming problems increasingly serve as valuable benchmarks to
evaluate the coding capabilities of large language models (LLMs) due to their
complexity and ease of verification. Yet, current coding benchmarks face
limitations such as lack of exceptionally challenging problems, insufficient
test case coverage, reliance on online platform APIs that limit accessibility.
To address these issues, we introduce LiveOIBench, a comprehensive benchmark
featuring 403 expert-curated Olympiad-level competitive programming problems,
each with an average of 60 expert-designed test cases. The problems are sourced
directly from 72 official Informatics Olympiads in different regions conducted
between 2023 and 2025. LiveOIBench distinguishes itself through four key
features: (1) meticulously curated high-quality tasks with detailed subtask
rubrics and extensive private test cases; (2) direct integration of elite
contestant performance data to enable informative comparison against
top-performing humans; (3) planned continuous, contamination-free updates from
newly released Olympiad problems; and (4) a self-contained evaluation system
facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular
general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable
81.76th percentile, a strong result that nonetheless falls short of top human
contestant performance, who usually place above 90th. In contrast, among
open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile,
underscoring significant capability disparities from frontier closed models.
Detailed analyses indicate that robust reasoning models prioritize precise
problem analysis over excessive exploration, suggesting future models should
emphasize structured analysis and minimize unnecessary exploration. All data,
code, and leaderboard results will be made publicly available on our website.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [176] [Man-Made Heuristics Are Dead. Long Live Code Generators!](https://arxiv.org/abs/2510.08803)
*Rohit Dwivedula,Divyanshu Saxena,Aditya Akella,Swarat Chaudhuri,Daehyeok Kim*

Main category: cs.OS

TL;DR: PolicySmith是一个利用LLM自动生成系统策略的框架，在Web缓存和拥塞控制两个领域发现了优于现有基准的启发式策略


<details>
  <summary>Details</summary>
Motivation: 传统系统控制器策略设计依赖人工手动制定，需要领域专家针对具体实例精心设计启发式方法，过程繁琐且效率低下

Method: 利用生成式模型特别是大语言模型的代码生成能力，开发PolicySmith框架来自动搜索和合成实例最优的启发式策略

Result: 在Web缓存方面，PolicySmith发现的启发式策略在标准开源跟踪数据上优于现有基准；在拥塞控制方面，能够生成可直接集成到Linux内核的安全策略

Conclusion: LLM驱动的启发式搜索为系统策略设计开辟了新途径，能够自动发现高性能的策略，减少对人工设计的依赖

Abstract: Policy design for various systems controllers has conventionally been a
manual process, with domain experts carefully tailoring heuristics for the
specific instance in which the policy will be deployed. In this paper, we
re-imagine policy design via a novel automated search technique fueled by
recent advances in generative models, specifically Large Language Model
(LLM)-driven code generation. We outline the design and implementation of
PolicySmith, a framework that applies LLMs to synthesize instance-optimal
heuristics. We apply PolicySmith to two long-standing systems policies - web
caching and congestion control, highlighting the opportunities unraveled by
this LLM-driven heuristic search. For caching, PolicySmith discovers heuristics
that outperform established baselines on standard open-source traces. For
congestion control, we show that PolicySmith can generate safe policies that
integrate directly into the Linux kernel.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [177] [Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising](https://arxiv.org/abs/2510.09084)
*Dildar Ali,Suman Benerjee,Yamuna Prasad*

Main category: cs.GT

TL;DR: 本文研究数字广告牌分配问题，旨在最小化影响提供商的遗憾。提出了两种解决方案：预算有效贪婪方法和随机化版本，通过实验验证了随机化方法在计算时间和遗憾最小化方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在数字广告牌投放中，影响提供商需要将广告位分配给多个广告商。如果提供的影响超过或低于广告商需求，都会造成损失（称为'遗憾'）。影响提供商的目标是分配广告位以最小化总遗憾。

Method: 提出了两种离散优化方法：1）预算有效贪婪方法，以增量方式选择广告位；2）随机化预算有效贪婪方法，在样本上计算边际收益而非所有广告位。分析了两种算法的时间和空间复杂度。

Result: 在真实数据集上的实验表明，随机化预算有效贪婪方法在合理计算时间内有效最小化遗憾。

Conclusion: 随机化预算有效贪婪方法在数字广告牌分配问题中表现出良好的性能平衡，既能控制计算成本，又能有效最小化影响提供商的遗憾。

Abstract: In a typical \emph{billboard advertisement} technique, a number of digital
billboards are owned by an \emph{influence provider}, and several commercial
houses approach the influence provider for a specific number of views of their
advertisement content on a payment basis. If the influence provider provides
the demanded or more influence, then he will receive the full payment else a
partial payment. In the context of an influence provider, if he provides more
or less than the advertisers demanded influence, it is a loss for him. This is
formalized as 'Regret', and naturally, in the context of the influence
provider, the goal will be to allocate the billboard slots among the
advertisers such that the total regret is minimized. In this paper, we study
this problem as a discrete optimization problem and propose two solution
approaches. The first one selects the billboard slots from the available ones
in an incremental greedy manner, and we call this method the Budget Effective
Greedy approach. In the second one, we introduce randomness in the first one,
where we do it for a sample of slots instead of calculating the marginal gains
of all the billboard slots. We analyze both algorithms to understand their time
and space complexity. We implement them with real-life datasets and conduct a
number of experiments. We observe that the randomized budget effective greedy
approach takes reasonable computational time while minimizing the regret.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [178] [MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation](https://arxiv.org/abs/2510.09065)
*Akira Takahashi,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: MMAudioSep是基于预训练视频到音频模型的生成式声音分离模型，利用预训练知识实现高效训练，在声音分离任务上优于现有方法，同时保留原始视频到音频生成能力。


<details>
  <summary>Details</summary>
Motivation: 利用预训练的音频生成模型学习到的视频/文本与音频关系知识，避免从零开始训练，提高训练效率。

Method: 基于预训练的视频到音频模型构建生成式声音分离模型，通过微调实现声音分离功能。

Result: 在声音分离任务上优于现有的确定性和生成式基线模型，同时模型保留原始视频到音频生成能力。

Conclusion: 基础声音生成模型有潜力被应用于声音相关的下游任务，展示了预训练模型的可迁移性。

Abstract: We introduce MMAudioSep, a generative model for video/text-queried sound
separation that is founded on a pretrained video-to-audio model. By leveraging
knowledge about the relationship between video/text and audio learned through a
pretrained audio generative model, we can train the model more efficiently,
i.e., the model does not need to be trained from scratch. We evaluate the
performance of MMAudioSep by comparing it to existing separation models,
including models based on both deterministic and generative approaches, and
find it is superior to the baseline models. Furthermore, we demonstrate that
even after acquiring functionality for sound separation via fine-tuning, the
model retains the ability for original video-to-audio generation. This
highlights the potential of foundational sound generation models to be adopted
for sound-related downstream tasks. Our code is available at
https://github.com/sony/mmaudiosep.

</details>


### [179] [Emotion-Disentangled Embedding Alignment for Noise-Robust and Cross-Corpus Speech Emotion Recognition](https://arxiv.org/abs/2510.09072)
*Upasana Tiwari,Rupayan Chakraborty,Sunil Kumar Kopparapu*

Main category: cs.SD

TL;DR: 提出了一种两阶段方法来增强语音情感识别的鲁棒性和泛化能力，通过情感解耦表示学习和多块嵌入对齐来改进表示学习。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的语音情感识别效果常受噪声环境和数据集间差异的阻碍，需要提升模型在噪声和跨语料场景下的性能。

Method: 使用EDRL提取类别特异性判别特征并保留情感类别间的共享相似性，然后通过MEA将表示投影到联合判别潜在子空间以最大化与原始语音输入的协方差。

Result: 在未见过的噪声和跨语料语音样本上评估，显示出改进的性能，证明了所提方法的有效性。

Conclusion: 该方法通过改进的表示学习有效提升了语音情感识别在挑战性条件下的鲁棒性和泛化能力。

Abstract: Effectiveness of speech emotion recognition in real-world scenarios is often
hindered by noisy environments and variability across datasets. This paper
introduces a two-step approach to enhance the robustness and generalization of
speech emotion recognition models through improved representation learning.
First, our model employs EDRL (Emotion-Disentangled Representation Learning) to
extract class-specific discriminative features while preserving shared
similarities across emotion categories. Next, MEA (Multiblock Embedding
Alignment) refines these representations by projecting them into a joint
discriminative latent subspace that maximizes covariance with the original
speech input. The learned EDRL-MEA embeddings are subsequently used to train an
emotion classifier using clean samples from publicly available datasets, and
are evaluated on unseen noisy and cross-corpus speech samples. Improved
performance under these challenging conditions demonstrates the effectiveness
of the proposed method.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [180] [Augmented data and neural networks for robust epidemic forecasting: application to COVID-19 in Italy](https://arxiv.org/abs/2510.09192)
*Giacomo Dimarco,Federica Ferrarese,Lorenzo Pareschi*

Main category: math.NA

TL;DR: 提出一种结合流行病学模型和深度学习的数据增强方法，通过生成合成数据来提升神经网络预测性能，在COVID-19疫情预测中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 提高神经网络训练效果和预测准确率，特别是在数据有限的情况下，通过数据增强来改善模型性能。

Method: 使用合适的流行病学模型生成合成数据并融入不确定性，结合深度学习技术进行数据增强，重点比较了PINNs和NAR两种神经网络架构。

Result: 使用增强数据集训练的神经网络预测性能显著提升，NAR模型在短期预测中表现优异，PINNs能捕捉长期定性行为。

Conclusion: 该数据增强策略有效提升了神经网络预测能力，NAR适合短期定量预测，PINNs适合长期定性趋势分析，在COVID-19疫情预测中得到验证。

Abstract: In this work, we propose a data augmentation strategy aimed at improving the
training phase of neural networks and, consequently, the accuracy of their
predictions. Our approach relies on generating synthetic data through a
suitable compartmental model combined with the incorporation of uncertainty.
The available data are then used to calibrate the model, which is further
integrated with deep learning techniques to produce additional synthetic data
for training. The results show that neural networks trained on these augmented
datasets exhibit significantly improved predictive performance. We focus in
particular on two different neural network architectures: Physics-Informed
Neural Networks (PINNs) and Nonlinear Autoregressive (NAR) models. The NAR
approach proves especially effective for short-term forecasting, providing
accurate quantitative estimates by directly learning the dynamics from data and
avoiding the additional computational cost of embedding physical constraints
into the training. In contrast, PINNs yield less accurate quantitative
predictions but capture the qualitative long-term behavior of the system,
making them more suitable for exploring broader dynamical trends. Numerical
simulations of the second phase of the COVID-19 pandemic in the Lombardy region
(Italy) validate the effectiveness of the proposed approach.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [181] [Agnostic Product Mixed State Tomography via Robust Statistics](https://arxiv.org/abs/2510.08472)
*Alvan Arulandu,Ilias Diakonikolas,Daniel Kane,Jerry Li*

Main category: quant-ph

TL;DR: 提出了首个针对混合态产品态的高效不可知层析算法，使用单量子比特测量，在多项式时间和样本复杂度下找到接近最优的产品态近似。


<details>
  <summary>Details</summary>
Motivation: 先前关于不可知层析的工作只能处理纯态假设类（如产品态或稳定子态），而混合态假设类的研究仍然空白。本文旨在解决混合态产品态的不可知层析问题。

Method: 通过两个主要步骤：1）将混合态产品态的不可知层析问题归约到经典鲁棒学习二元产品分布问题；2）提出新的最优二元产品分布接近性证书，通过精心设计的凸松弛算法解决该经典问题。

Result: 算法找到与目标态ρ距离为O(ε log 1/ε)的产品态，使用多项式样本且在多项式时间内运行，仅需单量子比特单副本测量。

Conclusion: 这是首个对任何混合态假设类实现非平凡不可知层析保证的高效算法，并证明了在仅使用单量子比特双结果投影测量时，适应性在信息论上是必要的。

Abstract: We consider the problem of agnostic tomography with \emph{mixed state}
ansatz, and specifically, the natural ansatz class of product mixed states. In
more detail, given $N$ copies of an $n$-qubit state $\rho$ which is
$\epsilon$-close to a product mixed state $\pi$, the goal is to output a
nearly-optimal product mixed state approximation to $\rho$. While there has
been a flurry of recent work on agnostic tomography, prior work could only
handle pure state ansatz, such as product states or stabilizer states. Here we
give an algorithm for agnostic tomography of product mixed states which finds a
product state which is $O(\epsilon \log 1 / \epsilon)$ close to $\rho$ which
uses polynomially many copies of $\rho$, and which runs in polynomial time.
Moreover, our algorithm only uses single-qubit, single-copy measurements. To
our knowledge, this is the first efficient algorithm that achieves any
non-trivial agnostic tomography guarantee for any class of mixed state ansatz.
  Our algorithm proceeds in two main conceptual steps, which we believe are of
independent interest. First, we demonstrate a novel, black-box efficient
reduction from agnostic tomography of product mixed states to the classical
task of \emph{robustly learning binary product distributions} -- a textbook
problem in robust statistics. We then demonstrate a nearly-optimal efficient
algorithm for the classical task of robustly learning a binary product,
answering an open problem in the literature. Our approach hinges on developing
a new optimal certificate of closeness for binary product distributions that
can be leveraged algorithmically via a carefully defined convex relaxation.
Finally, we complement our upper bounds with a lower bound demonstrating that
adaptivity is information-theoretically necessary for our agnostic tomography
task, so long as the algorithm only uses single-qubit two-outcome projective
measurements.

</details>


### [182] [QuIRK: Quantum-Inspired Re-uploading KAN](https://arxiv.org/abs/2510.08650)
*Vinayak Sharma,Ashish Padhy,Vijay Jagdish Karanjkar,Sourav Behera,Lord Sen,Shyamapada Mukherjee,Aviral Shrivastava*

Main category: quant-ph

TL;DR: 提出量子启发的KAN变体QuIRK，用单量子比特数据重上传模型替代B样条函数，在保持可解释性的同时减少参数数量并提升性能。


<details>
  <summary>Details</summary>
Motivation: KANs在科学领域的回归问题上表现出色且具有可解释性，但希望进一步减少参数数量并提升性能，特别是针对周期性函数。

Method: 将KAN中的B样条函数替换为量子数据重上传模型，构建单量子比特电路作为单变量函数逼近器。

Result: QuIRK模型在保持可解释性的同时，使用更少参数就能匹配或超越传统KANs性能，特别擅长处理周期性函数，且可通过GPU进行经典模拟。

Conclusion: QuIRK成功结合了量子启发的设计优势，在减少参数的同时保持甚至提升了KANs的性能和可解释性，为科学计算提供了新的高效工具。

Abstract: Kolmogorov-Arnold Networks or KANs have shown the ability to outperform
classical Deep Neural Networks, while using far fewer trainable parameters for
regression problems on scientific domains. Even more powerful has been their
interpretability due to their structure being composed of univariate B-Spline
functions. This enables us to derive closed-form equations from trained KANs
for a wide range of problems. This paper introduces a quantum-inspired variant
of the KAN based on Quantum Data Re-uploading~(DR) models. The Quantum-Inspired
Re-uploading KAN or QuIRK model replaces B-Splines with single-qubit DR models
as the univariate function approximator, allowing them to match or outperform
traditional KANs while using even fewer parameters. This is especially apparent
in the case of periodic functions. Additionally, since the model utilizes only
single-qubit circuits, it remains classically tractable to simulate with
straightforward GPU acceleration. Finally, we also demonstrate that QuIRK
retains the interpretability advantages and the ability to produce closed-form
solutions.

</details>


### [183] [Three Birds with One Stone: Improving Performance, Convergence, and System Throughput with Nest](https://arxiv.org/abs/2510.09578)
*Yuqian Huo,David Quiroga,Anastasios Kyrillidis,Tirthak Patel*

Main category: quant-ph

TL;DR: Nest技术通过在执行过程中动态调整VQA的量子比特保真度映射，同时优化性能、收敛速度和系统吞吐量


<details>
  <summary>Details</summary>
Motivation: 传统VQA算法通常在高保真度量子比特上运行以获得最佳性能，但这导致系统吞吐量低。需要一种方法在保持良好性能的同时提高系统效率

Method: 提出Nest技术，在VQA执行过程中精心变化量子比特保真度映射，初始使用低保真度量子比特，后期使用高保真度量子比特

Result: Nest技术不仅提升性能（接近最优结果），还加快收敛速度，并能同时在同一计算机上运行多个VQA，提高系统吞吐量

Conclusion: Nest技术成功平衡和优化了性能、收敛速度和系统吞吐量这三个相互冲突的指标

Abstract: Variational quantum algorithms (VQAs) have the potential to demonstrate
quantum utility on near-term quantum computers. However, these algorithms often
get executed on the highest-fidelity qubits and computers to achieve the best
performance, causing low system throughput. Recent efforts have shown that VQAs
can be run on low-fidelity qubits initially and high-fidelity qubits later on
to still achieve good performance. We take this effort forward and show that
carefully varying the qubit fidelity map of the VQA over its execution using
our technique, Nest, does not just (1) improve performance (i.e., help achieve
close to optimal results), but also (2) lead to faster convergence. We also use
Nest to co-locate multiple VQAs concurrently on the same computer, thus (3)
increasing the system throughput, and therefore, balancing and optimizing three
conflicting metrics simultaneously.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [184] [MAKO: Meta-Adaptive Koopman Operators for Learning-based Model Predictive Control of Parametrically Uncertain Nonlinear Systems](https://arxiv.org/abs/2510.09042)
*Minghao Han,Kiwan Wong,Adrian Wing-Keung Law,Xunyuan Yin*

Main category: eess.SY

TL;DR: 提出基于元学习的Koopman建模和预测控制方法，用于处理具有参数不确定性的非线性系统，通过元学习从多模态数据中学习元模型，并能快速适应新参数设置。


<details>
  <summary>Details</summary>
Motivation: 针对非线性系统中参数不确定性的挑战，传统方法难以处理未知参数变化，需要开发能够快速适应新参数设置的建模和控制方法。

Method: 提出MAKO（Meta Adaptive Koopman Operator）方法，使用深度元学习从多模态数据集学习元模型，利用在线数据快速适应新的参数设置，并基于学习的Koopman模型开发预测控制方案。

Result: 通过大量仿真验证，该方法在建模精度和控制效果方面均优于竞争基线方法，即使在面对未见过的参数设置时也能保持闭环系统稳定性。

Conclusion: MAKO方法能够有效处理非线性系统的参数不确定性，通过元学习实现快速适应，在建模和控制方面表现出优越性能。

Abstract: In this work, we propose a meta-learning-based Koopman modeling and
predictive control approach for nonlinear systems with parametric
uncertainties. An adaptive deep meta-learning-based modeling approach, called
Meta Adaptive Koopman Operator (MAKO), is proposed. Without knowledge of the
parametric uncertainty, the proposed MAKO approach can learn a meta-model from
a multi-modal dataset and efficiently adapt to new systems with previously
unseen parameter settings by using online data. Based on the learned meta
Koopman model, a predictive control scheme is developed, and the stability of
the closed-loop system is ensured even in the presence of previously unseen
parameter settings. Through extensive simulations, our proposed approach
demonstrates superior performance in both modeling accuracy and control
efficacy as compared to competitive baselines.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [185] [Unsupervised full-field Bayesian inference of orthotropic hyperelasticity from a single biaxial test: a myocardial case study](https://arxiv.org/abs/2510.09498)
*Rogier P. Krijnen,Akshay Joshi,Siddhant Kumar,Mathias Peirlinck*

Main category: q-bio.TO

TL;DR: 提出了一种基于贝叶斯推理的无监督方法EUCLID，用于从单个异质双轴拉伸试验中识别高度非线性正交各向异性材料模型参数，并量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统均质组织测试需要多种变形模式和多个样本，存在样本间变异性和操作损伤问题，影响参数识别准确性。

Method: 采用EUCLID无监督方法，结合贝叶斯推理和三维连续体单元，从单个异质双轴拉伸试验中识别材料参数。

Result: 该方法在不同噪声水平下能准确推断合成心肌组织板的材料参数，与真实模拟结果吻合良好，并提供了可信区间。

Conclusion: 该方法展示了从单个双轴拉伸试验表征高度非线性和正交各向异性材料模型的潜力，并具备不确定性量化能力。

Abstract: Fully capturing this behavior in traditional homogenized tissue testing
requires the excitation of multiple deformation modes, i.e. combined triaxial
shear tests and biaxial stretch tests. Inherently, such multimodal experimental
protocols necessitate multiple tissue samples and extensive sample
manipulations. Intrinsic inter-sample variability and manipulation-induced
tissue damage might have an adverse effect on the inversely identified tissue
behavior. In this work, we aim to overcome this gap by focusing our attention
to the use of heterogeneous deformation profiles in a parameter estimation
problem. More specifically, we adapt EUCLID, an unsupervised method for the
automated discovery of constitutive models, towards the purpose of parameter
identification for highly nonlinear, orthotropic constitutive models using a
Bayesian inference approach and three-dimensional continuum elements. We
showcase its strength to quantitatively infer, with varying noise levels, the
material model parameters of synthetic myocardial tissue slabs from a single
heterogeneous biaxial stretch test. This method shows good agreement with the
ground-truth simulations and with corresponding credibility intervals. Our work
highlights the potential for characterizing highly nonlinear and orthotropic
material models from a single biaxial stretch test with uncertainty
quantification.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [186] [A Multimodal Approach to SME Credit Scoring Integrating Transaction and Ownership Networks](https://arxiv.org/abs/2510.09407)
*Sahab Zandi,Kamesh Korangi,Juan C. Moreno-Paredes,María Óskarsdóttir,Christophe Mues,Cristián Bravo*

Main category: q-fin.GN

TL;DR: 该论文提出了一种使用图神经网络(GNN)预测中小企业信用违约的新方法，通过企业间的共同所有权和金融交易网络数据来改进传统信用评分模型。


<details>
  <summary>Details</summary>
Motivation: 中小企业在经济增长中发挥重要作用，但面临信贷获取困难。由于缺乏财务历史、抵押品限制和宏观经济冲击，准确评估其信用风险对贷款机构至关重要，特别是考虑到中小企业常在相互关联的企业网络中运营，违约风险可能传播。

Method: 使用图神经网络(GNN)预测中小企业违约，基于企业间的共同所有权和金融交易构建多层网络数据，并将这些网络信息与传统结构化数据相结合。

Result: 研究表明，将网络数据与传统数据结合不仅能提高申请评分性能，还能明确模拟企业间的传染风险。进一步分析揭示了这些连接的方向性和强度如何影响金融风险传染。

Conclusion: 网络数据具有显著的预测能力，供应链网络在暴露中小企业于相关违约风险中发挥着重要作用，为理解风险传染机制提供了更深入的见解。

Abstract: Small and Medium-sized Enterprises (SMEs) are known to play a vital role in
economic growth, employment, and innovation. However, they tend to face
significant challenges in accessing credit due to limited financial histories,
collateral constraints, and exposure to macroeconomic shocks. These challenges
make an accurate credit risk assessment by lenders crucial, particularly since
SMEs frequently operate within interconnected firm networks through which
default risk can propagate. This paper presents and tests a novel approach for
modelling the risk of SME credit, using a unique large data set of SME loans
provided by a prominent financial institution. Specifically, our approach
employs Graph Neural Networks to predict SME default using multilayer network
data derived from common ownership and financial transactions between firms. We
show that combining this information with traditional structured data not only
improves application scoring performance, but also explicitly models contagion
risk between companies. Further analysis shows how the directionality and
intensity of these connections influence financial risk contagion, offering a
deeper understanding of the underlying processes. Our findings highlight the
predictive power of network data, as well as the role of supply chain networks
in exposing SMEs to correlated default risk.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [187] [Hierarchical Scheduling for Multi-Vector Image Retrieval](https://arxiv.org/abs/2510.08976)
*Maoliang Li,Ke Li,Yaoyang Liu,Jiayu Chen,Zihao Zheng,Yinjun Wu,Xiang Chen*

Main category: cs.CV

TL;DR: 提出了HiMIR框架，通过分层多粒度图像检索和跨层级相似性一致性，在提高检索精度的同时减少3.5倍计算量


<details>
  <summary>Details</summary>
Motivation: 传统检索方法精度有限，现有多向量检索方法在查询与图像对象对齐方面存在不足，且存在冗余细粒度图像片段问题

Method: 1. 引入分层范式，使用多个中间粒度处理不同图像对象以增强对齐；2. 利用跨层级相似性一致性和层级稀疏性减少冗余检索；3. 自动配置数据集参数

Result: HiMIR在精度上取得显著提升，同时在计算量上比现有MVR系统减少高达3.5倍

Conclusion: HiMIR框架通过分层多粒度检索和冗余最小化策略，有效平衡了检索精度和计算效率

Abstract: To effectively leverage user-specific data, retrieval augmented generation
(RAG) is employed in multimodal large language model (MLLM) applications.
However, conventional retrieval approaches often suffer from limited retrieval
accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by
decomposing queries and matching against segmented images. They still suffer
from sub-optimal accuracy and efficiency, overlooking alignment between the
query and varying image objects and redundant fine-grained image segments. In
this work, we present an efficient scheduling framework for image retrieval -
HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple
intermediate granularities for varying image objects to enhance alignment.
Second, we minimize redundancy in retrieval by leveraging cross-hierarchy
similarity consistency and hierarchy sparsity to minimize unnecessary matching
computation. Furthermore, we configure parameters for each dataset
automatically for practicality across diverse scenarios. Our empirical study
shows that, HiMIR not only achieves substantial accuracy improvements but also
reduces computation by up to 3.5 times over the existing MVR system.

</details>


### [188] [Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation](https://arxiv.org/abs/2510.08617)
*Saumya B*

Main category: cs.CV

TL;DR: 该研究评估了使用焦点损失和数据增强的U-Net在脑肿瘤MRI分割中的性能，实现了90%的精确度，并提供了可复现的基准。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对诊断和治疗规划至关重要，但类别不平衡和模型泛化能力有限等问题阻碍了进展，需要建立可复现的基准来指导未来研究。

Method: 使用U-Net架构，结合焦点损失函数和三种基础数据增强技术（水平翻转、旋转、缩放），在公开MRI数据集上进行实验，重点调优焦点损失参数。

Result: U-Net结合焦点损失实现了90%的精确度，与最先进方法结果相当，同时所有代码和结果都公开可用。

Conclusion: 该研究为脑肿瘤分割领域建立了一个透明、可复现的基准，为未来数据增强策略和损失函数设计的研究提供了指导。

Abstract: Brain tumor segmentation is crucial for diagnosis and treatment planning, yet
challenges such as class imbalance and limited model generalization continue to
hinder progress. This work presents a reproducible evaluation of U-Net
segmentation performance on brain tumor MRI using focal loss and basic data
augmentation strategies. Experiments were conducted on a publicly available MRI
dataset, focusing on focal loss parameter tuning and assessing the impact of
three data augmentation techniques: horizontal flip, rotation, and scaling. The
U-Net with focal loss achieved a precision of 90%, comparable to
state-of-the-art results. By making all code and results publicly available,
this study establishes a transparent, reproducible baseline to guide future
research on augmentation strategies and loss function design in brain tumor
segmentation.

</details>


### [189] [Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs](https://arxiv.org/abs/2510.08631)
*Hanieh Shojaei Miandashti,Claus Brenner*

Main category: cs.CV

TL;DR: 提出了一种基于层次贝叶斯建模的无监督OOD检测方法，通过分离认知不确定性和偶然不确定性，在LiDAR点云语义分割中显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督OOD检测方法依赖预测熵，但混淆了认知不确定性和偶然不确定性，导致将分布内模糊区域误判为OOD。需要一种能够准确分离这两种不确定性的方法。

Method: 使用层次贝叶斯建模对深度神经网络特征空间中的高斯混合模型参数进行建模，从中提取认知不确定性，无需辅助数据或额外训练阶段。

Result: 在SemanticKITTI数据集上，相比预测熵方法，AUROC提升18%，AUPRC提升22%，FPR95从76%降低到40%（减少36%）。

Conclusion: 基于层次贝叶斯建模的认知不确定性方法能够有效分离不确定性的来源，显著提升无监督OOD检测性能，无需额外数据或训练。

Abstract: In addition to accurate scene understanding through precise semantic
segmentation of LiDAR point clouds, detecting out-of-distribution (OOD)
objects, instances not encountered during training, is essential to prevent the
incorrect assignment of unknown objects to known classes. While supervised OOD
detection methods depend on auxiliary OOD datasets, unsupervised methods avoid
this requirement but typically rely on predictive entropy, the entropy of the
predictive distribution obtained by averaging over an ensemble or multiple
posterior weight samples. However, these methods often conflate epistemic
(model) and aleatoric (data) uncertainties, misclassifying ambiguous in
distribution regions as OOD. To address this issue, we present an unsupervised
OOD detection approach that employs epistemic uncertainty derived from
hierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters in
the feature space of a deep neural network. Without requiring auxiliary data or
additional training stages, our approach outperforms existing uncertainty-based
methods on the SemanticKITTI dataset, achieving an 18\% improvement in AUROC,
22\% increase in AUPRC, and 36\% reduction in FPR95 (from 76\% to 40\%),
compared to the predictive entropy approach used in prior works.

</details>


### [190] [Structured Output Regularization: a framework for few-shot transfer learning](https://arxiv.org/abs/2510.08728)
*Nicolas Ewen,Jairo Diaz-Rodriguez,Kelly Ramsay*

Main category: cs.CV

TL;DR: 提出了结构化输出正则化(SOR)框架，通过冻结网络内部结构并使用组套索和L1惩罚，在少量数据下实现更好的迁移学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习冻结部分权重并添加任务特定层的方法虽然计算高效，但限制了模型适应领域特定特征的能力，在数据极少时仍容易过拟合。

Method: SOR框架冻结网络内部结构（如卷积滤波器），同时使用组套索和L1惩罚的组合，以最少的额外参数定制模型，适用于各种网络组件。

Result: 在三个少样本医学影像分类任务上评估SOR，使用DenseNet121和EfficientNetB4作为基础网络，相比已有基准取得了有竞争力的结果。

Conclusion: SOR是一个简单有效的框架，能够通过结构化正则化在少量数据下实现更好的迁移学习性能，具有广泛的适用性。

Abstract: Traditional transfer learning typically reuses large pre-trained networks by
freezing some of their weights and adding task-specific layers. While this
approach is computationally efficient, it limits the model's ability to adapt
to domain-specific features and can still lead to overfitting with very limited
data. To address these limitations, we propose Structured Output Regularization
(SOR), a simple yet effective framework that freezes the internal network
structures (e.g., convolutional filters) while using a combination of group
lasso and $L_1$ penalties. This framework tailors the model to specific data
with minimal additional parameters and is easily applicable to various network
components, such as convolutional filters or various blocks in neural networks
enabling broad applicability for transfer learning tasks. We evaluate SOR on
three few shot medical imaging classification tasks and we achieve competitive
results using DenseNet121, and EfficientNetB4 bases compared to established
benchmarks.

</details>


### [191] [Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform](https://arxiv.org/abs/2510.08770)
*Gregory Yeghiyan,Jurius Azar,Devson Butani,Chan-Jin Chung*

Main category: cs.CV

TL;DR: 基于预训练深度学习模型的实时泄漏检测系统，结合RGB和热成像技术，在消费级硬件上实现快速准确的泄漏分类


<details>
  <summary>Details</summary>
Motivation: 开发能够在不同环境条件下快速准确检测泄漏的系统，特别是在安全关键场景中需要可靠的实时检测能力

Method: 使用预训练的深度学习模型（VGG19、NasNetMobile）处理RGB和热成像数据，在包含4000张图像的平衡数据集上进行训练和测试

Result: 热成像模型在推理速度、准确率和模型大小方面表现更优，最高达到100%准确率，推理时间低至44毫秒，模型大小小于350MB

Conclusion: 基于热成像的VGG19模型表现最佳，系统在消费级硬件上实现了高效部署，适用于安全关键环境

Abstract: This paper presents a real-time spill detection system that utilizes
pretrained deep learning models with RGB and thermal imaging to classify spill
vs. no-spill scenarios across varied environments. Using a balanced binary
dataset (4,000 images), our experiments demonstrate the advantages of thermal
imaging in inference speed, accuracy, and model size. We achieve up to 100%
accuracy using lightweight models like VGG19 and NasNetMobile, with thermal
models performing faster and more robustly across different lighting
conditions. Our system runs on consumer-grade hardware (RTX 4080) and achieves
inference times as low as 44 ms with model sizes under 350 MB, highlighting its
deployability in safety-critical contexts. Results from experiments with a real
robot and test datasets indicate that a VGG19 model trained on thermal imaging
performs best.

</details>


### [192] [SkipSR: Faster Super Resolution with Token Skipping](https://arxiv.org/abs/2510.08799)
*Rohan Choudhury,Shanchuan Lin,Jianyi Wang,Hao Chen,Qi Zhao,Feng Cheng,Lu Jiang,Kris Kitani,Laszlo A. Jeni*

Main category: cs.CV

TL;DR: SkipSR是一个加速视频超分辨率的方法，通过识别低细节区域并跳过计算，在保持感知质量的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率方法在视频生成和修复中很关键，但计算成本高且速度慢，限制了在高分辨率和长视频上的可扩展性。

Method: 提出SkipSR框架，直接从低分辨率输入识别低细节区域，完全跳过这些区域的计算，只对需要细化的区域进行超分辨率处理。

Result: 在标准SR基准测试中，720p视频上比先前模型实现高达60%的端到端延迟加速，且没有可感知的质量损失。

Conclusion: SkipSR通过选择性计算策略有效加速视频超分辨率，在保持质量的同时显著提升效率。

Abstract: Diffusion-based super-resolution (SR) is a key component in video generation
and video restoration, but is slow and expensive, limiting scalability to
higher resolutions and longer videos. Our key insight is that many regions in
video are inherently low-detail and gain little from refinement, yet current
methods process all pixels uniformly. To take advantage of this, we propose
SkipSR, a simple framework for accelerating video SR by identifying low-detail
regions directly from low-resolution input, then skipping computation on them
entirely, only super-resolving the areas that require refinement. This simple
yet effective strategy preserves perceptual quality in both standard and
one-step diffusion SR models while significantly reducing computation. In
standard SR benchmarks, our method achieves up to 60% faster end-to-end latency
than prior models on 720p videos with no perceptible loss in quality. Video
demos are available at https://rccchoudhury.github.io/skipsr/

</details>


### [193] [PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning](https://arxiv.org/abs/2510.08919)
*Daiki Yoshikawa,Takashi Matsubara*

Main category: cs.CV

TL;DR: PHyCLIP使用双曲空间的ℓ₁-乘积度量来同时表达概念层次结构和跨概念组合性，在多项任务中优于单空间方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型难以同时表达概念家族内的层次结构（如狗⊆哺乳动物⊆动物）和跨概念家族的组合性（如"车里的狗"⊆狗,车），双曲空间虽能有效捕捉层次结构但对组合性的表示能力有限。

Method: 提出PHyCLIP，在双曲空间的笛卡尔积上使用ℓ₁-乘积度量，让概念家族内的层次结构出现在单个双曲因子中，跨概念组合通过ℓ₁-乘积度量捕获，类似于布尔代数。

Result: 在零样本分类、检索、层次分类和组合理解任务上的实验表明，PHyCLIP优于现有单空间方法，并在嵌入空间中提供更可解释的结构。

Conclusion: PHyCLIP通过双曲空间的乘积度量设计，成功解决了同时表达概念层次结构和组合性的挑战，为多模态表示学习提供了更有效的解决方案。

Abstract: Vision-language models have achieved remarkable success in multi-modal
representation learning from large-scale pairs of visual scenes and linguistic
descriptions. However, they still struggle to simultaneously express two
distinct types of semantic structures: the hierarchy within a concept family
(e.g., dog $\preceq$ mammal $\preceq$ animal) and the compositionality across
different concept families (e.g., "a dog in a car" $\preceq$ dog, car). Recent
works have addressed this challenge by employing hyperbolic space, which
efficiently captures tree-like hierarchy, yet its suitability for representing
compositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,
which employs an $\ell_1$-Product metric on a Cartesian product of Hyperbolic
factors. With our design, intra-family hierarchies emerge within individual
hyperbolic factors, and cross-family composition is captured by the
$\ell_1$-product metric, analogous to a Boolean algebra. Experiments on
zero-shot classification, retrieval, hierarchical classification, and
compositional understanding tasks demonstrate that PHyCLIP outperforms existing
single-space approaches and offers more interpretable structures in the
embedding space.

</details>


### [194] [Denoised Diffusion for Object-Focused Image Augmentation](https://arxiv.org/abs/2510.08955)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 提出了一种面向对象的动物健康监测数据增强框架，通过分割动物背景、变换和扩散合成来生成多样化场景，解决农业监测中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 农业监测系统依赖多源数据，但无人机动物健康监测面临数据稀缺和场景特定问题（如动物小、遮挡、部分可见）。迁移学习方法因缺乏反映特定农场条件的大数据集而效果有限。

Method: 提出对象聚焦的数据增强框架：分割动物与背景，通过变换和基于扩散的合成生成真实多样的场景，提升动物检测和监测性能。

Result: 初步实验表明，增强数据集在动物检测任务上表现优于基线模型。

Conclusion: 该方法通过生成领域特定数据，即使在数据稀缺场景下也能实现实时动物健康监测，弥合有限数据与实际应用之间的差距。

Abstract: Modern agricultural operations increasingly rely on integrated monitoring
systems that combine multiple data sources for farm optimization. Aerial
drone-based animal health monitoring serves as a key component but faces
limited data availability, compounded by scene-specific issues such as small,
occluded, or partially visible animals. Transfer learning approaches often fail
to address this limitation due to the unavailability of large datasets that
reflect specific farm conditions, including variations in animal breeds,
environments, and behaviors. Therefore, there is a need for developing a
problem-specific, animal-focused data augmentation strategy tailored to these
unique challenges. To address this gap, we propose an object-focused data
augmentation framework designed explicitly for animal health monitoring in
constrained data settings. Our approach segments animals from backgrounds and
augments them through transformations and diffusion-based synthesis to create
realistic, diverse scenes that enhance animal detection and monitoring
performance. Our initial experiments demonstrate that our augmented dataset
yields superior performance compared to our baseline models on the animal
detection task. By generating domain-specific data, our method empowers
real-time animal health monitoring solutions even in data-scarce scenarios,
bridging the gap between limited data and practical applicability.

</details>


### [195] [Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation](https://arxiv.org/abs/2510.08979)
*Yuki Nii,Futa Waseda,Ching-Chun Chang,Isao Echizen*

Main category: cs.CV

TL;DR: 该论文提出了首个防御AI非法上色的方法——Uncolorable Examples，通过在灰度图像中嵌入不可察觉的扰动来破坏未经授权的上色效果。


<details>
  <summary>Details</summary>
Motivation: AI上色技术虽然强大，但存在版权侵权风险，如未经授权对黑白漫画和电影进行上色并转售。目前缺乏有效防止此类滥用的方法。

Method: 提出了PAChroma方法，通过拉普拉斯滤波器优化不可察觉的扰动以保持感知质量，并在优化过程中应用多样化输入变换来增强跨模型迁移性和对常见后处理的鲁棒性。

Result: 在ImageNet和Danbooru数据集上的实验表明，PAChroma能有效降低上色质量，同时保持视觉外观不变。

Conclusion: 这是保护视觉内容免受非法AI上色的首个防御方法，为生成媒体中的版权保护防御开辟了新途径。

Abstract: AI-based colorization has shown remarkable capability in generating realistic
color images from grayscale inputs. However, it poses risks of copyright
infringement -- for example, the unauthorized colorization and resale of
monochrome manga and films. Despite these concerns, no effective method
currently exists to prevent such misuse. To address this, we introduce the
first defensive paradigm, Uncolorable Examples, which embed imperceptible
perturbations into grayscale images to invalidate unauthorized colorization. To
ensure real-world applicability, we establish four criteria: effectiveness,
imperceptibility, transferability, and robustness. Our method, Perception-Aware
Chroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that
meet these four criteria by optimizing imperceptible perturbations with a
Laplacian filter to preserve perceptual quality, and applying diverse input
transformations during optimization to enhance transferability across models
and robustness against common post-processing (e.g., compression). Experiments
on ImageNet and Danbooru datasets demonstrate that PAChroma effectively
degrades colorization quality while maintaining the visual appearance. This
work marks the first step toward protecting visual content from illegitimate AI
colorization, paving the way for copyright-aware defenses in generative media.

</details>


### [196] [Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels](https://arxiv.org/abs/2510.09035)
*Weitong Kong,Zichao Zeng,Di Wen,Jiale Wei,Kunyu Peng,June Moh Goo,Jan Boehm,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 提出了DGLSS-NL任务，即带噪声标签的LiDAR语义分割领域泛化，并开发了DuNe双视图框架来解决现有方法在LiDAR数据上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: LiDAR标注通常存在噪声，这些噪声在领域偏移下会被放大，威胁系统可靠性。虽然图像中的噪声标签学习已有研究，但在3D LiDAR分割领域泛化中的应用仍未被探索。

Method: 提出DuNe双视图框架，包含强分支和弱分支，通过特征级一致性约束和基于置信度过滤的交叉熵损失来处理噪声标签。

Result: 在10%对称标签噪声下，在SemanticKITTI上达到56.86% mIoU，nuScenes上42.28%，SemanticPOSS上52.58%，整体算术平均49.57%，调和平均48.50%。

Conclusion: DuNe框架在DGLSS-NL任务中表现出强大的领域泛化能力，显著优于现有方法。

Abstract: Accurate perception is critical for vehicle safety, with LiDAR as a key
enabler in autonomous driving. To ensure robust performance across
environments, sensor types, and weather conditions without costly
re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is
essential. However, LiDAR annotations are often noisy due to sensor
imperfections, occlusions, and human errors. Such noise degrades segmentation
accuracy and is further amplified under domain shifts, threatening system
reliability. While noisy-label learning is well-studied in images, its
extension to 3D LiDAR segmentation under domain generalization remains largely
unexplored, as the sparse and irregular structure of point clouds limits direct
use of 2D methods. To address this gap, we introduce the novel task Domain
Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL)
and establish the first benchmark by adapting three representative noisy-label
learning strategies from image classification to 3D segmentation. However, we
find that existing noisy-label learning approaches adapt poorly to LiDAR data.
We therefore propose DuNe, a dual-view framework with strong and weak branches
that enforce feature-level consistency and apply cross-entropy loss based on
confidence-aware filtering of predictions. Our approach shows state-of-the-art
performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and
52.58% on SemanticPOSS under 10% symmetric label noise, with an overall
Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby
demonstrating robust domain generalization in DGLSS-NL tasks. The code is
available on our project page.

</details>


### [197] [A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle Pathological Features in CT Scans](https://arxiv.org/abs/2510.09107)
*Irash Perera,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 提出了一种用于医学影像分析的多分支ConvNeXt架构，在COVID-19 CT扫描诊断中取得了优异性能，ROC-AUC达到0.9937，验证准确率0.9757，F1分数0.9825。


<details>
  <summary>Details</summary>
Motivation: 医学影像智能分析对临床诊断至关重要，特别是识别细微病理特征。针对医学图像分析的细微挑战，需要专门设计的深度学习架构。

Method: 采用多分支ConvNeXt架构，整合全局平均池化、全局最大池化和新的注意力加权池化三个并行分支特征。包含端到端流程：数据预处理、增强和两阶段训练策略，有效利用迁移学习。

Result: 在2,609个CT切片数据集上验证，COVID-19病例的ROC-AUC为0.9937，验证准确率0.9757，F1分数0.9825，优于所有先前报告的模型。

Conclusion: 现代多分支架构结合仔细的数据处理，可以达到或超越当前最先进模型的性能，证明了先进深度学习技术在稳健医学诊断中的有效性。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis, especially for identifying subtle pathological features.
This paper introduces a novel multi-branch ConvNeXt architecture designed
specifically for the nuanced challenges of medical image analysis. While
applied here to the specific problem of COVID-19 diagnosis, the methodology
offers a generalizable framework for classifying a wide range of pathologies
from CT scans. The proposed model incorporates a rigorous end-to-end pipeline,
from meticulous data preprocessing and augmentation to a disciplined two-phase
training strategy that leverages transfer learning effectively. The
architecture uniquely integrates features extracted from three parallel
branches: Global Average Pooling, Global Max Pooling, and a new
Attention-weighted Pooling mechanism. The model was trained and validated on a
combined dataset of 2,609 CT slices derived from two distinct datasets.
Experimental results demonstrate a superior performance on the validation set,
achieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and an
F1-score of 0.9825 for COVID-19 cases, outperforming all previously reported
models on this dataset. These findings indicate that a modern, multi-branch
architecture, coupled with careful data handling, can achieve performance
comparable to or exceeding contemporary state-of-the-art models, thereby
proving the efficacy of advanced deep learning techniques for robust medical
diagnostics.

</details>


### [198] [Training Feature Attribution for Vision Models](https://arxiv.org/abs/2510.09135)
*Aziz Bacha,Thomas George*

Main category: cs.CV

TL;DR: 提出训练特征归因方法，将测试预测与特定训练图像的特定区域联系起来，提供对深度模型内部工作机制的新见解。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络通常被认为是黑盒系统，需要可解释性方法来提高信任和问责。现有方法通常将测试时预测归因于输入特征或影响训练样本，但认为应该同时研究这两种视角。

Method: 探索训练特征归因方法，该方法将测试预测与特定训练图像的特定区域联系起来。

Result: 在视觉数据集上的实验表明，训练特征归因能够提供细粒度的、测试特定的解释：识别导致错误分类的有害示例，并揭示传统归因方法未能暴露的虚假相关性（如基于补丁的捷径）。

Conclusion: 训练特征归因方法为深度模型的内部工作机制提供了新的洞察，能够识别传统方法难以发现的模型行为模式。

Abstract: Deep neural networks are often considered opaque systems, prompting the need
for explainability methods to improve trust and accountability. Existing
approaches typically attribute test-time predictions either to input features
(e.g., pixels in an image) or to influential training examples. We argue that
both perspectives should be studied jointly. This work explores *training
feature attribution*, which links test predictions to specific regions of
specific training images and thereby provides new insights into the inner
workings of deep models. Our experiments on vision datasets show that training
feature attribution yields fine-grained, test-specific explanations: it
identifies harmful examples that drive misclassifications and reveals spurious
correlations, such as patch-based shortcuts, that conventional attribution
methods fail to expose.

</details>


### [199] [Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras](https://arxiv.org/abs/2510.09230)
*Jindong Hong,Wencheng Zhang,Shiqin Qiao,Jianhai Chen,Jianing Qiu,Chuanyang Zheng,Qian Xu,Yun Ji,Qianyue Wen,Weiwei Sun,Hao Li,Huizhen Li,Huichao Wang,Kai Wu,Meng Li,Yijun He,Lingjie Luo,Jiankai Sun*

Main category: cs.CV

TL;DR: 提出基于消费级设备视频的肩部疾病诊断框架HMVDx，使用多模态大语言模型分别处理动作理解和疾病诊断，诊断准确率比直接视频诊断提升79.6%。


<details>
  <summary>Details</summary>
Motivation: 在医疗资源匮乏地区，肩部疾病早期准确诊断面临挑战，急需低成本、易扩展的辅助诊断方案。

Method: 提出HMVDx框架，将动作理解和疾病诊断任务分离，分别由两个MLLM完成，并引入基于医疗决策逻辑的可用性指数新指标。

Result: HMVDx在肩关节损伤诊断中的准确率比直接视频诊断提高了79.6%。

Conclusion: 该研究展示了低成本MLLM在医疗应用中的潜力，为未来MLLM在医疗视频理解领域的研究提供了重要技术贡献。

Abstract: Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),
are common conditions affecting the health of people worldwide, and have a high
incidence rate among the elderly and workers engaged in repetitive shoulder
tasks. In regions with scarce medical resources, achieving early and accurate
diagnosis poses significant challenges, and there is an urgent need for
low-cost and easily scalable auxiliary diagnostic solutions. This research
introduces videos captured by consumer-grade devices as the basis for
diagnosis, reducing the cost for users. We focus on the innovative application
of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of
shoulder disorders and propose a Hybrid Motion Video Diagnosis framework
(HMVDx). This framework divides the two tasks of action understanding and
disease diagnosis, which are respectively completed by two MLLMs. In addition
to traditional evaluation indicators, this work proposes a novel metric called
Usability Index by the logical process of medical decision-making (action
recognition, movement diagnosis, and final diagnosis). This index evaluates the
effectiveness of MLLMs in the medical field from the perspective of the entire
medical diagnostic pathway, revealing the potential value of low-cost MLLMs in
medical applications for medical practitioners. In experimental comparisons,
the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by
79.6\% compared with direct video diagnosis, a significant technical
contribution to future research on the application of MLLMs for video
understanding in the medical field.

</details>


### [200] [Zero-shot image privacy classification with Vision-Language Models](https://arxiv.org/abs/2510.09253)
*Alina Elena Baia,Alessio Xompero,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 本文通过建立零样本基准测试，系统评估了大型视觉语言模型在图像隐私预测任务中的表现，发现尽管VLMs资源消耗大，但在隐私预测准确性上仍落后于专用的小型模型，但在图像扰动鲁棒性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前文献倾向于采用通用的大型视觉语言模型进行图像隐私预测，但缺乏系统评估，可能忽视了专用模型的性能上限。本文旨在公平比较VLMs与专用模型在隐私预测任务中的表现。

Method: 建立零样本图像隐私分类基准，使用任务对齐的提示词评估前3名开源VLMs，并与已有的视觉专用和多模态方法在性能、效率和鲁棒性方面进行对比。

Result: 反直觉地发现，尽管VLMs参数多、推理慢且资源密集，但在隐私预测准确性上仍落后于专用的小型模型。同时发现VLMs对图像扰动具有更高的鲁棒性。

Conclusion: VLMs在图像隐私预测任务中尚未超越专用模型，但其在鲁棒性方面的优势值得关注，为未来模型设计提供了重要参考。

Abstract: While specialized learning-based models have historically dominated image
privacy prediction, the current literature increasingly favours adopting large
Vision-Language Models (VLMs) designed for generic tasks. This trend risks
overlooking the performance ceiling set by purpose-built models due to a lack
of systematic evaluation. To address this problem, we establish a zero-shot
benchmark for image privacy classification, enabling a fair comparison. We
evaluate the top-3 open-source VLMs, according to a privacy benchmark, using
task-aligned prompts and we contrast their performance, efficiency, and
robustness against established vision-only and multi-modal methods.
Counter-intuitively, our results show that VLMs, despite their
resource-intensive nature in terms of high parameter count and slower
inference, currently lag behind specialized, smaller models in privacy
prediction accuracy. We also find that VLMs exhibit higher robustness to image
perturbations.

</details>


### [201] [SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests](https://arxiv.org/abs/2510.09458)
*David-Alexandre Duclos,William Guimont-Martin,Gabriel Jeanson,Arthur Larochelle-Tremblay,Théo Defosse,Frédéric Moore,Philippe Nolet,François Pomerleau,Philippe Giguère*

Main category: cs.CV

TL;DR: 提出了SilvaScenes数据集，用于从林下图像进行树种实例分割，包含5个生物气候区域的1476棵树和24个树种，展示了树种分类的挑战性。


<details>
  <summary>Details</summary>
Motivation: 森林管理中的机器人感知面临复杂自然环境的挑战，现有数据集不足以开发先进的感知系统，需要专门针对森林环境的数据集。

Method: 在加拿大魁北克的5个生物气候区域收集林下图像，由林业专家标注1476棵树和24个树种，构建SilvaScenes数据集并基准测试现代深度学习实例分割方法。

Result: 树分割相对容易，最高mAP达67.65%，但树种分类极具挑战性，mAP仅为35.69%。

Conclusion: SilvaScenes数据集填补了森林环境感知数据集的空白，树种分类仍是显著挑战，为森林管理机器人感知研究提供了重要资源。

Abstract: Interest in robotics for forest management is growing, but perception in
complex, natural environments remains a significant hurdle. Conditions such as
heavy occlusion, variable lighting, and dense vegetation pose challenges to
automated systems, which are essential for precision forestry, biodiversity
monitoring, and the automation of forestry equipment. These tasks rely on
advanced perceptual capabilities, such as detection and fine-grained species
classification of individual trees. Yet, existing datasets are inadequate to
develop such perception systems, as they often focus on urban settings or a
limited number of species. To address this, we present SilvaScenes, a new
dataset for instance segmentation of tree species from under-canopy images.
Collected across five bioclimatic domains in Quebec, Canada, SilvaScenes
features 1476 trees from 24 species with annotations from forestry experts. We
demonstrate the relevance and challenging nature of our dataset by benchmarking
modern deep learning approaches for instance segmentation. Our results show
that, while tree segmentation is easy, with a top mean average precision (mAP)
of 67.65%, species classification remains a significant challenge with an mAP
of only 35.69%. Our dataset and source code will be available at
https://github.com/norlab-ulaval/SilvaScenes.

</details>


### [202] [D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models](https://arxiv.org/abs/2510.09473)
*Jisu Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: 提出维度熵最大化方法，通过正则化文本特征分布来缓解对比视觉语言模型中主导维度的影响，改善测试时提示调优的校准性能


<details>
  <summary>Details</summary>
Motivation: 发现对比视觉语言模型中存在由单一主导特征维度引起的模态间隙问题，这些主导维度具有高预测敏感性，限制其影响可以改善校准误差

Method: 提出维度熵最大化方法，通过正则化文本特征分布使其趋向均匀分布，从而减轻对主导维度的依赖

Result: 该方法缓解了测试时提示调优中校准性能的退化问题

Conclusion: 为增强视觉语言模型在现实部署场景中的可靠性提供了一个简单有效的解决方案

Abstract: Test-time adaptation paradigm provides flexibility towards domain shifts by
performing immediate adaptation on unlabeled target data from the source model.
Vision-Language Models (VLMs) leverage their generalization capabilities for
diverse downstream tasks, and test-time prompt tuning has emerged as a
prominent solution for adapting VLMs. In this work, we explore contrastive VLMs
and identify the modality gap caused by a single dominant feature dimension
across modalities. We observe that the dominant dimensions in both text and
image modalities exhibit high predictive sensitivity, and that constraining
their influence can improve calibration error. Building on this insight, we
propose dimensional entropy maximization that regularizes the distribution of
textual features toward uniformity to mitigate the dependency of dominant
dimensions. Our method alleviates the degradation of calibration performance in
test-time prompt tuning, offering a simple yet effective solution to enhance
the reliability of VLMs in real-world deployment scenarios.

</details>


### [203] [Few-shot multi-token DreamBooth with LoRa for style-consistent character generation](https://arxiv.org/abs/2510.09475)
*Ruben Pascual,Mikel Sesma-Sara,Aranzazu Jurio,Daniel Paternain,Mikel Galar*

Main category: cs.CV

TL;DR: 提出一种基于DreamBooth的少样本角色生成方法，通过多token策略和LoRA微调，能够从少量参考角色中生成无限数量的新角色，同时保持原始艺术风格。


<details>
  <summary>Details</summary>
Motivation: 解决动画和游戏行业需要从少量人类设计角色中生成大量新角色，同时保持统一艺术风格的问题。

Method: 基于DreamBooth框架，采用多token策略（为单个角色和集体风格分配独立token），结合LoRA参数高效微调，移除类别特定正则化集，在生成时引入随机token和嵌入。

Result: 在五个小型专业数据集上的评估显示，该方法能生成高质量、多样化的角色，同时保持参考角色的独特美学特征，人类评估进一步证实了其有效性。

Conclusion: 该方法成功实现了从少量参考角色中无限生成新角色的目标，在保持艺术风格一致性的同时拓展了创作可能性。

Abstract: The audiovisual industry is undergoing a profound transformation as it is
integrating AI developments not only to automate routine tasks but also to
inspire new forms of art. This paper addresses the problem of producing a
virtually unlimited number of novel characters that preserve the artistic style
and shared visual traits of a small set of human-designed reference characters,
thus broadening creative possibilities in animation, gaming, and related
domains. Our solution builds upon DreamBooth, a well-established fine-tuning
technique for text-to-image diffusion models, and adapts it to tackle two core
challenges: capturing intricate character details beyond textual prompts and
the few-shot nature of the training data. To achieve this, we propose a
multi-token strategy, using clustering to assign separate tokens to individual
characters and their collective style, combined with LoRA-based
parameter-efficient fine-tuning. By removing the class-specific regularization
set and introducing random tokens and embeddings during generation, our
approach allows for unlimited character creation while preserving the learned
style. We evaluate our method on five small specialized datasets, comparing it
to relevant baselines using both quantitative metrics and a human evaluation
study. Our results demonstrate that our approach produces high-quality, diverse
characters while preserving the distinctive aesthetic features of the reference
characters, with human evaluation further reinforcing its effectiveness and
highlighting the potential of our method.

</details>


### [204] [A methodology for clinically driven interactive segmentation evaluation](https://arxiv.org/abs/2510.09499)
*Parhom Esmaeili,Virginia Fernandez,Pedro Borges,Eli Gibson,Sebastien Ourselin,M. Jorge Cardoso*

Main category: cs.CV

TL;DR: 提出了一个临床基础的交互式医学图像分割评估方法，构建了标准化评估框架，发现最小化用户交互信息损失、自适应缩放机制对模型鲁棒性至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有交互式医学图像分割评估方法不一致且不切实际，阻碍公平比较和真实性能评估。

Method: 提出临床基础的评估任务和指标定义方法，构建标准化评估框架软件，评估最先进算法在异质复杂任务上的表现。

Result: 发现最小化交互信息损失、自适应缩放机制提升鲁棒性；验证提示行为与训练不一致会降低性能；2D方法在板状图像和粗目标上表现好，3D上下文对大或不规则目标有帮助；非医学领域模型在对比度差和复杂形状时性能下降。

Conclusion: 标准化临床评估框架对交互式医学图像分割算法发展至关重要，揭示了关键性能影响因素和模型选择指导原则。

Abstract: Interactive segmentation is a promising strategy for building robust,
generalisable algorithms for volumetric medical image segmentation. However,
inconsistent and clinically unrealistic evaluation hinders fair comparison and
misrepresents real-world performance. We propose a clinically grounded
methodology for defining evaluation tasks and metrics, and built a software
framework for constructing standardised evaluation pipelines. We evaluate
state-of-the-art algorithms across heterogeneous and complex tasks and observe
that (i) minimising information loss when processing user interactions is
critical for model robustness, (ii) adaptive-zooming mechanisms boost
robustness and speed convergence, (iii) performance drops if validation
prompting behaviour/budgets differ from training, (iv) 2D methods perform well
with slab-like images and coarse targets, but 3D context helps with large or
irregularly shaped targets, (v) performance of non-medical-domain models (e.g.
SAM2) degrades with poor contrast and complex shapes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [205] [Bridging Research and Practice in Simulation-based Testing of Industrial Robot Navigation Systems](https://arxiv.org/abs/2510.09396)
*Sajad Khatiri,Francisco Eli Vina Barrientos,Maximilian Wulf,Paolo Tonella,Sebastiano Panichella*

Main category: cs.RO

TL;DR: 将Surrealist仿真测试框架从无人机扩展到ANYmal四足机器人工业检测应用，通过搜索算法自动生成障碍规避场景，在工业评估中验证了五个专有算法并提升了开发流程。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法难以覆盖动态环境中机器人导航的全部操作要求，需要自动化测试框架来发现手动测试遗漏的关键故障。

Method: 使用基于搜索的算法自动生成具有挑战性的障碍规避测试场景，并将该框架集成到ANYbotics工作流程中进行工业评估。

Result: 在试点阶段，生成的测试套件揭示了一个实验算法的关键弱点（成功率40.3%），并证明了另一个算法的优越鲁棒性（成功率71.2%）。在六个月工业评估中成功测试了五个专有算法。

Conclusion: 该框架增强了开发过程，发现关键故障，提供客观基准，并加强了整体验证流程，正式调查确认了其价值。

Abstract: Ensuring robust robotic navigation in dynamic environments is a key
challenge, as traditional testing methods often struggle to cover the full
spectrum of operational requirements. This paper presents the industrial
adoption of Surrealist, a simulation-based test generation framework originally
for UAVs, now applied to the ANYmal quadrupedal robot for industrial
inspection. Our method uses a search-based algorithm to automatically generate
challenging obstacle avoidance scenarios, uncovering failures often missed by
manual testing. In a pilot phase, generated test suites revealed critical
weaknesses in one experimental algorithm (40.3% success rate) and served as an
effective benchmark to prove the superior robustness of another (71.2% success
rate). The framework was then integrated into the ANYbotics workflow for a
six-month industrial evaluation, where it was used to test five proprietary
algorithms. A formal survey confirmed its value, showing it enhances the
development process, uncovers critical failures, provides objective benchmarks,
and strengthens the overall verification pipeline.

</details>


### [206] [Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation](https://arxiv.org/abs/2510.08807)
*Zhenyu Zhao,Hongyi Jing,Xiawei Liu,Jiageng Mao,Abha Jha,Hanwen Yang,Rong Xue,Sergey Zakharor,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: 提出了Humanoid Everyday数据集，这是一个大规模、多样化的人形机器人操作数据集，包含10.3k条轨迹和300万帧数据，涵盖260个任务，并提供了云端评估平台。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习数据集主要关注固定机械臂，现有的人形机器人数据集要么局限于固定环境，要么任务多样性不足，缺乏人机交互和下半身运动，且缺少标准化评估平台。

Method: 通过高效的人类监督遥操作流程，收集高质量多模态感官数据（RGB、深度、LiDAR、触觉输入）和自然语言标注。

Result: 构建了包含10.3k条轨迹、300万帧数据、260个任务的数据集，涵盖7大类任务，包括灵巧物体操作、人机交互、运动集成动作等。

Conclusion: 通过发布Humanoid Everyday数据集、策略学习分析和标准化云端评估平台，旨在推进通用人形机器人操作研究，为现实世界中更智能的具身机器人奠定基础。

Abstract: From loco-motion to dextrous manipulation, humanoid robots have made
remarkable strides in demonstrating complex full-body capabilities. However,
the majority of current robot learning datasets and benchmarks mainly focus on
stationary robot arms, and the few existing humanoid datasets are either
confined to fixed environments or limited in task diversity, often lacking
human-humanoid interaction and lower-body locomotion. Moreover, there are a few
standardized evaluation platforms for benchmarking learning-based policies on
humanoid data. In this work, we present Humanoid Everyday, a large-scale and
diverse humanoid manipulation dataset characterized by extensive task variety
involving dextrous object manipulation, human-humanoid interaction,
locomotion-integrated actions, and more. Leveraging a highly efficient
human-supervised teleoperation pipeline, Humanoid Everyday aggregates
high-quality multimodal sensory data, including RGB, depth, LiDAR, and tactile
inputs, together with natural language annotations, comprising 10.3k
trajectories and over 3 million frames of data across 260 tasks across 7 broad
categories. In addition, we conduct an analysis of representative policy
learning methods on our dataset, providing insights into their strengths and
limitations across different task categories. For standardized evaluation, we
introduce a cloud-based evaluation platform that allows researchers to
seamlessly deploy their policies in our controlled setting and receive
performance feedback. By releasing Humanoid Everyday along with our policy
learning analysis and a standardized cloud-based evaluation platform, we intend
to advance research in general-purpose humanoid manipulation and lay the
groundwork for more capable and embodied robotic agents in real-world
scenarios. Our dataset, data collection code, and cloud evaluation website are
made publicly available on our project website.

</details>


### [207] [When a Robot is More Capable than a Human: Learning from Constrained Demonstrators](https://arxiv.org/abs/2510.09096)
*Xinhu Li,Ayush Jain,Zhaojing Yang,Yigit Korkmaz,Erdem Bıyık*

Main category: cs.RO

TL;DR: 该论文提出了一种从受限专家演示中学习的方法，通过推断状态奖励信号和自标注未知状态奖励，使机器人能够学习比专家演示更优的策略。


<details>
  <summary>Details</summary>
Motivation: 现有演示接口（如遥操作、模拟到真实迁移）限制了专家展示最优行为的能力，导致收集的演示数据质量不高，学习到的策略性能欠佳。

Method: 使用演示数据推断仅基于状态的奖励信号来度量任务进展，并通过时间插值为未知状态自标注奖励，让智能体探索更短更高效的轨迹。

Result: 该方法在样本效率和任务完成时间上均优于常见的模仿学习方法，在真实WidowX机械臂上完成任务仅需12秒，比行为克隆快10倍。

Conclusion: 通过超越直接模仿专家动作，允许智能体探索更优轨迹，可以从受限专家的演示中学习到比演示本身更好的策略。

Abstract: Learning from demonstrations enables experts to teach robots complex tasks
using interfaces such as kinesthetic teaching, joystick control, and
sim-to-real transfer. However, these interfaces often constrain the expert's
ability to demonstrate optimal behavior due to indirect control, setup
restrictions, and hardware safety. For example, a joystick can move a robotic
arm only in a 2D plane, even though the robot operates in a higher-dimensional
space. As a result, the demonstrations collected by constrained experts lead to
suboptimal performance of the learned policies. This raises a key question: Can
a robot learn a better policy than the one demonstrated by a constrained
expert? We address this by allowing the agent to go beyond direct imitation of
expert actions and explore shorter and more efficient trajectories. We use the
demonstrations to infer a state-only reward signal that measures task progress,
and self-label reward for unknown states using temporal interpolation. Our
approach outperforms common imitation learning in both sample efficiency and
task completion time. On a real WidowX robotic arm, it completes the task in 12
seconds, 10x faster than behavioral cloning, as shown in real-robot videos on
https://sites.google.com/view/constrainedexpert .

</details>


### [208] [Flow-Opt: Scalable Centralized Multi-Robot Trajectory Optimization with Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2510.09204)
*Simon Idoko,Arun Kumar Singh*

Main category: cs.RO

TL;DR: Flow-Opt是一种基于学习的集中式多机器人轨迹优化方法，通过生成模型采样候选轨迹和使用安全过滤器确保约束满足，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 集中式多机器人轨迹优化在紧空间规划中能产生更平滑的轨迹，但计算复杂度随机器人数量增加而急剧上升，难以扩展到大规模群体。

Method: 使用基于流匹配的生成模型（扩散变换器）采样候选轨迹，配合可微分的安全过滤器确保约束满足，并训练初始化网络预测上下文特定的初始化。

Result: 能在几十毫秒内为数十个机器人生成在杂乱环境中的轨迹，比现有集中式优化方法快数倍，比基于扩散模型的基线方法快几个数量级。

Conclusion: 该方法在计算效率、批量处理能力和轨迹多样性方面均优于现有方法，为大规模多机器人轨迹优化提供了可行解决方案。

Abstract: Centralized trajectory optimization in the joint space of multiple robots
allows access to a larger feasible space that can result in smoother
trajectories, especially while planning in tight spaces. Unfortunately, it is
often computationally intractable beyond a very small swarm size. In this
paper, we propose Flow-Opt, a learning-based approach towards improving the
computational tractability of centralized multi-robot trajectory optimization.
Specifically, we reduce the problem to first learning a generative model to
sample different candidate trajectories and then using a learned
Safety-Filter(SF) to ensure fast inference-time constraint satisfaction. We
propose a flow-matching model with a diffusion transformer (DiT) augmented with
permutation invariant robot position and map encoders as the generative model.
We develop a custom solver for our SF and equip it with a neural network that
predicts context-specific initialization. The initialization network is trained
in a self-supervised manner, taking advantage of the differentiability of the
SF solver. We advance the state-of-the-art in the following respects. First, we
show that we can generate trajectories of tens of robots in cluttered
environments in a few tens of milliseconds. This is several times faster than
existing centralized optimization approaches. Moreover, our approach also
generates smoother trajectories orders of magnitude faster than competing
baselines based on diffusion models. Second, each component of our approach can
be batched, allowing us to solve a few tens of problem instances in a fraction
of a second. We believe this is a first such result; no existing approach
provides such capabilities. Finally, our approach can generate a diverse set of
trajectories between a given set of start and goal locations, which can capture
different collision-avoidance behaviors.

</details>


### [209] [Placeit! A Framework for Learning Robot Object Placement Skills](https://arxiv.org/abs/2510.09267)
*Amina Ferrad,Johann Huber,François Hélénon,Julien Gleyze,Mahdi Khoramshahi,Stéphane Doncieux*

Main category: cs.RO

TL;DR: 提出了Placeit!框架，使用进化计算自动生成刚性物体的有效放置位置，支持桌面放置、堆叠和插入等任务，在真实世界部署中达到90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人学习面临大规模高质量数据获取的瓶颈，手动收集数据过程繁琐。受Graspit!启发，希望开发自动生成放置位置的框架来克服这一挑战。

Method: 采用进化计算框架，结合质量多样性优化，自动生成多样化的有效放置姿势，支持多种放置场景。

Result: 在所有场景中显著优于现有方法，能够生成更多样化的有效姿势。基于该框架的拾放管道在120次真实世界部署中达到90%的成功率。

Conclusion: Placeit!是开放环境拾放任务的有力工具，也是训练基于仿真的机器人基础模型所需数据生成的重要引擎。

Abstract: Robotics research has made significant strides in learning, yet mastering
basic skills like object placement remains a fundamental challenge. A key
bottleneck is the acquisition of large-scale, high-quality data, which is often
a manual and laborious process. Inspired by Graspit!, a foundational work that
used simulation to automatically generate dexterous grasp poses, we introduce
Placeit!, an evolutionary-computation framework for generating valid placement
positions for rigid objects. Placeit! is highly versatile, supporting tasks
from placing objects on tables to stacking and inserting them. Our experiments
show that by leveraging quality-diversity optimization, Placeit! significantly
outperforms state-of-the-art methods across all scenarios for generating
diverse valid poses. A pick&place pipeline built on our framework achieved a
90% success rate over 120 real-world deployments. This work positions Placeit!
as a powerful tool for open-environment pick-and-place tasks and as a valuable
engine for generating the data needed to train simulation-based foundation
models in robotics.

</details>


### [210] [Failure Prediction at Runtime for Generative Robot Policies](https://arxiv.org/abs/2510.09459)
*Ralf Römer,Adrian Kobras,Luca Worbis,Angela P. Schoellig*

Main category: cs.RO

TL;DR: FIPER是一个无需失败数据的运行时故障预测框架，通过检测分布外观察和动作不确定性来预测生成式模仿学习策略的故障。


<details>
  <summary>Details</summary>
Motivation: 生成式模仿学习策略在运行时可能因分布偏移或动作误差累积而导致不可预测的不安全行为，需要早期故障预测来确保机器人在安全关键环境中的部署。

Method: 使用随机网络蒸馏检测分布外观察，提出新的动作块熵分数衡量动作不确定性，通过保形预测校准故障预测分数，当两个指标在短时间窗口内超过阈值时触发故障警报。

Result: 在五个仿真和真实环境中的评估表明，FIPER能更好地区分实际故障与良性分布外情况，比现有方法更准确、更早地预测故障。

Conclusion: FIPER是迈向更可解释、更安全的生成式机器人策略的重要一步，提高了生成式模仿学习策略在运行时故障预测的能力。

Abstract: Imitation learning (IL) with generative models, such as diffusion and flow
matching, has enabled robots to perform complex, long-horizon tasks. However,
distribution shifts from unseen environments or compounding action errors can
still cause unpredictable and unsafe behavior, leading to task failure. Early
failure prediction during runtime is therefore essential for deploying robots
in human-centered and safety-critical environments. We propose FIPER, a general
framework for Failure Prediction at Runtime for generative IL policies that
does not require failure data. FIPER identifies two key indicators of impending
failure: (i) out-of-distribution (OOD) observations detected via random network
distillation in the policy's embedding space, and (ii) high uncertainty in
generated actions measured by a novel action-chunk entropy score. Both failure
prediction scores are calibrated using a small set of successful rollouts via
conformal prediction. A failure alarm is triggered when both indicators,
aggregated over short time windows, exceed their thresholds. We evaluate FIPER
across five simulation and real-world environments involving diverse failure
modes. Our results demonstrate that FIPER better distinguishes actual failures
from benign OOD situations and predicts failures more accurately and earlier
than existing methods. We thus consider this work an important step towards
more interpretable and safer generative robot policies. Code, data and videos
are available at https://tum-lsy.github.io/fiper_website.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [211] [Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System](https://arxiv.org/abs/2510.08700)
*Zhuolun Li,Haluk Sonmezler,Faiza Shirazi,Febin Shaji,Tymoteusz Mroczkowski,Dexter Lardner,Matthew Alain Camus,Evangelos Pournaras*

Main category: cs.CR

TL;DR: 提出集体安全投票概念，让选民自愿成为秘密持有者来保护选票机密性，设计并实现了基于区块链的实用系统，结合阈值密码学和智能合约，在保持透明可验证的同时确保选票机密性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化的大规模选举中确保选票机密性具有挑战性，需要平衡强机密性保证与现实适用性。

Method: 设计基于区块链的集体安全投票系统，结合阈值密码学和智能合约，选民通过直观界面参与，无需区块链知识。

Result: 用户测试显示选民愿意成为秘密持有者，可靠参与份额释放，对系统安全性信心高。

Conclusion: 选民能够集体维护机密性，这种实际部署是可行的。

Abstract: Ensuring ballot secrecy is critical for fair and trustworthy electronic
voting systems, yet achieving strong secrecy guarantees in decentralized,
large-scale elections remains challenging. This paper proposes the concept of
collectively secure voting, in which voters themselves can opt in as secret
holders to protect ballot secrecy. A practical blockchain-based collectively
secure voting system is designed and implemented. Our design strikes a balance
between strong confidentiality guarantees and real-world applicability. The
proposed system combines threshold cryptography and smart contracts to ensure
ballots remain confidential during voting, while all protocol steps remain
transparent and verifiable. Voters can use the system without prior blockchain
knowledge through an intuitive user interface that hides underlying complexity.
To evaluate this approach, a user testing is conducted. Results show a high
willingness to act as secret holders, reliable participation in share release,
and high security confidence in the proposed system. The findings demonstrate
that voters can collectively maintain secrecy and that such a practical
deployment is feasible.

</details>


### [212] [The Data Enclave Advantage: A New Paradigm for Least-Privileged Data Access in a Zero-Trust World](https://arxiv.org/abs/2510.09494)
*Nico Bistolfi,Andreea Georgescu,Dave Hodson*

Main category: cs.CR

TL;DR: 提出基于按需数据飞地的创新架构，实现数据层的零常驻权限和即时权限原则，用临时数据合约替代静态权限，大幅减少攻击面并防止权限蔓延。


<details>
  <summary>Details</summary>
Motivation: 随着云基础设施支持动态分布式工作流，特别是AI驱动流程加速发展，常驻权限模型已成为关键漏洞，导致灾难性云泄露。现有安全工具主要关注网络和API安全，但细粒度数据访问安全仍是挑战。

Method: 引入基于按需数据飞地的创新架构，用临时数据合约替代静态权限，实现数据层的零常驻权限和即时权限原则，围绕按需请求的数据构建分离机制，提供精确访问和实时监控。

Result: 该解决方案大幅减少攻击面，防止权限蔓延，简化审计流程，为企业向更安全、更有弹性的数据环境过渡提供关键路径。

Conclusion: 在数据层消除常驻权限与在网络层同样关键，特别是对于大规模处理有价值数据的公司。按需数据飞地架构为云数据安全提供了有效的解决方案。

Abstract: As cloud infrastructure evolves to support dynamic and distributed workflows,
accelerated now by AI-driven processes, the outdated model of standing
permissions has become a critical vulnerability. Based on the Cloud Security
Alliance (CSA) Top Threats to Cloud Computing Deep Dive 2025 Report, our
analysis details how standing permissions cause catastrophic cloud breaches.
While current security tools are addressing network and API security, the
challenge of securing granular data access remains. Removing standing
permissions at the data level is as critical as it is at the network level,
especially for companies handling valuable data at scale.
  In this white paper, we introduce an innovative architecture based on
on-demand data enclaves to address this gap directly. Our approach enables Zero
Standing Privilege (ZSP) and Just-in-Time (JIT) principles at the data level.
We replace static permissions with temporary data contracts that enforce
proactive protection. This means separation is built around the data requested
on-demand, providing precise access and real time monitoring for individual
records instead of datasets. This solution drastically reduces the attack
surface, prevents privilege creep, and simplifies auditing, offering a vital
path for enterprises to transition to a more secure and resilient data
environment.

</details>


### [213] [CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization](https://arxiv.org/abs/2510.08829)
*Debeshee Das,Luca Beurer-Kellner,Marc Fischer,Maximilian Baader*

Main category: cs.CR

TL;DR: 提出了一种基于令牌级消毒的方法来防御间接提示注入攻击，通过从工具输出中移除针对AI系统的指令，无需依赖样本级分类或上下文信息。


<details>
  <summary>Details</summary>
Motivation: LLM代理广泛使用工具和敏感数据，扩大了间接提示注入的攻击面。现有防御方法无法可靠区分恶意和良性指令，导致高误报率，阻碍实际应用。

Method: 采用令牌级消毒过程，从工具输出中精确移除针对AI系统的指令。该方法无需校准，与工具输出上下文无关，仅使用现成的指令调优数据进行训练。

Result: 在多个基准测试中显著降低攻击成功率（从34%降至3%），攻击成功率降低7-10倍，同时不影响代理在良性和恶意场景下的效用。

Conclusion: 基于计算机安全基本原则的方法能有效防御间接提示注入攻击，具有通用性强、无需校准、不影响代理功能等优势。

Abstract: The increasing adoption of LLM agents with access to numerous tools and
sensitive data significantly widens the attack surface for indirect prompt
injections. Due to the context-dependent nature of attacks, however, current
defenses are often ill-calibrated as they cannot reliably differentiate
malicious and benign instructions, leading to high false positive rates that
prevent their real-world adoption. To address this, we present a novel approach
inspired by the fundamental principle of computer security: data should not
contain executable instructions. Instead of sample-level classification, we
propose a token-level sanitization process, which surgically removes any
instructions directed at AI systems from tool outputs, capturing malicious
instructions as a byproduct. In contrast to existing safety classifiers, this
approach is non-blocking, does not require calibration, and is agnostic to the
context of tool outputs. Further, we can train such token-level predictors with
readily available instruction-tuning data only, and don't have to rely on
unrealistic prompt injection examples from challenges or of other synthetic
origin. In our experiments, we find that this approach generalizes well across
a wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB
and SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on
AgentDojo), without impairing agent utility in both benign and malicious
settings.

</details>


### [214] [Provable Watermarking for Data Poisoning Attacks](https://arxiv.org/abs/2510.09210)
*Yifan Zhu,Lijia Yu,Xiao-Shan Gao*

Main category: cs.CR

TL;DR: 本文提出在数据投毒攻击中使用水印方案，以解决无害投毒可能引发的误解问题。作者引入了两种可证明且实用的水印方法：投毒后水印和投毒并发水印，确保水印可检测性和投毒效用。


<details>
  <summary>Details</summary>
Motivation: 近年来，数据投毒攻击被设计得看似无害甚至有益，用于验证数据集所有权或保护私有数据。但这些发展可能引发误解和冲突，因为传统上数据投毒被视为机器学习系统的安全威胁。

Method: 提出了两种水印方法：1) 投毒后水印 - 水印长度为Θ(√d/ε_w)；2) 投毒并发水印 - 水印长度在Θ(1/ε_w²)到O(√d/ε_p)范围内。这些方法可证明确保水印可检测性和投毒效用。

Result: 理论分析表明，当水印长度满足特定条件时，水印投毒数据集能够同时保证水印可检测性和投毒效用。实验验证了理论发现在多种攻击、模型和数据集上的有效性。

Conclusion: 水印方案为数据投毒攻击提供了一个实用的解决方案，使无害投毒生成器能够声明其数据集所有权，帮助用户识别潜在投毒以防止误用。

Abstract: In recent years, data poisoning attacks have been increasingly designed to
appear harmless and even beneficial, often with the intention of verifying
dataset ownership or safeguarding private data from unauthorized use. However,
these developments have the potential to cause misunderstandings and conflicts,
as data poisoning has traditionally been regarded as a security threat to
machine learning systems. To address this issue, it is imperative for harmless
poisoning generators to claim ownership of their generated datasets, enabling
users to identify potential poisoning to prevent misuse. In this paper, we
propose the deployment of watermarking schemes as a solution to this challenge.
We introduce two provable and practical watermarking approaches for data
poisoning: {\em post-poisoning watermarking} and {\em poisoning-concurrent
watermarking}. Our analyses demonstrate that when the watermarking length is
$\Theta(\sqrt{d}/\epsilon_w)$ for post-poisoning watermarking, and falls within
the range of $\Theta(1/\epsilon_w^2)$ to $O(\sqrt{d}/\epsilon_p)$ for
poisoning-concurrent watermarking, the watermarked poisoning dataset provably
ensures both watermarking detectability and poisoning utility, certifying the
practicality of watermarking under data poisoning attacks. We validate our
theoretical findings through experiments on several attacks, models, and
datasets.

</details>


### [215] [GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis](https://arxiv.org/abs/2510.09260)
*Subrat Kishore Dutta,Yuelin Xu,Piyush Pant,Xiao Zhang*

Main category: cs.CR

TL;DR: GREAT是一个通过情感感知触发合成在RLHF中构建可泛化后门攻击的新框架，针对具有语义暴力请求和情感愤怒触发器的易受攻击用户群体。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF后门攻击方法依赖静态、稀有令牌触发器，在现实场景中效果有限，需要开发更通用的后门攻击方法。

Method: GREAT在潜在嵌入空间中使用主成分分析和聚类技术识别最具代表性的触发器，并利用Erinyes数据集（包含5000多个愤怒触发器）进行情感感知触发合成。

Result: 在基准RLHF数据集上的实验表明，GREAT在攻击成功率上显著优于基线方法，特别是在未见触发器场景中，同时保持良性输入的响应质量。

Conclusion: GREAT框架成功展示了在RLHF中构建可泛化后门攻击的有效性，为理解RLHF系统的安全漏洞提供了重要见解。

Abstract: Recent work has shown that RLHF is highly susceptible to backdoor attacks,
poisoning schemes that inject malicious triggers in preference data. However,
existing methods often rely on static, rare-token-based triggers, limiting
their effectiveness in realistic scenarios. In this paper, we develop GREAT, a
novel framework for crafting generalizable backdoors in RLHF through
emotion-aware trigger synthesis. Specifically, GREAT targets harmful response
generation for a vulnerable user subgroup characterized by both semantically
violent requests and emotionally angry triggers. At the core of GREAT is a
trigger identification pipeline that operates in the latent embedding space,
leveraging principal component analysis and clustering techniques to identify
the most representative triggers. To enable this, we present Erinyes, a
high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a
principled, hierarchical, and diversity-promoting approach. Experiments on
benchmark RLHF datasets demonstrate that GREAT significantly outperforms
baseline methods in attack success rates, especially for unseen trigger
scenarios, while largely preserving the response quality on benign inputs.

</details>


### [216] [Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](https://arxiv.org/abs/2510.09269)
*Zirun Zhou,Zhengyang Xiao,Haochuan Xu,Jing Sun,Di Wang,Jingfeng Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种针对视觉-语言-动作模型的目标导向后门攻击方法GoBA，通过在训练数据中注入物理对象作为触发器，使模型在遇到物理触发器时执行预定义的目标动作，而正常输入下表现正常。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖未筛选的训练数据存在安全隐患，当前后门攻击大多假设白盒访问且导致任务失败而非执行特定动作。本文揭示更实际的威胁：攻击者可通过在训练数据中注入物理对象作为触发器来操控VLA模型。

Method: 基于LIBERO基准构建BadLIBERO数据集，包含多样化的物理触发器和目标导向后门动作。提出三级评估方法，将受害VLA在GoBA下的动作分为三个状态：无事可做、尝试执行、成功执行。

Result: 实验显示，当物理触发器存在时，GoBA使受害VLA在97%的输入中成功实现后门目标，同时在干净输入上造成零性能下降。动作轨迹和触发器颜色显著影响攻击性能，而触发器尺寸影响很小。

Conclusion: GoBA攻击证明了VLA模型面临严重的安全威胁，攻击者可通过物理触发器操控模型行为。该研究强调了VLA模型安全性评估的重要性，并提供了BadLIBERO数据集用于进一步研究。

Abstract: Recent advances in vision-language-action (VLA) models have greatly improved
embodied AI, enabling robots to follow natural language instructions and
perform diverse tasks. However, their reliance on uncurated training datasets
raises serious security concerns. Existing backdoor attacks on VLAs mostly
assume white-box access and result in task failures instead of enforcing
specific actions. In this work, we reveal a more practical threat: attackers
can manipulate VLAs by simply injecting physical objects as triggers into the
training dataset. We propose goal-oriented backdoor attacks (GoBA), where the
VLA behaves normally in the absence of physical triggers but executes
predefined and goal-oriented actions in the presence of physical triggers.
Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO
that incorporates diverse physical triggers and goal-oriented backdoor actions.
In addition, we propose a three-level evaluation that categorizes the victim
VLA's actions under GoBA into three states: nothing to do, try to do, and
success to do. Experiments show that GoBA enables the victim VLA to
successfully achieve the backdoor goal in 97 percentage of inputs when the
physical trigger is present, while causing zero performance degradation on
clean inputs. Finally, by investigating factors related to GoBA, we find that
the action trajectory and trigger color significantly influence attack
performance, while trigger size has surprisingly little effect. The code and
BadLIBERO dataset are accessible via the project page at
https://goba-attack.github.io/.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [217] [Decoding Positive Selection in Mycobacterium tuberculosis with Phylogeny-Guided Graph Attention Models](https://arxiv.org/abs/2510.08703)
*Linfeng Wang,Susana Campino,Taane G. Clark,Jody E. Phelan*

Main category: q-bio.PE

TL;DR: 提出了一种将系统发育树转换为图结构的方法，使用图注意力网络检测结核分枝杆菌中的适应性突变，在耐药性基因变异分类上达到0.88准确率，并识别出41个跨谱系趋同出现的候选适应性变异。


<details>
  <summary>Details</summary>
Motivation: 阳性选择驱动结核分枝杆菌适应性突变的出现，影响耐药性、传播性和毒力。系统发育树捕获了分离株间的进化关系，为检测此类适应性信号提供了自然框架。

Method: 开发了系统发育引导的图注意力网络方法，将SNP注释的系统发育树转换为适合神经网络分析的图结构。节点代表分离株，边反映系统发育距离，节点特征编码SNP存在与否。GAT架构包含两个注意力层、残差连接、全局注意力池化和多层感知器分类器。

Result: 模型在保留测试集上达到0.88准确率。应用于146个WHO分类为"不确定"的变异时，识别出41个在多个谱系中趋同出现的候选变异，与适应性进化一致。

Conclusion: 这项工作证明了将系统发育树转换为GNN兼容结构的可行性，并强调基于注意力的模型是检测阳性选择的有效工具，有助于基因组监测和变异优先级排序。

Abstract: Positive selection drives the emergence of adaptive mutations in
Mycobacterium tuberculosis, shaping drug resistance, transmissibility, and
virulence. Phylogenetic trees capture evolutionary relationships among isolates
and provide a natural framework for detecting such adaptive signals. We present
a phylogeny-guided graph attention network (GAT) approach, introducing a method
for converting SNP-annotated phylogenetic trees into graph structures suitable
for neural network analysis. Using 500 M. tuberculosis isolates from four major
lineages and 249 single-nucleotide variants (84 resistance-associated and 165
neutral) across 61 drug-resistance genes, we constructed graphs where nodes
represented isolates and edges reflected phylogenetic distances. Edges between
isolates separated by more than seven internal nodes were pruned to emphasise
local evolutionary structure. Node features encoded SNP presence or absence,
and the GAT architecture included two attention layers, a residual connection,
global attention pooling, and a multilayer perceptron classifier. The model
achieved an accuracy of 0.88 on a held-out test set and, when applied to 146
WHO-classified "uncertain" variants, identified 41 candidates with convergent
emergence across multiple lineages, consistent with adaptive evolution. This
work demonstrates the feasibility of transforming phylogenies into
GNN-compatible structures and highlights attention-based models as effective
tools for detecting positive selection, aiding genomic surveillance and variant
prioritisation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [218] [Gradient-Guided Furthest Point Sampling for Robust Training Set Selection](https://arxiv.org/abs/2510.08906)
*Morris Trestman,Stefan Gugler,Felix A. Faber,O. A. von Lilienfeld*

Main category: stat.ML

TL;DR: 提出了GGFPS方法，利用分子力梯度指导构型空间采样，相比FPS和均匀采样，在数据效率和预测鲁棒性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 智能训练集选择方法可以减少数据需求并提高化学相关机器学习问题的预测鲁棒性。传统FPS方法在分子构型采样中存在不足。

Method: GGFPS（梯度引导最远点采样），是FPS的简单扩展，利用分子力范数来指导分子构型空间的高效采样。

Result: 在Styblinski-Tang函数和MD17数据集上的实验表明：GGFPS相比FPS可减少高达两倍的训练成本；对平衡和应变结构都降低预测误差；在所有MD17构型空间中都系统性地减小了预测误差方差。

Conclusion: 梯度感知采样方法作为有效的训练集选择工具具有巨大潜力，而简单使用FPS可能导致训练不平衡和预测结果不一致。

Abstract: Smart training set selections procedures enable the reduction of data needs
and improves predictive robustness in machine learning problems relevant to
chemistry. We introduce Gradient Guided Furthest Point Sampling (GGFPS), a
simple extension of Furthest Point Sampling (FPS) that leverages molecular
force norms to guide efficient sampling of configurational spaces of molecules.
Numerical evidence is presented for a toy-system (Styblinski-Tang function) as
well as for molecular dynamics trajectories from the MD17 dataset. Compared to
FPS and uniform sampling, our numerical results indicate superior data
efficiency and robustness when using GGFPS. Distribution analysis of the MD17
data suggests that FPS systematically under-samples equilibrium geometries,
resulting in large test errors for relaxed structures. GGFPS cures this
artifact and (i) enables up to two fold reductions in training cost without
sacrificing predictive accuracy compared to FPS in the 2-dimensional
Styblinksi-Tang system, (ii) systematically lowers prediction errors for
equilibrium as well as strained structures in MD17, and (iii) systematically
decreases prediction error variances across all of the MD17 configuration
spaces. These results suggest that gradient-aware sampling methods hold great
promise as effective training set selection tools, and that naive use of FPS
may result in imbalanced training and inconsistent prediction outcomes.

</details>


### [219] [A Representer Theorem for Hawkes Processes via Penalized Least Squares Minimization](https://arxiv.org/abs/2510.08916)
*Hideaki Kim,Tomoharu Iwata*

Main category: stat.ML

TL;DR: 本文提出了一种新的表示定理形式，用于线性多元霍克斯过程中的潜在触发核估计，通过变换核和固定对偶系数为1的方式，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统表示定理将无限维优化问题转化为有限维对偶系数优化，但在霍克斯过程触发核估计中，求解对偶系数仍然计算昂贵。本文旨在开发更高效的核方法估计器。

Method: 在惩罚最小二乘最小化原则下，通过联立积分方程组定义变换核族，将每个触发核的最优估计表示为这些变换核在数据点处的线性组合，且所有对偶系数固定为1。

Result: 在合成数据集上的实证评估表明，该方法在保持竞争性预测精度的同时，相比现有最先进的基于核方法的估计器，显著提高了计算效率。

Conclusion: 提出的新型表示定理形式为大规模数据的霍克斯过程触发核估计提供了高效解决方案，避免了昂贵的对偶系数优化问题。

Abstract: The representer theorem is a cornerstone of kernel methods, which aim to
estimate latent functions in reproducing kernel Hilbert spaces (RKHSs) in a
nonparametric manner. Its significance lies in converting inherently
infinite-dimensional optimization problems into finite-dimensional ones over
dual coefficients, thereby enabling practical and computationally tractable
algorithms. In this paper, we address the problem of estimating the latent
triggering kernels--functions that encode the interaction structure between
events--for linear multivariate Hawkes processes based on observed event
sequences within an RKHS framework. We show that, under the principle of
penalized least squares minimization, a novel form of representer theorem
emerges: a family of transformed kernels can be defined via a system of
simultaneous integral equations, and the optimal estimator of each triggering
kernel is expressed as a linear combination of these transformed kernels
evaluated at the data points. Remarkably, the dual coefficients are all
analytically fixed to unity, obviating the need to solve a costly optimization
problem to obtain the dual coefficients. This leads to a highly efficient
estimator capable of handling large-scale data more effectively than
conventional nonparametric approaches. Empirical evaluations on synthetic
datasets reveal that the proposed method attains competitive predictive
accuracy while substantially improving computational efficiency over existing
state-of-the-art kernel method-based estimators.

</details>


### [220] [Mirror Flow Matching with Heavy-Tailed Priors for Generative Modeling on Convex Domains](https://arxiv.org/abs/2510.08929)
*Yunrui Guan,Krishnakumar Balasubramanian,Shiqian Ma*

Main category: stat.ML

TL;DR: 提出基于正则化镜像映射和Student-t先验的镜像流匹配方法，解决凸域生成建模中的两个关键挑战：标准对数障碍镜像映射导致的厚尾分布问题，以及高斯先验与厚尾目标不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 研究凸域上的生成建模，发现标准对数障碍镜像映射会导致厚尾对偶分布，造成动力学不适定；同时高斯先验与厚尾目标耦合效果差。

Method: 提出镜像流匹配方法，使用正则化镜像映射控制对偶尾部行为并保证有限矩，结合Student-t先验与厚尾目标对齐并稳定训练。

Result: 理论上保证了速度场的空间Lipschitz性和时间正则性，给出了Student-t先验流匹配的Wasserstein收敛速率和约束生成的原始空间保证。实证表明在合成凸域模拟中优于基线方法，在真实世界约束生成任务中达到竞争性样本质量。

Conclusion: 所提出的镜像流匹配方法有效解决了凸域生成建模中的关键挑战，在理论和实证上都表现出优越性能。

Abstract: We study generative modeling on convex domains using flow matching and mirror
maps, and identify two fundamental challenges. First, standard log-barrier
mirror maps induce heavy-tailed dual distributions, leading to ill-posed
dynamics. Second, coupling with Gaussian priors performs poorly when matching
heavy-tailed targets. To address these issues, we propose Mirror Flow Matching
based on a \emph{regularized mirror map} that controls dual tail behavior and
guarantees finite moments, together with coupling to a Student-$t$ prior that
aligns with heavy-tailed targets and stabilizes training. We provide
theoretical guarantees, including spatial Lipschitzness and temporal regularity
of the velocity field, Wasserstein convergence rates for flow matching with
Student-$t$ priors and primal-space guarantees for constrained generation,
under $\varepsilon$-accurate learned velocity fields. Empirically, our method
outperforms baselines in synthetic convex-domain simulations and achieves
competitive sample quality on real-world constrained generative tasks.

</details>


### [221] [Distributionally robust approximation property of neural networks](https://arxiv.org/abs/2510.09177)
*Mihriban Ceylan,David J. Prömel*

Main category: stat.ML

TL;DR: 该论文证明了多种神经网络在弱紧测度族上具有一致通用逼近性质，扩展了传统L^p空间中的逼近定理到Orlicz空间。


<details>
  <summary>Details</summary>
Motivation: 传统通用逼近定理主要局限于L^p空间，需要扩展到更一般的函数空间，特别是Orlicz空间，以适应更广泛的测度族。

Method: 通过证明神经网络在Orlicz空间中稠密，建立对弱紧测度族的一致通用逼近性质，涵盖前馈网络、深度窄网络和函数输入网络等架构。

Result: 成功证明了多种神经网络在Orlicz空间中稠密，从而获得对弱紧测度族的一致通用逼近性质。

Conclusion: 该研究将神经网络通用逼近理论扩展到Orlicz空间，为更广泛的函数逼近问题提供了理论基础。

Abstract: The universal approximation property uniformly with respect to weakly compact
families of measures is established for several classes of neural networks. To
that end, we prove that these neural networks are dense in Orlicz spaces,
thereby extending classical universal approximation theorems even beyond the
traditional $L^p$-setting. The covered classes of neural networks include
widely used architectures like feedforward neural networks with non-polynomial
activation functions, deep narrow networks with ReLU activation functions and
functional input neural networks.

</details>


### [222] [A unified Bayesian framework for adversarial robustness](https://arxiv.org/abs/2510.09288)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: 提出了一种基于贝叶斯框架的形式化方法，通过随机信道建模对抗性不确定性，提供主动和被动两种防御策略。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性防御方法通常采用确定性方法最小化最坏情况损失，但未考虑攻击者的不确定性。现有的随机防御缺乏统计严谨性且未明确其基本假设。

Method: 引入形式化贝叶斯框架，通过随机信道建模对抗性不确定性，明确所有概率假设。提供两种鲁棒化策略：训练期间的主动防御（类似对抗训练）和操作期间的被动防御（类似对抗净化）。

Result: 经验验证表明该方法能有效建模对抗性不确定性，多个先前防御方法可作为该模型的极限情况被恢复。

Conclusion: 明确建模对抗性不确定性具有显著优势，提出的贝叶斯框架为对抗性防御提供了统计严谨的基础。

Abstract: The vulnerability of machine learning models to adversarial attacks remains a
critical security challenge. Traditional defenses, such as adversarial
training, typically robustify models by minimizing a worst-case loss. However,
these deterministic approaches do not account for uncertainty in the
adversary's attack. While stochastic defenses placing a probability
distribution on the adversary exist, they often lack statistical rigor and fail
to make explicit their underlying assumptions. To resolve these issues, we
introduce a formal Bayesian framework that models adversarial uncertainty
through a stochastic channel, articulating all probabilistic assumptions. This
yields two robustification strategies: a proactive defense enacted during
training, aligned with adversarial training, and a reactive defense enacted
during operations, aligned with adversarial purification. Several previous
defenses can be recovered as limiting cases of our model. We empirically
validate our methodology, showcasing the benefits of explicitly modeling
adversarial uncertainty.

</details>


### [223] [Efficient Autoregressive Inference for Transformer Probabilistic Models](https://arxiv.org/abs/2510.09477)
*Conor Hassan,Nasrulloh Loka,Cen-You Li,Daolang Huang,Paul E. Chang,Yang Yang,Francesco Silvestrin,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: 提出了一种因果自回归缓冲区方法，将基于集合的条件化与自回归生成相结合，在保持元学习能力的同时实现高效的联合分布采样。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的概率推理模型在单次边际预测上表现优异，但缺乏生成连贯联合分布的能力。纯自回归架构能生成联合分布但牺牲了灵活的条件化能力，而标准方法需要昂贵的重复编码。

Method: 引入因果自回归缓冲区，将上下文编码与条件集更新解耦。模型一次性处理并缓存上下文，动态缓冲区捕获目标依赖关系，支持批量自回归生成和一次性联合对数似然评估。

Result: 在合成函数、EEG信号、认知模型和表格数据上，该方法在保持预测准确性的同时，联合采样速度提升高达20倍。

Conclusion: 该方法结合了自回归生成模型的效率和基于集合条件化的表示能力，使基于Transformer的概率模型能够实用化地进行联合预测。

Abstract: Transformer-based models for amortized probabilistic inference, such as
neural processes, prior-fitted networks, and tabular foundation models, excel
at single-pass marginal prediction. However, many real-world applications, from
signal interpolation to multi-column tabular predictions, require coherent
joint distributions that capture dependencies between predictions. While purely
autoregressive architectures efficiently generate such distributions, they
sacrifice the flexible set-conditioning that makes these models powerful for
meta-learning. Conversely, the standard approach to obtain joint distributions
from set-based models requires expensive re-encoding of the entire augmented
conditioning set at each autoregressive step. We introduce a causal
autoregressive buffer that preserves the advantages of both paradigms. Our
approach decouples context encoding from updating the conditioning set. The
model processes the context once and caches it. A dynamic buffer then captures
target dependencies: as targets are incorporated, they enter the buffer and
attend to both the cached context and previously buffered targets. This enables
efficient batched autoregressive generation and one-pass joint log-likelihood
evaluation. A unified training strategy allows seamless integration of
set-based and autoregressive modes at minimal additional cost. Across synthetic
functions, EEG signals, cognitive models, and tabular data, our method matches
predictive accuracy of strong baselines while delivering up to 20 times faster
joint sampling. Our approach combines the efficiency of autoregressive
generative models with the representational power of set-based conditioning,
making joint prediction practical for transformer-based probabilistic models.

</details>


### [224] [Interpretable Generative and Discriminative Learning for Multimodal and Incomplete Clinical Data](https://arxiv.org/abs/2510.09513)
*Albert Belenguer-Llorens,Carlos Sevilla-Salcedo,Janaina Mourao-Miranda,Vanessa Gómez-Verdejo*

Main category: stat.ML

TL;DR: 提出一种贝叶斯方法处理多模态临床数据，结合生成式和判别式策略，自动补全缺失视图并实现稳健推理。


<details>
  <summary>Details</summary>
Motivation: 现实临床问题常面临多模态数据不完整、样本量有限等挑战，传统机器学习算法难以有效处理。

Method: 采用贝叶斯方法，集成生成式建模捕捉跨视图关系（半监督策略）和判别式任务导向建模识别下游任务相关信息。

Result: 能够捕捉和解缠生物、心理和社会人口等多模态间的复杂交互作用。

Conclusion: 该生成-判别双框架既提供通用理解又支持任务特定洞察，在多模态临床数据中表现出显著潜力。

Abstract: Real-world clinical problems are often characterized by multimodal data,
usually associated with incomplete views and limited sample sizes in their
cohorts, posing significant limitations for machine learning algorithms. In
this work, we propose a Bayesian approach designed to efficiently handle these
challenges while providing interpretable solutions. Our approach integrates (1)
a generative formulation to capture cross-view relationships with a
semi-supervised strategy, and (2) a discriminative task-oriented formulation to
identify relevant information for specific downstream objectives. This dual
generative-discriminative formulation offers both general understanding and
task-specific insights; thus, it provides an automatic imputation of the
missing views while enabling robust inference across different data sources.
The potential of this approach becomes evident when applied to the multimodal
clinical data, where our algorithm is able to capture and disentangle the
complex interactions among biological, psychological, and sociodemographic
modalities.

</details>


### [225] [Conditional Flow Matching for Bayesian Posterior Inference](https://arxiv.org/abs/2510.09534)
*So Won Jeong,Percy S. Zhai,Veronika Ročová*

Main category: stat.ML

TL;DR: 提出了一种基于流匹配的生成式多元后验采样器，无需似然评估，通过块三角速度场实现从源分布到目标后验的确定性传输，并能快速生成贝叶斯可信集。


<details>
  <summary>Details</summary>
Motivation: 现有后验采样方法需要似然评估或计算复杂，本文旨在开发一种更简单、计算更轻量的生成式后验采样方法。

Method: 使用流匹配方法，在数据和参数的联合空间中学习动态块三角速度场，构建从源分布到后验的确定性传输映射。

Result: 该方法比基于GAN和扩散的方法计算更轻量，能捕捉复杂的后验结构，并能快速生成贝叶斯可信集。

Conclusion: 提出的流匹配后验采样器具有理论保证，能一致地恢复后验分布和贝叶斯可信集，为贝叶斯推断提供了高效工具。

Abstract: We propose a generative multivariate posterior sampler via flow matching. It
offers a simple training objective, and does not require access to likelihood
evaluation. The method learns a dynamic, block-triangular velocity field in the
joint space of data and parameters, which results in a deterministic transport
map from a source distribution to the desired posterior. The inverse map, named
vector rank, is accessible by reversibly integrating the velocity over time. It
is advantageous to leverage the dynamic design: proper constraints on the
velocity yield a monotone map, which leads to a conditional Brenier map,
enabling a fast and simultaneous generation of Bayesian credible sets whose
contours correspond to level sets of Monge-Kantorovich data depth. Our approach
is computationally lighter compared to GAN-based and diffusion-based
counterparts, and is capable of capturing complex posterior structures.
Finally, frequentist theoretical guarantee on the consistency of the recovered
posterior distribution, and of the corresponding Bayesian credible sets, is
provided.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [226] [Generating CodeMeta using declarative mapping rules: An open-ended approach using ShExML](https://arxiv.org/abs/2510.09172)
*Herminio García-González*

Main category: cs.DL

TL;DR: 提出使用声明式映射规则生成CodeMeta文件的方法，通过ShExML实现三个交叉映射，自动化生成研究软件元数据，提高软件FAIR性。


<details>
  <summary>Details</summary>
Motivation: 研究软件需要遵循FAIR原则以实现完全可复现性，但现有工具在灵活性和适应性方面存在不足，无法统一生成标准化的CodeMeta元数据文件。

Method: 使用声明式映射规则和ShExML语言实现三个交叉映射，扩展合并后为两个研究软件工件生成CodeMeta文件，并通过SHACL和ShEx进行验证，自动化整个生成流程。

Result: 成功实现了CodeMeta文件的自动化生成，验证了输出结果的正确性，整个工作流程在软件新版本发布时只需最少用户干预。

Conclusion: 该工作为其他开发者提供了CodeMeta生成工作流的参考示例，有助于促进CodeMeta的采用，最终提高研究软件的FAIR性。

Abstract: Nowadays, software is one of the cornerstones when conducting research in
several scientific fields which employ computer-based methodologies to answer
new research questions. However, for these experiments to be completely
reproducible, research software should comply with the FAIR principles, yet its
metadata can be represented following different data models and spread across
different locations. In order to bring some cohesion to the field, CodeMeta was
proposed as a vocabulary to represent research software metadata in a unified
and standardised manner. While existing tools can help users to generate
CodeMeta files for some specific use cases, they fall short on flexibility and
adaptability. Hence, in this work, I propose the use of declarative mapping
rules to generate CodeMeta files, illustrated through the implementation of
three crosswalks in ShExML which are then expanded and merged to cover the
generation of CodeMeta files for two existing research software artefacts.
Moreover, the outputs are validated using SHACL and ShEx and the whole
generation workflow is automated requiring minimal user intervention upon a new
version release. This work can, therefore, be used as an example upon which
other developers can include a CodeMeta generation workflow in their
repositories, facilitating the adoption of CodeMeta and, ultimately, increasing
research software FAIRness.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [227] [Concept-Based Generic Programming in C++](https://arxiv.org/abs/2510.08969)
*Bjarne Stroustrup*

Main category: cs.PL

TL;DR: 本文介绍C++概念编程技术，展示如何使用概念约束泛型代码，构建简单类型系统消除窄化转换并提供范围检查，同时保持零运行时开销。


<details>
  <summary>Details</summary>
Motivation: 展示C++概念编程的实用性和基本原理，说明概念如何作为泛型编程的核心机制，提供用户自定义类型系统扩展，而不是详细解释语言特性或标准库支持。

Method: 使用概念作为约束泛型代码的主要手段，构建简单类型系统来消除窄化转换和提供范围检查，利用概念实现用户定义的类型系统扩展。

Result: 开发出能够消除窄化转换、提供范围检查的类型系统，且不引入不必要的符号或运行时开销，展示了概念在泛型编程中的实际效用。

Conclusion: 概念是C++泛型编程的核心组成部分，支持通用编程和泛型编程的统一表示法，文章还提供了概念设计的关键原理和起源分析。

Abstract: We present programming techniques to illustrate the facilities and principles
of C++ generic programming using concepts. Concepts are C++'s way to express
constraints on generic code. As an initial example, we provide a simple type
system that eliminates narrowing conversions and provides range checking
without unnecessary notational or run-time overheads. Concepts are used
throughout to provide user-defined extensions to the type system. The aim is to
show their utility and the fundamental ideas behind them, rather than to
provide a detailed or complete explanation of C++'s language support for
generic programming or the extensive support provided by the standard library.
Generic programming is an integral part of C++, rather than an isolated
sub-language. In particular, key facilities support general programming as well
as generic programming (e.g., uniform notation for types, lambdas, variadic
templates, and C++26 static reflection). Finally, we give design rationales and
origins for key parts of the concept design, including use patterns, the
relationship to Object-Oriented Programming, value arguments, notation, concept
type-matching, and definition checking.

</details>


### [228] [Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs](https://arxiv.org/abs/2510.08726)
*Yifan Zhao,Egan Johnson,Prasanth Chatarasi,Vikram Adve,Sasa Misailovic*

Main category: cs.PL

TL;DR: Neptune是一个张量编译器，通过打破循环依赖并构建代数校正表达式，实现了复杂归约计算的先进算子融合，在注意力机制等任务上优于现有编译器。


<details>
  <summary>Details</summary>
Motivation: 现有张量编译器难以融合涉及循环依赖的复杂归约计算（如注意力机制），这限制了深度学习算子的优化效果。

Method: Neptune引入了一种新的先进算子融合方法：故意打破某些现有依赖关系，并通过构建代数校正表达式来补偿，确保内核产生正确结果。

Result: 在10个基于注意力的基准测试中，Neptune在四种不同GPU架构上平均比次优替代方案快1.35倍，超越了Triton、TVM和FlexAttention等现有编译器。

Conclusion: Neptune通过创新的依赖关系处理和代数校正方法，有效解决了复杂归约计算的融合问题，为深度学习工作负载提供了显著的性能提升。

Abstract: Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [229] [Smart navigation of a gravity-driven glider with adjustable centre-of-mass](https://arxiv.org/abs/2510.09250)
*X. Jiang,J. Qiu,K. Gustavsson,B. Mehlig,L. Zhao*

Main category: physics.flu-dyn

TL;DR: 人工滑翔机通过动态调整质心位置实现精确导航，研究发现两种最优导航策略取决于雷诺数：大雷诺数时通过快速翻滚产生惯性升力，小雷诺数时通过稳定倾斜姿态产生粘性力。


<details>
  <summary>Details</summary>
Motivation: 人工滑翔机在流体中沉降时需要精确导航到达目标位置，研究如何通过调整质心实现最优导航策略。

Method: 使用完全解析的直接数值模拟（DNS）和强化学习，分析滑翔机在不同雷诺数下的导航策略。

Result: 发现两种最优导航策略：大雷诺数时通过快速翻滚产生大水平惯性升力实现远距离移动；小雷诺数时通过稳定倾斜姿态产生较小的水平粘性力。

Conclusion: 最优导航策略强烈依赖于雷诺数，大雷诺数下惯性效应主导，小雷诺数下粘性效应主导，这解释了滑翔机在不同流体环境中的导航机制。

Abstract: Artificial gliders are designed to disperse as they settle through a fluid,
requiring precise navigation to reach target locations. We show that a compact
glider settling in a viscous fluid can navigate by dynamically adjusting its
centre-of-mass. Using fully resolved direct numerical simulations (DNS) and
reinforcement learning, we find two optimal navigation strategies that allow
the glider to reach its target location accurately. These strategies depend
sensitively on how the glider interacts with the surrounding fluid. The nature
of this interaction changes as the particle Reynolds number Re$_p$ changes. Our
results explain how the optimal strategy depends on Re$_p$. At large Re$_p$,
the glider learns to tumble rapidly by moving its centre-of-mass as its
orientation changes. This generates a large horizontal inertial lift force,
which allows the glider to travel far. At small Re$_p$, by contrast, high
viscosity hinders tumbling. In this case, the glider learns to adjust its
centre-of-mass so that it settles with a steady, inclined orientation that
results in a horizontal viscous force. The horizontal range is much smaller
than for large Re$_p$, because this viscous force is much smaller than the
inertial lift force at large Re$_p$.
  *These authors contributed equally.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [230] [Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model](https://arxiv.org/abs/2510.09415)
*Beige Jerry Jin,Leila Wehbe*

Main category: q-bio.NC

TL;DR: 提出基于Transformer的编码模型，结合MEG和fMRI数据，在自然语音理解实验中重建高时空分辨率的皮层源响应。


<details>
  <summary>Details</summary>
Motivation: 解决当前非侵入神经成像技术中空间分辨率与时间分辨率之间的权衡问题，现有源定位或MEG-fMRI融合方法无法在单试次自然数据中同时保持高分辨率。

Method: 开发基于Transformer的编码模型，同时训练预测多个被试的MEG和fMRI数据，通过潜在层表示重建的皮层源估计。

Result: 模型预测MEG优于单模态编码模型标准，在模拟实验中比经典最小范数解具有更高的时空保真度，估计的源活动在新数据集中预测ECoG优于ECoG训练模型。

Conclusion: 通过整合大型自然实验、MEG、fMRI和编码模型的力量，提出实现毫秒-毫米级脑映射的实用途径。

Abstract: Current non-invasive neuroimaging techniques trade off between spatial
resolution and temporal resolution. While magnetoencephalography (MEG) can
capture rapid neural dynamics and functional magnetic resonance imaging (fMRI)
can spatially localize brain activity, a unified picture that preserves both
high resolutions remains an unsolved challenge with existing source
localization or MEG-fMRI fusion methods, especially for single-trial
naturalistic data. We collected whole-head MEG when subjects listened passively
to more than seven hours of narrative stories, using the same stimuli in an
open fMRI dataset (LeBel et al., 2023). We developed a transformer-based
encoding model that combines the MEG and fMRI from these two naturalistic
speech comprehension experiments to estimate latent cortical source responses
with high spatiotemporal resolution. Our model is trained to predict MEG and
fMRI from multiple subjects simultaneously, with a latent layer that represents
our estimates of reconstructed cortical sources. Our model predicts MEG better
than the common standard of single-modality encoding models, and it also yields
source estimates with higher spatial and temporal fidelity than classic
minimum-norm solutions in simulation experiments. We validated the estimated
latent sources by showing its strong generalizability across unseen subjects
and modalities. Estimated activity in our source space predict
electrocorticography (ECoG) better than an ECoG-trained encoding model in an
entirely new dataset. By integrating the power of large naturalistic
experiments, MEG, fMRI, and encoding models, we propose a practical route
towards millisecond-and-millimeter brain mapping.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [231] [Physically Valid Biomolecular Interaction Modeling with Gauss-Seidel Projection](https://arxiv.org/abs/2510.08946)
*Siyuan Chen,Minghao Guo,Caoliwen Wang,Anka He Chen,Yikun Zhang,Jingjing Chai,Yin Yang,Wojciech Matusik,Peter Yichen Chen*

Main category: q-bio.BM

TL;DR: 提出一种可微分投影模块，通过Gauss-Seidel方案将扩散模型生成的原子坐标映射到最近的物理有效构型，确保生物分子相互作用的物理有效性


<details>
  <summary>Details</summary>
Motivation: 现有基础模型生成的生物分子全原子结构经常违反基本的空间可行性限制，需要解决物理有效性问题

Method: 使用可微分投影模块，通过Gauss-Seidel方案利用约束的局部性和稀疏性，实现稳定快速收敛，并通过隐式微分获得梯度

Result: 仅需2步去噪即可生成物理有效且结构准确的生物分子复合物，在6个基准测试中达到与200步扩散基线相同的结构精度，速度提升约10倍

Conclusion: 该方法成功将物理有效性作为严格约束集成到训练和推理中，显著提高了生物分子相互作用建模的效率和可靠性

Abstract: Biomolecular interaction modeling has been substantially advanced by
foundation models, yet they often produce all-atom structures that violate
basic steric feasibility. We address this limitation by enforcing physical
validity as a strict constraint during both training and inference with a
uniffed module. At its core is a differentiable projection that maps the
provisional atom coordinates from the diffusion model to the nearest physically
valid conffguration. This projection is achieved using a Gauss-Seidel scheme,
which exploits the locality and sparsity of the constraints to ensure stable
and fast convergence at scale. By implicit differentiation to obtain gradients,
our module integrates seamlessly into existing frameworks for end-to-end
ffnetuning. With our Gauss-Seidel projection module in place, two denoising
steps are sufffcient to produce biomolecular complexes that are both physically
valid and structurally accurate. Across six benchmarks, our 2-step model
achieves the same structural accuracy as state-of-the-art 200-step diffusion
baselines, delivering approximately 10 times faster wall-clock speed while
guaranteeing physical validity.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [232] [A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis](https://arxiv.org/abs/2510.09365)
*Valentin Biller,Lucas Zimmer,Can Erdur,Sandeep Nagar,Daniel Rückert,Niklas Bubeck,Jonas Weidner*

Main category: eess.IV

TL;DR: 本文提出了首个基于体素级连续肿瘤浓度生成高保真脑肿瘤MRI的生成模型，并应用于BraTS 2025修复挑战赛，通过将肿瘤浓度设为零实现健康组织修复。


<details>
  <summary>Details</summary>
Motivation: 磁共振成像修复支持众多临床和研究应用，需要开发能够生成空间一致且解剖学准确的3D图像的模型。

Method: 使用基于组织分割和肿瘤浓度条件的潜在扩散模型，生成3D空间一致且解剖学一致的图像，适用于肿瘤合成和健康组织修复。

Result: 健康组织修复的PSNR为18.5，肿瘤修复的PSNR为17.4，表明模型能够生成高质量的修复结果。

Conclusion: 该潜在扩散模型在脑肿瘤MRI生成和健康组织修复方面表现出色，为临床和研究应用提供了有效的工具。

Abstract: Magnetic resonance imaging (MRI) inpainting supports numerous clinical and
research applications. We introduce the first generative model that conditions
on voxel-level, continuous tumor concentrations to synthesize high-fidelity
brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this
architecture to the complementary task of healthy tissue restoration by setting
the tumor concentrations to zero. Our latent diffusion model conditioned on
both tissue segmentations and the tumor concentrations generates 3D spatially
coherent and anatomically consistent images for both tumor synthesis and
healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5,
and for tumor inpainting, we achieve 17.4. Our code is available at:
https://github.com/valentin-biller/ldm.git

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [233] [Evolutionary Computation as Natural Generative AI](https://arxiv.org/abs/2510.08590)
*Yaxin Shi,Abhishek Gupta,Ying Wu,Melvin Wong,Ivor Tsang,Thiago Rios,Stefan Menzel,Bernhard Sendhoff,Yaqing Hou,Yew-Soon Ong*

Main category: cs.NE

TL;DR: 该论文将进化计算重新定义为自然生成AI，提出通过探索性搜索和自然选择来增强生成多样性，突破传统生成AI的数据限制。


<details>
  <summary>Details</summary>
Motivation: 传统生成AI受限于有限训练数据和局部梯度信号，产生的结果往往缺乏真正创造性。进化计算提供了超越数据限制的搜索驱动路径，能够探索未知解空间。

Method: 将进化计算重新定义为自然生成AI，使用父中心算子模拟传统生成AI，通过破坏性算子实现结构化进化跳跃，并利用进化多任务方法整合跨域特征重组和适度选择机制。

Result: 破坏性进化计算能够在几代内生成分布外的人工制品，进化多任务方法能够持续促进创新，实现超越常规边界的生成能力。

Conclusion: 将进化计算重新定义为自然生成AI，强调结构化破坏和选择压力调节是创造力的关键驱动因素，为生成AI时代的探索性设计、创新和科学发现提供了重要途径。

Abstract: Generative AI (GenAI) has achieved remarkable success across a range of
domains, but its capabilities remain constrained to statistical models of
finite training sets and learning based on local gradient signals. This often
results in artifacts that are more derivative than genuinely generative. In
contrast, Evolutionary Computation (EC) offers a search-driven pathway to
greater diversity and creativity, expanding generative capabilities by
exploring uncharted solution spaces beyond the limits of available data. This
work establishes a fundamental connection between EC and GenAI, redefining EC
as Natural Generative AI (NatGenAI) -- a generative paradigm governed by
exploratory search under natural selection. We demonstrate that classical EC
with parent-centric operators mirrors conventional GenAI, while disruptive
operators enable structured evolutionary leaps, often within just a few
generations, to generate out-of-distribution artifacts. Moreover, the methods
of evolutionary multitasking provide an unparalleled means of integrating
disruptive EC (with cross-domain recombination of evolved features) and
moderated selection mechanisms (allowing novel solutions to survive), thereby
fostering sustained innovation. By reframing EC as NatGenAI, we emphasize
structured disruption and selection pressure moderation as essential drivers of
creativity. This perspective extends the generative paradigm beyond
conventional boundaries and positions EC as crucial to advancing exploratory
design, innovation, scientific discovery, and open-ended generation in the
GenAI era.

</details>


### [234] [The Enduring Dominance of Deep Neural Networks: A Critical Analysis of the Fundamental Limitations of Quantum Machine Learning and Spiking Neural Networks](https://arxiv.org/abs/2510.08591)
*Takehiro Ishikawa*

Main category: cs.NE

TL;DR: 量子机器学习和脉冲神经网络不太可能在短期内取代深度神经网络，因为它们面临技术挑战和效率问题，而DNNs凭借高效的反向传播、强大的正则化和推理时计算扩展等优势保持主导地位。


<details>
  <summary>Details</summary>
Motivation: 分析量子机器学习和脉冲神经网络作为DNNs替代技术的可行性，评估它们在实际应用中的局限性和挑战。

Method: 通过对比分析QML和SNNs的技术瓶颈（如量子硬件限制、测量开销、表示带宽限制等）与DNNs的技术优势（反向传播效率、正则化技术、推理时计算扩展等）。

Result: QML和SNNs面临严重的技术障碍，而DNNs通过优化和专用硬件持续提升性能，近期模型如Grok-4 Heavy和gpt-oss-120b证明了DNNs的优越性。

Conclusion: QML和SNNs可能在某些混合应用中发挥作用，但DNNs仍然是AI发展的主导和实用范式。

Abstract: Recent advancements in QML and SNNs have generated considerable excitement,
promising exponential speedups and brain-like energy efficiency to
revolutionize AI. However, this paper argues that they are unlikely to displace
DNNs in the near term. QML struggles with adapting backpropagation due to
unitary constraints, measurement-induced state collapse, barren plateaus, and
high measurement overheads, exacerbated by the limitations of current noisy
intermediate-scale quantum hardware, overfitting risks due to underdeveloped
regularization techniques, and a fundamental misalignment with machine
learning's generalization. SNNs face restricted representational bandwidth,
struggling with long-range dependencies and semantic encoding in language tasks
due to their discrete, spike-based processing. Furthermore, the goal of
faithfully emulating the brain might impose inherent inefficiencies like
cognitive biases, limited working memory, and slow learning speeds. Even their
touted energy-efficient advantages are overstated; optimized DNNs with
quantization can outperform SNNs in energy costs under realistic conditions.
Finally, SNN training incurs high computational overhead from temporal
unfolding. In contrast, DNNs leverage efficient backpropagation, robust
regularization, and innovations in LRMs that shift scaling to inference-time
compute, enabling self-improvement via RL and search algorithms like MCTS while
mitigating data scarcity. This superiority is evidenced by recent models such
as xAI's Grok-4 Heavy, which advances SOTA performance, and gpt-oss-120b, which
surpasses or approaches the performance of leading industry models despite its
modest 120-billion-parameter size deployable on a single 80GB GPU. Furthermore,
specialized ASICs amplify these efficiency gains. Ultimately, QML and SNNs may
serve niche hybrid roles, but DNNs remain the dominant, practical paradigm for
AI advancement.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [235] [Understanding Exoplanet Habitability: A Bayesian ML Framework for Predicting Atmospheric Absorption Spectra](https://arxiv.org/abs/2510.08766)
*Vasuda Trehan,Kevin H. Knuth,M. J. Way*

Main category: astro-ph.EP

TL;DR: 开发一个基于观测和合成光谱数据的系外行星大气吸收光谱预测模型，使用样条曲线描述光谱特征，并采用贝叶斯自适应探索优化参数空间采样。


<details>
  <summary>Details</summary>
Motivation: 随着空间技术和AI/ML的发展，系外行星研究产生了大量数据，需要开发更有效的模型来理解系外行星大气特征和宜居性。

Method: 使用样条曲线拟合模拟大气吸收光谱的bin高度，结合NASA GISS的ROCKE-3D GCM模型生成合成光谱数据，采用贝叶斯自适应探索识别需要更多数据的参数区域。

Result: 正在开发中的模型将作为前向模型，能够根据行星的大气吸收光谱推断行星参数。

Conclusion: 这项工作有望增进对系外行星特性、气候和宜居性的理解，为系外行星研究提供重要工具。

Abstract: The evolution of space technology in recent years, fueled by advancements in
computing such as Artificial Intelligence (AI) and machine learning (ML), has
profoundly transformed our capacity to explore the cosmos. Missions like the
James Webb Space Telescope (JWST) have made information about distant objects
more easily accessible, resulting in extensive amounts of valuable data. As
part of this work-in-progress study, we are working to create an atmospheric
absorption spectrum prediction model for exoplanets. The eventual model will be
based on both collected observational spectra and synthetic spectral data
generated by the ROCKE-3D general circulation model (GCM) developed by the
climate modeling program at NASA's Goddard Institute for Space Studies (GISS).
In this initial study, spline curves are used to describe the bin heights of
simulated atmospheric absorption spectra as a function of one of the values of
the planetary parameters. Bayesian Adaptive Exploration is then employed to
identify areas of the planetary parameter space for which more data are needed
to improve the model. The resulting system will be used as a forward model so
that planetary parameters can be inferred given a planet's atmospheric
absorption spectrum. This work is expected to contribute to a better
understanding of exoplanetary properties and general exoplanet climates and
habitability.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [236] [A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?](https://arxiv.org/abs/2510.08758)
*Graham Tierney,Srikar Katta,Christopher Bail,Sunshine Hillygus,Alexander Volfovsky*

Main category: stat.ME

TL;DR: 该论文提出了一种新的实验设计方法，用于解决文本属性因果效应估计中的潜在混杂问题和重叠偏差问题，并在政治沟通谦逊表达对说服力影响的实验中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究语言属性如何因果影响受众态度和行为是社会科学的重要问题。由于文本属性往往相互关联，需要控制潜在混杂因素来隔离因果效应。现有基于LLM的方法存在编码处理本身导致重叠偏差的风险。

Method: 引入新的实验设计方法，处理潜在混杂，避免重叠问题，无偏估计处理效应。在政治沟通谦逊表达对说服力影响的实验中应用该方法。

Result: 实验证明LLM方法在真实文本和实验结果中表现甚至不如简单的词袋模型。成功隔离了表达谦逊对政治声明感知说服力的因果效应。

Conclusion: 该方法为社交媒体平台、政策制定者和社会科学家提供了新的沟通效应见解，并展示了在真实文本分析中简单模型可能优于复杂LLM方法。

Abstract: Many social science questions ask how linguistic properties causally affect
an audience's attitudes and behaviors. Because text properties are often
interlinked (e.g., angry reviews use profane language), we must control for
possible latent confounding to isolate causal effects. Recent literature
proposes adapting large language models (LLMs) to learn latent representations
of text that successfully predict both treatment and the outcome. However,
because the treatment is a component of the text, these deep learning methods
risk learning representations that actually encode the treatment itself,
inducing overlap bias. Rather than depending on post-hoc adjustments, we
introduce a new experimental design that handles latent confounding, avoids the
overlap issue, and unbiasedly estimates treatment effects. We apply this design
in an experiment evaluating the persuasiveness of expressing humility in
political communication. Methodologically, we demonstrate that LLM-based
methods perform worse than even simple bag-of-words models using our real text
and outcomes from our experiment. Substantively, we isolate the causal effect
of expressing humility on the perceived persuasiveness of political statements,
offering new insights on communication effects for social media platforms,
policy makers, and social scientists.

</details>


### [237] [Reliability Sensitivity with Response Gradient](https://arxiv.org/abs/2510.09315)
*Siu-Kui Au,Zi-Jun Cao*

Main category: stat.ME

TL;DR: 提出了一种基于响应梯度的蒙特卡洛方法，用于计算工程风险中失效概率对系统参数的敏感性，解决了传统有限差分方法在稀有事件和黑盒响应中的计算困难。


<details>
  <summary>Details</summary>
Motivation: 工程风险分析需要评估失效概率对系统参数变化的敏感性，但传统方法在大量随机变量、稀有事件和隐式非线性黑盒响应情况下计算困难且效率低下。

Method: 通过响应值和相对于敏感性参数的梯度，利用核平滑技术计算条件期望，实现在单次蒙特卡洛运行中获得所有响应阈值的敏感性估计。

Result: 该方法能够有效计算不同性质敏感性参数的敏感性，为在可靠性分析中嵌入敏感性计算提供了基础。

Conclusion: 随着响应梯度计算越来越可行，该方法有望成为在单次蒙特卡洛运行中同时进行可靠性和敏感性分析的有效工具。

Abstract: Engineering risk is concerned with the likelihood of failure and the
scenarios when it occurs. The sensitivity of failure probability to change in
system parameters is relevant to risk-informed decision making. Computing
sensitivity is at least one level more difficult than the probability itself,
which is already challenged by a large number of input random variables, rare
events and implicit nonlinear `black-box' response. Finite difference with
Monte Carlo probability estimates is spurious, requiring the number of samples
to grow with the reciprocal of step size to suppress estimation variance. Many
existing works gain efficiency by exploiting a specific class of input
variables, sensitivity parameters, or response in its exact or surrogate form.
For general systems, this work presents a theory and associated Monte Carlo
strategy for computing sensitivity using response values and gradients with
respect to sensitivity parameters. It is shown that the sensitivity at a given
response threshold can be expressed via the expectation of response gradient
conditional on the threshold. Determining the expectation requires conditioning
on the threshold that is a zero-probability event, but it can be resolved by
the concept of kernel smoothing. The proposed method offers sensitivity
estimates for all response thresholds generated in a single Monte Carlo run. It
is investigated in a number of examples featuring sensitivity parameters of
different nature. As response gradient becomes increasingly available, it is
hoped that this work can provide the basis for embedding sensitivity
calculations with reliability in the same Monte Carlo run.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [238] [Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories](https://arxiv.org/abs/2510.08622)
*Francesco Dente,Fabiano Dalpiaz,Paolo Papotti*

Main category: cs.CL

TL;DR: 提出了Text2Stories任务和指标，用于量化需求（用户故事）与利益相关者实际需求之间的对齐程度，通过正确性和完整性两个指标评估需求生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM可以自动从自然语言输入生成软件需求，但评估这些需求是否真实反映利益相关者需求仍然主要依赖人工任务，需要自动化评估方法。

Method: 将访谈记录分段为文本块，将对齐问题建模为文本块与用户故事之间的匹配问题，使用基于LLM的匹配器和嵌入模型进行实验。

Result: 在四个数据集上的实验显示，基于LLM的匹配器在保留注释上达到0.86的宏F1分数，而仅使用嵌入模型效果较差但可用于有效阻塞。

Conclusion: Text2Stories指标能够比较不同故事集的质量，作为现有用户故事质量标准的可扩展、源可信的补充评估方法。

Abstract: Large language models (LLMs) can be employed for automating the generation of
software requirements from natural language inputs such as the transcripts of
elicitation interviews. However, evaluating whether those derived requirements
faithfully reflect the stakeholders' needs remains a largely manual task. We
introduce Text2Stories, a task and metrics for text-to-story alignment that
allow quantifying the extent to which requirements (in the form of user
stories) match the actual needs expressed by the elicitation session
participants. Given an interview transcript and a set of user stories, our
metric quantifies (i) correctness: the proportion of stories supported by the
transcript, and (ii) completeness: the proportion of transcript supported by at
least one story. We segment the transcript into text chunks and instantiate the
alignment as a matching problem between chunks and stories. Experiments over
four datasets show that an LLM-based matcher achieves 0.86 macro-F1 on held-out
annotations, while embedding models alone remain behind but enable effective
blocking. Finally, we show how our metrics enable the comparison across sets of
stories (e.g., human vs. generated), positioning Text2Stories as a scalable,
source-faithful complement to existing user-story quality criteria.

</details>


### [239] [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)
*Shahriar Kabir Nahin,Hadi Askari,Muhao Chen,Anshuman Chhabra*

Main category: cs.CL

TL;DR: Test-Time Scaling (TTS) 在多样性受限时会产生更多不安全输出，RefDiv攻击方法揭示了这一先前未被认识的故障模式。


<details>
  <summary>Details</summary>
Motivation: 揭示TTS方法中候选响应多样性假设的脆弱性，当多样性被限制时，TTS更可能产生不安全输出。

Method: 提出RefDiv参考引导多样性减少协议作为诊断攻击，通过约束候选池多样性来压力测试TTS管道。

Result: 在四个开源模型和两种TTS策略上的实验显示，限制多样性显著增加不安全输出率，且现有安全防护分类器无法检测此类攻击。

Conclusion: TTS存在基于多样性的通用故障模式，需要设计更鲁棒的TTS策略来抵御多样性目标攻击。

Abstract: Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple
candidate responses and then operating over this set to find the best output. A
tacit premise behind TTS is that sufficiently diverse candidate pools enhance
reliability. In this work, we show that this assumption in TTS introduces a
previously unrecognized failure mode. When candidate diversity is curtailed,
even by a modest amount, TTS becomes much more likely to produce unsafe
outputs. We present a reference-guided diversity reduction protocol (RefDiv)
that serves as a diagnostic attack to stress test TTS pipelines. Through
extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,
Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and
Best-of-N), constraining diversity consistently signifies the rate at which TTS
produces unsafe results. The effect is often stronger than that produced by
prompts directly with high adversarial intent scores. This observed phenomenon
also transfers across TTS strategies and to closed-source models (e.g. OpenAI
o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant
property of TTS rather than a model-specific artifact. Additionally, we find
that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and
OpenAI Moderation API), are unable to flag the adversarial input prompts
generated by RefDiv, demonstrating that existing defenses offer limited
protection against this diversity-driven failure mode. Through this work, we
hope to motivate future research on designing robust TTS strategies that are
both effective and secure against diversity-targeted stress tests as
illustrated by RefDiv.

</details>


### [240] [Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation](https://arxiv.org/abs/2510.08600)
*Devleena Das,Rajeev Patwari,Ashish Sirasao*

Main category: cs.CL

TL;DR: Recover-LoRA是一种轻量级、数据集无关的方法，通过合成数据和logit蒸馏学习选择性层的LoRA适配器，从模型权重退化中恢复语言模型精度，在MHA和GQA架构的小语言模型上实现5-17%的精度恢复。


<details>
  <summary>Details</summary>
Motivation: 推理优化（如量化、剪枝、格式转换、模型导出和序列化）可能导致语言模型任务性能的功能性退化。现有工作主要关注鲁棒量化技术，而本文关注从任何导致模型权重退化的来源（如不当模型序列化）中恢复模型精度。

Method: 提出Recover-LoRA方法，使用合成数据和logit蒸馏在选择性层上学习LoRA适配器，使退化模型与其全精度模型对齐。该方法轻量级且数据集无关。

Result: 在多种小语言模型（包括不同注意力架构的MHA和GQA模型）和多个评估数据集上的实验表明，Recover-LoRA能够恢复模型精度5-17%。

Conclusion: Recover-LoRA是一种有效的模型精度恢复方法，能够从各种模型权重退化情况中恢复语言模型的性能。

Abstract: Inference optimizations such as quantization, pruning, format and datatype
conversion, model export, and serialization can lead to functional degradations
in language model task performance. While most efforts on performance recovery
for deployment focus on robust quantization techniques, we focus on recovering
model accuracies from any sources that degrade model weights, such as improper
model serialization. In this work, we propose Recover-LoRA, a lightweight and
dataset agnostic method to recover accuracy in degraded models. Recover-LoRA
uses synthetic data and logit distillation to learn LoRA adapters on selective
layers that facilitate aligning the degraded model to its full precision model.
We investigate the utility of Recover-LoRA across a diverse set of small
language models (SLMs), including models with varying attention architectures,
multi-head attention (MHA) and group-query attention (GQA), as well as several
evaluation datasets. Our results show that Recover-LoRA recovers model
accuracies by 5-17% on MHA and GQA SLMs.

</details>


### [241] [Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs](https://arxiv.org/abs/2510.08601)
*Aneesh Jonelagadda,Christina Hahn,Haoze Zheng,Salvatore Penachio*

Main category: cs.CL

TL;DR: Mnemosyne是一个受人类记忆启发的无监督长期记忆架构，专为边缘设备上的大语言模型设计，通过图结构存储、记忆过滤和修剪机制，在医疗对话场景中显著提升了长期记忆能力和响应自然度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的记忆系统要么依赖暴力扩展上下文，要么使用静态检索管道，无法在资源受限的边缘设备上有效处理长期对话，特别是在医疗等需要长期记忆的场景中。

Method: 采用图结构存储，包含模块化内容和冗余过滤器、记忆提交和修剪机制，以及基于时间衰减和刷新过程的概率召回机制，还引入了从记忆图中高效提取的"核心摘要"来捕捉用户个性和长期细节。

Result: 在纵向医疗对话实验中，Mnemosyne在盲测人类评估中获得65.8%的最高胜率（基线RAG为31.1%），在LoCoMo基准测试中在时间推理和单跳检索方面达到当前最高分，总体得分54.6%排名第二，优于Mem0和OpenAI等常用方法。

Conclusion: Mnemosyne证明通过边缘兼容且易于迁移的无监督记忆架构，可以显著改善事实召回、增强时间推理能力，并生成更自然的用户响应。

Abstract: Long-term memory is essential for natural, realistic dialogue. However,
current large language model (LLM) memory systems rely on either brute-force
context expansion or static retrieval pipelines that fail on edge-constrained
devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term
memory architecture designed for edge-based LLMs. Our approach uses
graph-structured storage, modular substance and redundancy filters, memory
committing and pruning mechanisms, and probabilistic recall with temporal decay
and refresh processes modeled after human memory. Mnemosyne also introduces a
concentrated "core summary" efficiently derived from a fixed-length subset of
the memory graph to capture the user's personality and other domain-specific
long-term details such as, using healthcare application as an example,
post-recovery ambitions and attitude towards care. Unlike existing
retrieval-augmented methods, Mnemosyne is designed for use in longitudinal
healthcare assistants, where repetitive and semantically similar but temporally
distinct conversations are limited by naive retrieval. In experiments with
longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate
of 65.8% in blind human evaluations of realism and long-term memory capability
compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current
highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval
compared to other same-backboned techniques. Further, the average overall score
of 54.6% was second highest across all methods, beating commonly used Mem0 and
OpenAI baselines among others. This demonstrates that improved factual recall,
enhanced temporal reasoning, and much more natural user-facing responses can be
feasible with an edge-compatible and easily transferable unsupervised memory
architecture.

</details>


### [242] [Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection](https://arxiv.org/abs/2510.08602)
*Cong Zeng,Shengkun Tang,Yuanzhou Chen,Zhiqiang Shen,Wenchao Yu,Xujiang Zhao,Haifeng Chen,Wei Cheng,Zhiqiang Xu*

Main category: cs.CL

TL;DR: 本文提出将AI生成文本检测任务重新定义为离群检测问题，将人类文本视为分布外样本，机器生成文本视为分布内样本，使用单类学习和基于分数的方法实现更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将AI文本检测视为二分类问题，但人类文本并不构成统一分布，导致模型记忆OOD特征而非学习'非ID'行为的本质，限制了泛化能力。

Method: 使用DeepSVDD和HRN等单类学习方法，以及基于能量的分数学习方法，构建OOD检测框架。

Result: 在DeepFake数据集上达到98.3%的AUROC和AUPR，FPR95仅为8.9%。在多语言、受攻击、未见模型和领域文本设置下均表现出鲁棒性和泛化性。

Conclusion: 将AI文本检测重新定义为OOD检测问题能显著提升检测器的泛化能力和鲁棒性，特别是在处理多样化人类文本时表现优异。

Abstract: The rapid advancement of large language models (LLMs) such as ChatGPT,
DeepSeek, and Claude has significantly increased the presence of AI-generated
text in digital communication. This trend has heightened the need for reliable
detection methods to distinguish between human-authored and machine-generated
content. Existing approaches both zero-shot methods and supervised classifiers
largely conceptualize this task as a binary classification problem, often
leading to poor generalization across domains and models. In this paper, we
argue that such a binary formulation fundamentally mischaracterizes the
detection task by assuming a coherent representation of human-written texts. In
reality, human texts do not constitute a unified distribution, and their
diversity cannot be effectively captured through limited sampling. This causes
previous classifiers to memorize observed OOD characteristics rather than learn
the essence of `non-ID' behavior, limiting generalization to unseen
human-authored inputs. Based on this observation, we propose reframing the
detection task as an out-of-distribution (OOD) detection problem, treating
human-written texts as distributional outliers while machine-generated texts
are in-distribution (ID) samples. To this end, we develop a detection framework
using one-class learning method including DeepSVDD and HRN, and score-based
learning techniques such as energy-based method, enabling robust and
generalizable performance. Extensive experiments across multiple datasets
validate the effectiveness of our OOD-based approach. Specifically, the
OOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake
dataset. Moreover, we test our detection framework on multilingual, attacked,
and unseen-model and -domain text settings, demonstrating the robustness and
generalizability of our framework. Code, pretrained weights, and demo will be
released.

</details>


### [243] [LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback](https://arxiv.org/abs/2510.08604)
*Raffaele Mura,Giorgio Piras,Kamilė Lukošiūtė,Maura Pintor,Amin Karbasi,Battista Biggio*

Main category: cs.CL

TL;DR: 提出LatentBreak方法，通过语义等效词替换生成低困惑度的对抗性提示，有效绕过基于困惑度的安全检测机制。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击使用高困惑度的对抗后缀或长模板，容易被基于困惑度的过滤器检测到。

Method: 在潜在空间中最小化对抗提示与无害请求表示的距离，用语义等效词替换输入提示中的词汇。

Result: LatentBreak生成的提示更短、困惑度更低，在多个安全对齐模型上优于其他越狱算法。

Conclusion: 该方法能有效规避基于困惑度的防御机制，为模型安全研究提供了新的挑战。

Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.

</details>


### [244] [Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks](https://arxiv.org/abs/2510.08605)
*Nouar Aldahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 提出多语言多代理LLM框架，通过检索增强生成技术检测数字平台上的错误信息，可作为网络插件部署。


<details>
  <summary>Details</summary>
Motivation: 数字平台上错误信息的快速传播威胁公共话语、情绪稳定和决策制定，需要系统研究语言切换、查询长度膨胀和结构重构等对抗性攻击。

Method: 开发多语言多代理大语言模型框架，采用检索增强生成技术，支持英语、法语、西班牙语、阿拉伯语、印地语和中文的语言切换检测。

Result: 展示了插件式部署在真实网络应用中的可行性，能够有效防护多样化攻击。

Conclusion: AI驱动的错误信息检测对于保护在线事实完整性至关重要，插件化部署方案具有实际应用价值。

Abstract: The rapid spread of misinformation on digital platforms threatens public
discourse, emotional stability, and decision-making. While prior work has
explored various adversarial attacks in misinformation detection, the specific
transformations examined in this paper have not been systematically studied. In
particular, we investigate language-switching across English, French, Spanish,
Arabic, Hindi, and Chinese, followed by translation. We also study query length
inflation preceding summarization and structural reformatting into
multiple-choice questions. In this paper, we present a multilingual,
multi-agent large language model framework with retrieval-augmented generation
that can be deployed as a web plugin into online platforms. Our work
underscores the importance of AI-driven misinformation detection in
safeguarding online factual integrity against diverse attacks, while showcasing
the feasibility of plugin-based deployment for real-world web applications.

</details>


### [245] [PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction](https://arxiv.org/abs/2510.08623)
*Anubhav Shrimal,Aryan Jain,Soumyajit Chowdhury,Promod Yenigalla*

Main category: cs.CL

TL;DR: PARSE系统通过自动优化JSON模式和改进提取方法，显著提升了从非结构化文本中结构化信息提取的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将大语言模型应用于提取任务，但将JSON模式视为静态合同，导致提取性能不佳、幻觉频繁和代理行为不可靠。

Method: 开发PARSE系统，包含ARCHITECT组件（自动优化JSON模式）和SCOPE组件（基于反射的提取方法），结合静态和LLM防护机制。

Result: 在三个数据集上评估，SWDE提取准确率提升64.7%，框架整体改进达10%，首次重试减少92%的提取错误，保持实用延迟。

Conclusion: PARSE通过优化模式设计和改进提取方法，有效解决了现有信息提取系统的局限性，提升了LLM代理的可靠性。

Abstract: Structured information extraction from unstructured text is critical for
emerging Software 3.0 systems where LLM agents autonomously interact with APIs
and tools. Recent approaches apply large language models directly to extraction
tasks using existing JSON schemas, often with constraint decoding or
reinforcement learning approaches to ensure syntactic validity, but treat JSON
schemas as static contracts designed for human developers, leading to
suboptimal extraction performance, frequent hallucinations, and unreliable
agent behavior when schemas contain ambiguous or incomplete specifications. We
recognize that JSON schemas themselves are a form of natural language
understanding contract that encodes rules, relationships, and expectations
about data structure contracts that LLMs should be able to both interpret and
systematically improve. Consequently, we develop PARSE (Parameter Automated
Refinement and Schema Extraction), a novel system with two synergistic
components: ARCHITECT, which autonomously optimizes JSON schemas for LLM
consumption while maintaining backward compatibility through RELAY (an
integrated code generation system), and SCOPE, which implements
reflection-based extraction with combined static and LLM-based guardrails. We
evaluate PARSE qualitatively and quantitatively on three datasets including
Schema-Guided Dialogue (SGD), Structured Web Data Extraction (SWDE), and
internal retail conversation data, and find that it achieves up to 64.7%
improvement in extraction accuracy on SWDE with combined framework improvements
reaching 10% across models, while reducing extraction errors by 92% within the
first retry and and maintaining practical latency.

</details>


### [246] [Next Semantic Scale Prediction via Hierarchical Diffusion Language Models](https://arxiv.org/abs/2510.08632)
*Cai Zhou,Chenyu Wang,Dinghuai Zhang,Shangyuan Tong,Yifei Wang,Stephen Bates,Tommi Jaakkola*

Main category: cs.CL

TL;DR: HDLM是一种新型的离散扩散语言模型，通过分层词汇表实现从粗粒度到细粒度的语义生成过程，在文本生成任务中表现出优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型在语义层次建模方面存在局限，需要一种能够处理不同粒度语义的层次化扩散框架。

Method: 构建分层词汇表，前向过程将细粒度token扰动为粗粒度祖先token，反向过程逐步预测更详细的语义，实现时间变化的下一个语义尺度预测。

Result: 实验验证HDLM在验证集和生成困惑度上均优于基线模型，表现出更好的文本生成能力。

Conclusion: HDLM提供了一个通用的层次化扩散语言建模框架，能够有效处理多粒度语义信息，为扩散语言模型的发展提供了新思路。

Abstract: In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a
novel family of discrete diffusion models for language modeling. HDLM builds on
a hierarchical vocabulary where low-level tokens with detailed semantics are
surjectively mapped to high-level tokens with coarse-grained meanings. In the
forward process, each token is independently perturbed to its higher-level
ancestor with more abstract semantics according to the scheduler, while in the
reverse process the model progressively predicts the next, more detailed
semantics. Taken together, HDLM provides a general time-varying next semantic
scale prediction process for language modeling. We derive closed-form
expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM
can be implemented in a flexible manner while including the existing MDLM as a
special case. We also propose practical training techniques based on the
insights. Extensive text generation experiments validate the effectiveness of
HDLM, which demonstrates consistently lower validation and generative
perplexity than baselines.

</details>


### [247] [How Reliable is Language Model Micro-Benchmarking?](https://arxiv.org/abs/2510.08730)
*Gregory Yauney,Shahzaib Saqib Warraich,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文研究发现微基准测试在模型排名一致性方面存在局限性，即使模型性能差异达到3.5-4个准确率点，现有方法也无法可靠排名。需要250个样本才能获得稳定结果，此时随机采样与专门方法效果相当。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型开发中全基准测试时间成本过高的问题，评估微基准测试是否能像完整基准测试一样一致地排名模型，以及是否优于随机数据点子集。

Method: 引入元评估方法，分析微基准测试在不同模型性能差异下的排名能力，比较不同样本大小下微基准测试方法与随机采样的效果。

Result: 发现现有微基准测试方法无法在MMLU-Pro上一致排名3.5准确率点差异的模型对，在BIG-bench Hard上无法排名4点差异的模型对。需要250个样本才能获得可靠排名，此时随机采样与专门方法效果相似。

Conclusion: 微基准测试在效率和可靠性之间存在权衡，需要谨慎选择样本数量。当比较8B指令调优模型时，25个样本的微基准测试中超过一半的成对比较可能不可靠。

Abstract: Micro-benchmarking offers a solution to the often prohibitive time and cost
of language model development: evaluate on a very small subset of existing
benchmarks. Can these micro-benchmarks, however, rank models as consistently as
the full benchmarks they replace? And can they rank models more consistently
than selecting a random subset of data points? In many scenarios, we find that
the answer is no. We introduce a meta-evaluation measure for micro-benchmarking
which investigates how well a micro-benchmark can rank two models as a function
of their performance difference on the full benchmark. This approach can
determine which model pairs can be ranked correctly by a micro-benchmark,
allowing for a finer-grained analysis of the trade-off between micro-benchmark
size and reliability. Prior work has suggested selecting as few as 10 examples;
we find that no micro-benchmarking method can consistently rank model pairs 3.5
points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In
order to consistently rank model pairs with relatively similar performances, we
show that often as many as 250 examples must be selected, at which point random
sampling is competitive with existing micro-benchmarking methods. When
comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25
examples, we find that more than half of pairwise comparisons are not likely to
be preserved. Our work provides actionable guidance for both micro-benchmark
users and developers in navigating the trade-off between evaluation efficiency
and reliability.

</details>


### [248] [Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation](https://arxiv.org/abs/2510.09051)
*Muhammad Ali Shafique,Kanwal Mehreen,Muhammad Arham,Maaz Amjad,Sabur Butt,Hamza Farooq*

Main category: cs.CL

TL;DR: 提出了Alif-1.0-8B-Instruct，一个针对低资源乌尔都语的多语言模型，使用改进的自指导技术构建高质量合成数据集，在100美元预算内显著提升乌尔都语任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如乌尔都语）开发大语言模型面临的挑战：高质量数据集稀缺、多语言不一致性和安全问题。现有方法通过翻译数据但缺乏质量和文化细微差别。

Method: 使用改进的自指导技术构建高质量多语言合成数据集（Urdu-Instruct），包含乌尔都语本地思维链推理、双语翻译、文化相关性和伦理安全对齐。基于预训练的Llama-3.1-8B构建模型。

Result: Alif-1.0-8B-Instruct在乌尔都语特定任务上优于Llama-3.1-8B-Instruct，并超越了Mistral-7B-Instruct-v0.3、Qwen-2.5-7B-Instruct和Cohere-Aya-Expanse-8B等领先多语言LLM，训练预算低于100美元。

Conclusion: 证明使用改进的自指导方法可以高效开发高性能、低资源语言LLM，并实现文化对齐。所有数据集、模型和代码已公开。

Abstract: Developing a high-performing large language models (LLMs) for low-resource
languages such as Urdu, present several challenges. These challenges include
the scarcity of high-quality datasets, multilingual inconsistencies, and safety
concerns. Existing multilingual LLMs often address these issues by translating
large volumes of available data. However, such translations often lack quality
and cultural nuance while also incurring significant costs for data curation
and training. To address these issues, we propose Alif-1.0-8B-Instruct, a
multilingual Urdu-English model, that tackles these challenges with a unique
approach. We train the model on a high-quality, multilingual synthetic dataset
(Urdu-Instruct), developed using a modified self-instruct technique. By using
unique prompts and seed values for each task along with a global task pool,
this dataset incorporates Urdu-native chain-of-thought based reasoning,
bilingual translation, cultural relevance, and ethical safety alignments. This
technique significantly enhances the comprehension of Alif-1.0-8B-Instruct
model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon
the pretrained Llama-3.1-8B, demonstrates superior performance compared to
Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading
multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct,
and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our
results demonstrate that high-performance and low-resource language LLMs can be
developed efficiently and culturally aligned using our modified self-instruct
approach. All datasets, models, and code are publicly available at:
https://github.com/traversaal-ai/alif-urdu-llm.

</details>


### [249] [IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data](https://arxiv.org/abs/2510.09217)
*Tao Feng,Lizhen Qu,Niket Tandon,Gholamreza Haffari*

Main category: cs.CL

TL;DR: IRIS是一个实时因果发现框架，结合统计算法和LLM方法，从初始变量自动收集文档、提取变量并发现已知和新的因果关系，无需现有数据集。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法面临数据收集成本高、对已知关系重复计算、假设不现实等问题，而现有LLM方法只能发现已知关系，无法发现新关系。

Method: IRIS采用混合方法结合统计算法和LLM，自动收集相关文档、提取变量、发现因果关系，并包含缺失变量提议组件来扩展因果图。

Result: 该框架能够实时从初始变量进行因果发现，无需预存数据集，能够发现已知和新的因果关系。

Conclusion: IRIS解决了传统因果发现方法的局限性，实现了无需预存数据集的实时因果发现，能够发现新的因果关系。

Abstract: Causal discovery is fundamental to scientific research, yet traditional
statistical algorithms face significant challenges, including expensive data
collection, redundant computation for known relations, and unrealistic
assumptions. While recent LLM-based methods excel at identifying commonly known
causal relations, they fail to uncover novel relations. We introduce IRIS
(Iterative Retrieval and Integrated System for Real-Time Causal Discovery), a
novel framework that addresses these limitations. Starting with a set of
initial variables, IRIS automatically collects relevant documents, extracts
variables, and uncovers causal relations. Our hybrid causal discovery method
combines statistical algorithms and LLM-based methods to discover known and
novel causal relations. In addition to causal discovery on initial variables,
the missing variable proposal component of IRIS identifies and incorporates
missing variables to expand the causal graphs. Our approach enables real-time
causal discovery from only a set of initial variables without requiring
pre-existing datasets.

</details>


### [250] [Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models](https://arxiv.org/abs/2510.09259)
*Yongding Tao,Tian Wang,Yihong Dong,Huanyu Liu,Kechi Zhang,Xiaolong Hu,Ge Li*

Main category: cs.CL

TL;DR: 本文提出了Self-Critique方法，用于检测LLM在强化学习后训练阶段的数据污染问题，通过分析模型输出熵分布的变化来识别政策崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 数据污染严重威胁LLM评估的可靠性，特别是在强化学习后训练阶段缺乏专门的检测方法，这成为了一个关键漏洞。

Method: 提出Self-Critique方法，基于观察到RL后训练阶段LLM输出熵分布会塌陷到特定稀疏模式的现象，通过探测潜在的政策崩溃来检测数据污染。

Result: 实验表明Self-Critique在多个模型和污染任务中显著优于基线方法，AUC提升高达30%，而现有方法在RL阶段污染检测中接近随机猜测。

Conclusion: Self-Critique方法首次使RL后训练阶段的数据污染检测成为可能，填补了该领域的重要研究空白。

Abstract: Data contamination poses a significant threat to the reliable evaluation of
Large Language Models (LLMs). This issue arises when benchmark samples may
inadvertently appear in training sets, compromising the validity of reported
performance. While detection methods have been developed for the pre-training
and Supervised Fine-Tuning stages, a critical research gap exists for the
increasingly significant phase of Reinforcement Learning (RL) post-training. As
RL post-training becomes pivotal for advancing LLM reasoning, the absence of
specialized contamination detection methods in this paradigm presents a
critical vulnerability. To address this, we conduct the first systematic study
of data detection within RL post-training scenario and propose Self-Critique.
Our method is motivated by a key observation: after RL phase, the output
entropy distribution of LLMs tends to collapse into highly specific and sparse
modes. Self-Critique probes for the underlying policy collapse, i.e., the
model's convergence to a narrow reasoning path, which causes this entropy
reduction. To facilitate this research, we also introduce RL-MIA, a benchmark
constructed to simulate this specific contamination scenario. Extensive
experiments show that Self-Critique significantly outperforms baseline methods
across multiple models and contamination tasks, achieving an AUC improvement of
up to 30%. Whereas existing methods are close to a random guess for RL-phase
contamination, our method makes detection possible.

</details>


### [251] [Verifying Chain-of-Thought Reasoning via Its Computational Graph](https://arxiv.org/abs/2510.09312)
*Zheng Zhao,Yeskendir Koishekenov,Xianjun Yang,Naila Murray,Nicola Cancedda*

Main category: cs.CL

TL;DR: 提出了一种基于电路推理验证（CRV）的白盒方法，通过分析正确和错误推理步骤的属性图结构特征来检测推理错误，并能指导针对性干预来修正推理。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT验证方法（黑盒或灰盒）只能预测推理正确性，但无法解释计算失败的原因，需要一种能深入理解推理失败机制的白盒方法。

Method: 将正确CoT步骤的属性图视为潜在推理电路的执行轨迹，训练分类器识别这些图的结构特征差异，通过分析计算图的结构指纹来验证推理。

Result: 发现错误的结构特征具有高度预测性，在不同推理任务中表现出特定计算模式，通过针对性干预能成功修正模型的错误推理。

Conclusion: 通过审视模型的计算过程，可以从简单的错误检测转向对LLM推理的更深层因果理解，白盒方法提供了其他方法无法获得的新科学见解。

Abstract: Current Chain-of-Thought (CoT) verification methods predict reasoning
correctness based on outputs (black-box) or activations (gray-box), but offer
limited insight into why a computation fails. We introduce a white-box method:
Circuit-based Reasoning Verification (CRV). We hypothesize that attribution
graphs of correct CoT steps, viewed as execution traces of the model's latent
reasoning circuits, possess distinct structural fingerprints from those of
incorrect steps. By training a classifier on structural features of these
graphs, we show that these traces contain a powerful signal of reasoning
errors. Our white-box approach yields novel scientific insights unattainable by
other methods. (1) We demonstrate that structural signatures of error are
highly predictive, establishing the viability of verifying reasoning directly
via its computational graph. (2) We find these signatures to be highly
domain-specific, revealing that failures in different reasoning tasks manifest
as distinct computational patterns. (3) We provide evidence that these
signatures are not merely correlational; by using our analysis to guide
targeted interventions on individual transcoder features, we successfully
correct the model's faulty reasoning. Our work shows that, by scrutinizing a
model's computational process, we can move from simple error detection to a
deeper, causal understanding of LLM reasoning.

</details>


### [252] [Active Model Selection for Large Language Models](https://arxiv.org/abs/2510.09418)
*Yavuz Durmazkeser,Patrik Okanovic,Andreas Kirsch,Torsten Hoefler,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: LLM SELECTOR是一个主动选择大语言模型的框架，通过自适应选择少量信息量最大的查询进行标注，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法需要完全标注的数据集，标注成本高昂，需要开发能高效识别最佳LLM的框架。

Method: 使用自适应查询选择策略，优先标注对模型选择最有信息量的查询，并利用基于评判者的标注模型进一步降低成本。

Result: 在6个基准测试和151个LLM上的实验表明，该框架在选择最佳和接近最佳LLM时，标注成本降低高达59.62%。

Conclusion: LLM SELECTOR是第一个主动模型选择框架，能显著减少标注需求，为LLM评估提供高效解决方案。

Abstract: We introduce LLM SELECTOR, the first framework for active model selection of
Large Language Models (LLMs). Unlike prior evaluation and benchmarking
approaches that rely on fully annotated datasets, LLM SELECTOR efficiently
identifies the best LLM with limited annotations. In particular, for any given
task, LLM SELECTOR adaptively selects a small set of queries to annotate that
are most informative about the best model for the task. To further reduce
annotation cost, we leverage a judge-based oracle annotation model. Through
extensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR
reduces annotation costs by up to 59.62% when selecting the best and near-best
LLM for the task.

</details>


### [253] [The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach](https://arxiv.org/abs/2510.09424)
*Nizar El Ghazal,Antoine Caubrière,Valentin Vielzeuf*

Main category: cs.CL

TL;DR: 比较了三种上下文管理策略在端到端口语对话状态跟踪中的效果，发现完整口语历史输入性能最佳，注意力池化压缩方法在保持准确性的同时减少了上下文大小。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效管理口语对话中的上下文信息，以提升Speech-LLMs在口语对话状态跟踪任务中的性能。

Method: 系统评估了三种策略：传统多模态上下文（文本历史+当前口语轮次）、完整口语历史、压缩口语历史，并在SpokenWOZ语料库上进行实验。

Result: 完整口语历史输入在相似规模模型中表现最佳，显著超越先前方法；注意力池化压缩方法在保持竞争力的准确性的同时减少了上下文大小。

Conclusion: 改进源于更有效的上下文利用，完整口语历史是最佳策略，而压缩方法提供了良好的权衡方案。

Abstract: This paper presents a comparative study of context management strategies for
end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically
evaluate traditional multimodal context (combining text history and spoken
current turn), full spoken history, and compressed spoken history approaches.
Our experiments on the SpokenWOZ corpus demonstrate that providing the full
spoken conversation as input yields the highest performance among models of
similar size, significantly surpassing prior methods. Furthermore, we show that
attention-pooling-based compression of the spoken history offers a strong
trade-off, maintaining competitive accuracy with reduced context size. Detailed
analysis confirms that improvements stem from more effective context
utilization.

</details>


### [254] [Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic](https://arxiv.org/abs/2510.09472)
*Manuel Vargas Guzmán,Jakub Szymanik,Maciej Malicki*

Main category: cs.CL

TL;DR: 本文研究发现LLMs在逻辑推理中递归能力较好但组合能力不足，提出结合符号推理与神经计算的混合架构来解决泛化问题。


<details>
  <summary>Details</summary>
Motivation: 区分神经网络泛化能力的两个关键方面：组合性（抽象基础逻辑规则）和递归性（迭代应用推理规则），澄清文献中常混淆的概念。

Method: 使用三段论作为自然语言推理基准，评估预训练LLMs的逻辑泛化能力，并提出神经-符号混合推理架构。

Result: LLMs在递归性方面表现合理，但在组合性方面存在显著困难；混合架构在保持高效率的同时实现了稳健推理。

Conclusion: 混合模型有潜力有效解决神经推理系统中的关键泛化障碍，神经组件加速处理，符号推理确保完备性。

Abstract: Despite the remarkable progress in neural models, their ability to
generalize, a cornerstone for applications like logical reasoning, remains a
critical challenge. We delineate two fundamental aspects of this ability:
compositionality, the capacity to abstract atomic logical rules underlying
complex inferences, and recursiveness, the aptitude to build intricate
representations through iterative application of inference rules. In the
literature, these two aspects are often confounded together under the umbrella
term of generalization. To sharpen this distinction, we investigated the
logical generalization capabilities of pre-trained large language models (LLMs)
using the syllogistic fragment as a benchmark for natural language reasoning.
Though simple, this fragment provides a foundational yet expressive subset of
formal logic that supports controlled evaluation of essential reasoning
abilities. Our findings reveal a significant disparity: while LLMs demonstrate
reasonable proficiency in recursiveness, they struggle with compositionality.
To overcome these limitations and establish a reliable logical prover, we
propose a hybrid architecture integrating symbolic reasoning with neural
computation. This synergistic interaction enables robust and efficient
inference, neural components accelerate processing, while symbolic reasoning
ensures completeness. Our experiments show that high efficiency is preserved
even with relatively small neural components. As part of our proposed
methodology, this analysis gives a rationale and highlights the potential of
hybrid models to effectively address key generalization barriers in neural
reasoning systems.

</details>


### [255] [Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation](https://arxiv.org/abs/2510.09599)
*Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: P-TTS是一种推理时数据增强策略，仅使用90个手动选择推理实例，通过系统化改变提示强度来合成多样推理轨迹，显著提升LLM在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量人工标注的推理数据集，成本高昂且资源密集。P-TTS旨在通过少量示例和推理时数据增强来低成本提升LLM推理能力。

Method: 使用仅90个手动选择推理实例，通过系统化改变提示强度在推理时合成多样推理轨迹上下文，然后对Qwen-2.5模型进行微调。

Result: 在AIME2024、AIME2025、MATH500和GPQA-Diamond等数学推理基准上显著超越基线，7B模型在AIME'24上提升26.66-30.00%，在零样本泛化任务上也表现优异。

Conclusion: P-TTS提供了一种实用、低成本的LLM推理增强方法，有效探索推理模式的潜在空间，在资源受限或快速发展的领域中具有重要价值。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities when provided with chain-of-thought exemplars, but curating large
reasoning datasets remains laborious and resource-intensive. In this work, we
introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective
inference-time data augmentation strategy for enhancing LLM reasoning through
finetuning. Rather than collecting thousands or even millions of examples,
P-TTS leverages a small pool of only 90 manually selected reasoning instances
and systematically varies exemplar augmentation through principled instruction
prompting intensities at test time to synthesize diverse reasoning trajectory
contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.
Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and
GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive
baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of
+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);
P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and
+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better
performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances
zero-shot generalization accuracy on out-of-domain reasoning benchmarks of
Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our
analysis suggests that test-time scaling effectively explores the latent space
of reasoning patterns, amplifying LLM problem-solving with minimal annotation
overhead, and further unlocking the reasoning potential and capabilities of
LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit
LLM reasoning in resource-constrained or rapidly evolving domains.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [256] [Multiparty equality in the local broadcast model](https://arxiv.org/abs/2510.09143)
*Louis Esperet,Jean-Florent Raymond*

Main category: math.CO

TL;DR: 本文研究了图上的多方相等性问题，在局部广播模型下设计了针对2连通图的新协议，该协议基于全顶点覆盖概念，在多种网络拓扑中优于现有的4-近似算法。


<details>
  <summary>Details</summary>
Motivation: 研究网络拓扑对多方相等性问题的影响，旨在改进Khan和Vaidya在2021年提出的4-近似协议，为特定网络结构提供更优的通信协议。

Method: 利用图论中的全顶点覆盖概念，为2连通图设计新的通信协议，并将其应用于循环图、超立方体和网格等自然网络拓扑。

Result: 新协议在多种情况下优于现有的4-近似算法，为循环图、超立方体和网格等拓扑结构提供了最优或渐近最优的协议。

Conclusion: 通过利用网络拓扑特性，可以显著改进多方相等性问题的通信效率，全顶点覆盖是设计高效协议的关键工具。

Abstract: In this paper we consider the multiparty equality problem in graphs, where
every vertex of a graph $G$ is given an input, and the goal of the vertices is
to decide whether all inputs are equal. We study this problem in the local
broadcast model, where a message sent by a vertex is received by all its
neighbors and the total cost of a protocol is the sum of the lengths of the
messages sent by the vertices. This setting was studied by Khan and Vaidya, who
gave in 2021 a protocol achieving a 4-approximation in the general case.
  We study this multiparty communication problem through the lens of network
topology. We design a new protocol for 2-connected graphs, whose efficiency
relies on the notion of total vertex cover in graph theory. This protocol
outperforms the aforementioned 4-approximation in a number of cases. To
demonstrate its applicability, we apply it to obtain optimal or asymptotically
optimal protocols for several natural network topologies such as cycles,
hypercubes, and grids. On the way we also provide new bounds of independent
interest on the size of total vertex covers in regular graphs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [257] [MCMC: Bridging Rendering, Optimization and Generative AI](https://arxiv.org/abs/2510.09078)
*Gurprit Singh,Wenzel Jakob*

Main category: cs.GR

TL;DR: 该论文探讨了马尔可夫链蒙特卡洛（MCMC）方法在生成式AI和基于物理的渲染中的重要作用，提出了一个统一框架来连接这两个研究领域。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏连接生成式AI和基于物理的渲染的统一框架，而MCMC方法在这两个领域都发挥着关键作用，可以作为桥梁连接这两个紧密相关的研究方向。

Method: 通过理论分析和实践演示，探索MCMC方法如何作为生成式AI（特别是扩散模型）和基于物理的渲染之间的连接桥梁，使用Jupyter笔记本进行实际演示。

Result: 提出了一个初步框架，展示了MCMC方法在生成式AI和物理渲染中的共同理论基础和应用潜力，为后续研究提供了工具和方向。

Conclusion: MCMC方法有潜力成为连接生成式AI和基于物理的渲染的统一框架，为实现物理真实的生成渲染提供了理论基础和实践工具。

Abstract: Generative artificial intelligence (AI) has made unprecedented advances in
vision language models over the past two years. During the generative process,
new samples (images) are generated from an unknown high-dimensional
distribution. Markov Chain Monte Carlo (MCMC) methods are particularly
effective in drawing samples from such complex, high-dimensional distributions.
This makes MCMC methods an integral component for models like EBMs, ensuring
accurate sample generation.
  Gradient-based optimization is at the core of modern generative models. The
update step during the optimization forms a Markov chain where the new update
depends only on the current state. This allows exploration of the parameter
space in a memoryless manner, thus combining the benefits of gradient-based
optimization and MCMC sampling. MCMC methods have shown an equally important
role in physically based rendering where complex light paths are otherwise
quite challenging to sample from simple importance sampling techniques.
  A lot of research is dedicated towards bringing physical realism to samples
(images) generated from diffusion-based generative models in a data-driven
manner, however, a unified framework connecting these techniques is still
missing. In this course, we take the first steps toward understanding each of
these components and exploring how MCMC could potentially serve as a bridge,
linking these closely related areas of research. Our course aims to provide
necessary theoretical and practical tools to guide students, researchers and
practitioners towards the common goal of generative physically based rendering.
All Jupyter notebooks with demonstrations associated to this tutorial can be
found on the project webpage: https://sinbag.github.io/mcmc/

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [258] [Application of Deep Reinforcement Learning to At-the-Money S&P 500 Options Hedging](https://arxiv.org/abs/2510.09247)
*Zofia Bracha,Paweł Sakowski,Jakub Michańków*

Main category: q-fin.CP

TL;DR: 本文应用深度Q学习（TD3算法）对S&P 500指数平值期权进行对冲，在无明确价格动态模型假设下，通过历史数据训练智能体，在17年样本外评估中优于传统Delta对冲策略，尤其在波动或高成本环境中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习在期权对冲中的应用，旨在开发不依赖特定价格动态模型的灵活对冲策略，以应对传统方法在复杂市场环境中的局限性。

Method: 使用TD3算法构建深度强化学习智能体，基于六个预测变量（期权价格、标的资产价格、虚实程度、到期时间、已实现波动率、当前对冲头寸）进行训练，采用滚动窗口训练方法，覆盖2004-2024年历史日内价格数据。

Result: DRL智能体在17年样本外评估中持续优于Black-Scholes Delta对冲策略，特别是在波动市场和高交易成本环境下表现更稳健。但风险意识参数较高时性能下降，波动率估计周期越长结果越稳定。

Conclusion: 深度强化学习为期权对冲提供了有效的无模型方法，在现实交易环境中展现出优于传统策略的适应性和鲁棒性，但需注意风险参数设置和波动率估计周期的影响。

Abstract: This paper explores the application of deep Q-learning to hedging
at-the-money options on the S\&P~500 index. We develop an agent based on the
Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, trained to
simulate hedging decisions without making explicit model assumptions on price
dynamics. The agent was trained on historical intraday prices of S\&P~500 call
options across years 2004--2024, using a single time series of six predictor
variables: option price, underlying asset price, moneyness, time to maturity,
realized volatility, and current hedge position. A walk-forward procedure was
applied for training, which led to nearly 17~years of out-of-sample evaluation.
The performance of the deep reinforcement learning (DRL) agent is benchmarked
against the Black--Scholes delta-hedging strategy over the same period. We
assess both approaches using metrics such as annualized return, volatility,
information ratio, and Sharpe ratio. To test the models' adaptability, we
performed simulations across varying market conditions and added constraints
such as transaction costs and risk-awareness penalties. Our results show that
the DRL agent can outperform traditional hedging methods, particularly in
volatile or high-cost environments, highlighting its robustness and flexibility
in practical trading contexts. While the agent consistently outperforms
delta-hedging, its performance deteriorates when the risk-awareness parameter
is higher. We also observed that the longer the time interval used for
volatility estimation, the more stable the results.

</details>
