<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.CR](#cs.CR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [math.NT](#math.NT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 6]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Tabularis Formatus: Predictive Formatting for Tables](https://arxiv.org/abs/2508.11121)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Gust Verbruggen*

Main category: cs.DB

TL;DR: TaFo是一种神经符号方法，用于自动生成表格的条件格式建议，解决了用户意识不足、规则创建困难和界面不友好等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的条件格式规则创建复杂，需要技术知识和平台经验，TaFo旨在简化这一过程。

Method: 结合组件合成系统和语言模型的语义知识，采用多样性保留规则排名，自动学习规则触发和视觉格式属性。

Result: TaFo在180万公开工作簿上测试，比现有系统准确率高15.6%--26.5%，生成更准确、多样和完整的建议。

Conclusion: TaFo通过完全预测和自动化，显著提升了条件格式生成的效率和准确性。

Abstract: Spreadsheet manipulation software are widely used for data management and
analysis of tabular data, yet the creation of conditional formatting (CF) rules
remains a complex task requiring technical knowledge and experience with
specific platforms. In this paper we present TaFo, a neuro-symbolic approach to
generating CF suggestions for tables, addressing common challenges such as user
unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo
takes inspiration from component based synthesis systems and extends them with
semantic knowledge of language models and a diversity preserving rule
ranking.Unlike previous methods focused on structural formatting, TaFo uniquely
incorporates value-based formatting, automatically learning both the rule
trigger and the associated visual formatting properties for CF rules. By
removing the dependency on user specification used by existing techniques in
the form of formatted examples or natural language instruction, TaFo makes
formatting completely predictive and automated for the user. To evaluate TaFo,
we use a corpus of 1.8 Million public workbooks with CF and manual formatting.
We compare TaFo against a diverse set of symbolic and neural systems designed
for or adapted for the task of table formatting. Our results show that TaFo
generates more accurate, diverse and complete formatting suggestions than
current systems and outperforms these by 15.6\%--26.5\% on matching user added
ground truth rules in tables.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training](https://arxiv.org/abs/2508.11035)
*Hasibul Jamil,MD S Q Zulkar Nine,Tevfik Kosar*

Main category: cs.DC

TL;DR: EMLIO是一种高效的机器学习I/O服务，通过优化数据加载延迟和能耗，显著提升大规模深度学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模超过本地存储容量，GPU计算速度超过网络和磁盘延迟，大规模深度学习任务面临I/O瓶颈和能耗问题。

Method: EMLIO在存储节点部署轻量级数据服务守护进程，通过序列化、批处理、TCP流传输和乱序预取技术，结合GPU加速预处理。

Result: 在本地磁盘、LAN和WAN环境中，EMLIO比现有加载器快8.6倍，能耗降低10.9倍，且性能不受网络距离影响。

Conclusion: EMLIO为下一代AI云提供了可扩展的能源感知I/O架构。

Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks
as datasets grow beyond local storage capacities and GPU compute outpaces
network and disk latencies. While recent systems optimize data-loading time,
they overlook the energy cost of I/O - a critical factor at large scale. We
introduce EMLIO, an Efficient Machine Learning I/O service that jointly
minimizes end-to-end data-loading latency T and I/O energy consumption E across
variable-latency networked storage. EMLIO deploys a lightweight data-serving
daemon on storage nodes that serializes and batches raw samples, streams them
over TCP with out-of-order prefetching, and integrates seamlessly with
GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive
evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)
environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use
compared to state-of-the-art loaders, while maintaining constant performance
and energy profiles irrespective of network distance. EMLIO's service-based
architecture offers a scalable blueprint for energy-aware I/O in
next-generation AI clouds.

</details>


### [3] [Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets](https://arxiv.org/abs/2508.11266)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.DC

TL;DR: 提出了一种双层代币化架构，通过元素代币和整体代币增强复杂资产的流动性和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统框架难以交易或分割大型、异构的资产（如矿山、电厂），需要一种新方法提升其流动性和透明度。

Method: 引入元素代币（标准化、抵押的资产组件）和整体代币（资产的固定组合），支持双向转换和套利机制。

Result: 通过能源和工业领域的案例，证明该方法能将高价值非流动性资产分割并交易，类似股票或ETF。

Conclusion: 该架构降低了投资门槛，改善了价格发现和融资灵活性，但需考虑实施和监管问题。

Abstract: Alternative assets such as mines, power plants, or infrastructure projects
are often large, heterogeneous bundles of resources, rights, and outputs whose
value is difficult to trade or fractionalize under traditional frameworks. This
paper proposes a novel two-tier tokenization architecture to enhance the
liquidity and transparency of such complex assets. We introduce the concepts of
Element Tokens and Everything Tokens: elemental tokens represent standardized,
fully collateralized components of an asset (e.g., outputs, rights, or
credits), while an everything token represents the entire asset as a fixed
combination of those elements. The architecture enables both fine-grained
partial ownership and integrated whole-asset ownership through a system of
two-way convertibility. We detail the design and mechanics of this system,
including an arbitrage mechanism that keeps the price of the composite token
aligned with the net asset value of its constituents. Through illustrative
examples in the energy and industrial sectors, we demonstrate that our approach
allows previously illiquid, high-value projects to be fractionalized and traded
akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for
investors and asset owners, such as lower entry barriers, improved price
discovery, and flexible financing, as well as the considerations for
implementation and regulation.

</details>


### [4] [Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive](https://arxiv.org/abs/2508.11298)
*Gabin Schieffer,Jacob Wahlgren,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: 本文研究了AMD MI300A APU在多APU系统中的通信效率，通过设计基准测试比较了HIP API、MPI和RCCL库的性能，并优化了两个HPC应用。


<details>
  <summary>Details</summary>
Motivation: 随着GPU计算性能的提升，HPC应用中高效的数据移动成为关键。AMD MI300A APU通过集成CPU、GPU和HBM，为解决CPU-GPU数据移动问题提供了方案。

Method: 设计了基准测试评估GPU直接内存访问、显式APU间数据移动和多APU集体通信，并比较了HIP API、MPI和RCCL库的效率。

Result: 研究揭示了在Infinity Fabric上优化多APU通信的关键设计选择，包括编程接口、分配器和数据移动策略。

Conclusion: 通过优化Quicksilver和CloverLeaf两个HPC应用，验证了多APU系统的性能提升潜力。

Abstract: The ever-increasing compute performance of GPU accelerators drives up the
need for efficient data movements within HPC applications to sustain
performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD
MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth
memory (HBM) within a single physical package. Leadership supercomputers, such
as El Capitan, group four APUs within a single compute node, using Infinity
Fabric Interconnect. In this work, we design specific benchmarks to evaluate
direct memory access from the GPU, explicit inter-APU data movement, and
collective multi-APU communication. We also compare the efficiency of HIP APIs,
MPI routines, and the GPU-specialized RCCL library. Our results highlight key
design choices for optimizing inter-APU communication on multi-APU AMD MI300A
systems with Infinity Fabric, including programming interfaces, allocators, and
data movement. Finally, we optimize two real HPC applications, Quicksilver and
CloverLeaf, and evaluate them on a four MI100A APU system.

</details>


### [5] [Space-efficient population protocols for exact majority in general graphs](https://arxiv.org/abs/2508.11384)
*Joel Rybicki,Jakob Solnerzik,Olivier Stietel,Robin Vacus*

Main category: cs.DC

TL;DR: 研究了在群体协议模型中精确多数共识问题，改进了通用图的时间上下界，并提出了新的参数化上界。


<details>
  <summary>Details</summary>
Motivation: 解决在通用图中精确多数共识的时间复杂度和空间复杂度问题，优化协议性能。

Method: 提出新的协议，参数化松弛时间和图的度不平衡，分析其稳定时间和状态数。

Result: 在通用图中，协议稳定时间为O(Δ/δ τ_rel log²n)，状态数为O(log n (log(Δ/δ) + log(τ_rel/n))。对于正则扩展图，匹配最优空间复杂度Θ(log n)。

Conclusion: 改进了精确多数共识的上下界，为不同图结构提供了高效协议。

Abstract: We study exact majority consensus in the population protocol model. In this
model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in
each time step, a scheduler samples uniformly at random a pair of adjacent
nodes to interact. In the exact majority consensus task, each node is given a
binary input, and the goal is to design a protocol that almost surely reaches a
stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general
graphs. First, we give asymptotically tight time lower bounds for general
(unbounded space) protocols. Second, we obtain new upper bounds parameterized
by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced
by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically,
we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta}
\tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high
probability and uses $O\left( \log n \cdot \left(
\log\left(\tfrac{\Delta}{\delta}\right) + \log
\left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any
graph with minimum degree at least $\delta$ and maximum degree at most
$\Delta$.
  For regular expander graphs, this matches the optimal space complexity of
$\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA
2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of
$O(n \log^2 n)$ steps. Finally, we give a new upper bound of
$O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a
constant-state protocol.

</details>


### [6] [Time, Fences and the Ordering of Events in TSO](https://arxiv.org/abs/2508.11415)
*Raïssa Nataf,Yoram Moses*

Main category: cs.DC

TL;DR: 论文提出了一种语义框架，用于精确确定在TSO内存模型下何时需要同步操作（如内存栅栏或原子操作），并引入了一种新的TSO特定的occurs-before关系。


<details>
  <summary>Details</summary>
Motivation: TSO内存模型通过延迟写操作的可见性支持硬件优化，但增加了正确性推理的复杂性，需要同步操作，而同步操作可能带来性能开销。

Method: 引入TSO特定的occurs-before关系，扩展了Lamport的happens-before关系，并证明事件间时间顺序的唯一方式是通过occurs-before链。

Result: 通过分析栅栏和原子操作在创建occurs-before链中的作用，确定了这些同步操作不可避免的情况。

Conclusion: 研究为TSO模型提供了理论基础，扩展了异步系统中的通信推理方法，并推广了共享内存对象线性化实现的下界。

Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory
model in multiprocessor architectures, widely implemented, for example in
Intel's x86 and x64 platforms. It allows processes to delay the visibility of
writes through store buffering. While this supports hardware-level
optimizations and makes a significant contribution to multiprocessor
efficiency, it complicates reasoning about correctness, as executions may
violate sequential consistency. Ensuring correct behavior often requires
inserting synchronization primitives such as memory fences ($F$) or atomic
read-modify-write ($RMW$) operations, but this approach can incur significant
performance costs. In this work, we develop a semantic framework that precisely
characterizes when such synchronization is necessary under TSO. We introduce a
novel TSO-specific occurs-before relation, which adapts Lamport's celebrated
happens-before relation from asynchronous message-passing systems to the TSO
setting. Our main result is a theorem that proves that the only way to ensure
that two events that take place at different sites are temporally ordered is by
having the execution create an occurs-before chain between the events. By
studying the role of fences and $RMW$s in creating occurs-before chains, we are
then able to capture cases in which these costly synchronization operations are
unavoidable. Since proper real-time ordering of events is a fundamental aspect
of consistency conditions such as Linearizability, our analysis provides a
sound theoretical understanding of essential aspects of the TSO model. In
particular, we are able to generalize prior lower bounds for linearizable
implementations of shared memory objects. Our results capture the structure of
information flow and causality in the TSO model by extending the standard
communication-based reasoning from asynchronous systems to the TSO memory
model.

</details>


### [7] [Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method](https://arxiv.org/abs/2508.11467)
*Shifang Liu,Huiyuan Li,Hongjiao Sheng,Haoyuan Gui,Xiaoyu Zhang*

Main category: cs.DC

TL;DR: 提出了一种基于GPU的SVD算法，通过优化数据布局和计算流程，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法在异构系统中因CPU-GPU数据传输频繁和面板分解速度慢而受限。

Method: 采用GPU为中心的设计，引入基于GPU的双对角分治法（BDC），优化BLAS利用率和算术强度。

Result: 在AMD MI210和NVIDIA V100 GPU上，相比rocSOLVER/cuSOLVER和MAGMA，速度提升分别达1293.64x/7.47x和14.10x/12.38x。

Conclusion: 新方法通过消除CPU-GPU数据传输和异步执行，显著提升了SVD计算效率。

Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization
technique in linear algebra, widely applied in numerous matrix-related
problems. However, traditional SVD approaches are hindered by slow panel
factorization and frequent CPU-GPU data transfers in heterogeneous systems,
despite advancements in GPU computational capabilities. In this paper, we
introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based
bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and
data layout of different steps for SVD computation, performing all panel-level
computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU
data transfers. Furthermore, we integrate related computations to optimize BLAS
utilization, thereby increasing arithmetic intensity and fully leveraging the
computational capabilities of GPUs. Additionally, we introduce a newly
developed GPU-based BDC algorithm that restructures the workflow to eliminate
matrix-level CPU-GPU data transfers and enable asynchronous execution between
the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs
demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x
and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [8] [A Gentle Wakeup Call: Symmetry Breaking with Less Collision Cost](https://arxiv.org/abs/2508.11006)
*Umesh Biswas,Maxwell Young*

Main category: cs.DS

TL;DR: 论文提出了一种名为Aim-High的随机唤醒算法，旨在解决对称性打破问题，显著降低了碰撞成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有唤醒算法在延迟方面表现良好，但忽略了碰撞带来的高成本，导致总延迟主要由碰撞贡献。

Method: 设计了随机唤醒算法Aim-High，针对静态和动态版本分别优化，以减少碰撞成本和延迟。

Result: Aim-High在碰撞成本C较大时，延迟和碰撞成本接近O(√C)；否则，静态场景为O(poly(log n))，动态场景为O(n poly(log n))。

Conclusion: Aim-High算法在降低延迟和碰撞成本方面表现优异，并通过下界结果验证了其有效性。

Abstract: The wakeup problem addresses the fundamental challenge of symmetry breaking.
There are $n$ devices sharing a time-slotted multiple access channel. In any
fixed slot, if a single device sends a packet, it succeeds; however, if two or
more devices send, then there is a collision and none of the corresponding
packets succeed. For the static version of wakeup, all packets are initially
active (i.e., can send and listen on the channel); for the dynamic version, the
packets become active at arbitrary times. In both versions, the goal is to
successfully send a single packet.
  Prior results on wakeup have largely focused on the number of slots until the
first success; that is, the latency. However, in many modern systems,
collisions introduce significant delay, an aspect that current wakeup
algorithms do not address. For instance, while existing results for static
wakeup have polylogarithmic-in-$n$ latency, they can incur additional latency
that is {\it linear} in the cost of a collision $C$. Thus, the total latency is
large and dominated by the contributions from collisions.
  Here, we design and analyze a randomized wakeup algorithm, Aim-High. For
sufficiently large $C$ and with bounded error, Aim-High has latency and
expected collision cost that is nearly $O(\sqrt{C})$ for both the static and
dynamic versions. Otherwise, the latency and expected collision cost are
$O(\texttt{poly}{(\log n)})$ for the static setting, and
$O(n\,\texttt{poly}{(\log n)})$ for the dynamic setting. We also establish
lower bounds that complement these results.

</details>


### [9] [Sampling tree-weighted partitions without sampling trees](https://arxiv.org/abs/2508.11130)
*Sarah Cannon,Wesley Pegden,Jamie Tucker-Foltz*

Main category: cs.DS

TL;DR: 本文提出了一种新的算法，用于直接采样平衡树加权2分区，避免了先采样生成树的瓶颈，时间效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 计算重划分析中对平衡树加权k分区的需求推动了研究，现有方法因需先采样生成树而效率低下。

Method: 提出直接采样平衡树加权2分区的方法，无需先采样生成树，适用于一类典型平面图。

Result: 算法在典型平面图上预期线性时间O(n)，比现有生成树方法更快，且改进了一致随机树的精确采样效率。

Conclusion: 新算法显著提升了采样效率，为计算重划等应用提供了更高效的工具。

Abstract: This paper gives a new algorithm for sampling tree-weighted partitions of a
large class of planar graphs. Formally, the tree-weighted distribution on
$k$-partitions of a graph weights $k$-partitions proportional to the product of
the number of spanning trees of each partition class. Recent work on problems
in computational redistricting analysis has driven special interest in the
conditional distribution where all partition classes have the same size
(balanced partitions). One class of Markov chains in wide use aims to sample
from balanced tree-weighted $k$-partitions using a sampler for balanced
tree-weighted 2-partitions. Previous implementations of this 2-partition
sampler would draw a random spanning tree and check whether it contains an edge
whose removal produces a balanced 2-component forest; if it does, this
2-partition is accepted, otherwise the algorithm rejects and repeats. In
practice, this is a significant computational bottleneck.
  We show that in fact it is possible to sample from the balanced tree-weighted
2-partition distribution directly, without first sampling a spanning tree; the
acceptance and rejection rates are the same as in previous samplers. We prove
that on a wide class of planar graphs encompassing network structures typically
arising from the geographic data used in computational redistricting, our
algorithm takes expected linear time $O(n)$. Notably, this is asymptotically
faster than the best known method to generate random trees, which is $O(n
\log^2 n)$ for approximate sampling and $O(n^{1 + \log \log \log n / \log \log
n})$ for exact sampling. Additionally, we show that a variant of our algorithm
also gives a speedup to $O(n \log n)$ for exact sampling of uniformly random
trees on these families of graphs, improving the bounds for both exact and
approximate sampling.

</details>


### [10] [Face-hitting dominating sets in planar graphs: Alternative proof and linear-time algorithm](https://arxiv.org/abs/2508.11444)
*Therese Biedl*

Main category: cs.DS

TL;DR: 论文提出了一种新的构造性证明方法，将平面图的顶点划分为两个支配且面覆盖的集合，且算法复杂度为线性时间。


<details>
  <summary>Details</summary>
Motivation: 之前的证明依赖于四色定理且非构造性，难以实现。本文旨在提供一种更简单、可实现的构造性证明。

Method: 通过将图分解为2-连通分量、寻找耳分解以及在3-正则平面图中计算完美匹配，构造顶点划分。

Result: 证明了平面图可以在线性时间内划分为两个支配且面覆盖的集合。

Conclusion: 新方法简化了证明过程，并提供了高效的算法实现。

Abstract: In a recent paper, Francis, Illickan, Jose and Rajendraprasad showed that
every $n$-vertex plane graph $G$ has (under some natural restrictions) a
vertex-partition into two sets $V_1$ and $V_2$ such that each $V_i$ is
\emph{dominating} (every vertex of $G$ contains a vertex of $V_i$ in its closed
neighbourhood) and \emph{face-hitting} (every face of $G$ is incident to a
vertex of $V_i$). Their proof works by considering a supergraph $G'$ of $G$
that has certain properties, and among all such graphs, taking one that has the
fewest edges. As such, their proof is not algorithmic. Their proof also relies
on the 4-color theorem, for which a quadratic-time algorithm exists, but it
would not be easy to implement.
  In this paper, we give a new proof that every $n$-vertex plane graph $G$ has
(under the same restrictions) a vertex-partition into two dominating
face-hitting sets. Our proof is constructive, and requires nothing more
complicated than splitting a graph into 2-connected components, finding an ear
decomposition, and computing a perfect matching in a 3-regular plane graph. For
all these problems, linear-time algorithms are known and so we can find the
vertex-partition in linear time.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: 研究探讨了GPT在GitHub代码审查流程中的影响，发现其显著减少了解析时间，优化了各阶段效率，并帮助开发者。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在软件开发中广泛应用，但其在代码审查各阶段的具体效果尚未充分研究。

Method: 通过半自动化方法识别GPT辅助的PRs，并应用统计模型（线性回归和Mann-Whitney U检验）分析数据。

Result: GPT辅助的PRs中位解析时间减少60%，审查时间减少33%，等待时间减少87%。

Conclusion: GPT能显著提升代码审查效率，为团队协作提供实用建议。

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [12] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 论文探讨了如何利用预训练的代码扩散模型解决代码的最后一英里修复问题，提出了两种应用方法，并在三个领域进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 代码扩散模型在后期步骤中生成的代码片段差异类似于最后一英里修复，因此研究如何利用这一特性解决代码修复问题。

Method: 1. 通过向损坏代码片段添加噪声并恢复扩散过程进行修复；2. 通过采样中间程序和最终程序生成大量训练数据。

Result: 在Python、Excel和PowerShell三个领域进行了实验，验证了方法的有效性。

Conclusion: 代码扩散模型可以有效地用于最后一英里修复任务，并生成训练数据。

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [13] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: AI agentic programming是一种新兴范式，利用大型语言模型（LLMs）自主规划、执行并与外部工具交互，完成复杂软件开发任务。本文综述了其范围、技术基础和开放研究挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AI agentic编程快速发展，需要明确其范围、巩固技术基础并识别研究挑战。

Method: 提出代理行为和系统架构的分类法，并研究规划、内存管理、工具集成和执行监控等核心技术。

Result: 分析了现有基准和评估方法，指出处理长上下文、持久内存、安全性和与人类协作等挑战。

Conclusion: 总结了近期进展和未来方向，为构建下一代智能可信的AI编码代理提供基础。

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [14] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: RevPerf是一种通过Google Play评论检测移动应用性能问题的工具，结合评论分析和执行代理，成功率达到70%。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能问题在开发环境中难以检测，影响用户体验，需一种有效工具来解决。

Method: 利用Google Play评论，通过提示工程丰富信息，执行代理生成命令复现问题，结合日志、GUI和资源监控检测问题。

Result: 实验显示RevPerf在构建的数据集上复现性能问题的成功率为70%。

Conclusion: RevPerf能有效复现性能问题，为开发者提供实用工具。

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [15] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: PTMPicker是一种新方法，通过结构化模板和嵌入相似性计算，帮助用户更准确地选择预训练模型（PTM），解决了传统关键词搜索的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键词的PTM搜索方法难以全面捕捉用户意图，尤其是在考虑偏差缓解、硬件要求或许可证合规性等因素时。

Method: 定义结构化模板表示PTM和用户需求，计算嵌入相似性评估功能相关属性，并通过提示评估特殊约束。

Result: 在Hugging Face的543,949个PTM上测试，PTMPicker成功为85%的请求在前10名候选中找到合适模型。

Conclusion: PTMPicker显著提升了PTM选择的准确性和效率，适用于复杂需求场景。

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [16] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 论文提出ORFuzz框架，用于系统检测和分析大型语言模型（LLMs）的过度拒绝问题，通过进化测试生成多样化的测试用例，显著提高了检测率。


<details>
  <summary>Details</summary>
Motivation: LLMs因过度保守的安全措施而错误拒绝良性查询，现有测试方法存在缺陷，亟需改进。

Method: ORFuzz框架整合了安全类别感知种子选择、自适应变异优化和人类对齐的评判模型OR-Judge。

Result: ORFuzz检测率（6.98%）是现有方法的两倍，并构建了ORFuzzSet基准，覆盖10种LLMs，拒绝率达63.56%。

Conclusion: ORFuzz和ORFuzzSet为开发更可靠的LLM系统提供了自动化测试框架和社区资源。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [17] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 论文研究了代码生成中LLM的幻觉现象，特别是在汽车领域，发现复杂提示下GPT-4.1和GPT-4o表现最佳，但仍需缓解技术以确保安全性。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中潜力巨大，但幻觉问题限制了其实际应用，尤其是在安全关键领域如汽车软件。

Method: 通过案例研究评估多种代码LLM在不同提示复杂度下的表现，包括简单提示、带VSS上下文的提示和带代码骨架的提示。

Result: 复杂提示下GPT-4.1和GPT-4o能生成正确代码，但其他模型和简单提示均失败，且存在语法错误和API冲突。

Conclusion: 需开发有效缓解技术以确保LLM生成代码的安全性和可靠性，特别是在汽车等关键领域。

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [18] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 论文提出了一种全面的日志代码缺陷分类法，并构建了一个真实世界日志缺陷的数据集。通过评估大型语言模型（LLMs）在检测日志缺陷中的表现，发现仅基于源代码时效果不佳，但加入相关知识后检测准确率提升10.9%。


<details>
  <summary>Details</summary>
Motivation: 日志代码在系统运行时行为捕获中至关重要，但缺陷可能导致日志误用。现有研究局限于少数缺陷模式，缺乏系统性分析，且LLMs在日志缺陷检测中的潜力尚未充分探索。

Method: 提出七种日志代码缺陷模式及14种详细场景的分类法，构建包含164个真实缺陷的数据集，并设计自动化框架评估LLMs的检测能力。

Result: LLMs仅基于源代码检测效果不佳，但加入相关知识后检测准确率提升10.9%。

Conclusion: 研究为实践者提供了避免常见缺陷的指导，并为改进基于LLM的日志缺陷检测奠定了基础。

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [19] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY是首个评估LLM翻译代码执行效率的基准测试，揭示了当前LLM在代码翻译中效率问题，尤其是算法缺陷和资源处理不当。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码翻译中虽提高了正确性，但执行效率被忽视，需填补这一空白。

Method: 通过LLM驱动的两阶段流程构建TRACY：首先生成压力测试放大性能差异，再通过效率导向的任务剪枝筛选任务。

Result: 评估26个LLM显示，即使顶级模型在效率上也表现不佳，算法缺陷和资源处理问题导致显著性能下降。

Conclusion: 未来LLM代码翻译需同时优化正确性和效率。

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [20] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: 论文探讨了从微服务系统获取时间网络并用时间网络方法分析的挑战，指出现有数据规模限制了分析潜力。


<details>
  <summary>Details</summary>
Motivation: 研究微服务架构的时间网络特性，以更好地理解和优化其动态行为。

Method: 利用网络科学中的时间网络分析方法，研究微服务系统的依赖关系随时间的变化。

Result: 获取的最完整时间网络仅包含7个时间实例和42个微服务，限制了分析的深度和广度。

Conclusion: 当前数据规模不足以支持全面的时间网络分析，未来需扩展数据规模以提升研究价值。

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices](https://arxiv.org/abs/2508.11342)
*Linh-An Phan,MingXue Wang,Guangyu Wu,Wang Dawei,Chen Liqun,Li Jin*

Main category: cs.NI

TL;DR: CrossTrace是一种无需修改源代码的分布式追踪解决方案，通过贪婪算法和eBPF技术高效关联跨服务调用，解决了零代码实现中的跨度关联挑战。


<details>
  <summary>Details</summary>
Motivation: 现代微服务应用中，分布式追踪对调试至关重要，但现有零代码方案在跨度关联上存在性能、安全或复杂度问题。

Method: CrossTrace利用贪婪算法推断服务内跨度关系，并通过eBPF在TCP包头嵌入标识符实现跨服务关联。

Result: 实验表明，CrossTrace能在几秒内以超过90%的准确率关联数千个跨度，适合生产环境。

Conclusion: CrossTrace为微服务观测和诊断提供了一种高效、安全的解决方案。

Abstract: Distributed tracing has become an essential technique for debugging and
troubleshooting modern microservice-based applications, enabling software
engineers to detect performance bottlenecks, identify failures, and gain
insights into system behavior. However, implementing distributed tracing in
large-scale applications remains challenging due to the need for extensive
instrumentation. To reduce this burden, zero-code instrumentation solutions,
such as those based on eBPF, have emerged, allowing span data to be collected
without modifying application code. Despite this promise, span correlation, the
process of establishing causal relationships between spans, remains a critical
challenge in zero-code approaches. Existing solutions often rely on thread
affinity, compromise system security by requiring the kernel integrity mode to
be disabled, or incur significant computational overhead due to complex
inference algorithms. This paper presents CrossTrace, a practical and efficient
distributed tracing solution designed to support the debugging of microservice
applications without requiring source code modifications. CrossTrace employs a
greedy algorithm to infer intra-service span relationships from delay patterns,
eliminating reliance on thread identifiers. For inter-service correlation,
CrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling
secure and efficient correlation compromising system security policies.
Evaluation results show that CrossTrace can correlate thousands of spans within
seconds with over 90% accuracy, making it suitable for production deployment
and valuable for microservice observability and diagnosis.

</details>


### [22] [Optimizing ROS 2 Communication for Wireless Robotic Systems](https://arxiv.org/abs/2508.11366)
*Sanghoon Lee,Taehun Kim,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: 论文分析了ROS 2中DDS通信栈在无线传输大负载时的性能问题，提出了轻量级优化框架，显著提升了传输效率。


<details>
  <summary>Details</summary>
Motivation: ROS 2的DDS通信栈在无线传输大负载时性能下降，但原因未明，需深入分析并提出解决方案。

Method: 通过网络层分析发现IP分片过多、重传时机低效和缓冲区拥塞问题，提出基于XML QoS配置的优化框架。

Result: 优化框架在多种无线场景下成功传输大负载，同时保持低延迟。

Conclusion: 提出的框架无需修改协议或额外组件，通过简单配置即可显著提升ROS 2在无线环境中的性能。

Abstract: Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.

</details>


### [23] [D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications](https://arxiv.org/abs/2508.11475)
*Ioannis Panitsas,Akrit Mudvari,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出了一种基于强化学习的D2Q Synchronizer算法，用于优化分布式SDN中的网络和用户性能。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SDN同步策略未综合考虑网络和用户性能的联合优化。

Method: 采用强化学习算法D2Q Synchronizer，策略性地将时间敏感任务卸载到成本效益高的边缘服务器。

Result: 相比启发式和其他学习策略，网络成本分别降低至少45%和10%，同时满足所有任务的延迟要求。

Conclusion: D2Q Synchronizer在多域动态SDN网络中显著优化性能并满足QoS需求。

Abstract: In distributed Software-Defined Networking (SDN), distributed SDN controllers
require synchronization to maintain a global network state. Despite the
availability of synchronization policies for distributed SDN architectures,
most policies do not consider joint optimization of network and user
performance. In this work, we propose a reinforcement learning-based algorithm
called D2Q Synchronizer, to minimize long-term network costs by strategically
offloading time-sensitive tasks to cost-effective edge servers while satisfying
the latency requirements for all tasks. Evaluation results demonstrate the
superiority of our synchronizer compared to heuristic and other learning
policies in literature, by reducing network costs by at least 45% and 10%,
respectively, while ensuring the QoS requirements for all user tasks across
dynamic and multi-domain SDN networks.

</details>


### [24] [Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles](https://arxiv.org/abs/2508.11574)
*Mohammad Sajid Shahriar,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 论文提出了一种分布式计算架构，结合数字孪生和移动边缘计算，以提升智能交通系统的低延迟服务能力。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在智能交通系统中潜力巨大，但如何通过高效计算资源管理确保其不间断运行仍是一个挑战。

Method: 采用分布式计算架构，结合数字孪生和移动边缘计算，开发了一种网络感知的可扩展协作任务分配算法。

Result: 框架显著提升了数字孪生操作的鲁棒性和可扩展性，同步错误降至5%，边缘计算资源利用率高达99.5%。

Conclusion: 该架构为智能交通系统提供了高效、低延迟的解决方案，具有实际应用潜力。

Abstract: The next generation networks offers significant potential to advance
Intelligent Transportation Systems (ITS), particularly through the integration
of Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs
through efficient computing resource management remains an open challenge. This
paper introduces a distributed computing archi tecture that integrates DTs and
Mobile Edge Computing (MEC) within a software-defined vehicular networking
framework to enable intelligent, low-latency transportation services. A network
aware scalable collaborative task provisioning algorithm is de veloped to train
an autonomous agent, which is evaluated using a realistic connected autonomous
vehicle (CAV) traffic simulation. The proposed framework significantly enhances
the robustness and scalability of DT operations by reducing synchronization
errors to as low as 5% while achieving up to 99.5% utilization of edge
computing resources.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的压缩元学习框架，通过优化编码和解码阶段，提高了压缩学习的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的快速扩大，需要快速高效的参数学习技术。传统压缩学习方法未能充分利用数据的底层结构。

Method: 提出压缩元学习框架，使用神经网络优化编码和解码阶段，应用于多种任务（如PCA、岭回归、k-means和自编码器）。

Result: 新框架在速度和准确性上优于现有方法。

Conclusion: 压缩元学习框架为高效、隐私友好的学习提供了新方向。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [26] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 本文提出了一种基于合作博弈的多准则投票集成方法，以克服传统集成学习中单一评价标准的局限性，并通过实验验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 传统投票集成方法仅考虑单一评价标准，无法充分利用分类器的多种先验信息，限制了模型性能的提升。

Method: 提出了一种基于合作博弈的多准则决策方法，综合考虑分类器的多种先验信息，实现权重分配的优化。

Result: 在Open-ML-CC18数据集上的实验表明，该方法优于其他权重分配方法。

Conclusion: 通过合作博弈多准则决策，能够更全面地利用分类器信息，显著提升集成模型的性能。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [27] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker是一个15B参数的模型，性能媲美32B参数模型，但内存占用减半。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内存和计算成本上的限制，使其更适合企业应用。

Method: 采用四阶段训练流程：基础模型扩展、持续预训练、监督微调、GRPO强化学习。

Result: 在多项基准测试中表现优于或持平32B参数模型。

Conclusion: Apriel-Nemotron-15B-Thinker在性能和效率上取得了平衡，适合企业部署。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [28] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 提出了一种基于提示的持续学习方法（PCL），用于解决医学领域数据共享受限和分布偏移问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域的数据共享受限，传统方法易过拟合或遗忘知识，现有持续学习方法多针对自然图像，医学领域研究不足。

Method: 采用统一的提示池和最小扩展策略，通过冻结部分提示减少计算开销，并引入新的正则化项平衡保留与适应。

Result: 在三个糖尿病视网膜病变数据集上，分类准确率提升至少10%，F1分数提高9分，同时降低推理成本。

Conclusion: PCL方法有望推动可持续医学AI发展，支持实时诊断和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [29] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert是一个可解释的逆合成预测框架，结合大语言模型和专用模型，通过强化学习生成自然语言解释，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有逆合成预测模型依赖静态模式匹配，缺乏逻辑决策能力，导致黑箱决策。

Method: Retro-Expert通过三个组件实现协作推理：(1)专用模型进行浅层推理，(2)大语言模型进行关键推理，(3)强化学习优化决策策略。

Result: 实验表明，Retro-Expert在性能上超越现有模型，并提供专家认可的解释。

Conclusion: Retro-Expert不仅提升了预测性能，还通过解释性推理弥合了AI预测与化学实践之间的差距。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [30] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb框架通过优化合成数据生成，显著提升预训练性能，超越现有合成数据集，并提供关于合成数据质量的深入见解。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型预训练中数据量扩展的收益递减问题，探索合成数据作为性能提升的新途径。

Method: 开发BeyondWeb框架，生成高质量合成数据，并通过14项基准测试验证其性能。

Result: BeyondWeb在性能上超越Cosmopedia和Nemotron-Synth，训练速度更快，小模型表现优于大模型。

Conclusion: 高质量合成数据需多因素联合优化，BeyondWeb展示了其潜力，但需科学方法和实践经验支持。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [31] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为M&C的模型选择框架，帮助用户从模型平台中高效选择预训练的文本到图像（T2I）模型，而无需对所有模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 预训练的T2I模型虽促进了模型的民主化，但用户面临如何选择最适合目标数据域的模型的挑战。目前缺乏针对T2I模型的性能评估方法。

Method: M&C框架通过构建匹配图（包含模型和数据集节点及性能/相似性边），并基于图嵌入特征预测最佳微调模型。

Result: 在10个T2I模型和32个数据集的测试中，M&C成功预测最佳模型的准确率为61.3%，其余情况下也能选出性能接近的模型。

Conclusion: M&C为T2I模型的微调选择提供了高效解决方案，显著减少了实验成本。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [32] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: CURE框架通过两阶段方法（高熵关键令牌再生和静态初始状态采样）解决了RLVR中熵崩溃问题，提升了LLM的推理能力和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR中静态初始状态采样导致的熵崩溃和低多样性问题。

Method: 提出CURE框架，第一阶段通过高熵关键令牌再生优化轨迹，第二阶段继续静态采样以增强利用。

Result: 在Qwen-2.5-Math-7B上，CURE在六个数学基准测试中性能提升5%，熵和准确率均达到最优。

Conclusion: CURE有效平衡探索与利用，显著提升LLM的推理性能和多样性。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [33] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 本文通过扩展强彩票假设（SLTH）框架，研究了量化神经网络的理论基础，证明了在有限精度网络中，目标离散神经网络可以被精确表示，并给出了初始网络过参数化的最优界限。


<details>
  <summary>Details</summary>
Motivation: 量化是提高神经网络效率的关键技术，但现有理论主要针对连续设置，无法直接应用于量化网络。本文旨在填补这一理论空白。

Method: 基于Borgs等人的数划分问题研究，推导了量化随机子集和问题的新理论结果，并扩展了SLTH框架到有限精度网络。

Result: 在量化设置中，目标离散神经网络可以被精确表示，且初始网络的过参数化界限是最优的。

Conclusion: 本文为量化神经网络提供了新的理论基础，扩展了SLTH的应用范围，并证明了量化网络中的精确表示可能性。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [34] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出了一种名为zono-conformal prediction的新方法，通过构建预测zonotopes来解决现有方法计算成本高、数据需求大以及无法捕捉多维输出依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有conformal prediction方法计算成本高、数据需求大，且通常使用区间表示预测集，无法有效捕捉多维输出的依赖性。

Method: 引入zono-conformal prediction，基于区间预测模型和reachset-conformant identification，通过单一线性程序构建预测zonotopes。

Result: zono-conformal predictors在回归和分类任务中表现优异，比现有方法更少保守，同时保持相似的测试数据覆盖率。

Conclusion: zono-conformal prediction是一种高效、数据利用率高的方法，适用于非线性基础预测器，并在实验中展示了优越性能。

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [35] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 论文探讨了学习或更新信念中的“信心”概念，区分了它与概率或似然的不同，并提出了两种测量方法。


<details>
  <summary>Details</summary>
Motivation: 研究学习过程中信心的作用及其与概率的区别，以更全面地理解信念更新。

Method: 形式化学习信心的公理，提出两种连续测量方法，并证明其普适性。在附加假设下，推导出更简洁的向量场和损失函数表示。

Result: 证明了信心可以统一表示为连续测量，并展示了其在优化学习中的特殊形式（如贝叶斯规则）。

Conclusion: 信心是学习中的独立概念，可通过连续测量和优化表示统一理解，扩展了信念更新的理论框架。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [36] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 论文提出了一种广义非正态分布（广义非正态正态分布），其精度矩阵仍能反映变量间的条件独立性，类似于高斯分布。并提供了高效算法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究非高斯分布中如何通过精度矩阵推断变量间的条件独立性，扩展高斯分布的理论。

Method: 定义广义非正态正态分布，并提出一种计算高效算法，利用精度矩阵恢复条件独立性结构。

Result: 通过合成实验和实际数据验证了算法的有效性。

Conclusion: 广义非正态正态分布为分析非高斯数据的条件独立性提供了新工具，算法实用且高效。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [37] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 该论文研究了LIME和SHAP等事后解释方法在对抗性操纵下的脆弱性，并提出了一种模块化测试框架来评估增强和集成解释方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释方法（如LIME和SHAP）可能被操纵以掩盖模型的有害偏见，因此需要研究其脆弱性并改进鲁棒性。

Method: 通过复制COMPAS实验建立基线，引入模块化测试框架，评估多种LIME/SHAP集成配置在分布外模型上的表现。

Result: 研究发现某些配置能显著提高偏见检测能力，增强高风险机器学习系统的透明度。

Conclusion: 论文提出的方法有助于提升解释方法的鲁棒性，为高风险机器学习系统的透明部署提供了潜在解决方案。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [38] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 提出了一种基于丰度的Set Transformer方法，用于构建微生物组样本的固定大小嵌入，通过加权序列嵌入提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略分类群丰度的生物学重要性，仅通过简单平均序列嵌入表示样本，限制了微生物组样本表征的准确性。

Method: 提出一种丰度感知的Set Transformer变体，通过复制嵌入向量并按丰度加权，使用自注意力机制聚合，无需修改模型架构。

Result: 在真实微生物组分类任务中，该方法优于平均池化和未加权的Set Transformer，某些情况下达到完美性能。

Conclusion: 丰度感知聚合方法为微生物组表征提供了更鲁棒且生物学信息更丰富的解决方案，是首个将序列丰度整合到基于Transformer的样本嵌入中的方法之一。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [39] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: 本文提出了一种经济可行的预测编码方法，通过数据管理流程将即时消息分组为日聊天，结合特征选择和逻辑回归分类器，并通过降维提升性能。


<details>
  <summary>Details</summary>
Motivation: 即时消息因其非正式性和短小篇幅，为法律行业的预测编码（文档分类）带来了额外挑战。

Method: 采用数据管理流程将消息分组为日聊天，进行特征选择并使用逻辑回归分类器，同时通过降维（重点关注定量特征）提升模型性能。

Result: 在Instant Bloomberg数据集上测试，该方法在定量信息丰富的环境中表现良好，并展示了成本节约的实例。

Conclusion: 该方法为即时消息的预测编码提供了经济高效的解决方案，并通过降维进一步优化了性能。

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [40] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 提出了一种基于相对优势的去偏框架，通过比较用户和物品组的参考分布来修正观看时间，从而提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 原始观看时间受视频时长、流行度和用户行为等混杂因素影响，可能导致推荐模型偏差。

Method: 采用两阶段架构，分离分布估计和偏好学习，并使用分布嵌入参数化观看时间分位数。

Result: 离线和在线实验显示，该方法在推荐准确性和鲁棒性上显著优于基线方法。

Conclusion: 提出的框架有效解决了观看时间作为满意度代理的偏差问题，提升了推荐系统的性能。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [41] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 该论文提出了一种多模态系统，用于早期预测ICD代码分配，结合临床记录和电子健康记录中的表格数据，通过预训练编码器和跨模态注意力提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 早期预测ICD代码分配有助于识别健康风险、优化治疗方案和资源分配，但现有研究多关注出院后分类，缺乏对入院初期有限信息的建模。

Method: 提出多模态系统，融合临床记录和表格数据，使用预训练编码器、特征池化和跨模态注意力学习最优表示，并引入加权时间损失函数。

Result: 实验表明，该方法在早期预测任务中优于现有最佳系统。

Conclusion: 多模态融合和加权时间损失有效提升了ICD代码早期预测的准确性。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [42] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: 提出了一种名为FGAT的新框架，结合图神经网络和注意力机制，同时解决服装搭配兼容性和个性化推荐问题，显著提升了推荐系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 时尚产业的快速扩张和产品多样性增加，使得用户在电商平台上难以找到兼容的服装搭配。现有研究往往独立处理搭配兼容性和个性化推荐，忽略了物品与用户偏好的复杂交互。

Method: 基于HFGN模型，构建用户、搭配和物品的三层层次图，整合视觉和文本特征，利用图注意力机制动态加权节点重要性，生成精确的用户偏好和搭配兼容性表示。

Result: 在POG数据集上，FGAT在精度、HR、召回率、NDCG和准确率上优于基线模型HFGN。

Conclusion: 结合多模态视觉-文本特征、层次图结构和注意力机制，显著提升了个性化时尚推荐系统的准确性和效率。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [43] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: 论文研究了分段仿射正则化（PAR）在监督学习中的理论和应用，证明其在过参数化情况下能实现高度量化，并提供了多种PAR的闭式近端映射和统计保证。


<details>
  <summary>Details</summary>
Motivation: 解决离散或量化变量优化问题的组合复杂性，探索PAR在监督学习中的理论基础。

Method: 分析PAR-正则化损失函数的临界点，推导闭式近端映射，使用近端梯度法及其加速变体和ADMM求解问题。

Result: 在过参数化情况下，PAR-正则化的临界点具有高度量化特性；PAR能近似经典正则化方法并获得类似统计保证。

Conclusion: PAR为量化问题提供了灵活且理论支持的框架，适用于多种优化和统计场景。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [44] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 论文提出了一种名为CTRL的元学习方法，旨在在多个数据源中同时提高整体准确性并保留源级异质性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习任务中多源数据带来的挑战，如分布偏移和样本量差异，特别是在难民安置等应用中需要可靠且差异化的预测。

Method: 结合跨域残差学习和自适应池化/聚类技术，提出Clustered Transfer Residual Learning (CTRL)。

Result: 在5个大规模数据集上，CTRL在多个关键指标上优于现有基准方法。

Conclusion: CTRL能够有效平衡数据数量与质量的权衡，适用于需要保留源级差异的任务。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [45] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 提出了一种基于分布表示学习的高阶贝叶斯网络分类器NeuralKDB，通过神经网络架构学习特征值的分布表示，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器因参数爆炸和数据稀疏问题，难以建模高阶特征依赖，限制了其在复杂数据中的表现。

Method: 通过分布表示学习（类似词嵌入和图表示学习）捕捉特征间的语义关联，设计神经网络架构NeuralKDB，并基于随机梯度下降算法高效训练。

Result: 在60个UCI数据集上的实验表明，NeuralKDB能有效捕捉高阶特征依赖，性能显著优于传统贝叶斯网络分类器及其他竞争方法。

Conclusion: NeuralKDB通过分布表示学习解决了高阶依赖建模问题，为贝叶斯网络分类器提供了新的设计范式。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [46] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究了物联网（IoT）中多模态数据的分布式学习问题，提出了一种名为QQR的算法，以解决模态数量和质量不平衡（QQI）对学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着边缘智能的发展，IoT设备从简单的数据采集单元演变为具有计算能力的节点，但数据采集中的模态不平衡问题影响了多模态在线联邦学习（MMO-FL）的性能。

Method: 论文提出了Modality Quantity and Quality Rebalanced (QQR)算法，通过原型学习方法动态调整模态不平衡问题。

Result: 在两种真实多模态数据集上的实验表明，QQR算法在模态不平衡条件下显著优于基准方法。

Conclusion: QQR算法有效解决了MMO-FL中的模态不平衡问题，提升了学习性能。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [47] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出了一种半监督生成模型，结合标记和未标记数据，通过最大化未标记样本的似然和跨视图互信息，提升多视图学习性能。


<details>
  <summary>Details</summary>
Motivation: 多视图学习常面临视图和标签缺失问题，现有方法无法充分利用未标记数据。

Method: 提出半监督生成模型，结合信息瓶颈原则和跨视图互信息最大化。

Result: 在图像和多组学数据上，模型在预测和填补缺失视图方面表现更优。

Conclusion: 该方法有效解决了多视图学习中的缺失问题，提升了性能。

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [48] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 论文提出了一种结合量子计算的QBM-VAE模型，用于解决传统高斯先验在深度学习中的局限性，并在生物数据上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习依赖高斯先验，难以捕捉复杂非高斯数据（如生物数据），而基于Boltzmann分布的方法在经典计算机上计算困难。量子计算为解决这一问题提供了可能。

Method: 提出QBM-VAE，一种混合量子-经典架构，利用量子处理器高效采样Boltzmann分布，作为深度生成模型的先验。

Result: 在百万级单细胞数据集上，QBM-VAE生成的潜在空间更好地保留了复杂生物结构，性能优于传统高斯模型（如VAE和SCVI）。

Conclusion: QBM-VAE展示了量子计算在深度学习中的实际优势，为开发混合量子AI模型提供了可转移的蓝图。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [49] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 提出了一种基于调制的元学习框架，用于结构保持的动态建模，无需显式系统参数知识或重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统结构保持模型需要固定系统配置和显式参数知识，限制了其在多查询或参数变化场景中的应用。元学习提供了解决方案，但现有方法存在训练不稳定或泛化能力有限的问题。

Method: 引入调制策略的元学习框架，直接通过潜在表示条件化结构保持模型，避免显式优化和系统知识需求。

Result: 在标准基准测试中，该方法在少样本学习场景下实现了准确预测，同时保持了动态稳定性和泛化性能。

Conclusion: 该框架为参数化动态系统的学习和泛化提供了可扩展且高效的解决方案。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [50] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: BFF框架通过多模态对比学习，利用后期数据提升早期儿科风险评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 临床需要尽早进行可靠的风险评估，但早期预测通常精度较低。

Method: BFF将不同时间窗口视为独立模态，利用后期数据隐式监督早期学习。

Result: 在两个真实儿科预测任务中，BFF显著提升了早期风险评估性能。

Conclusion: BFF为早期儿科风险评估提供了一种有效的解决方案。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [51] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 论文探讨了如何通过因果抽象理论理解系统实现计算与表征的关系，并将其应用于深度学习与哲学认知的交叉领域。


<details>
  <summary>Details</summary>
Motivation: 研究旨在澄清系统如何通过因果抽象实现计算与表征，并探讨其在机器学习和认知科学中的意义。

Method: 采用因果抽象理论作为分析框架，结合深度学习与哲学认知的经典主题。

Result: 提出了一种基于因果抽象的计算实现理论，并分析了表征在其中的作用。

Conclusion: 认为这些问题与泛化和预测密切相关，是未来研究的重点。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [52] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出了一种基于CNN-LSTM混合架构的PM2.5浓度预测模型，结合了CNN的空间特征提取和LSTM的时间依赖性建模，实验结果显示其优于传统时间序列模型，但计算资源需求较高。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测空气质量指标（如PM2.5浓度）对环境保护、公共卫生和城市管理至关重要。

Method: 使用CNN提取局部空间特征，LSTM建模时间序列依赖性，基于北京工业区2010-2015年的多变量数据集进行预测。

Result: 模型在6小时平均PM2.5浓度预测中，RMSE为5.236，优于传统时间序列模型。

Conclusion: 模型在现实应用中潜力显著，但需优化计算资源需求和多元大气因素处理能力，未来将提升可扩展性和复杂天气预测任务支持。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [53] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: 本文提出了一种改进的交互式投票地图匹配算法，旨在高效处理不同采样率的轨迹数据，提高GPS轨迹重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决GPS轨迹数据质量不一致的问题，扩展算法适用范围，提升其在现实场景中的实用性。

Method: 在原有算法基础上集成轨迹插补，采用距离限制的交互投票策略降低计算复杂度，并改进以处理道路网络缺失数据。

Result: 算法在保持原有优势的同时，显著提升了适用性和准确性，可广泛应用于OpenStreetMap覆盖的地理区域。

Conclusion: 改进后的算法在多样化的现实场景中表现出色，为GPS轨迹重建提供了高效且通用的解决方案。

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [54] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: 论文提出了一种名为GODNF的框架，通过统一多种意见动态模型，解决了现有扩散图神经网络在适应性、深度和理论理解上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散图神经网络存在适应性差、深度受限和理论理解不足的问题，需要一种更灵活、高效且理论支持的方法。

Method: GODNF框架通过节点特定行为建模和动态邻居影响，实现了异构扩散模式和时序动态，同时保证了深度层的效率和可解释性。

Result: 理论分析表明GODNF能模拟多种收敛配置，实验验证了其在节点分类和影响力估计任务上的优越性。

Conclusion: GODNF为扩散图神经网络提供了一种灵活、高效且理论支持的新方法，显著优于现有技术。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [55] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种通过提示从封闭权重LLM中提取特征并训练轻量级公平分类器的框架，解决了传统方法在零样本或少样本学习场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决封闭权重LLM（如GPT-4、Gemini等）在公平性分类任务中的适用性问题，传统方法无法直接应用。

Method: 将LLM视为特征提取器，通过设计提示获取其概率预测特征，再应用公平算法训练轻量级分类器。

Result: 在五个数据集上验证了框架的有效性，实现了高准确性与公平性的平衡，且数据效率优于传统方法。

Conclusion: 该框架为封闭权重LLM的公平分类提供了一种高效且灵活的解决方案。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [56] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为RTE的训练框架，通过时间集成提升SNN的对抗鲁棒性，减少对抗扰动的时间传递性。


<details>
  <summary>Details</summary>
Motivation: SNN在高效能和类脑计算方面具有潜力，但其对抗扰动的脆弱性尚未被充分理解。

Method: 通过时间集成将SNN视为多个子网络的集合，提出RTE框架，结合统一损失函数和随机采样策略。

Result: RTE在多个基准测试中表现优于现有方法，改善了SNN的内部鲁棒性景观。

Conclusion: 研究强调了时间结构在对抗学习中的重要性，为构建鲁棒的SNN模型提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [57] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 论文提出HS-GPPT模型，通过频谱对齐解决图预训练与提示调优中的知识迁移问题，适用于不同同质性的图数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖同质性低频知识，无法处理现实图中多样化的频谱分布，需解决频谱对齐问题以实现高效知识迁移。

Method: 采用混合频谱滤波器主干和局部-全局对比学习获取频谱知识，设计提示图以对齐频谱分布。

Result: 实验验证了模型在转导和归纳学习中的有效性。

Conclusion: HS-GPPT通过频谱对齐优化知识迁移，适用于不同同质性的图任务。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [58] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS是一种新型的可微分架构搜索框架，专为提升加密货币交易性能而设计，通过显式集成市场状态感知。


<details>
  <summary>Details</summary>
Motivation: 解决静态深度学习模型在高度动态金融环境中的局限性。

Method: 1. 基于贝叶斯理论的搜索空间；2. 动态激活的神经模块（波动、趋势、区间块）；3. 多目标损失函数，包含市场特定惩罚和Lipschitz稳定性约束。市场状态识别采用多头注意力机制。

Result: 在真实加密货币数据上显著优于现有基准，平均绝对误差降低80.3%，收敛速度更快（9 vs. 50+ epochs）。

Conclusion: 将领域知识（如市场状态）直接嵌入NAS过程对开发金融应用的稳健自适应模型至关重要。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [59] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 论文提出了一种Tail-Aware Conformal Prediction (TACP)方法，通过利用长尾标签分布结构，减少头尾类别的覆盖差距，并进一步提出软TACP (sTACP)以提升覆盖平衡。


<details>
  <summary>Details</summary>
Motivation: 现有CP方法在长尾标签分布下，头尾类别覆盖不均衡，尾类别覆盖率不足，影响预测集的可靠性。

Method: 提出TACP方法，利用长尾结构缩小头尾覆盖差距；进一步引入sTACP，通过重加权机制提升覆盖平衡。

Result: 理论分析显示TACP的头尾覆盖差距更小；实验表明方法在多个长尾基准数据集上有效。

Conclusion: TACP和sTACP能显著改善长尾分布下的覆盖不均衡问题，提升预测集的可靠性。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [60] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo提出了一种可扩展且通用的模块化训练方法，适用于Transformer和CNN等架构，显著提升了模块分类精度并减少了模块大小。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型在现代软件系统中的广泛应用，其高昂的训练成本成为挑战。现有模块化方法难以适应多样化的DNN和大规模模型。

Method: NeMo在神经元级别操作，采用对比学习方法和复合损失函数，支持大规模模型的模块化训练。

Result: 实验表明，NeMo在模块分类精度上平均提升1.72%，模块大小减少58.10%，适用于CNN和Transformer模型。

Conclusion: NeMo为可扩展和通用的DNN模块化提供了有效解决方案，具有实际应用潜力。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [61] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 研究通过卫星图像和辅助数据验证全球植树项目的碳汇效果，提出LDIS指标评估数据完整性，发现多数项目存在地理数据问题。


<details>
  <summary>Details</summary>
Motivation: 由于植树项目的碳汇效果数据常由开发者自行报告或缺乏外部验证，导致数据可靠性存疑，研究旨在通过标准化方法增强问责制。

Method: 利用卫星图像和次级数据，构建全球植树项目数据集，引入LDIS指标评估地理数据完整性。

Result: 79%的植树项目地理数据存在至少1项LDIS问题，15%项目缺乏机器可读数据。

Conclusion: 数据集不仅提升碳市场透明度，还可用于计算机视觉任务训练。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [62] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: 提出了一种名为HGD的梯度下降算法，通过均衡不同类别的梯度范数来解决数据流中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据流常存在类别不平衡问题，传统方法如重采样或重加权效果有限，因此需要一种更高效的训练修改方法。

Method: 引入HGD算法，通过均衡梯度范数来避免小类别的欠拟合，无需额外参数或数据缓冲。

Result: 理论分析显示HGD具有次线性遗憾界，实验验证其在多种不平衡数据流场景中的高效性。

Conclusion: HGD是一种简洁高效的算法，适用于任何基于梯度下降的模型，能有效解决数据流中的类别不平衡问题。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [63] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于熵的机制，通过两种策略（ETMR和EAR）优化测试时强化学习的探索-利用平衡，显著提升了模型在无监督场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在复杂推理任务中表现优异，但其依赖标注数据且无监督适应性有限。测试时强化学习（TTRL）虽能自优化，但面临推理成本高和早期估计偏差等问题。

Method: 引入熵机制，提出ETMR和EAR策略，优化探索-利用平衡。

Result: 在AIME 2024基准测试中，Llama3.1-8B模型的Pass at 1指标相对提升68%，且仅消耗60%的预算。

Conclusion: 该方法有效平衡了推理效率、多样性和估计鲁棒性，推动了无监督强化学习在开放域推理任务中的应用。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [64] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTSM是一种新型框架，通过双分支掩码机制和信息论约束，实现跨被试EEG解码，无需特定校准即可零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试EEG解码中因个体差异和共享表征稀缺导致的挑战。

Method: 采用双分支掩码机制分解时空模式，结合信息论约束分离任务相关和个体相关子空间。

Result: 在跨被试运动想象数据集上表现优异，优于现有基线方法。

Conclusion: PTSM通过解耦神经表征，实现个性化和可迁移的解码。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [65] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA是一种强化学习算法，结合个体奖励和成对偏好，通过策略的对数概率直接建模偏好概率，无需单独奖励建模。实验表明DFA在控制环境中表现优于或匹配SAC，且在偏好数据集上优于RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法通常依赖单一奖励信号，而DFA旨在融合个体奖励和成对偏好，提升学习效率和稳定性。

Method: DFA利用策略的对数概率直接建模偏好概率，避免单独奖励建模步骤。偏好可来自人工标注或在线合成的Q值。

Result: DFA在六种控制环境中表现优于或匹配SAC，训练过程更稳定。在半合成偏好数据集上，DFA优于RLHF基线，接近真实奖励的性能。

Conclusion: DFA通过融合奖励和偏好信号，提供了一种高效且稳定的强化学习框架，尤其在偏好数据可用时表现优异。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [66] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 论文提出了一种通过最小化替代损失来改进梯度决策学习（DFL）的方法，解决了现有方法中梯度为零的问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有梯度决策学习方法在解决线性规划（LP）问题时，梯度几乎处处为零，导致优化效果不佳。论文旨在解决这一问题。

Method: 提出最小化替代损失的方法，即使在使用可微分优化层时也采用此策略，并结合高效的DYS-Net技术。

Result: 实验表明，该方法在减少训练时间的同时，实现了与现有方法相当或更好的决策遗憾。

Conclusion: 通过最小化替代损失并结合DYS-Net，论文提供了一种高效且有效的梯度决策学习方法。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [67] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 论文提出了一种基于Forman-Ricci曲率的图结构提升策略，用于解决图神经网络在长距离信息传递中的信息失真问题。


<details>
  <summary>Details</summary>
Motivation: 现实系统（如社交或生物网络）的复杂交互需要更高阶的拓扑表示，而传统图神经网络难以有效处理此类结构。

Method: 利用Forman-Ricci曲率定义边基网络特性，通过曲率揭示图的局部和全局属性，构建超边以建模信息流。

Result: 该方法有效缓解了图学习中长距离信息传递的过压缩问题。

Conclusion: 基于曲率的提升策略为高阶拓扑学习提供了新思路，解决了图神经网络的局限性。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [68] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: 论文提出CHORD框架，通过动态权重统一监督微调（SFT）和强化学习（RL），避免破坏模型模式并减少过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有方法整合SFT和RL时可能破坏模型模式并过拟合专家数据，需一种更稳定的方法。

Method: CHORD框架将SFT作为动态加权辅助目标融入RL过程，采用全局系数和令牌级权重函数。

Result: 实验表明CHORD能稳定高效学习，显著优于基线方法。

Conclusion: CHORD有效协调专家数据和探索，为未来研究提供启发。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [69] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD提出了一种基于共享潜在空间的抗体序列和结构协同设计框架，优化了传统方法在原始数据空间中的低效搜索问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化抗体互补决定区（CDRs）时效率低下，且无法同步设计序列和结构。LEAD旨在通过共享潜在空间优化解决这些问题。

Method: LEAD采用序列-结构协同设计框架，在共享潜在空间中优化，并设计了黑盒引导策略以应对非可微分评估器的场景。

Result: 实验表明，LEAD在单目标和多目标优化中表现优异，查询消耗减少一半，且性能超越基线方法。

Conclusion: LEAD通过潜在空间优化和黑盒策略，显著提升了抗体设计的效率和性能。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [70] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 论文提出通过收缩理论提升卷积神经常微分方程（NODEs）的鲁棒性，减少输入噪声和对抗攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络的脆弱性使其容易受到输入噪声和对抗攻击的影响，需要提升其鲁棒性。

Method: 利用收缩理论，通过正则化项或权重正则化项诱导NODEs的收缩性，从而增强模型的鲁棒性。

Result: 在MNIST和FashionMNIST数据集上验证了方法的有效性，能够显著提升模型对噪声和攻击的抵抗能力。

Conclusion: 通过收缩理论提升NODEs的鲁棒性是一种有效方法，且计算负担可控。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [71] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: mCOCO是一个基于Reservoir Computing的新型框架，用于生成具有认知能力的连接性脑模板（CBT），解决了现有方法在可解释性、计算成本和认知能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如传统机器学习和图神经网络）在生成CBT时存在可解释性差、计算成本高且忽略认知能力的问题。

Method: mCOCO利用Reservoir Computing处理BOLD信号，分两阶段生成CBT：先提取个体功能连接组并聚合为群体模板，再通过多感官输入赋予认知特性。

Result: mCOCO在中心性、区分性、拓扑合理性和多感官记忆保留方面显著优于基于GNN的CBT。

Conclusion: mCOCO为功能连接研究提供了一种高效、可解释且具有认知能力的解决方案。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [72] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 论文提出了一种基于学习理论的框架，用于定义解释算法是否提供决策函数信息，并证明许多流行算法对复杂模型无效。


<details>
  <summary>Details</summary>
Motivation: 探讨局部事后解释算法是否能真正揭示复杂机器学习模型的行为，并填补理论保证的空白。

Method: 引入一个理论框架，定义“信息性解释”为减少可能决策函数空间的复杂性，并分析多种流行算法的有效性。

Result: 证明许多流行算法（如梯度解释、SHAP等）对复杂模型无效，并提出了改进条件。

Conclusion: 研究结果对AI审计、监管和高风险应用具有重要实践意义。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [73] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: 论文通过贝叶斯推断框架和合成数据集测试，评估了六种概率机器学习算法在类别概率和不确定性估计中的表现，发现深度学习算法在分布外数据上的不确定性估计不足。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习等复杂数据模型的出现，不确定性量化变得困难，需要评估现有算法的表现。

Method: 使用近似贝叶斯推断框架和合成数据集，测试六种算法的校准性和分布外数据的不确定性估计能力。

Result: 所有算法校准性良好，但深度学习算法在分布外数据上的不确定性估计不足。

Conclusion: 研究为开发新的不确定性估计方法提供了参考，强调了深度学习算法在分布外数据上的局限性。

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [74] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 该研究通过AutoML和可解释AI方法，分析了俄亥俄州2017-2022年230万车辆级事故数据，识别出17个关键风险因素，并构建了预测严重事故的模型。


<details>
  <summary>Details</summary>
Motivation: 机动车事故是全球伤害和死亡的主要原因，需数据驱动的方法来理解和减轻事故严重性。

Method: 使用JADBio AutoML平台构建预测模型，结合SHAP解释特征贡献，最终采用Ridge Logistic Regression模型。

Result: 模型训练集AUC-ROC为85.6%，测试集为84.9%，识别出17个关键特征，如位置类型、限速等。

Conclusion: 研究提供了一个可扩展的框架，支持Vision Zero目标，强调方法严谨性和可解释性。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [75] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle是一种新型自解释图神经网络框架，旨在生成和评估类级别解释，优于现有方法如ProtGNN和PGIB。


<details>
  <summary>Details</summary>
Motivation: 增强图神经网络（GNN）的可解释性，确保其安全公平部署，现有方法仅关注实例级解释，缺乏类级别验证。

Method: GraphOracle联合学习GNN分类器和稀疏子图，通过掩码评估策略验证图-子图-预测依赖关系。

Result: GraphOracle在忠实性、可解释性和可扩展性上优于ProtGNN和PGIB，避免了计算瓶颈。

Conclusion: GraphOracle为GNN提供了一种实用且高效的类级别自解释解决方案。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [76] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出了一种双空间引导的测试框架，通过协调场景参数空间和智能体行为空间，生成兼顾多样性和关键性的测试场景。


<details>
  <summary>Details</summary>
Motivation: 动态环境中决策智能体的广泛部署增加了对安全验证的需求，现有方法在高维场景空间中难以平衡多样性和关键性。

Method: 采用分层表示框架结合降维和多维子空间评估，动态协调局部扰动和全局探索两种生成模式，同时利用智能体行为数据量化行为关键性和多样性。

Result: 实验表明，该框架在五种决策智能体上平均提高了56.23%的关键场景生成效率，并在新指标下表现出更高的多样性。

Conclusion: 双空间引导框架有效解决了高维场景空间中的多样性与关键性平衡问题，优于现有方法。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [77] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 论文提出了一种使用费曼图计算有限宽度修正的方法，以分析神经切线核（NTK）的统计特性，解决了无限宽度下无法捕捉训练动态的问题。


<details>
  <summary>Details</summary>
Motivation: 在无限宽度下，NTK的计算虽然简单，但无法反映训练中的关键特性（如NTK演化或特征学习）。因此，需要引入有限宽度修正。

Method: 通过费曼图计算有限宽度修正，简化代数操作，并推导出涉及预激活、NTK及高阶导数张量的层间递归关系。

Result: 验证了深度网络的稳定性结果可扩展到NTK，并证明了ReLU等尺度不变非线性在NTK Gram矩阵对角线上无有限宽度修正。

Conclusion: 提出的框架为有限宽度下的NTK分析提供了有效工具，并通过数值实验验证了其可行性。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [78] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息扩散模型的无监督异常检测方法，用于多元时间序列数据，通过加权物理信息损失提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列领域已表现出色，但如何结合物理信息进一步提升异常检测性能仍需探索。

Method: 使用加权物理信息损失训练扩散模型，学习物理依赖的时间分布。

Result: 实验表明，该方法在异常检测的F1分数、数据多样性和对数似然上优于基线和其他模型。

Conclusion: 物理信息训练显著提升了扩散模型的性能，尤其在合成和部分真实数据集上表现突出。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [79] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 提出了一种名为HXAI的全面可解释人工智能框架，旨在通过嵌入解释功能到数据分析的每个阶段，提升用户对AI模型的信任。


<details>
  <summary>Details</summary>
Motivation: 解决传统可解释AI方法仅关注单个预测而忽略上下游决策和质量检查的问题，提升AI模型的透明度和可信度。

Method: HXAI框架整合了六个组件（数据、分析设置、学习过程、模型输出、模型质量、沟通渠道），并通过112项问题库和用户调查，结合人类解释理论、人机交互原则和实证研究，设计清晰、可操作的解释。

Result: 提出了一个全面的分类法，减少了术语歧义，并展示了如何利用大型语言模型协调多种解释技术，生成针对不同利益相关者的叙述。

Conclusion: HXAI通过跨学科整合和实际项目经验，为AI的透明性、可信度和负责任部署提供了端到端的解决方案。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [80] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST是一种去中心化的联邦图学习框架，通过自适应通信机制优化客户端间的拓扑结构，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习（DFL）方法未考虑局部子图的拓扑信息，而联邦图学习（FGL）多为集中式，无法利用去中心化优势。

Method: 提出DFed-SST框架，采用双拓扑自适应通信机制，动态优化客户端间通信拓扑。

Result: 在八个真实数据集上，平均准确率比基线方法提高3.26%。

Conclusion: DFed-SST有效解决了异构性问题，为去中心化联邦图学习提供了高效解决方案。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [81] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的嵌套算子推断方法，用于从高维动态系统的快照数据中学习物理信息降阶模型。


<details>
  <summary>Details</summary>
Motivation: 利用降阶空间中的层次结构，优先考虑主导模式的相互作用，以构建更优的初始猜测，从而提升降阶模型的精度和效率。

Method: 采用嵌套算子推断算法，通过迭代构建初始猜测，并支持从先前学习的模型中进行热启动。

Result: 在立方热传导问题中，嵌套算子推断比标准方法误差减少四倍；在格陵兰冰盖模型中，平均误差为3%，计算加速超过19,000倍。

Conclusion: 嵌套算子推断方法在提升降阶模型精度和计算效率方面表现出显著优势，适用于动态基础和模型形式更新的场景。

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [82] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow是一个基于服务器的强化学习框架，解决了工业规模RL中的两个核心挑战：解耦训练与复杂执行流程，以及最大化GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 工业规模RL面临训练与执行流程耦合、GPU利用率低的问题，SeamlessFlow旨在解决这些问题。

Method: 引入数据平面解耦训练与代理执行，采用标签驱动调度和时空复用管道优化资源利用。

Result: SeamlessFlow实现了稳定性和高性能，适用于多代理和复杂RL任务。

Conclusion: SeamlessFlow通过创新设计，为大规模RL部署提供了高效且稳定的解决方案。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [83] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 论文提出了一种基于马尔可夫博弈的范式，研究不同联盟结构对碳捕集与封存（CCS）项目中利益相关者目标的影响，并通过多智能体强化学习优化策略。


<details>
  <summary>Details</summary>
Motivation: CCS项目涉及多方利益相关者，各自目标不同且地质条件复杂，需研究如何通过协作或独立行动实现最优管理。

Method: 采用马尔可夫博弈和多智能体强化学习框架，结合安全约束和替代模型（E2C框架）降低计算成本。

Result: 结果表明，所提框架能有效优化多利益相关者参与的CO2封存管理。

Conclusion: 协作联盟结构在复杂CCS项目中更有效，强化学习框架为多利益相关者管理提供了可行方案。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [84] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: 论文提出了一种名为RMSL的框架，利用弱标签（序列级而非行为级）提升行为级异常检测能力，通过多超球体表示正常行为模式，结合多实例学习和自适应自训练去偏方法，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏细粒度行为级标注，现有方法在检测用户行为序列中的异常时存在高误报率和漏报率。

Method: 提出RMSL框架，利用序列级弱标签，通过多超球体表示正常行为模式，结合多实例学习和自适应自训练去偏方法优化模型。

Result: 实验表明RMSL显著提升了行为级内部威胁检测的性能。

Conclusion: RMSL通过弱标签和多超球体学习，有效解决了行为级异常检测的挑战。

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [85] [A weighted U statistic for association analysis considering genetic heterogeneity](https://arxiv.org/abs/1504.08319)
*Changshuai Wei,Robert C. Elston,Qing Lu*

Main category: stat.ME

TL;DR: 本文提出了一种考虑遗传异质性的关联分析方法HWU，适用于多种表型，并在模拟和实际数据分析中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 复杂疾病可能具有不同的遗传病因，但现有方法通常假设遗传效应同质，导致在异质情况下功效不足。

Method: 提出HWU方法，适用于二元和连续表型，计算高效，能处理高维遗传数据。

Result: 模拟显示HWU在遗传异质情况下表现优越，实际数据分析发现两个新基因（CYP3A5和IKBKB）对尼古丁依赖的异质效应。

Conclusion: HWU是一种高效且稳健的方法，适用于遗传异质性研究。

Abstract: Converging evidence suggests that common complex diseases with the same or
similar clinical manifestations could have different underlying genetic
etiologies. While current research interests have shifted toward uncovering
rare variants and structural variations predisposing to human diseases, the
impact of heterogeneity in genetic studies of complex diseases has been largely
overlooked. Most of the existing statistical methods assume the disease under
investigation has a homogeneous genetic effect and could, therefore, have low
power if the disease undergoes heterogeneous pathophysiological and etiological
processes. In this paper, we propose a heterogeneity weighted U (HWU) method
for association analyses considering genetic heterogeneity. HWU can be applied
to various types of phenotypes (e.g., binary and continuous) and is
computationally effcient for high- dimensional genetic data. Through
simulations, we showed the advantage of HWU when the underlying genetic
etiology of a disease was heterogeneous, as well as the robustness of HWU
against different model assumptions (e.g., phenotype distributions). Using HWU,
we conducted a genome-wide analysis of nicotine dependence from the Study of
Addiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis
of nearly one million genetic markers took 7 hours, identifying heterogeneous
effects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.

</details>


### [86] [A Generalized Similarity U Test for Multivariate Analysis of Sequencing Data](https://arxiv.org/abs/1505.01179)
*Changshuai Wei,Qing Lu*

Main category: stat.ME

TL;DR: 论文提出了一种广义相似性U检验（GSU），用于处理高维基因型和表型数据，解决了传统统计方法在复杂疾病遗传关联研究中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法（如基于回归的单点分析）难以应对高维数据和低频遗传变异的挑战，且无法处理多表型的不同分布。

Method: 提出GSU方法，基于相似性检验，适用于高维基因型和表型数据，并提供了理论性质、p值计算及样本量/功效计算方法。

Result: 模拟实验显示GSU在功效和对表型分布的鲁棒性上优于现有方法；实际数据分析中识别出4个基因与5个代谢相关表型的联合关联。

Conclusion: GSU是一种高效、稳健的方法，适用于复杂疾病遗传关联研究，尤其在多表型分析中表现优异。

Abstract: Sequencing-based studies are emerging as a major tool for genetic association
studies of complex diseases. These studies pose great challenges to the
traditional statistical methods (e.g., single-locus analyses based on
regression methods) because of the high-dimensionality of data and the low
frequency of genetic variants. In addition, there is a great interest in
biology and epidemiology to identify genetic risk factors contributed to
multiple disease phenotypes. The multiple phenotypes can often follow different
distributions, which violates the assumptions of most current methods. In this
paper, we propose a generalized similarity U test, referred to as GSU. GSU is a
similarity-based test and can handle high-dimensional genotypes and phenotypes.
We studied the theoretical properties of GSU, and provided the efficient
p-value calculation for association test as well as the sample size and power
calculation for the study design. Through simulation, we found that GSU had
advantages over existing methods in terms of power and robustness to phenotype
distributions. Finally, we used GSU to perform a multivariate analysis of
sequencing data in the Dallas Heart Study and identified a joint association of
4 genes with 5 metabolic related phenotypes.

</details>


### [87] [A Weighted U Statistic for Genetic Association Analyses of Sequencing Data](https://arxiv.org/abs/1505.01204)
*Changshuai Wei,Ming Li,Zihuai He,Olga Vsevolozhskaya,Daniel J. Schaid,Qing Lu*

Main category: stat.ME

TL;DR: 论文提出了一种名为WU-seq的加权U统计量方法，用于高维测序数据的关联分析，解决了传统方法在低频变异和高维数据中功效不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着下一代测序技术的发展，海量测序数据为研究复杂疾病中罕见变异的遗传病因提供了机会，但传统统计方法因低频变异和高维数据导致功效不足。

Method: 开发了基于非参数U统计量的WU-seq方法，无需假设疾病模型或表型分布，适用于多种表型。

Result: 模拟和实证研究表明，WU-seq在假设不满足时优于SKAT方法，在假设满足时性能相当。在DHS数据中检测到ANGPTL4与极低密度脂蛋白胆固醇的关联。

Conclusion: WU-seq是一种灵活且高效的高维测序数据分析方法，尤其适用于假设不满足的情况。

Abstract: With advancements in next generation sequencing technology, a massive amount
of sequencing data are generated, offering a great opportunity to
comprehensively investigate the role of rare variants in the genetic etiology
of complex diseases. Nevertheless, this poses a great challenge for the
statistical analysis of high-dimensional sequencing data. The association
analyses based on traditional statistical methods suffer substantial power loss
because of the low frequency of genetic variants and the extremely high
dimensionality of the data. We developed a weighted U statistic, referred to as
WU-seq, for the high-dimensional association analysis of sequencing data. Based
on a non-parametric U statistic, WU-SEQ makes no assumption of the underlying
disease model and phenotype distribution, and can be applied to a variety of
phenotypes. Through simulation studies and an empirical study, we showed that
WU-SEQ outperformed a commonly used SKAT method when the underlying assumptions
were violated (e.g., the phenotype followed a heavy-tailed distribution). Even
when the assumptions were satisfied, WU-SEQ still attained comparable
performance to SKAT. Finally, we applied WU-seq to sequencing data from the
Dallas Heart Study (DHS), and detected an association between ANGPTL 4 and very
low density lipoprotein cholesterol.

</details>


### [88] [Generalized Similarity U: A Non-parametric Test of Association Based on Similarity](https://arxiv.org/abs/1801.01220)
*Changshuai Wei,Qing Lu*

Main category: stat.ME

TL;DR: 提出了一种基于相似性的测试方法GSU，用于复杂对象间的关联分析，并在基因组关联研究中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂对象（如基因型和表型）间的关联分析问题，提升现有方法的功效和鲁棒性。

Method: 提出广义相似性U（GSU）测试，采用拉普拉斯核相似性，并通过理论分析和模拟验证其性能。

Result: GSU在功效和鲁棒性上优于现有方法，并在ADNI数据中识别出三个与影像表型相关的基因。

Conclusion: GSU是一种有效的复杂对象关联分析方法，适用于基因组研究，并提供了开源工具支持。

Abstract: Second generation sequencing technologies are being increasingly used for
genetic association studies, where the main research interest is to identify
sets of genetic variants that contribute to various phenotype. The phenotype
can be univariate disease status, multivariate responses and even
high-dimensional outcomes. Considering the genotype and phenotype as two
complex objects, this also poses a general statistical problem of testing
association between complex objects. We here proposed a similarity-based test,
generalized similarity U (GSU), that can test the association between complex
objects. We first studied the theoretical properties of the test in a general
setting and then focused on the application of the test to sequencing
association studies. Based on theoretical analysis, we proposed to use
Laplacian kernel based similarity for GSU to boost power and enhance
robustness. Through simulation, we found that GSU did have advantages over
existing methods in terms of power and robustness. We further performed a whole
genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative
(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with
imaging phenotype. We developed a C++ package for analysis of whole genome
sequencing data using GSU. The source codes can be downloaded at
https://github.com/changshuaiwei/gsu.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [89] [Dyadically resolving trinomials for fast modular arithmetic](https://arxiv.org/abs/2508.11043)
*Robert Dougherty-Bliss,Mits Kobayashi,Natalya Ter-Saakov,Eugene Zima*

Main category: math.NT

TL;DR: 论文研究了基于形式为$2^n - 2^k + 1$的互质模数的剩余数系统，分析了其算术和位级特性，并提出了构造大规模互质模数集的方法。


<details>
  <summary>Details</summary>
Motivation: 探索一种高效的剩余数系统，利用互质模数加速整数运算，特别是针对形式为$2^n - 2^k + 1$的模数。

Method: 通过分析多项式$x^n - x^k + 1$的互质性，建立基于多项式结果的充分条件，并利用图论模型（最大团算法）构造大规模互质模数集。

Result: 提出了构造大规模互质模数集的方法，并通过图着色、结果理论和分圆多项式性质证明了模数集大小的上界。

Conclusion: 该研究为高效剩余数系统的设计提供了理论支持，并展示了实际应用中构造大规模互质模数集的可行性。

Abstract: Residue number systems based on pairwise relatively prime moduli are a
powerful tool for accelerating integer computations via the Chinese Remainder
Theorem. We study a structured family of moduli of the form $2^n - 2^k + 1$,
originally proposed for their efficient arithmetic and bit-level properties.
These trinomial moduli support fast modular operations and exhibit scalable
modular inverses.
  We investigate the problem of constructing large sets of pairwise relatively
prime trinomial moduli of fixed bit length. By analyzing the corresponding
trinomials $x^n - x^k + 1$, we establish a sufficient condition for coprimality
based on polynomial resultants. This leads to a graph-theoretic model where
maximal sets correspond to cliques in a compatibility graph, and we use maximum
clique-finding algorithms to construct large examples in practice. Using the
theory of graph colorings, resultants, and properties of cyclotomic
polynomials, we also prove upper bounds on the size of such sets as a function
of $n$.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [90] [Insect-Wing Structured Microfluidic System for Reservoir Computing](https://arxiv.org/abs/2508.10915)
*Jacob Clouse,Thomas Ramsey,Samitha Somathilaka,Nicholas Kleinsasser,Sangjin Ryu,Sasitharan Balasubramaniam*

Main category: cs.NE

TL;DR: 该研究提出了一种基于蜻蜓翅膀启发的微流体芯片的混合储层计算系统，用于在电子设备不适用的环境中实现低功耗、高弹性的计算。


<details>
  <summary>Details</summary>
Motivation: 随着对高效和自适应计算需求的增长，受自然启发的架构为传统电子设计提供了有前景的替代方案。微流体平台利用生物形态和流体动力学，为低功耗、高弹性计算提供了基础。

Method: 系统采用三个染料入口通道和三个摄像头监控的检测区域，将离散空间模式转换为动态颜色输出信号，并通过可训练的读出层进行分类。

Result: 实验结果显示，即使在粗分辨率和有限训练数据下，系统分类准确率仍高达91%。

Conclusion: 研究表明微流体储层计算具有可行性，为未来计算架构提供了新的可能性。

Abstract: As the demand for more efficient and adaptive computing grows,
nature-inspired architectures offer promising alternatives to conventional
electronic designs. Microfluidic platforms, drawing on biological forms and
fluid dynamics, present a compelling foundation for low-power, high-resilience
computing in environments where electronics are unsuitable. This study explores
a hybrid reservoir computing system based on a dragonfly-wing inspired
microfluidic chip, which encodes temporal input patterns as fluid interactions
within the micro channel network.
  The system operates with three dye-based inlet channels and three
camera-monitored detection areas, transforming discrete spatial patterns into
dynamic color output signals. These reservoir output signals are then modified
and passed to a simple and trainable readout layer for pattern classification.
Using a combination of raw reservoir outputs and synthetically generated
outputs, we evaluated system performance, system clarity, and data efficiency.
The results demonstrate consistent classification accuracies up to $91\%$, even
with coarse resolution and limited training data, highlighting the viability of
the microfluidic reservoir computing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [91] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: 提出HQ-OV3D框架，通过跨模态几何一致性和标注类辅助去噪机制，生成高质量伪标签，提升开放词汇3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测无法满足开放世界应用需求，现有开放词汇方法虽语义准确但几何质量不足。

Method: HQ-OV3D包含IMCV提案生成器和ACA去噪器，利用跨模态几何一致性和标注类几何先验优化伪标签。

Result: 相比现有方法，mAP提升7.37%，伪标签质量显著提高。

Conclusion: HQ-OV3D可作为独立检测器或插件，提升开放词汇检测或标注流程的质量。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [92] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoad是一个端到端系统，用于自动检测坑洼、GPS标记和实时地图绘制，适用于印度多样化的道路环境。


<details>
  <summary>Details</summary>
Motivation: 印度道路上的坑洼对安全和车辆寿命构成威胁，需要一种高效、低成本的解决方案。

Method: 利用自标注数据集和YOLO模型进行实时检测，结合OCR提取时间戳并与GPS同步，数据通过OSM可视化。

Result: 系统提高了检测准确性，并生成政府兼容的输出用于道路维护规划。

Conclusion: iWatchRoad是一种成本效益高、可扩展的解决方案，适用于发展中国家的道路管理。

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [93] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文研究了单目3D目标检测器在相机高度变化时的性能问题，提出了一种新方法CHARM3R，显著提升了模型对未知相机高度的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D检测器在相机高度变化时性能下降，尤其是深度估计问题突出，需要一种更鲁棒的方法。

Method: 通过系统分析相机高度变化对模型的影响，提出CHARM3R，结合回归和基于地面的深度估计。

Result: CHARM3R在CARLA数据集上对未知相机高度的泛化能力提升了45%，达到SOTA性能。

Conclusion: CHARM3R有效解决了相机高度变化对单目3D检测的影响，为实际应用提供了更鲁棒的解决方案。

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [94] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的不确定性模态建模（UMM）框架，用于解决行人重识别（ReID）中多模态输入不确定或缺失的问题，结合了模态映射、合成模态增强和跨模态交互学习，提升了鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等场景中，行人重识别需要处理多模态输入（如RGB、红外等）的不确定或缺失问题，现有方法计算开销大，难以实际部署。

Method: 提出UMM框架，包括多模态令牌映射器、合成模态增强策略和跨模态交互学习器，并利用CLIP的视觉-语言对齐能力高效融合多模态输入。

Result: 实验表明，UMM在不确定模态条件下表现出强鲁棒性、泛化能力和计算效率。

Conclusion: UMM为自动驾驶中的行人重识别提供了一种可扩展且实用的解决方案。

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [95] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 稀疏自编码器（SAEs）在视觉模型中的应用潜力显著，能够提升可解释性、泛化能力和可控生成能力。


<details>
  <summary>Details</summary>
Motivation: 尽管SAEs在语言模型中广泛应用，但在视觉领域的研究较少，本文旨在填补这一空白。

Method: 通过多种图像任务评估SAEs在视觉模型中的表现，涵盖视觉嵌入模型、多模态LMMs和扩散模型。

Result: SAE特征具有语义意义，能提升分布外泛化能力，并支持可控生成，同时揭示了跨模态共享表示。

Conclusion: SAEs在视觉模型中展现出强大的潜力，为未来研究提供了基础。

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [96] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义引导的对抗目标选择框架，利用预训练语言和视觉语言模型的跨模态知识转移，提升攻击的可解释性、可重复性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击的目标标签选择方法依赖随机性、模型预测或静态语义资源，限制了可解释性、可重复性和灵活性。

Method: 提出语义引导框架，利用BERT、TinyLLAMA和CLIP等预训练模型作为相似性来源，选择与真实标签最相关和最不相关的标签，构建最佳和最差对抗场景。

Result: 实验表明，预训练模型能有效生成对抗目标，优于静态语义数据库（如WordNet），尤其在远距离类别关系上表现更优。

Conclusion: 预训练模型适合构建可解释、标准化和可扩展的对抗基准，适用于不同架构和数据集。

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [97] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: RETFound模型首次被用于视盘分割任务，表现优异，仅需少量任务样本即可超越现有分割专用模型。


<details>
  <summary>Details</summary>
Motivation: 探索RETFound模型在视盘分割任务中的潜力，验证基础模型在特定任务中的适用性。

Method: 通过训练一个轻量级头部网络，将RETFound模型适配到视盘分割任务，并在多个公开和私有数据集上测试。

Result: 在五个数据集上平均Dice分数达到96%，表现优于现有分割专用模型，具备良好的泛化和适应能力。

Conclusion: RETFound模型在视盘分割任务中表现出色，支持基础模型作为任务特定架构的替代方案。

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [98] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: 提出了一种结合实例级嵌入蒸馏和关系型成对相似性蒸馏的统一知识蒸馏框架，显著提升了人脸识别模型在边缘设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在捕捉细粒度实例级细节和复杂关系结构方面表现不佳，导致性能不理想。

Method: 提出两种新型损失函数：实例级嵌入蒸馏（动态硬挖掘策略）和关系型成对相似性蒸馏（记忆库机制和样本挖掘策略）。

Result: 在多个基准人脸识别数据集上优于现有蒸馏方法，学生模型甚至能超越教师模型的准确率。

Conclusion: 统一框架实现了更全面的蒸馏过程，提升了模型性能。

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [99] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: SelfAdapt是一种无需标签的自适应方法，用于改进预训练细胞分割模型，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 通用模型（如Cellpose）在训练数据以外的领域性能下降，而监督微调需要标注数据。SelfAdapt旨在解决这一问题。

Method: 基于师生增强一致性训练，引入L2-SP正则化和无标签停止准则。

Result: 在LiveCell和TissueNet数据集上，AP0.5相对提升高达29.64%。

Conclusion: SelfAdapt是一种高效的无监督自适应方法，可进一步提升模型性能，并已集成到Cellpose框架中。

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [100] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: 该研究应用TrOCR模型处理16世纪拉丁手稿，通过针对性图像预处理和数据增强技术，显著提升了手写文本识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 历史手写文本识别（HTR）对档案文件的文化和学术价值至关重要，但稀缺的转录、语言变异和多样化的书写风格阻碍了数字化进程。

Method: 研究采用TrOCR模型，结合图像预处理和四种新颖的数据增强方法，并评估集成学习策略。

Result: 最佳单模型增强（Elastic）的字符错误率（CER）为1.86，而前5投票集成的CER为1.60，比之前的最佳结果提升了42%。

Conclusion: 领域特定的数据增强和集成策略显著提升了历史手稿的HTR性能。

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [101] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: 提出了一种基于改进ConvNeXt-Tiny架构的医学图像分类方法，通过结构优化和损失函数设计提升性能并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的计算环境中实现高效高精度的医学图像分类仍具挑战性。

Method: 引入双全局池化特征融合策略和轻量级通道注意力模块SEVector，结合特征平滑损失函数。

Result: 在CPU条件下，10个训练周期内测试集分类准确率达89.10%，损失值稳定收敛。

Conclusion: 该方法为资源受限环境下的医学图像分类提供了高效可行的解决方案。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [102] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 本文提出了一种通过奖励引导解码来适应多模态大语言模型（MLLMs）的方法，以提升其视觉定位能力。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs的广泛应用，需要适应多样化的用户需求，本文旨在通过控制解码实现这一目标。

Method: 构建两个独立的奖励模型，分别控制对象精确度和召回率，并通过动态调整奖励函数权重和解码搜索范围实现可控性。

Result: 在标准对象幻觉基准测试中，该方法显著优于现有幻觉缓解方法，并提供了对MLLM推理过程的灵活控制。

Conclusion: 该方法为MLLMs的推理过程提供了动态可控性，同时提升了视觉定位性能。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [103] [CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems](https://arxiv.org/abs/2508.11287)
*Xuran Liu,Nan Xue,Rui Bao,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 提出了一种延迟感知调度框架，通过重叠模型加载与计算和通信，减少边缘设备上大型语言模型推理的总延迟。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有方法忽略了按需模型加载导致的冷启动延迟，影响了低延迟和隐私保护的AI服务。

Method: 设计了一个动态调整层分区和分配的框架，将问题建模为混合整数非线性规划，并采用动态规划算法优化分区和设备分配。

Result: 实验表明，该方法显著降低了冷启动延迟，优于基线策略。

Conclusion: 提出的框架有效减少了推理延迟，为边缘设备上的大型语言模型部署提供了实用解决方案。

Abstract: While deploying large language models on edge devices promises low-latency
and privacy-preserving AI services, it is hindered by limited device resources.
Although pipeline parallelism facilitates distributed inference, existing
approaches often ignore the cold-start latency caused by on-demand model
loading. In this paper, we propose a latency-aware scheduling framework that
overlaps model loading with computation and communication to minimize total
inference latency. Based on device and model parameters, the framework
dynamically adjusts layer partitioning and allocation to effectively hide
loading time, thereby eliminating as many idle periods as possible. We
formulate the problem as a Mixed-Integer Non-Linear Program and design an
efficient dynamic programming algorithm to optimize model partitioning and
device assignment. Experimental results show that the proposed method
significantly reduces cold-start latency compared to baseline strategies.

</details>


### [104] [Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks](https://arxiv.org/abs/2508.11291)
*Rui Bao,Nan Xue,Yaping Sun,Zhiyong Chen*

Main category: cs.IT

TL;DR: 论文提出了一种动态路由框架，用于在无线边缘设备协作环境中平衡LLM的推理质量和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信与LLM结合时任务复杂性与资源分配不匹配的问题，避免简单查询的高延迟和本地模型能力不足。

Method: 设计了一个动态路由框架，结合轻量级移动设备模型和边缘服务器强大模型，使用BERT预测语义分数和上下文感知成本模型。

Result: 实验表明，框架在保持推理质量的同时，平均响应延迟降低5-15%，大模型调用减少10-20%。

Conclusion: 该框架有效解决了无线边缘环境中LLM部署的质量与延迟权衡问题。

Abstract: The integration of wireless communications and Large Language Models (LLMs)
is poised to unlock ubiquitous intelligent services, yet deploying them in
wireless edge-device collaborative environments presents a critical trade-off
between inference quality and end-to-end latency. A fundamental mismatch exists
between task complexity and resource allocation: offloading simple queries
invites prohibitive latency, while on-device models lack the capacity for
demanding computations. To address this challenge, we propose a dynamic,
quality-latency aware routing framework that orchestrates inference between a
lightweight model on the mobile device and a powerful model on the edge server.
Our framework employs two distinct cost models: for single-turn queries, it
fuses a BERT-predicted semantic score with communication and computation
overheads; for multi-turn dialogues, it further quantifies context-aware costs
arising from model switching and KV-cache management. While maintaining full
inference quality, extensive experiments demonstrate that our framework cuts
average response latency by 5-15% and reduces large model invocations by 10-20%
against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [105] [A Tight Lower Bound for Doubling Spanners](https://arxiv.org/abs/2508.11555)
*An La,Hung Le,Shay Solomon,Cuong Than,Vinayak,Shuang Yang,Tianyi Zhang*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Any $n$-point set in the $d$-dimensional Euclidean space $\mathbb{R}^d$, for
$d = O(1)$, admits a $(1+\epsilon)$-spanner with $\tilde{O}(n \cdot
\epsilon^{-d+1})$ edges and lightness $\tilde{O}(\epsilon^{-d})$, for any
$\epsilon > 0$. (The {lightness} is a normalized notion of weight, where we
divide the spanner weight by the MST weight. The $\tilde{O}$ and
$\tilde{\Omega}$ notations hide $\texttt{polylog}(\epsilon^{-1})$ terms.)
Moreover, this result is tight: For any $2 \le d = O(1)$, there exists an
$n$-point set in $\mathbb{R}^d$, for which any $(1+\epsilon)$-spanner has
$\tilde{\Omega}(n \cdot \epsilon^{-d+1})$ edges and lightness $\tilde{\Omega}(n
\cdot \epsilon^{-d})$.
  The upper bounds for Euclidean spanners rely heavily on the spatial property
of {cone partitioning} in $\mathbb{R}^d$, which does not seem to extend to the
wider family of {doubling metrics}, i.e., metric spaces of constant {doubling
dimension}. In doubling metrics, a {simple spanner construction from two
decades ago, the {net-tree spanner}}, has $\tilde{O}(n \cdot \epsilon^{-d})$
edges, and it could be transformed into a spanner of lightness $\tilde{O}(n
\cdot \epsilon^{-(d+1)})$ by pruning redundant edges. Despite a large body of
work, it has remained an open question whether the superior Euclidean bounds of
$\tilde{O}(n \cdot \epsilon^{-d+1})$ edges and lightness
$\tilde{O}(\epsilon^{-d})$ could be achieved also in doubling metrics. We
resolve this question in the negative by presenting a surprisingly simple and
tight lower bound, which shows, in particular, that the net-tree spanner and
its pruned version are both optimal.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [106] [The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators](https://arxiv.org/abs/2508.11175)
*Ali Karimi,Hadi Zadeh-Haghighi,Youssef Kora,Christoph Simon*

Main category: quant-ph

TL;DR: 量子储层计算（QRC）利用量子动力学高效处理时序数据。本文研究基于两个耦合Kerr非线性振荡器的QRC框架，分析其性能与关键物理参数的关系，并探讨纠缠对计算性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索量子储层计算在时间序列预测任务中的潜力，特别是纠缠对性能的影响，以推动高效量子机器学习和时间序列预测的发展。

Method: 采用两个耦合Kerr非线性振荡器作为QRC框架，通过调整输入驱动强度、Kerr非线性和振荡器耦合等参数，使用对数负性和NRMSE评估性能。

Result: 纠缠在平均性能上提供计算优势，但存在输入频率阈值；较高耗散率可能提升性能；纠缠对最佳误差无改善。

Conclusion: 纠缠在量子储层计算中具有潜在优势，但需进一步研究其在复杂条件下的表现，以优化量子机器学习应用。

Abstract: Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently
process temporal data. In this work, we investigate a QRC framework based on
two coupled Kerr nonlinear oscillators, a system well-suited for time-series
prediction tasks due to its complex nonlinear interactions and potentially
high-dimensional state space. We explore how its performance in time-series
prediction depends on key physical parameters: input drive strength, Kerr
nonlinearity, and oscillator coupling, and analyze the role of entanglement in
improving the reservoir's computational performance, focusing on its effect on
predicting non-trivial time series. Using logarithmic negativity to quantify
entanglement and normalized root mean square error (NRMSE) to evaluate
predictive accuracy, our results suggest that entanglement provides a
computational advantage on average-up to a threshold in the input
frequency-that persists under some levels of dissipation and dephasing. In
particular, we find that higher dissipation rates can enhance performance.
While the entanglement advantage manifests as improvements in both average and
worst-case performance, it does not lead to improvements in the best-case
error. These findings contribute to the broader understanding of quantum
reservoirs for high performance, efficient quantum machine learning and
time-series forecasting.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [107] [CleanCTG: A Deep Learning Model for Multi-Artefact Detection and Reconstruction in Cardiotocography](https://arxiv.org/abs/2508.10928)
*Sheng Wong,Beth Albert,Gabriel Davis Jones*

Main category: eess.AS

TL;DR: CleanCTG是一种双阶段模型，用于识别和校正胎心监护中的多种伪影，显著提升信号质量，减少误诊风险。


<details>
  <summary>Details</summary>
Motivation: 胎心监护中的伪影会掩盖真实胎心率模式，导致误诊或延迟干预，现有方法未能全面处理噪声问题。

Method: CleanCTG采用多尺度卷积和上下文感知交叉注意力识别伪影，并通过特定校正分支重建信号。训练数据为80万分钟合成噪声数据。

Result: 在合成数据上，CleanCTG完美检测伪影（AU-ROC=1.00），MSE显著降低；临床验证中AU-ROC=0.95，决策时间缩短33%。

Conclusion: CleanCTG通过显式伪影去除和信号重建，提高诊断准确性并缩短监测时间，为胎心监护提供更可靠解决方案。

Abstract: Cardiotocography (CTG) is essential for fetal monitoring but is frequently
compromised by diverse artefacts which obscure true fetal heart rate (FHR)
patterns and can lead to misdiagnosis or delayed intervention. Current
deep-learning approaches typically bypass comprehensive noise handling,
applying minimal preprocessing or focusing solely on downstream classification,
while traditional methods rely on simple interpolation or rule-based filtering
that addresses only missing samples and fail to correct complex artefact types.
We present CleanCTG, an end-to-end dual-stage model that first identifies
multiple artefact types via multi-scale convolution and context-aware
cross-attention, then reconstructs corrupted segments through artefact-specific
correction branches. Training utilised over 800,000 minutes of physiologically
realistic, synthetically corrupted CTGs derived from expert-verified "clean"
recordings. On synthetic data, CleanCTG achieved perfect artefact detection
(AU-ROC = 1.00) and reduced mean squared error (MSE) on corrupted segments to
2.74 x 10^-4 (clean-segment MSE = 2.40 x 10^-6), outperforming the next best
method by more than 60%. External validation on 10,190 minutes of
clinician-annotated segments yielded AU-ROC = 0.95 (sensitivity = 83.44%,
specificity 94.22%), surpassing six comparator classifiers. Finally, when
integrated with the Dawes-Redman system on 933 clinical CTG recordings,
denoised traces increased specificity (from 80.70% to 82.70%) and shortened
median time to decision by 33%. These findings suggest that explicit artefact
removal and signal reconstruction can both maintain diagnostic accuracy and
enable shorter monitoring sessions, offering a practical route to more reliable
CTG interpretation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [108] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出了一种基于数据驱动的逆优化器和PPO框架的自动治疗计划方法，用于头颈部癌症的质子PBS治疗计划，显著提高了效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统质子PBS治疗计划需要大量人工调整和计算密集型逆优化，效率低下且依赖经验。

Method: 结合L2O逆优化器和PPO框架，利用Transformer处理长上下文，自动调整目标参数并生成高质量计划。

Result: 相比L-BFGSB，L2O逆优化器效率和效果分别提升36.41%和22.97%，计划生成时间平均2.55小时，质量优于人工计划。

Conclusion: 该方法显著提升了质子PBS治疗计划的自动化水平和临床适用性。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [109] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE提出了一种动态知识图谱嵌入框架，通过自适应维度扩展和动态蒸馏机制，显著提升了动态知识图谱嵌入的性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的知识图谱是动态演化的，现有方法未能充分考虑更新规模和系统评估，导致性能不足。

Method: SAGE根据更新规模确定嵌入维度并扩展空间，采用动态蒸馏机制平衡新旧知识。

Result: 在七个基准测试中，SAGE显著优于基线方法，MRR、H@1和H@10分别提升1.38%、1.25%和1.6%。

Conclusion: SAGE证明了自适应嵌入维度在动态知识图谱嵌入中的重要性，并提供了公开代码。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [110] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo是一个包含1,315个自然复杂问题的基准测试，旨在填补现有LLM基准测试中缺乏耗时问题的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试缺乏自然且耗时的问题，无法充分评估模型在复杂信息查询任务中的表现。

Method: 通过分解标注流程，手动生成并回答大量自然耗时问题，构建MoNaCo基准测试。

Result: 前沿LLM在MoNaCo上的F1得分最高仅为61.2%，主要受低召回率和幻觉问题影响。

Conclusion: MoNaCo为评估和改进LLM在复杂信息查询任务中的表现提供了有效资源。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [111] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 该研究提出了一种计算框架，用于从新闻文章中自动提取公司风险因素，并比较了不同机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 识别公司风险对投资者和金融市场的健康至关重要，但现有方法可能效率不足。

Method: 构建了一个包含七个方面的风险分类框架，标注了744篇新闻文章，并测试了多种模型，包括零样本和少样本提示的LLMs以及微调的预训练模型。

Result: 实验表明，微调的预训练模型在大多数风险因素上表现优于LLMs。通过对27.7万篇彭博新闻的分析，验证了该方法的实用性。

Conclusion: 从新闻中识别风险因素能为公司和行业运营提供深入洞察，微调模型在此任务中更具优势。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [112] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了一种基于验证器的推理时间扩展方法，用于提升掩码扩散语言模型（MDM）的生成质量，并在文本风格转换任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型（MDM）因其可扩展性和易训练性成为离散数据生成的前沿方法，但现有方法在推理时间扩展方面仍有改进空间。

Method: 提出了一种基于验证器的推理时间扩展方法，结合预训练嵌入模型，优化MDM的生成过程。

Result: 实验表明，该方法在文本风格转换任务中优于自回归语言模型，且显著提升了生成质量。

Conclusion: 验证器辅助的MDM在生成任务中具有显著优势，为离散数据生成提供了新思路。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [113] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH是一个可解释且可扩展的框架，用于检测社交媒体上的多模态虚假信息，通过聚类、跨模态对齐和时间建模提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立处理帖子，无法捕捉事件级别的结构和时间模式变化，且存在类别不平衡问题。

Method: E-CaTCH通过文本相似性和时间接近性聚类帖子，提取文本和视觉特征，跨模态对齐，并利用时间窗口和LSTM建模时间演化。

Result: 在多个数据集上优于现有方法，具有鲁棒性和泛化能力。

Conclusion: E-CaTCH能有效检测多模态虚假信息，适用于多样化场景。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [114] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出了一种基于梯度优化和正则化的新方法，为神经网络的预测生成提取式解释，适用于文本和图像输入。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型的发展，对其预测提供解释的需求增加。

Method: 通过掩码输入的非关键部分，结合梯度优化和正则化，确保解释的充分性、全面性和紧凑性。

Result: 方法在文本和图像分类中均生成高质量解释，证明无需专门模型即可实现理由提取。

Conclusion: 该方法将模型可解释性与理由提取结合，展示了其广泛适用性。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [115] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出了一种端到端的可微分训练范式，用于稳定训练基于Transformer的分类器，并同时生成输入标记的相关性分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要训练三个独立模型（选择器、分类器和互补分类器），导致训练不稳定且效率低。本文旨在简化这一过程，提高训练效率和稳定性。

Method: 通过单一模型同时完成选择器、分类器和互补分类器的功能，简化训练流程，并结合参数化和正则化技术生成类别相关的解释。

Result: 该方法显著提高了与人类标注的对齐度，无需显式监督即可达到最先进水平。

Conclusion: 提出的方法不仅简化了训练流程，还提高了模型的稳定性和解释性，为生成类别相关解释提供了新思路。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [116] [Data-driven global ocean model resolving ocean-atmosphere coupling dynamics](https://arxiv.org/abs/2508.10908)
*Jeong-Hwan Kim,Daehyun Kang,Young-Min Yang,Jae-Heung Park,Yoo-Geun Ham*

Main category: physics.ao-ph

TL;DR: KIST-Ocean是一种基于深度学习的全球三维海洋环流模型，通过U形视觉注意力对抗网络架构，显著提升了海洋预测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值模型在季节以上时间尺度的预测中存在局限性，需要开发能够模拟复杂海洋-大气耦合响应的深度学习模型。

Method: 采用U形视觉注意力对抗网络架构，结合部分卷积、对抗训练和迁移学习，解决沿海复杂性和预测分布漂移问题。

Result: 模型表现出强大的海洋预测能力和效率，准确捕捉了热带太平洋的Kelvin和Rossby波传播等关键海洋-大气耦合机制。

Conclusion: KIST-Ocean的成功验证了深度学习在全球天气和气候模型中的潜力，为扩展地球系统建模提供了可能。

Abstract: Artificial intelligence has advanced global weather forecasting,
outperforming traditional numerical models in both accuracy and computational
efficiency. Nevertheless, extending predictions beyond subseasonal timescales
requires the development of deep learning (DL)-based ocean-atmosphere coupled
models that can realistically simulate complex oceanic responses to atmospheric
forcing. This study presents KIST-Ocean, a DL-based global three-dimensional
ocean general circulation model using a U-shaped visual attention adversarial
network architecture. KIST-Ocean integrates partial convolution, adversarial
training, and transfer learning to address coastal complexity and predictive
distribution drift in auto-regressive models. Comprehensive evaluations
confirmed the model's robust ocean predictive skill and efficiency. Moreover,
it accurately captures realistic ocean response, such as Kelvin and Rossby wave
propagation in the tropical Pacific, and vertical motions induced by cyclonic
and anticyclonic wind stress, demonstrating its ability to represent key
ocean-atmosphere coupling mechanisms underlying climate phenomena, including
the El Nino-Southern Oscillation. These findings reinforce confidence in
DL-based global weather and climate models and their extending DL-based
approaches to broader Earth system modeling, offering potential for enhancing
climate prediction capabilities.

</details>


### [117] [Approximating the universal thermal climate index using sparse regression with orthogonal polynomials](https://arxiv.org/abs/2508.11307)
*Sabin Roman,Gregor Skok,Ljupco Todorovski,Saso Dzeroski*

Main category: physics.ao-ph

TL;DR: 本文探讨了数据驱动建模方法，用于分析和近似通用热气候指数（UTCI），通过符号和稀疏回归技术实现高效且可解释的函数逼近。


<details>
  <summary>Details</summary>
Motivation: UTCI是一个非线性、多变量的生理学指标，需要高效且可解释的建模方法。

Method: 采用正交多项式基（如Legendre多项式）的稀疏回归框架，比较其稳定性、收敛性和层次可解释性。

Result: 模型在均方根损失上显著优于六次多项式基准，且参数更少，泛化能力强。

Conclusion: 结合稀疏性、正交性和符号结构，可实现对复杂环境指标的稳健、可解释建模，优于现有方法。

Abstract: This article explores novel data-driven modeling approaches for analyzing and
approximating the Universal Thermal Climate Index (UTCI), a
physiologically-based metric integrating multiple atmospheric variables to
assess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we
investigate symbolic and sparse regression techniques as tools for
interpretable and efficient function approximation. In particular, we highlight
the benefits of using orthogonal polynomial bases-such as Legendre
polynomials-in sparse regression frameworks, demonstrating their advantages in
stability, convergence, and hierarchical interpretability compared to standard
polynomial expansions. We demonstrate that our models achieve significantly
lower root-mean squared losses than the widely used sixth-degree polynomial
benchmark-while using the same or fewer parameters. By leveraging Legendre
polynomial bases, we construct models that efficiently populate a Pareto front
of accuracy versus complexity and exhibit stable, hierarchical coefficient
structures across varying model capacities. Training on just 20% of the data,
our models generalize robustly to the remaining 80%, with consistent
performance under bootstrapping. The decomposition effectively approximates the
UTCI as a Fourier-like expansion in an orthogonal basis, yielding results near
the theoretical optimum in the L2 (least squares) sense. We also connect these
findings to the broader context of equation discovery in environmental
modeling, referencing probabilistic grammar-based methods that enforce domain
consistency and compactness in symbolic expressions. Taken together, these
results illustrate how combining sparsity, orthogonality, and symbolic
structure enables robust, interpretable modeling of complex environmental
indices like UTCI - and significantly outperforms the state-of-the-art
approximation in both accuracy and efficiency.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [118] [Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas](https://arxiv.org/abs/2508.11278)
*Francesco Sovrano,Gabriele Dominici,Rita Sevastjanova,Alessandra Stramiglio,Alberto Bacchelli*

Main category: cs.HC

TL;DR: 论文提出了一种动态基准框架，用于评估通用AI系统在软件工程中表现出的认知偏见，发现这些系统确实存在偏见，且随任务复杂性增加而加剧。


<details>
  <summary>Details</summary>
Motivation: 研究通用AI系统是否因训练数据而表现出人类认知偏见，以揭示其在软件工程中的潜在风险。

Method: 开发动态基准框架，通过16个手工任务和AI生成的任务变体，测试8种认知偏见对AI决策的影响。

Result: 主流AI系统（如GPT、LLaMA、DeepSeek）均表现出认知偏见（5.9%-35%），且任务复杂性越高偏见越明显（最高49%）。

Conclusion: 通用AI系统在软件工程中存在认知偏见风险，需进一步研究以优化其决策可靠性。

Abstract: Human cognitive biases in software engineering can lead to costly errors.
While general-purpose AI (GPAI) systems may help mitigate these biases due to
their non-human nature, their training on human-generated data raises a
critical question: Do GPAI systems themselves exhibit cognitive biases?
  To investigate this, we present the first dynamic benchmarking framework to
evaluate data-induced cognitive biases in GPAI within software engineering
workflows. Starting with a seed set of 16 hand-crafted realistic tasks, each
featuring one of 8 cognitive biases (e.g., anchoring, framing) and
corresponding unbiased variants, we test whether bias-inducing linguistic cues
unrelated to task logic can lead GPAI systems from correct to incorrect
conclusions.
  To scale the benchmark and ensure realism, we develop an on-demand
augmentation pipeline relying on GPAI systems to generate task variants that
preserve bias-inducing cues while varying surface details. This pipeline
ensures correctness (88--99% on average, according to human evaluation),
promotes diversity, and controls reasoning complexity by leveraging
Prolog-based reasoning and LLM-as-a-judge validation. It also verifies that the
embedded biases are both harmful and undetectable by logic-based, unbiased
reasoners.
  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent
tendency to rely on shallow linguistic heuristics over deep reasoning. All
systems exhibit cognitive biases (ranging from 5.9% to 35% across types), with
bias sensitivity increasing sharply with task complexity (up to 49%),
highlighting critical risks in real-world software engineering deployments.

</details>


### [119] [Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil](https://arxiv.org/abs/2508.10911)
*Luis Vitor Zerkowski,Nina S. T. Hirata*

Main category: cs.HC

TL;DR: 论文介绍了一种利用人工智能技术提升巴西土著文化遗产在线平台Tainacan的可访问性和探索性的数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 面对系统性边缘化和城市发展对土著文化遗产的威胁，研究旨在通过技术手段增强文化遗产的保护和传播。

Method: 开发了两种语义管道：视觉管道建模图像相似性，文本管道捕捉描述中的语义关系，并将其整合为交互式可视化工具。

Result: 系统支持基于相似性、时间和地理的探索，提升了策展效率和公众参与度，揭示了藏品中的潜在联系。

Conclusion: 研究表明AI可以以伦理方式为文化遗产保护做出贡献。

Abstract: Indigenous communities face ongoing challenges in preserving their cultural
heritage, particularly in the face of systemic marginalization and urban
development. In Brazil, the Museu Nacional dos Povos Indigenas through the
Tainacan platform hosts the country's largest online collection of Indigenous
objects and iconographies, providing a critical resource for cultural
engagement. Using publicly available data from this repository, we present a
data-driven initiative that applies artificial intelligence to enhance
accessibility, interpretation, and exploration. We develop two semantic
pipelines: a visual pipeline that models image-based similarity and a textual
pipeline that captures semantic relationships from item descriptions. These
embedding spaces are projected into two dimensions and integrated into an
interactive visualization tool we also developed. In addition to
similarity-based navigation, users can explore the collection through temporal
and geographic lenses, enabling both semantic and contextualized perspectives.
The system supports curatorial tasks, aids public engagement, and reveals
latent connections within the collection. This work demonstrates how AI can
ethically contribute to cultural preservation practices.

</details>


### [120] [Human-in-the-Loop Systems for Adaptive Learning Using Generative AI](https://arxiv.org/abs/2508.11062)
*Bhavishya Tarun,Haoze Du,Dinesh Kannan,Edward F. Gehringer*

Main category: cs.HC

TL;DR: 论文提出了一种结合人类反馈与生成式AI的个性化学习方法，通过学生反馈标签实时调整AI生成内容，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用学生反馈优化AI生成的学习内容，增强学习参与度和理解深度，特别是在STEM教育中。

Method: 采用HITL方法，结合反馈标签和提示工程，通过RAG系统实时检索和调整教育内容。

Result: 初步研究表明，该方法在STEM学生中提高了学习效果和自信心。

Conclusion: 该工作展示了AI通过迭代优化创建动态、个性化学习环境的潜力。

Abstract: A Human-in-the-Loop (HITL) approach leverages generative AI to enhance
personalized learning by directly integrating student feedback into
AI-generated solutions. Students critique and modify AI responses using
predefined feedback tags, fostering deeper engagement and understanding. This
empowers students to actively shape their learning, with AI serving as an
adaptive partner. The system uses a tagging technique and prompt engineering to
personalize content, informing a Retrieval-Augmented Generation (RAG) system to
retrieve relevant educational material and adjust explanations in real time.
This builds on existing research in adaptive learning, demonstrating how
student-driven feedback loops can modify AI-generated responses for improved
student retention and engagement, particularly in STEM education. Preliminary
findings from a study with STEM students indicate improved learning outcomes
and confidence compared to traditional AI tools. This work highlights AI's
potential to create dynamic, feedback-driven, and personalized learning
environments through iterative refinement.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [121] [Repetitive TMS-based Identification of Methamphetamine-Dependent Individuals Using EEG Spectra](https://arxiv.org/abs/2508.11312)
*Ziyi Zeng,Yun-Hsuan Chen,Xurong Gao,Wenyao Zheng,Hemmings Wu,Zhoule Zhu,Jie Yang,Chengkai Wang,Lihua Zhong,Weiwei Cheng,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: 研究探讨了利用神经信号（EEG）评估rTMS对冰毒使用者渴求感的影响，发现gamma频段的相对功率（RBP）可作为区分冰毒使用者和健康人的生物标志物，并验证了rTMS的效果。


<details>
  <summary>Details</summary>
Motivation: 传统问卷评估冰毒使用者渴求感存在主观性，研究旨在通过EEG信号提供更客观的评估方法。

Method: 分析20名冰毒使用者在rTMS前后的EEG信号（MBT和MAT）及20名健康人（HC）的数据，提取各频段RBP，并利用随机森林（RF）进行分类。

Result: gamma RBP是区分MBT和HC的最佳频段（准确率90%），TP10和CP2通道的gamma RBP在分类任务中起主导作用。

Conclusion: gamma RBP可作为评估rTMS效果的生物标志物，并为定制闭环神经调控系统提供实时监测参数。

Abstract: The impact of repetitive transcranial magnetic stimulation (rTMS) on
methamphetamine (METH) users' craving levels is often assessed using
questionnaires. This study explores the feasibility of using neural signals to
obtain more objective results. EEG signals recorded from 20 METH-addicted
participants Before and After rTMS (MBT and MAT) and from 20 healthy
participants (HC) are analyzed. In each EEG paradigm, participants are shown 15
METH-related and 15 neutral pictures randomly, and the relative band power
(RBP) of each EEG sub-band frequency is derived. The average RBP across all 31
channels, as well as individual brain regions, is analyzed. Statistically,
MAT's alpha, beta, and gamma RBPs are more like those of HC compared to MBT, as
indicated by the power topographies. Utilizing a random forest (RF), the gamma
RBP is identified as the optimal frequency band for distinguishing between MBT
and HC with a 90% accuracy. The performance of classifying MAT versus HC is
lower than that of MBT versus HC, suggesting that the efficacy of rTMS can be
validated using RF with gamma RBP. Furthermore, the gamma RBP recorded by the
TP10 and CP2 channels dominates the classification task of MBT versus HC when
receiving METH-related image cues. The gamma RBP during exposure to
METH-related cues can serve as a biomarker for distinguishing between MBT and
HC and for evaluating the effectiveness of rTMS. Therefore, real-time
monitoring of gamma RBP variations holds promise as a parameter for
implementing a customized closed-loop neuromodulation system for treating METH
addiction.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [122] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: 论文提出了一种从仿真到现实的框架，用于开发和学习在复杂地形中导航的强化学习策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决学习型控制器在行星表面非结构化地形导航中的仿真与现实差距问题。

Method: 利用大规模并行仿真训练强化学习代理，并在随机生成的多样化环境中验证策略，随后零样本迁移到物理轮式漫游车。

Result: 实验表明，通过程序多样性训练的代理在零样本性能上优于静态场景训练的代理，同时分析了高保真粒子物理微调的权衡。

Conclusion: 该框架为开发可靠的基于学习的导航系统提供了验证的工作流程，是自主机器人在太空探索中部署的关键一步。

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [123] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine是一个模块化框架，通过共享基础模型和并行任务头，优化GPU使用，提升视觉多任务处理效率，实现3倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限机器人平台上部署多个机器学习模型时的计算冗余、内存占用大和集成复杂性问题。

Method: 利用共享基础模型提取图像特征，并行运行多个任务头，避免GPU-CPU内存传输，支持动态任务优先级调整。

Result: 在NVIDIA Jetson Orin AGX上实现≥50 Hz的实时性能，相比顺序执行提速3倍。

Conclusion: VPEngine为机器人社区提供了一个高效、可扩展且易用的视觉多任务处理框架。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [124] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: 论文研究了农业采摘中抓取状态的分类方法，通过集成多种传感器并使用随机森林分类器，实现了100%的准确率。


<details>
  <summary>Details</summary>
Motivation: 农业环境的复杂性和遮挡问题使得准确理解抓取状态成为关键挑战，需要选择合适的传感器和建模技术。

Method: 研究结合了IMU、红外反射、张力、触觉传感器和RGB摄像头，评估了随机森林和LSTM模型的性能。

Result: 随机森林分类器在实验室和实际樱桃番茄植株测试中实现了100%的准确率，优于基线性能。

Conclusion: IMU和张力传感器的组合足以有效分类抓取状态，为实时反馈和纠正动作提供了基础，提升了采摘效率和可靠性。

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [125] [Non-asymptotic convergence bound of conditional diffusion models](https://arxiv.org/abs/2508.10944)
*Mengze Li*

Main category: stat.ML

TL;DR: 论文提出了一种基于条件扩散模型（CARD）的新方法，用于分类和回归任务，通过整合预训练模型提升生成质量，并建立了理论分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管条件扩散模型在加速算法和生成质量方面取得进展，但缺乏非渐近性质阻碍了理论研究。本文旨在填补这一空白。

Method: 将预训练模型f_{\phi}(x)整合到扩散模型中，精确捕捉条件分布Y|f_{\phi}(x)，并基于Fokker-Planck方程推导其广义形式。

Result: 在Lipschitz假设下，利用二阶Wasserstein距离证明了生成分布与原始分布的上界误差，并推导了收敛上界。

Conclusion: CARD为条件扩散模型提供了坚实的理论基础，并在理论和实验上验证了其有效性。

Abstract: Learning and generating various types of data based on conditional diffusion
models has been a research hotspot in recent years. Although conditional
diffusion models have made considerable progress in improving acceleration
algorithms and enhancing generation quality, the lack of non-asymptotic
properties has hindered theoretical research. To address this gap, we focus on
a conditional diffusion model within the domains of classification and
regression (CARD), which aims to learn the original distribution with given
input x (denoted as Y|X). It innovatively integrates a pre-trained model
f_{\phi}(x) into the original diffusion model framework, allowing it to
precisely capture the original conditional distribution given f (expressed as
Y|f_{\phi}(x)). Remarkably, when f_{\phi}(x) performs satisfactorily,
Y|f_{\phi}(x) closely approximates Y|X. Theoretically, we deduce the stochastic
differential equations of CARD and establish its generalized form predicated on
the Fokker-Planck equation, thereby erecting a firm theoretical foundation for
analysis. Mainly under the Lipschitz assumptions, we utilize the second-order
Wasserstein distance to demonstrate the upper error bound between the original
and the generated conditional distributions. Additionally, by appending
assumptions such as light-tailedness to the original distribution, we derive
the convergence upper bound between the true value analogous to the score
function and the corresponding network-estimated value.

</details>


### [126] [Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting](https://arxiv.org/abs/2508.11060)
*Jeongjin Lee,Jong-Min Kim*

Main category: stat.ML

TL;DR: 提出了一种基于Buckley James Boost Q学习的框架，用于在右删失生存数据下估计最优动态治疗方案，适用于纵向随机临床试验。


<details>
  <summary>Details</summary>
Motivation: 传统Cox Q学习方法依赖于风险模型，可能在模型错误设定下产生偏差，而新方法避免了比例风险假设，提供了更稳健的估计。

Method: 结合加速失效时间模型与迭代提升技术（如分量最小二乘和回归树），在反事实Q学习框架下直接建模条件生存时间。

Result: 模拟研究和HIV试验分析表明，BJ Boost Q学习在治疗决策中具有更高准确性，尤其在多阶段设置中减少偏差累积。

Conclusion: BJ Boost Q学习为生存数据下的动态治疗方案提供了灵活且无偏的估计方法，优于传统Cox Q学习。

Abstract: We propose a Buckley James (BJ) Boost Q learning framework for estimating
optimal dynamic treatment regimes under right censored survival data, tailored
for longitudinal randomized clinical trial settings. The method integrates
accelerated failure time models with iterative boosting techniques, including
componentwise least squares and regression trees, within a counterfactual Q
learning framework. By directly modeling conditional survival time, BJ Boost Q
learning avoids the restrictive proportional hazards assumption and enables
unbiased estimation of stage specific Q functions. Grounded in potential
outcomes, this framework ensures identifiability of the optimal treatment
regime under standard causal assumptions. Compared to Cox based Q learning,
which relies on hazard modeling and may suffer from bias under
misspecification, our approach provides robust and flexible estimation.
Simulation studies and analysis of the ACTG175 HIV trial demonstrate that BJ
Boost Q learning yields higher accuracy in treatment decision making,
especially in multistage settings where bias can accumulate.

</details>


### [127] [Uniform convergence for Gaussian kernel ridge regression](https://arxiv.org/abs/2508.11274)
*Paul Dommel,Rajmadan Lakshmanan*

Main category: stat.ML

TL;DR: 本文首次为固定超参数的高斯核岭回归（KRR）在均匀范数和$L^{2}$-范数下建立了多项式收敛速率。


<details>
  <summary>Details</summary>
Motivation: 填补高斯核KRR理论理解的空白，特别是在均匀范数下缺乏收敛速率的问题，并为固定宽度参数的高斯核提供理论支持。

Method: 通过理论分析，证明高斯核KRR在固定超参数情况下的多项式收敛速率。

Result: 在均匀范数和$L^{2}$-范数下均获得了多项式收敛速率，为高斯核KRR的应用提供了新的理论依据。

Conclusion: 研究结果为固定超参数的高斯核KRR在非参数回归中的使用提供了理论支持，扩展了对平滑核的理解。

Abstract: This paper establishes the first polynomial convergence rates for Gaussian
kernel ridge regression (KRR) with a fixed hyperparameter in both the uniform
and the $L^{2}$-norm. The uniform convergence result closes a gap in the
theoretical understanding of KRR with the Gaussian kernel, where no such rates
were previously known. In addition, we prove a polynomial $L^{2}$-convergence
rate in the case, where the Gaussian kernel's width parameter is fixed. This
also contributes to the broader understanding of smooth kernels, for which
previously only sub-polynomial $L^{2}$-rates were known in similar settings.
Together, these results provide new theoretical justification for the use of
Gaussian KRR with fixed hyperparameters in nonparametric regression.

</details>


### [128] [ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization](https://arxiv.org/abs/2508.11551)
*Shengzhuang Chen,Xu Ouyang,Michael Arthur Leopold Pearce,Thomas Hartvigsen,Jonathan Richard Schwarz*

Main category: stat.ML

TL;DR: 本文提出了一种基于贝叶斯优化的黑盒超参数优化方法，用于确定大规模语言模型训练的最佳数据混合比例，显著提升了性能并降低了实验成本。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练中数据混合比例的选择仍依赖启发式方法，缺乏可靠的学习方法，影响模型性能。

Method: 将数据混合比例选择视为黑盒超参数优化问题，采用多保真度贝叶斯优化方法，平衡实验成本与模型性能。

Result: 在从100万到70亿参数的模型上验证了方法的有效性，性能提升高达500%，并公开了包含460次实验的数据集。

Conclusion: 该方法为数据混合比例选择提供了高效解决方案，同时通过共享数据集降低了研究成本。

Abstract: Determining the optimal data mixture for large language model training
remains a challenging problem with an outsized impact on performance. In
practice, language model developers continue to rely on heuristic exploration
since no learning-based approach has emerged as a reliable solution. In this
work, we propose to view the selection of training data mixtures as a black-box
hyperparameter optimization problem, for which Bayesian Optimization is a
well-established class of appropriate algorithms. Firstly, we cast data mixture
learning as a sequential decision-making problem, in which we aim to find a
suitable trade-off between the computational cost of training exploratory
(proxy-) models and final mixture performance. Secondly, we systematically
explore the properties of transferring mixtures learned at a small scale to
larger-scale experiments, providing insights and highlighting opportunities for
research at a modest scale. By proposing Multi-fidelity Bayesian Optimization
as a suitable method in this common scenario, we introduce a natural framework
to balance experiment cost with model fit, avoiding the risks of overfitting to
smaller scales while minimizing the number of experiments at high cost. We
present results for pre-training and instruction finetuning across models
ranging from 1 million to 7 billion parameters, varying from simple
architectures to state-of-the-art models and benchmarks spanning dozens of
datasets. We demonstrate consistently strong results relative to a wide range
of benchmarks, showingspeed-ups of over 500% in determining the best data
mixture on our largest experiments relative to recent baselines. In addition,
we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full
training & evaluation runs across various model sizes worth over 13,000 GPU
hours, greatly reducing the cost of conducting research in this area.

</details>


### [129] [Nonparametric learning of stochastic differential equations from sparse and noisy data](https://arxiv.org/abs/2508.11597)
*Arnab Ganguly,Riten Mitra,Jinpu Zhou*

Main category: stat.ML

TL;DR: 提出了一种从稀疏、噪声观测中构建数据驱动随机微分方程（SDE）模型的系统框架，通过无强结构假设直接学习漂移函数，适用于复杂或部分理解的系统动力学。


<details>
  <summary>Details</summary>
Motivation: 传统参数化方法假设漂移函数形式已知，而本文旨在直接从数据中学习漂移函数，适用于科学领域中对系统动力学理解有限或高度复杂的情况。

Method: 将估计问题转化为在再生核希尔伯特空间（RKHS）中最小化惩罚负对数似然函数，开发了一种结合期望最大化（EM）算法和顺序蒙特卡洛（SMC）的方法，并通过贝叶斯变体控制模型复杂度。

Result: 提出的EM-SMC-RKHS方法在低数据条件下能准确估计随机动力系统的漂移函数，并通过数值实验验证了其有效性。

Conclusion: 该方法为在观测受限条件下进行连续时间建模提供了广泛适用的解决方案，具有理论和实践意义。

Abstract: The paper proposes a systematic framework for building data-driven stochastic
differential equation (SDE) models from sparse, noisy observations. Unlike
traditional parametric approaches, which assume a known functional form for the
drift, our goal here is to learn the entire drift function directly from data
without strong structural assumptions, making it especially relevant in
scientific disciplines where system dynamics are partially understood or highly
complex. We cast the estimation problem as minimization of the penalized
negative log-likelihood functional over a reproducing kernel Hilbert space
(RKHS). In the sparse observation regime, the presence of unobserved trajectory
segments makes the SDE likelihood intractable. To address this, we develop an
Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte
Carlo (SMC) method to approximate the filtering distribution and generate Monte
Carlo estimates of the E-step objective. The M-step then reduces to a penalized
empirical risk minimization problem in the RKHS, whose minimizer is given by a
finite linear combination of kernel functions via a generalized representer
theorem. To control model complexity across EM iterations, we also develop a
hybrid Bayesian variant of the algorithm that uses shrinkage priors to identify
significant coefficients in the kernel expansion. We establish important
theoretical convergence results for both the exact and approximate EM
sequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of
the drift function of stochastic dynamical systems in low-data regimes and is
broadly applicable across domains requiring continuous-time modeling under
observational constraints. We demonstrate the effectiveness of our method
through a series of numerical experiments.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [130] [Functional Analysis of Variance for Association Studies](https://arxiv.org/abs/2508.11069)
*Olga A. Vsevolozhskaya,Dmitri V. Zaykin,Mark C. Greenwood,Changshuai Wei,Qing Lu*

Main category: stat.AP

TL;DR: 提出了一种功能方差分析（FANOVA）方法，用于测试基因组区域序列变异与定性性状的关联，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管已识别出与人类疾病相关的常见遗传变异，但对大多数复杂疾病而言，这些变异仅解释了遗传性的一小部分。需要更强大的统计方法发现新的疾病相关变异。

Method: 提出FANOVA方法，联合测试基因变异的效应，利用连锁不平衡和遗传位置信息，允许保护性或风险增加的因果变异。

Result: 模拟显示FANOVA在小样本或变异效应低至中等时优于SKAT和FLM。实证研究中，FANOVA同时检测到ANGPTL 4和ANGPTL 3与肥胖相关，而SKAT和FLM仅分别检测到其中之一。

Conclusion: FANOVA是一种高效且强大的方法，适用于复杂疾病遗传变异的研究。

Abstract: While progress has been made in identifying common genetic variants
associated with human diseases, for most of common complex diseases, the
identified genetic variants only account for a small proportion of
heritability. Challenges remain in finding additional unknown genetic variants
predisposing to complex diseases. With the advance in next-generation
sequencing technologies, sequencing studies have become commonplace in genetic
research. The ongoing exome-sequencing and whole-genome-sequencing studies
generate a massive amount of sequencing variants and allow researchers to
comprehensively investigate their role in human diseases. The discovery of new
disease-associated variants can be enhanced by utilizing powerful and
computationally efficient statistical methods. In this paper, we propose a
functional analysis of variance (FANOVA) method for testing an association of
sequence variants in a genomic region with a qualitative trait. The FANOVA has
a number of advantages: (1) it tests for a joint effect of gene variants,
including both common and rare; (2) it fully utilizes linkage disequilibrium
and genetic position information; and (3) allows for either protective or
risk-increasing causal variants. Through simulations, we show that FANOVA
outperform two popularly used methods - SKAT and a previously proposed method
based on functional linear models (FLM), - especially if a sample size of a
study is small and/or sequence variants have low to moderate effects. We
conduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to
sequencing data from Dallas Heart Study. While SKAT and FLM respectively
detected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to
identify both genes associated with obesity.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [131] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: 本文提出了一种数据并行的两级静态无冲突哈希映射实现方法，通过功能化Fredman等人的构造并扁平化处理。讨论了在功能数组语言中提供灵活、多态和抽象接口的挑战，特别是动态大小键的问题。算法在Futhark中实现，GPU性能优于传统树/搜索方法，但与cuCollections库相比在构造速度上仍有差距。


<details>
  <summary>Details</summary>
Motivation: 研究如何在功能数组语言中高效实现哈希映射，解决动态键大小问题，并探索性能优化的可能性。

Method: 功能化Fredman等人的哈希映射构造，扁平化处理，并在Futhark中实现。通过上下文关联解决动态键问题。

Result: GPU性能优于传统方法，但构造速度不及cuCollections库。性能差距部分源于Futhark编译器的限制，部分源于数据并行编程模型的表达不足。

Conclusion: 功能数组语言模型可能需要扩展以弥补性能差距，尤其是在构造速度和算法表达方面。

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [132] [HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis](https://arxiv.org/abs/2508.11181)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的深度学习框架，用于组织病理图像的多类肿瘤分类，显著提升了分类性能并减少了预处理需求。


<details>
  <summary>Details</summary>
Motivation: 现代病理学中，癌症诊断的准确性和可扩展性是一个关键挑战，尤其是对于乳腺癌、前列腺癌、骨癌和宫颈癌等具有复杂组织学变异性的恶性肿瘤。

Method: 采用微调的Vision Transformer (ViT)架构，通过简化的预处理流程将全切片图像转换为PyTorch张量并进行数据标准化，以适应组织病理图像。

Result: 在四个基准数据集上（乳腺癌、前列腺癌、骨癌和宫颈癌），分类准确率分别达到99.32%、96.92%、95.28%和96.94%，AUC分数均超过99%。

Conclusion: 基于Transformer的架构在数字病理学中展现出强大的鲁棒性、泛化能力和临床潜力，为可靠、自动化和可解释的癌症诊断系统提供了重要进展。

Abstract: Accurate and scalable cancer diagnosis remains a critical challenge in modern
pathology, particularly for malignancies such as breast, prostate, bone, and
cervical, which exhibit complex histological variability. In this study, we
propose a transformer-based deep learning framework for multi-class tumor
classification in histopathological images. Leveraging a fine-tuned Vision
Transformer (ViT) architecture, our method addresses key limitations of
conventional convolutional neural networks, offering improved performance,
reduced preprocessing requirements, and enhanced scalability across tissue
types. To adapt the model for histopathological cancer images, we implement a
streamlined preprocessing pipeline that converts tiled whole-slide images into
PyTorch tensors and standardizes them through data normalization. This ensures
compatibility with the ViT architecture and enhances both convergence stability
and overall classification performance. We evaluate our model on four benchmark
datasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and
SipakMed (cervical) dataset -- demonstrating consistent outperformance over
existing deep learning methods. Our approach achieves classification accuracies
of 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical
cancers respectively, with area under the ROC curve (AUC) scores exceeding 99%
across all datasets. These results confirm the robustness, generalizability,
and clinical potential of transformer-based architectures in digital pathology.
Our work represents a significant advancement toward reliable, automated, and
interpretable cancer diagnosis systems that can alleviate diagnostic burdens
and improve healthcare outcomes.

</details>


### [133] [Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification](https://arxiv.org/abs/2508.11511)
*Siyamalan Manivannan*

Main category: eess.IV

TL;DR: 提出了一种结合集成学习和在线知识蒸馏的半监督深度学习方法，用于皮肤病变分类，减少对大量标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完全监督学习，标注数据获取成本高且困难，需减轻标注负担。

Method: 训练卷积神经网络集成模型，通过在线知识蒸馏将集成模型的洞察传递给成员模型，提升整体性能。

Result: 在公开数据集上表现优于现有方法，知识蒸馏后的单个模型性能优于独立训练模型。

Conclusion: 该方法减少了标注数据需求，提供了更高效的皮肤病变分类解决方案。

Abstract: Deep Learning has emerged as a promising approach for skin lesion analysis.
However, existing methods mostly rely on fully supervised learning, requiring
extensive labeled data, which is challenging and costly to obtain. To alleviate
this annotation burden, this study introduces a novel semi-supervised deep
learning approach that integrates ensemble learning with online knowledge
distillation for enhanced skin lesion classification. Our methodology involves
training an ensemble of convolutional neural network models, using online
knowledge distillation to transfer insights from the ensemble to its members.
This process aims to enhance the performance of each model within the ensemble,
thereby elevating the overall performance of the ensemble itself.
Post-training, any individual model within the ensemble can be deployed at test
time, as each member is trained to deliver comparable performance to the
ensemble. This is particularly beneficial in resource-constrained environments.
Experimental results demonstrate that the knowledge-distilled individual model
performs better than independently trained models. Our approach demonstrates
superior performance on both the \emph{International Skin Imaging
Collaboration} 2018 and 2019 public benchmark datasets, surpassing current
state-of-the-art results. By leveraging ensemble learning and online knowledge
distillation, our method reduces the need for extensive labeled data while
providing a more resource-efficient solution for skin lesion classification in
real-world scenarios.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [134] [Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci](https://arxiv.org/abs/1505.01206)
*Changshuai Wei,Daniel J. Schaid,Qing Lu*

Main category: q-bio.QM

TL;DR: 该论文提出了一种名为TAMW的高效计算方法，用于分析低边际效应（LME）遗传变异的联合关联，并在模拟和实证研究中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管低边际效应遗传变异在复杂疾病中起重要作用，但其发现和联合关联分析仍具挑战性。

Method: 提出Trees Assembling Mann Whitney（TAMW）方法，通过并行计算高效处理高维数据。

Result: 在模拟和实证研究中，TAMW表现优于MDR和LRMW，并在克罗恩病GWAS中发现显著联合关联。

Conclusion: TAMW是一种高效且强大的工具，适用于LME遗传变异的联合关联分析，揭示了克罗恩病的新候选基因。

Abstract: Common complex diseases are likely influenced by the interplay of hundreds,
or even thousands, of genetic variants. Converging evidence shows that genetic
variants with low marginal effects (LME) play an important role in disease
development. Despite their potential significance, discovering LME genetic
variants and assessing their joint association on high dimensional data (e.g.,
genome wide association studies) remain a great challenge. To facilitate joint
association analysis among a large ensemble of LME genetic variants, we
proposed a computationally efficient and powerful approach, which we call Trees
Assembling Mann whitney (TAMW). Through simulation studies and an empirical
data application, we found that TAMW outperformed multifactor dimensionality
reduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW)
when the underlying complex disease involves multiple LME loci and their
interactions. For instance, in a simulation with 20 interacting LME loci, TAMW
attained a higher power (power=0.931) than both MDR (power=0.599) and LRMW
(power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci,
TAMW also identified a stronger joint association with CD than those detected
by MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct
a genome wide analysis. The analysis of 459K single nucleotide polymorphisms
was completed in 40 hours using parallel computing, and revealed a joint
association predisposing to CD (p-value=2.763e-19). Further analysis of the
newly discovered association suggested that 13 genes, such as ATG16L1 and
LACC1, may play an important role in CD pathophysiological and etiological
processes.

</details>
