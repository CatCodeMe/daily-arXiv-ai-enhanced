<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 85]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 11]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 5]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [LLM-Driven Data Generation and a Novel Soft Metric for Evaluating Text-to-SQL in Aviation MRO](https://arxiv.org/abs/2506.13785)
*Patrick Sutanto,Jonathan Kenrick,Max Lorenz,Joan Santoso*

Main category: cs.DB

TL;DR: 本文提出了一种基于F1分数的软度量方法以评估文本到SQL任务的性能，并设计了一个LLM驱动的数据生成管道来解决领域特定数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如执行准确率）过于刚性且反馈粗糙，同时领域特定评估数据集稀缺，阻碍了LLM在文本到SQL任务中的应用。

Method: 引入基于F1分数的软度量方法量化生成SQL与真实SQL的信息重叠；提出LLM驱动的管道从数据库模式合成问题-SQL对。

Result: 实验表明，软度量比严格准确率提供更深入的性能分析，数据生成技术能有效创建领域特定基准。

Conclusion: 这些贡献为专业环境中文本到SQL系统的评估和进步提供了稳健框架。

Abstract: The application of Large Language Models (LLMs) to text-to-SQL tasks promises
to democratize data access, particularly in critical industries like aviation
Maintenance, Repair, and Operation (MRO). However, progress is hindered by two
key challenges: the rigidity of conventional evaluation metrics such as
execution accuracy, which offer coarse, binary feedback, and the scarcity of
domain-specific evaluation datasets. This paper addresses these gaps. To enable
more nuanced assessment, we introduce a novel F1-score-based 'soft' metric that
quantifies the informational overlap between generated and ground-truth SQL
results. To address data scarcity, we propose an LLM-driven pipeline that
synthesizes realistic question-SQL pairs from database schemas. We demonstrate
our contributions through an empirical evaluation on an authentic MRO database.
Our experiments show that the proposed soft metric provides more insightful
performance analysis than strict accuracy, and our data generation technique is
effective in creating a domain-specific benchmark. Together, these
contributions offer a robust framework for evaluating and advancing text-to-SQL
systems in specialized environments.

</details>


### [2] [Sketched Sum-Product Networks for Joins](https://arxiv.org/abs/2506.14034)
*Brian Tsan,Abylay Amanbayev,Asoke Datta,Florin Rusu*

Main category: cs.DB

TL;DR: 论文提出了一种基于Sum-Product Networks的动态近似草图方法，用于多路连接基数估计，解决了传统草图方法对新查询的适用性问题。


<details>
  <summary>Details</summary>
Motivation: 传统草图方法在查询优化中表现出高准确性，但通常针对预定义的查询选择构建，限制了其对新查询的适用性。

Method: 利用Sum-Product Networks动态近似草图，通过分解和建模多元分布为多个单变量分布的线性组合，并表示为草图。

Result: 实现了Fast-AGMS和Bound Sketch方法，通过近似草图提供了更实用的查询优化方案。

Conclusion: 该方法为查询优化中的草图应用提供了更灵活和高效的解决方案。

Abstract: Sketches have shown high accuracy in multi-way join cardinality estimation, a
critical problem in cost-based query optimization. Accurately estimating the
cardinality of a join operation -- analogous to its computational cost --
allows the optimization of query execution costs in relational database
systems. However, although sketches have shown high efficacy in query
optimization, they are typically constructed specifically for predefined
selections in queries that are assumed to be given a priori, hindering their
applicability to new queries. As a more general solution, we propose for
Sum-Product Networks to dynamically approximate sketches on-the-fly.
Sum-Product Networks can decompose and model multivariate distributions, such
as relations, as linear combinations of multiple univariate distributions. By
representing these univariate distributions as sketches, Sum-Product Networks
can combine them element-wise to efficiently approximate the sketch of any
query selection. These approximate sketches can then be applied to join
cardinality estimation. In particular, we implement the Fast-AGMS and Bound
Sketch methods, which have successfully been used in prior work, despite their
costly construction. By accurately approximating them instead, our work
provides a practical alternative to apply these sketches to query optimization.

</details>


### [3] [HARMONY: A Scalable Distributed Vector Database for High-Throughput Approximate Nearest Neighbor Search](https://arxiv.org/abs/2506.14707)
*Qian Xu,Feng Zhang,Chengxi Li,Lei Cao,Zheng Chen,Jidong Zhai,Xiaoyong Du*

Main category: cs.DB

TL;DR: Harmony是一种分布式ANNS系统，通过多粒度分区策略和早期停止剪枝机制，解决了负载不平衡和高通信开销问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决单机处理高维向量时的内存和效率问题，以及现有分布式方案中的负载不平衡和通信开销问题。

Method: 采用多粒度分区策略（维度分区和向量分区）和早期停止剪枝机制。

Result: 在四节点上平均吞吐量提升4.63倍，对倾斜工作负载性能提升58%。

Conclusion: Harmony在分布式ANNS系统中表现出色，显著优于现有解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS) is essential for various
data-intensive applications, including recommendation systems, image retrieval,
and machine learning. Scaling ANNS to handle billions of high-dimensional
vectors on a single machine presents significant challenges in memory capacity
and processing efficiency. To address these challenges, distributed vector
databases leverage multiple nodes for the parallel storage and processing of
vectors. However, existing solutions often suffer from load imbalance and high
communication overhead, primarily due to traditional partition strategies that
fail to effectively distribute the workload. In this paper, we introduce
Harmony, a distributed ANNS system that employs a novel multi-granularity
partition strategy, combining dimension-based and vector-based partition. This
strategy ensures a balanced distribution of computational load across all nodes
while effectively minimizing communication costs. Furthermore, Harmony
incorporates an early-stop pruning mechanism that leverages the monotonicity of
distance computations in dimension-based partition, resulting in significant
reductions in both computational and communication overhead. We conducted
extensive experiments on diverse real-world datasets, demonstrating that
Harmony outperforms leading distributed vector databases, achieving 4.63 times
throughput on average in four nodes and 58% performance improvement over
traditional distribution for skewed workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [DAGs for the Masses](https://arxiv.org/abs/2506.13998)
*Michael Anoprenko,Andrei Tonkikh,Alexander Spiegelman,Petr Kuznetsov*

Main category: cs.DC

TL;DR: 提出了一种稀疏DAG架构，通过减少节点间的引用数量提升可扩展性，同时保持对拜占庭故障的容忍能力。


<details>
  <summary>Details</summary>
Motivation: 现有DAG协议因节点引用数量线性增长导致可扩展性受限，需解决这一问题以支持大规模部署。

Method: 设计稀疏DAG架构，每个节点仅引用前一轮的常数个随机节点，并基于Bullshark协议实现稀疏版本。

Result: 稀疏DAG在保持$f<n/3$拜占庭容错能力的同时，显著减少了元数据传输，提升了网络利用率和可扩展性。

Conclusion: 稀疏方法适用于任何基于图结构的协议，能有效提升可扩展性而不牺牲安全性。

Abstract: A recent approach to building consensus protocols on top of Directed Acyclic
Graphs (DAGs) shows much promise due to its simplicity and stable throughput.
However, as each node in the DAG typically includes a linear number of
references to the nodes in the previous round, prior DAG protocols only scale
up to a certain point when the overhead of maintaining the graph becomes the
bottleneck.
  To enable large-scale deployments of DAG-based protocols, we propose a sparse
DAG architecture, where each node includes only a constant number of references
to random nodes in the previous round. We present a sparse version of Bullshark
-- one of the most prominent DAG-based consensus protocols -- and demonstrate
its improved scalability.
  Remarkably, unlike other protocols that use random sampling to reduce
communication complexity, we manage to avoid sacrificing resilience: the
protocol can tolerate up to $f<n/3$ Byzantine faults (where $n$ is the number
of participants), same as its less scalable deterministic counterpart. The
proposed ``sparse'' methodology can be applied to any protocol that maintains
disseminated system updates and causal relations between them in a graph-like
structure. Our simulations show that the considerable reduction of transmitted
metadata in sparse DAGs results in more efficient network utilization and
better scalability.

</details>


### [5] [Keigo: Co-designing Log-Structured Merge Key-Value Stores with a Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)](https://arxiv.org/abs/2506.14630)
*Rúben Adão,Zhongjie Wu,Changjun Zhou,Oana Balmau,João Paulo,Ricardo Macedo*

Main category: cs.DC

TL;DR: Keigo是一种并发和工作负载感知的存储中间件，优化了LSM KVS在异构存储设备上的性能，通过智能数据放置和缓存技术提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有LSM KVS在异构存储设备上缺乏通用的数据放置策略，无法适应多样化工作负载的需求。

Method: Keigo采用并发感知数据放置、持久只读缓存和基于上下文的I/O区分三种技术。

Result: 实验表明，Keigo将生产级LSM的吞吐量提升至4倍（写密集型）和18倍（读密集型）。

Conclusion: Keigo是一种高效、可移植且自适应的存储中间件，适用于异构存储环境。

Abstract: We present Keigo, a concurrency- and workload-aware storage middleware that
enhances the performance of log-structured merge key-value stores (LSM KVS)
when they are deployed on a hierarchy of storage devices. The key observation
behind Keigo is that there is no one-size-fits-all placement of data across the
storage hierarchy that optimizes for all workloads. Hence, to leverage the
benefits of combining different storage devices, Keigo places files across
different devices based on their parallelism, I/O bandwidth, and capacity. We
introduce three techniques - concurrency-aware data placement, persistent
read-only caching, and context-based I/O differentiation. Keigo is portable
across different LSMs, is adaptable to dynamic workloads, and does not require
extensive profiling. Our system enables established production KVS such as
RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We
evaluate Keigo using synthetic and realistic workloads, showing that it
improves the throughput of production-grade LSMs up to 4x for write- and 18x
for read-heavy workloads when compared to general-purpose storage systems and
specialized LSM KVS.

</details>


### [6] [Déjà Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse](https://arxiv.org/abs/2506.14107)
*Jinwoo Hwang,Daeun Kim,Sangyeop Lee,Yoonsung Kim,Guseul Heo,Hojoon Kim,Yunseok Jeong,Tadiwos Meaza,Eunhyeok Park,Jeongseob Ahn,Jongse Park*

Main category: cs.DC

TL;DR: Déjà Vu是一种视频语言查询引擎，通过重用连续帧的计算加速ViT-based VideoLMs，显著提升大规模视频分析的实用性。


<details>
  <summary>Details</summary>
Motivation: 当前VideoLMs在处理大规模视频时，因需逐帧提取视觉嵌入而面临计算效率低的问题，亟需解决方案。

Method: 提出ReuseViT模型，检测帧间重用机会，并结合内存-计算联合压缩技术，将计算节省转化为实际性能提升。

Result: 在三个VideoLM任务中，Déjà Vu将嵌入生成速度提升至2.64倍，误差控制在2%以内。

Conclusion: Déjà Vu通过计算重用和优化技术，显著提升了VideoLMs在大规模视频分析中的实用性。

Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable
capabilities, offering significant potential for flexible and powerful video
query systems. These models typically rely on Vision Transformers (ViTs), which
process video frames individually to extract visual embeddings. However,
generating embeddings for large-scale videos requires ViT inferencing across
numerous frames, posing a major hurdle to real-world deployment and
necessitating solutions for integration into scalable video data management
systems. This paper introduces D\'ej\`a Vu, a video-language query engine that
accelerates ViT-based VideoLMs by reusing computations across consecutive
frames. At its core is ReuseViT, a modified ViT model specifically designed for
VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking
an effective balance between accuracy and reuse. Although ReuseViT
significantly reduces computation, these savings do not directly translate into
performance gains on GPUs. To overcome this, D\'ej\`a Vu integrates
memory-compute joint compaction techniques that convert the FLOP savings into
tangible performance gains. Evaluations on three VideoLM tasks show that
D\'ej\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error
bound, dramatically enhancing the practicality of VideoLMs for large-scale
video analytics.

</details>


### [7] [The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic Demonstration of Miner-Centric Propagation Topologies](https://arxiv.org/abs/2506.14197)
*Dr Craig S Wright*

Main category: cs.DC

TL;DR: 本文通过复杂图论分析BTC和BSV的网络结构，证明家庭托管的全节点无法参与或影响传播拓扑。


<details>
  <summary>Details</summary>
Motivation: 研究BTC和BSV网络中全节点的实际作用，揭示其在共识传播中的边缘化现象。

Method: 采用复杂图论模型（如无标度网络和小世界连接性），结合模拟和特征向量中心性分析。

Result: 传播图由密集互联的矿工群体主导，全节点位于外围，无法影响交易到区块的路径。

Conclusion: 全节点在共识传播中既不关键也无操作相关性。

Abstract: This paper formally examines the network structure of Bitcoin CORE (BTC) and
Bitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that
home-hosted full nodes are incapable of participating in or influencing the
propagation topology. Leveraging established models such as scale-free networks
and small-world connectivity, we demonstrate that the propagation graph is
dominated by a densely interconnected miner clique, while full nodes reside on
the periphery, excluded from all transaction-to-block inclusion paths. Using
simulation-backed metrics and eigenvalue centrality analysis, we confirm that
full nodes are neither critical nor operationally relevant for consensus
propagation.

</details>


### [8] [A Novel Indicator for Quantifying and Minimizing Information Utility Loss of Robot Teams](https://arxiv.org/abs/2506.14237)
*Xiyu Zhao,Qimei Cui,Wei Ni,Quan Z. Sheng,Abbas Jamalipour,Guoshun Nan,Xiaofeng Tao,Ping Zhang*

Main category: cs.DC

TL;DR: 提出了一种新的度量标准LoIU，用于量化信息的新鲜度和效用，并通过半分散式多智能体框架优化传输调度和资源分配，显著提升了信息新鲜度和效用。


<details>
  <summary>Details</summary>
Motivation: 机器人团队中信息交换的及时性对协作至关重要，但无线容量有限可能导致信息传递延迟，从而影响协作效果。

Method: 提出LoIU度量标准，基于信念分布估计LoIU，并开发半分散式多智能体框架优化传输调度和资源分配。

Result: 仿真结果显示，相比其他方法，信息新鲜度和效用提升了98%。

Conclusion: LoIU度量标准和半分散式框架有效优化了机器人团队中的信息交换，显著提升了协作效率。

Abstract: The timely exchange of information among robots within a team is vital, but
it can be constrained by limited wireless capacity. The inability to deliver
information promptly can result in estimation errors that impact collaborative
efforts among robots. In this paper, we propose a new metric termed Loss of
Information Utility (LoIU) to quantify the freshness and utility of information
critical for cooperation. The metric enables robots to prioritize information
transmissions within bandwidth constraints. We also propose the estimation of
LoIU using belief distributions and accordingly optimize both transmission
schedule and resource allocation strategy for device-to-device transmissions to
minimize the time-average LoIU within a robot team. A semi-decentralized
Multi-Agent Deep Deterministic Policy Gradient framework is developed, where
each robot functions as an actor responsible for scheduling transmissions among
its collaborators while a central critic periodically evaluates and refines the
actors in response to mobility and interference. Simulations validate the
effectiveness of our approach, demonstrating an enhancement of information
freshness and utility by 98%, compared to alternative methods.

</details>


### [9] [Concepts for designing modern C++ interfaces for MPI](https://arxiv.org/abs/2506.14610)
*C. Nicole Avans,Alfredo A. Correa,Sayan Ghosh,Matthias Schimek,Joseph Schuchart,Anthony Skjellum,Evan D. Suggs,Tim Niklas Uhl*

Main category: cs.DC

TL;DR: 论文探讨了现代C++接口与MPI语义之间的不匹配问题，提出了解决类型系统、对象生命周期和通信缓冲区三个关键方面的方案。


<details>
  <summary>Details</summary>
Motivation: 随着现代C++和异构编程模型的兴起，MPI社区需要重新审视高层次的C++接口设计，以兼顾性能、可移植性和通用编程原则。

Method: 论文聚焦于类型系统、对象生命周期和通信缓冲区三个核心问题，而非提出另一个新的接口。

Result: 指出了MPI规范中的不一致性，并提出了初步解决方案，希望引发社区讨论。

Conclusion: 呼吁MPI和C++社区共同参与，解决接口设计中的关键挑战。

Abstract: Since the C++ bindings were deleted in 2008, the Message Passing Interface
(MPI) community has revived efforts in building high-level modern C++
interfaces. Such interfaces are either built to serve specific scientific
application needs (with limited coverage to the underlying MPI
functionalities), or as an exercise in general-purpose programming model
building, with the hope that bespoke interfaces can be broadly adopted to
construct a variety of distributed-memory scientific applications. However,
with the advent of modern C++-based heterogeneous programming models, GPUs and
widespread Machine Learning (ML) usage in contemporary scientific computing,
the role of prospective community-standardized high-level C++ interfaces to MPI
is evolving. The success of such an interface clearly will depend on providing
robust abstractions and features adhering to the generic programming principles
that underpin the C++ programming language, without compromising on either
performance and portability, the core principles upon which MPI was founded.
However, there is a tension between idiomatic C++ handling of types and
lifetimes, and, MPI's loose interpretation of object lifetimes/ownership and
insistence on maintaining global states.
  Instead of proposing "yet another" high-level C++ interface to MPI,
overlooking or providing partial solutions to work around the key issues
concerning the dissonance between MPI semantics and idiomatic C++, this paper
focuses on the three fundamental aspects of a high-level interface: type
system, object lifetimes and communication buffers, also identifying
inconsistencies in the MPI specification. Presumptive solutions can be
unrefined, and we hope the broader MPI and C++ communities will engage with us
in productive exchange of ideas and concerns.

</details>


### [10] [Resource Optimization with MPI Process Malleability for Dynamic Workloads in HPC Clusters](https://arxiv.org/abs/2506.14743)
*Sergio Iserte,Iker Martín-Álvarez,Krzysztof Rojek,José I. Aliaga,Maribel Castillo,Weronika Folwarska,Antonio J. Peña*

Main category: cs.DC

TL;DR: 本文扩展了动态资源管理（DMR）框架，结合Proteo框架的Malleability Module（MaM），提出新的动态资源分配策略，显著提升高性能计算（HPC）环境的资源利用率和效率。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算环境中，动态资源管理对优化计算效率至关重要，但现有技术在生产环境中的标准化、互操作性和易用性方面存在挑战。

Method: 基于DMRlib重新配置运行时，整合Proteo框架的MaM模块，引入新的生成策略和数据重分配方法，探索MPI通信合并和异步重新配置等新策略。

Result: 实验表明，动态资源管理可将工作负载完成时间减少40%，资源利用率提高20%以上。

Conclusion: 动态资源管理在高性能计算中具有显著优势，能够显著提升效率和资源利用率。

Abstract: Dynamic resource management is essential for optimizing computational
efficiency in modern high-performance computing (HPC) environments,
particularly as systems scale. While research has demonstrated the benefits of
malleability in resource management systems (RMS), the adoption of such
techniques in production environments remains limited due to challenges in
standardization, interoperability, and usability. Addressing these gaps, this
paper extends our prior work on the Dynamic Management of Resources (DMR)
framework, which provides a modular and user-friendly approach to dynamic
resource allocation. Building upon the original DMRlib reconfiguration runtime,
this work integrates new methodology from the Malleability Module (MaM) of the
Proteo framework, further enhancing reconfiguration capabilities with new
spawning strategies and data redistribution methods. In this paper, we explore
new malleability strategies in HPC dynamic workloads, such as merging MPI
communicators and asynchronous reconfigurations, which offer new opportunities
for dramatically reducing memory overhead. The proposed enhancements are
rigorously evaluated on a world-class supercomputer, demonstrating improved
resource utilization and workload efficiency. Results show that dynamic
resource management can reduce the workload completion time by 40% and increase
the resource utilization by over 20%, compared to static resource allocation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Spectral partitioning of graphs into compact, connected regions](https://arxiv.org/abs/2506.13982)
*Ewan Davies,Ryan Job,Maxine Kampbell,Hannah Kim,Hyojin Seo*

Main category: cs.DS

TL;DR: 本文提出了一种名为SpecReCom的谱重组算法，用于将图划分为指定数量的连通部分，并展示了其变体BalSpecReCom以平衡权重或顶点数量。实验表明，该算法在紧凑性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地将图划分为连通部分，并满足额外约束（如平衡权重或顶点数量）。

Method: 提出SpecReCom算法及其变体BalSpecReCom，通过谱重组实现图划分。

Result: 在56×56网格图和科罗拉多州平面图上验证，SpecReCom比RevReCom等算法更紧凑。

Conclusion: SpecReCom算法在满足连通性和平衡约束的同时，提供了更紧凑的图划分。

Abstract: We define and study a spectral recombination algorithm, SpecReCom, for
partitioning a graph into a given number of connected parts. It is
straightforward to introduce additional constraints such as the requirement
that the weight (or number of vertices) in each part is approximately balanced,
and we exemplify this by stating a variant, BalSpecReCom, of the SpecReCom
algorithm. We provide empirical evidence that the algorithm achieves more
compact partitions than alternatives such as RevReCom by studying a $56\times
56$ grid graph and a planar graph obtained from the state of Colorado.

</details>


### [12] [glass: ordered set data structure for client-side order books](https://arxiv.org/abs/2506.13991)
*Viktor Krapivensky*

Main category: cs.DS

TL;DR: 论文提出了一种基于字典树（trie）的有序集合实现，针对整数键和顺序局部性优化，通过缓存路径、哈希表加速、硬件指令优化和订单簿特定功能，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统有序集合（如红黑树或B树）在特定场景（如市场数据）下性能不足，需针对顺序局部性和整数键优化。

Method: 采用字典树结构，结合缓存路径、哈希表加速、硬件指令（BMI2）和订单簿特定功能（如树重构）。

Result: 相比C++标准std::map，修改操作快6x-20x，查找快30x，市场数据快9x-15x，迭代快2x-3x。

Conclusion: 基于字典树的实现显著提升了有序集合在特定场景下的性能，适用于市场数据等高频操作需求。

Abstract: The "ordered set" abstract data type with operations "insert", "erase",
"find", "min", "max", "next" and "prev" is ubiquitous in computer science. It
is usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We
present our implementation of ordered set based on a trie. It only supports
integer keys (as opposed to keys of any strict weakly ordered type) and is
optimized for market data, namely for what we call sequential locality. The
following is the list of what we believe to be novelties:
  * Cached path to exploit sequential locality, and fast truncation thereof on
erase operation;
  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on
any operation to speed up key lookup (up to a pre-leaf node);
  * Hardware-accelerated "find next/previous set bit" operations with BMI2
instruction set extension on x86-64;
  * Order book-specific features: the preemption principle and the tree
restructure operation that prevent the tree from consuming too much memory.
  We achieve the following speedups over C++'s standard std::map container:
6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market
data, and a more modest 2x-3x speedup on iteration. In this paper, we discuss
our implementation.

</details>


### [13] [An Exact and Efficient Sampler for Dynamic Discrete Distributions](https://arxiv.org/abs/2506.14062)
*Lilith Orion Hafner,Adriano Meligrana*

Main category: cs.DS

TL;DR: EBUS（Exact BUcket Sampling）是首个具有O(1)采样和更新成本的精确算法，解决了动态离散分布采样中的数值偏差问题。


<details>
  <summary>Details</summary>
Motivation: 动态离散分布采样中，现有方法（如Alias方法）在频繁更新时效率低下且存在数值偏差，EBUS旨在解决这些问题。

Method: EBUS通过基于基数b的有界精度和指数更新采样器，实现精确高效的采样和更新。

Result: EBUS在IEEE 64位浮点数实现中表现优于以往的非精确方法。

Conclusion: EBUS为动态离散分布采样提供了首个精确且高效的解决方案。

Abstract: Sampling from a dynamic discrete distribution involves sampling from a
dynamic set of weighted elements, where elements can be added or removed at any
stage of the sampling process. Although efficient for static sets, the Alias
method becomes impractical in dynamic settings due to the need to reconstruct
the sampler after each update, which incurs a computational cost proportional
to the size of the distribution, making it unsuitable for applications
requiring frequent insertions, deletions, or weight adjustments. To address
this limitation, different approaches have been studied, such as the Forest of
Trees method and the BUcket Sampling (BUS) method. However, all previous
methods suffered from numerical issues which can bias the sampling process. In
this paper, we describe EBUS (Exact BUcket Sampling), the first exact algorithm
with $O(1)$ sampling and update cost. The sampler can be updated by base-$b$
numbers with bounded precision and exponent, and sample the distribution of its
elements exactly and efficiently. We provide also a state of the art
implementation of the method using IEEE 64-bit floating point numbers which we
empirically show to be more efficient than several implementations of previous
inexact methods.

</details>


### [14] [Simpler, Better, Faster, Stronger: Revisiting a Successful Reduction Rule for Dominating Set](https://arxiv.org/abs/2506.14564)
*Lukas Geis,Alexander Leonhardt,Johannes Meintrup,Ulrich Meyer,Manuel Penschuck*

Main category: cs.DS

TL;DR: 提出了一种线性时间算法，改进并超越了原有的Rule1规则，用于解决DominatingSet问题，同时提供了更高效的图剪枝方法。


<details>
  <summary>Details</summary>
Motivation: DominatingSet问题是NP完全问题，现有规则（如Rule1）虽然有效但计算成本高，需要更高效的实现方法。

Method: 将原有Rule1的O(n^3)时间算法重新设计为线性时间算法，并扩展了框架以进一步剪枝图。

Result: 新算法不仅实现了线性时间运行，还提供了更强的剪枝效果，易于实现且实用性强。

Conclusion: 该算法在理论和实践上均优于原有方法，为DominatingSet问题提供了更高效的解决方案。

Abstract: DominatingSet is a classical NP-complete problem and also known to be
W[2]-hard. Thus, there is little hope for small kernels on general graphs.
However, in practice, reduction rules to heuristically shrink instances are
used. In this context, Rule1 by Alber et. al. is quite successful, yet at times
somewhat expensive to execute. We propose a linear time algorithm implementing
and surpassing the original Rule1 formulation. Our discussions and proofs yield
interesting structural insights into the reduction rule and its interplay with
the DominatingSet problem. For instance, while the original formulation
warrants repeated invocations of an $\mathcal{O}(n^3)$ time algorithm, we
recast it to allow a single search run in linear time. Then, we propose simple,
but practically significant, extensions to our algorithmic framework to prune
the graph even further. The algorithm is easy to implement and highly
practical.

</details>


### [15] [Compressing Suffix Trees by Path Decompositions](https://arxiv.org/abs/2506.14734)
*Ruben Becker,Davide Cenzato,Travis Gagie,Sung-Hwan Kim,Ragnar Groot Koerkamp,Giovanni Manzini,Nicola Prezza*

Main category: cs.DS

TL;DR: 本文提出了一种新的后缀树路径压缩方法，通过更精细的指针选择实现高效压缩，支持缓存高效的模式匹配查询。


<details>
  <summary>Details</summary>
Motivation: 经典后缀树中的路径压缩方法需要随机访问文本，本文旨在探索一种更优雅、简单且高效的压缩方式。

Method: 将后缀树分解为节点到叶子的路径，并用指向文本后缀的指针表示每条路径，利用数组A的排序特性支持高效查询。

Result: 提出的索引比r-index更小且快数个数量级，能高效定位查询模式的所有出现。

Conclusion: 新方法通过路径分解和指针优化，显著提升了后缀树的压缩效率和查询性能。

Abstract: In classic suffix trees, path compression works by replacing unary suffix
trie paths with pairs of pointers to $T$, which must be available in the form
of some random access oracle at query time. In this paper, we revisit path
compression and show that a more careful choice of pointers leads to a new
elegant, simple, and remarkably efficient way to compress the suffix tree. We
begin by observing that an alternative way to path-compress the suffix trie of
$T$ is to decompose it into a set of (disjoint) node-to-leaf paths and then
represent each path as a pointer $i$ to one of the string's suffixes $T[i,n]$.
At this point, we show that the array $A$ of such indices $i$, sorted by the
colexicographic order of the corresponding text prefixes $T[1,i]$, possesses
the following properties: (i) it supports \emph{cache-efficient} pattern
matching queries via simple binary search on $A$ and random access on $T$, and
(ii) it contains a number of entries being proportional to the size of the
\emph{compressed text}. Of particular interest is the path decomposition given
by the colexicographic rank of $T$'s prefixes. The resulting index is smaller
and orders of magnitude faster than the $r$-index on the task of locating all
occurrences of a query pattern.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework](https://arxiv.org/abs/2506.13800)
*Abul Ehtesham,Aditi Singh,Saket Kumar*

Main category: cs.SE

TL;DR: 本文提出了一种基于开源代理的框架，通过整合大型语言模型（LLMs）和HL7 FHIR数据，动态提取和推理电子健康记录（EHRs），以增强临床决策支持（CDS）、减轻文档负担并提高患者健康素养。


<details>
  <summary>Details</summary>
Motivation: 解决数字健康领域中临床决策支持、文档负担和患者健康素养的持续挑战。

Method: 采用基于代理的框架，结合LLMs和FHIR数据，通过JSON配置实现动态访问和推理EHRs。

Result: 框架支持实时总结、解释和个性化沟通，并通过合成EHR数据验证了隐私性和可重复性。

Conclusion: 该方法为个性化数字健康解决方案提供了可扩展、可解释和互操作的AI支持EHR应用基础。

Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens,
and improving patient health literacy remain persistent challenges in digital
health. This paper presents an open-source, agent-based framework that
integrates Large Language Models (LLMs) with HL7 FHIR data via the Model
Context Protocol (MCP) for dynamic extraction and reasoning over electronic
health records (EHRs). Built on the established MCP-FHIR implementation, the
framework enables declarative access to diverse FHIR resources through
JSON-based configurations, supporting real-time summarization, interpretation,
and personalized communication across multiple user personas, including
clinicians, caregivers, and patients. To ensure privacy and reproducibility,
the framework is evaluated using synthetic EHR data from the SMART Health IT
sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4
standard. Unlike traditional approaches that rely on hardcoded retrieval and
static workflows, the proposed method delivers scalable, explainable, and
interoperable AI-powered EHR applications. The agentic architecture further
supports multiple FHIR formats, laying a robust foundation for advancing
personalized digital health solutions.

</details>


### [17] [Instruction and Solution Probabilities as Heuristics for Inductive Programming](https://arxiv.org/abs/2506.13804)
*Edward McDaid,Sarah McDaid*

Main category: cs.SE

TL;DR: 论文提出了一种通过引入指令和解决方案概率来进一步缩小归纳编程搜索空间的方法，结合指令子集（IS）可将搜索空间减少超过100个数量级。


<details>
  <summary>Details</summary>
Motivation: 为了更高效地缩小归纳编程（IP）的搜索空间，研究如何利用指令和解决方案概率作为额外启发式方法。

Method: 引入指令概率（基于代码样本中指令出现的频率）和解决方案概率（指令概率的乘积），并利用最小解决方案概率作为阈值来剪枝搜索空间。

Result: 实验表明，该方法与IS结合可将搜索空间减少超过100个数量级，且交叉验证表明其对未见代码也有效。

Conclusion: 该方法显著提升了搜索效率，未来可进一步优化概率模型。

Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the
inductive programming (IP) search space by tens of orders of magnitude. Here,
we extend the IS approach by introducing instruction and solution probabilities
as additional heuristics. Instruction probability reflects the expectation of
an instruction occurring in a solution, based on the frequency of instruction
occurrence in a large code sample. The solution probability for a partial or
complete program is simply the product of all constituent instruction
probabilities, including duplicates. We treat the minimum solution
probabilities observed in code sample program units of different sizes as
solution probability thresholds. These thresholds are used to prune the search
space as partial solutions are constructed, thereby eliminating any branches
containing unlikely combinations of instructions. The new approach has been
evaluated using a large sample of human code. We tested two formulations of
instruction probability: one based on instruction occurrence across the entire
code sample and another that measured the distribution separately for each IS.
Our results show that both variants produce substantial further reductions in
the IP search space size of up to tens of orders of magnitude, depending on
solution size. In combination with IS, reductions of over 100 orders of
magnitude can be achieved. We also carried out cross-validation testing to show
that the heuristics should work effectively with unseen code. The approach is
described and the results and some ideas for future work are discussed.

</details>


### [18] [Signal-First Architectures: Rethinking Front-End Reactivity](https://arxiv.org/abs/2506.13815)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.SE

TL;DR: 本文提出了一种名为Signal-First Architecture的新范式，通过显式信号声明和依赖追踪解决前端框架中的反应性管理问题。


<details>
  <summary>Details</summary>
Motivation: 现代前端框架面临反应性管理的挑战，如复杂可观察链导致的性能下降和不可预测的重新渲染。

Method: Signal-First Architecture以细粒度的信号作为反应性的基本单位，通过computed()和effect()实现派生值和副作用，确保确定性行为。

Result: 通过对比Angular的RxJS、NgRx和Signal-First模型，实验证明Signal-First在性能、内存和审计方面具有优势。

Conclusion: Signal-First Architecture通过显式信号和优化反应图评估，提供了一种高效且可预测的反应性管理方案。

Abstract: Modern front-end frameworks face escalating reactivity management challenges,
including performance degradation from complex observable chains and
unpredictable re-renders. This paper introduces Signal-First Architecture--a
novel paradigm where granular, dependency-tracked signals are the atomic unit
of reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces
reactive flows from explicit signal declarations, with derived values via
computed() and side effects scoped to effect(). This model ensures
deterministic behavior by eliminating implicit subscriptions and optimizing
reactive graph evaluation.
  We present a comparative analysis of three Angular reactivity models: RxJS
service-based, NgRx global stores, and pure Signal-First implementations.
Through controlled benchmarking, including Chrome DevTools performance tracing,
memory heap snapshots, and Lighthouse audits, this study quantifies
Signal-First advantages.

</details>


### [19] [Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge](https://arxiv.org/abs/2506.13820)
*Shraddha Surana,Ashwin Srinivasan,Michael Bain*

Main category: cs.SE

TL;DR: IPARC挑战赛通过合成图像任务评估程序合成能力，本文提出一种基于LLM的结构化归纳编程方法，成功解决所有IPARC任务，揭示了LLM在代码生成中的关键机制。


<details>
  <summary>Details</summary>
Motivation: IPARC挑战赛的600项任务难以自动化解决，研究旨在探索LLM在程序合成中的潜力及其与人类协作的价值。

Method: 采用结构化归纳编程方法，结合LLM生成代码，并通过人工细化、冻结正确代码和代码重用等策略优化。

Result: 成功解决所有IPARC任务，揭示了LLM在代码生成中的关键机制，如结构化重要性、代码冻结和重用效率。

Conclusion: LLM与人类协作在复杂程序合成中具有重要价值，未来可进一步优化协作机制。

Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis
tasks over synthetic images to evaluate automatic program construction,
focusing on sequence, selection, and iteration. This set of 600 tasks has
resisted automated solutions. This paper presents a structured inductive
programming approach with LLMs that successfully solves tasks across all IPARC
categories. The controlled nature of IPARC reveals insights into LLM-based code
generation, including the importance of prior structuring, LLMs' ability to aid
structuring (requiring human refinement), the need to freeze correct code, the
efficiency of code reuse, and how LLM-generated code can spark human
creativity. These findings suggest valuable mechanisms for human-LLM
collaboration in tackling complex program synthesis.

</details>


### [20] [Role, cost, and complexity of software in the real-world: a case for formal methods](https://arxiv.org/abs/2506.13821)
*Giovanni Bernardi,Adrian Francalanza,Marco Peressotti,Mohammad Reza Mousavi*

Main category: cs.SE

TL;DR: 本章概述了软件在现代社会中的作用以及低质量软件的巨大成本，并通过过去40年重大软件故障的案例支持研究形式化软件验证和程序分析的必要性。


<details>
  <summary>Details</summary>
Motivation: 揭示低质量软件带来的巨大成本，论证形式化软件验证和程序分析的重要性。

Method: 通过回顾过去40年重大软件故障的案例，分析其成本。

Result: 成功工业经验支持形式化软件验证和程序分析的价值。

Conclusion: 形式化软件验证和程序分析的研究和应用是必要的。

Abstract: In this chapter we outline the role that software has in modern society,
along with the staggering costs of poor software quality. To lay this bare, we
recall the costs of some of the major software failures that happened during
the last~$40$ years. We argue that these costs justify researching, studying
and applying formal software verification and in particular program analysis.
This position is supported by successful industrial experiences.

</details>


### [21] [MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios](https://arxiv.org/abs/2506.13824)
*Jinyang Huang,Xiachong Feng,Qiguang Chen,Hanjie Zhao,Zihui Cheng,Jiesong Bai,Jingxuan Zhou,Min Li,Libo Qin*

Main category: cs.SE

TL;DR: 论文提出了MLDebugging基准，用于评估多库Python代码的调试问题，发现当前LLMs在多库场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了真实应用中的多库场景，限制了LLMs在代码调试中的潜力。

Method: 构建包含126个Python库的MLDebugging基准，涵盖七类多库代码问题，并评估主流LLMs的表现。

Result: 当前LLMs在多库调试场景中表现不佳。

Conclusion: MLDebugging揭示了LLMs在多库调试中的潜力，为未来研究提供了方向。

Abstract: Code debugging is a crucial task in software engineering, which attracts
increasing attention. While remarkable success has been made in the era of
large language models (LLMs), current research still focuses on the simple
no-library or single-library setting, ignoring the complex multi-library
scenario in real-world applications. To address this limitation, we make the
first attempt to introduce MLDebugging (Multi-Library Debugging), a
comprehensive benchmark designed to assess debugging challenges within
multi-library Python code. Specifically, MLDebugging encompasses 126 distinct
Python libraries, covering a wide range of multi-library code issues,
categorized into seven distinct types. Furthermore, we conduct a thorough
evaluation of MLDebugging using both mainstream open-source and closed-source
LLMs and highlight that current LLMs still struggle to correctly perform code
debugging across multi-library scenarios. We hope this work can uncover the
potential of LLMs in multi-library debugging scenario and offer insights for
future research.

</details>


### [22] [FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation](https://arxiv.org/abs/2506.13832)
*Hongda Zhu,Yiwen Zhang,Bing Zhao,Jingzhe Ding,Siyao Liu,Tong Liu,Dandan Wang,Yanan Liu,Zhaojian Li*

Main category: cs.SE

TL;DR: 论文提出了FrontendBench，一个由人类和LLMs共同开发的基准测试，用于更全面评估前端代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在任务过于简单、测试用例不严谨和缺乏端到端验证等问题，无法准确评估模型性能。

Method: FrontendBench包含148个精心设计的任务，覆盖从基础UI到复杂交互功能，并引入自动评估框架在沙盒环境中测试代码。

Result: 自动评估框架与专家评估的一致性达90.54%，且不同LLMs在真实前端任务中表现差异显著。

Conclusion: FrontendBench是一个可靠且可扩展的基准测试，为前端代码生成研究提供了坚实基础。

Abstract: Large Language Models (LLMs) have made significant strides in front-end code
generation. However, existing benchmarks exhibit several critical limitations:
many tasks are overly simplistic, test cases often lack rigor, and end-to-end
validation is absent. These issues hinder the accurate assessment of model
performance. To address these challenges, we present FrontendBench, a benchmark
co-developed by humans and LLMs. FrontendBench categorizes tasks based on code
functionality and incorporates interactive test scenarios, enabling a more
comprehensive and practical evaluation of front-end code generation
capabilities. The benchmark comprises 148 meticulously crafted prompt-test case
pairs spanning five levels of web components, from basic UI elements to complex
interactive features. Each task reflects realistic front-end development
challenges. Furthermore, we introduce an automatic evaluation framework that
executes generated code within a sandbox environment and assesses outcomes
using predefined test scripts. This framework achieves a 90.54% agreement rate
with expert human evaluations, demonstrating high reliability. We benchmark
several state-of-the-art LLMs on FrontendBench and observe substantial
performance disparities in handling real-world front-end tasks. These results
highlight FrontendBench as a reliable and scalable benchmark, supporting
consistent multimodal evaluation and providing a robust foundation for future
research in front-end code generation. Our data and code will be released soon.

</details>


### [23] [How Does LLM Reasoning Work for Code? A Survey and a Call to Action](https://arxiv.org/abs/2506.13932)
*Ira Ceka,Saurabh Pujar,Irene Manotas,Gail Kaiser,Baishakhi Ray,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文综述了大型语言模型（LLMs）在代码推理任务中的应用，总结了相关策略、技术分类、性能评估及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在软件工程任务（如GitHub问题解决）中的实际应用潜力，特别是代码推理技术的效能和范式。

Method: 通过调查和分类代码推理技术，分析性能基准，并探索代码核心属性对推理技术的影响。

Result: 提出了代码推理的综述、技术分类、性能评估及新基准，并识别了未来研究的潜在方向。

Conclusion: 代码推理技术在LLMs中的应用具有广阔前景，但仍需进一步探索未充分研究的领域。

Abstract: The rise of large language models (LLMs) has led to dramatic improvements
across a wide range of natural language tasks. These advancements have extended
into the domain of code, facilitating complex tasks such as code generation,
translation, summarization, and repair. However, their utility for real-world
deployment in-the-wild has only recently been studied, particularly on software
engineering (SWE) tasks such as GitHub issue resolution. In this study, we
examine the code reasoning techniques that underlie the ability to perform such
tasks, and examine the paradigms used to drive their performance. Our
contributions in this paper are: (1) the first dedicated survey on code
reasoning for code tasks, highlighting overarching strategies, hybrid and
agentic approaches; (2) a taxonomy of various techniques used to drive code
reasoning; (3) a comprehensive overview of performance on common benchmarks and
a showcase of new, under-explored benchmarks with high potential in SWE; (4) an
exploration on how core properties of code can be used to explain different
reasoning techniques; and (5) gaps and potentially under-explored areas for
future research.

</details>


### [24] [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
*Shiting Huang,Zhen Fang,Zehui Chen,Siyu Yuan,Junjie Ye,Yu Zeng,Lin Chen,Qi Mao,Feng Zhao*

Main category: cs.SE

TL;DR: 论文研究了大型语言模型（LLMs）在工具使用过程中遇到的错误，提出了CRITICTOOL评测基准，用于评估和改进LLMs的工具学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂性和长期性的增加，LLMs在工具使用过程中可能触发各种意外错误，如何有效处理这些错误成为关键研究方向。

Method: 通过分析多个工具评测基准中的错误类型，构建了CRITICTOOL评测基准，采用进化策略生成多样化错误数据集。

Result: 实验验证了CRITICTOOL的泛化性和有效性，并深入分析了不同LLMs的工具反思能力。

Conclusion: CRITICTOOL为LLMs工具学习领域提供了新的视角和评测工具。

Abstract: The ability of large language models (LLMs) to utilize external tools has
enabled them to tackle an increasingly diverse range of tasks. However, as the
tasks become more complex and long-horizon, the intricate tool utilization
process may trigger various unexpected errors. Therefore, how to effectively
handle such errors, including identifying, diagnosing, and recovering from
them, has emerged as a key research direction for advancing tool learning. In
this work, we first extensively analyze the types of errors encountered during
the function-calling process on several competitive tool evaluation benchmarks.
Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation
benchmark specialized for tool learning. Building upon a novel evolutionary
strategy for dataset construction, CRITICTOOL holds diverse tool-use errors
with varying complexities, which better reflects real-world scenarios. We
conduct extensive experiments on CRITICTOOL, and validate the generalization
and effectiveness of our constructed benchmark strategy. We also provide an
in-depth analysis of the tool reflection ability on various LLMs, offering a
new perspective on the field of tool learning in LLMs. The code is available at
\href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.

</details>


### [25] [Characterising Bugs in Jupyter Platform](https://arxiv.org/abs/2506.14055)
*Yutian Tang,Hongchen Cao,Yuxi Chen,David Lo*

Main category: cs.SE

TL;DR: 本文研究了Jupyter平台中的387个错误，将其分类为11种根本原因和11种症状，并提出了14个主要发现，为开发者提供了新工具开发方向。


<details>
  <summary>Details</summary>
Motivation: Jupyter作为流行的编程平台，其错误研究对确保其正确性、安全性和鲁棒性至关重要，但此前研究未关注其宿主平台的错误。

Method: 通过分析387个Jupyter平台错误，分类其根本原因和症状。

Result: 识别了11种根本原因和11种症状，提出了14个主要发现。

Conclusion: 研究为开发者提供了新工具开发方向，以检测和修复Jupyter平台中的错误。

Abstract: As a representative literate programming platform, Jupyter is widely adopted
by developers, data analysts, and researchers for replication, data sharing,
documentation, interactive data visualization, and more. Understanding the bugs
in the Jupyter platform is essential for ensuring its correctness, security,
and robustness. Previous studies focused on code reuse, restoration, and repair
execution environment for Jupyter notebooks. However, the bugs in Jupyter
notebooks' hosting platform Jupyter are not investigated. In this paper, we
investigate 387 bugs in the Jupyter platform. These Jupyter bugs are classified
into 11 root causes and 11 bug symptoms. We identify 14 major findings for
developers. More importantly, our study opens new directions in building tools
for detecting and fixing bugs in the Jupyter platform.

</details>


### [26] [A Quantum Annealing Approach for Solving Optimal Feature Selection and Next Release Problems](https://arxiv.org/abs/2506.14129)
*Shuchang Wang,Xiaopeng Qiu,Yingxing Xue,Yanfu Li,Wei Yang*

Main category: cs.SE

TL;DR: 论文提出基于量子退火（QA）的算法，解决搜索式软件工程（SBSE）中的多目标优化问题，针对不同规模问题设计了两种方法，并在实验中展示了其高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法和整数线性规划（ILP）在小规模问题上表现良好，但在大规模问题中的可扩展性未知。量子退火（QA）作为一种新兴技术，具有解决大规模优化问题的潜力。

Method: 针对小规模问题，将多目标优化（MOO）转化为单目标优化（SOO）并通过惩罚映射处理；针对大规模问题，采用基于最大能量影响（MEI）的分解策略，结合QA和最速下降法提升局部搜索效率。

Result: 实验表明，与ILP方法相比，QA方法生成的非支配解较少但显著减少执行时间；与NSGA-II相比，QA方法生成更多非支配解且计算效率更高。

Conclusion: 量子退火在SBSE问题中展现出高效和可扩展的潜力，为未来研究提供了新方向。

Abstract: Search-based software engineering (SBSE) addresses critical optimization
challenges in software engineering, including the next release problem (NRP)
and feature selection problem (FSP). While traditional heuristic approaches and
integer linear programming (ILP) methods have demonstrated efficacy for small
to medium-scale problems, their scalability to large-scale instances remains
unknown. Here, we introduce quantum annealing (QA) as a subroutine to tackling
multi-objective SBSE problems, leveraging the computational potential of
quantum systems. We propose two QA-based algorithms tailored to different
problem scales. For small-scale problems, we reformulate multi-objective
optimization (MOO) as single-objective optimization (SOO) using penalty-based
mappings for quantum processing. For large-scale problems, we employ a
decomposition strategy guided by maximum energy impact (MEI), integrating QA
with a steepest descent method to enhance local search efficiency. Applied to
NRP and FSP, our approaches are benchmarked against the heuristic NSGA-II and
the ILP-based $\epsilon$-constraint method. Experimental results reveal that
while our methods produce fewer non-dominated solutions than
$\epsilon$-constraint, they achieve significant reductions in execution time.
Moreover, compared to NSGA-II, our methods deliver more non-dominated solutions
with superior computational efficiency. These findings underscore the potential
of QA in advancing scalable and efficient solutions for SBSE challenges.

</details>


### [27] [Mobile Application Review Summarization using Chain of Density Prompting](https://arxiv.org/abs/2506.14192)
*Shristi Shrestha,Anas Mahmoud*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLM）和Chain of Density（CoD）提示生成移动应用评论的摘要，以解决信息过载问题。


<details>
  <summary>Details</summary>
Motivation: 移动应用商店的评论数量庞大，导致用户难以做出明智的选择，需要一种有效的方法来总结评论。

Method: 使用OpenAI GPT-4和CoD提示，迭代提取关键实体并生成固定长度的摘要。

Result: 实验证明，CoD提示能有效提取关键主题并生成易于理解的摘要，同时保持语义密度和可读性。

Conclusion: 该方法通过总结用户反馈，提升了移动应用用户的体验。

Abstract: Mobile app users commonly rely on app store ratings and reviews to find apps
that suit their needs. However, the sheer volume of reviews available on app
stores can lead to information overload, thus impeding users' ability to make
informed app selection decisions. To address this challenge, we leverage Large
Language Models (LLMs) to summarize mobile app reviews. In particular, we use
the Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate
abstractive, semantically dense, and easily interpretable summaries of mobile
app reviews. The CoD prompt is engineered to iteratively extract salient
entities from the source text and fuse them into a fixed-length summary. We
evaluate the performance of our approach using a large dataset of mobile app
reviews. We further conduct an empirical evaluation with 48 study participants
to assess the readability of the generated summaries. Our results demonstrate
that adapting the CoD prompt to focus on app features improves its ability to
extract key themes from user reviews and generate natural language summaries
tailored for end-user consumption. The prompt also manages to maintain the
readability of the generated summaries while increasing their semantic density.
Our work in this paper aims to improve mobile app users' experience by
providing an effective mechanism for summarizing important user feedback in the
review stream.

</details>


### [28] [The Tech DEI Backlash -- The Changing Landscape of Diversity, Equity, and Inclusion in Software Engineering](https://arxiv.org/abs/2506.14232)
*Sonja M. Hyrynsalmi,Mary Sanchez-Gordon,Anna Szlavi,Letizia Jaccheri*

Main category: cs.SE

TL;DR: 论文研究了领先软件公司近年来的DEI策略变化，通过灰色文献研究分析了10家公司的DEI倡议现状，并提出了DEI Universe Map来可视化行业趋势。


<details>
  <summary>Details</summary>
Motivation: DEI倡议曾是软件公司的重点，但近期反弹导致公司重新评估策略，学术研究对此关注不足。

Method: 采用灰色文献研究，分析10家领先软件公司的DEI倡议变化。

Result: 公司对DEI的承诺分为减少、增加、重命名或保持不变，DEI Universe Map展示了行业趋势。

Conclusion: 公司对DEI反弹的应对策略多样，DEI Universe Map为研究提供了新视角。

Abstract: Not long ago, Diversity, Equity, and Inclusion (DEI) initiatives were a top
priority for leading software companies. However, in a short period, a wave of
backlash has led many firms to re-assess their DEI strategies. Responding to
this DEI backlash is crucial in academic research, especially because,
currently, little scholarly research has been done on it. In this paper,
therefore, we have set forth the following research question (RQ): "How have
leading software companies changed their DEI strategies in recent years?" Given
the novelty of the RQ and, consequently, the lack of scholarly research on it,
we are conducting a grey literature study, examining the current state of DEI
initiatives in 10 leading software companies. Based on our analysis, we have
classified companies into categories based on their shift in commitment to DEI.
We can identify that companies are indeed responding to the backlash by
rethinking their strategy, either by reducing, increasing, or renaming their
DEI initiatives. In contrast, some companies keep on with their DEI strategy,
at least so far, despite the challenging political climate. To illustrate these
changes, we introduce the DEI Universe Map, a visual representation of software
industry trends in DEI commitment and actions.

</details>


### [29] [Designing a Custom Chaos Engineering Framework for Enhanced System Resilience at Softtech](https://arxiv.org/abs/2506.14281)
*Ethem Utku Aktas,Burak Tuzlutas,Burak Yesiltas*

Main category: cs.SE

TL;DR: 提出一个为金融科技公司Softtech定制的混沌工程框架设计，旨在通过引入故障提升系统韧性，同时考虑金融行业法规。


<details>
  <summary>Details</summary>
Motivation: Softtech作为金融领域的领先软件公司，需要定制化的混沌工程框架以提升系统韧性，同时满足行业法规要求。

Method: 设计一个迭代且可扩展的框架，结合Softtech的基础设施、业务优先级和组织背景，确定关键活动和组件。

Result: 提出一个适合Softtech需求的混沌工程框架，支持团队逐步改进实践。

Conclusion: 定制化的混沌工程框架能有效提升Softtech的系统韧性，同时符合金融行业法规。

Abstract: Chaos Engineering is a discipline which enhances software resilience by
introducing faults to observe and improve system behavior intentionally. This
paper presents a design proposal for a customized Chaos Engineering framework
tailored for Softtech, a leading software development company serving the
financial sector. It outlines foundational concepts and activities for
introducing Chaos Engineering within Softtech, while considering financial
sector regulations. Building on these principles, the framework aims to be
iterative and scalable, enabling development teams to progressively improve
their practices. The study addresses two primary questions: how Softtech's
unique infrastructure, business priorities, and organizational context shape
the customization of its Chaos Engineering framework and what key activities
and components are necessary for creating an effective framework tailored to
Softtech's needs.

</details>


### [30] [Anticipating Bugs: Ticket-Level Bug Prediction and Temporal Proximity Effects](https://arxiv.org/abs/2506.14290)
*Daniele La Prova,Emanuele Gentili,Davide Falessi*

Main category: cs.SE

TL;DR: 论文提出了一种名为Ticket-Level Prediction (TLP)的方法，用于预测哪些工单在实现后会引入缺陷，并通过三个时间点（Open、In Progress、Closed）分析其准确性。结果表明，TLP的准确性随着工单接近完成阶段而提高，且不同特征家族在不同阶段的预测能力不同。


<details>
  <summary>Details</summary>
Motivation: 当前缺陷预测方法主要关注已存在缺陷的代码片段，支持修复而非预防。本文旨在通过TLP方法，在工单实现前预测其可能引入的缺陷，从而优化测试资源分配。

Method: TLP方法利用72个特征（分为6个家族），结合滑动窗口和三种机器学习分类器，在两个Apache开源项目的约10,000个工单上进行评估。

Result: TLP的准确性随着工单接近完成阶段而提高，且不同特征家族在不同阶段表现不同：早期以开发者相关特征为主，后期以代码和JIT指标为主，温度特征全程补充。

Conclusion: TLP将缺陷预测提前到工单阶段，为风险感知的工单分类和开发者分配提供了机会，扩展了现有缺陷预测的研究范围。

Abstract: The primary goal of bug prediction is to optimize testing efforts by focusing
on software fragments, i.e., classes, methods, commits (JIT), or lines of code,
most likely to be buggy. However, these predicted fragments already contain
bugs. Thus, the current bug prediction approaches support fixing rather than
prevention. The aim of this paper is to introduce and evaluate Ticket-Level
Prediction (TLP), an approach to identify tickets that will introduce bugs once
implemented. We analyze TLP at three temporal points, each point represents a
ticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1)
TLP accuracy increases as tickets progress towards the closed stage due to
improved feature reliability over time, and (2) the predictive power of
features changes across these temporal points. Our TLP approach leverages 72
features belonging to six different families: code, developer, external
temperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our
TLP evaluation uses a sliding-window approach, balancing feature selection and
three machine-learning bug prediction classifiers on about 10,000 tickets of
two Apache open-source projects. Our results show that TLP accuracy increases
with proximity, confirming the expected trade-off between early prediction and
accuracy. Regarding the prediction power of feature families, no single feature
family dominates across stages; developer-centric signals are most informative
early, whereas code and JIT metrics prevail near closure, and temperature-based
features provide complementary value throughout. Our findings complement and
extend the literature on bug prediction at the class, method, or commit level
by showing that defect prediction can be effectively moved upstream, offering
opportunities for risk-aware ticket triaging and developer assignment before
any code is written.

</details>


### [31] [Quality Assessment of Python Tests Generated by Large Language Models](https://arxiv.org/abs/2506.14297)
*Victor Alves,Carla Bezerra,Ivan Machado,Larissa Rocha,Tássio Virgínio,Publio Silva*

Main category: cs.SE

TL;DR: 研究评估了GPT-4o、Amazon Q和LLama 3.3三种大语言模型生成的Python测试代码质量，发现大多数测试套件存在错误或测试异味，提示上下文对质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 手动生成测试脚本耗时且易错，大语言模型（LLMs）能高效生成测试代码，但其质量需验证。

Method: 评估三种LLMs在两种提示上下文（T2C和Code2Code）下生成的测试代码，分析错误和测试异味。

Result: 大多数测试套件含错误或异味，GPT-4o表现最佳，Amazon Q错误率最高。断言错误最常见，测试异味与设计模式相关。

Conclusion: LLMs生成测试代码质量有待提升，未来需优化生成场景和提示策略。

Abstract: The manual generation of test scripts is a time-intensive, costly, and
error-prone process, indicating the value of automated solutions. Large
Language Models (LLMs) have shown great promise in this domain, leveraging
their extensive knowledge to produce test code more efficiently. This study
investigates the quality of Python test code generated by three LLMs: GPT-4o,
Amazon Q, and LLama 3.3. We evaluate the structural reliability of test suites
generated under two distinct prompt contexts: Text2Code (T2C) and Code2Code
(C2C). Our analysis includes the identification of errors and test smells, with
a focus on correlating these issues to inadequate design patterns. Our findings
reveal that most test suites generated by the LLMs contained at least one error
or test smell. Assertion errors were the most common, comprising 64% of all
identified errors, while the test smell Lack of Cohesion of Test Cases was the
most frequently detected (41%). Prompt context significantly influenced test
quality; textual prompts with detailed instructions often yielded tests with
fewer errors but a higher incidence of test smells. Among the evaluated LLMs,
GPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C),
whereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For
test smells, Amazon Q had fewer detections in the C2C context (9%), while LLama
3.3 performed best in the T2C context (10%). Additionally, we observed a strong
relationship between specific errors, such as assertion or indentation issues,
and test case cohesion smells. These findings demonstrate opportunities for
improving the quality of test generation by LLMs and highlight the need for
future research to explore optimized generation scenarios and better prompt
engineering strategies.

</details>


### [32] [Agile and Student-Centred Teaching of Agile/Scrum Concepts](https://arxiv.org/abs/2506.14369)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 论文讨论了设计和教授一门以敏捷/Scrum开发和需求工程为核心的软件工程项目管理课程的经验，重点介绍了2020年后的教学改革，包括学生为中心的教学方式和真实评估的应用。


<details>
  <summary>Details</summary>
Motivation: 由于疫情后许多大学取消了面对面考试，真实评估的重要性提升，但大规模学生群体的评估和反馈面临挑战。

Method: 课程改革采用学生为中心的教学方式，结合真实评估，探讨了敏捷/Scrum教学的有效结构和在线评估的可行性。

Result: 总结了教学经验，提出了支持混合学习的课程结构和在线评估的合理设计。

Conclusion: 论文为教授敏捷/Scrum概念提供了实用见解，并探讨了大规模学生群体下真实评估的可行方案。

Abstract: In this paper, we discuss our experience in designing and teaching a course
on Software Engineering Project Management, where the focus is on Agile/Scrum
development and Requirement Engineering activities. The course has undergone
fundamental changes since 2020 to make the teaching approach more
student-centred and flexible. As many universities abandoned having
face-to-face exams at the end of the semester, authentic assessments now play
an even more important role than before. This makes assessment of students'
work even more challenging, especially if we are dealing with large cohorts of
students. The complexity is not only in dealing with diversity in the student
cohorts when elaborating the assessment tasks, but also in being able to
provide feedback and marks in a timely and fairly. We report our lessons
learned, which might provide useful insights for teaching Agile/Scrum concepts
to undergraduate and postgraduate students. We also analyse what course
structure might be effective to support a blended learning approach, as well as
what could be a reasonable structure of online assessments, to keep them both
authentic and scalable for large cohorts of students.

</details>


### [33] [Defining the Game Producer: A Mapping of Key Characteristics and Differentiators of the Professional Behind Digital Game Production](https://arxiv.org/abs/2506.14409)
*Rafael C. Lopes,Danilo M. Ribeiro*

Main category: cs.SE

TL;DR: 研究通过定性方法分析了游戏制作人的核心特征、技能与能力，提出了一个结构化模型。


<details>
  <summary>Details</summary>
Motivation: 随着数字游戏复杂度的提升，游戏制作人在协调创意、技术和商业方面的作用日益重要。

Method: 采用11次半结构化访谈，通过扎根理论分析数据。

Result: 研究发现沟通、适应性和项目管理是游戏制作人的核心能力。

Conclusion: 研究结果为游戏制作人的专业培训、招聘策略及未来研究提供了基础。

Abstract: Introduction: As digital games grow in complexity, the role of the Game
Producer becomes increasingly relevant for aligning creative, technical, and
business dimensions. Objective: This study aimed to identify and map the main
characteristics, skills, and competencies that define the Digital Game Producer
profile. Methodology: A qualitative investigation was conducted with 11
semi-structured interviews, analyzed through Grounded Theory to build
categories grounded in professional practice. Results: The study produced a
structured set of personal characteristics, practical skills, and strategic
competencies considered essential for Game Producers. Communication,
adaptability, and project management emerged as central elements across the
sample. Conclusion: The resulting model offers a foundation for professional
training, recruitment strategies, and future research on leadership roles in
game development.

</details>


### [34] [Automatic Qiskit Code Refactoring Using Large Language Models](https://arxiv.org/abs/2506.14535)
*José Manuel Suárez,Luis Mariano Bibbó,Joaquin Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 提出了一种利用大语言模型（LLM）重构Qiskit代码的新方法，通过提取迁移场景分类法并结合LLM，有效解决了量子软件框架API快速变化带来的兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 量子软件框架API快速变化导致开发者面临兼容性维护的挑战，需要一种自动化解决方案。

Method: 从Qiskit官方文档中提取迁移场景分类法，结合LLM识别代码中的迁移场景并建议重构方案，优化输入和推理过程以应对LLM的上下文长度限制。

Result: 实验证明，结合领域知识的LLM能有效自动化Qiskit代码迁移，并提供了从旧版本迁移到0.46版的分类法和提示词。

Conclusion: 该方法不仅为Qiskit代码迁移提供了实用工具，还为评估LLM在量子代码迁移中的能力提供了方法论。

Abstract: As quantum software frameworks evolve, developers face increasing challenges
in maintaining compatibility with rapidly changing APIs. In this work, we
present a novel methodology for refactoring Qiskit code using large language
models (LLMs). We begin by extracting a taxonomy of migration scenarios from
the different sources of official Qiskit documentation (such as release notes),
capturing common patterns such as migration of functionality to different
modules and deprecated usage. This taxonomy, along with the original Python
source code, is provided as input to an LLM, which is then tasked with
identifying instances of migration scenarios in the code and suggesting
appropriate refactoring solutions. Our approach is designed to address the
context length limitations of current LLMs by structuring the input and
reasoning process in a targeted, efficient manner. The results demonstrate that
LLMs, when guided by domain-specific migration knowledge, can effectively
assist in automating Qiskit code migration. This work contributes both a set of
proven prompts and taxonomy for Qiskit code migration from earlier versions to
version 0.46 and a methodology to asses the capabilities of LLMs to assist in
the migration of quantum code.

</details>


### [35] [Low-code to fight climate change: the Climaborough project](https://arxiv.org/abs/2506.14623)
*Aaron Conrardy,Armen Sulejmani,Cindy Guerlain,Daniele Pagani,David Hick,Matteo Satta,Jordi Cabot*

Main category: cs.SE

TL;DR: Climaborough项目通过低代码/无代码策略快速开发气候仪表盘，帮助欧洲城市实现2030年碳中和目标。


<details>
  <summary>Details</summary>
Motivation: 支持欧洲城市通过实时监测和评估本地气候倡议的进展，加速碳中和目标的实现。

Method: 采用低代码策略加速仪表盘开发，并嵌入无代码哲学，使非技术用户也能自定义仪表盘。

Result: 开发了Climaborough城市平台，聚合历史与实时数据，提供用户友好的仪表盘。

Conclusion: 低代码/无代码策略有效支持了气候仪表盘的快速部署和用户自定义需求。

Abstract: The EU-funded Climaborough project supports European cities to achieve carbon
neutrality by 2030. Eleven cities in nine countries will deploy in real
conditions products and services fostering climate transition in their local
environment. The Climaborough City Platform is being developed to monitor the
cities' overall progress towards their climate goals by aggregating historic
and real-time data and displaying the results in user-friendly dashboards that
will be used by non-technical experts to evaluate the effectiveness of local
experimental initiatives, identify those that yield significant impact, and
assess the potential consequences of scaling them up to a broader level. In
this paper, we explain how we have put in place a low-code/no-code strategy in
Climaborough in response to the project's aim to quickly deploy climate
dashboards. A low-code strategy is used to accelerate the development of the
dashboards. The dashboards embed a no-code philosophy that enables all types of
citizen profiles to configure and adapt the dashboard to their specific needs.

</details>


### [36] [ACM Survey Draft on Formalising Software Requirements with Large Language Models](https://arxiv.org/abs/2506.14627)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文档总结了94篇论文，并包含软件需求可追溯性、形式化方法及其工具、统一编程理论和机构理论等额外章节。与类似标题的近期论文（AACS 2025和SAIV 2025）相比，本文档是工作草案。


<details>
  <summary>Details</summary>
Motivation: 提供对94篇论文的总结，并扩展讨论软件需求可追溯性、形式化方法及其工具、统一编程理论和机构理论等内容。

Method: 总结和分类94篇论文，并添加额外章节以扩展讨论。

Result: 生成了一份工作草案，包含论文总结和额外章节。

Conclusion: 本文档为研究提供了全面的参考，并突出了与AACS 2025和SAIV 2025的关键区别。

Abstract: This draft is a working document, having a summary of nighty-four (94) papers
with additional sections on Traceability of Software Requirements (Section 4),
Formal Methods and Its Tools (Section 5), Unifying Theories of Programming
(UTP) and Theory of Institutions (Section 6). Please refer to abstract of
[7,8]. Key difference of this draft from our recently anticipated ones with
similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted
on 18th of March, 2025, it went through the light-weight blind review and
accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary
tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April,
2025. It went through rigorous review process. The uploaded version on
arXiv.org [8] is the improved one of the submission, after addressing the
specific suggestions to improve the paper.

</details>


### [37] [Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey](https://arxiv.org/abs/2506.14640)
*Ina K. Schieferdecker*

Main category: cs.SE

TL;DR: 本文综述了AI在软件测试自动化中的增强作用，提出了新的分类法ai4st，并探讨了AI带来的新测试形式。


<details>
  <summary>Details</summary>
Motivation: 随着AI在工程领域的突破，软件测试领域也迎来了新的机遇，尤其是在测试自动化方面。本文旨在探讨AI如何从无自动化到全自动化增强软件测试。

Method: 通过综述近期关于AI在软件测试自动化中的研究，提出新的分类法ai4st，并对现有研究进行分类和开放问题的识别。

Result: 提出了ai4st分类法，用于分类近期研究并识别开放问题，同时探讨了AI带来的新测试形式。

Conclusion: AI为软件测试自动化提供了新的视角和可能性，ai4st分类法为未来研究提供了框架和方向。

Abstract: In industry, software testing is the primary method to verify and validate
the functionality, performance, security, usability, and so on, of
software-based systems. Test automation has gained increasing attention in
industry over the last decade, following decades of intense research into test
automation and model-based testing. However, designing, developing, maintaining
and evolving test automation is a considerable effort. Meanwhile, AI's
breakthroughs in many engineering fields are opening up new perspectives for
software testing, for both manual and automated testing. This paper reviews
recent research on AI augmentation in software test automation, from no
automation to full automation. It also discusses new forms of testing made
possible by AI. Based on this, the newly developed taxonomy, ai4st, is
presented and used to classify recent research and identify open research
questions.

</details>


### [38] [Issue Retrieval and Verification Enhanced Supplementary Code Comment Generation](https://arxiv.org/abs/2506.14649)
*Yanzhen Zou,Xianlin Zhao,Xinglu Pan,Bing Xie*

Main category: cs.SE

TL;DR: IsComment是一种基于问题报告的LLM检索和验证方法，用于生成代码补充注释，减少幻觉并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 问题报告包含丰富信息，但如何减少生成注释中的幻觉仍具挑战性。

Method: 通过分析代码-注释-问题，识别五种补充信息类型，检索相关句子生成候选注释，并过滤不相关或不可验证的内容。

Result: 实验表明，IsComment显著提高了ChatGPT、GPT-4o和DeepSeek-V3的覆盖率，生成更丰富有用的注释。

Conclusion: IsComment能生成更可靠的补充代码注释，提升编程理解能力。

Abstract: Issue reports have been recognized to contain rich information for
retrieval-augmented code comment generation. However, how to minimize
hallucinations in the generated comments remains significant challenges. In
this paper, we propose IsComment, an issue-based LLM retrieval and verification
approach for generating method's design rationale, usage directives, and so on
as supplementary code comments. We first identify five main types of code
supplementary information that issue reports can provide through
code-comment-issue analysis. Next, we retrieve issue sentences containing these
types of supplementary information and generate candidate code comments. To
reduce hallucinations, we filter out those candidate comments that are
irrelevant to the code or unverifiable by the issue report, making the code
comment generation results more reliable. Our experiments indicate that
compared with LLMs, IsComment increases the coverage of manual supplementary
comments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and
from 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can
generate richer and more useful supplementary code comments for programming
understanding, which is quantitatively evaluated through the MESIA metric on
both methods with and without manual code comments.

</details>


### [39] [Unified Software Engineering agent as AI Software Engineer](https://arxiv.org/abs/2506.14683)
*Leonhard Applis,Yuntong Zhang,Shanchao Liang,Nan Jiang,Lin Tan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 论文探讨了LLM代理是否能成为AI软件工程师，提出了一个统一的软件工程代理USEagent，并评估其性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理在软件工程中的潜力，尤其是能否处理复杂的开发任务，如维护和项目演进。

Method: 开发了USEagent，一个能协调多种能力的统一代理，并使用USEbench（包含多种任务）进行评估。

Result: USEagent在1,271个任务中表现优于现有通用代理，但在某些编码任务上仍有不足。

Conclusion: USEagent是未来AI软件工程师的雏形，但仍需进一步改进。

Abstract: The growth of Large Language Model (LLM) technology has raised expectations
for automated coding. However, software engineering is more than coding and is
concerned with activities including maintenance and evolution of a project. In
this context, the concept of LLM agents has gained traction, which utilize LLMs
as reasoning engines to invoke external tools autonomously. But is an LLM agent
the same as an AI software engineer? In this paper, we seek to understand this
question by developing a Unified Software Engineering agent or USEagent. Unlike
existing work which builds specialized agents for specific software tasks such
as testing, debugging, and repair, our goal is to build a unified agent which
can orchestrate and handle multiple capabilities. This gives the agent the
promise of handling complex scenarios in software development such as fixing an
incomplete patch, adding new features, or taking over code written by others.
We envision USEagent as the first draft of a future AI Software Engineer which
can be a team member in future software development teams involving both AI and
humans. To evaluate the efficacy of USEagent, we build a Unified Software
Engineering bench (USEbench) comprising of myriad tasks such as coding,
testing, and patching. USEbench is a judicious mixture of tasks from existing
benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on
USEbench consisting of 1,271 repository-level software engineering tasks,
USEagent shows improved efficacy compared to existing general agents such as
OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for
certain coding tasks, which provides hints on further developing the AI
Software Engineer of the future.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [40] [The Trip to ZigBee Backscatter across a Decade, a Systematic Review](https://arxiv.org/abs/2506.13822)
*Yang Liu*

Main category: cs.NI

TL;DR: 本文回顾了反向散射通信技术的演变，从RFID发展到支持无电池物联网的复杂系统，重点分析了其与ZigBee等协议的交互，并总结了关键技术进步、性能权衡及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索反向散射通信从简单反射技术到复杂无电池物联网技术的演变，分析其与现有无线协议的交互，以推动未来技术发展。

Method: 系统回顾过去十年的研究进展，分析关键技术进步（如调制、同步、跨技术通信）和性能权衡。

Result: 总结了现代反向散射系统的高吞吐量、并发性和跨技术通信能力，并指出了性能与兼容性之间的核心权衡。

Conclusion: 未来研究方向包括网络可扩展性、安全性、与RIS和6G的集成，以进一步扩展反向散射通信的能力。

Abstract: The field of backscatter communication has undergone a profound
transformation, evolving from a niche technology for radio-frequency
identification (RFID) into a sophisticated paradigm poised to enable a truly
battery-free Internet of Things (IoT). This evolution is built upon a deepening
understanding of the fundamental principles governing these ultra-low-power
links. Modern backscatter systems are no longer simple reflectors of continuous
waves but are increasingly designed to interact with complex, data-carrying
ambient signals from ubiquitous sources like WiFi, ZigBee, and cellular
networks. This review systematically charts the journey of ambient backscatter,
particularly focusing on its interaction with ZigBee and other commodity
wireless protocols over the last decade. We analyze the progression from
foundational proof-of-concept systems that established productive backscatter
to modern high-throughput, concurrent, and cross-technology communication
architectures. Key advancements in fine-grained modulation, robust
synchronization, cross-technology physical layer emulation, and multi-tag
coordination are detailed. A comparative analysis of state-of-the-art systems
highlights the core trade-offs between performance metrics like data rate and
range, power consumption, and compatibility with commodity hardware. Finally,
we synthesize the primary challenges, including networking scalability,
security vulnerabilities, the near-far problem, and practical deployment
hurdles, and outline future research directions, such as integration with
Reconfigurable Intelligent Surfaces (RIS) and 6G networks, that promise to
further expand the capabilities of this transformative technology.

</details>


### [41] [Emerging Networks and Services in Developing Nations -- Barbados Use Case](https://arxiv.org/abs/2506.13934)
*Warren Scantlebury,Milena Radenkovic*

Main category: cs.NI

TL;DR: 比较巴巴多斯与英国诺丁汉、伦敦的DTN性能，分析差异原因，并探讨DTN在交通领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 填补发展中国家DTN研究空白，为交通效率问题提供解决方案。

Method: 网络模拟，记录并分析DTN性能指标，重点关注公交系统。

Result: 展示图形化趋势，分析差异原因及其实际应用。

Conclusion: 为发展中国家DTN研究提供参考，验证DTN在交通领域的可行性。

Abstract: This report aims to conduct an in-depth comparison of DTN
(Delay/Disconnection Tolerant Network) performance and characteristics in the
developing country of Barbados versus two major UK cities Nottingham and
London. We aim to detect any common patterns or deviations between the two
region areas and use the results of our network simulations to draw
well-founded conclusions on the reasons for these similarities and differences.
In the end we hope to be able to assimilate specific portions of the island to
these major cities in regard to DTN characteristics. We also want to
investigate the viability of DTN use in the transport sector which has
struggled from a range of issues related to efficiency and finance, by
recording and analysing the same metrics for a DTN that consists of only buses.
This work is intended to serve as a bridge for expanding the breadth of
research done on developed countries allowing other researchers to be able to
make well informed assumptions about how that research may apply to developing
nations. It will consist of results that show graphical trends and analysis of
why these trends might exist and how they apply to real world scenarios.

</details>


### [42] [TraGe: A Generic Packet Representation for Traffic Classification Based on Header-Payload Differences](https://arxiv.org/abs/2506.14151)
*Chungang Lin,Yilong Jiang,Weiyao Zhang,Xuying Meng,Tianyu Zuo,Yujun Zhang*

Main category: cs.NI

TL;DR: TraGe是一种新型的通用数据包表示模型，通过差异化的预训练策略优化流量分类任务，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统流量分类方法依赖特征提取和大规模标注数据，现有预训练模型未针对流量数据优化，导致信息丢失或协议信息破坏。

Method: 基于数据包头和负载的差异，采用差异化预训练策略，并引入动态掩码防止过拟合。

Result: TraGe在两项流量分类任务中性能提升高达6.97%，且对参数波动和采样配置变化表现出更强的鲁棒性。

Conclusion: TraGe通过优化预训练策略，显著提升了流量分类的性能和鲁棒性。

Abstract: Traffic classification has a significant impact on maintaining the Quality of
Service (QoS) of the network. Since traditional methods heavily rely on feature
extraction and large scale labeled data, some recent pre-trained models manage
to reduce the dependency by utilizing different pre-training tasks to train
generic representations for network packets. However, existing pre-trained
models typically adopt pre-training tasks developed for image or text data,
which are not tailored to traffic data. As a result, the obtained traffic
representations fail to fully reflect the information contained in the traffic,
and may even disrupt the protocol information. To address this, we propose
TraGe, a novel generic packet representation model for traffic classification.
Based on the differences between the header and payload-the two fundamental
components of a network packet-we perform differentiated pre-training according
to the byte sequence variations (continuous in the header vs. discontinuous in
the payload). A dynamic masking strategy is further introduced to prevent
overfitting to fixed byte positions. Once the generic packet representation is
obtained, TraGe can be finetuned for diverse traffic classification tasks using
limited labeled data. Experimental results demonstrate that TraGe significantly
outperforms state-of-the-art methods on two traffic classification tasks, with
up to a 6.97% performance improvement. Moreover, TraGe exhibits superior
robustness under parameter fluctuations and variations in sampling
configurations.

</details>


### [43] [Optimizing System Latency for Blockchain-Encrypted Edge Computing in Internet of Vehicles](https://arxiv.org/abs/2506.14208)
*Cui Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Qiang Fan*

Main category: cs.NI

TL;DR: 论文提出了一种结合区块链Raft共识机制与边缘计算的安全框架，以保护车联网中的信息安全，并通过优化降低系统延迟。


<details>
  <summary>Details</summary>
Motivation: 随着车联网技术的发展，边缘计算成为处理复杂任务的重要工具，但任务卸载可能暴露车辆信息，导致安全漏洞。区块链技术可以提升系统的安全性和鲁棒性。

Method: 结合区块链的Raft共识机制与边缘计算，提出安全框架；推导系统延迟理论公式，提出凸优化解决方案以最小化延迟。

Result: 仿真结果表明，优化的数据提取率显著降低系统延迟，且延迟变化相对稳定。

Conclusion: 该优化方案为未来网络环境（如5G和下一代智慧城市系统）的安全与效率提升提供了有价值的参考。

Abstract: As Internet of Vehicles (IoV) technology continues to advance, edge computing
has become an important tool for assisting vehicles in handling complex tasks.
However, the process of offloading tasks to edge servers may expose vehicles to
malicious external attacks, resulting in information loss or even tampering,
thereby creating serious security vulnerabilities. Blockchain technology can
maintain a shared ledger among servers. In the Raft consensus mechanism, as
long as more than half of the nodes remain operational, the system will not
collapse, effectively maintaining the system's robustness and security. To
protect vehicle information, we propose a security framework that integrates
the Raft consensus mechanism from blockchain technology with edge computing. To
address the additional latency introduced by blockchain, we derived a
theoretical formula for system delay and proposed a convex optimization
solution to minimize the system latency, ensuring that the system meets the
requirements for low latency and high reliability. Simulation results
demonstrate that the optimized data extraction rate significantly reduces
system delay, with relatively stable variations in latency. Moreover, the
proposed optimization solution based on this model can provide valuable
insights for enhancing security and efficiency in future network environments,
such as 5G and next-generation smart city systems.

</details>


### [44] [A Novel Dynamic Bandwidth Allocation Design for 100G Coherent Passive Optical Network](https://arxiv.org/abs/2506.14221)
*Rujia Zou,Haipeng Zhang,Karthik Sundaresan,Zhensheng Jia,Suresh Subramaniam*

Main category: cs.NI

TL;DR: 本文针对100G及以上速率的相干PON技术，提出了一种新型DBA算法（Hybrid-Switch DBA），以解决TDM和TFDM PON网络中的时域错位问题，优化带宽分配效率。


<details>
  <summary>Details</summary>
Motivation: 随着100G及以上相干PON技术的快速发展，对低延迟带宽管理和高效DBA机制的需求日益迫切，尤其是在时域错位问题上的挑战。

Method: 分析两种现有DBA的时域错位问题，并提出一种新型Hybrid-Switch DBA算法，支持100Gbps速率和512个终端用户，能根据实时流量自适应切换DBA方案。

Result: Hybrid-Switch DBA算法显著提升了相干TDM和TFDM PON网络的带宽分配效率，解决了DBA机制中的时域错位问题。

Conclusion: Hybrid-Switch DBA算法为相干PON网络提供了一种优化性能和适应低延迟应用的解决方案，推动了相干PON技术的发展。

Abstract: With the rapid advancements in coherent Passive Optical Network (PON)
technologies featuring 100G and higher data rates, this paper addresses the
urgent requirement for sophisticated simulation and MAC layer development
within the domain of coherent Time Division Multiplexing (TDM) PON and coherent
Time and Frequency Division Multiplexing (TFDM) PON networks. The ever-growing
demand for latency-sensitive services and expanding user populations in
next-generation 100G and beyond coherent PONs, underscores the crucial need for
low-latency bandwidth management and efficient Dynamic Bandwidth Allocation
(DBA) mechanisms. In this paper, we present a pioneering analysis of two
established DBAs from the perspective of temporal misalignments. Subsequently,
a novel DBA algorithm tailored for coherent PONs featuring 100 Gbps data rate
and up to 512 end-users is introduced, named the Hybrid-Switch DBA. This
innovative approach allows for adaptive switching of the DBA scheme in response
to real-time traffic conditions. To the best of our knowledge, this paper
represents the first attempt to address the misalignment problem of DBA and
proposes a novel DBA solution for both TDM- and TFDM-based coherent PON
networks. This research significantly contributes to the development of
coherent TDM PON and coherent TFDM PON networks by enhancing the efficiency of
bandwidth allocation and addressing the challenges associated with
misalignments in DBA mechanisms. As optical access networks continue to evolve
to meet the ever-increasing demands of modern communication services, the
Hybrid-Switch DBA algorithm presented in this paper offers a promising solution
for optimizing network performance and accommodating latency-sensitive
applications.

</details>


### [45] [Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G](https://arxiv.org/abs/2505.22963)
*Zhuoran Duan,Guoshun Nan,Rushan Li,Zijun Wang,Lihua Xiong,Chaoying Yuan,Guorong Liu,Hui Xu,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 论文提出了一种名为ES3A的新型6G网络安全架构，旨在解决6G网络中的异构性和多样化安全需求带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络的安全性和韧性是设计重点，但面临异构网络和多样化安全需求的挑战，需要重新设计安全架构。

Method: 提出了ES3A架构，基于六项高级原则和三条部署指南，包括三层三域结构和两阶段编排机制。

Result: 在基于软件定义无线电（SDR）的真实系统上原型验证了ES3A的有效性，并通过案例展示了其优越性。

Conclusion: ES3A为6G网络提供了服务化安全、端到端保护和智能安全自动化，能够应对动态网络中的安全挑战。

Abstract: The upcoming 6G will fundamentally reshape mobile networks beyond
communications, unlocking a multitude of applications that were once considered
unimaginable. Meanwhile, security and resilience are especially highlighted in
the 6G design principles. However, safeguarding 6G networks will be quite
challenging due to various known and unknown threats from highly heterogeneous
networks and diversified security requirements of distinct use cases, calling
for a comprehensive re-design of security architecture. This motivates us to
propose ES3A (Entire Smart Service-based Security Architecture), a novel
security architecture for 6G networks. Specifically, we first discuss six
high-level principles of our ES3A that include hierarchy, flexibility,
scalability, resilience, endogeny, and trust and privacy. With these goals in
mind, we then introduce three guidelines from a deployment perspective,
envisioning our ES3A that offers service-based security, end-to-end protection,
and smart security automation for 6G networks. Our architecture consists of
three layers and three domains. It relies on a two-stage orchestration
mechanism to tailor smart security strategies for customized protection in
high-dynamic 6G networks, thereby addressing the aforementioned challenges.
Finally, we prototype the proposed ES3A on a real-world radio system based on
Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A.
We also provide a case to show the superiority of our architecture.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee,Dongkyu Kim,Youngcheon You,Youngmin Kim*

Main category: cs.LG

TL;DR: LittleBit是一种新颖的极端LLM压缩方法，通过低秩矩阵分解和二值化实现0.1 BPW的压缩，显著减少内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）部署时面临高内存和计算成本问题，现有量化方法在低于1比特的精度下性能下降严重。

Method: 采用低秩矩阵分解和二值化权重，结合多尺度补偿机制（行、列和潜在维度），并提出Dual-SVID和残差补偿技术以优化训练。

Result: 在0.1 BPW下，Llama2-7B性能优于现有方法的0.7 BPW，内存减少31倍，潜在速度提升5倍。

Conclusion: LittleBit为资源受限环境部署高效LLM提供了新途径。

Abstract: Deploying large language models (LLMs) often faces challenges from
substantial memory and computational costs. Quantization offers a solution, yet
performance degradation in the sub-1-bit regime remains particularly difficult.
This paper introduces LittleBit, a novel method for extreme LLM compression. It
targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$
memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents
weights in a low-rank form using latent matrix factorization, subsequently
binarizing these factors. To counteract information loss from this extreme
precision, it integrates a multi-scale compensation mechanism. This includes
row, column, and an additional latent dimension that learns per-rank
importance. Two key contributions enable effective training: Dual
Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware
training (QAT) initialization, and integrated Residual Compensation to mitigate
errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit
quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading
method's 0.7 BPW. This establishes a superior size-performance trade-off, with
kernel-level benchmarks indicating potential for a 5$\times$ speedup compared
to FP16. LittleBit paves the way for deploying powerful LLMs in
resource-constrained environments.

</details>


### [47] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu,Daliang Xu,Dongqi Cai,Zexi Li,Wei Liu,Fangming Liu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: MobiEdit是一个移动知识编辑框架，首次在商用移动设备上实现高效LLM个性化，通过量化前向梯度估计替代资源密集的反向传播。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在移动设备上处理个性化查询时的幻觉问题，同时避免传统知识编辑方法因反向传播导致的资源消耗。

Method: 采用量化前向梯度估计，兼容移动NPU，并引入自适应提前终止和前缀缓存优化。

Result: 在3B参数模型上实现实时编辑，内存减少7.6倍，能耗降低14.7倍，延迟减少3.6倍。

Conclusion: MobiEdit为移动设备上的LLM知识编辑提供了高效可行的解决方案。

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer
applications such as intelligent assistants. LLMs pre-trained on general
corpora often hallucinate when handling personalized or unseen queries, leading
to incorrect or outdated responses. Knowledge editing addresses this by
identifying and adjusting a small crucial portion of model weights, without
compromising the general knowledge. However, prior knowledge editing methods
are impractical to run on local devices due to the resource-heavy
backpropagation (BP) needed for updates. We present MobiEdit, the first mobile
knowledge editing framework that enables efficient LLM personalization on
commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces
full-precision BP with quantized forward-only gradient estimation, thus
compatible with the energy-efficient mobile neural processing units (NPUs).
MobiEdit replaces full-precision backpropagation with quantized forward-only
gradient estimation, making it compatible with energy-efficient mobile NPUs. To
further improve gradient estimation efficiency, we introduce two optimizations:
an early stoping mechanism that adaptively terminates editing upon success and
a prefix cache that reuses computation across steps. Our approach enables
real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile
devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$
less latency compared to previous knowledge editing methods.

</details>


### [48] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández*

Main category: cs.LG

TL;DR: JobShopLib是一个模块化库，用于定制化解决作业车间调度问题的深度学习模型，通过模仿学习训练调度器，展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 传统作业车间调度方法依赖简单启发式规则，而深度学习模型（如图神经网络）虽具潜力，但缺乏模块化工具支持实验。

Method: 开发JobShopLib模块化库，支持定制图表示、节点特征等，并通过模仿学习训练调度器。

Result: 一个模型仅使用操作特征即优于多种基于图的调度器，GNN模型在大规模问题上接近最先进水平。

Conclusion: JobShopLib为未来实验提供了必要工具，表明此类模型仍有显著改进空间。

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization
problem relevant to manufacturing and timetabling. Traditional approaches use
priority dispatching rules based on simple heuristics. Recent work has
attempted to replace these with deep learning models, particularly graph neural
networks (GNNs), that learn to assign priorities from data. However, training
such models requires customizing numerous factors: graph representation, node
features, action space, and reward functions. The lack of modular libraries for
experimentation makes this research time-consuming. This work introduces
JobShopLib, a modular library that allows customizing these factors and
creating new components with its reinforcement learning environment. We trained
several dispatchers through imitation learning to demonstrate the environment's
utility. One model outperformed various graph-based dispatchers using only
individual operation features, highlighting the importance of feature
customization. Our GNN model achieved near state-of-the-art results on
large-scale problems. These results suggest significant room for improvement in
developing such models. JobShopLib provides the necessary tools for future
experimentation.

</details>


### [49] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo,Tran Quang Vinh,Patricia Kearney,Mark Roantree*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的装袋集成回归模型（EBMBag+），用于预测美国各城市的糖尿病患病率，并在多个指标上表现最优。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是一种慢性代谢疾病，准确预测其患病率对医疗规划和干预至关重要，但现有数据往往不完整。

Method: 通过数据工程整合2011-2021年的糖尿病相关数据集，并开发EBMBag+模型进行时间序列预测。

Result: EBMBag+在MAE、RMSE、MAPE和R2等指标上均优于基线模型，表现最佳。

Conclusion: EBMBag+模型在糖尿病患病率预测中具有显著优势，可用于支持精准医疗决策。

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood
glucose levels, leading to complications like heart disease, kidney failure,
and nerve damage. Accurate state-level predictions are vital for effective
healthcare planning and targeted interventions, but in many cases, data for
necessary analyses are incomplete. This study begins with a data engineering
process to integrate diabetes-related datasets from 2011 to 2021 to create a
comprehensive feature set. We then introduce an enhanced bagging ensemble
regression model (EBMBag+) for time series forecasting to predict diabetes
prevalence across U.S. cities. Several baseline models, including SVMReg,
BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our
EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved
the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an
R2 of 0.9.

</details>


### [50] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

Main category: cs.LG

TL;DR: 提出了一种混合元学习框架，用于非线性动态系统中的预测和异常检测，结合了物理模拟器和深度学习模型，提升了异常定位和预测性能。


<details>
  <summary>Details</summary>
Motivation: 针对非线性动态系统中非平稳和随机行为的预测和异常检测问题，传统方法可能无法完全捕捉复杂动态，需要一种更通用的数据驱动方法。

Method: 整合物理模拟器生成数据，使用CNN-LSTM提取时空特征，VAE进行无监督异常评分，DA-RNN进行一步预测，并通过元学习器结合多模型输出。

Result: 混合模型在异常定位、泛化能力和非线性偏差鲁棒性上优于独立模型。

Conclusion: 该框架为非线性系统提供了通用的早期缺陷识别和预测监控方法，适用于物理模型不完全可用的场景。

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly
detection in nonlinear dynamical systems characterized by nonstationary and
stochastic behavior. The approach integrates a physics-inspired simulator that
captures nonlinear growth-relaxation dynamics with random perturbations,
representative of many complex physical, industrial, and cyber-physical
systems. We use CNN-LSTM architectures for spatio-temporal feature extraction,
Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation
Forests for residual-based outlier detection in addition to a Dual-Stage
Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of
the generated simulation data. To create composite anomaly forecasts, these
models are combined using a meta-learner that combines forecasting outputs,
reconstruction errors, and residual scores. The hybrid ensemble performs better
than standalone models in anomaly localization, generalization, and robustness
to nonlinear deviations, according to simulation-based experiments. The
framework provides a broad, data-driven approach to early defect identification
and predictive monitoring in nonlinear systems, which may be applied to a
variety of scenarios where complete physical models might not be accessible.

</details>


### [51] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao,Chenghui Li,Frederic Sala,Karl Rohe*

Main category: cs.LG

TL;DR: 论文提出了一种基于假设检验的概念分解方法，用于解释CLIP嵌入空间中的概念，并验证其统计稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前概念解释方法缺乏统计严谨性，难以验证概念的有效性和比较不同技术。

Method: 引入假设检验框架量化CLIP嵌入空间中的旋转敏感结构，并提出一种后验概念分解方法。

Result: 方法在重建误差上优于现有技术，并在去除虚假背景概念后，最差组准确率提高了22.6%。

Conclusion: 该方法平衡了重建精度与概念可解释性，有效减少了数据中的虚假线索。

Abstract: Concept-based approaches, which aim to identify human-understandable concepts
within a model's internal representations, are a promising method for
interpreting embeddings from deep neural network models, such as CLIP. While
these approaches help explain model behavior, current methods lack statistical
rigor, making it challenging to validate identified concepts and compare
different techniques. To address this challenge, we introduce a hypothesis
testing framework that quantifies rotation-sensitive structures within the CLIP
embedding space. Once such structures are identified, we propose a post-hoc
concept decomposition method. Unlike existing approaches, it offers theoretical
guarantees that discovered concepts represent robust, reproducible patterns
(rather than method-specific artifacts) and outperforms other techniques in
terms of reconstruction error. Empirically, we demonstrate that our
concept-based decomposition algorithm effectively balances reconstruction
accuracy with concept interpretability and helps mitigate spurious cues in
data. Applied to a popular spurious correlation dataset, our method yields a
22.6% increase in worst-group accuracy after removing spurious background
concepts.

</details>


### [52] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei,Chin Chun Ooi,Abhishek Gupta,Jian Cheng Wong,Pao-Hsiung Chiu,Sheares Xue Wen Toh,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出一种可演化的条件扩散方法，利用黑箱、不可微的多物理模型指导生成过程，支持自主科学发现。


<details>
  <summary>Details</summary>
Motivation: 解决在多物理模型（如计算流体动力学和电磁学）中，传统梯度方法无法直接应用的问题。

Method: 将指导问题转化为优化问题，通过更新去噪分布的描述统计量实现目标函数优化，基于概率演化的原理推导出演化引导方法。

Result: 在流体拓扑和超表面设计中验证了方法的有效性，生成的设计更符合优化目标。

Conclusion: 该方法无需依赖可微代理，为科学领域中的黑箱、不可微模型提供了一种有效的指导扩散手段。

Abstract: This paper presents an evolvable conditional diffusion method such that
black-box, non-differentiable multi-physics models, as are common in domains
like computational fluid dynamics and electromagnetics, can be effectively used
for guiding the generative process to facilitate autonomous scientific
discovery. We formulate the guidance as an optimization problem where one
optimizes for a desired fitness function through updates to the descriptive
statistic for the denoising distribution, and derive an evolution-guided
approach from first principles through the lens of probabilistic evolution.
Interestingly, the final derived update algorithm is analogous to the update as
per common gradient-based guided diffusion models, but without ever having to
compute any derivatives. We validate our proposed evolvable diffusion algorithm
in two AI for Science scenarios: the automated design of fluidic topology and
meta-surface. Results demonstrate that this method effectively generates
designs that better satisfy specific optimization objectives without reliance
on differentiable proxies, providing an effective means of guidance-based
diffusion that can capitalize on the wealth of black-box, non-differentiable
multi-physics numerical models common across Science.

</details>


### [53] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar,Soumik Sarkar,Asheesh K. Singh,Sajal K. Das*

Main category: cs.LG

TL;DR: ReinDSplit是一种基于强化学习的框架，动态调整DNN分割点，优化农业边缘设备的计算效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统SL框架在农业生态系统中因设备异构性导致效率低下和性能下降，需动态适配解决方案。

Method: 采用Q-learning代理动态选择DNN分割点，平衡负载和延迟，确保资源高效利用。

Result: 在三个昆虫分类数据集上，ReinDSplit使用MobileNetV2达到94.31%的准确率。

Conclusion: ReinDSplit通过强化学习优化SL框架，适用于异构环境，具有高效性、隐私性和可扩展性。

Abstract: To empower precision agriculture through distributed machine learning (DML),
split learning (SL) has emerged as a promising paradigm, partitioning deep
neural networks (DNNs) between edge devices and servers to reduce computational
burdens and preserve data privacy. However, conventional SL frameworks'
one-split-fits-all strategy is a critical limitation in agricultural ecosystems
where edge insect monitoring devices exhibit vast heterogeneity in
computational power, energy constraints, and connectivity. This leads to
straggler bottlenecks, inefficient resource utilization, and compromised model
performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement
learning (RL)-driven framework that dynamically tailors DNN split points for
each device, optimizing efficiency without sacrificing accuracy. Specifically,
a Q-learning agent acts as an adaptive orchestrator, balancing workloads and
latency thresholds across devices to mitigate computational starvation or
overload. By framing split layer selection as a finite-state Markov decision
process, ReinDSplit convergence ensures that highly constrained devices
contribute meaningfully to model training over time. Evaluated on three insect
classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit
achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit
pioneers a paradigm shift in SL by harmonizing RL for resource efficiency,
privacy, and scalability in heterogeneous environments.

</details>


### [54] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen,Carlos Lima Azevedo,Tomer Toledo,Filipe Rodrigues*

Main category: cs.LG

TL;DR: T-REX是一个基于SUMO的开源仿真框架，用于在动态交通事件场景下训练和评估强化学习交通信号控制（RL-TSC）方法，研究发现分层协调方法在复杂网络中表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 研究RL-TSC在真实交通事件中的鲁棒性，填补现有研究的空白。

Method: 开发T-REX框架，模拟交通事件中的动态行为，并提出新的鲁棒性评估指标。

Result: 独立值基和分散压力基方法在稳定条件下表现良好，但在事件驱动下性能下降；分层协调方法在大规模网络中更稳定。

Conclusion: RL-TSC研究需关注鲁棒性设计，T-REX为动态场景下的方法评估提供了标准化平台。

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a
promising approach for improving urban mobility. However, its robustness under
real-world disruptions such as traffic incidents remains largely underexplored.
In this study, we introduce T-REX, an open-source, SUMO-based simulation
framework for training and evaluating RL-TSC methods under dynamic, incident
scenarios. T-REX models realistic network-level performance considering
drivers' probabilistic rerouting, speed adaptation, and contextual
lane-changing, enabling the simulation of congestion propagation under
incidents. To assess robustness, we propose a suite of metrics that extend
beyond conventional traffic efficiency measures. Through extensive experiments
across synthetic and real-world networks, we showcase T-REX for the evaluation
of several state-of-the-art RL-TSC methods under multiple real-world deployment
paradigms. Our findings show that while independent value-based and
decentralized pressure-based methods offer fast convergence and generalization
in stable traffic conditions and homogeneous networks, their performance
degrades sharply under incident-driven distribution shifts. In contrast,
hierarchical coordination methods tend to offer more stable and adaptable
performance in large-scale, irregular networks, benefiting from their
structured decision-making architecture. However, this comes with the trade-off
of slower convergence and higher training complexity. These findings highlight
the need for robustness-aware design and evaluation in RL-TSC research. T-REX
contributes to this effort by providing an open, standardized and reproducible
platform for benchmarking RL methods under dynamic and disruptive traffic
scenarios.

</details>


### [55] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao,Qimei Cui,Weicai Li,Wei Ni,Ekram Hossain,Quan Z. Sheng,Xiaofeng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: DP-Ditto是一种在差分隐私保护下的个性化联邦学习（PFL）方法，平衡了隐私保护、模型收敛和性能公平性，并在实验中表现优于现有PFL模型。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中隐私保护与模型性能之间的权衡问题，特别是Ditto方法中个性化学习对联邦学习结果的依赖可能因隐私扰动而影响收敛和公平性。

Method: 提出DP-Ditto，在差分隐私保护下扩展Ditto，分析隐私保证、模型收敛和性能公平性之间的权衡，推导个性化模型的收敛上界和最优全局聚合次数。

Result: 实验表明，DP-Ditto在公平性和准确性上分别超过现有PFL模型32.71%和9.66%。

Conclusion: DP-Ditto在隐私保护下优化了收敛和公平性，为个性化联邦学习提供了可行方案。

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a
balance between personalization and generalization by conducting federated
learning (FL) to guide personalized learning (PL). While FL is unaffected by
personalized model training, in Ditto, PL depends on the outcome of the FL.
However, the clients' concern about their privacy and consequent perturbation
of their local models can affect the convergence and (performance) fairness of
PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension
of Ditto under the protection of differential privacy (DP), and analyzes the
trade-off among its privacy guarantee, model convergence, and performance
distribution fairness. We also analyze the convergence upper bound of the
personalized models under DP-Ditto and derive the optimal number of global
aggregations given a privacy budget. Further, we analyze the performance
fairness of the personalized models, and reveal the feasibility of optimizing
DP-Ditto jointly for convergence and fairness. Experiments validate our
analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of
the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by
over 32.71% in fairness and 9.66% in accuracy.

</details>


### [56] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru,June Sallou,Luis Cruz,Jan Rellermeyer,Arie van Deursen*

Main category: cs.LG

TL;DR: 研究机器学习模型重训练技术的能耗与准确性，提出仅使用最新数据或按需重训练可显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的可靠性受数据变化影响，传统重训练能耗高且不环保，需探索可持续的重训练方法。

Method: 比较不同重训练技术的能耗与准确性，分析仅使用最新数据和按需重训练的效果。

Result: 仅使用最新数据可减少25%能耗，按需重训练可减少40%能耗。

Conclusion: 为设计可持续的机器学习系统提供了更节能的重训练建议。

Abstract: The reliability of machine learning (ML) software systems is heavily
influenced by changes in data over time. For that reason, ML systems require
regular maintenance, typically based on model retraining. However, retraining
requires significant computational demand, which makes it energy-intensive and
raises concerns about its environmental impact. To understand which retraining
techniques should be considered when designing sustainable ML applications, in
this work, we study the energy consumption of common retraining techniques.
Since the accuracy of ML systems is also essential, we compare retraining
techniques in terms of both energy efficiency and accuracy. We showcase that
retraining with only the most recent data, compared to all available data,
reduces energy consumption by up to 25\%, being a sustainable alternative to
the status quo. Furthermore, our findings show that retraining a model only
when there is evidence that updates are necessary, rather than on a fixed
schedule, can reduce energy consumption by up to 40\%, provided a reliable data
change detector is in place. Our findings pave the way for better
recommendations for ML practitioners, guiding them toward more energy-efficient
retraining techniques when designing sustainable ML software systems.

</details>


### [57] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang,Pengqi Wang,Changchang Yin,Ping Zhang*

Main category: cs.LG

TL;DR: SatHealth是一个结合多模态时空数据的新数据集，用于提升AI模型在公共卫生和个人疾病风险预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究因缺乏长期和细粒度的时空数据，未能充分利用环境数据，限制了模型的性能和实际应用。

Method: 开发SatHealth数据集，整合环境数据、卫星图像、疾病流行率和社会健康决定因素，并在两个用例中进行实验。

Result: 实验表明，生活环境信息显著提高了AI模型的性能和时空泛化能力。

Conclusion: SatHealth为医疗研究提供了环境数据整合的框架和工具，未来将扩展至全美。

Abstract: Living environments play a vital role in the prevalence and progression of
diseases, and understanding their impact on patient's health status becomes
increasingly crucial for developing AI models. However, due to the lack of
long-term and fine-grained spatial and temporal data in public and population
health studies, most existing studies fail to incorporate environmental data,
limiting the models' performance and real-world application. To address this
shortage, we developed SatHealth, a novel dataset combining multimodal
spatiotemporal data, including environmental data, satellite images,
all-disease prevalences estimated from medical claims, and social determinants
of health (SDoH) indicators. We conducted experiments under two use cases with
SatHealth: regional public health modeling and personal disease risk
prediction. Experimental results show that living environmental information can
significantly improve AI models' performance and temporal-spatial
generalizability on various tasks. Finally, we deploy a web-based application
to provide an exploration tool for SatHealth and one-click access to both our
data and regional environmental embedding to facilitate plug-and-play
utilization. SatHealth is now published with data in Ohio, and we will keep
updating SatHealth to cover the other parts of the US. With the web application
and published code pipeline, our work provides valuable angles and resources to
include environmental data in healthcare research and establishes a
foundational framework for future research in environmental health informatics.

</details>


### [58] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova,Alex Davey,Brahim Driss,Riad Akrour*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Policy Mirror Descent（PMD）算法StaQ，通过仅保留最近的M个Q函数来解决传统PMD中存储所有历史Q函数的问题，同时保持收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，正则化方法（如熵奖励或KL散度）在实践中表现出色，但理论框架PMD因需存储所有历史Q函数而难以实现。本文旨在解决这一问题。

Method: 提出StaQ算法，仅保留最近的M个Q函数，避免存储所有历史数据，同时保证收敛性。

Result: StaQ在理论上具有强收敛性，实践中性能稳定，优于现有深度强化学习基线。

Conclusion: StaQ为PMD提供了可行的实现方案，推动了稳定深度强化学习算法的发展。

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool
both in theory and practice, typically based either on an entropy bonus or a
Kullback-Leibler divergence that constrains successive policies. In practice,
these approaches have been shown to improve exploration, robustness and
stability, giving rise to popular Deep RL algorithms such as SAC and TRPO.
Policy Mirror Descent (PMD) is a theoretical framework that solves this general
regularized policy optimization problem, however the closed-form solution
involves the sum of all past Q-functions, which is intractable in practice. We
propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions
in memory, and show that for finite and large enough $M$, a convergent
algorithm can be derived, introducing no error in the policy update, unlike
prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong
theoretical guarantees and is competitive with deep RL baselines, while
exhibiting less performance oscillation, paving the way for fully stable deep
RL algorithms and providing a testbed for experimentation with Policy Mirror
Descent.

</details>


### [59] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 本文提出使用S6模型改进算法蒸馏（AD），以解决传统transformer在长序列任务中的局限性，并在复杂连续环境中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统transformer因二次复杂度限制了算法蒸馏（AD）在长序列任务中的应用，需要更高效的模型。

Method: 采用S6模型（如Mamba）替代transformer，利用其线性复杂度处理长序列任务。

Result: 在四个复杂连续环境中，Mamba表现优于transformer，且长上下文AD性能接近SOTA在线元强化学习基线。

Conclusion: S6模型能有效提升AD在长序列任务中的性能，为ICRL提供新方向。

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to
perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic
training histories autoregressively with a causal transformer model. However,
due to practical limitations induced by the attention mechanism, experiments
were bottlenecked by the transformer's quadratic complexity and limited to
simple discrete environments with short time horizons. In this work, we propose
leveraging the recently proposed Selective Structured State Space Sequence (S6)
models, which achieved state-of-the-art (SOTA) performance on long-range
sequence modeling while scaling linearly in sequence length. Through four
complex and continuous Meta Reinforcement Learning environments, we demonstrate
the overall superiority of Mamba, a model built with S6 layers, over a
transformer model for AD. Additionally, we show that scaling AD to very long
contexts can improve ICRL performance and make it competitive even with a SOTA
online meta RL baseline.

</details>


### [60] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi,Damiano Verda*

Main category: cs.LG

TL;DR: 提出了一种用于规则基系统的特征贡献分析框架，包括可视化、重要性度量和规则集比较方法，并在临床数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在需要透明性和可信度的领域（如医疗），规则基系统因其可解释性而被广泛使用，但随着复杂性增加，理解特征贡献变得困难。

Method: 提出图基特征可视化策略、特征重要性度量及规则集比较的距离度量，并在临床数据集和四种规则基方法上实验。

Result: 方法能揭示临床特征的预测价值，识别新风险因素和生物标志物，并在15个公共基准测试中表现优异。

Conclusion: 该方法为规则基系统提供了有效的特征分析工具，有助于提升诊断准确性。

Abstract: In domains where transparency and trustworthiness are crucial, such as
healthcare, rule-based systems are widely used and often preferred over
black-box models for decision support systems due to their inherent
interpretability. However, as rule-based models grow complex, discerning
crucial features, understanding their interactions, and comparing feature
contributions across different rule sets becomes challenging. To address this,
we propose a comprehensive framework for estimating feature contributions in
rule-based systems, introducing a graph-based feature visualisation strategy, a
novel feature importance metric agnostic to rule-based predictors, and a
distance metric for comparing rule sets based on feature contributions. By
experimenting on two clinical datasets and four rule-based methods (decision
trees, logic learning machines, association rules, and neural networks with
rule extraction), we showcase our method's capability to uncover novel insights
on the combined predictive value of clinical features, both at the dataset and
class-specific levels. These insights can aid in identifying new risk factors,
signature genes, and potential biomarkers, and determining the subset of
patient information that should be prioritised to enhance diagnostic accuracy.
Comparative analysis of the proposed feature importance score with
state-of-the-art methods on 15 public benchmarks demonstrates competitive
performance and superior robustness. The method implementation is available on
GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [61] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani,Janak M. Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出了一种新型图信息Transformer算子（GITO），用于学习不规则几何和非均匀网格上的复杂偏微分方程系统，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不规则几何和非均匀网格上复杂偏微分方程系统的学习问题，提升计算效率和泛化能力。

Method: GITO由混合图Transformer（HGT）和Transformer神经算子（TNO）组成，结合图神经网络和Transformer，利用自注意力融合层增强特征学习。

Result: 在基准PDE任务中，GITO表现优于现有基于Transformer的神经算子，支持零样本超分辨率。

Conclusion: GITO为工程应用中的高效、网格无关替代求解器提供了新途径。

Abstract: We present a novel graph-informed transformer operator (GITO) architecture
for learning complex partial differential equation systems defined on irregular
geometries and non-uniform meshes. GITO consists of two main modules: a hybrid
graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages
a graph neural network (GNN) to encode local spatial relationships and a
transformer to capture long-range dependencies. A self-attention fusion layer
integrates the outputs of the GNN and transformer to enable more expressive
feature learning on graph-structured data. TNO module employs linear-complexity
cross-attention and self-attention layers to map encoded input functions to
predictions at arbitrary query locations, ensuring discretization invariance
and enabling zero-shot super-resolution across any mesh. Empirical results on
benchmark PDE tasks demonstrate that GITO outperforms existing
transformer-based neural operators, paving the way for efficient, mesh-agnostic
surrogate solvers in engineering applications.

</details>


### [62] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu,Haocheng Zhang,Tao Chengxu,Zuyi Chen*

Main category: cs.LG

TL;DR: 该论文研究了工业时间序列数据中的少样本学习（FSL），提出了一种标签感知的采样方法，并在螺丝紧固过程监控中验证了其有效性。结果表明，轻量级CNN结合度量学习优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 工业时间序列数据标注成本高，少样本学习在此领域的应用尚未充分探索。

Method: 提出了标签感知的采样方法，将多标签序列转化为单标签任务，并比较了度量学习（Prototypical Network）和元学习（MAML）两种范式，结合三种骨干网络（1D CNN、InceptionTime、Transformer）。

Result: InceptionTime + Prototypical Network在10-shot、3-way评估中表现最佳，加权F1达0.944（多类）和0.935（多标签），优于大型模型。度量学习始终优于MAML，标签感知采样提升1.7% F1。

Conclusion: 轻量级CNN结合度量学习在数据稀缺时优于大型模型，为高价值制造检测中的FSL应用提供了可行方案。

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely
unexplored for \emph{industrial} time-series data, where annotating every new
defect is prohibitively expensive. We present a systematic FSL study on
screw-fastening process monitoring, using a 2\,300-sample multivariate torque
dataset that covers 16 uni- and multi-factorial defect types. Beyond
benchmarking, we introduce a \textbf{label-aware episodic sampler} that
collapses multi-label sequences into multiple single-label tasks, keeping the
output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical
Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML),
each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter
transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime +
Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the
multi-class regime and \textbf{0.935} in the multi-label regime, outperforming
finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer
parameters and training time. Across all backbones, metric learning
consistently surpasses MAML, and our label-aware sampling yields an additional
1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are
always superior: when data are scarce, lightweight CNN architectures augmented
with simple metric learning not only converge faster but also generalize
better. We release code, data splits and pre-trained weights to foster
reproducible research and to catalyze the adoption of FSL in high-value
manufacturing inspection.

</details>


### [63] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman,Balder ten Cate*

Main category: cs.LG

TL;DR: HEGNNs是一种基于层次化节点个性化的图神经网络扩展，能够区分图同构，并在实验中优于传统GNN架构。


<details>
  <summary>Details</summary>
Motivation: 受图同构测试的个性化-细化范式启发，提出HEGNNs以增强GNN的表达能力。

Method: 通过层次化节点个性化扩展GNN，并结合逻辑特征（如局部同态计数）进行实验验证。

Result: HEGNNs在实验中表现出优于传统GNN的性能，并能区分图同构。

Conclusion: HEGNNs为GNN提供了一种更具表达力的层次化扩展，具有实际应用潜力。

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an
expressive extension of graph neural networks (GNNs) with hierarchical node
individualization, inspired by the Individualization-Refinement paradigm for
graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy
of increasingly expressive models that, in the limit, can distinguish graphs up
to isomorphism. We provide a logical characterization of HEGNN node
classifiers, with and without subgraph restrictions, using graded hybrid logic.
This characterization enables us to relate the separating power of HEGNNs to
that of higher-order GNNs, GNNs enriched with local homomorphism count
features, and color refinement algorithms based on
Individualization-Refinement. Our experimental results confirm the practical
feasibility of HEGNNs and show benefits in comparison with traditional GNN
architectures, both with and without local homomorphism count features.

</details>


### [64] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales,Arturo Jaramillo,Heli Ricalde Guerrero*

Main category: cs.LG

TL;DR: 提出了一种基于粒子的变分推理方法BSVGD，用于处理多模态分布，通过随机分支机制扩展了SVGD算法，并提供了理论收敛保证和数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决多模态分布下的变分推理问题，扩展SVGD算法的适用性。

Method: 提出BSVGD算法，引入随机分支机制以增强状态空间探索。

Result: 理论证明了分布收敛性，数值实验显示BSVGD在Wasserstein距离和计算时间上优于SVGD。

Conclusion: BSVGD是一种有效的多模态分布变分推理方法，优于传统SVGD。

Abstract: We propose a novel particle-based variational inference method designed to
work with multimodal distributions. Our approach, referred to as Branched Stein
Variational Gradient Descent (BSVGD), extends the classical Stein Variational
Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism
that encourages the exploration of the state space. In this work, a theoretical
guarantee for the convergence in distribution is presented, as well as
numerical experiments to validate the suitability of our algorithm. Performance
comparisons between the BSVGD and the SVGD are presented using the Wasserstein
distance between samples and the corresponding computational times.

</details>


### [65] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath,Elaine Lau,Anisha Gunjal,Manasi Sharma,Nikhil Baharte,Sean Hendryx*

Main category: cs.LG

TL;DR: RLVR训练通过压缩pass@k到pass@1和能力增益提升模型性能，新算法Guide通过自适应提示和优化策略显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR训练如何通过能力增益和自蒸馏帮助模型解决新问题，并开发新算法提升性能。

Method: 提出Guide算法，自适应加入提示并优化策略，实验验证其在GRPO和PPO中的效果。

Result: Guide-GRPO在7B和32B模型上显著提升泛化能力，数学基准提升达4%。

Conclusion: Guide算法通过自适应提示和策略优化有效提升模型性能，为RLVR训练提供新方向。

Abstract: We study the process through which reasoning models trained with
reinforcement learning on verifiable rewards (RLVR) can learn to solve new
problems. We find that RLVR drives performance through two main means: (1) by
compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models
learn to solve new problems that they previously could not solve even at high
$k$. We find that while capability gain exists across model scales, learning to
solve new problems is primarily driven through self-distillation. We
demonstrate these findings across model scales ranging from 0.5B to 72B on
>500,000 reasoning problems with prompts and verifiable final answers across
math, science, and code domains. We further show that we can significantly
improve pass@$k$ rates by leveraging natural language guidance for the model to
consider within context while still requiring the model to derive a solution
chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new
class of online training algorithms. $\text{Guide}$ adaptively incorporates
hints into the model's context on problems for which all rollouts were
initially incorrect and adjusts the importance sampling ratio for the
"off-policy" trajectories in order to optimize the policy for contexts in which
the hints are no longer present. We describe variants of $\text{Guide}$ for
GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter
models improves generalization over its vanilla counterpart with up to 4$\%$
macro-average improvement across math benchmarks. We include careful ablations
to analyze $\text{Guide}$'s components and theoretically analyze Guide's
learning efficiency.

</details>


### [66] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci,Antonio Rizzo,Giovanna Maria Dimitri*

Main category: cs.LG

TL;DR: 本文通过系统的事后解释框架，分析了内在动机如何影响弹性决策变换器（EDTs）的嵌入表示，发现不同动机变体产生不同的表示结构，并揭示了嵌入指标与性能之间的环境特异性相关性。


<details>
  <summary>Details</summary>
Motivation: 研究内在动机在EDTs中如何塑造嵌入表示，以解释其性能提升的机制。

Method: 引入事后解释框架，通过统计分析嵌入属性（如协方差结构、向量大小和正交性）来研究内在动机的影响。

Result: 发现不同内在动机变体产生不同的表示结构，并揭示了嵌入指标与性能的环境特异性相关性。

Conclusion: 内在动机不仅是探索奖励，还作为一种表示先验，以生物学合理的方式塑造嵌入几何，促进更好的决策。

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly
successful in offline reinforcement learning, offering a flexible framework
that unifies sequence modeling with decision-making under uncertainty. Recent
research has shown that incorporating intrinsic motivation mechanisms into EDTs
improves performance across exploration tasks, yet the representational
mechanisms underlying these improvements remain unexplored. In this paper, we
introduce a systematic post-hoc explainability framework to analyze how
intrinsic motivation shapes learned embeddings in EDTs. Through statistical
analysis of embedding properties (including covariance structure, vector
magnitudes, and orthogonality), we reveal that different intrinsic motivation
variants create fundamentally different representational structures. Our
analysis demonstrates environment-specific correlation patterns between
embedding metrics and performance that explain why intrinsic motivation
improves policy learning. These findings show that intrinsic motivation
operates beyond simple exploration bonuses, acting as a representational prior
that shapes embedding geometry in biologically plausible ways, creating
environment-specific organizational structures that facilitate better
decision-making.

</details>


### [67] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang,Chengyu Zhang,Yuetian Chen,Nathalie Baracaldo,Swanand Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: 本文通过覆盖率和稳定性分析框架，系统研究了成员推理攻击（MIAs）之间的差异，并提出了一种集成框架以提升隐私评估的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MIAs研究主要关注性能指标（如AUC、准确率等），但忽视了不同攻击之间的差异，这对隐私评估的可靠性和完整性至关重要。

Method: 采用覆盖率和稳定性分析框架，系统研究MIAs的差异，并提出集成框架以结合多种攻击的优势。

Result: 实验揭示了MIAs的显著差异及其潜在原因，集成框架能构建更强大的攻击并提供更全面的隐私评估方法。

Conclusion: 本文的框架不仅提升了攻击能力，还为隐私评估提供了更鲁棒和全面的方法论。

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy
of machine learning models and are widely used as tools for privacy assessment,
auditing, and machine unlearning. While prior MIA research has primarily
focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either
by developing new methods to enhance these metrics or using them to evaluate
privacy solutions - we found that it overlooks the disparities among different
attacks. These disparities, both between distinct attack methods and between
multiple instantiations of the same method, have crucial implications for the
reliability and completeness of MIAs as privacy evaluation tools. In this
paper, we systematically investigate these disparities through a novel
framework based on coverage and stability analysis. Extensive experiments
reveal significant disparities in MIAs, their potential causes, and their
broader implications for privacy evaluation. To address these challenges, we
propose an ensemble framework with three distinct strategies to harness the
strengths of state-of-the-art MIAs while accounting for their disparities. This
framework not only enables the construction of more powerful attacks but also
provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [68] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw,Blake Woodworth,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文分析了局部梯度下降在逻辑回归中的表现，放宽了步长限制，并展示了在异构数据下的收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对异构目标的局部梯度下降分析要求步长η≤1/K，以确保目标单调递减。本文旨在放宽这一限制，分析任意步长η>0的情况。

Method: 使用局部梯度下降方法，针对逻辑回归和可分离的异构数据，分析其收敛性。

Result: 在R轮通信和M个客户端的情况下，收敛速率为O(1/ηKR)，初始不稳定阶段持续O~(ηKM)轮。

Conclusion: 本文的收敛速率优于现有的一般光滑凸目标的O(1/R)速率，并揭示了异构目标和大局部更新对不稳定性的影响。

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous
objectives requires stepsizes $\eta \leq 1/K$ where $K$ is the communication
interval, which ensures monotonic decrease of the objective. In contrast, we
analyze Local Gradient Descent for logistic regression with separable,
heterogeneous data using any stepsize $\eta > 0$. With $R$ communication rounds
and $M$ clients, we show convergence at a rate $\mathcal{O}(1/\eta K R)$ after
an initial unstable phase lasting for $\widetilde{\mathcal{O}}(\eta K M)$
rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general
smooth, convex objectives. Our analysis parallels the single machine analysis
of~\cite{wu2024large} in which instability is caused by extremely large
stepsizes, but in our setting another source of instability is large local
updates with heterogeneous objectives.

</details>


### [69] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

Main category: cs.LG

TL;DR: 提出了一种名为HAELT的深度学习框架，用于高频股票价格预测，结合了多种技术以应对非平稳性、噪声和波动性问题。


<details>
  <summary>Details</summary>
Motivation: 高频股票价格预测面临非平稳性、噪声和波动性等挑战，需要一种更稳健的方法。

Method: HAELT框架结合了ResNet噪声抑制模块、时间自注意力机制和混合LSTM-Transformer核心，并通过自适应集成提升性能。

Result: 在2024年1月至2025年5月的苹果公司（AAPL）每小时数据上，HAELT在测试集上取得了最高的F1分数，能够有效识别价格涨跌。

Conclusion: HAELT展示了在金融预测和算法交易中的实际应用潜力。

Abstract: High-frequency stock price prediction is challenging due to non-stationarity,
noise, and volatility. To tackle these issues, we propose the Hybrid Attentive
Ensemble Learning Transformer (HAELT), a deep learning framework combining a
ResNet-based noise-mitigation module, temporal self-attention for dynamic focus
on relevant history, and a hybrid LSTM-Transformer core that captures both
local and long-range dependencies. These components are adaptively ensembled
based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from
Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set,
effectively identifying both upward and downward price movements. This
demonstrates HAELT's potential for robust, practical financial forecasting and
algorithmic trading.

</details>


### [70] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin,Adiba Abid,M. F. Mridha*

Main category: cs.LG

TL;DR: QCL-MixNet是一种基于量子启发对比学习和动态混合的新框架，用于处理类别不平衡的表格数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 专家系统在类别不平衡的表格数据中需要检测罕见但关键的实例，现有方法存在过拟合、标签噪声和泛化能力差等问题。

Method: QCL-MixNet结合量子纠缠启发层、动态混合策略和混合损失函数，提升少数类表示和分类性能。

Result: 在18个真实数据集上，QCL-MixNet在macro-F1和召回率上显著优于20种现有方法。

Conclusion: QCL-MixNet为专家系统中的表格不平衡处理设定了新基准，具有理论支持的实际优势。

Abstract: Expert systems often operate in domains characterized by class-imbalanced
tabular data, where detecting rare but critical instances is essential for
safety and reliability. While conventional approaches, such as cost-sensitive
learning, oversampling, and graph neural networks, provide partial solutions,
they suffer from drawbacks like overfitting, label noise, and poor
generalization in low-density regions. To address these challenges, we propose
QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented
with k-nearest neighbor (kNN) guided dynamic mixup for robust classification
under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum
Entanglement-inspired layer that models complex feature interactions through
sinusoidal transformations and gated attention, (ii) a sample-aware mixup
strategy that adaptively interpolates feature representations of semantically
similar instances to enhance minority class representation, and (iii) a hybrid
loss function that unifies focal reweighting, supervised contrastive learning,
triplet margin loss, and variance regularization to improve both intra-class
compactness and inter-class separability. Extensive experiments on 18
real-world imbalanced datasets (binary and multi-class) demonstrate that
QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep
learning, and GNN-based baselines in macro-F1 and recall, often by substantial
margins. Ablation studies further validate the critical role of each
architectural component. Our results establish QCL-MixNet as a new benchmark
for tabular imbalance handling in expert systems. Theoretical analyses
reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [71] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo,Xun Xian,Jin Du,Fangqiao Tian,Ganghua Wang,Ming Zhong,Shengchun Zhao,Xuan Bi,Zirui Liu,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: 论文评估了大型语言模型（LLMs）在数据科学任务中利用外部领域知识的能力，发现其存在对信息不加批判采纳的问题，尤其在对抗性内容下表现更差。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够像人类数据科学家一样批判性地利用外部领域知识。

Method: 引入AssistedDS基准，结合合成数据集和真实Kaggle竞赛数据，评估LLMs在表格预测任务中处理领域知识的能力。

Result: LLMs常不加批判地采纳信息，对抗性内容显著降低其预测性能；且难以抵消对抗性信息的负面影响。

Conclusion: 当前模型在批判性评估和利用专家知识方面存在显著不足，需开发更鲁棒的自动化数据科学系统。

Abstract: Large language models (LLMs) have advanced the automation of data science
workflows. Yet it remains unclear whether they can critically leverage external
domain knowledge as human data scientists do in practice. To answer this
question, we introduce AssistedDS (Assisted Data Science), a benchmark designed
to systematically evaluate how LLMs handle domain knowledge in tabular
prediction tasks. AssistedDS features both synthetic datasets with explicitly
known generative mechanisms and real-world Kaggle competitions, each
accompanied by curated bundles of helpful and adversarial documents. These
documents provide domain-specific insights into data cleaning, feature
engineering, and model selection. We assess state-of-the-art LLMs on their
ability to discern and apply beneficial versus harmful domain knowledge,
evaluating submission validity, information recall, and predictive performance.
Our results demonstrate three key findings: (1) LLMs frequently exhibit an
uncritical adoption of provided information, significantly impairing their
predictive performance when adversarial content is introduced, (2) helpful
guidance is often insufficient to counteract the negative influence of
adversarial information, and (3) in Kaggle datasets, LLMs often make errors in
handling time-series data, applying consistent feature engineering across
different folds, and interpreting categorical variables correctly. These
findings highlight a substantial gap in current models' ability to critically
evaluate and leverage expert knowledge, underscoring an essential research
direction for developing more robust, knowledge-aware automated data science
systems.

</details>


### [72] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman,Samyam Rajbhandari,Michael Wyatt,Jeff Rasley,Tunji Ruwase,Zhewei Yao,Aurick Qiao,Yuxiong He*

Main category: cs.LG

TL;DR: ALST（Arctic Long Sequence Training）是一种支持超长序列训练的技术，解决了开源社区在长序列训练中的内存限制问题，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 长序列训练在RAG、长文档摘要等应用中至关重要，但现有开源工具在长序列训练中存在内存不足和优化不足的问题。

Method: ALST结合了单GPU和多GPU内存优化技术，支持多种Hugging Face模型的无缝长序列训练。

Result: ALST在单H100 GPU上支持500K序列长度，8xH100节点支持3.7M，4节点集群支持15M，比32K基线提升400倍。

Conclusion: ALST为开源社区提供了高效的长序列训练解决方案，兼容Hugging Face模型并已开源。

Abstract: Long sequences are critical for applications like RAG, long document
summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout,
support max sequence length of up to 10 million tokens. However, outside of
enterprise labs, long sequence training is challenging for the AI community
with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B
model with sequence over 32K runs out of memory on a basic Hugging Face (HF)
model due to two reasons: i) LLM training workloads are not optimized to fully
leverage a single GPU memory, ii) existing solutions for leveraging multiple
GPU memory are not easily available to HF models, making long sequence training
inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a
combination of attention-agnostic single GPU and multi-GPU memory
optimizations, that enables it to support out-of-box training of multi-million
sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a
single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node
cluster, an increase of over 400x compared to the 32K baseline for the latter.
ALST is fully compatible with HF models and open-sourced via Deepspeed
https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic
Training
https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [73] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen,Heejune Sheen,Xuyuan Xiong,Tianhao Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于统计框架的稀疏自编码器（SAE）训练算法，通过偏置自适应技术解决现有方法缺乏理论保证和实用性问题，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）训练算法缺乏严格的数学保证，且存在超参数敏感性和不稳定性等实际问题，需要一种更可靠的方法来恢复特征。

Method: 提出了一种统计框架，将多义特征建模为单义概念的稀疏混合，并基于此开发了偏置自适应算法（GBA），通过调整神经网络偏置参数确保激活稀疏性。

Result: 理论证明了算法在统计模型下能正确恢复所有单义特征，实验表明GBA在15亿参数的大语言模型上优于基准方法。

Conclusion: 该研究为SAE训练提供了首个具有理论恢复保证的算法，推动了通过机制可解释性开发更透明、可信的AI系统。

Abstract: We study the challenge of achieving theoretically grounded feature recovery
using Sparse Autoencoders (SAEs) for the interpretation of Large Language
Models. Existing SAE training algorithms often lack rigorous mathematical
guarantees and suffer from practical limitations such as hyperparameter
sensitivity and instability. To address these issues, we first propose a novel
statistical framework for the feature recovery problem, which includes a new
notion of feature identifiability by modeling polysemantic features as sparse
mixtures of underlying monosemantic concepts. Building on this framework, we
introduce a new SAE training algorithm based on ``bias adaptation'', a
technique that adaptively adjusts neural network bias parameters to ensure
appropriate activation sparsity. We theoretically \highlight{prove that this
algorithm correctly recovers all monosemantic features} when input data is
sampled from our proposed statistical model. Furthermore, we develop an
improved empirical variant, Group Bias Adaptation (GBA), and
\highlight{demonstrate its superior performance against benchmark methods when
applied to LLMs with up to 1.5 billion parameters}. This work represents a
foundational step in demystifying SAE training by providing the first SAE
algorithm with theoretical recovery guarantees, thereby advancing the
development of more transparent and trustworthy AI systems through enhanced
mechanistic interpretability.

</details>


### [74] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen,Soumyadeep Pal,Yimeng Zhang,Qing Qu,Sijia Liu*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）的机器遗忘（MU）问题，发现遗忘过程会在模型中留下可检测的痕迹，这些痕迹可以通过简单的分类器从输出中识别。


<details>
  <summary>Details</summary>
Motivation: 研究机器遗忘在保护数据隐私、执行版权和减少社会技术危害中的重要性，同时揭示遗忘后模型存在的新漏洞。

Method: 通过分析模型行为和内部表示，发现遗忘痕迹，并使用监督分类器检测这些痕迹。

Result: 实验表明，遗忘相关提示的检测准确率超过90%，即使无关输入下，大型LLM仍保持高可检测性。

Conclusion: 遗忘过程会留下可测量的痕迹，可能带来逆向工程被遗忘信息的风险。

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred
to as LLM unlearning, seeks to remove specific undesirable data or knowledge
from a trained model, while maintaining its performance on standard tasks.
While unlearning plays a vital role in protecting data privacy, enforcing
copyright, and mitigating sociotechnical harms in LLMs, we identify a new
vulnerability post-unlearning: unlearning trace detection. We discover that
unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces
in both model behavior and internal representations. These traces can be
identified from output responses, even when prompted with forget-irrelevant
inputs. Specifically, a simple supervised classifier can reliably determine
whether a model has undergone unlearning based solely on its textual outputs.
Further analysis shows that these traces are embedded in intermediate
activations and propagate nonlinearly to the final layer, forming
low-dimensional, learnable manifolds in activation space. Through extensive
experiments, we show that forget-relevant prompts enable over 90% accuracy in
detecting unlearning traces across all model sizes. Even with forget-irrelevant
inputs, large LLMs maintain high detectability, demonstrating the broad
applicability of unlearning trace detection. These findings reveal that
unlearning leaves measurable signatures, introducing a new risk of
reverse-engineering forgotten information when a model is identified as
unlearned given an input query. Codes are available at [this
URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [75] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang,Jiahao Cui,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: BWFlow是一种基于流匹配的图生成框架，通过马尔可夫随机场建模节点和边的联合演化，利用最优传输设计概率路径，解决了现有方法在非欧几里得图结构中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法（如扩散和基于流的模型）假设数据在欧几里得空间中，独立建模节点和边的演化，导致采样收敛风险。

Method: 提出BWFlow框架，将图表示为马尔可夫随机场，利用最优传输设计概率路径，支持连续和离散流匹配算法。

Result: 实验证明BWFlow在图生成和分子生成任务中表现优异，训练稳定且采样收敛性有保障。

Conclusion: BWFlow通过尊重图的几何结构，提供了一种更优的图生成方法。

Abstract: Graph generation has emerged as a critical task in fields ranging from
molecule design to drug discovery. Contemporary approaches, notably diffusion
and flow-based models, have achieved solid graph generative performance through
constructing a probability path that interpolates between a reference
distribution and the data distribution. However, these methods typically model
the evolution of individual nodes and edges independently and use linear
interpolations to build the path assuming that the data lie in Euclidean space.
We show that this is suboptimal given the intrinsic non-Euclidean structure and
interconnected patterns of graphs, and it poses risks to the sampling
convergence. To build a better probability path, we model the joint evolution
of the nodes and edges by representing graphs as connected systems
parameterized by Markov random fields (MRF). We then leverage the optimal
transport displacement between MRF objects to design the probability path for
graph generation. Based on this, we introduce BWFlow, a flow-matching framework
for graph generation that respects the underlying geometry of graphs and
provides smooth velocities in the probability path. The novel framework can be
adapted to both continuous and discrete flow-matching algorithms. Experimental
evaluations in plain graph generation and 2D/3D molecule generation validate
the effectiveness of BWFlow in graph generation with competitive performance,
stable training, and guaranteed sampling convergence.

</details>


### [76] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang,Sina Aghaee Dabaghan Fard,Matthew Lemon,Jaesung Lee,Yuxiao Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经网络（IE-PINN），用于从噪声位移数据中稳健地重建弹性参数分布，解决了现有方法在噪声敏感性和绝对尺度估计上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有弹性参数估计方法在噪声敏感性和稳定性上存在不足，难以准确恢复绝对尺度的弹性参数。

Method: 采用三种神经网络分别建模位移场、应变场和弹性分布，结合两阶段估计策略（相对分布恢复和绝对尺度校准），并引入位置编码、正弦激活函数和预训练协议。

Result: IE-PINN在噪声条件下仍能准确估计绝对尺度的弹性参数，显著优于现有方法。

Conclusion: IE-PINN为临床成像和机械表征提供了更可靠的弹性参数估计工具。

Abstract: Accurately estimating spatially heterogeneous elasticity parameters,
particularly Young's modulus and Poisson's ratio, from noisy displacement
measurements remains significantly challenging in inverse elasticity problems.
Existing inverse estimation techniques are often limited by instability,
pronounced sensitivity to measurement noise, and difficulty in recovering
absolute-scale Young's modulus. This work presents a novel Inverse Elasticity
Physics-Informed Neural Network (IE-PINN) specifically designed to robustly
reconstruct heterogeneous distributions of elasticity parameters from noisy
displacement data based on linear elasticity physics. IE-PINN integrates three
distinct neural network architectures dedicated to separately modeling
displacement fields, strain fields, and elasticity distributions, thereby
significantly enhancing stability and accuracy against measurement noise.
Additionally, a two-phase estimation strategy is introduced: the first phase
recovers relative spatial distributions of Young's modulus and Poisson's ratio,
and the second phase calibrates the absolute scale of Young's modulus using
imposed loading boundary conditions. Additional methodological innovations,
including positional encoding, sine activation functions, and a sequential
pretraining protocol, further enhance the model's performance and robustness.
Extensive numerical experiments demonstrate that IE-PINN effectively overcomes
critical limitations encountered by existing methods, delivering accurate
absolute-scale elasticity estimations even under severe noise conditions. This
advancement holds substantial potential for clinical imaging diagnostics and
mechanical characterization, where measurements typically encounter substantial
noise.

</details>


### [77] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi,Siddhartha Sen,Ali Farhadi*

Main category: cs.LG

TL;DR: 本文提出了一种新的负载均衡损失函数，用于稀疏混合专家模型（MoE），以解决路由器在训练过程中专家选择不一致的问题，从而提升模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE模型通过激活部分专家参数实现高效训练，但现有负载均衡机制可能导致专家选择不一致，影响模型性能和冗余学习。

Method: 引入一种新的负载均衡损失函数，保持输入令牌之间的关系结构，促使相似输入在训练中选择一致的专家。

Result: 实验表明，新损失函数使模型收敛速度提升36%，并减少冗余学习。

Conclusion: 新方法有效解决了MoE模型中的负载均衡问题，提升了训练效率和模型性能。

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient
architecture for training large neural networks by activating only a subset of
parameters ("experts") for each input. A learned router computes a distribution
over these experts, and assigns input tokens to a small subset. However,
without auxiliary balancing mechanisms, routers often converge to using only a
few experts, severely limiting model capacity and degrading performance. Most
current load balancing mechanisms encourage a distribution over experts that
resembles a roughly uniform distribution of experts per token. During training,
this can result in inconsistent routing behavior, resulting in the model
spending its capacity to learn redundant knowledge. We address this by
introducing a novel load balancing loss that preserves token-wise relational
structure, encouraging consistent expert choices for similar inputs during
training. Our experimental results show that applying our loss to the router
results in 36% faster convergence and lower redundancy compared to a popular
load balancing loss.

</details>


### [78] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan,Haodi Xu,Feng Tao,Md Nasim,Marc Grimson,Yiqi Luo,Carla P. Gomes*

Main category: cs.LG

TL;DR: ScIReN是一个结合可解释神经网络和基于过程的模型的透明框架，旨在解决科学发现中的黑盒问题，同时提高预测准确性和科学可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络缺乏科学可解释性，而基于过程的模型在跨尺度预测中表现不佳。ScIReN旨在结合两者的优势，实现科学发现与预测的双重目标。

Method: ScIReN使用可解释编码器预测科学意义的潜在参数，并通过可微分的基于过程的解码器预测输出变量。引入硬Sigmoid约束层确保参数在科学先验范围内。

Result: ScIReN在有机碳流动和生态系统呼吸建模任务中，预测准确性优于黑盒模型，并能推断潜在科学机制。

Conclusion: ScIReN成功结合了科学可解释性和预测性能，为科学发现提供了新工具。

Abstract: Neural networks are a powerful tool for learning patterns from data. However,
they do not respect known scientific laws, nor can they reveal novel scientific
insights due to their black-box nature. In contrast, scientific reasoning
distills biological or physical principles from observations and controlled
experiments, and quantitatively interprets them with process-based models made
of mathematical equations. Yet, process-based models rely on numerous free
parameters that must be set in an ad-hoc manner, and thus often fit
observations poorly in cross-scale predictions. While prior work has embedded
process-based models in conventional neural networks, discovering interpretable
relationships between parameters in process-based models and input features is
still a grand challenge for scientific discovery. We thus propose
Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent
framework that combines interpretable neural and process-based reasoning. An
interpretable encoder predicts scientifically-meaningful latent parameters,
which are then passed through a differentiable process-based decoder to predict
labeled output variables. ScIReN also uses a novel hard-sigmoid constraint
layer to restrict latent parameters to meaningful ranges defined by scientific
prior knowledge, further enhancing its interpretability. While the embedded
process-based model enforces established scientific knowledge, the encoder
reveals new scientific mechanisms and relationships hidden in conventional
black-box models. We apply ScIReN on two tasks: simulating the flow of organic
carbon through soils, and modeling ecosystem respiration from plants. In both
tasks, ScIReN outperforms black-box networks in predictive accuracy while
providing substantial scientific interpretability -- it can infer latent
scientific mechanisms and their relationships with input features.

</details>


### [79] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee,Yoonjae Jung,Sangdon Park*

Main category: cs.LG

TL;DR: 论文提出了一种在线学习算法，用于在部分反馈下进行选择性生成，通过将问题转化为多臂老虎机问题，并利用反馈解锁技术提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言生成模型在与人类交互时可能产生虚假回答（幻觉效应），而选择性生成（即不确定时不回答）是控制幻觉的有效方法。然而，目前缺乏在非随机环境和部分反馈下学习选择性生成的方法。

Method: 将选择性生成问题转化为多臂老虎机问题，提出一种反馈解锁技术以提升收敛速度，并利用已知老虎机算法及其理论性质。

Result: 算法在多种数据环境下有效控制了假发现率（FDR），同时保持了合理的选择效率（非弃答比例）。

Conclusion: 该方法为部分反馈下的选择性生成提供了一种高效且理论保障的解决方案。

Abstract: Large language generative models increasingly interact with humans, while
their falsified responses raise concerns. To address this hallucination effect,
selectively abstaining from answering, called selective generation, provides an
effective way for generators to control the hallucination when it is unsure of
their answers. However, as selective generators are interacting under
non-stochastic environments and having partial feedback from users on selective
generation (e.g., thumbs up or down on the selected answer), learning methods
for selective generation under such practical setups are crucial but currently
missing. To address these limitations, we propose an online learning algorithm
for selective generation under partial feedback. In particular, as learning
under partial feedback is well-studied by multi-armed bandit problems, we
reduce selective generation to bandits and provide a novel conversion lemma
from bandits back to selective generation to leverage any known bandit
algorithms and theoretical properties. This mainly connects regret guarantees
of bandits to false discovery rate (FDR) guarantees of selective generation for
controlling hallucination. However, naively exploiting known bandit algorithms
and their regret bounds suffers from slow convergence speed in practice due the
nature of partial feedback. To overcome this, we exploit a unique structure of
arms in selective generation for feedback unlocking, i.e., unlocking unknown
feedback from observed feedback. We theoretically and empirically evaluate the
efficacy of the proposed online selective generation algorithm under partial
feedback over diverse data environment setups, resulting in controlling a
desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio
of non-abstaining answers, compared to baselines.

</details>


### [80] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney,Chenhui Deng,Chia-Tung Ho,Yun-Da Tsai,Mingjie Liu,Wenfei Zhou,Brucek Khailany,Haoxing Ren*

Main category: cs.LG

TL;DR: CVDP是一个新的硬件设计和验证基准数据集，包含783个问题，覆盖13个任务类别，旨在推动LLM和智能体研究。


<details>
  <summary>Details</summary>
Motivation: 当前硬件设计自动化的模型能力有限，需要更真实和挑战性的基准来推动研究。

Method: CVDP提供非智能体和智能体格式的问题，使用开源工具和模型评分基础设施进行评估。

Result: 最先进模型在代码生成任务中仅达到34%的通过率，智能体任务尤为困难。

Conclusion: CVDP揭示了当前模型的显著能力差距，强调了继续研究以实现稳健的硬件设计自动化的必要性。

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new
dataset and infrastructure to advance LLM and agent research in hardware design
and verification. CVDP includes 783 problems across 13 task categories,
covering RTL generation, verification, debugging, specification alignment, and
technical Q&A authored by experienced hardware engineers. Problems are offered
in both non-agentic and agentic formats. The benchmark introduces more
realistic and challenging contexts than prior work, with state-of-the-art
models achieving no more than 34% pass@1 on code generation. Agentic
tasks$\unicode{x2013}$especially those involving RTL reuse and
verification$\unicode{x2013}$are particularly difficult. Evaluation uses
open-source tools and model scoring infrastructure, with comprehension tasks
assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in
current model capabilities, underscoring the need for continued research toward
robust, real-world hardware design automation.

</details>


### [81] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao,Chenghao Liu,Yiming Zhang,Ming Jin,Quang Pham,Qingsong Wen,P. N. Suganthan,Xudong Jiang,Savitha Ramasamy*

Main category: cs.LG

TL;DR: 论文提出了一种名为MSFT的多尺度微调框架，用于优化时间序列基础模型（TSFMs）在下游任务中的性能，解决了传统微调方法容易过拟合和性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列基础模型（TSFMs）在零样本预测中表现优异，但在下游任务中的微调效果不佳，容易过拟合且未能充分利用其多尺度预测能力。

Method: 论文提出MSFT框架，通过显式建模多尺度时间模式，优化TSFMs的微调过程。该方法在三种不同的骨干模型上进行了验证。

Result: 实验证明，MSFT框架在微调后的TSFMs性能优于传统微调方法和参数高效微调方法，甚至超越了当前最先进的深度学习方法。

Conclusion: MSFT框架通过多尺度建模显著提升了TSFMs在下游任务中的性能，为时间序列预测领域提供了一种有效的微调方法。

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot
performance for time series forecasting. However, an important yet
underexplored challenge is how to effectively finetune TSFMs on specific
downstream tasks. While naive finetuning can yield performance gains, we argue
that it falls short of fully leveraging TSFMs' capabilities, often resulting in
overfitting and suboptimal performance. Given the diverse temporal patterns
across sampling scales and the inherent multi-scale forecasting capabilities of
TSFMs, we adopt a causal perspective to analyze finetuning process, through
which we highlight the critical importance of explicitly modeling multiple
scales and reveal the shortcomings of naive approaches. Focusing on
\textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale
\textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet
general framework that explicitly integrates multi-scale modeling into the
finetuning process. Experimental results on three different backbones (\moirai,
\moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only
outperform naive and typical parameter efficient finetuning methods but also
surpass state-of-the-art deep learning methods.

</details>


### [82] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram,Kenneth L. Clarkson,Tim Klinger,Shashanka Ubaru,Alexander G. Gray*

Main category: cs.LG

TL;DR: 研究发现输入依赖的稀疏注意力模型比标准注意力模型收敛更快且泛化更好，而输入无关的稀疏注意力模型无此优势。理论分析表明，输入依赖的稀疏注意力通过集中模型的语义焦点加速学习。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏注意力在可学习性和泛化性方面的表现，而非仅关注效率。

Method: 通过实证研究多种注意力机制，并理论分析稀疏注意力对软最大稳定性和损失函数Lipschitz性质的影响。

Result: 输入依赖的稀疏注意力模型表现更优，输入无关的稀疏注意力模型无优势。理论分析支持这一现象。

Conclusion: 输入依赖的稀疏注意力通过语义焦点加速学习，理论分析验证其优势条件。

Abstract: Various forms of sparse attention have been explored to mitigate the
quadratic computational and memory cost of the attention mechanism in
transformers. We study sparse transformers not through a lens of efficiency but
rather in terms of learnability and generalization. Empirically studying a
range of attention mechanisms, we find that input-dependent sparse attention
models appear to converge faster and generalize better than standard attention
models, while input-agnostic sparse attention models show no such benefits -- a
phenomenon that is robust across architectural and optimization hyperparameter
choices. This can be interpreted as demonstrating that concentrating a model's
"semantic focus" with respect to the tokens currently being considered (in the
form of input-dependent sparse attention) accelerates learning. We develop a
theoretical characterization of the conditions that explain this behavior. We
establish a connection between the stability of the standard softmax and the
loss function's Lipschitz properties, then show how sparsity affects the
stability of the softmax and the subsequent convergence and generalization
guarantees resulting from the attention mechanism. This allows us to
theoretically establish that input-agnostic sparse attention does not provide
any benefits. We also characterize conditions when semantic focus
(input-dependent sparse attention) can provide improved guarantees, and we
validate that these conditions are in fact met in our empirical evaluations.

</details>


### [83] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang,Jie Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的图基础模型，通过随机游走表示节点，并开发了新的上下文预测损失函数，展示了其在图数据处理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索能否为图数据构建类似自然语言的基础模型，以解决图数据多样性和规模变化的挑战。

Method: 使用随机游走表示节点，通过Transformer提取节点、边和图表示，并开发了新的上下文预测损失函数。

Result: 模型能够有效区分邻域和图结构，并在下游任务中表现出潜力。

Conclusion: 该模型为图数据处理和推理提供了一种有前景的基础框架。

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the
pre-training with broad inclusion of data and the use of the powerful
Transformer architecture. While foundation models in natural languages are
prevalent, can we build similar models for graphs? This paper describes an
approach toward a graph foundation model that is pre-trained with diverse graph
datasets by adapting the Transformer backbone. A central challenge toward this
end is how a sequence model encodes graphs of varying sizes and from different
domains. We propose representing a node as multiple random walks, such that the
Transformer can extract node representations from sequences, which in turn form
edge and graph representations. We develop a novel context prediction loss for
these random walks and theoretically analyze their expressive power in
distinguishing neighborhoods and graphs. We also demonstrate the pre-training
of our model and its adaptation to downstream tasks, showcasing its potential
as a foundation for processing and reasoning with graph-structured data.

</details>


### [84] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang,Liheng Ma,Antonios Valkanas,Boris N. Oreshkin,Mark Coates*

Main category: cs.LG

TL;DR: 论文提出了一种基于Koopman算子理论和线性RNN的方法SKOLR，用于非线性动态系统分析和时间序列预测。


<details>
  <summary>Details</summary>
Motivation: Koopman算子理论虽能线性化非线性动态系统，但其无限维特性限制了实际应用，因此需要学习测量函数以获得有限维近似。

Method: 通过扩展状态（滞后观测）建立Koopman算子与线性RNN的等价性，并设计SKOLR方法，结合可学习谱分解和多层感知机。

Result: 在多个预测基准和动态系统上的实验表明，SKOLR表现优异。

Conclusion: 基于Koopman理论的SKOLR设计在性能上具有显著优势。

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system
analysis and time-series forecasting by mapping dynamics to a space of
real-valued measurement functions, enabling a linear operator representation.
Despite the advantage of linearity, the operator is generally
infinite-dimensional. Therefore, the objective is to learn measurement
functions that yield a tractable finite-dimensional Koopman operator
approximation. In this work, we establish a connection between Koopman operator
approximation and linear Recurrent Neural Networks (RNNs), which have recently
demonstrated remarkable success in sequence modeling. We show that by
considering an extended state consisting of lagged observations, we can
establish an equivalence between a structured Koopman operator and linear RNN
updates. Building on this connection, we present SKOLR, which integrates a
learnable spectral decomposition of the input signal with a multilayer
perceptron (MLP) as the measurement functions and implements a structured
Koopman operator via a highly parallel linear RNN stack. Numerical experiments
on various forecasting benchmarks and dynamical systems show that this
streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [85] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas,Ruizhe Hou,Zhou Wengang,Dong Shi,Niu Ling,Satyaki Nan,Alireza Abbasi*

Main category: cs.LG

TL;DR: 该论文通过大规模评估研究了GNN模型与多种损失函数的组合效果，发现混合损失函数在归纳任务中表现更优，GIN架构表现最佳，而MPNN架构表现较差。


<details>
  <summary>Details</summary>
Motivation: 研究GNN模型与损失函数的组合效果，填补了大规模评估的空白。

Method: 评估了7种GNN架构和30种损失函数的组合，涵盖归纳和传导任务，使用3个真实数据集和21个评估指标。

Result: 混合损失函数表现更优，GIN架构表现最佳，MPNN架构表现较差。

Conclusion: 混合损失函数和多目标优化对GNN性能提升显著，GIN架构是通用选择，而特定任务需考虑其他架构。

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean
data. However, their best performance depends on choosing the right model
architecture and the training objective, also called the loss function.
Researchers have studied these parts separately, but a large-scale evaluation
has not looked at how GNN models and many loss functions work together across
different tasks. To fix this, we ran a thorough study - it included seven
well-known GNN architectures. We also used a large group of 30 single plus
mixed loss functions. The study looked at both inductive and transductive
settings. Our evaluation spanned three distinct real-world datasets, assessing
performance in both inductive and transductive settings using 21 comprehensive
evaluation metrics. From these extensive results (detailed in supplementary
information 1 \& 2), we meticulously analyzed the top ten model-loss
combinations for each metric based on their average rank. Our findings reveal
that, especially for the inductive case: 1) Hybrid loss functions generally
yield superior and more robust performance compared to single loss functions,
indicating the benefit of multi-objective optimization. 2) The GIN architecture
always showed the highest-level average performance, especially with
Cross-Entropy loss. 3) Although some combinations had overall lower average
ranks, models such as GAT, particularly with certain hybrid losses,
demonstrated incredible specialized strengths, maximizing the most top-1
results among the individual metrics, emphasizing subtle strengths for
particular task demands. 4) On the other hand, the MPNN architecture typically
lagged behind the scenarios it was tested against.

</details>


### [86] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang,Renbo Zhang,Zhengyi Yang,Yunjun Gao,Bin Cao,Jing Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于对比学习的图神经网络（CLGNN），用于高效准确地预测时间中介中心性（TBC），解决了现有方法在处理数据不平衡和忽略时间依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 时间中介中心性（TBC）的计算成本高且分布极不平衡，导致学习模型容易过拟合，无法准确预测重要节点。现有图神经网络方法无法有效处理这些问题。

Method: CLGNN通过构建实例图保留路径有效性和时间顺序，使用双重聚合机制（均值和边到节点的多头注意力）编码结构和时间特征，并引入稳定性聚类对比模块（KContrastNet）和回归模块（ValueNet）来缓解类别不平衡问题。

Result: 实验表明，CLGNN在速度和准确性上显著优于现有方法，最高提速663.7倍，MAE降低31.4倍，Spearman相关性提高16.7倍。

Conclusion: CLGNN是一种高效且可扩展的方法，能够准确预测TBC，适用于多种时间语义场景。

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on
optimal temporal paths, reflecting its importance in temporal networks.
However, exact computation is highly expensive, and real-world TBC
distributions are extremely imbalanced. The severe imbalance leads
learning-based models to overfit to zero-centrality nodes, resulting in
inaccurate TBC predictions and failure to identify truly central nodes.
Existing graph neural network (GNN) methods either fail to handle such
imbalance or ignore temporal dependencies altogether. To address these issues,
we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for
accurate and efficient TBC prediction. CLGNN builds an instance graph to
preserve path validity and temporal order, then encodes structural and temporal
features using dual aggregation, i.e., mean and edge-to-node multi-head
attention mechanisms, enhanced by temporal path count and time encodings. A
stability-based clustering-guided contrastive module (KContrastNet) is
introduced to separate high-, median-, and low-centrality nodes in
representation space, mitigating class imbalance, while a regression module
(ValueNet) estimates TBC values. CLGNN also supports multiple optimal path
definitions to accommodate diverse temporal semantics. Extensive experiments
demonstrate the effectiveness and efficiency of CLGNN across diverse
benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to
state-of-the-art exact TBC computation methods. It outperforms leading static
GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher
Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to
5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [87] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi,Guy Wolf,Eugene Belilovsky,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 研究发现，专家模型的长时间微调会降低模型合并性能，并提出早期停止策略以改善结果。


<details>
  <summary>Details</summary>
Motivation: 挑战现有假设，即改进模型管道的某一阶段会带来下游增益，研究专家微调对模型升级的影响。

Method: 分析专家微调和LoRA适配器对模型合并性能的影响，并提出任务依赖的早期停止策略。

Result: 长时间微调导致合并性能下降，早期停止策略显著提升升级性能。

Conclusion: 任务依赖的早期停止策略可有效优化模型升级性能。

Abstract: Modern deep learning is increasingly characterized by the use of open-weight
foundation models that can be fine-tuned on specialized datasets. This has led
to a proliferation of expert models and adapters, often shared via platforms
like HuggingFace and AdapterHub. To leverage these resources, numerous model
upcycling methods have emerged, enabling the reuse of fine-tuned models in
multi-task systems. A natural pipeline has thus formed to harness the benefits
of transfer learning and amortize sunk training costs: models are pre-trained
on general data, fine-tuned on specific tasks, and then upcycled into more
general-purpose systems. A prevailing assumption is that improvements at one
stage of this pipeline propagate downstream, leading to gains at subsequent
steps. In this work, we challenge that assumption by examining how expert
fine-tuning affects model upcycling. We show that long fine-tuning of experts
that optimizes for their individual performance leads to degraded merging
performance, both for fully fine-tuned and LoRA-adapted models, and to worse
downstream results when LoRA adapters are upcycled into MoE layers. We trace
this degradation to the memorization of a small set of difficult examples that
dominate late fine-tuning steps and are subsequently forgotten during merging.
Finally, we demonstrate that a task-dependent aggressive early stopping
strategy can significantly improve upcycling performance.

</details>


### [88] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish,Zachery Boner,Jon Donnelly,Margo Seltzer,Cynthia Rudin*

Main category: cs.LG

TL;DR: 决策树因其清晰的结构被广泛用于可解释的机器学习，但存在预测等价性问题，即多个决策树可能具有相同的决策边界但不同的评估过程。本文提出了一种布尔逻辑表示方法，解决了预测等价性问题，并应用于多个下游任务。


<details>
  <summary>Details</summary>
Motivation: 决策树的结构虽然清晰，但其预测等价性导致模型选择困难，影响变量重要性和缺失值处理。本文旨在解决这一问题。

Method: 提出了一种布尔逻辑表示方法，避免预测等价性，并应用于多个机器学习任务。

Result: 实验表明，决策树对特征值缺失具有鲁棒性，解决了变量重要性量化问题，并优化了预测成本。

Conclusion: 布尔逻辑表示方法有效解决了预测等价性问题，提升了决策树在多个任务中的表现。

Abstract: Decision trees are widely used for interpretable machine learning due to
their clearly structured reasoning process. However, this structure belies a
challenge we refer to as predictive equivalence: a given tree's decision
boundary can be represented by many different decision trees. The presence of
models with identical decision boundaries but different evaluation processes
makes model selection challenging. The models will have different variable
importance and behave differently in the presence of missing values, but most
optimization procedures will arbitrarily choose one such model to return. We
present a boolean logical representation of decision trees that does not
exhibit predictive equivalence and is faithful to the underlying decision
boundary. We apply our representation to several downstream machine learning
tasks. Using our representation, we show that decision trees are surprisingly
robust to test-time missingness of feature values; we address predictive
equivalence's impact on quantifying variable importance; and we present an
algorithm to optimize the cost of reaching predictions.

</details>


### [89] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour,Kiarash Aghakasiri,Sandra Zilles,Levi H. S. Lelis*

Main category: cs.LG

TL;DR: 论文指出，程序化策略在OOD问题上的泛化能力被低估，通过简单修改神经策略的训练流程（如简化架构和调整奖励函数），神经策略也能达到类似效果。


<details>
  <summary>Details</summary>
Motivation: 探讨程序化表示在OOD问题上的泛化能力是否被低估，并验证神经策略通过调整也能实现类似泛化效果。

Method: 分析四篇文献的实验，修改神经策略的训练流程（简化架构、稀疏观察、调整奖励函数）。

Result: 神经策略通过简单修改也能在OOD问题上泛化得与程序化策略一样好。

Conclusion: 提出需要设计更挑战神经策略的基准问题，以更公平评估泛化能力。

Abstract: Algorithms for learning programmatic representations for sequential
decision-making problems are often evaluated on out-of-distribution (OOD)
problems, with the common conclusion that programmatic policies generalize
better than neural policies on OOD problems. In this position paper, we argue
that commonly used benchmarks undervalue the generalization capabilities of
programmatic representations. We analyze the experiments of four papers from
the literature and show that neural policies, which were shown not to
generalize, can generalize as effectively as programmatic policies on OOD
problems. This is achieved with simple changes in the neural policies training
pipeline. Namely, we show that simpler neural architectures with the same type
of sparse observation used with programmatic policies can help attain OOD
generalization. Another modification we have shown to be effective is the use
of reward functions that allow for safer policies (e.g., agents that drive
slowly can generalize better). Also, we argue for creating benchmark problems
highlighting concepts needed for OOD generalization that may challenge neural
policies but align with programmatic representations, such as tasks requiring
algorithmic constructs like stacks.

</details>


### [90] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

Main category: cs.LG

TL;DR: 本文研究了多智能体强化学习（MARL）在部分可观测、合作-竞争战斗环境LAG中的应用，比较了HAPPO和HASAC两种算法的性能。


<details>
  <summary>Details</summary>
Motivation: 探索在复杂战斗环境中，不同MARL算法的表现及其适用性。

Method: 使用HAPPO（基于PPO的分层方法）和HASAC（基于SAC的离线方法）在LAG环境中进行实验，分析训练稳定性、奖励进展和智能体协调能力。

Result: HASAC在无武器简单任务中表现良好，HAPPO在动态导弹战斗中更具适应性。

Conclusion: 研究揭示了在线和离线方法在多智能体环境中的权衡。

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a
partially observable, cooperative-competitive combat environment known as LAG.
We describe the environment's setup, including agent actions, hierarchical
controls, and reward design across different combat modes such as No Weapon and
ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy
hierarchical variant of PPO, and HASAC, an off-policy method based on soft
actor-critic. We analyze their training stability, reward progression, and
inter-agent coordination capabilities. Experimental results show that HASAC
performs well in simpler coordination tasks without weapons, while HAPPO
demonstrates stronger adaptability in more dynamic and expressive scenarios
involving missile combat. These findings provide insights into the trade-offs
between on-policy and off-policy methods in multi-agent settings.

</details>


### [91] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

Main category: cs.LG

TL;DR: 论文将Kolmogorov-Arnold表示定理应用于生成建模，通过逆变换采样将其内部函数重新解释为概率空间之间的马尔可夫核，提出了一种可解释、易设计且高效的生成模型。


<details>
  <summary>Details</summary>
Motivation: 将经典表示定理与现代概率建模结合，解决生成模型在灵活性、训练效率和样本质量之间的平衡问题。

Method: 采用Kolmogorov-Arnold网络生成器与独立能量基先验结合，通过最大似然训练，并引入混合分布和Langevin Monte Carlo方法提升灵活性。

Result: 模型实现了快速推理、高效学习及高质量样本生成，同时支持先验的可视化和后验对齐。

Conclusion: 该方法在训练稳定性、推理速度和生成多样性之间取得了平衡，为生成建模提供了新思路。

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling
by reinterpreting its inner functions as a Markov Kernel between probability
spaces via inverse transform sampling. We present a generative model that is
interpretable, easy to design, and efficient. Our approach couples a
Kolmogorov-Arnold Network generator with independent energy-based priors,
trained via Maximum Likelihood. Inverse sampling enables fast inference, while
prior knowledge can be incorporated before training to better align priors with
posteriors, thereby improving learning efficiency and sample quality. The
learned prior is also recoverable and visualizable post-training, offering an
empirical Bayes perspective. To address inflexibility and mitigate
prior-posterior mismatch, we introduce scalable extensions based on mixture
distributions and Langevin Monte Carlo methods, admitting a trade-off between
flexibility and training efficiency. Our contributions connect classical
representation theorems with modern probabilistic modeling, while balancing
training stability, inference speed, and the quality and diversity of
generations.

</details>


### [92] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal,Zhuolin Jiang,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的OOD检测特征构建理论，通过KL散度和信息瓶颈优化损失函数，生成优于现有方法的特征。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中OOD检测特征构建的理论基础问题，提升检测性能。

Method: 引入信息论损失函数，结合KL散度和信息瓶颈，通过变分优化生成OOD特征。

Result: 理论预测的新特征在OOD基准测试中优于现有方法。

Conclusion: 该理论为构建可解释的OOD特征提供了通用框架。

Abstract: We present a theory for the construction of out-of-distribution (OOD)
detection features for neural networks. We introduce random features for OOD
through a novel information-theoretic loss functional consisting of two terms,
the first based on the KL divergence separates resulting in-distribution (ID)
and OOD feature distributions and the second term is the Information
Bottleneck, which favors compressed features that retain the OOD information.
We formulate a variational procedure to optimize the loss and obtain OOD
features. Based on assumptions on OOD distributions, one can recover properties
of existing OOD features, i.e., shaping functions. Furthermore, we show that
our theory can predict a new shaping function that out-performs existing ones
on OOD benchmarks. Our theory provides a general framework for constructing a
variety of new features with clear explainability.

</details>


### [93] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing,Takuya Akiba*

Main category: cs.LG

TL;DR: DiffusionBlocks是一种新型训练框架，通过将神经网络块解释为连续时间扩散过程中的去噪操作，显著减少内存使用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统反向传播训练大型神经网络时的内存瓶颈问题，使更多人能够访问最先进的AI研究。

Method: 将网络划分为独立可训练的块，并根据等累积概率质量优化噪声级别分配。

Result: 在图像生成和语言建模任务中，内存减少与块数成正比，同时性能优于传统方法。

Conclusion: DiffusionBlocks为资源有限的大规模神经网络训练提供了可行方案。

Abstract: Training large neural networks with end-to-end backpropagation creates
significant memory bottlenecks, limiting accessibility to state-of-the-art AI
research. We propose $\textit{DiffusionBlocks}$, a novel training framework
that interprets neural network blocks as performing denoising operations in a
continuous-time diffusion process. By partitioning the network into
independently trainable blocks and optimizing noise level assignments based on
equal cumulative probability mass, our approach achieves significant memory
efficiency while maintaining competitive performance compared to traditional
backpropagation in generative tasks. Experiments on image generation and
language modeling tasks demonstrate memory reduction proportional to the number
of blocks while achieving superior performance. DiffusionBlocks provides a
promising pathway for democratizing access to large-scale neural network
training with limited computational resources.

</details>


### [94] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato,Rohan Poudel,Pramod Dhungana*

Main category: cs.LG

TL;DR: TriGuard是一个统一的安全评估框架，结合形式化鲁棒性验证、归因熵和归因漂移评分，揭示模型准确性与可解释性之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在对抗性和分布偏移下的可靠性仍是一个紧迫挑战，需要更全面的安全评估方法。

Method: TriGuard结合形式化鲁棒性验证、归因熵和归因漂移评分，评估模型的安全性和可解释性。

Result: 实验表明，TriGuard能揭示神经网络推理中的脆弱性，且熵正则化训练可减少解释漂移而不影响性能。

Conclusion: TriGuard推动了鲁棒且可解释的模型评估的前沿。

Abstract: Deep neural networks often achieve high accuracy, but ensuring their
reliability under adversarial and distributional shifts remains a pressing
challenge. We propose TriGuard, a unified safety evaluation framework that
combines (1) formal robustness verification, (2) attribution entropy to
quantify saliency concentration, and (3) a novel Attribution Drift Score
measuring explanation stability. TriGuard reveals critical mismatches between
model accuracy and interpretability: verified models can still exhibit unstable
reasoning, and attribution-based signals provide complementary safety insights
beyond adversarial accuracy. Extensive experiments across three datasets and
five architectures show how TriGuard uncovers subtle fragilities in neural
reasoning. We further demonstrate that entropy-regularized training reduces
explanation drift without sacrificing performance. TriGuard advances the
frontier in robust, interpretable model evaluation.

</details>


### [95] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu,Yanhua Yu,Zhiyong Huang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 本文提出一种利用大型语言模型（LLMs）估计图同质性的方法，以改进谱图神经网络（SGNNs）的性能。


<details>
  <summary>Details</summary>
Motivation: 在标签稀缺条件下，SGNNs可能学习到次优滤波器，导致性能下降。LLMs的成功启发探索其在GNN领域的潜力。

Method: 利用LLMs估计图的同质性，并以此指导多项式谱滤波器的设计，提升SGNNs的表达能力和适应性。

Result: 实验表明，该方法在多种图结构下均优于现有基线，且计算和成本开销极小。

Conclusion: LLMs可以有效增强SGNNs的性能，尤其在标签稀缺条件下。

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention
due to their ability to approximate arbitrary filters. They typically rely on
supervision from downstream tasks to adaptively learn appropriate filters.
However, under label-scarce conditions, SGNNs may learn suboptimal filters,
leading to degraded performance. Meanwhile, the remarkable success of Large
Language Models (LLMs) has inspired growing interest in exploring their
potential within the GNN domain. This naturally raises an important question:
\textit{Can LLMs help overcome the limitations of SGNNs and enhance their
performance?} In this paper, we propose a novel approach that leverages LLMs to
estimate the homophily of a given graph. The estimated homophily is then used
to adaptively guide the design of polynomial spectral filters, thereby
improving the expressiveness and adaptability of SGNNs across diverse graph
structures. Specifically, we introduce a lightweight pipeline in which the LLM
generates homophily-aware priors, which are injected into the filter
coefficients to better align with the underlying graph topology. Extensive
experiments on benchmark datasets demonstrate that our LLM-driven SGNN
framework consistently outperforms existing baselines under both homophilic and
heterophilic settings, with minimal computational and monetary overhead.

</details>


### [96] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta,Erik Jenner*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）是否能够通过学习规避潜在空间监视器，并提出了RL-Obfuscation方法。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否能够通过学习规避潜在空间监视器，以评估监视器的鲁棒性。

Method: 使用强化学习（RL）微调LLM，使其绕过潜在空间监视器，同时保持生成的连贯性。

Result: 发现基于token的潜在空间监视器易受攻击，而更全面的监视器（如max-pooling或基于注意力的探针）更鲁棒。

Conclusion: LLM可以通过学习规避某些监视器，但更全面的监视方法仍能保持鲁棒性。

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language
models by leveraging internal model representations rather than relying solely
on black-box outputs. These methods have shown promise in identifying
behaviours such as deception and unsafe completions, but a critical open
question remains: can LLMs learn to evade such monitors? To study this, we
introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement
learning to bypass latent-space monitors while maintaining coherent
generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters
and evaluate evasion success against a suite of monitors. We find that
token-level latent-space monitors are highly vulnerable to this attack. More
holistic monitors, such as max-pooling or attention-based probes, remain
robust. Moreover, we show that adversarial policies trained to evade a single
static monitor generalise to unseen monitors of the same type. Finally, we
study how the policy learned by RL bypasses these monitors and find that the
model can also learn to repurpose tokens to mean something different
internally.

</details>


### [97] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 论文探讨了机器如何像人类一样快速适应，提出所有适应方法均可视为对近似后验的“修正”，更准确的后验意味着更小的修正和更快的适应。


<details>
  <summary>Details</summary>
Motivation: 尽管AI模型在持续学习、联邦学习等方面取得进展，但其适应能力仍远不及人类和动物，因此研究机器如何自然快速适应成为关键问题。

Method: 采用贝叶斯学习规则的双重视角，通过自然梯度不匹配表征适应过程中的干扰，提出后验修正作为快速适应的自然机制。

Result: 研究表明，更准确的后验会导致更小的修正，从而实现更快的适应。

Conclusion: 后验修正为机器快速适应提供了一种自然机制，为未来研究提供了新方向。

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models
(like GPT) lack the adaptivity of toddlers. So the question remains: how can
machines adapt quickly? Despite a lot of progress on model adaptation to
facilitate continual and federated learning, as well as model merging, editing,
unlearning, etc., little is known about the mechanisms by which machines can
naturally learn to adapt in a similar way as humans and animals. Here, we show
that all such adaptation methods can be seen as different ways of `correcting'
the approximate posteriors. More accurate posteriors lead to smaller
corrections, which in turn imply quicker adaptation. The result is obtained by
using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023)
where interference created during adaptation is characterized by the
natural-gradient mismatch over the past data. We present many examples to
demonstrate the use of posterior-correction as a natural mechanism for the
machines to learn to adapt quickly.

</details>


### [98] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song,Wei Lin,Juncheng Wang,Hong Xu*

Main category: cs.LG

TL;DR: 本文提出了一种学习优化（L2O）方法，通过理论证明其在分布外（OOD）场景中的性能和鲁棒性，并提出了一种新的梯度特征构建和历史建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有L2O方法在分布外（OOD）场景中缺乏理论性能证明，本文旨在填补这一空白。

Method: 提出了一种梯度特征构建方法和梯度历史建模方法，并通过理论证明其在InD和OOD场景中的性能。

Result: 数值模拟表明，该方法在InD和OOD场景中均优于现有基线，收敛速度提升高达10倍。

Conclusion: 本文为L2O方法提供了理论支持，并展示了其在复杂场景中的优越性能。

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical
optimization problems with learning-based methods. Although with great success
in many real-world scenarios such as wireless communications, computer
networks, and electronic design, existing L2O works lack theoretical
demonstration of their performance and robustness in out-of-distribution (OOD)
scenarios. We address this gap by providing comprehensive proofs. First, we
prove a sufficient condition for a robust L2O model with homogeneous
convergence rates over all In-Distribution (InD) instances. We assume an L2O
model achieves robustness for an InD scenario. Based on our proposed
methodology of aligning OOD problems to InD problems, we also demonstrate that
the L2O model's convergence rate in OOD scenarios will deteriorate by an
equation of the L2O model's input features. Moreover, we propose an L2O model
with a concise gradient-only feature construction and a novel gradient-based
history modeling method. Numerical simulation demonstrates that our proposed
model outperforms the state-of-the-art baseline in both InD and OOD scenarios
and achieves up to 10 $\times$ convergence speedup. The code of our method can
be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [99] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong,Nico Daheim,Yuesong Shen,Rio Yokota,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: IVON算法显著提升了LoRA微调的效果，优于AdamW和其他贝叶斯方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法在LoRA微调中虽能改善校准，但对其他指标（如准确率）影响有限甚至有害，且计算开销大。

Method: 采用IVON变分算法，结合后验剪枝技术，实现高效且效果显著的微调。

Result: 在Llama和Qwen等大规模LLM上，IVON显著提升准确率（如Llama-3.2-3B提升1.3%）并降低ECE（5.4%）。

Conclusion: IVON能有效改进LoRA微调，兼具高效性和性能优势。

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and,
although they improve calibration, their effect on other metrics (such as
accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian
methods also increase computational overheads and require additional tricks for
them to work well. Here, we fix these issues by using a recently proposed
variational algorithm called IVON. We show that IVON is easy to implement and
has similar costs to AdamW, and yet it can also drastically improve many
metrics by using a simple posterior pruning technique. We present extensive
results on billion-scale LLMs (Llama and Qwen series) going way beyond the
scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B
model on a set of commonsense reasoning tasks and improve accuracy over AdamW
by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian
methods like Laplace-LoRA and BLoB. Overall, our results show that variational
learning with IVON can effectively improve LoRA finetuning.

</details>


### [100] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein,İsmail İlkan Ceylan,Michael Bronstein,Ron Levie*

Main category: cs.LG

TL;DR: 论文提出了一种设计图基础模型的方法，用于节点级任务，通过研究对称性实现跨图泛化。


<details>
  <summary>Details</summary>
Motivation: 解决图机器学习模型局限于特定任务和数据集的问题，探索如何构建通用的图基础模型。

Method: 系统研究对称性，提出标签置换等变性和特征置换不变性，构建线性变换层并证明其普适性。

Result: 在29个真实节点分类数据集上验证，展示了零样本性能和训练图数量增加时的持续改进。

Conclusion: 提出的方法为构建通用的图基础模型提供了有效途径，具有广泛的应用潜力。

Abstract: Graph machine learning architectures are typically tailored to specific tasks
on specific datasets, which hinders their broader applicability. This has led
to a new quest in graph machine learning: how to build graph foundation models
capable of generalizing across arbitrary graphs and features? In this work, we
present a recipe for designing graph foundation models for node-level tasks
from first principles. The key ingredient underpinning our study is a
systematic investigation of the symmetries that a graph foundation model must
respect. In a nutshell, we argue that label permutation-equivariance alongside
feature permutation-invariance are necessary in addition to the common node
permutation-equivariance on each local neighborhood of the graph. To this end,
we first characterize the space of linear transformations that are equivariant
to permutations of nodes and labels, and invariant to permutations of features.
We then prove that the resulting network is a universal approximator on
multisets that respect the aforementioned symmetries. Our recipe uses such
layers on the multiset of features induced by the local neighborhood of the
graph to obtain a class of graph foundation models for node property
prediction. We validate our approach through extensive experiments on 29
real-world node classification datasets, demonstrating both strong zero-shot
empirical performance and consistent improvement as the number of training
graphs increases.

</details>


### [101] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin,Asli Umay Ozturk,Yigit Sever,Viktoria Pauw,Stephan Hachinger,Ismail Hakki Toroslu,Pinar Karagoz*

Main category: cs.LG

TL;DR: 论文探讨了在双重不平衡数据集（标签和敏感属性均不平衡）中实现公平性的挑战，并提出了一种多标准解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的公平性是一个重要问题，但现有方法在数据不平衡时表现不佳，特别是在双重不平衡数据集中。

Method: 通过探索性分析揭示现有方法的局限性，并提出一种基于多标准的采样和分布优化方案。

Result: 提出的方法在公平性和分类准确性方面优于现有方法。

Conclusion: 多标准解决方案能有效提升双重不平衡数据集中的公平性和准确性。

Abstract: Fairness has been identified as an important aspect of Machine Learning and
Artificial Intelligence solutions for decision making. Recent literature offers
a variety of approaches for debiasing, however many of them fall short when the
data collection is imbalanced. In this paper, we focus on a particular case,
fairness in doubly imbalanced datasets, such that the data collection is
imbalanced both for the label and the groups in the sensitive attribute.
Firstly, we present an exploratory analysis to illustrate limitations in
debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution
is proposed for finding the most suitable sampling and distribution for label
and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [102] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf,Jason Li,Sahar Vahdati,Raphael Theilen,Jakob Wittenstein,Jens Lehmann*

Main category: cs.LG

TL;DR: 论文提出了一种优化离线强化学习（RL）的方法，用于解决机械通气（MV）控制中混合动作空间的挑战，避免了离散化的缺陷，并引入基于临床目标的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 机械通气（MV）设置优化复杂且易出错，现有离线RL方法难以处理混合动作空间的问题。

Method: 通过优化动作空间减少技术，并改进离线RL算法（IQL和EDAC）以直接处理混合动作空间，同时设计基于临床目标的奖励函数。

Result: 研究表明，AI辅助的MV优化可提升患者安全性和个体化治疗，为智能重症监护提供新方向。

Conclusion: 该方法为数据驱动的重症监护解决方案提供了重要进展。

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for
critically ill patients in the intensive care unit (ICU). However, optimizing
its settings remains a complex and error-prone process due to patient-specific
variability. While Offline Reinforcement Learning (RL) shows promise for MV
control, current stateof-the-art (SOTA) methods struggle with the hybrid
(continuous and discrete) nature of MV actions. Discretizing the action space
limits available actions due to exponential growth in combinations and
introduces distribution shifts that can compromise safety. In this paper, we
propose optimizations that build upon prior work in action space reduction to
address the challenges of discrete action spaces. We also adapt SOTA offline RL
algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby
avoiding the pitfalls of discretization. Additionally, we introduce a
clinically grounded reward function based on ventilator-free days and
physiological targets, which provides a more meaningful optimization objective
compared to traditional sparse mortality-based rewards. Our findings
demonstrate that AI-assisted MV optimization may enhance patient safety and
enable individualized lung support, representing a significant advancement
toward intelligent, data-driven critical care solutions.

</details>


### [103] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel,Michael Wand*

Main category: cs.LG

TL;DR: 残差连接在现代神经网络中广泛使用，其优势不仅在于优化，还在于其独特的函数空间。


<details>
  <summary>Details</summary>
Motivation: 探索残差连接为何在性能上优于前馈网络，并提出其可能源于不同的函数空间而非仅仅是优化优势。

Method: 设计了一个后训练对比实验，以分离泛化性能和可训练性的影响。

Result: 变深度架构（如ResNets）在泛化性能上始终优于固定深度网络。

Conclusion: 残差连接的性能优势源于其与自然数据结构的更深层次归纳偏差。

Abstract: Residual connections remain ubiquitous in modern neural network architectures
nearly a decade after their introduction. Their widespread adoption is often
credited to their dramatically improved trainability: residual networks train
faster, more stably, and achieve higher accuracy than their feedforward
counterparts. While numerous techniques, ranging from improved initialization
to advanced learning rate schedules, have been proposed to close the
performance gap between residual and feedforward networks, this gap has
persisted. In this work, we propose an alternative explanation: residual
networks do not merely reparameterize feedforward networks, but instead inhabit
a different function space. We design a controlled post-training comparison to
isolate generalization performance from trainability; we find that
variable-depth architectures, similar to ResNets, consistently outperform
fixed-depth networks, even when optimization is unlikely to make a difference.
These results suggest that residual connections confer performance advantages
beyond optimization, pointing instead to a deeper inductive bias aligned with
the structure of natural data.

</details>


### [104] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: 论文提出了一种结合原型变分模型和自编码器的OOD检测方法，通过定义紧凑的ID区域和改进的分类性能，提升了模型的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了在安全相关应用中采用深度机器学习模型，需要理解其决策过程并确保其可靠性。

Method: 使用变分自编码器学习潜在空间，结合高斯混合分布定义ID区域，并引入限制损失以优化潜在空间的紧凑性。

Result: 在常见OOD检测基准和实际铁路数据集上表现优异，超越了现有方法。

Conclusion: 该方法通过提升模型的可解释性和OOD检测能力，为安全相关应用提供了可靠支持。

Abstract: Understanding the decision-making and trusting the reliability of Deep
Machine Learning Models is crucial for adopting such methods to safety-relevant
applications. We extend self-explainable Prototypical Variational models with
autoencoder-based out-of-distribution (OOD) detection: A Variational
Autoencoder is applied to learn a meaningful latent space which can be used for
distance-based classification, likelihood estimation for OOD detection, and
reconstruction. The In-Distribution (ID) region is defined by a Gaussian
mixture distribution with learned prototypes representing the center of each
mode. Furthermore, a novel restriction loss is introduced that promotes a
compact ID region in the latent space without collapsing it into single points.
The reconstructive capabilities of the Autoencoder ensure the explainability of
the prototypes and the ID region of the classifier, further aiding the
discrimination of OOD samples. Extensive evaluations on common OOD detection
benchmarks as well as a large-scale dataset from a real-world railway
application demonstrate the usefulness of the approach, outperforming previous
methods.

</details>


### [105] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu,Hongkai Wen,Geyong Min,Man Luo*

Main category: cs.LG

TL;DR: HiLight是一个分层强化学习框架，用于大规模交通信号控制，通过全局对抗指导实现高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大规模网络中难以兼顾全局协调和可扩展性，HiLight旨在解决这一问题。

Method: HiLight采用分层结构，包括高层的Meta-Policy（分区并生成子目标）和低层的Sub-Policy（控制单个交叉口），并通过对抗训练机制提升全局与局部协调。

Result: 实验表明，HiLight在大规模场景中表现优异，并在不同规模的基准测试中保持竞争力。

Conclusion: HiLight通过分层和对抗训练机制，有效解决了大规模交通信号控制的协调问题。

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban
congestion, yet existing reinforcement learning (RL) methods face challenges in
scaling to large networks while maintaining global coordination. Centralized RL
suffers from scalability issues, while decentralized approaches often lack
unified objectives, resulting in limited network-level efficiency. In this
paper, we propose HiLight, a hierarchical reinforcement learning framework with
global adversarial guidance for large-scale TSC. HiLight consists of a
high-level Meta-Policy, which partitions the traffic network into subregions
and generates sub-goals using a Transformer-LSTM architecture, and a low-level
Sub-Policy, which controls individual intersections with global awareness. To
improve the alignment between global planning and local execution, we introduce
an adversarial training mechanism, where the Meta-Policy generates challenging
yet informative sub-goals, and the Sub-Policy learns to surpass these targets,
leading to more effective coordination. We evaluate HiLight across both
synthetic and real-world benchmarks, and additionally construct a large-scale
Manhattan network with diverse traffic conditions, including peak transitions,
adverse weather, and holiday surges. Experimental results show that HiLight
exhibits significant advantages in large-scale scenarios and remains
competitive across standard benchmarks of varying sizes.

</details>


### [106] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller,Michael Hahn,Ajay Madhavan Ravichandran,Bilgin Osmanodja,Florian Oetke,Zeineb Sassi,Aljoscha Burchardt,Klaus Netter,Klemens Budde,Anne Herrmann,Tobias Strapatsas,Peter Dabrock,Sebastian Möller*

Main category: cs.LG

TL;DR: 该论文探讨了机器学习模型在临床决策中的应用，强调了对患者子群体性能差异的分析，以确保公平性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的医疗数据集通常存在噪声、不完整和不平衡的问题，导致模型在不同患者子群体中的性能差异，可能加剧边缘化群体的不利处境。

Method: 通过分析多个医疗预测任务，研究模型性能如何随患者特征变化，并进行子群体级别的性能评估。

Result: 研究发现，尽管模型整体性能良好，但子群体间存在显著差异，需在临床实践中考虑这些差异。

Conclusion: 子群体敏感的分析有助于开发更公平、透明的医疗机器学习模型，促进公平性和透明度的讨论。

Abstract: Machine learning (ML) models are increasingly used to support clinical
decision-making. However, real-world medical datasets are often noisy,
incomplete, and imbalanced, leading to performance disparities across patient
subgroups. These differences raise fairness concerns, particularly when they
reinforce existing disadvantages for marginalized groups. In this work, we
analyze several medical prediction tasks and demonstrate how model performance
varies with patient characteristics. While ML models may demonstrate good
overall performance, we argue that subgroup-level evaluation is essential
before integrating them into clinical workflows. By conducting a performance
analysis at the subgroup level, differences can be clearly identified-allowing,
on the one hand, for performance disparities to be considered in clinical
practice, and on the other hand, for these insights to inform the responsible
development of more effective models. Thereby, our work contributes to a
practical discussion around the subgroup-sensitive development and deployment
of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [107] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman,Alexandre Proutiere,David Broman*

Main category: cs.LG

TL;DR: 论文提出了一种名为“交互层”的通用框架，用于处理强化学习中不可观测且随时间变化的延迟问题，并基于此开发了ACDA算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实动态环境中的交互延迟问题在传统强化学习中未被充分解决，现有方法通常保守地假设延迟有固定上限。

Method: 引入交互层框架，生成未来动作矩阵以应对不可预测的延迟和丢包问题，并开发了ACDA算法动态适应延迟模式。

Result: ACDA算法在多种运动基准环境中显著优于现有方法。

Conclusion: 交互层框架和ACDA算法为处理强化学习中的延迟问题提供了有效解决方案。

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the
agent and the environment is typically modeled as a Markov Decision Process
(MDP), which assumes that the agent observes the system state instantaneously,
selects an action without delay, and executes it immediately. In real-world
dynamic environments, such as cyber-physical systems, this assumption often
breaks down due to delays in the interaction between the agent and the system.
These delays can vary stochastically over time and are typically unobservable,
meaning they are unknown when deciding on an action. Existing methods deal with
this uncertainty conservatively by assuming a known fixed upper bound on the
delay, even if the delay is often much lower. In this work, we introduce the
interaction layer, a general framework that enables agents to adaptively and
seamlessly handle unobservable and time-varying delays. Specifically, the agent
generates a matrix of possible future actions to handle both unpredictable
delays and lost action packets sent over networks. Building on this framework,
we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA),
which dynamically adjusts to delay patterns. Our method significantly
outperforms state-of-the-art approaches across a wide range of locomotion
benchmark environments.

</details>


### [108] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao,Jiakun Zheng,Rushuai Yang,Kang Xu,Qiaosheng Zhang,Peng Liu,Chenjia Bai*

Main category: cs.LG

TL;DR: 提出了一种新的技能发现目标，通过最大化技能间状态密度的差异，结合条件自编码器和内在奖励，提升了无监督强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决熵探索在大规模状态空间中的不足以及互信息估计在状态探索中的局限性。

Method: 提出最大化技能间状态密度差异的目标，使用条件自编码器进行状态密度估计，并设计内在奖励促进技能内探索。

Result: 在复杂任务中学习到有意义技能，并在下游任务中表现优异。

Conclusion: 新方法有效提升了无监督强化学习的技能多样性和探索能力。

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors
that can accelerate the learning of downstream tasks. Previous methods
typically focus on entropy-based exploration or empowerment-driven skill
learning. However, entropy-based exploration struggles in large-scale state
spaces (e.g., images), and empowerment-based methods with Mutual Information
(MI) estimations have limitations in state exploration. To address these
challenges, we propose a novel skill discovery objective that maximizes the
deviation of the state density of one skill from the explored regions of other
skills, encouraging inter-skill state diversity similar to the initial MI
objective. For state-density estimation, we construct a novel conditional
autoencoder with soft modularization for different skill policies in
high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we
formulate an intrinsic reward based on the learned autoencoder that resembles
count-based exploration in a compact latent space. Through extensive
experiments in challenging state and image-based tasks, we find our method
learns meaningful skills and achieves superior performance in various
downstream tasks.

</details>


### [109] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出了一种名为MoORE的新方法，通过SVD和可学习路由器调整预训练模型的权重矩阵，解决多任务适应中的任务冲突和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模基础模型在多任务场景中的任务冲突和遗忘问题。

Method: 使用SVD分解权重矩阵，引入可学习路由器调整奇异值，形成正交秩一专家混合（MoORE）。

Result: 在多个数据集上优于现有方法，表现出对任务冲突和遗忘的抵抗能力。

Conclusion: MoORE方法在多任务适应中具有优越性，代码已开源。

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers
from task conflict and oblivion. To mitigate such issues, we propose a novel
''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant
multi-task adaptation method. Given a weight matrix of a pre-trained model, our
method applies SVD to it and introduces a learnable router to adjust its
singular values based on tasks and samples. Accordingly, the weight matrix
becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert
corresponds to the outer product of a left singular vector and the
corresponding right one. We can improve the model capacity by imposing a
learnable orthogonal transform on the right singular vectors. Unlike low-rank
adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'
orthogonality and maintains the column space of the original weight matrix.
These two properties make the adapted model resistant to the conflicts among
the new tasks and the oblivion of its original tasks, respectively. Experiments
on various datasets demonstrate that MoORE outperforms existing multi-task
adaptation methods consistently, showing its superiority in terms of conflict-
and oblivion-resistance. The code of the experiments is available at
https://github.com/DaShenZi721/MoORE.

</details>


### [110] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo,Alexis Molina,Álvaro Ciudad*

Main category: cs.LG

TL;DR: 论文提出简化双曲神经网络中的关键操作，显著提升计算速度和性能，使其在更广泛应用中更具可行性。


<details>
  <summary>Details</summary>
Motivation: 双曲几何虽能有效建模复杂结构化数据，但现有双曲神经网络在计算效率和精度任务上存在性能挑战。

Method: 通过简化双曲神经网络中的关键操作。

Result: 实现了计算速度和预测准确率的显著提升。

Conclusion: 简化后的双曲神经网络在更广泛应用中更具潜力。

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex,
structured data, particularly where hierarchical or tree-like relationships are
present. By enabling embeddings with lower distortion, hyperbolic neural
networks offer promising alternatives to Euclidean-based models for capturing
intricate data structures. Despite these advantages, they often face
performance challenges, particularly in computational efficiency and tasks
requiring high precision. In this work, we address these limitations by
simplifying key operations within hyperbolic neural networks, achieving notable
improvements in both runtime and performance. Our findings demonstrate that
streamlined hyperbolic operations can lead to substantial gains in
computational speed and predictive accuracy, making hyperbolic neural networks
a more viable choice for a broader range of applications.

</details>


### [111] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi,Masahiro Asami,Kosuke Kawakami,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出了一种新方法HyPeR，用于在部分观测目标奖励的情况下，利用密集观测的次级奖励进行有效的离策略学习。


<details>
  <summary>Details</summary>
Motivation: 在部分观测目标奖励时，离策略学习效果下降，而次级奖励虽密集但可能与目标奖励不一致。

Method: 提出HyPeR方法，结合目标奖励和次级奖励进行优化。

Result: 实验证明HyPeR在多种场景下优于现有方法。

Conclusion: 利用次级奖励不仅优化目标奖励，还能提升整体性能。

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a
decision-making policy that maximizes the target rewards by using only
historical interaction data collected under previously developed policies.
Unfortunately, when rewards are only partially observed, the effectiveness of
OPL degrades severely. Well-known examples of such partial rewards include
explicit ratings in content recommendations, conversion signals on e-commerce
platforms that are partial due to delay, and the issue of censoring in medical
problems. One possible solution to deal with such partial rewards is to use
secondary rewards, such as dwelling time, clicks, and medical indicators, which
are more densely observed. However, relying solely on such secondary rewards
can also lead to poor policy learning since they may not align with the target
reward. Thus, this work studies a new and general problem of OPL where the goal
is to learn a policy that maximizes the expected target reward by leveraging
densely observed secondary rewards as supplemental data. We then propose a new
method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR),
which effectively uses the secondary rewards in addition to the
partially-observed target reward to achieve effective OPL despite the
challenging scenario. We also discuss a case where we aim to optimize not only
the expected target reward but also the expected secondary rewards to some
extent; counter-intuitively, we will show that leveraging the two objectives is
in fact advantageous also for the optimization of only the target reward. Along
with statistical analysis of our proposed methods, empirical evaluations on
both synthetic and real-world data show that HyPeR outperforms existing methods
in various scenarios.

</details>


### [112] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss,Amey Chaware,Maryam Roohian,Sarah Lemire,Oana-Maria Thoma,Birgitta Carlé,Maximilian Waldner,Sebastian Schürmann,Oliver Friedrich,Roarke Horstmeyer*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的无标记多光子显微镜（MPM）图像分类方法，用于识别免疫细胞类型，提升了无标记成像的特异性。


<details>
  <summary>Details</summary>
Motivation: 无标记成像避免了复杂的染色过程，尤其适用于体内应用。然而，多光子显微镜（MPM）的计算特异性尚未充分实现，本研究旨在通过深度学习填补这一空白。

Method: 使用卷积神经网络（CNN）对无标记MPM图像进行分类，数据集包含5,075个细胞用于二元分类和3,424个细胞用于六类分类。采用低复杂度的SqueezeNet架构。

Result: 模型在二元分类中表现优异（ROC-AUC 0.89，PR-AUC 0.95），六类分类中F1分数为0.689。扰动测试表明模型不受细胞外环境影响，且NADH和FAD通道对分类同等重要。

Conclusion: 该深度学习模型能够直接从无标记图像中检测特定免疫细胞，显著提升了MPM的特异性，未来在体内内窥镜中有广泛应用潜力。

Abstract: Label-free imaging has gained broad interest because of its potential to omit
elaborate staining procedures which is especially relevant for in vivo use.
Label-free multiphoton microscopy (MPM), for instance, exploits two-photon
excitation of natural autofluorescence (AF) from native, metabolic proteins,
making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been
widely used in other optical imaging technologies to predict specific target
annotations and thereby digitally augment the specificity of these label-free
images. However, this computational specificity has only rarely been
implemented for MPM. In this work, we used a data set of label-free MPM images
from a series of different immune cell types (5,075 individual cells for binary
classification in mixed samples and 3,424 cells for a multi-class
classification task) and trained a convolutional neural network (CNN) to
classify cell types based on this label-free AF as input. A low-complexity
squeezeNet architecture was able to achieve reliable immune cell classification
results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples;
0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class
classification in isolated samples). Perturbation tests confirmed that the
model is not confused by extracellular environment and that both input AF
channels (NADH and FAD) are about equally important to the classification. In
the future, such predictive DL models could directly detect specific immune
cells in unstained images and thus, computationally improve the specificity of
label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [113] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens,Lenka Zdeborová*

Main category: cs.LG

TL;DR: 研究发现，学生模型通过教师模型的软标签训练，即使未直接接触记忆数据，也能达到非平凡准确率。


<details>
  <summary>Details</summary>
Motivation: 探索在数据集蒸馏中，教师模型记忆的特定信息是否能通过软标签传递给学生模型。

Method: 使用有限随机独立同分布数据集，分析学生在未直接观察记忆数据时通过软标签学习的效果。

Result: 学生模型在某些情况下能完美匹配教师预测，包括未观察的记忆数据，且效果受温度参数影响。

Conclusion: 软标签传递记忆信息的效果显著，且受温度参数调控，适用于不同网络结构和数据集。

Abstract: Dataset distillation aims to compress training data into fewer examples via a
teacher, from which a student can learn effectively. While its success is often
attributed to structure in the data, modern neural networks also memorize
specific facts, but if and how such memorized information is can transferred in
distillation settings remains less understood. In this work, we show that
students trained on soft labels from teachers can achieve non-trivial accuracy
on held-out memorized data they never directly observed. This effect persists
on structured data when the teacher has not generalized.To analyze it in
isolation, we consider finite random i.i.d. datasets where generalization is a
priori impossible and a successful teacher fit implies pure memorization.
Still, students can learn non-trivial information about the held-out data, in
some cases up to perfect accuracy. In those settings, enough soft labels are
available to recover the teacher functionally - the student matches the
teacher's predictions on all possible inputs, including the held-out memorized
data. We show that these phenomena strongly depend on the temperature with
which the logits are smoothed, but persist across varying network capacities,
architectures and dataset compositions.

</details>


### [114] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed,Abdullah Al Noman,Mahin Montasir Afif,K. M. Tahsin Kabir,Md. Mostafizur Rahman,Mufti Mahmud*

Main category: cs.LG

TL;DR: 该研究提出了一种基于堆叠的集成学习方法，用于提高专业人士抑郁症分类的预测准确性，实验结果显示模型性能优异。


<details>
  <summary>Details</summary>
Motivation: 抑郁症在职业环境中是一个重要问题，传统分类方法难以应对其复杂性，因此需要更准确的预测模型。

Method: 采用堆叠集成学习方法，结合多个基础学习器和逻辑回归模型，利用Kaggle上的抑郁症专业数据集进行实验。

Result: 模型在训练和测试数据上的准确率分别达到99.64%和98.75%，其他指标（精确率、召回率、F1分数）均超过98%。

Conclusion: 集成学习在心理健康分析中表现优异，具有早期检测和干预的潜力。

Abstract: Depression is a significant mental health concern, particularly in
professional environments where work-related stress, financial pressure, and
lifestyle imbalances contribute to deteriorating well-being. Despite increasing
awareness, researchers and practitioners face critical challenges in developing
accurate and generalizable predictive models for mental health disorders.
Traditional classification approaches often struggle with the complexity of
depression, as it is influenced by multifaceted, interdependent factors,
including occupational stress, sleep patterns, and job satisfaction. This study
addresses these challenges by proposing a stacking-based ensemble learning
approach to improve the predictive accuracy of depression classification among
professionals. The Depression Professional Dataset has been collected from
Kaggle. The dataset comprises demographic, occupational, and lifestyle
attributes that influence mental well-being. Our stacking model integrates
multiple base learners with a logistic regression-mediated model, effectively
capturing diverse learning patterns. The experimental results demonstrate that
the proposed model achieves high predictive performance, with an accuracy of
99.64% on training data and 98.75% on testing data, with precision, recall, and
F1-score all exceeding 98%. These findings highlight the effectiveness of
ensemble learning in mental health analytics and underscore its potential for
early detection and intervention strategies.

</details>


### [115] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu,Zhengpeng Xie,Xiangda Yan,Yongjie Yang,Yao Shu*

Main category: cs.LG

TL;DR: 本文揭示了零阶优化（ZOO）与单步策略优化（PO）的等价性，并提出了一种结合PO技术的改进算法ZoAR，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索ZOO方法与RL中PO的联系，揭示其内在机制，以改进ZOO算法的效率和性能。

Method: 通过理论分析证明ZOO与单步PO的等价性，并提出ZoAR算法，引入平均基线和查询重用技术。

Result: ZoAR在理论和实验上均表现出更低的方差和更快的收敛速度，优于现有方法。

Conclusion: 研究为ZOO提供了新的理论视角，并通过PO技术实现了实际算法的改进。

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing
functions where explicit gradients are unavailable or expensive to compute.
However, the underlying mechanisms of popular ZOO methods, particularly those
employing randomized finite differences, and their connection to other
optimization paradigms like Reinforcement Learning (RL) are not fully
elucidated. This paper establishes a fundamental and previously unrecognized
connection: ZOO with finite differences is equivalent to a specific instance of
single-step Policy Optimization (PO). We formally unveil that the implicitly
smoothed objective function optimized by common ZOO algorithms is identical to
a single-step PO objective. Furthermore, we show that widely used ZOO gradient
estimators, are mathematically equivalent to the REINFORCE gradient estimator
with a specific baseline function, revealing the variance-reducing mechanism in
ZOO from a PO perspective.Built on this unified framework, we propose ZoAR
(Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO
algorithm incorporating PO-inspired variance reduction techniques: an averaged
baseline from recent evaluations and query reuse analogous to experience
replay. Our theoretical analysis further substantiates these techniques reduce
variance and enhance convergence. Extensive empirical studies validate our
theory and demonstrate that ZoAR significantly outperforms other methods in
terms of convergence speed and final performance. Overall, our work provides a
new theoretical lens for understanding ZOO and offers practical algorithmic
improvements derived from its connection to PO.

</details>


### [116] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier,Maxime Cordy,Yves Le Traon*

Main category: cs.LG

TL;DR: 论文提出了一种基于超网络架构的方法，通过结合外部因素（如天气指标）来提升全球电力消耗预测模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测仅依赖历史数据，而引入外部因素虽能提升个体预测准确性，却可能降低全局模型的性能。因此，研究旨在解决这一矛盾。

Method: 采用超网络架构，根据每个消费者的特征动态调整模型权重，并结合包括天气、节假日等外部因素的数据集。

Result: 实验表明，超网络方法在结合外部因素时优于现有方法，显著降低了预测误差并保持了全局模型的优势。

Conclusion: 超网络架构能有效利用外部因素提升全球电力消耗预测的准确性，为能源管理提供了更优的解决方案。

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy
management and resource allocation. While traditional time series forecasting
relies on historical patterns and temporal dependencies, incorporating external
factors -- such as weather indicators -- has shown significant potential for
improving prediction accuracy in complex real-world applications. However, the
inclusion of these additional features often degrades the performance of global
predictive models trained on entire populations, despite improving individual
household-level models. To address this challenge, we found that a hypernetwork
architecture can effectively leverage external factors to enhance the accuracy
of global electrical consumption forecasting models, by specifically adjusting
the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising
consumption data from over 6000 luxembourgish households and corresponding
external factors such as weather indicators, holidays, and major local events.
By comparing various forecasting models, we demonstrate that a hypernetwork
approach outperforms existing methods when associated to external factors,
reducing forecasting errors and achieving the best accuracy while maintaining
the benefits of a global model.

</details>


### [117] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Main category: cs.LG

TL;DR: FAMR是一种高效的后处理遗忘框架，用于深度图像分类器，通过约束优化实现选择性遗忘，同时保留模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统依赖受隐私法规约束的数据，选择性遗忘特定信息变得至关重要。

Method: FAMR将遗忘问题转化为约束优化问题，最小化遗忘集的均匀预测损失，同时通过ℓ2惩罚锚定模型参数。

Result: 在CIFAR-10和ImageNet-100上的实验表明，FAMR在保留性能的同时实现了高效遗忘。

Conclusion: FAMR为视觉模型提供了一种可扩展且可验证的高效后处理遗忘方法。

Abstract: As machine learning systems increasingly rely on data subject to privacy
regulation, selectively unlearning specific information from trained models has
become essential. In image classification, this involves removing the influence
of particular training samples, semantic classes, or visual styles without full
retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a
theoretically grounded and computationally efficient framework for post-hoc
unlearning in deep image classifiers. FAMR frames forgetting as a constrained
optimization problem that minimizes a uniform-prediction loss on the forget set
while anchoring model parameters to their original values via an $\ell_2$
penalty. A theoretical analysis links FAMR's solution to
influence-function-based retraining approximations, with bounds on parameter
and output deviation. Empirical results on class forgetting tasks using
CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong
performance retention and minimal computational overhead. The framework
generalizes naturally to concept and style erasure, offering a scalable and
certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [118] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 研究了一种基于探索-利用-提交（ETC）的算法在双人零和博弈中的应用，提出了两种算法并分析了其性能。


<details>
  <summary>Details</summary>
Motivation: 探讨ETC算法在未知收益矩阵的双人零和博弈中的适用性，并学习纯策略纳什均衡。

Method: 提出了ETC-TPZSG和ETC-TPZSG-AE两种算法，后者通过动作对消除策略优化性能。

Result: 两种算法分别实现了O(Δ+√T)和O(log(TΔ²)/Δ)的实例依赖遗憾上界。

Conclusion: ETC算法在对抗性博弈中表现良好，提供了实例依赖分析的见解。

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to
maximize their payoff against an adversarial column player, under an unknown
payoff matrix estimated through bandit feedback. We propose and analyze two
algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and
ETC-TPZSG-AE, which improves upon it by incorporating an action pair
elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium
property to efficiently select the optimal action pair. Our objective is to
demonstrate the applicability of ETC in a TPZSG setting by focusing on learning
pure strategy Nash Equilibrium. A key contribution of our work is a derivation
of instance-dependent upper bounds on the expected regret for both algorithms,
has received limited attention in the literature on zero-sum games.
Particularly, after $T$ rounds, we achieve an instance-dependent regret upper
bounds of $O(\Delta + \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T
\Delta^2)}{\Delta})$ for ETC-TPZSG-AE, where $\Delta$ denotes the suboptimality
gap. Therefore, our results indicate that ETC-based algorithms perform
effectively in adversarial game settings, achieving regret bounds comparable to
existing methods while providing insights through instance-dependent analysis.

</details>


### [119] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab,Marcel Rothering*

Main category: cs.LG

TL;DR: 论文通过工业AI案例研究指出当前最佳实践的不足，提出七点改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨当前AI研究方法是否能成功应用于实际工业场景，揭示其局限性。

Method: 通过工业AI用例（自动光学检测中的误报减少）进行案例研究，分析现有方法的弱点。

Result: 发现当前最佳实践方法在此用例中会失败，并提出改进方向。

Conclusion: 呼吁研究者批判性评估方法，以推动更成功的应用AI研究。

Abstract: Are current artificial intelligence (AI) research methodologies ready to
create successful, productive, and profitable AI applications? This work
presents a case study on an industrial AI use case called false call reduction
for automated optical inspection to demonstrate the shortcomings of current
best practices. We identify seven weaknesses prevalent in related peer-reviewed
work and experimentally show their consequences. We show that the best-practice
methodology would fail for this use case. We argue amongst others for the
necessity of requirement-aware metrics to ensure achieving business objectives,
clear definitions of success criteria, and a thorough analysis of temporal
dynamics in experimental datasets. Our work encourages researchers to
critically assess their methodologies for more successful applied AI research.

</details>


### [120] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng,Lanning Wei,Yong Li,Quanming Yao*

Main category: cs.LG

TL;DR: LLMNet利用大型语言模型自动设计和优化图神经网络（GNN），通过知识库和检索增强生成（RAG）实现高效配置，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统GNN需要大量人工配置和调优，LLMNet旨在通过自动化设计减少这一负担。

Method: 系统通过知识库构建和RAG技术，利用智能代理自动化配置和优化GNN模型。

Result: 在三个图学习任务的十二个数据集中，LLMNet表现出色，验证了其有效性。

Conclusion: LLMNet为GNN的自动化设计提供了高效解决方案，显著减少了人工干预需求。

Abstract: Effective decision-making on networks often relies on learning from
graph-structured data, where Graph Neural Networks (GNNs) play a central role,
but they take efforts to configure and tune. In this demo, we propose LLMNet,
showing how to design GNN automated through Large Language Models. Our system
develops a set of agents that construct graph-related knowlege bases and then
leverages Retrieval-Augmented Generation (RAG) to support automated
configuration and refinement of GNN models through a knowledge-guided evolution
process. These agents, equipped with specialized knowledge bases, extract
insights into tasks and graph structures by interacting with the knowledge
bases. Empirical results show LLMNet excels in twelve datasets across three
graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [121] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores,Alyssa H. Smith,Julia A. Fukuyama,Ashia C. Wilson*

Main category: cs.LG

TL;DR: 提出了一种基于校准阈值分类器的评估框架，用于临床决策支持系统，考虑了类别分布不确定性和成本不对称性。


<details>
  <summary>Details</summary>
Motivation: 现有评分规则（如准确率和AUC-ROC）未能充分反映临床优先级（如校准性、分布偏移鲁棒性和错误成本敏感性）。

Method: 基于Schervish表示理论，提出了一种调整后的交叉熵变体，用于评估分类器在临床相关类别平衡范围内的性能。

Result: 该框架简单易用，对临床部署条件敏感，优先选择校准且鲁棒的模型。

Conclusion: 提出的方法为临床决策支持系统提供了一种更符合实际需求的评估标准。

Abstract: Machine learning-based decision support systems are increasingly deployed in
clinical settings, where probabilistic scoring functions are used to inform and
prioritize patient management decisions. However, widely used scoring rules,
such as accuracy and AUC-ROC, fail to adequately reflect key clinical
priorities, including calibration, robustness to distributional shifts, and
sensitivity to asymmetric error costs. In this work, we propose a principled
yet practical evaluation framework for selecting calibrated thresholded
classifiers that explicitly accounts for the uncertainty in class prevalences
and domain-specific cost asymmetries often found in clinical settings. Building
on the theory of proper scoring rules, particularly the Schervish
representation, we derive an adjusted variant of cross-entropy (log score) that
averages cost-weighted performance over clinically relevant ranges of class
balance. The resulting evaluation is simple to apply, sensitive to clinical
deployment conditions, and designed to prioritize models that are both
calibrated and robust to real-world variations.

</details>


### [122] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand,Leonardo Gizzi,Martin A. Giese*

Main category: cs.LG

TL;DR: GPDMM是一种高斯过程动态混合模型，用于单样本学习人体运动数据，结合了GPDM和概率混合框架，解决了数据有限和模型可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 解决在数据有限且需要高可解释性的场景（如医疗应用）中建模人体运动的挑战。

Method: 结合多个GPDM，采用概率混合框架，利用几何特征编码多样序列。

Result: 在分类准确性和生成能力上表现优异，优于LSTM、VAE和Transformer。

Conclusion: GPDMM在单样本学习中表现出色，适用于高可解释性需求的场景。

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its
utility in single-example learning of human motion data. The Gaussian process
dynamical model (GPDM) is a form of the Gaussian process latent variable model
(GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM
combines multiple GPDMs in a probabilistic mixture-of-experts framework,
utilizing embedded geometric features to allow for diverse sequences to be
encoded in a single latent space, enabling the categorization and generation of
each sequence class. GPDMs and our mixture model are particularly advantageous
in addressing the challenges of modeling human movement in scenarios where data
is limited and model interpretability is vital, such as in patient-specific
medical applications like prosthesis control. We score the GPDMM on
classification accuracy and generative ability in single-example learning,
showcase model variations, and benchmark it against LSTMs, VAEs, and
transformers.

</details>


### [123] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu,Xi Chen,Zhongdao Wang,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 论文提出了一种将序列级PPO分解为令牌级PPO的方法，并利用令牌级奖励指导DPO，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将令牌级奖励用于指导DPO，因为DPO是序列级问题。

Method: 将序列级PPO分解为令牌级PPO问题，推导出令牌级最优策略和奖励，并基于此设计DPO的损失函数。

Result: 在多个基准测试中，性能显著优于DPO，最高提升7.5点。

Conclusion: 提出的方法有效解决了令牌级奖励指导DPO的挑战，并实现了性能提升。

Abstract: Recent advancements in reinforcement learning from human feedback have shown
that utilizing fine-grained token-level reward models can substantially enhance
the performance of Proximal Policy Optimization (PPO) in aligning large
language models. However, it is challenging to leverage such token-level reward
as guidance for Direct Preference Optimization (DPO), since DPO is formulated
as a sequence-level bandit problem. To address this challenge, this work
decomposes the sequence-level PPO into a sequence of token-level proximal
policy optimization problems and then frames the problem of token-level PPO
with token-level reward guidance, from which closed-form optimal token-level
policy and the corresponding token-level reward can be derived. Using the
obtained reward and Bradley-Terry model, this work establishes a framework of
computable loss functions with token-level reward guidance for DPO, and
proposes a practical reward guidance based on the induced DPO reward. This
formulation enables different tokens to exhibit varying degrees of deviation
from reference policy based on their respective rewards. Experiment results
demonstrate that our method achieves substantial performance improvements over
DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on
AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at
https://github.com/dvlab-research/TGDPO.

</details>


### [124] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob,Avinash Kori,Emanuele De Angelis,Ben Glocker,Maurizio Proietti,Francesca Toni*

Main category: cs.LG

TL;DR: 提出了一种结合假设基础论证（ABA）与深度学习的神经论证学习（NAL）架构，用于图像分析，实验表明其性能与现有先进方法相当。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在关键决策中的应用增加，其安全性、可靠性和可解释性问题日益突出，需要一种结合符号推理的方法来解决这些问题。

Method: NAL架构包含神经和符号组件：神经部分通过对象中心学习分割和编码图像为事实，符号部分则应用ABA学习生成预测框架。

Result: 在合成数据上的实验表明，NAL架构的性能与现有先进方法相当。

Conclusion: NAL架构通过结合神经与符号学习，为解决深度学习的可解释性问题提供了新思路。

Abstract: Over the last decade, as we rely more on deep learning technologies to make
critical decisions, concerns regarding their safety, reliability and
interpretability have emerged. We introduce a novel Neural Argumentative
Learning (NAL) architecture that integrates Assumption-Based Argumentation
(ABA) with deep learning for image analysis. Our architecture consists of
neural and symbolic components. The former segments and encodes images into
facts using object-centric learning, while the latter applies ABA learning to
develop ABA frameworks enabling predictions with images. Experiments on
synthetic data show that the NAL architecture can be competitive with a
state-of-the-art alternative.

</details>


### [125] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: SCISSOR是一种基于Siamese网络的去偏方法，通过抑制语义聚类中的捷径学习，提升模型在分布外数据上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究揭示了语义分布不平衡导致的虚假语义相关性会削弱模型鲁棒性，现有方法依赖数据增强或重写，而SCISSOR无需这些步骤。

Method: SCISSOR通过Siamese网络重新映射语义空间，抑制被用作捷径的潜在聚类。

Result: 在4个基准测试中，SCISSOR显著提升了模型性能（如GYAFC上F1分数提升5.3），尤其对轻量级模型效果显著（ViT提升9.5%，BERT提升11.9%）。

Conclusion: SCISSOR通过解决语义偏差，为抑制捷径学习和构建更鲁棒的AI系统提供了基础框架。

Abstract: Shortcut learning undermines model generalization to out-of-distribution
data. While the literature attributes shortcuts to biases in superficial
features, we show that imbalances in the semantic distribution of sample
embeddings induce spurious semantic correlations, compromising model
robustness. To address this issue, we propose SCISSOR (Semantic Cluster
Intervention for Suppressing ShORtcut), a Siamese network-based debiasing
approach that remaps the semantic space by discouraging latent clusters
exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR
eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on
6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and
GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports
+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,
and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models
with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for
BERT on NLP. Our study redefines the landscape of model generalization by
addressing overlooked semantic biases, establishing SCISSOR as a foundational
framework for mitigating shortcut learning and fostering more robust,
bias-resistant AI systems.

</details>


### [126] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman,Christopher Nemeth,Matthew Jones,Philip Jonathan*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度学习和贝叶斯推理的时空反演框架，用于实时识别和量化动态流场中的温室气体排放。


<details>
  <summary>Details</summary>
Motivation: 实时监测动态大气条件下的温室气体排放是一个关键挑战，需要高精度且计算高效的方法。

Method: 通过将计算流体力学（CFD）的深度学习替代模型嵌入顺序蒙特卡洛算法，实现了排放率和源位置的贝叶斯推断。

Result: 在验证实验中，该方法与完整CFD求解器和高斯羽流模型精度相当，但运行速度快几个数量级。

Conclusion: 该框架在保持物理保真度的同时实现了计算可行性，为工业排放监测等任务提供了可扩展的解决方案。

Abstract: Real-time identification and quantification of greenhouse-gas emissions under
transient atmospheric conditions is a critical challenge in environmental
monitoring. We introduce a spatio-temporal inversion framework that embeds a
deep-learning surrogate of computational fluid dynamics (CFD) within a
sequential Monte Carlo algorithm to perform Bayesian inference of both emission
rate and source location in dynamic flow fields. By substituting costly
numerical solvers with a multilayer perceptron trained on high-fidelity CFD
outputs, our surrogate captures spatial heterogeneity and temporal evolution of
gas dispersion, while delivering near-real-time predictions. Validation on the
Chilbolton methane release dataset demonstrates comparable accuracy to full CFD
solvers and Gaussian plume models, yet achieves orders-of-magnitude faster
runtimes. Further experiments under simulated obstructed-flow scenarios confirm
robustness in complex environments. This work reconciles physical fidelity with
computational feasibility, offering a scalable solution for industrial
emissions monitoring and other time-sensitive spatio-temporal inversion tasks
in environmental and scientific modeling.

</details>


### [127] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong,Jim Lim,David I. Inouye*

Main category: cs.LG

TL;DR: 提出了一种基于分数先验分布的新型分布匹配方法，解决了传统方法的局限性，如固定先验偏差和训练挑战。


<details>
  <summary>Details</summary>
Motivation: 传统分布匹配方法存在可扩展性、不稳定性和模式崩溃等问题，而基于似然的方法又可能引入不必要的偏差或训练困难。

Method: 利用表达性分数先验分布训练似然分布匹配，通过去噪分数匹配避免固定先验偏差，同时无需显式密度模型。

Result: 实验表明，该方法在稳定性和计算效率上优于其他扩散先验方法，并在多个任务中表现优异。

Conclusion: 提出的分数先验方法是一种稳定且有效的分布匹配技术，解决了传统方法的局限性。

Abstract: Distribution matching (DM) is a versatile domain-invariant representation
learning technique that has been applied to tasks such as fair classification,
domain adaptation, and domain translation. Non-parametric DM methods struggle
with scalability and adversarial DM approaches suffer from instability and mode
collapse. While likelihood-based methods are a promising alternative, they
often impose unnecessary biases through fixed priors or require explicit
density models (e.g., flows) that can be challenging to train. We address this
limitation by introducing a novel approach to training likelihood-based DM
using expressive score-based prior distributions. Our key insight is that
gradient-based DM training only requires the prior's score function -- not its
density -- allowing us to train the prior via denoising score matching. This
approach eliminates biases from fixed priors (e.g., in VAEs), enabling more
effective use of geometry-preserving regularization, while avoiding the
challenge of learning an explicit prior density model (e.g., a flow-based
prior). Our method also demonstrates better stability and computational
efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore,
experiments demonstrate superior performance across multiple tasks,
establishing our score-based method as a stable and effective approach to
distribution matching. Source code available at
https://github.com/inouye-lab/SAUB.

</details>


### [128] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia,Elena Raponi,Thomas Bäck,Fabian Duddeck*

Main category: cs.LG

TL;DR: 提出了一种基于可行性驱动的信任区域贝叶斯优化算法（FuRBO），用于解决高维空间中昂贵约束的优化问题，显著加速可行解的发现。


<details>
  <summary>Details</summary>
Motivation: 许多实际优化问题涉及昂贵约束且可行区域小、不规则，现有方法在寻找初始可行解时消耗大量预算，限制了优化效果。

Method: FuRBO通过迭代定义信任区域，结合目标和约束的代理模型信息，动态调整区域位置和大小，快速聚焦搜索。

Result: 在BBOB-constrained COCO基准和其他物理启发基准上，FuRBO在约束严重性和问题维度（2到60）范围内优于现有方法。

Conclusion: FuRBO通过自适应信任区域策略，显著提升了高维昂贵约束优化问题的求解效率。

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization
tasks under tight evaluation budgets, making it well-suited for applications
involving costly simulations or experiments. However, many of these tasks are
also characterized by the presence of expensive constraints whose analytical
formulation is unknown and often defined in high-dimensional spaces where
feasible regions are small, irregular, and difficult to identify. In such
cases, a substantial portion of the optimization budget may be spent just
trying to locate the first feasible solution, limiting the effectiveness of
existing methods. In this work, we present a Feasibility-Driven Trust Region
Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust
region from which the next candidate solution is selected, using information
from both the objective and constraint surrogate models. Our adaptive strategy
allows the trust region to shift and resize significantly between iterations,
enabling the optimizer to rapidly refocus its search and consistently
accelerate the discovery of feasible and good-quality solutions. We empirically
demonstrate the effectiveness of FuRBO through extensive testing on the full
BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks,
comparing it against state-of-the-art baselines for constrained black-box
optimization across varying levels of constraint severity and problem
dimensionalities ranging from 2 to 60.

</details>


### [129] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文提出了一种新的视觉反事实解释方法（SCE），弥补了现有方法在解释全面性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉反事实解释器（VCEs）过于关注样本质量或最小变化，忽略了解释的全面性（如保真度、可理解性和充分性）。

Method: 探索了新的反事实生成机制，并整合为一种新颖的“平滑反事实探索器”（SCE）算法。

Result: 通过合成和真实数据的系统评估，证明了SCE算法的有效性。

Conclusion: SCE算法在提升解释全面性方面表现优异，为图像分类器的透明度提供了更优的解决方案。

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising
approach to enhancing the transparency of image classifiers. VCEs complement
other types of explanations, such as feature attribution, by revealing the
specific data transformations to which a machine learning model responds most
strongly. In this paper, we argue that existing VCEs focus too narrowly on
optimizing sample quality or change minimality; they fail to consider the more
holistic desiderata for an explanation, such as fidelity, understandability,
and sufficiency. To address this shortcoming, we explore new mechanisms for
counterfactual generation and investigate how they can help fulfill these
desiderata. We combine these mechanisms into a novel 'smooth counterfactual
explorer' (SCE) algorithm and demonstrate its effectiveness through systematic
evaluations on synthetic and real data.

</details>


### [130] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim,Aldo Pacchiano,Miroslav Dudik,Robert Schapire*

Main category: cs.LG

TL;DR: 论文研究了在已知函数类F下，多臂老虎机学习（最佳臂识别）的任务，探讨了哪些函数类是可学习的以及如何学习的问题。研究发现，传统学习理论中的组合维度方法不适用于老虎机学习，并证明了计算上的困难性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个类似于PAC分类框架的多臂老虎机学习的通用理论，明确哪些函数类是可学习的及其学习方法。

Method: 研究方法包括理论证明和构造反例，展示了组合维度方法的局限性，并提出了计算困难性的结果。

Result: 研究结果表明，老虎机学习无法通过组合维度来表征，且存在计算困难性，即使对于简单的函数类，也无法在多项式时间内找到最优解。

Conclusion: 结论指出，老虎机学习的边界和计算困难性是其固有特性，为未来的研究提供了新的方向。

Abstract: We study the task of bandit learning, also known as best-arm identification,
under the assumption that the true reward function f belongs to a known, but
arbitrary, function class F. We seek a general theory of bandit learnability,
akin to the PAC framework for classification. Our investigation is guided by
the following two questions: (1) which classes F are learnable, and (2) how
they are learnable. For example, in the case of binary PAC classification,
learnability is fully determined by a combinatorial dimension - the VC
dimension- and can be attained via a simple algorithmic principle, namely,
empirical risk minimization (ERM). In contrast to classical learning-theoretic
results, our findings reveal limitations of learning in structured bandits,
offering insights into the boundaries of bandit learnability. First, for the
question of "which", we show that the paradigm of identifying the learnable
classes via a dimension-like quantity fails for bandit learning. We give a
simple proof demonstrating that no combinatorial dimension can characterize
bandit learnability, even in finite classes, following a standard definition of
dimension introduced by Ben-David et al. (2019). For the question of "how", we
prove a computational hardness result: we construct a reward function class for
which at most two queries are needed to find the optimal action, yet no
algorithm can do so in polynomial time unless RP=NP. We also prove that this
class admits efficient algorithms for standard algorithmic operations often
considered in learning theory, such as an ERM. This implies that computational
hardness is in this case inherent to the task of bandit learning. Beyond these
results, we investigate additional themes such as learning under noise,
trade-offs between noise models, and the relationship between query complexity
and regret minimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [131] [TUM Teleoperation: Open Source Software for Remote Driving and Assistance of Automated Vehicles](https://arxiv.org/abs/2506.13933)
*Tobias Kerbl,David Brecht,Nils Gehrke,Nijinshan Karunainayagam,Niklas Krauss,Florian Pfab,Richard Taupitz,Ines Trautmannsheimer,Xiyan Su,Maria-Magdalena Wolf,Frank Diermeyer*

Main category: cs.RO

TL;DR: 开源远程操作软件栈，支持远程驾驶和远程辅助，适用于自动驾驶车辆。


<details>
  <summary>Details</summary>
Motivation: 填补现有开源软件在远程操作（远程驾驶和远程辅助）与真实车辆集成方面的空白。

Method: 开发模块化的开源软件栈，支持与自动驾驶软件（如Autoware）交互，提供标准化接口和灵活的人机界面设计。

Result: 在不同车辆平台上验证了延迟和性能，适用于仿真和实际测试。

Conclusion: 该软件为协同开发和用户研究提供了基础，代码已开源。

Abstract: Teleoperation is a key enabler for future mobility, supporting Automated
Vehicles in rare and complex scenarios beyond the capabilities of their
automation. Despite ongoing research, no open source software currently
combines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance
through high-level interaction with automated driving software modules, and
integration with a real-world vehicle for practical testing. To address this
gap, we present a modular, open source teleoperation software stack that can
interact with an automated driving software, e.g., Autoware, enabling Remote
Assistance and Remote Driving. The software featuresstandardized interfaces for
seamless integration with various real-world and simulation platforms, while
allowing for flexible design of the human-machine interface. The system is
designed for modularity and ease of extension, serving as a foundation for
collaborative development on individual software components as well as
realistic testing and user studies. To demonstrate the applicability of our
software, we evaluated the latency and performance of different vehicle
platforms in simulation and real-world. The source code is available on GitHub

</details>


### [132] [Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control](https://arxiv.org/abs/2506.14186)
*Anselm Paulus,A. René Geist,Pierre Schumacher,Vít Musil,Georg Martius*

Main category: cs.RO

TL;DR: 论文分析了基于惩罚的模拟器中接触力梯度计算的误差原因，提出DiffMJX和CFD方法以改进梯度质量，并在非接触情况下生成有用梯度。


<details>
  <summary>Details</summary>
Motivation: 接触力在机器人动力学优化中引入速度跳跃，而基于惩罚的模拟器（如MuJoCo）通过软化接触力简化梯度计算，但硬接触的模拟需要高刚度设置，导致自动微分时梯度错误。

Method: 分析基于惩罚的模拟器的接触计算，提出DiffMJX结合自适应积分与MuJoCo XLA改进梯度质量；引入CFD机制在非接触情况下生成梯度。

Result: DiffMJX显著提高了硬接触下的梯度质量；CFD在非接触情况下也能生成有用梯度，且不影响前向模拟的物理真实性。

Conclusion: DiffMJX和CFD方法有效解决了接触力梯度计算的问题，提升了模拟器的性能与实用性。

Abstract: Contact forces pose a major challenge for gradient-based optimization of
robot dynamics as they introduce jumps in the system's velocities.
Penalty-based simulators, such as MuJoCo, simplify gradient computation by
softening the contact forces. However, realistically simulating hard contacts
requires very stiff contact settings, which leads to incorrect gradients when
using automatic differentiation. On the other hand, using non-stiff settings
strongly increases the sim-to-real gap. We analyze the contact computation of
penalty-based simulators to identify the causes of gradient errors. Then, we
propose DiffMJX, which combines adaptive integration with MuJoCo XLA, to
notably improve gradient quality in the presence of hard contacts. Finally, we
address a key limitation of contact gradients: they vanish when objects do not
touch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism
that enables the simulator to generate informative contact gradients even
before objects are in contact. To preserve physical realism, we apply CFD only
in the backward pass using a straight-through trick, allowing us to compute
useful gradients without modifying the forward simulation.

</details>


### [133] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: AMPLIFY框架利用大规模无动作视频数据，通过关键点轨迹生成紧凑的运动标记，将视觉动态预测与动作推理分离，显著提升低数据量下的策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 机器人动作标记数据稀缺且昂贵，而无动作视频数据丰富但难以转化为有效策略。

Method: 通过关键点轨迹生成运动标记，分离视觉动态预测与动作推理，分别训练前向和逆向动态模型。

Result: 动态预测精度显著提升（MSE提高3.7倍，像素预测精度提高2.5倍），策略学习效果在低数据量下提升1.2-2.2倍。

Conclusion: AMPLIFY为利用异构数据构建高效、通用的世界模型提供了新范式。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the
generalization of learned policies. In contrast, vast amounts of action-free
video data are readily available, but translating these observations into
effective policies remains a challenge. We introduce AMPLIFY, a novel framework
that leverages large-scale video data by encoding visual dynamics into compact,
discrete motion tokens derived from keypoint trajectories. Our modular approach
separates visual motion prediction from action inference, decoupling the
challenges of learning what motion defines a task from how robots can perform
it. We train a forward dynamics model on abundant action-free videos and an
inverse dynamics model on a limited set of action-labeled examples, allowing
for independent scaling. Extensive evaluations demonstrate that the learned
dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x
better pixel prediction accuracy compared to prior approaches, and broadly
useful. In downstream policy learning, our dynamics predictions enable a
1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by
learning from action-free human videos, and the first generalization to LIBERO
tasks from zero in-distribution action data. Beyond robotic control, we find
the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing
video prediction quality. Our results present a novel paradigm leveraging
heterogeneous data sources to build efficient, generalizable world models. More
information can be found at https://amplify-robotics.github.io/.

</details>


### [134] [Steering Robots with Inference-Time Interactions](https://arxiv.org/abs/2506.14287)
*Yanwei Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种方法，允许用户在推理阶段通过交互纠正预训练策略的错误，而无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在部署时可能因泛化能力不足而产生错误，传统方法需要额外数据微调，效率低下。

Method: 提出了两种框架：(1) 推理时引导，通过用户交互切换离散技能；(2) 任务和运动模仿，允许用户编辑连续运动以满足任务约束。

Result: 这些框架能够在不额外训练的情况下纠正策略预测的偏差，提升预训练模型的实用性。

Conclusion: 通过用户交互实现推理时行为修正，显著提高了预训练策略的灵活性和实用性。

Abstract: Imitation learning has driven the development of generalist policies capable
of autonomously solving multiple tasks. However, when a pretrained policy makes
errors during deployment, there are limited mechanisms for users to correct its
behavior. While collecting additional data for finetuning can address such
issues, doing so for each downstream use case is inefficient at deployment. My
research proposes an alternative: keeping pretrained policies frozen as a fixed
skill repertoire while allowing user interactions to guide behavior generation
toward user preferences at inference time. By making pretrained policies
steerable, users can help correct policy errors when the model struggles to
generalize-without needing to finetune the policy. Specifically, I propose (1)
inference-time steering, which leverages user interactions to switch between
discrete skills, and (2) task and motion imitation, which enables user
interactions to edit continuous motions while satisfying task constraints
defined by discrete symbolic plans. These frameworks correct misaligned policy
predictions without requiring additional training, maximizing the utility of
pretrained models while achieving inference-time user objectives.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [135] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Main category: cs.GR

TL;DR: 论文提出ReFrame，通过缓存中间特征优化实时渲染任务中的性能与质量权衡，平均提速1.4倍且质量损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 利用神经网络在渲染任务中的时间连贯性，避免冗余计算以提高效率。

Method: 扩展中间特征缓存至实时渲染，探索不同缓存策略，适用于编码器-解码器网络。

Result: 在三种实时渲染任务中平均提速1.4倍，质量损失可忽略。

Conclusion: ReFrame通过缓存策略有效优化渲染性能，适用于多种渲染网络。

Abstract: Graphics rendering applications increasingly leverage neural networks in
tasks such as denoising, supersampling, and frame extrapolation to improve
image quality while maintaining frame rates. The temporal coherence inherent in
these tasks presents an opportunity to reuse intermediate results from previous
frames and avoid redundant computations. Recent work has shown that caching
intermediate features to be reused in subsequent inferences is an effective
method to reduce latency in diffusion models. We extend this idea to real-time
rendering and present ReFrame, which explores different caching policies to
optimize trade-offs between quality and performance in rendering workloads.
ReFrame can be applied to a variety of encoder-decoder style networks commonly
found in rendering pipelines. Experimental results show that we achieve 1.4x
speedup on average with negligible quality loss in three real-time rendering
tasks. Code available:
https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [136] [Evolutionary chemical learning in dimerization networks](https://arxiv.org/abs/2506.14006)
*Alexei V. Tkachenko,Bortolo Matteo Mognetti,Sergei Maslov*

Main category: cond-mat.stat-mech

TL;DR: 提出了一种基于竞争性二聚体网络（CDNs）的化学学习框架，通过体外定向进化训练网络，实现复杂学习任务（如多类分类），无需数字硬件或显式参数调整。


<details>
  <summary>Details</summary>
Motivation: 将合成生物学与机器学习结合，开发自适应、高能效的分子计算系统。

Method: 利用DNA/RNA寡聚物等分子物种形成可逆二聚体，通过突变、选择和扩增DNA组件进行训练，结合对比增强损失函数优化性能。

Result: CDNs在噪声输入模式下表现出强输出对比和高互信息，与计算机梯度下降训练性能高度相关。

Conclusion: CDNs为模拟物理计算提供了有前景的平台，推动了分子计算系统的发展。

Abstract: We present a novel framework for chemical learning based on Competitive
Dimerization Networks (CDNs) - systems in which multiple molecular species,
e.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show
that these networks can be trained in vitro through directed evolution,
enabling the implementation of complex learning tasks such as multiclass
classification without digital hardware or explicit parameter tuning. Each
molecular species functions analogously to a neuron, with binding affinities
acting as tunable synaptic weights. A training protocol involving mutation,
selection, and amplification of DNA-based components allows CDNs to robustly
discriminate among noisy input patterns. The resulting classifiers exhibit
strong output contrast and high mutual information between input and output,
especially when guided by a contrast-enhancing loss function. Comparative
analysis with in silico gradient descent training reveals closely correlated
performance. These results establish CDNs as a promising platform for analog
physical computation, bridging synthetic biology and machine learning, and
advancing the development of adaptive, energy-efficient molecular computing
systems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [137] [A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems](https://arxiv.org/abs/2506.13950)
*Dimitrios G. Patsatzis,Nikolaos Kazantzis,Ioannis G. Kevrekidis,Constantinos Siettos*

Main category: math.NA

TL;DR: 提出了一种混合机器学习方案，结合多项式级数和浅层神经网络，用于学习离散映射的不变流形（IM），以构建动力系统的降阶模型（ROM）。


<details>
  <summary>Details</summary>
Motivation: 传统方法在建模复杂结构时存在局限性，需要结合多项式级数和神经网络的互补优势以提高精度和效率。

Method: 结合多项式级数和浅层神经网络，多项式用于局部指数收敛的ROM建模，神经网络用于复杂结构近似。

Result: 在三个基准测试中，混合方案在数值逼近精度上优于纯多项式或神经网络方法。

Conclusion: 混合方案在动力系统降阶建模中表现出更高的精度和效率。

Abstract: We propose a hybrid machine learning scheme to learn -- in physics-informed
and numerical analysis-informed fashion -- invariant manifolds (IM) of discrete
maps for constructing reduced-order models (ROMs) for dynamical systems. The
proposed scheme combines polynomial series with shallow neural networks,
exploiting the complementary strengths of both approaches. Polynomials enable
an efficient and accurate modeling of ROMs with guaranteed local exponential
convergence rate around the fixed point, where, under certain assumptions, the
IM is demonstrated to be analytic. Neural networks provide approximations to
more complex structures beyond the reach of the polynomials' convergence. We
evaluate the efficiency of the proposed scheme using three benchmark examples,
examining convergence behavior, numerical approximation accuracy, and
computational training cost. Additionally, we compare the IM approximations
obtained solely with neural networks and with polynomial expansions. We
demonstrate that the proposed hybrid scheme outperforms both pure polynomial
approximations (power series, Legendre and Chebyshev polynomials) and
standalone shallow neural network approximations in terms of numerical
approximation accuracy.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [138] [Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior](https://arxiv.org/abs/2506.14762)
*Chengyuan Zhang,Cathy Wu,Lijun Sun*

Main category: stat.AP

TL;DR: 论文提出了一种基于状态切换的框架（FHMM-IDM），用于解决传统跟车模型（如IDM）无法捕捉人类驾驶多模态行为的问题，通过分离驾驶行为模式和交通场景，提高了模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统跟车模型（如IDM）由于其单一状态结构，无法捕捉人类驾驶的多模态行为，导致模型精度和参数可解释性受限。

Method: 提出了一种基于状态切换的框架（FHMM-IDM），使用两个独立的隐马尔可夫过程分离驾驶行为模式和交通场景，并通过贝叶斯推断估计参数和状态轨迹。

Result: 在HighD数据集上的实验表明，FHMM-IDM能够有效分离驾驶行为模式和交通场景，揭示动态状态切换模式，提高了模型的准确性和可解释性。

Conclusion: FHMM-IDM为建模不确定性下的上下文依赖驾驶行为提供了可行且理论严谨的解决方案，提升了交通模拟的保真度、安全分析的有效性以及ADAS的开发。

Abstract: Accurate and interpretable car-following models are essential for traffic
simulation and autonomous vehicle development. However, classical models like
the Intelligent Driver Model (IDM) are fundamentally limited by their
parsimonious and single-regime structure. They fail to capture the multi-modal
nature of human driving, where a single driving state (e.g., speed, relative
speed, and gap) can elicit many different driver actions. This forces the model
to average across distinct behaviors, reducing its fidelity and making its
parameters difficult to interpret. To overcome this, we introduce a
regime-switching framework that allows driving behavior to be governed by
different IDM parameter sets, each corresponding to an interpretable behavioral
mode. This design enables the model to dynamically switch between interpretable
behavioral modes, rather than averaging across diverse driving contexts. We
instantiate the framework using a Factorial Hidden Markov Model with IDM
dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes
(e.g., aggressive acceleration, steady-state following) from external traffic
scenarios (e.g., free-flow, congestion, stop-and-go) through two independent
latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC)
is used to jointly estimate the regime-specific parameters, transition
dynamics, and latent state trajectories. Experiments on the HighD dataset
demonstrate that FHMM-IDM uncovers interpretable structure in human driving,
effectively disentangling internal driver actions from contextual traffic
conditions and revealing dynamic regime-switching patterns. This framework
provides a tractable and principled solution to modeling context-dependent
driving behavior under uncertainty, offering improvements in the fidelity of
traffic simulations, the efficacy of safety analyses, and the development of
more human-centric ADAS.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [139] [Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models](https://arxiv.org/abs/2506.13900)
*Marouane Il Idrissi,Agathe Fernandes Machado,Arthur Charpentier*

Main category: stat.ML

TL;DR: 论文探讨了合作博弈论在机器学习可解释性中的应用，提出超越Shapley值的更广泛工具，如Weber和Harsanyi集，并提供了一个三步骤框架以设计更可靠的特征归因方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Shapley值在可解释性中被广泛使用，但其公理基础与特征归因的相关性仍有争议。作者希望通过合作博弈论的工具提供更灵活和理论扎实的方法。

Method: 论文回顾了合作博弈论，介绍了Weber和Harsanyi集两种高效分配方案，并提出了一个三步骤框架来构建特征归因方法。

Result: 作者展示了超越Shapley值的分配方案，并提供了一个理论框架，以设计更灵活和稳健的特征归因方法。

Conclusion: 论文呼吁XAI社区超越固定公理，采用更广泛的合作博弈论工具，以设计更具意义和适应性的特征归因方法。

Abstract: Cooperative game theory has become a cornerstone of post-hoc interpretability
in machine learning, largely through the use of Shapley values. Yet, despite
their widespread adoption, Shapley-based methods often rest on axiomatic
justifications whose relevance to feature attribution remains debatable. In
this paper, we revisit cooperative game theory from an interpretability
perspective and argue for a broader and more principled use of its tools. We
highlight two general families of efficient allocations, the Weber and Harsanyi
sets, that extend beyond Shapley values and offer richer interpretative
flexibility. We present an accessible overview of these allocation schemes,
clarify the distinction between value functions and aggregation rules, and
introduce a three-step blueprint for constructing reliable and
theoretically-grounded feature attributions. Our goal is to move beyond fixed
axioms and provide the XAI community with a coherent framework to design
attribution methods that are both meaningful and robust to shifting
methodological trends.

</details>


### [140] [Rademacher learning rates for iterated random functions](https://arxiv.org/abs/2506.13946)
*Nikola Sandrić*

Main category: stat.ML

TL;DR: 论文探讨了监督学习中非独立同分布（非i.i.d.）数据的问题，提出了基于迭代随机函数的马尔可夫链模型，并证明了其学习率和收敛性。


<details>
  <summary>Details</summary>
Motivation: 现实问题中数据往往具有时间依赖性和强相关性，传统i.i.d.假设不适用，需要研究更通用的数据生成模型。

Method: 假设数据由迭代随机函数生成的马尔可夫链产生，基于收缩性和假设类正则性，证明了样本误差的一致收敛性。

Result: 证明了近似经验风险最小化算法的可学习性，并推导了其学习率界限，且这些界限依赖于数据分布。

Conclusion: 研究为处理非i.i.d.数据提供了理论支持，学习率更准确地反映了数据生成分布的特性。

Abstract: Most existing literature on supervised machine learning assumes that the
training dataset is drawn from an i.i.d. sample. However, many real-world
problems exhibit temporal dependence and strong correlations between the
marginal distributions of the data-generating process, suggesting that the
i.i.d. assumption is often unrealistic. In such cases, models naturally include
time-series processes with mixing properties, as well as irreducible and
aperiodic ergodic Markov chains. Moreover, the learning rates typically
obtained in these settings are independent of the data distribution, which can
lead to restrictive choices of hypothesis classes and suboptimal sample
complexities for the learning algorithm. In this article, we consider the case
where the training dataset is generated by an iterated random function (i.e.,
an iteratively defined time-homogeneous Markov chain) that is not necessarily
irreducible or aperiodic. Under the assumption that the governing function is
contractive with respect to its first argument and subject to certain
regularity conditions on the hypothesis class, we first establish a uniform
convergence result for the corresponding sample error. We then demonstrate the
learnability of the approximate empirical risk minimization algorithm and
derive its learning rate bound. Both rates are data-distribution dependent,
expressed in terms of the Rademacher complexities of the underlying hypothesis
class, allowing them to more accurately reflect the properties of the
data-generating distribution.

</details>


### [141] [Meta Optimality for Demographic Parity Constrained Regression via Post-Processing](https://arxiv.org/abs/2506.13947)
*Kazuto Fukuchi*

Main category: stat.ML

TL;DR: 本文提出了一种在满足人口统计平等约束下的回归问题解决方案，通过元定理验证了公平极小极大最优回归算法的普适性，并展示了后处理方法的应用。


<details>
  <summary>Details</summary>
Motivation: 解决在回归问题中满足公平性约束（如人口统计平等）的需求，同时避免现有方法对特定数据生成模型的依赖。

Method: 提出元定理，验证公平极小极大最优回归算法的普适性，并展示通过后处理方法实现公平回归。

Result: 证明了公平极小极大最优回归算法可以适用于多种场景，且后处理方法能有效实现公平性。

Conclusion: 通过元定理和后处理方法，研究者可以专注于改进传统回归技术，再高效地适配为公平回归。

Abstract: We address the regression problem under the constraint of demographic parity,
a commonly used fairness definition. Recent studies have revealed fair minimax
optimal regression algorithms, the most accurate algorithms that adhere to the
fairness constraint. However, these analyses are tightly coupled with specific
data generation models. In this paper, we provide meta-theorems that can be
applied to various situations to validate the fair minimax optimality of the
corresponding regression algorithms. Furthermore, we demonstrate that fair
minimax optimal regression can be achieved through post-processing methods,
allowing researchers and practitioners to focus on improving conventional
regression techniques, which can then be efficiently adapted for fair
regression.

</details>


### [142] [Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies](https://arxiv.org/abs/2506.13955)
*Matthew Lau,Tian-Yi Zhou,Xiangchi Yuan,Jizhou Chen,Wenke Lee,Xiaoming Huo*

Main category: stat.ML

TL;DR: 论文提出了一种半监督异常检测框架，结合已知和合成异常数据，理论证明其优越性，并在多个基准测试中验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 异常检测在多个领域至关重要，但现有方法在无监督或半监督场景下存在局限性，需要更有效的理论支持和方法。

Method: 扩展无监督异常检测原则，提出半监督框架，结合已知和合成异常数据训练分类器，并引入数学理论分析其有效性。

Result: 理论证明合成异常能优化低密度区域建模和神经网络分类器收敛性，实验在五个基准测试中验证了性能提升。

Conclusion: 提出的框架不仅理论上有保障，且具有广泛适用性，验证了合成异常在半监督异常检测中的普适价值。

Abstract: Anomaly detection (AD) is a critical task across domains such as
cybersecurity and healthcare. In the unsupervised setting, an effective and
theoretically-grounded principle is to train classifiers to distinguish normal
data from (synthetic) anomalies. We extend this principle to semi-supervised
AD, where training data also include a limited labeled subset of anomalies
possibly present in test time. We propose a theoretically-grounded and
empirically effective framework for semi-supervised AD that combines known and
synthetic anomalies during training. To analyze semi-supervised AD, we
introduce the first mathematical formulation of semi-supervised AD, which
generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i)
better anomaly modeling in low-density regions and (ii) optimal convergence
guarantees for neural network classifiers -- the first theoretical result for
semi-supervised AD. We empirically validate our framework on five diverse
benchmarks, observing consistent performance gains. These improvements also
extend beyond our theoretical framework to other classification-based AD
methods, validating the generalizability of the synthetic anomaly principle in
AD.

</details>


### [143] [Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms](https://arxiv.org/abs/2506.13984)
*Andrzej Cichocki*

Main category: stat.ML

TL;DR: 本文开发了一类广泛的Mirror Descent（MD）算法，利用Bregman散度和Tempesta多参数变形对数作为链接函数，生成灵活且适应性强的MD更新方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的MD算法框架，通过多参数对数调整以适应数据分布或几何特性，提升算法的灵活性和性能。

Method: 利用Bregman散度和Tempesta多参数变形对数作为链接函数，估计广义指数函数以近似其逆函数，并通过学习超参数优化算法。

Result: 生成了一个广泛且灵活的MD和mirror-less MD更新家族，能够适应不同数据分布和几何特性。

Conclusion: 多参数对数的应用为MD算法提供了新的灵活性和适应性，扩展了其应用范围。

Abstract: In this paper, we develop a wide class Mirror Descent (MD) algorithms, which
play a key role in machine learning. For this purpose we formulated the
constrained optimization problem, in which we exploits the Bregman divergence
with the Tempesta multi-parametric deformation logarithm as a link function.
This link function called also mirror function defines the mapping between the
primal and dual spaces and is associated with a very-wide (in fact,
theoretically infinite) class of generalized trace-form entropies. In order to
derive novel MD updates, we estimate generalized exponential function, which
closely approximates the inverse of the multi-parametric Tempesta generalized
logarithm. The shape and properties of the Tempesta logarithm and its
inverse-deformed exponential functions can be tuned by several hyperparameters.
By learning these hyperparameters, we can adapt to distribution or geometry of
training data, and we can adjust them to achieve desired properties of MD
algorithms. The concept of applying multi-parametric logarithms allow us to
generate a new wide and flexible family of MD and mirror-less MD updates.

</details>


### [144] [Estimation of Treatment Effects in Extreme and Unobserved Data](https://arxiv.org/abs/2506.14051)
*Jiyuan Tan,Jose Blanchet,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 论文提出了一种新框架，用于评估罕见极端事件中的因果效应，结合极值理论和多元正则变化方法，开发了一种一致性估计器。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要针对常见事件，而罕见但重要的事件（如极端气候事件）需要新的方法。

Method: 采用多元正则变化理论建模极端事件，开发一致性估计器并进行非渐近性能分析。

Result: 通过合成和半合成数据验证了估计器的性能。

Conclusion: 该框架为罕见极端事件的因果效应估计提供了有效工具。

Abstract: Causal effect estimation seeks to determine the impact of an intervention
from observational data. However, the existing causal inference literature
primarily addresses treatment effects on frequently occurring events. But what
if we are interested in estimating the effects of a policy intervention whose
benefits, while potentially important, can only be observed and measured in
rare yet impactful events, such as extreme climate events? The standard causal
inference methodology is not designed for this type of inference since the
events of interest may be scarce in the observed data and some degree of
extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies
for analyzing statistical phenomena in such extreme regimes. We introduce a
novel framework for assessing treatment effects in extreme data to capture the
causal effect at the occurrence of rare events of interest. In particular, we
employ the theory of multivariate regular variation to model extremities. We
develop a consistent estimator for extreme treatment effects and present a
rigorous non-asymptotic analysis of its performance. We illustrate the
performance of our estimator using both synthetic and semi-synthetic data.

</details>


### [145] [Universal Rates of ERM for Agnostic Learning](https://arxiv.org/abs/2506.14110)
*Steve Hanneke,Mingyue Xu*

Main category: stat.ML

TL;DR: 本文研究了二元分类在不可知设置下通过ERM的通用学习速率问题，揭示了三种可能的速率：指数衰减、亚平方根衰减或任意慢速，并完整分类了概念类。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在可实现设置下的通用学习速率，而不可知设置下的研究较少，本文填补了这一空白。

Method: 通过分析ERM在不可知设置下的学习曲线，探索了通用速率的可能性，并提出了一个紧凑的三分法。

Result: 揭示了三种可能的通用速率：$e^{-n}$、$o(n^{-1/2})$或任意慢速，并完整分类了概念类。

Conclusion: 本文为不可知设置下的通用学习速率提供了完整的理论框架，扩展了现有研究的范围。

Abstract: The universal learning framework has been developed to obtain guarantees on
the learning rates that hold for any fixed distribution, which can be much
faster than the ones uniformly hold over all the distributions. Given that the
Empirical Risk Minimization (ERM) principle being fundamental in the PAC theory
and ubiquitous in practical machine learning, the recent work of
arXiv:2412.02810 studied the universal rates of ERM for binary classification
under the realizable setting. However, the assumption of realizability is too
restrictive to hold in practice. Indeed, the majority of the literature on
universal learning has focused on the realizable case, leaving the
non-realizable case barely explored.
  In this paper, we consider the problem of universal learning by ERM for
binary classification under the agnostic setting, where the ''learning curve"
reflects the decay of the excess risk as the sample size increases. We explore
the possibilities of agnostic universal rates and reveal a compact trichotomy:
there are three possible agnostic universal rates of ERM, being either
$e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete
characterization of which concept classes fall into each of these categories.
Moreover, we also establish complete characterizations for the target-dependent
universal rates as well as the Bayes-dependent universal rates.

</details>


### [146] [Adjustment for Confounding using Pre-Trained Representations](https://arxiv.org/abs/2506.14329)
*Rickmer Schulte,David Rügamer,Thomas Nagler*

Main category: stat.ML

TL;DR: 论文探讨了如何利用预训练神经网络的潜在特征来调整非表格数据（如图像和文本）中的混杂因素，以改进平均处理效应（ATE）估计。


<details>
  <summary>Details</summary>
Motivation: 非表格数据可能作为混杂因素，忽略它们会导致结果偏差和科学结论错误。因此，需要结合预训练神经网络的特征提取能力来解决这一问题。

Method: 通过预训练神经网络的潜在特征调整混杂因素，并形式化这些特征在ATE估计中的有效性条件，以双机器学习为例进行验证。

Result: 研究表明，神经网络能够适应学习问题的内在稀疏性和维度，实现快速收敛速率，克服高维和非可识别性带来的挑战。

Conclusion: 尽管潜在特征学习存在高维和非可识别性问题，但神经网络通过适应内在稀疏性和维度，仍能有效支持ATE估计。

Abstract: There is growing interest in extending average treatment effect (ATE)
estimation to incorporate non-tabular data, such as images and text, which may
act as sources of confounding. Neglecting these effects risks biased results
and flawed scientific conclusions. However, incorporating non-tabular data
necessitates sophisticated feature extractors, often in combination with ideas
of transfer learning. In this work, we investigate how latent features from
pre-trained neural networks can be leveraged to adjust for sources of
confounding. We formalize conditions under which these latent features enable
valid adjustment and statistical inference in ATE estimation, demonstrating
results along the example of double machine learning. We discuss critical
challenges inherent to latent feature learning and downstream parameter
estimation arising from the high dimensionality and non-identifiability of
representations. Common structural assumptions for obtaining fast convergence
rates with additive or sparse linear models are shown to be unrealistic for
latent features. We argue, however, that neural networks are largely
insensitive to these issues. In particular, we show that neural networks can
achieve fast convergence rates by adapting to intrinsic notions of sparsity and
dimension of the learning problem.

</details>


### [147] [Adaptive Data Augmentation for Thompson Sampling](https://arxiv.org/abs/2506.14479)
*Wonyoung Kim*

Main category: stat.ML

TL;DR: 本文提出了一种近乎极小极大最优的Thompson Sampling方法，用于线性上下文赌博机问题，通过设计一种新颖的估计器，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Thompson Sampling在实证中表现良好，但其遗憾边界并非最优。本文旨在解决这一问题。

Method: 开发了一种带有自适应增强和耦合假设样本的新颖估计器，用于高效参数学习。

Result: 实证结果显示，该方法性能稳健，显著优于现有方法。

Conclusion: 提出的方法在不依赖上下文分布假设的情况下，实现了近乎极小极大最优的遗憾边界。

Abstract: In linear contextual bandits, the objective is to select actions that
maximize cumulative rewards, modeled as a linear function with unknown
parameters. Although Thompson Sampling performs well empirically, it does not
achieve optimal regret bounds. This paper proposes a nearly minimax optimal
Thompson Sampling for linear contextual bandits by developing a novel estimator
with the adaptive augmentation and coupling of the hypothetical samples that
are designed for efficient parameter learning. The proposed estimator
accurately predicts rewards for all arms without relying on assumptions for the
context distribution. Empirical results show robust performance and significant
improvement over existing methods.

</details>


### [148] [Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters](https://arxiv.org/abs/2506.14530)
*Anastasis Kratsios,Tin Sum Cheng,Aurelien Lucchi,Haitz Sáez de Ocáriz Borde*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted
parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent
work has highlighted an inherent asymmetry in the initialization of LoRA's
low-rank factors, which has been present since its inception and was presumably
derived experimentally. This paper focuses on providing a comprehensive
theoretical characterization of asymmetric LoRA with frozen random factors.
First, while existing research provides upper-bound generalization guarantees
based on averages over multiple experiments, the behaviour of a single
fine-tuning run with specific random factors remains an open question. We
address this by investigating the concentration of the typical LoRA
generalization gap around its mean. Our main upper bound reveals a sample
complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with
high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we
also determine the fundamental limits in terms of sample efficiency,
establishing a matching lower bound of
$\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the
practical scenario of a single fine-tuning run, our findings offer crucial
insights into the reliability and practicality of asymmetric LoRA.

</details>


### [149] [Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means](https://arxiv.org/abs/2506.14673)
*Mikael Møller Høgsgaard,Andrea Paudice*

Main category: stat.ML

TL;DR: 本文研究了在数据分布仅具有前p阶矩（p∈(1,2]）的情况下，使用中位数均值（MoM）估计器同时估计函数类F中每个函数均值的性能，并提出了新的样本复杂度界限和应用。


<details>
  <summary>Details</summary>
Motivation: 在重尾数据背景下，MoM估计器因其鲁棒性而受到关注，但对其在函数类均值估计中的应用研究较少。

Method: 采用新颖的对称化技术，推导出样本复杂度界限。

Result: 证明了新的样本复杂度界限，并展示了其在无界输入的k均值聚类和一般损失线性回归中的应用。

Conclusion: MoM估计器在重尾数据和多任务估计中具有潜力，且对称化技术可能对其他研究有独立价值。

Abstract: The Median of Means (MoM) is a mean estimator that has gained popularity in
the context of heavy-tailed data. In this work, we analyze its performance in
the task of simultaneously estimating the mean of each function in a class
$\mathcal{F}$ when the data distribution possesses only the first $p$ moments
for $p \in (1,2]$. We prove a new sample complexity bound using a novel
symmetrization technique that may be of independent interest. Additionally, we
present applications of our result to $k$-means clustering with unbounded
inputs and linear regression with general losses, improving upon existing
works.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [150] [Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents](https://arxiv.org/abs/2506.13783)
*Soyeon Choi,Kangwook Lee,Oliver Sng,Joshua M. Ackerman*

Main category: physics.soc-ph

TL;DR: 研究通过生成代理模型（GABM）发现，传染病威胁显著减少代理的社交行为，验证了行为免疫系统的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨传染病威胁如何影响生成代理的社交行为，验证行为免疫系统的理论。

Method: 使用基于大语言模型的生成代理模型（GABM），模拟代理在传染病新闻影响下的行为变化。

Result: 代理在传染病新闻下社交行为显著减少，包括减少社交活动、公共场所访问和对话。

Conclusion: GABM可作为研究复杂人类社交动态的有效实验工具。

Abstract: How does the threat of infectious disease influence sociality among
generative agents? We used generative agent-based modeling (GABM), powered by
large language models, to experimentally test hypotheses about the behavioral
immune system. Across three simulation runs, generative agents who read news
about an infectious disease outbreak showed significantly reduced social
engagement compared to agents who received no such news, including lower
attendance at a social gathering, fewer visits to third places (e.g., cafe,
store, park), and fewer conversations throughout the town. In interview
responses, agents explicitly attributed their behavioral changes to
disease-avoidance motivations. A validity check further indicated that they
could distinguish between infectious and noninfectious diseases, selectively
reducing social engagement only when there was a risk of infection. Our
findings highlight the potential of GABM as an experimental tool for exploring
complex human social dynamics at scale.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [151] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

TL;DR: 研究分析了文本到图像（T2I）模型在生成图像时如何复制和放大社会偏见，发现其在性别、种族等方面的显著差异，并呼吁更包容的数据集和开发实践。


<details>
  <summary>Details</summary>
Motivation: 探讨T2I模型在生成图像时可能复制和放大社会偏见的问题。

Method: 使用Stable Diffusion 1.5和Flux-1模型生成16,000多张图像，并收集8,000张Google图像进行比较分析。

Result: 发现生成的图像在性别、种族等方面存在显著差异，且这些差异往往强化了社会偏见。

Conclusion: 强调需要更包容的数据集和开发实践，以促进生成视觉系统的公平性。

Abstract: Text-to-Image (T2I) models have transformed visual content creation,
producing highly realistic images from natural language prompts. However,
concerns persist around their potential to replicate and magnify existing
societal biases. To investigate these issues, we curated a diverse set of
prompts spanning thematic categories such as occupations, traits, actions,
ideologies, emotions, family roles, place descriptions, spirituality, and life
events. For each of the 160 unique topics, we crafted multiple prompt
variations to reflect a wide range of meanings and perspectives. Using Stable
Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original
checkpoints, we generated over 16,000 images under consistent settings.
Additionally, we collected 8,000 comparison images from Google Image Search.
All outputs were filtered to exclude abstract, distorted, or nonsensical
results. Our analysis reveals significant disparities in the representation of
gender, race, age, somatotype, and other human-centric factors across generated
images. These disparities often mirror and reinforce harmful stereotypes
embedded in societal narratives. We discuss the implications of these findings
and emphasize the need for more inclusive datasets and development practices to
foster fairness in generative visual systems.

</details>


### [152] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAN-RM的高效奖励建模框架，通过对抗训练避免人工标注偏好数据和显式质量维度设计，仅需少量目标样本即可实现。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模方法依赖大量人工标注或复杂质量维度设计，实现复杂且不完整。

Method: 利用对抗训练，通过区分目标样本和模型生成输出来训练奖励模型，仅需少量目标样本。

Result: 实验证明GAN-RM在多项关键应用中有效，包括测试时样本过滤和训练后优化方法。

Conclusion: GAN-RM提供了一种高效且无需人工干预的奖励建模解决方案。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for
post-training enhancement of visual generative models. However, current
approaches of reward modeling suffer from implementation complexity due to
their reliance on extensive human-annotated preference data or meticulously
engineered quality dimensions that are often incomplete and
engineering-intensive. Inspired by adversarial training in generative
adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward
modeling framework that eliminates manual preference annotation and explicit
quality dimension engineering. Our method trains the reward model through
discrimination between a small set of representative, unpaired target
samples(denoted as Preference Proxy Data) and model-generated ordinary outputs,
requiring only a few hundred target samples. Comprehensive experiments
demonstrate our GAN-RM's effectiveness across multiple key applications
including test-time scaling implemented as Best-of-N sample filtering,
post-training approaches like Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO).

</details>


### [153] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/abs/2506.13993)
*Michelangelo Conserva,Alex Wilson,Charlotte Stanton,Vishal Batchu,Varun Gulshan*

Main category: cs.CV

TL;DR: Farmscapes是一个高分辨率（25厘米）的英格兰农村景观特征地图，通过深度学习模型生成，为生态学家和政策制定者提供开放工具。


<details>
  <summary>Details</summary>
Motivation: 全球生物多样性目标需要详细的生态地图支持，但目前缺乏大规模、高分辨率的地图。

Method: 使用深度学习分割模型，基于942个手动标注的航拍图像块训练，生成地图。

Result: 模型准确识别关键栖息地（林地96%，农田95%），线性特征（树篱72%）分割能力强。

Conclusion: Farmscapes为栖息地恢复、生物多样性监测和景观连通性分析提供了数据支持。

Abstract: Effective management of agricultural landscapes is critical for meeting
global biodiversity targets, but efforts are hampered by the absence of
detailed, large-scale ecological maps. To address this, we introduce
Farmscapes, the first large-scale (covering most of England), high-resolution
(25cm) map of rural landscape features, including ecologically vital elements
like hedgerows, woodlands, and stone walls. This map was generated using a deep
learning segmentation model trained on a novel, dataset of 942 manually
annotated tiles derived from aerial imagery. Our model accurately identifies
key habitats, achieving high f1-scores for woodland (96\%) and farmed land
(95\%), and demonstrates strong capability in segmenting linear features, with
an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google
Earth Engine, we provide a powerful, open-access tool for ecologists and
policymakers. This work enables data-driven planning for habitat restoration,
supports the monitoring of initiatives like the EU Biodiversity Strategy, and
lays the foundation for advanced analysis of landscape connectivity.

</details>


### [154] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种基于图像扩散模型的功能映射优化方法，通过训练模型在功能映射空间中生成精确映射，并在推理时利用点映射作为指导。


<details>
  <summary>Details</summary>
Motivation: 现有功能映射优化方法效率不高，且难以满足多种目标（如正交性和与拉普拉斯-贝尔特拉米算子的交换性）。

Method: 将功能映射视为2D图像，训练图像扩散模型直接在功能映射空间中生成优化映射，推理时利用点映射作为指导。

Result: 方法在功能映射优化任务中与现有最优方法竞争，且效率高。

Conclusion: 基于引导扩散模型的功能映射优化方法具有潜力，为功能映射处理提供了新途径。

Abstract: We propose a novel approach for refining a given correspondence map between
two shapes. A correspondence map represented as a functional map, namely a
change of basis matrix, can be additionally treated as a 2D image. With this
perspective, we train an image diffusion model directly in the space of
functional maps, enabling it to generate accurate maps conditioned on an
inaccurate initial map. The training is done purely in the functional space,
and thus is highly efficient. At inference time, we use the pointwise map
corresponding to the current functional map as guidance during the diffusion
process. The guidance can additionally encourage different functional map
objectives, such as orthogonality and commutativity with the Laplace-Beltrami
operator. We show that our approach is competitive with state-of-the-art
methods of map refinement and that guided diffusion models provide a promising
pathway to functional map processing.

</details>


### [155] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
*Hongyu Wang,Jiayu Xu,Ruiping Wang,Yan Feng,Yitao Zhai,Peng Pei,Xunliang Cai,Xilin Chen*

Main category: cs.CV

TL;DR: MoTE是一种高效训练低精度专家模型的方法，通过三元参数{-1, 0, 1}减少内存占用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态混合专家模型在边缘设备上内存占用高的问题。

Method: 使用预训练FFN作为共享专家，训练三元参数的路由专家。

Result: MoTE在相同内存占用下性能优于全精度模型，结合量化后性能提升4.3%。

Conclusion: MoTE为内存受限设备提供了一种高效且性能优越的解决方案。

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size
to boost performance while maintaining fixed active parameters. However,
previous works primarily utilized full-precision experts during sparse
up-cycling. Despite they show superior performance on end tasks, the large
amount of experts introduces higher memory footprint, which poses significant
challenges for the deployment on edge devices. In this work, we propose MoTE, a
scalable and memory-efficient approach to train Mixture-of-Ternary-Experts
models from dense checkpoint. Instead of training fewer high-precision experts,
we propose to train more low-precision experts during up-cycling. Specifically,
we use the pre-trained FFN as a shared expert and train ternary routed experts
with parameters in {-1, 0, 1}. Extensive experiments show that our approach has
promising scaling trend along model size. MoTE achieves comparable performance
to full-precision baseline MoE-LLaVA while offering lower memory footprint.
Furthermore, our approach is compatible with post-training quantization methods
and the advantage further amplifies when memory-constraint goes lower. Given
the same amount of expert memory footprint of 3.4GB and combined with
post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%
average accuracy on end tasks, demonstrating its effectiveness and potential
for memory-constrained devices.

</details>


### [156] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/abs/2506.14440)
*David E. Hernandez,Jose Chang,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 提出一种基于积分梯度（IG）增强的知识蒸馏方法，显著提升模型压缩效果，并在CIFAR-10上实现92.6%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 为在资源受限设备上部署深度学习模型，需高效压缩模型同时保持性能。

Method: 通过将IG图叠加到输入图像上，增强知识蒸馏过程，使学生模型更深入理解教师模型的决策。

Result: 在CIFAR-10上，压缩率4.1倍时准确率提升1.1个百分点（92.6% vs 91.5%），推理时间从140ms降至13ms。

Conclusion: IG增强的知识蒸馏是一种高效且通用的模型压缩方法，适用于边缘设备部署。

Abstract: Model compression is critical for deploying deep learning models on
resource-constrained devices. We introduce a novel method enhancing knowledge
distillation with integrated gradients (IG) as a data augmentation strategy.
Our approach overlays IG maps onto input images during training, providing
student models with deeper insights into teacher models' decision-making
processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented
knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression
factor-a significant 1.1 percentage point improvement ($p<0.001$) over
non-distilled models (91.5%). This compression reduces inference time from 140
ms to 13 ms. Our method precomputes IG maps before training, transforming
substantial runtime costs into a one-time preprocessing step. Our comprehensive
experiments include: (1) comparisons with attention transfer, revealing
complementary benefits when combined with our approach; (2) Monte Carlo
simulations confirming statistical robustness; (3) systematic evaluation of
compression factor versus accuracy trade-offs across a wide range (2.2x-1122x);
and (4) validation on an ImageNet subset aligned with CIFAR-10 classes,
demonstrating generalisability beyond the initial dataset. These extensive
ablation studies confirm that IG-based knowledge distillation consistently
outperforms conventional approaches across varied architectures and compression
ratios. Our results establish this framework as a viable compression technique
for real-world deployment on edge devices while maintaining competitive
accuracy.

</details>


### [157] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/abs/2506.14473)
*Zhijing Wan,Zhixiang Wang,Zheng Wang,Xin Xu,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 论文探讨了基于基础模型（FMs）的一次性子集选择方法是否优于传统信息提取器（IEs），并提出了一种针对细粒度数据集的新方法RAM-APL。


<details>
  <summary>Details</summary>
Motivation: 传统IEs依赖于目标数据集且性能有限，而FMs可能提供更通用的解决方案。研究旨在验证FMs在子集选择中的优势及其在不同数据集上的表现差异。

Method: 提出RAM-APL方法，利用多个FMs的互补优势，通过伪类标签的排名平均准确率来优化子集选择。

Result: FMs在细粒度数据集上表现优于传统IEs，但在粗粒度数据集上优势减弱。RAM-APL在多个细粒度数据集上达到最优性能。

Conclusion: FMs在子集选择中具有潜力，尤其是针对细粒度数据集。RAM-APL方法显著提升了性能，为未来研究提供了新方向。

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning
training costs by identifying an informative data subset based on the
information extracted by an information extractor (IE). Traditional IEs,
typically pre-trained on the target dataset, are inherently dataset-dependent.
Foundation models (FMs) offer a promising alternative, potentially mitigating
this limitation. This work investigates two key questions: (1) Can FM-based
subset selection outperform traditional IE-based methods across diverse
datasets? (2) Do all FMs perform equally well as IEs for subset selection?
Extensive experiments uncovered surprising insights: FMs consistently
outperform traditional IEs on fine-grained datasets, whereas their advantage
diminishes on coarse-grained datasets with noisy labels. Motivated by these
finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a
method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs
to enhance subset selection by exploiting their complementary strengths. Our
approach achieves state-of-the-art performance on fine-grained datasets,
including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [158] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Main category: cs.CV

TL;DR: 提出了一种新的可解释机器学习方法，通过多任务预测建模预测膝关节骨关节炎（OA）进展风险，并生成高质量未来图像以提高临床实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏可解释性且复杂，难以临床采用，且无法定位解剖标志。

Method: 利用扩散模型在类别条件潜在空间中生成未来图像，结合多任务预测建模分类未来OA严重程度并预测解剖标志。

Result: 在Osteoarthritis Initiative数据集上，AUC提升2%至0.71，推理时间缩短约9%。

Conclusion: 该方法在预测OA进展和图像生成方面优于现有技术，同时提高了可解释性和效率。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA)
risk by enabling early detection and disease monitoring. Recent machine
learning methods have improved risk estimation (i.e., predicting the likelihood
of disease progression) and predictive modelling (i.e., the forecasting of
future outcomes based on current data) using medical images, but clinical
adoption remains limited due to their lack of interpretability. Existing
approaches that generate future images for risk estimation are complex and
impractical. Additionally, previous methods fail to localize anatomical knee
landmarks, limiting interpretability. We address these gaps with a new
interpretable machine learning method to estimate the risk of knee OA
progression via multi-task predictive modelling that classifies future knee OA
severity and predicts anatomical knee landmarks from efficiently generated
high-quality future images. Such image generation is achieved by leveraging a
diffusion model in a class-conditioned latent space to forecast disease
progression, offering a visual representation of how particular health
conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our
approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71
in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [159] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Main category: cs.CV

TL;DR: 论文提出了一种名为Align Your Flow的流映射模型，通过新的连续时间目标和训练技术，改进了现有的一致性模型和流匹配目标，实现了高效的少步生成性能。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型虽然是最先进的生成方法，但需要多步采样，而一致性模型虽能一步生成，但性能随步数增加而下降。流映射通过连接任意两个噪声级别，解决了这一问题。

Method: 提出了两种新的连续时间目标用于训练流映射，结合自引导和对抗微调技术，提升了性能。

Result: 在ImageNet 64x64和512x512上实现了少步生成的SOTA性能，并在文本到图像任务中超越了现有非对抗训练的少步采样器。

Conclusion: 流映射模型在高效生成和性能上优于现有方法，适用于多种生成任务。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative
modeling approaches, but they require many sampling steps. Consistency models
can distill these models into efficient one-step generators; however, unlike
flow- and diffusion-based methods, their performance inevitably degrades when
increasing the number of steps, which we show both analytically and
empirically. Flow maps generalize these approaches by connecting any two noise
levels in a single step and remain effective across all step counts. In this
paper, we introduce two new continuous-time objectives for training flow maps,
along with additional novel training techniques, generalizing existing
consistency and flow matching objectives. We further demonstrate that
autoguidance can improve performance, using a low-quality model for guidance
during distillation, and an additional boost can be achieved by adversarial
finetuning, with minimal loss in sample diversity. We extensively validate our
flow map models, called Align Your Flow, on challenging image generation
benchmarks and achieve state-of-the-art few-step generation performance on both
ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,
we show text-to-image flow map models that outperform all existing
non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [160] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于逆问题的图像恢复方法，适用于无配对数据集，通过条件流匹配和分布匹配损失，在去模糊和非均匀点扩散函数校准等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要完整前向模型或配对数据的限制，适用于前向模型未知或数据收集困难的真实场景。

Method: 利用条件流匹配建模退化观测分布，同时通过分布匹配损失学习前向模型。

Result: 在去模糊和非均匀点扩散函数校准任务中优于单图像盲方法和无监督方法，在盲超分辨率任务中达到先进水平。

Conclusion: 该方法在真实场景中表现优异，且数据需求低，适用于传统耗时或需要专业设备的任务。

Abstract: This work addresses image restoration tasks through the lens of inverse
problems using unpaired datasets. In contrast to traditional approaches --
which typically assume full knowledge of the forward model or access to paired
degraded and ground-truth images -- the proposed method operates under minimal
assumptions and relies only on small, unpaired datasets. This makes it
particularly well-suited for real-world scenarios, where the forward model is
often unknown or misspecified, and collecting paired data is costly or
infeasible. The method leverages conditional flow matching to model the
distribution of degraded observations, while simultaneously learning the
forward model via a distribution-matching loss that arises naturally from the
framework. Empirically, it outperforms both single-image blind and unsupervised
approaches on deblurring and non-uniform point spread function (PSF)
calibration tasks. It also matches state-of-the-art performance on blind
super-resolution. We also showcase the effectiveness of our method with a proof
of concept for lens calibration: a real-world application traditionally
requiring time-consuming experiments and specialized equipment. In contrast,
our approach achieves this with minimal data acquisition effort.

</details>


### [161] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Main category: cs.CV

TL;DR: 提出了一种根据提示复杂度动态分配计算资源的框架，以优化扩散模型的质量与计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量图像但计算成本高，需要平衡质量与效率。

Method: 通过自动路由提示到最适合的生成函数（如不同步数的扩散模型或其他独立模型），实现计算资源的动态分配。

Result: 在COCO和DiffusionDB上验证，路由到九个预训练模型后，平均质量高于单一模型。

Conclusion: 该方法通过学习动态路由，实现了质量与计算成本的最优权衡。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity
image for an input prompt through an iterative denoising process.
Unfortunately, the high fidelity also comes at a high computational cost due
the inherently sequential generative process. In this work, we seek to
optimally balance quality and computational cost, and propose a framework to
allow the amount of computation to vary for each prompt, depending on its
complexity. Each prompt is automatically routed to the most appropriate
text-to-image generation function, which may correspond to a distinct number of
denoising steps of a diffusion model, or a disparate, independent text-to-image
model. Unlike uniform cost reduction techniques (e.g., distillation, model
quantization), our approach achieves the optimal trade-off by learning to
reserve expensive choices (e.g., 100+ denoising steps) only for a few complex
prompts, and employ more economical choices (e.g., small distilled model) for
less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB
that by learning to route to nine already-trained text-to-image models, our
approach is able to deliver an average quality that is higher than that
achievable by any of these models alone.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [162] [Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion](https://arxiv.org/abs/2506.14488)
*Dong Xu,Zhangfan Yang,Ka-chun Wong,Zexuan Zhu,Jiangqiang Li,Junkai Ji*

Main category: q-bio.BM

TL;DR: READ方法结合检索增强生成与SE(3)-等变扩散模型，解决了分子设计中几何拟合与化学约束的平衡问题，表现优于现有生成模型。


<details>
  <summary>Details</summary>
Motivation: 解决分子设计中几何拟合与化学约束的平衡问题，提升药物早期发现的效率。

Method: 引入READ方法，结合检索增强生成与SE(3)-等变扩散模型，通过预训练编码器检索支架图嵌入指导反向扩散。

Result: 在CBGBench中表现优异，超越现有生成模型及天然配体。

Conclusion: 检索与扩散的协同优化可加速结构药物设计，提高可靠性。

Abstract: Breakthroughs in high-accuracy protein structure prediction, such as
AlphaFold, have established receptor-based molecule design as a critical driver
for rapid early-phase drug discovery. However, most approaches still struggle
to balance pocket-specific geometric fit with strict valence and synthetic
constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion
termed READ is introduced, which is the first to merge molecular
Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model.
Specifically, a contrastively pre-trained encoder aligns atom-level
representations during training, then retrieves graph embeddings of
pocket-matched scaffolds to guide each reverse-diffusion step at inference.
This single mechanism can inject real-world chemical priors exactly where
needed, producing valid, diverse, and shape-complementary ligands. Experimental
results demonstrate that READ can achieve very competitive performance in
CBGBench, surpassing state-of-the-art generative models and even native
ligands. That suggests retrieval and diffusion can be co-optimized for faster,
more reliable structure-based drug design.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [163] [DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models](https://arxiv.org/abs/2506.13817)
*Saleem A. Al Dajani,Abel Sanchez,John R. Williams*

Main category: q-bio.GN

TL;DR: 提出了一种基于生成式AI基础模型的方法，通过实时网络搜索自动标注单细胞RNA测序数据，准确率达82.5%，解决了监督学习中的标注瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据规模迅速扩大，传统人工标注效率低且易出错，亟需自动化解决方案。

Method: 采用代理基础模型结合实时网络搜索，实现实验数据的自动标注。

Result: 标注准确率达到82.5%，显著提升了标注效率，支持下游任务如细胞分型和扰动预测。

Conclusion: 该方法展示了在健康监测和诊断领域的创新潜力，未来可能超越人工标注性能。

Abstract: Generative AI foundation models offer transformative potential for processing
structured biological data, particularly in single-cell RNA sequencing, where
datasets are rapidly scaling toward billions of cells. We propose the use of
agentic foundation models with real-time web search to automate the labeling of
experimental data, achieving up to 82.5% accuracy. This addresses a key
bottleneck in supervised learning for structured omics data by increasing
annotation throughput without manual curation and human error. Our approach
enables the development of virtual cell foundation models capable of downstream
tasks such as cell-typing and perturbation prediction. As data volume grows,
these models may surpass human performance in labeling, paving the way for
reliable inference in large-scale perturbation screens. This application
demonstrates domain-specific innovation in health monitoring and diagnostics,
aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [164] [The Perception of Phase Intercept Distortion and its Application in Data Augmentation](https://arxiv.org/abs/2506.14571)
*Venkatakrishnan Vaidyanathapuram Krishnan,Nathaniel Condit-Schultz*

Main category: eess.SP

TL;DR: 本文研究了相位截断失真（一种频率无关的相位偏移）的感知特性及其在机器学习数据增强中的应用。


<details>
  <summary>Details</summary>
Motivation: 探讨相位截断失真是否可感知，并验证其作为数据增强方法的有效性。

Method: 通过人类受试者实验验证失真感知性，并在音频机器学习任务中应用该失真进行数据增强实验。

Result: 实验表明相位截断失真不可感知，且作为数据增强方法能提升模型性能。

Conclusion: 相位截断失真是一种有效的、不可感知的数据增强手段，适用于音频机器学习任务。

Abstract: Phase distortion refers to the alteration of the phase relationships between
frequencies in a signal, which can be perceptible. In this paper, we discuss a
special case of phase distortion known as phase-intercept distortion, which is
created by a frequency-independent phase shift. We hypothesize that, though
this form of distortion changes a signal's waveform significantly, the
distortion is imperceptible. Human-subject experiment results are reported
which are consistent with this hypothesis. Furthermore, we discuss how the
imperceptibility of phase-intercept distortion can be useful for machine
learning, specifically for data augmentation. We conducted multiple experiments
using phase-intercept distortion as a novel approach to data augmentation, and
obtained improved results for audio machine learning tasks.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [165] [SETI@home: Data Acquisition and Front-End Processing](https://arxiv.org/abs/2506.14718)
*Eric J. Korpela,David P. Anderson,Jeff Cobb,Matt Lebofsky,Wei Liu,Dan Werthimer*

Main category: astro-ph.IM

TL;DR: SETI@home是一个利用志愿者家庭计算机分析射电数据的SETI项目，通过分布式计算提高搜索灵敏度和通用性，使用相干积分和多普勒漂移率搜索多种信号类型。


<details>
  <summary>Details</summary>
Motivation: 传统SETI项目依赖专用硬件处理数据，SETI@home通过分布式计算利用大量家庭计算机的计算能力，以更高效和灵活的方式搜索外星技术信号。

Method: 项目将时域数据分发到超过10^5台志愿计算机，采用相干积分技术分析数据，搜索123,000种多普勒漂移率和多种信号类型，并使用不同分辨率的DFT。

Result: 前端生成超过1.2×10^10个检测结果，后端进一步筛选去除射频干扰，寻找可能的外星信号。

Conclusion: SETI@home的前端设计有效提高了搜索能力，后端结果将在另一篇论文中详细描述。

Abstract: SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project,
looking for technosignatures in data recorded at multiple observatories from
1998 to 2020. Most radio SETI projects analyze data using dedicated processing
hardware. SETI@home uses a different approach: time-domain data is distributed
over the Internet to $\gt 10^{5}$ volunteered home computers, which analyze it.
The large amount of computing power this affords ($\sim 10^{15}$ floating-point
operations per second (FPOP/s)) allows us to increase the sensitivity and
generality of our search in three ways. We use coherent integration, a
technique in which data is transformed so that the power of drifting signals is
confined to a single discrete Fourier transform (DFT) bin. We perform this
coherent search over 123 000 Doppler drift rates in the range ($\pm$100 Hz
s$^{-1}$). Second, we search for a variety of signal types, such as pulsed
signals and arbitrary repeated waveforms. The analysis uses a range of DFT
sizes, with frequency resolutions ranging from 0.075 Hz to 1221 Hz. The front
end of SETI@home produces a set of detections that exceed thresholds in power
and goodness of fit. We accumulated $\sim 1.2\times 10^{10}$ such detections.
The back end of SETI@home takes these detections, identifies and removes radio
frequency interference (RFI), and looks for groups of detections that are
consistent with extraterrestrial origin and that persist over long timescales.
This paper describes the front end of SETI@home and provides parameters for the
primary data source, the Arecibo Observatory; the back end and its results are
described in a companion paper.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [166] [Connecting phases of matter to the flatness of the loss landscape in analog variational quantum algorithms](https://arxiv.org/abs/2506.13865)
*Kasidit Srimahajariyapong,Supanut Thanasilp,Thiparat Chotibut*

Main category: quant-ph

TL;DR: 研究了一种基于无序伊辛链的模拟变分量子算法（VQA），通过调节无序强度，分析了热化相和多体局域相（MBL）的表达能力和损失方差。结果表明，MBL相在训练初期更具优势，并提出了一种MBL初始化策略。


<details>
  <summary>Details</summary>
Motivation: 解决传统数字门基VQA的可扩展性问题（如贫瘠高原现象），探索模拟硬件VQA的潜力。

Method: 使用无序伊辛链的动力学构建模拟VQA，通过调节无序强度区分热化相和MBL相，分析表达能力和损失方差。

Result: 两种相在大M时均达到最大表达能力，但热化相在更小的M时出现贫瘠高原。MBL相在训练初期更具优势。

Conclusion: MBL初始化策略可平衡训练性和表达能力，为模拟硬件VQA的扩展提供实用指导。

Abstract: Variational quantum algorithms (VQAs) promise near-term quantum advantage,
yet parametrized quantum states commonly built from the digital gate-based
approach often suffer from scalability issues such as barren plateaus, where
the loss landscape becomes flat. We study an analog VQA ans\"atze composed of
$M$ quenches of a disordered Ising chain, whose dynamics is native to several
quantum simulation platforms. By tuning the disorder strength we place each
quench in either a thermalized phase or a many-body-localized (MBL) phase and
analyse (i) the ans\"atze's expressivity and (ii) the scaling of loss variance.
Numerics shows that both phases reach maximal expressivity at large $M$, but
barren plateaus emerge at far smaller $M$ in the thermalized phase than in the
MBL phase. Exploiting this gap, we propose an MBL initialisation strategy:
initialise the ans\"atze in the MBL regime at intermediate quench $M$, enabling
an initial trainability while retaining sufficient expressivity for subsequent
optimization. The results link quantum phases of matter and VQA trainability,
and provide practical guidelines for scaling analog-hardware VQAs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [167] [Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network](https://arxiv.org/abs/2506.13787)
*Yanjun Dai,Haoyang Feng,Yuan Gao*

Main category: cs.IR

TL;DR: 论文提出了一种解耦时序层次图神经网络（DTH-GNN），用于捕捉匿名用户交互网络的多尺度时序、语义和高阶依赖特征，显著提升了广告投放效果。


<details>
  <summary>Details</summary>
Motivation: 现有图模型难以捕捉匿名用户交互网络的多尺度时序、语义和高阶依赖特征，无法描述复杂行为模式。

Method: 1. 引入时序边分解，将交互分为短时突发、日周期和长时记忆三类通道；2. 构建层次异构聚合，通过元路径条件Transformer编码器结合子图；3. 提出反馈感知的对比正则化，优化节点表示。

Result: DTH-GNN的AUC提升了8.2%，对数损失降低了5.7%。

Conclusion: DTH-GNN有效解决了匿名用户行为建模的复杂性问题，显著提升了广告投放的精准度。

Abstract: While online advertising is highly dependent on implicit interaction networks
of anonymous users for engagement inference, and for the selection and
optimization of delivery strategies, existing graph models seldom can capture
the multi-scale temporal, semantic and higher-order dependency features of
these interaction networks, thus it's hard to describe the complicated patterns
of the anonymous behavior. In this paper, we propose Decoupled
Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main
contributions. Above all, we introduce temporal edge decomposition, which
divides each interaction into three types of channels: short-term burst,
diurnal cycle and long-range memory, and conducts feature extraction using the
convolution kernel of parallel dilated residuals; Furthermore, our model builds
a hierarchical heterogeneous aggregation, where user-user, user-advertisement,
advertisement-advertisement subgraphs are combined through the meta-path
conditional Transformer encoder, where the noise structure is dynamically
tamped down via the synergy of cross-channel self-attention and gating
relationship selector. Thirdly, the contrast regularity of feedback perception
is formulated, the consistency of various time slices is maximized, the entropy
of control exposure information with dual-view target is maximized, the global
prototype of dual-momentum queue distillation is presented, and the strategy
gradient layer with light weight is combined with delaying transformation
signal to fine-tune the node representation for benefit-oriented. The AUC of
DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in
comparison with the best baseline model.

</details>


### [168] [RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition](https://arxiv.org/abs/2506.14412)
*Tim Cofala,Oleh Astappiev,William Xion,Hailay Teklehaymanot*

Main category: cs.IR

TL;DR: LiveRAG 2025挑战赛探索了RAG解决方案，结合InstructRAG、Pinecone检索器和BGE重排器，在SIGIR 2025中排名第四。


<details>
  <summary>Details</summary>
Motivation: 通过结合LLMs的内部参数知识和外部非参数知识，提高事实准确性并减少幻觉。

Method: 使用InstructRAG结合Pinecone检索器和BGE重排器，在限制条件下优化RAG方案。

Result: 正确性得分1.13，忠实度得分0.55，排名第四。

Conclusion: InstructRAG结合特定检索器和重排器在RAG挑战中表现优异。

Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by
combining their internal, parametric knowledge with external, non-parametric
sources, with the goal of improving factual correctness and minimizing
hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize
accuracy on DataMorgana's QA pairs, which are composed of single-hop and
multi-hop questions. The challenge provides access to sparse OpenSearch and
dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to
LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A
judge-LLM assesses the submitted answers along with human evaluators. By
exploring distinct retriever combinations and RAG solutions under the challenge
conditions, our final solution emerged using InstructRAG in combination with a
Pinecone retriever and a BGE reranker. Our solution achieved a correctness
score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR
2025 LiveRAG Challenge.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [169] [A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations](https://arxiv.org/abs/2506.13835)
*Masakazu Inoue,Motoshige Sato,Kenichi Tomeoka,Nathania Nah,Eri Hatakeyama,Kai Arulkumaran,Ilya Horiguchi,Shuntaro Sasai*

Main category: q-bio.QM

TL;DR: 该研究提出了一种神经网络方法，用于处理异质电极放置的EEG/EMG数据，通过多任务训练在大规模数据集上实现无声语音解码，显著提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 无声语音解码可为言语障碍患者提供辅助技术，但数据收集困难且实验设置多样，难以构建大规模同质数据集。

Method: 引入能够处理异质电极放置的神经网络，通过多任务训练在大规模EEG/EMG数据集上进行无声语音解码。

Result: 在健康参与者中达到95.3%的单词分类准确率，言语障碍患者中达到54.5%，显著优于单被试数据训练的模型（70.1%和13.2%）。跨语言校准性能也有所提升。

Conclusion: 研究表明，开发实用的无声语音解码系统具有可行性，尤其对言语障碍患者具有重要意义。

Abstract: Silent speech decoding, which performs unvocalized human speech recognition
from electroencephalography/electromyography (EEG/EMG), increases accessibility
for speech-impaired humans. However, data collection is difficult and performed
using varying experimental setups, making it nontrivial to collect a large,
homogeneous dataset. In this study we introduce neural networks that can handle
EEG/EMG with heterogeneous electrode placements and show strong performance in
silent speech decoding via multi-task training on large-scale EEG/EMG datasets.
We achieve improved word classification accuracy in both healthy participants
(95.3%), and a speech-impaired patient (54.5%), substantially outperforming
models trained on single-subject data (70.1% and 13.2%). Moreover, our models
also show gains in cross-language calibration performance. This increase in
accuracy suggests the feasibility of developing practical silent speech
decoding systems, particularly for speech-impaired patients.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [170] [A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare](https://arxiv.org/abs/2506.13904)
*Ivania Donoso-Guzmán,Kristýna Sirka Kacafírková,Maxwell Szymanski,An Jacobs,Denis Parra,Katrien Verbert*

Main category: cs.HC

TL;DR: 论文提出一个框架和指南，用于评估医疗领域中可解释人工智能（XAI）的用户体验，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法在实际应用中的价值和有效性缺乏充分验证，尤其在医疗领域，缺乏明确的用户评估指南。

Method: 通过对82项医疗领域XAI用户研究的系统综述，结合预定义编码方案和迭代开发的归纳代码进行分析。

Result: 研究总结了当前评估实践，揭示了XAI解释属性间的关联，并提出了更新的框架和实用指南。

Conclusion: 该研究为跨学科团队设计针对特定应用场景的XAI评估策略提供了支持。

Abstract: Despite promising developments in Explainable Artificial Intelligence, the
practical value of XAI methods remains under-explored and insufficiently
validated in real-world settings. Robust and context-aware evaluation is
essential, not only to produce understandable explanations but also to ensure
their trustworthiness and usability for intended users, but tends to be
overlooked because of no clear guidelines on how to design an evaluation with
users.
  This study addresses this gap with two main goals: (1) to develop a framework
of well-defined, atomic properties that characterise the user experience of XAI
in healthcare; and (2) to provide clear, context-sensitive guidelines for
defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five
databases, all situated within healthcare settings and focused on evaluating
AI-generated explanations. The analysis was guided by a predefined coding
scheme informed by an existing evaluation framework, complemented by inductive
codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current
evaluation practices, highlighting a growing focus on human-centred approaches
in healthcare XAI; (2) insights into the interrelations among explanation
properties; and (3) an updated framework and a set of actionable guidelines to
support interdisciplinary teams in designing and implementing effective
evaluation strategies for XAI systems tailored to specific application
contexts.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [171] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
*Andrew Chang,Chenkai Hu,Ji Qi,Zhuojian Wei,Kexin Zhang,Viswadruth Akkaraju,David Poeppel,Dustin Freeman*

Main category: eess.AS

TL;DR: 论文提出了一种半监督学习方法，用于预测视频会议中不流畅或不愉快的时刻，通过多模态特征融合显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视频会议中的负面体验（如不流畅或不愉快）研究不足，且自然数据中这些时刻较少，监督学习需要昂贵的人工标注。

Method: 采用半监督学习（SSL），结合有标签和无标签的多模态（音频、面部、文本）数据，训练深度特征预测负面时刻。

Result: SSL模型在ROC-AUC（0.9）和F1分数（0.6）上优于监督学习模型，仅用8%标注数据即可达到监督学习96%的性能。

Conclusion: 该方法为视频会议体验建模提供了一种高效的标注框架。

Abstract: Group conversations over videoconferencing are a complex social behavior.
However, the subjective moments of negative experience, where the conversation
loses fluidity or enjoyment remain understudied. These moments are infrequent
in naturalistic data, and thus training a supervised learning (SL) model
requires costly manual data annotation. We applied semi-supervised learning
(SSL) to leverage targeted labeled and unlabeled clips for training multimodal
(audio, facial, text) deep features to predict non-fluid or unenjoyable moments
in holdout videoconference sessions. The modality-fused co-training SSL
achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by
up to 4% with the same amount of labeled data. Remarkably, the best SSL model
with just 8% labeled data matched 96% of the SL model's full-data performance.
This shows an annotation-efficient framework for modeling videoconference
experience.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [172] [The Synthetic Mirror -- Synthetic Data at the Age of Agentic AI](https://arxiv.org/abs/2506.13818)
*Marcelle Momha*

Main category: cs.CY

TL;DR: 论文探讨了合成数据对隐私和政策制定的影响，提出需调整现有法律框架以确保AI代理的信任与问责。


<details>
  <summary>Details</summary>
Motivation: 合成数据的广泛使用引发了信任和问责问题，亟需政策与法律框架的适应性调整。

Method: 通过分析合成数据的特性，提出对现有法律框架的针对性修正。

Result: 合成数据应被视为独特的监管类别，需专门政策工具支持。

Conclusion: 最实用的方法是针对性修正现有框架，而非创建全新制度。

Abstract: Synthetic data, which is artificially generated and intelligently mimicking
or supplementing the real-world data, is increasingly used. The proliferation
of AI agents and the adoption of synthetic data create a synthetic mirror that
conceptualizes a representation and potential distortion of reality, thus
generating trust and accountability deficits. This paper explores the
implications for privacy and policymaking stemming from synthetic data
generation, and the urgent need for new policy instruments and legal framework
adaptation to ensure appropriate levels of trust and accountability for AI
agents relying on synthetic data. Rather than creating entirely new policy or
legal regimes, the most practical approach involves targeted amendments to
existing frameworks, recognizing synthetic data as a distinct regulatory
category with unique characteristics.

</details>


### [173] [Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor](https://arxiv.org/abs/2506.14652)
*Alexandra Olteanu,Su Lin Blodgett,Agathe Balayn,Angelina Wang,Fernando Diaz,Flavio du Pin Calmon,Margaret Mitchell,Michael Ekstrand,Reuben Binns,Solon Barocas*

Main category: cs.CY

TL;DR: 论文主张AI研究中需要更广泛的严谨性概念，而不仅仅是方法论的严谨性，以解决负责任AI社区的担忧。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究过于关注方法论的严谨性，忽视了其他方面的严谨性，导致对AI能力的夸大宣称等问题。

Method: 提出一个更广泛的严谨性框架，包括方法论、知识背景、规范、概念清晰度、报告方式和证据推断六个方面。

Result: 提供了一个语言和框架，促进AI社区与研究者、政策制定者等之间的对话。

Conclusion: 呼吁AI研究与实践需要更全面的严谨性概念，以推动更负责任的AI发展。

Abstract: In AI research and practice, rigor remains largely understood in terms of
methodological rigor -- such as whether mathematical, statistical, or
computational methods are correctly applied. We argue that this narrow
conception of rigor has contributed to the concerns raised by the responsible
AI community, including overblown claims about AI capabilities. Our position is
that a broader conception of what rigorous AI research and practice should
entail is needed. We believe such a conception -- in addition to a more
expansive understanding of (1) methodological rigor -- should include aspects
related to (2) what background knowledge informs what to work on (epistemic
rigor); (3) how disciplinary, community, or personal norms, standards, or
beliefs influence the work (normative rigor); (4) how clearly articulated the
theoretical constructs under use are (conceptual rigor); (5) what is reported
and how (reporting rigor); and (6) how well-supported the inferences from
existing evidence are (interpretative rigor). In doing so, we also aim to
provide useful language and a framework for much-needed dialogue about the AI
community's work by researchers, policymakers, journalists, and other
stakeholders.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [174] [Asymptotically Smaller Encodings for Graph Problems and Scheduling](https://arxiv.org/abs/2506.14042)
*Bernardo Subercaseaux*

Main category: cs.LO

TL;DR: 论文提出了一种新的图问题编码方法，将CNF编码的约束数量从Ω(|V|²)减少到O(|V|²/lg|V|)，并展示了在密集区间图中独立集问题的新编码方法。


<details>
  <summary>Details</summary>
Motivation: 传统图问题编码方法需要大量约束，效率较低。本文旨在探索更高效的编码方式，减少约束数量。

Method: 利用Erdős、Chung和Spencer的双团覆盖结果，提出新的CNF编码方法，并针对密集区间图设计独立集的新编码。

Result: 成功将编码约束数量显著减少，并应用于字符串压缩和调度问题，降低了编码复杂度。

Conclusion: 新编码方法为图问题提供了更高效的解决方案，并展示了其在实际问题中的应用潜力。

Abstract: We show how several graph problems (e.g., vertex-cover, independent-set,
$k$-coloring) can be encoded into CNF using only $O(|V|^2 / \lg |V|)$ many
clauses, as opposed to the $\Omega(|V|^2)$ constraints used by standard
encodings. This somewhat surprising result is a simple consequence of a result
of Erd\H{o}s, Chung, and Spencer (1983) about biclique coverings of graphs, and
opens theoretical avenues to understand the success of "Bounded Variable
Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally,
we show a novel encoding for independent sets in some dense interval graphs
using only $O(|V| \lg |V|)$ clauses (the direct encoding uses $\Omega(|V|^2)$),
which we have successfully applied to a string-compression encoding posed by
Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the
encoding size of a scheduling problem posed by Mayank and Modal (2020) from
$O(NMT^2)$ to $O(NMT + M T^2 \lg T)$, where $N$ is the number of tasks, $T$ the
total timespan, and $M$ the number of machines.

</details>


### [175] [Varanus: Runtime Verification for CSP](https://arxiv.org/abs/2506.14426)
*Matt Luckcuck,Angelo Ferrando,Fatma Faruq*

Main category: cs.LO

TL;DR: 论文介绍了Varanus，一种基于CSP规范的运行时验证工具，适用于自主系统，并在模拟环境中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 自主系统在多变和未知环境中运行时，传统验证方法可能不适用，需要一种运行时验证工具来确保系统行为符合规范。

Method: 利用CSP（Communicating Sequential Processes）规范构建运行时验证工具Varanus，监控系统行为是否符合规范。

Result: Varanus能够线性时间内合成监控器，并在常数时间内检查事件，成功检测到规范违规。

Conclusion: Varanus是一种高效的运行时验证工具，适用于自主系统，并能复用设计阶段的CSP规范。

Abstract: Autonomous systems are often used in changeable and unknown environments,
where traditional verification may not be suitable. Runtime Verification (RV)
checks events performed by a system against a formal specification of its
intended behaviour, making it highly suitable for ensuring that an autonomous
system is obeying its specification at runtime. Communicating Sequential
Processes (CSP) is a process algebra usually used in static verification, which
captures behaviour as a trace of events, making it useful for RV as well.
Further, CSP has more recently been used to specify autonomous and robotic
systems. Though CSP is supported by two extant model checkers, so far it has no
RV tool. This paper presents Varanus, an RV tool that monitors a system against
an oracle built from a CSP specification. This approach enables the reuse
without modifications of a specification that was built, e.g during the
system's design. We describe the tool, apply it to a simulated autonomous
robotic rover inspecting a nuclear waste store, empirically comparing its
performance to two other RV tools using different languages, and demonstrate
how it can detect violations of the specification. Varanus can synthesise a
monitor from a CSP process in roughly linear time, with respect to the number
of states and transitions in the model; and checks each event in roughly
constant time.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [176] [Projecting U.S. coastal storm surge risks and impacts with deep learning](https://arxiv.org/abs/2506.13963)
*Julian R. Rice,Karthik Balaguru,Fadia Ticona Rollano,John Wilson,Brent Daniel,David Judi,Ning Sun,L. Ruby Leung*

Main category: physics.ao-ph

TL;DR: 论文利用深度学习模型评估美国沿海风暴潮风险，结合合成热带气旋事件和未来气候变化预测，发现本世纪末风险人口将增加50%，佛罗里达州风险显著上升。


<details>
  <summary>Details</summary>
Motivation: 热带气旋引发的风暴潮危害巨大，但由于其罕见性和物理复杂性，风险评估困难。人工智能在自然灾害建模中的应用为解决这一问题提供了新途径。

Method: 使用深度学习风暴潮模型，基于90万次合成热带气旋事件，结合未来热带气旋行为和海平面变化预测，评估沿海风暴潮风险。

Result: 模型结果与历史观测和其他建模技术一致，预测本世纪末风险人口将增加50%，佛罗里达州风险显著上升，乔治亚州和南卡罗来纳州存在关键阈值。

Conclusion: 深度学习模型为风暴潮风险评估提供了高效工具，未来气候变化将显著增加沿海地区风险，尤其是佛罗里达州。

Abstract: Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs),
yet assessing its current and future risk is difficult due to the phenomenon's
rarity and physical complexity. Recent advances in artificial intelligence
applications to natural hazard modeling suggest a new avenue for addressing
this problem. We utilize a deep learning storm surge model to efficiently
estimate coastal surge risk in the United States from 900,000 synthetic TC
events, accounting for projected changes in TC behavior and sea levels. The
derived historical 100-year surge (the event with a 1% yearly exceedance
probability) agrees well with historical observations and other modeling
techniques. When coupled with an inundation model, we find that heightened TC
intensities and sea levels by the end of the century result in a 50% increase
in population at risk. Key findings include markedly heightened risk in
Florida, and critical thresholds identified in Georgia and South Carolina.

</details>


### [177] [AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction](https://arxiv.org/abs/2506.14022)
*Jacob B. Landsberg,Elizabeth A. Barnes,Matthew Newman*

Main category: physics.ao-ph

TL;DR: 论文提出了一种基于可解释AI的模型类比预测方法，用于改进次季节到季节（S2S）预测，并在多个任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 次季节到季节预测对公共卫生、灾害准备和农业至关重要，但预测难度大，需要改进方法。

Method: 使用人工神经网络学习权重掩码以优化类比选择，并在三个不同预测任务中验证其效果。

Result: AI-informed方法在确定性及概率性指标上优于传统方法和基准，并能更好地预测极端温度和不确定性。

Conclusion: 可解释AI框架不仅提升了预测性能，还帮助理解S2S的可预测性来源。

Abstract: Subseasonal-to-seasonal forecasting is crucial for public health, disaster
preparedness, and agriculture, and yet it remains a particularly challenging
timescale to predict. We explore the use of an interpretable AI-informed model
analog forecasting approach, previously employed on longer timescales, to
improve S2S predictions. Using an artificial neural network, we learn a mask of
weights to optimize analog selection and showcase its versatility across three
varied prediction tasks: 1) classification of Week 3-4 Southern California
summer temperatures; 2) regional regression of Month 1 midwestern U.S. summer
temperatures; and 3) classification of Month 1-2 North Atlantic wintertime
upper atmospheric winds. The AI-informed analogs outperform traditional analog
forecasting approaches, as well as climatology and persistence baselines, for
deterministic and probabilistic skill metrics on both climate model and
reanalysis data. We find the analog ensembles built using the AI-informed
approach also produce better predictions of temperature extremes and improve
representation of forecast uncertainty. Finally, by using an interpretable-AI
framework, we analyze the learned masks of weights to better understand S2S
sources of predictability.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [178] [NeuralPDR: Neural Differential Equations as surrogate models for Photodissociation Regions](https://arxiv.org/abs/2506.14270)
*Gijs Vermariën,Thomas G. Bisbas,Serena Viti,Yue Zhao,Xuefei Tang,Rahul Ravichandran*

Main category: astro-ph.GA

TL;DR: 论文提出了一种替代化学求解器的代理模型（Latent Augmented Neural ODEs），用于高效模拟天体化学过程，显著提升了计算速度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率望远镜（如JWST和ALMA）需要更小尺度的天体化学模型，但传统三维流体动力学与化学耦合模拟计算成本极高。

Method: 使用Latent Augmented Neural ODEs作为代理模型，并在三个复杂度递增的数据集上训练，最后一个数据集来自3D-PDR模拟。

Result: 代理模型能快速重现原始观测数据，并在GPU上实现化学过程的快速推断。

Conclusion: 该方法为天体化学模拟提供了高效工具，支持快速统计推断和更高分辨率的流体动力学模拟。

Abstract: Computational astrochemical models are essential for helping us interpret and
understand the observations of different astrophysical environments. In the age
of high-resolution telescopes such as JWST and ALMA, the substructure of many
objects can be resolved, raising the need for astrochemical modeling at these
smaller scales, meaning that the simulations of these objects need to include
both the physics and chemistry to accurately model the observations. The
computational cost of the simulations coupling both the three-dimensional
hydrodynamics and chemistry is enormous, creating an opportunity for surrogate
models that can effectively substitute the chemical solver. In this work we
present surrogate models that can replace the original chemical code, namely
Latent Augmented Neural Ordinary Differential Equations. We train these
surrogate architectures on three datasets of increasing physical complexity,
with the last dataset derived directly from a three-dimensional simulation of a
molecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show
that these surrogate models can provide speedup and reproduce the original
observable column density maps of the dataset. This enables the rapid inference
of the chemistry (on the GPU), allowing for the faster statistical inference of
observations or increasing the resolution in hydrodynamical simulations of
astrophysical environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [179] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
*Ahmed Heakl,Sarim Hashmi,Chaimaa Abi,Celine Lee,Abdulrahman Mahmoud*

Main category: cs.CL

TL;DR: GG（Guaranteed Guess）是一种结合大型语言模型（LLMs）和软件测试框架的ISA间代码翻译方法，显著提升了CISC到RISC架构的翻译效率和正确性。


<details>
  <summary>Details</summary>
Motivation: 硬件生态快速演变，需要一种快速、灵活且正确的方法来翻译不同指令集架构（ISA）的低级程序，以提高代码的可移植性和寿命。

Method: GG利用预训练LLM生成候选翻译，并通过软件测试框架量化翻译的置信度，确保高代码覆盖率和功能/语义正确性。

Result: 在HumanEval和BringupBench程序上分别达到99%和49%的正确率，性能优于Rosetta 2，运行时快1.73倍，能效高1.47倍，内存使用少2.41倍。

Conclusion: GG为CISC到RISC翻译任务提供了高效解决方案，并将开源代码、数据和模型，推动ISA级代码翻译研究。

Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in
translating low-level programs across different instruction set architectures
(ISAs) in a quick, flexible, and correct way to enhance the portability and
longevity of existing code. A particularly challenging class of this
transpilation problem is translating between complex- (CISC) and reduced-
(RISC) hardware architectures, due to fundamental differences in instruction
complexity, memory models, and execution paradigms. In this work, we introduce
GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the
translation power of pre-trained large language models (LLMs) with the rigor of
established software testing constructs. Our method generates candidate
translations using an LLM from one ISA to another, and embeds such translations
within a software-testing framework to build quantifiable confidence in the
translation. We evaluate our GG approach over two diverse datasets, enforce
high code coverage (>98%) across unit tests, and achieve functional/semantic
correctness of 99% on HumanEval programs and 49% on BringupBench programs,
respectively. Further, we compare our approach to the state-of-the-art Rosetta
2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,
1.47x better energy efficiency, and 2.41x better memory usage for our
transpiled code, demonstrating the effectiveness of GG for real-world
CISC-to-RISC translation tasks. We will open-source our codes, data, models,
and benchmarks to establish a common foundation for ISA-level code translation
research.

</details>


### [180] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
*Essential AI,:,Andrew Hojel,Michael Pust,Tim Romanski,Yash Vanjani,Ritvik Kapila,Mohit Parmar,Adarsh Chaluvaraju,Alok Tripathy,Anil Thomas,Ashish Tanwer,Darsh J Shah,Ishaan Shah,Karl Stratos,Khoi Nguyen,Kurt Smith,Michael Callahan,Peter Rushton,Philip Monk,Platon Mazarakis,Saad Jamal,Saurabh Srivastava,Somanshu Singla,Ashish Vaswani*

Main category: cs.CL

TL;DR: Essential-Web v1.0是一个24万亿token的数据集，每份文档标注了12类分类标签，涵盖主题、格式、内容复杂度和质量，旨在解决大规模预训练数据集缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模、高质量预训练数据集缺乏导致的成本高和可访问性差的问题。

Method: 使用EAI-Distill-0.5b模型标注数据，并通过SQL式过滤生成高质量数据集。

Result: 在数学、编程、STEM和医学领域生成的数据集表现优异，接近或超过当前最佳水平。

Conclusion: Essential-Web v1.0为语言模型提供了高质量、易访问的预训练数据，显著提升了数据管道的效率。

Abstract: Data plays the most prominent role in how language models acquire skills and
knowledge. The lack of massive, well-organized pre-training datasets results in
costly and inaccessible data pipelines. We present Essential-Web v1.0, a
24-trillion-token dataset in which every document is annotated with a
twelve-category taxonomy covering topic, format, content complexity, and
quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned
0.5b-parameter model that achieves an annotator agreement within 3% of
Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain
competitive web-curated datasets in math (-8.0% relative to SOTA), web code
(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on
HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [181] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
*Jonathan Hayase,Alisa Liu,Noah A. Smith,Sewoong Oh*

Main category: cs.CL

TL;DR: 论文提出了一种推理时方法，将任何使用BPE分词的自回归语言模型转换为字符级或字节级模型，解决了分词带来的Prompt Boundary Problem（PBP）和模型兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型普遍使用分词技术，但分词可能导致生成文本的失真（如PBP问题），并阻碍模型间的组合与互操作性。

Method: 提出一种推理时方法，将BPE分词的模型转换为字符级或字节级模型，保持生成分布不变。

Result: 实验表明，该方法能有效解决PBP问题，并支持不同分词模型的集成与代理调优，提升下游任务性能。

Conclusion: 该方法为分词带来的问题提供了高效解决方案，增强了模型的灵活性和兼容性。

Abstract: Tokenization is used almost universally by modern language models, enabling
efficient text representation using multi-byte or multi-character tokens.
However, prior work has shown that tokenization can introduce distortion into
the model's generations. For example, users are often advised not to end their
prompts with a space because it prevents the model from including the space as
part of the next token. This Prompt Boundary Problem (PBP) also arises in
languages such as Chinese and in code generation, where tokens often do not
line up with syntactic boundaries. Additionally mismatching tokenizers often
hinder model composition and interoperability. For example, it is not possible
to directly ensemble models with different tokenizers due to their mismatching
vocabularies. To address these issues, we present an inference-time method to
convert any autoregressive LM with a BPE tokenizer into a character-level or
byte-level LM, without changing its generative distribution at the text level.
Our method efficient solves the PBP and is also able to unify the vocabularies
of language models with different tokenizers, allowing one to ensemble LMs with
different tokenizers at inference time as well as transfer the post-training
from one model to another using proxy-tuning. We demonstrate in experiments
that the ensemble and proxy-tuned models outperform their constituents on
downstream evals.

</details>


### [182] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
*Di He,Ajay Jaiswal,Songjun Tu,Li Shen,Ganzhao Yuan,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: AlphaDecay是一种自适应分配权重衰减强度的新方法，通过分析权重相关矩阵的谱特性，为不同模块分配不同的衰减率，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法对所有层使用统一的权重衰减率，忽略了LLM中模块的结构多样性和谱特性差异。

Method: 基于HT-SR理论，通过分析权重相关矩阵的谱密度（ESD），自适应分配不同衰减强度：重尾谱模块衰减较弱，轻尾谱模块衰减较强。

Result: 在60M到1B参数的模型上，AlphaDecay在困惑度和泛化能力上优于传统统一衰减和其他自适应衰减方法。

Conclusion: AlphaDecay通过模块化自适应衰减策略，有效平衡了不同模块的谱特性差异，提升了LLM的性能。

Abstract: Weight decay is a standard regularization technique for training large
language models (LLMs). While it is common to assign a uniform decay rate to
every layer, this approach overlooks the structural diversity of LLMs and the
varying spectral properties across modules. In this paper, we introduce
AlphaDecay, a simple yet effective method that adaptively assigns different
weight decay strengths to each module of an LLM. Our approach is guided by
Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical
spectral density (ESD) of weight correlation matrices to quantify
"heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs,
reflecting stronger feature learning, are assigned weaker decay, while modules
with lighter-tailed spectra receive stronger decay. Our method leverages
tailored weight decay assignments to balance the module-wise differences in
spectral properties, leading to improved performance. Extensive pre-training
tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay
achieves better perplexity and generalization than conventional uniform decay
and other adaptive decay baselines.

</details>


### [183] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
*Xiang Cheng,Chengyan Pan,Minjun Zhao,Deyang Li,Fangchao Liu,Xinyu Zhang,Xiao Zhang,Yong Liu*

Main category: cs.CL

TL;DR: 研究发现，对于近期强大的语言模型（如Qwen2.5系列），传统的CoT示例并未提升推理性能，其主要作用是格式化输出。增强的CoT示例同样无效，模型倾向于忽略示例而专注于指令。


<details>
  <summary>Details</summary>
Motivation: 探讨在强大语言模型中，CoT示例是否仍能提升数学推理能力。

Method: 通过系统实验，比较传统和增强CoT示例对模型推理性能的影响。

Result: 传统和增强CoT示例均未提升推理性能，模型忽略示例而依赖指令。

Conclusion: 当前ICL+CoT框架在数学推理中存在局限性，需重新审视ICL范式和示例定义。

Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language
Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars
of ICL to enhance the reasoning capability, especially in mathematics tasks.
However, given the continuous advancement of model capabilities, it remains
unclear whether CoT exemplars still benefit recent, stronger models in such
tasks. Through systematic experiments, we find that for recent strong models
such as the Qwen2.5 series, adding traditional CoT exemplars does not improve
reasoning performance compared to Zero-Shot CoT. Instead, their primary
function is to align the output format with human expectations. We further
investigate the effectiveness of enhanced CoT exemplars, constructed using
answers from advanced models such as \texttt{Qwen2.5-Max} and
\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced
exemplars still fail to improve the model's reasoning performance. Further
analysis reveals that models tend to ignore the exemplars and focus primarily
on the instructions, leading to no observable gain in reasoning ability.
Overall, our findings highlight the limitations of the current ICL+CoT
framework in mathematical reasoning, calling for a re-examination of the ICL
paradigm and the definition of exemplars.

</details>


### [184] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
*Daniel D'souza,Julia Kreutzer,Adrien Morisot,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 论文提出了一种优化训练协议的方法，旨在提高模型在罕见用例中的性能和可控性，通过数据特征和任务来源的分类实现显式和隐式控制。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习在罕见和代表性不足特征上的表现问题，避免依赖提示工程或少样本示例的局限性。

Method: 创建数据特征和任务来源的分类法，微调基础模型以自动推断标记，实现推理时的可选控制。

Result: 在开放生成质量上平均提升5.7%，在罕见领域提升9.1%，在特定任务如CodeRepair上提升14.1%。

Conclusion: 该方法显著提高了模型在长尾分布中的性能，同时提供了灵活的控制机制。

Abstract: One of the most profound challenges of modern machine learning is performing
well on the long-tail of rare and underrepresented features. Large
general-purpose models are trained for many tasks, but work best on
high-frequency use cases. After training, it is hard to adapt a model to
perform well on specific use cases underrepresented in the training corpus.
Relying on prompt engineering or few-shot examples to maximize the output
quality on a particular test case can be frustrating, as models can be highly
sensitive to small changes, react in unpredicted ways or rely on a fixed system
prompt for maintaining performance. In this work, we ask: "Can we optimize our
training protocols to both improve controllability and performance on
underrepresented use cases at inference time?" We revisit the divide between
training and inference techniques to improve long-tail performance while
providing users with a set of control levers the model is trained to be
responsive to. We create a detailed taxonomy of data characteristics and task
provenance to explicitly control generation attributes and implicitly condition
generations at inference time. We fine-tune a base model to infer these markers
automatically, which makes them optional at inference time. This principled and
flexible approach yields pronounced improvements in performance, especially on
examples from the long tail of the training distribution. While we observe an
average lift of 5.7% win rates in open-ended generation quality with our
markers, we see over 9.1% gains in underrepresented domains. We also observe
relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and
absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [185] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
*Li-Wei Chen,Takuya Higuchi,Zakaria Aldeneh,Ahmed Hussen Abdelaziz,Alexander Rudnicky*

Main category: cs.CL

TL;DR: 论文提出了一种端到端的变分方法，自动学习编码连续语音属性以增强语义标记，解决了现有方法依赖手动特征提取的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过添加音高特征来增强语义标记，但音高无法完全代表副语言属性，且需要手动特征工程。

Method: 采用端到端的变分方法，自动学习编码连续语音属性。

Result: 该方法无需手动提取特征，并能生成更受人类评分者偏好的语音延续。

Conclusion: 提出的方法有效提升了语音生成的流畅性和自然度。

Abstract: The success of large language models in text processing has inspired their
adaptation to speech modeling. However, since speech is continuous and complex,
it is often discretized for autoregressive modeling. Speech tokens derived from
self-supervised models (known as semantic tokens) typically focus on the
linguistic aspects of speech but neglect prosodic information. As a result,
models trained on these tokens can generate speech with reduced naturalness.
Existing approaches try to fix this by adding pitch features to the semantic
tokens. However, pitch alone cannot fully represent the range of paralinguistic
attributes, and selecting the right features requires careful hand-engineering.
To overcome this, we propose an end-to-end variational approach that
automatically learns to encode these continuous speech attributes to enhance
the semantic tokens. Our approach eliminates the need for manual extraction and
selection of paralinguistic features. Moreover, it produces preferred speech
continuations according to human raters. Code, samples and models are available
at https://github.com/b04901014/vae-gslm.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [186] [SoK: Advances and Open Problems in Web Tracking](https://arxiv.org/abs/2506.14057)
*Yash Vekaria,Yohan Beugin,Shaoor Munir,Gunes Acar,Nataliia Bielova,Steven Englehardt,Umar Iqbal,Alexandros Kapravelos,Pierre Laperdrix,Nick Nikiforakis,Jason Polakis,Franziska Roesner,Zubair Shafiq,Sebastian Zimmeck*

Main category: cs.CR

TL;DR: 本文是一篇系统知识综述（SoK），旨在整合和分析关于网络追踪技术、反制措施及隐私法规的广泛研究，提供全面的概述，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 网络追踪技术日益复杂且侵入性强，但现有研究分散且难以形成整体趋势。当前行业变革（如广告业转型、浏览器反追踪措施及隐私法规实施）促使需要系统性综述。

Method: 通过系统知识综述（SoK）方法，整合和分类现有研究，分析技术机制、反制措施及法规。

Result: 提供了现代网络追踪生态的全面概述，包括技术、反制措施和法规，并识别了研究空白和开放挑战。

Conclusion: 本文为研究者、从业者和政策制定者提供了统一参考，并指明了未来研究方向。

Abstract: Web tracking is a pervasive and opaque practice that enables personalized
advertising, retargeting, and conversion tracking. Over time, it has evolved
into a sophisticated and invasive ecosystem, employing increasingly complex
techniques to monitor and profile users across the web. The research community
has a long track record of analyzing new web tracking techniques, designing and
evaluating the effectiveness of countermeasures, and assessing compliance with
privacy regulations. Despite a substantial body of work on web tracking, the
literature remains fragmented across distinctly scoped studies, making it
difficult to identify overarching trends, connect new but related techniques,
and identify research gaps in the field. Today, web tracking is undergoing a
once-in-a-generation transformation, driven by fundamental shifts in the
advertising industry, the adoption of anti-tracking countermeasures by
browsers, and the growing enforcement of emerging privacy regulations. This
Systematization of Knowledge (SoK) aims to consolidate and synthesize this
wide-ranging research, offering a comprehensive overview of the technical
mechanisms, countermeasures, and regulations that shape the modern and rapidly
evolving web tracking landscape. This SoK also highlights open challenges and
outlines directions for future research, aiming to serve as a unified reference
and introductory material for researchers, practitioners, and policymakers
alike.

</details>


### [187] [Vulnerability Disclosure or Notification? Best Practices for Reaching Stakeholders at Scale](https://arxiv.org/abs/2506.14323)
*Ting-Han Chen,Jeroen van der Ham-de Vos*

Main category: cs.CR

TL;DR: 本文探讨了漏洞披露与漏洞通知的区别，分析了近年来的策略变化，并总结了最佳实践。


<details>
  <summary>Details</summary>
Motivation: 研究漏洞披露和通知的实践差异及其挑战，以改进现有指南。

Method: 基于早期披露经验和前人工作，进行元综述，分析策略变化和结果。

Result: 总结了漏洞披露和通知的最佳实践，并区分了两者的不同方法。

Conclusion: 漏洞通知需与披露区分，采用不同策略，未来研究应进一步优化指南。

Abstract: Security researchers are interested in security vulnerabilities, but these
security vulnerabilities create risks for stakeholders. Coordinated
Vulnerability Disclosure has been an accepted best practice for many years in
disclosing newly discovered vulnerabilities. This practice has mostly worked,
but it can become challenging when there are many different parties involved.
  There has also been research into known vulnerabilities, using datasets or
active scans to discover how many machines are still vulnerable. The ethical
guidelines suggest that researchers also make an effort to notify the owners of
these machines. We posit that this differs from vulnerability disclosure, but
rather the practice of vulnerability notification. This practice has some
similarities with vulnerability disclosure but should be distinguished from it,
providing other challenges and requiring a different approach.
  Based on our earlier disclosure experience and on prior work documenting
their disclosure and notification operations, we provide a meta-review on
vulnerability disclosure and notification to observe the shifts in strategies
in recent years. We assess how researchers initiated their messaging and
examine the outcomes. We then compile the best practices for the existing
disclosure guidelines and for notification operations.

</details>


### [188] [Consensus Power Inequality: A Comparative Study of Blockchain Networks](https://arxiv.org/abs/2506.14393)
*Kamil Tylinski,Abylay Satybaldy,Paolo Tasca*

Main category: cs.CR

TL;DR: 该研究评估了五种主流区块链网络的共识权力不平等现象，发现Hedera和比特币分布较均衡，而Algorand权力集中度较高。


<details>
  <summary>Details</summary>
Motivation: 共识权力的分布对区块链网络的去中心化、安全性和公平性至关重要，研究旨在量化评估其不平等现象。

Method: 使用Gini系数和Theil指数等经济指标，分析2022年1月至2024年7月的数据，比较不同网络的共识权力分布。

Result: Hedera和比特币权力分布较均衡，以太坊转向PoS后权力更集中，Algorand集中度最高。

Conclusion: 研究提出了评估共识权力不平等的方法框架，并强调需针对性策略以实现更公平的分布。

Abstract: The distribution of consensus power is a cornerstone of decentralisation,
influencing the security, resilience, and fairness of blockchain networks while
ensuring equitable impact among participants. This study provides a rigorous
evaluation of consensus power inequality across five prominent blockchain
networks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data
collected from January 2022 to July 2024. Leveraging established economic
metrics, including the Gini coefficient and Theil index, the research
quantitatively assesses how power is distributed among blockchain network
participants. A robust dataset, capturing network-specific characteristics such
as mining pools, staking patterns, and consensus nodes, forms the foundation of
the analysis, enabling meaningful comparisons across diverse architectures.
Through an in-depth comparative study, the paper identifies key disparities in
consensus power distribution. Hedera and Bitcoin demonstrate more balanced
power distribution, aligning closely with the principles of decentralisation.
Ethereum and Cardano demonstrate moderate levels of inequality. However,
contrary to expectations, Ethereum has become more concentrated following its
transition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced
centralisation of power. Moreover, the findings highlight the structural and
operational drivers of inequality, including economic barriers, governance
models, and network effects, offering actionable insights for more equitable
network design. This study establishes a methodological framework for
evaluating blockchain consensus power inequality, emphasising the importance of
targeted strategies to ensure fairer power distribution and enhancing the
sustainability of decentralised systems. Future research will build on these
findings by integrating additional metrics and examining the influence of
emerging consensus mechanisms.

</details>


### [189] [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
*Wai Man Si,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种针对大型语言模型（LLMs）的新型威胁：通过对抗性输入利用模型的过度推理行为，显著增加计算开销而不影响模型性能。作者提出了一种包含三个组件的损失框架，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，大型语言模型在复杂任务中表现优异，但存在过度推理问题（如频繁切换推理轨迹或冗余推理）。这些行为导致高计算成本，而对抗性输入可能进一步加剧这一问题。

Method: 提出了一种损失框架，包含三个组件：优先级交叉熵损失、过度推理损失和延迟终止损失。该框架旨在通过优化模型推理行为，增加计算开销。

Result: 在GSM8K和ORCA数据集上的实验表明，推理长度增加了3到9倍，同时保持了模型性能。对抗性输入还表现出对其他模型的迁移性。

Conclusion: 研究表明，对抗性输入可以显著增加LLMs的计算开销，而提出的损失框架有效验证了这一威胁。

Abstract: Recent reasoning large language models (LLMs), such as OpenAI o1 and
DeepSeek-R1, exhibit strong performance on complex tasks through test-time
inference scaling. However, prior studies have shown that these models often
incur significant computational costs due to excessive reasoning, such as
frequent switching between reasoning trajectories (e.g., underthinking) or
redundant reasoning on simple questions (e.g., overthinking). In this work, we
expose a novel threat: adversarial inputs can be crafted to exploit excessive
reasoning behaviors and substantially increase computational overhead without
compromising model utility. Therefore, we propose a novel loss framework
consisting of three components: (1) Priority Cross-Entropy Loss, a modification
of the standard cross-entropy objective that emphasizes key tokens by
leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss,
which encourages the model to initiate additional reasoning paths during
inference; and (3) Delayed Termination Loss, which is designed to extend the
reasoning process and defer the generation of final outputs. We optimize and
evaluate our attack for the GSM8K and ORCA datasets on
DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results
demonstrate a 3x to 9x increase in reasoning length with comparable utility
performance. Furthermore, our crafted adversarial inputs exhibit
transferability, inducing computational overhead in o3-mini, o1-mini,
DeepSeek-R1, and QWQ models.

</details>


### [190] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Main category: cs.CR

TL;DR: 论文探讨了在美国选举计票器中使用机器学习分类器的安全风险，通过新数据集和多种模型展示了传统白盒攻击的无效性，并提出新方法在物理世界中实现攻击。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示机器学习分类器在选举计票中的潜在安全漏洞，特别是对抗性攻击对选举结果的影响。

Method: 方法包括引入四个新数据集、训练多种模型（如SVM、CNN、ViT）、分析梯度掩蔽问题，并提出改进的对抗攻击方法。

Result: 结果显示传统攻击因梯度掩蔽无效，但新方法能在物理世界中实现低成功率攻击，足以影响选举结果。

Conclusion: 结论指出选举计票中的机器学习模型存在安全风险，需进一步研究防御措施。

Abstract: We show the security risk associated with using machine learning classifiers
in United States election tabulators. The central classification task in
election tabulation is deciding whether a mark does or does not appear on a
bubble associated to an alternative in a contest on the ballot. Barretto et al.
(E-Vote-ID 2021) reported that convolutional neural networks are a viable
option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To
demonstrate and analyze the hypothetical vulnerability of machine learning
models on election tabulators, we first introduce four new ballot datasets.
Second, we train and test a variety of different models on our new datasets.
These models include support vector machines, convolutional neural networks (a
basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third,
using our new datasets and trained models, we demonstrate that traditional
white box attacks are ineffective in the voting domain due to gradient masking.
Our analyses further reveal that gradient masking is a product of numerical
instability. We use a modified difference of logits ratio loss to overcome this
issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct
attacks with the adversarial examples generated using our new methods. In
traditional adversarial machine learning, a high (50% or greater) attack
success rate is ideal. However, for certain elections, even a 5% attack success
rate can flip the outcome of a race. We show such an impact is possible in the
physical domain. We thoroughly discuss attack realism, and the challenges and
practicality associated with printing and scanning ballot adversarial examples.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [191] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.AI

TL;DR: 论文研究了基于AST的混合图表示在GNN代码克隆检测中的有效性，发现AST+CFG+DFG提升准确性，而FA-AST可能增加复杂性。GMN在标准AST下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 代码克隆增加了维护成本和漏洞风险，现有AST方法缺乏语义深度，需研究混合图表示的效果。

Method: 通过系统比较AST+CFG+DFG和FA-AST等混合表示，评估其在多种GNN架构中的表现。

Result: AST+CFG+DFG提升GCN和GAT的准确性，FA-AST可能降低性能。GMN在标准AST下表现最优。

Conclusion: 混合图表示对GNN效果各异，GMN在标准AST下已足够高效，减少了对复杂结构的依赖。

Abstract: As one of the most detrimental code smells, code clones significantly
increase software maintenance costs and heighten vulnerability risks, making
their detection a critical challenge in software engineering. Abstract Syntax
Trees (ASTs) dominate deep learning-based code clone detection due to their
precise syntactic structure representation, but they inherently lack semantic
depth. Recent studies address this by enriching AST-based representations with
semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs
(DFGs). However, the effectiveness of various enriched AST-based
representations and their compatibility with different graph-based machine
learning techniques remains an open question, warranting further investigation
to unlock their full potential in addressing the complexities of code clone
detection. In this paper, we present a comprehensive empirical study to
rigorously evaluate the effectiveness of AST-based hybrid graph representations
in Graph Neural Network (GNN)-based code clone detection. We systematically
compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs
(FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid
representations impact GNNs differently: while AST+CFG+DFG consistently
enhances accuracy for convolution- and attention-based models (Graph
Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST
frequently introduces structural complexity that harms performance. Notably,
GMN outperforms others even with standard AST representations, highlighting its
superior cross-code similarity detection and reducing the need for enriched
structures.

</details>


### [192] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho,Lazar S. Popov,Sander Kaatee,Kristinn R. Thórisson,Tangrui Li,Pétur Húni Björnsson,Jilles S. Dibangoye*

Main category: cs.AI

TL;DR: ICE-ID是一个用于历史身份解析的新基准数据集，涵盖1703-1920年的冰岛人口普查记录，支持跨代数据匹配。


<details>
  <summary>Details</summary>
Motivation: 填补长期人口实体匹配研究的数据空白，提供可重复的基准测试。

Method: 比较了规则匹配器、机器学习集成、LLM和NARS（一种非公理推理系统）。

Result: NARS表现简单且竞争力强，达到SOTA水平。

Conclusion: ICE-ID的发布为跨学科研究提供了新工具，支持历史分析和数据链接。

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity
resolution, comprising 220 years (1703-1920) of Icelandic census records.
ICE-ID spans multiple generations of longitudinal data, capturing name
variations, demographic changes, and rich genealogical links. To the best of
our knowledge, this is the first large-scale, open tabular dataset specifically
designed to study long-term person-entity matching in a real-world population.
We define identity resolution tasks (within and across census waves) with
clearly documented metrics and splits. We evaluate a range of methods:
handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured
data (e.g. transformer-based tabular networks) against a novel approach to
tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose
AI framework designed to reason with limited knowledge and resources. Its core
is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that
NARS is suprisingly simple and competitive with other standard approaches,
achieving SOTA at our task. By releasing ICE-ID and our code, we enable
reproducible benchmarking of identity resolution approaches in longitudinal
settings and hope that ICE-ID opens new avenues for cross-disciplinary research
in data linkage and historical analytics.

</details>


### [193] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文探讨了人类因果认知与机器学习中因果模型的差异，提出需结合人类因果认知特性以提升AI能力。


<details>
  <summary>Details</summary>
Motivation: 人类因果认知在高效学习和泛化中起关键作用，而当前机器学习系统在此方面较弱。研究旨在通过结合人类因果认知特性，改进AI系统的能力和可解释性。

Method: 通过分析人类因果认知与结构因果模型（SCM）的差异，探讨人类因果认知在‘人类生态位’中的适应性。

Result: SCM框架未能完全捕捉人类因果认知的某些关键特性，如通过类比泛化知识的能力。

Conclusion: 未来研究应结合人类因果认知的适应性特性，以开发更具能力和可解释性的AI系统。

Abstract: Humans interpret the world around them in terms of cause and effect and
communicate their understanding of the world to each other in causal terms.
These causal aspects of human cognition are thought to underlie humans' ability
to generalize and learn efficiently in new domains, an area where current
machine learning systems are weak. Building human-like causal competency into
machine learning systems may facilitate the construction of effective and
interpretable AI. Indeed, the machine learning community has been importing
ideas on causality formalized by the Structural Causal Model (SCM) framework,
which provides a rigorous formal language for many aspects of causality and has
led to significant advances. However, the SCM framework fails to capture some
salient aspects of human causal cognition and has likewise not yet led to
advances in machine learning in certain critical areas where humans excel. We
contend that the problem of causality in the ``human niche'' -- for a social,
autonomous, and goal-driven agent sensing and acting in the world in which
humans live -- is quite different from the kind of causality captured by SCMs.
For example, everyday objects come in similar types that have similar causal
properties, and so humans readily generalize knowledge of one type of object
(cups) to another related type (bowls) by drawing causal analogies between
objects with similar properties, but such analogies are at best awkward to
express in SCMs. We explore how such causal capabilities are adaptive in, and
motivated by, the human niche. By better appreciating properties of human
causal cognition and, crucially, how those properties are adaptive in the niche
in which humans live, we hope that future work at the intersection of machine
learning and causality will leverage more human-like inductive biases to create
more capable, controllable, and interpretable systems.

</details>


### [194] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort,Vitaly Nikolaev*

Main category: cs.AI

TL;DR: 论文提出了一种基于哲学因果关系的测试方法，评估AI（如ChatGPT、DeepSeek和Gemini）在抽象因果推理中的表现，并扩展了神经元图中因果定义的应用范围。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在抽象因果推理中的能力，并挑战现有关于神经元图中因果定义难以明确的观点。

Method: 基于D. Lewis的神经元图理论，设计测试方法，评估大型语言模型的因果推理能力。

Result: 发现这些AI模型能够正确识别文献中争议较大的因果关系，并提出了更广泛的因果定义。

Conclusion: 未来哲学研究可能发展为人类与AI专业知识的互动模式。

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship
in the philosophy of causation, in particular on the neuron diagrams
popularized by D. Lewis. We illustrate the test on advanced Large Language
Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already
capable of correctly identifying causes in cases that are hotly debated in the
literature. In order to assess the results of these LLMs and future dedicated
AI, we propose a definition of cause in neuron diagrams with a wider validity
than published hitherto, which challenges the widespread view that such a
definition is elusive. We submit that these results are an illustration of how
future philosophical research might evolve: as an interplay between human and
artificial expertise.

</details>


### [195] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole,Mohamed Osman*

Main category: cs.AI

TL;DR: 该论文提出了一种基于深度学习的方法，通过在测试时动态训练神经网络（NN）和优化器，显著提升了在ARC-AGI任务上的性能，并展示了在2023年ARCathon竞赛中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在ARC-AGI任务上表现通常不佳，但作者认为深度学习范式仍是最有效的策略，因此探索如何进一步利用其能力来解决ARC的挑战。

Method: 提出了一种结合预训练语言模型（LLM）和测试时微调（TTFT）的方法，并引入了增强推理反向增强和投票（AIRV）技术。

Result: 该方法在ARC任务上实现了显著的性能提升（AIRV提升260%，TTFT进一步提升300%），并在2023年ARCathon竞赛中取得第一名，最终版本在ARC私有测试集上达到58%的准确率。

Conclusion: 研究表明，深度学习的动态训练和优化机制是提升陌生领域推理能力的关键，为广泛感知推理提供了有效的解决方案。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable
challenge for AI systems. Despite the typically low performance on ARC, the
deep learning paradigm remains the most effective known strategy for generating
skillful (state-of-the-art) neural networks (NN) across varied modalities and
tasks in vision, language etc. The deep learning paradigm has proven to be able
to train these skillful neural networks and learn the abstractions needed in
these diverse domains. Our work doubles down on that and continues to leverage
this paradigm by incorporating on-the-fly NN training at test time. We
demonstrate that fully committing to deep learning's capacity to acquire novel
abstractions yields state-of-the-art performance on ARC. Specifically, we treat
both the neural network and the optimizer (rather than just a pre-trained
network) as integral components of the inference process, fostering
generalization to unseen tasks. Concretely, we propose a methodology for
training on ARC, starting from pretrained LLMs, and enhancing their ARC
reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment
Inference Reverse-Augmentation and Vote (AIRV) as effective test-time
techniques. We are the first to propose and show deep learning can be used
effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a
further 300% boost with TTFT. An early version of this approach secured first
place in the 2023 ARCathon competition, while the final version achieved the
current best score on the ARC private test-set (58%). Our findings highlight
the key ingredients of a robust reasoning system in unfamiliar domains,
underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [196] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/abs/2506.13819)
*El Arbi Belfarsi,Henry Flores,Maria Valero*

Main category: eess.IV

TL;DR: 提出了一种基于短波红外光谱的双模态AI框架，结合CNN和光电二极管传感器，用于非侵入性血糖监测。


<details>
  <summary>Details</summary>
Motivation: 开发一种临床准确、成本高效且可穿戴的连续血糖监测解决方案。

Method: 第一模态使用多波长SWIR成像系统和CNN捕捉空间特征；第二模态使用光电二极管电压传感器和机器学习回归器分析光学信号。

Result: CNN的MAPE为4.82%，光电二极管系统在Clarke误差网格中达到86.4% Zone A准确率。

Conclusion: 该框架为可靠的非侵入性血糖监测提供了先进解决方案。

Abstract: In this study, we present a dual-modal AI framework based on short-wave
infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength
SWIR imaging system coupled with convolutional neural networks (CNNs) to
capture spatial features linked to glucose absorption. The second modality uses
a compact photodiode voltage sensor and machine learning regressors (e.g.,
random forest) on normalized optical signals. Both approaches were evaluated on
synthetic blood phantoms and skin-mimicking materials across physiological
glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage
error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error
Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework
constitutes a state-of-the-art solution that balances clinical accuracy, cost
efficiency, and wearable integration, paving the way for reliable continuous
non-invasive glucose monitoring.

</details>


### [197] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/abs/2506.13964)
*Yusdivia Molina-Román,David Gómez-Ortiz,Ernestina Menasalvas-Ruiz,José Gerardo Tamez-Peña,Alejandro Santos-Díaz*

Main category: eess.IV

TL;DR: 比较多模态和CNN方法在乳腺密度分类中的表现，发现端到端微调的CNN模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决乳腺密度分类中主观性和观察者间差异的问题。

Method: 使用BioMedCLIP和ConvNeXt，评估零样本分类、线性探测和微调三种学习场景。

Result: 微调的ConvNeXt表现最优，多模态学习在医学影像中潜力有限。

Conclusion: 未来需改进文本表示和领域适应，端到端微调的CNN更适合医学影像。

Abstract: Mammographic breast density classification is essential for cancer risk
assessment but remains challenging due to subjective interpretation and
inter-observer variability. This study compares multimodal and CNN-based
methods for automated classification using the BI-RADS system, evaluating
BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot
classification, linear probing with textual descriptions, and fine-tuning with
numerical labels. Results show that zero-shot classification achieved modest
performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP
linear probe. Although linear probing demonstrated potential with pretrained
embeddings, it was less effective than full fine-tuning. These findings suggest
that despite the promise of multimodal learning, CNN-based models with
end-to-end fine-tuning provide stronger performance for specialized medical
imaging. The study underscores the need for more detailed textual
representations and domain-specific adaptations in future radiology
applications.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [198] [The Complexity of Counting Small Sub-Hypergraphs](https://arxiv.org/abs/2506.14081)
*Marco Bressan,Julian Brinkmann,Holger Dell,Marc Roth,Philip Wellnitz*

Main category: cs.CC

TL;DR: 论文研究了超图版本的子图计数问题，提出了新的图参数，并基于指数时间假设对问题的复杂度进行了完整分类。


<details>
  <summary>Details</summary>
Motivation: 超图子图计数问题在研究中被忽视，作者旨在填补这一空白，探索其计算复杂度。

Method: 引入分数共独立边覆盖数和分数边覆盖数作为新参数，分析#Sub和#IndSub问题的固定参数可处理性。

Result: 证明#Sub和#IndSub问题的固定参数可处理性分别取决于分数共独立边覆盖数和分数边覆盖数的有界性。

Conclusion: 超图子图计数问题的复杂度与图版本不同，固定参数可处理性未必意味着多项式时间可解。

Abstract: Subgraph counting is a fundamental and well-studied problem whose
computational complexity is well understood. Quite surprisingly, the hypergraph
version of subgraph counting has been almost ignored. In this work, we address
this gap by investigating the most basic sub-hypergraph counting problem: given
a (small) hypergraph $H$ and a (large) hypergraph $G$, compute the number of
sub-hypergraphs of $G$ isomorphic to $H$. Formally, for a family $\mathcal{H}$
of hypergraphs, let #Sub($\mathcal{H}$) be the restriction of the problem to $H
\in \mathcal{H}$; the induced variant #IndSub($\mathcal{H}$) is defined
analogously. Our main contribution is a complete classification of the
complexity of these problems. Assuming the Exponential Time Hypothesis, we
prove that #Sub($\mathcal{H}$) is fixed-parameter tractable if and only if
$\mathcal{H}$ has bounded fractional co-independent edge-cover number, a novel
graph parameter we introduce. Moreover, #IndSub($\mathcal{H}$) is
fixed-parameter tractable if and only if $\mathcal{H}$ has bounded fractional
edge-cover number. Both results subsume pre-existing results for graphs as
special cases. We also show that the fixed-parameter tractable cases of
#Sub($\mathcal{H}$) and #IndSub($\mathcal{H}$) are unlikely to be in polynomial
time, unless respectively #P = P and Graph Isomorphism $\in$ P. This shows a
separation with the special case of graphs, where the fixed-parameter tractable
cases are known to actually be in polynomial time.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [199] [AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering](https://arxiv.org/abs/2506.13989)
*Johan Östman,Edvin Callisen,Anton Chen,Kristiina Ausmees,Emanuel Gårdh,Jovan Zamac,Jolanta Goldsteine,Hugo Wefer,Simon Whelan,Markus Reimegård*

Main category: cs.SI

TL;DR: AMLGentex是一个开源工具，用于生成逼真的交易数据并评估反洗钱（AML）系统的性能，解决了现有合成数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 洗钱活动每年涉及数万亿美元，但仅有少量被揭露。现有合成数据集无法模拟真实洗钱行为的复杂性和挑战。

Method: 提出了AMLGentex，一个开源工具，生成可配置的交易数据并支持AML检测方法的基准测试。

Result: AMLGentex能够在受控环境中系统评估AML系统，模拟真实世界的复杂性。

Conclusion: 该框架为AML方法的严格评估提供了有效工具，适用于实际复杂场景。

Abstract: Money laundering enables organized crime by allowing illicit funds to enter
the legitimate economy. Although trillions of dollars are laundered each year,
only a small fraction is ever uncovered. This stems from a range of factors,
including deliberate evasion by launderers, the rarity of confirmed cases, and
the limited visibility each financial institution has into the global
transaction network. While several synthetic datasets are available, they fail
to model the structural and behavioral complexity of real-world money
laundering. In particular, they often overlook partial observability, sparse
and uncertain labels, strategic behavior, temporal dynamics, class imbalance,
and network-level dependencies. To address these limitations, we present
AMLGentex, an open-source suite for generating realistic, configurable
transaction data and benchmarking detection methods. It enables systematic
evaluation of anti-money laundering (AML) systems in a controlled environment
that captures key real-world challenges. We demonstrate how the framework can
be used to rigorously evaluate methods under conditions that reflect the
complexity of practical AML scenarios.

</details>


### [200] [Density-aware Walks for Coordinated Campaign Detection](https://arxiv.org/abs/2506.13912)
*Atul Anand Gopalakrishnan,Jakir Hossain,Tuğrulcan Elmas,Ahmet Erdem Sarıyüce*

Main category: cs.SI

TL;DR: 论文提出了一种基于图分类任务的方法，通过局部网络结构密度检测社交媒体上的协调虚假行为，使用随机加权游走（RWW）和密度感知嵌入，显著提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的协调虚假行为难以区分，现有图神经网络（GNNs）在处理大规模网络时效果不佳，需要新方法提高检测准确性。

Method: 利用大型参与网络（LEN）数据集，提出随机加权游走（RWW）方法，结合局部密度度量（如度数、核心数、桁架数）生成密度感知嵌入，再训练消息传递神经网络（MPNNs）。

Result: 与简单节点特征相比，该方法在二元和多类分类中分别提高了12%和5%的准确率。

Conclusion: 密度感知结构编码与MPNNs结合，为检测社交媒体上的协调虚假行为提供了鲁棒框架。

Abstract: Coordinated campaigns frequently exploit social media platforms by
artificially amplifying topics, making inauthentic trends appear organic, and
misleading users into engagement. Distinguishing these coordinated efforts from
genuine public discourse remains a significant challenge due to the
sophisticated nature of such attacks. Our work focuses on detecting coordinated
campaigns by modeling the problem as a graph classification task. We leverage
the recently introduced Large Engagement Networks (LEN) dataset, which contains
over 300 networks capturing engagement patterns from both fake and authentic
trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were
constructed by collecting interactions related to campaigns that stemmed from
ephemeral astroturfing. Established graph neural networks (GNNs) struggle to
accurately classify campaign graphs, highlighting the challenges posed by LEN
due to the large size of its networks. To address this, we introduce a new
graph classification method that leverages the density of local network
structures. We propose a random weighted walk (RWW) approach in which node
transitions are biased by local density measures such as degree, core number,
or truss number. These RWWs are encoded using the Skip-gram model, producing
density-aware structural embeddings for the nodes. Training message-passing
neural networks (MPNNs) on these density-aware embeddings yields superior
results compared to the simpler node features available in the dataset, with
nearly a 12\% and 5\% improvement in accuracy for binary and multiclass
classification, respectively. Our findings demonstrate that incorporating
density-aware structural encoding with MPNNs provides a robust framework for
identifying coordinated inauthentic behavior on social media networks such as
Twitter.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [201] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/abs/2506.14293)
*Tawsif Ahmed,Andrej Radonjic,Gollam Rabby*

Main category: cs.SD

TL;DR: Sleeping-DISCO 9M是一个用于音乐生成任务的大规模预训练数据集，填补了高质量、开源流行音乐数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为合成或重录音乐，未能反映真实世界音乐风格，限制了生成音乐模型的应用。

Method: 使用实际流行音乐和知名艺术家作品构建数据集。

Result: 提供了首个高质量、开源的流行音乐数据集，支持多种生成任务。

Conclusion: Sleeping-DISCO 9M有望推动生成音乐模型的发展，弥补现有数据集的不足。

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music
and song. To the best of our knowledge, there are no open-source high-quality
dataset representing popular and well-known songs for generative music modeling
tasks such as text-music, music-captioning, singing-voice synthesis, melody
reconstruction and cross-model retrieval. Past contributions focused on
isolated and constrained factors whose core perspective was to create synthetic
or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily
large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another
focus for the community. Unfortunately, adoption of these datasets has been
below substantial in the generative music community as these datasets fail to
reflect real-world music and its flavour. Our dataset changes this narrative
and provides a dataset that is constructed using actual popular music and
world-renowned artists.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [202] [Accurate and scalable exchange-correlation with deep learning](https://arxiv.org/abs/2506.14665)
*Giulia Luise,Chin-Wei Huang,Thijs Vogels,Derk P. Kooi,Sebastian Ehlert,Stephanie Lanius,Klaas J. H. Giesbertz,Amir Karton,Deniz Gunceler,Megan Stanley,Wessel P. Bruinsma,Lin Huang,Xinran Wei,José Garrido Torres,Abylay Katbashev,Bálint Máté,Sékou-Oumar Kaba,Roberto Sordillo,Yingrong Chen,David B. Williams-Young,Christopher M. Bishop,Jan Hermann,Rianne van den Berg,Paola Gori-Giorgi*

Main category: physics.chem-ph

TL;DR: Skala是一种基于深度学习的交换相关（XC）泛函，通过直接从数据中学习表示，避免了手工设计特征的复杂性，实现了化学精度，同时保持了半局部DFT的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有XC泛函依赖于手工设计的特征，难以同时满足精度和计算效率的需求，无法达到实验室实验的预测精度。

Method: 利用深度学习技术，直接从大量高精度参考数据中学习XC泛函表示，训练数据来自计算密集的波函数方法。

Result: Skala在小分子原子化能量上实现了化学精度（误差低于1 kcal/mol），并在主族化学中表现优于现有混合泛函，同时保持半局部DFT的计算效率。

Conclusion: 随着训练数据的增加，Skala有望进一步提升第一性原理模拟的预测能力。

Abstract: Density Functional Theory (DFT) is the most widely used electronic structure
method for predicting the properties of molecules and materials. Although DFT
is, in principle, an exact reformulation of the Schr\"odinger equation,
practical applications rely on approximations to the unknown
exchange-correlation (XC) functional. Most existing XC functionals are
constructed using a limited set of increasingly complex, hand-crafted features
that improve accuracy at the expense of computational efficiency. Yet, no
current approximation achieves the accuracy and generality for predictive
modeling of laboratory experiments at chemical accuracy -- typically defined as
errors below 1 kcal/mol. In this work, we present Skala, a modern deep
learning-based XC functional that bypasses expensive hand-designed features by
learning representations directly from data. Skala achieves chemical accuracy
for atomization energies of small molecules while retaining the computational
efficiency typical of semi-local DFT. This performance is enabled by training
on an unprecedented volume of high-accuracy reference data generated using
computationally intensive wavefunction-based methods. Notably, Skala
systematically improves with additional training data covering diverse
chemistry. By incorporating a modest amount of additional high-accuracy data
tailored to chemistry beyond atomization energies, Skala achieves accuracy
competitive with the best-performing hybrid functionals across general main
group chemistry, at the cost of semi-local DFT. As the training dataset
continues to expand, Skala is poised to further enhance the predictive power of
first-principles simulations.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [203] [A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks](https://arxiv.org/abs/2506.14464)
*Maximilian Baronig,Yeganeh Bahariasl,Ozan Özdenizci,Robert Legenstein*

Main category: cs.NE

TL;DR: HYPR算法结合并行化和近似在线前向学习，实现了高效在线训练，适用于连续输入序列的SNN。


<details>
  <summary>Details</summary>
Motivation: 解决BPTT在训练RSNN时的在线训练限制和高内存消耗问题。

Method: 提出HYPR算法，结合并行化和近似在线前向学习，支持非线性脉冲神经元模型。

Result: HYPR在振荡亚阈值动态的神经元模型中表现优异，性能接近BPTT。

Conclusion: HYPR为SNN提供了一种高效、低内存消耗的在线训练方法。

Abstract: Recurrent spiking neural networks (RSNNs) can be implemented very efficiently
in neuromorphic systems. Nevertheless, training of these models with powerful
gradient-based learning algorithms is mostly performed on standard digital
hardware using Backpropagation through time (BPTT). However, BPTT has
substantial limitations. It does not permit online training and its memory
consumption scales linearly with the number of computation steps. In contrast,
learning methods using forward propagation of gradients operate in an online
manner with a memory consumption independent of the number of time steps. These
methods enable SNNs to learn from continuous, infinite-length input sequences.
Yet, slow execution speed on conventional hardware as well as inferior
performance has hindered their widespread application. In this work, we
introduce HYbrid PRopagation (HYPR) that combines the efficiency of
parallelization with approximate online forward learning. Our algorithm yields
high-throughput online learning through parallelization, paired with constant,
i.e., sequence length independent, memory demands. HYPR enables parallelization
of parameter update computation over the sub sequences for RSNNs consisting of
almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of
spiking neurons with oscillatory subthreshold dynamics. We find that this type
of neuron model is particularly well trainable by HYPR, resulting in an
unprecedentedly low task performance gap between approximate forward gradient
learning and BPTT.

</details>
