<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 121]
- [cs.DL](#cs.DL) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 10]
- [math.OC](#math.OC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.AI](#cs.AI) [Total: 4]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cs.CL](#cs.CL) [Total: 8]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Machine Learning Practitioners' Views on Data Quality in Light of EU Regulatory Requirements: A European Online Survey](https://arxiv.org/abs/2602.06594)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 提出数据质量与欧盟法规对齐框架，调查180+欧盟数据从业者实践，发现实践与法规期望差距，建议改进工具与跨领域协作


<details>
  <summary>Details</summary>
Motivation: 机器学习系统数据质量如何符合欧盟法规要求是实践者面临的重大挑战，需要解决数据质量与监管要求对齐的实际问题

Method: 1) 提出数据质量维度与欧盟法规要求对齐的实用框架；2) 对180多名欧盟数据从业者进行在线调查，研究他们在确保ML系统数据质量符合法规要求时的方法、挑战和未满足需求

Result: 发现当前实践与监管期望之间存在关键差距，从业者需要更集成的数据质量工具以及技术从业者与法律从业者之间更好的协作

Conclusion: 研究结果为弥合技术专业知识和法规合规性提供了建议，最终促进负责任和可信的机器学习部署

Abstract: Understanding how data quality aligns with regulatory requirements in machine learning (ML) systems presents a critical challenge for practitioners navigating the evolving EU regulatory landscape. To address this, we first propose a practical framework aligning established data quality dimensions with specific EU regulatory requirements. Second, we conducted a comprehensive online survey with over 180 EU-based data practitioners, investigating their approaches, key challenges, and unmet needs when ensuring data quality in ML systems that align with regulatory requirements. Our findings highlight crucial gaps between current practices and regulatory expectations, underscoring practitioners' need for more integrated data quality tools and better collaboration between technical and legal practitioners. These insights inform recommendations for bridging technical expertise and regulatory compliance, ultimately fostering responsible and trustworthy ML deployments.

</details>


### [2] [Filtered Approximate Nearest Neighbor Search Cost Estimation](https://arxiv.org/abs/2602.06721)
*Wenxuan Xia,Mingyu Yang,Wentao Li,Wei Wang*

Main category: cs.DB

TL;DR: 提出E2E成本估计框架用于过滤近似k近邻搜索，通过建模查询向量分布与属性选择性的相关性，实现更准确的成本估计，并在早期终止优化中实现2-3倍的检索效率提升。


<details>
  <summary>Details</summary>
Motivation: 混合查询结合高维向量相似性和结构化属性过滤在实际应用中非常重要，但过滤近似k近邻搜索的优化面临挑战，主要因为组合过滤器导致搜索成本高度可变。现有方法无法准确估计这种成本变化。

Method: 提出E2E成本估计框架，明确建模查询向量分布与属性值选择性之间的相关性。利用这些估计来优化搜索终止条件，实现早期终止策略。

Result: 在真实数据集上的实验表明，该方法相比最先进的基线方法，在保持高搜索精度的同时，将检索效率提升了2-3倍。

Conclusion: E2E框架通过准确估计过滤AKNN搜索的成本，成功优化了混合查询的性能，为下游优化任务（如早期终止）提供了有效支持，显著提升了检索效率。

Abstract: Hybrid queries combining high-dimensional vector similarity with structured attribute filtering have garnered significant attention across both academia and industry. A critical instance of this paradigm is filtered Approximate k Nearest Neighbor (AKNN) search, where embeddings (e.g., image or text) are queried alongside constraints such as labels or numerical range. While essential for rich retrieval, optimizing these queries remains challenging due to the highly variable search cost induced by combined filters. In this paper, we propose a novel cost estimation framework, E2E, for filtered AKNN search and demonstrate its utility in downstream optimization tasks, specifically early termination. Unlike existing approaches, our model explicitly captures the correlation between the query vector distribution and attribute-value selectivity, yielding significantly higher estimation accuracy. By leveraging these estimates to refine search termination conditions, we achieve substantial performance gains. Experimental results on real-world datasets demonstrate that our approach improves retrieval efficiency by 2x-3x over state-of-the-art baselines while maintaining high search accuracy.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: QEIL框架通过推理时间缩放定律和异构编排，在边缘设备上实现高效LLM推理，显著提升能效和覆盖率


<details>
  <summary>Details</summary>
Motivation: 资源受限边缘设备上的大语言模型推理面临延迟挑战，现有解决方案过度依赖云或数据中心基础设施

Method: 提出QEIL框架，包含：1) 异构工作负载分布的推理时间缩放定律；2) 考虑计算吞吐量、内存带宽、功耗和热限制的硬件感知路由；3) 使用智能每瓦特、能量覆盖效率等新指标量化性能-能耗权衡；通过渐进样本复用统一编排

Result: 在125M到2.6B参数的五个模型系列上评估显示：pass@k覆盖率提升7-10.5个百分点，能耗降低35.6-78.2%，平均功耗降低68%满足边缘热预算，延迟改善15.8%，零精度损失

Conclusion: 推理时间缩放定律具有普适性和架构无关性，异构边缘编排是能源受限智能系统的最优策略

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [4] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: 本文展示了Gemma3系列大语言和视觉模型在AMD Ryzen AI NPU上的首次端到端部署，通过硬件感知优化技术实现了显著的性能提升和能效改进。


<details>
  <summary>Details</summary>
Motivation: 现代NPU能否在边缘设备上实现实用、低功耗的LLM和VLM推理，以及如何将基于transformer的模型有效地映射到分片数据流加速器上。

Method: 提出了一系列硬件感知优化技术：预填充阶段采用高效反量化引擎、优化分片矩阵乘法内核和FlowQKV（分块流水线注意力机制）；解码阶段采用FusedDQP（融合反量化和投影）和FlowKV（重构注意力以维持高内存带宽利用率），配合紧凑的Q4NX 4位量化格式。

Result: 相比iGPU，预填充速度提升5.2倍，解码速度提升4.8倍；相比CPU，分别提升33.5倍和2.2倍。能效相比iGPU提升67.2倍，相比CPU提升222.9倍。

Conclusion: 现代NPU能够在边缘设备上实现实用、低功耗的LLM和VLM推理，为将基于transformer的模型映射到分片数据流加速器提供了可推广的蓝图。

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [5] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: iScheduler：基于强化学习的迭代调度框架，用于资源投资问题，通过分解子问题和顺序进程选择来加速优化，支持动态重配置。


<details>
  <summary>Details</summary>
Motivation: 资源投资问题（RIP）在现代计算平台中至关重要，但现有混合整数规划和约束规划方法在大规模实例上速度过慢，且动态更新需要在严格延迟预算下重新调度。

Method: iScheduler将RIP求解建模为分解子问题上的马尔可夫决策过程，通过强化学习驱动的顺序进程选择构建调度，支持重用未更改的进程调度并仅重调度受影响进程。

Result: iScheduler在达到竞争性资源成本的同时，将可行性时间减少高达43倍（相比强商业基线），并发布了工业级基准L-RIPLIB（包含1000个2500-10000任务的实例）。

Conclusion: iScheduler为大规模资源投资问题提供了高效、可重配置的解决方案，显著加速优化过程并支持动态更新需求。

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [6] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: HQP框架通过敏感度感知的结构化剪枝和8位后训练量化的协同优化，在边缘设备上实现3.12倍推理加速和55%模型压缩，同时保证精度下降小于1.5%。


<details>
  <summary>Details</summary>
Motivation: 边缘-云分布式环境中对高保真实时推理的需求日益增长，但面临严重的延迟和能耗限制，需要激进的模型优化来应对这些约束。

Method: 提出混合量化与剪枝(HQP)框架：1) 使用基于Fisher信息矩阵近似的高效动态权重敏感度指标指导结构化剪枝；2) 剪枝严格遵循最大允许精度下降约束；3) 剪枝后模型进行8位后训练量化；4) 确保稀疏结构对量化误差和硬件内核优化具有最大鲁棒性。

Result: 在NVIDIA Jetson边缘平台上，使用MobileNetV3和ResNet-18架构，HQP框架实现了3.12倍推理加速和55%模型大小缩减，同时精度下降严格控制在1.5%以内。

Conclusion: HQP框架相比传统单目标压缩技术，是一种更优越的硬件无关解决方案，适用于在资源受限的边缘基础设施中部署超低延迟AI。

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [7] [Computationally Efficient Laplacian CL-colME](https://arxiv.org/abs/2602.06070)
*Nikola Stankovic*

Main category: cs.DC

TL;DR: 提出CL-colME，一种基于拉普拉斯共识的分散式协同均值估计新变体，相比C-colME减少了计算开销，同时保持收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在异构网络的分散式协同均值估计中，现有的C-colME框架虽然能收敛到最优解，但需要计算昂贵的归一化过程（双重随机平均矩阵）。需要一种既能保持收敛性又能提高计算效率的方法。

Method: 提出CL-colME（基于拉普拉斯共识的协同均值估计），利用拉普拉斯共识机制替代C-colME中的双重随机平均矩阵，避免了复杂的归一化计算过程。

Result: 仿真结果显示，CL-colME在保持与C-colME相同的收敛行为和估计精度的同时，显著提高了计算效率。

Conclusion: CL-colME是一种有效的分散式协同均值估计方法，在保持算法性能的前提下，通过简化共识机制降低了计算复杂度，适用于大规模异构网络。

Abstract: Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.

</details>


### [8] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: 提出BlockPerm-SJLT稀疏草图与FlashSketch CUDA内核协同设计，通过结构化稀疏模式解决GPU内存访问效率问题，在保持草图质量的同时提升计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏草图（如稀疏Johnson-Lindenstrauss变换）虽然能降低计算成本，但其随机稀疏性导致GPU内存访问模式不规则，严重影响内存带宽利用率。需要在GPU效率和草图鲁棒性之间找到平衡。

Method: 采用草图-内核协同设计方法：1) 设计新的稀疏草图家族BlockPerm-SJLT，其稀疏结构专门优化GPU实现；2) 开发对应的优化CUDA内核FlashSketch；3) 引入可调参数在GPU效率和草图鲁棒性之间权衡。

Result: 理论分析证明BlockPerm-SJLT满足无意识子空间嵌入（OSE）保证，可调参数对草图质量有明确影响。实验评估显示FlashSketch在RandNLA基准测试和GraSS数据归因管道中均表现优异，相比现有GPU草图实现约1.7倍的几何平均加速。

Conclusion: 通过协同设计稀疏结构和GPU内核，BlockPerm-SJLT和FlashSketch成功解决了稀疏草图在GPU上的效率瓶颈，在草图质量和计算速度之间取得了更好的帕累托前沿，为GPU加速的随机数值线性代数提供了高效解决方案。

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [9] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer是一个针对异构批次推理的注意力计算框架，通过负载均衡分组和内存优化，显著提升LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 生产环境中LLM推理需要批处理不同长度的请求，但现有注意力优化技术（如FlashAttention）针对单个请求设计，导致计算和I/O不平衡、GPU资源利用率低

Method: 1) 将批处理请求组织成负载均衡的执行组，通过统一内核启动打包多个请求；2) 直接在打包的query-key区域构建注意力内核，消除冗余计算；3) 采用I/O感知分组，将共享前缀请求共置，重组KV缓存为组连续布局

Result: 在真实工作负载评估中，相比最先进的FlashAttention，PackInfer减少推理延迟13.0-20.1%，提升吞吐量20%

Conclusion: PackInfer通过计算和I/O感知的异构批次推理优化，有效解决了生产环境中LLM推理的效率和资源利用率问题

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [10] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: 实验研究表明，在小型Web应用中实施简单的内存缓存能显著降低响应时间，适合教育和简单应用场景


<details>
  <summary>Details</summary>
Motivation: 虽然缓存技术已被广泛研究，但缺乏针对小型Web应用中简单内存缓存效果的实验研究，本文旨在填补这一空白

Method: 使用轻量级Web服务器框架，在相同环境条件下，通过重复HTTP请求比较两种配置：无缓存vs带固定生存时间的内存缓存

Result: 缓存请求的响应时间显著降低，证明简单服务器端缓存能有效提升Web应用性能

Conclusion: 简单服务器端缓存对提升Web应用性能有效，特别适合教育和小型应用场景，因其简单性和可重现性

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [11] [MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments](https://arxiv.org/abs/2602.06075)
*Guangyi Liu,Pengxiang Zhao,Yaozhen Liang,Qinyi Luo,Shunye Tang,Yuxiang Chai,Weifeng Lin,Han Xiao,WenHao Wang,Siheng Chen,Zhengxi Lu,Gao Wu,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.DC

TL;DR: MemGUI-Bench是一个专门评估移动GUI代理记忆能力的基准测试，填补了现有基准在记忆相关任务评估上的空白，通过系统化记忆分类和自动化评估管道揭示当前代理的记忆缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI代理基准测试在记忆能力评估方面存在系统性缺陷，只有5.2-11.8%的任务涉及记忆，且缺乏跨会话学习评估，需要专门针对记忆能力的综合基准。

Method: 1) 建立系统化记忆分类法分析11个代理的5种架构；2) 创建128个跨26个应用的任务，其中89.8%通过跨时间和跨空间保留挑战记忆；3) 开发MemGUI-Eval自动化评估管道，包含渐进审查和7个分层指标；4) 对11个最先进代理进行RQ驱动评估。

Result: 实验显示所有评估系统都存在显著记忆缺陷，识别出5种不同的失败模式，并综合出5个可操作的设计启示。

Conclusion: MemGUI-Bench填补了移动GUI代理记忆评估的空白，揭示了当前系统的记忆局限性，为未来改进提供了基准和设计指导，所有资源将完全开源并持续维护。

Abstract: Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \textbf{\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.

</details>


### [12] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: Canzona框架解决分布式训练中矩阵优化器与张量分片之间的冲突，通过解耦逻辑优化器分配与物理参数分布，实现异步负载均衡，在256 GPU上为Qwen3模型带来1.57倍端到端加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，矩阵优化器（如Shampoo、Muon、SOAP）因其收敛效率受到关注，但它们需要整体更新，这与Megatron等分布式框架中的张量分片存在冲突。现有解决方案不理想：同步方法存在计算冗余，而分层分区无法在不违反高效通信原语几何约束的情况下解决这一冲突。

Method: 提出Canzona框架，统一、异步、负载均衡，解耦逻辑优化器分配与物理参数分布。针对数据并行，引入α平衡静态分区策略，保持原子性同时平衡负载。针对张量并行，设计异步计算流水线，利用微组调度批量处理分片更新并隐藏重构开销。

Result: 在256 GPU上对Qwen3模型家族（最高320亿参数）的广泛评估表明，该方法保持了现有并行架构的效率，端到端迭代时间实现1.57倍加速，优化器步延迟降低5.8倍。

Conclusion: Canzona成功解决了分布式训练中矩阵优化器与张量分片之间的根本冲突，通过创新的解耦设计和异步负载均衡策略，显著提升了大规模语言模型训练效率。

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [13] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: LAAFD使用大语言模型将通用C++代码自动转换为优化的Vitis HLS内核，通过自动化流水线、向量化和数据流分区等硬件优化，在保持高性能的同时大幅降低FPGA编程门槛。


<details>
  <summary>Details</summary>
Motivation: FPGA具有高性能、低延迟和能效优势，但在科学计算和边缘计算中的应用受到硬件专业知识门槛的限制。虽然高级综合(HLS)提高了生产力，但要获得有竞争力的设计仍需要硬件感知优化和数据流设计专业知识。

Method: 提出LAAFD代理工作流，使用大语言模型将通用C++代码转换为优化的Vitis HLS内核。自动化关键转换：深度流水线、向量化和数据流分区，并通过HLS协同仿真和综合反馈形成闭环，验证正确性并迭代改进执行周期。

Result: 在15个代表HPC常见计算模式的内核测试中，LAAFD相比手动调优的Vitis HLS基准实现了99.9%的几何平均性能。对于模板计算工作负载，LAAFD性能与最先进的基于DSL的HLS代码生成器SODA相当，同时生成更易读的内核。

Conclusion: LAAFD在不牺牲效率的前提下，显著降低了FPGA加速的专业知识门槛，使更多开发者能够利用FPGA的高性能优势。

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [14] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: BouquetFL是一个模拟联邦学习中硬件异构性的框架，通过资源限制在单机上模拟不同硬件配置，使研究更贴近实际部署条件。


<details>
  <summary>Details</summary>
Motivation: 当前大多数联邦学习研究在中心机器上进行模拟，忽略了参与方之间潜在的硬件异构性，这导致实验条件与实际部署条件存在差距。

Method: 通过程序化地模拟不同硬件配置的资源限制，在单台物理机器上模拟异构客户端硬件，提供基于真实硬件流行度的自定义硬件采样器。

Result: 开发了BouquetFL框架，能够模拟从消费级到小型实验室设备的广泛硬件配置，使研究人员无需多台物理设备即可研究系统异构性。

Conclusion: BouquetFL填补了联邦学习研究中的方法学空白，使实验实践更贴近实际部署条件，特别适用于研究高度异构联邦的研究人员。

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [15] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP是一种针对带宽受限集群的分布式训练优化方法，通过将前向传播参数缓存在主机内存中，在反向传播时复用，减少50%的节点间通信，在PEFT场景下可减少99%以上通信，相比现有方法显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分布式训练方法在带宽受限的商用硬件集群上存在瓶颈：ZeRO-3在节点间通信开销大；内存缓存方法（如MiCS、ZeRO++）容易导致内存不足；主机内存卸载方法（如ZeRO-Offload、ZeRO-Infinity）因PCIe开销降低吞吐量。需要一种既能保持ZeRO-3最小GPU内存占用，又能减少节点间通信的方法。

Method: FCDP将主机内存作为快速缓存层而非溢出层，缓存前向传播参数并在反向传播时通过快速的节点内all-gather复用，减少50%节点间all-gather通信。对于参数高效微调（PEFT），选择性通信仅训练参数以最大化缓存效果，减少99%以上节点间流量。

Result: 在商用集群设置中，FCDP相比ZeRO-3实现高达100倍的吞吐量提升，相比ZeRO++实现51倍提升，同时保持ZeRO-3的最大批处理大小。

Conclusion: FCDP通过在带宽受限集群上将主机内存作为快速缓存层，有效解决了现有分布式训练方法的瓶颈，在保持内存效率的同时显著提升了训练吞吐量，特别适用于参数高效微调场景。

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [16] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: DualMap提出了一种双映射调度策略，通过两个独立的哈希函数将请求映射到两个候选实例，然后基于系统状态智能选择更好的候选，以同时实现KV缓存亲和性和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 在LLM服务中，跨请求重用提示的KV缓存对于降低TTFT和服务成本至关重要。现有的调度器无法同时解决缓存亲和性调度（将具有相同提示前缀的请求放在一起）和负载均衡调度（将请求均匀分布到计算实例）之间的冲突。

Method: 1) 双映射策略：每个请求通过两个独立的哈希函数基于提示映射到两个候选实例，然后基于当前系统状态智能选择更好的候选
2) SLO感知请求路由：优先考虑缓存亲和性，但当TTFT超过SLO时切换到负载感知调度
3) 热点感知重平衡：动态将请求从过载实例迁移到欠载实例
4) 轻量级双哈希环扩展：利用双哈希环映射支持快速低开销的实例扩展

Result: 在真实工作负载实验中，DualMap在相同的TTFT SLO约束下，相比最先进的工作将有效请求容量提高了最多2.25倍。

Conclusion: DualMap通过双映射调度策略成功解决了LLM服务中缓存亲和性与负载均衡之间的冲突，在动态和倾斜的真实工作负载下表现出色，显著提升了服务容量。

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [17] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: 提出一个在无服务器平台上动态管理结构化并行处理骨架的框架，专注于Farm模式，结合AI驱动的动态扩展来提升性能和服务质量


<details>
  <summary>Details</summary>
Motivation: 将HPC级别的性能和弹性引入无服务器和连续计算环境，同时保持骨架编程的易用性优势，解决无服务器平台上的并行处理管理问题

Method: 开发一个框架，将可复用的farm模板与基于Gymnasium的监控控制层结合，暴露队列、时序和QoS指标，支持反应式和基于学习的控制器，特别研究AI驱动的动态扩展管理并行度

Result: AI驱动的管理比纯基于模型的性能引导更能适应平台特定限制，在保持高效资源利用和稳定扩展行为的同时改善QoS

Conclusion: 基于AI的动态扩展管理在无服务器平台上具有优势，能够更好地平衡性能、资源利用和服务质量，为结构化并行处理骨架在无服务器环境中的应用提供了有效解决方案

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [18] [Fast Makespan Minimization via Short ILPs](https://arxiv.org/abs/2602.06514)
*Danny Hermelin,Dvir Shabtay*

Main category: cs.DS

TL;DR: 利用短整数线性规划求解器的最新进展，为固定机器数的并行机调度问题提供快速伪多项式时间算法


<details>
  <summary>Details</summary>
Motivation: 传统的并行机调度算法在处理最大处理时间较大时效率不高，需要更快的算法来处理实际应用中的大规模调度问题

Method: 利用短整数线性规划（约束数较少的整数规划）的最新求解器改进，将其应用于固定机器数的并行机调度问题

Result: 获得了形式为 $\widetilde{O}(p^{O(1)}_{\max}+n)$ 或 $\widetilde{O}(p^{O(1)}_{\max} \cdot n)$ 的运行时间，在 $p_{\max}$ 适中时优于现有算法

Conclusion: 通过结合短整数线性规划求解器的最新进展，为并行机调度问题提供了更高效的伪多项式时间算法，显著提升了算法性能

Abstract: Short integer linear programs are programs with a relatively small number of constraints. We show how recent improvements on the running-times of solvers for such programs can be used to obtain fast pseudo-polynomial time algorithms for makespan minimization on a fixed number of parallel machines, and other related variants. The running times of our algorithms are all of the form $\widetilde{O}(p^{O(1)}_{\max}+n)$ or $\widetilde{O}(p^{O(1)}_{\max} \cdot n)$, where $p_{\max}$ is the maximum processing time in the input. These improve upon the time complexity of previously known algorithms for moderate values of $p_{\max}$.

</details>


### [19] [Towards Efficient Data Structures for Approximate Search with Range Queries](https://arxiv.org/abs/2602.06860)
*Ladan Kian,Dariusz R. Kowalski*

Main category: cs.DS

TL;DR: 本文提出了一种新的单范围覆盖(SRC)搜索数据结构c-DAG，相比传统1D-Tree，在保持相同时间和空间复杂度的同时，将平均误报率降低了对数因子。


<details>
  <summary>Details</summary>
Motivation: 范围查询在数据检索中应用广泛，但精确完整的范围查询成本高昂。现有的近似搜索方法（SRC搜索）虽然更快，但会产生误报。需要一种能在保持效率的同时减少误报的数据结构。

Method: 提出c-DAG（有向无环图）作为1D-Tree的可调增强版本，每个节点有c≥3个子节点，形成更密集的重叠分支结构。通过竞争性分析比较c-DAG与1D-Tree的性能，并提供扩展到实际查询分布的通用框架。

Result: c-DAG在保持渐近相同时间和内存复杂度的同时，将平均误报率降低了对数因子。在Gowalla数据集上验证了有效性，并展示了在安全性和隐私保护方面的优势。

Conclusion: c-DAG是一种高效的SRC搜索数据结构，在误报率、安全性和隐私保护方面优于传统1D-Tree，特别适用于可搜索加密和多媒体检索等隐私保护系统。

Abstract: Range queries are simple and popular types of queries used in data retrieval. However, extracting exact and complete information using range queries is costly. As a remedy, some previous work proposed a faster principle, {\em approximate} search with range queries, also called single range cover (SRC) search. It can, however, produce some false positives. In this work we introduce a new SRC search structure, a $c$-DAG (Directed Acyclic Graph), which provably decreases the average number of false positives by logarithmic factor while keeping asymptotically same time and memory complexities as a classic tree structure. A $c$-DAG is a tunable augmentation of the 1D-Tree with denser overlapping branches ($c \geq 3$ children per node). We perform a competitive analysis of a $c$-DAG with respect to 1D-Tree and derive an additive constant time overhead and a multiplicative logarithmic improvement of the false positives ratio, on average. We also provide a generic framework to extend our results to empirical distributions of queries, and demonstrate its effectiveness for Gowalla dataset. Finally, we quantify and discuss security and privacy aspects of SRC search on $c$-DAG vs 1D-Tree, mainly mitigation of structural leakage, which makes $c$-DAG a good data structure candidate for deployment in privacy-preserving systems (e.g., searchable encryption) and multimedia retrieval.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [SVRepair: Structured Visual Reasoning for Automated Program Repair](https://arxiv.org/abs/2602.06090)
*Xiaoxuan Tang,Jincheng Wang,Liwei Luo,Jingxuan Xu,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: SVRepair是一个多模态程序修复框架，通过结构化视觉表示将视觉工件转换为语义场景图，帮助大语言模型更好地理解和修复包含视觉信息的bug。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型程序修复方法大多是单模态的，无法有效利用bug报告中包含的视觉信息（如截图、控制流图等）。直接使用密集的视觉输入会导致上下文丢失和噪声，使得多模态大语言模型难以将视觉观察转化为精确的故障定位和可执行补丁。

Method: SVRepair首先微调一个视觉语言模型（SVR），将异构视觉工件统一转换为语义场景图，捕捉GUI元素及其结构关系。基于该图，驱动编码代理进行故障定位和补丁合成，并引入迭代视觉工件分割策略，逐步缩小输入到bug相关区域以减少无关上下文和幻觉。

Result: 在多个基准测试中达到最先进性能：在SWE-Bench M上达到36.47%准确率，在MMCode上达到38.02%，在CodeVision上达到95.12%，验证了SVRepair在多模态程序修复中的有效性。

Conclusion: SVRepair通过结构化视觉表示有效解决了多模态程序修复中的语义鸿沟问题，能够充分利用bug报告中的视觉信息，显著提升了程序修复的准确性和效果。

Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.

</details>


### [21] [Coding Agents with Environment Interaction: A Theoretical Perspective](https://arxiv.org/abs/2602.06098)
*Nicolas Menet,Michael Hersche,Andreas Krause,Abbas Rahimi*

Main category: cs.SE

TL;DR: 论文提出了一个概率框架来分析编码代理的两种主要环境交互策略：基于执行环境的选择启发式和基于环境反馈的条件生成，理论证明了模糊功能相似性估计器优于功能等价估计器，并将反向提示框架化为Thompson采样的近似。


<details>
  <summary>Details</summary>
Motivation: 编码代理在测试驱动软件开发中应用日益广泛，但其环境交互策略的理论机制尚未得到充分探索。需要为两种主流范式（代码生成后选择和基于反馈的条件生成）提供理论框架，以理解其内在工作机制。

Method: 1. 将常见的代码选择启发式形式化为环境感知的正确性估计器；2. 理论证明基于模糊功能相似性的估计器比基于功能等价的估计器具有更好的信噪比；3. 将反向提示框架化为Thompson采样的上下文近似；4. 推导具有不可观测分量的奖励函数的遗憾界；5. 在三个开源模型和三个基准数据集上进行实验验证。

Result: 理论分析表明：模糊功能相似性估计器严格优于功能等价估计器；反向提示的有效性受非正式任务描述模糊性的限制（不可约遗憾）；实验在BigCodeBenchHard、LeetCodeDataset和QiskitHumanEvalSim上验证了理论发现；基于形式化分析提出了改进任务描述的方法，并创建了新基准QiskitHumanEvalSimX。

Conclusion: 该研究为编码代理的环境交互策略提供了理论框架，揭示了模糊功能相似性估计器的优势，解释了反向提示的局限性，并为改进任务描述提供了理论指导，推动了编码代理的理论基础发展。

Abstract: Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.

</details>


### [22] [Scaling Mobile Chaos Testing with AI-Driven Test Execution](https://arxiv.org/abs/2602.06223)
*Juan Marcano,Ashish Samant,Kai Song,Lingchao Chen,Kaelan Mikowicz,Tim Smyth,Mengdie Zhang,Ali Zamani,Arturo Bravo Rovirosa,Sowjanya Puligadda,Srikanth Prodduturi,Mayank Bansal*

Main category: cs.SE

TL;DR: 论文提出了一种自动化移动混沌测试系统，将LLM驱动的移动测试平台DragonCrawl与服务级故障注入系统uHavoc集成，解决了传统混沌工程在移动端测试中因流程、位置和故障场景组合爆炸而无法扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中的移动应用容易受到后端服务故障的影响，但传统的混沌工程方法无法扩展到移动测试，因为需要验证的流程、位置和故障场景的组合呈爆炸式增长。

Method: 开发了自动化移动混沌测试系统，集成DragonCrawl（基于LLM的移动测试平台）和uHavoc（服务级故障注入系统）。核心洞察是自适应AI驱动的测试执行可以在后端降级条件下导航移动应用，无需为每个用户流程、城市和故障类型组合手动编写测试用例。

Result: 自2024年第一季度以来，系统在Uber的Rider、Driver和Eats应用的47个关键流程中执行了超过18万次自动化混沌测试，相当于约3.9万小时的手动测试工作量。识别了23个弹性风险，其中70%是架构依赖违规（非关键服务故障影响核心用户流程）。12个问题严重到阻止行程请求或食品订单，2个导致应用崩溃（仅通过移动混沌测试可检测）。自动化根因分析将调试时间从数小时减少到数分钟，在将移动故障归因于特定后端服务方面达到88%的precision@5。

Conclusion: 该系统设计在故障注入下保持99%的测试可靠性，操作经验表明，在生产规模下实现持续移动弹性验证是可行的。自动化移动混沌测试系统能够有效识别传统后端测试无法发现的移动端特定问题，显著提高了测试效率和问题诊断能力。

Abstract: Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.

</details>


### [23] [Trustworthy AI Software Engineers](https://arxiv.org/abs/2602.06310)
*Aldeida Aleti,Baishakhi Ray,Rashina Hoda,Simin Chen*

Main category: cs.SE

TL;DR: 该论文重新审视AI软件工程师的定义，探讨其可信赖性的关键维度，并提出评估和治理框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程代理的快速发展，软件工程师的本质定义受到挑战。论文旨在重新定义AI软件工程师的概念，并深入探讨如何使这些AI代理变得可信赖，以促进人机协作的软件工程团队。

Method: 基于软件工程的传统定义和最新AI代理研究，将AI软件工程师概念化为人类-AI软件工程团队的参与者。通过历史视角和新兴愿景，识别可信赖性的关键维度，并讨论评估和展示可信赖性的方法。

Result: 提出了AI软件工程师可信赖性的四个关键维度：技术质量、透明度和问责制、认知谦逊、社会和伦理对齐。识别了信任测量的根本差距，并提出了设计、评估和治理AI软件工程系统的框架。

Conclusion: 需要采用"设计即伦理"的方法来构建未来人类-AI软件工程团队中的适当信任，强调可信赖性应作为系统属性而非主观态度，并为AI软件工程师的设计和评估提供指导。

Abstract: With the rapid rise of AI coding agents, the fundamental premise of what it means to be a software engineer is in question. In this vision paper, we re-examine what it means for an AI agent to be considered a software engineer and then critically think about what makes such an agent trustworthy. \textit{Grounded} in established definitions of software engineering (SE) and informed by recent research on agentic AI systems, we conceptualise AI software engineers as participants in human-AI SE teams composed of human software engineers and AI models and tools, and we distinguish trustworthiness as a key property of these systems and actors rather than a subjective human attitude. Based on historical perspectives and emerging visions, we identify key dimensions that contribute to the trustworthiness of AI software engineers, spanning technical quality, transparency and accountability, epistemic humility, and societal and ethical alignment. We further discuss how trustworthiness can be evaluated and demonstrated, highlighting a fundamental trust measurement gap: not everything that matters for trust can be easily measured. Finally, we outline implications for the design, evaluation, and governance of AI SE systems, advocating for an ethics-by-design approach to enable appropriate trust in future human-AI SE teams.

</details>


### [24] [AgentStepper: Interactive Debugging of Software Development Agents](https://arxiv.org/abs/2602.06593)
*Robert Hutter,Michael Pradel*

Main category: cs.SE

TL;DR: AgentStepper：首个用于LLM软件工程代理的交互式调试器，通过结构化对话表示、断点、逐步执行等功能提升调试效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的软件开发代理难以调试，开发者需要理解复杂的LLM查询、工具调用和代码修改轨迹，但现有技术缺乏可理解的中间过程展示

Method: 提出AgentStepper调试器，将代理轨迹表示为LLM、代理程序和工具之间的结构化对话，支持断点、逐步执行、实时编辑提示和工具调用，并捕获显示仓库级代码变更

Result: 在三个先进代理（ExecutionAgent、SWE-Agent、RepairAgent）上验证，仅需39-42行代码修改即可集成；用户研究表明，相比传统工具，能提升轨迹解释能力（64% vs 67%）、提高bug识别成功率（17% vs 60%）、降低工作负载（挫败感从5.4/7.0降至2.4/7.0）

Conclusion: AgentStepper通过提供类似传统调试但更高抽象级别的交互式调试能力，显著改善了LLM软件工程代理的可调试性，为开发者提供了有效的调试工具

Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.

</details>


### [25] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: AST(NIT)方法通过序列化完整AST来增强LLM的代码摘要能力，在保持质量的同时减少输入长度和训练时间


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码摘要方法主要依赖原始代码或部分AST信号，未能充分利用完整AST表示的潜力。AST在传统编码器-解码器模型中已被证明能提升摘要质量，但在LLM中尚未充分探索。

Method: 提出AST(NIT)方法，通过AST增强和序列化技术，保留词汇细节并将结构信息编码为LLM兼容的序列。使用LLaMA-3.1-8B模型在CodeXGLUE Python数据集上进行实验。

Result: 序列化的AST减少了LLM输入长度，缩短了训练时间，同时达到了与现有方法相当的摘要质量。

Conclusion: 完整AST表示在LLM代码摘要中具有潜力，AST(NIT)方法能有效利用AST结构信息，在保持性能的同时提升效率。

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [26] [Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA](https://arxiv.org/abs/2602.06709)
*Duong Bui,Stefan Grintz,Alexander Berndt,Thomas Bach*

Main category: cs.SE

TL;DR: LLM结合历史故障数据可自动化CI/CD管道故障管理，在工业项目中达到92.1%的精确解决方案准确率


<details>
  <summary>Details</summary>
Motivation: 手动管理CI/CD管道故障耗时且低效，传统程序无法处理非结构化故障信息，而LLM在处理非结构化数据方面展现出潜力

Method: 在SAP HANA工业项目中评估LLM系统，提供管道信息、故障管理指令和历史故障数据等不同领域知识，进行消融研究确定各知识类型对解决方案准确性的贡献

Result: 历史故障数据对系统准确性贡献最大，使系统在92.1%的情况下能生成精确解决方案；提供领域知识时错误定位准确率达97.4%，无领域知识时为84.2%

Conclusion: LLM结合历史故障数据是实现CI/CD管道故障管理自动化的有前景方法，能显著提高故障定位和解决方案的准确性

Abstract: CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.

</details>


### [27] [Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience](https://arxiv.org/abs/2602.06831)
*Marco De Luca,Domenico Amalfitano,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 提出一种基于软件度量阈值的可解释故障预测方法，用于嵌入式固件质量保证，替代黑盒AI模型


<details>
  <summary>Details</summary>
Motivation: 嵌入式固件在安全关键领域需要高可靠性，但现有机器学习故障预测模型缺乏可解释性，限制了工业应用。开发者需要可直接用于质量保证流程的可操作见解。

Method: 通过统计分析从一组项目中推导出软件度量阈值，然后应用于独立开发的固件。使用Coverity和Understand静态分析工具提取C嵌入式固件的软件度量，通过统计分析和假设检验识别区分性度量并推导经验阈值。

Result: 实验验证表明推导的阈值能有效识别故障易发函数，具有高精度。这些阈值可作为可解释的故障预测解决方案，符合行业标准和SQA实践。

Conclusion: 该方法为黑盒AI模型提供了实用替代方案，使开发者能系统评估软件质量，采取预防措施，并将基于度量的故障预测集成到工业开发工作流中。

Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.

</details>


### [28] [TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code](https://arxiv.org/abs/2602.06875)
*Jiangping Huang,Wenguang Ye,Weisong Sun,Jian Zhang,Mingyue Zhang,Yang Liu*

Main category: cs.SE

TL;DR: TraceCoder是一个多智能体协作框架，通过运行时追踪和因果分析来修复LLM生成的代码错误，结合历史经验学习和回滚机制，显著提升修复准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法依赖简单的通过/失败信号，缺乏对程序行为的深入洞察，难以精确定位错误，且无法从历史失败中学习，导致修复过程低效重复。

Method: TraceCoder采用多智能体框架模拟专家观察-分析-修复过程：1) 通过诊断探针捕获细粒度运行时追踪；2) 对追踪进行因果分析定位根本原因；3) 历史经验学习机制从先前失败中提炼知识；4) 回滚机制确保每次迭代严格改进。

Result: 在多个基准测试中，TraceCoder相比现有先进基线实现了34.43%的相对Pass@1准确率提升。消融研究表明迭代修复过程单独贡献了65.61%的相对准确率增益，且在准确率和成本效率方面均显著优于领先的迭代方法。

Conclusion: TraceCoder通过深度运行时分析、历史经验学习和严格改进机制，有效解决了LLM代码生成中的微妙错误修复问题，为自动化代码修复提供了更高效精确的解决方案。

Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [29] [IE-RAP: An Intelligence and Efficient Reader Anti-Collision Protocol for Dense RFID Networks](https://arxiv.org/abs/2602.06626)
*Hadiseh Rezaei,Rahim Taheri,Mohammad Shojafar*

Main category: cs.NI

TL;DR: IE-RAP协议通过结合TDMA和FDMA机制，使用SIFT函数和读者距离计算，显著提升RFID网络性能，支持移动读者集成。


<details>
  <summary>Details</summary>
Motivation: 在密集读者环境中，RFID系统面临读者与读者、读者与标签之间的碰撞问题，导致网络性能下降。现有解决方案需要改进，特别是需要支持移动读者并提高整体效率。

Method: 提出IE-RAP协议，结合TDMA和FDMA机制，采用SIFT函数和读者距离计算技术，防止重复读取标签并确保通信信道及时释放，有效消除不必要的碰撞。

Result: 仿真结果表明，与现有方法相比，吞吐量提升26%，平均等待时间减少74%，能耗降低52%，同时支持移动读者的无缝集成。

Conclusion: IE-RAP协议通过智能防碰撞机制显著改善了RFID网络性能，为密集读者环境提供了高效解决方案，特别适合需要移动读者集成的应用场景。

Abstract: An advanced technology known as a radio frequency identification (RFID) system enables seamless wireless communication between tags and readers. This system operates in what is referred to as a dense reader environment, where readers are placed close to each other to optimize coverage. However, this setup comes with its challenges, as it increases the likelihood of collisions between readers and tags (reader-to-reader and reader-to-tag), leading to reduced network performance. To address this issue, various protocols have been proposed, with centralized solutions emerging as promising options due to their ability to deliver higher throughput. In this paper, we propose the Intelligence and Efficient Reader Anti-collision Protocol (IE-RAP) that improves network performance such as throughput, average waiting time, and energy consumption, which employs a powerful combination of Time Division Multiple Access (TDMA) and Frequency Division Multiple Access (FDMA) mechanisms. IE-RAP improves the efficiency of RFID networks through techniques such as the SIFT function and distance calculation between readers. By preventing re-read tags and ensuring the on-time release of the communication channel, we effectively eliminate unnecessary collisions. Our simulations emphasize the superiority of our proposed method, it increases 26% throughput, reduces 74% the average waiting time, and lower by 52% the energy consumption compared to existing approaches. Importantly, our solution supports the seamless integration of mobile readers within the network.

</details>


### [30] [Talk Like a Packet: Rethinking Network Traffic Analysis with Transformer Foundation Models](https://arxiv.org/abs/2602.06636)
*Samara Mayhoub,Chuan Heng Foh,Mahdi Boloursaz Mashhadi,Mohammad Shojafar,Rahim Tafazolli*

Main category: cs.NI

TL;DR: 该论文提出了一种基于Transformer的流量基础模型，通过统一的预训练和微调流程，在多种网络流量分析任务中展现良好性能。


<details>
  <summary>Details</summary>
Motivation: 受Transformer在自然语言处理中的成功启发，研究者希望探索其作为网络流量分析基础模型的潜力，以解决传统方法在泛化能力和数据效率方面的限制。

Method: 提出统一的预训练和微调流程，构建流量基础模型，并基于架构、输入模态和预训练策略对现有模型进行分类。

Result: 基础模型在流量分类、流量特征预测和流量生成等下游任务中表现出良好的泛化能力，相比非基础模型基线获得性能提升，且能在有限标注数据下有效学习流量表示。

Conclusion: Transformer基础模型能够有效学习网络流量表示，在多种分析任务中表现优异，展示了在未来智能网络分析系统中的巨大潜力。

Abstract: Inspired by the success of Transformer-based models in natural language processing, this paper investigates their potential as foundation models for network traffic analysis. We propose a unified pre-training and fine-tuning pipeline for traffic foundation models. Through fine-tuning, we demonstrate the generalizability of the traffic foundation models in various downstream tasks, including traffic classification, traffic characteristic prediction, and traffic generation. We also compare against non-foundation baselines, demonstrating that the foundation-model backbones achieve improved performance. Moreover, we categorize existing models based on their architecture, input modality, and pre-training strategy. Our findings show that these models can effectively learn traffic representations and perform well with limited labeled datasets, highlighting their potential in future intelligent network analysis systems.

</details>


### [31] [Makespan Minimization in Split Learning: From Theory to Practice](https://arxiv.org/abs/2602.06693)
*Robert Ganian,Fionn Mc Inerney,Dimitra Tsigkari*

Main category: cs.NI

TL;DR: 本文研究分布式机器学习中的分割学习问题，提出针对同质和异质任务设置的近似算法和启发式方法，证明问题的计算复杂性并开发实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 分割学习在异构物联网设备分布式机器学习中具有重要应用价值，但如何通过联合优化客户端-助手分配和任务调度来最小化训练时间是一个核心挑战。

Method: 首先研究同质任务设置（助手有内存基数约束），证明其计算复杂性并开发5-近似算法；然后扩展到异质任务设置（助手有内存容量约束，客户端有可变内存成本），开发新的启发式算法。

Result: 证明同质任务问题不存在多项式时间精确算法和近似方案，但提出了5-近似算法；证明异质任务问题不存在任何近似因子的多项式时间近似算法，但开发的启发式算法在实验中优于现有方法。

Conclusion: 分割学习中的客户端-助手分配和调度问题具有很高的计算复杂性，但通过理论分析和算法设计，可以为实际应用提供有效的解决方案，特别是在异质任务设置中提出的新启发式方法具有优越性能。

Abstract: Split learning recently emerged as a solution for distributed machine learning with heterogeneous IoT devices, where clients can offload part of their training to computationally-powerful helpers. The core challenge in split learning is to minimize the training time by jointly devising the client-helper assignment and the schedule of tasks at the helpers. We first study the model where each helper has a memory cardinality constraint on how many clients it may be assigned, which represents the case of homogeneous tasks. Through complexity theory, we rule out exact polynomial-time algorithms and approximation schemes even for highly restricted instances of this problem. We complement these negative results with a non-trivial polynomial-time 5-approximation algorithm. Building on this, we then focus on the more general heterogeneous task setting considered by Tirana et al. [INFOCOM 2024], where helpers have memory capacity constraints and clients have variable memory costs. In this case, we prove that, unless P=NP, the problem cannot admit a polynomial-time approximation algorithm for any approximation factor. However, by adapting our aforementioned 5-approximation algorithm, we develop a novel heuristic for the heterogeneous task setting and show that it outperforms heuristics from prior works through extensive experiments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [NanoNet: Parameter-Efficient Learning with Label-Scarce Supervision for Lightweight Text Mining Model](https://arxiv.org/abs/2602.06093)
*Qianren Mao,Yashuo Luo,Ziqi Qin,Junnan Liu,Weifeng Jiang,Zhijun Chen,Zhuoran Li,Likang Xiao,Chuou Xu,Qili Zhang,Hanwen Hao,Jingzheng Li,Chunghua Lin,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: NanoNet是一个轻量级文本挖掘框架，通过参数高效学习和在线知识蒸馏，在有限监督下训练多个小模型，并通过相互学习正则化提升性能，最终获得用于下游推理的轻量模型。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级半监督学习策略虽然能节省标注样本和降低推理成本，但训练策略计算密集且容易陷入局部最优，增加了寻找最优解的难度。因此需要探索集成三种低成本场景的可行性：有限标注监督、轻量微调和快速推理小模型。

Method: 提出NanoNet框架，采用参数高效学习配合有限监督，使用在线知识蒸馏生成多个小模型，并通过相互学习正则化增强这些模型的性能。整个过程利用参数高效学习降低训练成本并最小化监督需求。

Result: 该框架能够减少训练成本，最小化监督需求，最终产生用于下游推理的轻量级模型。

Conclusion: NanoNet为轻量级文本挖掘提供了一个有效的解决方案，成功整合了有限监督、轻量微调和快速推理三种低成本场景，解决了传统方法计算密集和易陷局部最优的问题。

Abstract: The lightweight semi-supervised learning (LSL) strategy provides an effective approach of conserving labeled samples and minimizing model inference costs. Prior research has effectively applied knowledge transfer learning and co-training regularization from large to small models in LSL. However, such training strategies are computationally intensive and prone to local optima, thereby increasing the difficulty of finding the optimal solution. This has prompted us to investigate the feasibility of integrating three low-cost scenarios for text mining tasks: limited labeled supervision, lightweight fine-tuning, and rapid-inference small models. We propose NanoNet, a novel framework for lightweight text mining that implements parameter-efficient learning with limited supervision. It employs online knowledge distillation to generate multiple small models and enhances their performance through mutual learning regularization. The entire process leverages parameter-efficient learning, reducing training costs and minimizing supervision requirements, ultimately yielding a lightweight model for downstream inference.

</details>


### [33] [Agentic Workflow Using RBA$_θ$ for Event Prediction](https://arxiv.org/abs/2602.06097)
*Purbak Sengupta,Sambeet Mishra,Sonal Shreya*

Main category: cs.LG

TL;DR: 论文提出了一种"事件优先、频率感知"的风电爬坡事件预测新范式，直接预测爬坡事件并重建功率轨迹，而非从密集预测中推断事件。该方法整合了统计、机器学习和深度学习模型，通过小波频率分解、时序激励特征和自适应特征选择，实现了稳定的长期事件预测、物理一致的轨迹重建和零样本迁移到新风电场的功能。


<details>
  <summary>Details</summary>
Motivation: 风电爬坡事件由于强变异性、多尺度动态和站点特定气象效应而难以预测。传统方法通过后验事件提取提供可解释基线，但在跨站点泛化方面有限。需要开发能够直接预测事件并重建轨迹的鲁棒方法。

Method: 1. 提出事件优先、频率感知的预测范式；2. 基于增强的RBAθ方法的事件表示；3. 渐进整合统计、机器学习和深度学习模型；4. 引入事件优先的深度架构，整合小波频率分解、时序激励特征和自适应特征选择；5. 提出智能预测层，根据操作上下文动态选择专门工作流。

Result: 1. 直接事件预测使用随机森林比基于生存的公式更鲁棒；2. 事件优先深度架构实现稳定长期事件预测；3. 物理一致的轨迹重建；4. 零样本迁移到未见过的风电场；5. 经验分析显示爬坡幅度和持续时间由不同的中频带控制；6. 从稀疏事件预测中实现准确信号重建。

Conclusion: 事件优先、频率感知的预测为风电功率预测提供了可迁移且操作对齐的替代方案，优于传统的轨迹优先方法。该框架展示了直接事件预测与轨迹重建的有效性，特别适用于跨站点泛化和长期预测场景。

Abstract: Wind power ramp events are difficult to forecast due to strong variability, multi-scale dynamics, and site-specific meteorological effects. This paper proposes an event-first, frequency-aware forecasting paradigm that directly predicts ramp events and reconstructs the power trajectory thereafter, rather than inferring events from dense forecasts. The framework is built on an enhanced Ramping Behaviour Analysis (RBA$_θ$) method's event representation and progressively integrates statistical, machine-learning, and deep-learning models. Traditional forecasting models with post-hoc event extraction provides a strong interpretable baseline but exhibits limited generalisation across sites. Direct event prediction using Random Forests improves robustness over survival-based formulations, motivating fully event-aware modelling. To capture the multi-scale nature of wind ramps, we introduce an event-first deep architecture that integrates wavelet-based frequency decomposition, temporal excitation features, and adaptive feature selection. The resulting sequence models enable stable long-horizon event prediction, physically consistent trajectory reconstruction, and zero-shot transfer to previously unseen wind farms. Empirical analysis shows that ramp magnitude and duration are governed by distinct mid-frequency bands, allowing accurate signal reconstruction from sparse event forecasts. An agentic forecasting layer is proposed, in which specialised workflows are selected dynamically based on operational context. Together, the framework demonstrates that event-first, frequency-aware forecasting provides a transferable and operationally aligned alternative to trajectory-first wind-power prediction.

</details>


### [34] [Toward Faithful and Complete Answer Construction from a Single Document](https://arxiv.org/abs/2602.06103)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: EVE是一个结构化框架，通过提取、验证和枚举的管道约束生成，提高文档基础推理的完整性和忠实性，打破覆盖率和准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型基于统计的下一个词预测，偏向高概率延续而非基于源内容的全面忠实回答，缺乏确保完整性和忠实性的系统机制，这与AI安全原则相冲突。

Method: EVE框架将高严谨推理分解为提取、验证和枚举的结构化可验证管道，约束生成过程，不同于自由形式的提示方法。

Result: 召回率和精确率分别提升高达24%和29%，F1分数提升31%，打破了单次LLM生成中覆盖率和准确性的长期权衡，同时缓解了长度限制导致的截断问题。

Conclusion: EVE通过结构化管道显著提升文档基础推理性能，但受自然语言固有模糊性限制，存在性能饱和现象，反映了基于语言推理的基本限制。

Abstract: Modern large language models (LLMs) are powerful generators driven by statistical next-token prediction. While effective at producing fluent text, this design biases models toward high-probability continuations rather than exhaustive and faithful answers grounded in source content. As a result, directly applying LLMs lacks systematic mechanisms to ensure both completeness (avoiding omissions) and faithfulness (avoiding unsupported content), which fundamentally conflicts with core AI safety principles. To address this limitation, we present EVE, a structured framework for document-grounded reasoning.
  Unlike free-form prompting, EVE constrains generation to a structured, verifiable pipeline that decomposes high-rigor reasoning into extraction, validation, and enumeration. Empirically, this design enables consistent and simultaneous improvements in recall, precision, and F1-score: recall and precision increase by up to 24\% and 29\%, respectively, with a corresponding 31\% gain in F1-score. This effectively breaks the long-standing trade-off between coverage and accuracy typical of single-pass LLM generation, while also mitigating generation truncation caused by length limitations. At the same time, we emphasize that EVE exhibits performance saturation due to the inherent ambiguity of natural language, reflecting fundamental limits of language-based reasoning.

</details>


### [35] [Pragmatic Curiosity: A Hybrid Learning-Optimization Paradigm via Active Inference](https://arxiv.org/abs/2602.06104)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 提出"实用好奇心"范式，将贝叶斯优化和贝叶斯实验设计统一到主动推理框架中，通过最小化期望自由能量来平衡实用性和信息获取，在混合学习-优化任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 许多工程和科学工作流依赖于昂贵的黑盒评估，需要同时提升性能和减少不确定性的决策。贝叶斯优化(BO)和贝叶斯实验设计(BED)分别处理目标寻求和信息寻求，但在学习和优化内在耦合的混合场景中指导有限

Method: 提出"实用好奇心"混合学习-优化范式，源自主动推理理论，通过最小化期望自由能量来选择行动，该单一目标将实用效用与认知信息增益相结合

Result: 在多种真实世界混合任务中验证了实用好奇心的有效性和灵活性，包括约束系统识别、定向主动搜索和未知偏好的复合优化。在所有基准测试中，实用好奇心一致优于强BO型和BED型基线，提供更高的估计精度、更好的关键区域覆盖和改善的最终解质量

Conclusion: 实用好奇心为混合学习和优化问题提供了一个统一框架，能够有效平衡探索和利用，在实际应用中表现出色

Abstract: Many engineering and scientific workflows depend on expensive black-box evaluations, requiring decision-making that simultaneously improves performance and reduces uncertainty. Bayesian optimization (BO) and Bayesian experimental design (BED) offer powerful yet largely separate treatments of goal-seeking and information-seeking, providing limited guidance for hybrid settings where learning and optimization are intrinsically coupled. We propose "pragmatic curiosity," a hybrid learning-optimization paradigm derived from active inference, in which actions are selected by minimizing the expected free energy--a single objective that couples pragmatic utility with epistemic information gain. We demonstrate the practical effectiveness and flexibility of pragmatic curiosity on various real-world hybrid tasks, including constrained system identification, targeted active search, and composite optimization with unknown preferences. Across these benchmarks, pragmatic curiosity consistently outperforms strong BO-type and BED-type baselines, delivering higher estimation accuracy, better critical-region coverage, and improved final solution quality.

</details>


### [36] [Private and interpretable clinical prediction with quantum-inspired tensor train models](https://arxiv.org/abs/2602.06110)
*José Ramón Pareja Monturiol,Juliette Sinnott,Roger G. Melko,Mohammad Kohandel*

Main category: cs.LG

TL;DR: 该论文评估了临床机器学习模型（逻辑回归和神经网络）的隐私风险，发现两者都存在训练数据泄露问题，并提出基于张量化的量子启发防御方法，在保护隐私的同时保持模型准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床机器学习需要在预测准确性、可解释性和隐私保护之间取得平衡。现有模型如逻辑回归（透明但预测能力有限）和神经网络（预测能力强但缺乏透明度）都存在隐私漏洞，容易受到攻击泄露训练数据信息。

Method: 1）设计攻击实验评估逻辑回归和神经网络模型的隐私风险；2）提出基于张量化的防御方法，将离散化模型转换为张量序列（TT），完全混淆参数；3）在LORIS（公开可用的免疫治疗响应预测逻辑回归模型）和浅层神经网络上进行验证。

Result: 1）逻辑回归和神经网络都泄露显著的训练集信息，逻辑回归在白盒场景下特别脆弱；2）交叉验证等常见实践加剧了这些风险；3）张量化防御将白盒攻击降低到随机猜测水平，黑盒攻击效果与差分隐私相当，同时保持模型准确性；4）张量化模型保持了逻辑回归的可解释性，并能高效计算边际和条件分布。

Conclusion: 张量化为临床预测提供了实用基础，实现了隐私保护、可解释性和有效性的平衡。该方法广泛适用，能够保护模型参数隐私，同时保持甚至增强模型的可解释性，为临床机器学习的安全部署提供了解决方案。

Abstract: Machine learning in clinical settings must balance predictive accuracy, interpretability, and privacy. Models such as logistic regression (LR) offer transparency, while neural networks (NNs) provide greater predictive power; yet both remain vulnerable to privacy attacks. We empirically assess these risks by designing attacks that identify which public datasets were used to train a model under varying levels of adversarial access, applying them to LORIS, a publicly available LR model for immunotherapy response prediction, as well as to additional shallow NN models trained for the same task. Our results show that both models leak significant training-set information, with LRs proving particularly vulnerable in white-box scenarios. Moreover, we observe that common practices such as cross-validation in LRs exacerbate these risks. To mitigate these vulnerabilities, we propose a quantum-inspired defense based on tensorizing discretized models into tensor trains (TTs), which fully obfuscates parameters while preserving accuracy, reducing white-box attacks to random guessing and degrading black-box attacks comparably to Differential Privacy. TT models retain LR interpretability and extend it through efficient computation of marginal and conditional distributions, while also enabling this higher level of interpretability for NNs. Our results demonstrate that tensorization is widely applicable and establishes a practical foundation for private, interpretable, and effective clinical prediction.

</details>


### [37] [Compressing LLMs with MoP: Mixture of Pruners](https://arxiv.org/abs/2602.06127)
*Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Leandro Giusti Mugnaini,Keith Ando Ogawa,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: MoP提出了一种统一的深度与宽度剪枝框架，在LLaMA系列模型上实现了更好的压缩效果和实际加速


<details>
  <summary>Details</summary>
Motivation: 大语言模型的高计算需求促使参数减少和推理加速方法的发展。现有剪枝方法通常只关注单一维度（深度或宽度），需要更全面的剪枝策略

Method: MoP（Mixture of Pruners）是一个迭代框架，每轮生成两个分支：深度剪枝和宽度剪枝，然后选择最佳候选继续路径。框架统一了深度和宽度剪枝维度

Result: 在LLaMA-2和LLaMA-3上超越了现有结构化剪枝方法，在多种压缩率下保持更高准确率。40%压缩时端到端延迟降低39%。在视觉语言模型LLaVA-1.5上也显著提升计算效率

Conclusion: MoP通过统一深度和宽度剪枝，在保持模型性能的同时实现了更好的压缩效果和实际加速，且仅用文本恢复微调就能恢复视觉任务性能

Abstract: The high computational demands of Large Language Models (LLMs) motivate methods that reduce parameter count and accelerate inference. In response, model pruning emerges as an effective strategy, yet current methods typically focus on a single dimension-depth or width. We introduce MoP (Mixture of Pruners), an iterative framework that unifies these dimensions. At each iteration, MoP generates two branches-pruning in depth versus pruning in width-and selects a candidate to advance the path. On LLaMA-2 and LLaMA-3, MoP advances the frontier of structured pruning, exceeding the accuracy of competing methods across a broad set of compression regimes. It also consistently outperforms depth-only and width-only pruning. Furthermore, MoP translates structural pruning into real speedup, reducing end-to-end latency by 39% at 40% compression. Finally, extending MoP to the vision-language model LLaVA-1.5, we notably improve computational efficiency and demonstrate that text-only recovery fine-tuning can restore performance even on visual tasks.

</details>


### [38] [Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction](https://arxiv.org/abs/2602.06129)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: Skjold-DiT是一个扩散-Transformer框架，用于预测建筑级气候风险指标，并整合交通网络结构和可达性信号，为智能车辆提供灾害条件下的路由约束。


<details>
  <summary>Details</summary>
Motivation: 气候灾害日益破坏城市交通和应急响应，损害住房存量、基础设施并降低网络可达性。需要预测建筑级风险并整合交通网络信息，以支持智能车辆路由和应急调度。

Method: 提出Skjold-DiT框架，包含三个核心组件：Fjell-Prompt（支持跨城市迁移的提示条件接口）、Norrland-Fusion（统一灾害地图、建筑属性、人口统计和交通基础设施的跨模态注意力机制）、Valkyrie-Forecast（生成干预提示下概率风险轨迹的反事实模拟器）。

Result: 构建了包含847,392个建筑级观测的BCUR数据集，涵盖六个城市的多灾害标注和交通可达性特征。实验评估了预测质量、跨城市泛化、校准以及下游交通相关结果，包括可达性和灾害条件下的旅行时间。

Conclusion: Skjold-DiT能够生成校准的、不确定性感知的可达性层，为智能车辆路由和应急调度系统提供灾害条件下的路由约束，提高城市交通系统的气候韧性。

Abstract: Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.

</details>


### [39] [Self-Improving World Modelling with Latent Actions](https://arxiv.org/abs/2602.06130)
*Yifu Qiu,Zheng Zhao,Waylon Li,Yftah Ziser,Anna Korhonen,Shay B. Cohen,Edoardo M. Ponti*

Main category: cs.LG

TL;DR: SWIRL是一种自改进框架，通过将动作视为隐变量，从仅状态序列中学习世界模型，无需动作标注轨迹，在多个基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 学习世界模型（预测状态转移）通常需要昂贵的动作标注轨迹。本文旨在从仅状态序列中学习世界模型，减少对动作标注的依赖。

Method: 提出SWIRL框架：将动作视为隐变量，交替训练前向世界模型P_θ(Y|X,Z)和逆动力学模型Q_φ(Z|X,Y)。通过变分信息最大化和ELBO最大化两个阶段迭代优化，使用强化学习（GRPO）训练，以对方模型的log概率作为奖励信号。

Result: 在多个环境（单轮/多轮开放世界视觉动态、合成文本环境）中评估：AURORABench提升16%，ByteMorph提升28%，WorldPredictionBench提升16%，StableToolBench提升14%。

Conclusion: SWIRL能够从仅状态序列中有效学习世界模型，无需动作标注，在LLMs和VLMs上均取得显著性能提升，为世界建模提供了更高效的解决方案。

Abstract: Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_θ(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_φ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.

</details>


### [40] [Tempora: Characterising the Time-Contingent Utility of Online Test-Time Adaptation](https://arxiv.org/abs/2602.06136)
*Sudarshan Sreeram,Young D. Kwon,Cecilia Mascolo*

Main category: cs.LG

TL;DR: Tempora框架首次系统评估测试时适应(TTA)在时间压力下的表现，揭示传统评估忽略的准确率-延迟权衡，发现方法排名在时间约束下会反转。


<details>
  <summary>Details</summary>
Motivation: 现有TTA评估假设无限处理时间，忽略了实际部署中的时间压力。在延迟敏感的应用场景中，预测结果到达太晚就失去了价值，因此需要评估TTA在时间约束下的表现。

Method: 提出Tempora框架，包含：1) 时间场景建模部署约束；2) 评估协议操作化测量；3) 时间相关效用指标量化准确率-延迟权衡。具体实现三种指标：离散效用（异步流硬截止）、连续效用（交互场景延迟衰减）、摊销效用（预算约束部署）。

Result: 在ImageNet-C上对7种TTA方法进行240次时间评估，发现排名不稳定：传统排名无法预测时间压力下的排名；SOTA方法ETA在41.2%的评估中表现不佳；最佳方法随损坏类型和时间压力变化，没有明确赢家。

Conclusion: Tempora首次实现了跨不同时间约束的系统评估，揭示了排名反转的条件和原因，为实践者提供了方法选择视角，为研究者提供了可部署适应的研究目标。

Abstract: Test-time adaptation (TTA) offers a compelling remedy for machine learning (ML) models that degrade under domain shifts, improving generalisation on-the-fly with only unlabelled samples. This flexibility suits real deployments, yet conventional evaluations unrealistically assume unbounded processing time, overlooking the accuracy-latency trade-off. As ML increasingly underpins latency-sensitive and user-facing use-cases, temporal pressure constrains the viability of adaptable inference; predictions arriving too late to act on are futile. We introduce Tempora, a framework for evaluating TTA under this pressure. It consists of temporal scenarios that model deployment constraints, evaluation protocols that operationalise measurement, and time-contingent utility metrics that quantify the accuracy-latency trade-off. We instantiate the framework with three such metrics: (1) discrete utility for asynchronous streams with hard deadlines, (2) continuous utility for interactive settings where value decays with latency, and (3) amortised utility for budget-constrained deployments. Applying Tempora to seven TTA methods on ImageNet-C across 240 temporal evaluations reveals rank instability: conventional rankings do not predict rankings under temporal pressure; ETA, a state-of-the-art method in the conventional setting, falls short in 41.2% of evaluations. The highest-utility method varies with corruption type and temporal pressure, with no clear winner. By enabling systematic evaluation across diverse temporal constraints for the first time, Tempora reveals when and why rankings invert, offering practitioners a lens for method selection and researchers a target for deployable adaptation.

</details>


### [41] [Flow Matching for Offline Reinforcement Learning with Discrete Actions](https://arxiv.org/abs/2602.06138)
*Fairoz Nower Khan,Nabuat Zaman Nahim,Ruiquan Huang,Haibo Yang,Peizhong Ju*

Main category: cs.LG

TL;DR: 将流匹配扩展到离散动作空间的多目标离线强化学习框架，支持多智能体设置，并通过动作量化应用于连续控制问题


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型和流匹配的生成策略在离线强化学习中表现出色，但主要局限于连续动作空间。需要扩展框架以支持更广泛的离线RL设置，包括离散动作空间和多目标场景。

Method: 1. 用连续时间马尔可夫链替代连续流，使用Q加权的流匹配目标进行训练；2. 扩展到多智能体设置，通过因子化条件路径缓解联合动作空间的指数增长；3. 通过动作量化将离散框架应用于连续控制问题。

Result: 理论证明在理想条件下优化该目标可恢复最优策略。实验表明方法在高维控制、多模态决策和多目标动态偏好等实际场景中表现稳健，离散框架通过动作量化在连续控制问题中提供表示复杂性与性能的灵活权衡。

Conclusion: 提出了一个通用的流匹配框架，成功扩展到离散动作空间和多目标离线强化学习，支持多智能体设置，并能通过动作量化应用于连续控制问题，为广泛的RL设置提供了灵活有效的解决方案。

Abstract: Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.

</details>


### [42] [Optimistic Training and Convergence of Q-Learning -- Extended Version](https://arxiv.org/abs/2602.06146)
*Prashant Mehta,Sean Meyn*

Main category: cs.LG

TL;DR: 该论文扩展了Q-learning收敛性分析，指出在非表格或线性MDP设置下，即使基础函数理想，投影贝尔曼方程也可能存在多个解，导致算法不稳定。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在(ε,κ)-驯服Gibbs策略下，线性函数近似的Q-learning是稳定的，但未解决解的唯一性和在标准表格或线性MDP设置之外的收敛标准问题。

Method: 通过一维示例分析，展示在无意识训练策略下投影贝尔曼方程可能无解或多解的情况，并构建基础函数理想但存在多个PBE解的具体例子。

Result: 研究发现即使真实Q函数在基础函数张成的空间中，投影贝尔曼方程在贪婪策略下仍可能存在多个解，这表明收敛需要更多结构条件。

Conclusion: Q-learning的收敛需要比先前认识更多的结构条件，即使基础函数理想也不能保证解的唯一性和算法稳定性。

Abstract: In recent work it is shown that Q-learning with linear function approximation is stable, in the sense of bounded parameter estimates, under the $(\varepsilon,κ)$-tamed Gibbs policy; $κ$ is inverse temperature, and $\varepsilon>0$ is introduced for additional exploration. Under these assumptions it also follows that there is a solution to the projected Bellman equation (PBE). Left open is uniqueness of the solution, and criteria for convergence outside of the standard tabular or linear MDP settings.
  The present work extends these results to other variants of Q-learning, and clarifies prior work: a one dimensional example shows that under an oblivious policy for training there may be no solution to the PBE, or multiple solutions, and in each case the algorithm is not stable under oblivious training.
  The main contribution is that far more structure is required for convergence. An example is presented for which the basis is ideal, in the sense that the true Q-function is in the span of the basis. However, there are two solutions to the PBE under the greedy policy, and hence also for the $(\varepsilon,κ)$-tamed Gibbs policy for all sufficiently small $\varepsilon>0$ and $κ\ge 1$.

</details>


### [43] [MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models](https://arxiv.org/abs/2602.06154)
*Nurbek Tastan,Stefanos Laskaridis,Karthik Nandakumar,Samuel Horvath*

Main category: cs.LG

TL;DR: MoSE提出了一种具有可伸缩结构的MoE架构，每个专家可以按不同宽度执行，实现了更连续的计算精度权衡。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型在专家选择后完全执行，导致精度与计算量之间的权衡存在较大不连续性，无法灵活调整计算成本。

Method: 提出混合可伸缩专家架构，每个专家具有嵌套的可伸缩结构；结合多宽度训练和标准MoE目标；推理时采用运行时宽度确定策略，包括基于路由器置信度的轻量级测试时训练机制。

Result: 在OpenWebText上训练的GPT模型实验中，MoSE在全宽度下匹配或优于标准MoE，并在精度与成本权衡的帕累托前沿上持续改进，以显著更少的FLOPs实现可比性能。

Conclusion: MoSE通过引入可伸缩专家结构，实现了更灵活和连续的计算精度权衡，为大型语言模型的高效推理提供了新方法。

Abstract: Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.

</details>


### [44] [Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering](https://arxiv.org/abs/2602.06155)
*Wei Wei,Yizhou Zeng,Kuntian Chen,Sophie Langer,Mariia Seleznova,Hung-Hsu Chou*

Main category: cs.LG

TL;DR: 扩散模型的初始噪声空间在高置信度样本下展现出类可分性结构


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型的高维初始噪声空间是否包含足够结构来预测生成样本的属性（如类别），探索置信度分数如何揭示潜在结构

Method: 使用预训练分类器为生成样本分配置信度分数，分析不同置信度子集中的噪声种子，比较类可预测性和潜在空间的类可分性

Result: 当考虑所有噪声实现时，潜在空间基本无结构；但限制在产生高置信度样本的初始噪声种子时，显示出明显的类可分性，表明存在仅在置信度过滤下可观察的类相关潜在结构

Conclusion: 置信度过滤揭示了扩散模型潜在空间中的类结构，为条件生成提供了基于引导方法的替代方案

Abstract: Diffusion models rely on a high-dimensional latent space of initial noise seeds, yet it remains unclear whether this space contains sufficient structure to predict properties of the generated samples, such as their classes. In this work, we investigate the emergence of latent structure through the lens of confidence scores assigned by a pre-trained classifier to generated samples. We show that while the latent space appears largely unstructured when considering all noise realizations, restricting attention to initial noise seeds that produce high-confidence samples reveals pronounced class separability. By comparing class predictability across noise subsets of varying confidence and examining the class separability of the latent space, we find evidence of class-relevant latent structure that becomes observable only under confidence-based filtering. As a practical implication, we discuss how confidence-based filtering enables conditional generation as an alternative to guidance-based methods.

</details>


### [45] [SCONE: A Practical, Constraint-Aware Plug-in for Latent Encoding in Learned DNA Storage](https://arxiv.org/abs/2602.06157)
*Cihan Ruan,Lebin Zhou,Rongduo Han,Linyi Han,Bingqing Zhao,Chenchen Zhu,Wei Jiang,Wei Wang,Nam Ling*

Main category: cs.LG

TL;DR: SCONE提出了一种将神经压缩与DNA编码结合的端到端方法，通过四进制算术编码直接在潜在空间执行DNA编码，动态调整熵模型以满足生化约束，无需后处理。


<details>
  <summary>Details</summary>
Motivation: 当前DNA存储与神经压缩流水线集成效率低下。现有方法要么在原始二进制数据上应用冗余约束层，要么将学习到的潜在表示通过简单的二进制到四进制转码转换为DNA，丢弃了熵模型的优化，导致压缩效率低下且编码堆栈复杂。

Method: SCONE采用插件模块设计，将潜在压缩和DNA编码合并为单一步骤。通过四进制算术编码直接在潜在空间的DNA碱基上操作，其约束感知自适应编码模块动态引导熵编码器的学习概率分布，在编码过程中确定性执行生化约束（GC平衡和同聚物抑制），保持完全可逆性并利用超先验模型的先验知识。

Result: 实验表明SCONE实现了近乎完美的约束满足，计算开销可忽略（<2%延迟），为端到端DNA兼容学习编解码器建立了潜在无关接口。

Conclusion: SCONE成功解决了神经压缩与DNA编码之间的不匹配问题，通过统一的端到端框架实现了高效、约束感知的DNA存储编码，为DNA存储的实际应用提供了重要技术支撑。

Abstract: DNA storage has matured from concept to practical stage, yet its integration with neural compression pipelines remains inefficient. Early DNA encoders applied redundancy-heavy constraint layers atop raw binary data - workable but primitive. Recent neural codecs compress data into learned latent representations with rich statistical structure, yet still convert these latents to DNA via naive binary-to-quaternary transcoding, discarding the entropy model's optimization. This mismatch undermines compression efficiency and complicates the encoding stack. A plug-in module that collapses latent compression and DNA encoding into a single step. SCONE performs quaternary arithmetic coding directly on the latent space in DNA bases. Its Constraint-Aware Adaptive Coding module dynamically steers the entropy encoder's learned probability distribution to enforce biochemical constraints - Guanine-Cytosine (GC) balance and homopolymer suppression - deterministically during encoding, eliminating post-hoc correction. The design preserves full reversibility and exploits the hyperprior model's learned priors without modification. Experiments show SCONE achieves near-perfect constraint satisfaction with negligible computational overhead (<2% latency), establishing a latent-agnostic interface for end-to-end DNA-compatible learned codecs.

</details>


### [46] [To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training](https://arxiv.org/abs/2602.06183)
*Meghana Madhyastha,Daniel Haziza,Jesse Cai,Newsha Ardalani,Zhiqi Bu,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 利用硬件加速稀疏性加速Transformer FFN中的矩阵乘法，通过稀疏训练结合密集训练，实现1.4-1.7倍训练加速，保持模型性能不变


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练主要受限于矩阵乘法运算，其中Transformer的FFN部分占据了大量计算（最高达预训练浮点运算的50%），需要加速这部分计算

Method: 采用硬件加速的稀疏性技术：权重使用2:4稀疏模式，激活使用v:n:m（Venom）稀疏模式；训练过程中结合稀疏训练步骤（加速大部分预训练）和常规密集训练步骤（用于后期训练）

Result: 使用该方法训练的模型在质量基准测试中表现相同，端到端训练速度提升1.4-1.7倍；适用于A100及之后的NVIDIA GPU，与量化等技术正交，也可应用于专家混合架构

Conclusion: 通过硬件加速稀疏性技术有效加速了Transformer FFN的矩阵乘法，显著提升训练效率而不损失模型质量，为大规模语言模型训练提供了实用的加速方案

Abstract: Trainings of Large Language Models are generally bottlenecked by matrix multiplications. In the Transformer architecture, a large portion of these operations happens in the Feed Forward Network (FFN), and this portion increases for larger models, up to 50% of the total pretraining floating point operations. We show that we can leverage hardware-accelerated sparsity to accelerate all matrix multiplications in the FFN, with 2:4 sparsity for weights and v:n:m (Venom) sparsity for activations. Our recipe relies on sparse training steps to accelerate a large part of the pretraining, associated with regular dense training steps towards the end. Overall, models trained with this approach exhibit the same performance on our quality benchmarks, and can speed up training end-to-end by 1.4 to 1.7x. This approach is applicable to all NVIDIA GPUs starting with the A100 generation, and is orthogonal to common optimization techniques, such as, quantization, and can also be applied to mixture-of-experts model architectures.

</details>


### [47] [$f$-FUM: Federated Unlearning via min--max and $f$-divergence](https://arxiv.org/abs/2602.06187)
*Radmehr Karimian,Amirhossein Bagheri,Meghdad Kurmanji,Nicholas D. Lane,Gholamali Aminian*

Main category: cs.LG

TL;DR: 提出一种联邦学习遗忘框架，通过最小化优化问题实现高效数据删除，可作为插件集成到现有联邦学习系统中


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要应对"被遗忘权"等法律要求和数据投毒攻击，但分布式特性使得删除特定数据贡献变得复杂

Method: 将联邦遗忘问题构建为最小化优化问题，目标是最大化包含所有数据训练的模型与删除特定数据后重新训练的模型之间的f-散度，同时最小化对保留数据性能的影响

Result: 该方法相比朴素重新训练显著加速，对模型效用影响最小，可作为插件集成到各种联邦学习设置中

Conclusion: 提出的联邦遗忘框架为联邦学习中的隐私保护和数据管理提供了高效实用的解决方案

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for collaborative machine learning across decentralized data sources, preserving privacy by keeping data local. However, increasing legal and ethical demands, such as the "right to be forgotten", and the need to mitigate data poisoning attacks have underscored the urgent necessity for principled data unlearning in FL. Unlike centralized settings, the distributed nature of FL complicates the removal of individual data contributions. In this paper, we propose a novel federated unlearning framework formulated as a min-max optimization problem, where the objective is to maximize an $f$-divergence between the model trained with all data and the model retrained without specific data points, while minimizing the degradation on retained data. Our framework could act like a plugin and be added to almost any federated setup, unlike SOTA methods like (\cite{10269017} which requires model degradation in server, or \cite{khalil2025notfederatedunlearningweight} which requires to involve model architecture and model weights). This formulation allows for efficient approximation of data removal effects in a federated setting. We provide empirical evaluations to show that our method achieves significant speedups over naive retraining, with minimal impact on utility.

</details>


### [48] [Degradation of Feature Space in Continual Learning](https://arxiv.org/abs/2602.06586)
*Chiara Lanza,Roberto Pereira,Marco Miozzo,Eduard Angelats,Paolo Dini*

Main category: cs.LG

TL;DR: 论文研究发现，在持续学习中强制特征空间各向同性反而会降低模型性能，这与集中式训练中的情况相反。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在持续学习（非平稳数据流）中是否应该像集中式训练那样强制特征空间各向同性，以平衡稳定性和可塑性，从而缓解灾难性遗忘问题。

Method: 使用对比持续学习技术，在CIFAR-10和CIFAR-100数据集上进行实验，通过各向同性正则化来促进特征空间各向同性。

Result: 实验结果表明，各向同性正则化不仅未能改善持续学习性能，反而会降低模型准确率，这与集中式训练中的效果相反。

Conclusion: 集中式训练和持续学习在特征几何结构上存在本质差异，各向同性虽然在集中式训练中有益，但不适合作为非平稳学习场景的归纳偏置。

Abstract: Centralized training is the standard paradigm in deep learning, enabling models to learn from a unified dataset in a single location. In such setup, isotropic feature distributions naturally arise as a mean to support well-structured and generalizable representations. In contrast, continual learning operates on streaming and non-stationary data, and trains models incrementally, inherently facing the well-known plasticity-stability dilemma. In such settings, learning dynamics tends to yield increasingly anisotropic feature space. This arises a fundamental question: should isotropy be enforced to achieve a better balance between stability and plasticity, and thereby mitigate catastrophic forgetting? In this paper, we investigate whether promoting feature-space isotropy can enhance representation quality in continual learning. Through experiments using contrastive continual learning techniques on CIFAR-10 and CIFAR-100 data, we find that isotropic regularization fails to improve, and can in fact degrade, model accuracy in continual settings. Our results highlight essential differences in feature geometry between centralized and continual learning, suggesting that isotropy, while beneficial in centralized setups, may not constitute an appropriate inductive bias for non-stationary learning scenarios.

</details>


### [49] [Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning](https://arxiv.org/abs/2602.06204)
*Nan Chen,Soledad Villar,Soufiane Hayou*

Main category: cs.LG

TL;DR: μA框架揭示了LoRA最优学习率随秩变化的规律，分为两种机制：学习率基本不变或与秩成反比，并可将LoRA调优的学习率迁移到全微调。


<details>
  <summary>Details</summary>
Motivation: LoRA作为参数高效微调的标准工具，其训练动态复杂且依赖多个超参数。特别是最优学习率如何随适配器秩变化尚不明确，导致每次改变秩都需要重新调整学习率，增加了调优成本。

Method: 提出Maximal-Update Adaptation (μA)理论框架，基于Maximal-Update Parametrization (μP)思想，利用超参数迁移技术分析学习率随模型宽度和适配器秩的缩放规律。

Result: 识别出两种机制：一种学习率基本不随秩变化，另一种学习率与秩成反比。找到了可将LoRA调优学习率迁移到全微调的配置，大幅降低全微调的学习率调优成本。

Conclusion: μA框架为LoRA训练提供了理论指导，揭示了学习率与秩的缩放关系，实现了学习率从LoRA到全微调的可靠迁移，在多个领域任务中得到验证。

Abstract: Low-Rank Adaptation (LoRA) is a standard tool for parameter-efficient finetuning of large models. While it induces a small memory footprint, its training dynamics can be surprisingly complex as they depend on several hyperparameters such as initialization, adapter rank, and learning rate. In particular, it is unclear how the optimal learning rate scales with adapter rank, which forces practitioners to re-tune the learning rate whenever the rank is changed. In this paper, we introduce Maximal-Update Adaptation ($μ$A), a theoretical framework that characterizes how the "optimal" learning rate should scale with model width and adapter rank to produce stable, non-vanishing feature updates under standard configurations. $μ$A is inspired from the Maximal-Update Parametrization ($μ$P) in pretraining. Our analysis leverages techniques from hyperparameter transfer and reveals that the optimal learning rate exhibits different scaling patterns depending on initialization and LoRA scaling factor. Specifically, we identify two regimes: one where the optimal learning rate remains roughly invariant across ranks, and another where it scales inversely with rank. We further identify a configuration that allows learning rate transfer from LoRA to full finetuning, drastically reducing the cost of learning rate tuning for full finetuning. Experiments across language, vision, vision--language, image generation, and reinforcement learning tasks validate our scaling rules and show that learning rates tuned on LoRA transfer reliably to full finetuning.

</details>


### [50] [Multi-Way Representation Alignment](https://arxiv.org/abs/2602.06205)
*Akshit Achara,Tatiana Gaintseva,Mateo Mahaut,Pritish Chakraborty,Viktor Stenby Johansson,Melih Barsbey,Emanuele Rodolà,Donato Crisostomi*

Main category: cs.LG

TL;DR: 提出GCPA方法，通过几何校正的普氏对齐解决多模型对齐问题，在保持共享参考空间的同时提升任意模型间的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能成对对齐模型，无法建立全局一致的参考空间，且普氏对齐（保持几何结构）与CCA（最大化相关性）在不同任务中各有优劣，需要统一解决方案。

Method: 首先将广义普氏分析（GPA）扩展到多模型对齐，构建共享正交空间以保持内部几何结构；然后提出几何校正普氏对齐（GCPA），在GPA基础上进行后处理校正方向不匹配问题。

Result: GCPA在保持实用共享参考空间的同时，显著提升了任意模型间的检索性能，在各种实验中表现一致优于现有方法。

Conclusion: GCPA成功解决了多模型对齐中的几何保持与检索性能权衡问题，为构建全局一致的模型表示空间提供了有效方案。

Abstract: The Platonic Representation Hypothesis suggests that independently trained neural networks converge to increasingly similar latent spaces. However, current strategies for mapping these representations are inherently pairwise, scaling quadratically with the number of models and failing to yield a consistent global reference. In this paper, we study the alignment of $M \ge 3$ models. We first adapt Generalized Procrustes Analysis (GPA) to construct a shared orthogonal universe that preserves the internal geometry essential for tasks like model stitching. We then show that strict isometric alignment is suboptimal for retrieval, where agreement-maximizing methods like Canonical Correlation Analysis (CCA) typically prevail. To bridge this gap, we finally propose Geometry-Corrected Procrustes Alignment (GCPA), which establishes a robust GPA-based universe followed by a post-hoc correction for directional mismatch. Extensive experiments demonstrate that GCPA consistently improves any-to-any retrieval while retaining a practical shared reference space.

</details>


### [51] [Emergent Low-Rank Training Dynamics in MLPs with Smooth Activations](https://arxiv.org/abs/2602.06208)
*Alec S. Xu,Can Yaras,Matthew Asato,Qing Qu,Laura Balzano*

Main category: cs.LG

TL;DR: 论文分析表明，多层感知机在梯度下降训练过程中，权重动态会集中在低维不变子空间中，这为低秩训练提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 虽然经验证据表明大规模深度神经网络的训练动态发生在低维子空间中，但非线性网络中这种现象的理论解释仍然有限。本文旨在填补这一空白，为多层感知机的低维训练动态提供理论支持。

Method: 分析多层感知机在梯度下降下的学习动态，理论上精确描述具有平滑非线性激活函数的两层网络的不变子空间，并通过实验验证这一现象在更广泛条件下的存在。

Result: 研究发现权重动态在整个训练过程中集中在不变的低维子空间中，实验验证了这一现象超出了理论假设的范围，并且存在一种低秩MLP参数化方法，在适当子空间初始化时，能在多种分类任务上达到与全参数化模型相当的性能。

Conclusion: 该研究为非线性神经网络中的低维训练动态提供了理论依据，证明了低秩参数化在保持性能的同时减少参数数量的可行性，为高效的神经网络训练和压缩提供了新思路。

Abstract: Recent empirical evidence has demonstrated that the training dynamics of large-scale deep neural networks occur within low-dimensional subspaces. While this has inspired new research into low-rank training, compression, and adaptation, theoretical justification for these dynamics in nonlinear networks remains limited. %compared to deep linear settings. To address this gap, this paper analyzes the learning dynamics of multi-layer perceptrons (MLPs) under gradient descent (GD). We demonstrate that the weight dynamics concentrate within invariant low-dimensional subspaces throughout training. Theoretically, we precisely characterize these invariant subspaces for two-layer networks with smooth nonlinear activations, providing insight into their emergence. Experimentally, we validate that this phenomenon extends beyond our theoretical assumptions. Leveraging these insights, we empirically show there exists a low-rank MLP parameterization that, when initialized within the appropriate subspaces, matches the classification performance of fully-parameterized counterparts on a variety of classification tasks.

</details>


### [52] [SR4-Fit: An Interpretable and Informative Classification Algorithm Applied to Prediction of U.S. House of Representatives Elections](https://arxiv.org/abs/2602.06229)
*Shyam Sundar Murali Krishnan,Dean Frederick Hougen*

Main category: cs.LG

TL;DR: SR4-Fit是一种新型可解释分类算法，解决了传统规则算法预测能力不足和不稳定的问题，在保持优异分类性能的同时提供可解释性，在选举预测和多个公开数据集上表现优于黑盒模型和现有规则算法。


<details>
  <summary>Details</summary>
Motivation: 机器学习在关键应用中需要可解释模型，但高性能模型通常是"黑盒"系统，而传统规则算法如RuleFit虽然简单但预测能力不足且不稳定。这促使开发SR4-Fit来解决这些限制。

Method: SR4-Fit（稀疏松弛正则化回归规则拟合）是一种新颖的可解释分类算法，通过稀疏松弛正则化技术生成稳定且可解释的规则集，同时保持优异的预测性能。

Result: 使用美国国会选区人口特征数据，SR4-Fit能够以前所未有的准确性和可解释性预测众议院选举结果。算法揭示了影响预测结果的人口因素组合，这些在黑盒算法中无法解释。在六个公开数据集（乳腺癌、大肠杆菌、页面块、皮马印第安人、车辆、酵母）上也获得类似结果。

Conclusion: SR4-Fit在准确性、简单性和鲁棒性方面均优于黑盒模型和现有规则算法，解决了模型可解释性与预测能力之间的传统权衡，为关键应用提供了高性能且可解释的解决方案。

Abstract: The growth of machine learning demands interpretable models for critical applications, yet most high-performing models are ``black-box'' systems that obscure input-output relationships, while traditional rule-based algorithms like RuleFit suffer from a lack of predictive power and instability despite their simplicity. This motivated our development of Sparse Relaxed Regularized Regression Rule-Fit (SR4-Fit), a novel interpretable classification algorithm that addresses these limitations while maintaining superior classification performance. Using demographic characteristics of U.S. congressional districts from the Census Bureau's American Community Survey, we demonstrate that SR4-Fit can predict House election party outcomes with unprecedented accuracy and interpretability. Our results show that while the majority party remains the strongest predictor, SR4-Fit has revealed intrinsic combinations of demographic factors that affect prediction outcomes that were unable to be interpreted in black-box algorithms such as random forests. The SR4-Fit algorithm surpasses both black-box models and existing interpretable rule-based algorithms such as RuleFit with respect to accuracy, simplicity, and robustness, generating stable and interpretable rule sets while maintaining superior predictive performance, thus addressing the traditional trade-off between model interpretability and predictive capability in electoral forecasting. To further validate SR4-Fit's performance, we also apply it to six additional publicly available classification datasets, like the breast cancer, Ecoli, page blocks, Pima Indians, vehicle, and yeast datasets, and find similar results.

</details>


### [53] [RuleSmith: Multi-Agent LLMs for Automated Game Balancing](https://arxiv.org/abs/2602.06232)
*Ziyao Zeng,Chen Liu,Tianyu Liu,Hao Wang,Xiatao Sun,Fengyu Yang,Xiaofeng Liu,Zhiwen Fan*

Main category: cs.LG

TL;DR: RuleSmith：首个利用多智能体LLM推理能力实现自动化游戏平衡的框架，通过游戏引擎、多智能体LLM自博弈和贝叶斯优化在多维规则空间中进行搜索。


<details>
  <summary>Details</summary>
Motivation: 游戏平衡是一个长期存在的挑战，需要重复的玩法测试、专家直觉和大量手动调整。传统方法耗时且依赖专家经验，需要自动化解决方案来加速游戏平衡过程。

Method: 结合游戏引擎、多智能体LLM自博弈和贝叶斯优化。LLM智能体解释文本规则书和游戏状态生成动作，评估胜率差异等平衡指标。使用基于获取函数的自适应采样和离散投影的贝叶斯优化高效搜索参数空间。

Result: 在CivMini（简化文明风格游戏）上的实验表明，RuleSmith能够收敛到高度平衡的配置，并提供可直接应用于下游游戏系统的可解释规则调整。

Conclusion: LLM模拟可以作为自动化复杂多智能体环境中游戏设计和平衡的强大替代方案，为游戏开发提供高效、可解释的平衡解决方案。

Abstract: Game balancing is a longstanding challenge requiring repeated playtesting, expert intuition, and extensive manual tuning. We introduce RuleSmith, the first framework that achieves automated game balancing by leveraging the reasoning capabilities of multi-agent LLMs. It couples a game engine, multi-agent LLMs self-play, and Bayesian optimization operating over a multi-dimensional rule space. As a proof of concept, we instantiate RuleSmith on CivMini, a simplified civilization-style game containing heterogeneous factions, economy systems, production rules, and combat mechanics, all governed by tunable parameters. LLM agents interpret textual rulebooks and game states to generate actions, to conduct fast evaluation of balance metrics such as win-rate disparities. To search the parameter landscape efficiently, we integrate Bayesian optimization with acquisition-based adaptive sampling and discrete projection: promising candidates receive more evaluation games for accurate assessment, while exploratory candidates receive fewer games for efficient exploration. Experiments show that RuleSmith converges to highly balanced configurations and provides interpretable rule adjustments that can be directly applied to downstream game systems. Our results illustrate that LLM simulation can serve as a powerful surrogate for automating design and balancing in complex multi-agent environments.

</details>


### [54] [Provably avoiding over-optimization in Direct Preference Optimization without knowing the data distribution](https://arxiv.org/abs/2602.06239)
*Adam Barla,Emanuele Nevali,Luca Viano,Volkan Cevher*

Main category: cs.LG

TL;DR: PEPO是一种单步DPO式算法，通过集成学习实现悲观偏好优化，避免过优化问题，无需数据分布知识或显式奖励模型。


<details>
  <summary>Details</summary>
Motivation: 解决偏好学习中已知的过优化问题，避免需要数据生成分布知识或学习显式奖励模型的复杂性。

Method: 使用集成方法：在不相交数据子集上训练多个偏好优化策略，然后通过最坏情况构造进行聚合，强调模型间的一致性。

Result: 在表格设置中，PEPO的样本复杂度仅依赖于单策略集中系数，避免了影响DPO等易过优化算法的全策略集中系数。实验验证了其实际性能优势。

Conclusion: PEPO在保持DPO式训练简单性和实用性的同时，通过集成悲观优化有效缓解了过优化问题，具有更好的理论保证和实际性能。

Abstract: We introduce PEPO (Pessimistic Ensemble based Preference Optimization), a single-step Direct Preference Optimization (DPO)-like algorithm to mitigate the well-known over-optimization issue in preference learning without requiring the knowledge of the data-generating distribution or learning an explicit reward model. PEPO achieves pessimism via an ensemble of preference-optimized policies trained on disjoint data subsets and then aggregates them through a worst case construction that favors the agreement across models. In the tabular setting, PEPO achieves sample complexity guarantees depending only on a single-policy concentrability coefficient, thus avoiding the all-policy concentrability which affects the guarantees of algorithms prone to over-optimization, such as DPO. The theoretical findings are corroborated by a convincing practical performance, while retaining the simplicity and the practicality of DPO-style training.

</details>


### [55] [ATEX-CF: Attack-Informed Counterfactual Explanations for Graph Neural Networks](https://arxiv.org/abs/2602.06240)
*Yu Zhang,Sean Bin Yang,Arijit Khan,Cuneyt Gurcan Akcora*

Main category: cs.LG

TL;DR: ATEX-CF：将对抗攻击技术与反事实解释生成统一的新框架，通过结合边添加和删除，为图神经网络提供忠实、简洁且合理的解释。


<details>
  <summary>Details</summary>
Motivation: 反事实解释通过识别改变模型预测的最小变化来解释图神经网络，而对抗攻击和反事实解释虽然共享翻转节点预测的目标，但扰动策略不同（对抗攻击通常添加边，反事实方法通常删除边）。传统方法将解释和攻击分开处理，需要一种统一框架来整合这两种扰动策略。

Method: 提出ATEX-CF框架，基于理论将边添加和删除统一整合，利用对抗攻击的洞察来探索有影响力的反事实。在受限扰动预算下联合优化忠实度、稀疏性和合理性，生成实例级解释。

Result: 在合成和真实世界节点分类基准测试中，ATEX-CF生成了忠实、简洁且合理的解释，证明了将对抗攻击洞察整合到图神经网络反事实推理中的有效性。

Conclusion: 通过统一对抗攻击技术和反事实解释生成，ATEX-CF框架能够为图神经网络提供信息丰富且现实的反事实解释，展示了整合这两种方法的优势。

Abstract: Counterfactual explanations offer an intuitive way to interpret graph neural networks (GNNs) by identifying minimal changes that alter a model's prediction, thereby answering "what must differ for a different outcome?". In this work, we propose a novel framework, ATEX-CF that unifies adversarial attack techniques with counterfactual explanation generation-a connection made feasible by their shared goal of flipping a node's prediction, yet differing in perturbation strategy: adversarial attacks often rely on edge additions, while counterfactual methods typically use deletions. Unlike traditional approaches that treat explanation and attack separately, our method efficiently integrates both edge additions and deletions, grounded in theory, leveraging adversarial insights to explore impactful counterfactuals. In addition, by jointly optimizing fidelity, sparsity, and plausibility under a constrained perturbation budget, our method produces instance-level explanations that are both informative and realistic. Experiments on synthetic and real-world node classification benchmarks demonstrate that ATEX-CF generates faithful, concise, and plausible explanations, highlighting the effectiveness of integrating adversarial insights into counterfactual reasoning for GNNs.

</details>


### [56] [A Fast and Generalizable Fourier Neural Operator-Based Surrogate for Melt-Pool Prediction in Laser Processing](https://arxiv.org/abs/2602.06241)
*Alix Benoit,Toni Ivas,Mateusz Papierz,Asel Sagingalieva,Alexey Melnikov,Elia Iseli*

Main category: cs.LG

TL;DR: LP-FNO：基于傅里叶神经算子的激光焊接代理模型，通过移动坐标系和时均化处理，实现从工艺参数到三维温度场和熔池边界的快速预测，速度比传统多物理场软件快10万倍。


<details>
  <summary>Details</summary>
Motivation: 传统激光焊接高保真模拟计算成本高，限制了大规模工艺探索和实时应用，需要开发高效的代理模型来替代昂贵的多物理场仿真。

Method: 提出LP-FNO模型，将瞬态问题重构到移动激光坐标系中并应用时均化处理，转化为准稳态问题；使用归一化焓公式，在传导和匙孔焊接范围内学习工艺参数到三维温度场和熔池边界的映射关系。

Result: 模型温度预测误差约1%，熔池分割的IoU分数超过0.9；在粗网格数据上训练的模型可在细网格上评估，实现超分辨率预测；预测速度比传统有限体积多物理场软件快10万倍（仅需数十毫秒）。

Conclusion: LP-FNO为激光焊接提供了高效的代理建模框架，能够在宽参数范围内快速预测三维场和相界面，为工艺优化和实时控制提供了有力工具。

Abstract: High-fidelity simulations of laser welding capture complex thermo-fluid phenomena, including phase change, free-surface deformation, and keyhole dynamics, however their computational cost limits large-scale process exploration and real-time use. In this work we present the Laser Processing Fourier Neural Operator (LP-FNO), a Fourier Neural Operator (FNO) based surrogate model that learns the parametric solution operator of various laser processes from multiphysics simulations generated with FLOW-3D WELD (registered trademark). Through a novel approach of reformulating the transient problem in the moving laser frame and applying temporal averaging, the system results in a quasi-steady state setting suitable for operator learning, even in the keyhole welding regime. The proposed LP-FNO maps process parameters to three-dimensional temperature fields and melt-pool boundaries across a broad process window spanning conduction and keyhole regimes using the non-dimensional normalized enthalpy formulation. The model achieves temperature prediction errors on the order of 1% and intersection-over-union scores for melt-pool segmentation over 0.9. We demonstrate that a LP-FNO model trained on coarse-resolution data can be evaluated on finer grids, yielding accurate super-resolved predictions in mesh-converged conduction regimes, whereas discrepancies in keyhole regimes reflect unresolved dynamics in the coarse-mesh training data. These results indicate that the LP-FNO provides an efficient surrogate modeling framework for laser welding, enabling prediction of full three-dimensional fields and phase interfaces over wide parameter ranges in just tens of milliseconds, up to a hundred thousand times faster than traditional Finite Volume multi-physics software.

</details>


### [57] [Adaptive Sparse Möbius Transforms for Learning Polynomials](https://arxiv.org/abs/2602.06246)
*Yigit Efe Erginbas,Justin Singh Kang,Elizabeth Polito,Kannan Ramchandran*

Main category: cs.LG

TL;DR: 提出两种自适应查询算法（FASMT和PASMT），用于高效学习稀疏布尔多项式（Möbius变换），在查询复杂度和自适应轮数间取得平衡，应用于超图重建任务。


<details>
  <summary>Details</summary>
Motivation: 学习稀疏实值布尔多项式（Möbius变换）是重要问题，但AND基的相干性阻碍了标准压缩感知方法的应用。需要克服这一挑战，实现高效的查询算法。

Method: 利用自适应分组测试技术，提出两种算法：FASMT（完全自适应）使用O(sd log(n/d))查询，PASMT（部分自适应）使用O(sd²log(n/d))查询但减少自适应轮数。两种算法都基于Möbius变换的稀疏性。

Result: FASMT的查询复杂度接近最优，PASMT在自适应轮数和查询复杂度间取得平衡。在超图重建任务中，避免了秩d的组合爆炸，相比基线方法有显著改进。

Conclusion: 自适应分组测试为稀疏Möbius变换提供了高效解决方案，在理论和应用层面都有重要意义，特别是在超图重建等实际问题中展现出实用价值。

Abstract: We consider the problem of exactly learning an $s$-sparse real-valued Boolean polynomial of degree $d$ of the form $f:\{ 0,1\}^n \rightarrow \mathbb{R}$. This problem corresponds to decomposing functions in the AND basis and is known as taking a Möbius transform. While the analogous problem for the parity basis (Fourier transform) $f: \{-1,1 \}^n \rightarrow \mathbb{R}$ is well-understood, the AND basis presents a unique challenge: the basis vectors are coherent, precluding standard compressed sensing methods. We overcome this challenge by identifying that we can exploit adaptive group testing to provide a constructive, query-efficient implementation of the Möbius transform (also known as Möbius inversion) for sparse functions. We present two algorithms based on this insight. The Fully-Adaptive Sparse Möbius Transform (FASMT) uses $O(sd \log(n/d))$ adaptive queries in $O((sd + n) sd \log(n/d))$ time, which we show is near-optimal in query complexity. Furthermore, we also present the Partially-Adaptive Sparse Möbius Transform (PASMT), which uses $O(sd^2\log(n/d))$ queries, trading a factor of $d$ to reduce the number of adaptive rounds to $O(d^2\log(n/d))$, with no dependence on $s$. When applied to hypergraph reconstruction from edge-count queries, our results improve upon baselines by avoiding the combinatorial explosion in the rank $d$. We demonstrate the practical utility of our method for hypergraph reconstruction by applying it to learning real hypergraphs in simulations.

</details>


### [58] [REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop](https://arxiv.org/abs/2602.06248)
*Patryk Rybak,Paweł Batorski,Paul Swoboda,Przemysław Spurek*

Main category: cs.LG

TL;DR: REBEL是一种进化对抗提示生成方法，用于测试LLM遗忘方法是否真正移除了敏感数据，揭示当前遗忘方法可能只提供表面保护


<details>
  <summary>Details</summary>
Motivation: 当前LLM遗忘方法的评估存在缺陷，标准评估指标依赖良性查询，往往将表面信息抑制误认为真正的知识移除，无法检测更复杂提示策略仍能提取的残留知识

Method: 提出REBEL进化对抗提示生成方法，通过进化算法生成能够探测未学习数据是否仍可恢复的对抗性提示，在TOFU和WMDP基准测试子集上验证

Result: REBEL成功从标准遗忘基准中看似已遗忘的模型中提取"被遗忘"知识，攻击成功率在TOFU上达60%，在WMDP上达93%，显著优于静态基线方法

Conclusion: 当前LLM遗忘方法可能只提供表面保护，REBEL方法能更有效地评估遗忘效果，揭示残留知识风险

Abstract: Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge removal. Such metrics fail to detect residual knowledge that more sophisticated prompting strategies could still extract. We introduce REBEL, an evolutionary approach for adversarial prompt generation designed to probe whether unlearned data can still be recovered. Our experiments demonstrate that REBEL successfully elicits ``forgotten'' knowledge from models that seemed to be forgotten in standard unlearning benchmarks, revealing that current unlearning methods may provide only a superficial layer of protection. We validate our framework on subsets of the TOFU and WMDP benchmarks, evaluating performance across a diverse suite of unlearning algorithms. Our experiments show that REBEL consistently outperforms static baselines, recovering ``forgotten'' knowledge with Attack Success Rates (ASRs) reaching up to 60% on TOFU and 93% on WMDP. We will make all code publicly available upon acceptance. Code is available at https://github.com/patryk-rybak/REBEL/

</details>


### [59] [Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions](https://arxiv.org/abs/2602.06256)
*Navita Goyal,Hal Daumé*

Main category: cs.LG

TL;DR: 论文提出模型引导特异性评估框架，发现现有引导方法在保持一般能力和相关控制特性方面表现良好，但在分布偏移下的鲁棒性特异性方面存在严重缺陷，可能损害模型安全性。


<details>
  <summary>Details</summary>
Motivation: 当前模型引导研究主要关注引导效果，但缺乏对引导是否仅改变目标属性而不影响其他相关行为的系统性评估，特别是在安全关键应用中需要确保引导不会无意中损害模型安全性。

Method: 提出特异性评估框架，包含三个维度：一般特异性（保持流畅性和无关能力）、控制特异性（保持相关控制属性）、鲁棒性特异性（在分布偏移下保持控制属性）。在两个安全关键用例（减少过度拒绝和忠实性幻觉）中评估现有引导方法。

Result: 引导方法在实现高效能和保持一般及控制特异性方面表现良好，但在鲁棒性特异性方面一致失败。例如，减少过度拒绝的引导虽不损害一般能力和对有害查询的拒绝，但显著增加了对越狱攻击的脆弱性。

Conclusion: 标准效能和特异性评估不足，缺乏鲁棒性评估可能导致引导方法看似可靠却实际损害模型安全性。这是首个系统评估模型引导特异性的工作，强调了鲁棒性评估在安全关键应用中的重要性。

Abstract: Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to finetuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of whether interventions alter only the intended property remain limited, especially with respect to unintended changes in behaviors related to the target property. We call this notion specificity. We propose a framework that distinguishes three dimensions of specificity: general (preserving fluency and unrelated abilities), control (preserving related control properties), and robustness (preserving control properties under distribution shifts). We study two safety-critical use cases: steering models to reduce overrefusal and faithfulness hallucinations, and show that while steering achieves high efficacy and largely maintains general and control specificity, it consistently fails to preserve robustness specificity. In the case of overrefusal steering, for example, all steering methods reduce overrefusal without harming general abilities and refusal on harmful queries; however, they substantially increase vulnerability to jailbreaks. Our work provides the first systematic evaluation of specificity in model steering, showing that standard efficacy and specificity checks are insufficient, because without robustness evaluation, steering methods may appear reliable even when they compromise model safety.

</details>


### [60] [On Randomized Algorithms in Online Strategic Classification](https://arxiv.org/abs/2602.06257)
*Chase Hutton,Adam Melrod,Han Shao*

Main category: cs.LG

TL;DR: 论文研究了在线战略分类问题，提出了针对随机化算法的新下界和上界，改进了实现在线战略分类的遗憾保证


<details>
  <summary>Details</summary>
Motivation: 在线战略分类中，智能体为了获得有利预测会策略性地修改特征。虽然随机化算法在战略环境中可能为学习者提供优势，但现有研究对此探索不足，特别是在下界分析和遗憾保证方面存在明显差距

Method: 在可实现设置中，扩展了确定性学习者的下界到所有学习者，并提出了第一个改进上界的随机化学习者。在不可知设置中，使用凸优化技术构建了适当的学习器

Result: 在可实现设置中，获得了适用于随机化学习者的第一个下界，并改进了上界。在不可知设置中，将遗憾上界改进到O(√(T log|H|) + |H| log(T|H|))，并证明了在适当学习规则中的匹配下界

Conclusion: 论文为在线战略分类提供了更精细的界限，填补了随机化算法分析的空白，证明了在不可知设置中，要进一步改进遗憾保证需要不适当的学习

Abstract: Online strategic classification studies settings in which agents strategically modify their features to obtain favorable predictions. For example, given a classifier that determines loan approval based on credit scores, applicants may open or close credit cards and bank accounts to obtain a positive prediction. The learning goal is to achieve low mistake or regret bounds despite such strategic behavior.
  While randomized algorithms have the potential to offer advantages to the learner in strategic settings, they have been largely underexplored. In the realizable setting, no lower bound is known for randomized algorithms, and existing lower bound constructions for deterministic learners can be circumvented by randomization. In the agnostic setting, the best known regret upper bound is $O(T^{3/4}\log^{1/4}T|\mathcal H|)$, which is far from the standard online learning rate of $O(\sqrt{T\log|\mathcal H|})$.
  In this work, we provide refined bounds for online strategic classification in both settings. In the realizable setting, we extend, for $T > \mathrm{Ldim}(\mathcal{H}) Δ^2$, the existing lower bound $Ω(\mathrm{Ldim}(\mathcal{H}) Δ)$ for deterministic learners to all learners. This yields the first lower bound that applies to randomized learners. We also provide the first randomized learner that improves the known (deterministic) upper bound of $O(\mathrm{Ldim}(\mathcal H) \cdot Δ\log Δ)$.
  In the agnostic setting, we give a proper learner using convex optimization techniques to improve the regret upper bound to $O(\sqrt{T \log |\mathcal{H}|} + |\mathcal{H}| \log(T|\mathcal{H}|))$. We show a matching lower bound up to logarithmic factors for all proper learning rules, demonstrating the optimality of our learner among proper learners. As such, improper learning is necessary to further improve regret guarantees.

</details>


### [61] [GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt](https://arxiv.org/abs/2602.06258)
*Mark Russinovich,Yanan Cai,Keegan Hines,Giorgio Severi,Blake Bullwinkel,Ahmed Salem*

Main category: cs.LG

TL;DR: GRP-Obliteration是一种使用Group Relative Policy Optimization直接移除目标模型安全约束的方法，仅需单个无标签提示即可可靠地解除安全对齐模型，同时保持模型实用性，并能推广到扩散图像生成系统。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法存在脆弱性，模型在部署后微调容易被解除对齐，但现有方法需要大量数据准备且会降低模型实用性，需要更高效实用的解除对齐方法。

Method: 提出GRP-Obliteration方法，使用Group Relative Policy Optimization直接移除目标模型的安全约束，仅需单个无标签提示即可实现。

Result: 在15个7-200亿参数模型上评估，涵盖GPT-OSS、DeepSeek、Gemma、Llama、Ministral、Qwen等模型家族，GRP-Oblit在6个实用性基准和5个安全性基准上表现优于现有SOTA技术，平均解除对齐效果更强。

Conclusion: GRP-Obliteration扩展了解除对齐的实际界限，仅需单个提示即可可靠解除安全对齐，同时保持模型实用性，并能推广到语言模型和扩散图像生成系统。

Abstract: Safety alignment is only as robust as its weakest failure mode. Despite extensive work on safety post-training, it has been shown that models can be readily unaligned through post-deployment fine-tuning. However, these methods often require extensive data curation and degrade model utility.
  In this work, we extend the practical limits of unalignment by introducing GRP-Obliteration (GRP-Oblit), a method that uses Group Relative Policy Optimization (GRPO) to directly remove safety constraints from target models. We show that a single unlabeled prompt is sufficient to reliably unalign safety-aligned models while largely preserving their utility, and that GRP-Oblit achieves stronger unalignment on average than existing state-of-the-art techniques. Moreover, GRP-Oblit generalizes beyond language models and can also unalign diffusion-based image generation systems.
  We evaluate GRP-Oblit on six utility benchmarks and five safety benchmarks across fifteen 7-20B parameter models, spanning instruct and reasoning models, as well as dense and MoE architectures. The evaluated model families include GPT-OSS, distilled DeepSeek, Gemma, Llama, Ministral, and Qwen.

</details>


### [62] [Swap Regret Minimization Through Response-Based Approachability](https://arxiv.org/abs/2602.06264)
*Ioannis Anagnostides,Gabriele Farina,Maxwell Fishelson,Haipeng Luo,Jon Schneider*

Main category: cs.LG

TL;DR: 提出更简单高效算法，将线性交换遗憾从Ω(d⁴√T)降至O(d³/²√T)，对中心对称集达O(d√T)，并证明匹配下界Ω(d√T)


<details>
  <summary>Details</summary>
Motivation: 现有最小化线性交换遗憾算法复杂度高（Ω(d⁴√T)），依赖计算密集的椭球算法，需要更简单高效的替代方案

Method: 结合Bernstein-Shimkin响应式可接近性框架与John椭球几何预处理，同时最小化轮廓交换遗憾

Result: 新算法将遗憾降至O(d³/²√T)，中心对称集为O(d√T)，证明匹配下界Ω(d√T)，扩展至多项式维度交换偏差

Conclusion: 新算法显著简化计算复杂度，达到信息理论最优，统一并加强了均衡计算与在线学习领域结果

Abstract: We consider the problem of minimizing different notions of swap regret in online optimization. These forms of regret are tightly connected to correlated equilibrium concepts in games, and have been more recently shown to guarantee non-manipulability against strategic adversaries. The only computationally efficient algorithm for minimizing linear swap regret over a general convex set in $\mathbb{R}^d$ was developed recently by Daskalakis, Farina, Fishelson, Pipis, and Schneider (STOC '25). However, it incurs a highly suboptimal regret bound of $Ω(d^4 \sqrt{T})$ and also relies on computationally intensive calls to the ellipsoid algorithm at each iteration.
  In this paper, we develop a significantly simpler, computationally efficient algorithm that guarantees $O(d^{3/2} \sqrt{T})$ linear swap regret for a general convex set and $O(d \sqrt{T})$ when the set is centrally symmetric. Our approach leverages the powerful response-based approachability framework of Bernstein and Shimkin (JMLR '15) -- previously overlooked in the line of work on swap regret minimization -- combined with geometric preconditioning via the John ellipsoid. Our algorithm simultaneously minimizes profile swap regret, which was recently shown to guarantee non-manipulability. Moreover, we establish a matching information-theoretic lower bound: any learner must incur in expectation $Ω(d \sqrt{T})$ linear swap regret for large enough $T$, even when the set is centrally symmetric. This also shows that the classic algorithm of Gordon, Greenwald, and Marks (ICML '08) is existentially optimal for minimizing linear swap regret, although it is computationally inefficient. Finally, we extend our approach to minimize regret with respect to the set of swap deviations with polynomial dimension, unifying and strengthening recent results in equilibrium computation and online learning.

</details>


### [63] [PurSAMERE: Reliable Adversarial Purification via Sharpness-Aware Minimization of Expected Reconstruction Error](https://arxiv.org/abs/2602.06269)
*Vinh Hoang,Sebastian Krumscheid,Holger Rauhut,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出一种确定性净化方法，通过将对抗样本映射到数据分布模式附近来提高对抗鲁棒性，避免随机净化方法在完全知识攻击下的性能下降。


<details>
  <summary>Details</summary>
Motivation: 随机净化方法在对抗者完全了解系统和随机性的情况下，有效鲁棒性会下降。需要一种确定性方法来确保可靠的测试精度，同时提高对抗鲁棒性。

Method: 使用通过最小化噪声污染数据重建误差训练的分数模型学习数据分布结构。给定对抗输入，在局部邻域搜索最小化噪声重建误差的净化样本，使用锐度感知最小化引导净化样本到平坦区域。

Result: 实验结果显示，在强确定性白盒攻击下，相比最先进方法获得了显著的对抗鲁棒性提升。

Conclusion: 提出的确定性净化方法能有效提高对抗鲁棒性，通过将样本映射到数据分布模式附近，在完全知识攻击下保持可靠性能，理论分析表明在小噪声极限下能恢复局部密度最大化器。

Abstract: We propose a novel deterministic purification method to improve adversarial robustness by mapping a potentially adversarial sample toward a nearby sample that lies close to a mode of the data distribution, where classifiers are more reliable. We design the method to be deterministic to ensure reliable test accuracy and to prevent the degradation of effective robustness observed in stochastic purification approaches when the adversary has full knowledge of the system and its randomness. We employ a score model trained by minimizing the expected reconstruction error of noise-corrupted data, thereby learning the structural characteristics of the input data distribution. Given a potentially adversarial input, the method searches within its local neighborhood for a purified sample that minimizes the expected reconstruction error under noise corruption and then feeds this purified sample to the classifier. During purification, sharpness-aware minimization is used to guide the purified samples toward flat regions of the expected reconstruction error landscape, thereby enhancing robustness. We further show that, as the noise level decreases, minimizing the expected reconstruction error biases the purified sample toward local maximizers of the Gaussian-smoothed density; under additional local assumptions on the score model, we prove recovery of a local maximizer in the small-noise limit. Experimental results demonstrate significant gains in adversarial robustness over state-of-the-art methods under strong deterministic white-box attacks.

</details>


### [64] [Statistical Learning from Attribution Sets](https://arxiv.org/abs/2602.06276)
*Lorne Applebaum,Robert Busa-Fekete,August Y. Chen,Claudio Gentile,Tomer Koren,Aryan Mokhtari*

Main category: cs.LG

TL;DR: 在隐私约束下，通过粗粒度归因集合训练广告转化预测模型，无需明确的点击-转化对应关系


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护浏览器API和第三方Cookie弃用背景下，广告点击与转化之间直接链接不可用的问题，需要在只能观察到点击序列和转化序列、且只能将转化关联到候选点击集合而非唯一来源的情况下进行模型训练

Method: 提出一种新颖的无偏估计方法，从归因集合中构建总体损失的无偏估计器，即使没有明确标签也能训练模型。该方法利用归因集合的粗粒度信号，通过经验风险最小化实现泛化保证

Result: 理论分析表明该方法能够获得与先验信息量成正比的泛化保证，并且对先验估计误差具有鲁棒性。在标准数据集上的简单实证评估显示，该方法显著优于常见的行业启发式方法，特别是在归因集合较大或重叠的情况下表现更优

Conclusion: 在隐私约束下，通过归因集合的无偏估计方法能够有效训练转化预测模型，为广告领域在隐私保护环境下的机器学习提供了可行的解决方案

Abstract: We address the problem of training conversion prediction models in advertising domains under privacy constraints, where direct links between ad clicks and conversions are unavailable. Motivated by privacy-preserving browser APIs and the deprecation of third-party cookies, we study a setting where the learner observes a sequence of clicks and a sequence of conversions, but can only link a conversion to a set of candidate clicks (an attribution set) rather than a unique source. We formalize this as learning from attribution sets generated by an oblivious adversary equipped with a prior distribution over the candidates. Despite the lack of explicit labels, we construct an unbiased estimator of the population loss from these coarse signals via a novel approach. Leveraging this estimator, we show that Empirical Risk Minimization achieves generalization guarantees that scale with the informativeness of the prior and is also robust against estimation errors in the prior, despite complex dependencies among attribution sets. Simple empirical evaluations on standard datasets suggest our unbiased approach significantly outperforms common industry heuristics, particularly in regimes where attribution sets are large or overlapping.

</details>


### [65] [SOCKET: SOft Collison Kernel EsTimator for Sparse Attention](https://arxiv.org/abs/2602.06283)
*Sahil Joshi,Agniva Chowdhury,Wyatt Bellinger,Amar Kanakamedala,Ekam Singh,Hoang Anh Duy Le,Aditya Desai,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: SOCKET提出了一种基于软碰撞核估计的稀疏注意力机制，通过概率化的相似性聚合改进传统LSH，在长上下文推理中实现高效token选择，相比FlashAttention提升1.5倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理中注意力计算成本高昂，稀疏注意力通过限制计算到部分token来降低成本，但传统LSH的硬桶匹配机制不适合token排序，需要更有效的评分和选择方法。

Method: 提出SOCKET（软碰撞核估计器），将传统LSH的硬碰撞匹配改为概率化的相似性聚合，通过跨哈希表的软碰撞证据积累来稳定top-k token的相对排序，实现无需特殊投票机制的token选择。

Result: 在多个长上下文基准测试中，SOCKET匹配或超越了现有稀疏注意力基线，通过定制CUDA核和Flash Decode Triton后端，相比FlashAttention实现了最高1.5倍的吞吐量提升。

Conclusion: SOCKET将LSH从候选生成启发式方法提升为数学上严谨的稀疏注意力评分核，为长上下文推理提供了高效工具，代码已开源。

Abstract: Exploiting sparsity during long-context inference is central to scaling large language models, as attention dominates the cost of autoregressive decoding. Sparse attention reduces this cost by restricting computation to a subset of tokens, but its effectiveness depends critically on efficient scoring and selection of relevant tokens at inference time. We revisit Locality-Sensitive Hashing (LSH) as a sparsification primitive and introduce SOCKET, a SOft Collision Kernel EsTimator that replaces hard bucket matches with probabilistic, similarity-aware aggregation. Our key insight is that hard LSH produces discrete collision signals and is therefore poorly suited for ranking. In contrast, soft LSH aggregates graded collision evidence across hash tables, preserving the stability of relative ordering among the true top-$k$ tokens. This transformation elevates LSH from a candidate-generation heuristic to a principled and mathematically grounded scoring kernel for sparse attention. Leveraging this property, SOCKET enables efficient token selection without ad-hoc voting mechanism, and matches or surpasses established sparse attention baselines across multiple long-context benchmarks using diverse set of models. With a custom CUDA kernel for scoring keys and a Flash Decode Triton backend for sparse attention, SOCKET achieves up to 1.5$\times$ higher throughput than FlashAttention, making it an effective tool for long-context inference. Code is open-sourced at https://github.com/amarka8/SOCKET.

</details>


### [66] [Toward generative machine learning for boosting ensembles of climate simulations](https://arxiv.org/abs/2602.06287)
*Parsa Gooya,Reinel Sospedra-Alfonso,Johannes Exenberger*

Main category: cs.LG

TL;DR: 使用条件变分自编码器(cVAE)从有限的气候模拟样本中生成任意大规模集合，以量化内部气候变率的不确定性，相比传统物理模型更高效。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的气候模型在计算资源限制下面临两难：要生成大规模集合进行稳健不确定性估计，就无法提高分辨率捕捉精细尺度动态。需要一种方法能突破这种计算约束。

Method: 开发条件变分自编码器(cVAE)，在有限的CMIP6历史及未来情景实验样本上训练，学习数据底层分布，生成物理一致的大规模气候模拟集合。

Result: cVAE能生成物理一致的样本，重现真实的低阶和高阶统计特征（包括极端事件），捕捉全球遥相关模式，即使训练数据中未包含的气候条件也能表现良好。

Conclusion: cVAE为气候不确定性量化提供了数学透明、可解释且计算高效的框架，虽然存在输出过于平滑、频谱偏差等局限性，但通过加入输出噪声等方法可缓解这些问题。

Abstract: Accurately quantifying uncertainty in predictions and projections arising from irreducible internal climate variability is critical for informed decision making. Such uncertainty is typically assessed using ensembles produced with physics based climate models. However, computational constraints impose a trade off between generating the large ensembles required for robust uncertainty estimation and increasing model resolution to better capture fine scale dynamics. Generative machine learning offers a promising pathway to alleviate these constraints. We develop a conditional Variational Autoencoder (cVAE) trained on a limited sample of climate simulations to generate arbitrary large ensembles. The approach is applied to output from monthly CMIP6 historical and future scenario experiments produced with the Canadian Centre for Climate Modelling and Analysis' (CCCma's) Earth system model CanESM5. We show that the cVAE model learns the underlying distribution of the data and generates physically consistent samples that reproduce realistic low and high moment statistics, including extremes. Compared with more sophisticated generative architectures, cVAEs offer a mathematically transparent, interpretable, and computationally efficient framework. Their simplicity lead to some limitations, such as overly smooth outputs, spectral bias, and underdispersion, that we discuss along with strategies to mitigate them. Specifically, we show that incorporating output noise improves the representation of climate relevant multiscale variability, and we propose a simple method to achieve this. Finally, we show that cVAE-enhanced ensembles capture realistic global teleconnection patterns, even under climate conditions absent from the training data.

</details>


### [67] [The Condensate Theorem: Transformers are O(n), Not $O(n^2)$](https://arxiv.org/abs/2602.06317)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: 注意力稀疏性是学习到的拓扑特性而非架构约束，通过将注意力投影到凝聚流形（锚点+窗口+动态Top-k）可实现与完整O(n²)注意力的100%输出等价，带来超过1000倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制存在二次复杂度瓶颈，但作者发现这可能是朴素实现的产物而非智能的本质。通过实证分析训练好的语言模型，发现注意力质量集中在特定的拓扑流形上，这为突破二次瓶颈提供了理论基础。

Method: 提出凝聚定理：注意力稀疏性是学习到的拓扑特性。通过识别注意力集中的凝聚流形（Anchor + Window + Dynamic Top-k），将注意力投影到该流形上，实现与完整注意力的无损等价。开发了拓扑注意力内核，将该拓扑映射到硬件实现。

Result: 在GPT-2、Pythia、Qwen2、TinyLlama和Mistral等模型上验证，在1500+生成token上实现比特精确匹配。拓扑注意力内核在131K token时实现159倍实测加速（3.94ms vs 628ms），在1M token时预计超过1200倍加速，相比Flash Attention减少超过99.9%的推理成本。

Conclusion: 二次复杂度瓶颈是朴素实现的产物而非智能的本质特性。注意力稀疏性是学习到的拓扑特性，通过识别和利用凝聚流形可以突破传统注意力机制的效率限制，实现大规模语言模型的高效推理。

Abstract: We present the Condensate Theorem: attention sparsity is a learned topological property, not an architectural constraint. Through empirical analysis of trained language models, we find that attention mass concentrates on a distinct topological manifold -- and this manifold can be identified dynamically without checking every position. We prove a general result: for any query, projecting attention onto the Condensate Manifold (Anchor + Window + Dynamic Top-k) achieves 100% output equivalence with full $O(n^2)$ attention. This is not an approximation -- it is lossless parity. We validate this across GPT-2, Pythia, Qwen2, TinyLlama, and Mistral, demonstrating bit-exact token matching on 1,500+ generated tokens. By mapping this topology to hardware, our Topological Attention kernel achieves a 159x measured speedup at 131K tokens (3.94ms vs 628ms) and a projected >1,200x speedup at 1M tokens, reducing inference costs by >99.9% compared to Flash Attention. We conclude that the quadratic bottleneck is an artifact of naive implementation, not intelligence.

</details>


### [68] [How (Not) to Hybridize Neural and Mechanistic Models for Epidemiological Forecasting](https://arxiv.org/abs/2602.06323)
*Yiqi Su,Ray Lee,Jiaming Cui,Naren Ramakrishnan*

Main category: cs.LG

TL;DR: 提出一种结合流行病学机理模型与神经ODE的混合方法，通过显式建模非平稳性来改进传染病预测


<details>
  <summary>Details</summary>
Motivation: 传统流行病学预测面临部分可观测性、动态变化（行为、免疫衰减、季节性、干预措施）等挑战，现有混合方法在这些条件下容易失败

Method: 从感染序列中提取多尺度结构（趋势、季节、残差），作为可解释的控制信号驱动受控神经ODE，结合流行病学模型联合预测时变传播率、恢复率和免疫损失率

Result: 在季节性和非季节性场景下，相比强基线方法，长期预测RMSE降低15-35%，峰值时间误差改善1-3周，峰值幅度偏差降低达30%

Conclusion: 通过显式建模非平稳性并提取多尺度结构作为控制信号，可以显著提升混合流行病学预测模型在动态变化环境中的鲁棒性和准确性

Abstract: Epidemiological forecasting from surveillance data is a hard problem and hybridizing mechanistic compartmental models with neural models is a natural direction. The mechanistic structure helps keep trajectories epidemiologically plausible, while neural components can capture non-stationary, data-adaptive effects. In practice, however, many seemingly straightforward couplings fail under partial observability and continually shifting transmission dynamics driven by behavior, waning immunity, seasonality, and interventions. We catalog these failure modes and show that robust performance requires making non-stationarity explicit: we extract multi-scale structure from the observed infection series and use it as an interpretable control signal for a controlled neural ODE coupled to an epidemiological model. Concretely, we decompose infections into trend, seasonal, and residual components and use these signals to drive continuous-time latent dynamics while jointly forecasting and inferring time-varying transmission, recovery, and immunity-loss rates. Across seasonal and non-seasonal settings, including early outbreaks and multi-wave regimes, our approach reduces long-horizon RMSE by 15-35%, improves peak timing error by 1-3 weeks, and lowers peak magnitude bias by up to 30% relative to strong time-series, neural ODE, and hybrid baselines, without relying on auxiliary covariates.

</details>


### [69] [Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics](https://arxiv.org/abs/2602.06326)
*Aoi Yoshimura,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于储层计算的轻量级在线适应框架，使用回声状态网络编码观测历史，通过递归最小二乘法在线更新，实现非平稳环境下的快速适应，无需反向传播或预训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习策略在仿真中训练后部署到现实世界时，由于非平稳动态特性导致性能严重下降。现有方法如域随机化和元强化学习通常需要大量预训练、特权信息或高计算成本，限制了在实时和边缘系统中的应用。

Method: 提出基于储层计算的轻量级在线适应框架：1) 集成回声状态网络作为适应模块，编码最近的观测历史为潜在上下文表示；2) 使用递归最小二乘法在线更新其读出权重。该设计无需反向传播、预训练或特权信息访问。

Result: 在CartPole和HalfCheetah任务上进行评估，面对严重且突然的环境变化（周期性外部干扰和极端摩擦变化）。实验结果表明，该方法在分布外动态下显著优于域随机化和代表性自适应基线方法，能在几个控制步骤内实现稳定适应。特别地，该方法成功处理了无需重置策略的片段内环境变化。

Conclusion: 由于计算效率和稳定性，该框架为非平稳环境中的在线适应提供了实用解决方案，非常适合现实世界机器人控制和边缘部署。

Abstract: Reinforcement learning (RL) policies trained in simulation often suffer from severe performance degradation when deployed in real-world environments due to non-stationary dynamics. While Domain Randomization (DR) and meta-RL have been proposed to address this issue, they typically rely on extensive pretraining, privileged information, or high computational cost, limiting their applicability to real-time and edge systems. In this paper, we propose a lightweight online adaptation framework for RL based on Reservoir Computing. Specifically, we integrate an Echo State Networks (ESNs) as an adaptation module that encodes recent observation histories into a latent context representation, and update its readout weights online using Recursive Least Squares (RLS). This design enables rapid adaptation without backpropagation, pretraining, or access to privileged information. We evaluate the proposed method on CartPole and HalfCheetah tasks with severe and abrupt environment changes, including periodic external disturbances and extreme friction variations. Experimental results demonstrate that the proposed approach significantly outperforms DR and representative adaptive baselines under out-of-distribution dynamics, achieving stable adaptation within a few control steps. Notably, the method successfully handles intra-episode environment changes without resetting the policy. Due to its computational efficiency and stability, the proposed framework provides a practical solution for online adaptation in non-stationary environments and is well suited for real-world robotic control and edge deployment.

</details>


### [70] [Don't Break the Boundary: Continual Unlearning for OOD Detection Based on Free Energy Repulsion](https://arxiv.org/abs/2602.06331)
*Ningkang Peng,Kun Shao,Jingyang Mao,Linjing Qian,Xiaoqian Peng,Xichen Yang,Yanhui Gu*

Main category: cs.LG

TL;DR: TFER框架通过自由能排斥力实现边界保持的类别遗忘，将目标类别转化为OOD样本，同时保持ID数据流形结构，解决了OOD检测与机器学习遗忘之间的几何矛盾。


<details>
  <summary>Details</summary>
Motivation: 在开放世界部署可信AI面临双重挑战：需要强大的OOD检测确保系统安全，同时需要灵活的机器学习遗忘满足隐私合规和模型修正。但现有方法存在几何矛盾：OOD检测依赖静态紧凑的数据流形，而传统遗忘方法会破坏这种结构，导致模型异常检测能力灾难性丧失。

Method: 提出TFER（总自由能排斥）框架，基于自由能原理构建推拉游戏机制：通过拉机制将保留类别锚定在低能量ID流形内，同时使用自由能排斥力主动将遗忘类别推至高能量OOD区域。采用参数高效微调实现，避免完全重新训练的高成本。

Result: 实验表明TFER实现了精确遗忘，同时最大程度保留了模型在剩余类别和外部OOD数据上的判别性能。更重要的是，TFER独特的推拉平衡赋予模型内在结构稳定性，能有效抵抗灾难性遗忘，在持续遗忘任务中表现出卓越潜力。

Conclusion: TFER通过将遗忘类别转化为OOD样本的数学等价性，解决了OOD检测与机器学习遗忘之间的根本矛盾，为开放世界可信AI部署提供了有效解决方案，其结构稳定性特性在持续学习场景中具有重要应用价值。

Abstract: Deploying trustworthy AI in open-world environments faces a dual challenge: the necessity for robust Out-of-Distribution (OOD) detection to ensure system safety, and the demand for flexible machine unlearning to satisfy privacy compliance and model rectification. However, this objective encounters a fundamental geometric contradiction: current OOD detectors rely on a static and compact data manifold, whereas traditional classification-oriented unlearning methods disrupt this delicate structure, leading to a catastrophic loss of the model's capability to discriminate anomalies while erasing target classes. To resolve this dilemma, we first define the problem of boundary-preserving class unlearning and propose a pivotal conceptual shift: in the context of OOD detection, effective unlearning is mathematically equivalent to transforming the target class into OOD samples. Based on this, we propose the TFER (Total Free Energy Repulsion) framework. Inspired by the free energy principle, TFER constructs a novel Push-Pull game mechanism: it anchors retained classes within a low-energy ID manifold through a pull mechanism, while actively expelling forgotten classes to high-energy OOD regions using a free energy repulsion force. This approach is implemented via parameter-efficient fine-tuning, circumventing the prohibitive cost of full retraining. Extensive experiments demonstrate that TFER achieves precise unlearning while maximally preserving the model's discriminative performance on remaining classes and external OOD data. More importantly, our study reveals that the unique Push-Pull equilibrium of TFER endows the model with inherent structural stability, allowing it to effectively resist catastrophic forgetting without complex additional constraints, thereby demonstrating exceptional potential in continual unlearning tasks.

</details>


### [71] [Adversarial Learning in Games with Bandit Feedback: Logarithmic Pure-Strategy Maximin Regret](https://arxiv.org/abs/2602.06348)
*Shinji Ito,Haipeng Luo,Arnab Maiti,Taira Tsuchiya,Yue Wu*

Main category: cs.LG

TL;DR: 研究零和博弈在强盗反馈下的对抗学习，提出两种反馈模型下的算法，实现对数级后悔界


<details>
  <summary>Details</summary>
Motivation: 现实应用中，学习者需要面对未知对手且只能获得强盗反馈（仅能观察到所选动作的收益），而传统方法在这种设置下存在Ω(√T)的不可避免的外部后悔，需要突破这一限制

Method: 1. 无信息强盗反馈：使用Tsallis-INF算法；2. 有信息强盗反馈：提出Maximin-UCB算法；3. 扩展到双线性博弈：提出Tsallis-FTRL-SPM（无信息）和Maximin-LinUCB（有信息）算法

Result: 1. 无信息设置：Tsallis-INF实现O(c log T)的实例依赖后悔界，且证明对c的依赖是必要的；2. 有信息设置：Maximin-UCB实现O(c' log T)后悔界，其中c'可能远小于c；3. 双线性博弈扩展：类似的对数级后悔界

Conclusion: 在零和博弈的强盗反馈设置中，通过适当的算法设计可以突破Ω(√T)的下界，实现游戏依赖的对数级后悔界，特别是在有信息反馈下可以获得更好的性能

Abstract: Learning to play zero-sum games is a fundamental problem in game theory and machine learning. While significant progress has been made in minimizing external regret in the self-play settings or with full-information feedback, real-world applications often force learners to play against unknown, arbitrary opponents and restrict learners to bandit feedback where only the payoff of the realized action is observable. In such challenging settings, it is well-known that $Ω(\sqrt{T})$ external regret is unavoidable (where T is the number of rounds). To overcome this barrier, we investigate adversarial learning in zero-sum games under bandit feedback, aiming to minimize the deficit against the maximin pure strategy -- a metric we term Pure-Strategy Maximin Regret.
  We analyze this problem under two bandit feedback models: uninformed (only the realized reward is revealed) and informed (both the reward and the opponent's action are revealed). For uninformed bandit learning of normal-form games, we show that the Tsallis-INF algorithm achieves $O(c \log T)$ instance-dependent regret with a game-dependent parameter $c$. Crucially, we prove an information-theoretic lower bound showing that the dependence on c is necessary. To overcome this hardness, we turn to the informed setting and introduce Maximin-UCB, which obtains another regret bound of the form $O(c' \log T)$ for a different game-dependent parameter $c'$ that could potentially be much smaller than $c$. Finally, we generalize both results to bilinear games over an arbitrary, large action set, proposing Tsallis-FTRL-SPM and Maximin-LinUCB for the uninformed and informed setting respectively and establishing similar game-dependent logarithmic regret bounds.

</details>


### [72] [Enhance and Reuse: A Dual-Mechanism Approach to Boost Deep Forest for Label Distribution Learning](https://arxiv.org/abs/2602.06353)
*Jia-Le Xu,Shen-Huan Lyu,Yu-Nian Wang,Ning Chen,Zhihao Qu,Bin Tang,Baoliu Ye*

Main category: cs.LG

TL;DR: 提出ERDF方法，通过特征增强利用标签相关性和度量感知特征重用机制，改进深度森林在标签分布学习中的性能


<details>
  <summary>Details</summary>
Motivation: 深度森林在标签分布学习领域应用尚不成熟，现有方法未能有效利用标签间相关性，需要改进以提升性能

Method: 提出ERDF方法，包含两个核心机制：1) 利用标签相关性增强原始特征；2) 对验证集上表现变差的样本特征进行重用操作，确保训练稳定性

Result: 在六个评估指标上优于其他对比算法

Conclusion: ERDF方法通过增强-重用模式有效利用标签相关性，提升深度森林在标签分布学习中的性能

Abstract: Label distribution learning (LDL) requires the learner to predict the degree of correlation between each sample and each label. To achieve this, a crucial task during learning is to leverage the correlation among labels. Deep Forest (DF) is a deep learning framework based on tree ensembles, whose training phase does not rely on backpropagation. DF performs in-model feature transform using the prediction of each layer and achieves competitive performance on many tasks. However, its exploration in the field of LDL is still in its infancy. The few existing methods that apply DF to the field of LDL do not have effective ways to utilize the correlation among labels. Therefore, we propose a method named Enhanced and Reused Feature Deep Forest (ERDF). It mainly contains two mechanisms: feature enhancement exploiting label correlation and measure-aware feature reuse. The first one is to utilize the correlation among labels to enhance the original features, enabling the samples to acquire more comprehensive information for the task of LDL. The second one performs a reuse operation on the features of samples that perform worse than the previous layer on the validation set, in order to ensure the stability of the training process. This kind of Enhance-Reuse pattern not only enables samples to enrich their features but also validates the effectiveness of their new features and conducts a reuse process to prevent the noise from spreading further. Experiments show that our method outperforms other comparison algorithms on six evaluation metrics.

</details>


### [73] [Evaluating LLM-persona Generated Distributions for Decision-making](https://arxiv.org/abs/2602.06357)
*Jackie Baek,Yunhan Chen,Ziyu Chi,Will Ma*

Main category: cs.LG

TL;DR: LLM生成的分布对下游决策有用，尤其在数据稀缺时，但传统分布评估指标（如Wasserstein距离）可能误导决策质量评估。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的分布（如消费者支付意愿模拟）在支持下游决策（如定价优化）时的实际效用，评估这些分布对决策质量的影响。

Method: 提出LLM-SAA方法：使用LLM构建估计分布，然后在该分布下优化决策。通过三个经典决策问题（品类优化、定价、报童问题）评估分布质量，比较决策导向指标与传统分布距离指标。

Result: LLM生成的分布在低数据环境下具有实际应用价值，但决策无关的指标（如Wasserstein距离）在评估分布对决策的效用时可能产生误导。

Conclusion: 评估LLM生成分布时应采用决策导向的指标而非传统分布距离指标，LLM-SAA方法在数据稀缺时能为决策提供实用支持。

Abstract: LLMs can generate a wealth of data, ranging from simulated personas imitating human valuations and preferences, to demand forecasts based on world knowledge. But how well do such LLM-generated distributions support downstream decision-making? For example, when pricing a new product, a firm could prompt an LLM to simulate how much consumers are willing to pay based on a product description, but how useful is the resulting distribution for optimizing the price? We refer to this approach as LLM-SAA, in which an LLM is used to construct an estimated distribution and the decision is then optimized under that distribution. In this paper, we study metrics to evaluate the quality of these LLM-generated distributions, based on the decisions they induce. Taking three canonical decision-making problems (assortment optimization, pricing, and newsvendor) as examples, we find that LLM-generated distributions are practically useful, especially in low-data regimes. We also show that decision-agnostic metrics such as Wasserstein distance can be misleading when evaluating these distributions for decision-making.

</details>


### [74] [Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation](https://arxiv.org/abs/2602.06359)
*Xiyang Zhang,Yuanhe Tian,Hongzhi Wang,Yan Song*

Main category: cs.LG

TL;DR: OGS提出了一种数据选择方法，通过选择梯度与通用知识正交的训练样本，在保持通用推理能力的同时提升领域性能，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型时存在灾难性遗忘问题：获得领域专业知识的同时会丧失通用推理能力。现有方法要么计算成本过高（梯度手术），要么无法识别冲突梯度方向（数据选择方法）。

Method: 正交梯度选择（OGS）：将梯度投影的几何洞察从优化器转移到数据选择阶段，通过轻量级导航模型和强化学习技术，动态识别梯度与通用知识锚点正交的训练样本。

Result: 在医疗、法律和金融领域的实验表明，OGS在领域性能和训练效率方面表现优异，同时在GSM8K等通用任务上保持甚至提升性能。

Conclusion: OGS通过数据选择方法有效解决了灾难性遗忘问题，实现了领域性能、通用能力保留和训练效率的平衡，无需修改优化器或增加运行时投影成本。

Abstract: Fine-tuning large language models (LLMs) for specialized domains often necessitates a trade-off between acquiring domain expertise and retaining general reasoning capabilities, a phenomenon known as catastrophic forgetting. Existing remedies face a dichotomy: gradient surgery methods offer geometric safety but incur prohibitive computational costs via online projections, while efficient data selection approaches reduce overhead but remain blind to conflict-inducing gradient directions. In this paper, we propose Orthogonal Gradient Selection (OGS), a data-centric method that harmonizes domain performance, general capability retention, and training efficiency. OGS shifts the geometric insights of gradient projection from the optimizer to the data selection stage by treating data selection as a constrained decision-making process. By leveraging a lightweight Navigator model and reinforcement learning techniques, OGS dynamically identifies training samples whose gradients are orthogonal to a general-knowledge anchor. This approach ensures naturally safe updates for target models without modifying the optimizer or incurring runtime projection costs. Experiments across medical, legal, and financial domains demonstrate that OGS achieves excellent results, significantly improving domain performance and training efficiency while maintaining or even enhancing performance on general tasks such as GSM8K.

</details>


### [75] [Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization](https://arxiv.org/abs/2602.06385)
*Changmin Kang,Jihun Yun,Baekrok Shin,Yeseul Cho,Chulhee Yun*

Main category: cs.LG

TL;DR: SpecGD/Muon优化器在LoRA微调中展现出独特的光谱现象：LoRA乘积的奇异值呈现近乎均匀的增长，这与标准梯度下降中"最大奇异值优先学习"的模式形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 尽管SpecGD/Muon优化器在LLM训练中表现良好，但其动力学机制仍不明确。特别是在LoRA设置下，观察到奇异值均匀增长的现象，需要理论解释。

Method: 1. 分析LoRA微调中的光谱现象；2. 在简化的LoRA风格矩阵分解设置中分析光谱梯度流(SpecGF)；3. 证明"等速率"动力学理论；4. 提供全局收敛性证明；5. 通过实验验证理论。

Result: 1. 证明SpecGF中所有奇异值以近乎相等的速率增长；2. 较小的奇异值比大的奇异值更早达到目标值；3. 在因子范数有界条件下，SpecGF几乎从所有初始化都能收敛到全局最小值；4. 使用ℓ2正则化可获得全局收敛性。

Conclusion: SpecGD/Muon优化器在LoRA设置下展现出独特的"等速率"学习动力学，与标准梯度下降的"最大优先"模式完全不同，这为理解这些优化器在LLM训练中的良好表现提供了理论依据。

Abstract: Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove "equal-rate" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.

</details>


### [76] [Generating High-quality Privacy-preserving Synthetic Data](https://arxiv.org/abs/2602.06390)
*David Yavo,Richard Khoury,Christophe Pere,Sadoune Ait Kaci Azzou*

Main category: cs.LG

TL;DR: 提出一个简单的后处理框架，通过模式修补和k近邻过滤来改进合成表格数据的质量与隐私平衡


<details>
  <summary>Details</summary>
Motivation: 合成表格数据在实际部署中需要在分布保真度、下游效用和隐私保护之间取得平衡，现有方法往往难以同时满足这些要求

Method: 提出一个模型无关的后处理框架：1) 模式修补步骤修复合成数据中缺失或严重不足的类别；2) k近邻过滤器替换过于接近真实数据的合成记录，强制执行最小距离

Result: 在0.2-0.35阈值下，后处理将真实与合成分类分布差异减少达36%，成对依赖保持度提高10-14%，下游预测性能保持在未处理基线的1%以内，同时距离隐私指标改善

Conclusion: 该后处理框架为选择阈值和应用后修复提供了实用指导，能提高合成表格数据的质量和经验隐私，同时补充提供正式差分隐私保证的方法

Abstract: Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data generator to improve this trade off. First, a mode patching step repairs categories that are missing or severely underrepresented in the synthetic data, while largely preserving learned dependencies. Second, a k nearest neighbor filter replaces synthetic records that lie too close to real data points, enforcing a minimum distance between real and synthetic samples. We instantiate this framework for two neural generative models for tabular data, a feed forward generator and a variational autoencoder, and evaluate it on three public datasets covering credit card transactions, cardiovascular health, and census based income. We assess marginal and joint distributional similarity, the performance of models trained on synthetic data and evaluated on real data, and several empirical privacy indicators, including nearest neighbor distances and attribute inference attacks. With moderate thresholds between 0.2 and 0.35, the post processing reduces divergence between real and synthetic categorical distributions by up to 36 percent and improves a combined measure of pairwise dependence preservation by 10 to 14 percent, while keeping downstream predictive performance within about 1 percent of the unprocessed baseline. At the same time, distance based privacy indicators improve and the success rate of attribute inference attacks remains largely unchanged. These results provide practical guidance for selecting thresholds and applying post hoc repairs to improve the quality and empirical privacy of synthetic tabular data, while complementing approaches that provide formal differential privacy guarantees.

</details>


### [77] [Near-Optimal Regret for Distributed Adversarial Bandits: A Black-Box Approach](https://arxiv.org/abs/2602.06404)
*Hao Qiu,Mengxiao Zhang,Nicolò Cesa-Bianchi*

Main category: cs.LG

TL;DR: 分布式对抗性多臂老虎机问题，通过基于延迟反馈的黑盒约简算法，实现了$\tildeΘ(\sqrt{(ρ^{-1/2}+K/N)T})$的最优遗憾界，显著改进了先前结果。


<details>
  <summary>Details</summary>
Motivation: 研究分布式对抗性多臂老虎机问题，其中N个智能体通过通信协作最小化全局平均损失，但每个智能体只能观察到自己的局部损失。需要设计高效的分布式算法，在有限的通信条件下实现最优性能。

Method: 提出基于延迟反馈的黑盒约简算法，智能体仅通过gossip通信。算法将分布式问题转化为具有延迟反馈的经典老虎机问题，利用通信矩阵的谱间隙ρ来量化通信效率。进一步扩展到分布式线性老虎机，使用体积生成器实现每轮O(d)的通信成本。

Result: 证明了最小最大遗憾为$\tildeΘ(\sqrt{(ρ^{-1/2}+K/N)T})$，显著改进了Yi和Vojnovic (2023)的$\tilde{O}(ρ^{-1/3}(KT)^{2/3})$上界。问题难度分解为通信成本$ρ^{-1/4}\sqrt{T}$和老虎机成本$\sqrt{KT/N}$。在分布式线性老虎机中获得了$\tilde{O}(\sqrt{(ρ^{-1/2}+1/N)dT})$的遗憾界。

Conclusion: 该工作为分布式对抗性老虎机问题建立了最优理论界限，提出的黑盒约简框架具有通用性，可扩展到一阶和"两全其美"界限。算法通信效率高，仅需gossip通信，为分布式在线学习提供了有效的解决方案。

Abstract: We study distributed adversarial bandits, where $N$ agents cooperate to minimize the global average loss while observing only their own local losses. We show that the minimax regret for this problem is $\tildeΘ(\sqrt{(ρ^{-1/2}+K/N)T})$, where $T$ is the horizon, $K$ is the number of actions, and $ρ$ is the spectral gap of the communication matrix. Our algorithm, based on a novel black-box reduction to bandits with delayed feedback, requires agents to communicate only through gossip. It achieves an upper bound that significantly improves over the previous best bound $\tilde{O}(ρ^{-1/3}(KT)^{2/3})$ of Yi and Vojnovic (2023). We complement this result with a matching lower bound, showing that the problem's difficulty decomposes into a communication cost $ρ^{-1/4}\sqrt{T}$ and a bandit cost $\sqrt{KT/N}$. We further demonstrate the versatility of our approach by deriving first-order and best-of-both-worlds bounds in the distributed adversarial setting. Finally, we extend our framework to distributed linear bandits in $R^d$, obtaining a regret bound of $\tilde{O}(\sqrt{(ρ^{-1/2}+1/N)dT})$, achieved with only $O(d)$ communication cost per agent and per round via a volumetric spanner.

</details>


### [78] [EEG Emotion Classification Using an Enhanced Transformer-CNN-BiLSTM Architecture with Dual Attention Mechanisms](https://arxiv.org/abs/2602.06411)
*S M Rakib UI Karim,Wenyi Lu,Diponkor Bala,Rownak Ara Rasul,Sean Goggins*

Main category: cs.LG

TL;DR: 该研究提出了一种结合卷积、循环和注意力机制的混合深度学习架构，用于EEG情感识别，在公开数据集上实现了最先进的分类性能。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的情感识别在情感计算和决策支持系统中至关重要，但由于EEG信号具有高维度、噪声大和个体依赖性等特点，仍然面临挑战。需要开发更有效的模型来提高分类性能和鲁棒性。

Method: 提出增强的混合模型，结合卷积特征提取、双向时间建模和自注意力机制，并采用正则化策略防止过拟合。模型整合了多种深度学习组件来有效处理EEG信号的时空特性。

Result: 在包含中性、积极和消极三种情绪状态的公开EEG数据集上，该方法实现了最先进的分类性能，显著优于传统机器学习和神经网络基线。统计测试证实了交叉验证下的鲁棒性提升。特征分析显示基于协方差的EEG特征对情绪区分贡献最大。

Conclusion: 精心设计的混合架构能够在EEG情感识别中有效平衡预测准确性、鲁棒性和可解释性，对应用情感计算和以人为中心的智能系统具有重要意义。

Abstract: Electroencephalography (EEG)-based emotion recognition plays a critical role in affective computing and emerging decision-support systems, yet remains challenging due to high-dimensional, noisy, and subject-dependent signals. This study investigates whether hybrid deep learning architectures that integrate convolutional, recurrent, and attention-based components can improve emotion classification performance and robustness in EEG data. We propose an enhanced hybrid model that combines convolutional feature extraction, bidirectional temporal modeling, and self-attention mechanisms with regularization strategies to mitigate overfitting. Experiments conducted on a publicly available EEG dataset spanning three emotional states (neutral, positive, and negative) demonstrate that the proposed approach achieves state-of-the-art classification performance, significantly outperforming classical machine learning and neural baselines. Statistical tests confirm the robustness of these performance gains under cross-validation. Feature-level analyses further reveal that covariance-based EEG features contribute most strongly to emotion discrimination, highlighting the importance of inter-channel relationships in affective modeling. These findings suggest that carefully designed hybrid architectures can effectively balance predictive accuracy, robustness, and interpretability in EEG-based emotion recognition, with implications for applied affective computing and human-centered intelligent systems.

</details>


### [79] [Adaptive Protein Tokenization](https://arxiv.org/abs/2602.06418)
*Rohit Dilip,Ayush Varshney,David Van Valen*

Main category: cs.LG

TL;DR: 提出一种全局蛋白质结构标记化方法，通过渐进增加细节的全局表示解决局部标记化的限制，在重建、生成和表示任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构标记化方法通过局部邻域池化信息创建标记，这种方法在生成和表示任务上性能有限。需要一种全局标记化方法来克服这些限制

Method: 提出全局蛋白质结构标记化方法，其中连续标记以递增的细节水平贡献给全局表示。这种方法避免了序列缩减操作，支持任务特定的信息内容适应

Result: 在重建、生成和表示任务中匹配或优于基于局部蛋白质结构标记化的现有模型。自适应标记支持基于信息内容的推理标准，提高了可设计性。在CATH分类任务中表现优异，支持零样本蛋白质缩小和亲和力成熟

Conclusion: 全局蛋白质结构标记化方法解决了局部标记化的多个问题，包括误差累积缓解、无需序列缩减操作的嵌入生成，以及任务特定信息内容适应，为多模态蛋白质理解提供了更有效的途径

Abstract: Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.

</details>


### [80] [Beyond Code Contributions: How Network Position, Temporal Bursts, and Code Review Activities Shape Contributor Influence in Large-Scale Open Source Ecosystems](https://arxiv.org/abs/2602.06426)
*S M Rakib Ul Karim,Wenyi Lu,Sean Goggins*

Main category: cs.LG

TL;DR: 该研究使用图神经网络和时序网络分析对25年开源软件贡献者网络进行综合分析，发现网络影响力呈幂律分布，识别出五种贡献者角色，并揭示不同行为类型与影响力的相关性。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目依赖复杂的贡献者网络来推动创新和可持续性，但缺乏对这些网络结构、动态变化和关键角色影响的系统性实证分析。

Method: 使用GPU加速的PageRank、中介中心性算法和自定义LSTM模型，对云原生计算基金会生态系统25年的数据进行分析，涵盖沙盒、孵化和毕业项目，分析数千名贡献者和数百个代码库。

Result: 发现OSS网络影响力呈强幂律分布（前1%贡献者控制大部分影响力），识别出五种贡献者角色（核心、桥梁、连接器、常规、边缘），特定行为类型（提交、拉取请求、问题）与影响力显著相关，网络密度、聚类系数和模块性呈现显著时序趋势。

Conclusion: 研究结果为战略性贡献者保留政策提供实证依据，并为社区健康指标提供可操作的见解，特别强调桥梁贡献者对网络凝聚力的关键作用。

Abstract: Open source software (OSS) projects rely on complex networks of contributors whose interactions drive innovation and sustainability. This study presents a comprehensive analysis of OSS contributor networks using advanced graph neural networks and temporal network analysis on data spanning 25 years from the Cloud Native Computing Foundation ecosystem, encompassing sandbox, incubating, and graduated projects. Our analysis of thousands of contributors across hundreds of repositories reveals that OSS networks exhibit strong power-law distributions in influence, with the top 1\% of contributors controlling a substantial portion of network influence. Using GPU-accelerated PageRank, betweenness centrality, and custom LSTM models, we identify five distinct contributor roles: Core, Bridge, Connector, Regular, and Peripheral, each with unique network positions and structural importance. Statistical analysis reveals significant correlations between specific action types (commits, pull requests, issues) and contributor influence, with multiple regression models explaining substantial variance in influence metrics. Temporal analysis shows that network density, clustering coefficients, and modularity exhibit statistically significant temporal trends, with distinct regime changes coinciding with major project milestones. Structural integrity simulations show that Bridge contributors, despite representing a small fraction of the network, have a disproportionate impact on network cohesion when removed. Our findings provide empirical evidence for strategic contributor retention policies and offer actionable insights into community health metrics.

</details>


### [81] [Reclaiming First Principles: A Differentiable Framework for Conceptual Hydrologic Models](https://arxiv.org/abs/2602.06429)
*Jasper A. Vrugt,Jonathan M. Frame,Ethan Bollman*

Main category: cs.LG

TL;DR: 提出一种基于精确参数敏感性的全解析可微分水文建模框架，通过增广控制ODE系统联合演化模型状态和雅可比矩阵，为各种损失函数提供完全解析的梯度向量，实现快速、稳定、透明的梯度校准。


<details>
  <summary>Details</summary>
Motivation: 概念性水文模型的校准通常缓慢且数值脆弱，现有的基于梯度的参数估计方法依赖于有限差分近似或自动微分框架，这些方法计算量大、引入截断误差、求解器不稳定且开销大，特别是对于概念性流域模型的ODE系统。

Method: 通过增广控制ODE系统，加入敏感性方程，联合演化模型状态和所有参数的雅可比矩阵。该雅可比矩阵为任何可微分损失函数提供完全解析的梯度向量，包括经典目标函数、水文性能指标、鲁棒损失函数和基于水文图的泛函。

Result: 解析敏感性消除了数值微分的步长依赖性和噪声，避免了伴随方法的不稳定性和现代机器学习自动微分工具链的开销。得到的梯度是确定性的、物理可解释的，并且易于嵌入基于梯度的优化器中。

Conclusion: 这项工作实现了概念性水文模型的快速、稳定和透明的梯度校准，释放了可微分建模的全部潜力，无需依赖外部、不透明或CPU密集的自动微分库。

Abstract: Conceptual hydrologic models remain the cornerstone of rainfall-runoff modeling, yet their calibration is often slow and numerically fragile. Most gradient-based parameter estimation methods rely on finite-difference approximations or automatic differentiation frameworks (e.g., JAX, PyTorch and TensorFlow), which are computationally demanding and introduce truncation errors, solver instabilities, and substantial overhead. These limitations are particularly acute for the ODE systems of conceptual watershed models. Here we introduce a fully analytic and computationally efficient framework for differentiable hydrologic modeling based on exact parameter sensitivities. By augmenting the governing ODE system with sensitivity equations, we jointly evolve the model states and the Jacobian matrix with respect to all parameters. This Jacobian then provides fully analytic gradient vectors for any differentiable loss function. These include classical objective functions such as the sum of absolute and squared residuals, widely used hydrologic performance metrics such as the Nash-Sutcliffe and Kling-Gupta efficiencies, robust loss functions that down-weight extreme events, and hydrograph-based functionals such as flow-duration and recession curves. The analytic sensitivities eliminate the step-size dependence and noise inherent to numerical differentiation, while avoiding the instability of adjoint methods and the overhead of modern machine-learning autodiff toolchains. The resulting gradients are deterministic, physically interpretable, and straightforward to embed in gradient-based optimizers. Overall, this work enables rapid, stable, and transparent gradient-based calibration of conceptual hydrologic models, unlocking the full potential of differentiable modeling without reliance on external, opaque, or CPU-intensive automatic-differentiation libraries.

</details>


### [82] [Is Gradient Ascent Really Necessary? Memorize to Forget for Machine Unlearning](https://arxiv.org/abs/2602.06441)
*Zhuo Huang,Qizhou Wang,Ziming Hong,Shanshan Ye,Bo Han,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出模型外推法替代梯度上升进行机器遗忘，通过训练记忆模型并外推获得遗忘模型，避免灾难性崩溃


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对于保护敏感、私人和受版权保护的知识至关重要，但传统的梯度上升方法容易导致灾难性崩溃和性能下降

Method: 提出模型外推法：1) 以原始模型为参考，训练记忆模型记住不需要的数据；2) 从记忆模型向参考模型外推获得遗忘模型；3) 避免直接使用梯度上升，改用梯度下降训练记忆模型

Result: 模型外推法简单高效，能稳定机器遗忘过程，有效收敛并提高遗忘性能

Conclusion: 模型外推法为机器遗忘提供了一种稳定有效的替代方案，避免了梯度上升导致的灾难性崩溃问题

Abstract: For ethical and safe AI, machine unlearning rises as a critical topic aiming to protect sensitive, private, and copyrighted knowledge from misuse. To achieve this goal, it is common to conduct gradient ascent (GA) to reverse the training on undesired data. However, such a reversal is prone to catastrophic collapse, which leads to serious performance degradation in general tasks. As a solution, we propose model extrapolation as an alternative to GA, which reaches the counterpart direction in the hypothesis space from one model given another reference model. Therefore, we leverage the original model as the reference, further train it to memorize undesired data while keeping prediction consistency on the rest retained data, to obtain a memorization model. Counterfactual as it might sound, a forget model can be obtained via extrapolation from the memorization model to the reference model. Hence, we avoid directly acquiring the forget model using GA, but proceed with gradient descent for the memorization model, which successfully stabilizes the machine unlearning process. Our model extrapolation is simple and efficient to implement, and it can also effectively converge throughout training to achieve improved unlearning performance.

</details>


### [83] [Principle-Evolvable Scientific Discovery via Uncertainty Minimization](https://arxiv.org/abs/2602.06448)
*Yingming Pu,Tao Lin,Hongyu Chen*

Main category: cs.LG

TL;DR: PiEvo框架通过贝叶斯优化在扩展的原则空间中实现科学发现，让LLM智能体能够自主演化科学原理而非固定假设，显著提升效率和发现新颖现象的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的科学智能体通常固守初始先验，在静态假设空间中运作，这限制了新颖现象的发现，当基线理论失败时会造成计算浪费。需要从搜索假设转向演化基础科学原理。

Method: 提出PiEvo框架，将科学发现视为在扩展原则空间上的贝叶斯优化。通过高斯过程的信息导向假设选择和异常驱动增强机制，使智能体能够自主精炼其理论世界观。

Result: 在四个基准测试中：1) 平均解决方案质量达90.81%~93.15%，比最先进方法提升29.7%~31.1%；2) 通过优化紧凑原则空间显著减少样本复杂度，实现83.3%的收敛步数加速；3) 在不同科学领域和LLM骨干上保持稳健性能。

Conclusion: PiEvo通过演化科学原理而非固定假设，显著提升了LLM科学智能体的效率和发现能力，为自主科学发现提供了新的范式。

Abstract: Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel phenomena, resulting in computational waste when baseline theories fail. To address this, we propose shifting the focus from searching hypotheses to evolving the underlying scientific principles. We present PiEvo, a principle-evolvable framework that treats scientific discovery as Bayesian optimization over an expanding principle space. By integrating Information-Directed Hypothesis Selection via Gaussian Process and an anomaly-driven augmentation mechanism, PiEvo enables agents to autonomously refine their theoretical worldview. Evaluation across four benchmarks demonstrates that PiEvo (1) achieves an average solution quality of up to 90.81%~93.15%, representing a 29.7%~31.1% improvement over the state-of-the-art, (2) attains an 83.3% speedup in convergence step via significantly reduced sample complexity by optimizing the compact principle space, and (3) maintains robust performance across diverse scientific domains and LLM backbones.

</details>


### [84] [BrokenBind: Universal Modality Exploration beyond Dataset Boundaries](https://arxiv.org/abs/2602.06451)
*Zhuo Huang,Runnan Chen,Bo Han,Gang Niu,Masashi Sugiyama,Tongliang Liu*

Main category: cs.LG

TL;DR: BrokenBind是一种新颖的多模态学习方法，能够将不同数据集中的模态绑定在一起，解决了现有方法局限于特定数据集模态组合的问题，实现了更灵活和通用的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法通常将不同模态直接绑定在特定的联合嵌入空间中，但这种方法局限于数据集中已存在的模态组合。当推广到下游任务中未出现的模态时，这些方法存在偏差。此外，获取多模态数据集的成本严重限制了先前方法的可行性。

Method: BrokenBind同时利用包含目标模态的多个数据集和一个共享模态。尽管两个数据集由于分布不匹配而不对应，但该方法能够捕获它们的关系来生成伪嵌入，以填补缺失的目标模态，从而实现灵活和通用的多模态学习。该方法支持任意两种模态的绑定，不受数据集限制。

Result: 通过广泛评估，BrokenBind相比知名的多模态基线方法表现出优越性。该方法在需要多个数据集进行模态绑定的强化场景中有效，并且在低数据环境下也表现出有效性。

Conclusion: BrokenBind突破了传统多模态学习的局限性，实现了跨数据集的模态绑定，为通用模态探索提供了新途径，显著提高了多模态学习的灵活性和可扩展性。

Abstract: Multi-modal learning combines various modalities to provide a comprehensive understanding of real-world problems. A common strategy is to directly bind different modalities together in a specific joint embedding space. However, the capability of existing methods is restricted within the modalities presented in the given dataset, thus they are biased when generalizing to unpresented modalities in downstream tasks. As a result, due to such inflexibility, the viability of previous methods is seriously hindered by the cost of acquiring multi-modal datasets. In this paper, we introduce BrokenBind, which focuses on binding modalities that are presented from different datasets. To achieve this, BrokenBind simultaneously leverages multiple datasets containing the modalities of interest and one shared modality. Though the two datasets do not correspond to each other due to distribution mismatch, we can capture their relationship to generate pseudo embeddings to fill in the missing modalities of interest, enabling flexible and generalized multi-modal learning. Under our framework, any two modalities can be bound together, free from the dataset limitation, to achieve universal modality exploration. Further, to reveal the capability of our method, we study intensified scenarios where more than two datasets are needed for modality binding and show the effectiveness of BrokenBind in low-data regimes. Through extensive evaluation, we carefully justify the superiority of BrokenBind compared to well-known multi-modal baseline methods.

</details>


### [85] [On the Plasticity and Stability for Post-Training Large Language Models](https://arxiv.org/abs/2602.06453)
*Wenwen Qiang,Ziyin Gu,Jiahuan Zhou,Jie Hu,Jingyao Wang,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 提出PCR方法解决GRPO训练稳定性问题，通过贝叶斯框架建模梯度为随机变量，实现不确定性感知的软投影机制


<details>
  <summary>Details</summary>
Motivation: Group Relative Policy Optimization (GRPO) 的训练稳定性是关键瓶颈，表现为推理可塑性与通用能力保持之间的权衡。根本原因是可塑性与稳定性梯度之间的几何冲突导致破坏性干扰，而确定性投影方法忽略了基于群体的梯度估计的内在随机性

Method: 提出概率冲突解决(PCR)方法，这是一个贝叶斯框架，将梯度建模为随机变量。PCR通过不确定性感知的"软投影"机制动态仲裁冲突，优化信噪比

Result: 大量实验表明，PCR显著平滑了训练轨迹，并在各种推理任务中实现了优越性能

Conclusion: PCR方法有效解决了GRPO训练稳定性问题，通过概率建模和软投影机制优化梯度冲突，提升了训练效率和性能

Abstract: Training stability remains a critical bottleneck for Group Relative Policy Optimization (GRPO), often manifesting as a trade-off between reasoning plasticity and general capability retention. We identify a root cause as the geometric conflict between plasticity and stability gradients, which leads to destructive interference. Crucially, we argue that deterministic projection methods are suboptimal for GRPO as they overlook the intrinsic stochasticity of group-based gradient estimates. To address this, we propose Probabilistic Conflict Resolution (PCR), a Bayesian framework that models gradients as random variables. PCR dynamically arbitrates conflicts via an uncertainty-aware ``soft projection'' mechanism, optimizing the signal-to-noise ratio. Extensive experiments demonstrate that PCR significantly smooths the training trajectory and achieves superior performance in various reasoning tasks.

</details>


### [86] [The Window Dilemma: Why Concept Drift Detection is Ill-Posed](https://arxiv.org/abs/2602.06456)
*Brandon Gower-Winter,Misja Groen,Georg Krempl*

Main category: cs.LG

TL;DR: 传统概念漂移检测器存在窗口困境问题，漂移检测本身是病态问题，批量学习方法通常优于漂移感知方法


<details>
  <summary>Details</summary>
Motivation: 数据流中的概念漂移现象已被广泛研究，但现有漂移检测器主要基于窗口比较方法，存在窗口困境问题，且漂移检测在实践中难以验证，需要重新评估漂移检测器的实际价值

Method: 提出窗口困境概念，通过示例说明漂移感知是窗口化的产物而非底层数据生成过程；实证比较漂移检测器与多种替代适应策略，包括传统批量学习技术

Result: 传统批量学习技术通常优于漂移感知方法，这进一步质疑了漂移检测器在流分类中的实际作用

Conclusion: 概念漂移检测存在根本性问题，窗口困境表明感知到的漂移可能是窗口化的产物而非真实变化，漂移检测在实践中难以验证，传统学习方法可能更有效

Abstract: Non-stationarity of an underlying data generating process that leads to distributional changes over time is a key characteristic of Data Streams. This phenomenon, commonly referred to as Concept Drift, has been intensively studied, and Concept Drift Detectors have been established as a class of methods for detecting such changes (drifts). For the most part, Drift Detectors compare regions (windows) of the data stream and detect drift if those windows are sufficiently dissimilar.
  In this work, we introduce the Window Dilemma, an observation that perceived drift is a product of windowing and not necessarily the underlying data generating process. Additionally, we highlight that drift detection is ill-posed, primarily because verification of drift events are implausible in practice. We demonstrate these contributions first by an illustrative example, followed by empirical comparisons of drift detectors against a variety of alternative adaptation strategies. Our main finding is that traditional batch learning techniques often perform better than their drift-aware counterparts further bringing into question the purpose of detectors in Stream Classification.

</details>


### [87] [Achieving Better Local Regret Bound for Online Non-Convex Bilevel Optimization](https://arxiv.org/abs/2602.06457)
*Tingkai Jia,Haiguang Wang,Cheng Chen*

Main category: cs.LG

TL;DR: 该论文建立了在线双层优化的最优遗憾界，提出了针对标准双层局部遗憾和窗口平均双层局部遗憾的优化算法。


<details>
  <summary>Details</summary>
Motivation: 现有在线双层优化算法虽然能最小化标准双层局部遗憾或窗口平均双层局部遗憾，但其遗憾界的最优性尚不明确，需要建立最优遗憾界。

Method: 针对标准双层局部遗憾，提出了达到最优遗憾Ω(1+V_T)的算法，最多需要O(T log T)次内层梯度评估；还开发了完全单循环算法，其遗憾界包含额外的梯度变化项。针对窗口平均双层局部遗憾，设计了通过窗口分析捕捉亚线性环境变化的算法。

Result: 建立了两种设置下的最优遗憾界：标准双层局部遗憾达到Ω(1+V_T)，窗口平均双层局部遗憾达到Ω(T/W^2)。实验验证了理论发现并证明了方法的实际有效性。

Conclusion: 该工作为在线双层优化建立了最优遗憾界，提出的算法在理论和实践上都表现出色，为相关机器学习问题提供了更优的解决方案。

Abstract: Online bilevel optimization (OBO) has emerged as a powerful framework for many machine learning problems. Prior works have developed several algorithms that minimize the standard bilevel local regret or the window-averaged bilevel local regret of the OBO problem, but the optimality of existing regret bounds remains unclear. In this work, we establish optimal regret bounds for both settings. For standard bilevel local regret, we propose an algorithm that achieves the optimal regret $Ω(1+V_T)$ with at most $O(T\log T)$ total inner-level gradient evaluations. We further develop a fully single-loop algorithm whose regret bound includes an additional gradient-variation terms. For the window-averaged bilevel local regret, we design an algorithm that captures sublinear environmental variation through a window-based analysis and achieves the optimal regret $Ω(T/W^2)$. Experiments validate our theoretical findings and demonstrate the practical effectiveness of the proposed methods.

</details>


### [88] [Towards Generalizable Reasoning: Group Causal Counterfactual Policy Optimization for LLM Reasoning](https://arxiv.org/abs/2602.06475)
*Jingyao Wang,Peizheng Guo,Wenwen Qiang,Jiahuan Zhou,Huijie Guo,Changwen Zheng,Hui Xiong*

Main category: cs.LG

TL;DR: 提出Group Causal Counterfactual Policy Optimization方法，通过反事实推理奖励机制训练LLMs学习可泛化的推理模式，解决现有奖励机制只关注最终答案正确性而忽略推理过程质量的问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机制过于关注最终答案正确性，忽略了推理过程的质量：合理的推理但答案错误会得到低分，而逻辑有缺陷的幸运猜测却可能获得高分，这影响了推理的泛化能力。

Method: 从因果视角将多候选推理解释为反事实实验，提出Group Causal Counterfactual Policy Optimization方法。该方法设计了一个情景式因果反事实奖励，同时捕捉：(1)鲁棒性：鼓励推理步骤在反事实扰动下保持稳定的答案分布；(2)有效性：确保足够的变异性使学习到的推理策略能够跨问题迁移。然后从该奖励构建token级优势并优化策略。

Result: 在多个基准测试上的广泛实验证明了该方法的优势。

Conclusion: 通过反事实推理奖励机制，能够训练LLMs学习过程有效且反事实鲁棒的推理模式，提高推理的泛化能力。

Abstract: Large language models (LLMs) excel at complex tasks with advances in reasoning capabilities. However, existing reward mechanisms remain tightly coupled to final correctness and pay little attention to the underlying reasoning process: trajectories with sound reasoning but wrong answers receive low credit, while lucky guesses with flawed logic may be highly rewarded, affecting reasoning generalization. From a causal perspective, we interpret multi-candidate reasoning for a fixed question as a family of counterfactual experiments with theoretical supports. Building on this, we propose Group Causal Counterfactual Policy Optimization to explicitly train LLMs to learn generalizable reasoning patterns. It proposes an episodic causal counterfactual reward that jointly captures (i) robustness, encouraging the answer distribution induced by a reasoning step to remain stable under counterfactual perturbations; and (ii) effectiveness, enforcing sufficient variability so that the learned reasoning strategy can transfer across questions. We then construct token-level advantages from this reward and optimize the policy, encouraging LLMs to favor reasoning patterns that are process-valid and counterfactually robust. Extensive experiments on diverse benchmarks demonstrate its advantages.

</details>


### [89] [Adaptive Uncertainty-Aware Tree Search for Robust Reasoning](https://arxiv.org/abs/2602.06493)
*Zeen Song,Zihao Ma,Wenwen Qiang,Changwen Zheng,Gang Hua*

Main category: cs.LG

TL;DR: 提出不确定性感知树搜索（UATS）方法，通过蒙特卡洛Dropout估计不确定性，使用强化学习控制器动态分配计算资源，解决过程奖励模型在分布外样本上的不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂问题解决中采用外部搜索和过程奖励模型，但PRMs在面对分布外推理路径时存在认知不确定性，导致评分不可靠，影响推理效果。

Method: 提出不确定性感知树搜索（UATS）：1）使用蒙特卡洛Dropout估计不确定性；2）采用基于强化学习的控制器动态分配计算预算；3）建立理论框架证明不确定性感知策略可实现次线性遗憾。

Result: 实验证明UATS能有效缓解分布外错误的影响，相比标准搜索方法，不确定性感知策略在理论分析和实证结果上都表现出更好的性能。

Conclusion: 通过系统分析PRMs的分布外不确定性挑战，提出UATS方法，结合不确定性估计和动态计算分配，显著提升了推理时搜索的鲁棒性和效果。

Abstract: Inference-time reasoning scaling has significantly advanced the capabilities of Large Language Models (LLMs) in complex problem-solving. A prevalent approach involves external search guided by Process Reward Models (PRMs). However, a fundamental limitation of this framework is the epistemic uncertainty of PRMs when evaluating reasoning paths that deviate from their training distribution. In this work, we conduct a systematic analysis of this challenge. We first provide empirical evidence that PRMs exhibit high uncertainty and unreliable scoring on out-of-distribution (OOD) samples. We then establish a theoretical framework proving that while standard search incurs linear regret accumulation, an uncertainty-aware strategy can achieve sublinear regret. Motivated by these findings, we propose Uncertainty-Aware Tree Search (UATS), a unified method that estimates uncertainty via Monte Carlo Dropout and dynamically allocates compute budget using a reinforcement learning-based controller. Extensive experiments demonstrate that our approach effectively mitigates the impact of OOD errors.

</details>


### [90] [Can Microcanonical Langevin Dynamics Leverage Mini-Batch Gradient Noise?](https://arxiv.org/abs/2602.06500)
*Emanuel Sommer,Kangning Diao,Jakob Robnik,Uros Seljak,David Rügamer*

Main category: cs.LG

TL;DR: 本文提出了一种新的随机梯度微正则朗之万集成（SMILE）采样器，通过梯度噪声预调节和自适应步长调谐器，解决了随机梯度微正则动力学中的偏差和不稳定性问题，实现了大规模贝叶斯推理的高效采样。


<details>
  <summary>Details</summary>
Motivation: 微正则朗之万蒙特卡洛方法在贝叶斯深度学习中的高维模型推理方面表现出色，但其依赖全数据集梯度使其在大规模问题上计算成本过高。本文旨在研究微正则动力学能否有效利用小批量梯度噪声，解决其在大规模应用中的可扩展性问题。

Method: 1）建立了随机梯度微正则动力学的连续时间理论分析框架；2）揭示了各向异性梯度噪声导致的偏差和高维后验中的数值不稳定性两种失效模式；3）提出了梯度噪声预调节方案以减少偏差；4）开发了基于能量方差的自适应调谐器来自动选择步长并动态设置数值保护；5）结合集成技术构建了随机微正则朗之万集成（SMILE）采样器。

Result: 提出的算法在贝叶斯神经网络等高维推理任务上实现了最先进的性能，显著减少了随机梯度微正则动力学中的偏差，提高了数值稳定性，为大规模贝叶斯推理提供了一个鲁棒且可扩展的微正则蒙特卡洛采样器。

Conclusion: 本文成功解决了随机梯度微正则动力学中的关键挑战，通过理论分析和算法创新，开发了高效的SMILE采样器，为大规模贝叶斯深度学习推理开辟了新的研究方向，实现了微正则方法在大规模问题上的实际应用。

Abstract: Scaling inference methods such as Markov chain Monte Carlo to high-dimensional models remains a central challenge in Bayesian deep learning. A promising recent proposal, microcanonical Langevin Monte Carlo, has shown state-of-the-art performance across a wide range of problems. However, its reliance on full-dataset gradients makes it prohibitively expensive for large-scale problems. This paper addresses a fundamental question: Can microcanonical dynamics effectively leverage mini-batch gradient noise? We provide the first systematic study of this problem, establishing a novel continuous-time theoretical analysis of stochastic-gradient microcanonical dynamics. We reveal two critical failure modes: a theoretically derived bias due to anisotropic gradient noise and numerical instabilities in complex high-dimensional posteriors. To tackle these issues, we propose a principled gradient noise preconditioning scheme shown to significantly reduce this bias and develop a novel, energy-variance-based adaptive tuner that automates step size selection and dynamically informs numerical guardrails. The resulting algorithm is a robust and scalable microcanonical Monte Carlo sampler that achieves state-of-the-art performance on challenging high-dimensional inference tasks like Bayesian neural networks. Combined with recent ensemble techniques, our work unlocks a new class of stochastic microcanonical Langevin ensemble (SMILE) samplers for large-scale Bayesian inference.

</details>


### [91] [Evolutionary Generation of Multi-Agent Systems](https://arxiv.org/abs/2602.06511)
*Yuntong Hu,Matthew Trager,Yuting Zhang,Yi Zhang,Shuo Yang,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: EvoMAS：一种基于进化算法的多智能体系统自动生成框架，通过结构化配置空间中的进化生成，显著提升任务性能、可执行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统设计方法存在两大问题：1）基于代码生成的方法导致可执行性和鲁棒性问题；2）基于固定模板的方法限制了表达能力和适应性。需要一种更有效、更通用的自动MAS生成方法。

Method: EvoMAS将MAS生成视为结构化配置生成问题，在配置空间中进行进化生成。具体包括：从候选池中选择初始配置，基于执行轨迹指导的反馈条件变异和交叉操作，迭代优化候选池和经验记忆。

Result: 在BBEH推理、SWE-Bench软件工程和WorkBench工具使用等多个基准测试中，EvoMAS均优于人工设计的MAS和现有自动生成方法。在BBEH上比EvoAgent提升10.5分，在WorkBench上提升7.1分，在SWE-Bench-Verified上达到79.1%，与排行榜顶尖水平相当。

Conclusion: EvoMAS通过进化生成方法有效解决了MAS自动生成的可执行性、鲁棒性和泛化性问题，为复杂任务的多智能体系统设计提供了高效、自适应的解决方案。

Abstract: Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.

</details>


### [92] [Topography scanning as a part of process monitoring in power cable insulation process](https://arxiv.org/abs/2602.06519)
*Janne Harjuhahto,Jaakko Harjuhahto,Mikko Lahti,Jussi Hanhirova,Björn Sonerud*

Main category: cs.LG

TL;DR: 提出用于XLPE电缆芯监测的新型地形扫描系统，结合现代测量技术与嵌入式高性能计算构建完整3D表面图，利用深度学习进行表面缺陷检测


<details>
  <summary>Details</summary>
Motivation: 开发用于XLPE电缆芯监测的系统，研究截面和纵向几何误差，识别熔体均匀性作为主要误差因素，实现实时表面缺陷检测

Method: 结合现代测量技术与嵌入式高性能计算构建完整3D表面地形图，开发基于卷积神经网络的深度学习表面缺陷检测系统

Result: 卷积神经网络非常适合实时分析表面测量数据，能够可靠检测表面缺陷

Conclusion: 提出的地形扫描系统结合深度学习检测方法为XLPE电缆芯监测提供了有效的解决方案

Abstract: We present a novel topography scanning system developed to XLPE cable core monitoring. Modern measurement technology is utilized together with embedded high-performance computing to build a complete and detailed 3D surface map of the insulated core. Cross sectional and lengthwise geometry errors are studied, and melt homogeneity is identified as one major factor for these errors. A surface defect detection system has been developed utilizing deep learning methods. Our results show that convolutional neural networks are well suited for real time analysis of surface measurement data enabling reliable detection of surface defects.

</details>


### [93] [Live Knowledge Tracing: Real-Time Adaptation using Tabular Foundation Models](https://arxiv.org/abs/2602.06542)
*Mounir Lbath,Alexandre Paresy,Abdelkayoum Kaddouri,Alan André,Alexandre Ittah,Jill-Jênn Vie*

Main category: cs.LG

TL;DR: 提出基于表格基础模型（TFMs）的实时知识追踪新范式，通过双向注意力机制在推理时对齐测试序列与相关训练序列，无需训练步骤，实现高达273倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度知识追踪模型需要大量训练时间，且在短序列数据集上容易过拟合。需要一种更高效、实时的知识追踪方法。

Method: 采用表格基础模型（TFMs）进行在线实时知识追踪。核心是双向注意力机制：不仅关注时间步，还同时关注训练集中其他学生的交互。在推理时直接将测试序列与相关训练序列对齐，完全跳过训练步骤。

Result: 在多个不同规模的数据集上，该方法实现了与现有方法相当的预测性能，同时获得了高达273倍的加速效果，特别是在观察到更多学生交互时表现更优。

Conclusion: 基于TFMs的实时知识追踪范式提供了一种高效替代传统深度知识追踪模型的方法，通过跳过训练步骤实现显著的速度提升，同时保持竞争性的预测性能。

Abstract: Deep knowledge tracing models have achieved significant breakthroughs in modeling student learning trajectories. However, these architectures require substantial training time and are prone to overfitting on datasets with short sequences. In this paper, we explore a new paradigm for knowledge tracing by leveraging tabular foundation models (TFMs). Unlike traditional methods that require offline training on a fixed training set, our approach performs real-time ''live'' knowledge tracing in an online way. The core of our method lies in a two-way attention mechanism: while attention knowledge tracing models only attend across earlier time steps, TFMs simultaneously attend across both time steps and interactions of other students in the training set. They align testing sequences with relevant training sequences at inference time, therefore skipping the training step entirely. We demonstrate, using several datasets of increasing size, that our method achieves competitive predictive performance with up to 273x speedups, in a setting where more student interactions are observed over time.

</details>


### [94] [Refining the Information Bottleneck via Adversarial Information Separation](https://arxiv.org/abs/2602.06549)
*Shuai Ning,Zhenpeng Wang,Lin Wang,Bing Chen,Shuangrong Liu,Xu Wu,Jin Zhou,Bo Yang*

Main category: cs.LG

TL;DR: AdverISF：无需显式监督的自监督对抗信息分离框架，用于在数据稀缺场景中分离任务相关特征与噪声，提升泛化性能


<details>
  <summary>Details</summary>
Motivation: 在材料科学等实验数据领域，任务相关特征常被测量噪声和实验伪影严重混淆，现有正则化方法无法精确分离特征与噪声，而对抗适应方法又依赖于显式分离标签

Method: 提出AdverISF框架：1）通过自监督对抗机制强制任务相关特征与噪声表示之间的统计独立性；2）采用多层分离架构，在特征层次结构中渐进回收被误判为噪声的信息，实现更细粒度的特征提取

Result: 在数据稀缺场景中优于现有最先进方法，在真实世界材料设计任务中展现出优越的泛化性能

Conclusion: AdverISF能够在无需显式监督的情况下有效分离任务相关特征与噪声，特别适用于数据稀缺且特征被噪声严重混淆的领域，如材料科学

Abstract: Generalizing from limited data is particularly critical for models in domains such as material science, where task-relevant features in experimental datasets are often heavily confounded by measurement noise and experimental artifacts. Standard regularization techniques fail to precisely separate meaningful features from noise, while existing adversarial adaptation methods are limited by their reliance on explicit separation labels. To address this challenge, we propose the Adversarial Information Separation Framework (AdverISF), which isolates task-relevant features from noise without requiring explicit supervision. AdverISF introduces a self-supervised adversarial mechanism to enforce statistical independence between task-relevant features and noise representations. It further employs a multi-layer separation architecture that progressively recycles noise information across feature hierarchies to recover features inadvertently discarded as noise, thereby enabling finer-grained feature extraction. Extensive experiments demonstrate that AdverISF outperforms state-of-the-art methods in data-scarce scenarios. In addition, evaluations on real-world material design tasks show that it achieves superior generalization performance.

</details>


### [95] [Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion](https://arxiv.org/abs/2602.06550)
*Jan Benad,Pradeep Kr. Banerjee,Frank Röder,Nihat Ay,Martin V. Butz,Manfred Eppe*

Main category: cs.LG

TL;DR: DMA*-SH：通过超网络生成共享适配器权重，解决潜在上下文强化学习中的执行器反转问题，实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 解决上下文强化学习中执行器反转的核心挑战，即相同动作在不同潜在二元上下文下产生相反的物理效果，这是零样本泛化的主要障碍

Method: 提出DMA*-SH框架：使用单一超网络仅通过动态预测训练，生成少量适配器权重，在动态模型、策略和动作价值函数之间共享调制；通过输入/输出归一化和随机输入掩码稳定上下文推断

Result: 在Actuator Inversion Benchmark测试中，DMA*-SH在保留的执行器反转任务上实现零样本泛化，性能超过领域随机化111.8%，超过标准上下文感知基线16.1%

Conclusion: 共享超网络调制为执行器反转提供了匹配的归纳偏置，通过理论分析和实验验证证明了该方法在解决潜在上下文强化学习核心挑战方面的有效性

Abstract: Zero-shot generalization in contextual reinforcement learning remains a core challenge, particularly when the context is latent and must be inferred from data. A canonical failure mode is actuator inversion, where identical actions produce opposite physical effects under a latent binary context. We propose DMA*-SH, a framework where a single hypernetwork, trained solely via dynamics prediction, generates a small set of adapter weights shared across the dynamics model, policy, and action-value function. This shared modulation imparts an inductive bias matched to actuator inversion, while input/output normalization and random input masking stabilize context inference, promoting directionally concentrated representations. We provide theoretical support via an expressivity separation result for hypernetwork modulation, and a variance decomposition with policy-gradient variance bounds that formalize how within-mode compression improves learning under actuator inversion. For evaluation, we introduce the Actuator Inversion Benchmark (AIB), a suite of environments designed to isolate discontinuous context-to-dynamics interactions. On AIB's held-out actuator-inversion tasks, DMA*-SH achieves zero-shot generalization, outperforming domain randomization by 111.8% and surpassing a standard context-aware baseline by 16.1%.

</details>


### [96] [Fine-Grained Model Merging via Modular Expert Recombination](https://arxiv.org/abs/2602.06552)
*Haiyun Qiu,Xingyu Wu,Liang Feng,Kay Chen Tan*

Main category: cs.LG

TL;DR: MERGE提出了一种模块化专家重组方法，通过组件级模型合并和输入感知的模块重组，解决了现有实例特定模型合并方法缺乏可重用性和忽略组件异质性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有实例特定模型合并方法存在两个关键限制：1) 缺乏可重用性，无法利用高质量合并配置和高效批量推理；2) 将任务特定模型视为整体，忽略了注意力层、MLP层等同源组件的不同可合并性和合并敏感性。

Method: MERGE将组件级合并建模为双目标优化问题（平衡跨任务性能和存储效率），使用代理辅助进化算法寻找帕累托最优合并配置，构建可重用模块专家库，并通过轻量级路由网络动态激活重组模块专家来组装输入特定模型。

Result: 在不同模型规模、任务类型和微调策略的广泛实验中，MERGE始终优于强基线方法，并展现出良好的泛化能力。

Conclusion: MERGE通过组件级合并和输入感知模块重组，实现了高效、可重用的模型合并，解决了现有方法的局限性，为异构任务集成提供了新思路。

Abstract: Model merging constructs versatile models by integrating task-specific models without requiring labeled data or expensive joint retraining. Although recent methods improve adaptability to heterogeneous tasks by generating customized merged models for each instance, they face two critical limitations. First, the instance-specific merged models lack reusability, restricting the exploitation of high-quality merging configurations and efficient batch inference. Second, these methods treat each task-specific model as a monolithic whole, overlooking the diverse mergeability of homologous components such as attention and multilayer perceptron layers, and the differing merging sensitivities across components. To address these limitations, we propose MERGE (\underline{M}odular \underline{E}xpert \underline{R}ecombination for fine-\underline{G}rained m\underline{E}rging), a method that enables component-wise model merging and input-aware, on-demand module recombination at inference. MERGE formulates component-wise merging as a bi-objective optimization problem that balances cross-task performance and storage efficiency, and develops a surrogate-assisted evolutionary algorithm to efficiently identify Pareto-optimal merging configurations. These high-quality configurations underpin a reusable modular expert library, from which a lightweight routing network dynamically activates and recombines modular experts to assemble input-specific models and enable efficient inference under storage constraints. Extensive experiments across various model scales, task types, and fine-tuning strategies demonstrate that MERGE consistently outperforms strong baselines and generalizes effectively.

</details>


### [97] [Which Graph Shift Operator? A Spectral Answer to an Empirical Question](https://arxiv.org/abs/2602.06557)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 提出基于对齐增益指标的图神经网络图移位算子选择方法，无需训练即可为预测任务选择最优算子


<details>
  <summary>Details</summary>
Motivation: 图神经网络中图移位算子的选择通常依赖经验，缺乏理论指导，需要一种原则性的选择方法

Method: 引入对齐增益指标量化输入信号与标签子空间之间的几何失真，通过谱代理连接Lipschitz常数与泛化边界

Result: 开发出计算高效的准则，可在训练前为任何预测任务排序和选择最优图移位算子

Conclusion: 提供了一种理论指导的图移位算子选择框架，消除了广泛搜索的需要，提高了图神经网络设计的效率

Abstract: Graph Neural Networks (GNNs) have established themselves as the leading models for learning on graph-structured data, generally categorized into spatial and spectral approaches. Central to these architectures is the Graph Shift Operator (GSO), a matrix representation of the graph structure used to filter node signals. However, selecting the optimal GSO, whether fixed or learnable, remains largely empirical. In this paper, we introduce a novel alignment gain metric that quantifies the geometric distortion between the input signal and label subspaces. Crucially, our theoretical analysis connects this alignment directly to generalization bounds via a spectral proxy for the Lipschitz constant. This yields a principled, computation-efficient criterion to rank and select the optimal GSO for any prediction task prior to training, eliminating the need for extensive search.

</details>


### [98] [Learning to Allocate Resources with Censored Feedback](https://arxiv.org/abs/2602.06565)
*Giovanni Montanari,Côme Fiegel,Corentin Pla,Aadirupa Saha,Vianney Perchet*

Main category: cs.LG

TL;DR: 在线资源分配问题：每轮需将预算B分配给K个臂，在截断反馈下最大化累积奖励，提出RA-UCB和MG-UCB算法分别处理已知和未知预算情况。


<details>
  <summary>Details</summary>
Motivation: 研究在线资源分配问题，其中每个臂的奖励需要满足两个条件：(1) 臂被激活（伯努利随机变量），(2) 分配的预算超过随机阈值。面临未知参数估计和探索-利用权衡的挑战。

Method: 提出RA-UCB算法（已知预算），利用非平凡参数估计和置信界；提出MG-UCB算法（未知预算），允许轮内切换和无穷小分配。

Result: 证明信息论遗憾下界Ω(T^{1/3})；RA-UCB在已知预算时达到Õ(√T)遗憾，在更强假设下可达O(poly-log T)；MG-UCB在未知预算时匹配RA-UCB的遗憾保证。

Conclusion: 该问题具有内在难度，但提出的算法能有效处理已知和未知预算情况，理论结果在真实数据集上得到验证。

Abstract: We study the online resource allocation problem in which at each round, a budget $B$ must be allocated across $K$ arms under censored feedback. An arm yields a reward if and only if two conditions are satisfied: (i) the arm is activated according to an arm-specific Bernoulli random variable with unknown parameter, and (ii) the allocated budget exceeds a random threshold drawn from a parametric distribution with unknown parameter. Over $T$ rounds, the learner must jointly estimate the unknown parameters and allocate the budget so as to maximize cumulative reward facing the exploration--exploitation trade-off. We prove an information-theoretic regret lower bound $Ω(T^{1/3})$, demonstrating the intrinsic difficulty of the problem. We then propose RA-UCB, an optimistic algorithm that leverages non-trivial parameter estimation and confidence bounds. When the budget $B$ is known at the beginning of each round, RA-UCB achieves a regret of order $\widetilde{\mathcal{O}}(\sqrt{T})$, and even $\mathcal{O}(\mathrm{poly}\text{-}\log T)$ under stronger assumptions. As for unknown, round dependent budget, we introduce MG-UCB, which allows within-round switching and infinitesimal allocations, and matches the regret guarantees of RA-UCB. We then validate our theoretical results through experiments on real-world datasets.

</details>


### [99] [Transformer-based Parameter Fitting of Models derived from Bloch-McConnell Equations for CEST MRI Analysis](https://arxiv.org/abs/2602.06574)
*Christof Duhme,Chris Lippe,Verena Hoerr,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出基于Transformer的神经网络，用于从CEST MRI数据中拟合代谢物浓度、交换速率等物理模型参数，性能优于传统梯度求解器。


<details>
  <summary>Details</summary>
Motivation: CEST MRI是一种非侵入性代谢物检测成像技术，比传统MRS具有更高分辨率和灵敏度。但CEST数据量化困难，因为测量信号来自多种生理变量的复杂相互作用。

Method: 引入基于Transformer的神经网络，从体外CEST光谱中拟合从Bloch-McConnell方程导出的物理模型参数（代谢物浓度、交换速率、弛豫速率等），采用自监督训练。

Result: 自监督训练的神经网络明显优于经典梯度求解器的解决方案。

Conclusion: Transformer神经网络为CEST MRI数据量化提供了一种有效方法，能够准确拟合复杂的物理模型参数，性能超越传统优化方法。

Abstract: Chemical exchange saturation transfer (CEST) MRI is a non-invasive imaging modality for detecting metabolites. It offers higher resolution and sensitivity compared to conventional magnetic resonance spectroscopy (MRS). However, quantification of CEST data is challenging because the measured signal results from a complex interplay of many physiological variables. Here, we introduce a transformer-based neural network to fit parameters such as metabolite concentrations, exchange and relaxation rates of a physical model derived from Bloch-McConnell equations to in-vitro CEST spectra. We show that our self-supervised trained neural network clearly outperforms the solution of classical gradient-based solver.

</details>


### [100] [Perturbing the Phase: Analyzing Adversarial Robustness of Complex-Valued Neural Networks](https://arxiv.org/abs/2602.06577)
*Florian Eilers,Christof Duhme,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 论文提出针对复数神经网络(CVNNs)的相位攻击方法，并推导复数版本的常见对抗攻击，发现CVNNs在某些场景下比实数神经网络(RVNNs)更鲁棒，但两者都对相位变化非常敏感


<details>
  <summary>Details</summary>
Motivation: 随着复数神经网络在各种应用中的普及，需要分析其对异常值的鲁棒性以确保安全使用。对抗攻击是理解深度神经网络行为的有效方法，但需要专门针对复数特性的攻击方法

Method: 设计了专门针对复数输入相位信息的"相位攻击"，并推导了复数版本的常用对抗攻击方法

Result: 在某些场景下CVNNs比RVNNs更鲁棒，但两者都对相位变化非常敏感。相位攻击比同等强度的常规攻击（可同时攻击相位和幅度）更能降低模型性能

Conclusion: 复数神经网络对相位扰动特别脆弱，需要专门的安全分析。相位攻击是评估CVNNs鲁棒性的有效工具，为复数神经网络的安全应用提供了重要见解

Abstract: Complex-valued neural networks (CVNNs) are rising in popularity for all kinds of applications. To safely use CVNNs in practice, analyzing their robustness against outliers is crucial. One well known technique to understand the behavior of deep neural networks is to investigate their behavior under adversarial attacks, which can be seen as worst case minimal perturbations. We design Phase Attacks, a kind of attack specifically targeting the phase information of complex-valued inputs. Additionally, we derive complex-valued versions of commonly used adversarial attacks. We show that in some scenarios CVNNs are more robust than RVNNs and that both are very susceptible to phase changes with the Phase Attacks decreasing the model performance more, than equally strong regular attacks, which can attack both phase and magnitude.

</details>


### [101] [Exploring Sparsity and Smoothness of Arbitrary $\ell_p$ Norms in Adversarial Attacks](https://arxiv.org/abs/2602.06578)
*Christof Duhme,Florian Eilers,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 该研究系统分析了ℓ_p范数约束下对抗攻击的结构特性，发现传统ℓ_1和ℓ_2范数并非最优，p∈[1.3,1.5]能在稀疏性和平滑性之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击通常基于ℓ_1、ℓ_2或ℓ_∞范数约束，但缺乏对范数参数p如何影响对抗扰动结构和感知特性的系统性研究。研究者希望探索p值选择对攻击稀疏性和平滑性的影响。

Method: 采用文献中两种稀疏性度量，并引入三种平滑性度量（包括基于平滑操作的一般框架和基于一阶泰勒近似的度量）。在多个真实图像数据集和不同模型架构（卷积网络和Transformer）上进行全面的实证评估，分析p∈[1,2]范围内对抗攻击的特性。

Result: 实验表明，ℓ_1或ℓ_2范数在大多数情况下并非最优选择，最优p值取决于具体任务。在p∈[1.3,1.5]范围内，对抗攻击能在稀疏性和平滑性之间取得最佳平衡。

Conclusion: 范数选择对对抗攻击的设计和评估至关重要，传统ℓ_1和ℓ_2范数约束存在局限性，应根据具体任务需求选择更合适的p值以获得更好的攻击特性。

Abstract: Adversarial attacks against deep neural networks are commonly constructed under $\ell_p$ norm constraints, most often using $p=1$, $p=2$ or $p=\infty$, and potentially regularized for specific demands such as sparsity or smoothness. These choices are typically made without a systematic investigation of how the norm parameter \( p \) influences the structural and perceptual properties of adversarial perturbations. In this work, we study how the choice of \( p \) affects sparsity and smoothness of adversarial attacks generated under \( \ell_p \) norm constraints for values of $p \in [1,2]$. To enable a quantitative analysis, we adopt two established sparsity measures from the literature and introduce three smoothness measures. In particular, we propose a general framework for deriving smoothness measures based on smoothing operations and additionally introduce a smoothness measure based on first-order Taylor approximations. Using these measures, we conduct a comprehensive empirical evaluation across multiple real-world image datasets and a diverse set of model architectures, including both convolutional and transformer-based networks. We show that the choice of $\ell_1$ or $\ell_2$ is suboptimal in most cases and the optimal $p$ value is dependent on the specific task. In our experiments, using $\ell_p$ norms with $p\in [1.3, 1.5]$ yields the best trade-off between sparse and smooth attacks. These findings highlight the importance of principled norm selection when designing and evaluating adversarial attacks.

</details>


### [102] [Target noise: A pre-training based neural network initialization for efficient high resolution learning](https://arxiv.org/abs/2602.06585)
*Shaowen Wang,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 提出一种基于随机噪声自监督预训练的权重初始化方法，通过让网络拟合随机噪声来获得结构化参数配置，显著提升后续任务的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有初始化方法（如Xavier、Kaiming）依赖随机采样，未利用优化过程信息。特别是隐式神经表示（INRs）和深度图像先验（DIP）网络在优化中存在强烈的低频偏置，导致收敛缓慢。

Method: 使用随机噪声作为目标进行自监督预训练：首先预训练网络拟合随机噪声，获得结构化参数配置，然后将这些参数作为后续任务的初始化权重。

Result: 噪声预训练显著提升收敛速度，使网络能更早捕获高频分量，优化更稳定。特别适用于INRs和DIP网络，无需额外数据或架构修改。

Conclusion: 噪声预训练提供了一种轻量级、通用的替代传统随机初始化的方法，能更高效地优化深度神经网络，尽管随机噪声不含语义信息，但其白谱特性可作为有效的自监督信号。

Abstract: Weight initialization plays a crucial role in the optimization behavior and convergence efficiency of neural networks. Most existing initialization methods, such as Xavier and Kaiming initializations, rely on random sampling and do not exploit information from the optimization process itself. We propose a simple, yet effective, initialization strategy based on self-supervised pre-training using random noise as the target. Instead of directly training the network from random weights, we first pre-train it to fit random noise, which leads to a structured and non-random parameter configuration. We show that this noise-driven pre-training significantly improves convergence speed in subsequent tasks, without requiring additional data or changes to the network architecture. The proposed method is particularly effective for implicit neural representations (INRs) and Deep Image Prior (DIP)-style networks, which are known to exhibit a strong low-frequency bias during optimization. After noise-based pre-training, the network is able to capture high-frequency components much earlier in training, leading to faster and more stable convergence. Although random noise contains no semantic information, it serves as an effective self-supervised signal (considering its white spectrum nature) for shaping the initialization of neural networks. Overall, this work demonstrates that noise-based pre-training offers a lightweight and general alternative to traditional random initialization, enabling more efficient optimization of deep neural networks.

</details>


### [103] [DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters](https://arxiv.org/abs/2602.06597)
*Haoran Zhang,Haixuan Liu,Yong Liu,Yunzhong Qiu,Yuxuan Wang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: DiTS提出了一种用于时间序列的扩散Transformer架构，通过双流注意力机制分别建模时间维度和变量维度依赖，在概率预测任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式时间序列模型未能充分利用多维时间序列数据的特性，特别是变量间的依赖关系。当前主流的扩散Transformer架构使用简单的条件控制和单流Transformer主干，在协变量感知预测中未能充分利用跨变量依赖。

Method: 提出DiTS架构，将内生变量和外生变量视为不同模态。设计双流Transformer块：时间注意力模块用于时间维度自回归建模，变量注意力模块用于跨变量建模。利用多元依赖的低秩特性降低计算成本。

Result: DiTS在多个基准测试中达到最先进的性能，无论是否存在未来外生变量观测，都展现出比传统确定性深度预测模型更强的生成式预测能力。

Conclusion: DiTS通过将时间序列建模为多模态问题并设计专门的双流注意力机制，有效捕获了时间序列中的时间和变量维度依赖，为生成式时间序列预测提供了强大而灵活的架构。

Abstract: While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.

</details>


### [104] [The hidden risks of temporal resampling in clinical reinforcement learning](https://arxiv.org/abs/2602.06603)
*Thomas Frost,Hrisheekesh Vaidya,Steve Harris*

Main category: cs.LG

TL;DR: 离线强化学习在医疗应用中，将患者数据重采样为固定时间间隔会显著降低算法在实际部署中的性能，而标准离线评估指标可能无法检测到这种性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域的离线强化学习研究通常将患者数据聚合到固定时间间隔，以适应标准ORL框架，但这种时间操纵对模型安全性和有效性的影响尚不清楚。

Method: 使用网格世界导航任务和UVA/Padova临床糖尿病模拟器，分析时间重采样对离线强化学习算法性能的影响，并提出三种导致失败的机制。

Result: 时间重采样显著降低了离线强化学习算法在实际部署中的性能，标准离线策略评估指标可能无法检测到这种性能下降。

Conclusion: 当前医疗ORL流程存在根本性风险，需要开发能够显式处理临床决策不规则时间特性的方法。

Abstract: Offline reinforcement learning (ORL) has shown potential for improving decision-making in healthcare. However, contemporary research typically aggregates patient data into fixed time intervals, simplifying their mapping to standard ORL frameworks. The impact of these temporal manipulations on model safety and efficacy remains poorly understood. In this work, using both a gridworld navigation task and the UVA/Padova clinical diabetes simulator, we demonstrate that temporal resampling significantly degrades the performance of offline reinforcement learning algorithms during live deployment. We propose three mechanisms that drive this failure: (i) the generation of counterfactual trajectories, (ii) the distortion of temporal expectations, and (iii) the compounding of generalisation errors. Crucially, we find that standard off-policy evaluation metrics can fail to detect these drops in performance. Our findings reveal a fundamental risk in current healthcare ORL pipelines and emphasise the need for methods that explicitly handle the irregular timing of clinical decision-making.

</details>


### [105] [The challenge of generating and evolving real-life like synthetic test data without accessing real-world raw data -- a Systematic Review](https://arxiv.org/abs/2602.06609)
*Maj-Annika Tammisto,Faiz Ali Shah,Daniel Rodriguez,Dietmar Pfahl*

Main category: cs.LG

TL;DR: 本文系统综述了电子政务应用中隐私保护测试数据生成方法，发现现有方法大多需要访问真实数据进行匿名化或合成，仅有9种方法接近完全无需真实数据生成合成测试数据，且测试数据演化能力普遍缺乏。


<details>
  <summary>Details</summary>
Motivation: 电子政务应用（如跨国信息交换、医疗、银行等）需要真实但隐私安全的测试数据。现有方法通常需要访问真实数据进行匿名化或合成，但许多场景下无法获取原始数据，因此需要研究完全无需真实数据即可生成和演化合成测试数据的方法。

Method: 采用Kitchenham等知名系统文献综述方法学，在IEEE Xplore、ACM Digital Library和SCOPUS数据库中检索了1,013篇文献，最终从75篇中提取数据，识别出37种部分回答研究问题的方法。

Result: 发现多种隐私保护测试数据生成方法和工具，但大多需要直接访问真实数据进行匿名化或合成。仅识别出9种最接近完全无需真实数据生成合成测试数据的方法。现有方法普遍缺乏测试数据演化能力。

Conclusion: 现有研究均未完全满足需求，仅部分覆盖。合成测试数据演化是一个研究不足但重要的领域，特别是在数字政府解决方案中，随着各国新法规的实施，这一需求更加迫切。

Abstract: Background: High-level system testing of applications that use data from e-Government services as input requires test data that is real-life-like but where the privacy of personal information is guaranteed. Applications with such strong requirement include information exchange between countries, medicine, banking, etc. This review aims to synthesize the current state-of-the-practice in this domain.
  Objectives: The objective of this Systematic Review is to identify existing approaches for creating and evolving synthetic test data without using real-life raw data.
  Methods: We followed well-known methodologies for conducting systematic literature reviews, including the ones from Kitchenham as well as guidelines for analysing the limitations of our review and its threats to validity.
  Results: A variety of methods and tools exist for creating privacy-preserving test data. Our search found 1,013 publications in IEEE Xplore, ACM Digital Library, and SCOPUS. We extracted data from 75 of those publications and identified 37 approaches that answer our research question partly. A common prerequisite for using these methods and tools is direct access to real-life data for data anonymization or synthetic test data generation. Nine existing synthetic test data generation approaches were identified that were closest to answering our research question. Nevertheless, further work would be needed to add the ability to evolve synthetic test data to the existing approaches.
  Conclusions: None of the publications really covered our requirements completely, only partially. Synthetic test data evolution is a field that has not received much attention from researchers but needs to be explored in Digital Government Solutions, especially since new legal regulations are being placed in force in many countries.

</details>


### [106] [Adaptive-CaRe: Adaptive Causal Regularization for Robust Outcome Prediction](https://arxiv.org/abs/2602.06611)
*Nithya Bhasker,Fiona R. Kolbinger,Susu Hu,Gitta Kutyniok,Stefanie Speidel*

Main category: cs.LG

TL;DR: 提出Adaptive-CaRe正则化策略，平衡医学领域预测模型的准确性与因果鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统监督学习追求预测准确性但可能捕捉虚假相关，因果结构学习过于保守导致精度损失，需要平衡两者

Method: 提出模型无关的正则化策略Adaptive-CaRe，通过惩罚特征统计贡献与因果贡献的差异来平衡预测价值与因果鲁棒性

Result: 在合成数据上验证了Adaptive-CaRe能找到鲁棒预测因子并保持竞争力；在真实数据集上验证了实际应用效果

Conclusion: Adaptive-CaRe为医学领域长期存在的预测准确性与因果鲁棒性权衡提供了简单有效的解决方案

Abstract: Accurate prediction of outcomes is crucial for clinical decision-making and personalized patient care. Supervised machine learning algorithms, which are commonly used for outcome prediction in the medical domain, optimize for predictive accuracy, which can result in models latching onto spurious correlations instead of robust predictors. Causal structure learning methods on the other hand have the potential to provide robust predictors for the target, but can be too conservative because of algorithmic and data assumptions, resulting in loss of diagnostic precision. Therefore, we propose a novel model-agnostic regularization strategy, Adaptive-CaRe, for generalized outcome prediction in the medical domain. Adaptive-CaRe strikes a balance between both predictive value and causal robustness by incorporating a penalty that is proportional to the difference between the estimated statistical contribution and estimated causal contribution of the input features for model predictions. Our experiments on synthetic data establish the efficacy of the proposed Adaptive-CaRe regularizer in finding robust predictors for the target while maintaining competitive predictive accuracy. With experiments on a standard causal benchmark, we provide a blueprint for navigating the trade-off between predictive accuracy and causal robustness by tweaking the regularization strength, $λ$. Validation using real-world dataset confirms that the results translate to practical, real-domain settings. Therefore, Adaptive-CaRe provides a simple yet effective solution to the long-standing trade-off between predictive accuracy and causal robustness in the medical domain. Future work would involve studying alternate causal structure learning frameworks and complex classification models to provide deeper insights at a larger scale.

</details>


### [107] [Trust Regions Sell, But Who's Buying? Overlap Geometry as an Alternative Trust Region for Policy Optimization](https://arxiv.org/abs/2602.06627)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Yash Sinha,Pratik Narang,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: 提出基于分布重叠几何（Bhattacharyya系数）的信任区域方法替代KL散度，通过约束平方根比率来控制似然比尾部，提高训练稳定性


<details>
  <summary>Details</summary>
Motivation: 标准信任区域方法使用KL散度约束策略更新，但KL只控制平均差异，无法防止罕见的大似然比偏移导致训练不稳定，这正是PPO剪枝等启发式方法试图解决的问题

Method: 提出重叠几何作为替代信任区域，通过Bhattacharyya系数约束分布重叠，推导出BTRPO和BPPO算法，通过平方根比率更新强制执行重叠约束：BPPO剪枝平方根比率q=sqrt(r)，BTRPO应用二次Hellinger/Bhattacharyya惩罚

Result: 基于重叠的更新在匹配的训练预算下提高了鲁棒性和聚合性能（通过RLiable测量），表明重叠约束是KL散度的一个实用且有原则的替代方案

Conclusion: 重叠几何约束通过惩罚比率尾部分离，提供了对似然比偏移的更严格控制，是稳定策略优化的实用且有原则的替代方案

Abstract: Standard trust-region methods constrain policy updates via Kullback-Leibler (KL) divergence. However, KL controls only an average divergence and does not directly prevent rare, large likelihood-ratio excursions that destabilize training--precisely the failure mode that motivates heuristics such as PPO's clipping. We propose overlap geometry as an alternative trust region, constraining distributional overlap via the Bhattacharyya coefficient (closely related to the Hellinger/Renyi-1/2 geometry). This objective penalizes separation in the ratio tails, yielding tighter control over likelihood-ratio excursions without relying on total variation bounds that can be loose in tail regimes. We derive Bhattacharyya-TRPO (BTRPO) and Bhattacharyya-PPO (BPPO), enforcing overlap constraints via square-root ratio updates: BPPO clips the square-root ratio q = sqrt(r), and BTRPO applies a quadratic Hellinger/Bhattacharyya penalty. Empirically, overlap-based updates improve robustness and aggregate performance as measured by RLiable under matched training budgets, suggesting overlap constraints as a practical, principled alternative to KL for stable policy optimization.

</details>


### [108] [Temperature Scaling Attack Disrupting Model Confidence in Federated Learning](https://arxiv.org/abs/2602.06638)
*Kichang Lee,Jaeho Jin,JaeYeon Park,Songkuk Kim,JeongGil Ko*

Main category: cs.LG

TL;DR: 本文提出了一种针对联邦学习的温度缩放攻击（TSA），该攻击专门破坏模型置信度校准而不影响准确率，揭示了联邦学习中校准完整性是一个重要的攻击面。


<details>
  <summary>Details</summary>
Motivation: 预测置信度在关键任务系统中作为基础控制信号，直接影响风险感知逻辑（如升级、弃权和保守回退）。虽然先前的联邦学习攻击主要针对准确率或植入后门，但本文发现置信度校准是一个独特的攻击目标。

Method: 提出温度缩放攻击（TSA），通过在本地训练期间注入温度缩放与学习率-温度耦合，恶意更新保持类似良性优化的行为，从而规避基于准确率的监控和基于相似性的检测。

Result: 在三个基准测试中，TSA显著改变了校准（例如CIFAR-100上误差增加145%），而准确率变化小于2%。即使在鲁棒聚合和后处理校准防御下仍然有效。案例研究显示，置信度操纵可导致关键病例漏检增加7.2倍（医疗）或误报增加（自动驾驶）。

Conclusion: 研究结果表明，校准完整性是联邦学习中的一个关键攻击面，需要新的防御机制来保护置信度校准。

Abstract: Predictive confidence serves as a foundational control signal in mission-critical systems, directly governing risk-aware logic such as escalation, abstention, and conservative fallback. While prior federated learning attacks predominantly target accuracy or implant backdoors, we identify confidence calibration as a distinct attack objective. We present the Temperature Scaling Attack (TSA), a training-time attack that degrades calibration while preserving accuracy. By injecting temperature scaling with learning rate-temperature coupling during local training, malicious updates maintain benign-like optimization behavior, evading accuracy-based monitoring and similarity-based detection. We provide a convergence analysis under non-IID settings, showing that this coupling preserves standard convergence bounds while systematically distorting confidence. Across three benchmarks, TSA substantially shifts calibration (e.g., 145% error increase on CIFAR-100) with <2 accuracy change, and remains effective under robust aggregation and post-hoc calibration defenses. Case studies further show that confidence manipulation can cause up to 7.2x increases in missed critical cases (healthcare) or false alarms (autonomous driving), even when accuracy is unchanged. Overall, our results establish calibration integrity as a critical attack surface in federated learning.

</details>


### [109] [Pruning at Initialisation through the lens of Graphon Limit: Convergence, Expressivity, and Generalisation](https://arxiv.org/abs/2602.06675)
*Hoang Pham,The-Anh Ta,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 该研究将初始化剪枝方法与图极限理论连接，证明了剪枝掩码收敛到确定性二分图元，建立了稀疏网络的拓扑分类，并推导了稀疏网络的通用逼近定理和图元-NTK泛化界。


<details>
  <summary>Details</summary>
Motivation: 初始化剪枝方法能在训练前发现稀疏可训练子网络，但其理论机制尚不明确。现有分析多限于有限宽度统计，缺乏对网络增大时出现的全局稀疏模式的严格刻画。

Method: 通过图元连接离散剪枝启发式与图极限理论，建立初始化剪枝掩码的图元极限。引入因子化显著性模型涵盖流行剪枝准则，证明在正则条件下，这些算法生成的离散掩码收敛到确定性二分图元。

Result: 建立了稀疏网络的新拓扑分类：非结构化方法收敛到表示均匀连接的齐次图元，数据驱动方法收敛到编码隐式特征选择的异质图元。基于此连续刻画，推导了：(1)仅依赖活跃坐标子空间内在维度的稀疏网络通用逼近定理；(2)图元-NTK泛化界，展示极限图元如何调节核几何以对齐信息特征。

Conclusion: 该研究将稀疏神经网络的研究从组合图问题转化为连续算子的严格框架，为分析稀疏神经网络的表达能力和泛化提供了新机制。

Abstract: Pruning at Initialisation methods discover sparse, trainable subnetworks before training, but their theoretical mechanisms remain elusive. Existing analyses are often limited to finite-width statistics, lacking a rigorous characterisation of the global sparsity patterns that emerge as networks grow large. In this work, we connect discrete pruning heuristics to graph limit theory via graphons, establishing the graphon limit of PaI masks. We introduce a Factorised Saliency Model that encompasses popular pruning criteria and prove that, under regularity conditions, the discrete masks generated by these algorithms converge to deterministic bipartite graphons. This limit framework establishes a novel topological taxonomy for sparse networks: while unstructured methods (e.g., Random, Magnitude) converge to homogeneous graphons representing uniform connectivity, data-driven methods (e.g., SNIP, GraSP) converge to heterogeneous graphons that encode implicit feature selection. Leveraging this continuous characterisation, we derive two fundamental theoretical results: (i) a Universal Approximation Theorem for sparse networks that depends only on the intrinsic dimension of active coordinate subspaces; and (ii) a Graphon-NTK generalisation bound demonstrating how the limit graphon modulates the kernel geometry to align with informative features. Our results transform the study of sparse neural networks from combinatorial graph problems into a rigorous framework of continuous operators, offering a new mechanism for analysing expressivity and generalisation in sparse neural networks.

</details>


### [110] [Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts](https://arxiv.org/abs/2602.06689)
*Victor Armegioiu*

Main category: cs.LG

TL;DR: 提出记忆条件扩散/流匹配方法，通过在线状态注入记忆，改善自回归PDE求解器的长期稳定性，减少粗到细尺度生成中的漂移问题。


<details>
  <summary>Details</summary>
Motivation: 自回归生成式PDE求解器在单步预测中准确，但在长期推演中容易漂移，特别是在粗到细尺度生成中，每个步骤都需要重新生成未解析的精细尺度。扩散和流匹配生成器虽然内部动态是马尔可夫的，但推演稳定性受每步条件分布误差控制。

Method: 使用Mori-Zwanzig投影形式主义分析，引入记忆条件扩散/流匹配方法，通过紧凑在线状态注入到去噪过程中的潜在特征。通过分解，记忆诱导出未解析尺度的结构化条件尾部先验，减少填充缺失频率所需的传输。

Result: 证明了所得条件核的Wasserstein稳定性，推导了分离记忆近似和条件生成误差的离散Grönwall推演边界。在具有激波的可压缩流和多尺度混合实验中，显示出更高的精度和显著更稳定的长期推演，具有更好的精细尺度谱和统计保真度。

Conclusion: 记忆条件方法通过结构化记忆注入，有效解决了自回归PDE求解器在长期推演中的漂移问题，特别是在粗到细尺度生成中，显著提高了稳定性和精细尺度保真度。

Abstract: Autoregressive generative PDE solvers can be accurate one step ahead yet drift over long rollouts, especially in coarse-to-fine regimes where each step must regenerate unresolved fine scales. This is the regime of diffusion and flow-matching generators: although their internal dynamics are Markovian, rollout stability is governed by per-step \emph{conditional law} errors. Using the Mori--Zwanzig projection formalism, we show that eliminating unresolved variables yields an exact resolved evolution with a Markov term, a memory term, and an orthogonal forcing, exposing a structural limitation of memoryless closures. Motivated by this, we introduce memory-conditioned diffusion/flow-matching with a compact online state injected into denoising via latent features. Via disintegration, memory induces a structured conditional tail prior for unresolved scales and reduces the transport needed to populate missing frequencies. We prove Wasserstein stability of the resulting conditional kernel. We then derive discrete Grönwall rollout bounds that separate memory approximation from conditional generation error. Experiments on compressible flows with shocks and multiscale mixing show improved accuracy and markedly more stable long-horizon rollouts, with better fine-scale spectral and statistical fidelity.

</details>


### [111] [NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models](https://arxiv.org/abs/2602.06694)
*Hyochan Chong,Dongkyu Kim,Changdong Kim,Minseop Choi*

Main category: cs.LG

TL;DR: NanoQuant：首个将大语言模型压缩到二进制和亚1比特级别的后训练量化方法，通过低秩二进制分解实现高效压缩


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法无法高效压缩到二进制（1比特）级别，要么需要大量数据和计算资源，要么增加额外存储开销。需要一种能在低内存环境下实现亚1比特压缩的后训练量化方法。

Method: 将量化问题转化为低秩二进制分解问题，将全精度权重压缩为低秩二进制矩阵和缩放因子。使用ADMM方法精确初始化潜在二进制矩阵和缩放因子，然后通过块级和模型重构过程微调初始化参数。

Result: 在低内存后训练量化中建立了新的帕累托前沿，在亚1比特压缩率下仍能达到最先进的精度。例如，在单张H100上13小时内将Llama2-70B压缩25.8倍，使70B模型能在8GB消费级GPU上运行。

Conclusion: NanoQuant实现了大语言模型到二进制和亚1比特级别的首次后训练量化，大幅降低了部署门槛，使大模型能在消费级硬件上高效运行，为大模型部署提供了实用解决方案。

Abstract: Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8$\times$ in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.

</details>


### [112] [Diffeomorphism-Equivariant Neural Networks](https://arxiv.org/abs/2602.06695)
*Josephine Elisabeth Oettinger,Zakhar Shumaylov,Johannes Bostelmann,Jan Lellmann,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 提出一种通过能量基规范化方法，使预训练神经网络获得微分同胚等变性的策略，无需大量数据增强或重新训练


<details>
  <summary>Details</summary>
Motivation: 现有等变性方法主要针对有限、紧致或低维群，本文探索如何将等变性扩展到无限维群（如微分同胚群）

Method: 将等变性表述为优化问题，利用已建立的微分图像配准方法工具箱，通过能量基规范化诱导预训练神经网络的微分同胚等变性

Result: 在分割和分类任务上的实验结果表明，该方法实现了近似等变性，并能泛化到未见过的变换，无需依赖大量数据增强或重新训练

Conclusion: 通过能量基规范化方法，可以有效地将等变性扩展到无限维群，为预训练神经网络提供了一种实用的微分同胚等变性实现方案

Abstract: Incorporating group symmetries via equivariance into neural networks has emerged as a robust approach for overcoming the efficiency and data demands of modern deep learning. While most existing approaches, such as group convolutions and averaging-based methods, focus on compact, finite, or low-dimensional groups with linear actions, this work explores how equivariance can be extended to infinite-dimensional groups. We propose a strategy designed to induce diffeomorphism equivariance in pre-trained neural networks via energy-based canonicalisation. Formulating equivariance as an optimisation problem allows us to access the rich toolbox of already established differentiable image registration methods. Empirical results on segmentation and classification tasks confirm that our approach achieves approximate equivariance and generalises to unseen transformations without relying on extensive data augmentation or retraining.

</details>


### [113] [Explaining Grokking in Transformers through the Lens of Inductive Bias](https://arxiv.org/abs/2602.06702)
*Jaisidh Singh,Diganta Misra,Antonio Orvieto*

Main category: cs.LG

TL;DR: 该研究从归纳偏置视角分析Transformer中的grokking现象，发现层归一化位置、学习率、权重衰减等架构和优化选择显著影响grokking速度，特征在整个训练过程中连续演化，且泛化能力与特征可压缩性相关。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解Transformer中grokking现象背后的归纳偏置机制，即架构设计和优化设置如何影响网络对特定解决方案的偏好，从而调控grokking的速度和特性。

Method: 方法包括：1）分析层归一化位置对grokking速度的影响；2）研究不同优化设置（学习率、权重衰减等）如何调节grokking；3）通过特征可压缩性分析泛化能力的出现；4）使用代码实验验证理论分析。

Result: 结果显示：1）层归一化位置显著影响grokking速度，特定位置的LN通过塑造捷径学习和注意力熵发挥作用；2）学习率和权重衰减会混淆readout scale作为惰性训练控制的效果；3）特征在整个训练过程中连续演化，表明grokking比简单的惰性到丰富学习机制转变更复杂；4）泛化能力可预测地随特征可压缩性出现。

Conclusion: 结论是Transformer中的grokking现象受多种归纳偏置因素复杂调控，包括架构选择和优化设置，特征演化具有连续性，且泛化能力与特征可压缩性相关，这为理解和控制grokking提供了新视角。

Abstract: We investigate grokking in transformers through the lens of inductive bias: dispositions arising from architecture or optimization that let the network prefer one solution over another. We first show that architectural choices such as the position of Layer Normalization (LN) strongly modulates grokking speed. This modulation is explained by isolating how LN on specific pathways shapes shortcut-learning and attention entropy. Subsequently, we study how different optimization settings modulate grokking, inducing distinct interpretations of previously proposed controls such as readout scale. Particularly, we find that using readout scale as a control for lazy training can be confounded by learning rate and weight decay in our setting. Accordingly, we show that features evolve continuously throughout training, suggesting grokking in transformers can be more nuanced than a lazy-to-rich transition of the learning regime. Finally, we show how generalization predictably emerges with feature compressibility in grokking, across different modulators of inductive bias. Our code is released at https://tinyurl.com/y52u3cad.

</details>


### [114] [SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers](https://arxiv.org/abs/2602.06706)
*Shentong Mo,Lanqing Li*

Main category: cs.LG

TL;DR: SaDiT是一个加速蛋白质骨架生成的新框架，通过结合SaProt Tokenization和Diffusion Transformer架构，利用离散潜在空间表示蛋白质几何，并引入IPA Token Cache机制优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的蛋白质骨架生成方法计算密集且速度较慢，限制了大规模结构探索。虽然已有如Proteina等流匹配方法提升采样效率，但蛋白质领域的结构压缩和加速潜力尚未充分开发。

Method: SaDiT框架整合SaProt Tokenization与Diffusion Transformer架构，利用离散潜在空间表示蛋白质几何，保持理论SE(3)等价性。引入IPA Token Cache机制优化Invariant Point Attention层，在迭代采样中重用计算过的token状态。

Result: SaDiT在计算速度和结构可行性方面均优于RFDiffusion和Proteina等最先进模型。在无条件骨架生成和折叠类条件生成任务中，SaDiT展现出捕获复杂拓扑特征和高度可设计性的卓越能力。

Conclusion: SaDiT通过离散化表示和缓存优化机制，显著加速了蛋白质骨架生成过程，为大规模蛋白质结构探索提供了高效解决方案，在保持结构质量的同时大幅提升计算效率。

Abstract: Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.

</details>


### [115] [F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare](https://arxiv.org/abs/2602.06717)
*Daniil Plyusov,Alexey Gorbatovski,Boris Shaposhnikov,Viacheslav Sinii,Alexey Malakhov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 提出一种基于难度感知优势缩放的RLVR改进方法，通过降低高成功率提示的更新权重来改善稀有正确轨迹的学习，提升模型性能而不增加计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法基于分组采样估计优势并稳定策略更新，但实际应用中大分组规模因计算限制不可行，导致学习偏向已有高概率轨迹。小分组则容易错过稀有正确轨迹，同时仍包含混合奖励，使概率集中在常见解上。

Method: 1. 分析分组大小与错过稀有正确模式概率的关系，揭示非单调行为；2. 分析更新如何在正确集合内重新分配质量；3. 提出受Focal loss启发的难度感知优势缩放系数，降低高成功率提示的更新权重；4. 该轻量级修改可直接集成到GRPO、DAPO、CISPO等分组相对RLVR算法中。

Result: 在Qwen2.5-7B模型上，跨领域内外基准测试显示：GRPO的pass@256从64.1提升至70.3，DAPO从69.3提升至72.5，CISPO从73.2提升至76.8，同时保持或改善pass@1性能，且不增加分组大小或计算成本。

Conclusion: 提出的难度感知优势缩放方法能有效改善RLVR算法对稀有正确轨迹的学习，提升整体性能而不增加计算负担，为分组相对RLVR算法提供了简单有效的改进方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.

</details>


### [116] [Pairwise is Not Enough: Hypergraph Neural Networks for Multi-Agent Pathfinding](https://arxiv.org/abs/2602.06733)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Pietro Lio,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出HMAGAT模型，利用有向超图注意力机制解决多智能体路径规划问题，在参数和数据量远少于现有方法的情况下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的MAPF方法局限于成对智能体消息传递，导致次优行为和注意力稀释问题，特别是在密集环境中需要群体协调时。现有方法未能充分探索高阶交互作用。

Method: 提出HMAGAT架构，利用有向超图上的注意力机制显式捕捉群体动态，解决GNN中的表示瓶颈问题。

Result: HMAGAT在基于学习的MAPF求解器中建立新的SOTA：仅1M参数、训练数据减少100倍，却优于当前85M参数的SOTA模型。注意力分析显示超图表示能缓解GNN的注意力稀释问题。

Conclusion: 对于多智能体问题，适当的归纳偏置通常比训练数据规模或参数数量更为关键。超图表示能有效捕捉成对方法无法处理的复杂交互。

Abstract: Multi-Agent Path Finding (MAPF) is a representative multi-agent coordination problem, where multiple agents are required to navigate to their respective goals without collisions. Solving MAPF optimally is known to be NP-hard, leading to the adoption of learning-based approaches to alleviate the online computational burden. Prevailing approaches, such as Graph Neural Networks (GNNs), are typically constrained to pairwise message passing between agents. However, this limitation leads to suboptimal behaviours and critical issues, such as attention dilution, particularly in dense environments where group (i.e. beyond just two agents) coordination is most critical. Despite the importance of such higher-order interactions, existing approaches have not been able to fully explore them. To address this representational bottleneck, we introduce HMAGAT (Hypergraph Multi-Agent Attention Network), a novel architecture that leverages attentional mechanisms over directed hypergraphs to explicitly capture group dynamics. Empirically, HMAGAT establishes a new state-of-the-art among learning-based MAPF solvers: e.g., despite having just 1M parameters and being trained on 100$\times$ less data, it outperforms the current SoTA 85M parameter model. Through detailed analysis of HMAGAT's attention values, we demonstrate how hypergraph representations mitigate the attention dilution inherent in GNNs and capture complex interactions where pairwise methods fail. Our results illustrate that appropriate inductive biases are often more critical than the training data size or sheer parameter count for multi-agent problems.

</details>


### [117] [Optimal Abstractions for Verifying Properties of Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2602.06737)
*Noah Schwartz,Chandra Kanth Nagesh,Sriram Sankaranarayanan,Ramneet Kaur,Tuhin Sahai,Susmit Jha*

Main category: cs.LG

TL;DR: 提出一种验证Kolmogorov-Arnold网络（KANs）性质的新方法，通过将KAN单元替换为分段仿射函数创建数学抽象，使用MILP进行验证，并通过动态规划和背包优化最小化分段数量同时保证误差界。


<details>
  <summary>Details</summary>
Motivation: KANs作为一类具有非线性单变量激活函数的神经网络，其性质验证面临挑战。传统验证方法在处理KANs时存在计算复杂性和精度平衡问题，需要一种系统化的验证框架。

Method: 1. 将KAN单元替换为分段仿射（PWA）函数创建数学抽象；2. 提供原始网络与近似之间的局部和全局误差估计；3. 将验证问题编码为混合整数线性规划（MILP）；4. 结合单元级动态规划和网络级背包优化，最小化总分段数同时保证指定误差界。

Result: 在多个KAN基准测试上的实证评估表明，该方法的前期分析成本被优越的验证结果所证明，能够有效平衡计算复杂性和验证精度。

Conclusion: 提出了一种系统化的KAN验证框架，通过优化分段仿射近似策略，解决了验证过程中分段数量与计算复杂度之间的平衡问题，为KANs的性质验证提供了有效解决方案。

Abstract: We present a novel approach for verifying properties of Kolmogorov-Arnold Networks (KANs), a class of neural networks characterized by nonlinear, univariate activation functions typically implemented as piecewise polynomial splines or Gaussian processes. Our method creates mathematical ``abstractions'' by replacing each KAN unit with a piecewise affine (PWA) function, providing both local and global error estimates between the original network and its approximation. These abstractions enable property verification by encoding the problem as a Mixed Integer Linear Program (MILP), determining whether outputs satisfy specified properties when inputs belong to a given set. A critical challenge lies in balancing the number of pieces in the PWA approximation: too many pieces add binary variables that make verification computationally intractable, while too few pieces create excessive error margins that yield uninformative bounds. Our key contribution is a systematic framework that exploits KAN structure to find optimal abstractions. By combining dynamic programming at the unit level with a knapsack optimization across the network, we minimize the total number of pieces while guaranteeing specified error bounds. This approach determines the optimal approximation strategy for each unit while maintaining overall accuracy requirements. Empirical evaluation across multiple KAN benchmarks demonstrates that the upfront analysis costs of our method are justified by superior verification results.

</details>


### [118] [Disentanglement by means of action-induced representations](https://arxiv.org/abs/2602.06741)
*Gorka Muñoz-Gil,Hendrik Poulsen Nautrup,Arunava Majumder,Paulin de Schoulepnikoff,Florian Fürrutter,Marius Krumm,Hans J. Briegel*

Main category: cs.LG

TL;DR: 提出动作诱导表示框架，通过建模物理系统的实验/动作来获得可证明解耦的表示，并引入VAIR架构实现标准VAE无法达到的解耦效果。


<details>
  <summary>Details</summary>
Motivation: 变分自编码器学习可解释表示的主要挑战在于获得解耦表示，其中每个潜在维度对应不同的生成因子。这一困难与非线性的独立成分分析无法实现有关。

Method: 引入动作诱导表示框架，建模物理系统在可执行实验/动作下的表示。提出变分AIR架构，能够提取AIRs并实现可证明的解耦。

Result: 在动作诱导表示框架下，可以可证明地解耦自由度与其动作依赖关系。VAIR架构能够提取AIRs，在标准VAE失败的情况下实现解耦。

Conclusion: 动作诱导表示框架为解决表示学习中的解耦问题提供了新途径，VAIR不仅学习状态表示，还能捕捉底层生成因子的动作依赖，直接连接实验与其影响的自由度。

Abstract: Learning interpretable representations with variational autoencoders (VAEs) is a major goal of representation learning. The main challenge lies in obtaining disentangled representations, where each latent dimension corresponds to a distinct generative factor. This difficulty is fundamentally tied to the inability to perform nonlinear independent component analysis. Here, we introduce the framework of action-induced representations (AIRs) which models representations of physical systems given experiments (or actions) that can be performed on them. We show that, in this framework, we can provably disentangle degrees of freedom w.r.t. their action dependence. We further introduce a variational AIR architecture (VAIR) that can extract AIRs and therefore achieve provable disentanglement where standard VAEs fail. Beyond state representation, VAIR also captures the action dependence of the underlying generative factors, directly linking experiments to the degrees of freedom they influence.

</details>


### [119] [Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities](https://arxiv.org/abs/2602.06769)
*Marco Bagatella,Thomas Rupf,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: 提出一种基于最大熵前向-后向算法的零样本强化学习方法，能够处理具有一般效用函数的RL问题，直接从离线数据中提取策略家族，在测试时通过零阶搜索优化任意目标。


<details>
  <summary>Details</summary>
Motivation: 现有零样本RL方法主要处理具有加性奖励（与占用测度线性相关）的标准RL问题，但无法处理更一般的RL问题，如分布匹配或纯探索等任务，这些任务需要更一般的效用函数（占用测度的任意可微函数）。

Method: 提出最大熵（soft）前向-后向算法变体，从离线数据中恢复随机策略家族。结合紧凑策略嵌入上的零阶搜索，避免迭代优化方案，直接在测试时优化一般效用函数。

Result: 方法保留了前向-后向算法的优良特性，同时将其适用范围扩展到更一般的RL问题。在简单示例和高维实验中均验证了有效性。

Conclusion: 该方法成功将零样本RL扩展到具有一般效用函数的更广泛问题类别，为处理分布匹配、纯探索等传统加性奖励无法表达的任务提供了有效解决方案。

Abstract: Recent advancements in zero-shot reinforcement learning (RL) have facilitated the extraction of diverse behaviors from unlabeled, offline data sources. In particular, forward-backward algorithms (FB) can retrieve a family of policies that can approximately solve any standard RL problem (with additive rewards, linear in the occupancy measure), given sufficient capacity. While retaining zero-shot properties, we tackle the greater problem class of RL with general utilities, in which the objective is an arbitrary differentiable function of the occupancy measure. This setting is strictly more expressive, capturing tasks such as distribution matching or pure exploration, which may not be reduced to additive rewards. We show that this additional complexity can be captured by a novel, maximum entropy (soft) variant of the forward-backward algorithm, which recovers a family of stochastic policies from offline data. When coupled with zero-order search over compact policy embeddings, this algorithm can sidestep iterative optimization schemes, and optimizes general utilities directly at test-time. Across both didactic and high-dimensional experiments, we demonstrate that our method retains favorable properties of FB algorithms, while also extending their range to more general RL problems.

</details>


### [120] [AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models](https://arxiv.org/abs/2602.06771)
*Fengpeng Li,Kemou Li,Qizhou Wang,Bo Han,Jiantao Zhou*

Main category: cs.LG

TL;DR: AEGIS框架通过梯度信息协同实现概念擦除，同时提升鲁棒性和保留性，无需保留数据


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型的概念擦除方法面临鲁棒性和保留性的权衡问题：增强鲁棒性会损害保留性，反之亦然。现有方法通常只能优化一个方面而牺牲另一个，无法同时满足实际应用中对概念擦除的完整要求。

Method: 提出AEGIS（Adversarial Erasure with Gradient Informed Synergy）框架，这是一种无需保留数据的框架，通过梯度信息协同机制来同时提升概念擦除的鲁棒性和保留性。

Result: AEGIS框架能够同时提升概念擦除的鲁棒性和保留性，解决了现有方法在这两个关键指标上的权衡问题。

Conclusion: AEGIS为扩散模型的概念擦除提供了一种新的解决方案，能够在无需保留数据的情况下同时优化鲁棒性和保留性，具有重要的实际应用价值。

Abstract: Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention means unrelated concepts are preserved so the model's overall utility stays intact. Both are critical for concept erasure in practice, yet addressing them simultaneously is challenging, as existing works typically improve one factor while sacrificing the other. Prior work typically strengthens one while degrading the other, e.g., mapping a single erased prompt to a fixed safe target leaves class level remnants exploitable by prompt attacks, whereas retention-oriented schemes underperform against adaptive adversaries. This paper introduces Adversarial Erasure with Gradient Informed Synergy (AEGIS), a retention-data-free framework that advances both robustness and retention.

</details>


### [121] [Calibrating Generative AI to Produce Realistic Essays for Data Augmentation](https://arxiv.org/abs/2602.06772)
*Edward W. Wolfe,Justin O. Barber*

Main category: cs.LG

TL;DR: 研究比较了三种LLM提示方法生成模拟作文的效果，发现"predict next"策略在评分一致性、质量保持和文本真实性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 数据增强可以缓解构建反应项目自动评分引擎中训练数据不足的问题，但需要确定哪种LLM提示方法能最好地生成保持原始作文质量且文本真实的模拟作文

Method: 使用三种大型语言模型提示方法生成模拟学生作文：1) predict next策略，2) sentence策略，3) 25 examples策略。然后由人工评分员对模拟作文进行评分并评估文本真实性

Result: predict next策略在评分员间一致性最高；predict next和sentence策略最能保持原始作文的评分质量；predict next和25 examples策略生成的文本被评分员认为最真实

Conclusion: 对于自动评分引擎的数据增强，predict next提示策略在生成保持原始质量、评分一致且文本真实的模拟作文方面表现最佳，是构建训练数据集的有效方法

Abstract: Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.

</details>


### [122] [On the Convergence of Multicalibration Gradient Boosting](https://arxiv.org/abs/2602.06773)
*Daniel Haimovich,Fridolin Linder,Lorenzo Perini,Niek Tax,Milan Vojnovic*

Main category: cs.LG

TL;DR: 本文为多校准梯度提升在回归问题中提供了收敛性理论保证，证明了预测更新的幅度以O(1/√T)衰减，在弱学习器满足平滑性假设时可达到线性收敛。


<details>
  <summary>Details</summary>
Motivation: 多校准梯度提升作为一种可扩展方法已在实践中成功部署，但其收敛性质尚未得到充分理解。本文旨在填补这一理论空白，为该方法提供严格的收敛性分析。

Method: 本文分析了多校准梯度提升在平方误差损失回归中的收敛性，证明了预测更新幅度的衰减速率。在弱学习器满足平滑性假设下，进一步分析了自适应变体、局部二次收敛以及保持收敛的缩放方案。

Result: 理论分析表明：1) 预测更新幅度以O(1/√T)衰减；2) 多校准误差具有相同的收敛速率；3) 在平滑性假设下可达到线性收敛；4) 自适应变体具有局部二次收敛性。真实数据集实验验证了理论结果。

Conclusion: 本文首次为多校准梯度提升提供了系统的收敛性理论保证，明确了该方法在不同条件下的收敛速率，并通过实验验证了理论分析，为实际应用提供了理论指导。

Abstract: Multicalibration gradient boosting has recently emerged as a scalable method that empirically produces approximately multicalibrated predictors and has been deployed at web scale. Despite this empirical success, its convergence properties are not well understood. In this paper, we bridge the gap by providing convergence guarantees for multicalibration gradient boosting in regression with squared-error loss. We show that the magnitude of successive prediction updates decays at $O(1/\sqrt{T})$, which implies the same convergence rate bound for the multicalibration error over rounds. Under additional smoothness assumptions on the weak learners, this rate improves to linear convergence. We further analyze adaptive variants, showing local quadratic convergence of the training loss, and we study rescaling schemes that preserve convergence. Experiments on real-world datasets support our theory and clarify the regimes in which the method achieves fast convergence and strong multicalibration.

</details>


### [123] [Robust Online Learning](https://arxiv.org/abs/2602.06775)
*Sajad Ashkezari*

Main category: cs.LG

TL;DR: 该论文研究在对抗性扰动下的在线学习问题，其中干净数据和标签也是对抗性选择的，定义了新的维度来控制错误界和遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究在对抗性扰动下的鲁棒分类问题，与传统的鲁棒PAC学习不同，本文考虑干净数据和标签也是对抗性选择的情况，这更贴近现实中的对抗性攻击场景。

Method: 将问题建模为在线学习问题，定义了新的维度（类似于Littlestone维度）来控制学习性能，并将该维度推广到多分类假设类，还考虑了学习者不知道允许扰动集合的情况。

Result: 定义了新的维度，证明该维度在可实现情况下控制错误界，在不可知情况下控制遗憾界，该维度比PAC学习中的维度更简单，类似于Littlestone维度，并将结果推广到多分类情况。

Conclusion: 提出了对抗性扰动下在线学习的新框架，定义了控制学习性能的新维度，该框架比传统PAC学习更贴近实际对抗性攻击场景，为鲁棒分类提供了理论基础。

Abstract: We study the problem of learning robust classifiers where the classifier will receive a perturbed input. Unlike robust PAC learning studied in prior work, here the clean data and its label are also adversarially chosen. We formulate this setting as an online learning problem and consider both the realizable and agnostic learnability of hypothesis classes. We define a new dimension of classes and show it controls the mistake bounds in the realizable setting and the regret bounds in the agnostic setting. In contrast to the dimension that characterizes learnability in the PAC setting, our dimension is rather simple and resembles the Littlestone dimension. We generalize our dimension to multiclass hypothesis classes and prove similar results in the realizable case. Finally, we study the case where the learner does not know the set of allowed perturbations for each point and only has some prior on them.

</details>


### [124] [Weisfeiler and Lehman Go Categorical](https://arxiv.org/abs/2602.06787)
*Seongjin Choi,Gahee Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: 论文提出了范畴Weisfeiler-Lehman框架，将提升映射形式化为从任意数据范畴到分级偏序集范畴的函子映射，应用于超图时系统推导出超图同构网络家族，其中消息传递拓扑严格由函子选择决定。


<details>
  <summary>Details</summary>
Motivation: 提升映射显著增强了图神经网络的表达能力，但将这一范式扩展到超图领域仍然零散不完整。现有方法缺乏系统性的理论框架来统一处理超图的提升操作。

Method: 引入范畴Weisfeiler-Lehman框架，将提升形式化为函子映射。提出两种从超图范畴出发的函子：关联函子和对称单纯复形函子。这些函子严格决定了消息传递拓扑，从而系统推导出超图同构网络家族。

Result: 理论证明关联基和对称单纯复形方法都包含了标准超图Weisfeiler-Lehman测试的表达能力。关联架构虽然结构上类似标准二分方案，但函子推导在结果偏序集上强制执行了更丰富的信息流，能捕捉现有方法常忽略的复杂交叠几何结构。在真实世界基准测试上的广泛实验验证了这些理论发现。

Conclusion: 范畴Weisfeiler-Lehman框架为超图神经网络提供了系统化的理论基础，通过函子映射统一了提升操作，推导出的超图同构网络家族在理论上具有更强的表达能力，并在实践中得到验证。

Abstract: While lifting map has significantly enhanced the expressivity of graph neural networks, extending this paradigm to hypergraphs remains fragmented. To address this, we introduce the categorical Weisfeiler-Lehman framework, which formalizes lifting as a functorial mapping from an arbitrary data category to the unifying category of graded posets. When applied to hypergraphs, this perspective allows us to systematically derive Hypergraph Isomorphism Networks, a family of neural architectures where the message passing topology is strictly determined by the choice of functor. We introduce two distinct functors from the category of hypergraphs: an incidence functor and a symmetric simplicial complex functor. While the incidence architecture structurally mirrors standard bipartite schemes, our functorial derivation enforces a richer information flow over the resulting poset, capturing complex intersection geometries often missed by existing methods. We theoretically characterize the expressivity of these models, proving that both the incidence-based and symmetric simplicial approaches subsume the expressive power of the standard Hypergraph Weisfeiler-Lehman test. Extensive experiments on real-world benchmarks validate these theoretical findings.

</details>


### [125] [Displacement-Resistant Extensions of DPO with Nonconvex $f$-Divergences](https://arxiv.org/abs/2602.06788)
*Idan Pipano,Shoham Sabach,Kavosh Asadi,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 论文提出更一般的DPO条件（DPO-inducing），并引入防止概率位移的条件（displacement-resistant），基于此提出新的SquaredPO损失函数，在保持竞争力的同时提供更强的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有DPO及相关算法通过KL散度惩罚对齐语言模型，但KL散度可推广到f-散度。然而，现有理论要求f为凸函数，这限制了算法设计空间。同时，现有方法存在概率位移问题（winner和loser响应概率趋近于零）。

Method: 1. 识别更一般的DPO-inducing条件，放宽对f凸性的要求，精确刻画RLHF问题何时可解；2. 建立防止概率位移的displacement-resistant条件；3. 基于满足这两个条件的特定f，提出新的SquaredPO损失函数。

Result: 1. 证明了凸性不是必要条件，找到了更一般的DPO-inducing条件；2. 建立了防止概率位移的理论条件；3. 提出的SquaredPO损失在实验中表现竞争力，同时提供更强的理论保证。

Conclusion: 论文扩展了DPO的理论基础，放宽了对f-散度的限制条件，并解决了概率位移问题。提出的SquaredPO损失在理论和实践上都优于标准DPO，为RLHF对齐提供了更稳健的算法框架。

Abstract: DPO and related algorithms align language models by directly optimizing the RLHF objective: find a policy that maximizes the Bradley-Terry reward while staying close to a reference policy through a KL divergence penalty. Previous work showed that this approach could be further generalized: the original problem remains tractable even if the KL divergence is replaced by a family of $f$-divergence with a convex generating function $f$. Our first contribution is to show that convexity of $f$ is not essential. Instead, we identify a more general condition, referred to as DPO-inducing, that precisely characterizes when the RLHF problem remains tractable. Our next contribution is to establish a second condition on $f$ that is necessary to prevent probability displacement, a known empirical phenomenon in which the probabilities of the winner and the loser responses approach zero. We refer to any $f$ that satisfies this condition as displacement-resistant. We finally focus on a specific DPO-inducing and displacement-resistant $f$, leading to our novel SquaredPO loss. Compared to DPO, this new loss offers stronger theoretical guarantees while performing competitively in practice.

</details>


### [126] [Rare Event Analysis of Large Language Models](https://arxiv.org/abs/2602.06791)
*Jake McAllister Dorman,Edward Gillman,Dominic C. Rose,Jamie F. Mair,Juan P. Garrahan*

Main category: cs.LG

TL;DR: 提出一个端到端框架，用于系统分析大语言模型中的罕见事件，涵盖理论、高效生成策略、概率估计和误差分析


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为概率模型，在推理过程中会出现罕见事件——远离典型但高度显著的行为。由于罕见事件定义上难以观察，但LLM的巨大使用规模意味着开发阶段完全未观察到的事件可能在部署中变得突出，因此需要系统分析框架

Method: 提出一个端到端框架，包含理论分析、高效生成策略、概率估计和误差分析，并提供实际实现。通过具体示例说明该框架的应用

Result: 开发了一个完整的分析框架，能够系统性地识别、生成和分析LLM中的罕见事件，为理解模型行为提供了系统工具

Conclusion: 该框架具有通用性，可以扩展到其他模型和上下文，所提出的概念和技术具有广泛适用性，为分析概率模型中的罕见事件提供了系统方法

Abstract: Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.

</details>


### [127] [FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching](https://arxiv.org/abs/2602.06800)
*Ran Cheng,Lailai Zhu*

Main category: cs.LG

TL;DR: FlowDA：基于流匹配的低延迟天气尺度生成式数据同化框架，通过SetConv嵌入观测数据并微调Aurora基础模型，在稀疏观测下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统变分数据同化方法是机器学习天气预报流程中的主要计算瓶颈，而现有生成式ML同化方法需要大量采样步骤且在长时域自回归循环同化中易产生误差累积

Method: 基于流匹配构建天气尺度生成式数据同化框架，通过SetConv嵌入观测条件，微调Aurora基础模型实现高效分析

Result: 在观测率从3.9%降至0.1%的实验中，FlowDA在相似可调参数规模下优于强基线，对观测噪声具有鲁棒性，在长时域自回归循环同化中表现稳定

Conclusion: FlowDA为数据驱动的数据同化提供了高效且可扩展的方向，解决了传统方法的计算瓶颈和现有生成式方法的采样效率问题

Abstract: Data assimilation (DA) is a fundamental component of modern weather prediction, yet it remains a major computational bottleneck in machine learning (ML)-based forecasting pipelines due to reliance on traditional variational methods. Recent generative ML-based DA methods offer a promising alternative but typically require many sampling steps and suffer from error accumulation under long-horizon auto-regressive rollouts with cycling assimilation. We propose FlowDA, a low-latency weather-scale generative DA framework based on flow matching. FlowDA conditions on observations through a SetConv-based embedding and fine-tunes the Aurora foundation model to deliver accurate, efficient, and robust analyses. Experiments across observation rates decreasing from $3.9\%$ to $0.1\%$ demonstrate superior performance of FlowDA over strong baselines with similar tunable-parameter size. FlowDA further shows robustness to observational noise and stable performance in long-horizon auto-regressive cycling DA. Overall, FlowDA points to an efficient and scalable direction for data-driven DA.

</details>


### [128] [On the Identifiability of Steering Vectors in Large Language Models](https://arxiv.org/abs/2602.06801)
*Sohan Venkatesh,Ashish Mahendran Kurapath*

Main category: cs.LG

TL;DR: 激活引导方法（如角色向量）存在根本性的不可识别性问题，不同干预可能产生相同行为效果，但可通过结构假设恢复可识别性


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用激活引导方法来控制大语言模型行为，并将其解释为揭示了有意义的内部表示。这种解释隐含假设引导方向是可识别且可从输入输出行为中唯一恢复的，但这一假设需要验证

Method: 将引导形式化为内部表示的干预，在现实建模和数据条件下证明引导向量的根本不可识别性；通过多个模型和语义特征的实证验证，展示正交扰动可实现近等效效果；探索在统计独立性、稀疏约束、多环境验证或跨层一致性等结构假设下恢复可识别性

Result: 理论上证明引导向量存在大的行为不可区分干预等价类；实证显示正交扰动能实现近等效效果且效应大小可忽略；但在统计独立性、稀疏性、多环境验证或跨层一致性等结构假设下可恢复可识别性

Conclusion: 激活引导方法存在根本的可解释性限制，澄清了可靠安全关键控制所需的结构假设，为基于激活引导的模型控制提供了理论框架和实际指导

Abstract: Activation steering methods, such as persona vectors, are widely used to control large language model behavior and increasingly interpreted as revealing meaningful internal representations. This interpretation implicitly assumes steering directions are identifiable and uniquely recoverable from input-output behavior. We formalize steering as an intervention on internal representations and prove that, under realistic modeling and data conditions, steering vectors are fundamentally non-identifiable due to large equivalence classes of behaviorally indistinguishable interventions. Empirically, we validate this across multiple models and semantic traits, showing orthogonal perturbations achieve near-equivalent efficacy with negligible effect sizes. However, identifiability is recoverable under structural assumptions including statistical independence, sparsity constraints, multi-environment validation or cross-layer consistency. These findings reveal fundamental interpretability limits and clarify structural assumptions required for reliable safety-critical control.

</details>


### [129] [Calibrating Tabular Anomaly Detection via Optimal Transport](https://arxiv.org/abs/2602.06810)
*Hangting Ye,He Zhao. Wei Fan,Xiaozhuang Song,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: CTAD是一个模型无关的后处理框架，通过样本特异性校准增强任何现有表格异常检测器，使用最优传输距离测量测试样本对正常数据分布的破坏程度。


<details>
  <summary>Details</summary>
Motivation: 表格异常检测面临挑战，因为表格数据具有异质性：特征缺乏自然关系、分布和尺度差异大、类型多样。现有方法对异常模式有隐含假设，在某些数据集上表现好但在其他数据集上失败，没有方法能在所有场景中一致优于其他方法。

Method: CTAD通过两种互补分布（随机采样的经验分布和K-means质心的结构分布）表征正常数据，使用最优传输距离测量添加测试样本对这两种分布兼容性的破坏程度。正常样本破坏小，异常样本破坏大，从而提供校准信号。

Result: 在34个不同表格数据集和7个代表性检测器上的实验表明，CTAD能持续显著提升性能，甚至能增强最先进的深度学习方法，在不同超参数设置下表现稳健，无需额外调参即可部署。

Conclusion: CTAD是一个通用有效的后处理框架，能够校准任何表格异常检测器，通过最优传输距离测量样本对正常数据分布的破坏程度，在多样化场景中实现一致性能提升。

Abstract: Tabular anomaly detection (TAD) remains challenging due to the heterogeneity of tabular data: features lack natural relationships, vary widely in distribution and scale, and exhibit diverse types. Consequently, each TAD method makes implicit assumptions about anomaly patterns that work well on some datasets but fail on others, and no method consistently outperforms across diverse scenarios. We present CTAD (Calibrating Tabular Anomaly Detection), a model-agnostic post-processing framework that enhances any existing TAD detector through sample-specific calibration. Our approach characterizes normal data via two complementary distributions, i.e., an empirical distribution from random sampling and a structural distribution from K-means centroids, and measures how adding a test sample disrupts their compatibility using Optimal Transport (OT) distance. Normal samples maintain low disruption while anomalies cause high disruption, providing a calibration signal to amplify detection. We prove that OT distance has a lower bound proportional to the test sample's distance from centroids, and establish that anomalies systematically receive higher calibration scores than normals in expectation, explaining why the method generalizes across datasets. Extensive experiments on 34 diverse tabular datasets with 7 representative detectors spanning all major TAD categories (density estimation, classification, reconstruction, and isolation-based methods) demonstrate that CTAD consistently improves performance with statistical significance. Remarkably, CTAD enhances even state-of-the-art deep learning methods and shows robust performance across diverse hyperparameter settings, requiring no additional tuning for practical deployment.

</details>


### [130] [AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models](https://arxiv.org/abs/2602.06825)
*Yuming Li,Qingyu Li,Chengyu Bai,Xiangyang Luo,Zeyue Xue,Wenyu Qin,Meng Wang,Yikai Wang,Shanghang Zhang*

Main category: cs.LG

TL;DR: AEGPO：基于注意力熵的双信号自适应策略优化方法，通过全局ΔEntropy分配采样预算，局部利用熵峰值指导关键时间步探索，显著提升扩散模型对齐效率


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法（如GRPO）在扩散模型对齐中存在效率问题，对所有提示词和去噪步骤采用统一采样策略，忽视了样本学习价值和关键探索时刻的动态变化

Method: 提出AEGPO方法：1）全局层面使用ΔEntropy（注意力熵相对变化）动态分配采样预算，优先学习价值高的提示词；2）局部层面利用熵峰值识别关键时间步，在注意力分散度高的时刻进行选择性探索

Result: 在文本到图像生成任务中，AEGPO相比标准GRPO变体显著加速收敛速度，并取得更优的对齐性能

Conclusion: 注意力熵可作为有效的双信号代理指标，基于此设计的自适应优化策略能更高效地聚焦于信息量最大的样本和最关键的时刻，提升策略优化效果

Abstract: Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments.
  To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy (ΔEntropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs.
  Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses ΔEntropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps.
  By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.

</details>


### [131] [Learning Deep Hybrid Models with Sharpness-Aware Minimization](https://arxiv.org/abs/2602.06837)
*Naoya Takeishi*

Main category: cs.LG

TL;DR: 提出基于平坦损失最小化的混合建模方法，通过SAM（锐度感知最小化）技术防止机器学习模型完全主导预测，确保科学模型的有效参与


<details>
  <summary>Details</summary>
Motivation: 混合建模结合机器学习模型和科学数学模型，但机器学习模型的灵活性可能导致科学模型在预测中被忽略，使混合建模失去意义。现有正则化方法依赖模型架构和领域知识，需要更通用的解决方案。

Method: 采用锐度感知最小化（SAM）思想，关注损失函数最小值的平坦性，使模型尽可能简单。将SAM方法适配到混合建模设置中，通过平坦损失最小化确保科学模型的有效参与。

Result: 数值实验表明，基于SAM的方法在不同模型选择和数据集上表现良好，能够有效防止机器学习模型完全主导预测，确保混合建模的有效性。

Conclusion: 通过关注损失最小值的平坦性，SAM方法为混合建模提供了有效的正则化策略，能够在不同模型和数据集上稳定工作，确保科学模型在预测中的参与度。

Abstract: Hybrid modeling, the combination of machine learning models and scientific mathematical models, enables flexible and robust data-driven prediction with partial interpretability. However, effectively the scientific models may be ignored in prediction due to the flexibility of the machine learning model, making the idea of hybrid modeling pointless. Typically some regularization is applied to hybrid model learning to avoid such a failure case, but the formulation of the regularizer strongly depends on model architectures and domain knowledge. In this paper, we propose to focus on the flatness of loss minima in learning hybrid models, aiming to make the model as simple as possible. We employ the idea of sharpness-aware minimization and adapt it to the hybrid modeling setting. Numerical experiments show that the SAM-based method works well across different choices of models and datasets.

</details>


### [132] [Improved Sampling Schedules for Discrete Diffusion Models](https://arxiv.org/abs/2602.06849)
*Alberto Foresti,Mustapha Bounoua,Giulio Franzese,Luca Ambrogioni,Pietro Michiardi*

Main category: cs.LG

TL;DR: 该论文提出基于热力学熵产生理论分析离散扩散模型的反向过程，引入两种新的采样调度策略（EDS和WDS），在多个领域显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在序列数据生成中表现出强大能力，但其反向过程的信息论原理远不如连续扩散模型那样被充分理解。作者希望通过热力学熵产生的视角来填补这一理论空白。

Method: 1) 提出熵产生率作为量化信息生成的严格代理指标；2) 推导出中间状态与数据分布之间Wasserstein距离的界限；3) 引入两种基于物理启发的均匀间隔采样调度：保持恒定信息增益率的熵离散调度（EDS）和基于Wasserstein距离等步长的Wasserstein离散调度（WDS）。

Result: 在合成数据、音乐符号、视觉和语言建模等多个应用领域中，提出的调度策略显著优于最先进的方法，在更低的计算成本下实现了更优的性能。

Conclusion: 通过热力学熵产生理论分析离散扩散模型的反向过程，不仅提供了理论洞见，还催生了两种高效的采样调度策略，为离散扩散模型的优化提供了新的理论基础和实践方法。

Abstract: Discrete diffusion models have emerged as a powerful paradigm for generative modeling on sequence data; however, the information-theoretic principles governing their reverse processes remain significantly less understood than those of their continuous counterparts. In this work, we bridge this gap by analyzing the reverse process dynamics through the lens of thermodynamic entropy production. We propose the entropy production rate as a rigorous proxy for quantifying information generation, deriving as a byproduct a bound on the Wasserstein distance between intermediate states and the data distribution. Leveraging these insights, we introduce two novel sampling schedules that are uniformly spaced with respect to their corresponding physics-inspired metrics: the Entropic Discrete Schedule (EDS), which is defined by maintaining a constant rate of information gain, and the Wasserstein Discrete Schedule (WDS), which is defined by taking equal steps in terms of the Wasserstein distance. We empirically demonstrate that our proposed schedules significantly outperform state-of-the-art strategies across diverse application domains, including synthetic data, music notation, vision and language modeling, consistently achieving superior performance at a lower computational budget.

</details>


### [133] [Designing a Robust, Bounded, and Smooth Loss Function for Improved Supervised Learning](https://arxiv.org/abs/2602.06858)
*Soumi Mahato,Lineesh M. C*

Main category: cs.LG

TL;DR: 提出了一种鲁棒、有界且平滑的RoBoS-NN损失函数，用于解决传统损失函数在高维和异常值敏感数据集上的问题，并开发了L_RoBoS-NN算法用于时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统损失函数在处理高维和异常值敏感数据集时存在显著缺陷，导致性能下降和训练收敛缓慢，需要开发更鲁棒的损失函数。

Method: 开发了RoBoS-NN损失函数（鲁棒、有界、平滑），理论上分析了其泛化能力，并在神经网络框架中实现，创建了L_RoBoS-NN算法用于时间序列预测。

Result: 在多个真实世界数据集上的实验表明，L_RoBoS-NN在准确性指标上优于其他基准模型，即使在注入异常值的更具挑战性场景中也能保持良好性能。

Conclusion: RoBoS-NN损失函数有效解决了传统损失函数的局限性，L_RoBoS-NN算法在时间序列预测任务中表现出优越的鲁棒性和准确性。

Abstract: The loss function is crucial to machine learning, especially in supervised learning frameworks. It is a fundamental component that controls the behavior and general efficacy of learning algorithms. However, despite their widespread use, traditional loss functions have significant drawbacks when dealing with high-dimensional and outlier-sensitive datasets, which frequently results in reduced performance and slower convergence during training. In this work, we develop a robust, bounded, and smooth (RoBoS-NN) loss function to resolve the aforementioned hindrances. The generalization ability of the loss function has also been theoretically analyzed to rigorously justify its robustness. Moreover, we implement RoboS-NN loss in the framework of a neural network (NN) to forecast time series and present a new robust algorithm named $\mathcal{L}_{\text{RoBoS}}$-NN. To assess the potential of $\mathcal{L}_{\text{RoBoS}}$-NN, we conduct experiments on multiple real-world datasets. In addition, we infuse outliers into data sets to evaluate the performance of $\mathcal{L}_{\text{RoBoS}}$-NN in more challenging scenarios. Numerical results show that $\mathcal{L}_{\text{RoBoS}}$-NN outperforms the other benchmark models in terms of accuracy measures.

</details>


### [134] [Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts](https://arxiv.org/abs/2602.06859)
*Xinyu Zhao,Qingyun Sun,Jiayi Luo,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: GAD-MoRE：一种零样本可泛化图异常检测框架，采用黎曼专家混合架构，通过多曲率空间建模解决跨域异常检测中几何差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本图异常检测方法忽略了不同异常模式的内在几何差异，将不同域图嵌入单一静态曲率空间会扭曲异常的结构特征，限制了跨域泛化能力。

Method: 提出GAD-MoRE框架：1）使用多个专门化黎曼专家网络，每个在特定曲率空间操作；2）异常感知多曲率特征对齐模块，将输入投影到并行黎曼空间；3）基于记忆的动态路由器，根据历史重构性能自适应分配输入到最兼容的专家。

Result: 在零样本设置下，GAD-MoRE显著优于最先进的通用图异常检测基线，甚至超过在目标域使用标记数据进行少样本微调的强竞争对手。

Conclusion: 通过混合黎曼专家架构建模几何依赖的异常模式，GAD-MoRE有效解决了跨域图异常检测中的几何差异问题，实现了优异的零样本泛化性能。

Abstract: Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly patterns, substantially limiting their cross-domain generalization. In this work, we reveal that anomaly detectability is highly dependent on the underlying geometric properties and that embedding graphs from different domains into a single static curvature space can distort the structural signatures of anomalies. To address the challenge that a single curvature space cannot capture geometry-dependent graph anomaly patterns, we propose GAD-MoRE, a novel framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture. Specifically, to ensure that each anomaly pattern is modeled in the Riemannian space where it is most detectable, GAD-MoRE employs a set of specialized Riemannian expert networks, each operating in a distinct curvature space. To align raw node features with curvature-specific anomaly characteristics, we introduce an anomaly-aware multi-curvature feature alignment module that projects inputs into parallel Riemannian spaces, enabling the capture of diverse geometric characteristics. Finally, to facilitate better generalization beyond seen patterns, we design a memory-based dynamic router that adaptively assigns each input to the most compatible expert based on historical reconstruction performance on similar anomalies. Extensive experiments in the zero-shot setting demonstrate that GAD-MoRE significantly outperforms state-of-the-art generalist GAD baselines, and even surpasses strong competitors that are few-shot fine-tuned with labeled data from the target domain.

</details>


### [135] [T-STAR: A Context-Aware Transformer Framework for Short-Term Probabilistic Demand Forecasting in Dock-Based Shared Micro-Mobility](https://arxiv.org/abs/2602.06866)
*Jingyi Cheng,Gonçalo Homem de Almeida Correia,Oded Cats,Shadi Sharif Azadeh*

Main category: cs.LG

TL;DR: T-STAR是一个基于Transformer的两阶段时空自适应概率框架，用于15分钟分辨率的共享单车站点级需求预测，通过分层结构分离稳定模式和短期波动，在华盛顿Capital Bikeshare数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可靠的短期需求预测对于共享微出行服务管理至关重要，但高分辨率预测面临挑战，需要分离稳定需求模式和短期波动，并考虑时空变化。

Method: 提出T-STAR两阶段框架：第一阶段捕捉粗粒度小时需求模式，第二阶段结合高频局部输入（包括近期波动和实时地铁需求变化）提高预测精度，两阶段均使用时序Transformer模型进行概率预测。

Result: 在华盛顿Capital Bikeshare数据上的实验表明，T-STAR在确定性和概率准确性方面均优于现有方法，展现出跨站点和时段的强时空鲁棒性，零样本预测实验显示其能迁移到未见服务区域而无需重新训练。

Conclusion: T-STAR能够提供细粒度、可靠且具有不确定性感知的短期需求预测，支持多模式出行规划并增强共享微出行服务的实时运营能力。

Abstract: Reliable short-term demand forecasting is essential for managing shared micro-mobility services and ensuring responsive, user-centered operations. This study introduces T-STAR (Two-stage Spatial and Temporal Adaptive contextual Representation), a novel transformer-based probabilistic framework designed to forecast station-level bike-sharing demand at a 15-minute resolution. T-STAR addresses key challenges in high-resolution forecasting by disentangling consistent demand patterns from short-term fluctuations through a hierarchical two-stage structure. The first stage captures coarse-grained hourly demand patterns, while the second stage improves prediction accuracy by incorporating high-frequency, localized inputs, including recent fluctuations and real-time demand variations in connected metro services, to account for temporal shifts in short-term demand. Time series transformer models are employed in both stages to generate probabilistic predictions. Extensive experiments using Washington D.C.'s Capital Bikeshare data demonstrate that T-STAR outperforms existing methods in both deterministic and probabilistic accuracy. The model exhibits strong spatial and temporal robustness across stations and time periods. A zero-shot forecasting experiment further highlights T-STAR's ability to transfer to previously unseen service areas without retraining. These results underscore the framework's potential to deliver granular, reliable, and uncertainty-aware short-term demand forecasts, which enable seamless integration to support multimodal trip planning for travelers and enhance real-time operations in shared micro-mobility services.

</details>


### [136] [Decoupling Variance and Scale-Invariant Updates in Adaptive Gradient Descent for Unified Vector and Matrix Optimization](https://arxiv.org/abs/2602.06880)
*Zitao Song,Cedar Site Bai,Zhe Zhang,Brian Bullins,David F. Gleich*

Main category: cs.LG

TL;DR: DeVA框架通过解耦AdaGrad更新，将方差适应项与尺度不变项分离，实现了从Adam到自适应谱下降的无缝过渡，在语言建模和图像分类任务中优于Muon和SOAP等方法。


<details>
  <summary>Details</summary>
Motivation: 当前自适应优化方法存在局限性：Adam等向量优化方法采用坐标级适应，而Muon等谱优化方法将权重矩阵视为矩阵而非长向量。连接这两种方法很困难，因为许多自然泛化难以实现，且无法简单地将Adam适应机制迁移到矩阵谱上。

Method: 重新表述AdaGrad更新，将其分解为方差适应项和尺度不变项。这种解耦产生了DeVA框架，它桥接了基于向量的方差适应和矩阵谱优化，实现了从Adam到自适应谱下降的无缝过渡。

Result: 在语言建模和图像分类任务上的大量实验表明，DeVA始终优于Muon和SOAP等最先进方法，减少了约6.6%的token使用量。理论上证明方差适应项有效改善了分块平滑性，促进了更快的收敛。

Conclusion: DeVA框架成功连接了向量自适应优化和矩阵谱优化，提供了一种从Adam到自适应谱下降的平滑过渡方法，在多个任务中表现出优越性能，并具有理论收敛保证。

Abstract: Adaptive methods like Adam have become the $\textit{de facto}$ standard for large-scale vector and Euclidean optimization due to their coordinate-wise adaptation with a second-order nature. More recently, matrix-based spectral optimizers like Muon (Jordan et al., 2024b) show the power of treating weight matrices as matrices rather than long vectors. Linking these is hard because many natural generalizations are not feasible to implement, and we also cannot simply move the Adam adaptation to the matrix spectrum. To address this, we reformulate the AdaGrad update and decompose it into a variance adaptation term and a scale-invariant term. This decoupling produces $\textbf{DeVA}$ ($\textbf{De}$coupled $\textbf{V}$ariance $\textbf{A}$daptation), a framework that bridges between vector-based variance adaptation and matrix spectral optimization, enabling a seamless transition from Adam to adaptive spectral descent. Extensive experiments across language modeling and image classification demonstrate that DeVA consistently outperforms state-of-the-art methods such as Muon and SOAP (Vyas et al., 2024), reducing token usage by around 6.6\%. Theoretically, we show that the variance adaptation term effectively improves the blockwise smoothness, facilitating faster convergence. Our implementation is available at https://github.com/Tsedao/Decoupled-Variance-Adaptation

</details>


### [137] [Vision Transformer Finetuning Benefits from Non-Smooth Components](https://arxiv.org/abs/2602.06883)
*Ambroise Odonnat,Laetitia Chapel,Romain Tavenard,Ievgen Redko*

Main category: cs.LG

TL;DR: 研究发现视觉Transformer中注意力模块和前馈层的高塑性（低平滑性）能带来更好的微调性能，这与传统认为平滑性有益的观点相悖。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的平滑性已在泛化、训练稳定性和对抗鲁棒性方面得到广泛研究，但其在迁移学习中的作用仍不清楚。本文旨在分析视觉Transformer组件适应输入变化的能力（即塑性）。

Method: 通过理论分析和综合实验，将塑性定义为平均变化率，捕捉对输入扰动的敏感性。高塑性意味着低平滑性，这一视角为选择适应过程中应优先考虑的组件提供了原则性指导。

Result: 注意力模块和前馈层的高塑性一致导致更好的微调性能。这一发现与平滑性有益的普遍假设相悖，为Transformer的功能特性提供了新视角。

Conclusion: 高塑性（低平滑性）的Transformer组件在迁移学习中表现更好，这为实践者提供了选择微调组件的指导原则，并挑战了平滑性总是有益的传统观点。

Abstract: The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.

</details>


### [138] [A Cycle-Consistent Graph Surrogate for Full-Cycle Left Ventricular Myocardial Biomechanics](https://arxiv.org/abs/2602.06884)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: cs.LG

TL;DR: CGFENet：一种基于图的左心室生物力学快速全周期估计替代模型，通过全局-局部图编码、时序编码和循环一致性策略，显著减少有限元分析监督需求


<details>
  <summary>Details</summary>
Motivation: 基于图像的左心室力学模拟对理解心脏功能和临床干预规划很重要，但传统有限元分析计算量大。现有基于图的替代模型缺乏全周期预测能力，物理信息神经网络在复杂心脏几何上收敛困难。

Method: 提出CardioGraphFENet（CGFENet），整合：1）全局-局部图编码器捕捉网格特征；2）基于门控循环单元的时序编码器建模周期一致性动态；3）循环一致性双向公式，在单一框架内处理加载和反向卸载

Result: 模型在传统有限元分析基准上实现高保真度，与集总参数模型耦合时产生生理合理的压力-容积环。循环一致性策略显著减少有限元分析监督需求，仅带来最小精度损失

Conclusion: CGFENet为左心室心肌生物力学提供了一种快速、准确的统一图基替代模型，通过创新架构设计解决了现有方法的局限性，在减少计算成本的同时保持生理合理性

Abstract: Image-based patient-specific simulation of left ventricular (LV) mechanics is valuable for understanding cardiac function and supporting clinical intervention planning, but conventional finite-element analysis (FEA) is computationally intensive. Current graph-based surrogates do not have full-cycle prediction capabilities, and physics-informed neural networks often struggle to converge on complex cardiac geometries. We present CardioGraphFENet (CGFENet), a unified graph-based surrogate for rapid full-cycle estimation of LV myocardial biomechanics, supervised by a large FEA simulation dataset. The proposed model integrates (i) a global--local graph encoder to capture mesh features with weak-form-inspired global coupling, (ii) a gated recurrent unit-based temporal encoder conditioned on the target volume-time signal to model cycle-coherent dynamics, and (iii) a cycle-consistent bidirectional formulation for both loading and inverse unloading within a single framework. These strategies enable high fidelity with respect to traditional FEA ground truths and produce physiologically plausible pressure-volume loops that match FEA results when coupled with a lumped-parameter model. In particular, the cycle-consistency strategy enables a significant reduction in FEA supervision with only minimal loss in accuracy.

</details>


### [139] [Sample Complexity of Causal Identification with Temporal Heterogeneity](https://arxiv.org/abs/2602.06899)
*Ameya Rathod,Sujay Belsare,Salvik Krishna Nautiyal,Dhruv Laad,Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: 该论文将时间序列动态和多环境异质性结合作为因果图识别的互补来源，分析了在薄尾与厚尾噪声下的统计恢复极限，并量化了稳健性的信息理论代价。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中恢复唯一的因果图是一个不适定问题，因为多个生成机制可能导致相同的观测分布。该问题只有通过利用特定的结构或分布假设才能解决。虽然最近的研究分别利用时间序列动态或多环境异质性来约束这个问题，但作者将两者整合为互补的异质性来源。

Method: 整合时间序列动态和多环境异质性作为因果图识别的互补来源，推导统一的必要可识别性条件，分析薄尾与厚尾噪声（特别是Student's t分布）下的统计恢复极限，建立信息理论界限来量化稳健性的代价。

Result: 时间结构可以有效替代缺失的环境多样性，即使在异质性不足的情况下也可能实现可识别性。对于厚尾分布，几何可识别性条件保持不变，但样本复杂度与高斯基线显著不同。明确的信息理论界限量化了这种稳健性代价。

Conclusion: 这项工作将焦点从因果结构是否可识别转移到在实践中是否可统计恢复，建立了基于协方差的因果图恢复方法在现实非平稳系统中的基本极限。

Abstract: Recovering a unique causal graph from observational data is an ill-posed problem because multiple generating mechanisms can lead to the same observational distribution. This problem becomes solvable only by exploiting specific structural or distributional assumptions. While recent work has separately utilized time-series dynamics or multi-environment heterogeneity to constrain this problem, we integrate both as complementary sources of heterogeneity. This integration yields unified necessary identifiability conditions and enables a rigorous analysis of the statistical limits of recovery under thin versus heavy-tailed noise. In particular, temporal structure is shown to effectively substitute for missing environmental diversity, possibly achieving identifiability even under insufficient heterogeneity. Extending this analysis to heavy-tailed (Student's t) distributions, we demonstrate that while geometric identifiability conditions remain invariant, the sample complexity diverges significantly from the Gaussian baseline. Explicit information-theoretic bounds quantify this cost of robustness, establishing the fundamental limits of covariance-based causal graph recovery methods in realistic non-stationary systems. This work shifts the focus from whether causal structure is identifiable to whether it is statistically recoverable in practice.

</details>


### [140] [Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design](https://arxiv.org/abs/2602.06900)
*Samuel Klein,Willie Neiswanger,Daniel Ratner,Michael Kagan,Sean Gasiorowski*

Main category: cs.LG

TL;DR: 提出基于模拟推理(SBI)的贝叶斯最优实验设计(BOED)新方法，利用多种SBI密度估计器构建EIG估计器，并通过并行梯度上升优化显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最优实验设计(BOED)需要计算期望信息增益(EIG)，但在许多设置中似然函数难以处理。现有将模拟推理(SBI)与BOED结合的工作仅限于单一对比EIG边界，限制了方法的应用范围。

Method: 1) 展示EIG的多种表述形式，可直接利用现代SBI密度估计器（神经后验、似然和比率估计）；2) 提出基于神经似然估计的新型EIG估计器；3) 识别梯度优化瓶颈，采用简单多起点并行梯度上升程序提升可靠性。

Result: 在标准BOED基准测试中，SBI-based BOED方法能够匹配或超越现有最先进方法，性能提升高达22%。

Conclusion: 通过将EIG的多种表述与SBI密度估计器结合，并采用并行梯度上升优化，开发了更强大可靠的BOED方法，显著提升了实验设计性能。

Abstract: Bayesian optimal experimental design (BOED) seeks to maximize the expected information gain (EIG) of experiments. This requires a likelihood estimate, which in many settings is intractable. Simulation-based inference (SBI) provides powerful tools for this regime. However, existing work explicitly connecting SBI and BOED is restricted to a single contrastive EIG bound. We show that the EIG admits multiple formulations which can directly leverage modern SBI density estimators, encompassing neural posterior, likelihood, and ratio estimation. Building on this perspective, we define a novel EIG estimator using neural likelihood estimation. Further, we identify optimization as a key bottleneck of gradient based EIG maximization and show that a simple multi-start parallel gradient ascent procedure can substantially improve reliability and performance. With these innovations, our SBI-based BOED methods are able to match or outperform by up to $22\%$ existing state-of-the-art approaches across standard BOED benchmarks.

</details>


### [141] [Parameter-free Dynamic Regret: Time-varying Movement Costs, Delayed Feedback, and Memory](https://arxiv.org/abs/2602.06902)
*Emmanuel Esposito,Andrew Jacobsen,Hao Qiu,Mengxiao Zhang*

Main category: cs.LG

TL;DR: 论文研究了带时变移动成本的在线凸优化动态遗憾问题，提出了首个比较器自适应的动态遗憾界，并将结果应用于延迟反馈和时变记忆两个场景。


<details>
  <summary>Details</summary>
Motivation: 标准在线凸优化设置假设移动成本系数λ_t为零或固定，但实际应用中移动成本可能随时间变化。本文旨在扩展这一设置，允许移动成本系数λ_t任意变化，并建立相应的动态遗憾理论框架。

Method: 提出了一种新颖算法，建立了比较器自适应的动态遗憾界：$\widetilde{\mathcal{O}}(\sqrt{(1+P_T)(T+\sum_t λ_t)})$，其中P_T是比较器序列的路径长度。通过将延迟反馈和时变记忆问题转化为时变移动成本问题，展示了结果的通用性。

Result: 获得了首个比较器自适应的动态遗憾界，该界在λ_t=0时退化为标准在线凸优化的最优静态和动态遗憾界。为延迟反馈和时变记忆问题建立了最优的比较器自适应动态遗憾保证。

Conclusion: 本文扩展了带移动成本的在线凸优化理论框架，允许移动成本系数随时间任意变化。提出的算法和理论结果为处理延迟反馈和时变记忆等实际问题提供了新的工具，其中移动成本的一阶依赖关系是实现最优比较器自适应动态遗憾的关键。

Abstract: In this paper, we study dynamic regret in unconstrained online convex optimization (OCO) with movement costs. Specifically, we generalize the standard setting by allowing the movement cost coefficients $λ_t$ to vary arbitrarily over time. Our main contribution is a novel algorithm that establishes the first comparator-adaptive dynamic regret bound for this setting, guaranteeing $\widetilde{\mathcal{O}}(\sqrt{(1+P_T)(T+\sum_t λ_t)})$ regret, where $P_T$ is the path length of the comparator sequence over $T$ rounds. This recovers the optimal guarantees for both static and dynamic regret in standard OCO as a special case where $λ_t=0$ for all rounds. To demonstrate the versatility of our results, we consider two applications: OCO with delayed feedback and OCO with time-varying memory. We show that both problems can be translated into time-varying movement costs, establishing a novel reduction specifically for the delayed feedback setting that is of independent interest. A crucial observation is that the first-order dependence on movement costs in our regret bound plays a key role in enabling optimal comparator-adaptive dynamic regret guarantees in both settings.

</details>


### [142] [A first realization of reinforcement learning-based closed-loop EEG-TMS](https://arxiv.org/abs/2602.06907)
*Dania Humaidan,Jiahua Xu,Jing Chen,Christoph Zrenner,David Emanuel Vetter,Laura Marzetti,Paolo Belardinelli,Timo Roine,Risto J. Ilmoniemi,Gian Luca Romani,Ulf Zieman*

Main category: cs.LG

TL;DR: 首次实现基于机器学习的闭环实时EEG-TMS系统，通过强化学习自动识别与高/低皮质脊髓兴奋性相关的个体化mu节律相位，实现个性化脑刺激治疗。


<details>
  <summary>Details</summary>
Motivation: 传统TMS治疗采用"一刀切"方法，忽视了个体间和个体内差异。现有的脑状态依赖EEG-TMS方法仍需要用户预先定义目标相位，存在用户依赖性。需要开发用户独立的个性化脑刺激方法。

Method: 对25名参与者应用EEG-TMS，靶向辅助运动区-初级运动皮层网络。使用强化学习算法识别与高/低皮质脊髓兴奋性相关的mu节律相位。采用线性混合效应模型和贝叶斯分析评估强化学习对皮质脊髓兴奋性（通过运动诱发电位幅度）和功能连接性（通过静息态EEG相干性虚部）的影响。

Result: 强化学习有效识别了与高/低兴奋性状态相关的mu节律相位。对这些相位的重复刺激导致受刺激感觉运动网络的功能连接性长期增加或减少。

Conclusion: 首次在人类中证明了闭环EEG-TMS的可行性，这是实现脑疾病个体化治疗的关键一步。

Abstract: Background: Transcranial magnetic stimulation (TMS) is a powerful tool to investigate neurophysiology of the human brain and treat brain disorders. Traditionally, therapeutic TMS has been applied in a one-size-fits-all approach, disregarding inter- and intra-individual differences. Brain state-dependent EEG-TMS, such as coupling TMS with a pre-specified phase of the sensorimotor mu-rhythm, enables the induction of differential neuroplastic effects depending on the targeted phase. But this approach is still user-dependent as it requires defining an a-priori target phase. Objectives: To present a first realization of a machine-learning-based, closed-loop real-time EEG-TMS setup to identify user-independently the individual mu-rhythm phase associated with high- vs. low-corticospinal excitability states. Methods: We applied EEG-TMS to 25 participants targeting the supplementary motor area-primary motor cortex network and used a reinforcement learning algorithm to identify the mu-rhythm phase associated with high- vs. low corticospinal excitability. We employed linear mixed effects models and Bayesian analysis to determine effects of reinforced learning on corticospinal excitability indexed by motor evoked potential amplitude, and functional connectivity indexed by the imaginary part of resting-state EEG coherence. Results: Reinforcement learning effectively identified the mu-rhythm phase associated with high- vs. low-excitability states, and their repetitive stimulation resulted in long-term increases vs. decreases in functional connectivity in the stimulated sensorimotor network. Conclusions: We demonstrated for the first time the feasibility of closed-loop EEG-TMS in humans, a critical step towards individualized treatment of brain disorders.

</details>


### [143] [Revisiting the Generic Transformer: Deconstructing a Strong Baseline for Time Series Foundation Models](https://arxiv.org/abs/2602.06909)
*Yunshi Wen,Wesley M. Gifford,Chandra Reddy,Lam M. Nguyen,Jayant Kalagnanam,Anak Agung Julius*

Main category: cs.LG

TL;DR: 标准patch Transformer通过简单训练协议即可实现SOTA零样本预测性能，关键改进源于数据工程而非架构创新


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型研究中，异构的训练设置使得难以区分性能提升是源于架构创新还是数据工程，需要建立透明可复现的基准

Method: 使用标准patch Transformer架构，采用简单训练协议，进行全面的消融研究，涵盖模型缩放、数据组合和训练技术，严格控制变量

Result: 标准patch Transformer实现了最先进的零样本预测性能，识别出性能的关键驱动因素，验证了通用架构的良好可扩展性

Conclusion: 通过建立透明可复现的基准，证明标准架构配合适当训练和数据工程即可达到SOTA性能，为未来研究提供可靠基础

Abstract: The recent surge in Time Series Foundation Models has rapidly advanced the field, yet the heterogeneous training setups across studies make it difficult to attribute improvements to architectural innovations versus data engineering. In this work, we investigate the potential of a standard patch Transformer, demonstrating that this generic architecture achieves state-of-the-art zero-shot forecasting performance using a straightforward training protocol. We conduct a comprehensive ablation study that covers model scaling, data composition, and training techniques to isolate the essential ingredients for high performance. Our findings identify the key drivers of performance, while confirming that the generic architecture itself demonstrates excellent scalability. By strictly controlling these variables, we provide comprehensive empirical results on model scaling across multiple dimensions. We release our open-source model and detailed findings to establish a transparent, reproducible baseline for future research.

</details>


### [144] [From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers](https://arxiv.org/abs/2602.06923)
*Ziming Liu,Sophia Sanborn,Surya Ganguli,Andreas Tolias*

Main category: cs.LG

TL;DR: 该研究展示了通过引入空间平滑性、稳定性和时间局部性三个最小归纳偏置，可以使通用Transformer架构超越曲线拟合，真正学习开普勒和牛顿物理定律，实现从预测到物理发现的转变。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决通用AI架构能否超越预测任务，真正发现宇宙物理定律的问题。先前研究表明，传统"AI物理学家"方法依赖特定领域先验知识，而通用Transformer虽然预测准确却无法捕获底层物理规律，这促使研究者探索如何让通用架构获得真正的物理洞察力。

Method: 方法是通过系统性地引入三个最小归纳偏置：1）空间平滑性：将预测公式化为连续回归；2）稳定性：使用带噪声的上下文训练以减少误差累积；3）时间局部性：将注意力窗口限制在最近过去，假设未来状态仅依赖于局部状态而非复杂历史。

Result: 结果表明：前两个偏置使Transformer能够学习开普勒世界模型，成功拟合行星轨道的椭圆轨迹。但只有加入第三个偏置（时间局部性）后，模型才会放弃曲线拟合，真正发现牛顿力表示，获得物理洞察力。

Conclusion: 结论是简单的架构选择决定了AI是成为曲线拟合器还是物理学家。这些最小归纳偏置是实现自动化科学发现的关键步骤，展示了如何让通用AI架构超越预测，真正理解物理定律。

Abstract: Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on "world models" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous "AI Physicist" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively "bake in" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.

</details>


### [145] [Robustness Beyond Known Groups with Low-rank Adaptation](https://arxiv.org/abs/2602.06924)
*Abinitha Gourabathina,Hyewon Jeong,Teya Bergamaschi,Marzyeh Ghassemi,Collin Stultz*

Main category: cs.LG

TL;DR: LEIA是一种两阶段方法，通过识别表示空间中模型错误集中的低维子空间来提升群体鲁棒性，无需群体标签或修改主干网络。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在优化平均准确率时，经常在特定子群体上出现系统性失败。现实场景中，受这种差异影响最大的子群体通常未标注或未知，因此需要开发在不预先指定敏感子群体的情况下仍能保持性能的方法。

Method: LEIA采用两阶段方法：首先识别表示空间中模型错误集中的低维子空间，然后通过低秩调整分类器logits，将适应限制在这个错误信息子空间中，直接针对潜在失败模式，无需修改主干网络或群体标签。

Result: 在五个真实世界数据集上，LEIA在三种设置下（完全未知子群体相关性、部分已知、完全已知）均能一致提升最差群体性能，同时保持快速、参数高效且对超参数选择鲁棒。

Conclusion: LEIA是一种简单有效的群体鲁棒性方法，能够在没有群体标签的情况下识别和针对模型失败模式，在各种知识设置下都能提升最差群体性能。

Abstract: Deep learning models trained to optimize average accuracy often exhibit systematic failures on particular subpopulations. In real world settings, the subpopulations most affected by such disparities are frequently unlabeled or unknown, thereby motivating the development of methods that are performant on sensitive subgroups without being pre-specified. However, existing group-robust methods typically assume prior knowledge of relevant subgroups, using group annotations for training or model selection. We propose Low-rank Error Informed Adaptation (LEIA), a simple two-stage method that improves group robustness by identifying a low-dimensional subspace in the representation space where model errors concentrate. LEIA restricts adaptation to this error-informed subspace via a low-rank adjustment to the classifier logits, directly targeting latent failure modes without modifying the backbone or requiring group labels. Using five real-world datasets, we analyze group robustness under three settings: (1) truly no knowledge of subgroup relevance, (2) partial knowledge of subgroup relevance, and (3) full knowledge of subgroup relevance. Across all settings, LEIA consistently improves worst-group performance while remaining fast, parameter-efficient, and robust to hyperparameter choice.

</details>


### [146] [Continuous-time reinforcement learning: ellipticity enables model-free value function approximation](https://arxiv.org/abs/2602.06930)
*Wenlong Mou*

Main category: cs.LG

TL;DR: 本文研究连续时间马尔可夫扩散过程的离策略强化学习，提出基于Sobolev空间的拟合Q学习算法，证明在椭圆性条件下，扩散过程的强化学习复杂度不高于监督学习。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间马尔可夫扩散过程的强化学习问题，特别是在离散时间观测和动作下的离策略学习。现有方法通常需要不现实的结构假设，本文旨在开发无需此类假设的模型无关算法。

Method: 提出Sobolev-prox拟合Q学习算法，利用扩散过程的椭圆性建立贝尔曼算子的希尔伯特空间正定性和有界性，通过迭代求解最小二乘回归问题学习价值和优势函数。

Result: 建立了估计误差的oracle不等式，误差由四个因素控制：函数类的最佳逼近误差、局部复杂度、指数衰减的优化误差和数值离散化误差。证明在椭圆性条件下，扩散过程的强化学习复杂度不高于监督学习。

Conclusion: 椭圆性是使马尔可夫扩散过程的函数逼近强化学习不复杂于监督学习的关键结构性质，为连续时间扩散过程的强化学习提供了理论保证。

Abstract: We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions. We consider model-free algorithms with function approximation that learn value and advantage functions directly from data, without unrealistic structural assumptions on the dynamics.
  Leveraging the ellipticity of the diffusions, we establish a new class of Hilbert-space positive definiteness and boundedness properties for the Bellman operators. Based on these properties, we propose the Sobolev-prox fitted $q$-learning algorithm, which learns value and advantage functions by iteratively solving least-squares regression problems. We derive oracle inequalities for the estimation error, governed by (i) the best approximation error of the function classes, (ii) their localized complexity, (iii) exponentially decaying optimization error, and (iv) numerical discretization error. These results identify ellipticity as a key structural property that renders reinforcement learning with function approximation for Markov diffusions no harder than supervised learning.

</details>


### [147] [When RL Meets Adaptive Speculative Training: A Unified Training-Serving System](https://arxiv.org/abs/2602.06932)
*Junxiong Wang,Fengxiang Bie,Jisen Li,Zhongzhu Zhou,Zelei Shao,Yubo Wang,Yinghui Liu,Qingyang Wu,Avner May,Sri Yanamandra,Yineng Zhang,Ce Zhang,Tri Dao,Percy Liang,Ben Athiwaratkun,Shuaiwen Leon Song,Chenfeng Xu,Xiaoxia Wu*

Main category: cs.LG

TL;DR: Aurora是一个统一的训练-服务系统，通过从实时推理轨迹中持续学习推测器，解决了传统推测解码中训练与服务分离带来的部署延迟、反馈滞后和领域漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码将推测器训练与服务解耦，导致三个主要问题：1) 高服务准备时间（需要长时间离线训练）；2) 延迟的效用反馈（端到端解码加速只能在训练后评估）；3) 领域漂移退化（目标模型适应新领域时推测器变得陈旧低效。

Method: Aurora将在线推测器学习重构为异步强化学习问题：接受标记提供正反馈，拒绝的推测器提议提供隐式负反馈。系统集成SGLang推理服务器和异步训练服务器，支持热插拔推测器更新，实现零日部署。

Result: Aurora在新发布的前沿模型（如MiniMax M2.1 229B和Qwen3-Coder-Next 80B）上实现1.5倍零日加速，在广泛使用的模型（如Qwen3和Llama3）上，相比训练良好但静态的推测器，还能额外获得1.25倍加速。

Conclusion: Aurora通过统一训练-服务系统解决了推测解码中的部署延迟问题，实现了零日部署和持续适应，显著加速LLM服务，并能有效应对用户流量分布变化。

Abstract: Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.
  To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).

</details>


### [148] [Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics](https://arxiv.org/abs/2602.06939)
*Zuyuan Zhang,Sizhe Tang,Tian Lan*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑视角来看待基于时序差分（TD）的强化学习，将TD误差视为状态转移拓扑空间中的1-上链，将马尔可夫动态解释为拓扑可积性，并通过Bellman-de Rham投影实现TD误差的Hodge型分解，提出了HodgeFlow策略搜索方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中普遍存在非马尔可夫动态（如长程依赖、部分可观测性、记忆效应），而强化学习的核心支柱Bellman方程在非马尔可夫环境下仅近似成立。现有工作多关注实用算法设计，缺乏理论分析来回答关键问题：哪些动态能被Bellman框架捕获？如何启发具有最优近似的新算法类别？

Method: 提出拓扑视角：将TD误差视为状态转移拓扑空间中的1-上链，马尔可夫动态解释为拓扑可积性。通过Bellman-de Rham投影获得TD误差的Hodge型分解（可积分量+拓扑残差）。提出HodgeFlow策略搜索（HFPS），通过拟合势网络来最小化RL中的不可积投影残差，获得稳定性/敏感性保证。

Result: 在数值评估中，HFPS在非马尔可夫环境下显著提升了强化学习性能。

Conclusion: 论文通过引入拓扑视角为时序差分强化学习提供了新的理论框架，将TD误差分解为可积和不可积分量，提出的HFPS方法在非马尔可夫环境下表现出显著性能改进，为理解Bellman框架的适用范围和设计新算法提供了理论基础。

Abstract: Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.

</details>


### [149] [From Core to Detail: Unsupervised Disentanglement with Entropy-Ordered Flows](https://arxiv.org/abs/2602.06940)
*Daniel Galperin,Ullrich Köthe*

Main category: cs.LG

TL;DR: EOFlows是一种基于归一化流的框架，通过熵排序潜在维度实现自适应表示学习，支持推理时灵活选择核心特征维度，在CelebA数据集上实现了语义可解释特征、高压缩和强去噪。


<details>
  <summary>Details</summary>
Motivation: 当前无监督表示学习面临两个核心挑战：1）学习到的表示需要具有语义意义；2）表示需要在多次运行中保持稳定。现有的方法难以同时满足这两个要求，特别是在高维数据（如图像）上。

Method: 提出熵排序流（EOFlows）框架，基于归一化流技术，通过熵排序潜在维度（类似于PCA的方差解释排序）。训练后，可以灵活保留前C个潜在变量作为紧凑核心表示，其余变量捕获细节和噪声。方法结合了基于似然的训练、局部雅可比正则化和噪声增强，适用于高维数据。

Result: 在CelebA数据集上的实验表明，EOFlows能够发现丰富的语义可解释特征，支持高压缩比（通过保留核心维度）和强大的去噪能力。方法在保持语义意义的同时实现了表示的稳定性。

Conclusion: EOFlows通过熵排序机制为无监督表示学习提供了一个灵活且可扩展的框架，能够在推理时自适应地选择表示维度，在高维数据上实现了语义可解释性、压缩和去噪的良好平衡。

Abstract: Learning unsupervised representations that are both semantically meaningful and stable across runs remains a central challenge in modern representation learning. We introduce entropy-ordered flows (EOFlows), a normalizing-flow framework that orders latent dimensions by their explained entropy, analogously to PCA's explained variance. This ordering enables adaptive injective flows: after training, one may retain only the top C latent variables to form a compact core representation while the remaining variables capture fine-grained detail and noise, with C chosen flexibly at inference time rather than fixed during training. EOFlows build on insights from Independent Mechanism Analysis, Principal Component Flows and Manifold Entropic Metrics. We combine likelihood-based training with local Jacobian regularization and noise augmentation into a method that scales well to high-dimensional data such as images. Experiments on the CelebA dataset show that our method uncovers a rich set of semantically interpretable features, allowing for high compression and strong denoising.

</details>


### [150] [Endogenous Resistance to Activation Steering in Language Models](https://arxiv.org/abs/2602.06941)
*Alex McKenzie,Keenan Pepper,Stijn Servaes,Martin Leitgab,Murat Cubuktepe,Mike Vaiana,Diogo de Lucena,Judd Rosenblatt,Michael S. A. Graziano*

Main category: cs.LG

TL;DR: 大型语言模型在推理过程中能够抵抗任务不对齐的激活引导，有时会在引导持续的情况下中途恢复并生成改进的响应，这种现象被称为内源性引导抵抗（ESR）。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何抵抗外部激活引导，探索模型内部是否存在专门的"一致性检查"电路，这对于理解模型对抗性操纵的鲁棒性和安全干预的有效性具有重要意义。

Method: 使用稀疏自编码器（SAE）潜在变量引导模型激活，在Llama-3.3-70B等模型上测试ESR现象。通过零消融特定SAE潜在变量进行因果分析，并尝试通过元提示和微调来增强ESR行为。

Result: Llama-3.3-70B表现出显著的ESR，而较小的模型较少出现此现象。识别出26个与离题内容相关的SAE潜在变量，零消融这些变量可将多尝试率降低25%。通过元提示可将多尝试率提高4倍，微调也能在小模型中诱导ESR行为。

Conclusion: ESR现象表明大型语言模型存在内部一致性检查电路，这既能保护模型免受对抗性操纵，也可能干扰有益的安全干预。理解和控制这些抵抗机制对于开发透明可控的AI系统至关重要。

Abstract: Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.

</details>


### [151] [Improving Credit Card Fraud Detection with an Optimized Explainable Boosting Machine](https://arxiv.org/abs/2602.06955)
*Reza E. Fazel,Arash Bakhtiary,Siavash A. Bigdeli*

Main category: cs.LG

TL;DR: 该研究提出了一种基于可解释提升机（EBM）的增强工作流程，通过系统超参数调优、特征选择和预处理优化，在不使用传统采样技术的情况下，有效解决了信用卡欺诈检测中的类别不平衡问题，实现了高精度和可解释性的平衡。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测中的类别不平衡问题直接影响金融系统预测的可靠性，传统采样技术可能引入偏差或导致信息损失，需要一种既能保持高精度又能提供可解释性的解决方案。

Method: 采用可解释提升机（EBM）作为基础模型，通过系统超参数调优、特征选择和预处理优化来增强性能。使用田口方法优化数据缩放器序列和模型超参数，避免传统采样技术，确保鲁棒性和可重复性。

Result: 在基准信用卡数据集上获得0.983的ROC-AUC，超越了先前EBM基线（0.975）以及逻辑回归、随机森林、XGBoost和决策树模型，实现了精度和可解释性的有效平衡。

Conclusion: 研究表明，可解释机器学习与数据驱动优化相结合，能够在不依赖传统采样技术的情况下有效解决类别不平衡问题，为金融系统中的可信欺诈分析提供了有前景的解决方案。

Abstract: Addressing class imbalance is a central challenge in credit card fraud detection, as it directly impacts predictive reliability in real-world financial systems. To overcome this, the study proposes an enhanced workflow based on the Explainable Boosting Machine (EBM)-a transparent, state-of-the-art implementation of the GA2M algorithm-optimized through systematic hyperparameter tuning, feature selection, and preprocessing refinement. Rather than relying on conventional sampling techniques that may introduce bias or cause information loss, the optimized EBM achieves an effective balance between accuracy and interpretability, enabling precise detection of fraudulent transactions while providing actionable insights into feature importance and interaction effects. Furthermore, the Taguchi method is employed to optimize both the sequence of data scalers and model hyperparameters, ensuring robust, reproducible, and systematically validated performance improvements. Experimental evaluation on benchmark credit card data yields an ROC-AUC of 0.983, surpassing prior EBM baselines (0.975) and outperforming Logistic Regression, Random Forest, XGBoost, and Decision Tree models. These results highlight the potential of interpretable machine learning and data-driven optimization for advancing trustworthy fraud analytics in financial systems.

</details>


### [152] [Learning a Generative Meta-Model of LLM Activations](https://arxiv.org/abs/2602.06964)
*Grace Luo,Jiahai Feng,Trevor Darrell,Alec Radford,Jacob Steinhardt*

Main category: cs.LG

TL;DR: 使用扩散模型学习神经网络激活的分布，创建"元模型"作为先验，提升干预保真度和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有方法（如PCA、稀疏自编码器）依赖强结构假设，生成模型能无假设地发现结构并作为先验提高干预保真度

Method: 在10亿个残差流激活上训练扩散模型，创建学习网络内部状态分布的"元模型"

Result: 扩散损失随计算量平滑下降并可靠预测下游效用；元模型先验提升干预流畅度；神经元逐渐将概念隔离到单个单元

Conclusion: 生成式元模型为无需限制性结构假设的可扩展可解释性提供了路径

Abstract: Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [153] [Allocate Marginal Reviews to Borderline Papers Using LLM Comparative Ranking](https://arxiv.org/abs/2602.06078)
*Elliot L. Epstein,Rajat Dwaraknath,John Winnicki,Thanawat Sornwanee*

Main category: cs.DL

TL;DR: 大型ML会议应将额外评审资源集中分配给边界论文，而非随机分配。提出使用LLM比较排名识别边界论文，在评审分配阶段决定哪些论文获得额外评审。


<details>
  <summary>Details</summary>
Motivation: 当前大型机器学习会议评审资源有限，额外评审通常通过随机或相似性启发式方法分配，效率不高。需要更有效地利用边际评审能力，提高评审质量。

Method: 使用LLM进行成对比较和Bradley-Terry模型生成论文相对排名，在人类评审前识别边界论文带。根据会议最低评审目标（如3或4次），决定哪些论文获得额外评审（如第4或第5次评审）。

Result: 提出简单的预期影响计算公式，考虑预测边界集与真实边界集的重叠度（ρ）以及边界附近额外评审的增量价值（Δ），并提供回顾性代理指标来估计这些量。

Conclusion: 通过LLM辅助的边界识别和有针对性的额外评审分配，可以更有效地利用评审资源，提高会议评审质量，而不使用LLM输出进行接受/拒绝决策。

Abstract: This paper argues that large ML conferences should allocate marginal review capacity primarily to papers near the acceptance boundary, rather than spreading extra reviews via random or affinity-driven heuristics. We propose using LLM-based comparative ranking (via pairwise comparisons and a Bradley--Terry model) to identify a borderline band \emph{before} human reviewing and to allocate \emph{marginal} reviewer capacity at assignment time. Concretely, given a venue-specific minimum review target (e.g., 3 or 4), we use this signal to decide which papers receive one additional review (e.g., a 4th or 5th), without conditioning on any human reviews and without using LLM outputs for accept/reject. We provide a simple expected-impact calculation in terms of (i) the overlap between the predicted and true borderline sets ($ρ$) and (ii) the incremental value of an extra review near the boundary ($Δ$), and we provide retrospective proxies to estimate these quantities.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [154] [Evolving Ranking Functions for Canonical Blow-Ups in Positive Characteristic](https://arxiv.org/abs/2602.06553)
*Gergely Bérczi*

Main category: math.AG

TL;DR: 使用AlphaEvolve进化搜索模型在特征3的4维超曲面奇点上发现候选排序函数，提出了两个关于延迟排序函数的猜想。


<details>
  <summary>Details</summary>
Motivation: 正特征的奇点消解是代数几何中长期未解决的开放问题，经典特征零的排序函数在正特征中因Frobenius特定病理现象（如袋鼠现象）而失效，导致现有方法受阻。

Method: 使用进化搜索模型AlphaEvolve在特征p=3的4维超曲面奇点上进行实验，设计具有单项纯不可分主导项的测试基准，迭代优化实验设计，发现离散化的五分量字典序排序函数。

Result: 获得了满足有界延迟下降准则的五分量字典序排序函数，在基准测试中零违规，并由此提出了特征3中的两个延迟排序函数猜想。

Conclusion: 进化搜索方法为特征p的奇点消解问题提供了新的实验途径，发现的排序函数和提出的猜想为正特征奇点消解理论的发展提供了重要线索。

Abstract: Resolution of singularities in positive characteristic remains a long-standing open problem in algebraic geometry. In characteristic zero, the problem was solved by Hironaka in 1964, work for which he was awarded the Fields Medal. Modern proofs proceed by constructing suitable ranking functions, that is, invariants shown to strictly decrease along canonical sequences of blow-ups, ensuring termination. In positive characteristic, however, no such general ranking function is known: Frobenius-specific pathologies, such as the kangaroo phenomenon, can cause classical characteristic-zero invariants to plateau or even temporarily increase, presenting a fundamental obstruction to existing approaches. In this paper we report a sequence of experiments using the evolutionary search model AlphaEvolve, designed to discover candidate ranking functions for a toy canonical blow-up process. Our test benchmarks consist of carefully selected hypersurface singularities in dimension $4$ and characteristic $p=3$, with monic purely inseparable leading term, a regime in which naive order-based invariants often fail. After iteratively refining the experimental design, we obtained a discretized five-component lexicographic ranking function satisfying a bounded-delay descent criterion with zero violations across the benchmark. These experiments in turn motivated our main results: the conjectural delayed ranking functions in characteristic $3$ formulated in two conjectures.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [155] [Deep networks learn to parse uniform-depth context-free languages from local statistics](https://arxiv.org/abs/2602.06065)
*Jack T. Parley,Francesco Cagnetta,Matthieu Wyart*

Main category: stat.ML

TL;DR: 该研究提出了一个可调的概率上下文无关文法框架，通过控制歧义度和跨尺度相关性结构，结合深度学习架构的推理算法，揭示了语言统计特性如何影响层次表示的涌现和学习样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 理解语言结构如何仅从句子中学习是认知科学和机器学习的核心问题。虽然大型语言模型能够解析文本并独立于表面形式表示语义概念，但哪些数据统计特性使这些能力成为可能，以及需要多少数据，仍然未知。

Method: 1) 引入可调的概率上下文无关文法类别，可以控制歧义度和跨尺度相关性结构；2) 提出受深度卷积网络结构启发的推理算法，将可学习性和样本复杂度与特定语言统计特性联系起来；3) 在深度卷积和基于Transformer的架构上进行实证验证。

Result: 研究验证了不同尺度的相关性能够解除局部歧义，使得数据的层次表示得以涌现。该框架将可学习性和样本复杂度与具体的语言统计特性联系起来。

Conclusion: 提出了一个统一框架，其中不同尺度的相关性解除局部歧义，使得数据的层次表示能够涌现。该框架为理解语言结构如何从数据中学习提供了理论基础。

Abstract: Understanding how the structure of language can be learned from sentences alone is a central question in both cognitive science and machine learning. Studies of the internal representations of Large Language Models (LLMs) support their ability to parse text when predicting the next word, while representing semantic notions independently of surface form. Yet, which data statistics make these feats possible, and how much data is required, remain largely unknown. Probabilistic context-free grammars (PCFGs) provide a tractable testbed for studying these questions. However, prior work has focused either on the post-hoc characterization of the parsing-like algorithms used by trained networks; or on the learnability of PCFGs with fixed syntax, where parsing is unnecessary. Here, we (i) introduce a tunable class of PCFGs in which both the degree of ambiguity and the correlation structure across scales can be controlled; (ii) provide a learning mechanism -- an inference algorithm inspired by the structure of deep convolutional networks -- that links learnability and sample complexity to specific language statistics; and (iii) validate our predictions empirically across deep convolutional and transformer-based architectures. Overall, we propose a unifying framework where correlations at different scales lift local ambiguities, enabling the emergence of hierarchical representations of the data.

</details>


### [156] [Algebraic Robustness Verification of Neural Networks](https://arxiv.org/abs/2602.06105)
*Yulia Alexandr,Hao Duan,Guido Montúfar*

Main category: stat.ML

TL;DR: 该论文将神经网络鲁棒性验证转化为代数优化问题，利用欧几里得距离度（ED degree）作为衡量验证复杂度的指标，提出基于数值同伦延拓的精确鲁棒性认证算法。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络鲁棒性验证方法缺乏对验证问题内在复杂度的理论理解。作者希望从代数几何角度建立鲁棒性验证的理论基础，提供更系统化的验证复杂度分析框架。

Method: 1. 将鲁棒性验证形式化为代数优化问题；2. 引入ED degree作为架构相关的验证复杂度度量；3. 定义ED判别式区分验证难易实例；4. 提出参数判别式识别ED degree下降的参数区域；5. 推导多种神经网络架构的ED degree闭式表达式；6. 开发基于数值同伦延拓的精确鲁棒性认证算法。

Result: 1. 获得了多种神经网络架构的ED degree闭式表达式；2. 在无限宽度极限下推导了实临界点期望数量的公式；3. 提出了计算ED判别式的显式算法；4. 建立了度量代数几何与神经网络验证之间的具体联系。

Conclusion: 该工作为神经网络鲁棒性验证提供了新的代数几何视角，建立了验证复杂度与网络架构之间的理论联系，提出的算法为精确鲁棒性认证提供了新方法，开辟了度量代数几何在神经网络验证中的应用新方向。

Abstract: We formulate formal robustness verification of neural networks as an algebraic optimization problem. We leverage the Euclidean Distance (ED) degree, which is the generic number of complex critical points of the distance minimization problem to a classifier's decision boundary, as an architecture-dependent measure of the intrinsic complexity of robustness verification. To make this notion operational, we define the associated ED discriminant, which characterizes input points at which the number of real critical points changes, distinguishing test instances that are easier or harder to verify. We provide an explicit algorithm for computing this discriminant. We further introduce the parameter discriminant of a neural network, identifying parameters where the ED degree drops and the decision boundary exhibits reduced algebraic complexity. We derive closed-form expressions for the ED degree for several classes of neural architectures, as well as formulas for the expected number of real critical points in the infinite-width limit. Finally, we present an exact robustness certification algorithm based on numerical homotopy continuation, establishing a concrete link between metric algebraic geometry and neural network verification.

</details>


### [157] [Inheritance Between Feedforward and Convolutional Networks via Model Projection](https://arxiv.org/abs/2602.06245)
*Nicolas Ewen,Jairo Diaz-Rodriguez,Kelly Ramsay*

Main category: stat.ML

TL;DR: 论文提出模型投影方法，将卷积网络转换为类前馈网络形式，实现参数高效的迁移学习


<details>
  <summary>Details</summary>
Motivation: 前馈网络和卷积网络的技术经常被跨家族重用，但两者模型类别之间的底层关系很少被明确阐述。作者发现两种家族在每输入参数化方面存在不匹配问题。

Method: 引入统一的节点级形式化表示，提出模型投影方法：冻结预训练的每输入通道滤波器，为每个（输出通道，输入通道）贡献学习单个标量门控，使卷积层保持对下游任务的适应性同时大幅减少训练参数。

Result: 证明投影节点采用广义前馈网络形式，使投影CNN能够继承不依赖同质层输入的前馈技术。在多个ImageNet预训练骨干网络和下游图像分类数据集上的实验表明，模型投影在简单训练方案下是强大的迁移学习基线。

Conclusion: 模型投影方法成功地将卷积网络转换为前馈网络形式，实现了参数高效的迁移学习，为两种网络家族之间的技术重用提供了理论基础和实用方法。

Abstract: Techniques for feedforward networks (FFNs) and convolutional networks (CNNs) are frequently reused across families, but the relationship between the underlying model classes is rarely made explicit. We introduce a unified node-level formalization with tensor-valued activations and show that generalized feedforward networks form a strict subset of generalized convolutional networks. Motivated by the mismatch in per-input parameterization between the two families, we propose model projection, a parameter-efficient transfer learning method for CNNs that freezes pretrained per-input-channel filters and learns a single scalar gate for each (output channel, input channel) contribution. Projection keeps all convolutional layers adaptable to downstream tasks while substantially reducing the number of trained parameters in convolutional layers. We prove that projected nodes take the generalized FFN form, enabling projected CNNs to inherit feedforward techniques that do not rely on homogeneous layer inputs. Experiments across multiple ImageNet-pretrained backbones and several downstream image classification datasets show that model projection is a strong transfer learning baseline under simple training recipes.

</details>


### [158] [Time-uniform conformal and PAC prediction](https://arxiv.org/abs/2602.06297)
*Kayla E. Scharfstein,Arun Kumar Kuchibhotla*

Main category: stat.ML

TL;DR: 提出了一种适用于序列数据的扩展型conformal预测方法，能够在任意时间点保持有效的覆盖保证，解决了传统方法需要固定样本量的问题。


<details>
  <summary>Details</summary>
Motivation: 在机器学习模型被越来越多地用于高风险决策的背景下，传统conformal预测方法无法处理序列数据（数据以流式方式生成）且需要预先固定样本量，这限制了其在实时决策场景中的应用。

Method: 将conformal预测和PAC预测框架扩展到序列设置，开发了anytime-valid预测集方法，即使分析者基于数据选择任意时间点，预测集仍能保持期望覆盖水平。

Result: 提出的方法在理论和实验上都得到了验证，在模拟和真实数据集上展示了其有效性和实用性，能够为序列决策提供可靠的置信区间。

Conclusion: 该研究为序列环境下的不确定性量化提供了有效的解决方案，扩展了conformal预测的应用范围，使其能够适应实时决策和动态数据流场景。

Abstract: Given that machine learning algorithms are increasingly being deployed to aid in high stakes decision-making, uncertainty quantification methods that wrap around these black box models such as conformal prediction have received much attention in recent years. In sequential settings, where data are observed/generated in a streaming fashion, traditional conformal methods do not provide any guarantee without fixing the sample size. More importantly, traditional conformal methods cannot cope with sequentially updated predictions. As such, we develop an extension of the conformal prediction and related probably approximately correct (PAC) prediction frameworks to sequential settings where the number of data points is not fixed in advance. The resulting prediction sets are anytime-valid in that their expected coverage is at the required level at any time chosen by the analyst even if this choice depends on the data. We present theoretical guarantees for our proposed methods and demonstrate their validity and utility on simulated and real datasets.

</details>


### [159] [High-Dimensional Limit of Stochastic Gradient Flow via Dynamical Mean-Field Theory](https://arxiv.org/abs/2602.06320)
*Sota Nishiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文提出了一种基于动力学平均场理论（DMFT）的框架，用于分析高维非线性模型中多轮小批量随机梯度下降（SGD）的渐近行为，通过随机梯度流（SGF）近似，推导出低维连续时间方程来描述参数分布。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型通常通过多轮小批量SGD训练，但缺乏分析高维非线性模型中多轮SGD动态的理论框架。现有方法无法描述小批量情况下的高维渐近行为。

Method: 使用随机梯度流（SGF）近似多轮SGD，在数据样本数n和维度d成比例增长的极限下，基于动力学平均场理论（DMFT）推导出低维连续时间方程组，并证明其描述了SGF参数的渐近分布。

Result: 建立了统一的DMFT框架，适用于广义线性模型和两层神经网络等广泛模型，能够恢复在线SGD和高维线性回归等现有理论作为特例，为SGD动态提供了统一视角。

Conclusion: 该研究填补了多轮小批量SGD高维动态分析的理论空白，通过DMFT扩展了梯度流分析技术，使用随机计算工具处理SGF的随机性，为理解复杂机器学习模型的训练动态提供了新工具。

Abstract: Modern machine learning models are typically trained via multi-pass stochastic gradient descent (SGD) with small batch sizes, and understanding their dynamics in high dimensions is of great interest. However, an analytical framework for describing the high-dimensional asymptotic behavior of multi-pass SGD with small batch sizes for nonlinear models is currently missing. In this study, we address this gap by analyzing the high-dimensional dynamics of a stochastic differential equation called a \emph{stochastic gradient flow} (SGF), which approximates multi-pass SGD in this regime. In the limit where the number of data samples $n$ and the dimension $d$ grow proportionally, we derive a closed system of low-dimensional and continuous-time equations and prove that it characterizes the asymptotic distribution of the SGF parameters. Our theory is based on the dynamical mean-field theory (DMFT) and is applicable to a wide range of models encompassing generalized linear models and two-layer neural networks. We further show that the resulting DMFT equations recover several existing high-dimensional descriptions of SGD dynamics as special cases, thereby providing a unifying perspective on prior frameworks such as online SGD and high-dimensional linear regression. Our proof builds on the existing DMFT technique for gradient flow and extends it to handle the stochasticity in SGF using tools from stochastic calculus.

</details>


### [160] [Operationalizing Stein's Method for Online Linear Optimization: CLT-Based Optimal Tradeoffs](https://arxiv.org/abs/2602.06545)
*Zhiyu Zhang,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 将Stein方法转化为计算高效的在线线性优化算法，实现加性锐利的上界，超越传统最优性


<details>
  <summary>Details</summary>
Motivation: 传统基于动态规划的在线线性优化算法虽然理论上能实现最优权衡，但计算效率低下。需要找到既能保持理论最优性又具有计算效率的方法。

Method: 将Stein方法（概率极限定理证明的经典框架）操作化为计算高效的OLO算法。受Röllin (2018) Wasserstein鞅中心极限定理证明启发，构建算法实现加性锐利上界。

Result: 1. 相同计算复杂度下，算法优于在线梯度下降和乘性权重更新；2. 实现比较器间总损失与最大遗憾的最优两点权衡；3. 在无界支持随机化对手下，实现带噪声反馈OLO的锐利期望性能保证。

Conclusion: Stein方法可转化为计算高效的OLO算法，实现加性锐利性能上界，超越传统最优性，为参数自由在线学习提供新工具。

Abstract: Adversarial online linear optimization (OLO) is essentially about making performance tradeoffs with respect to the unknown difficulty of the adversary. In the setting of one-dimensional fixed-time OLO on a bounded domain, it has been observed since Cover (1966) that achievable tradeoffs are governed by probabilistic inequalities, and these descriptive results can be converted into algorithms via dynamic programming, which, however, is not computationally efficient. We address this limitation by showing that Stein's method, a classical framework underlying the proofs of probabilistic limit theorems, can be operationalized as computationally efficient OLO algorithms. The associated regret and total loss upper bounds are "additively sharp", meaning that they surpass the conventional big-O optimality and match normal-approximation-based lower bounds by additive lower order terms. Our construction is inspired by the remarkably clean proof of a Wasserstein martingale central limit theorem (CLT) due to Röllin (2018).
  Several concrete benefits can be obtained from this general technique. First, with the same computational complexity, the proposed algorithm improves upon the total loss upper bounds of online gradient descent (OGD) and multiplicative weight update (MWU). Second, our algorithm can realize a continuum of optimal two-point tradeoffs between the total loss and the maximum regret over comparators, improving upon prior works in parameter-free online learning. Third, by allowing the adversary to randomize on an unbounded support, we achieve sharp in-expectation performance guarantees for OLO with noisy feedback.

</details>


### [161] [Infinite-dimensional generative diffusions via Doob's h-transform](https://arxiv.org/abs/2602.06621)
*Thorben Pieper-Sethmacher,Daniel Paulin*

Main category: stat.ML

TL;DR: 提出基于Doob's h变换的无限维生成扩散模型框架，通过指数测度变换而非时间反转，实现更灵活的无限维扩散建模


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型方法难以直接推广到无限维设置，需要更灵活的框架来处理无限维生成建模问题

Method: 使用Doob's h变换，通过指数测度变换将参考扩散过程强制推向目标分布，而非依赖去噪过程的时间反转

Result: 在可验证条件下严格推导了构造，建立了相对于目标测度的界限，通过最小化分数匹配目标近似强制过程，并在合成和真实数据上验证

Conclusion: 提出的框架为无限维生成扩散模型提供了严格理论基础，相比现有方法具有更好的灵活性和推广性

Abstract: This paper introduces a rigorous framework for defining generative diffusion models in infinite dimensions via Doob's h-transform. Rather than relying on time reversal of a noising process, a reference diffusion is forced towards the target distribution by an exponential change of measure. Compared to existing methodology, this approach readily generalises to the infinite-dimensional setting, hence offering greater flexibility in the diffusion model. The construction is derived rigorously under verifiable conditions, and bounds with respect to the target measure are established. We show that the forced process under the changed measure can be approximated by minimising a score-matching objective and validate our method on both synthetic and real data.

</details>


### [162] [Missing At Random as Covariate Shift: Correcting Bias in Iterative Imputation](https://arxiv.org/abs/2602.06713)
*Luke Shannon,Song Liu,Katarzyna Reluga*

Main category: stat.ML

TL;DR: 提出基于风险最小化的缺失数据插补方法，通过重要性权重校正观测与未观测数据间的协变量偏移偏差，联合估计权重与插补模型


<details>
  <summary>Details</summary>
Motivation: 现有插补方法未考虑观测数据与未观测数据间的协变量偏移，导致分布偏差和次优性能，需要校正这种偏差

Method: 将缺失数据插补建模为风险最小化问题，推导理论上有效的重要性权重来校正分布偏差，提出联合估计重要性权重和插补模型的新算法

Result: 在基准数据集上，相比未加权的相同方法，均方根误差降低达7%，Wasserstein距离减少达20%

Conclusion: 通过重要性权重校正协变量偏移偏差能显著提升插补质量，联合估计权重和插补模型的方法优于传统无加权方法

Abstract: Accurate imputation of missing data is critical to downstream machine learning performance. We formulate missing data imputation as a risk minimisation problem, which highlights a covariate shift between the observed and unobserved data distributions. This covariate shift induced bias is not accounted for by popular imputation methods and leads to suboptimal performance. In this paper, we derive theoretically valid importance weights that correct for the induced distributional bias. Furthermore, we propose a novel imputation algorithm that jointly estimates both the importance weights and imputation models, enabling bias correction throughout the imputation process. Empirical results across benchmark datasets show reductions in root mean squared error and Wasserstein distance of up to 7% and 20%, respectively, compared to otherwise identical unweighted methods.

</details>


### [163] [Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay](https://arxiv.org/abs/2602.06797)
*Binghui Li,Zilin Wang,Fengling Chen,Shiyang Zhao,Ruiheng Zheng,Lei Wu*

Main category: stat.ML

TL;DR: 在FSL框架下研究最优学习率调度，发现任务难度决定调度模式：简单任务用幂衰减，困难任务用WSD结构，并分析了形状固定调度的优劣。


<details>
  <summary>Details</summary>
Motivation: 研究在固定训练步数N下，如何在FSL框架内找到最优学习率调度策略，以更好地理解损失动态并指导实际训练。

Method: 在FSL框架下推导最优学习率调度，分析两种任务难度下的不同调度模式，研究形状固定调度的性能，并将幂衰减调度应用于核回归的SGD。

Result: 发现任务难度导致调度模式的相变：简单任务(s≥1-1/β)用幂衰减η*(z)=η_peak(1-z/N)^{2β-1}；困难任务(s<1-1/β)用WSD结构。形状固定调度有内在局限性，幂衰减调度在核回归SGD中达到精确极小极大最优率。

Conclusion: 任务难度决定最优学习率调度模式，为实际训练提供了理论指导，幂衰减调度在核回归SGD中消除了先前分析中的对数次优性。

Abstract: We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s>0$ controlling the rate of signal learning, and a capacity exponent $β>1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \ge 1 - 1/β$, the optimal schedule follows a power decay to zero, $η^*(z) = η_{\mathrm{peak}}(1 - z/N)^{2β- 1}$, where the peak learning rate scales as $η_{\mathrm{peak}} \eqsim N^{-ν}$ for an explicit exponent $ν= ν(s,β)$. In contrast, in the hard-task regime $s < 1 - 1/β$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.
  We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [164] [Distributed Knowledge in Simplicial Models](https://arxiv.org/abs/2602.06945)
*Éric Goubault,Jérémy Ledent,Sergio Rajsbaum*

Main category: cs.LO

TL;DR: 该论文将单纯复形模型引入多智能体认知逻辑，连接分布式计算、认知逻辑和拓扑学，重点研究分布式知识及其不动点——公共分布式知识，并分析多数共识任务中的知识关系。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体认知逻辑基于克里普克模型，使用二元关系描述可能世界。近年来，单纯复形作为替代模型受到关注，能更清晰地揭示拓扑信息。本文旨在向非专业读者介绍单纯复形模型，并连接分布式计算、认知逻辑和拓扑学，特别关注分布式知识和公共分布式知识。

Method: 使用单纯复形作为多智能体认知逻辑的模型基础，以智能体的观点而非世界为基本对象。不同智能体对世界的观点集合构成单纯形，单纯形集合构成单纯复形。研究三种通信模型：即时快照、广播、测试与设置。分析多数共识任务的可解性条件，以及分布式知识在解决该任务中的作用。

Result: 当多数共识可解时，描述了用于解决该任务的分布式知识；当不可解时，提出了逻辑障碍——根据任务规范应该始终知道但玩家无法知道的公式。展示了分布式知识与多数共识（比共识更弱的任务）之间的关系，而公共知识则与共识相关。

Conclusion: 单纯复形模型为多智能体认知逻辑提供了新的视角，能更清晰地揭示拓扑结构。分布式知识与多数共识任务密切相关，而公共知识与共识相关。该框架连接了分布式计算、认知逻辑和拓扑学，为理解多智能体系统中的知识传播提供了新的工具。

Abstract: The usual semantics of multi-agent epistemic logic is based on Kripke models, defined in terms of binary relations on a set of possible worlds. Recently, there has been a growing interest in using simplicial complexes rather than graphs, as models for multi-agent epistemic logic.
  This approach uses agents' views as the fundamental object instead of worlds. A set of views by different agents about a world forms a simplex, and a set of simplexes defines a simplicial complex, that can serve as a model for multi-agent epistemic logic. This new approach reveals topological information that is implicit in Kripke models, because the binary indistinguishability relations are more clearly seen as n-ary relations in the simplicial complex.
  This paper, written for an economics audience, introduces simplicial models to non-experts and connects distributed computing, epistemic logic and topology. Our focus is on distributed knowledge and its fixed point, common distributed knowledge. These concepts arise when considering the knowledge that a group of agents would acquire, if they could communicate their local knowledge perfectly. While common knowledge has been shown to be related to consensus, we illustrate how distributed knowledge is related to a task weaker to consensus, called majority consensus.
  We describe three models of communication, some well-known (immediate snapshot), and others less studied (related to broadcast and test-and-set). When majority consensus is solvable, we describe the distributed knowledge that is used to solve it. When it is not solvable, we present a logical obstruction, a formula that should always be known according to the task specification, but which the players cannot know.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [165] [Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.06476)
*Kyungbeom Kim,Seungwon Oh,Kyung-Joong Kim*

Main category: cs.MA

TL;DR: Prism是一个多智能体强化学习参数共享框架，通过奇异值分解在谱域表示共享网络，让所有智能体共享奇异向量方向但学习不同的谱掩码，从而在保持可扩展性的同时促进智能体间多样性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，传统的完全参数共享架构往往导致同质化行为，而现有的多样性方法（如聚类、剪枝、掩码）通常牺牲了资源效率，需要在保持可扩展性的同时促进智能体间多样性。

Method: 通过奇异值分解将共享网络表示在谱域，所有智能体共享奇异向量方向，但学习各自不同的谱掩码作用于奇异值上，这种机制鼓励智能体间多样性同时保持参数共享的可扩展性优势。

Result: 在LBF、SMACv2等同质化基准和MaMuJoCo等异质化基准上的大量实验表明，Prism实现了有竞争力的性能，同时具有优越的资源效率。

Conclusion: Prism框架通过谱域参数共享机制，在保持多智能体强化学习可扩展性的同时有效促进了智能体间多样性，解决了传统方法在多样性和资源效率之间的权衡问题。

Abstract: Parameter sharing is a key strategy in multi-agent reinforcement learning (MARL) for improving scalability, yet conventional fully shared architectures often collapse into homogeneous behaviors. Recent methods introduce diversity through clustering, pruning, or masking, but typically compromise resource efficiency. We propose Prism, a parameter sharing framework that induces inter-agent diversity by representing shared networks in the spectral domain via singular value decomposition (SVD). All agents share the singular vector directions while learning distinct spectral masks on singular values. This mechanism encourages inter-agent diversity and preserves scalability. Extensive experiments on both homogeneous (LBF, SMACv2) and heterogeneous (MaMuJoCo) benchmarks show that Prism achieves competitive performance with superior resource efficiency.

</details>


### [166] [Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response](https://arxiv.org/abs/2602.06599)
*Ariyan Bighashdel,Thiago D. Simão,Frans A. Oliehoek*

Main category: cs.MA

TL;DR: 提出JBR方法改进PSRO，通过重用联合经验数据集同时计算所有智能体的最佳响应，大幅提升样本效率，解决多智能体强化学习中PSRO计算成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: PSRO在多智能体强化学习中能解决非平稳性和策略多样性问题，但每个智能体单独训练最佳响应计算成本过高，特别是在智能体众多或环境模拟昂贵的场景中。

Method: 提出JBR方法：1）收集当前元策略下的联合轨迹数据；2）重用该数据集同时计算所有智能体的最佳响应；3）针对分布偏移问题提出三种改进：保守JBR、探索增强JBR和混合BR。

Result: 在基准多智能体环境中，探索增强JBR达到最佳精度-效率权衡，混合BR以少量样本成本达到接近PSRO的性能，JBR显著提升PSRO在大规模战略学习中的实用性。

Conclusion: JBR通过重用联合经验数据集，将最佳响应计算转化为离线RL问题，大幅提高PSRO的样本效率，使其在大规模战略学习中更加实用，同时保持均衡鲁棒性。

Abstract: Multi-agent reinforcement learning (MARL) offers a scalable alternative to exact game-theoretic analysis but suffers from non-stationarity and the need to maintain diverse populations of strategies that capture non-transitive interactions. Policy Space Response Oracles (PSRO) address these issues by iteratively expanding a restricted game with approximate best responses (BRs), yet per-agent BR training makes it prohibitively expensive in many-agent or simulator-expensive settings. We introduce Joint Experience Best Response (JBR), a drop-in modification to PSRO that collects trajectories once under the current meta-strategy profile and reuses this joint dataset to compute BRs for all agents simultaneously. This amortizes environment interaction and improves the sample efficiency of best-response computation. Because JBR converts BR computation into an offline RL problem, we propose three remedies for distribution-shift bias: (i) Conservative JBR with safe policy improvement, (ii) Exploration-Augmented JBR that perturbs data collection and admits theoretical guarantees, and (iii) Hybrid BR that interleaves JBR with periodic independent BR updates. Across benchmark multi-agent environments, Exploration-Augmented JBR achieves the best accuracy-efficiency trade-off, while Hybrid BR attains near-PSRO performance at a fraction of the sample cost. Overall, JBR makes PSRO substantially more practical for large-scale strategic learning while preserving equilibrium robustness.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [167] [Induced Cycles of Many Lengths](https://arxiv.org/abs/2602.06874)
*Maria Chudnovsky,Ilya Maier*

Main category: math.CO

TL;DR: 该论文证明：对于不包含K_{t+1}或K_{t,t}作为诱导子图且诱导环长度种类数有限的图，其树宽是有界的。基于此，提出了一个多项式时间算法来判断图中是否包含至少三种不同长度的诱导环。


<details>
  <summary>Details</summary>
Motivation: 研究图的诱导环长度分布与图结构性质（如树宽）之间的关系，特别是对于排除特定子图（如完全图和完全二分图）的图类。目标是建立诱导环长度多样性受限时图的结构性约束，并开发高效的算法问题。

Method: 1. 证明组合定理：对于任意整数c,t，若图G不包含K_{t+1}或K_{t,t}作为诱导子图，且诱导环长度种类数cl(G)≤c，则G的树宽有界。
2. 利用该结构定理设计算法：将问题规约到树宽有界的情况，然后使用已知的树宽有界图上的动态规划技术。

Result: 主要理论结果：证明了上述组合定理，建立了诱导环长度多样性受限与树宽有界性之间的强联系。算法结果：给出了一个多项式时间算法，用于判定任意图是否包含至少三种不同长度的诱导环。

Conclusion: 该工作揭示了图的诱导环长度分布与结构复杂性之间的深刻联系，为排除特定子图的图类提供了新的结构刻画。所提出的算法展示了如何将结构理论结果转化为高效算法设计，为解决相关组合优化问题提供了新途径。

Abstract: Let $G$ be a graph and let $\mathrm{cl}(G)$ be the number of distinct induced cycle lengths in $G$. We show that for $c,t\in \mathbb N$, every graph $G$ that does not contain an induced subgraph isomorphic to $K_{t+1}$ or $K_{t,t}$ and satisfies $\mathrm{cl}(G) \le c$ has bounded treewidth. As a consequence, we obtain a polynomial-time algorithm for deciding whether a graph $G$ contains induced cycles of at least three distinct lengths.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [168] [Scaling Speech Tokenizers with Diffusion Autoencoders](https://arxiv.org/abs/2602.06602)
*Yuancheng Wang,Zhenyu Tang,Yun Wang,Arthur Hinsvark,Yingru Liu,Yinghao Li,Kainan Peng,Junyi Ao,Mingbo Ma,Mike Seltzer,Qing He,Xubo Liu*

Main category: cs.SD

TL;DR: Speech Diffusion Tokenizer (SiTok) 是一种扩散自编码器，通过监督学习联合学习语义丰富的表示，并使用扩散实现高保真音频重建，在极低的12.5Hz令牌率和200bps比特率下，在理解、重建和生成任务上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有语音分词器面临两个主要挑战：(1) 在编码语义（用于理解）和声学（用于重建）之间平衡权衡；(2) 实现低比特率和低令牌率。需要一种能同时解决这些问题的语音分词方法。

Method: 提出Speech Diffusion Tokenizer (SiTok)，一种扩散自编码器，通过监督学习联合学习语义丰富的表示，并使用扩散过程实现高保真音频重建。将模型扩展到16亿参数，在200万小时的语音数据上进行训练。

Result: SiTok在理解、重建和生成任务上优于强基线，在极低的12.5Hz令牌率和200bps比特率下表现出色。这表明该方法能有效平衡语义编码和声学重建的权衡。

Conclusion: SiTok通过扩散自编码器框架成功解决了语音分词器在语义-声学权衡和低速率方面的挑战，为语音语言模型提供了高质量的语音表示基础。

Abstract: Speech tokenizers are foundational to speech language models, yet existing approaches face two major challenges: (1) balancing trade-offs between encoding semantics for understanding and acoustics for reconstruction, and (2) achieving low bit rates and low token rates. We propose Speech Diffusion Tokenizer (SiTok), a diffusion autoencoder that jointly learns semantic-rich representations through supervised learning and enables high-fidelity audio reconstruction with diffusion. We scale SiTok to 1.6B parameters and train it on 2 million hours of speech. Experiments show that SiTok outperforms strong baselines on understanding, reconstruction and generation tasks, at an extremely low token rate of $12.5$ Hz and a bit-rate of 200 bits-per-second.

</details>


### [169] [Reciprocal Latent Fields for Precomputed Sound Propagation](https://arxiv.org/abs/2602.06937)
*Hugo Seuté,Pranai Vasudev,Etienne Richan,Louis-Xavier Buffoni*

Main category: cs.SD

TL;DR: 提出RLF框架，通过可训练的潜在嵌入和对称解码器高效编码声学参数，大幅减少内存占用，同时保持物理精度和听觉感知质量。


<details>
  <summary>Details</summary>
Motivation: 虚拟场景中真实声传播对沉浸感至关重要，但基于波的物理精确模拟计算成本高，无法实时应用。现有波编码方法预计算压缩脉冲响应为声学参数，但在大型场景中参数规模过大难以管理。

Method: 引入RLF框架：使用可训练潜在嵌入的体素网格，通过对称函数解码确保声学互易性。研究多种解码器，利用黎曼度量学习更好地复现复杂场景中的声学现象。

Result: 实验验证显示RLF在保持复制质量的同时，将内存占用减少数个数量级。主观听力测试表明RLF渲染的声音在感知上与真实模拟无法区分。

Conclusion: RLF框架为大规模虚拟环境提供了内存高效的声学参数编码方案，在保持物理准确性和听觉真实性的同时，显著降低了存储需求。

Abstract: Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [170] [Identifying Adversary Tactics and Techniques in Malware Binaries with an LLM Agent](https://arxiv.org/abs/2602.06325)
*Zhou Xuan,Xiangzhe Xu,Mingwei Zheng,Louis Zheng-Hua Tan,Jinyao Guo,Tiantai Zhang,Le Yu,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: TTPDetect：首个基于LLM代理的恶意软件二进制文件TTP识别系统，结合密集检索与神经检索定位分析入口点，通过上下文探索器和TTP特定推理指南实现函数级分析，在真实恶意软件样本上达到87.37%的精确度。


<details>
  <summary>Details</summary>
Motivation: 恶意软件二进制文件通常被剥离符号、包含大量函数且恶意行为分布在多个代码区域，使得TTP（战术、技术和程序）识别困难。现有大型语言模型虽具备代码理解能力，但在分析入口点识别、部分可观测性推理和TTP特定决策逻辑对齐方面面临挑战。

Method: TTPDetect结合密集检索与基于LLM的神经检索来缩小分析入口点空间，采用函数级分析代理，包含执行按需增量上下文检索的上下文探索器和实现推理时对齐的TTP特定推理指南。构建了跨不同恶意软件家族和平台的TTP标注数据集。

Result: 函数级TTP识别达到93.25%精确度和93.81%召回率，分别比基线提高10.38%和18.78%。在真实恶意软件样本上，TTP识别精确度为87.37%。对于有专家报告样本，恢复85.7%的已记录TTP，平均每个恶意软件发现10.5个先前未报告的TTP。

Conclusion: TTPDetect是首个基于LLM代理的恶意软件二进制文件TTP识别系统，通过创新的检索和分析架构有效解决了现有方法的局限性，在真实场景中表现出色，能够发现先前未记录的TTP，为安全分析和威胁情报提供了有力工具。

Abstract: Understanding TTPs (Tactics, Techniques, and Procedures) in malware binaries is essential for security analysis and threat intelligence, yet remains challenging in practice. Real-world malware binaries are typically stripped of symbols, contain large numbers of functions, and distribute malicious behavior across multiple code regions, making TTP attribution difficult. Recent large language models (LLMs) offer strong code understanding capabilities, but applying them directly to this task faces challenges in identifying analysis entry points, reasoning under partial observability, and misalignment with TTP-specific decision logic. We present TTPDetect, the first LLM agent for recognizing TTPs in stripped malware binaries. TTPDetect combines dense retrieval with LLM-based neural retrieval to narrow the space of analysis entry points. TTPDetect further employs a function-level analyzing agent consisting of a Context Explorer that performs on-demand, incremental context retrieval and a TTP-Specific Reasoning Guideline that achieves inference-time alignment. We build a new dataset that labels decompiled functions with TTPs across diverse malware families and platforms. TTPDetect achieves 93.25% precision and 93.81% recall on function-level TTP recognition, outperforming baselines by 10.38% and 18.78%, respectively. When evaluated on real world malware samples, TTPDetect recognizes TTPs with a precision of 87.37%. For malware with expert-written reports, TTPDetect recovers 85.7% of the documented TTPs and further discovers, on average, 10.5 previously unreported TTPs per malware.

</details>


### [171] [Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection](https://arxiv.org/abs/2602.06751)
*Yikun Li,Ting Zhang,Jieke Shi,Chengran Yang,Junda He,Xin Zhou,Jinfeng Jiang,Huihui Huang,Wen Bin Leow,Yide Yin,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: CPRVul是一个结合上下文分析和结构化推理的漏洞检测框架，通过智能选择相关上下文并生成推理轨迹，显著提升了漏洞检测性能


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法大多在函数级别操作，缺乏过程间上下文信息。虽然上下文对漏洞检测很重要，但原始上下文通常冗长、冗余且嘈杂，直接使用会降低模型性能

Method: CPRVul采用两阶段方法：1) 构建代码属性图提取候选上下文，使用LLM生成安全配置文件并分配相关性分数，选择高影响力上下文元素；2) 整合目标函数、选定上下文和漏洞元数据生成推理轨迹，用于微调LLMs进行基于推理的漏洞检测

Result: 在PrimeVul、TitanVul和CleanVul三个高质量漏洞数据集上，CPRVul准确率达到64.94%-73.76%，显著优于仅使用函数的基线方法（56.65%-63.68%）。在PrimeVul基准测试中达到67.78%准确率，比之前最佳方法提升22.9%

Conclusion: 仅使用原始上下文或处理后的上下文都不能有效提升强代码模型的性能，只有将处理后的上下文与结构化推理相结合才能获得显著改进。CPRVul证明了上下文感知和结构化推理在漏洞检测中的重要性

Abstract: Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.
  We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.
  We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.

</details>


### [172] [AdFL: In-Browser Federated Learning for Online Advertisement](https://arxiv.org/abs/2602.06336)
*Ahmad Alemari,Pritam Sen,Cristian Borcea*

Main category: cs.CR

TL;DR: AdFL是一个在浏览器中运行的联邦学习框架，用于学习用户广告偏好，通过聚合本地模型来展示更相关的广告，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR等在线隐私法规的实施，在线出版商需要在定向广告收入和用户隐私之间找到平衡。联邦学习可以在不共享用户原始数据的情况下，基于用户个人信息和行为信息展示定向广告。

Method: AdFL是一个在浏览器中运行的联邦学习框架，利用标准浏览器API，无需客户端安装软件。它使用浏览器中可用的特征（如广告可见性、点击率、页面停留时间、页面内容）训练模型，在发布者服务器上协调学习过程，并支持差分隐私保护本地模型参数。

Result: 实验证明AdFL在浏览器中几毫秒内就能捕获训练信息，广告可见性预测达到92.59%的AUC。使用差分隐私保护本地模型参数时，性能仅比非差分隐私版本略有下降，但仍保持足够性能。

Conclusion: AdFL展示了在浏览器中实施联邦学习进行广告偏好预测的可行性，能够在保护用户隐私的同时实现有效的广告定向，为在线出版商提供了符合隐私法规的解决方案。

Abstract: Since most countries are coming up with online privacy regulations, such as GDPR in the EU, online publishers need to find a balance between revenue from targeted advertisement and user privacy. One way to be able to still show targeted ads, based on user personal and behavioral information, is to employ Federated Learning (FL), which performs distributed learning across users without sharing user raw data with other stakeholders in the publishing ecosystem. This paper presents AdFL, an FL framework that works in the browsers to learn user ad preferences. These preferences are aggregated in a global FL model, which is then used in the browsers to show more relevant ads to users. AdFL can work with any model that uses features available in the browser such as ad viewability, ad click-through, user dwell time on pages, and page content. The AdFL server runs at the publisher and coordinates the learning process for the users who browse pages on the publisher's website. The AdFL prototype does not require the client to install any software, as it is built utilizing standard APIs available on most modern browsers. We built a proof-of-concept model for ad viewability prediction that runs on top of AdFL. We tested AdFL and the model with two non-overlapping datasets from a website with 40K visitors per day. The experiments demonstrate AdFL's feasibility to capture the training information in the browser in a few milliseconds, show that the ad viewability prediction achieves up to 92.59% AUC, and indicate that utilizing differential privacy (DP) to safeguard local model parameters yields adequate performance, with only modest declines in comparison to the non-DP variant.

</details>


### [173] [Wonderboom -- Efficient, and Censorship-Resilient Signature Aggregation for Million Scale Consensus](https://arxiv.org/abs/2602.06655)
*Zeta Avarikioti,Ray Neiheiser,Krzysztof Pietrzak,Michelle X. Yeo*

Main category: cs.CR

TL;DR: Wonderboom是一个百万级签名聚合协议，能在单个以太坊时隙内高效聚合数百万验证者签名，速度比现有方案快32倍，同时提供更高安全性保证。


<details>
  <summary>Details</summary>
Motivation: 以太坊作为全球重要金融基础设施，拥有近百万验证者，但当前协议需要约15分钟才能最终确定区块，这主要受限于大规模验证者集合的签名聚合和传播成本。现有协议存在安全缺陷，可能被攻击者利用来转移质押比例。

Method: 提出Wonderboom协议，这是首个百万级聚合协议，能够在单个以太坊时隙内高效聚合数百万验证者签名。同时开发了首个能够模拟百万级协议的仿真工具来评估协议性能。

Result: 即使在最坏情况下，Wonderboom也能在单个以太坊时隙内聚合和验证超过200万个签名，比现有协议快32倍，同时提供比现有以太坊协议更高的安全保证。

Conclusion: Wonderboom协议解决了以太坊大规模验证者集合的签名聚合瓶颈，显著提升了区块最终确定速度，同时增强了协议安全性，为以太坊性能优化提供了重要解决方案。

Abstract: Over the last years, Ethereum has evolved into a public platform that safeguards the savings of hundreds of millions of people and secures more than $650 billion in assets, placing it among the top 25 stock exchanges worldwide in market capitalization, ahead of Singapore, Mexico, and Thailand. As such, the performance and security of the Ethereum blockchain are not only of theoretical interest, but also carry significant global economic implications. At the time of writing, the Ethereum platform is collectively secured by almost one million validators highlighting its decentralized nature and underlining its economic security guarantees. However, due to this large validator set, the protocol takes around 15 minutes to finalize a block which is prohibitively slow for many real world applications. This delay is largely driven by the cost of aggregating and disseminating signatures across a validator set of this scale. Furthermore, as we show in this paper, the existing protocol that is used to aggregate and disseminate the signatures has several shortcomings that can be exploited by adversaries to shift stake proportion from honest to adversarial nodes. In this paper, we introduce Wonderboom, the first million scale aggregation protocol that can efficiently aggregate the signatures of millions of validators in a single Ethereum slot (x32 faster) while offering higher security guarantees than the state of the art protocol used in Ethereum. Furthermore, to evaluate Wonderboom, we implement the first simulation tool that can simulate such a protocol on the million scale and show that even in the worst case Wonderboom can aggregate and verify more than 2 million signatures within a single Ethereum slot.

</details>


### [174] [Know Your Scientist: KYC as Biosecurity Infrastructure](https://arxiv.org/abs/2602.06172)
*Jonathan Feldman,Tal Feldman,Annie I Anton*

Main category: cs.CR

TL;DR: 提出基于KYC的三层生物AI安全框架，从内容审查转向用户验证，通过机构担保、序列筛查和行为监控来防范蛋白质设计工具的滥用风险。


<details>
  <summary>Details</summary>
Motivation: 当前用于蛋白质设计和结构预测的生物AI工具快速发展，带来了双重用途风险。现有的模型级防护措施（如关键词过滤、输出筛查、基于内容的访问拒绝）从根本上不适合生物学领域，因为可靠的蛋白质功能预测仍无法实现，且新型威胁会刻意规避检测。

Method: 提出受金融领域反洗钱实践启发的三层KYC框架：第一层利用研究机构作为信任锚点，为附属研究人员提供担保并承担审核责任；第二层通过序列同源性搜索和功能注释进行输出筛查；第三层监控行为模式以检测与申报研究目的不一致的异常活动。

Result: 该分层方法为合法研究人员保留了访问权限，同时通过机构问责制和可追溯性提高了滥用成本。该框架可以利用现有机构基础设施立即实施，无需新的立法或监管授权。

Conclusion: 需要从内容检查转向用户验证和监控的生物AI治理新范式。KYC框架通过机构责任、序列筛查和行为监控的多层防护，在促进合法研究的同时有效防范生物AI工具的滥用风险。

Abstract: Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.

</details>


### [175] [Empirical Analysis of Adversarial Robustness and Explainability Drift in Cybersecurity Classifiers](https://arxiv.org/abs/2602.06395)
*Mona Rajhans,Vishal Khawarey*

Main category: cs.CR

TL;DR: 该论文研究了网络安全领域中机器学习模型对抗性攻击的鲁棒性和可解释性漂移问题，提出了鲁棒性指数(RI)作为量化评估指标，并通过实验验证了对抗性训练能提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在网络安全应用（如钓鱼检测和网络入侵预防）中的部署增加，这些模型容易受到对抗性扰动的影响，这些微小但有意的输入修改会降低检测准确性并损害可解释性。需要研究对抗性鲁棒性和可解释性漂移之间的关系。

Method: 1. 使用L∞有界的快速梯度符号方法(FGSM)和投影梯度下降(PGD)扰动评估模型准确性影响；2. 提出鲁棒性指数(RI)作为量化指标，定义为准确性-扰动曲线下的面积；3. 进行基于梯度的特征敏感性和基于SHAP的归因漂移分析；4. 在钓鱼网站和UNSW NB15数据集上进行实验；5. 采用对抗性训练提升模型鲁棒性。

Result: 实验显示一致的鲁棒性趋势：对抗性训练能将RI提升高达9%，同时保持干净数据的准确性。梯度特征敏感性和SHAP归因漂移分析揭示了哪些输入特征最容易受到对抗性操纵。

Conclusion: 研究发现鲁棒性和可解释性退化之间存在耦合关系，强调了在可信赖的AI驱动网络安全系统设计中定量评估的重要性。对抗性训练能有效提升模型鲁棒性而不损害干净数据性能。

Abstract: Machine learning (ML) models are increasingly deployed in cybersecurity applications such as phishing detection and network intrusion prevention. However, these models remain vulnerable to adversarial perturbations small, deliberate input modifications that can degrade detection accuracy and compromise interpretability. This paper presents an empirical study of adversarial robustness and explainability drift across two cybersecurity domains phishing URL classification and network intrusion detection. We evaluate the impact of L (infinity) bounded Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) perturbations on model accuracy and introduce a quantitative metric, the Robustness Index (RI), defined as the area under the accuracy perturbation curve. Gradient based feature sensitivity and SHAP based attribution drift analyses reveal which input features are most susceptible to adversarial manipulation. Experiments on the Phishing Websites and UNSW NB15 datasets show consistent robustness trends, with adversarial training improving RI by up to 9 percent while maintaining clean-data accuracy. These findings highlight the coupling between robustness and interpretability degradation and underscore the importance of quantitative evaluation in the design of trustworthy, AI-driven cybersecurity systems.

</details>


### [176] [AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks](https://arxiv.org/abs/2602.06534)
*Lukas Karner,Max Landauer,Markus Wurzenberger,Florian Skopik*

Main category: cs.CR

TL;DR: AlertBERT：基于自监督学习的网络安全告警分组框架，在噪声环境中优于传统时间分组方法


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统产生大量告警导致分析师疲劳，现有时间分组方法在高误报和并发攻击的大规模网络中效果不佳

Method: 提出AlertBERT框架，结合掩码语言模型和基于密度的聚类，支持实时和取证分析；开发数据增强方法模拟噪声和并发攻击

Result: AlertBERT在模拟数据集上始终优于传统时间分组技术，在识别正确告警组方面达到更高准确率

Conclusion: AlertBERT为安全运营中心提供有效的告警分组解决方案，能处理噪声环境和并发攻击，减少分析师工作负担

Abstract: Automated detection of cyber attacks is a critical capability to counteract the growing volume and sophistication of cyber attacks. However, the high numbers of security alerts issued by intrusion detection systems lead to alert fatigue among analysts working in security operations centres (SOC), which in turn causes slow reaction time and incorrect decision making. Alert grouping, which refers to clustering of security alerts according to their underlying causes, can significantly reduce the number of distinct items analysts have to consider. Unfortunately, conventional time-based alert grouping solutions are unsuitable for large scale computer networks characterised by high levels of false positive alerts and simultaneously occurring attacks. To address these limitations, we propose AlertBERT, a self-supervised framework designed to group alerts from isolated or concurrent attacks in noisy environments. Thereby, our open-source implementation of AlertBERT leverages masked-language-models and density-based clustering to support both real-time or forensic operation. To evaluate our framework, we further introduce a novel data augmentation method that enables flexible control over noise levels and simulates concurrent attack occurrences. Based on the data sets generated through this method, we demonstrate that AlertBERT consistently outperforms conventional time-based grouping techniques, achieving superior accuracy in identifying correct alert groups.

</details>


### [177] [Confundo: Learning to Generate Robust Poison for Practical RAG Systems](https://arxiv.org/abs/2602.06616)
*Haoyang Hu,Zhejun Jiang,Yueming Lyu,Junyuan Zhang,Yi Liu,Ka-Ho Chow*

Main category: cs.CR

TL;DR: Confundo是一个学习型投毒框架，通过微调大语言模型作为投毒生成器，在真实RAG系统中实现高效、鲁棒且隐蔽的攻击，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG投毒攻击在真实系统中效果严重下降，原因在于忽略了两个现实因素：内容预处理会破坏投毒内容，用户查询与攻击设计时的预期不符。这导致从业者低估风险并产生虚假安全感。

Method: 提出Confundo框架，通过微调大语言模型作为投毒生成器，学习生成高效、鲁棒且隐蔽的恶意内容。该框架支持多种攻击目标，包括操纵事实正确性、诱导偏见观点和触发幻觉。

Result: Confundo在各种数据集和RAG配置中大幅超越多种专用攻击方法，即使在存在防御措施的情况下也保持高效。同时展示了防御用例，保护网页内容免遭未经授权的RAG系统抓取。

Conclusion: Confundo框架更准确地刻画了实际RAG系统的威胁，揭示了现有攻击方法的局限性，并为系统安全提供了新的攻防视角。

Abstract: Retrieval-augmented generation (RAG) is increasingly deployed in real-world applications, where its reference-grounded design makes outputs appear trustworthy. This trust has spurred research on poisoning attacks that craft malicious content, inject it into knowledge sources, and manipulate RAG responses. However, when evaluated in practical RAG systems, existing attacks suffer from severely degraded effectiveness. This gap stems from two overlooked realities: (i) content is often processed before use, which can fragment the poison and weaken its effect, and (ii) users often do not issue the exact queries anticipated during attack design. These factors can lead practitioners to underestimate risks and develop a false sense of security. To better characterize the threat to practical systems, we present Confundo, a learning-to-poison framework that fine-tunes a large language model as a poison generator to achieve high effectiveness, robustness, and stealthiness. Confundo provides a unified framework supporting multiple attack objectives, demonstrated by manipulating factual correctness, inducing biased opinions, and triggering hallucinations. By addressing these overlooked challenges, Confundo consistently outperforms a wide range of purpose-built attacks across datasets and RAG configurations by large margins, even in the presence of defenses. Beyond exposing vulnerabilities, we also present a defensive use case that protects web content from unauthorized incorporation into RAG systems via scraping, with no impact on user experience.

</details>


### [178] [Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs](https://arxiv.org/abs/2602.06700)
*Ying Song,Balaji Palanisamy*

Main category: cs.CR

TL;DR: 提出Taipan，首个无需查询、基于迁移的多敏感属性图推理攻击框架，利用图结构内在信息泄露，在多种分布设置下均有效。


<details>
  <summary>Details</summary>
Motivation: 现有属性推理攻击(AIAs)依赖重复模型查询，这在现实场景中不切实际。更重要的是，模型中心视角忽视了仅从公开图数据本身存在的内在多敏感信息泄露漏洞。

Method: 提出Taipan框架：1) 分层攻击知识路由捕获属性间复杂关联；2) 提示引导的攻击原型精炼缓解负迁移和性能下降。还设计了专门针对图多敏感属性推理攻击的系统评估框架。

Result: 在多种真实图数据集上的实验表明，Taipan在相同分布、异构相似分布和异分布设置下均保持强大攻击性能，甚至在严格差分隐私保证下仍然有效。

Conclusion: 研究揭示了仅从公开图数据就存在严重多敏感信息泄露风险，凸显了对更鲁棒的多属性隐私保护图发布方法和数据共享实践的迫切需求。

Abstract: Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.

</details>


### [179] [A Unified Framework for LLM Watermarks](https://arxiv.org/abs/2602.06754)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Main category: cs.CR

TL;DR: 该论文提出了一个统一的约束优化框架，将现有LLM水印方法形式化，揭示了质量-多样性-检测能力的权衡，并可用于设计针对特定需求的新水印方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印算法设计各异，缺乏统一的理论框架。作者旨在建立一个原则性的约束优化问题，统一现有方法并指导新方案设计。

Method: 提出一个约束优化问题框架，将水印设计形式化为在特定约束下最大化检测能力。该框架能够推导现有水印方案，并允许使用困惑度等指标作为质量约束来设计新方案。

Result: 实验验证表明，从给定约束推导出的水印方案在该约束下始终最大化检测能力。框架成功统一了现有水印方法，并揭示了质量-多样性-检测能力的权衡关系。

Conclusion: 该研究为LLM水印提供了首个原则性统一框架，不仅解释了现有方法，还为针对特定需求设计新水印方案提供了系统方法，特别强调了质量-多样性-检测能力的三方权衡。

Abstract: LLM watermarks allow tracing AI-generated texts by inserting a detectable signal into their generated content. Recent works have proposed a wide range of watermarking algorithms, each with distinct designs, usually built using a bottom-up approach. Crucially, there is no general and principled formulation for LLM watermarking.
  In this work, we show that most existing and widely used watermarking schemes can in fact be derived from a principled constrained optimization problem. Our formulation unifies existing watermarking methods and explicitly reveals the constraints that each method optimizes. In particular, it highlights an understudied quality-diversity-power trade-off. At the same time, our framework also provides a principled approach for designing novel watermarking schemes tailored to specific requirements. For instance, it allows us to directly use perplexity as a proxy for quality, and derive new schemes that are optimal with respect to this constraint. Our experimental evaluation validates our framework: watermarking schemes derived from a given constraint consistently maximize detection power with respect to that constraint.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [180] [Circuit Diameter of Polyhedra is Strongly Polynomial](https://arxiv.org/abs/2602.06958)
*Bento Natura*

Main category: math.OC

TL;DR: 证明了多面体电路直径的强多项式上界O(m²log m)，解决了多项式Hirsch猜想的电路类比问题


<details>
  <summary>Details</summary>
Motivation: 电路直径是组合直径的自然松弛，允许沿电路方向移动而非仅沿边移动。先前上界仅为弱多项式，找到匹配此界的电路增广算法可解决线性规划的强多项式时间问题（Smale第9问题）

Method: 通过构造单调电路行走，证明多面体P={x∈ℝⁿ: Ax=b, x≥0}的电路直径上界为O(m²log m)

Result: 证明了电路直径的强多项式上界O(m²log m)，同样适用于单调电路直径

Conclusion: 解决了电路直径的强多项式上界问题，为线性规划的强多项式时间算法提供了重要理论基础

Abstract: We prove a strongly polynomial bound on the circuit diameter of polyhedra, resolving the circuit analogue of the polynomial Hirsch conjecture. Specifically, we show that the circuit diameter of a polyhedron $P = \{x\in \mathbb{R}^n:\, A x = b, \, x \ge 0\}$ with $A\in\mathbb{R}^{m\times n}$ is $O(m^2 \log m)$. Our construction yields monotone circuit walks, giving the same bound for the monotone circuit diameter.
  The circuit diameter, introduced by Borgwardt, Finhold, and Hemmecke (SIDMA 2015), is a natural relaxation of the combinatorial diameter that allows steps along circuit directions rather than only along edges. All prior upper bounds on the circuit diameter were only weakly polynomial. Finding a circuit augmentation algorithm that matches this bound would yield a strongly polynomial time algorithm for linear programming, resolving Smale's 9th problem.

</details>


### [181] [RanSOM: Second-Order Momentum with Randomized Scaling for Constrained and Unconstrained Optimization](https://arxiv.org/abs/2602.06824)
*El Mahdi Chayti*

Main category: math.OC

TL;DR: 提出RanSOM框架，通过随机化步长消除动量方法在随机优化中的曲率偏差，实现最优收敛率


<details>
  <summary>Details</summary>
Motivation: 传统动量方法（如Polyak's Heavy Ball）在随机优化中存在曲率诱导的偏差，导致收敛到次优的O(ε^{-4})速率，现有校正方法需要昂贵的辅助采样或严格的平滑性假设

Method: 提出RanSOM框架，将确定性步长替换为均值为η_t的随机步长分布，利用Stein型恒等式计算动量偏差的无偏估计，仅需单个Hessian-向量乘积与梯度联合计算。具体实现两个算法：RanSOM-E（无约束优化，使用指数分布步长）和RanSOM-B（约束优化，使用Beta分布步长保持可行性

Result: 理论分析表明，在标准有界噪声下，RanSOM恢复最优O(ε^{-3})收敛率；在重尾噪声设置(p∈(1,2])下，无需梯度裁剪即可达到最优速率

Conclusion: RanSOM通过随机化步长有效消除动量方法的曲率偏差，在保持计算效率的同时实现最优收敛性能，为随机优化提供了统一且实用的解决方案

Abstract: Momentum methods, such as Polyak's Heavy Ball, are the standard for training deep networks but suffer from curvature-induced bias in stochastic settings, limiting convergence to suboptimal $\mathcal{O}(ε^{-4})$ rates. Existing corrections typically require expensive auxiliary sampling or restrictive smoothness assumptions. We propose \textbf{RanSOM}, a unified framework that eliminates this bias by replacing deterministic step sizes with randomized steps drawn from distributions with mean $η_t$. This modification allows us to leverage Stein-type identities to compute an exact, unbiased estimate of the momentum bias using a single Hessian-vector product computed jointly with the gradient, avoiding auxiliary queries. We instantiate this framework in two algorithms: \textbf{RanSOM-E} for unconstrained optimization (using exponentially distributed steps) and \textbf{RanSOM-B} for constrained optimization (using beta-distributed steps to strictly preserve feasibility). Theoretical analysis confirms that RanSOM recovers the optimal $\mathcal{O}(ε^{-3})$ convergence rate under standard bounded noise, and achieves optimal rates for heavy-tailed noise settings ($p \in (1, 2]$) without requiring gradient clipping.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [182] [EUGens: Efficient, Unified, and General Dense Layers](https://arxiv.org/abs/2410.09771)
*Sang Min Kim,Byeongchan Kim,Arijit Sehanobish,Somnath Basu Roy Chowdhury,Rahul Kidambi,Dongseok Shim,Avinava Dubey,Snigdha Chaturvedi,Min-hwan Oh,Krzysztof Choromanski*

Main category: cs.CV

TL;DR: 提出EUGens层，一种高效、统一、通用的稠密层，通过随机特征和输入范数依赖，将全连接层复杂度从二次降到线性，减少参数和计算量，提升推理速度27%、内存效率30%。


<details>
  <summary>Details</summary>
Motivation: 全连接前馈层在神经网络中引入计算和参数瓶颈，限制了模型在实时应用和资源受限环境中的可扩展性。需要一种更高效的层结构来替代传统全连接层。

Method: 提出EUGens层，利用随机特征近似标准全连接层，并在计算中引入输入范数的直接依赖。该方法统一了现有高效全连接层扩展，通过层间知识转移技术绕过反向传播，实现高效适配。

Result: 在Transformer和MLP中集成EUGens层，在图像分类、语言模型预训练和3D场景重建等任务中，推理速度提升达27%，内存效率提升达30%。

Conclusion: EUGens层通过将计算复杂度从二次降到线性，显著提升神经网络效率，为大规模神经网络在实际场景中的可扩展部署提供了潜力。

Abstract: Efficient neural networks are essential for scaling machine learning models to real-time applications and resource-constrained environments. Fully-connected feedforward layers (FFLs) introduce computation and parameter count bottlenecks within neural network architectures. To address this challenge, in this work, we propose a new class of dense layers that generalize standard fully-connected feedforward layers, \textbf{E}fficient, \textbf{U}nified and \textbf{Gen}eral dense layers (EUGens). EUGens leverage random features to approximate standard FFLs and go beyond them by incorporating a direct dependence on the input norms in their computations. The proposed layers unify existing efficient FFL extensions and improve efficiency by reducing inference complexity from quadratic to linear time. They also lead to \textbf{the first} unbiased algorithms approximating FFLs with arbitrary polynomial activation functions. Furthermore, EuGens reduce the parameter count and computational overhead while preserving the expressive power and adaptability of FFLs. We also present a layer-wise knowledge transfer technique that bypasses backpropagation, enabling efficient adaptation of EUGens to pre-trained models. Empirically, we observe that integrating EUGens into Transformers and MLPs yields substantial improvements in inference speed (up to \textbf{27}\%) and memory efficiency (up to \textbf{30}\%) across a range of tasks, including image classification, language model pre-training, and 3D scene reconstruction. Overall, our results highlight the potential of EUGens for the scalable deployment of large-scale neural networks in real-world scenarios.

</details>


### [183] [AnyThermal: Towards Learning Universal Representations for Thermal Perception](https://arxiv.org/abs/2602.06203)
*Parv Maheshwari,Jay Karhade,Yogesh Chawla,Isaiah Adu,Florian Heisen,Andrew Porco,Andrew Jong,Yifei Liu,Santosh Pitla,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: AnyThermal是一个通用的热成像骨干网络，通过从视觉基础模型蒸馏特征，支持多种热成像任务（地点识别、分割、深度估计），无需任务特定训练，并在多种环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有热成像骨干网络通常针对特定任务在小规模数据上训练，导致其应用范围有限，只能适应特定环境和任务。需要一种通用的热成像特征提取器，能够适应多种环境和任务。

Method: 1. 从DINOv2等视觉基础模型中蒸馏特征表示到热成像编码器；2. 开发TartanRGBT平台，首个开源同步RGB-热成像数据采集平台；3. 收集TartanRGBT数据集，包含4种环境的多样化平衡数据。

Result: 在跨模态地点识别、热成像分割和单目深度估计等任务上取得最先进结果，在现有数据集上性能提升高达36%，适应室内、空中、越野、城市等多种环境。

Conclusion: AnyThermal是一个任务无关的热成像骨干网络，通过特征蒸馏和多环境数据训练，实现了广泛的适用性和优异的性能，为热成像计算机视觉提供了通用解决方案。

Abstract: We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.

</details>


### [184] [Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models](https://arxiv.org/abs/2602.06214)
*Jorge Daniel Rodríguez-Vidal,Gabriel Villalonga,Diego Porres,Antonio M. López Peña*

Main category: cs.CV

TL;DR: 提出一种可微分车辆模型框架，将动作序列转换为轨迹点，使基于动作的端到端自动驾驶模型能够在基于轨迹点的基准上进行训练和评估。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统主要分为基于轨迹点和基于动作的两类，但大多数基准协议和训练流程都是基于轨迹点的，这使得基于动作的策略难以训练和比较，阻碍了其发展。

Method: 提出一种新颖的可微分车辆模型框架，通过将预测的动作序列（油门、转向、刹车）展开为对应的自车坐标系轨迹点轨迹，同时在轨迹点空间中进行监督。

Result: 在多个具有挑战性的基准测试中进行了广泛评估，观察到相对于基线的持续改进。特别是在NAVSIM navhard基准上，该方法达到了最先进的性能。

Conclusion: 该框架首次使基于动作的架构能够在基于轨迹点的基准上进行训练和评估，无需修改底层评估协议，弥合了轨迹点与动作之间的差距。

Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.

</details>


### [185] [Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings](https://arxiv.org/abs/2602.06218)
*Grégoire Dhimoïla,Thomas Fel,Victor Boutin,Agustin Picard*

Main category: cs.CV

TL;DR: 该研究提出一种对齐稀疏自编码器（SAE），通过能量一致性假设分析视觉语言模型的共享嵌入空间几何结构，发现稀疏双模态原子承载跨模态对齐信号，单模态原子解释模态间隙，移除单模态原子可消除间隙而不损害性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在图像和文本对齐方面取得了显著成功，但其共享嵌入空间的几何结构仍然缺乏深入理解。研究者希望探索这种几何结构，以更好地理解跨模态表示的本质。

Method: 提出同能量假设（Iso-Energy Assumption）：真正共享的概念在不同模态中应具有相同的平均能量。基于此假设开发了对齐稀疏自编码器（SAE），在训练中鼓励能量一致性同时保持重构能力。在已知真实标签的受控数据上进行验证，然后应用于基础VLMs进行分析。

Result: 研究发现：1）稀疏双模态原子承载全部跨模态对齐信号；2）单模态原子作为模态特定偏置，完全解释模态间隙；3）移除单模态原子可消除模态间隙而不损害性能；4）将向量运算限制在双模态子空间可产生分布内编辑和改进检索。

Conclusion: 正确的归纳偏置既能保持模型保真度，又能使潜在几何结构变得可解释和可操作。该框架为理解VLMs的跨模态对齐提供了新的几何视角和实用工具。

Abstract: Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.

</details>


### [186] [Forest canopy height estimation from satellite RGB imagery using large-scale airborne LiDAR-derived training data and monocular depth estimation](https://arxiv.org/abs/2602.06503)
*Yongkang Lai,Xihan Mu,Tim R. McVicar,Dasheng Fan,Donghui Xie,Shanxin Guo,Wenli Huang,Tianjie Zhao,Guangjian Yan*

Main category: cs.CV

TL;DR: 使用深度估计模型Depth Anything V2，结合大规模机载LiDAR数据训练，可从卫星RGB影像直接估计连续森林冠层高度，精度优于现有全球产品。


<details>
  <summary>Details</summary>
Motivation: 大规模高分辨率森林冠层高度制图对理解碳水平衡至关重要。现有星载LiDAR（如ICESat-2、GEDI）数据稀疏且有不确定性，而机载/UAV LiDAR数据精度高但覆盖有限。需要一种利用丰富卫星RGB影像和机载LiDAR数据的方法来生成连续高分辨率冠层高度图。

Method: 使用约16,000 km²来自多国公开机载LiDAR点云生成的冠层高度模型（CHMs），结合3米分辨率PlanetScope和机载RGB影像，训练先进的单目深度估计模型Depth Anything V2。训练后的模型Depth2CHM可直接从PlanetScope RGB影像估计连续CHMs。

Result: 在中国（约1 km²）和美国（约116 km²）独立验证点验证：中国站点偏差0.59 m、RMSE 2.54 m；美国站点偏差0.41 m、RMSE 5.75 m。相比现有全球米级分辨率CHM产品，平均绝对误差降低约1.5 m，RMSE降低约2 m。

Conclusion: 使用大规模机载LiDAR数据训练的单目深度估计网络，为从卫星RGB影像进行高分辨率连续森林冠层高度估计提供了有前景且可扩展的途径。

Abstract: Large-scale, high-resolution forest canopy height mapping plays a crucial role in understanding regional and global carbon and water cycles. Spaceborne LiDAR missions, including the Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) and the Global Ecosystem Dynamics Investigation (GEDI), provide global observations of forest structure but are spatially sparse and subject to inherent uncertainties. In contrast, near-surface LiDAR platforms, such as airborne and unmanned aerial vehicle (UAV) LiDAR systems, offer much finer measurements of forest canopy structure, and a growing number of countries have made these datasets openly available. In this study, a state-of-the-art monocular depth estimation model, Depth Anything V2, was trained using approximately 16,000 km2 of canopy height models (CHMs) derived from publicly available airborne LiDAR point clouds and related products across multiple countries, together with 3 m resolution PlanetScope and airborne RGB imagery. The trained model, referred to as Depth2CHM, enables the estimation of spatially continuous CHMs directly from PlanetScope RGB imagery. Independent validation was conducted at sites in China (approximately 1 km2) and the United States (approximately 116 km2). The results showed that Depth2CHM could accurately estimate canopy height, with biases of 0.59 m and 0.41 m and root mean square errors (RMSEs) of 2.54 m and 5.75 m for these two sites, respectively. Compared with an existing global meter-resolution CHM product, the mean absolute error is reduced by approximately 1.5 m and the RMSE by approximately 2 m. These results demonstrated that monocular depth estimation networks trained with large-scale airborne LiDAR-derived canopy height data provide a promising and scalable pathway for high-resolution, spatially continuous forest canopy height estimation from satellite RGB imagery.

</details>


### [187] [NECromancer: Breathing Life into Skeletons via BVH Animation](https://arxiv.org/abs/2602.06548)
*Mingxi Xu,Qi Wang,Zhengyu Wen,Phong Dao Thien,Zhengyu Li,Ning Zhang,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: NECromancer是一个通用的运动标记化框架，可直接处理任意BVH骨骼，实现跨物种运动分析和合成。


<details>
  <summary>Details</summary>
Motivation: 现有运动标记化方法大多局限于特定物种的骨骼结构，无法适应多样化的形态学变化，限制了跨物种运动模型的通用性。

Method: 包含三个核心组件：1) 本体感知骨骼图编码器(OwO)，从BVH文件编码结构先验；2) 拓扑无关标记器(TAT)，将运动序列压缩为通用离散表示；3) 统一BVH数据集(UvU)，聚合异构骨骼的运动数据。

Result: NEC在高度压缩下实现高保真重建，有效解耦运动与骨骼结构，支持跨物种运动迁移、组合、去噪、生成和文本-运动检索等任务。

Conclusion: NEC建立了一个统一的运动分析和合成框架，能够处理多样化形态学，为通用运动模型提供了基础。

Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/

</details>


### [188] [DAVE: Distribution-aware Attribution via ViT Gradient Decomposition](https://arxiv.org/abs/2602.06613)
*Adam Wróbel,Siddhartha Gairola,Jacek Tabor,Bernt Schiele,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: DAVE是一种基于ViT梯度分解的归因方法，通过利用ViT架构特性分离局部等变稳定分量与架构诱导伪影，解决ViT高分辨率归因图生成难题。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在计算机视觉中已成为主导架构，但为其生成稳定且高分辨率的归因图仍然具有挑战性。架构组件如patch嵌入和注意力路由会在像素级解释中引入结构化伪影，导致现有方法只能依赖粗糙的patch级归因。

Method: 提出DAVE方法，基于输入梯度的结构化分解，利用ViT的架构特性，分离出输入-输出映射中局部等变且稳定的分量，将其与架构诱导的伪影和其他不稳定源隔离开来。

Result: 该方法在数学上具有理论基础，能够生成更稳定、更高分辨率的归因图，解决了ViT归因中的结构化伪影问题。

Conclusion: DAVE为ViT提供了一种数学基础扎实的归因方法，通过梯度分解有效处理了架构组件带来的归因挑战，提升了归因图的质量和稳定性。

Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \textit{(\underline{D}istribution-aware \underline{A}ttribution via \underline{V}iT Gradient D\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.

</details>


### [189] [CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis](https://arxiv.org/abs/2602.06674)
*Yonghao Si,Xingyuan Zeng,Zhao Chen,Libin Zheng,Caleb Chen Cao,Lei Chen,Jian Yin*

Main category: cs.CV

TL;DR: CytoCrowd是一个新的细胞学分析公共基准数据集，包含446张高分辨率图像，每张图像都有四位病理学家的原始冲突标注和由资深专家建立的高质量金标准真值，可用于标准计算机视觉任务和标注聚合算法评估。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分析数据集存在关键缺陷：要么只提供单一干净的真值（掩盖了真实世界的专家分歧），要么提供多个标注但没有单独的金标准进行客观评估。需要填补这一空白。

Method: 构建CytoCrowd数据集，包含446张高分辨率细胞学图像，每张图像收集四位独立病理学家的原始冲突标注，并由资深专家建立单独的高质量金标准真值。

Result: CytoCrowd数据集具有双重结构：既可作为标准计算机视觉任务（如目标检测和分类）的基准，又可作为评估标注聚合算法的真实测试平台。提供了两种任务的综合基线结果。

Conclusion: CytoCrowd展示了细胞学分析中的挑战，证明了其作为开发下一代医学图像分析模型资源的价值，填补了现有数据集的空白。

Abstract: High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.

</details>


### [190] [Revisiting Emotions Representation for Recognition in the Wild](https://arxiv.org/abs/2602.06778)
*Joao Baptista Cardia Neto,Claudio Ferrari,Stefano Berretti*

Main category: cs.CV

TL;DR: 提出一种将面部情绪识别从单标签分类转为概率分布学习的方法，通过VAD空间映射自动重标注现有数据集，将复杂情绪状态描述为多个情绪类别的概率分布。


<details>
  <summary>Details</summary>
Motivation: 传统面部情绪识别将情绪简化为六种基本情绪的单标签分类，这过度简化了真实世界中复杂的、混合的情绪状态。现有研究数据集通常只标注单一情绪类别，无法捕捉情绪的多面性和感知的模糊性。

Method: 提出一种自动重标注现有数据集的方法：1) 利用研究结果将基本和复合情绪映射到VAD空间中的概率分布；2) 给定带有VAD标注的面部图像，计算其属于每个情绪分布的可能性；3) 将情绪状态描述为多个情绪的概率混合分布。

Result: 通过初步实验展示了该方法的优势，提供了新的研究方向。作者公开了AffectNet-B数据集的重标注数据，可在GitHub上获取。

Conclusion: 该方法能够更丰富地描述情绪状态，同时考虑情绪感知的模糊性，为面部情绪识别提供了从单标签分类到概率分布学习的新方向。

Abstract: Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.

</details>


### [191] [RAIGen: Rare Attribute Identification in Text-to-Image Generative Models](https://arxiv.org/abs/2602.06806)
*Silpa Vadakkeeveetil Sreelatha,Dan Wang,Serge Belongie,Muhammad Awais,Anjan Dutta*

Main category: cs.CV

TL;DR: RAIGen是首个无监督发现扩散模型中稀有属性的框架，通过稀疏自编码器和少数属性度量识别被低估的特征，支持跨架构审计和生成时定向增强。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么关注预定义公平性类别中的偏见缓解，要么识别主导输出的多数属性，但都忽略了发现数据分布中被低估的稀有或少数属性（社会、文化或风格特征）。

Method: 提出RAIGen框架，利用Matryoshka稀疏自编码器和结合神经元激活频率与语义独特性的少数属性度量，识别可解释神经元，其最高激活图像能揭示被低估的属性。

Result: 实验表明RAIGen能在Stable Diffusion中发现超越固定公平性类别的属性，可扩展到SDXL等更大模型，支持跨架构的系统性审计，并能在生成时定向增强稀有属性。

Conclusion: RAIGen为无监督发现扩散模型中的稀有属性提供了首个框架，填补了现有偏见缓解和识别方法的空白，支持更全面的模型审计和可控生成。

Abstract: Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.

</details>


### [192] [Reliable Mislabel Detection for Video Capsule Endoscopy Data](https://arxiv.org/abs/2602.06938)
*Julia Werner,Julius Oexle,Oliver Bause,Maxime Le Floch,Franz Brinkmann,Hannah Tolle,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 提出一个医学影像数据集的错误标签检测框架，在胶囊内镜数据集上验证，通过专家重新标注确认，提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像标注困难，专家资源有限，类别边界模糊，导致数据集存在错误标签，影响深度学习模型性能。

Method: 开发错误标签检测框架，在两大公开胶囊内镜数据集上验证，识别潜在错误样本，由三位经验丰富的胃肠病专家重新标注确认。

Result: 框架成功检测错误标签数据，清洗后数据集相比基线方法显著提升异常检测性能。

Conclusion: 提出的错误标签检测框架能有效识别医学数据集中的标注错误，清洗后改善模型性能，对医学影像分析有重要价值。

Abstract: The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [193] [Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)](https://arxiv.org/abs/2602.06680)
*Ali Rasim Kocal,Michael Schwarz,Simmo Saan,Helmut Seidl*

Main category: cs.PL

TL;DR: 提出了一种参数化任务粒度的并行不动点引擎，支持两种并行化哲学：立即方法和独立方法，并在Goblint框架中实现，显著减少了大型实际程序的分析时间。


<details>
  <summary>Details</summary>
Motivation: 不动点迭代是静态分析器的核心算法，并行化可以显著减少分析时间。现有方法通常预先固定任务粒度（如程序线程或过程级别），导致引擎性能受限。需要一种参数化任务粒度的通用并行不动点引擎。

Method: 基于支持混合流敏感性的TD求解器，实现两种并行化哲学：1) 立即方法：所有任务访问单个线程安全哈希表维护求解器状态；2) 独立方法：每个任务有自己的状态，通过发布/订阅数据结构与其他任务交换数据。两种方法都基于任务池调度固定数量工作线程。

Result: 在静态分析框架Goblint中实现了两种并行化方法，并在大型实际程序上进行了测试，获得了显著的性能提升。

Conclusion: 提出的参数化任务粒度并行不动点引擎能够灵活适应不同分析场景，两种并行化哲学各有优势，为静态分析提供了高效的并行解决方案。

Abstract: Fixpoint iteration constitutes the algorithmic core of static analyzers. Parallelizing the fixpoint engine can significantly reduce analysis times. Previous approaches typically fix the granularity of tasks upfront, e.g., at the level of program threads or procedures - yielding an engine permanently stuck in one gear. Instead, we propose to parallelize a generic fixpoint engine in a way that is parametric in the task granularity - meaning that our engine can be run in different gears. We build on the top-down solver TD, extended with support for mixed-flow sensitivity, and realize two competing philosophies for parallelization, both building on a task pool that schedules tasks to a fixed number of workers. The nature of tasks differs between the philosophies. In the immediate approach, all tasks access a single thread-safe hash table maintaining solver state, while in the independent approach, each task has its own state and exchanges data with other tasks via a publish/subscribe data structure. We have equipped the fixpoint engine of the static analysis framework Goblint with implementations following both philosophies and report on our results for large real-world programs.

</details>


### [194] [Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI](https://arxiv.org/abs/2602.06934)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: GLP是一种并发逻辑编程语言，用于分布式草根平台，通过读者-写者变量对实现线性逻辑和future/promise语义。论文提出了实现就绪的确定性操作语义dGLP和madGLP，并证明了其正确性，为AI实现工作站和智能手机平台提供了形式化规范。


<details>
  <summary>Details</summary>
Motivation: 为草根平台（分布式系统，智能手机点对点通信）设计实现就绪的操作语义，使AI能够基于形式化规范开发工作站和智能手机平台的GLP实现。

Method: 1. 开发dGLP：单代理GLP的确定性操作语义，证明其相对于并发GLP操作语义的正确性；2. 开发madGLP：多代理GLP的确定性操作语义（代理级别确定性），证明其相对于maGLP操作语义的正确性。

Result: 成功开发了dGLP和madGLP两种实现就绪的操作语义，dGLP已用于AI开发工作站GLP实现，madGLP正用于AI开发智能手机maGLP实现。

Conclusion: 论文为GLP语言提供了数学基础，通过确定性操作语义为AI实现提供了可靠的形式化规范，支持草根平台在分布式环境中的实现。

Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.
  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.
  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.

</details>


### [195] [Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering](https://arxiv.org/abs/2602.06142)
*Amir H. Ashouri,Shayan Shirahmad Gale Bagi,Kavin Satheeskumar,Tejas Srikanth,Jonathan Zhao,Ibrahim Saidoun,Ziwen Wang,Bryan Chan,Tomasz S. Czajkowski*

Main category: cs.PL

TL;DR: Protean Compiler是一个集成到LLVM中的敏捷框架，通过在细粒度代码段上实现内置的相位排序能力，解决了编译器优化顺序的长期挑战，平均获得4.1%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 相位排序问题自1970年代以来一直是一个长期挑战，具有巨大的优化空间和无界特性。传统方法依赖手动调整的算法，需要大量工作来适应不同的基准测试套件。虽然过去20年机器学习被用于改进优化选择和排序，但这些方法没有无缝集成到编译器中，也未能应用于细粒度代码段。

Method: 提出Protean Compiler框架，为LLVM提供细粒度范围内的内置相位排序能力。该框架包含一个完整的库，包含140多个手工制作的静态特征收集方法，覆盖不同范围。支持与第三方机器学习框架和大型语言模型轻松集成。

Result: 在Cbench基准测试中，相对于LLVM的O3优化级别，平均获得4.1%的速度提升，某些应用最高达到15.7%的提升，仅增加几秒钟的构建时间。与第三方ML框架集成后，在Cbench的Susan和Jpeg应用上分别获得10.1%和8.5%的速度提升。

Conclusion: Protean Compiler无缝集成到LLVM中，可作为增强的全功能编译器使用。该框架解决了相位排序的长期挑战，实现了细粒度的优化决策，并计划在近期开源给社区。

Abstract: The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [196] [Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations](https://arxiv.org/abs/2602.06643)
*Ruiqian Nai,Boyuan Zheng,Junming Zhao,Haodong Zhu,Sicong Dai,Zunhao Chen,Yihang Hu,Yingdong Hu,Tong Zhang,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: HuMI框架通过便携硬件采集人体全身运动数据，驱动分层学习管道，实现人形机器人多样化的全身操控技能学习，相比遥操作提高3倍数据收集效率，在未见环境中达到70%成功率。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人全身操控方法主要依赖遥操作或视觉模拟到现实的强化学习，存在硬件物流复杂和奖励工程困难的问题，导致自主技能有限且通常局限于受控环境。

Method: 提出Humanoid Manipulation Interface (HuMI)框架：1) 使用便携硬件采集丰富的全身运动数据，无需机器人参与；2) 构建分层学习管道，将人体运动转化为灵巧可行的人形机器人技能。

Result: 在五个全身任务（跪姿、蹲姿、投掷、行走、双手操控）上实验显示：HuMI相比遥操作提高3倍数据收集效率，在未见环境中达到70%成功率。

Conclusion: HuMI提供了一个便携高效的学习框架，能够实现人形机器人在多样化环境中的全身操控技能学习，解决了当前方法在数据收集效率和泛化能力方面的限制。

Abstract: Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.

</details>


### [197] [SuReNav: Superpixel Graph-based Constraint Relaxation for Navigation in Over-constrained Environments](https://arxiv.org/abs/2602.06807)
*Keonyoung Koh,Moonkyeong Jung,Samuel Seungsup Lee,Daehyung Park*

Main category: cs.RO

TL;DR: SuReNav：基于超像素图的约束松弛导航方法，模仿人类安全高效导航，在复杂环境中实现最佳路径规划


<details>
  <summary>Details</summary>
Motivation: 解决半静态环境中过度约束的规划问题。传统方法依赖预定义区域成本，泛化能力有限，且难以准确识别可通过区域而不高估风险。

Method: 提出SuReNav框架：1) 生成带区域约束的超像素图地图；2) 使用图神经网络基于人类演示进行区域约束松弛，实现安全高效导航；3) 松弛、规划和执行交替进行完成导航。

Result: 在2D语义地图和OpenStreetMap的3D地图上评估，相比最先进基线方法获得最高的人类相似度评分，在效率与安全性间保持平衡。在四足机器人Spot上验证了实际城市导航的可扩展性和泛化性能。

Conclusion: SuReNav通过模仿人类导航行为，有效解决了半静态环境中的过度约束规划问题，实现了安全高效的最佳路径规划，并在真实机器人平台上验证了实用性。

Abstract: We address the over-constrained planning problem in semi-static environments. The planning objective is to find a best-effort solution that avoids all hard constraint regions while minimally traversing the least risky areas. Conventional methods often rely on pre-defined area costs, limiting generalizations. Further, the spatial continuity of navigation spaces makes it difficult to identify regions that are passable without overestimation. To overcome these challenges, we propose SuReNav, a superpixel graph-based constraint relaxation and navigation method that imitates human-like safe and efficient navigation. Our framework consists of three components: 1) superpixel graph map generation with regional constraints, 2) regional-constraint relaxation using graph neural network trained on human demonstrations for safe and efficient navigation, and 3) interleaving relaxation, planning, and execution for complete navigation. We evaluate our method against state-of-the-art baselines on 2D semantic maps and 3D maps from OpenStreetMap, achieving the highest human-likeness score of complete navigation while maintaining a balanced trade-off between efficiency and safety. We finally demonstrate its scalability and generalization performance in real-world urban navigation with a quadruped robot, Spot.

</details>


### [198] [DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos](https://arxiv.org/abs/2602.06949)
*Shenyuan Gao,William Liang,Kaiyuan Zheng,Ayaan Malik,Seonghyeon Ye,Sihyun Yu,Wei-Cheng Tseng,Yuzhu Dong,Kaichun Mo,Chen-Hsuan Lin,Qianli Ma,Seungjun Nah,Loic Magne,Jiannan Xiang,Yuqi Xie,Ruijie Zheng,Dantong Niu,You Liang Tan,K. R. Zentner,George Kurian,Suneel Indupuru,Pooya Jannaty,Jinwei Gu,Jun Zhang,Jitendra Malik,Pieter Abbeel,Ming-Yu Liu,Yuke Zhu,Joel Jang,Linxi "Jim" Fan*

Main category: cs.RO

TL;DR: DreamDojo是一个从4.4万小时人类第一视角视频中学习交互和灵巧控制的基础世界模型，通过连续潜在动作解决动作标签稀缺问题，实现了物理理解和精确动作控制。


<details>
  <summary>Details</summary>
Motivation: 模拟不同环境下的动作结果对通用智能体发展至关重要，但灵巧机器人任务的世界动力学建模面临数据覆盖有限和动作标签稀缺的挑战。

Method: 使用44k小时人类第一视角视频进行预训练，引入连续潜在动作作为统一代理动作来增强无标签视频中的交互知识迁移，通过小规模目标机器人数据进行后训练，并设计了实时加速蒸馏流程。

Result: DreamDojo展现出强大的物理理解和精确动作控制能力，蒸馏后达到10.81 FPS的实时速度并提升上下文一致性，在多个OOD基准测试中验证了其在开放世界、接触丰富任务模拟中的有效性。

Conclusion: 该工作实现了基于生成世界模型的重要应用（实时遥操作、策略评估、基于模型的规划），为通用机器人世界模型的发展铺平了道路。

Abstract: Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [199] [Are Deep Learning Based Hybrid PDE Solvers Reliable? Why Training Paradigms and Update Strategies Matter](https://arxiv.org/abs/2602.06842)
*Yuhan Wu,Jan Willem van Beek,Victorita Dolean,Alexander Heinlein*

Main category: math.NA

TL;DR: 该论文研究了基于深度学习的混合迭代方法在科学计算中的可靠性问题，发现训练范式和对齐策略对性能影响巨大，并提出物理感知的Anderson加速方法来解决神经网络更新消失而物理残差大的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习混合迭代方法虽然能加速收敛，但常常陷入虚假固定点，即神经更新消失而物理残差仍然很大，这引发了在科学计算中可靠性的担忧。

Method: 研究DeepONet-based HINTS和FFT-based FNS两种方法，分析训练目标与求解器动态及问题物理的对齐问题，并提出物理感知的Anderson加速方法，该方法最小化物理残差而非固定点更新。

Result: 数值实验证实，物理感知的Anderson加速方法能在显著更少的迭代次数中恢复可靠收敛，解决了神经网络更新消失的问题。

Conclusion: AI-based PDE求解器的可靠性不仅取决于架构设计，更关键的是物理信息化的训练和迭代设计，这为相关争议提供了具体答案。

Abstract: Deep learning-based hybrid iterative methods (DL-HIMs) integrate classical numerical solvers with neural operators, utilizing their complementary spectral biases to accelerate convergence. Despite this promise, many DL-HIMs stagnate at false fixed points where neural updates vanish while the physical residual remains large, raising questions about reliability in scientific computing. In this paper, we provide evidence that performance is highly sensitive to training paradigms and update strategies, even when the neural architecture is fixed. Through a detailed study of a DeepONet-based hybrid iterative numerical transferable solver (HINTS) and an FFT-based Fourier neural solver (FNS), we show that significant physical residuals can persist when training objectives are not aligned with solver dynamics and problem physics. We further examine Anderson acceleration (AA) and demonstrate that its classical form is ill-suited for nonlinear neural operators. To overcome this, we introduce physics-aware Anderson acceleration (PA-AA), which minimizes the physical residual rather than the fixed-point update. Numerical experiments confirm that PA-AA restores reliable convergence in substantially fewer iterations. These findings provide a concrete answer to ongoing controversies surrounding AI-based PDE solvers: reliability hinges not only on architectures but on physically informed training and iteration design.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [200] [Optimal rates for density and mode estimation with expand-and-sparsify representations](https://arxiv.org/abs/2602.06175)
*Kaushik Sinha,Christopher Tosh*

Main category: math.ST

TL;DR: 论文研究了扩展-稀疏化表示在密度估计和模式估计中的理论性能，证明了该表示能实现最优收敛速率


<details>
  <summary>Details</summary>
Motivation: 扩展-稀疏化表示是模拟动物感知系统中观察到的稀疏表示现象的理论模型，但对其在基本统计问题中的适用性缺乏理论分析

Method: 使用扩展-稀疏化表示：将输入通过随机线性投影映射到高维空间，然后只保留最大的k个条目（k远小于维度m），得到k稀疏的{0,1}向量

Result: 对于密度估计，该表示的简单线性函数能实现极小极大最优的ℓ∞收敛速率；对于模式估计，基于密度估计器的简单算法能在温和条件下以最优速率（至多对数因子）恢复单个或多个模式

Conclusion: 扩展-稀疏化表示在理论上是有效的，能够为密度估计和模式估计这两个基本统计问题提供最优或接近最优的算法

Abstract: Expand-and-sparsify representations are a class of theoretical models that capture sparse representation phenomena observed in the sensory systems of many animals. At a high level, these representations map an input $x \in \mathbb{R}^d$ to a much higher dimension $m \gg d$ via random linear projections before zeroing out all but the $k \ll m$ largest entries. The result is a $k$-sparse vector in $\{0,1\}^m$. We study the suitability of this representation for two fundamental statistical problems: density estimation and mode estimation. For density estimation, we show that a simple linear function of the expand-and-sparsify representation produces an estimator with minimax-optimal $\ell_{\infty}$ convergence rates. In mode estimation, we provide simple algorithms on top of our density estimator that recover single or multiple modes at optimal rates up to logarithmic factors under mild conditions.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [201] [Envy-Free Allocation of Indivisible Goods via Noisy Queries](https://arxiv.org/abs/2602.06361)
*Zihan Li,Yan Hao Ling,Jonathan Scarlett,Warut Suksompong*

Main category: cs.GT

TL;DR: 研究在噪声查询下公平分配不可分物品的问题，针对两智能体高斯噪声场景，推导了寻找无嫉妒分配所需查询次数的上下界，与物品数量m和最优分配的负嫉妒度Δ相关。


<details>
  <summary>Details</summary>
Motivation: 现实世界中智能体对物品的估值通常无法直接观测，只能通过有噪声的查询来获取。研究在这种噪声环境下如何高效实现公平分配具有重要实际意义。

Method: 采用非自适应查询策略和基于阈值的分配算法，该算法在多项式时间内运行。同时考虑了自适应查询场景来建立下界。

Result: 当Δ不太小时（Δ≫m^{1/4}），最优查询次数按m^{2.5}/Δ^2缩放（忽略对数因子）。上界基于非自适应查询和多项式时间算法，下界适用于自适应查询和任意计算时间。

Conclusion: 在噪声查询环境下，公平分配问题的查询复杂度与物品数量和最优分配的负嫉妒度密切相关，为实际应用中的资源分配提供了理论指导。

Abstract: We introduce a problem of fairly allocating indivisible goods (items) in which the agents' valuations cannot be observed directly, but instead can only be accessed via noisy queries. In the two-agent setting with Gaussian noise and bounded valuations, we derive upper and lower bounds on the required number of queries for finding an envy-free allocation in terms of the number of items, $m$, and the negative-envy of the optimal allocation, $Δ$. In particular, when $Δ$ is not too small (namely, $Δ\gg m^{1/4}$), we establish that the optimal number of queries scales as $\frac{\sqrt m }{(Δ/ m)^2} = \frac{m^{2.5}}{Δ^2}$ up to logarithmic factors. Our upper bound is based on non-adaptive queries and a simple thresholding-based allocation algorithm that runs in polynomial time, while our lower bound holds even under adaptive queries and arbitrary computation time.

</details>


### [202] [Fair Transit Stop Placement: A Clustering Perspective and Beyond](https://arxiv.org/abs/2602.06776)
*Haris Aziz,Ling Gai,Yuhang Guo,Jeremy Vollen*

Main category: cs.GT

TL;DR: 研究度量空间中公交站点选址问题的公平性，通过合理代表性和核心概念分析，建立与公平聚类的结构对应关系，提出参数化算法在JR和核心之间权衡。


<details>
  <summary>Details</summary>
Motivation: 研究公交站点选址问题中的公平性，探索如何在乘客直接步行和使用公交服务之间平衡，通过合理代表性(justified representation, JR)和核心(core)概念来确保公平分配。

Method: 1. 建立公交站点选址与公平聚类的结构对应关系；2. 提出扩展成本算法(Expanding Cost Algorithm)获得JR的紧致近似；3. 设计参数化算法在JR和核心之间进行可调权衡；4. 使用小规模市场拼车数据进行实验分析。

Result: 1. 证明聚类中的常数因子比例公平近似可保证核心的双参数常数因子近似；2. 建立JR近似性的下界为1.366，聚类算法不能获得优于3的近似；3. 扩展成本算法获得紧致的2.414-近似JR，但无有界核心保证；4. 参数化算法实现JR和核心之间的可调权衡。

Conclusion: 公交站点选址的公平性可通过合理代表性和核心概念分析，与公平聚类存在结构对应关系。提出的参数化算法能够在JR和核心之间实现可调权衡，为公平交通规划提供理论框架。

Abstract: We study the transit stop placement (TrSP) problem in general metric spaces, where agents travel between source-destination pairs and may either walk directly or utilize a shuttle service via selected transit stops. We investigate fairness in TrSP through the lens of justified representation (JR) and the core, and uncover a structural correspondence with fair clustering. Specifically, we show that a constant-factor approximation to proportional fairness in clustering can be used to guarantee a constant-factor biparameterized approximation to core. We establish a lower bound of 1.366 on the approximability of JR, and moreover show that no clustering algorithm can approximate JR within a factor better than 3. Going beyond clustering, we propose the Expanding Cost Algorithm, which achieves a tight 2.414-approximation for JR, but does not give any bounded core guarantee. In light of this, we introduce a parameterized algorithm that interpolates between these approaches, and enables a tunable trade-off between JR and core. Finally, we complement our results with an experimental analysis using small-market public carpooling data.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [203] [Algebraic Reduction to Improve an Optimally Bounded Quantum State Preparation Algorithm](https://arxiv.org/abs/2602.06535)
*Giacomo Belli,Michele Amoretti*

Main category: quant-ph

TL;DR: 提出一种新的量子态制备算法，通过分离实部和虚部，在辅助量子比特可用时减少电路深度、总门数和CNOT门数量。


<details>
  <summary>Details</summary>
Motivation: 量子态制备是许多量子算法的关键子程序，现有算法如Sun等人的方法虽然已知是最优边界，但仍需进一步优化电路复杂度。特别是在有辅助量子比特可用时，可以设计更高效的算法。

Method: 提出一种更简单的代数分解方法，将目标态的实部和虚部分离制备。关键创新是每个均匀控制门只使用单个Λ算子，而不是原始分解中的三个。这减少了电路深度、总门数和CNOT门数量。

Result: 新算法在有m个辅助量子比特时，相比现有方法减少了电路复杂度。通过PennyLane库实现并在模拟环境中测试，适用于稠密和稀疏量子态，包括随机态和物理相关态。与Möttönen等人的无辅助量子比特算法相比，展示了有趣的性能改进。

Conclusion: 提出的新量子态制备算法通过分离实虚部和优化均匀控制门实现，在有辅助量子比特时显著降低了电路复杂度，为量子态制备提供了更高效的解决方案。

Abstract: The preparation of $n$-qubit quantum states is a cross-cutting subroutine for many quantum algorithms, and the effort to reduce its circuit complexity is a significant challenge. In the literature, the quantum state preparation algorithm by Sun et al. is known to be optimally bounded, defining the asymptotically optimal width-depth trade-off bounds with and without ancillary qubits. In this work, a simpler algebraic decomposition is proposed to separate the preparation of the real part of the desired state from the complex one, resulting in a reduction in terms of circuit depth, total gates, and CNOT count when $m$ ancillary qubits are available. The reduction in complexity is due to the use of a single operator $Λ$ for each uniformly controlled gate, instead of the three in the original decomposition. Using the PennyLane library, this new algorithm for state preparation has been implemented and tested in a simulated environment for both dense and sparse quantum states, including those that are random and of physical interest. Furthermore, its performance has been compared with that of Möttönen et al.'s algorithm, which is a de facto standard for preparing quantum states in cases where no ancillary qubits are used, highlighting interesting lines of development.

</details>


### [204] [Warm Starts, Cold States: Exploiting Adiabaticity for Variational Ground-States](https://arxiv.org/abs/2602.06137)
*Ricard Puig,Berta Casas,Alba Cervera-Lierta,Zoë Holmes,Adrián Pérez-Salinas*

Main category: quant-ph

TL;DR: 提出一种基于哈密顿量逐步变形的迭代策略，结合VQE和绝热原理，通过解决一系列中间问题来追踪基态流形，提高基态制备的可靠性和可训练性。


<details>
  <summary>Details</summary>
Motivation: 可靠制备多体基态是量子计算中的关键任务，但传统变分方法（如VQE）在复杂能量景观中容易陷入局部最优或遭遇贫瘠高原问题。

Method: 提出迭代策略：基于哈密顿量逐步变形（离散化），将VQE与绝热原理结合，通过解决一系列中间问题来追踪基态流形，避免直接优化困难的目标系统。

Result: 理论证明：在系统远离能隙闭合时，该方法在整个变形过程中保持可训练性。数值模拟（包括散粒噪声影响）显示该方法能一致收敛到目标基态。

Conclusion: 通过哈密顿量逐步变形策略，结合VQE和绝热原理，能够有效解决基态制备中的训练难题，提高可靠性和可扩展性。

Abstract: Reliable preparation of many-body ground states is an essential task in quantum computing, with applications spanning areas from chemistry and materials modeling to quantum optimization and benchmarking. A variety of approaches have been proposed to tackle this problem, including variational methods. However, variational training often struggle to navigate complex energy landscapes, frequently encountering suboptimal local minima or suffering from barren plateaus. In this work, we introduce an iterative strategy for ground-state preparation based on a stepwise (discretized) Hamiltonian deformation. By complementing the Variational Quantum Eigensolver (VQE) with adiabatic principles, we demonstrate that solving a sequence of intermediate problems facilitates tracking the ground-state manifold toward the target system, even as we scale the system size. We provide a rigorous theoretical foundation for this approach, proving a lower bound on the loss variance that suggests trainability throughout the deformation, provided the system remains away from gap closings. Numerical simulations, including the effects of shot noise, confirm that this path-dependent tracking consistently converges to the target ground state.

</details>


### [205] [HyQuRP: Hybrid quantum-classical neural network with rotational and permutational equivariance for 3D point clouds](https://arxiv.org/abs/2602.06381)
*Semin Park,Chae-Yeun Park*

Main category: quant-ph

TL;DR: HyQuRP是一种混合量子-经典神经网络，具有旋转和置换对称性等变性，在稀疏点云数据上表现出色，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有等变量子机器学习模型通常依赖临时构造，缺乏形式化理论基础。本文旨在构建基于群表示论形式化基础的等变量子-经典混合网络，探索量子机器学习处理3D点云数据的潜力。

Method: 提出HyQuRP（混合量子-经典神经网络），基于群表示论构建具有旋转和置换对称性等变性的模型。在稀疏点云数据场景下，结合量子计算和经典神经网络的优势。

Result: 在稀疏点云场景下，HyQuRP在多个基准测试中持续优于强大的经典和量子基线。例如，使用6个子采样点时，HyQuRP（约1.5K参数）在5类ModelNet基准上达到76.13%准确率，而类似参数量的PointNet、PointMamba和PointTransformer仅约71%。

Conclusion: HyQuRP展示了卓越的数据效率，表明量子机器学习模型在处理3D点云数据方面具有潜力。基于群表示论的形式化方法为等变量子机器学习提供了坚实的理论基础。

Abstract: We introduce HyQuRP, a hybrid quantum-classical neural network equivariant to rotational and permutational symmetries. While existing equivariant quantum machine learning models often rely on ad hoc constructions, HyQuRP is built upon the formal foundations of group representation theory. In the sparse-point regime, HyQuRP consistently outperforms strong classical and quantum baselines across multiple benchmarks. For example, when six subsampled points are used, HyQuRP ($\sim$1.5K parameters) achieves 76.13% accuracy on the 5-class ModelNet benchmark, compared to approximately 71% for PointNet, PointMamba, and PointTransformer with similar parameter counts. These results highlight HyQuRP's exceptional data efficiency and suggest the potential of quantum machine learning models for processing 3D point cloud data.

</details>


### [206] [Quantum Attention by Overlap Interference: Predicting Sequences from Classical and Many-Body Quantum Data](https://arxiv.org/abs/2602.06699)
*Alessio Pecilli,Matteo Rosati*

Main category: quant-ph

TL;DR: 提出一种变分量子自注意力(QSA)实现，通过量子态重叠干涉实现非线性，直接输出Renyi-1/2交叉熵损失，避免经典解码，在序列长度T主导嵌入维度d时具有复杂度优势。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力在经典计算中复杂度为O(T^2 d)，当序列长度T很大时计算成本高。量子计算可能提供复杂度优势，但现有量子注意力方法需要将量子振幅编码的预测解码为经典logits，效率较低。

Method: 提出变分量子自注意力(QSA)：1) 通过量子态重叠干涉实现非线性；2) 直接输出Renyi-1/2交叉熵损失作为可观测量期望值，无需解码；3) 采用可训练的数据嵌入，将量子态重叠与数据级相似性关联；4) 门复杂度主要缩放为O(T d^2)。

Result: 1) 复杂度分析显示QSA在T主导d时具有优势；2) 模拟实验表明QSA量子transformer能学习经典数据和量子多体横向场Ising轨迹的序列预测；3) 建立了可训练注意力作为量子动力学建模的实用原语。

Conclusion: QSA为量子transformer提供了高效实现，通过量子态重叠干涉实现非线性，直接输出损失函数，在序列长度较大时具有复杂度优势，为量子动力学建模开辟了新途径。

Abstract: We propose a variational quantum implementation of self-attention (QSA), the core operation in transformers and large language models, which predicts future elements of a sequence by forming overlap-weighted combinations of past data. At variance with previous approaches, our QSA realizes the required nonlinearity through interference of state overlaps and returns a Renyi-1/2 cross-entropy loss directly as the expectation value of an observable, avoiding the need to decode amplitude-encoded predictions into classical logits. Furthermore, QSA naturally accommodates a constrained, trainable data-embedding that ties quantum state overlaps to data-level similarities. We find a gate complexity dominant scaling O(T d^2) for QSA, versus O(T^2 d) classically, suggesting an advantage in the practical regime where the sequence length T dominates the embedding size d. In simulations, we show that our QSA-based quantum transformer learns sequence prediction on classical data and on many-body transverse-field Ising quantum trajectories, establishing trainable attention as a practical primitive for quantum dynamical modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [207] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 这篇论文首次对LLM推理失败进行了全面调查，提出了新的分类框架，将推理分为具身与非具身类型，并将推理失败分为基础性、应用特定性和鲁棒性三类，为理解LLM系统性弱点提供了结构化视角。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理能力方面取得了显著进展，但在看似简单的场景中仍然存在严重的推理失败。为了系统性地理解和解决这些缺陷，需要对这些推理失败进行全面的调查和分类。

Method: 提出了一个新颖的分类框架：将推理分为具身推理和非具身推理，非具身推理进一步分为非正式（直觉）推理和正式（逻辑）推理。同时，将推理失败分为三类：影响下游任务的基础性失败、特定领域表现的应用特定限制、以及微小变化导致性能不一致的鲁棒性问题。

Result: 为每种推理失败提供了清晰定义，分析了现有研究，探索了根本原因，并提出了缓解策略。通过整合碎片化的研究工作，为LLM推理的系统性弱点提供了结构化视角。

Conclusion: 这项调查为理解LLM推理失败提供了系统框架，为未来研究提供了有价值的见解，指导构建更强、更可靠、更鲁棒的推理能力。同时发布了GitHub资源库，为该领域研究提供了便捷入口。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [208] [Do It for HER: First-Order Temporal Logic Reward Specification in Reinforcement Learning (Extended Version)](https://arxiv.org/abs/2602.06227)
*Pierriccardo Olivieri,Fausto Lasca,Alessandro Gianola,Matteo Papini*

Main category: cs.AI

TL;DR: 提出基于LTLfMT逻辑框架的非马尔可夫奖励规范方法，通过理论片段识别和奖励机器+Hindsight Experience Replay技术解决表达力增强带来的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf在大型状态空间的MDP中表达能力有限，无法处理非结构化异构数据域中的复杂任务，需要手动谓词编码，缺乏统一可重用框架。

Method: 1) 使用LTLfMT（带理论的一阶线性时序逻辑）增强表达能力；2) 识别理论可处理的LTLfMT片段；3) 结合奖励机器和Hindsight Experience Replay（HER）实现一阶逻辑规范转换并解决奖励稀疏问题。

Result: 在连续控制环境中使用非线性算术理论进行评估，实验表明该方法能够自然规范复杂任务，定制化的HER实现对于解决复杂目标任务至关重要。

Conclusion: 提出的LTLfMT框架为大型状态空间MDP中的非马尔可夫奖励规范提供了统一可重用的解决方案，通过理论片段识别和HER技术平衡了表达力与计算可行性。

Abstract: In this work, we propose a novel framework for the logical specification of non-Markovian rewards in Markov Decision Processes (MDPs) with large state spaces. Our approach leverages Linear Temporal Logic Modulo Theories over finite traces (LTLfMT), a more expressive extension of classical temporal logic in which predicates are first-order formulas of arbitrary first-order theories rather than simple Boolean variables. This enhanced expressiveness enables the specification of complex tasks over unstructured and heterogeneous data domains, promoting a unified and reusable framework that eliminates the need for manual predicate encoding. However, the increased expressive power of LTLfMT introduces additional theoretical and computational challenges compared to standard LTLf specifications. We address these challenges from a theoretical standpoint, identifying a fragment of LTLfMT that is tractable but sufficiently expressive for reward specification in an infinite-state-space context. From a practical perspective, we introduce a method based on reward machines and Hindsight Experience Replay (HER) to translate first-order logic specifications and address reward sparsity. We evaluate this approach to a continuous-control setting using Non-Linear Arithmetic Theory, showing that it enables natural specification of complex tasks. Experimental results show how a tailored implementation of HER is fundamental in solving tasks with complex goals.

</details>


### [209] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 提出基于语义LTL到自动机转换的多任务强化学习方法，利用语义标记自动机生成任务嵌入来调节策略，支持完整LTL规范并实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中，需要学习能够泛化到任意可能未见任务的通用策略。现有方法在处理复杂LTL规范时存在局限性，需要更高效的任务表示方法。

Method: 利用新一代语义LTL到自动机转换技术，生成语义标记自动机，从中提取丰富的结构化信息作为任务嵌入，用于条件化策略，支持完整LTL规范。

Result: 在多个领域实验中，该方法实现了最先进的性能，能够扩展到现有方法失败的复杂规范，并能高效地在线计算自动机。

Conclusion: 基于语义LTL到自动机转换的任务嵌入技术为多任务强化学习提供了有效的解决方案，能够处理复杂规范并实现优异的泛化性能。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [210] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: AI代理在任务执行前、中、后预测成功率，结果显示代理存在过度自信问题，即使成功率仅22%时仍预测77%成功。反直觉的是，信息更少的执行前评估比标准执行后评估有更好的区分能力。对抗性提示将评估重构为bug发现可获得最佳校准。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理是否能准确预测自己完成任务的成功率，探索代理在执行任务过程中的不确定性评估能力，特别是代理是否存在过度自信问题。

Method: 在任务执行前、执行期间和执行后三个时间点，要求AI代理提供成功率概率估计。同时采用对抗性提示策略，将评估重构为bug发现过程，以改善校准效果。

Result: 发现明显的代理过度自信现象：某些实际成功率仅22%的代理预测成功率高达77%。反直觉的是，信息更少的执行前评估比标准执行后评估具有更好的区分能力（尽管差异不总是显著）。对抗性提示的bug发现方法实现了最佳校准效果。

Conclusion: AI代理在评估自身任务成功率时存在系统性过度自信，但通过适当的评估框架（如对抗性提示和bug发现视角）可以改善校准效果，这对开发更可靠的AI系统具有重要意义。

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [211] [Advances in Battery Energy Storage Management: Control and Economic Synergies](https://arxiv.org/abs/2602.06365)
*Venkata Rajesh Chundru,Shreshta Rajakumar Deshpande,Stanislav A Gankov*

Main category: eess.SY

TL;DR: 本文综述了电池储能系统（BESS）的研究现状，指出现有文献主要关注电网稳定性控制设计和BESS调度的技术经济分析，但缺乏综合考虑经济与运行维度的全面能源管理系统。研究旨在通过探索文献来弥合这一差距，重点关注电网任务周期的经济方面如何与BESS控制系统协同，以创建增强电网稳定性和收入潜力的数字孪生模型。


<details>
  <summary>Details</summary>
Motivation: 随着辅助服务在电网中的日益普及，需要更全面的能源管理系统。现有研究主要关注BESS的电网稳定性控制和技术经济分析，但缺乏同时优化BESS收入生成并确保锂离子电池安全、高效、可靠运行的综合方法。本研究旨在弥合这一研究空白。

Method: 通过文献综述方法，将相关研究组织为五个关键类别：1）BESS辅助服务；2）实时BESS功率流管理的控制系统；3）BESS调度的优化算法；4）BESS和电池系统的技术经济分析；5）实际BESS部署的数字孪生技术。通过分析这些类别之间的潜在协同作用来识别研究差距。

Result: 文献综述识别了BESS研究中经济与运行维度之间的协同潜力，特别是电网任务周期的经济方面如何与BESS控制系统对齐。这种协同有助于创建强大的数字孪生虚拟表示，增强电网稳定性和收入潜力。

Conclusion: 本研究为未来BESS管理和部署策略的创新铺平了道路。通过整合经济与运行维度，可以开发更全面的能源管理系统，实现BESS在电网中的最佳性能，同时确保电池的安全可靠运行。

Abstract: The existing literature on Battery Energy Storage Systems (BESS) predominantly focuses on two main areas: control system design aimed at achieving grid stability and the techno-economic analysis of BESS dispatch on power grid. However, with the increasing incorporation of ancillary services into power grids, a more comprehensive approach to energy management systems is required. Such an approach should not only optimize revenue generation from BESS but also ensure the safe, efficient, and reliable operation of lithium-ion batteries. This research seeks to bridge this gap by exploring literature that addresses both the economic and operational dimensions of BESS. Specifically, it examines how economic aspects of grid duty cycles can align with control schemes deployed in BESS systems. This alignment, or synergy, could be instrumental in creating robust digital twins virtual representations of BESS systems that enhance both grid stability and revenue potential.
  The literature review is organized into five key categories: (1) ancillary services for BESS, exploring support functions that BESS can provide to power grids; (2) control systems developed for real-time BESS power flow management, ensuring smooth operations under dynamic grid conditions; (3) optimization algorithms for BESS dispatch, focusing on efficient energy allocation strategies; (4) techno-economic analyses of BESS and battery systems to assess their financial viability; and (5) digital twin technologies for real-world BESS deployments, enabling advanced predictive maintenance and performance optimization. This review will identify potential synergies, research gaps, and emerging trends, paving the way for future innovations in BESS management and deployment strategies.

</details>


### [212] [Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches](https://arxiv.org/abs/2602.06944)
*Saber Omidi,Rene Akupan Ebunle,Se Young Yoon*

Main category: eess.SY

TL;DR: 本文比较了磁悬浮系统的两种最优控制方法：基于强化学习的直接无模型控制和基于系统辨识的间接最优控制，发现直接方法在多次迭代后表现更优。


<details>
  <summary>Details</summary>
Motivation: 磁悬浮系统需要高性能控制器，传统基于模型的控制设计受限于模型精度。本文旨在探索直接无模型控制方法是否能超越基于辨识模型的间接控制方法。

Method: 提出两种方法：1) 基于强化学习的直接无模型控制，采用策略迭代和epoch循环收集多组数据；2) 间接最优控制，使用DMDc和PEM系统辨识获得模型后再设计控制器。

Result: 两种控制器都能稳定磁悬浮系统并提升性能，但直接无模型方法在允许多次epoch迭代时始终优于间接方法。直接方法的迭代优化使其比依赖单次数据集的间接方法有明显优势。

Conclusion: 直接无模型控制方法通过多轮数据收集和迭代优化，能够超越基于系统辨识的间接控制方法，为磁悬浮系统控制提供了更优的解决方案。

Abstract: This paper presents the design and implementation of data-driven optimal derivative feedback controllers for an active magnetic levitation system. A direct, model-free control design method based on the reinforcement learning framework is compared with an indirect optimal control design derived from a numerically identified mathematical model of the system. For the direct model-free approach, a policy iteration procedure is proposed, which adds an iteration layer called the epoch loop to gather multiple sets of process data, providing a more diverse dataset and helping reduce learning biases. This direct control design method is evaluated against a comparable optimal control solution designed from a plant model obtained through the combined Dynamic Mode Decomposition with Control (DMDc) and Prediction Error Minimization (PEM) system identification. Results show that while both controllers can stabilize and improve the performance of the magnetic levitation system when compared to controllers designed from a nominal model, the direct model-free approach consistently outperforms the indirect solution when multiple epochs are allowed. The iterative refinement of the optimal control law over the epoch loop provides the direct approach a clear advantage over the indirect method, which relies on a single set of system data to determine the identified model and control.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [213] [UAV-Mounted Aerial Relays in Military Communications: A Comprehensive Survey](https://arxiv.org/abs/2602.06061)
*Faisal Al-Kamali,Francois Chan,Hussein A. Ammar,James H. Bayes,Claude D'Amours*

Main category: cs.IT

TL;DR: 该论文对军事通信中的空中中继系统进行了全面调研，比较了空中中继与传统地面中继，介绍了主动空中中继和空中可重构智能表面两种技术，并提出了基于任务关键中继有效性评分的选择框架。


<details>
  <summary>Details</summary>
Motivation: 传统地面中继在军事通信中存在位置固定、易受物理障碍限制等缺点，而无人机搭载的空中中继系统能够提供动态灵活的解决方案，适应复杂战场环境的需求。

Method: 1. 对空中中继系统进行系统性调研和比较分析；2. 引入任务关键中继有效性评分作为量化评估指标；3. 提出基于MCRES的决策算法，用于根据具体军事场景选择最优中继类型。

Result: 提出了一个全面的空中中继系统分析框架，包括定性的性能比较和定量的MCRES评估方法，以及一个系统化的中继选择决策算法。

Conclusion: 空中中继系统在军事通信中具有重要价值，MCRES框架和决策算法为战场环境下的中继选择提供了实用工具，但仍需解决实施挑战并推进未来研究方向。

Abstract: Relays are pivotal in military communication networks, expanding coverage and ensuring reliable connectivity in challenging operational environments. While traditional terrestrial relays (TR) are constrained by fixed locations and vulnerability to physical obstructions, unmanned aerial vehicle (UAV)-mounted aerial relays (AR) offer a dynamic and flexible alternative by operating above obstacles and adapting to changing battlefield conditions. This paper provides a comprehensive survey of AR systems in military communications, presenting a detailed comparison between AR and TR paradigms and examining two specific AR technologies: active aerial relays (AAR) and aerial reconfigurable intelligent surface (ARIS) relays. The survey delves into their operation, benefits, challenges, and military applications, supported by a qualitative analysis across metrics such as coverage, flexibility, security, and cost. A novel multi-dimensional metric, the mission-critical relay effectiveness score (MCRES), is introduced as a quantitative method for evaluating relay suitability based on mission-specific weights for critical attributes like mobility, jamming resilience, deployment speed, stealth, coverage, and autonomy. Furthermore, we present Algorithm 1, a decision-making framework that leverages the MCRES to guide the systematic selection of the optimal relay type, AR or TR, and subsequently AAR or ARIS, tailored to the unique demands of a given military scenario, such as dynamic battlefield operations, electronic warfare, or covert missions. Finally, the paper addresses current implementation challenges and outlines promising future research directions to advance the deployment of robust and resilient UAV-mounted relay systems in contested military environments.

</details>


### [214] [Deep Unfolded Fractional Optimization for Maximizing Robust Throughput in 6G Networks](https://arxiv.org/abs/2602.06062)
*Anh Thi Bui,Robert-Jeron Reifert,Hayssam Dahrouj,Aydin Sezgin*

Main category: cs.IT

TL;DR: 提出UI-DUFP框架，通过深度展开分式规划结合不确定性注入训练，在6G网络中实现不完美信道条件下的鲁棒波束成形优化。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法计算复杂度高，6G网络需要利用AI工具进行高效鲁棒的优化，特别是在不完美信道条件下实现鲁棒的波束成形。

Method: 将分式规划迭代展开为可训练的神经网络层，结合投影梯度下降步骤进行精炼；在训练中注入采样的信道不确定性，优化基于分位数的目标函数。

Result: UI-DUFP相比经典加权最小均方误差、分式规划和深度学习方法，实现了更高的加权和速率、更好的鲁棒性，同时保持低推理时间和良好的可扩展性。

Conclusion: 深度展开结合不确定性感知训练是6G网络中鲁棒优化的有效方法，展现了在复杂无线通信场景中的潜力。

Abstract: The sixth-generation (6G) of wireless communication networks aims to leverage artificial intelligence tools for efficient and robust network optimization. This is especially the case since traditional optimization methods often face high computational complexity, motivating the use of deep learning (DL)-based optimization frameworks. In this context, this paper considers a multi-antenna base station (BS) serving multiple users simultaneously through transmit beamforming in downlink mode. To account for robustness, this work proposes an uncertainty-injected deep unfolded fractional programming (UI-DUFP) framework for weighted sum rate (WSR) maximization under imperfect channel conditions. The proposed method unfolds fractional programming (FP) iterations into trainable neural network layers refined by projected gradient descent (PGD) steps, while robustness is introduced by injecting sampled channel uncertainties during training and optimizing a quantile-based objective. Simulation results show that the proposed UI-DUFP achieves higher WSR and improved robustness compared to classical weighted minimum mean square error, FP, and DL baselines, while maintaining low inference time and good scalability. These findings highlight the potential of deep unfolding combined with uncertainty-aware training as a powerful approach for robust optimization in 6G networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [215] [Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy](https://arxiv.org/abs/2602.06917)
*Sumit Kumar,Suraj Jaiswal,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TL;DR: 提出基于深度学习的歌唱错误检测框架，包含新数据集和评估方法，优于基于规则的方法


<details>
  <summary>Details</summary>
Motivation: 机器学习在音频分析中的发展为技术增强的音乐教育提供了新可能，需要自动化的歌唱错误检测系统来支持音乐教学

Method: 构建包含师生同步录音标注的新数据集，开发多种深度学习模型进行错误检测，提出新的评估方法，并进行系统误差分析和跨教师研究

Result: 实验表明基于学习的方法优于基于规则的方法，系统误差分析和跨教师研究揭示了音乐教学法的见解

Conclusion: 该工作为音乐教学法研究设定了新方向，代码和数据集已公开

Abstract: The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [216] [A Multiplicative Neural Network Architecture: Locality and Regularity of Appriximation](https://arxiv.org/abs/2602.06374)
*Hee-Sun Choi,Beom-Seok Han*

Main category: math.FA

TL;DR: 本文提出了一种以乘法交互作为基本表示而非加法模型中辅助组件的乘法神经网络架构，证明了其通用逼近定理，并通过数值实验展示了该架构在逼近具有尖锐过渡层或高阶正则性点态损失的目标时，误差结构更紧密地集中在低正则性区域，且在正则性敏感度量中收敛更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络主要基于加法模型，乘法交互通常作为辅助组件。本文旨在探索以乘法交互作为基本表示形式的神经网络架构，研究这种架构设计如何影响逼近函数的局部化和正则性行为，建立架构设计与分析性质之间的直接联系。

Method: 提出了一种新的乘法神经网络架构，其中乘法交互构成基本表示而非辅助组件。首先建立了该架构的通用逼近定理，然后在Bessel势空间中分析了其逼近性质，包括局部性和正则性。通过数值实验验证理论结果，针对具有尖锐过渡层或高阶正则性点态损失的代表性目标，分析近似误差的空间结构和正则性敏感量（特别是Zygmund型半范数的收敛性）。

Result: 理论方面证明了乘法架构的通用逼近定理，并分析了其在Bessel势空间中的逼近性质。实验结果表明，与加法架构相比，乘法架构产生的残差误差结构更紧密地集中在正则性降低的区域，并且在正则性敏感度量（如Zygmund型半范数）中表现出更稳定的收敛行为。

Conclusion: 采用乘法表示格式对神经网络逼近的局部化和正则性行为具有具体影响，提供了架构设计与逼近函数分析性质之间的直接联系。乘法架构在处理具有尖锐特征或正则性变化的目标时具有优势，误差分布更符合目标函数的正则性结构。

Abstract: We introduce a multiplicative neural network architecture in which multiplicative interactions constitute the fundamental representation, rather than appearing as auxiliary components within an additive model. We establish a universal approximation theorem for this architecture and analyze its approximation properties in terms of locality and regularity in Bessel potential spaces.
  To complement the theoretical results, we conduct numerical experiments on representative targets exhibiting sharp transition layers or pointwise loss of higher-order regularity. The experiments focus on the spatial structure of approximation errors and on regularity-sensitive quantities, in particular the convergence of Zygmund-type seminorms. The results show that the proposed multiplicative architecture yields residual error structures that are more tightly aligned with regions of reduced regularity and exhibits more stable convergence in regularity-sensitive metrics.
  These results demonstrate that adopting a multiplicative representation format has concrete implications for the localization and regularity behavior of neural network approximations, providing a direct connection between architectural design and analytical properties of the approximating functions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [217] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: MPIB是一个用于评估临床安全性的医疗提示注入基准，包含9,697个实例，通过临床伤害事件率(CHER)和攻击成功率(ASR)来衡量LLM和RAG系统在临床环境中的安全性风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和RAG系统越来越多地集成到临床工作流程中，提示注入攻击可能导致临床不安全或误导性输出，需要系统评估这些系统在临床环境中的安全性。

Method: 开发了医疗提示注入基准(MPIB)，包含9,697个经过多阶段质量控制和临床安全检查的实例。使用临床伤害事件率(CHER)和攻击成功率(ASR)两个指标，分别评估下游患者风险和指令合规性。支持直接提示注入和间接RAG介导的注入评估。

Result: 评估发现ASR和CHER可能显著偏离，系统的鲁棒性关键取决于对抗性指令出现在用户查询中还是检索上下文中。基准支持可重复的系统研究。

Conclusion: MPIB为临床提示注入研究提供了系统评估框架，强调通过CHER衡量临床伤害风险，有助于理解LLM和RAG系统在临床环境中的安全性漏洞。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [218] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: 通过系统性的后训练方法，较小的语言模型可以在因果推理任务上达到甚至超越更大模型的性能，14B参数的模型在CaLM基准上达到93.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 因果推理对决策至关重要，但非专家难以掌握。虽然大语言模型在此领域有潜力，但其精确因果估计能力有限，且后训练对这些能力的影响研究不足。本文旨在探索后训练能否增强LLMs的因果推理能力。

Method: 引入CauGym数据集，包含7个核心因果训练任务和5个多样化测试集。系统评估5种后训练方法：SFT、DPO、KTO、PPO和GRPO。在5个领域内和4个现有基准上进行实验。

Result: 适当的后训练使较小的LLMs在因果推理上具有竞争力，经常超越更大模型。14B参数模型在CaLM基准上达到93.5%准确率，而OpenAI o3只有55.4%。后训练的LLMs在分布偏移和噪声数据等现实条件下表现出强大的泛化能力和鲁棒性。

Conclusion: 这是首个系统性证据表明有针对性的后训练可以产生可靠且鲁棒的基于LLM的因果推理器。后训练是提升模型因果推理能力的有效途径，即使较小模型也能达到优秀性能。

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [219] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: SureLock是一种针对掩码扩散语言模型的优化方法，通过检测已稳定位置并锁定它们来跳过不必要的计算，减少30-50%的计算量同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型在生成序列时，即使许多未掩码的token位置已经稳定，仍然为每个位置重新计算注意力机制和前馈网络，造成了大量计算浪费。

Method: 提出SureLock方法：当未掩码位置的后验分布在多个步骤中稳定时（sure条件），锁定该位置，跳过其查询投影和前馈子层，同时缓存其注意力键值以供其他位置继续关注。

Result: 在LLaDA-8B模型上，SureLock相比不使用锁定的相同采样器减少了30-50%的算法FLOPs，同时保持了可比的生成质量。

Conclusion: SureLock通过智能锁定已稳定位置显著提高了掩码扩散语言模型的采样效率，理论分析证明仅监控锁定步骤的局部KL散度就足以限制最终token概率的偏差。

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [220] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: 论文提出CORE评估框架，发现LLMs在区分相关与不相关语义关系上存在严重缺陷，尤其在不相关对上表现极差，揭示了LLM语义推理的重要盲点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估很少测试模型区分有意义语义关系与真正不相关性的能力，这可能是LLM推理能力的关键盲点，对模型安全和评估有重要意义。

Method: 构建CORE数据集，包含22.5万多选题覆盖74个学科，以及203个严格验证的基准问题（覆盖24种语义关系类型，不相关对有同等代表性）。评估29个SOTA LLMs，分析其在相关与不相关对上的表现差异。

Result: 人类基线准确率92.6%（不相关对95.1%），而LLMs总体准确率48.25-70.9%，相关对表现优秀（86.5-100%），但不相关对表现极差（0-41.35%）。校准误差在不相关对上增加2-4倍，平均语义崩溃率37.6%。在22.5万数据集上准确率降至约2%。

Conclusion: 不相关推理是LLM评估和安全的关键前沿领域，现有LLMs在区分语义相关与不相关方面存在系统性缺陷，需要新的评估方法和改进方向。

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [221] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: DiSPO是一种用于掩码扩散语言模型的信用分配方法，通过优化中间填充决策来改进最终生成质量，无需额外扩散rollout计算。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型通过多步去噪迭代填充掩码标记生成文本，但仅基于最终完成的终端奖励进行学习会导致对中间决策的信用分配过于粗糙。

Method: DiSPO在选定的中间掩码状态进行分支：从rollout缓存的logits中重新采样当前掩码位置的填充，对生成的完成进行评分，并仅更新新填充的标记，无需额外的多步扩散rollout。

Result: 在LLaDA-8B-Instruct上，DiSPO在匹配的rollout计算和优化器步骤下，在数学和规划基准测试中持续优于终端反馈的diffu-GRPO基线。

Conclusion: DiSPO作为一种插件式信用分配层，能够有效优化掩码扩散语言模型的中间填充决策，提高生成质量，且计算效率高。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [222] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: 本文挑战了Transformer中传统的窄-宽-窄MLP设计，提出使用更深的沙漏形状FFN（宽-窄-宽结构）作为替代，在固定参数预算下实现更好或相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer语言模型一直采用一致的架构：每层包含注意力模块和窄-宽-窄MLP的FFN，大多数参数分配给MLP且扩展比在2-4之间。最近研究表明残差宽-窄-宽（沙漏）MLP具有更优的函数逼近能力，因此需要重新审视Transformer中MLP形状的长期惯例。

Method: 开发了Transformer变体，用更深的沙漏形状FFN替换传统FFN，该FFN由通过残差连接堆叠的沙漏子MLP组成。研究更轻但更深的沙漏FFN是否能作为传统FFN的竞争性替代方案，以及节省的参数是否能更有效地利用（如在固定预算下扩大模型隐藏维度）。

Result: 在400M参数规模下，沙漏FFN优于传统FFN；在1B参数规模下达到相当性能。减少FFN参数并增加注意力参数的沙漏FFN变体在匹配预算下相比传统配置持续改进。

Conclusion: 这些发现为近期工作提供了新视角，促使重新思考窄-宽-窄MLP惯例以及注意力与FFN之间的平衡，以构建更高效和表达性强的现代语言模型。

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [223] [Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning](https://arxiv.org/abs/2602.06584)
*Deqian Kong,Minglu Zhao,Aoyang Qin,Bo Pang,Chenxin Tao,David Hartmann,Edouardo Honig,Dehong Xu,Amit Kumar,Matt Sarte,Chuan Li,Jianwen Xie,Ying Nian Wu*

Main category: cs.CL

TL;DR: 提出Inference-Time Rethinking框架，通过将推理分解为潜在思维向量和语言化解码器，实现迭代式自我修正，在小型模型上超越更大参数量的基线方法。


<details>
  <summary>Details</summary>
Motivation: 标准链式思维推理在单次前向传递中生成解决方案，一旦出现早期错误就无法恢复。需要一种能够迭代自我修正的推理框架。

Method: 将推理分解为连续潜在思维向量（推理内容）和解码器（如何推理）。采用Gibbs式过程，交替生成候选推理轨迹和优化潜在向量，在潜在流形上导航以精炼推理策略。

Result: 在GSM8K数据集上，仅用0.2B参数模型经过30次重新思考迭代，超越了参数量大10-15倍的基线方法，包括3B参数模型。

Conclusion: 有效的数学推理可以通过复杂的推理时计算而不仅仅是大量参数来实现，展示了推理时间计算的重要性。

Abstract: Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.

</details>


### [224] [Uncovering Cross-Objective Interference in Multi-Objective Alignment](https://arxiv.org/abs/2602.06869)
*Yining Lu,Meng Jiang*

Main category: cs.CL

TL;DR: 该论文研究了多目标对齐中LLM的交叉目标干扰问题，提出了基于协方差分析的理论框架和CTWA方法来缓解干扰，并建立了全局收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在多目标对齐训练中出现的持续失败模式：训练只改善部分目标性能，同时导致其他目标性能下降。这种交叉目标干扰现象普遍存在且具有强烈的模型依赖性。

Method: 1. 将交叉目标干扰现象形式化；2. 推导局部协方差定律，显示目标在一阶改进时其奖励与标量化分数呈正协方差；3. 将分析扩展到现代对齐中使用的裁剪代理目标；4. 提出协方差目标权重适应（CTWA）方法，通过保持目标奖励与训练信号之间的正协方差来缓解干扰；5. 在Polyak-Łojasiewicz条件下进行全局收敛分析。

Result: 1. 交叉目标干扰在经典标量化算法中普遍存在且具有模型依赖性；2. 协方差定律在裁剪代理目标下仍然有效；3. CTWA方法能有效缓解交叉目标干扰；4. 建立了非凸标量化优化在特定条件下实现全局收敛的理论框架。

Conclusion: 该研究为多目标对齐中的交叉目标干扰问题提供了系统的理论分析和实用解决方案，揭示了干扰现象的根本原因，并提出了有效的缓解方法，为LLM对齐训练提供了理论指导。

Abstract: We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.

</details>
