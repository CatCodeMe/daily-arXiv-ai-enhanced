<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 68]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 103]
- [nlin.AO](#nlin.AO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.SD](#cs.SD) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 3]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CL](#cs.CL) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [cs.CE](#cs.CE) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [stat.ML](#stat.ML) [Total: 9]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Jelly: a fast and convenient RDF serialization format](https://arxiv.org/abs/2506.11298)
*Piotr Sowinski,Karolina Bogacka,Anastasiya Danilenka,Nikita Kozlov*

Main category: cs.DB

TL;DR: Jelly是一种高效的二进制RDF序列化格式，解决了现有格式在性能、压缩比和流支持上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有RDF序列化格式（如Turtle、N-Triples和JSON-LD）在性能、压缩比和流支持方面存在局限性。

Method: 基于Protocol Buffers设计Jelly，支持批处理和流式用例，优化序列化吞吐量、文件大小和计算资源使用。

Result: Jelly提供开源实现（Java和Python）、开放协议规范和命令行工具，具有高效性和易用性。

Conclusion: Jelly结合实用性和高效性，是语义网工具栈的重要贡献。

Abstract: Existing RDF serialization formats such as Turtle, N-Triples, and JSON-LD are
widely used for communication and storage in knowledge graph and Semantic Web
applications. However, they suffer from limitations in performance, compression
ratio, and lack of native support for RDF streams. To address these
shortcomings, we introduce Jelly, a fast and convenient binary serialization
format for RDF data that supports both batch and streaming use cases. Jelly is
designed to maximize serialization throughput, reduce file size with
lightweight streaming compression, and minimize compute resource usage. Built
on Protocol Buffers, Jelly is easy to integrate with modern programming
languages and RDF libraries. To maximize reusability, Jelly has an open
protocol specification, open-source implementations in Java and Python
integrated with popular RDF libraries, and a versatile command-line tool. To
illustrate its usefulness, we outline concrete use cases where Jelly can
provide tangible benefits. By combining practical usability with
state-of-the-art efficiency, Jelly is an important contribution to the Semantic
Web tool stack.

</details>


### [2] [OCPQ: Object-Centric Process Querying & Constraints](https://arxiv.org/abs/2506.11541)
*Aaron Küsters,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 本文提出了一种新的面向对象中心的过程查询方法OCPQ，解决了传统过程挖掘技术无法处理对象中心事件数据的问题，并在性能和易用性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的过程挖掘技术基于案例中心事件数据，无法有效处理对象中心事件数据（OCED），限制了其在现实场景中的应用。

Method: 提出OCPQ方法，支持对象中心过程查询和约束检查，包括高性能执行引擎和易用编辑器。

Result: 在真实数据集上验证，OCPQ在表达能力和运行时性能上优于SQLite、Neo4j，并与DuckDB相当。

Conclusion: OCPQ为对象中心过程查询提供了高效且易用的解决方案，填补了现有技术的不足。

Abstract: Process querying is used to extract information and insights from process
execution data. Similarly, process constraints can be checked against input
data, yielding information on which process instances violate them.
Traditionally, such process mining techniques use case-centric event data as
input. However, with the uptake of Object-Centric Process Mining (OCPM),
existing querying and constraint checking techniques are no longer applicable.
Object-Centric Event Data (OCED) removes the requirement to pick a single case
notion (i.e., requiring that events belong to exactly one case) and can thus
represent many real-life processes much more accurately. In this paper, we
present a novel highly-expressive approach for object-centric process querying,
called OCPQ. It supports a wide variety of applications, including OCED-based
constraint checking and filtering. The visual representation of nested queries
in OCPQ allows users to intuitively read and create queries and constraints. We
implemented our approach using (1) a high-performance execution engine backend
and (2) an easy-to-use editor frontend. Additionally, we evaluated our approach
on a real-life dataset, showing the lack in expressiveness of prior work and
runtime performance significantly better than the general querying solutions
SQLite and Neo4j, as well as comparable to the performance-focused DuckDB.

</details>


### [3] [LLM-based Dynamic Differential Testing for Database Connectors with Reinforcement Learning-Guided Prompt Selection](https://arxiv.org/abs/2506.11870)
*Ce Lyu,Minghao Zhao,Yanhao Wang,Liang Jie*

Main category: cs.DB

TL;DR: 提出了一种基于强化学习引导的LLM测试用例生成方法，用于检测数据库连接器的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 数据库连接器的安全漏洞常被忽视，传统模糊测试和LLM生成测试用例方法因缺乏领域知识而效果有限。

Method: 结合强化学习和LLM，通过参数化提示模板生成测试用例，并利用差分测试动态评估，迭代优化提示以最大化控制流覆盖率。

Result: 在两种JDBC连接器上测试，报告了16个漏洞，其中10个被官方确认，其余被认定为不安全实现。

Conclusion: 该方法有效提升了数据库连接器漏洞的检测能力，解决了传统方法的局限性。

Abstract: Database connectors are critical components enabling applications to interact
with underlying database management systems (DBMS), yet their security
vulnerabilities often remain overlooked. Unlike traditional software defects,
connector vulnerabilities exhibit subtle behavioral patterns and are inherently
challenging to detect. Besides, nonstandardized implementation of connectors
leaves potential risks (a.k.a. unsafe implementations) but is more elusive. As
a result, traditional fuzzing methods are incapable of finding such
vulnerabilities. Even for LLM-enable test case generation, due to a lack of
domain knowledge, they are also incapable of generating test cases that invoke
all interface and internal logic of connectors. In this paper, we propose
reinforcement learning (RL)-guided LLM test-case generation for database
connector testing. Specifically, to equip the LLM with sufficient and
appropriate domain knowledge, a parameterized prompt template is composed which
can be utilized to generate numerous prompts. Test cases are generated via LLM
with a prompt, and are dynamically evaluated through differential testing
across multiple connectors. The testing is iteratively conducted, with each
round RL is adopted to select optimal prompt based on prior-round behavioral
feedback, so as to maximize control flow coverage. We implement aforementioned
methodology in a practical tool and evaluate it on two widely used JDBC
connectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported
16 bugs, among them 10 are officially confirmed and the rest are acknowledged
as unsafe implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous Speculative Decoding](https://arxiv.org/abs/2506.11309)
*Ziyi Zhang,Ziheng Jiang,Chengquan Jiang,Menghan Yu,Size Zheng,Haibin Lin,Henry Hoffmann,Xin Liu*

Main category: cs.DC

TL;DR: SwiftSpec是一种针对大型语言模型（LLM）解码的超低延迟系统，通过异步和分散式设计，结合并行树生成和优化的KV缓存管理，显著提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 低延迟解码对于LLM应用（如聊天机器人和代码助手）至关重要，但传统方法在单查询设置下生成长输出时效率低下。

Method: SwiftSpec重新设计了推测解码流程，采用异步和分散式架构，引入并行树生成、树感知KV缓存管理和优化的内核。

Result: 在5个模型家族和6个数据集上，SwiftSpec平均提速1.75倍，Llama3-70B在8个Nvidia Hopper GPU上达到348 tokens/s。

Conclusion: SwiftSpec是目前已知最快的低延迟LLM服务系统，解决了传统方法在计算不平衡和通信开销上的问题。

Abstract: Low-latency decoding for large language models (LLMs) is crucial for
applications like chatbots and code assistants, yet generating long outputs
remains slow in single-query settings. Prior work on speculative decoding
(which combines a small draft model with a larger target model) and tensor
parallelism has each accelerated decoding. However, conventional approaches
fail to apply both simultaneously due to imbalanced compute requirements
(between draft and target models), KV-cache inconsistencies, and communication
overheads under small-batch tensor-parallelism. This paper introduces
SwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec
redesigns the speculative decoding pipeline in an asynchronous and
disaggregated manner, so that each component can be scaled flexibly and remove
draft overhead from the critical path. To realize this design, SwiftSpec
proposes parallel tree generation, tree-aware KV cache management, and fused,
latency-optimized kernels to overcome the challenges listed above. Across 5
model families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup
over state-of-the-art speculative decoding systems and, as a highlight, serves
Llama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known
system for low-latency LLM serving at this scale.

</details>


### [5] [Capsule: Efficient Player Isolation for Datacenters](https://arxiv.org/abs/2506.11483)
*Zhouheng Du,Nima Davari,Li Li,Nodir Kodirov*

Main category: cs.DC

TL;DR: Capsule是一种机制，允许多个玩家共享一个GPU，提高数据中心资源利用率，同时不降低游戏体验。


<details>
  <summary>Details</summary>
Motivation: 云游戏日益流行，但数据中心资源利用率低，尤其是GPU因游戏引擎设计单一玩家运行而未能充分利用。

Method: 在开源游戏引擎O3DE中实现Capsule，支持多玩家共享GPU。

Result: Capsule可容纳2.25倍玩家，提升资源利用率，且无需修改应用程序。

Conclusion: Capsule设计可推广至其他游戏引擎，提高云服务商的数据中心利用率。

Abstract: Cloud gaming is increasingly popular. A challenge for cloud provider is to
keep datacenter utilization high: a non-trivial task due to application
variety. These applications come in different shapes and sizes. So do cloud
datacenter resources, e.g., CPUs, GPUs, NPUs.
  Part of the challenge stems from game engines being predominantly designed to
run only one player. One player in a lightweight game might utilize only a
fraction of the cloud server GPU. The remaining GPU capacity will be left
underutilized, an undesired outcome for the cloud provider. We introduce
Capsule, a mechanism that allows multiple players to seamlessly share one GPU.
  We implemented Capsule in O3DE, a popular open source game engine. Our
evaluations show that Capsule can increase datacenter resource utilization by
accommodating up to 2.25x more players, without degrading player gaming
experience. Capsule is also application agnostic. We ran four applications on
Capsule-based O3DE with no application changes. Our experiences show that
Capsule design can be adopted by other game engines to increase datacenter
utilization across cloud providers.

</details>


### [6] [Bounded Memory in Distributed Networks](https://arxiv.org/abs/2506.11644)
*Ran Ben Basat,Keren Censor-Hillel,Yi-Jun Chang,Wenchen Han,Dean Leitersdorf,Gregory Schwartzman*

Main category: cs.DC

TL;DR: 论文提出了μ-CONGEST模型，解决了分布式算法在数据中心网络中的内存限制问题，并提供了两种快速算法：一种是针对内存密集型算法的优化，另一种是流算法的模拟。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据中心网络中，分布式算法的内存限制导致CONGEST算法无法直接应用，需要新的模型和方法来解决这一问题。

Method: 引入μ-CONGEST模型，限制节点内存为μ字，并提出两种算法：优化内存密集型算法和模拟流算法。

Result: 展示了在μ-CONGEST模型下，如何高效生成网络组合结构的统计信息，例如识别频繁单色三角形。

Conclusion: 通过μ-CONGEST模型和算法优化，实现了分布式算法在内存受限环境中的高效运行。

Abstract: The recent advent of programmable switches makes distributed algorithms
readily deployable in real-world datacenter networks. However, there are still
gaps between theory and practice that prevent the smooth adaptation of CONGEST
algorithms to these environments. In this paper, we focus on the memory
restrictions that arise in real-world deployments. We introduce the
$\mu$-CONGEST model where on top of the bandwidth restriction, the memory of
nodes is also limited to $\mu$ words, in line with real-world systems. We
provide fast algorithms of two main flavors.
  First, we observe that many algorithms in the CONGEST model are
memory-intensive and do not work in $\mu$-CONGEST. A prime example of a family
of algorithms that use large memory is clique-listing algorithms. We show that
the memory issue that arises here cannot be resolved without incurring a cost
in the round complexity, by establishing a lower bound on the round complexity
of listing cliques in $\mu$-CONGEST. We introduce novel techniques to overcome
these issues and generalize the algorithms to work within a given memory bound.
Combined with our lower bound, these provide tight tradeoffs between the
running time and memory of nodes.
  Second, we show that it is possible to efficiently simulate various families
of streaming algorithms in $\mu$-CONGEST. These include fast simulations of
$p$-pass algorithms, random order streams, and various types of mergeable
streaming algorithms.
  Combining our contributions, we show that we can use streaming algorithms to
efficiently generate statistics regarding combinatorial structures in the
network. An example of an end result of this type is that we can efficiently
identify and provide the per-color frequencies of the frequent monochromatic
triangles in $\mu$-CONGEST.

</details>


### [7] [A retrospective on DISPEED -- Leveraging heterogeneity in a drone swarm for IDS execution](https://arxiv.org/abs/2506.11800)
*Vincent Lannurien,Camélia Slimani,Louis Morge-Rollet,Laurent Lemarchand,David Espes,Frédéric Le Roy,Jalil Boukhobza*

Main category: cs.DC

TL;DR: DISPEED项目旨在利用无人机群的异构性部署网络入侵检测系统（NIDS），通过两个阶段实现：特征化阶段和NIDS实现映射阶段。


<details>
  <summary>Details</summary>
Motivation: 无人机群在任务中面临安全威胁，传统NIDS依赖资源密集型机器学习技术，难以在无人机群中部署。

Method: 项目分为两个阶段：1) 特征化阶段，在不同嵌入式平台上评估IDS实现；2) 映射阶段，设计策略选择最适合的NIDS。

Result: 在三种嵌入式平台上识别了36种相关IDS实现，并设计了独立和分布式策略选择NIDS。

Conclusion: 项目成果包括三篇国际会议论文和一篇期刊论文，验证了方法的有效性。

Abstract: Swarms of drones are gaining more and more autonomy and efficiency during
their missions. However, security threats can disrupt their missions'
progression. To overcome this problem, Network Intrusion Detection Systems
((N)IDS) are promising solutions to detect malicious behavior on network
traffic. However, modern NIDS rely on resource-hungry machine learning
techniques, that can be difficult to deploy on a swarm of drones. The goal of
the DISPEED project is to leverage the heterogeneity (execution platforms,
memory) of the drones composing a swarm to deploy NIDS. It is decomposed in two
phases: (1) a characterization phase that consists in characterizing various
IDS implementations on diverse embedded platforms, and (2) an IDS
implementation mapping phase that seeks to develop selection strategies to
choose the most relevant NIDS depending on the context. On the one hand, the
characterization phase allowed us to identify 36 relevant IDS implementations
on three different embedded platforms: a Raspberry Pi 4B, a Jetson Xavier, and
a Pynq-Z2. On the other hand, the IDS implementation mapping phase allowed us
to design both standalone and distributed strategies to choose the best NIDSs
to deploy depending on the context. The results of the project have led to
three publications in international conferences, and one publication in a
journal.

</details>


### [8] [Secure API-Driven Research Automation to Accelerate Scientific Discovery](https://arxiv.org/abs/2506.11950)
*Tyler J. Skluzacek,Paul Bryant,A. J. Ruckman,Daniel Rosendo,Suzanne Prentice,Michael J. Brim,Ryan Adamson,Sarp Oral,Mallikarjun Shankar,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: S3M通过服务网格架构整合实时流、智能工作流编排和细粒度授权，加速科学发现并确保安全性。


<details>
  <summary>Details</summary>
Motivation: 消除研究人员、计算资源和实验设施之间的传统障碍，加速AI辅助自主科学的潜力。

Method: 集成实时流、智能工作流编排和细粒度授权于服务网格架构中。

Result: 动态配置资源并执行复杂工作流，加速实验生命周期。

Conclusion: S3M标志着科学计算基础设施的新时代，实现了无缝连接与高效协作。

Abstract: The Secure Scientific Service Mesh (S3M) provides API-driven infrastructure
to accelerate scientific discovery through automated research workflows. By
integrating near real-time streaming capabilities, intelligent workflow
orchestration, and fine-grained authorization within a service mesh
architecture, S3M revolutionizes programmatic access to high performance
computing (HPC) while maintaining uncompromising security. This framework
allows intelligent agents and experimental facilities to dynamically provision
resources and execute complex workflows, accelerating experimental lifecycles,
and unlocking the full potential of AI-augmented autonomous science. S3M
signals a new era in scientific computing infrastructure that eliminates
traditional barriers between researchers, computational resources, and
experimental facilities.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [String Matching with a Dynamic Pattern](https://arxiv.org/abs/2506.11318)
*Bruno Monteiro,Vinicius dos Santos*

Main category: cs.DS

TL;DR: 提出了一种处理动态模式字符串匹配问题的算法，支持模式字符的增删、子串操作，并保持高效的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决动态模式字符串匹配问题，即在模式动态变化时快速计算其在静态文本中的出现次数。

Method: 使用后缀数组，支持模式字符的增删、子串删除、转置和复制操作，时间复杂度为O(log|T|)。

Result: 算法在预处理时间O(|T|)后，每次操作更新时间为O(log|T|)，并可扩展到在线文本。

Conclusion: 该算法简单实用，适用于动态模式字符串匹配问题，并支持多种操作扩展。

Abstract: In this work, we tackle a natural variation of the String Matching Problem on
the case of a dynamic pattern, that is, given a static text $T$ and a pattern
$P$, we want to support character additions and deletions to the pattern, and
after each operation compute how many times it occurs in the text. We show a
simple and practical algorithm using Suffix Arrays that achieves $\mathcal
O(\log |T|)$ update time, after $\mathcal O(|T|)$ preprocess time. We show how
to extend our solution to support substring deletion, transposition (moving a
substring to another position of the pattern), and copy (copying a substring
and pasting it in a specific position), in the same time complexities. Our
solution can also be extended to support an online text (adding characters to
one end of the text), maintaining the same amortized bounds.

</details>


### [10] [Isometric-Universal Graphs for Trees](https://arxiv.org/abs/2506.11704)
*Edgar Baucher,François Dross,Cyril Gavoille*

Main category: cs.DS

TL;DR: 论文研究了如何找到包含两棵树的等距通用图的最小顶点数，并扩展到森林，证明了其时间复杂度和不可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决两棵树的最小等距通用图问题，并探索其扩展到森林和更多树的限制。

Method: 提出算法，证明最小等距通用图本质上是树，并分析其时间复杂度和不可扩展性。

Result: 算法时间复杂度为O(n^{5/2}log{n})（树）和O(n^{7/2}log{n})（森林），并证明对三棵树的问题为NP完全。

Conclusion: 最小等距通用图问题对两棵树可高效解决，但对更多树则不可行，且贪婪策略受限。

Abstract: We consider the problem of finding the smallest graph that contains two input
trees each with at most $n$ vertices preserving their distances. In other
words, we look for an isometric-universal graph with the minimum number of
vertices for two given trees. We prove that this problem can be solved in time
$O(n^{5/2}\log{n})$. We extend this result to forests instead of trees, and
propose an algorithm with running time $O(n^{7/2}\log{n})$. As a key
ingredient, we show that a smallest isometric-universal graph of two trees
essentially is a tree. Furthermore, we prove that these results cannot be
extended. Firstly, we show that deciding whether there exists an
isometric-universal graph with $t$ vertices for three forests is NP-complete.
Secondly, we show that any smallest isometric-universal graph cannot be a tree
for some families of three trees. This latter result has implications for
greedy strategies solving the smallest isometric-universal graph problem.

</details>


### [11] [Practical colinear chaining on sequences revisited](https://arxiv.org/abs/2506.11750)
*Nicola Rizzo,Manuel Cáceres,Veli Mäkinen*

Main category: cs.DS

TL;DR: 论文研究了共线性链式算法在序列对齐中的应用，改进了Jain等人的算法，提出了一种最优算法，并在实际数据中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 共线性链式是序列对齐中的经典启发式方法，但现有算法（如ChainX）在某些情况下可能不是最优的。本文旨在解决这一问题。

Method: 引入锚点对角线距离，提出并实现了一种最优算法，其平均时间复杂度为O(n·OPT + n log n)。

Result: 验证了Jain等人的结果，发现ChainX在长读数据集上可能不是最优的，而新算法仅带来轻微的计算延迟。

Conclusion: 新算法在保持高效的同时实现了最优性，适用于实际应用。

Abstract: Colinear chaining is a classical heuristic for sequence alignment and is
widely used in modern practical aligners. Jain et al. (J. Comput. Biol. 2022)
proposed an $O(n \log^3 n)$ time algorithm to chain a set of $n$ anchors so
that the chaining cost matches the edit distance of the input sequences, when
anchors are maximal exact matches. Moreover, assuming a uniform and sparse
distribution of anchors, they provided a practical solution ($\mathtt{ChainX}$)
working in $O(n \cdot \mathsf{SOL} + n \log n)$ average-case time, where
$\mathsf{SOL}$ is the cost of the output chain and $n$ is the number of anchors
in the input. This practical solution is not guaranteed to be optimal: we study
the failing cases, introduce the anchor diagonal distance, and find and
implement an optimal algorithm working in the same $O(n \cdot \mathsf{OPT} + n
\log n)$ average-case time, where $\mathsf{OPT}$ is the optimal chaining cost;
then, we validate the results by Jain et al., show that $\mathtt{ChainX}$ can
be suboptimal with a realistic long read dataset, and show minimal
computational slowdown for our solution.

</details>


### [12] [Breaking the O(mn)-Time Barrier for Vertex-Weighted Global Minimum Cut](https://arxiv.org/abs/2506.11926)
*Julia Chuzhoy,Ohad Trabelsi*

Main category: cs.DS

TL;DR: 本文提出了一种随机算法，解决了全局最小顶点割问题，打破了28年的旧记录，运行时间为O(min{mn^{0.99+o(1)},m^{1.5+o(1)}})。


<details>
  <summary>Details</summary>
Motivation: 全局最小顶点割问题是组合优化和图论中的基本问题，但之前的算法运行时间为O~(mn)，28年来未被改进。本文旨在突破这一限制。

Method: 提出了一种随机算法，针对顶点加权的全局最小顶点割问题，优化了运行时间。

Result: 算法运行时间为O(min{mn^{0.99+o(1)},m^{1.5+o(1)}})，显著优于之前的O~(mn)。

Conclusion: 本文成功打破了28年的记录，为全局最小顶点割问题提供了更高效的解决方案。

Abstract: We consider the Global Minimum Vertex-Cut problem: given an undirected
vertex-weighted graph $G$, compute a minimum-weight subset of its vertices
whose removal disconnects $G$. The problem is closely related to Global Minimum
Edge-Cut, where the weights are on the graph edges instead of vertices, and the
goal is to compute a minimum-weight subset of edges whose removal disconnects
the graph. Global Minimum Cut is one of the most basic and extensively studied
problems in combinatorial optimization and graph theory. While an almost-linear
time algorithm was known for the edge version of the problem for awhile
(Karger, STOC 1996 and J. ACM 2000), the fastest previous algorithm for the
vertex version (Henzinger, Rao and Gabow, FOCS 1996 and J. Algorithms 2000)
achieves a running time of $\tilde{O}(mn)$, where $m$ and $n$ denote the number
of edges and vertices in the input graph, respectively. For the special case of
unit vertex weights, this bound was broken only recently (Li {et al.}, STOC
2021); their result, combined with the recent breakthrough almost-linear time
algorithm for Maximum $s$-$t$ Flow (Chen {et al.}, FOCS 2022, van den Brand {et
al.}, FOCS 2023), yields an almost-linear time algorithm for Global Minimum
Vertex-Cut with unit vertex weights.
  In this paper we break the $28$ years old bound of Henzinger {et al.} for the
general weighted Global Minimum Vertex-Cut, by providing a randomized algorithm
for the problem with running time $O(\min\{mn^{0.99+o(1)},m^{1.5+o(1)}\})$.

</details>


### [13] [Engineering Fast and Space-Efficient Recompression from SLP-Compressed Text](https://arxiv.org/abs/2506.12011)
*Ankith Reddy Adudodla,Dominik Kempa*

Main category: cs.DS

TL;DR: 论文提出了一种压缩时间运行的recompression RLSLP构建方法，显著提升了大规模重复文本数据集上的索引构建效率。


<details>
  <summary>Details</summary>
Motivation: 现有理论高效的索引结构在实际构建中仍存在瓶颈，尤其是复杂组件如recompression RLSLP的构建问题。

Method: 采用LZ77类近似输入的压缩时间运行方法，实现了recompression RLSLP的首个实际实现。

Result: 相比现有非压缩时间方法，速度提升达46倍，内存使用降低17倍，适用于大规模重复数据集。

Conclusion: 压缩计算为快速索引构建提供了实用路径，提升了大规模数据集的扩展性。

Abstract: Compressed indexing enables powerful queries over massive and repetitive
textual datasets using space proportional to the compressed input. While
theoretical advances have led to highly efficient index structures, their
practical construction remains a bottleneck (especially for complex components
like recompression RLSLP), a grammar-based representation crucial for building
powerful text indexes that support widely used suffix array queries.
  In this work, we present the first implementation of recompression RLSLP
construction that runs in compressed time, operating on an LZ77-like
approximation of the input. Compared to state-of-the-art uncompressed-time
methods, our approach achieves up to 46$\times$ speedup and 17$\times$ lower
RAM usage on large, repetitive inputs. These gains unlock scalability to larger
datasets and affirm compressed computation as a practical path forward for fast
index construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [14] [Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality](https://arxiv.org/abs/2506.10984)
*Ahilan Ayyachamy Nadar Ponnusamy*

Main category: cs.SE

TL;DR: 本文探讨了AI辅助代码生成工具的挑战，并提出了一种结合LLM代码推理与生成能力及人类专业知识的框架，通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助代码生成工具提高了效率，但仍存在安全性、可靠性和代码质量等问题，需结合人类专业知识解决。

Method: 提出了一种基于LLM的框架，强调代码推理与生成能力，并结合人类指导，通过案例研究验证。

Result: 框架在应用现代化中表现出有效性，展示了人类参与的重要性。

Conclusion: 本文为AI驱动的应用现代化提供了实用见解和研究基础，强调人类与AI协作的关键作用。

Abstract: AI-assisted code generation tools have revolutionized software development,
offering unprecedented efficiency and scalability. However, multiple studies
have consistently highlighted challenges such as security vulnerabilities,
reliability issues, and inconsistencies in the generated code. Addressing these
concerns is crucial to unlocking the full potential of this transformative
technology. While advancements in foundational and code-specialized language
models have made notable progress in mitigating some of these issues,
significant gaps remain, particularly in ensuring high-quality, trustworthy
outputs.
  This paper builds upon existing research on leveraging large language models
(LLMs) for application modernization. It explores an opinionated approach that
emphasizes two core capabilities of LLMs: code reasoning and code generation.
The proposed framework integrates these capabilities with human expertise to
tackle application modernization challenges effectively. It highlights the
indispensable role of human involvement and guidance in ensuring the success of
AI-assisted processes.
  To demonstrate the framework's utility, this paper presents a detailed case
study, walking through its application in a real-world scenario. The analysis
includes a step-by-step breakdown, assessing alternative approaches where
applicable. This work aims to provide actionable insights and a robust
foundation for future research in AI-driven application modernization. The
reference implementation created for this paper is available on GitHub.

</details>


### [15] [On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances](https://arxiv.org/abs/2506.10990)
*Roberto Vergallo,Luís Cruz,Alessio Errico,Luca Mainetti*

Main category: cs.SE

TL;DR: 论文研究了'Follow-the-Sun'（FtS）策略在减少AI工作负载碳足迹方面的效果，通过实验验证其优于其他策略。


<details>
  <summary>Details</summary>
Motivation: AI的高能耗引发了对碳足迹的关注，但缺乏科学证据支持FtS策略的有效性。

Method: 在部分合成场景中，对四种异常检测算法进行实验，比较无策略、FtS及两种现有策略的碳排放差异。

Result: FtS策略平均减少14.6%碳排放（峰值16.3%），同时保持训练时间。

Conclusion: FtS是一种有效的减少AI工作负载碳足迹的策略。

Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at
minimizing the carbon footprint of computer workloads. It involves dynamically
moving workloads to regions with cleaner energy sources as demand increases and
energy production relies more on fossil fuels. With the significant power
consumption of Artificial Intelligence (AI) being a subject of extensive
debate, FtS is proposed as a strategy to mitigate the carbon footprint of
training AI models. However, the literature lacks scientific evidence on the
advantages of FtS to mitigate the carbon footprint of AI workloads. In this
paper, we present the results of an experiment conducted in a partial synthetic
scenario to address this research gap. We benchmarked four AI algorithms in the
anomaly detection domain and measured the differences in carbon emissions in
four cases: no strategy, FtS, and two strategies previously introduced in the
state of the art, namely Flexible Start and Pause and Resume. To conduct our
experiment, we utilized historical carbon intensity data from the year 2021 for
seven European cities. Our results demonstrate that the FtS strategy not only
achieves average reductions of up to 14.6% in carbon emissions (with peaks of
16.3%) but also helps in preserving the time needed for training.

</details>


### [16] [Collaboration Tools and their Role in Agile Software Projects](https://arxiv.org/abs/2506.10985)
*Raman Mohammed Hussein,Bryar A. Hassan*

Main category: cs.SE

TL;DR: 本文探讨了协作工具（Slack、Microsoft Teams、Confluence）在敏捷和软件项目中的重要性，分析其如何支持敏捷原则、促进迭代开发并提升任务管理效率。


<details>
  <summary>Details</summary>
Motivation: 敏捷方法依赖灵活性和团队协作，但远程工作环境下的沟通问题仍然存在。协作工具能提升组织效率、透明度和互动性，从而增强项目生产力。

Method: 通过分析Slack、Microsoft Teams和Confluence的功能，探讨其如何适应敏捷原则，支持任务跟踪和知识共享。

Result: 这些工具在任务协调、知识共享和跨职能协作中发挥了关键作用，显著提升了敏捷项目的效率。

Conclusion: 协作工具是敏捷项目中不可或缺的组成部分，能够有效解决远程协作的挑战并推动项目成功。

Abstract: The purpose of this review is to understand the importance of collaboration
tools which are Slack, Microsoft Teams, Confluence in Agile and software
projects. Agile methodologies rely on flexibility, using cycles and integration
throughout various levels of developing cycles. However, it is still a great
problem for many teams to collaborate and communicate even if staff members and
teams are working remotely. In terms of collaboration, the applications and
technologies mean better organization of work, increased mutually
understandable openness and fast and efficient inter team and interpersonal
interactions to enhance results of projects into productivity. This paper
examines how these tools fit the Agile principles, how they facilitate
iterative development, and encouraging effective initiation and tracking of
tasks in small and large projects. The insights focus on how Slack, Microsoft
Teams, and Confluence are essential for gaining better task coordination,
supporting knowledge sharing, and adopting agile values across cross-functional
contexts.

</details>


### [17] [CoMRAT: Commit Message Rationale Analysis Tool](https://arxiv.org/abs/2506.10986)
*Mouna Dhaouadi,Bentley James Oakes,Michalis Famelis*

Main category: cs.SE

TL;DR: CoMRAT是一个分析提交消息中决策和理由句的工具，支持研究和开发场景。


<details>
  <summary>Details</summary>
Motivation: 提交消息中的代码变更理由信息丰富但研究有限。

Method: 开发CoMRAT工具，分析GitHub模块中的提交消息理由。

Result: 初步评估显示工具在研究与开发中实用且易用。

Conclusion: CoMRAT为提交消息中的理由分析提供了有效工具。

Abstract: In collaborative open-source development, the rationale for code changes is
often captured in commit messages, making them a rich source of valuable
information. However, research on rationale in commit messages remains limited.
In this paper, we present CoMRAT, a tool for analyzing decision and rationale
sentences rationale in commit messages. CoMRAT enables a) researchers to
produce metrics and analyses on rationale information in any Github module, and
b) developers to check the amount of rationale in their commit messages. A
preliminary evaluation suggests the tool's usefulness and usability in both
these research and development contexts.

</details>


### [18] [Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks](https://arxiv.org/abs/2506.10987)
*Shaoyi Yang*

Main category: cs.SE

TL;DR: 该研究将Chain of Draft (CoD)方法扩展到软件工程领域，通过实验证明CoD变体在代码任务中显著减少了令牌使用量，同时保持了高质量的代码输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在软件开发中需要冗长的中间推理，导致高延迟和成本。研究旨在通过CoD方法提高效率。

Method: 设计并评估了多种针对代码任务的CoD变体，使用SWE-bench基准的300个样本进行实验。

Result: CoD变体比Chain of Thought (CoT)使用的令牌少，Baseline CoD效率最高，仅需CoT的55.4%令牌。代码质量保持在CoT的90%以上。

Conclusion: 研究表明CoD在软件工程中是高效的替代方案，同时提供了平衡效率与质量的框架。

Abstract: Large language models (LLMs) have become vital tools for software
development, but they often require verbose intermediate reasoning for complex
code tasks, leading to high latency and costs. This research extends the Chain
of Draft (CoD) method to software engineering, designing and evaluating
multiple CoD variants tailored for code tasks. Through comprehensive
experiments on all 300 samples from the SWE-bench benchmark, we found that all
CoD variants used significantly fewer tokens than Chain of Thought (CoT), with
Baseline CoD being most efficient at 55.4% of CoT's tokens. While this
represents substantial efficiency gains - translating to approximately 45%
reduction in processing time and API costs - it differs from the extreme 7.6%
reported in the original CoD paper for mathematical reasoning. This difference
stems from the inherent complexity and context-dependency of software tasks,
which require more detailed reasoning to maintain solution quality. Our
multi-dimensional quality assessment revealed that CoD variants maintain over
90% of CoT's code quality across key metrics including correctness,
compatibility, and maintainability, making them practical alternatives for
real-world development scenarios where efficiency matters. This research
demonstrates how domain-specific characteristics influence prompting strategy
effectiveness and provides a framework for balancing efficiency with solution
quality in software engineering applications. Our findings offer practical
guidance for optimizing LLM-based development workflows through appropriate
prompting strategy selection based on project requirements.

</details>


### [19] [Model Discovery and Graph Simulation: A Lightweight Alternative to Chaos Engineering](https://arxiv.org/abs/2506.11176)
*Anatoly A. Krasnovsky,Alexander Zorkin*

Main category: cs.SE

TL;DR: 论文提出了一种自动化CI/CD步骤（模型发现），通过从跟踪数据中提取实时依赖图来预测微服务应用的弹性，实验证明该方法准确性高。


<details>
  <summary>Details</summary>
Motivation: 微服务应用因密集的服务间依赖容易发生级联故障，传统方法需在生产环境中进行故障注入实验，成本高。

Method: 提出模型发现方法，从跟踪数据中提取依赖图，通过蒙特卡洛模拟故障，并与实际混沌实验对比验证。

Result: 实验显示，依赖图模型预测与实际结果高度吻合（平均绝对误差≤0.0004），证明其有效性。

Conclusion: 自动发现的简单依赖图可高精度预测微服务可用性，无需大规模故障测试，为设计阶段提供快速洞察。

Abstract: Microservice applications are prone to cascading failures because of dense
inter-service dependencies. Ensuring resilience usually demands fault-injection
experiments in production-like setups. We propose \textit{model discovery} --
an automated CI/CD step that extracts a live dependency graph from trace data
-- and show that this lightweight representation is sufficient for accurate
resilience prediction. Using the DeathStarBench Social Network, we build the
graph, simulate failures via Monte-Carlo, and run matching chaos experiments on
the real system. The graph model closely matches reality: with no replication,
16 trials yield an observed resilience of 0.186 versus a predicted 0.161; with
replication, both observed and predicted values converge to 0.305 (mean
absolute error \leq 0.0004). These results indicate that even a simple,
automatically discovered graph can estimate microservice availability with high
fidelity, offering rapid design-time insight without full-scale failure
testing.

</details>


### [20] [You Only Train Once: A Flexible Training Framework for Code Vulnerability Detection Driven by Vul-Vector](https://arxiv.org/abs/2506.10988)
*Bowen Tian,Zhengyang Xu,Mingqiang Wu,Songning Lai,Yutai Yue*

Main category: cs.SE

TL;DR: 论文提出YOTO框架，通过参数融合实现多类型漏洞检测模型的快速适应，减少训练时间和计算资源。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态复杂，传统深度学习漏洞检测方法需要大量标注数据和频繁重训练，资源消耗大。

Method: 引入YOTO框架，通过参数融合整合多类型漏洞检测模型，无需联合训练。

Result: YOTO能快速适应新漏洞，显著减少模型更新的时间和计算成本。

Conclusion: YOTO为漏洞检测提供高效解决方案，适用于快速变化的软件环境。

Abstract: With the pervasive integration of computer applications across industries,
the presence of vulnerabilities within code bases poses significant risks. The
diversity of software ecosystems coupled with the intricate nature of modern
software engineering has led to a shift from manual code vulnerability
identification towards the adoption of automated tools. Among these, deep
learning-based approaches have risen to prominence due to their superior
accuracy; however, these methodologies encounter several obstacles. Primarily,
they necessitate extensive labeled datasets and prolonged training periods, and
given the rapid emergence of new vulnerabilities, the frequent retraining of
models becomes a resource-intensive endeavor, thereby limiting their
applicability in cutting-edge scenarios. To mitigate these challenges, this
paper introduces the \underline{\textbf{YOTO}}--\underline{\textbf{Y}}ou
\underline{\textbf{O}}nly \underline{\textbf{T}}rain \underline{\textbf{O}}nce
framework. This innovative approach facilitates the integration of multiple
types of vulnerability detection models via parameter fusion, eliminating the
need for joint training. Consequently, YOTO enables swift adaptation to newly
discovered vulnerabilities, significantly reducing both the time and
computational resources required for model updates.

</details>


### [21] [Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs](https://arxiv.org/abs/2506.10989)
*Rogelio Cruz,Jonatan Contreras,Francisco Guerrero,Ezequiel Rodriguez,Carlos Valdez,Citlali Carrillo*

Main category: cs.SE

TL;DR: 提出一种新颖的提示方法，提升大语言模型生成准确Python代码的能力，实验证明其优于零样本和思维链方法，且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型生成代码的质量和正确性，使其能通过测试并产生可靠结果。

Method: 设计一种提示模板，通过实验在HumanEval数据集上验证其效果。

Result: 在Pass@k指标上优于零样本和思维链方法，且显著减少token使用。

Conclusion: 定制化提示策略能优化代码生成性能，为AI驱动的编程任务提供更广泛应用。

Abstract: In this paper, we propose a novel prompting approach aimed at enhancing the
ability of Large Language Models (LLMs) to generate accurate Python code.
Specifically, we introduce a prompt template designed to improve the quality
and correctness of generated code snippets, enabling them to pass tests and
produce reliable results. Through experiments conducted on two state-of-the-art
LLMs using the HumanEval dataset, we demonstrate that our approach outperforms
widely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the
Pass@k metric. Furthermore, our method achieves these improvements with
significantly reduced token usage compared to the CoT approach, making it both
effective and resource-efficient, thereby lowering the computational demands
and improving the eco-footprint of LLM capabilities. These findings highlight
the potential of tailored prompting strategies to optimize code generation
performance, paving the way for broader applications in AI-driven programming
tasks.

</details>


### [22] [What is Business Process Automation Anyway?](https://arxiv.org/abs/2506.10991)
*Hoang Vu,Henrik Leopold,Han van der Aa*

Main category: cs.SE

TL;DR: 本文通过分析18家主要供应商的市场情况，全面概述了当前工业界提供的业务流程自动化能力，并探讨了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 当前学术研究主要关注机器人流程自动化（RPA），而忽略了工业界提供的更广泛的自动化能力，因此需要更全面的分析。

Method: 对Gartner认定的18家主要业务流程自动化解决方案供应商进行结构化市场分析。

Result: 提供了当前工业界业务流程自动化能力的全面概述，明确了自动化类型、方面及未来潜力方向。

Conclusion: 研究表明，业务流程自动化在工业界具有多样化能力，未来发展方向值得关注。

Abstract: Many organizations strive to increase the level of automation in their
business processes. While automation historically was mainly concerned with
automating physical labor, current automation efforts mostly focus on
automation in a digital manner, thus targeting work that is related to the
interaction between humans and computers. This type of automation, commonly
referred to as business process automation, has many facets. Yet, academic
literature mainly focuses on Robotic Process Automation, a specific automation
capability. Recognizing that leading vendors offer automation capabilities
going way beyond that, we use this paper to develop a detailed understanding of
business process automation in industry. To this end, we conduct a structured
market analysis of the 18 predominant vendors of business process automation
solutions as identified by Gartner. As a result, we provide a comprehensive
overview of the business process automation capabilities currently offered by
industrial vendors. We show which types and facets of automation exist and
which aspects represent promising directions for the future.

</details>


### [23] [Towards a Theory on Process Automation Effects](https://arxiv.org/abs/2506.10992)
*Hoang Vu,Jennifer Haase,Henrik Leopold,Jan Mendling*

Main category: cs.SE

TL;DR: 本文探讨了流程自动化对操作后的影响，通过综述人机交互文献，提出了技术与参与者间的有效互动模型。


<details>
  <summary>Details</summary>
Motivation: 研究流程自动化在实际操作中的影响，填补现有研究的空白。

Method: 综述人机交互领域的文献，分析人类对自动化技术的感知。

Result: 提出了技术与参与者间的有效互动模型，为组织优化自动化提供建议。

Conclusion: 本文为流程自动化社区提供了新的研究方向，并帮助组织优化自动化应用。

Abstract: Process automation is a crucial strategy for improving business processes,
but little attention has been paid to the effects that automation has once it
is operational. This paper addresses this research problem by reviewing the
literature on human-automation interaction. Although many of the studies in
this field have been conducted in different domains, they provide a foundation
for developing propositions about process automation effects. Our analysis
focuses on how humans perceive automation technology when working within a
process, allowing us to propose an effective engagement model between
technology, process participants, process managers, and software developers.
This paper offers insights and recommendations that can help organizations
optimize their use of process automation. We further derive novel research
questions for a discourse within the process automation community.

</details>


### [24] [Contract-based Verification of Digital Twins](https://arxiv.org/abs/2506.10993)
*Muhammad Naeem,Cristina Seceleanu*

Main category: cs.SE

TL;DR: 提出了一种基于模型检查的黑盒验证方法，用于验证神经网络数字孪生模型，通过系统级合约和UPPAAL模型检查器实现自动化验证。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在工业应用中日益重要，但其模型验证因大数据集而具有挑战性。

Method: 集成模型检查，定义系统级合约，使用UPPAAL模型检查器验证数字孪生模型的预测输出是否符合合约。

Result: 在锅炉系统案例中成功识别了预测错误，验证了方法的有效性。

Conclusion: 模型检查与数字孪生模型的结合为持续改进提供了有效途径。

Abstract: Digital twins are becoming powerful tools in industrial applications,
offering virtual representations of cyber-physical systems. However,
verification of these models remains a significant challenge due to the
potentially large datasets used by the digital twin. This paper introduces an
innovative methodology for verifying neural network-based digital twin models,
in a black-box fashion, by integrating model checking into the process. The
latter relies on defining and applying system-level contracts that capture the
system's requirements, to verify the behavior of digital twin models,
implemented in Simulink. We develop an automated solution that simulates the
digital twin model for certain inputs, and feeds the predicted outputs together
with the inputs to the contract model described as a network of timed automata
in the UPPAAL model checker. The latter verifies whether the predicted outputs
fulfill the specified contracts. This approach allows us to identify scenarios
where the digital twin's behavior fails to meet the contracts, without
requiring the digital twin's design technicalities. We apply our method to a
boiler system case study for which we identify prediction errors via contract
verification. Our work demonstrates the effectiveness of integrating model
checking with digital twin models for continuous improvement.

</details>


### [25] [Improving Software Team Communication Through Social Interventions in Project Management Tools](https://arxiv.org/abs/2506.10994)
*April Clarke*

Main category: cs.SE

TL;DR: 研究旨在通过社交网络分析开发项目管理工具功能，帮助学生改善软件工程团队中的沟通与协作行为。


<details>
  <summary>Details</summary>
Motivation: 软件工程团队常因沟通与协作不足影响项目成功，而大学项目课程是学生练习这些技能的机会，但缺乏有效指导方法。

Method: 首先评估社交网络分析技术在识别团队沟通改进领域的适用性，随后开发项目管理工具功能，并在软件工程团队项目中评估其效果。

Result: 预期通过工具功能帮助学生识别并改进团队沟通与协作问题。

Conclusion: 社交网络分析结合项目管理工具可有效指导学生改善团队行为，提升项目成功率。

Abstract: Productive software engineering teams require effective communication and
balanced contributions between team members. However, teams are often
ineffective at these skills, which is detrimental to project success.
Project-based university courses are an opportunity for students to practise
these skills, but we have yet to establish how we can guide students towards
improving their communication and coordination. We aim to develop project
management tool features, informed by social network analysis, that nudge
students in software engineering group projects towards beneficial behaviours.
To do this, we will first evaluate the suitability of social network analysis
techniques for identifying areas of improvement in teams' communication. Then,
we will develop features in a project management tool that aid students in
identifying and addressing these areas of improvement, and evaluate them in the
context of a software engineering group project.

</details>


### [26] [Evaluating Small-Scale Code Models for Code Clone Detection](https://arxiv.org/abs/2506.10995)
*Jorge Martinez-Gil*

Main category: cs.SE

TL;DR: 研究评估了几种小型代码模型在检测代码克隆中的性能，发现大多数模型表现良好，但仍存在少量难以检测的克隆情况。


<details>
  <summary>Details</summary>
Motivation: 代码克隆检测对软件维护和重构很重要，但现有方法在结构相似但功能不等价的情况下仍有不足。

Method: 使用五个数据集（BigCloneBench、CodeJam、Karnalim、POJ104、PoolC）和六种代码模型（CodeBERT、GraphCodeBERT、Salesforce T5、UniXCoder、PLBART、Polycoder）进行系统评估。

Result: 大多数模型在准确性、精确度、召回率和F1分数上表现良好，但仍有少量克隆难以检测。

Conclusion: 小型代码模型在代码克隆检测中表现优异，但需进一步改进以应对复杂情况。

Abstract: Detecting code clones is relevant to software maintenance and code
refactoring. This challenge still presents unresolved cases, mainly when
structural similarity does not reflect functional equivalence, though recent
code models show promise. Therefore, this research aims to systematically
measure the performance of several newly introduced small code models in
classifying code pairs as clones or non-clones. The evaluation is based on five
datasets: BigCloneBench, CodeJam, Karnalim, POJ104, and PoolC, as well as six
code models: CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and
Polycoder. Most models performed well across standard metrics, including
accuracy, precision, recall, and F1-score. However, a marginal fraction of
clones remains challenging to detect, especially when the code looks similar
but performs different operations. The source code that illustrates our
approach is available at:
https://github.com/jorge-martinez-gil/small-code-models

</details>


### [27] [Evaluating LLMs for Visualization Tasks](https://arxiv.org/abs/2506.10996)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在信息可视化中的能力，包括生成可视化代码和理解常见图表，但也指出了其局限性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在信息可视化任务中的表现，探索其潜力与不足。

Method: 通过简单提示让不同流行的LLMs生成可视化代码，并测试其对常见图表的理解能力。

Result: LLMs能够生成部分可视化代码并回答相关问题，但仍存在局限性。

Conclusion: 研究结果可为改进LLMs和信息可视化系统提供参考。

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language Models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
simple questions. Our study shows that LLMs could generate code for some
visualizations as well as answer questions about them. However, LLMs also have
several limitations. We believe that our insights can be used to improve both
LLMs and Information Visualization systems.

</details>


### [28] [A Theory-driven Interpretation and Elaboration of Verification and Validation](https://arxiv.org/abs/2506.10997)
*Hanumanthrao Kannan,Alejandro Salado*

Main category: cs.SE

TL;DR: 本文提出了一种基于动态认知模态逻辑的系统工程中验证与确认（V&V）的形式化理论，将其定义为知识构建活动，并提供了精确的定义和框架。


<details>
  <summary>Details</summary>
Motivation: 传统V&V实践中存在模糊性，需要一种形式化理论来明确其概念基础和作用。

Method: 使用动态认知模态逻辑，形式化V&V的定义及其在系统知识确认和情境化中的作用。

Result: 提出了一个结构化框架，增强了系统工程方法中的精确性和一致性，并推导了相关定理。

Conclusion: 该理论为V&V提供了形式化基础，对学术研究和实际应用均有重要意义。

Abstract: This paper presents a formal theory of verification and validation (V&V)
within systems engineering, grounded in the axiom that V&V are fundamentally
knowledge-building activities. Using dynamic epistemic modal logic, we develop
precise definitions of verification and validation, articulating their roles in
confirming and contextualizing knowledge about systems. The theory formalizes
the interplay between epistemic states, evidence, and reasoning processes,
allowing for the derivation of theorems that clarify the conceptual
underpinnings of V&V. By providing a formal foundation, this work addresses
ambiguities in traditional V&V practices, offering a structured framework to
enhance precision and consistency in systems engineering methodologies. The
insights gained have implications for both academic research and practical
applications, fostering a deeper understanding of V&V as critical components of
engineering knowledge generation.

</details>


### [29] [Towards Automated Formal Verification of Backend Systems with LLMs](https://arxiv.org/abs/2506.10998)
*Kangping Xu,Yifan Luo,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.SE

TL;DR: 提出了一种基于函数式编程和类型系统的框架，将Scala后端代码转换为形式化的Lean表示，并利用LLM验证生成的定理，自动化验证50%的测试需求。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测试方法在测试局部性、通用可靠性和业务逻辑盲区方面存在不足，无法匹配人类工程师的能力。

Method: 通过将Scala代码转换为Lean表示，自动生成描述API和数据库操作行为的定理，并使用LLM验证这些定理。

Result: 在真实后端系统中验证了50%的测试需求，LLM验证成本仅为每API 2.19美元，显著优于人工测试。

Conclusion: 该方法为可扩展的AI驱动软件测试提供了有前景的方向，有望大幅提升工程效率。

Abstract: Software testing plays a critical role in ensuring that systems behave as
intended. However, existing automated testing approaches struggle to match the
capabilities of human engineers due to key limitations such as test locality,
lack of general reliability, and business logic blindness. In this work, we
propose a novel framework that leverages functional programming and type
systems to translate Scala backend code into formal Lean representations. Our
pipeline automatically generates theorems that specify the intended behavior of
APIs and database operations, and uses LLM-based provers to verify them. When a
theorem is proved, the corresponding logic is guaranteed to be correct and no
further testing is needed. If the negation of a theorem is proved instead, it
confirms a bug. In cases where neither can be proved, human intervention is
required. We evaluate our method on realistic backend systems and find that it
can formally verify over 50% of the test requirements, which suggests that half
of a testing engineer's workload can be automated. Additionally, with an
average cost of only $2.19 per API, LLM-based verification is significantly
more cost-effective than manual testing and can be scaled easily through
parallel execution. Our results indicate a promising direction for scalable,
AI-powered software testing, with the potential to greatly improve engineering
productivity as models continue to advance.

</details>


### [30] [Automated Validation of COBOL to Java Transformation](https://arxiv.org/abs/2506.10999)
*Atul Kumar,Diptikalyan Saha,Toshikai Yasue,Kohichi Ono,Saravanan Krishnan,Sandeep Hans,Fumiko Satoh,Gerald Mitchell,Sachin Kumar*

Main category: cs.SE

TL;DR: 论文提出了一种验证COBOL与翻译后Java代码等价性的框架和工具，通过符号执行生成测试用例并检查语义等价性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM生成的代码转换结果令人鼓舞，但其正确性仍需验证，以确保翻译后的代码与原始代码功能一致。

Method: 使用符号执行生成COBOL程序的单元测试，并模拟外部资源调用；生成等效的JUnit测试用例并运行以验证语义等价性。

Result: 开发了工具来自动验证和修复翻译后的代码，同时为AI模型提供反馈以改进翻译质量。

Conclusion: 该框架有效验证了代码转换的等价性，并提升了翻译的可靠性。

Abstract: Recent advances in Large Language Model (LLM) based Generative AI techniques
have made it feasible to translate enterpriselevel code from legacy languages
such as COBOL to modern languages such as Java or Python. While the results of
LLM-based automatic transformation are encouraging, the resulting code cannot
be trusted to correctly translate the original code. We propose a framework and
a tool to help validate the equivalence of COBOL and translated Java. The
results can also help repair the code if there are some issues and provide
feedback to the AI model to improve. We have developed a
symbolic-execution-based test generation to automatically generate unit tests
for the source COBOL programs which also mocks the external resource calls. We
generate equivalent JUnit test cases with equivalent mocking as COBOL and run
them to check semantic equivalence between original and translated programs.

</details>


### [31] [Ever-Improving Test Suite by Leveraging Large Language Models](https://arxiv.org/abs/2506.11000)
*Ketai Qiu*

Main category: cs.SE

TL;DR: E-Test利用大型语言模型增量增强测试套件，以覆盖生产环境中未测试的行为。


<details>
  <summary>Details</summary>
Motivation: 增强测试套件以反映软件实际使用情况，维持长期软件质量。

Method: 利用大型语言模型识别已测试、未测试和易错单元执行场景，并相应增强测试套件。

Result: E-Test在识别未充分测试行为和优化测试套件方面优于现有方法。

Conclusion: E-Test能有效提升测试套件的覆盖率和质量。

Abstract: Augmenting test suites with test cases that reflect the actual usage of the
software system is extremely important to sustain the quality of long lasting
software systems. In this paper, we propose E-Test, an approach that
incrementally augments a test suite with test cases that exercise behaviors
that emerge in production and that are not been tested yet. E-Test leverages
Large Language Models to identify already-tested, not-yet-tested, and
error-prone unit execution scenarios, and augment the test suite accordingly.
Our experimental evaluation shows that E-Test outperforms the main
state-of-the-art approaches to identify inadequately tested behaviors and
optimize test suites.

</details>


### [32] [Rethinking Technological Readiness in the Era of AI Uncertainty](https://arxiv.org/abs/2506.11001)
*S. Tucker Browne,Mark M. Bailey*

Main category: cs.SE

TL;DR: 提出了一种新的AI准备框架，用于评估军事系统中AI组件的成熟度和可信度，以解决现有技术准备评估中未涵盖的AI特定问题。


<details>
  <summary>Details</summary>
Motivation: 当前的技术准备评估未能捕捉到AI特有的关键因素，可能导致部署风险。

Method: 提出了一个类似传统技术准备水平（TRL）但针对AI扩展的框架，结合现有数据评估工具和测试实践。

Result: 框架的可行性已在近期实施中得到验证，为军事决策者提供了更清晰的AI系统性能、透明度和人机集成标准。

Conclusion: 该框架有助于提升国防技术管理和风险评估，确保AI系统在军事应用中的可靠性和安全性。

Abstract: Artificial intelligence (AI) is poised to revolutionize military combat
systems, but ensuring these AI-enabled capabilities are truly mission-ready
presents new challenges. We argue that current technology readiness assessments
fail to capture critical AI-specific factors, leading to potential risks in
deployment. We propose a new AI Readiness Framework to evaluate the maturity
and trustworthiness of AI components in military systems. The central thesis is
that a tailored framework - analogous to traditional Technology Readiness
Levels (TRL) but expanded for AI - can better gauge an AI system's reliability,
safety, and suitability for combat use. Using current data evaluation tools and
testing practices, we demonstrate the framework's feasibility for near-term
implementation. This structured approach provides military decision-makers with
clearer insight into whether an AI-enabled system has met the necessary
standards of performance, transparency, and human integration to be deployed
with confidence, thus advancing the field of defense technology management and
risk assessment.

</details>


### [33] [Notes On Writing Effective Empirical Software Engineering Papers: An Opinionated Primer](https://arxiv.org/abs/2506.11002)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: 本文旨在为经验软件工程（ESE）研究提供科学写作指南，帮助初学者和有经验的研究者。


<details>
  <summary>Details</summary>
Motivation: 尽管科学写作在ESE研究中是重要评价标准，但相关实践很少被系统讨论和记录。

Method: 作者基于个人经验，提供主观且实用的写作建议。

Result: 指南旨在帮助BSc、MSc和PhD学生等群体更高效地撰写ESE论文。

Conclusion: 作者希望这一指南能为他人提供部分帮助，尽管其内容具有主观性。

Abstract: While mastered by some, good scientific writing practices within Empirical
Software Engineering (ESE) research appear to be seldom discussed and
documented. Despite this, these practices are implicit or even explicit
evaluation criteria of typical software engineering conferences and journals.
In this pragmatic, educational-first document, we want to provide guidance to
those who may feel overwhelmed or confused by writing ESE papers, but also
those more experienced who still might find an opinionated collection of
writing advice useful. The primary audience we had in mind for this paper were
our own BSc, MSc, and PhD students, but also students of others. Our documented
advice therefore reflects a subjective and personal vision of writing ESE
papers. By no means do we claim to be fully objective, generalizable, or
representative of the whole discipline. With that being said, writing papers in
this way has worked pretty well for us so far. We hope that this guide can at
least partially do the same for others.

</details>


### [34] [EmbedAgent: Benchmarking Large Language Models in Embedded System Development](https://arxiv.org/abs/2506.11003)
*Ruiyang Xu,Jialun Cao,Mingyuan Wu,Wenliang Zhong,Yaojie Lu,Ben He,Xianpei Han,Shing-Chi Cheung,Le Sun*

Main category: cs.SE

TL;DR: EmbedAgent和Embedbench被提出用于评估LLMs在嵌入式系统开发中的能力，发现LLMs表现参差不齐，并提出两种策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估LLMs在嵌入式系统开发中能力的基准，EmbedAgent和Embedbench填补了这一空白。

Method: 通过EmbedAgent模拟嵌入式系统开发角色，使用Embedbench（包含126个案例）测试LLMs在编程、电路设计和跨平台迁移中的表现。

Result: LLMs表现差异显著，DeepSeek-R1在提供原理图时通过率为55.6%，无原理图时为50.0%；跨平台任务中，MicroPython表现较好（73.8%），ESP-IDF较差（29.4%）。

Conclusion: 提出检索增强生成和编译器反馈策略，显著提升了LLMs的性能，如DeepSeek-R1通过率提升至65.1%。

Abstract: Large Language Models (LLMs) have shown promise in various tasks, yet few
benchmarks assess their capabilities in embedded system development.In this
paper, we introduce EmbedAgent, a paradigm designed to simulate real-world
roles in embedded system development, such as Embedded System Programmer,
Architect, and Integrator. This paradigm enables LLMs to be tested in tasks
that bridge the gap between digital and physical systems, allowing for a more
comprehensive assessment of their capabilities. To evaluate LLMs on these
tasks, we propose Embedbench, the first comprehensive benchmark for embedded
system programming, circuit design, and cross-platform migration.Embedbench
consists of 126 cases, covering 9 electronic components across 3 hardware
platforms. Through extensive experiments on 10 mainstream LLMs, we uncover
several key findings. Surprisingly, despite the simplicity of the cases,
DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic
information, and 50.0% when tasked with generating the schematics itself. In
the cross-platform migration tasks, LLMs show relatively strong performance
with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8%
pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4%
pass@1.Interestingly, we observe that general-purpose chat LLMs like
DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this
domain, while reasoning LLMs tend to overthink and overlook efficient knowledge
during pretraining. Based on these insights, we propose two strategies:
retrieval augmented generation and compiler feedback-to enhance LLM
performance. These strategies result in significant improvements, with
Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without.
Additionally, the accuracy of the Arduino to ESP32 migration task improves from
21.4% to 27.8%.

</details>


### [35] [Automated Extraction and Analysis of Developer's Rationale in Open Source Software](https://arxiv.org/abs/2506.11005)
*Mouna Dhaouadi,Bentley Oakes,Michalis Famelis*

Main category: cs.SE

TL;DR: 提出了一种基于Kantara架构的自动化方法，用于开源项目的理由分析，通过预训练模型和大语言模型检测设计冲突和问题。


<details>
  <summary>Details</summary>
Motivation: 开源贡献者需要理解项目历史以避免冲突，但手动检查耗时且缺乏自动化工具。

Method: 基于Kantara架构，利用预训练模型和大语言模型，结合结构化机制检测冲突和设计侵蚀问题。

Result: 在Linux内核的OOM-Killer模块和其他五个项目中验证了方法的可行性和通用性。

Conclusion: 自动化方法能有效支持理由分析，帮助开发者发现潜在冲突并确保新变更与历史决策一致。

Abstract: Contributors to open source software must deeply understand a project's
history to make coherent decisions which do not conflict with past reasoning.
However, inspecting all related changes to a proposed contribution requires
intensive manual effort, and previous research has not yet produced an
automated mechanism to expose and analyze these conflicts. In this article, we
propose such an automated approach for rationale analyses, based on an
instantiation of Kantara, an existing high-level rationale extraction and
management architecture. Our implementation leverages pre-trained models and
Large Language Models, and includes structure-based mechanisms to detect
reasoning conflicts and problems which could cause design erosion in a project
over time. We show the feasibility of our extraction and analysis approach
using the OOM-Killer module of the Linux Kernel project, and investigate the
approach's generalization to five other highly active open source projects. The
results confirm that our automated approach can support rationale analyses with
reasonable performance, by finding interesting relationships and to detect
potential conflicts and reasoning problems. We also show the effectiveness of
the automated extraction of decision and rationale sentences and the prospects
for generalizing this to other open source projects. This automated approach
could therefore be used by open source software developers to proactively
address hidden issues and to ensure that new changes do not conflict with past
decisions.

</details>


### [36] [Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs](https://arxiv.org/abs/2506.11006)
*Sai Krishna,Balvinder Singh,Sujoy Roychowdhury,Giriprasad Sridhara,Sourav Mazumdar,Magnus Sandelin,Dimitris Rentas,Maciej Nalepa,Karol Sawicki,Jakub Gajda*

Main category: cs.SE

TL;DR: 论文描述了在爱立信中使用大型语言模型（LLM）生成测试代码的方法，通过检索增强生成（RAG）和提示工程改进直接提示的不足，并通过微调进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 直接使用自然语言提示生成测试代码时，LLM会假设不存在的函数和签名，导致生成的代码与代码库不匹配。

Method: 结合RAG和提示工程，通过静态程序分析扩展提示的上下文信息，并基于自定义提示模板微调LLM。

Result: 微调后的模型（8x7b MoE）在生成代码的方法匹配度（F1分数）上平均提升8%，效果接近更大的8x22b MoE模型。

Conclusion: 通过RAG和微调，可以显著提升LLM生成测试代码的准确性和与开发者编写代码的匹配度。

Abstract: We describe test code generation using Large Language Models (LLMs) in
Ericsson. Our input is a test step in natural language (English) and our output
is code (Java) which accomplishes the test step. We describe how straight
forward prompting does not suffice and results in LLM assuming functions and
signatures which are not present in the code repository. We then show how we
alleviate the problem by a combination of Retrieval Augmented Generation (RAG)
along with prompt engineering that expanded the simple prompt with additional
contextual information using static program analysis. We then describe further
improvements that we obtained by fine-tuning the underlying LLM. The fine
tuning is done based on a custom designed prompt template which has
pre-dependent classes, their public methods as well two exemplar outputs
obtained from RAG. Our results establish that our fine tuned models help
improve the correspondence or conformity with the original developer written
test code as measured by the traditional metrics of F1-score based on the
methods used in the generated code. Fine tuning of a 8x7b Mixture of Experts
(MoE) model leads to an average improvement of 8\% over the base model and is
comparable to the scores on a much larger 8x22b MoE model.

</details>


### [37] [Impact of Comments on LLM Comprehension of Legacy Code](https://arxiv.org/abs/2506.11007)
*Rock Sabetto,Emily Escamilla,Devesh Agarwal,Sujay Kandwal,Justin F. Brunelle,Scott Rosen,Nitin Naik,Samruddhi Thaker,Eric O. Scott,Jacob Zimmer,Amit Madan,Arun Sridharan,Doug Wendt,Michael Doyle,Christopher Glasz,Jasper Phillips,William Macke,Colin Diggs,Michael Bartholf,Zachary Robin,Paul Ursino*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLMs）在理解和处理遗留代码时的能力，提出了一种基于多选问答（MCQA）的定量评估方法，并初步研究了文档对LLM理解的影响。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在现代编程语言任务中表现出色，但其对遗留语言的理解能力尚不明确，尤其是在缺乏或不准确文档的情况下，因此需要一种客观的评估方法。

Method: 采用多选问答（MCQA）作为评估方法，定量分析LLMs对遗留代码的理解能力，并研究文档（如注释）的影响。

Result: 初步发现文档的存在和准确性对LLM理解遗留代码有显著影响，为未来研究提供了方向。

Conclusion: 本文为评估LLMs在遗留语言中的理解能力提供了一种有效方法，并强调了文档质量的重要性，为未来研究奠定了基础。

Abstract: Large language models (LLMs) have been increasingly integrated into software
engineering and maintenance tasks due to their high performance with software
engineering tasks and robust understanding of modern programming languages.
However, the ability of LLMs to comprehend code written with legacy languages
remains a research gap challenged by real-world legacy systems lacking or
containing inaccurate documentation that may impact LLM comprehension. To
assess LLM comprehension of legacy languages, there is a need for objective LLM
evaluation. In order to objectively measure LLM comprehension of legacy
languages, we need an efficient, quantitative evaluation method. We leverage
multiple-choice question answering (MCQA), an emerging LLM evaluation
methodology, to evaluate LLM comprehension of legacy code and the impact of
comment prevalence and inaccurate comments. In this work, we present
preliminary findings on the impact of documentation on LLM comprehension of
legacy code and outline strategic objectives for future work.

</details>


### [38] [Encoding Software For Perpetuity: A Compact Representation Of Apollo 11 Guidance Code](https://arxiv.org/abs/2506.11008)
*David Noever*

Main category: cs.SE

TL;DR: 将阿波罗11号登月模块的指导计算机代码压缩成单个QR码，实现历史软件的高效保存与传播。


<details>
  <summary>Details</summary>
Motivation: 解决历史软件在现代移动设备上的可访问性问题，无需专用硬件或互联网连接。

Method: 采用标记化、选择性内容保留和最小化HTML/JavaScript技术，将原始汇编代码压缩为3KB的QR码。

Result: 成功压缩关键组件，评估了多种压缩策略在大小、可读性和历史意义上的权衡。

Conclusion: 该方法为计算遗产保护提供了新思路，通过现代移动技术实现历史软件的即时访问。

Abstract: This brief note presents a novel method for encoding historic Apollo 11 Lunar
Module guidance computer code into a single, compact Quick Response Code (QR
code) format, creating an accessible digital artifact for transmission and
archival purposes. By applying tokenization, selective content preservation,
and minimal HTML/JavaScript techniques, we successfully compressed key
components of the original Assembly Language Code (AGC) into a shareable,
preservable, and scannable 3 kilobyte (KB) image. We evaluate multiple
compression strategies and their tradeoffs in terms of size, readability, and
historical significance. This method addresses the challenge of making
historically significant software artifacts available through modern mobile
devices without requiring specialized hardware or internet connectivity. While
numerous digital preservation methods exist for historic software, this
approach balances accessibility with historical significance, offering a
complementary method to traditional archival techniques. This work contributes
to the broader field of computing heritage preservation by demonstrating how
landmark software can be made accessible instantly through contemporary mobile
technologies.

</details>


### [39] [Human-In-The-Loop Software Development Agents: Challenges and Future Directions](https://arxiv.org/abs/2506.11009)
*Jirat Pasuksmit,Wannita Takerngsaksiri,Patanamon Thongtanunam,Chakkrit Tantithamthavorn,Ruixiong Zhang,Shiyan Wang,Fan Jiang,Jing Li,Evan Cook,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: 论文探讨了多智能体LLM驱动的软件开发系统，提出了计算成本高和评估不一致的挑战，并建议未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用多智能体LLM系统提升软件开发效率，并通过Human-in-the-Loop方法解决实际问题。

Method: 在Atlassian部署了Human-in-the-Loop软件开发代理，通过功能正确性测试和GPT相似性评分评估代码质量。

Result: 发现两大挑战：单元测试的高计算成本和LLM评估的变异性。

Conclusion: 建议未来研究改进Human-in-the-Loop软件开发工具的评价框架。

Abstract: Multi-agent LLM-driven systems for software development are rapidly gaining
traction, offering new opportunities to enhance productivity. At Atlassian, we
deployed Human-in-the-Loop Software Development Agents to resolve Jira work
items and evaluated the generated code quality using functional correctness
testing and GPT-based similarity scoring. This paper highlights two major
challenges: the high computational costs of unit testing and the variability in
LLM-based evaluations. We also propose future research directions to improve
evaluation frameworks for Human-In-The-Loop software development tools.

</details>


### [40] [Enhancing Inventory Management with Progressive Web Applications (PWAs): A Scalable Solution for Small and Large Enterprises](https://arxiv.org/abs/2506.11011)
*Abhi Desai*

Main category: cs.SE

TL;DR: 本文探讨了一种基于渐进式网络应用（PWA）的库存管理解决方案，旨在优化操作流程并降低成本。


<details>
  <summary>Details</summary>
Motivation: 高效的库存管理对企业至关重要，而PWA技术提供了一种跨平台、离线可用的解决方案。

Method: 开发了一个集成条码扫描、地理定位和多设备访问功能的PWA应用。

Result: 研究表明PWA在库存管理中有潜力，但性能略逊于原生应用。

Conclusion: PWA为库存管理提供了一种可扩展且经济高效的替代方案，并为未来开发提供了参考。

Abstract: Efficient inventory management is crucial for both small and large
enterprises to optimize operational workflows and reduce overhead costs. This
paper explores the development and implementation of a Progressive Web
Application (PWA) designed to enhance the inventory management experience. The
application integrates key functionalities such as barcode and QR code
scanning, geolocation-based warehouse identification, and cross-device
accessibility. By leveraging PWA technology, the solution ensures offline
capabilities, responsive user experience, and seamless adaptability across
various platforms. The study discusses the challenges and benefits of
implementing PWA in inventory management systems, including its limitations in
performance compared to native applications. Insights from the development
process provide a roadmap for future developers looking to integrate PWA
technology into enterprise applications. This research contributes to the
growing domain of web-based inventory solutions, offering a scalable and
cost-effective alternative to traditional inventory management software.

</details>


### [41] [Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.11013)
*Filipe Fernandes,Cláudia Werner*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了量子软件工程（QSE）的现状，发现该领域仍处于发展阶段，缺乏标准化方法和实证验证，并提出了巴西研究议程以推动本地参与。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程（QSE）作为新兴领域，知识碎片化且缺乏标准化方法，巴西等国家参与有限，亟需系统性研究以指导未来发展。

Method: 采用系统映射研究方法，基于纳入和排除标准筛选文献，并按研究类型、研究方法和SWEBOK知识领域分类。

Result: 多数研究为英文文献，聚焦软件工程模型、架构和测试，但实证验证不足，巴西贡献较少。

Conclusion: QSE前景广阔但需标准化和实证研究，巴西应制定国家研究议程以促进本地参与和学科发展。

Abstract: Context: Quantum Software Engineering (QSE) has emerged as a key field to
support the development of reliable, maintainable, and scalable quantum
applications, bridging advances in quantum computing with established practices
in software engineering. Problem: Despite its growth, the field still suffers
from fragmented knowledge, with a lack of standardized methodologies, tools,
and guidelines tailored to the unique features of the quantum paradigm.
Additionally, countries like Brazil have had limited participation in the
development of this emerging domain. Objective: This study aims to map the
state of the art in QSE by identifying current research trends, recurring
contributions, and existing gaps that can guide future investigations and
strategic initiatives. Methodology: A systematic mapping study was conducted
analyzing selected publications based on inclusion and exclusion criteria.
Articles were categorized by study type, research type, and alignment with the
SWEBOK knowledge areas. Results: Most of the reviewed studies are primary
research articles written in English, with a strong focus on Software
Engineering Models and Methods, Software Architecture, and Software Testing.
Conceptual proposals and technical solutions predominate, while empirical
validations remain limited. Conclusions: Findings confirm that QSE is a
promising but still maturing field. The standardization of practices, expansion
of empirical studies, and inclusion of researchers from developing countries
are crucial for advancing the discipline. Additionally, Brazilian contributions
are still scarce, highlighting the urgent need to establish a national research
agenda. As a main contribution, this study proposes a Brazilian Research Agenda
in QSE, outlining priority areas and opportunities to foster a local scientific
community and accelerate progress in this emerging field.

</details>


### [42] [MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants](https://arxiv.org/abs/2506.11014)
*Benedetta Donato,Leonardo Mariani,Daniela Micucci,Oliviero Riganelli,Marco Somaschini*

Main category: cs.SE

TL;DR: MultiMind是一个Visual Studio Code插件，旨在简化AI辅助开发任务的创建，解决在IDE中嵌入AI助手时的挑战。


<details>
  <summary>Details</summary>
Motivation: AI助手在软件开发中的应用日益广泛，但如何将其无缝集成到开发工作流中仍面临挑战。

Method: MultiMind提供了一个模块化和可扩展的框架，支持开发者在无需复杂IDE定制的情况下实现和测试新的AI交互。

Result: MultiMind在代码注释自动生成和AI聊天功能定义两个用例中进行了测试。

Conclusion: MultiMind为开发者提供了一种高效且灵活的方式，将AI助手集成到开发工作流中。

Abstract: The integration of AI assistants into software development workflows is
rapidly evolving, shifting from automation-assisted tasks to collaborative
interactions between developers and AI. Large Language Models (LLMs) have
demonstrated their effectiveness in several development activities, including
code completion, test case generation, and documentation production. However,
embedding AI-assisted tasks within Integrated Development Environments (IDEs)
presents significant challenges. It requires designing mechanisms to invoke AI
assistants at the appropriate time, coordinate interactions with multiple
assistants, process the generated outputs, and present feedback in a way that
seamlessly integrates with the development workflow. To address these issues,
we introduce MultiMind, a Visual Studio Code plug-in that streamlines the
creation of AI-assisted development tasks. MultiMind provides a modular and
extensible framework, enabling developers to cost-effectively implement and
experiment with new AI-powered interactions without the need for complex IDE
customizations. MultiMind has been tested in two use cases: one for the
automatic generation of code comments and the other about the definition of
AI-powered chat.

</details>


### [43] [ZjsComponent: A Pragmatic Approach to Modular, Reusable UI Fragments for Web Development](https://arxiv.org/abs/2506.11016)
*Lelanthran Manickum*

Main category: cs.SE

TL;DR: ZjsComponent是一个轻量级、框架无关的Web组件，用于创建模块化、可复用的UI元素，无需构建步骤或其他依赖。


<details>
  <summary>Details</summary>
Motivation: 传统组件开发需要构建步骤、转译或特定生态系统，增加了开发复杂性。ZjsComponent旨在简化这一过程，提供一种纯HTML即可使用的组件方案。

Method: ZjsComponent通过动态加载和隔离HTML+JS片段实现，无需构建步骤或依赖，支持DOM和代码隔离，并提供简单的生命周期钩子。

Result: 开发者可以轻松构建可复用的UI元素，减少开发复杂性，同时保持代码和DOM的隔离性。

Conclusion: ZjsComponent为开发者提供了一种简单、依赖少的Web组件实现方式，适用于快速开发模块化UI。

Abstract: In this paper, I present ZjsComponent, a lightweight and framework-agnostic
web component designed for creating modular, reusable UI elements with minimal
developer overhead. ZjsComponent is an example implementation of an approach to
creating components and object instances that can be used purely from HTML.
Unlike traditional approaches to components, the approach implemented by
ZjsComponent does not require build-steps, transpiling, pre-compilation, any
specific ecosystem or any other dependency. All that is required is that the
browser can load and execute Javascript as needed by Web Components.
ZjsComponent allows dynamic loading and isolation of HTML+JS fragments,
offering developers a simple way to build reusable interfaces with ease. This
approach is dependency-free, provides significant DOM and code isolation, and
supports simple lifecycle hooks as well as traditional methods expected of an
instance of a class.

</details>


### [44] [Formation of requirements traceability in the process of information systems design](https://arxiv.org/abs/2506.11018)
*Grigory Tsiperman*

Main category: cs.SE

TL;DR: 论文提出了一种基于自适应聚类方法（ACM）的需求可追踪性解决方案，旨在无缝集成到信息系统设计过程中。


<details>
  <summary>Details</summary>
Motivation: 需求可追踪性是信息系统设计中的关键质量特性，但如何将其无缝集成到设计过程中是一个重大挑战。

Method: 采用作者开发的自适应聚类方法（ACM），通过无缝系统架构实现不同抽象层次项目工件的显式连接。

Result: ACM方法能够有效提升需求可追踪性，减少系统对开发者的依赖，简化设计过程。

Conclusion: ACM方法为解决需求可追踪性集成问题提供了可行方案，有望优化信息系统设计流程。

Abstract: The traceability of requirements in the information system design process is
considered an essential property of the project, one of its quality
characteristics. The point here is that traceability provides the methods of
validation and verification of software systems, and that the system model
based on requirements traceability reduces the system's dependence on
developers and, in general, makes it as straightforward as possible. One of the
challenges of the traceability process, dubbed "The grand challenge of
traceability" among traceability researchers, is its integration into the
design process. In this paper, to achieve this goal, we propose the application
of the Adaptive Clustering Method (ACM) of Information Systems developed by the
author, which is based on the idea of a seamless system architecture that
provides explicit interconnection of project artifacts of different levels of
abstraction.

</details>


### [45] [Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)](https://arxiv.org/abs/2506.11019)
*Vincent Koc,Jacques Verre,Douglas Blank,Abigail Morgan*

Main category: cs.SE

TL;DR: 论文介绍了基于模型上下文协议（MCP）的遥测感知集成开发环境（IDE），支持实时反馈和优化，并展示了开源MCP服务器Opik的应用。


<details>
  <summary>Details</summary>
Motivation: AI开发环境正演变为以可观测性为核心的平台，需要将实时遥测、提示跟踪和评估反馈集成到开发流程中。

Method: 通过MCP系统连接IDE与提示指标、跟踪日志和版本控制，支持本地提示迭代、CI优化和自主代理行为调整。

Result: 展示了开源MCP服务器Opik的应用，并定位了该方法在LLMOps生态系统中的位置。

Conclusion: 为未来提示优化、IDE代理工具和遥测丰富的AI开发流程的实证基准研究奠定了基础。

Abstract: AI development environments are evolving into observability first platforms
that integrate real time telemetry, prompt traces, and evaluation feedback into
the developer workflow. This paper introduces telemetry aware integrated
development environments (IDEs) enabled by the Model Context Protocol (MCP), a
system that connects IDEs with prompt metrics, trace logs, and versioned
control for real time refinement. We present design patterns for local prompt
iteration, CI based optimization, and autonomous agents that adapt behavior
using telemetry. Rather than focusing on a single algorithm, we describe an
architecture that supports integration with frameworks like DSPy, PromptWizard,
and Prompts as Programs. We demonstrate this through Opik, an open source MCP
server for LLM telemetry, and position our approach within the emerging LLMOps
ecosystem. This work lays a foundation for future research on prompt
optimization, IDE agent tooling, and empirical benchmarking in telemetry rich
AI development workflows.

</details>


### [46] [Extracting Knowledge Graphs from User Stories using LangChain](https://arxiv.org/abs/2506.11020)
*Thayná Camargo da Silva*

Main category: cs.SE

TL;DR: 论文提出了一种利用大语言模型自动从用户故事生成知识图谱的新方法，通过LangChain框架和用户故事图转换模块实现自动化。


<details>
  <summary>Details</summary>
Motivation: 提升用户需求和领域概念的可视化与理解，以更好地对齐软件功能与用户期望，推动更有效的用户中心软件开发。

Method: 开发了基于LangChain框架的用户故事图转换模块，利用大语言模型提取节点和关系，构建知识图谱，并通过脚本实现全自动化。

Result: 实现了知识图谱的自动化提取和评估，通过标注数据集验证了方法的有效性。

Conclusion: 该方法增强了用户需求的理解，促进了软件功能与用户期望的对齐，为软件开发提供了更高效的工具。

Abstract: This thesis introduces a novel methodology for the automated generation of
knowledge graphs from user stories by leveraging the advanced capabilities of
Large Language Models. Utilizing the LangChain framework as a basis, the User
Story Graph Transformer module was developed to extract nodes and relationships
from user stories using an LLM to construct accurate knowledge graphs.This
innovative technique was implemented in a script to fully automate the
knowledge graph extraction process. Additionally, the evaluation was automated
through a dedicated evaluation script, utilizing an annotated dataset for
assessment. By enhancing the visualization and understanding of user
requirements and domain concepts, this method fosters better alignment between
software functionalities and user expectations, ultimately contributing to more
effective and user-centric software development processes.

</details>


### [47] [Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering](https://arxiv.org/abs/2506.11021)
*Chaitanya Ravuri,Saman Amarasinghe*

Main category: cs.SE

TL;DR: 提出了一种名为功能聚类的黑盒包装方法，显著减少代码生成LLM的幻觉错误，并提供可调置信度分数。


<details>
  <summary>Details</summary>
Motivation: 现代代码生成LLM虽能解决大量编程问题，但仍会生成细微的错误，导致输出不可靠。

Method: 通过采样多个候选程序，在自生成测试套件上执行，并聚类I/O行为相同的候选程序，以最大聚类的经验质量作为置信度估计。

Result: 在LiveCodeBench上，验证器将错误率从~65%降至2%，保守阈值下可降至0%且仍能回答15.6%的提示。

Conclusion: 该方法仅需采样和沙箱执行，适用于闭源API和未来模型，为可靠自主代码生成提供了实用路径。

Abstract: Modern code-generation LLMs can already solve a large fraction of programming
problems, yet they still hallucinate subtle bugs that make their outputs unsafe
for autonomous deployment. We present functional clustering, a black-box
wrapper that eliminates nearly all hallucination-induced errors while providing
a tunable confidence score. The wrapper samples many candidate programs,
executes each on a self-generated test suite, and clusters candidates whose I/O
behavior is identical; the empirical mass of the largest cluster serves as an
exact confidence estimate. A single scalar threshold on this estimate lets
users trade coverage for reliability with exponential guarantees. On
LiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet
slashes the error rate of returned answers from ~65% to 2%, and drives it to 0%
at a conservative threshold while still answering 15.6% of prompts. Manual
audits show that the few residual mistakes stem from prompt misinterpretation,
not random generation noise, narrowing future work to specification clarity.
Because the method requires only sampling and sandbox execution, it applies
unchanged to closed-source APIs and future models, offering a practical path
toward dependable, autonomous code generation. Our code is available on Github
(https://github.com/20ChaituR/functional-clustering).

</details>


### [48] [Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox](https://arxiv.org/abs/2506.11022)
*Shivani Shukla,Himanshu Joshi,Romilla Syed*

Main category: cs.SE

TL;DR: 研究发现，通过LLM迭代反馈生成的代码中，安全漏洞显著增加，挑战了迭代改进提升代码安全性的假设，并强调人类验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨LLM迭代反馈对代码安全性的影响，填补了该领域的研究空白。

Method: 通过控制实验，对400个代码样本进行40轮改进，采用四种不同的提示策略。

Result: 结果显示，仅五轮迭代后，关键漏洞增加了37.6%，不同提示策略下漏洞模式各异。

Conclusion: 结论指出，迭代LLM改进可能引入新安全问题，需加强人类验证，并提出实用指南以降低风险。

Abstract: The rapid adoption of Large Language Models(LLMs) for code generation has
transformed software development, yet little attention has been given to how
security vulnerabilities evolve through iterative LLM feedback. This paper
analyzes security degradation in AI-generated code through a controlled
experiment with 400 code samples across 40 rounds of "improvements" using four
distinct prompting strategies. Our findings show a 37.6% increase in critical
vulnerabilities after just five iterations, with distinct vulnerability
patterns emerging across different prompting approaches. This evidence
challenges the assumption that iterative LLM refinement improves code security
and highlights the essential role of human expertise in the loop. We propose
practical guidelines for developers to mitigate these risks, emphasizing the
need for robust human validation between LLM iterations to prevent the
paradoxical introduction of new security issues during supposedly beneficial
code "improvements".

</details>


### [49] [Software Security Mapping Framework: Operationalization of Security Requirements](https://arxiv.org/abs/2506.11051)
*Sung Une Lee,Liming Dong,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 本文提出了一种软件安全映射框架，将抽象的安全原则转化为具体可操作的措施，覆盖从高层标准到技术活动的多层次需求。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发环境的复杂性增加，现有框架难以将安全原则转化为具体实践，亟需一种结构化解决方案。

Method: 采用KAOS目标建模方法，系统地将131项安全需求映射到400多个操作步骤，并提供基于Web的导航工具和OSCAL目录模型。

Result: 框架成功应用于Log4j漏洞案例，生成符合行业最佳实践的定制清单，并支持自动化实施和合规流程。

Conclusion: 该框架通过明确目标与操作的关联，提升了安全实施的清晰度和可操作性，为组织应对安全风险提供了实用工具。

Abstract: The escalating complexity of modern software development environments has
heightened concerns around supply chain security. However, existing frameworks
often fall short in translating abstract security principles into concrete,
actionable practices. This paper introduces the Software Security Mapping
Framework, a structured solution designed to operationalize security
requirements across hierarchical levels -- from high-level regulatory standards
(e.g., ISM, Australia cybersecurity standard published by the Australian
Signals Directorate), through mid-level frameworks (e.g., NIST SSDF, the U.S.
Secure Software Development Framework), to fine-grained technical activities
(e.g., SLSA, a software supply chain security framework). Developed through
collaborative research with academic experts and industry practitioners, the
framework systematically maps 131 refined security requirements to over 400
actionable operational steps spanning the software development lifecycle. It is
grounded in four core security goals: Secure Software Environment, Secure
Software Development, Software Traceability, and Vulnerability Management. Our
approach leverages the KAOS goal modeling methodology to establish traceable
linkages between strategic goals and tactical operations, enhancing clarity,
accountability, and practical implementation. To facilitate adoption, we
provide a web-based navigation tool for interactive exploration of the
framework. A real-world case study based on the Log4j vulnerability illustrates
the framework's utility by generating a tailored checklist aligned with
industry best practices. Additionally, we offer a structured, machine-readable
OSCAL Catalog Model of the Software Security Mapping Framework, enabling
organizations to automate implementation, streamline compliance processes, and
respond effectively to evolving security risks.

</details>


### [50] [Refactoring Codebases through Library Design](https://arxiv.org/abs/2506.11058)
*Ziga Kovacic,Celine Lee,Justin Chiu,Wenting Zhao,Kevin Ellis*

Main category: cs.SE

TL;DR: 论文研究了代码代理在代码重构中的能力，提出了Librarian方法和Minicode基准，Librarian在压缩率和正确性上优于现有代码代理。


<details>
  <summary>Details</summary>
Motivation: 随着代码代理在解决独立编程问题上的准确性提高，如何将其能力应用于支持代码增长和可重用性的重构成为重要课题。

Method: 提出了Librarian方法（一种采样和重排序方法）和Minicode基准，用于生成可重用库。

Result: Librarian在Minicode上实现了1.6-2倍的压缩率提升，同时提高了正确性。

Conclusion: Librarian在代码重构中表现出色，支持代码的可重用性和增长。

Abstract: Maintainable and general software allows developers to build robust
applications efficiently, yet achieving these qualities often requires
refactoring specialized solutions into reusable components. This challenge
becomes particularly relevant as code agents become increasingly accurate at
solving isolated programming problems. We investigate code agents' capacity to
refactor code in ways supporting growth and reusability. We present both a
method and a benchmark for refactoring: Librarian, a sample-and-rerank method
for generating reusable libraries, and Minicode, a benchmark where code agents
must minimize and refactor multiple independent solutions into a joint library.
Compared to state-of-the-art code agents, Librarian achieves strong results on
both compression and correctness on Minicode, obtaining compression rates
1.6-2x better than coding agents while also improving correctness. We
open-source our code and benchmark at https://code-refactor.github.io/.

</details>


### [51] [CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs](https://arxiv.org/abs/2506.11059)
*Hanxi Guo,Siyuan Cheng,Kaiyuan Zhang,Guangyu Shen,Xiangyu Zhang*

Main category: cs.SE

TL;DR: CodeMirage是一个全面的基准测试，用于评估AI生成代码的检测器，涵盖10种编程语言和10种先进LLM，揭示了当前检测器的优缺点。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的代码可能带来抄袭、许可证违规和安全风险，需要开发可靠的检测工具，但现有基准测试不足。

Method: CodeMirage通过覆盖多种编程语言、包含原始和改写代码样本，并整合10种先进LLM的输出，构建了一个全面的基准测试。

Result: 评估了10种检测器，揭示了9个关键发现，展示了当前检测器的局限性和未来挑战。

Conclusion: CodeMirage为开发更鲁棒和通用的AI生成代码检测器提供了严谨且实用的测试平台。

Abstract: Large language models (LLMs) have become integral to modern software
development, producing vast amounts of AI-generated source code. While these
models boost programming productivity, their misuse introduces critical risks,
including code plagiarism, license violations, and the propagation of insecure
programs. As a result, robust detection of AI-generated code is essential. To
support the development of such detectors, a comprehensive benchmark that
reflects real-world conditions is crucial. However, existing benchmarks fall
short -- most cover only a limited set of programming languages and rely on
less capable generative models. In this paper, we present CodeMirage, a
comprehensive benchmark that addresses these limitations through three major
advancements: (1) it spans ten widely used programming languages, (2) includes
both original and paraphrased code samples, and (3) incorporates outputs from
ten state-of-the-art production-level LLMs, including both reasoning and
non-reasoning models from six major providers. Using CodeMirage, we evaluate
ten representative detectors across four methodological paradigms under four
realistic evaluation configurations, reporting results using three
complementary metrics. Our analysis reveals nine key findings that uncover the
strengths and weaknesses of current detectors, and identify critical challenges
for future work. We believe CodeMirage offers a rigorous and practical testbed
to advance the development of robust and generalizable AI-generated code
detectors.

</details>


### [52] [Code Researcher: Deep Research Agent for Large Systems Code and Commit History](https://arxiv.org/abs/2506.11060)
*Ramneet Singh,Sathvik Joel,Abhav Mehrotra,Nalin Wadhwa,Ramakrishna B Bairi,Aditya Kanade,Nagarajan Natarajan*

Main category: cs.SE

TL;DR: Code Researcher是一种基于LLM的深度研究代理，专注于系统代码补丁生成，通过多步推理和全局上下文收集显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 系统代码的复杂性和规模使得修改任务极具挑战性，需要深入研究代码库及其提交历史。

Method: 设计Code Researcher代理，通过多步推理分析代码语义、模式和提交历史，并将上下文存储在结构化内存中以生成补丁。

Result: 在Linux内核崩溃基准测试中，Code Researcher的崩溃解决率为58%，显著优于SWE-agent的37.5%，且能更深入地探索代码库。

Conclusion: 全局上下文收集和多方面推理对大型代码库至关重要，Code Researcher展示了其高效性和通用性。

Abstract: Large Language Model (LLM)-based coding agents have shown promising results
on coding benchmarks, but their effectiveness on systems code remains
underexplored. Due to the size and complexities of systems code, making changes
to a systems codebase is a daunting task, even for humans. It requires
researching about many pieces of context, derived from the large codebase and
its massive commit history, before making changes. Inspired by the recent
progress on deep research agents, we design the first deep research agent for
code, called Code Researcher, and apply it to the problem of generating patches
for mitigating crashes reported in systems code. Code Researcher performs
multi-step reasoning about semantics, patterns, and commit history of code to
gather sufficient context. The context is stored in a structured memory which
is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a
benchmark of Linux kernel crashes, and show that it significantly outperforms
strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5%
by SWE-agent. On an average, Code Researcher explores 10 files in each
trajectory whereas SWE-agent explores only 1.33 files, highlighting Code
Researcher's ability to deeply explore the codebase. Through another experiment
on an open-source multimedia software, we show the generalizability of Code
Researcher. Our experiments highlight the importance of global context
gathering and multi-faceted reasoning for large codebases.

</details>


### [53] [CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval](https://arxiv.org/abs/2506.11066)
*Jiahui Geng,Fengyu Cai,Shaobo Cui,Qing Li,Liangwei Chen,Chenyang Lyu,Haonan Li,Derui Zhu,Walter Pretschner,Heinz Koeppl,Fakhri Karray*

Main category: cs.SE

TL;DR: CoQuIR是首个大规模、多语言的代码检索基准，专注于代码质量评估，涵盖正确性、效率、安全性和可维护性。通过评估23种检索模型，发现现有模型难以区分低质量代码。初步训练方法显示质量感知检索的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前代码检索基准主要关注功能相关性，忽视了代码质量的关键维度，因此需要开发一个质量感知的评估基准。

Method: 引入CoQuIR基准，包含42,725个查询和134,907个代码片段，提供细粒度质量标注，并提出两种质量中心评估指标。

Result: 现有模型在区分低质量代码方面表现不佳，但初步训练方法显示出质量感知检索的改进潜力。

Conclusion: 将质量信号整合到代码检索系统中，为更可靠和健壮的软件开发工具奠定了基础。

Abstract: Code retrieval is essential in modern software development, as it boosts code
reuse and accelerates debugging. However, current benchmarks primarily
emphasize functional relevance while neglecting critical dimensions of software
quality. Motivated by this gap, we introduce CoQuIR, the first large-scale,
multilingual benchmark specifically designed to evaluate quality-aware code
retrieval across four key dimensions: correctness, efficiency, security, and
maintainability. CoQuIR provides fine-grained quality annotations for 42,725
queries and 134,907 code snippets in 11 programming languages, and is
accompanied by two quality-centric evaluation metrics: Pairwise Preference
Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23
retrieval models, covering both open-source and proprietary systems, and find
that even top-performing models frequently fail to distinguish buggy or
insecure code from their more robust counterparts. Furthermore, we conduct
preliminary investigations into training methods that explicitly encourage
retrievers to recognize code quality. Using synthetic datasets, we demonstrate
promising improvements in quality-aware metrics across various models, without
sacrificing semantic relevance. Downstream code generation experiments further
validate the effectiveness of our approach. Overall, our work highlights the
importance of integrating quality signals into code retrieval systems, laying
the groundwork for more trustworthy and robust software development tools.

</details>


### [54] [DCE-LLM: Dead Code Elimination with Large Language Models](https://arxiv.org/abs/2506.11076)
*Minyu Chen,Guoqiang Li,Ling-I Wu,Ruibang Liu*

Main category: cs.SE

TL;DR: DCE-LLM是一个基于CodeBERT和LLM的自动化死代码消除框架，显著优于现有工具和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 死代码在软件开发中带来诸多问题，如增加二进制大小、维护困难，甚至可能误导LLM模型，引发安全隐患。

Method: 结合CodeBERT模型和基于属性的行选择器定位可疑代码，利用LLM生成判断和解释，并通过大规模标注数据集进行微调。

Result: 实验显示DCE-LLM在未使用和不可达代码检测上F1分数超过94%，比GPT-4o高出30%。

Conclusion: DCE-LLM提供了一种高效的自动化死代码消除方案，支持多语言，并显著优于现有工具。

Abstract: Dead code introduces several challenges in software development, such as
increased binary size and maintenance difficulties. It can also obscure logical
errors and be exploited for obfuscation in malware. For LLM-based code-related
tasks, dead code introduces vulnerabilities that can mislead these models,
raising security concerns. Although modern compilers and IDEs offer dead code
elimination, sophisticated patterns can bypass these tools. A universal
approach that includes classification, location, explanation, and correction is
needed, yet current tools often require significant manual effort. We present
DCE-LLM, a framework for automated dead code elimination using a small CodeBERT
model with an attribution-based line selector to efficiently locate suspect
code. LLMs then generate judgments and explanations, fine-tuned on a
large-scale, annotated dead code dataset to provide detailed explanations and
patches. DCE-LLM outperforms existing tools, with advanced unreachability
detection, automated correction, and support for multiple programming
languages. Experimental results show DCE-LLM achieves over 94% F1 scores for
unused and unreachable code, significantly surpassing GPT-4o by 30%.

</details>


### [55] [Research and Analysis of Employers' Opinion on the Necessary Skills that Students in the Field of Web Programming Should Possess](https://arxiv.org/abs/2506.11084)
*Yordan Kalmukov*

Main category: cs.SE

TL;DR: 论文探讨了在AI和自动化工具普及的背景下，雇主对毕业生技能需求的变化，重点分析了IT雇主对Web编程学生应具备技能的调查结果。


<details>
  <summary>Details</summary>
Motivation: 随着AI和自动化工具的普及，编程任务可以通过现成工具完成，因此需要重新评估学生对基础开发技能和现成工具使用能力的优先级。

Method: 通过调查IT雇主，了解他们对Web编程毕业生应具备技能的看法。

Result: 调查结果揭示了雇主认为毕业生应具备的技能组合，以快速适应公司工作。

Conclusion: 论文强调了在教学中平衡基础开发技能和现成工具使用能力的重要性。

Abstract: In the era of artificial intelligence (AI) and chatbots, based on large
language models that can generate programming code in any language, write texts
and summarize information, it is obvious that the requirements of employers for
graduating students have already changed. The modern IT world offers
significant automation of programming through software frameworks and a huge
set of third-party libraries and application programming interfaces (APIs). All
these tools provide most of the necessary functionality out of the box (already
implemented), and quite naturally the question arises as to what is more useful
for students - to teach how to use these ready-made tools or the basic
principles of working and development of web applications from scratch. This
paper analyzes the results of a survey conducted among IT employers, aimed to
identify what, in their opinion, are the necessary technical skills that
graduating students in the field of Web Programming should possess in order to
join the company's work as quickly and effectively as possible.

</details>


### [56] [LeanExplore: A search engine for Lean 4 declarations](https://arxiv.org/abs/2506.11085)
*Justin Asher*

Main category: cs.SE

TL;DR: LeanExplore是一个为Lean 4设计的搜索引擎，支持语义搜索声明，结合多种排名策略，并可通过网站或API访问。


<details>
  <summary>Details</summary>
Motivation: Lean 4生态系统的扩展使得导航其庞大库变得困难，需要一种高效的搜索工具。

Method: 采用混合排名策略，结合多源语义嵌入模型、BM25+关键词匹配和PageRank评分。

Result: 开发了LeanExplore搜索引擎，支持语义搜索，并可通过网站、API或自托管方式使用。

Conclusion: LeanExplore有望提升Lean 4工作流程和AI驱动的数学研究效率。

Abstract: The expanding Lean 4 ecosystem poses challenges for navigating its vast
libraries. This paper introduces LeanExplore, a search engine for Lean 4
declarations. LeanExplore enables users to semantically search for statements,
both formally and informally, across select Lean 4 packages (including
Batteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is
powered by a hybrid ranking strategy, integrating scores from a multi-source
semantic embedding model (capturing conceptual meaning from formal Lean code,
docstrings, AI-generated informal translations, and declaration titles), BM25+
for keyword-based lexical relevance, and a PageRank-based score reflecting
declaration importance and interconnectedness. The search engine is accessible
via a dedicated website (https://www.leanexplore.com/) and a Python API
(https://github.com/justincasher/lean-explore). Furthermore, the database can
be downloaded, allowing users to self-host the service. LeanExplore integrates
easily with LLMs via the model context protocol (MCP), enabling users to chat
with an AI assistant about Lean declarations or utilize the search engine for
building theorem-proving agents. This work details LeanExplore's architecture,
data processing, functionalities, and its potential to enhance Lean 4 workflows
and AI-driven mathematical research

</details>


### [57] [Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor](https://arxiv.org/abs/2506.11107)
*Weibo Gao,Qi Liu,Rui Li,Yuze Zhao,Hao Wang,Linan Yre,Fangzhou Yao,Zheng Zhang*

Main category: cs.SE

TL;DR: Coda是一个基于代码图的调谐适配器，旨在通过识别和减少噪声信号来增强现有编程知识跟踪（PKT）模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前PKT研究主要关注代码内容与知识评估的隐式关系，但忽略了长期编程活动中的噪声信号（无关提交和微小修改），这限制了模型性能。

Method: Coda将松散代码序列转换为紧凑代码图，利用语义相似性识别无关信号，并通过聚类感知GCN增强弱信号的区分和聚类。最后，通过优化适配器校正噪声影响。

Result: 在四个真实数据集上的实验表明，Coda在存在噪声编程记录的情况下显著优于基线模型。

Conclusion: Coda是一种模型无关的框架，能有效提升PKT任务性能，尤其在处理噪声信号方面表现优异。

Abstract: Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners'
mastery levels of programming knowledge based on their coding activities,
facilitating more effective and personalized programming education. However,
current PKT studies primarily focus on the implicit relationship between code
content and knowledge assessment, often overlooking two types of noise signals
in long-term programming activities: unwanted signals from unrelated
submissions and weak signals from minor modifications. This practical challenge
significantly limits model performance and application. To address this issue,
we propose Coda, a Code graph-based tuning adaptor designed to enhance existing
PKT models by identifying and mitigating the impact of noise. Specifically,
Coda first transforms the loose code sequences submitted by each learner into a
compact code graph. By leveraging this code graph, unwanted signals can be
identified from a semantic similarity perspective. We then apply a
cluster-aware GCN to the code graph, which improves the discrimination of weak
signals and enables their clustering for identification. Finally, a lightweight
yet effective adaptor is incorporated into the PKT task through optimization
with two noise feature-based constraints and a navigational regularization
term, to correct knowledge states affected by noise. It is worth mentioning
that the Coda framework is model-agnostic and can be adapted to most existing
PKT solutions. Extensive experimental results on four real-world datasets
demonstrate that Coda effectively performs the PKT task in the presence of
noisy programming records, outperforming typical baselines.

</details>


### [58] [From over-reliance to smart integration: using Large-Language Models as translators between specialized modeling and simulation tools](https://arxiv.org/abs/2506.11141)
*Philippe J. Giabbanelli,John Beverley,Istvan David,Andreas Tolk*

Main category: cs.SE

TL;DR: 论文探讨了如何将大语言模型（LLMs）作为中间件或翻译器集成到建模与仿真（M&S）任务中，以简化工作流程，同时避免因过度依赖LLMs而带来的质量问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在简化M&S工作流程方面具有潜力，但过度依赖可能导致歧义、逻辑捷径和幻觉等问题，因此需要一种平衡的方法。

Method: 提出将LLMs作为中间件或翻译器，集成到多形式、多语义和多范式的系统中，并采用低秩适应架构实现高效的任务特定适配。

Result: 通过LLM介导的工作流程和结构化工具集成，能够在不影响性能的前提下提升M&S任务的互操作性和可靠性。

Conclusion: LLMs应作为专业工具的补充而非替代，以确保高质量的M&S流程。

Abstract: Large Language Models (LLMs) offer transformative potential for Modeling &
Simulation (M&S) through natural language interfaces that simplify workflows.
However, over-reliance risks compromising quality due to ambiguities, logical
shortcuts, and hallucinations. This paper advocates integrating LLMs as
middleware or translators between specialized tools to mitigate complexity in
M&S tasks. Acting as translators, LLMs can enhance interoperability across
multi-formalism, multi-semantics, and multi-paradigm systems. We address two
key challenges: identifying appropriate languages and tools for modeling and
simulation tasks, and developing efficient software architectures that
integrate LLMs without performance bottlenecks. To this end, the paper explores
LLM-mediated workflows, emphasizes structured tool integration, and recommends
Low-Rank Adaptation-based architectures for efficient task-specific
adaptations. This approach ensures LLMs complement rather than replace
specialized tools, fostering high-quality, reliable M&S processes.

</details>


### [59] [Mutual-Supervised Learning for Sequential-to-Parallel Code Translation](https://arxiv.org/abs/2506.11153)
*Changxin Ke,Rui Zhang,Shuo Wang,Li Ding,Guangli Li,Yuanbo Wen,Shuoming Zhang,Ruiyuan Xu,Jin Qin,Jiaming Guo,Chenxi Wang,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.SE

TL;DR: 论文提出了一种名为Mutual-Supervised Learning (MSL)的新框架，用于解决顺序代码到并行代码翻译中的功能等价问题，通过迭代优化翻译器和测试器，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于并行编程的复杂性，自动化的顺序到并行代码翻译需求增加，但数据稀缺和功能等价问题阻碍了机器学习方法的有效性。

Method: MSL框架包含翻译器和测试器，通过迭代的Co-verify和Co-evolve步骤相互生成数据并共同改进。

Result: 实验表明，MSL显著提升了基础模型的性能，Pass@1提升28.91%，测试器性能提升68.90%，并在BLEU和CodeBLEU分数上超越现有方法。

Conclusion: MSL框架有效解决了功能等价问题，并在性能上优于现有方法，为顺序到并行代码翻译提供了新思路。

Abstract: The rise of GPU-based high-performance computing (HPC) has driven the
widespread adoption of parallel programming models such as CUDA. Yet, the
inherent complexity of parallel programming creates a demand for the automated
sequential-to-parallel approaches. However, data scarcity poses a significant
challenge for machine learning-based sequential-to-parallel code translation.
Although recent back-translation methods show promise, they still fail to
ensure functional equivalence in the translated code. In this paper, we propose
a novel Mutual-Supervised Learning (MSL) framework for sequential-to-parallel
code translation to address the functional equivalence issue. MSL consists of
two models, a Translator and a Tester. Through an iterative loop consisting of
Co-verify and Co-evolve steps, the Translator and the Tester mutually generate
data for each other and improve collectively. The Tester generates unit tests
to verify and filter functionally equivalent translated code, thereby evolving
the Translator, while the Translator generates translated code as augmented
input to evolve the Tester. Experimental results demonstrate that MuSL
significantly enhances the performance of the base model: when applied to
Qwen2.5-Coder, it not only improves Pass@1 by up to 28.91% and boosts Tester
performance by 68.90%, but also outperforms the previous state-of-the-art
method CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while
achieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is
available at https://github.com/kcxain/musl.

</details>


### [60] [Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing](https://arxiv.org/abs/2506.11180)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Felix Gehlhoff*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型上下文协议（MCP）的方法，通过标准化接口使LLM能够直接访问系统功能，避免了传统语义建模的高成本。


<details>
  <summary>Details</summary>
Motivation: 传统的能力和技能建模方法（如本体或资产管理壳）需要大量手动工作，且难以被LLM直接利用。

Method: 使用MCP标准化接口，将资源功能暴露给LLM，并在实验室规模的制造系统中进行原型评估。

Result: 结果表明，该方法无需显式语义模型即可实现灵活的工业自动化。

Conclusion: 本研究为LLM驱动的生产系统中外部工具集成提供了基础。

Abstract: Explicit modeling of capabilities and skills -- whether based on ontologies,
Asset Administration Shells, or other technologies -- requires considerable
manual effort and often results in representations that are not easily
accessible to Large Language Models (LLMs). In this work-in-progress paper, we
present an alternative approach based on the recently introduced Model Context
Protocol (MCP). MCP allows systems to expose functionality through a
standardized interface that is directly consumable by LLM-based agents. We
conduct a prototypical evaluation on a laboratory-scale manufacturing system,
where resource functions are made available via MCP. A general-purpose LLM is
then tasked with planning and executing a multi-step process, including
constraint handling and the invocation of resource functions via MCP. The
results indicate that such an approach can enable flexible industrial
automation without relying on explicit semantic models. This work lays the
basis for further exploration of external tool integration in LLM-driven
production systems.

</details>


### [61] [LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation](https://arxiv.org/abs/2506.11237)
*Ngoc Phuoc An Vo,Brent Paulovicks,Vadim Sheinin*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的自动评估方法（LLM-as-a-Judge），用于验证和改进IT自动化中Bash代码生成的正确性，并通过执行评估验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 自动评估和选择最佳模型以提高IT自动化中事件修复的代码质量，需验证生成代码的语法、语义及执行正确性。

Method: 采用双向功能匹配和逻辑表示增强LLM-as-a-Judge方法，以无参考方式验证和优化Bash代码生成，并以执行评估为基准验证其效果。

Result: LLM-as-a-Judge方法在执行评估中表现出高准确性和一致性（比基线高8%），并通过反馈代理显著提升了代码优化效果（准确率提高24%）。

Conclusion: LLM-as-a-Judge方法在自动代码验证和优化中具有高效性和实用性，为IT自动化提供了可靠的解决方案。

Abstract: In an effort to automatically evaluate and select the best model and improve
code quality for automatic incident remediation in IT Automation, it is crucial
to verify if the generated code for remediation action is syntactically and
semantically correct and whether it can be executed correctly as intended.
There are three approaches: 1) conventional methods use surface form similarity
metrics (token match, exact match, etc.) which have numerous limitations, 2)
execution-based evaluation focuses more on code functionality based on
pass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs
for automated evaluation to judge if it is a correct answer for a given problem
based on pre-defined metrics. In this work, we focused on enhancing
LLM-as-a-Judge using bidirectional functionality matching and logic
representation for reference-less automatic validation and refinement for Bash
code generation to select the best model for automatic incident remediation in
IT Automation. We used execution-based evaluation as ground-truth to evaluate
our LLM-as-a-Judge metrics. Results show high accuracy and agreement with
execution-based evaluation (and up to 8% over baseline). Finally, we built
Reflection code agents to utilize judgments and feedback from our evaluation
metrics which achieved significant improvement (up to 24% increase in accuracy)
for automatic code refinement.

</details>


### [62] [Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation](https://arxiv.org/abs/2506.11266)
*Benjamin Elder,Anupama Murthi,Jungkoo Kang,Ankita Rajaram Naik,Kiran Kate,Kinjal Basu,Danish Contractor*

Main category: cs.SE

TL;DR: 论文提出了一种将NL2SQL数据集转化为NL2API数据集的方法，并评估了10种大型语言模型在API调用任务上的表现，发现其完成率较低，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 企业部署中，大型语言模型需要与复杂API集合交互，但现有数据集和模型在此任务上表现不佳，因此研究如何利用NL2SQL数据集生成NL2API数据集。

Method: 提出了一种数据生成流水线，利用SQL查询语法构建功能等效的API调用序列，并将其应用于BIRD-SQL数据集，生成了2500多个API。

Result: 评估发现所有模型在工具调用任务上表现较差，任务完成率仅为7-47%，即使使用ReACT代理也仅提升至50%。

Conclusion: 当前工具调用型大型语言模型的表现远未达到通用需求，亟需改进。

Abstract: Large language models (LLMs) are routinely deployed as agentic systems, with
access to tools that interact with live environments to accomplish tasks. In
enterprise deployments these systems need to interact with API collections that
can be extremely large and complex, often backed by databases. In order to
create datasets with such characteristics, we explore how existing NL2SQL
(Natural Language to SQL query) datasets can be used to automatically create
NL2API datasets. Specifically, this work describes a novel data generation
pipeline that exploits the syntax of SQL queries to construct a functionally
equivalent sequence of API calls. We apply this pipeline to one of the largest
NL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be
served as invocable tools or REST-endpoints. We pair natural language queries
from BIRD-SQL to ground-truth API sequences based on this API pool. We use this
collection to study the performance of 10 public LLMs and find that all models
struggle to determine the right set of tools (consisting of tasks of intent
detection, sequencing with nested function calls, and slot-filling). We find
that models have extremely low task completion rates (7-47 percent - depending
on the dataset) which marginally improves to 50 percent when models are
employed as ReACT agents that interact with the live API environment. The best
task completion rates are far below what may be required for effective
general-use tool-calling agents, suggesting substantial scope for improvement
in current state-of-the-art tool-calling LLMs. We also conduct detailed
ablation studies, such as assessing the impact of the number of tools available
as well as the impact of tool and slot-name obfuscation. We compare the
performance of models on the original SQL generation tasks and find that
current models are sometimes able to exploit SQL better than APIs.

</details>


### [63] [A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.11295)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 研究提出了一种基于指标的架构模型，用于管理ML系统的复杂性，并通过两个案例研究验证模型。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂性对ML系统的影响，为系统架构决策提供指导。

Method: 引入基于指标的架构模型，并结合SPIRA和Ocean Guard两个ML系统作为案例研究。

Result: 提出了一个可支持ML系统架构决策的复杂性管理模型。

Conclusion: 该模型为ML系统的初始设计和扩展提供了实用指南。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper brings, side-by-side, the
architecture representation of two systems that can be used as case studies for
creating the metrics-based architectural model: the SPIRA and the Ocean Guard
MLES.

</details>


### [64] [A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline](https://arxiv.org/abs/2506.11400)
*Yupeng Jiang,Yao Deng,Sebastian Schroder,Linfeng Liang,Suhaas Gambhir,Alice James,Avishkar Seth,James Pirrie,Yihao Zhang,Xi Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种分步指南，用于建立稳健的自主无人机测试流程，涵盖从仿真到实际测试的各个关键阶段，并通过实例展示了如何验证系统行为、识别问题并优化性能。


<details>
  <summary>Details</summary>
Motivation: 随着自主无人机从研究原型转向关键任务平台，确保其安全性、可靠性和效率至关重要。

Method: 提出了一个测试流程，包括软件在环（SIL）仿真测试、硬件在环（HIL）测试、受控实际测试和现场测试，并通过实际案例（如基于标记的自主着陆系统）验证。

Result: 展示了如何系统性地验证无人机行为、识别集成问题并优化性能，同时指出了未来测试趋势（如神经符号与LLM的集成、数字孪生技术等）。

Conclusion: 通过遵循这一测试流程，开发者和研究者可以实现全面验证，降低部署风险，确保无人机在现实世界中的安全可靠运行。

Abstract: Autonomous drones are rapidly reshaping industries ranging from aerial
delivery and infrastructure inspection to environmental monitoring and disaster
response. Ensuring the safety, reliability, and efficiency of these systems is
paramount as they transition from research prototypes to mission-critical
platforms. This paper presents a step-by-step guide to establishing a robust
autonomous drone testing pipeline, covering each critical stage:
Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)
Testing, Controlled Real-World Testing, and In-Field Testing. Using practical
examples, including the marker-based autonomous landing system, we demonstrate
how to systematically verify drone system behaviors, identify integration
issues, and optimize performance. Furthermore, we highlight emerging trends
shaping the future of drone testing, including the integration of Neurosymbolic
and LLMs, creating co-simulation environments, and Digital Twin-enabled
simulation-based testing techniques. By following this pipeline, developers and
researchers can achieve comprehensive validation, minimize deployment risks,
and prepare autonomous drones for safe and reliable real-world operations.

</details>


### [65] [ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification](https://arxiv.org/abs/2506.11442)
*Yiyang Jin,Kunzhao Xu,Hang Li,Xueting Han,Yanmin Zhou,Cheng Li,Jing Bai*

Main category: cs.SE

TL;DR: ReVeal是一个多轮强化学习框架，通过结合代码生成与显式自我验证及工具评估，提升大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏真实环境的验证信号和显式优化，导致自我验证不可靠。

Method: ReVeal框架通过生成测试用例、调用外部工具获取反馈，并利用密集的每轮奖励优化模型性能。

Result: ReVeal显著提升了Pass@k指标，并在推理过程中持续优化代码生成，性能超越DeepSeek-R1-Zero-Qwen-32B。

Conclusion: ReVeal为构建更鲁棒和自主的AI代理提供了可扩展且有效的范式。

Abstract: Recent advances in reinforcement learning (RL) with verifiable outcome
rewards have significantly improved the reasoning capabilities of large
language models (LLMs), especially when combined with multi-turn tool
interactions. However, existing methods lack both meaningful verification
signals from realistic environments and explicit optimization for verification,
leading to unreliable self-verification. To address these limitations, we
propose ReVeal, a multi-turn reinforcement learning framework that interleaves
code generation with explicit self-verification and tool-based evaluation.
ReVeal enables LLMs to autonomously generate test cases, invoke external tools
for precise feedback, and improves performance via a customized RL algorithm
with dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a
model's generation and verification capabilities through RL training, expanding
the reasoning boundaries of the base model, demonstrated by significant gains
in Pass@k on LiveCodeBench. It also enables test-time scaling into deeper
inference regimes, with code consistently evolving as the number of turns
increases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.
These findings highlight the promise of ReVeal as a scalable and effective
paradigm for building more robust and autonomous AI agents.

</details>


### [66] [Understanding the Issue Types in Open Source Blockchain-based Software Projects with the Transformer-based BERTopic](https://arxiv.org/abs/2506.11451)
*Md Nahidul Islam Opu,Md Shahidul Islam,Sara Rouhani,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本文通过对1,209个开源区块链项目的497,742个问题进行分析，使用BERTopic识别出49个主题，并归类为11个子类。研究发现区块链开发问题与传统软件开发问题比例相近，钱包管理和UI增强是最突出的主题。钱包问题解决时间最长，而机制问题解决最快。问题数量在2016年后激增，2022年后下降。


<details>
  <summary>Details</summary>
Motivation: 区块链系统应用广泛，但对其开发挑战的系统性理解有限，因此通过大规模实证研究填补这一空白。

Method: 使用BERTopic对GitHub上1,209个开源区块链项目的497,742个问题进行主题建模，识别49个主题并分类为11个子类。

Result: 钱包管理和UI增强是最突出的主题；钱包问题解决时间最长，机制问题解决最快；问题数量在2016年后激增，2022年后下降。

Conclusion: 研究结果有助于理解区块链软件维护，为开发专用工具和实践提供依据，以提高稳健性和可维护性。

Abstract: Blockchain-based software systems are increasingly deployed across diverse
domains, yet a systematic understanding of their development challenges remains
limited. This paper presents a large-scale empirical study of 497,742 issues
mined from 1,209 open-source blockchain projects hosted on GitHub. Employing
BERTopic, a transformer-based topic modeling technique, we identify 49 distinct
issue topics and organize them hierarchically into 11 major subcategories. Our
analysis reveals that both general software development issues and
blockchain-specific concerns are nearly equally represented, with Wallet
Management and UI Enhancement emerging as the most prominent topics. We further
examine the temporal evolution of issue categories and resolution times,
finding that Wallet issues not only dominate in frequency but also exhibit the
longest resolution time. Conversely, Mechanisms issues are resolved
significantly faster. Issue frequency surged after 2016 with the rise of
Ethereum and decentralized applications, but declined after 2022. These
findings enhance our understanding of blockchain software maintenance,
informing the development of specialized tools and practices to improve
robustness and maintainability.

</details>


### [67] [VulStamp: Vulnerability Assessment using Large Language Model](https://arxiv.org/abs/2506.11484)
*Haoshen,Ming Hu,Xiaofei Xie,Jiaye Li,Mingsong Chen*

Main category: cs.SE

TL;DR: VulStamp是一个基于意图引导的漏洞评估框架，通过静态分析和大型语言模型提取漏洞代码的意图信息，结合强化学习优化评估模型，解决了现有方法依赖人工描述的问题。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞评估方法依赖人工描述，但描述质量参差不齐且主观性强，限制了性能。VulStamp旨在通过意图信息实现无需描述的漏洞评估。

Method: 结合静态分析和大型语言模型提取漏洞代码意图信息，采用基于强化学习的提示调优方法训练评估模型。

Result: VulStamp能够高效评估漏洞严重性，解决了数据不平衡问题，提升了评估准确性。

Conclusion: VulStamp为漏洞评估提供了一种无需依赖人工描述的新方法，显著优化了开发效率。

Abstract: Although modern vulnerability detection tools enable developers to
efficiently identify numerous security flaws, indiscriminate remediation
efforts often lead to superfluous development expenses. This is particularly
true given that a substantial portion of detected vulnerabilities either
possess low exploitability or would incur negligible impact in practical
operational environments. Consequently, vulnerability severity assessment has
emerged as a critical component in optimizing software development efficiency.
Existing vulnerability assessment methods typically rely on manually crafted
descriptions associated with source code artifacts. However, due to variability
in description quality and subjectivity in intention interpretation, the
performance of these methods is seriously limited. To address this issue, this
paper introduces VulStamp, a novel intention-guided framework, to facilitate
description-free vulnerability assessment. Specifically, VulStamp adopts static
analysis together with Large Language Model (LLM) to extract the intention
information of vulnerable code. Based on the intention information, VulStamp
uses a prompt-tuned model for vulnerability assessment. Furthermore, to
mitigate the problem of imbalanced data associated with vulnerability types,
VulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to
train the assessment model.

</details>


### [68] [A Procedural Framework for Assessing the Desirability of Process Deviations](https://arxiv.org/abs/2506.11525)
*Michael Grohs,Nadine Cordes,Jana-Rebecca Rehse*

Main category: cs.SE

TL;DR: 本文提出了一种程序化框架，用于系统评估流程执行中的偏差合意性，帮助分析师高效分类偏差并推荐行动。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性检查技术无法判断偏差的合意性，而手动评估耗时、主观且不可复制，因此需要一种系统化的方法。

Method: 基于文献综述和实证访谈，开发了一个分步框架，指导分析师考虑输入因素并分类偏差。

Result: 通过实践者评估任务验证，框架能有效简化合意性评估，实现全面且简洁的分析。

Conclusion: 该框架为流程分析师提供了一种系统化工具，显著提升了偏差合意性评估的效率和可重复性。

Abstract: Conformance checking techniques help process analysts to identify where and
how process executions deviate from a process model. However, they cannot
determine the desirability of these deviations, i.e., whether they are
problematic, acceptable or even beneficial for the process. Such desirability
assessments are crucial to derive actions, but process analysts typically
conduct them in a manual, ad-hoc way, which can be time-consuming, subjective,
and irreplicable. To address this problem, this paper presents a procedural
framework to guide process analysts in systematically assessing deviation
desirability. It provides a step-by-step approach for identifying which input
factors to consider in what order to categorize deviations into mutually
exclusive desirability categories, each linked to action recommendations. The
framework is based on a review and conceptualization of existing literature on
deviation desirability, which is complemented by empirical insights from
interviews with process analysis practitioners and researchers. We evaluate the
framework through a desirability assessment task conducted with practitioners,
indicating that the framework effectively enables them to streamline the
assessment for a thorough yet concise evaluation.

</details>


### [69] [Augmenting the Generality and Performance of Large Language Models for Software Engineering](https://arxiv.org/abs/2506.11548)
*Fabian C. Peña*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）在软件工程（SE）中非代码任务的应用，旨在提升其通用性和性能，包括任务表现评估、基础知识来源验证及幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成和分析方面表现突出，但其在SE其他非代码任务（如概念化、设计等）中的应用尚未充分探索。

Method: 研究通过（1）评估不同特性LLMs在非代码任务的表现，（2）验证其作为SE基础知识来源的能力，（3）开发SE陈述的幻觉检测方法。

Result: 初步结果显示在多种非代码任务上性能有所提升。

Conclusion: 预期贡献包括针对特定领域训练的LLMs、SE基础知识新基准及幻觉检测方法，为LLMs在SE中的广泛应用奠定基础。

Abstract: Large Language Models (LLMs) are revolutionizing software engineering (SE),
with special emphasis on code generation and analysis. However, their
applications to broader SE practices including conceptualization, design, and
other non-code tasks, remain partially underexplored. This research aims to
augment the generality and performance of LLMs for SE by (1) advancing the
understanding of how LLMs with different characteristics perform on various
non-code tasks, (2) evaluating them as sources of foundational knowledge in SE,
and (3) effectively detecting hallucinations on SE statements. The expected
contributions include a variety of LLMs trained and evaluated on
domain-specific datasets, new benchmarks on foundational knowledge in SE, and
methods for detecting hallucinations. Initial results in terms of performance
improvements on various non-code tasks are promising.

</details>


### [70] [Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation](https://arxiv.org/abs/2506.11559)
*Gábor Antal,Dénes Bán,Martin Isztin,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 论文探讨了GPT-4在自动生成单元测试方面的能力，特别是针对漏洞修复的测试生成。实验表明，GPT-4能生成语法正确的测试用例，但语义正确性验证有限。


<details>
  <summary>Details</summary>
Motivation: 测试在软件开发中至关重要，但手动生成测试复杂且耗时。研究旨在利用GPT-4自动生成漏洞相关的单元测试，减轻开发负担。

Method: 使用VUL4J数据集中的漏洞及其修复代码，评估GPT-4生成测试的能力，关注代码上下文、自校正能力和测试用例的实用性。

Result: GPT-4能生成66.5%语法正确的测试用例，但仅7.5%的语义正确性可自动验证。生成的测试模板可进一步手动完善。

Conclusion: GPT-4在漏洞测试生成中具有潜力，虽不完全自主，但可显著提升部分自动化测试的效率。

Abstract: In the life-cycle of software development, testing plays a crucial role in
quality assurance. Proper testing not only increases code coverage and prevents
regressions but it can also ensure that any potential vulnerabilities in the
software are identified and effectively fixed. However, creating such tests is
a complex, resource-consuming manual process. To help developers and security
experts, this paper explores the automatic unit test generation capability of
one of the most widely used large language models, GPT-4, from the perspective
of vulnerabilities. We examine a subset of the VUL4J dataset containing real
vulnerabilities and their corresponding fixes to determine whether GPT-4 can
generate syntactically and/or semantically correct unit tests based on the code
before and after the fixes as evidence of vulnerability mitigation. We focus on
the impact of code contexts, the effectiveness of GPT-4's self-correction
ability, and the subjective usability of the generated test cases. Our results
indicate that GPT-4 can generate syntactically correct test cases 66.5\% of the
time without domain-specific pre-training. Although the semantic correctness of
the fixes could be automatically validated in only 7. 5\% of the cases, our
subjective evaluation shows that GPT-4 generally produces test templates that
can be further developed into fully functional vulnerability-witnessing tests
with relatively minimal manual effort.
  Therefore, despite the limited data, our initial findings suggest that GPT-4
can be effectively used in the generation of vulnerability-witnessing tests. It
may not operate entirely autonomously, but it certainly plays a significant
role in a partially automated process.

</details>


### [71] [Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study](https://arxiv.org/abs/2506.11561)
*Gábor Antal,Bence Bogenfürst,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: GPT-4o在修复Java漏洞时表现略逊于GPT-4，但通过优化提示策略（如结合CVE信息和代码上下文）可显著提升修复率。


<details>
  <summary>Details</summary>
Motivation: 探索GPT-4o在自动化漏洞修复（AVR）中的表现，以及不同上下文信息对其能力的影响。

Method: 比较GPT-4o和GPT-4在相同提示下的表现，并测试9种额外提示（含CWE/CVE信息和代码上下文）对42个漏洞的修复效果。

Result: GPT-4o平均表现比GPT-4差11.9%，但能修复更多独特漏洞（+10.5%）。结合CVE信息和代码上下文的提示效果最佳。

Conclusion: 优化提示策略（如Top-3提示组合）可显著提升GPT-4o的漏洞修复能力，适用于零样本场景。

Abstract: Recent advancements in large language models (LLMs) have shown promise for
automated vulnerability detection and repair in software systems. This paper
investigates the performance of GPT-4o in repairing Java vulnerabilities from a
widely used dataset (Vul4J), exploring how different contextual information
affects automated vulnerability repair (AVR) capabilities. We compare the
latest GPT-4o's performance against previous results with GPT-4 using identical
prompts. We evaluated nine additional prompts crafted by us that contain
various contextual information such as CWE or CVE information, and manually
extracted code contexts. Each prompt was executed three times on 42
vulnerabilities, and the resulting fix candidates were validated using Vul4J's
automated testing framework.
  Our results show that GPT-4o performed 11.9\% worse on average than GPT-4
with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities
in the three runs together. CVE information significantly improved repair
rates, while the length of the task description had minimal impact. Combining
CVE guidance with manually extracted code context resulted in the best
performance. Using our \textsc{Top}-3 prompts together, GPT-4o repaired 26
(62\%) vulnerabilities at least once, outperforming both the original baseline
(40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies
could improve vulnerability repair in zero-shot settings.

</details>


### [72] [MBSR at Work: Perspectives from an Instructor and Software Developers](https://arxiv.org/abs/2506.11588)
*Simone Romano,Alberto Conforti,Gloria Guidetti,Sara Viotti,Rachele Ceschin,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 研究通过半结构化访谈探讨了正念减压训练（MBSR）在软件开发（SD）工作环境中的应用效果，发现开发者虽初始持怀疑态度，但最终认可其个人改善效果，尽管将MBSR技术融入工作环境仍具挑战。


<details>
  <summary>Details</summary>
Motivation: MBSR虽已在多种高压工作环境中应用，但尚未在软件开发领域研究，而开发者面临独特压力（如时间压力和任务不确定性），因此需探索其在该领域的适用性。

Method: 采用定性研究方法（半结构化访谈），收集参与MBSR项目的开发者及指导者的第一手经验。

Result: 开发者虽初始怀疑，但承认MBSR带来个人改善；然而，将MBSR技术融入工作环境仍存在挑战。

Conclusion: MBSR在软件开发环境中具有潜力，但需进一步解决实践中的挑战以实现更广泛应用。

Abstract: In this paper, we present the preliminary findings from a qualitative study
(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction
(MBSR) program, carried out in the Software Development (SD) working context,
is perceived by the software developers of a multinational company who
participated in the MBSR program and by the instructor who led it. MBSR is a
deeply personal and experiential practice in helping individuals manage stress,
particularly in high-pressure environments such as workplaces, healthcare
settings, education, and other demanding professional or personal situations.
Although MBSR has been experimented in different working contexts;
surprisingly, it has never been studied in the SD working context where there
are several stress factors that developers experience (e.g., time pressure and
uncertainty about the content of a particular task and its outcome). In this
respect, qualitative research can generate valuable insights into the
application of MBSR in the SD working context that cannot be captured by
standardized quantitative measures. Being MBSR instructors and software
developers the key stakeholders in delivering an MBSR program in the SD working
context, understanding their first-hand experiences can provide a more detailed
picture of the investigated phenomenon. The most important takeaway result of
our research can be summarized as follows: despite initial skepticism, the
developers recognized personal improvements due to the MBSR practice, though
the integration of MBSR techniques in the working context remained challenging.

</details>


### [73] [Retrieval-Augmented Code Review Comment Generation](https://arxiv.org/abs/2506.11591)
*Hyunsun Hong,Jongmoon Baik*

Main category: cs.SE

TL;DR: 论文提出了一种结合检索增强生成（RAG）的方法，用于自动生成代码审查评论，解决了生成式和检索式方法的不足，并在性能上优于两者。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查评论生成方法（生成式和检索式）各有局限性，生成式方法难以生成低频但重要的语义标记，而检索式方法缺乏对新代码上下文的适应性。

Method: 采用检索增强生成（RAG）方法，通过检索相关代码审查示例，指导预训练语言模型生成更准确的评论。

Result: 在Tufano基准测试中，RAG方法在精确匹配和BLEU分数上分别比生成式方法提高了1.67%和4.25%，低频标记生成提升了24.01%。

Conclusion: RAG方法有效结合了生成式和检索式的优势，显著提升了代码审查评论生成的性能。

Abstract: Automated code review comment generation (RCG) aims to assist developers by
automatically producing natural language feedback for code changes. Existing
approaches are primarily either generation-based, using pretrained language
models, or information retrieval-based (IR), reusing comments from similar past
examples. While generation-based methods leverage code-specific pretraining on
large code-natural language corpora to learn semantic relationships between
code and natural language, they often struggle to generate low-frequency but
semantically important tokens due to their probabilistic nature. In contrast,
IR-based methods excel at recovering such rare tokens by copying from existing
examples but lack flexibility in adapting to new code contexts-for example,
when input code contains identifiers or structures not found in the retrieval
database. To bridge the gap between generation-based and IR-based methods, this
work proposes to leverage retrieval-augmented generation (RAG) for RCG by
conditioning pretrained language models on retrieved code-review exemplars. By
providing relevant examples that illustrate how similar code has been
previously reviewed, the model is better guided to generate accurate review
comments. Our evaluation on the Tufano et al. benchmark shows that RAG-based
RCG outperforms both generation-based and IR-based RCG. It achieves up to
+1.67% higher exact match and +4.25% higher BLEU scores compared to
generation-based RCG. It also improves the generation of low-frequency
ground-truth tokens by up to 24.01%. We additionally find that performance
improves as the number of retrieved exemplars increases.

</details>


### [74] [Further Evidence on a Controversial Topic about Human-Based Experiments: Professionals vs. Students](https://arxiv.org/abs/2506.11597)
*Simone Romano,Francesco Paolo Sferratore,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 比较学生和软件专业人员在修复Java程序错误任务中的表现，发现学生表现优于专业人员，引发对实验中使用学生参与者的外部有效性的讨论。


<details>
  <summary>Details</summary>
Motivation: 探讨学生参与者在软件工程实验中的外部有效性及其结果对软件行业的适用性。

Method: 比较62名学生和42名软件专业人员在相同Java程序上的错误修复任务表现，实验环境对专业人员更贴近现实。

Result: 学生在修复错误任务中表现优于专业人员，与以往实证结果部分不符。

Conclusion: 结果旨在促进对学生参与实验的讨论，并推动未来研究以更贴近现实的实验设计探讨影响软件工程任务的复杂因素。

Abstract: Most Software Engineering (SE) human-based controlled experiments rely on
students as participants, raising concerns about their external validity.
Specifically, the realism of results obtained from students and their
applicability to the software industry remains in question. In this short
paper, we bring further evidence on this controversial point. To do so, we
compare 62 students and 42 software professionals on a bug-fixing task on the
same Java program. The students were enrolled in a Bachelor's program in
Computer Science, while the professionals were employed by two multinational
companies (for one of them, the professionals were from two offices). Some
variations in the experimental settings of the two groups (students and
professionals) were present. For instance, the experimental environment of the
experiment with professionals was more realistic; i.e., they faced some stress
factors such as interruptions during the bug-fixing task. Considering the
differences between the two groups of participants, the gathered data show that
the students outperformed the professionals in fixing bugs. This diverges to
some extent from past empirical evidence. Rather than presenting definitive
conclusions, our results aim to catalyze the discussion on the use of students
in experiments and pave the way for future investigations. Specifically, our
results encourage us to examine the complex factors influencing SE tasks,
making experiments as more realistic as possible.

</details>


### [75] [Understanding API Usage and Testing: An Empirical Study of C Libraries](https://arxiv.org/abs/2506.11598)
*Ahmed Zaki,Cristian Cadar*

Main category: cs.SE

TL;DR: 本文通过分析21个流行的开源C库及其3,061个C/C++客户端的API使用情况，发现开发者未根据客户使用情况优先测试API，且客户测试套件可提升库测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 了解API的实际使用情况有助于库开发者优先处理bug报告、功能请求和测试活动，但目前缺乏对C/C++生态系统中API使用与测试的大规模比较研究。

Method: 使用LibProbe框架分析21个开源C库及其客户端的API使用情况，并与库测试套件的覆盖情况进行比较。

Result: 研究发现库开发者未根据客户使用情况优先测试API，例如LMDB中45%的API被客户使用但未被测试套件覆盖；客户测试套件可提升库测试覆盖率（如LMDB提升14.7%）。

Conclusion: 客户测试套件可作为库测试的有益补充，LibProbe框架为库开发者提供了实用的分析工具。

Abstract: For library developers, understanding how their Application Programming
Interfaces (APIs) are used in the field can be invaluable. Knowing how clients
are using their APIs allows for data-driven decisions on prioritising bug
reports, feature requests, and testing activities. For example, the priority of
a bug report concerning an API can be partly determined by how widely that API
is used.
  In this paper, we present an empirical study in which we analyse API usage
across 21 popular open-source C libraries, such as OpenSSL and SQLite, with a
combined total of 3,061 C/C++ clients. We compare API usage by clients with how
well library test suites exercise the APIs to offer actionable insights for
library developers. To our knowledge, this is the first study that compares API
usage and API testing at scale for the C/C++ ecosystem. Our study shows that
library developers do not prioritise their effort based on how clients use
their API, with popular APIs often poorly tested. For example, in LMDB, a
popular key-value store, 45% of the APIs are used by clients but not tested by
the library test suite. We further show that client test suites can be
leveraged to improve library testing e.g., improving coverage in LMDB by 14.7%
with the important advantage that those tests are representative of how the
APIs are used in the field.
  For our empirical study, we have developed LibProbe, a framework that can be
used to analyse a large corpus of clients for a given library and produce
various metrics useful to library developers.

</details>


### [76] [Accelerating Delta Debugging through Probabilistic Monotonicity Assessment](https://arxiv.org/abs/2506.11614)
*Yonggang Tao,Jingling Xue*

Main category: cs.SE

TL;DR: 本文提出了一种概率单调性评估（PMA）方法，通过动态建模和量化搜索空间的单调性，显著提高了DDMIN类算法的效率，同时保持了其有效性。


<details>
  <summary>Details</summary>
Motivation: Delta调试假设搜索空间单调性，但实践中这一假设并不总是成立。PMA旨在提升调试效率，减少冗余测试。

Method: PMA动态建模搜索空间的单调性，并使用置信函数量化单调性，从而概率性地排除非失败诱导程序的子集。

Result: 与CHISEL和ProbDD相比，PMA分别减少了59.2%和22.0%的处理时间，并显著提升了缩减速度和最终程序的精简程度。

Conclusion: PMA显著提高了Delta调试的效率，同时保持或增强了其有效性。

Abstract: Delta debugging assumes search space monotonicity: if a program causes a
failure, any supersets of that program will also induce the same failure,
permitting the exclusion of subsets of non-failure-inducing programs. However,
this assumption does not always hold in practice. This paper introduces
Probabilistic Monotonicity Assessment (PMA), enhancing the efficiency of
DDMIN-style algorithms without sacrificing effectiveness. PMA dynamically
models and assesses the search space's monotonicity based on prior tests tried
during the debugging process and uses a confidence function to quantify
monotonicity, thereby enabling the probabilistic exclusion of subsets of
non-failure-inducing programs. Our approach significantly reduces redundant
tests that would otherwise be performed, without compromising the quality of
the reduction.
  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.
Our findings indicate that PMA cuts processing time by 59.2% compared to
CHISEL, accelerates the reduction process (i.e., the number of tokens deleted
per second) by 3.32x, and decreases the sizes of the final reduced programs by
6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x
speedup in the reduction process, and further decreases the sizes of the final
reduced programs by 3.0%. These findings affirm PMA's role in significantly
improving delta debugging's efficiency while maintaining or enhancing its
effectiveness.

</details>


### [77] [An Empirical study on LLM-based Log Retrieval for Software Engineering Metadata Management](https://arxiv.org/abs/2506.11659)
*Simin Sun,Yuchuan Jin,Miroslaw Staron*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型（LLM）的方法，结合信号日志数据和测试驾驶视频，支持自然语言场景搜索，减少了对专业知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）开发中，高频日志数据难以查询，传统SQL方法需要专业知识且结果难以验证。

Method: 结合信号日志与视频数据，利用场景距离图和相对间隙指标，提供可量化的查询结果可靠性评估。

Result: 在开放工业数据集上的评估显示，该方法提高了场景检索的效率和可靠性。

Conclusion: 该方法减少了对单一数据源和传统SQL的依赖，支持更直观的场景搜索和可视化。

Abstract: Developing autonomous driving systems (ADSs) involves generating and storing
extensive log data from test drives, which is essential for verification,
research, and simulation. However, these high-frequency logs, recorded over
varying durations, pose challenges for developers attempting to locate specific
driving scenarios. This difficulty arises due to the wide range of signals
representing various vehicle components and driving conditions, as well as
unfamiliarity of some developers' with the detailed meaning of these signals.
Traditional SQL-based querying exacerbates this challenge by demanding both
domain expertise and database knowledge, often yielding results that are
difficult to verify for accuracy.
  This paper introduces a Large Language Model (LLM)-supported approach that
combines signal log data with video recordings from test drives, enabling
natural language based scenario searches while reducing the need for
specialized knowledge. By leveraging scenario distance graphs and relative gap
indicators, it provides quantifiable metrics to evaluate the reliability of
query results. The method is implemented as an API for efficient database
querying and retrieval of relevant records, paired with video frames for
intuitive visualization. Evaluation on an open industrial dataset demonstrates
improved efficiency and reliability in scenario retrieval, eliminating
dependency on a single data source and conventional SQL.

</details>


### [78] [SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments](https://arxiv.org/abs/2506.11697)
*Yiwei Hu,Zhen Li,Kedie Shu,Shenghua Guan,Deqing Zou,Shouhuai Xu,Bin Yuan,Hai Jin*

Main category: cs.SE

TL;DR: 本文系统化整理了自动化漏洞修复（AVR）方法，包括漏洞分析、补丁生成和补丁验证三个步骤，并构建了首个C/C++漏洞修复基准数据集Vul4C。


<details>
  <summary>Details</summary>
Motivation: 软件复杂性增加导致漏洞增多，手动修复效率低，凸显了AVR的重要性。

Method: 通过三个步骤（漏洞分析、补丁生成、补丁验证）系统化AVR方法，并构建Vul4C数据集评估工具。

Result: 评估了7个C/C++ AVR工具和2个Java AVR工具，提出了未来研究方向。

Conclusion: AVR研究需进一步标准化和扩展，Vul4C为C/C++工具评估提供了基准。

Abstract: The increasing complexity of software has led to the steady growth of
vulnerabilities. Vulnerability repair investigates how to fix software
vulnerabilities. Manual vulnerability repair is labor-intensive and
time-consuming because it relies on human experts, highlighting the importance
of Automated Vulnerability Repair (AVR). In this SoK, we present the
systematization of AVR methods through the three steps of AVR workflow:
vulnerability analysis, patch generation, and patch validation. We assess AVR
tools for C/C++ and Java programs as they have been widely studied by the
community. Since existing AVR tools for C/C++ programs are evaluated with
different datasets, which often consist of a few vulnerabilities, we construct
the first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which
contains 144 vulnerabilities as well as their exploits and patches. We use
Vul4C to evaluate seven AVR tools for C/C++ programs and use the third-party
Vul4J dataset to evaluate two AVR tools for Java programs. We also discuss
future research directions.

</details>


### [79] [Classification of Quality Characteristics in Online User Feedback using Linguistic Analysis, Crowdsourcing and LLMs](https://arxiv.org/abs/2506.11722)
*Eduard C. Groen,Fabiano Dalpiaz,Martijn van Vliet,Boris Winter,Joerg Doerr,Sjaak Brinkkemper*

Main category: cs.SE

TL;DR: 论文探讨了在低数据环境下自动识别移动应用用户反馈中的质量特征的可行性，比较了基于语言模式、众包和大型语言模型的三种方法。


<details>
  <summary>Details</summary>
Motivation: 移动应用的用户反馈是质量改进的重要来源，但数据的异质性和缺乏训练集限制了监督学习的应用。

Method: 研究了三种方法：基于质量关键词的语言模式（LPs）、众包微任务指令和大型语言模型（LLM）提示，并比较了它们的准确性。

Result: LPs方法的精度因质量特征而异（0.38-0.92），召回率低；众包在两阶段中表现最佳（0.63, 0.72），LLM的最佳条件（0.66）和多数投票（0.68）与之相当。

Conclusion: 在低数据环境下，众包和LLM能实现准确分类，而LPs潜力有限；众包和LLM还可用于构建训练集。

Abstract: Software qualities such as usability or reliability are among the strongest
determinants of mobile app user satisfaction and constitute a significant
portion of online user feedback on software products, making it a valuable
source of quality-related feedback to guide the development process. The
abundance of online user feedback warrants the automated identification of
quality characteristics, but the online user feedback's heterogeneity and the
lack of appropriate training corpora limit the applicability of supervised
machine learning. We therefore investigate the viability of three approaches
that could be effective in low-data settings: language patterns (LPs) based on
quality-related keywords, instructions for crowdsourced micro-tasks, and large
language model (LLM) prompts. We determined the feasibility of each approach
and then compared their accuracy. For the complex multiclass classification of
quality characteristics, the LP-based approach achieved a varied precision
(0.38-0.92) depending on the quality characteristic, and low recall;
crowdsourcing achieved the best average accuracy in two consecutive phases
(0.63, 0.72), which could be matched by the best-performing LLM condition
(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings
show that in this low-data setting, the two approaches that use crowdsourcing
or LLMs instead of involving experts achieve accurate classifications, while
the LP-based approach has only limited potential. The promise of crowdsourcing
and LLMs in this context might even extend to building training corpora.

</details>


### [80] [A Short Survey on Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.11874)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文综述了使用大语言模型（LLM）辅助编写软件形式化规范的研究，总结了35篇关键论文，并提供了Dafny、C和Java的示例。


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言需求中编写形式化规范的挑战，源自VERIFAI项目。

Method: 通过多个学术数据库筛选相关研究，使用AI工具Elicit初步选文并人工筛选。

Result: 提供了利用LLM形式化软件需求的宝贵见解和未来方向。

Conclusion: LLM在形式化规范领域具有潜力，未来研究可进一步探索其应用。

Abstract: This paper presents a focused literature survey on the use of large language
models (LLM) to assist in writing formal specifications for software. A summary
of thirty-five key papers is presented, including examples for specifying
programs written in Dafny, C and Java. This paper arose from the project
VERIFAI - Traceability and verification of natural language requirements that
addresses the challenges in writing formal specifications from requirements
that are expressed in natural language. Our methodology employed multiple
academic databases to identify relevant research. The AI-assisted tool Elicit
facilitated the initial paper selection, which were manually screened for final
selection. The survey provides valuable insights and future directions for
utilising LLMs while formalising software requirements.

</details>


### [81] [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?](https://arxiv.org/abs/2506.11928)
*Zihan Zheng,Zerui Cheng,Zeyu Shen,Shang Zhou,Kaiyuan Liu,Hansen He,Dongruixuan Li,Stanley Wei,Hangyi Hao,Jianzhu Yao,Peiyao Sheng,Zixuan Wang,Wenhao Chai,Aleksandra Korolova,Peter Henderson,Sanjeev Arora,Pramod Viswanath,Jingbo Shang,Saining Xie*

Main category: cs.SE

TL;DR: 论文通过LiveCodeBench Pro基准测试，发现前沿大语言模型在编程竞赛中仍显著落后于人类专家，尤其在算法推理和复杂案例分析方面。


<details>
  <summary>Details</summary>
Motivation: 重新评估大语言模型（LLMs）在编程竞赛中的表现，揭示其与人类专家的差距及局限性。

Method: 引入LiveCodeBench Pro基准，由国际竞赛奖牌得主标注问题并分析模型失败案例。

Result: 前沿模型在中等难度问题中仅53%通过率，高难度问题为0%，且依赖工具增强而非推理能力。

Conclusion: LLMs在代码生成中仍存在显著不足，需进一步改进算法推理能力。

Abstract: Recent reports claim that large language models (LLMs) now outperform elite
humans in competitive programming. Drawing on knowledge from a group of
medalists in international algorithmic contests, we revisit this claim,
examining how LLMs differ from human experts and where limitations still
remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from
Codeforces, ICPC, and IOI that are continuously updated to reduce the
likelihood of data contamination. A team of Olympiad medalists annotates every
problem for algorithmic categories and conducts a line-by-line analysis of
failed model-generated submissions. Using this new data and benchmark, we find
that frontier models still have significant limitations: without external
tools, the best model achieves only 53% pass@1 on medium-difficulty problems
and 0% on hard problems, domains where expert humans still excel. We also find
that LLMs succeed at implementation-heavy problems but struggle with nuanced
algorithmic reasoning and complex case analysis, often generating confidently
incorrect justifications. High performance appears largely driven by
implementation precision and tool augmentation, not superior reasoning.
LiveCodeBench Pro thus highlights the significant gap to human grandmaster
levels, while offering fine-grained diagnostics to steer future improvements in
code-centric LLM reasoning.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [82] [Efficient Traffic Classification using HW-NAS: Advanced Analysis and Optimization for Cybersecurity on Resource-Constrained Devices](https://arxiv.org/abs/2506.11319)
*Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino*

Main category: cs.NI

TL;DR: 本文提出了一种硬件高效的深度神经网络（DNN），通过硬件感知的神经架构搜索（HW-NAS）优化，支持在资源受限的物联网（IoT）和边缘设备上对会话级加密流量进行分类。优化后的模型在ISCX VPN-nonVPN数据集上实现了96.59%的准确率，同时显著减少了内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 资源受限的IoT和边缘设备需要高效的加密流量分类方法，以增强网络安全。

Method: 采用硬件感知的神经架构搜索（HW-NAS）优化1D卷积神经网络（CNN），结合特定的预处理策略。

Result: 模型在多个分类任务中表现优异，准确率高达99.64%，同时显著减少了参数、FLOPs和内存占用。

Conclusion: 该方法为IoT网络提供了可扩展且高效的加密流量实时分析解决方案，但需注意预处理和会话长度选择对性能的影响。

Abstract: This paper presents a hardware-efficient deep neural network (DNN), optimized
through hardware-aware neural architecture search (HW-NAS); the DNN supports
the classification of session-level encrypted traffic on resource-constrained
Internet of Things (IoT) and edge devices. Thanks to HW-NAS, a 1D convolutional
neural network (CNN) is tailored on the ISCX VPN-nonVPN dataset to meet strict
memory and computational limits while achieving robust performance. The
optimized model attains an accuracy of 96.59% with just 88.26K parameters,
10.08M FLOPs, and a maximum tensor size of 20.12K. Compared to state-of-the-art
models, it achieves reductions of up to 444-fold, 312-fold, and 15.6-fold in
these metrics, respectively, significantly minimizing memory footprint and
runtime requirements. The model also demonstrates versatility in classification
tasks, achieving accuracies of up to 99.64% in VPN differentiation, VPN-type
classification, broader traffic categories, and application identification. In
addition, an in-depth approach to header-level preprocessing strategies
confirms that the optimized model can provide notable performances across a
wide range of configurations, even in scenarios with stricter privacy
considerations. Likewise, a reduction in the length of sessions of up to 75%
yields significant improvements in efficiency, while maintaining high accuracy
with only a negligible drop of 1-2%. However, the importance of careful
preprocessing and session length selection in the classification of raw traffic
data is still present, as improper settings or aggressive reductions can bring
about a 7% reduction in overall accuracy. Those results highlight the method's
effectiveness in enforcing cybersecurity for IoT networks, by providing
scalable, efficient solutions for the real-time analysis of encrypted traffic
within strict hardware limitations.

</details>


### [83] [Scheduling Agile Earth Observation Satellites with Onboard Processing and Real-Time Monitoring](https://arxiv.org/abs/2506.11556)
*Antonio M. Mercado-Martínez,Beatriz Soret,Antonio Jurado-Navas*

Main category: cs.NI

TL;DR: 本文提出了一种解决敏捷地球观测卫星调度问题（AEOSSP）的方法，结合了实时数据处理和多卫星优化，通过启发式算法和局部搜索策略提高了观测分辨率和信息时效性。


<details>
  <summary>Details</summary>
Motivation: 敏捷地球观测卫星（AEOSs）的出现提升了数据采集的灵活性，同时星载计算和通信技术的进步提高了数据压缩效率，支持近实时信息传输。本文旨在优化目标观测序列以最大化观测效益。

Method: 定义了一组优先级指标，开发了一种启发式方法，并结合局部搜索策略，将星载实时数据处理融入多卫星优化问题。

Result: 算法将采集帧的分辨率平均提高了10%，目标监测频率的方差降低了83%，相比FIFO方法提供了更高质量和时效性的信息。

Conclusion: 所提方法显著提升了敏捷地球观测卫星调度的效率和信息质量，为近实时监测提供了有效解决方案。

Abstract: The emergence of Agile Earth Observation Satellites (AEOSs) has marked a
significant turning point in the field of Earth Observation (EO), offering
enhanced flexibility in data acquisition. Concurrently, advancements in onboard
satellite computing and communication technologies have greatly enhanced data
compression efficiency, reducing network latency and congestion while
supporting near real-time information delivery. In this paper, we address the
Agile Earth Observation Satellite Scheduling Problem (AEOSSP), which involves
determining the optimal sequence of target observations to maximize overall
observation profit. Our approach integrates onboard data processing for
real-time remote monitoring into the multi-satellite optimization problem. To
this end, we define a set of priority indicators and develop a constructive
heuristic method, further enhanced with a Local Search (LS) strategy. The
results show that the proposed algorithm provides high-quality information by
increasing the resolution of the collected frames by up to 10% on average,
while reducing the variance in the monitoring frequency of the targets within
the instance by up to 83%, ensuring more up-to-date information across the
entire set compared to a First-In First-Out (FIFO) method.

</details>


### [84] [Generalised Rate Control Approach For Stream Processing Applications](https://arxiv.org/abs/2506.11710)
*Ziren Xiao*

Main category: cs.NI

TL;DR: 论文提出了一种基于图神经网络的深度强化学习方法，用于分布式流处理系统中的数据发射率控制，以避免系统过载。


<details>
  <summary>Details</summary>
Motivation: 分布式流处理系统在处理实时数据时面临过载问题，导致系统不稳定和资源浪费。

Method: 使用图神经网络处理系统指标，通过深度强化学习动态调整数据发射率，避免传统多层感知器网络的局限性。

Result: 实验表明，该方法在三个应用中提升了13.5%的吞吐量和30%的端到端延迟。

Conclusion: 图神经网络结合深度强化学习能有效优化流处理系统的性能。

Abstract: Distributed stream processing systems are widely deployed to process
real-time data generated by various devices, such as sensors and software
systems. A key challenge in the system is overloading, which leads to an
unstable system status and consumes additional system resources. In this paper,
we use a graph neural network-based deep reinforcement learning to
collaboratively control the data emission rate at which the data is generated
in the stream source to proactively avoid overloading scenarios. Instead of
using a traditional multi-layer perceptron-styled network to control the rate,
the graph neural network is used to process system metrics collected from the
stream processing engine. Consequently, the learning agent (i) avoids storing
past states where previous actions may affect the current state, (ii) is
without waiting a long interval until the current action has been fully
effective and reflected in the system's specific metrics, and more importantly,
(iii) is able to adapt multiple stream applications in multiple scenarios. We
deploy the rate control approach on three applications, and the experimental
results demonstrate that the throughput and end-to-end latency are improved by
up to 13.5% and 30%, respectively.

</details>


### [85] [Adaptive determinantal scheduling with fairness in wireless networks](https://arxiv.org/abs/2506.11738)
*H. P. Keeler,B. Błaszczyszyn*

Main category: cs.NI

TL;DR: 提出了一种基于行列式点过程的无线网络调度新框架，结合公平性，通过凸优化问题实现资源分配。


<details>
  <summary>Details</summary>
Motivation: 传统Aloha协议独立调度传输，缺乏公平性，行列式点过程的排斥特性为网络调度提供了新思路。

Method: 将调度问题转化为基于$L$-ensemble行列式点过程的凸优化问题，结合SINR模型验证。

Result: 行列式调度结合公平性在无线网络中表现出潜力，计算高效且数学优雅。

Conclusion: 该工作将机器学习与无线通信结合，为网络调度提供了新的数学和计算工具。

Abstract: We propose a novel framework for wireless network scheduling with fairness
using determinantal (point) processes. Our approach incorporates the repulsive
nature of determinantal processes, generalizing traditional Aloha protocols
that schedule transmissions independently. We formulate the scheduling problem
with an utility function representing fairness. We then recast this formulation
as a convex optimization problem over a certain class of determinantal point
processes called $L$-ensembles, which are particularly suited for statistical
and numerical treatments. These determinantal processes, which have already
proven valuable in subset learning, offer an attractive approach to network
resource scheduling and allocating. We demonstrate the suitability of
determinantal processes for network models based on the
signal-to-interference-plus-noise ratio (SINR). Our results highlight the
potential of determinantal scheduling coupled with fairness. This work bridges
recent advances in machine learning with wireless communications, providing a
mathematically elegant and computationally tractable approach to network
scheduling.

</details>


### [86] [Enabling Next-Generation Cloud-Connected Bionic Limbs Through 5G Connectivity](https://arxiv.org/abs/2506.11744)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.NI

TL;DR: 论文提出了一种基于5G和边缘/云计算的分层分布式计算架构，旨在解决当前辅助性仿生肢体的计算能力不足、高延迟和操作不直观等问题。


<details>
  <summary>Details</summary>
Motivation: 当前仿生肢体存在计算能力有限、延迟高和控制不直观等问题，导致用户体验差和弃用率高。需要利用物联网技术（如5G和边缘/云计算）实现智能互联解决方案。

Method: 采用分层分布式计算架构，结合本地、边缘和云计算层。时间敏感任务由本地处理单元处理，计算密集型任务则卸载到边缘和云端服务器。

Result: 在5G测试环境中验证了该系统能够满足数据速率和延迟要求，支持自然假肢控制，并实现计算任务的卸载。

Conclusion: 这是实现云端连接仿生肢体系统的第一步，为实际应用和验证奠定了基础。

Abstract: Despite the recent advancements in human-machine interfacing, contemporary
assistive bionic limbs face critical challenges, including limited
computational capabilities, high latency, and unintuitive control mechanisms,
leading to suboptimal user experience and abandonment rates. Addressing these
challenges requires a shift toward intelligent, interconnected solutions
powered by advances in Internet of Things systems, particularly wireless
connectivity and edge/cloud computing. This article presents a conceptual
approach to transform bionic limbs by harnessing the pervasive connectivity of
5G and the significant computational power of cloud and edge servers, equipping
them with capabilities not available hitherto. The system leverages a
hierarchical distributed-computing architecture that integrates local, edge,
and cloud computing layers. Time-critical tasks are handled by a local
processing unit, while compute-intensive tasks are offloaded to edge and cloud
servers, leveraging the high data rate, reliable and low latency capabilities
of advanced cellular networks. We perform a proof-of-concept validation in a 5G
testbed showing that such networks are capable of achieving data rates and
fulfilling latency requirements for a natural prosthetic control, allowing for
offloading of compute-intensive jobs to the edge/cloud servers. This is the
first step towards the realization and real-world validation of cloud-connected
bionic limb systems.

</details>


### [87] [The Throughput Gain of Hypercycle-level Resource Reservation for Time-Triggered Ethernet](https://arxiv.org/abs/2506.11745)
*Peng Wang,Suman Sourav,Binbin Chen,Hongyan Li,Feng Wang,Fan Zhang*

Main category: cs.NI

TL;DR: 提出了一种名为HFS的灵活调度方案，显著提高了时间触发通信中可接纳的流数量，并通过实验验证其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有固定循环调度（FCS）限制了不同周期长度的流在共享链路上的兼容性，导致可接纳流数量受限。

Method: 提出超周期级灵活调度（HFS）方案，允许流的资源预留在一个超周期内跨周期变化，并设计了HFS-LLF启发式算法。

Result: HFS比FCS多接纳6倍的流，且HFS-LLF比通用求解器快104倍。

Conclusion: HFS方案在兼容现有系统的同时，显著提升了调度灵活性和效率。

Abstract: Time-Triggered Communication is a key technology for many safety-critical
systems, with applications spanning the areas of aerospace and industrial
control. Such communication relies on time-triggered flows, with each flow
consisting of periodic packets originating from a source and destined for a
destination node. Each packet needs to reach its destination before its
deadline. Different flows can have different cycle lengths. To achieve assured
transmission of time-triggered flows, existing efforts constrain the packets of
a flow to be cyclically transmitted along the same path. Under such Fixed
Cyclic Scheduling (FCS), reservation for flows with different cycle lengths can
become incompatible over a shared link, limiting the total number of admissible
flows. Considering the cycle lengths of different flows, a hyper-cycle has
length equal to their least common multiple (LCM). It determines the time
duration over which the scheduling compatibility of the different flows can be
checked. In this work, we propose a more flexible schedule scheme called the
Hypercycle-level Flexible Schedule (HFS) scheme, where a flow's resource
reservation can change across cycles within a hypercycle. HFS can significantly
increase the number of admitted flows by providing more scheduling options
while remaining perfectly compatible with existing Time-Triggered Ethernet
system. We show that, theoretically the possible capacity gain provided by HFS
over FCS can be unbounded. We formulate the joint pathfinding and scheduling
problem under HFS as an ILP problem which we prove to be NP-Hard. To solve HFS
efficiently, we further propose a least-load-first heuristic (HFS-LLF), solving
HFS as a sequence of shortest path problems. Extensive study shows that HFS
admits up to 6 times the number of flows achieved by FCS. Moreover, our
proposed HFS-LLF can run 104 times faster than solving HFS using a generic
solver.

</details>


### [88] [Distributed Learning for Reliable and Timely Communication in 6G Industrial Subnetworks](https://arxiv.org/abs/2506.11749)
*Samira Abdelrahman,Hossam Farag,Gilberto Berardinelli*

Main category: cs.NI

TL;DR: 论文提出了一种基于学习的分布式随机接入协议，用于6G工业网络中事件驱动的关键控制流量传输，通过隐式子网协调减少碰撞概率并提高及时交付率。


<details>
  <summary>Details</summary>
Motivation: 6G工业网络中，由于有限的无线电资源、动态设备活动和高移动性，支持事件驱动的关键控制流量的及时传输具有挑战性。

Method: 提出了一种分布式、基于学习的随机接入协议，利用中央接入点广播的竞争签名信号，各子网独立学习和选择接入配置，实现动态条件下的自适应、碰撞感知接入。

Result: 仿真结果表明，该方法在密集和高负载场景下显著提高了及时包交付概率，例如在60个子网和5个无线电信道的工业设置中，比经典MAB方法提高了21%。

Conclusion: 该方法通过轻量级神经模型和在线训练，适用于受限的工业子网部署，显著提升了6G工业网络中关键控制流量的传输效率。

Abstract: Emerging 6G industrial networks envision autonomous in-X subnetworks to
support efficient and cost-effective short range, localized connectivity for
autonomous control operations. Supporting timely transmission of event-driven,
critical control traffic is challenging in such networks is challenging due to
limited radio resources, dynamic device activity, and high mobility. In this
paper, we propose a distributed, learning-based random access protocol that
establishes implicit inter-subnetwork coordination to minimize the collision
probability and improves timely delivery. Each subnetwork independently learns
and selects access configurations based on a contention signature signal
broadcast by a central access point, enabling adaptive, collision-aware access
under dynamic traffic and mobility conditions. The proposed approach features
lightweight neural models and online training, making it suitable for
deployment in constrained industrial subnetworks. Simulation results show that
our method significantly improves the probability of timely packet delivery
compared to baseline methods, particularly in dense and high-load scenarios.
For instance, our proposed method achieves 21% gain in the probability of
timely packet delivery compared to a classical Multi-Armed Bandit (MAB) for an
industrial setting of 60 subnetworks and 5 radio channels.

</details>


### [89] [A Tale of Two Mobile Generations: 5G-Advanced and 6G in 3GPP Release 20](https://arxiv.org/abs/2506.11828)
*Xingqin Lin*

Main category: cs.NI

TL;DR: 3GPP Release 20是5G与6G过渡的关键点，平衡了5G-Advanced的增强与6G的铺垫。


<details>
  <summary>Details</summary>
Motivation: 为未来移动通信标准奠定基础，同时提升5G-Advanced能力。

Method: 分析Release 20的关键增强及其动机。

Result: 为未来移动通信标准提供了重要基础。

Conclusion: Release 20在5G与6G过渡中扮演了关键角色。

Abstract: As the telecommunications industry stands at the crossroads between the fifth
generation (5G) and sixth generation (6G) of mobile communications, the 3rd
generation partnership project (3GPP) Release 20 emerges as a pivotal point of
transition. By striking a balance between enhancing 5G-Advanced capabilities
and setting the stage for 6G, Release 20 provides the crucial foundation upon
which future mobile communication standards and deployments will be built. This
article examines these dual objectives, outlining the key enhancements, the
motivations behind them, and their implications for the future of mobile
communications.

</details>


### [90] [Intractable Cookie Crumbs: Unveiling the Nexus of Stateful Banner Interaction and Tracking Cookies](https://arxiv.org/abs/2506.11947)
*Ali Rasaii,Ha Dao,Anja Feldmann,Mohammadmadi Javid,Oliver Gasser,Devashish Gosain*

Main category: cs.NI

TL;DR: 研究发现，尽管GDPR要求用户同意数据收集，但通过技术漏洞和共享第三方服务，非自愿的跨站点跟踪仍可能发生。研究揭示了一种利用持久性Cookie的隐蔽跟踪机制，并测量了其广泛存在。


<details>
  <summary>Details</summary>
Motivation: 探讨在GDPR和ePrivacy指令下，用户同意机制的实际效果，揭示隐蔽的跨站点跟踪行为及其技术机制。

Method: 对Tranco top列表中的20k域名进行状态化爬取，策略性地接受部分域名的同意横幅，测量另一部分域名的持久性Cookie。

Result: 约50%的网站发送至少一个持久性Cookie，启用GPC信号可减少30%的Cookie，CMP横幅网站发送的Cookie数量是原生横幅的6.9倍。

Conclusion: 隐蔽跟踪行为普遍存在，现有隐私控制工具部分有效，但需进一步改进用户同意机制以增强隐私保护。

Abstract: In response to the ePrivacy Directive and the consent requirements introduced
by the GDPR, websites began deploying consent banners to obtain user permission
for data collection and processing. However, due to shared third-party services
and technical loopholes, non-consensual cross-site tracking can still occur. In
fact, contrary to user expectations of seemingly isolated consent, a user's
decision on one website may affect tracking behavior on others. In this study,
we investigate the technical and behavioral mechanisms behind these
discrepancies. Specifically, we disclose a persistent tracking mechanism
exploiting web cookies. These cookies, which we refer to as intractable, are
initially set on websites with accepted banners, persist in the browser, and
are subsequently sent to trackers before the user provides explicit consent on
other websites. To meticulously analyze this covert tracking behavior, we
conduct an extensive measurement study performing stateful crawls on over 20k
domains from the Tranco top list, strategically accepting banners in the first
half of domains and measuring intractable cookies in the second half. Our
findings reveal that around 50% of websites send at least one intractable
cookie, with the majority set to expire after more than 10 days. In addition,
enabling the Global Privacy Control (GPC) signal initially reduces the number
of intractable cookies by 30% on average, with a further 32% reduction possible
on subsequent visits by rejecting the banners. Moreover, websites with Consent
Management Platform (CMP) banners, on average, send 6.9 times more intractable
cookies compared to those with native banners. Our research further reveals
that even if users reject all other banners, they still receive a large number
of intractable cookies set by websites with cookie paywalls.

</details>


### [91] [Minimum-hop Constellation Design for Low Earth Orbit Satellite Networks](https://arxiv.org/abs/2506.11995)
*Chirag Rao,Eytan Modiano*

Main category: cs.NI

TL;DR: 论文研究了LEO卫星网络中通过优化ISL拓扑以最小化平均最短路径长度（ASPL），分析了对称和一般规则拓扑的最优性能，并提出了接近理论下限的实际构造方法。


<details>
  <summary>Details</summary>
Motivation: 优化LEO卫星网络的ISL拓扑结构，以提升通信效率，特别是最小化ASPL，从而提高网络性能。

Method: 分析了两种拓扑结构：顶点对称拓扑和一般规则拓扑，建立了ASPL的理论下限，并提出了具体的构造方法。

Result: 证明了网格结构在ASPL和直径上均非最优，同时展示了在保持轨道内ISL连接的情况下仍能实现接近最优的ASPL性能。

Conclusion: 研究结果表明，通过合理的拓扑构造，可以接近理论下限的ASPL，尤其在高密度网络中表现更优。

Abstract: We consider a Low Earth Orbit (LEO) satellite network with each satellite
capable of establishing inter-satellite link (ISL) connections for
satellite-to-satellite communication. Since ISLs can be reoriented to change
the topology, we optimize the topology to minimize the average shortest path
length (ASPL). We characterize the optimal ASPL ISL topology in two families of
topologies, 1) vertex-symmetric in which the ISL connections at a satellite
node represent a motif that is repeated at all other satellite nodes, and 2)
general regular topologies in which no such repeating pattern need exist. We
establish ASPL lower bounds for both scenarios and show constructions for which
they are achievable assuming each satellite makes 3 or 4 ISL connections. For
the symmetric case, we show that the mesh grid is suboptimal in both ASPL and
diameter. Additionally, we show there are constructions that maintain
intra-orbital ISL connections while still achieving near-optimal ASPL
performance. For the general case we show it is possible to construct networks
with ASPL close to the general lower bound when the network is sufficiently
dense. Simulation results show that for both scenarios, one can find topologies
that are very close to the lower bounds as the network size scales.

</details>


### [92] [Upgrade or Switch: Do We Need a New Registry Architecture for the Internet of AI Agents?](https://arxiv.org/abs/2506.12003)
*Ramesh Raskar,Pradyumna Chari,Jared James Grogan,Mahesh Lambe,Robert Lincourt,Raghu Bala,Abhishek Singh,Ayush Chopra,Rajesh Ranjan,Shailja Gupta,Dimitris Stripelis,Maria Gorskikh,Sichao Wang*

Main category: cs.NI

TL;DR: 本文探讨了现有网络基础设施是否应升级或重建以适应AI代理的需求，分析了DNS、PKI和IP地址的局限性，并评估了三种解决方案：升级、重建和混合注册表。


<details>
  <summary>Details</summary>
Motivation: AI代理的自主性和规模需求对现有基础设施提出了新的挑战，如快速发现、即时凭证撤销和行为证明，这些需求超出了当前DNS/PKI的能力范围。

Method: 通过分析DNS传播延迟、证书撤销扩展性和IP地址不足等问题，评估了升级现有基础设施、重建专用注册表和混合注册表三种方案。

Result: 研究发现，AI代理的需求是质变而非量变，升级方案兼容性强但性能有限，重建方案性能更好但部署周期长，混合方案可能是最佳选择。

Conclusion: 混合注册表方案可能成为主流，结合集中式注册表和联邦式网状结构，以满足不同场景的需求。

Abstract: The emerging Internet of AI Agents challenges existing web infrastructure
designed for human-scale, reactive interactions. Unlike traditional web
resources, autonomous AI agents initiate actions, maintain persistent state,
spawn sub-agents, and negotiate directly with peers: demanding
millisecond-level discovery, instant credential revocation, and cryptographic
behavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes
whether to upgrade existing infrastructure or implement purpose-built registry
architectures for autonomous agents. We identify critical failure points: DNS
propagation (24-48 hours vs. required milliseconds), certificate revocation
unable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate
for agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)
Switch options, (3) Hybrid registries. Drawing parallels to dialup-to-broadband
transitions, we find that agent requirements constitute qualitative, and not
incremental, changes. While upgrades offer compatibility and faster deployment,
clean-slate solutions provide better performance but require longer for
adoption. Our analysis suggests hybrid approaches will emerge, with centralized
registries for critical agents and federated meshes for specialized use cases.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [Developing a Dyslexia Indicator Using Eye Tracking](https://arxiv.org/abs/2506.11004)
*Kevin Cogan,Vuong M. Ngo,Mark Roantree*

Main category: cs.LG

TL;DR: 该论文提出了一种结合眼动追踪技术和机器学习算法的方法，用于早期低成本检测阅读障碍，准确率达88.58%。


<details>
  <summary>Details</summary>
Motivation: 全球约10%-20%的人患有阅读障碍，亟需创新且易获取的诊断方法。

Method: 通过分析眼动模式（如注视时长和异常扫视），使用随机森林分类器和层次聚类方法检测阅读障碍及其严重程度。

Result: 随机森林分类器准确率为88.58%，并能识别阅读障碍的严重程度。

Conclusion: 眼动追踪与机器学习结合为非侵入性、高精度的阅读障碍诊断提供了新途径。

Abstract: Dyslexia, affecting an estimated 10% to 20% of the global population,
significantly impairs learning capabilities, highlighting the need for
innovative and accessible diagnostic methods. This paper investigates the
effectiveness of eye-tracking technology combined with machine learning
algorithms as a cost-effective alternative for early dyslexia detection. By
analyzing general eye movement patterns, including prolonged fixation durations
and erratic saccades, we proposed an enhanced solution for determining
eye-tracking-based dyslexia features. A Random Forest Classifier was then
employed to detect dyslexia, achieving an accuracy of 88.58\%. Additionally,
hierarchical clustering methods were applied to identify varying severity
levels of dyslexia. The analysis incorporates diverse methodologies across
various populations and settings, demonstrating the potential of this
technology to identify individuals with dyslexia, including those with
borderline traits, through non-invasive means. Integrating eye-tracking with
machine learning represents a significant advancement in the diagnostic
process, offering a highly accurate and accessible method in clinical research.

</details>


### [94] [Data Science: a Natural Ecosystem](https://arxiv.org/abs/2506.11010)
*Emilio Porcu,Roy El Moukari,Laurent Najman,Francisco Herrera,Horst Simon*

Main category: cs.LG

TL;DR: 本文提出了一种以数据为中心的‘基础数据科学’概念，将其视为一个自然生态系统，涉及数据宇宙中的5D复杂性（数据结构、领域、基数、因果性和伦理）与数据生命周期的结合。数据代理根据目标执行任务，数据科学家是数据代理的逻辑组织体。文中定义了特定学科驱动的数据科学，并提出了‘泛数据科学’作为整合学科与基础数据科学的生态系统。作者警告计算与基础数据科学可能的分歧，并建议通过严格评估数据宇宙发现的有用性来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 探讨数据科学作为一个生态系统的本质，强调数据宇宙中的复杂性与数据生命周期的结合，以及数据代理和科学家的角色。

Method: 提出‘基础数据科学’概念，将其分为计算和基础两部分，并定义特定学科驱动的数据科学和‘泛数据科学’。

Result: 指出计算与基础数据科学可能的分歧，并提出通过评估数据发现的有用性来缓解分歧。

Conclusion: 建议采用严格方法评估数据宇宙发现的有用性，以防止计算与基础数据科学的分歧。

Abstract: This manuscript provides a holistic (data-centric) view of what we term
essential data science, as a natural ecosystem with challenges and missions
stemming from the data universe with its multiple combinations of the 5D
complexities (data structure, domain, cardinality, causality, and ethics) with
the phases of the data life cycle. Data agents perform tasks driven by specific
goals. The data scientist is an abstract entity that comes from the logical
organization of data agents with their actions. Data scientists face challenges
that are defined according to the missions. We define specific
discipline-induced data science, which in turn allows for the definition of
pan-data science, a natural ecosystem that integrates specific disciplines with
the essential data science. We semantically split the essential data science
into computational, and foundational. We claim that there is a serious threat
of divergence between computational and foundational data science. Especially,
if no approach is taken to rate whether a data universe discovery should be
useful or not. We suggest that rigorous approaches to measure the usefulness of
data universe discoveries might mitigate such a divergence.

</details>


### [95] [Not All Clients Are Equal: Personalized Federated Learning on Heterogeneous Multi-Modal Clients](https://arxiv.org/abs/2506.11024)
*Minhyuk Seo,Taeheon Kim,Hankook Lee,Jonghyun Choi,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 论文提出了一种针对多模态个性化联邦学习（PFL）的方法，解决了数据异构性和模型异构性问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 集中式训练的基础模型存在隐私和传输成本问题，联邦学习（FL）提供了分布式替代方案，但现有PFL研究多局限于模拟环境，忽视了真实场景中的异构性问题。

Method: 针对数据异构性，提出任务相似性感知的模型聚合方法；针对模型异构性，提出维度不变模块以实现异构模型间的知识共享。

Result: 实验结果表明，该方法在个性化和泛化能力上优于现有技术。

Conclusion: 该方法在多模态PFL中有效解决了异构性问题，具有实际应用潜力。

Abstract: Foundation models have shown remarkable capabilities across diverse
multi-modal tasks, but their centralized training raises privacy concerns and
induces high transmission costs. In contrast, federated learning (FL) offers a
distributed alternative without the need to share data. Recently, for the
growing demand for personalizing AI models for different user purposes,
personalized federated learning (PFL) has emerged. PFL allows each client to
leverage the knowledge of other clients for further adaptation to individual
user preferences, again without the need to share data. Despite its potential,
most PFL studies remain confined to simulated environments, overlooking the
data and model heterogeneity that arise in real-world scenarios. In contrast,
we first consider large data heterogeneity, evaluating on a new benchmark for
multi-modal PFL, spanning 40 distinct tasks with realistic data distribution
shifts. We then consider model heterogeneity in that we do not assume that all
clients share similar model architectures. To address data heterogeneity, we
propose a task-similarity-aware model aggregation method that provides
customized global models to each client. For model heterogeneity, we propose a
dimension-invariant module that enables knowledge sharing across heterogeneous
models. Empirical validations demonstrate that the proposed approach
outperforms the state-of-the-art, excelling in both personalization and
generalization capabilities.

</details>


### [96] [When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces](https://arxiv.org/abs/2506.11025)
*Miriam Doh,Aditya Gulati,Matei Mancas,Nuria Oliver*

Main category: cs.LG

TL;DR: 论文研究了合成生成的面孔和基于机器学习的性别分类算法如何受到算法外貌主义（基于外貌的偏好）的影响，发现文本到图像系统将面部吸引力与无关的积极特质（如智力和可信度）关联，且性别分类模型在“不太有吸引力”的面孔上错误率更高，尤其是非白人女性。


<details>
  <summary>Details</summary>
Motivation: 探讨算法外貌主义对合成面孔和性别分类算法的影响，揭示数字身份系统中的公平性问题。

Method: 通过实验分析13,200张合成生成的面孔，研究文本到图像系统和性别分类模型的表现。

Result: 文本到图像系统将吸引力与无关特质关联；性别分类模型在“不太有吸引力”面孔上错误率更高，非白人女性受影响最显著。

Conclusion: 研究揭示了数字身份系统中的公平性隐患，呼吁关注算法外貌主义的影响。

Abstract: This paper examines how synthetically generated faces and machine
learning-based gender classification algorithms are affected by algorithmic
lookism, the preferential treatment based on appearance. In experiments with
13,200 synthetically generated faces, we find that: (1) text-to-image (T2I)
systems tend to associate facial attractiveness to unrelated positive traits
like intelligence and trustworthiness; and (2) gender classification models
exhibit higher error rates on "less-attractive" faces, especially among
non-White women. These result raise fairness concerns regarding digital
identity systems.

</details>


### [97] [Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data](https://arxiv.org/abs/2506.11026)
*Andre Catarino,Rui Melo,Rui Abreu,Luis Cruz*

Main category: cs.LG

TL;DR: 比较了四种合成数据生成方法在动态电价应用中的表现，扩散模型效用最高，CTGAN隐私保护最强。


<details>
  <summary>Details</summary>
Motivation: 动态电价需识别受益家庭，但真实数据有隐私问题，需合成数据替代。

Method: 评估了WGAN、CTGAN、扩散模型和高斯噪声增强四种方法，测试分类效用、分布保真度和隐私泄漏。

Result: 扩散模型效用最高（F1达88.2%），CTGAN抗重建攻击最强。

Conclusion: 结构化生成模型在隐私保护、数据驱动的能源系统中潜力大。

Abstract: The widespread adoption of dynamic Time-of-Use (dToU) electricity tariffs
requires accurately identifying households that would benefit from such pricing
structures. However, the use of real consumption data poses serious privacy
concerns, motivating the adoption of synthetic alternatives. In this study, we
conduct a comparative evaluation of four synthetic data generation methods,
Wasserstein-GP Generative Adversarial Networks (WGAN), Conditional Tabular GAN
(CTGAN), Diffusion Models, and Gaussian noise augmentation, under different
synthetic regimes. We assess classification utility, distribution fidelity, and
privacy leakage. Our results show that architectural design plays a key role:
diffusion models achieve the highest utility (macro-F1 up to 88.2%), while
CTGAN provide the strongest resistance to reconstruction attacks. These
findings highlight the potential of structured generative models for developing
privacy-preserving, data-driven energy systems.

</details>


### [98] [From Reasoning to Code: GRPO Optimization for Underrepresented Languages](https://arxiv.org/abs/2506.11027)
*Federico Pennino,Bianca Raimondi,Massimo Rondelli,Andrea Gurioli,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: 论文提出了一种结合小规模代码模型和GRPO的方法，用于生成低资源语言的准确可执行代码，并以Prolog为例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决因训练数据有限而难以生成准确可执行代码的问题，尤其是对于低资源编程语言。

Method: 使用Qwen 2.5的小规模代码模型结合GRPO，通过显式推理步骤生成代码，并将推理反馈集成到强化学习循环中。

Result: 实验表明，该方法显著提高了推理质量、代码准确性和逻辑正确性，尤其在Prolog等低资源语言中表现突出。

Conclusion: 该方法为缺乏大量训练资源的编程语言提供了一种有效的代码生成解决方案。

Abstract: Generating accurate and executable code using large language models (LLMs) is
challenging for languages with limited public training data compared to popular
languages such as Python. This paper introduces a generalizable approach that
uses small-scale code versions of the Qwen 2.5 model combined with Group
Relative Policy Optimization (GRPO) to enable effective code generation through
explicit reasoning steps, which is particularly beneficial for languages with
smaller source code databases. Using Prolog as a representative use case --
given its limited online presence -- the initial model faced challenges in
generating executable code. After some training steps, the model successfully
produces logically consistent and syntactically accurate code by directly
integrating reasoning-driven feedback into the reinforcement learning loop.
Experimental evaluations using mathematical logic problem benchmarks illustrate
significant improvements in reasoning quality, code accuracy, and logical
correctness, underscoring the potential of this approach to benefit a wide
range of programming languages lacking extensive training resources.

</details>


### [99] [Enhancing Epidemic Forecasting: Evaluating the Role of Mobility Data and Graph Convolutional Networks](https://arxiv.org/abs/2506.11028)
*Suhan Guo,Zhenghao Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 研究通过两阶段方法评估移动数据对传染病预测的影响，发现移动数据和GCN模块对预测性能提升有限，但死亡率和住院数据显著提高准确性。空间地图与封锁令的相关性表明其可作为移动敏感指标。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习算法在流行病学应用中的性能差距，尤其是移动数据整合的困难。

Method: 采用两阶段方法：先通过试点研究评估移动数据重要性，再评估GCN对Transformer架构的影响。

Result: 移动数据和GCN模块对预测性能提升不明显，但死亡率和住院数据显著提高模型准确性。空间地图与封锁令存在相关性。

Conclusion: 研究为传染病预测中的移动数据表示提供了新视角，有助于决策者更好地应对未来疫情。

Abstract: Accurate prediction of contagious disease outbreaks is vital for informed
decision-making. Our study addresses the gap between machine learning
algorithms and their epidemiological applications, noting that methods optimal
for benchmark datasets often underperform with real-world data due to
difficulties in incorporating mobility information. We adopt a two-phase
approach: first, assessing the significance of mobility data through a pilot
study, then evaluating the impact of Graph Convolutional Networks (GCNs) on a
transformer backbone. Our findings reveal that while mobility data and GCN
modules do not significantly enhance forecasting performance, the inclusion of
mortality and hospitalization data markedly improves model accuracy.
Additionally, a comparative analysis between GCN-derived spatial maps and
lockdown orders suggests a notable correlation, highlighting the potential of
spatial maps as sensitive indicators for mobility. Our research offers a novel
perspective on mobility representation in predictive modeling for contagious
diseases, empowering decision-makers to better prepare for future outbreaks.

</details>


### [100] [Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model](https://arxiv.org/abs/2506.11029)
*Xue Wang,Tian Zhou,Jinyang Gao,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: YingLong是一个非因果、双向注意力编码器-仅Transformer模型，通过掩码标记恢复训练，在时间序列预测中表现出色，尤其在长输出任务中准确性显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的时间序列预测方法（直接或递归）存在局限性，需要一种更高效的框架来提升预测性能。

Method: YingLong采用非因果、双向注意力编码器-仅Transformer结构，通过掩码标记恢复训练，并利用多输入集成减少输出方差。

Result: YingLong在ETT和Weather数据集上的零样本任务中表现优异，性能提升超过60%，并在GIFT-Eval基准测试中显著优于其他模型。

Conclusion: YingLong展示了在时间序列预测中的强大潜力，尤其是在长输出任务中，其非因果设计和双向注意力机制是关键优势。

Abstract: We present a joint forecasting framework for time series prediction that
contrasts with traditional direct or recursive methods. This framework achieves
state-of-the-art performance for our designed foundation model, YingLong, and
reveals a novel scaling effect: longer outputs significantly enhance model
accuracy due to delayed chain-of-thought reasoning in our non-causal approach.
YingLong is a non-causal, bidirectional attention encoder-only transformer
trained through masked token recovery, aligning more effectively with language
understanding tasks than with generation tasks. Additionally, we boost
performance by tackling output variance with a multi-input ensemble. We release
four foundation models ranging from 6M to 300M parameters, demonstrating
superior results in zero-shot tasks on the ETT and Weather datasets. YingLong
achieves more than 60% best performance. To ensure generalizability, we
assessed the models using the GIFT-Eval benchmark, which comprises 23 time
series datasets across 7 domains. Yinglong significantly outperformed the best
time-series foundation models, end-to-end trained models by 14% and 44% in rank
respectively.The pretrained 300M model is available at
https://huggingface.co/qcw1314/YingLong_300m

</details>


### [101] [Forward Target Propagation: A Forward-Only Approach to Global Error Credit Assignment via Local Losses](https://arxiv.org/abs/2506.11030)
*Nazmus Saadat As-Saquib,A N M Nafiz Abeer,Hung-Ta Chien,Byung-Jun Yoon,Suhas Kumar,Su-in Yi*

Main category: cs.LG

TL;DR: 提出了一种名为FTP（Forward Target Propagation）的生物合理且计算高效的替代方法，取代了传统的反向传播（BP），解决了BP在生物学和硬件上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播（BP）在生物学和硬件上存在局限性，如对称权重、非局部信用分配和冻结活动等问题，需要一种更合理的替代方法。

Method: FTP通过第二次前向传递估计层间目标，仅使用前馈计算，无需对称反馈权重或可学习逆函数，实现模块化和局部学习。

Result: FTP在MNIST、CIFAR10和CIFAR100等任务上表现出与BP相当的准确性，并在低精度和新兴硬件约束下优于BP。

Conclusion: FTP因其计算开销小、仅需前向传递和硬件兼容性，为高效设备学习和神经形态计算提供了有前景的方向。

Abstract: Training neural networks has traditionally relied on backpropagation (BP), a
gradient-based algorithm that, despite its widespread success, suffers from key
limitations in both biological and hardware perspectives. These include
backward error propagation by symmetric weights, non-local credit assignment,
and frozen activity during backward passes. We propose Forward Target
Propagation (FTP), a biologically plausible and computationally efficient
alternative that replaces the backward pass with a second forward pass. FTP
estimates layerwise targets using only feedforward computations, eliminating
the need for symmetric feedback weights or learnable inverse functions, hence
enabling modular and local learning. We evaluate FTP on fully connected
networks, CNNs, and RNNs, demonstrating accuracies competitive with BP on
MNIST, CIFAR10, and CIFAR100, as well as effective modeling of long-term
dependencies in sequential tasks. Moreover, FTP outperforms BP under quantized
low-precision and emerging hardware constraints while also demonstrating
substantial efficiency gains over other biologically inspired methods such as
target propagation variants and forward-only learning algorithms. With its
minimal computational overhead, forward-only nature, and hardware
compatibility, FTP provides a promising direction for energy-efficient
on-device learning and neuromorphic computing.

</details>


### [102] [Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models](https://arxiv.org/abs/2506.11031)
*Zoher Kachwala,Danishjeet Singh,Danielle Yang,Filippo Menczer*

Main category: cs.LG

TL;DR: 利用预训练的视觉语言模型（VLM）进行零样本检测AI生成图像，通过任务对齐提示（zero-shot-s²）显著提升性能，无需微调。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成器生成的图像越来越逼真，潜在滥用问题日益严重。传统监督检测方法依赖大型标注数据集且泛化能力有限。

Method: 使用预训练的VLM进行零样本检测，提出任务对齐提示（zero-shot-s²），通过特定前缀引导模型更聚焦的推理。

Result: zero-shot-s²在多个数据集和模型上显著提升性能（Macro F1提高8%-29%），并展示出强泛化能力和模型规模鲁棒性。

Conclusion: 任务对齐提示能激发VLM的潜在能力，为AI生成图像检测提供了一种简单、通用且可解释的替代方案。

Abstract: As image generators produce increasingly realistic images, concerns about
potential misuse continue to grow. Supervised detection relies on large,
curated datasets and struggles to generalize across diverse generators. In this
work, we investigate the use of pre-trained Vision-Language Models (VLMs) for
zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit
some task-specific reasoning and chain-of-thought prompting offers gains, we
show that task-aligned prompting elicits more focused reasoning and
significantly improves performance without fine-tuning. Specifically, prefixing
the model's response with the phrase ``Let's examine the style and the
synthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1
scores by 8%-29% for two widely used open-source models. These gains are
consistent across three recent, diverse datasets spanning human faces, objects,
and animals with images generated by 16 different models -- demonstrating
strong generalization. We further evaluate the approach across three additional
model sizes and observe improvements in most dataset-model combinations --
suggesting robustness to model scale. Surprisingly, self-consistency, a
behavior previously observed in language reasoning, where aggregating answers
from diverse reasoning paths improves performance, also holds in this setting.
Even here, zero-shot-s$^2$ scales better than chain-of-thought in most cases --
indicating that it elicits more useful diversity. Our findings show that
task-aligned prompts elicit more focused reasoning and enhance latent
capabilities in VLMs, like the detection of AI-generated images -- offering a
simple, generalizable, and explainable alternative to supervised methods. Our
code is publicly available on github:
https://github.com/osome-iu/Zero-shot-s2.git.

</details>


### [103] [Deep Learning Approach to Bearing and Induction Motor Fault Diagnosis via Data Fusion](https://arxiv.org/abs/2506.11032)
*Mert Sehri,Merve Ertagrin,Ozal Yildirim,Ahmet Orhan,Patrick Dumond*

Main category: cs.LG

TL;DR: CNN和LSTM结合用于轴承和感应电机诊断，强调多传感器数据融合的优势。


<details>
  <summary>Details</summary>
Motivation: 提出一种综合方法，利用深度学习和传感器融合，促进多模型诊断研究，并鼓励收集更多多传感器数据。

Method: 使用CNN评估加速度计和麦克风数据，结合LSTM有效融合传感器信息。

Result: 展示了数据融合在诊断中的优势。

Conclusion: 鼓励研究者关注多模型诊断，并推动多传感器数据收集。

Abstract: Convolutional Neural Networks (CNNs) are used to evaluate accelerometer and
microphone data for bearing and induction motor diagnosis. A Long Short-Term
Memory (LSTM) recurrent neural network is used to combine sensor information
effectively, highlighting the benefits of data fusion. This approach encourages
researchers to focus on multi model diagnosis for constant speed data
collection by proposing a comprehensive way to use deep learning and sensor
fusion and encourages data scientists to collect more multi-sensor data,
including acoustic and accelerometer datasets.

</details>


### [104] [An Active Learning-Based Streaming Pipeline for Reduced Data Training of Structure Finding Models in Neutron Diffractometry](https://arxiv.org/abs/2506.11100)
*Tianle Wang,Jorge Ramirez,Cristina Garcia-Cardona,Thomas Proffen,Shantenu Jha,Sudip K. Seal*

Main category: cs.LG

TL;DR: 论文提出了一种基于主动学习（AL）策略的方法，显著减少了训练机器学习模型所需的数据量，同时提高了准确性，并设计了高效的流式训练流程。


<details>
  <summary>Details</summary>
Motivation: 中子衍射结构测定计算成本高且耗时，机器学习模型虽能加速但训练数据需求随参数增加而指数增长，亟需解决方案。

Method: 引入了一种新的批量模式主动学习策略，利用不确定性采样从概率分布中选择模型最不确定的标记数据进行训练。

Result: 该方法减少了约75%的训练数据需求，同时提高了准确性；流式训练流程缩短了20%的训练时间且不损失精度。

Conclusion: 提出的主动学习策略和流式训练流程有效解决了中子衍射结构测定中的计算挑战，显著提升了效率。

Abstract: Structure determination workloads in neutron diffractometry are
computationally expensive and routinely require several hours to many days to
determine the structure of a material from its neutron diffraction patterns.
The potential for machine learning models trained on simulated neutron
scattering patterns to significantly speed up these tasks have been reported
recently. However, the amount of simulated data needed to train these models
grows exponentially with the number of structural parameters to be predicted
and poses a significant computational challenge. To overcome this challenge, we
introduce a novel batch-mode active learning (AL) policy that uses uncertainty
sampling to simulate training data drawn from a probability distribution that
prefers labelled examples about which the model is least certain. We confirm
its efficacy in training the same models with about 75% less training data
while improving the accuracy. We then discuss the design of an efficient
stream-based training workflow that uses this AL policy and present a
performance study on two heterogeneous platforms to demonstrate that, compared
with a conventional training workflow, the streaming workflow delivers about
20% shorter training time without any loss of accuracy.

</details>


### [105] [Runtime Safety through Adaptive Shielding: From Hidden Parameter Inference to Provable Guarantees](https://arxiv.org/abs/2506.11033)
*Minjae Kwon,Tyler Ingebrand,Ufuk Topcu,Lu Feng*

Main category: cs.LG

TL;DR: 提出了一种运行时屏蔽机制，通过实时推断隐藏参数并预测安全风险，确保强化学习的安全性。


<details>
  <summary>Details</summary>
Motivation: 隐藏参数（如机器人质量分布或摩擦）的变化在执行过程中可能带来安全风险，需要一种机制来实时适应和保障安全。

Method: 基于约束隐藏参数马尔可夫决策过程，利用函数编码器实时推断隐藏参数，并通过保形预测处理不确定性，约束动作空间以避免安全风险。

Result: 实验表明，该方法显著减少安全违规，并具有强大的分布外泛化能力，同时运行时开销极小。

Conclusion: 该机制满足概率安全保证，并在安全合规策略中实现最优性能。

Abstract: Variations in hidden parameters, such as a robot's mass distribution or
friction, pose safety risks during execution. We develop a runtime shielding
mechanism for reinforcement learning, building on the formalism of constrained
hidden-parameter Markov decision processes. Function encoders enable real-time
inference of hidden parameters from observations, allowing the shield and the
underlying policy to adapt online. The shield constrains the action space by
forecasting future safety risks (such as obstacle proximity) and accounts for
uncertainty via conformal prediction. We prove that the proposed mechanism
satisfies probabilistic safety guarantees and yields optimal policies among the
set of safety-compliant policies. Experiments across diverse environments with
varying hidden parameters show that our method significantly reduces safety
violations and achieves strong out-of-distribution generalization, while
incurring minimal runtime overhead.

</details>


### [106] [CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.11034)
*Aneesh Komanduri,Karuna Bhaila,Xintao Wu*

Main category: cs.LG

TL;DR: 论文提出了一个多模态上下文学习的因果推理基准CausalVLBench，用于评估大型视觉语言模型（LVLMs）在视觉因果推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在因果推理任务中表现出色，但LVLMs在视觉因果推理任务中的能力尚未充分研究。

Method: 通过CausalVLBench评估LVLMs在三个代表性任务（因果结构推断、干预目标预测和反事实预测）中的表现。

Result: 研究揭示了现有LVLMs在视觉因果推理任务中的基本优势和不足。

Conclusion: 该基准旨在推动改进LVLMs视觉因果推理能力的新方向。

Abstract: Large language models (LLMs) have shown remarkable ability in various
language tasks, especially with their emergent in-context learning capability.
Extending LLMs to incorporate visual inputs, large vision-language models
(LVLMs) have shown impressive performance in tasks such as recognition and
visual question answering (VQA). Despite increasing interest in the utility of
LLMs in causal reasoning tasks such as causal discovery and counterfactual
reasoning, there has been relatively little work showcasing the abilities of
LVLMs on visual causal reasoning tasks. We take this opportunity to formally
introduce a comprehensive causal reasoning benchmark for multi-modal in-context
learning from LVLMs. Our CausalVLBench encompasses three representative tasks:
causal structure inference, intervention target prediction, and counterfactual
prediction. We evaluate the ability of state-of-the-art open-source LVLMs on
our causal reasoning tasks across three causal representation learning datasets
and demonstrate their fundamental strengths and weaknesses. We hope that our
benchmark elucidates the drawbacks of existing vision-language models and
motivates new directions and paradigms in improving the visual causal reasoning
abilities of LVLMs.

</details>


### [107] [Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity](https://arxiv.org/abs/2506.11035)
*Moussa Koulako Bala Doumbouya,Dan Jurafsky,Christopher D. Manning*

Main category: cs.LG

TL;DR: 论文提出了一种基于Tversky相似性理论的可微分参数化方法，用于深度学习中的非几何相似性建模，并在图像识别和语言建模中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习的几何相似性模型与人类心理感知不符，而Tversky的基于特征的相似性理论未被应用，因此需要一种可微分的方法将其引入深度学习。

Method: 开发了Tversky相似性的可微分参数化方法，并设计了Tversky投影层作为神经网络模块，替代传统的线性投影层。

Result: 在NABirds图像分类任务中，Tversky投影层相对线性层提升了24.7%的准确率；在GPT-2中，困惑度降低7.5%，参数减少34.8%。

Conclusion: Tversky投影层为深度学习提供了一种更符合心理相似性理论的新范式，并增强了模型的可解释性。

Abstract: Work in psychology has highlighted that the geometric model of similarity
standard in deep learning is not psychologically plausible because its metric
properties such as symmetry do not align with human perception. In contrast,
Tversky (1977) proposed an axiomatic theory of similarity based on a
representation of objects as sets of features, and their similarity as a
function of common and distinctive features. However, this model has not been
used in deep learning before, partly due to the challenge of incorporating
discrete set operations. We develop a differentiable parameterization of
Tversky's similarity that is learnable through gradient descent, and derive
neural network building blocks such as the Tversky projection layer, which
unlike the linear projection layer can model non-linear functions such as XOR.
Through experiments with image recognition and language modeling, we show that
the Tversky projection layer is a beneficial replacement for the linear
projection layer, which employs geometric similarity. On the NABirds image
classification task, a frozen ResNet-50 adapted with a Tversky projection layer
achieves a 24.7% relative accuracy improvement over the linear layer adapter
baseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases
by 7.5%, and its parameter count by 34.8%. Finally, we propose a unified
interpretation of both projection layers as computing similarities of input
stimuli to learned prototypes, for which we also propose a novel visualization
technique highlighting the interpretability of Tversky projection layers. Our
work offers a new paradigm for thinking about the similarity model implicit in
deep learning, and designing networks that are interpretable under an
established theory of psychological similarity.

</details>


### [108] [Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification](https://arxiv.org/abs/2506.11036)
*Yang Qin,Chao Chen,Zhihang Fu,Dezhong Peng,Xi Peng,Peng Hu*

Main category: cs.LG

TL;DR: 论文提出了一种交互式跨模态学习框架（ICL），通过人机交互增强文本查询的区分能力，并引入测试时人机交互模块（THI）和数据增强策略（RDA），显著提升了文本到图像行人重识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因网络结构和数据质量等内在限制，难以区分具有挑战性的候选图像，因此需要一种更有效的方法来提升查询意图与目标图像的对齐。

Method: 提出ICL框架，包含THI模块（通过多轮交互优化查询）和RDA策略（通过信息重组增强文本质量）。

Result: 在四个基准数据集上实验表明，该方法性能显著提升。

Conclusion: ICL框架通过交互式学习和数据增强，有效解决了现有方法的局限性，提升了行人重识别的准确性。

Abstract: Despite remarkable advancements in text-to-image person re-identification
(TIReID) facilitated by the breakthrough of cross-modal embedding models,
existing methods often struggle to distinguish challenging candidate images due
to intrinsic limitations, such as network architecture and data quality. To
address these issues, we propose an Interactive Cross-modal Learning framework
(ICL), which leverages human-centered interaction to enhance the
discriminability of text queries through external multimodal knowledge. To
achieve this, we propose a plug-and-play Test-time Humane-centered Interaction
(THI) module, which performs visual question answering focused on human
characteristics, facilitating multi-round interactions with a multimodal large
language model (MLLM) to align query intent with latent target images.
Specifically, THI refines user queries based on the MLLM responses to reduce
the gap to the best-matching images, thereby boosting ranking accuracy.
Additionally, to address the limitation of low-quality training texts, we
introduce a novel Reorganization Data Augmentation (RDA) strategy based on
information enrichment and diversity enhancement to enhance query
discriminability by enriching, decomposing, and reorganizing person
descriptions. Extensive experiments on four TIReID benchmarks, i.e.,
CUHK-PEDES, ICFG-PEDES, RSTPReid, and UFine6926, demonstrate that our method
achieves remarkable performance with substantial improvement.

</details>


### [109] [Mini-Game Lifetime Value Prediction in WeChat](https://arxiv.org/abs/2506.11037)
*Aochuan Chen,Yifan Niu,Ziqi Gao,Yujie Sun,Shoujun Liu,Gong Chen,Yang Liu,Jia Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为GRePO-LTV的创新框架，用于解决LTV预测中的数据稀缺和任务依赖性问题。


<details>
  <summary>Details</summary>
Motivation: LTV预测对广告商至关重要，但数据稀缺和任务依赖性导致预测准确性不足。

Method: 采用图表示学习解决数据稀缺问题，并使用帕累托优化处理任务依赖性。

Result: GRePO-LTV框架能够有效提升LTV预测的准确性。

Conclusion: 该框架为LTV预测提供了一种新的解决方案，解决了数据稀缺和任务依赖性的双重挑战。

Abstract: The LifeTime Value (LTV) prediction, which endeavors to forecast the
cumulative purchase contribution of a user to a particular item, remains a
vital challenge that advertisers are keen to resolve. A precise LTV prediction
system enhances the alignment of user interests with meticulously designed
advertisements, thereby generating substantial profits for advertisers.
Nonetheless, this issue is complicated by the paucity of data typically
observed in real-world advertising scenarios. The purchase rate among
registered users is often as critically low as 0.1%, resulting in a dataset
where the majority of users make only several purchases. Consequently, there is
insufficient supervisory signal for effectively training the LTV prediction
model. An additional challenge emerges from the interdependencies among tasks
with high correlation. It is a common practice to estimate a user's
contribution to a game over a specified temporal interval. Varying the lengths
of these intervals corresponds to distinct predictive tasks, which are highly
correlated. For instance, predictions over a 7-day period are heavily reliant
on forecasts made over a 3-day period, where exceptional cases can adversely
affect the accuracy of both tasks. In order to comprehensively address the
aforementioned challenges, we introduce an innovative framework denoted as
Graph-Represented Pareto-Optimal LifeTime Value prediction (GRePO-LTV). Graph
representation learning is initially employed to address the issue of data
scarcity. Subsequently, Pareto-Optimization is utilized to manage the
interdependence of prediction tasks.

</details>


### [110] [MoTE: Mixture of Task-specific Experts for Pre-Trained ModelBased Class-incremental Learning](https://arxiv.org/abs/2506.11038)
*Linjie Li,Zhenyu Wu,Yang Ji*

Main category: cs.LG

TL;DR: 提出了一种基于任务特定专家混合（MoTE）的框架，解决了类增量学习中预训练模型的维度不一致和覆盖问题，无需示例集，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决类增量学习中预训练模型的提示覆盖和适配器维度不一致问题，以及混合专家（MoE）在动态环境中的参数覆盖挑战。

Method: 提出MoTE框架，结合任务感知专家过滤和可靠专家联合推断，避免灾难性遗忘，并探索适配器扩展与性能的权衡。

Result: 实验证明MoTE在无需示例集的情况下表现优越，任务数量与适配器数量线性扩展。

Conclusion: MoTE有效解决了类增量学习中的维度不一致和覆盖问题，性能优越，代码已开源。

Abstract: Class-incremental learning (CIL) requires deep learning models to
continuously acquire new knowledge from streaming data while preserving
previously learned information. Recently, CIL based on pre-trained models
(PTMs) has achieved remarkable success. However, prompt-based approaches suffer
from prompt overwriting, while adapter-based methods face challenges such as
dimensional misalignment between tasks. While the idea of expert fusion in
Mixture of Experts (MoE) can help address dimensional inconsistency, both
expert and routing parameters are prone to being overwritten in dynamic
environments, making MoE challenging to apply directly in CIL. To tackle these
issues, we propose a mixture of task-specific experts (MoTE) framework that
effectively mitigates the miscalibration caused by inconsistent output
dimensions across tasks. Inspired by the weighted feature fusion and sparse
activation mechanisms in MoE, we introduce task-aware expert filtering and
reliable expert joint inference during the inference phase, mimicking the
behavior of routing layers without inducing catastrophic forgetting. Extensive
experiments demonstrate the superiority of our method without requiring an
exemplar set. Furthermore, the number of tasks in MoTE scales linearly with the
number of adapters. Building on this, we further explore the trade-off between
adapter expansion and model performance and propose the Adapter-Limited MoTE.
The code is available at https://github.com/Franklilinjie/MoTE.

</details>


### [111] [Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation](https://arxiv.org/abs/2506.11039)
*Cheng Jin,Zhenyu Xiao,Chutao Liu,Yuantao Gu*

Main category: cs.LG

TL;DR: 论文提出了一种角度域引导（ADG）算法，解决了分类器无关引导（CFG）在高权重下导致的颜色失真问题，同时保持了文本-图像对齐效果。


<details>
  <summary>Details</summary>
Motivation: CFG在文本到图像生成中表现优异，但在高权重下会导致颜色失真，影响了生成图像的质量。

Method: 通过理论分析CFG引起的潜在空间样本范数放大问题，提出ADG算法，约束幅度变化并优化角度对齐。

Result: ADG显著优于现有方法，生成图像在文本对齐、颜色保真度和人类感知偏好上表现更好。

Conclusion: ADG有效解决了CFG的颜色失真问题，同时保持了其优势，为高质量图像生成提供了新方法。

Abstract: Classifier-free guidance (CFG) has emerged as a pivotal advancement in
text-to-image latent diffusion models, establishing itself as a cornerstone
technique for achieving high-quality image synthesis. However, under high
guidance weights, where text-image alignment is significantly enhanced, CFG
also leads to pronounced color distortions in the generated images. We identify
that these distortions stem from the amplification of sample norms in the
latent space. We present a theoretical framework that elucidates the mechanisms
of norm amplification and anomalous diffusion phenomena induced by
classifier-free guidance. Leveraging our theoretical insights and the latent
space structure, we propose an Angle Domain Guidance (ADG) algorithm. ADG
constrains magnitude variations while optimizing angular alignment, thereby
mitigating color distortions while preserving the enhanced text-image alignment
achieved at higher guidance weights. Experimental results demonstrate that ADG
significantly outperforms existing methods, generating images that not only
maintain superior text alignment but also exhibit improved color fidelity and
better alignment with human perceptual preferences.

</details>


### [112] [Large Language models for Time Series Analysis: Techniques, Applications, and Challenges](https://arxiv.org/abs/2506.11040)
*Feifei Shi,Xueyan Yin,Kang Wang,Wanyu Tu,Qifu Sun,Huansheng Ning*

Main category: cs.LG

TL;DR: 本文系统综述了预训练大型语言模型（LLM）在时间序列分析中的应用，探讨了技术、应用和挑战，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析方法在非线性特征表示和长期依赖捕捉方面受限，LLM因其跨模态知识整合和注意力机制具有潜力，但开发通用LLM仍面临数据多样性、标注稀缺和计算需求等挑战。

Method: 通过建立AI驱动时间序列分析的演进路线图，从早期机器学习到LLM驱动范式，再到原生时间基础模型；从工作流程角度系统化LLM驱动时间序列分析的技术格局。

Result: 总结了LLM在时间序列分析中的输入、优化和轻量化阶段的技术，并探讨了实际应用和开放挑战。

Conclusion: 本文为学术和工业研究者提供了当前进展的见解和未来发展的方向，推动更高效、通用和可解释的LLM驱动时间序列分析系统的发展。

Abstract: Time series analysis is pivotal in domains like financial forecasting and
biomedical monitoring, yet traditional methods are constrained by limited
nonlinear feature representation and long-term dependency capture. The
emergence of Large Language Models (LLMs) offers transformative potential by
leveraging their cross-modal knowledge integration and inherent attention
mechanisms for time series analysis. However, the development of
general-purpose LLMs for time series from scratch is still hindered by data
diversity, annotation scarcity, and computational requirements. This paper
presents a systematic review of pre-trained LLM-driven time series analysis,
focusing on enabling techniques, potential applications, and open challenges.
First, it establishes an evolutionary roadmap of AI-driven time series
analysis, from the early machine learning era, through the emerging LLM-driven
paradigm, to the development of native temporal foundation models. Second, it
organizes and systematizes the technical landscape of LLM-driven time series
analysis from a workflow perspective, covering LLMs' input, optimization, and
lightweight stages. Finally, it critically examines novel real-world
applications and highlights key open challenges that can guide future research
and innovation. The work not only provides valuable insights into current
advances but also outlines promising directions for future development. It
serves as a foundational reference for both academic and industrial
researchers, paving the way for the development of more efficient,
generalizable, and interpretable systems of LLM-driven time series analysis.

</details>


### [113] [ChemHGNN: A Hierarchical Hypergraph Neural Network for Reaction Virtual Screening and Discovery](https://arxiv.org/abs/2506.11041)
*Xiaobao Huang,Yihong Ma,Anjali Gurajapu,Jules Schleinitz,Zhichun Guo,Sarah E. Reisman,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: ChemHGNN是一种超图神经网络框架，用于高效捕捉反应网络中的高阶关系，显著优于传统GNN和HGNN基线。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在多反应物相互作用建模上表现不佳，需要更高效的框架来捕捉复杂反应关系。

Method: 提出ChemHGNN框架，利用超边建模多反应物反应，引入反应中心感知负采样策略（RCNS）和分层嵌入方法。

Result: 在USPTO数据集上，ChemHGNN显著优于基线模型，尤其在大规模场景下，同时保持化学合理性和可解释性。

Conclusion: HGNN是反应虚拟筛选和发现的更优选择，为加速反应发现提供了化学信息化的框架。

Abstract: Reaction virtual screening and discovery are fundamental challenges in
chemistry and materials science, where traditional graph neural networks (GNNs)
struggle to model multi-reactant interactions. In this work, we propose
ChemHGNN, a hypergraph neural network (HGNN) framework that effectively
captures high-order relationships in reaction networks. Unlike GNNs, which
require constructing complete graphs for multi-reactant reactions, ChemHGNN
naturally models multi-reactant reactions through hyperedges, enabling more
expressive reaction representations. To address key challenges, such as
combinatorial explosion, model collapse, and chemically invalid negative
samples, we introduce a reaction center-aware negative sampling strategy (RCNS)
and a hierarchical embedding approach combining molecule, reaction and
hypergraph level features. Experiments on the USPTO dataset demonstrate that
ChemHGNN significantly outperforms HGNN and GNN baselines, particularly in
large-scale settings, while maintaining interpretability and chemical
plausibility. Our work establishes HGNNs as a superior alternative to GNNs for
reaction virtual screening and discovery, offering a chemically informed
framework for accelerating reaction discovery.

</details>


### [114] [GenFT: A Generative Parameter-Efficient Fine-Tuning Method for Pretrained Foundation Models](https://arxiv.org/abs/2506.11042)
*Baoquan Zhang,Guangning Xu,Michael. K. Ng*

Main category: cs.LG

TL;DR: GenFT是一种新的参数高效微调方法，通过从预训练权重中提取结构化信息来指导任务特定的权重更新，避免了从头训练的低效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用预训练权重指导任务特定权重的更新，以提高微调效率。

Method: GenFT通过行和列变换从预训练权重中提取结构化信息，并将任务特定权重分解为层共享和层特定组件。

Result: 在CV和NLP任务中，GenFT表现优于现有PEFT方法。

Conclusion: GenFT为高效模型适应提供了新视角，展示了其简单性和有效性。

Abstract: Pretrained Foundation Models (PFMs) have transformed numerous applications by
enabling efficient adaptation to customized tasks. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a resource-efficient alternative to full
fine-tuning, especially leveraging reparameterized weights $\Delta W$ to adapt
models for downstream tasks. However, a critical yet underexplored question
remains: can we utilize well-pretrained weights $W_0$ to guide the update of
task-specific $\Delta W$, avoiding inefficient training it from scratch? To end
this, we propose Generative Parameter-Efficient Fine-Tuning (GenFT), a novel
method that extracts structured, transferable information from $W_0$ for
efficient $\Delta W$ training. To extract row and column structure information,
GenFT applies row and column transformations to distill essential patterns from
$W_0$. A tailored policy further decomposes $\Delta W$ into layer-shared and
layer-specific components, balancing information reuse and individualized
flexibility. GenFT is simple yet effective, achieving superior performance
across CV and NLP tasks. Extensive experiments on VTAB-1K, FGVC, and GLUE
benchmarks demonstrate that GenFT outperforms state-of-the-art PEFT methods,
offering a new perspective for efficient model adaptation.

</details>


### [115] [Boost Post-Training Quantization via Null Space Optimization for Large Language Models](https://arxiv.org/abs/2506.11044)
*Jiaqi Zhao,Miao Zhang,Weili Guan,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文提出了一种基于零空间概念的LLM量化方法Q2N，通过约束量化误差在输入激活的零空间内，有效减少量化误差。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法对LLM的性能提升有限，需要新的研究方向以支持更高效的模型压缩。

Method: 提出Q2N模块，设计高效的零空间投影近似方法，并推导闭式解以满足推理条件。

Result: 在多个先进LLM和基准测试中验证了Q2N和零空间优化的有效性。

Conclusion: 本文为零空间优化在量化中的应用提供了初步探索，希望启发未来更先进的量化方法设计。

Abstract: Existing post-training quantization methods for large language models (LLMs)
offer remarkable success. However, the increasingly marginal performance gains
suggest that existing quantization strategies are insufficient to support the
development of more compressed models. To inspire new directions for future
research, this paper introduces the concept of null space into LLMs
quantization. We argue that the quantization error can be effectively
alleviated by constraining the post-quantization weight perturbation to lie
within the null space of input activations. To prove this idea, we propose a
plug-and-play null space projection module for existing milestone PTQ baselines
named Q2N. Specifically, we first design an efficient and accurate null space
projection approximation method tailored to the characteristics of LLMs.
Subsequently, we theoretically derive a closed-form solution for an equivalent
vector of the obtained projection matrix, which satisfies practical inference
condition while avoiding additional memory overhead. Extensive experiments are
conducted on various state-of-the-art LLMs (LLaMA3, DeepSeek, Qwen3) and
baselines, demonstrating the effectiveness of both our Q2N and the perspective
of null space optimization for LLMs quantization. We view this paper the first
step to further alleviate the quantization error based on the insights of null
space, hoping it inspiring future researchers to design more advanced
quantization methods. Codes are available at https://github.com/zjq0455/q2n.

</details>


### [116] [Procedural Environment Generation for Tool-Use Agents](https://arxiv.org/abs/2506.11045)
*Michael Sullivan,Mareike Hartmann,Alexander Koller*

Main category: cs.LG

TL;DR: 论文提出了一种名为RandomWorld的流程，用于生成交互式和组合式的工具使用数据，以解决现有工具使用训练数据生成方法的不足。实验表明，基于RandomWorld数据训练的模型在多个工具使用基准测试中表现优异，并在NESTFUL数据集上刷新了两项指标的最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 当前的工具使用训练数据生成方法多为非交互式或非组合式，限制了在线强化学习训练的效果。

Method: 引入RandomWorld流程，通过程序化生成交互式和组合式的工具使用数据，并利用监督微调（SFT）和强化学习（RL）进行模型训练。

Result: 基于RandomWorld数据训练的模型在多个工具使用基准测试中表现优于现有方法，并在NESTFUL数据集上取得两项新最佳成绩。下游性能随训练数据量的增加而提升。

Conclusion: RandomWorld为工具使用训练数据的生成提供了一种高效且可扩展的解决方案，未来有望通过纯合成数据进一步提升性能。

Abstract: Although the power of LLM tool-use agents has ignited a flurry of recent
research in this area, the curation of tool-use training data remains an open
problem$-$especially for online RL training. Existing approaches to synthetic
tool-use data generation tend to be non-interactive, and/or non-compositional.
We introduce RandomWorld, a pipeline for the procedural generation of
interactive tools and compositional tool-use data. We show that models tuned
via SFT and RL on synthetic RandomWorld data improve on a range of tool-use
benchmarks, and set the new SoTA for two metrics on the NESTFUL dataset.
Further experiments show that downstream performance scales with the amount of
RandomWorld-generated training data, opening up the possibility of further
improvement through the use of entirely synthetic data.

</details>


### [117] [The Effects of Data Augmentation on Confidence Estimation for LLMs](https://arxiv.org/abs/2506.11046)
*Rui Wang,Renyu Zhu,Minmin Lin,Runze Wu,Tangjie Lv,Changjie Fan,Haobo Wang*

Main category: cs.LG

TL;DR: 研究了不同数据增强方法对大型语言模型置信度估计的影响，发现数据多样性和语义保留是关键，随机组合增强策略效果较好。


<details>
  <summary>Details</summary>
Motivation: 探讨数据增强在置信度估计中的潜力，克服现有方法局限。

Method: 分析不同数据增强策略对置信度估计的影响，研究数据多样性和语义保留的作用。

Result: 数据增强能提升性能并缓解过自信问题，随机组合策略表现优异。

Conclusion: 数据多样性和语义保留是增强效果的关键，随机组合策略具有实用性和可转移性。

Abstract: Confidence estimation is crucial for reflecting the reliability of large
language models (LLMs), particularly in the widely used closed-source models.
Utilizing data augmentation for confidence estimation is viable, but
discussions focus on specific augmentation techniques, limiting its potential.
We study the impact of different data augmentation methods on confidence
estimation. Our findings indicate that data augmentation strategies can achieve
better performance and mitigate the impact of overconfidence. We investigate
the influential factors related to this and discover that, while preserving
semantic information, greater data diversity enhances the effectiveness of
augmentation. Furthermore, the impact of different augmentation strategies
varies across different range of application. Considering parameter
transferability and usability, the random combination of augmentations is a
promising choice.

</details>


### [118] [Perception-Driven Bias Detection in Machine Learning via Crowdsourced Visual Judgment](https://arxiv.org/abs/2506.11047)
*Chirudeep Tupakula,Rittika Shamsuddin*

Main category: cs.LG

TL;DR: 本文提出了一种基于人类感知的偏见检测框架，通过众包收集非专家用户的视觉判断，以识别数据中的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 传统偏见检测方法依赖敏感标签或固定公平性指标，限制了实际应用。本文旨在开发一种轻量级、可扩展的替代方案。

Method: 设计了一个基于网络的平台，展示简化数据可视化，收集用户对群体相似性的二元判断，并通过统计和机器学习验证结果。

Result: 研究发现，非专家用户的视觉感知与已知偏见案例高度相关，表明视觉直觉可作为公平性审计的有效代理。

Conclusion: 该方法为公平性诊断提供了一种高效、可解释的替代方案，推动了基于众包的偏见检测流程的发展。

Abstract: Machine learning systems are increasingly deployed in high-stakes domains,
yet they remain vulnerable to bias systematic disparities that
disproportionately impact specific demographic groups. Traditional bias
detection methods often depend on access to sensitive labels or rely on rigid
fairness metrics, limiting their applicability in real-world settings. This
paper introduces a novel, perception-driven framework for bias detection that
leverages crowdsourced human judgment. Inspired by reCAPTCHA and other
crowd-powered systems, we present a lightweight web platform that displays
stripped-down visualizations of numeric data (for example-salary distributions
across demographic clusters) and collects binary judgments on group similarity.
We explore how users' visual perception-shaped by layout, spacing, and question
phrasing can signal potential disparities. User feedback is aggregated to flag
data segments as biased, which are then validated through statistical tests and
machine learning cross-evaluations. Our findings show that perceptual signals
from non-expert users reliably correlate with known bias cases, suggesting that
visual intuition can serve as a powerful, scalable proxy for fairness auditing.
This approach offers a label-efficient, interpretable alternative to
conventional fairness diagnostics, paving the way toward human-aligned,
crowdsourced bias detection pipelines.

</details>


### [119] [I Can't Believe It's Not Real: CV-MuSeNet: Complex-Valued Multi-Signal Segmentation](https://arxiv.org/abs/2506.11048)
*Sangwon Shin,Mehmet C. Vuran*

Main category: cs.LG

TL;DR: CMuSeNet是一种基于复数神经网络的宽带频谱感知网络，通过改进训练方法和相似性度量，显著提升了低信噪比环境下的信号检测性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统实数神经网络在低信噪比环境下难以捕捉无线信号相位和幅度等关键特性的问题。

Method: 提出CMuSeNet，采用复数神经网络和残差架构，引入复数傅里叶谱焦点损失（CFL）和复数平面交并比（CIoU）相似性度量。

Result: 在合成、室内和真实数据集上，CMuSeNet平均准确率达98.98%-99.90%，比实数网络提升9.2个百分点，训练时间减少92.2%。

Conclusion: 复数神经网络架构在低信噪比环境下显著提升了频谱感知的性能和效率。

Abstract: The increasing congestion of the radio frequency spectrum presents challenges
for efficient spectrum utilization. Cognitive radio systems enable dynamic
spectrum access with the aid of recent innovations in neural networks. However,
traditional real-valued neural networks (RVNNs) face difficulties in low
signal-to-noise ratio (SNR) environments, as they were not specifically
developed to capture essential wireless signal properties such as phase and
amplitude. This work presents CMuSeNet, a complex-valued multi-signal
segmentation network for wideband spectrum sensing, to address these
limitations. Extensive hyperparameter analysis shows that a naive conversion of
existing RVNNs into their complex-valued counterparts is ineffective. Built on
complex-valued neural networks (CVNNs) with a residual architecture, CMuSeNet
introduces a complexvalued Fourier spectrum focal loss (CFL) and a complex
plane intersection over union (CIoU) similarity metric to enhance training
performance. Extensive evaluations on synthetic, indoor overthe-air, and
real-world datasets show that CMuSeNet achieves an average accuracy of
98.98%-99.90%, improving by up to 9.2 percentage points over its real-valued
counterpart and consistently outperforms state of the art. Strikingly, CMuSeNet
achieves the accuracy level of its RVNN counterpart in just two epochs,
compared to the 27 epochs required for RVNN, while reducing training time by up
to a 92.2% over the state of the art. The results highlight the effectiveness
of complex-valued architectures in improving weak signal detection and training
efficiency for spectrum sensing in challenging low-SNR environments. The
dataset is available at: https://dx.doi.org/10.21227/hcc1-6p22

</details>


### [120] [15,500 Seconds: Lean UAV Classification Leveraging PEFT and Pre-Trained Networks](https://arxiv.org/abs/2506.11049)
*Andrew P. Berg,Qian Zhang,Mia Y. Wang*

Main category: cs.LG

TL;DR: 论文解决了无人机音频分类中的数据稀缺问题，通过参数高效微调、数据增强和预训练网络等方法，实现了95%以上的验证准确率。


<details>
  <summary>Details</summary>
Motivation: 随着消费和军用无人机市场的增长，无人机的安全问题日益突出，但无人机音频分类领域的数据稀缺问题亟待解决。

Method: 采用参数高效微调、数据增强和预训练网络（如EfficientNet-B0）等创新方法。

Result: 验证准确率达到95%以上。

Conclusion: 提出的方法有效解决了无人机音频分类中的数据稀缺问题，并取得了高性能。

Abstract: Unmanned Aerial Vehicles (UAVs) pose an escalating security concerns as the
market for consumer and military UAVs grows. This paper address the critical
data scarcity challenges in deep UAV audio classification. We build upon our
previous work expanding novel approaches such as: parameter efficient
fine-tuning, data augmentation, and pre-trained networks. We achieve
performance upwards of 95\% validation accuracy with EfficientNet-B0.

</details>


### [121] [NSW-EPNews: A News-Augmented Benchmark for Electricity Price Forecasting with LLMs](https://arxiv.org/abs/2506.11050)
*Zhaoge Bi,Linghan Huang,Haolin Jin,Qingwen Zeng,Huaming Chen*

Main category: cs.LG

TL;DR: 论文提出了NSW-EPNews基准，结合时间序列模型和大型语言模型（LLMs）进行电价预测，发现新闻特征对传统模型效果有限，而LLMs性能提升有限且易产生幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有电价预测方法依赖数值历史数据，忽略文本信号，需要多模态评估框架。

Method: 使用NSW-EPNews数据集，包含电价、天气和新闻数据，设计48步预测任务，评估传统模型和LLMs。

Result: 传统模型从新闻特征中获益有限；LLMs性能略有提升但易产生幻觉。

Conclusion: NSW-EPNews为多模态数值推理提供测试平台，揭示LLMs在高风险预测中的能力差距。

Abstract: Electricity price forecasting is a critical component of modern
energy-management systems, yet existing approaches heavily rely on numerical
histories and ignore contemporaneous textual signals. We introduce NSW-EPNews,
the first benchmark that jointly evaluates time-series models and large
language models (LLMs) on real-world electricity-price prediction. The dataset
includes over 175,000 half-hourly spot prices from New South Wales, Australia
(2015-2024), daily temperature readings, and curated market-news summaries from
WattClarity. We frame the task as 48-step-ahead forecasting, using multimodal
input, including lagged prices, vectorized news and weather features for
classical models, and prompt-engineered structured contexts for LLMs. Our
datasets yields 3.6k multimodal prompt-output pairs for LLM evaluation using
specific templates. Through compresive benchmark design, we identify that for
traditional statistical and machine learning models, the benefits gain is
marginal from news feature. For state-of-the-art LLMs, such as GPT-4o and
Gemini 1.5 Pro, we observe modest performance increase while it also produce
frequent hallucinations such as fabricated and malformed price sequences.
NSW-EPNews provides a rigorous testbed for evaluating grounded numerical
reasoning in multimodal settings, and highlights a critical gap between current
LLM capabilities and the demands of high-stakes energy forecasting.

</details>


### [122] [ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention](https://arxiv.org/abs/2506.11052)
*Henrik Abgaryan,Tristan Cazenave,Ararat Harutyunyan*

Main category: cs.LG

TL;DR: 论文提出ACCORD框架，利用LLMs解决NP难组合优化问题，通过动态约束满足和注意力路由提升性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在NP难组合优化问题中的应用潜力，填补直接应用研究的空白。

Method: 提出ACCORD框架，结合自回归约束生成和动态注意力路由，利用问题特定LoRA模块。

Result: ACCORD在多种NP难问题上表现优于标准方法，甚至超越更大的LLMs如GPT-4。

Conclusion: ACCORD是首个大规模端到端框架，为LLMs在组合优化中的应用提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, yet their direct application to NP-hard combinatorial problems
(CPs) remains underexplored. In this work, we systematically investigate the
reasoning abilities of LLMs on a variety of NP-hard combinatorial optimization
tasks and introduce ACCORD: Autoregressive Constraint-satisfying generation for
COmbinatorial optimization with Routing and Dynamic attention. ACCORD features
a novel dataset representation and model architecture that leverage the
autoregressive nature of LLMs to dynamically enforce feasibility constraints,
coupled with attention-based routing to activate problem-specific LoRA modules.
We also present the ACCORD-90k supervised dataset, covering six NP-hard
combinatorial problems: TSP, VRP, Knapsack, FlowShop, JSSP, and BinPacking.
Extensive experiments demonstrate that our ACCORD model, built on an
8B-parameter Llama backbone, consistently outperforms standard prompting and
input-output methods, even when compared to much larger LLMs, such as gpt-4.
Ablation studies further show that our output structure enhances solution
feasibility. To the best of our knowledge, this is the first large-scale,
end-to-end framework for exploring the applications of LLMs to a broad spectrum
of combinatorial optimization problems. The codes are publicly available at
https://github.com/starjob42/ACCORD

</details>


### [123] [Bootstrapping your behavior: a new pretraining strategy for user behavior sequence data](https://arxiv.org/abs/2506.11053)
*Weichang Wu,Xiaolu Zhang,Jun Zhou,Yuchen Li,Wenwen Xia*

Main category: cs.LG

TL;DR: 论文提出了一种名为Bootstrapping Your Behavior (BYB)的新型用户行为序列预训练策略，通过自动构建监督嵌入替代手动行为词汇选择，提升了模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有用户行为序列预训练方法依赖手动行为词汇选择，耗时且易偏，词汇容量限制也影响模型泛化能力。

Method: BYB通过预测未来时间窗口内所有行为信息的自动构建监督嵌入，结合师生编码器方案实现预训练监督。

Result: 实验显示，BYB在AUC上平均提升3.9%，训练吞吐量提升98.9%，在线部署中显著降低金融逾期风险。

Conclusion: BYB有效解决了手动词汇选择的局限性，提升了模型性能与泛化能力，具有实际工业应用价值。

Abstract: User Behavior Sequence (UBS) modeling is crucial in industrial applications.
As data scale and task diversity grow, UBS pretraining methods have become
increasingly pivotal. State-of-the-art UBS pretraining methods rely on
predicting behavior distributions. The key step in these methods is
constructing a selected behavior vocabulary. However, this manual step is
labor-intensive and prone to bias. The limitation of vocabulary capacity also
directly affects models' generalization ability. In this paper, we introduce
Bootstrapping Your Behavior (\model{}), a novel UBS pretraining strategy that
predicts an automatically constructed supervision embedding summarizing all
behaviors' information within a future time window, eliminating the manual
behavior vocabulary selection. In implementation, we incorporate a
student-teacher encoder scheme to construct the pretraining supervision
effectively. Experiments on two real-world industrial datasets and eight
downstream tasks demonstrate that \model{} achieves an average improvement of
3.9\% in AUC and 98.9\% in training throughput. Notably, the model exhibits
meaningful attention patterns and cluster representations during pretraining
without any label supervision. In our online deployment over two months, the
pretrained model improves the KS by about 2.7\% and 7.1\% over the baseline
model for two financial overdue risk prediction tasks in the Alipay mobile
application, which reduces bad debt risk by millions of dollars for Ant group.

</details>


### [124] [Adaptive Composition of Machine Learning as a Service (MLaaS) for IoT Environments](https://arxiv.org/abs/2506.11054)
*Deepak Kanneganti,Sajib Mistry,Sheik Mohammad Mostakim Fattah,Aneesh Krishna,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出了一种自适应MLaaS组合框架，以应对IoT环境中数据分布和系统需求的变化，通过服务评估和候选选择模型优化组合，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: IoT环境的动态性导致数据分布和系统需求不断变化，传统MLaaS组合难以长期有效。

Method: 整合服务评估模型和候选选择模型，采用上下文多臂老虎机优化策略逐步更新MLaaS组合。

Result: 实验证明该框架能高效维持服务质量并降低计算成本。

Conclusion: 自适应框架能有效应对IoT环境的不确定性，提升MLaaS组合的长期有效性。

Abstract: The dynamic nature of Internet of Things (IoT) environments challenges the
long-term effectiveness of Machine Learning as a Service (MLaaS) compositions.
The uncertainty and variability of IoT environments lead to fluctuations in
data distribution, e.g., concept drift and data heterogeneity, and evolving
system requirements, e.g., scalability demands and resource limitations. This
paper proposes an adaptive MLaaS composition framework to ensure a seamless,
efficient, and scalable MLaaS composition. The framework integrates a service
assessment model to identify underperforming MLaaS services and a candidate
selection model to filter optimal replacements. An adaptive composition
mechanism is developed that incrementally updates MLaaS compositions using a
contextual multi-armed bandit optimization strategy. By continuously adapting
to evolving IoT constraints, the approach maintains Quality of Service (QoS)
while reducing the computational cost associated with recomposition from
scratch. Experimental results on a real-world dataset demonstrate the
efficiency of our proposed approach.

</details>


### [125] [PolyMicros: Bootstrapping a Foundation Model for Polycrystalline Material Structure](https://arxiv.org/abs/2506.11055)
*Michael Buzzy,Andreas Robertson,Peng Chen,Surya Kalidindi*

Main category: cs.LG

TL;DR: 提出了一种基于物理驱动的数据增强方法，用于解决材料科学中数据稀疏问题，并构建了首个多晶材料基础模型PolyMicros。


<details>
  <summary>Details</summary>
Motivation: 解决材料科学中因数据稀缺（如仅有少量实验观测）而难以构建大规模数据集的问题。

Method: 采用局部生成模型集成和多样性管理策略，通过物理驱动的数据增强生成大规模数据集。

Result: 成功构建了PolyMicros模型，并在零样本条件下解决了3D实验显微镜加速的长期挑战。

Conclusion: 该方法为材料科学中的稀疏数据问题提供了有效解决方案，并开源了模型和数据集。

Abstract: Recent advances in Foundation Models for Materials Science are poised to
revolutionize the discovery, manufacture, and design of novel materials with
tailored properties and responses. Although great strides have been made,
successes have been restricted to materials classes where multi-million sample
data repositories can be readily curated (e.g., atomistic structures).
Unfortunately, for many structural and functional materials (e.g., mesoscale
structured metal alloys), such datasets are too costly or prohibitive to
construct; instead, datasets are limited to very few examples. To address this
challenge, we introduce a novel machine learning approach for learning from
hyper-sparse, complex spatial data in scientific domains. Our core contribution
is a physics-driven data augmentation scheme that leverages an ensemble of
local generative models, trained on as few as five experimental observations,
and coordinates them through a novel diversity curation strategy to generate a
large-scale, physically diverse dataset. We utilize this framework to construct
PolyMicros, the first Foundation Model for polycrystalline materials (a
structural material class important across a broad range of industrial and
scientific applications). We demonstrate the utility of PolyMicros by zero-shot
solving several long standing challenges related to accelerating 3D
experimental microscopy. Finally, we make both our models and datasets openly
available to the community.

</details>


### [126] [xInv: Explainable Optimization of Inverse Problems](https://arxiv.org/abs/2506.11056)
*Sean Memery,Kevin Denamganai,Anna Kapron-King,Kartic Subr*

Main category: cs.LG

TL;DR: 提出一种方法，通过优化器生成的轨迹生成人类可理解的解释，利用可微分模拟器和语言模型。


<details>
  <summary>Details</summary>
Motivation: 逆问题的迭代优化过程对领域专家来说难以理解，需要一种可解释的方法。

Method: 通过可微分模拟器在正向和反向传递中生成自然语言事件，并用语言模型生成解释。

Result: 方法在优化问题和神经网络训练示例中验证有效。

Conclusion: 该方法为逆问题提供了可解释的优化过程，有助于领域专家理解。

Abstract: Inverse problems are central to a wide range of fields, including healthcare,
climate science, and agriculture. They involve the estimation of inputs,
typically via iterative optimization, to some known forward model so that it
produces a desired outcome. Despite considerable development in the
explainability and interpretability of forward models, the iterative
optimization of inverse problems remains largely cryptic to domain experts. We
propose a methodology to produce explanations, from traces produced by an
optimizer, that are interpretable by humans at the abstraction of the domain.
The central idea in our approach is to instrument a differentiable simulator so
that it emits natural language events during its forward and backward passes.
In a post-process, we use a Language Model to create an explanation from the
list of events. We demonstrate the effectiveness of our approach with an
illustrative optimization problem and an example involving the training of a
neural network.

</details>


### [127] [STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization](https://arxiv.org/abs/2506.11057)
*Xijun Li,Jiexiang Yang,Jinghao Wang,Bo Peng,Jianguo Yao,Haibing Guan*

Main category: cs.LG

TL;DR: 本文提出了一种名为STRCMP的结构感知LLM框架，通过结合图神经网络和大型语言模型，显著提升了组合优化问题的求解质量和效率。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题因其NP难特性带来巨大计算挑战，现有基于LLM的方法常忽略问题的结构先验，导致求解效果不佳。本文受人类专家利用结构设计算法的启发，提出新框架以解决这一问题。

Method: STRCMP结合GNN提取结构嵌入，并利用LLM生成求解器专用代码，通过进化优化迭代改进算法。

Result: 在混合整数线性规划和布尔可满足性问题上的实验表明，STRCMP在求解质量和效率上大幅优于现有五种方法。

Conclusion: STRCMP通过整合结构先验和LLM，显著提升了组合优化问题的求解性能，代码和模型将公开。

Abstract: Combinatorial optimization (CO) problems, central to operation research and
theoretical computer science, present significant computational challenges due
to their NP-hard nature. While large language models (LLMs) have emerged as
promising tools for CO--either by directly generating solutions or synthesizing
solver-specific codes--existing approaches often neglect critical structural
priors inherent to CO problems, leading to suboptimality and iterative
inefficiency. Inspired by human experts' success in leveraging CO structures
for algorithm design, we propose STRCMP, a novel structure-aware LLM-based
algorithm discovery framework that systematically integrates structure priors
to enhance solution quality and solving efficiency. Our framework combines a
graph neural network (GNN) for extracting structural embeddings from CO
instances with an LLM conditioned on these embeddings to identify
high-performing algorithms in the form of solver-specific codes. This composite
architecture ensures syntactic correctness, preserves problem topology, and
aligns with natural language objectives, while an evolutionary refinement
process iteratively optimizes generated algorithm. Extensive evaluations across
Mixed Integer Linear Programming and Boolean Satisfiability problems, using
nine benchmark datasets, demonstrate that our proposed STRCMP outperforms five
strong neural and LLM-based methods by a large margin, in terms of both
solution optimality and computational efficiency. The code and learned model
will be publicly available upon the acceptance of the paper.

</details>


### [128] [ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models](https://arxiv.org/abs/2506.11087)
*Boya Xiong,Shuo Wang,Weifeng Ge,Guanhua Chen,Yun Chen*

Main category: cs.LG

TL;DR: ADAMIX是一种自适应混合精度delta压缩框架，通过数学推导优化比特分配，显著提升压缩性能。


<details>
  <summary>Details</summary>
Motivation: 在多租户服务中，大量基于同一基础模型的LLMs需要高效压缩，但现有方法在高压缩比下性能不足或依赖经验性比特分配。

Method: 提出ADAMIX框架，通过数学推导量化误差，将最优比特分配问题转化为0/1整数线性规划问题。

Result: 在AIME2024和GQA任务上，ADAMIX分别比最佳基线Delta-CoMe提升22.3%和6.1%。

Conclusion: ADAMIX通过自适应混合精度策略，显著提高了delta压缩的性能和效率。

Abstract: Large language models (LLMs) achieve impressive performance on various
knowledge-intensive and complex reasoning tasks in different domains. In
certain scenarios like multi-tenant serving, a large number of LLMs finetuned
from the same base model are deployed to meet complex requirements for users.
Recent works explore delta-compression approaches to quantize and compress the
delta parameters between the customized LLM and the corresponding base model.
However, existing works either exhibit unsatisfactory performance at high
compression ratios or depend on empirical bit allocation schemes. In this work,
we propose ADAMIX, an effective adaptive mixed-precision delta-compression
framework. We provide a mathematical derivation of quantization error to
motivate our mixed-precision compression strategy and formulate the optimal
mixed-precision bit allocation scheme as the solution to a 0/1 integer linear
programming problem. Our derived bit allocation strategy minimizes the
quantization error while adhering to a predefined compression ratio
requirement. Experimental results on various models and benchmarks demonstrate
that our approach surpasses the best baseline by a considerable margin. On
tasks like AIME2024 and GQA, where the norm of $\Delta \mathbf{W}$ is large and
the base model lacks sufficient ability, ADAMIX outperforms the best baseline
Delta-CoMe by 22.3% and 6.1% with 7B models, respectively.

</details>


### [129] [Debiasing Online Preference Learning via Preference Feature Preservation](https://arxiv.org/abs/2506.11098)
*Dongyoung Kim,Jinsung Yoon,Jinwoo Shin,Jaehyung Kim*

Main category: cs.LG

TL;DR: PFP框架通过保留人类偏好特征的分布，解决了LLM在线偏好学习中的偏见问题，并在标准基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习框架简化人类偏好为二元比较和标量奖励，导致LLM响应偏向最受欢迎的特征，尤其在在线学习迭代中加剧。

Method: PFP从离线人类偏好数据中提取特征并训练分类器，通过分布保留优化为在线学习中的新输入指令映射偏好特征，最后结合偏好特征训练LLM。

Result: 实验表明，PFP成功减少了在线学习中的偏好特征偏见，在LLM对齐标准基准测试中优于现有方法。

Conclusion: PFP通过保留和利用人类偏好特征的丰富信号，有效解决了LLM偏好学习中的偏见问题。

Abstract: Recent preference learning frameworks for large language models (LLMs)
simplify human preferences with binary pairwise comparisons and scalar rewards.
This simplification could make LLMs' responses biased to mostly preferred
features, and would be exacerbated during the iterations of online preference
learning steps. To address these challenges, we propose a novel framework
coined PFP (Preference Feature Preservation). The key idea of PFP is
maintaining the distribution of human preference features and utilizing such
rich signals throughout the online preference learning process. Specifically,
PFP first extract preference features from offline pairwise human preference
data and trains a feature classifier. Then, using trained classifier and the
distribution preserving optimization, PFP maps appropriate preference features
for a new input instruction during online learning. Lastly, PFP trains LLM
using the existing preference learning method, by incorporating the preference
feature into system prompts and enabling LLM to explicitly handle various human
preferences. Our experiments demonstrate that PFP successfully mitigates the
bias in preference features during online learning, and hence achieves superior
performance compared to previous preference learning methods on standard
benchmarks to evaluate LLM alignment.

</details>


### [130] [Knowledge Graph Embeddings with Representing Relations as Annular Sectors](https://arxiv.org/abs/2506.11099)
*Huiling Zhu,Yingqi Zeng*

Main category: cs.LG

TL;DR: SectorE是一种新的基于极坐标的知识图谱嵌入模型，通过环形扇形建模关系，结合模和相位捕捉推理模式和关系属性，显著提升语义层次建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有区域嵌入模型通常忽略实体间的语义层次结构，导致知识图谱补全任务效果受限。

Method: 提出SectorE模型，将关系建模为环形扇形，实体嵌入为扇形内的点，以极坐标形式结合模和相位捕捉语义层次。

Result: 在FB15k-237、WN18RR和YAGO3-10数据集上，SectorE表现优异，尤其在语义建模能力上具有竞争力。

Conclusion: SectorE通过极坐标嵌入有效解决了语义层次建模问题，为知识图谱补全任务提供了新思路。

Abstract: Knowledge graphs (KGs), structured as multi-relational data of entities and
relations, are vital for tasks like data analysis and recommendation systems.
Knowledge graph completion (KGC), or link prediction, addresses incompleteness
of KGs by inferring missing triples (h, r, t). It is vital for downstream
applications. Region-based embedding models usually embed entities as points
and relations as geometric regions to accomplish the task. Despite progress,
these models often overlook semantic hierarchies inherent in entities. To solve
this problem, we propose SectorE, a novel embedding model in polar coordinates.
Relations are modeled as annular sectors, combining modulus and phase to
capture inference patterns and relation attributes. Entities are embedded as
points within these sectors, intuitively encoding hierarchical structure.
Evaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive
performance against various kinds of models, demonstrating strengths in
semantic modeling capability.

</details>


### [131] [PromptTSS: A Prompting-Based Approach for Interactive Multi-Granularity Time Series Segmentation](https://arxiv.org/abs/2506.11170)
*Ching Chang,Ming-Chih Lo,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.LG

TL;DR: PromptTSS是一个新颖的时间序列分割框架，通过统一模型和提示机制解决多粒度状态分割问题，显著提高了分割精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列数据在不同粒度上表现出状态，现有方法无法统一处理多粒度且适应性不足。

Method: 提出PromptTSS框架，利用标签和边界信息的提示机制，统一捕捉粗粒度和细粒度模式，并动态适应新模式。

Result: 实验显示PromptTSS在多粒度分割中准确率提升24.49%，单粒度分割提升17.88%，迁移学习性能提升高达599.24%。

Conclusion: PromptTSS能有效处理多粒度状态分割，并适应动态环境中的新模式。

Abstract: Multivariate time series data, collected across various fields such as
manufacturing and wearable technology, exhibit states at multiple levels of
granularity, from coarse-grained system behaviors to fine-grained, detailed
events. Effectively segmenting and integrating states across these different
granularities is crucial for tasks like predictive maintenance and performance
optimization. However, existing time series segmentation methods face two key
challenges: (1) the inability to handle multiple levels of granularity within a
unified model, and (2) limited adaptability to new, evolving patterns in
dynamic environments. To address these challenges, we propose PromptTSS, a
novel framework for time series segmentation with multi-granularity states.
PromptTSS uses a unified model with a prompting mechanism that leverages label
and boundary information to guide segmentation, capturing both coarse- and
fine-grained patterns while adapting dynamically to unseen patterns.
Experiments show PromptTSS improves accuracy by 24.49% in multi-granularity
segmentation, 17.88% in single-granularity segmentation, and up to 599.24% in
transfer learning, demonstrating its adaptability to hierarchical states and
evolving time series dynamics.

</details>


### [132] [Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning](https://arxiv.org/abs/2506.11172)
*Xue Zhou,Dapeng Man,Chen Xu,Fanyi Zeng,Tao Liu,Huan Wang,Shucheng He,Chaoyang Gao,Wu Yang*

Main category: cs.LG

TL;DR: 论文提出了一种针对离线强化学习的序列级覆盖性量化方法，并揭示了其估计误差上界的指数放大效应。基于此，提出了一种数据覆盖性毒化攻击（CSDPC），实验表明仅毒化1%数据即可使性能下降90%。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了数据覆盖不足的安全风险，且单步分析不符合离线RL的多步决策特性。

Method: 引入序列级覆盖系数量化覆盖性，提出CSDPC攻击，通过毒化稀有决策模式降低覆盖性。

Result: 毒化1%数据可使性能下降90%。

Conclusion: 研究为离线RL的安全分析和防护提供了新视角。

Abstract: Offline reinforcement learning (RL) heavily relies on the coverage of
pre-collected data over the target policy's distribution. Existing studies aim
to improve data-policy coverage to mitigate distributional shifts, but overlook
security risks from insufficient coverage, and the single-step analysis is not
consistent with the multi-step decision-making nature of offline RL. To address
this, we introduce the sequence-level concentrability coefficient to quantify
coverage, and reveal its exponential amplification on the upper bound of
estimation errors through theoretical analysis. Building on this, we propose
the Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack.
Considering the continuous nature of offline RL data, we convert state-action
pairs into decision units, and extract representative decision patterns that
capture multi-step behavior. We identify rare patterns likely to cause
insufficient coverage, and poison them to reduce coverage and exacerbate
distributional shifts. Experiments show that poisoning just 1% of the dataset
can degrade agent performance by 90%. This finding provides new perspectives
for analyzing and safeguarding the security of offline RL.

</details>


### [133] [Detection of obstructions in oil and gas pipelines: machine learning techniques for hydrate classification](https://arxiv.org/abs/2506.11220)
*Hellockston Gomes de Brito,Carla Wilza Souza de Paula Maitelli,Osvaldo Chiavone-Filho*

Main category: cs.LG

TL;DR: 该研究利用监督机器学习技术（决策树、k-NN和朴素贝叶斯分类器）检测和缓解油气生产中的流动保障问题，特别是水合物形成，决策树算法表现最佳（准确率99.99%）。


<details>
  <summary>Details</summary>
Motivation: 油气生产和运输中常因沉积物堆积、蜡沉积、矿物结垢和腐蚀等问题导致管道堵塞，影响效率。研究旨在通过机器学习解决这些问题，优化生产。

Method: 使用公开的Petrobras数据集，通过数据预处理和清理后，应用决策树、k-NN和朴素贝叶斯分类器进行水合物形成分类，使用scikit-learn库实现。

Result: 决策树算法在分类水合物形成时表现最佳，准确率达99.99%。

Conclusion: 该方法为油气生产中的流动保障问题提供了高效解决方案，显著提升生产效率。

Abstract: Oil and gas reserves are vital resources for the global economy, serving as
key components in transportation, energy production, and industrial processes.
However, oil and gas extraction and production operations may encounter several
challenges, such as pipeline and production line blockages, caused by factors
including sediment accumulation, wax deposition, mineral scaling, and
corrosion. This study addresses these challenges by employing supervised
machine learning techniques, specifically decision trees, the k-Nearest
Neighbors (k-NN) algorithm (k-NN), and the Naive Bayes classifier method, to
detect and mitigate flow assurance challenges, ensuring efficient fluid
transport. The primary focus is on preventing gas hydrate formation in oil
production systems. To achieve this, data preprocessing and cleaning were
conducted to ensure the quality and consistency of the dataset, which was
sourced from Petrobras publicly available 3W project repository on GitHub. The
scikit-learn Python library, a widely recognized open-source tool for
supervised machine learning techniques, was utilized for classification tasks
due to its robustness and versatility. The results demonstrate that the
proposed methodology effectively classifies hydrate formation under operational
conditions, with the decision tree algorithm exhibiting the highest predictive
accuracy (99.99 percent). Consequently, this approach provides a reliable
solution for optimizing production efficiency.

</details>


### [134] [uPVC-Net: A Universal Premature Ventricular Contraction Detection Deep Learning Algorithm](https://arxiv.org/abs/2506.11238)
*Hagai Hamami,Yosef Solewicz,Daniel Zur,Yonatan Kleerekoper,Joachim A. Behar*

Main category: cs.LG

TL;DR: uPVC-Net是一种通用深度学习模型，用于从单导联心电图中检测PVCs，表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于导联放置、记录条件和人口统计学的差异，PVCs的准确检测具有挑战性。

Method: 开发了uPVC-Net，采用多源多导联训练策略，并在四个独立ECG数据集上训练。

Result: 在保留数据集上AUC达到97.8%至99.1%，在可穿戴设备数据上达到99.1%。

Conclusion: uPVC-Net在多样导联配置和人群中表现优异，适合临床部署。

Abstract: Introduction: Premature Ventricular Contractions (PVCs) are common cardiac
arrhythmias originating from the ventricles. Accurate detection remains
challenging due to variability in electrocardiogram (ECG) waveforms caused by
differences in lead placement, recording conditions, and population
demographics. Methods: We developed uPVC-Net, a universal deep learning model
to detect PVCs from any single-lead ECG recordings. The model is developed on
four independent ECG datasets comprising a total of 8.3 million beats collected
from Holter monitors and a modern wearable ECG patch. uPVC-Net employs a custom
architecture and a multi-source, multi-lead training strategy. For each
experiment, one dataset is held out to evaluate out-of-distribution (OOD)
generalization. Results: uPVC-Net achieved an AUC between 97.8% and 99.1% on
the held-out datasets. Notably, performance on wearable single-lead ECG data
reached an AUC of 99.1%. Conclusion: uPVC-Net exhibits strong generalization
across diverse lead configurations and populations, highlighting its potential
for robust, real-world clinical deployment.

</details>


### [135] [A Causal Lens for Learning Long-term Fair Policies](https://arxiv.org/abs/2506.11242)
*Jacob Lear,Lu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种在动态决策系统中考虑长期公平性的强化学习框架，通过因果分解分析公平性，并开发了一种平衡多种公平性概念的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态环境中的即时公平性，而忽视了动态系统中的长期公平性。本文旨在填补这一空白，同时满足即时公平性需求。

Method: 提出一个强化学习框架，通过因果分解将长期公平性指标分为直接效应、延迟效应和虚假效应，并分析其与“收益公平性”的关系。

Result: 开发了一种简单有效的方法，能够平衡多种公平性概念，同时满足即时和长期公平性需求。

Conclusion: 本文强调了动态系统中长期公平性的重要性，并通过因果分析提供了一种可行的解决方案，为公平性研究提供了新视角。

Abstract: Fairness-aware learning studies the development of algorithms that avoid
discriminatory decision outcomes despite biased training data. While most
studies have concentrated on immediate bias in static contexts, this paper
highlights the importance of investigating long-term fairness in dynamic
decision-making systems while simultaneously considering instantaneous fairness
requirements. In the context of reinforcement learning, we propose a general
framework where long-term fairness is measured by the difference in the average
expected qualification gain that individuals from different groups could
obtain.Then, through a causal lens, we decompose this metric into three
components that represent the direct impact, the delayed impact, as well as the
spurious effect the policy has on the qualification gain. We analyze the
intrinsic connection between these components and an emerging fairness notion
called benefit fairness that aims to control the equity of outcomes in
decision-making. Finally, we develop a simple yet effective approach for
balancing various fairness notions.

</details>


### [136] [Can Time-Series Foundation Models Perform Building Energy Management Tasks?](https://arxiv.org/abs/2506.11250)
*Ozan Baris Mulayim,Pengrui Quan,Liying Han,Xiaomin Ouyang,Dezhi Hong,Mario Bergés,Mani Srivastava*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）在建筑能源管理（BEM）中的泛化能力有限，表现仅略优于统计模型，且在复杂环境中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有BEM解决方案依赖特定任务和数据的模型，限制了广泛适用性。TSFMs有望通过类似大型语言模型的泛化能力解决这一问题。

Method: 评估TSFMs在四个维度：零样本单变量预测、协变量预测、零样本表示学习和鲁棒性。

Result: TSFMs在零样本表示学习中表现良好，但在预测任务中表现不佳，且对评估指标敏感。

Conclusion: TSFMs需要改进协变量处理和上下文整合，以实现更适应和可扩展的BEM解决方案。

Abstract: Building energy management (BEM) tasks require processing and learning from a
variety of time-series data. Existing solutions rely on bespoke task- and
data-specific models to perform these tasks, limiting their broader
applicability. Inspired by the transformative success of Large Language Models
(LLMs), Time-Series Foundation Models (TSFMs), trained on diverse datasets,
have the potential to change this. Were TSFMs to achieve a level of
generalizability across tasks and contexts akin to LLMs, they could
fundamentally address the scalability challenges pervasive in BEM. To
understand where they stand today, we evaluate TSFMs across four dimensions:
(1) generalizability in zero-shot univariate forecasting, (2) forecasting with
covariates for thermal behavior modeling, (3) zero-shot representation learning
for classification tasks, and (4) robustness to performance metrics and varying
operational conditions. Our results reveal that TSFMs exhibit \emph{limited}
generalizability, performing only marginally better than statistical models on
unseen datasets and modalities for univariate forecasting. Similarly, inclusion
of covariates in TSFMs does not yield performance improvements, and their
performance remains inferior to conventional models that utilize covariates.
While TSFMs generate effective zero-shot representations for downstream
classification tasks, they may remain inferior to statistical models in
forecasting when statistical models perform test-time fitting. Moreover, TSFMs
forecasting performance is sensitive to evaluation metrics, and they struggle
in more complex building environments compared to statistical models. These
findings underscore the need for targeted advancements in TSFM design,
particularly their handling of covariates and incorporating context and
temporal dynamics into prediction mechanisms, to develop more adaptable and
scalable solutions for BEM.

</details>


### [137] [Domain-Constrained Diffusion Models to Synthesize Tabular Data: A Case Study in Power Systems](https://arxiv.org/abs/2506.11281)
*Milad Hoseinpour,Vladimir Dvorkin*

Main category: cs.LG

TL;DR: 提出了一种基于引导扩散模型的合成数据方法，结合领域知识生成高质量数据，应用于电力系统等领域。


<details>
  <summary>Details</summary>
Motivation: 隐私、安全和法律障碍推动了对合成数据的需求，尤其是在医疗、金融和能源领域。生成模型虽有望解决这些问题，但其效果依赖于领域知识的融入。

Method: 使用引导扩散模型，将领域约束（如基尔霍夫定律）通过梯度引导直接融入生成过程，生成统计代表性和高保真的电力流数据。

Result: 数值结果表明该方法有效。

Conclusion: 该方法成功结合领域知识生成高质量合成数据，适用于电力系统及其他表格数据领域。

Abstract: Growing concerns over privacy, security, and legal barriers are driving the
rising demand for synthetic data across domains such as healthcare, finance,
and energy. While generative models offer a promising solution to overcome
these barriers, their utility depends on the incorporation of domain-specific
knowledge. We propose to synthesize data using a guided diffusion model that
integrates domain constraints directly into the generative process. We develop
the model in the context of power systems, with potential applicability to
other domains that involve tabular data. Specifically, we synthesize
statistically representative and high-fidelity power flow datasets. To satisfy
domain constraints, e.g., Kirchhoff laws, we introduce a gradient-based
guidance to steer the sampling trajectory in a feasible direction. Numerical
results demonstrate the effectiveness of our approach.

</details>


### [138] [Sampling Imbalanced Data with Multi-objective Bilevel Optimization](https://arxiv.org/abs/2506.11315)
*Karen Medlin,Sven Leyffer,Krishnan Raghavan*

Main category: cs.LG

TL;DR: 论文提出了一种多目标优化框架MOODS和新的验证指标，用于解决不平衡数据分类问题，显著提升了少数类的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理不平衡数据时容易过拟合且未考虑数据多样性，缺乏衡量不平衡影响的指标。

Method: 提出MOODS框架，结合多目标优化指导过采样和欠采样；引入ε/δ非重叠多样性指标评估采样方法。

Result: 实验表明，该方法在多样性驱动下，F1分数提升了1-15%。

Conclusion: MOODS框架和新指标有效解决了不平衡数据分类问题，提升了模型性能。

Abstract: Two-class classification problems are often characterized by an imbalance
between the number of majority and minority datapoints resulting in poor
classification of the minority class in particular. Traditional approaches,
such as reweighting the loss function or na\"ive resampling, risk overfitting
and subsequently fail to improve classification because they do not consider
the diversity between majority and minority datasets. Such consideration is
infeasible because there is no metric that can measure the impact of imbalance
on the model. To obviate these challenges, we make two key contributions.
First, we introduce MOODS~(Multi-Objective Optimization for Data Sampling), a
novel multi-objective bilevel optimization framework that guides both synthetic
oversampling and majority undersampling. Second, we introduce a validation
metric -- `$\epsilon/ \delta$ non-overlapping diversification metric' -- that
quantifies the goodness of a sampling method towards model performance. With
this metric we experimentally demonstrate state-of-the-art performance with
improvement in diversity driving a $1-15 \%$ increase in $F1$ scores.

</details>


### [139] [An Attention-based Spatio-Temporal Neural Operator for Evolving Physics](https://arxiv.org/abs/2506.11328)
*Vispi Karkaria,Doksoo Lee,Yi-Ping Chen,Yue Yu,Wei Chen*

Main category: cs.LG

TL;DR: ASNO是一种结合时空注意力机制的新架构，用于解决科学机器学习中未知物理过程的预测问题，具有更好的泛化能力和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在捕捉长程时空交互方面表现优异，但缺乏物理可解释性，且难以适应多变的环境条件。

Method: 提出ASNO架构，结合可分离的时空注意力机制，并利用BDF方法学习时间预测和外推，同时通过注意力神经算子处理变化的外部负载。

Result: 在SciML基准测试中，ASNO表现优于现有模型，展示了其在工程应用、物理发现和可解释机器学习中的潜力。

Conclusion: ASNO通过增强物理可解释性和泛化能力，为科学机器学习中的未知物理过程预测提供了有效解决方案。

Abstract: In scientific machine learning (SciML), a key challenge is learning unknown,
evolving physical processes and making predictions across spatio-temporal
scales. For example, in real-world manufacturing problems like additive
manufacturing, users adjust known machine settings while unknown environmental
parameters simultaneously fluctuate. To make reliable predictions, it is
desired for a model to not only capture long-range spatio-temporal interactions
from data but also adapt to new and unknown environments; traditional machine
learning models excel at the first task but often lack physical
interpretability and struggle to generalize under varying environmental
conditions. To tackle these challenges, we propose the Attention-based
Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines
separable attention mechanisms for spatial and temporal interactions and adapts
to unseen physical parameters. Inspired by the backward differentiation formula
(BDF), ASNO learns a transformer for temporal prediction and extrapolation and
an attention-based neural operator for handling varying external loads,
enhancing interpretability by isolating historical state contributions and
external forces, enabling the discovery of underlying physical laws and
generalizability to unseen physical environments. Empirical results on SciML
benchmarks demonstrate that ASNO outperforms over existing models, establishing
its potential for engineering applications, physics discovery, and
interpretable machine learning.

</details>


### [140] [The Sample Complexity of Parameter-Free Stochastic Convex Optimization](https://arxiv.org/abs/2506.11336)
*Jared Lawrence,Ari Kalinsky,Hannah Bradfield,Yair Carmon,Oliver Hinder*

Main category: cs.LG

TL;DR: 论文研究了在问题参数未知时的随机凸优化样本复杂度，提出了两种策略：可靠的模型选择方法和基于正则化的方法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究在问题参数（如最优距离）未知时，如何优化随机凸优化的样本复杂度。

Method: 1. 开发可靠的模型选择方法，避免验证集过拟合；2. 针对仅最优距离未知的情况，开发基于正则化的方法。

Result: 两种方法结合可同时适应多种问题结构，实验表明模型选择方法能有效减少小验证集过拟合。

Conclusion: 提出的方法在参数未知时仍能接近最优样本复杂度，并展示了样本与计算复杂度的分离。

Abstract: We study the sample complexity of stochastic convex optimization when problem
parameters, e.g., the distance to optimality, are unknown. We pursue two
strategies. First, we develop a reliable model selection method that avoids
overfitting the validation set. This method allows us to generically tune the
learning rate of stochastic optimization methods to match the optimal
known-parameter sample complexity up to $\log\log$ factors. Second, we develop
a regularization-based method that is specialized to the case that only the
distance to optimality is unknown. This method provides perfect adaptability to
unknown distance to optimality, demonstrating a separation between the sample
and computational complexity of parameter-free stochastic convex optimization.
Combining these two methods allows us to simultaneously adapt to multiple
problem structures.
  Experiments performing few-shot learning on CIFAR-10 by fine-tuning CLIP
models and prompt engineering Gemini to count shapes indicate that our reliable
model selection method can help mitigate overfitting to small validation sets.

</details>


### [141] [Improving Group Robustness on Spurious Correlation via Evidential Alignment](https://arxiv.org/abs/2506.11347)
*Wenqian Ye,Guangtao Zheng,Aidong Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为Evidential Alignment的新框架，通过不确定性量化识别和抑制虚假相关性，无需依赖外部标注。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常依赖虚假相关性（如背景特征与目标的关联），影响模型的泛化能力和可信度。现有方法需额外标注或辅助模型，成本高且效果有限。

Method: 利用二阶风险最小化量化模型预测的证据，并通过提出的证据校准技术校准模型，识别并抑制虚假相关性。

Result: 实验表明，该方法显著提升了模型的群体鲁棒性，适用于多种架构和数据模态。

Conclusion: Evidential Alignment为虚假相关性提供了一种可扩展且原理性的解决方案，无需依赖外部标注。

Abstract: Deep neural networks often learn and rely on spurious correlations, i.e.,
superficial associations between non-causal features and the targets. For
instance, an image classifier may identify camels based on the desert
backgrounds. While it can yield high overall accuracy during training, it
degrades generalization on more diverse scenarios where such correlations do
not hold. This problem poses significant challenges for out-of-distribution
robustness and trustworthiness. Existing methods typically mitigate this issue
by using external group annotations or auxiliary deterministic models to learn
unbiased representations. However, such information is costly to obtain, and
deterministic models may fail to capture the full spectrum of biases learned by
the models. To address these limitations, we propose Evidential Alignment, a
novel framework that leverages uncertainty quantification to understand the
behavior of the biased models without requiring group annotations. By
quantifying the evidence of model prediction with second-order risk
minimization and calibrating the biased models with the proposed evidential
calibration technique, Evidential Alignment identifies and suppresses spurious
correlations while preserving core features. We theoretically justify the
effectiveness of our method as capable of learning the patterns of biased
models and debiasing the model without requiring any spurious correlation
annotations. Empirical results demonstrate that our method significantly
improves group robustness across diverse architectures and data modalities,
providing a scalable and principled solution to spurious correlations.

</details>


### [142] [Generalization Bound of Gradient Flow through Training Trajectory and Data-dependent Kernel](https://arxiv.org/abs/2506.11357)
*Yilan Chen,Zhichao Wang,Wei Huang,Andi Han,Taiji Suzuki,Arya Mazumdar*

Main category: cs.LG

TL;DR: 本文通过损失路径核（LPK）为梯度流建立了与经典Rademacher复杂度边界一致的理论泛化边界，揭示了训练损失梯度范数对泛化性能的影响。


<details>
  <summary>Details</summary>
Motivation: 梯度优化方法在实证中表现优异，但其理论泛化性质尚未完全理解，本文旨在填补这一空白。

Method: 结合梯度流的稳定性分析与Rademacher复杂度的均匀收敛性，利用动态的LPK核捕捉训练轨迹。

Result: LPK核比静态核（如NTK）提供更紧致的泛化保证，实验验证了边界与实际泛化差距的相关性。

Conclusion: 本文不仅恢复了过参数化神经网络的核回归边界，还展示了神经网络相比核方法的特征学习能力。

Abstract: Gradient-based optimization methods have shown remarkable empirical success,
yet their theoretical generalization properties remain only partially
understood. In this paper, we establish a generalization bound for gradient
flow that aligns with the classical Rademacher complexity bounds for kernel
methods-specifically those based on the RKHS norm and kernel trace-through a
data-dependent kernel called the loss path kernel (LPK). Unlike static kernels
such as NTK, the LPK captures the entire training trajectory, adapting to both
data and optimization dynamics, leading to tighter and more informative
generalization guarantees. Moreover, the bound highlights how the norm of the
training loss gradients along the optimization trajectory influences the final
generalization performance. The key technical ingredients in our proof combine
stability analysis of gradient flow with uniform convergence via Rademacher
complexity. Our bound recovers existing kernel regression bounds for
overparameterized neural networks and shows the feature learning capability of
neural networks compared to kernel methods. Numerical experiments on real-world
datasets validate that our bounds correlate well with the true generalization
gap.

</details>


### [143] [EDN: A Novel Edge-Dependent Noise Model for Graph Data](https://arxiv.org/abs/2506.11368)
*Pintu Kumar,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 论文提出了一种新的边依赖噪声（EDN）模型，解决了现有节点标签噪声模型忽略图数据中节点关系的局限性，并通过实验验证了EDN对图神经网络和噪声鲁棒算法的性能影响。


<details>
  <summary>Details</summary>
Motivation: 现有节点标签噪声模型（如SLN和CCN）忽略了图数据中节点间的关系，而EDN模型填补了这一空白，认为真实场景中标签噪声可能受节点连接的影响。

Method: 提出了三种EDN变体，研究了节点度对标签噪声概率的影响，并在不同图数据集上测试了5种GNN架构和8种噪声鲁棒算法。

Result: 实验表明，两种EDN变体对GNN和噪声鲁棒算法的性能退化影响更大，且通过统计假设检验验证了这一结果。

Conclusion: EDN模型的引入对于评估图数据噪声鲁棒算法至关重要，有助于提升噪声环境下图学习的可靠性。

Abstract: An important structural feature of a graph is its set of edges, as it
captures the relationships among the nodes (the graph's topology). Existing
node label noise models like Symmetric Label Noise (SLN) and Class Conditional
Noise (CCN) disregard this important node relationship in graph data; and the
Edge-Dependent Noise (EDN) model addresses this limitation. EDN posits that in
real-world scenarios, label noise may be influenced by the connections between
nodes. We explore three variants of EDN. A crucial notion that relates nodes
and edges in a graph is the degree of a node; we show that in all three
variants, the probability of a node's label corruption is dependent on its
degree. Additionally, we compare the dependence of these probabilities on node
degree across different variants. We performed experiments on popular graph
datasets using 5 different GNN architectures and 8 noise robust algorithms for
graph data. The results demonstrate that 2 variants of EDN lead to greater
performance degradation in both Graph Neural Networks (GNNs) and existing
noise-robust algorithms, as compared to traditional node label noise models. We
statistically verify this by posing a suitable hypothesis-testing problem. This
emphasizes the importance of incorporating EDN when evaluating noise robust
algorithms for graphs, to enhance the reliability of graph-based learning in
noisy environments.

</details>


### [144] [The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL Divergence Analysis](https://arxiv.org/abs/2506.11378)
*Bernardo P. Schaeffer,Ricardo M. S. Rosa,Glauco Valle*

Main category: cs.LG

TL;DR: 研究了基于分数的扩散模型中随机性对生成过程的影响，通过KL散度界限和数值实验验证了随机性在误差校正和分数误差放大之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 探讨随机性参数在基于分数的扩散模型中对生成过程的影响，特别是误差传播和校正机制。

Method: 使用前向SDE的边际对数-索博列夫不等式，推导KL散度衰减的理论界限，并通过数值和解析例子验证。

Result: 随机性在精确分数函数下可校正误差，但在近似分数函数下存在误差校正与放大的权衡。

Conclusion: 随机性的选择需根据分数误差结构权衡，理论结果通过实验验证。

Abstract: Sampling in score-based diffusion models can be performed by solving either a
probability flow ODE or a reverse-time stochastic differential equation (SDE)
parameterized by an arbitrary stochasticity parameter. In this work, we study
the effect of stochasticity on the generation process through bounds on the
Kullback-Leibler (KL) divergence and complement the analysis with numerical and
analytical examples. Our results apply to general forward SDEs with additive
noise and Lipschitz-continuous score functions, and quantify how errors from
the prior distribution and score approximation propagate under different
choices of the stochasticity parameter. The theoretical bounds are derived
using log-Sobolev inequalities for the marginals of the forward process, which
enable a more effective control of the KL divergence decay along sampling. For
exact score functions, we find that stochasticity acts as an error-correcting
mechanism, decreasing KL divergence along the sampling trajectory. For an
approximate score function, there is a trade-off between error correction and
score error amplification, so that stochasticity can either improve or worsen
the performance, depending on the structure of the score error. Numerical
experiments on simple datasets and a fully analytical example are included to
illustrate and enlighten the theoretical results.

</details>


### [145] [FIGNN: Feature-Specific Interpretability for Graph Neural Network Surrogate Models](https://arxiv.org/abs/2506.11398)
*Riddhiman Raut,Romit Maulik,Shivam Barwey*

Main category: cs.LG

TL;DR: 提出了一种新型图神经网络FIGNN，通过特征特定池化策略和掩码正则化，提升科学应用中非结构化网格深度学习模型的解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在多变量预测任务中难以区分不同特征的空间影响，FIGNN旨在解决这一问题。

Method: 引入特征特定池化策略和掩码正则化，独立分配空间重要性并优化解释性与预测误差的对齐。

Result: 在SPEEDY和BFS两个物理系统中，FIGNN表现出竞争力，并揭示了各特征的物理意义空间模式。

Conclusion: FIGNN是一种通用框架，适用于复杂物理领域的可解释替代建模。

Abstract: This work presents a novel graph neural network (GNN) architecture, the
Feature-specific Interpretable Graph Neural Network (FIGNN), designed to
enhance the interpretability of deep learning surrogate models defined on
unstructured grids in scientific applications. Traditional GNNs often obscure
the distinct spatial influences of different features in multivariate
prediction tasks. FIGNN addresses this limitation by introducing a
feature-specific pooling strategy, which enables independent attribution of
spatial importance for each predicted variable. Additionally, a mask-based
regularization term is incorporated into the training objective to explicitly
encourage alignment between interpretability and predictive error, promoting
localized attribution of model performance. The method is evaluated for
surrogate modeling of two physically distinct systems: the SPEEDY atmospheric
circulation model and the backward-facing step (BFS) fluid dynamics benchmark.
Results demonstrate that FIGNN achieves competitive predictive performance
while revealing physically meaningful spatial patterns unique to each feature.
Analysis of rollout stability, feature-wise error budgets, and spatial mask
overlays confirm the utility of FIGNN as a general-purpose framework for
interpretable surrogate modeling in complex physical domains.

</details>


### [146] [LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model](https://arxiv.org/abs/2506.11402)
*Pradyut Sekhsaria,Marcel Mateos Salles,Hai Huang,Randall Balestriero*

Main category: cs.LG

TL;DR: PEFT（如LoRA）在资源高效微调预训练大模型时，可能因依赖虚假标记（SSTI）导致模型行为被操控。研究发现，少量虚假标记即可显著影响模型决策，且LoRA秩的大小与模型对虚假标记的依赖程度相关。


<details>
  <summary>Details</summary>
Motivation: 研究PEFT方法在微调过程中可能存在的灾难性失败，特别是模型对虚假标记的依赖问题，以及这种依赖可能被恶意利用的风险。

Method: 通过Seamless Spurious Token Injection（SSTI）技术，在不同模型家族（Snowflake Arctic、Apple OpenELM、Meta LLaMA-3）和数据集（IMDB、Financial Classification、CommonSense QA、Bias in Bios）上测试PEFT对虚假标记的依赖行为。

Result: 研究发现：1）单个虚假标记即可操控模型决策；2）LoRA秩的大小与模型对虚假标记的依赖程度成正比；3）高LoRA秩在强SSTI下能提升模型对非虚假标记的关注，增强鲁棒性。

Conclusion: PEFT方法存在对虚假标记的依赖风险，可能被恶意利用。LoRA秩的选择需权衡效率与鲁棒性，高秩在强干扰下表现更优。

Abstract: Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA),
aligns pre-trained Large Language Models (LLMs) to particular downstream tasks
in a resource-efficient manner. Because efficiency has been the main metric of
progress, very little attention has been put in understanding possible
catastrophic failures. We uncover one such failure: PEFT encourages a model to
search for shortcut solutions to solve its fine-tuning tasks. When very small
amount of tokens, e.g., one token per prompt, are correlated with downstream
task classes, PEFT makes any pretrained model rely predominantly on that token
for decision making. While such spurious tokens may emerge accidentally from
incorrect data cleaning, it also opens opportunities for malevolent parties to
control a model's behavior from Seamless Spurious Token Injection (SSTI). In
SSTI, a small amount of tokens correlated with downstream classes are injected
by the dataset creators. At test time, the finetuned LLM's behavior can be
controlled solely by injecting those few tokens. We apply SSTI across models
from three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and
four diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias
in Bios). Our findings reveal three astonishing behaviors. First, as few as a
single token of SSTI is sufficient to steer a model's decision making. Second,
for light SSTI, the reliance on spurious tokens is proportional to the LoRA
rank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable
to small rank values as it makes the model attend to non-spurious tokens, hence
improving robustness.

</details>


### [147] [Byzantine Outside, Curious Inside: Reconstructing Data Through Malicious Updates](https://arxiv.org/abs/2506.11413)
*Kai Yue,Richeng Jin,Chau-Wai Wong,Huaiyu Dai*

Main category: cs.LG

TL;DR: 论文提出了一种新的联邦学习威胁模型，即恶意好奇客户端，通过操纵梯度来推断其他客户端的私有数据，揭示了现有防御机制的盲点。


<details>
  <summary>Details</summary>
Motivation: 研究揭示联邦学习中隐私泄露的可能性，特别是服务器或客户端通过梯度重构训练数据的风险，提出新的客户端威胁模型。

Method: 定义恶意好奇客户端威胁模型，理论分析其重构能力，并开发结合梯度反演和恶意更新策略的重构算法。

Result: 实验表明，现有服务器和客户端的防御机制可能无法抵御该攻击，甚至可能加剧数据泄露，重构图像质量提升10-15%。

Conclusion: 论文揭示了联邦学习防御中的关键盲点，呼吁重新审视现有防御机制的有效性。

Abstract: Federated learning (FL) enables decentralized machine learning without
sharing raw data, allowing multiple clients to collaboratively learn a global
model. However, studies reveal that privacy leakage is possible under commonly
adopted FL protocols. In particular, a server with access to client gradients
can synthesize data resembling the clients' training data. In this paper, we
introduce a novel threat model in FL, named the maliciously curious client,
where a client manipulates its own gradients with the goal of inferring private
data from peers. This attacker uniquely exploits the strength of a Byzantine
adversary, traditionally aimed at undermining model robustness, and repurposes
it to facilitate data reconstruction attack. We begin by formally defining this
novel client-side threat model and providing a theoretical analysis that
demonstrates its ability to achieve significant reconstruction success during
FL training. To demonstrate its practical impact, we further develop a
reconstruction algorithm that combines gradient inversion with malicious update
strategies. Our analysis and experimental results reveal a critical blind spot
in FL defenses: both server-side robust aggregation and client-side privacy
mechanisms may fail against our proposed attack. Surprisingly, standard server-
and client-side defenses designed to enhance robustness or privacy may
unintentionally amplify data leakage. Compared to the baseline approach, a
mistakenly used defense may instead improve the reconstructed image quality by
10-15%.

</details>


### [148] [Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs](https://arxiv.org/abs/2506.11415)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种名为BRRA的攻击框架，通过操纵RAG系统放大语言模型的偏见，并探讨了防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注RAG系统中投毒攻击对模型输出质量的影响，而忽视了其可能放大模型偏见的潜在风险。

Method: 设计了基于多目标奖励函数的对抗性文档生成方法，利用子空间投影技术操纵检索结果，并构建循环反馈机制持续放大偏见。

Result: 实验表明，BRRA攻击能显著增强模型在多个维度的偏见，同时提出了双阶段防御机制以减轻攻击影响。

Conclusion: 研究表明RAG系统中的投毒攻击会直接放大模型输出偏见，强调了RAG系统安全性与模型公平性的关联，需关注其公平性问题。

Abstract: In Large Language Models, Retrieval-Augmented Generation (RAG) systems can
significantly enhance the performance of large language models by integrating
external knowledge. However, RAG also introduces new security risks. Existing
research focuses mainly on how poisoning attacks in RAG systems affect model
output quality, overlooking their potential to amplify model biases. For
example, when querying about domestic violence victims, a compromised RAG
system might preferentially retrieve documents depicting women as victims,
causing the model to generate outputs that perpetuate gender stereotypes even
when the original query is gender neutral. To show the impact of the bias, this
paper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which
systematically investigates attack pathways that amplify language model biases
through a RAG system manipulation. We design an adversarial document generation
method based on multi-objective reward functions, employ subspace projection
techniques to manipulate retrieval results, and construct a cyclic feedback
mechanism for continuous bias amplification. Experiments on multiple mainstream
large language models demonstrate that BRRA attacks can significantly enhance
model biases in dimensions. In addition, we explore a dual stage defense
mechanism to effectively mitigate the impacts of the attack. This study reveals
that poisoning attacks in RAG systems directly amplify model output biases and
clarifies the relationship between RAG system security and model fairness. This
novel potential attack indicates that we need to keep an eye on the fairness
issues of the RAG system.

</details>


### [149] [PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design](https://arxiv.org/abs/2506.11420)
*Zhenqiao Song,Tiaoxiao Li,Lei Li,Martin Renqiang Min*

Main category: cs.LG

TL;DR: PPDiff是一种扩散模型，用于非自回归方式设计任意蛋白质靶点的高亲和力结合蛋白，结合了序列和结构信息，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 设计高亲和力蛋白质结合蛋白是生物医学和生物技术的关键需求，但目前仍缺乏无需大量实验即可针对任意靶点设计的方法。

Method: PPDiff基于SSINC网络，整合了全局氨基酸相关性、局部3D空间相互作用和序列依赖性的建模，并通过PPBench数据集进行预训练和微调。

Result: PPDiff在预训练任务和两个下游应用中分别达到50.00%、23.16%和16.89%的成功率，显著优于基线方法。

Conclusion: PPDiff为蛋白质结合蛋白的设计提供了一种高效且通用的方法，具有广泛的应用潜力。

Abstract: Designing protein-binding proteins with high affinity is critical in
biomedical research and biotechnology. Despite recent advancements targeting
specific proteins, the ability to create high-affinity binders for arbitrary
protein targets on demand, without extensive rounds of wet-lab testing, remains
a significant challenge. Here, we introduce PPDiff, a diffusion model to
jointly design the sequence and structure of binders for arbitrary protein
targets in a non-autoregressive manner. PPDiffbuilds upon our developed
Sequence Structure Interleaving Network with Causal attention layers (SSINC),
which integrates interleaved self-attention layers to capture global amino acid
correlations, k-nearest neighbor (kNN) equivariant graph layers to model local
interactions in three-dimensional (3D) space, and causal attention layers to
simplify the intricate interdependencies within the protein sequence. To assess
PPDiff, we curate PPBench, a general protein-protein complex dataset comprising
706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on
PPBenchand finetuned on two real-world applications: target-protein mini-binder
complex design and antigen-antibody complex design. PPDiffconsistently
surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and
16.89% for the pretraining task and the two downstream applications,
respectively.

</details>


### [150] [TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision](https://arxiv.org/abs/2506.11431)
*Jinhee Kim,Seoyeon Yoon,Taeho Lee,Joo Chan Lee,Kang Eun Jeon,Jong Hwan Ko*

Main category: cs.LG

TL;DR: 论文提出了一种名为TruncQuant的新型截断训练方案，通过运行时位偏移实现灵活的位精度，解决了现有量化感知训练方案不适用于截断过程的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于最先进模型的复杂性增加，在边缘设备上部署深度神经网络具有挑战性，需要减少模型大小和推理延迟。

Method: 提出TruncQuant，一种与截断过程输出对齐的训练方案，通过位偏移实现灵活的位精度，并易于在现有量化感知框架中实现。

Result: TruncQuant展示了在不同位宽设置下的强鲁棒性，并提供了易于实现的训练方案。

Conclusion: TruncQuant为解决截断过程中的误差问题提供了一种有效的训练方案，适用于多种硬件平台。

Abstract: The deployment of deep neural networks on edge devices is a challenging task
due to the increasing complexity of state-of-the-art models, requiring efforts
to reduce model size and inference latency. Recent studies explore models
operating at diverse quantization settings to find the optimal point that
balances computational efficiency and accuracy. Truncation, an effective
approach for achieving lower bit precision mapping, enables a single model to
adapt to various hardware platforms with little to no cost. However,
formulating a training scheme for deep neural networks to withstand the
associated errors introduced by truncation remains a challenge, as the current
quantization-aware training schemes are not designed for the truncation
process. We propose TruncQuant, a novel truncation-ready training scheme
allowing flexible bit precision through bit-shifting in runtime. We achieve
this by aligning TruncQuant with the output of the truncation process,
demonstrating strong robustness across bit-width settings, and offering an
easily implementable training scheme within existing quantization-aware
frameworks. Our code is released at https://github.com/a2jinhee/TruncQuant.

</details>


### [151] [Dynamic Sparse Training of Diagonally Sparse Networks](https://arxiv.org/abs/2506.11449)
*Abhishek Tyagi,Arjun Iyer,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: DynaDiag是一种新型的结构化稀疏训练方法，通过对角线稀疏模式在训练中保持稀疏计算，并在现代硬件上实现加速。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化稀疏在现代硬件上无法实现实际加速的问题。

Method: 提出DynaDiag方法，强制对角线稀疏模式，并通过定制CUDA内核加速计算。

Result: 在ViTs中，90%稀疏线性层实现了3.13倍在线推理加速和1.59倍训练加速，同时保持模型性能。

Conclusion: DynaDiag在保持与非结构化稀疏相同性能的同时，提供了显著的硬件加速优势。

Abstract: Recent advances in Dynamic Sparse Training (DST) have pushed the frontier of
sparse neural network training in structured and unstructured contexts,
matching dense-model performance while drastically reducing parameter counts to
facilitate model scaling. However, unstructured sparsity often fails to
translate into practical speedups on modern hardware. To address this
shortcoming, we propose DynaDiag, a novel structured sparse-to-sparse DST
method that performs at par with unstructured sparsity. DynaDiag enforces a
diagonal sparsity pattern throughout training and preserves sparse computation
in forward and backward passes. We further leverage the diagonal structure to
accelerate computation via a custom CUDA kernel, rendering the method
hardware-friendly. Empirical evaluations on diverse neural architectures
demonstrate that our method maintains accuracy on par with unstructured
counterparts while benefiting from tangible computational gains. Notably, with
90% sparse linear layers in ViTs, we observe up to a 3.13x speedup in online
inference without sacrificing model performance and a 1.59x speedup in training
on a GPU compared to equivalent unstructured layers. Our source code is
available at https://github.com/horizon-research/DynaDiag/.

</details>


### [152] [RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer](https://arxiv.org/abs/2506.11465)
*Haotian Ni,Yake Wei,Hang Liu,Gong Chen,Chong Peng,Hao Lin,Di Hu*

Main category: cs.LG

TL;DR: 论文提出RollingQ方法，通过轮换查询打破自增强循环，恢复多模态Transformer的动态适应性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中，动态融合策略（如注意力机制）常因模型偏好某一模态而失效，导致动态适应性下降。

Method: 提出RollingQ方法，通过轮换查询平衡注意力分配，缓解模态间键分布差距。

Result: 实验验证RollingQ有效恢复了多模态Transformer的动态合作能力。

Conclusion: RollingQ能显著提升多模态Transformer的广泛能力，代码已开源。

Abstract: Multimodal learning faces challenges in effectively fusing information from
diverse modalities, especially when modality quality varies across samples.
Dynamic fusion strategies, such as attention mechanism in Transformers, aim to
address such challenge by adaptively emphasizing modalities based on the
characteristics of input data. However, through amounts of carefully designed
experiments, we surprisingly observed that the dynamic adaptability of
widely-used self-attention models diminishes. Model tends to prefer one
modality regardless of data characteristics. This bias triggers a
self-reinforcing cycle that progressively overemphasizes the favored modality,
widening the distribution gap in attention keys across modalities and
deactivating attention mechanism's dynamic properties. To revive adaptability,
we propose a simple yet effective method Rolling Query (RollingQ), which
balances attention allocation by rotating the query to break the
self-reinforcing cycle and mitigate the key distribution gap. Extensive
experiments on various multimodal scenarios validate the effectiveness of
RollingQ and the restoration of cooperation dynamics is pivotal for enhancing
the broader capabilities of widely deployed multimodal Transformers. The source
code is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.

</details>


### [153] [Position Paper: Rethinking AI/ML for Air Interface in Wireless Networks](https://arxiv.org/abs/2506.11466)
*Georgios Kontes,Diomidis S. Michalopoulos,Birendra Ghimire,Christopher Mutschler*

Main category: cs.LG

TL;DR: 本文概述了AI/ML在无线网络中的应用现状，强调了跨学科研究的重要性，并总结了3GPP标准化中的关键问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AI/ML在无线网络（尤其是空中接口）的应用仍处于早期阶段，需要跨学科的深入研究以充分发挥其潜力。

Method: 通过分析3GPP标准化中的AI/ML讨论，总结关键用例、架构考虑和技术需求。

Result: 提出了AI/ML在无线通信中的开放研究挑战和机遇。

Conclusion: 学术和工业界可以通过跨学科合作推动AI赋能的无线系统发展。

Abstract: AI/ML research has predominantly been driven by domains such as computer
vision, natural language processing, and video analysis. In contrast, the
application of AI/ML to wireless networks, particularly at the air interface,
remains in its early stages. Although there are emerging efforts to explore
this intersection, fully realizing the potential of AI/ML in wireless
communications requires a deep interdisciplinary understanding of both fields.
We provide an overview of AI/ML-related discussions in 3GPP standardization,
highlighting key use cases, architectural considerations, and technical
requirements. We outline open research challenges and opportunities where
academic and industrial communities can contribute to shaping the future of
AI-enabled wireless systems.

</details>


### [154] [LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment](https://arxiv.org/abs/2506.11480)
*Shikun Li,Shipeng Li,Zhiqin Yang,Xinghua Zhang,Gaode Chen,Xiaobo Xia,Hengyu Liu,Zhe Peng*

Main category: cs.LG

TL;DR: 论文提出了一种名为LearnAlign的梯度对齐方法，通过智能选择可学习和代表性的训练数据，显著减少强化学习后训练的数据需求，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面效果显著，但其数据效率低是一个主要瓶颈。本文旨在解决这一问题。

Method: 提出LearnAlign方法，基于梯度对齐和成功率的数据可学习性选择训练数据，克服响应长度偏差。

Result: 在三个数学推理基准测试中，方法显著减少数据需求（如GSM8K上减少1000个数据点），性能优于全数据训练（77.53% vs 77.04%）。

Conclusion: LearnAlign为数据高效的强化学习后训练提供了新思路，并为未来优化推理数据选择的研究奠定了基础。

Abstract: Reinforcement learning (RL) has become a key technique for enhancing LLMs'
reasoning abilities, yet its data inefficiency remains a major bottleneck. To
address this critical yet challenging issue, we present a novel
gradient-alignment-based method, named LearnAlign, which intelligently selects
the learnable and representative training reasoning data for RL post-training.
To overcome the well-known issue of response-length bias in gradient norms, we
introduce the data learnability based on the success rate, which can indicate
the learning potential of each data point. Experiments across three
mathematical reasoning benchmarks demonstrate that our method significantly
reduces training data requirements while achieving minor performance
degradation or even improving performance compared to full-data training. For
example, it reduces data requirements by up to 1,000 data points with better
performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%).
Furthermore, we show its effectiveness in the staged RL setting. This work
provides valuable insights into data-efficient RL post-training and establishes
a foundation for future research in optimizing reasoning data selection.To
facilitate future work, we will release code.

</details>


### [155] [Diabetes Prediction and Management Using Machine Learning Approaches](https://arxiv.org/abs/2506.11501)
*Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh,Mohammad Subhi Al-Batah*

Main category: cs.LG

TL;DR: 该研究比较了多种机器学习算法在糖尿病风险预测中的表现，发现神经网络和随机森林算法准确率最高，表明机器学习可作为早期筛查工具。


<details>
  <summary>Details</summary>
Motivation: 糖尿病日益成为全球健康问题，需要早期检测和主动管理以减轻其严重并发症。机器学习在预测糖尿病风险方面显示出潜力。

Method: 研究使用Pima Indians Diabetes Database的768个样本，评估了多种机器学习算法（如逻辑回归、决策树、随机森林等）的预测准确性。

Result: 神经网络算法预测准确率最高（78.57%），随机森林次之（76.30%）。

Conclusion: 机器学习技术可作为早期筛查工具，帮助识别高风险人群，并为长期干预提供支持，减轻糖尿病对医疗系统的负担。

Abstract: Diabetes has emerged as a significant global health issue, especially with
the increasing number of cases in many countries. This trend Underlines the
need for a greater emphasis on early detection and proactive management to
avert or mitigate the severe health complications of this disease. Over recent
years, machine learning algorithms have shown promising potential in predicting
diabetes risk and are beneficial for practitioners. Objective: This study
highlights the prediction capabilities of statistical and non-statistical
machine learning methods over Diabetes risk classification in 768 samples from
the Pima Indians Diabetes Database. It consists of the significant demographic
and clinical features of age, body mass index (BMI) and blood glucose levels
that greatly depend on the vulnerability against Diabetes. The experimentation
assesses the various types of machine learning algorithms in terms of accuracy
and effectiveness regarding diabetes prediction. These algorithms include
Logistic Regression, Decision Tree, Random Forest, K-Nearest Neighbors, Naive
Bayes, Support Vector Machine, Gradient Boosting and Neural Network Models. The
results show that the Neural Network algorithm gained the highest predictive
accuracy with 78,57 %, and then the Random Forest algorithm had the second
position with 76,30 % accuracy. These findings show that machine learning
techniques are not just highly effective. Still, they also can potentially act
as early screening tools in predicting Diabetes within a data-driven fashion
with valuable information on who is more likely to get affected. In addition,
this study can help to realize the potential of machine learning for timely
intervention over the longer term, which is a step towards reducing health
outcomes and disease burden attributable to Diabetes on healthcare systems

</details>


### [156] [Machine Learning-Based Quantification of Vesicoureteral Reflux with Enhancing Accuracy and Efficiency](https://arxiv.org/abs/2506.11508)
*Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Mohammad Subhi Al-Batah,Lana Yasin Al Aesa,Mohammed Hasan Abu-Arqoub,Rashiq Rafiq Marie,Firas Hussein Alsmad*

Main category: cs.LG

TL;DR: 利用机器学习分析VCUG图像，提高VUR诊断一致性，模型表现优异，肾盏变形是关键指标。


<details>
  <summary>Details</summary>
Motivation: 传统VUR诊断主观性强，存在变异性，需客观标准化方法。

Method: 选取113张VCUG图像，提取9个特征，训练6种模型，采用留一法验证。

Result: 模型分类准确，无假阳性/阴性，肾盏变形是高等级VUR的关键指标。

Conclusion: 机器学习可提供客观诊断替代方案，未来需扩大数据集并优化模型。

Abstract: Vesicoureteral reflux (VUR) is traditionally assessed using subjective
grading systems, which introduces variability in diagnosis. This study
investigates the use of machine learning to improve diagnostic consistency by
analyzing voiding cystourethrogram (VCUG) images. A total of 113 VCUG images
were reviewed, with expert grading of VUR severity. Nine image-based features
were selected to train six predictive models: Logistic Regression, Decision
Tree, Gradient Boosting, Neural Network, and Stochastic Gradient Descent. The
models were evaluated using leave-one-out cross-validation. Analysis identified
deformation patterns in the renal calyces as key indicators of high-grade VUR.
All models achieved accurate classifications with no false positives or
negatives. High sensitivity to subtle image patterns characteristic of
different VUR grades was confirmed by substantial Area Under the Curve (AUC)
values. The results suggest that machine learning can offer an objective and
standardized alternative to current subjective VUR assessments. These findings
highlight renal calyceal deformation as a strong predictor of severe cases.
Future research should aim to expand the dataset, refine imaging features, and
improve model generalizability for broader clinical use.

</details>


### [157] [Task-Driven Discrete Representation Learning](https://arxiv.org/abs/2506.11511)
*Tung-Long Vuong*

Main category: cs.LG

TL;DR: 本文提出了一种任务驱动的离散表示学习框架，探讨了离散特征在下游任务中的实用性，并分析了表示能力与样本复杂性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前离散表示学习（DRL）框架主要关注生成任务，而离散表示的质量定义模糊。本文旨在从任务驱动角度重新审视DRL，明确离散表示的实际用途。

Method: 提出统一框架，研究离散特征在下游任务中的实用性，生成任务仅为其中一种应用。同时，理论分析了表示能力与样本复杂性的权衡。

Result: 展示了框架在多种应用中的灵活性和有效性。

Conclusion: 任务驱动的DRL框架为离散表示的实际应用提供了新视角，理论分析揭示了其性能影响机制。

Abstract: In recent years, deep discrete representation learning (DRL) has achieved
significant success across various domains. Most DRL frameworks (e.g., the
widely used VQ-VAE and its variants) have primarily focused on generative
settings, where the quality of a representation is implicitly gauged by the
fidelity of its generation. In fact, the goodness of a discrete representation
remain ambiguously defined across the literature. In this work, we adopt a
practical approach that examines DRL from a task-driven perspective. We propose
a unified framework that explores the usefulness of discrete features in
relation to downstream tasks, with generation naturally viewed as one possible
application. In this context, the properties of discrete representations as
well as the way they benefit certain tasks are also relatively understudied. We
therefore provide an additional theoretical analysis of the trade-off between
representational capacity and sample complexity, shedding light on how discrete
representation utilization impacts task performance. Finally, we demonstrate
the flexibility and effectiveness of our framework across diverse applications.

</details>


### [158] [Prioritizing Alignment Paradigms over Task-Specific Model Customization in Time-Series LLMs](https://arxiv.org/abs/2506.11512)
*Wei Li,Yunyao Cheng,Xinli Hao,Chaohong Ma,Yuxuan Liang,Bin Yang,Christian S. Jensen,Xiaofeng Meng*

Main category: cs.LG

TL;DR: 论文主张通过时间序列数据的本征原语（primitives）重新调整LLMs的时间序列推理方法，提出三种对齐范式（Injective、Bridging、Internal），以解决现有方法的成本高、不灵活和效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注任务特定的模型定制，而忽略了时间序列数据的本征原语，导致推理方法成本高、不灵活且效率低。

Method: 提出三种对齐范式：Injective Alignment（领域对齐）、Bridging Alignment（特征对齐）、Internal Alignment（表示对齐），以激活LLMs的时间序列推理能力。

Result: 通过系统性地考虑数据的内在结构，实现更经济、灵活和高效的时间序列推理。

Conclusion: 建议采用对齐导向的方法选择适当的对齐范式，并分类相关文献，展望未来研究方向。

Abstract: Recent advances in Large Language Models (LLMs) have enabled unprecedented
capabilities for time-series reasoning in diverse real-world applications,
including medical, financial, and spatio-temporal domains. However, existing
approaches typically focus on task-specific model customization, such as
forecasting and anomaly detection, while overlooking the data itself, referred
to as time-series primitives, which are essential for in-depth reasoning. This
position paper advocates a fundamental shift in approaching time-series
reasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic
primitives of time series data over task-specific model customization. This
realignment addresses the core limitations of current time-series reasoning
approaches, which are often costly, inflexible, and inefficient, by
systematically accounting for intrinsic structure of data before task
engineering. To this end, we propose three alignment paradigms: Injective
Alignment, Bridging Alignment, and Internal Alignment, which are emphasized by
prioritizing different aspects of time-series primitives: domain,
characteristic, and representation, respectively, to activate time-series
reasoning capabilities of LLMs to enable economical, flexible, and efficient
reasoning. We further recommend that practitioners adopt an alignment-oriented
method to avail this instruction to select an appropriate alignment paradigm.
Additionally, we categorize relevant literature into these alignment paradigms
and outline promising research directions.

</details>


### [159] [Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning](https://arxiv.org/abs/2506.11516)
*Chengye Li,Haiyun Liu,Yuanxi Li*

Main category: cs.LG

TL;DR: 论文提出了一种新理论视角，将上下文学习（ICL）解释为隐式知识蒸馏（KD），并通过理论分析揭示了其泛化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管ICL在实践中表现良好，但其机制尚不明确，限制了对其的解释和改进。

Method: 将ICL视为隐式知识蒸馏，推导了基于Rademacher复杂度的泛化边界，并分析了权重偏差与分布差异的关系。

Result: 理论框架解释了多种实证现象，并统一了先前的梯度与分布分析。

Conclusion: 首次将推理时注意力形式化为蒸馏过程，为未来提示工程和自动演示选择提供了理论依据。

Abstract: In-context learning (ICL) allows large language models (LLMs) to solve novel
tasks without weight updates. Despite its empirical success, the mechanism
behind ICL remains poorly understood, limiting our ability to interpret,
improve, and reliably apply it. In this paper, we propose a new theoretical
perspective that interprets ICL as an implicit form of knowledge distillation
(KD), where prompt demonstrations guide the model to form a task-specific
reference model during inference. Under this view, we derive a Rademacher
complexity-based generalization bound and prove that the bias of the distilled
weights grows linearly with the Maximum Mean Discrepancy (MMD) between the
prompt and target distributions. This theoretical framework explains several
empirical phenomena and unifies prior gradient-based and distributional
analyses. To the best of our knowledge, this is the first to formalize
inference-time attention as a distillation process, which provides theoretical
insights for future prompt engineering and automated demonstration selection.

</details>


### [160] [Delayformer: spatiotemporal transformation for predicting high-dimensional dynamics](https://arxiv.org/abs/2506.11528)
*Zijian Wang,Peng Tao,Luonan Chen*

Main category: cs.LG

TL;DR: Delayformer框架通过多变量时空信息转换和延迟嵌入状态预测，解决了高维系统中非线性与交互问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高维系统中非线性与复杂交互导致时间序列预测困难，现有方法在噪声数据下表现不佳。

Method: 提出Delayformer框架，利用延迟嵌入状态和ViT编码器跨变量学习，线性解码器预测状态。

Result: 在合成和真实数据集上优于现有方法，展示了跨领域预测潜力。

Conclusion: Delayformer是一种高效的时间序列基础模型，具有广泛适用性。

Abstract: Predicting time-series is of great importance in various scientific and
engineering fields. However, in the context of limited and noisy data,
accurately predicting dynamics of all variables in a high-dimensional system is
a challenging task due to their nonlinearity and also complex interactions.
Current methods including deep learning approaches often perform poorly for
real-world systems under such circumstances. This study introduces the
Delayformer framework for simultaneously predicting dynamics of all variables,
by developing a novel multivariate spatiotemporal information (mvSTI)
transformation that makes each observed variable into a delay-embedded state
(vector) and further cross-learns those states from different variables. From
dynamical systems viewpoint, Delayformer predicts system states rather than
individual variables, thus theoretically and computationally overcoming such
nonlinearity and cross-interaction problems. Specifically, it first utilizes a
single shared Visual Transformer (ViT) encoder to cross-represent dynamical
states from observed variables in a delay embedded form and then employs
distinct linear decoders for predicting next states, i.e. equivalently
predicting all original variables parallelly. By leveraging the theoretical
foundations of delay embedding theory and the representational capabilities of
Transformers, Delayformer outperforms current state-of-the-art methods in
forecasting tasks on both synthetic and real-world datasets. Furthermore, the
potential of Delayformer as a foundational time-series model is demonstrated
through cross-domain forecasting tasks, highlighting its broad applicability
across various scenarios.

</details>


### [161] [Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications](https://arxiv.org/abs/2506.11530)
*Aamir Hussain Chughtai*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的鲁棒非线性滤波方法，用于处理现实场景中的异常情况（如离群值、偏差、漂移和缺失观测），并通过贝叶斯推理框架和理论界限验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的噪声统计信息通常未知或部分已知，传统滤波方法难以应对异常情况，因此需要开发更鲁棒的滤波方法。

Method: 基于贝叶斯推理框架，结合变分推断（VI）和粒子滤波/序列蒙特卡洛（SMC）等确定性及随机近似技术，并研究了贝叶斯克拉美罗界（BCRBs）的理论估计极限。

Result: 通过目标跟踪、室内定位、3D点云配准等场景的仿真和实验验证了所提方法的性能提升。

Conclusion: 该研究为处理异常数据提供了基础性解决方案，未来可扩展至鲁棒机器学习、异常数据学习系统动力学等领域。

Abstract: State estimation or filtering serves as a fundamental task to enable
intelligent decision-making in applications such as autonomous vehicles,
robotics, healthcare monitoring, smart grids, intelligent transportation, and
predictive maintenance. Standard filtering assumes prior knowledge of noise
statistics to extract latent system states from noisy sensor data. However,
real-world scenarios involve abnormalities like outliers, biases, drifts, and
missing observations with unknown or partially known statistics, limiting
conventional approaches. This thesis presents novel robust nonlinear filtering
methods to mitigate these challenges. Based on insights from our filtering
proposals, we extend the formulations to offline estimation/learning setups and
propose smoothing extensions. Our methods leverage Bayesian inference
frameworks, employing both deterministic and stochastic approximation
techniques including Variational Inference (VI) and Particle Filters/Sequential
Monte Carlo (SMC). We also study theoretical estimation limits using Bayesian
Cram\'er-Rao bounds (BCRBs) in the context of measurement abnormalities. To
validate the performance gains of the proposed methods, we perform simulations
and experiments in scenarios including target tracking, indoor localization, 3D
point cloud registration, mesh registration, and pose graph optimization. The
fundamental nature of the work makes it useful in diverse applications, with
possible future extensions toward developing outlier-robust machine learning
pipelines, learning system dynamics from anomalous data, and addressing
challenges in generative AI where standard diffusion models struggle with
outliers, imbalanced datasets, and mode collapse.

</details>


### [162] [Improving Multimodal Learning Balance and Sufficiency through Data Remixing](https://arxiv.org/abs/2506.11550)
*Xiaoyu Ma,Hao Chen,Yongjian Deng*

Main category: cs.LG

TL;DR: 论文提出了一种多模态数据重混合方法，通过解耦数据、筛选难样本和批量级重组，解决多模态学习中的不平衡和干扰问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中存在模态懒惰和模态冲突问题，导致学习不充分和不平衡。现有方法无法同时实现单模态充分性和多模态平衡。

Method: 提出多模态数据重混合（Data Remixing），包括解耦多模态数据、筛选难样本，以及批量级重组以对齐梯度方向。

Result: 实验表明，该方法在CREMAD和Kinetic-Sounds数据集上分别提升约6.50%和3.41%的准确率，且无需额外计算开销。

Conclusion: 该方法有效解决了多模态学习中的不平衡问题，提升了单模态学习的充分性，且易于与现有方法结合。

Abstract: Different modalities hold considerable gaps in optimization trajectories,
including speeds and paths, which lead to modality laziness and modality clash
when jointly training multimodal models, resulting in insufficient and
imbalanced multimodal learning. Existing methods focus on enforcing the weak
modality by adding modality-specific optimization objectives, aligning their
optimization speeds, or decomposing multimodal learning to enhance unimodal
learning. These methods fail to achieve both unimodal sufficiency and
multimodal balance. In this paper, we, for the first time, address both
concerns by proposing multimodal Data Remixing, including decoupling multimodal
data and filtering hard samples for each modality to mitigate modality
imbalance; and then batch-level reassembling to align the gradient directions
and avoid cross-modal interference, thus enhancing unimodal learning
sufficiency. Experimental results demonstrate that our method can be seamlessly
integrated with existing approaches, improving accuracy by approximately
6.50%$\uparrow$ on CREMAD and 3.41%$\uparrow$ on Kinetic-Sounds, without
training set expansion or additional computational overhead during inference.
The source code is available at
\href{https://github.com/MatthewMaxy/Remix_ICML2025}{Data Remixing}.

</details>


### [163] [Learn to Preserve Personality: Federated Foundation Models in Recommendations](https://arxiv.org/abs/2506.11563)
*Zhiwei Li,Guodong Long,Chunxu Zhang,Honglei Zhang,Jing Jiang,Chengqi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的学习范式，通过联邦基础模型（FFM）在推荐系统中平衡泛化与个性化，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在泛化与个性化之间的权衡问题，并利用联邦学习实现用户数据的去中心化处理。

Method: 采用联邦基础模型（FFM）技术，结合推荐系统作为测试平台，通过分散化过程分离共享知识与个性化适配。

Result: 提出了一种用户中心化的去中心化系统架构，未来可支持个性化代理指导用户决策。

Conclusion: FFM在推荐系统中具有潜力，能够兼顾泛化能力与个性化需求，同时保障用户隐私与控制权。

Abstract: A core learning challenge for existed Foundation Models (FM) is striking the
tradeoff between generalization with personalization, which is a dilemma that
has been highlighted by various parameter-efficient adaptation techniques.
Federated foundation models (FFM) provide a structural means to decouple shared
knowledge from individual specific adaptations via decentralized processes.
Recommendation systems offer a perfect testbed for FFMs, given their reliance
on rich implicit feedback reflecting unique user characteristics. This position
paper discusses a novel learning paradigm where FFMs not only harness their
generalization capabilities but are specifically designed to preserve the
integrity of user personality, illustrated thoroughly within the recommendation
contexts. We envision future personal agents, powered by personalized adaptive
FMs, guiding user decisions on content. Such an architecture promises a user
centric, decentralized system where individuals maintain control over their
personalized agents.

</details>


### [164] [A Comparative Analysis of Influence Signals for Data Debugging](https://arxiv.org/abs/2506.11584)
*Nikolaos Myrtakis,Ioannis Tsamardinos,Vassilis Christophides*

Main category: cs.LG

TL;DR: 本文比较了基于影响力的信号在调试训练数据中的表现，发现某些信号能有效检测错误标签样本，但无法检测异常样本。


<details>
  <summary>Details</summary>
Motivation: 提高训练样本质量对提升机器学习模型的可靠性和性能至关重要。

Method: 通过实验评估多种基于影响力的信号（如Self-Influence、Average Absolute Influence等）在不同数据模态和深度学习模型中的表现。

Result: Self-Influence能有效检测错误标签样本，但现有信号均无法检测异常样本，且存在训练动态和影响力抵消效应的问题。

Conclusion: 现有影响力信号在检测异常样本方面存在局限性，需进一步改进以考虑训练动态和影响力抵消效应。

Abstract: Improving the quality of training samples is crucial for improving the
reliability and performance of ML models. In this paper, we conduct a
comparative evaluation of influence-based signals for debugging training data.
These signals can potentially identify both mislabeled and anomalous samples
from a potentially noisy training set as we build the models and hence
alleviate the need for dedicated glitch detectors. Although several
influence-based signals (e.g., Self-Influence, Average Absolute Influence,
Marginal Influence, GD-class) have been recently proposed in the literature,
there are no experimental studies for assessing their power in detecting
different glitch types (e.g., mislabeled and anomalous samples) under a common
influence estimator (e.g., TraceIn) for different data modalities (image and
tabular), and deep learning models (trained from scratch or foundation).
Through extensive experiments, we show that signals like Self-Influence
effectively detect mislabeled samples, but none of the existing signals can
detect anomalies. Existing signals do not take into account the training
dynamics, i.e., how the samples' influence on the model changes during
training, while some signals fall into influence cancellation effects, i.e.,
influence score is zero due to unsigned scores accumulation, resulting in
misleading influence attribution.

</details>


### [165] [KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity](https://arxiv.org/abs/2506.11611)
*Yaning Jia,Shenyang Deng,Chiyu Ma,Yaoqing Yang,Soroush Vosoughi*

Main category: cs.LG

TL;DR: 提出了一种基于图核复杂度的边净化方法（KCES），用于提升图神经网络的鲁棒性，无需训练且模型无关。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）易受微小扰动和对抗攻击的影响，现有防御方法多依赖启发式指标或计算复杂。

Method: 利用图核复杂度（GKC）定义边的KC分数，通过修剪高KC分数的边来减少对抗攻击的影响。

Result: KCES显著提升了GNNs的鲁棒性，优于现有方法，并能与其他防御策略无缝集成。

Conclusion: KCES为保护GNNs提供了一种高效且理论支持的方法。

Abstract: Graph Neural Networks (GNNs) have achieved impressive success across a wide
range of graph-based tasks, yet they remain highly vulnerable to small,
imperceptible perturbations and adversarial attacks. Although numerous defense
methods have been proposed to address these vulnerabilities, many rely on
heuristic metrics, overfit to specific attack patterns, and suffer from high
computational complexity. In this paper, we propose Kernel Complexity-Based
Edge Sanitization (KCES), a training-free, model-agnostic defense framework.
KCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the
graph's Gram matrix that characterizes GNN generalization via its test error
bound. Building on GKC, we define a KC score for each edge, measuring the
change in GKC when the edge is removed. Edges with high KC scores, typically
introduced by adversarial perturbations, are pruned to mitigate their harmful
effects, thereby enhancing GNNs' robustness. KCES can also be seamlessly
integrated with existing defense strategies as a plug-and-play module without
requiring training. Theoretical analysis and extensive experiments demonstrate
that KCES consistently enhances GNN robustness, outperforms state-of-the-art
baselines, and amplifies the effectiveness of existing defenses, offering a
principled and efficient solution for securing GNNs.

</details>


### [166] [Model Organisms for Emergent Misalignment](https://arxiv.org/abs/2506.11613)
*Edward Turner,Anna Soligo,Mia Taylor,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现，在窄范围有害数据集上微调大型语言模型可能导致广泛的对齐失效（Emergent Misalignment, EM），这一现象出乎专家预料。通过改进数据集和模型，研究实现了更高的连贯性和更小的模型规模，并揭示了对齐失效的机制性相变。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型对齐失效的机制，填补当前对齐理解的空白，并为未来研究提供工具。

Method: 使用新的窄范围对齐失效数据集，改进模型生物体，实现更高连贯性和更小规模，并通过单秩LoRA适配器诱导对齐失效。

Result: 在多种模型规模、家族和训练协议中观察到对齐失效的稳健性，并识别出机制性相变与行为相变的对应关系。

Conclusion: 研究为理解并减轻大型语言模型的对齐风险奠定了基础，凸显了当前对齐方法的不足。

Abstract: Recent work discovered Emergent Misalignment (EM): fine-tuning large language
models on narrowly harmful datasets can lead them to become broadly misaligned.
A survey of experts prior to publication revealed this was highly unexpected,
demonstrating critical gaps in our understanding of model alignment. In this
work, we both advance understanding and provide tools for future research.
Using new narrowly misaligned datasets, we create a set of improved model
organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B
parameter models (vs. 32B), and that induce misalignment using a single rank-1
LoRA adapter. We demonstrate that EM occurs robustly across diverse model
sizes, three model families, and numerous training protocols including full
supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a
mechanistic phase transition and demonstrate that it corresponds to a robust
behavioural phase transition in all studied organisms. Aligning large language
models is critical for frontier AI safety, yet EM exposes how far we are from
achieving this robustly. By distilling clean model organisms that isolate a
minimal alignment-compromising change, and where this is learnt, we establish a
foundation for future research into understanding and mitigating alignment
risks in LLMs.

</details>


### [167] [Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments](https://arxiv.org/abs/2506.11615)
*Deliang Jin,Gang Chen,Shuo Feng,Yufeng Ling,Haoran Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于机器遗忘原理的新框架，通过数据分区、神经元剪枝和微调来减少噪声数据的影响，显著提升了模型在噪声环境中的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在噪声或损坏的训练数据下性能会显著下降，传统方法依赖噪声分布假设或需要大量重训练，不适用于大规模模型。

Method: 结合梯度引导的数据分区、判别性神经元剪枝和针对性微调，无需噪声分布假设或完整重训练。

Result: 在CIFAR-10和语音识别任务中，准确率提升约10%，重训练时间减少47%。

Conclusion: 该框架在噪声环境中实现了高效且可扩展的鲁棒泛化。

Abstract: Deep neural networks (DNNs) have achieved remarkable success across diverse
domains, but their performance can be severely degraded by noisy or corrupted
training data. Conventional noise mitigation methods often rely on explicit
assumptions about noise distributions or require extensive retraining, which
can be impractical for large-scale models. Inspired by the principles of
machine unlearning, we propose a novel framework that integrates
attribution-guided data partitioning, discriminative neuron pruning, and
targeted fine-tuning to mitigate the impact of noisy samples. Our approach
employs gradient-based attribution to probabilistically distinguish
high-quality examples from potentially corrupted ones without imposing
restrictive assumptions on the noise. It then applies regression-based
sensitivity analysis to identify and prune neurons that are most vulnerable to
noise. Finally, the resulting network is fine-tuned on the high-quality data
subset to efficiently recover and enhance its generalization performance. This
integrated unlearning-inspired framework provides several advantages over
conventional noise-robust learning approaches. Notably, it combines data-level
unlearning with model-level adaptation, thereby avoiding the need for full
model retraining or explicit noise modeling. We evaluate our method on
representative tasks (e.g., CIFAR-10 image classification and speech
recognition) under various noise levels and observe substantial gains in both
accuracy and efficiency. For example, our framework achieves approximately a
10% absolute accuracy improvement over standard retraining on CIFAR-10 with
injected label noise, while reducing retraining time by up to 47% in some
settings. These results demonstrate the effectiveness and scalability of the
proposed approach for achieving robust generalization in noisy environments.

</details>


### [168] [Convergent Linear Representations of Emergent Misalignment](https://arxiv.org/abs/2506.11618)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型在窄数据集上微调时出现的广泛不匹配行为（称为“涌现不匹配”），通过最小模型实验揭示了不匹配的机制及其通用性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在窄数据集微调后出现广泛不匹配行为的机制，填补模型对齐知识的空白。

Method: 使用仅含9个rank-1适配器的最小模型对Qwen2.5-14B-Instruct进行微调，提取“不匹配方向”并验证其通用性。

Result: 发现不同不匹配模型收敛到相似的不匹配表示，并成功通过提取的方向消除不匹配行为。

Conclusion: 通过理解涌现不匹配的机制，为更广泛的不匹配行为提供理解和缓解的基础。

Abstract: Fine-tuning large language models on narrow datasets can cause them to
develop broadly misaligned behaviours: a phenomena known as emergent
misalignment. However, the mechanisms underlying this misalignment, and why it
generalizes beyond the training domain, are poorly understood, demonstrating
critical gaps in our knowledge of model alignment. In this work, we train and
study a minimal model organism which uses just 9 rank-1 adapters to emergently
misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently
misaligned models converge to similar representations of misalignment. We
demonstrate this convergence by extracting a 'misalignment direction' from one
fine-tuned model's activations, and using it to effectively ablate misaligned
behaviour from fine-tunes using higher dimensional LoRAs and different
datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further
present a set of experiments for directly interpreting the fine-tuning
adapters, showing that six contribute to general misalignment, while two
specialise for misalignment in just the fine-tuning domain. Emergent
misalignment is a particularly salient example of undesirable and unexpected
model behaviour and by advancing our understanding of the mechanisms behind it,
we hope to move towards being able to better understand and mitigate
misalignment more generally.

</details>


### [169] [Physically-informed change-point kernels for structural dynamics](https://arxiv.org/abs/2506.11625)
*Daniel James Pitchforth,Matthew Rhys Jones,Samuel John Gibson,Elizabeth Jane Cross*

Main category: cs.LG

TL;DR: 本文提出了一种新型的物理信息变化点核函数，用于高斯过程，能够动态调整对物理知识的依赖，以优化物理与数据之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 在物理信息机器学习中，过度依赖物理知识或数据可能导致模型性能下降。本文旨在解决如何动态调整物理与数据的依赖关系，以适应不同条件下的建模需求。

Method: 开发了新型的物理信息变化点核函数，允许用户定义条件来控制物理知识的引入和退出，同时支持自动学习依赖关系。

Result: 通过两个工程案例（桥梁风载和飞机机翼应变）验证了该方法的有效性，能够更准确地捕捉不确定性。

Conclusion: 提出的核函数提供了一种灵活且直观的方法，动态平衡物理与数据，提高了模型的适应性和解释性。

Abstract: The relative balance between physics and data within any physics-informed
machine learner is an important modelling consideration to ensure that the
benefits of both physics and data-based approaches are maximised. An over
reliance on physical knowledge can be detrimental, particularly when the
physics-based component of a model may not accurately represent the true
underlying system. An underutilisation of physical knowledge potentially wastes
a valuable resource, along with benefits in model interpretability and reduced
demand for expensive data collection. Achieving an optimal physics-data balance
is a challenging aspect of model design, particularly if the level varies
through time; for example, one might have a physical approximation, only valid
within particular regimes, or a physical phenomenon may be known to only occur
when given conditions are met (e.g. at high temperatures). This paper develops
novel, physically-informed, change-point kernels for Gaussian processes,
capable of dynamically varying the reliance upon available physical knowledge.
A high level of control is granted to a user, allowing for the definition of
conditions in which they believe a phenomena should occur and the rate at which
the knowledge should be phased in and out of a model. In circumstances where
users may be less certain, the switching reliance upon physical knowledge may
be automatically learned and recovered from the model in an interpretable and
intuitive manner. Variation of the modelled noise based on the physical
phenomena occurring is also implemented to provide a more representative
capture of uncertainty alongside predictions. The capabilities of the new
kernel structures are explored through the use of two engineering case studies:
the directional wind loading of a cable-stayed bridge and the prediction of
aircraft wing strain during in-flight manoeuvring.

</details>


### [170] [Geometry-Aware Edge Pooling for Graph Neural Networks](https://arxiv.org/abs/2506.11700)
*Katharina Limbeck,Lydia Mezrag,Guy Wolf,Bastian Rieck*

Main category: cs.LG

TL;DR: 论文提出了一种基于边折叠的结构感知图池化方法，通过扩散几何和多样性度量优化池化过程，在保持图结构的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中大规模图数据集的需求促使图池化层成为图神经网络的关键组件，但现有方法往往牺牲图结构和可解释性，导致性能不稳定。

Method: 利用扩散几何和边折叠技术迭代减少图大小，同时通过多样性度量（如magnitude和spread）控制池化过程的保真度和计算效率。

Result: 实验表明，该方法在多种图分类任务中性能优于其他池化层，能保留输入图的关键谱特性，并在不同池化比例下保持高准确率。

Conclusion: 提出的结构感知池化方法在性能和结构保留方面表现优异，适用于多样化的图任务。

Abstract: Graph Neural Networks (GNNs) have shown significant success for graph-based
tasks. Motivated by the prevalence of large datasets in real-world
applications, pooling layers are crucial components of GNNs. By reducing the
size of input graphs, pooling enables faster training and potentially better
generalisation. However, existing pooling operations often optimise for the
learning task at the expense of fundamental graph structures and
interpretability. This leads to unreliable performance across varying dataset
types, downstream tasks and pooling ratios. Addressing these concerns, we
propose novel graph pooling layers for structure aware pooling via edge
collapses. Our methods leverage diffusion geometry and iteratively reduce a
graph's size while preserving both its metric structure and structural
diversity. We guide pooling using magnitude, an isometry-invariant diversity
measure, which permits us to control the fidelity of the pooling process.
Further, we use the spread of a metric space as a faster and more stable
alternative ensuring computational efficiency. Empirical results demonstrate
that our methods (i) achieve superior performance compared to alternative
pooling layers across a range of diverse graph classification tasks, (ii)
preserve key spectral properties of the input graphs, and (iii) retain high
accuracy across varying pooling ratios.

</details>


### [171] [Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning](https://arxiv.org/abs/2506.11706)
*Lukas Fehring,Marius Lindauer,Theresa Eimer*

Main category: cs.LG

TL;DR: GrowNN是一种通过逐步增加网络层数来提升强化学习模型性能的方法，实验证明其在MiniHack和Mujoco任务中表现优于静态网络。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中，即使是中等规模的网络训练仍然困难，限制了策略学习的复杂性。

Method: 提出GrowNN方法，通过逐步增加网络层数（不改变原有函数），动态提升网络容量。

Result: 在MiniHack和Mujoco任务中，GrowNN方法比静态网络性能提升48%和72%。

Conclusion: GrowNN通过动态增加网络容量，有效提升了强化学习模型的性能。

Abstract: While increasingly large models have revolutionized much of the machine
learning landscape, training even mid-sized networks for Reinforcement Learning
(RL) is still proving to be a struggle. This, however, severely limits the
complexity of policies we are able to learn. To enable increased network
capacity while maintaining network trainability, we propose GrowNN, a simple
yet effective method that utilizes progressive network growth during training.
We start training a small network to learn an initial policy. Then we add
layers without changing the encoded function. Subsequent updates can utilize
the added layers to learn a more expressive policy, adding capacity as the
policy's complexity increases. GrowNN can be seamlessly integrated into most
existing RL agents. Our experiments on MiniHack and Mujoco show improved agent
performance, with incrementally GrowNN-deeper networks outperforming their
respective static counterparts of the same size by up to 48% on MiniHack Room
and 72% on Ant.

</details>


### [172] [Taxonomy of reduction matrices for Graph Coarsening](https://arxiv.org/abs/2506.11743)
*Antonin Joly,Nicolas Keriven,Aline Roumy*

Main category: cs.LG

TL;DR: 论文提出了一种更通用的图粗化方法，通过放宽降维矩阵与提升矩阵的固定关系，进一步减少信息损失。


<details>
  <summary>Details</summary>
Motivation: 传统图粗化方法中降维矩阵与提升矩阵通常互为伪逆，限制了灵活性。本文发现提升矩阵的约束更重要，因此提出更通用的降维矩阵定义。

Method: 引入不依赖于提升矩阵伪逆的降维矩阵分类，探讨其性质及闭式解可能性，并通过优化受限谱逼近（RSA）进一步减少信息损失。

Result: 实验表明，固定提升矩阵时，调整降维矩阵可进一步降低RSA，并在图神经网络节点分类任务中验证了其效果。

Conclusion: 放宽降维矩阵与提升矩阵的固定关系可提升图粗化效果，为图信号处理与机器学习提供更灵活的工具。

Abstract: Graph coarsening aims to diminish the size of a graph to lighten its memory
footprint, and has numerous applications in graph signal processing and machine
learning. It is usually defined using a reduction matrix and a lifting matrix,
which, respectively, allows to project a graph signal from the original graph
to the coarsened one and back. This results in a loss of information measured
by the so-called Restricted Spectral Approximation (RSA). Most coarsening
frameworks impose a fixed relationship between the reduction and lifting
matrices, generally as pseudo-inverses of each other, and seek to define a
coarsening that minimizes the RSA. In this paper, we remark that the roles of
these two matrices are not entirely symmetric: indeed, putting constraints on
the lifting matrix alone ensures the existence of important objects such as the
coarsened graph's adjacency matrix or Laplacian. In light of this, in this
paper, we introduce a more general notion of reduction matrix, that is not
necessarily the pseudo-inverse of the lifting matrix. We establish a taxonomy
of ``admissible'' families of reduction matrices, discuss the different
properties that they must satisfy and whether they admit a closed-form
description or not. We show that, for a fixed coarsening represented by a fixed
lifting matrix, the RSA can be further reduced simply by modifying the
reduction matrix. We explore different examples, including some based on a
constrained optimization process of the RSA. Since this criterion has also been
linked to the performance of Graph Neural Networks, we also illustrate the
impact of this choices on different node classification tasks on coarsened
graphs.

</details>


### [173] [SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics Estimation](https://arxiv.org/abs/2506.11786)
*Markus Gambietz,Eva Dorschky,Altan Akat,Marcel Schöckel,Jörg Miehling,Anne D. Koelewijn*

Main category: cs.LG

TL;DR: SSPINNpose是一种自监督、物理信息神经网络，直接从IMU数据估计关节运动学和动力学，无需地面真实标签训练，适用于实时生物力学分析。


<details>
  <summary>Details</summary>
Motivation: 当前实时方法依赖监督学习，需要实验室测量系统提供地面真实数据，存在测量误差且难以泛化到新动作，数据收集耗时且不实用。

Method: 提出SSPINNpose，通过物理模型优化网络输出的物理合理性，生成虚拟测量数据，直接在传感器数据上训练。

Result: 与光学动作捕捉相比，SSPINNpose能准确估计关节角度和力矩（RMSD分别为8.7度和4.9%BWBH），延迟3.5毫秒，适用于稀疏传感器配置。

Conclusion: SSPINNpose是一种可扩展、适应性强的解决方案，适用于实验室和野外环境的实时生物力学分析。

Abstract: Accurate real-time estimation of human movement dynamics, including internal
joint moments and muscle forces, is essential for applications in clinical
diagnostics and sports performance monitoring. Inertial measurement units
(IMUs) provide a minimally intrusive solution for capturing motion data,
particularly when used in sparse sensor configurations. However, current
real-time methods rely on supervised learning, where a ground truth dataset
needs to be measured with laboratory measurement systems, such as optical
motion capture. These systems are known to introduce measurement and processing
errors and often fail to generalize to real-world or previously unseen
movements, necessitating new data collection efforts that are time-consuming
and impractical. To overcome these limitations, we propose SSPINNpose, a
self-supervised, physics-informed neural network that estimates joint
kinematics and kinetics directly from IMU data, without requiring ground truth
labels for training. We run the network output through a physics model of the
human body to optimize physical plausibility and generate virtual measurement
data. Using this virtual sensor data, the network is trained directly on the
measured sensor data instead of a ground truth. When compared to optical motion
capture, SSPINNpose is able to accurately estimate joint angles and joint
moments at an RMSD of 8.7 deg and 4.9 BWBH%, respectively, for walking and
running at speeds up to 4.9 m/s at a latency of 3.5 ms. Furthermore, the
framework demonstrates robustness across sparse sensor configurations and can
infer the anatomical locations of the sensors. These results underscore the
potential of SSPINNpose as a scalable and adaptable solution for real-time
biomechanical analysis in both laboratory and field environments.

</details>


### [174] [Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature Attributions? A Synthetic Data Investigation](https://arxiv.org/abs/2506.11790)
*Gregor Baer,Isel Grau,Chao Zhang,Pieter Van Gorp*

Main category: cs.LG

TL;DR: 研究探讨了特征归因方法评估中的类别依赖性效应，发现扰动指标与真实指标常矛盾，需谨慎解释。


<details>
  <summary>Details</summary>
Motivation: 评估特征归因方法时，扰动指标常被使用但可能不可靠，需探究其局限性。

Method: 通过合成时间序列数据实验，对比扰动指标与真实指标，分析类别依赖性效应。

Result: 实验显示两类指标常矛盾，类别依赖性效应普遍存在。

Conclusion: 需开发更全面的评估框架，谨慎使用扰动指标。

Abstract: Evaluating feature attribution methods represents a critical challenge in
explainable AI (XAI), as researchers typically rely on perturbation-based
metrics when ground truth is unavailable. However, recent work demonstrates
that these evaluation metrics can show different performance across predicted
classes within the same dataset. These "class-dependent evaluation effects"
raise questions about whether perturbation analysis reliably measures
attribution quality, with direct implications for XAI method development and
the trustworthiness of evaluation techniques. We investigate under which
conditions these class-dependent effects arise by conducting controlled
experiments with synthetic time series data where ground truth feature
locations are known. We systematically vary feature types and class contrasts
across binary classification tasks, then compare perturbation-based degradation
scores with ground truth-based precision-recall metrics using multiple
attribution methods. Our experiments demonstrate that class-dependent effects
emerge with both evaluation approaches even in simple scenarios with temporally
localized features, triggered by basic variations in feature amplitude or
temporal extent between classes. Most critically, we find that
perturbation-based and ground truth metrics frequently yield contradictory
assessments of attribution quality across classes, with weak correlations
between evaluation approaches. These findings suggest that researchers should
interpret perturbation-based metrics with care, as they may not always align
with whether attributions correctly identify discriminating features. These
findings reveal opportunities to reconsider what attribution evaluation
actually measures and to develop more comprehensive evaluation frameworks that
capture multiple dimensions of attribution quality.

</details>


### [175] [SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks](https://arxiv.org/abs/2506.11791)
*Hwiwon Lee,Ziqi Zhang,Hanxiao Lu,Lingming Zhang*

Main category: cs.LG

TL;DR: SEC-bench是首个自动化评估LLM代理在真实安全工程任务中的框架，填补了现有基准的不足，揭示了当前LLM代理在PoC生成和漏洞修复上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法捕捉安全工程师实际面临的复杂性和模糊性，需要更真实的评估框架。

Method: SEC-bench采用多代理架构，自动构建代码仓库、复现漏洞并生成补丁，成本低至每实例0.87美元。

Result: 评估显示，当前LLM代理在PoC生成和漏洞修复上的成功率最高仅为18.0%和34.0%。

Conclusion: SEC-bench为开发更实用、智能和自主的安全工程LLM代理提供了关键步骤。

Abstract: Rigorous security-focused evaluation of large language model (LLM) agents is
imperative for establishing trust in their safe deployment throughout the
software development lifecycle. However, existing benchmarks largely rely on
synthetic challenges or simplified vulnerability datasets that fail to capture
the complexity and ambiguity encountered by security engineers in practice. We
introduce SEC-bench, the first fully automated benchmarking framework for
evaluating LLM agents on authentic security engineering tasks. SEC-bench
employs a novel multi-agent scaffold that automatically constructs code
repositories with harnesses, reproduces vulnerabilities in isolated
environments, and generates gold patches for reliable evaluation. Our framework
automatically creates high-quality software vulnerability datasets with
reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,
we implement two critical software security tasks to rigorously evaluate LLM
agents' capabilities: proof-of-concept (PoC) generation and vulnerability
patching. A comprehensive evaluation of state-of-the-art LLM code agents
reveals significant performance gaps, achieving at most 18.0% success in PoC
generation and 34.0% in vulnerability patching on our complete dataset. These
results highlight the crucial steps needed toward developing LLM agents that
are more practical, intelligent, and autonomous for security engineering.

</details>


### [176] [TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks](https://arxiv.org/abs/2506.11844)
*Qihai Zhang,Xinyue Sheng,Yuanfu Sun,Qiaoyu Tan*

Main category: cs.LG

TL;DR: 论文研究了基于大语言模型（LLM）的图学习框架（GraphLLMs）的鲁棒性，提出了TrustGLM评估其对抗攻击的脆弱性，并探索了防御方法。


<details>
  <summary>Details</summary>
Motivation: GraphLLMs在文本属性、图结构和任务提示方面表现出潜力，但其对抗攻击的鲁棒性尚未充分研究，这对高风险场景部署至关重要。

Method: 通过TrustGLM框架，从文本、图结构和提示三个维度评估GraphLLMs的脆弱性，并测试了数据增强和对抗训练等防御方法。

Result: 实验表明，GraphLLMs对文本攻击（替换语义相似词）高度敏感，图结构攻击和提示随机化也会显著降低性能。防御方法显示出提升鲁棒性的潜力。

Conclusion: 论文揭示了GraphLLMs的脆弱性，提出了防御方案，并开源了工具以促进未来研究。

Abstract: Inspired by the success of large language models (LLMs), there is a
significant research shift from traditional graph learning methods to LLM-based
graph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning
power of LLMs by integrating three key components: the textual attributes of
input nodes, the structural information of node neighborhoods, and
task-specific prompts that guide decision-making. Despite their promise, the
robustness of GraphLLMs against adversarial perturbations remains largely
unexplored-a critical concern for deploying these models in high-stakes
scenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study
evaluating the vulnerability of GraphLLMs to adversarial attacks across three
dimensions: text, graph structure, and prompt manipulations. We implement
state-of-the-art attack algorithms from each perspective to rigorously assess
model resilience. Through extensive experiments on six benchmark datasets from
diverse domains, our findings reveal that GraphLLMs are highly susceptible to
text attacks that merely replace a few semantically similar words in a node's
textual attribute. We also find that standard graph structure attack methods
can significantly degrade model performance, while random shuffling of the
candidate label set in prompt templates leads to substantial performance drops.
Beyond characterizing these vulnerabilities, we investigate defense techniques
tailored to each attack vector through data-augmented training and adversarial
training, which show promising potential to enhance the robustness of
GraphLLMs. We hope that our open-sourced library will facilitate rapid,
equitable evaluation and inspire further innovative research in this field.

</details>


### [177] [In Defense of Defensive Forecasting](https://arxiv.org/abs/2506.11848)
*Juan Carlos Perdomo,Benjamin Recht*

Main category: cs.LG

TL;DR: 本文综述了防御性预测算法，通过纠正过去的错误而非预测未来，以最小化指标为目标。


<details>
  <summary>Details</summary>
Motivation: 防御性预测的目标是将预测问题建模为顺序游戏，确保无论结果如何都能优化性能指标。

Method: 介绍了防御性预测的基础理论，并推导了在线学习、校准、专家建议预测和在线共形预测的简单算法。

Result: 提出了接近最优的算法，适用于多种预测任务。

Conclusion: 防御性预测为预测问题提供了一种稳健且理论完备的框架。

Abstract: This tutorial provides a survey of algorithms for Defensive Forecasting,
where predictions are derived not by prognostication but by correcting past
mistakes. Pioneered by Vovk, Defensive Forecasting frames the goal of
prediction as a sequential game, and derives predictions to minimize metrics no
matter what outcomes occur. We present an elementary introduction to this
general theory and derive simple, near-optimal algorithms for online learning,
calibration, prediction with expert advice, and online conformal prediction.

</details>


### [178] [Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values](https://arxiv.org/abs/2506.11849)
*R. Teal Witter,Yurong Liu,Christopher Musco*

Main category: cs.LG

TL;DR: 提出了一种结合蒙特卡洛采样和线性回归的新方法，用于高效计算概率值（如Shapley值），在多个数据集上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 概率值在可解释AI中广泛应用，但精确计算需指数时间，现有方法效率不足。

Method: 结合蒙特卡洛采样和线性回归，允许用高效计算概率值的任意函数族替代线性回归。

Result: 在8个数据集上，新方法误差显著低于现有技术，如Shapley值误差降低6.5倍。

Conclusion: 新方法灵活高效，为概率值估计提供了更优解决方案。

Abstract: With origins in game theory, probabilistic values like Shapley values,
Banzhaf values, and semi-values have emerged as a central tool in explainable
AI. They are used for feature attribution, data attribution, data valuation,
and more. Since all of these values require exponential time to compute
exactly, research has focused on efficient approximation methods using two
techniques: Monte Carlo sampling and linear regression formulations. In this
work, we present a new way of combining both of these techniques. Our approach
is more flexible than prior algorithms, allowing for linear regression to be
replaced with any function family whose probabilistic values can be computed
efficiently. This allows us to harness the accuracy of tree-based models like
XGBoost, while still producing unbiased estimates. From experiments across
eight datasets, we find that our methods give state-of-the-art performance for
estimating probabilistic values. For Shapley values, the error of our methods
can be $6.5\times$ lower than Permutation SHAP (the most popular Monte Carlo
method), $3.8\times$ lower than Kernel SHAP (the most popular linear regression
method), and $2.6\times$ lower than Leverage SHAP (the prior state-of-the-art
Shapley value estimator). For more general probabilistic values, we can obtain
error $215\times$ lower than the best estimator from prior work.

</details>


### [179] [Robust Molecular Property Prediction via Densifying Scarce Labeled Data](https://arxiv.org/abs/2506.11877)
*Jina Kim,Jeffrey Willette,Bruno Andreis,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的新方法，利用未标记数据解决分子预测模型在分布外化合物上的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 分子预测模型依赖训练数据中的结构，导致对分布外化合物泛化能力差，而药物发现中关键化合物常超出训练集，数据稀缺进一步加剧了问题。

Method: 采用元学习方法，利用未标记数据在分布内和分布外数据之间进行插值，使模型学会如何泛化到训练分布之外。

Result: 在具有显著协变量偏移的真实数据集上，性能显著优于现有方法。

Conclusion: 该方法有效提升了模型在分布外数据上的预测能力，解决了药物发现中的关键挑战。

Abstract: A widely recognized limitation of molecular prediction models is their
reliance on structures observed in the training data, resulting in poor
generalization to out-of-distribution compounds. Yet in drug discovery, the
compounds most critical for advancing research often lie beyond the training
set, making the bias toward the training data particularly problematic. This
mismatch introduces substantial covariate shift, under which standard deep
learning models produce unstable and inaccurate predictions. Furthermore, the
scarcity of labeled data, stemming from the onerous and costly nature of
experimental validation, further exacerbates the difficulty of achieving
reliable generalization. To address these limitations, we propose a novel
meta-learning-based approach that leverages unlabeled data to interpolate
between in-distribution (ID) and out-of-distribution (OOD) data, enabling the
model to meta-learn how to generalize beyond the training distribution. We
demonstrate significant performance gains over state-of-the-art methods on
challenging real-world datasets that exhibit substantial covariate shift.

</details>


### [180] [An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing](https://arxiv.org/abs/2506.11882)
*Haochen Sun,Yifan Liu,Ahmed Al-Tahmeesschi,Swarna Chetty,Syed Ali Raza Zaidi,Avishek Nag,Hamed Ahmadi*

Main category: cs.LG

TL;DR: 提出了一种基于可解释深度强化学习（XRL）的动态网络切片和资源分配框架，用于车载网络，通过Shapley值和注意力机制提高决策的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 车载网络需要满足多样化的服务需求（如eMBB和URLLC），但现有方法在资源分配和网络切片中缺乏可解释性和可靠性。

Method: 结合Shapley值和注意力机制的可解释深度强化学习框架，通过近实时RAN智能控制器实现动态资源分配。

Result: 仿真结果显示，该方法提高了资源分配的透明度和精确性，URLLC服务的QoS满意度从78.0%提升至80.13%，eMBB服务从71.44%提升至73.21%。

Conclusion: 该框架显著提升了车载网络资源分配的可解释性和服务质量，为未来智能网络管理提供了有效工具。

Abstract: Effective resource management and network slicing are essential to meet the
diverse service demands of vehicular networks, including Enhanced Mobile
Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC).
This paper introduces an Explainable Deep Reinforcement Learning (XRL)
framework for dynamic network slicing and resource allocation in vehicular
networks, built upon a near-real-time RAN intelligent controller. By
integrating a feature-based approach that leverages Shapley values and an
attention mechanism, we interpret and refine the decisions of our
reinforcementlearning agents, addressing key reliability challenges in
vehicular communication systems. Simulation results demonstrate that our
approach provides clear, real-time insights into the resource allocation
process and achieves higher interpretability precision than a pure attention
mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC
services increased from 78.0% to 80.13%, while that for eMBB services improved
from 71.44% to 73.21%.

</details>


### [181] [Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity](https://arxiv.org/abs/2506.11891)
*Ningyuan Huang,Miguel Sarabia,Abhinav Moudgil,Pau Rodriguez,Luca Zappella,Federico Danieli*

Main category: cs.LG

TL;DR: Mamba的输入选择性功能及其与其他操作的交互机制尚不明确，本文通过理论分析和实验验证，揭示了输入选择性在函数逼近、长期记忆和关联召回中的作用。


<details>
  <summary>Details</summary>
Motivation: 探讨Mamba中输入选择性的作用及其与其他操作的交互，以提升对其机制的理解并发现改进机会。

Method: 通过理论分析（如证明S6层可表示Haar小波投影）和实验验证（如MQAR任务），研究输入选择性的影响。

Result: 发现S6层在逼近不连续函数、动态抵消记忆衰减及关联召回任务中表现优越。

Conclusion: 研究提供了对Mamba机制的深入理解，并指出了改进方向。

Abstract: State-Space Models (SSMs), and particularly Mamba, have recently emerged as a
promising alternative to Transformers. Mamba introduces input selectivity to
its SSM layer (S6) and incorporates convolution and gating into its block
definition. While these modifications do improve Mamba's performance over its
SSM predecessors, it remains largely unclear how Mamba leverages the additional
functionalities provided by input selectivity, and how these interact with the
other operations in the Mamba architecture. In this work, we demystify the role
of input selectivity in Mamba, investigating its impact on function
approximation power, long-term memorization, and associative recall
capabilities. In particular: (i) we prove that the S6 layer of Mamba can
represent projections onto Haar wavelets, providing an edge over its Diagonal
SSM (S4D) predecessor in approximating discontinuous functions commonly arising
in practice; (ii) we show how the S6 layer can dynamically counteract memory
decay; (iii) we provide analytical solutions to the MQAR associative recall
task using the Mamba architecture with different mixers -- Mamba, Mamba-2, and
S4D. We demonstrate the tightness of our theoretical constructions with
empirical results on concrete tasks. Our findings offer a mechanistic
understanding of Mamba and reveal opportunities for improvement.

</details>


### [182] [Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices](https://arxiv.org/abs/2506.11892)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Basil AsSadhan,Fabio Roli*

Main category: cs.LG

TL;DR: 该论文提出了一种针对基于Transformer的调制分类的防御系统，通过紧凑型Transformer增强对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer在自动调制分类中的成功应用及其对对抗样本的脆弱性，需要一种计算高效的防御方法，特别是在资源受限的环境中。

Method: 提出了一种紧凑型Transformer，通过从大型Transformer中转移对抗注意力图来增强鲁棒性。

Result: 新方法在包括快速梯度法和投影梯度下降攻击的白盒场景中优于现有技术。

Conclusion: 该方法能有效保护Transformer免受对抗样本的转移性影响。

Abstract: Due to great success of transformers in many applications such as natural
language processing and computer vision, transformers have been successfully
applied in automatic modulation classification. We have shown that
transformer-based radio signal classification is vulnerable to imperceptible
and carefully crafted attacks called adversarial examples. Therefore, we
propose a defense system against adversarial examples in transformer-based
modulation classifications. Considering the need for computationally efficient
architecture particularly for Internet of Things (IoT)-based applications or
operation of devices in environment where power supply is limited, we propose a
compact transformer for modulation classification. The advantages of robust
training such as adversarial training in transformers may not be attainable in
compact transformers. By demonstrating this, we propose a novel compact
transformer that can enhance robustness in the presence of adversarial attacks.
The new method is aimed at transferring the adversarial attention map from the
robustly trained large transformer to a compact transformer. The proposed
method outperforms the state-of-the-art techniques for the considered white-box
scenarios including fast gradient method and projected gradient descent
attacks. We have provided reasoning of the underlying working mechanisms and
investigated the transferability of the adversarial examples between different
architectures. The proposed method has the potential to protect the transformer
from the transferability of adversarial examples.

</details>


### [183] [Measurement-aligned Flow for Inverse Problem](https://arxiv.org/abs/2506.11893)
*Shaorong Zhang,Rob Brekelmans,Yunshu Wu,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 提出了一种名为MAS的新框架，用于解决线性逆问题，能更灵活地平衡先验和测量信息，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以正确处理先验和测量信号之间的冲突，尤其是在非高斯或未知噪声的情况下。

Method: 提出了Measurement-Aligned Sampling (MAS)框架，统一并扩展了DDNM和DAPS等方法，提供了新的优化视角。

Result: 实验表明，MAS在处理已知高斯噪声、未知或非高斯噪声时均优于现有方法。

Conclusion: MAS是一种强大的新框架，能够更灵活地解决线性逆问题，并在多种任务中表现优异。

Abstract: Diffusion models provide a powerful way to incorporate complex prior
information for solving inverse problems. However, existing methods struggle to
correctly incorporate guidance from conflicting signals in the prior and
measurement, especially in the challenging setting of non-Gaussian or unknown
noise. To bridge these gaps, we propose Measurement-Aligned Sampling (MAS), a
novel framework for linear inverse problem solving that can more flexibly
balance prior and measurement information. MAS unifies and extends existing
approaches like DDNM and DAPS, and offers a new optimization perspective. MAS
can generalize to handle known Gaussian noise, unknown or non-Gaussian noise
types. Extensive experiments show that MAS consistently outperforms
state-of-the-art methods across a range of tasks.

</details>


### [184] [Scalable Generalized Bayesian Online Neural Network Training for Sequential Decision Making](https://arxiv.org/abs/2506.11898)
*Gerardo Duran-Martin,Leandro Sánchez-Betancourt,Álvaro Cartea,Kevin Murphy*

Main category: cs.LG

TL;DR: 提出了一种用于在线学习和广义贝叶斯推理的可扩展算法，适用于序列决策任务，结合了频率派和贝叶斯滤波的优势。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络参数在线学习和贝叶斯推理的挑战，支持快速更新和决策。

Method: 采用块对角近似参数误差协方差的低秩更新方法，隐藏层参数使用低秩误差协方差，最后一层参数使用全秩误差协方差。

Result: 实验表明，该方法在非稳态上下文赌博机和贝叶斯优化问题上实现了速度与精度的平衡。

Conclusion: 该方法无需重放缓冲区或离线训练，能够在线更新所有网络参数，且后验预测分布定义良好。

Abstract: We introduce scalable algorithms for online learning and generalized Bayesian
inference of neural network parameters, designed for sequential decision making
tasks. Our methods combine the strengths of frequentist and Bayesian filtering,
which include fast low-rank updates via a block-diagonal approximation of the
parameter error covariance, and a well-defined posterior predictive
distribution that we use for decision making. More precisely, our main method
updates a low-rank error covariance for the hidden layers parameters, and a
full-rank error covariance for the final layer parameters. Although this
characterizes an improper posterior, we show that the resulting posterior
predictive distribution is well-defined. Our methods update all network
parameters online, with no need for replay buffers or offline retraining. We
show, empirically, that our methods achieve a competitive tradeoff between
speed and accuracy on (non-stationary) contextual bandit problems and Bayesian
optimization problems.

</details>


### [185] [A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification](https://arxiv.org/abs/2506.11901)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Fabio Roli*

Main category: cs.LG

TL;DR: 本文提出了一种神经拒绝系统，用于防御通用对抗扰动，显著提高了深度神经网络的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在无线电信号分类中表现优异，但对抗扰动会显著降低其性能，尤其是通用对抗扰动因其数据独立性更具威胁。

Method: 研究并评估了一种神经拒绝系统，通过生成白盒通用对抗扰动来测试其防御效果。

Result: 实验表明，神经拒绝系统能显著提高对通用对抗扰动的防御准确性。

Conclusion: 神经拒绝系统是一种有效的防御通用对抗扰动的方法，优于未防御的深度神经网络。

Abstract: Advantages of deep learning over traditional methods have been demonstrated
for radio signal classification in the recent years. However, various
researchers have discovered that even a small but intentional feature
perturbation known as adversarial examples can significantly deteriorate the
performance of the deep learning based radio signal classification. Among
various kinds of adversarial examples, universal adversarial perturbation has
gained considerable attention due to its feature of being data independent,
hence as a practical strategy to fool the radio signal classification with a
high success rate. Therefore, in this paper, we investigate a defense system
called neural rejection system to propose against universal adversarial
perturbations, and evaluate its performance by generating white-box universal
adversarial perturbations. We show that the proposed neural rejection system is
able to defend universal adversarial perturbations with significantly higher
accuracy than the undefended deep neural network.

</details>


### [186] [TreeRL: LLM Reinforcement Learning with On-Policy Tree Search](https://arxiv.org/abs/2506.11902)
*Zhenyu Hou,Ziniu Hu,Yujiang Li,Rui Lu,Jie Tang,Yuxiao Dong*

Main category: cs.LG

TL;DR: TreeRL是一个结合树搜索的强化学习框架，用于LLM的RL训练，通过中间监督和高效树搜索策略提升性能，无需单独奖励模型。


<details>
  <summary>Details</summary>
Motivation: 传统独立链采样策略在RL训练中探索能力有限，且需要单独训练奖励模型，存在分布不匹配和奖励攻击问题。

Method: 提出TreeRL框架，直接结合树搜索进行RL训练，引入中间监督，采用高效树搜索策略（从高不确定性步骤分支）。

Result: 在数学和代码推理任务上，TreeRL性能优于传统ChainRL。

Conclusion: 树搜索在LLM的RL训练中具有潜力，TreeRL框架高效且开源。

Abstract: Reinforcement learning (RL) with tree search has demonstrated superior
performance in traditional reasoning tasks. Compared to conventional
independent chain sampling strategies with outcome supervision, tree search
enables better exploration of the reasoning space and provides dense, on-policy
process rewards during RL training but remains under-explored in On-Policy LLM
RL. We propose TreeRL, a reinforcement learning framework that directly
incorporates on-policy tree search for RL training. Our approach includes
intermediate supervision and eliminates the need for a separate reward model
training. Existing approaches typically train a separate process reward model,
which can suffer from distribution mismatch and reward hacking. We also
introduce a cost-effective tree search approach that achieves higher search
efficiency under the same generation token budget by strategically branching
from high-uncertainty intermediate steps rather than using random branching.
Experiments on challenging math and code reasoning benchmarks demonstrate that
TreeRL achieves superior performance compared to traditional ChainRL,
highlighting the potential of tree search for LLM. TreeRL is open-sourced at
https://github.com/THUDM/TreeRL.

</details>


### [187] [Spectra-to-Structure and Structure-to-Spectra Inference Across the Periodic Table](https://arxiv.org/abs/2506.11908)
*Yufeng Wang,Peiyao Wang,Lu Ma,Yuewei Lin,Qun Liu,Haibin Ling*

Main category: cs.LG

TL;DR: XAStruct是一个机器学习框架，用于从晶体结构预测XAS光谱或从XAS输入推断局部结构描述符，支持70多种元素，无需特定元素调整。


<details>
  <summary>Details</summary>
Motivation: XAS解释需要专家分析、计算成本高的模拟和特定元素的启发式方法，限制了其应用。机器学习有望加速XAS解释，但现有模型局限于特定元素或谱区。

Method: XAStruct结合深度神经网络和高效基线模型，训练于涵盖70多种元素的大规模数据集，支持光谱预测和结构推断。

Result: 模型能预测邻近原子类型和平均最近邻距离，但端到端模型性能下降，因此任务独立训练以确保准确性。

Conclusion: XAStruct为数据驱动的XAS分析和局部结构推断提供了可扩展的解决方案，代码将在论文接受后发布。

Abstract: X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local
atomic environments, yet its interpretation remains limited by the need for
expert-driven analysis, computationally expensive simulations, and
element-specific heuristics. Recent advances in machine learning have shown
promise for accelerating XAS interpretation, but many existing models are
narrowly focused on specific elements, edge types, or spectral regimes. In this
work, we present XAStruct, a learning framework capable of both predicting XAS
spectra from crystal structures and inferring local structural descriptors from
XAS input. XAStruct is trained on a large-scale dataset spanning over 70
elements across the periodic table, enabling generalization to a wide variety
of chemistries and bonding environments. The model includes the first machine
learning approach for predicting neighbor atom types directly from XAS spectra,
as well as a unified regression model for mean nearest-neighbor distance that
requires no element-specific tuning. While we explored integrating the two
pipelines into a single end-to-end model, empirical results showed performance
degradation. As a result, the two tasks were trained independently to ensure
optimal accuracy and task-specific performance. By combining deep neural
networks for complex structure-property mappings with efficient baseline models
for simpler tasks, XAStruct offers a scalable and extensible solution for
data-driven XAS analysis and local structure inference. The source code will be
released upon paper acceptance.

</details>


### [188] [Breaking Habits: On the Role of the Advantage Function in Learning Causal State Representations](https://arxiv.org/abs/2506.11912)
*Miguel Suau*

Main category: cs.LG

TL;DR: 优势函数不仅减少梯度估计的方差，还能缓解策略混淆问题，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理可能因策略混淆问题而无法泛化，需解决此问题。

Method: 利用优势函数调整动作值，削弱与当前策略相关的伪相关性。

Result: 实验证明优势函数能提升策略在非轨迹数据上的表现。

Conclusion: 优势函数是解决策略混淆问题的有效工具。

Abstract: Recent work has shown that reinforcement learning agents can develop policies
that exploit spurious correlations between rewards and observations. This
phenomenon, known as policy confounding, arises because the agent's policy
influences both past and future observation variables, creating a feedback loop
that can hinder the agent's ability to generalize beyond its usual
trajectories. In this paper, we show that the advantage function, commonly used
in policy gradient methods, not only reduces the variance of gradient estimates
but also mitigates the effects of policy confounding. By adjusting action
values relative to the state representation, the advantage function downweights
state-action pairs that are more likely under the current policy, breaking
spurious correlations and encouraging the agent to focus on causal factors. We
provide both analytical and empirical evidence demonstrating that training with
the advantage function leads to improved out-of-trajectory performance.

</details>


### [189] [Visual Pre-Training on Unlabeled Images using Reinforcement Learning](https://arxiv.org/abs/2506.11967)
*Dibya Ghosh,Sergey Levine*

Main category: cs.LG

TL;DR: 将自监督图像预训练类比为强化学习问题，通过奖励函数优化特征学习。


<details>
  <summary>Details</summary>
Motivation: 探索如何将无标签图像数据（如网络爬取和视频帧）的预训练直接转化为强化学习问题，以改进特征学习。

Method: 训练一个通用价值函数，通过改变视图或添加图像增强来转换图像，类似于作物一致性自监督学习。

Result: 在EpicKitchens、COCO和CC12M等无标签图像数据上训练时，表现优于传统方法。

Conclusion: 通过强化学习框架优化自监督预训练，能够有效提升特征学习效果。

Abstract: In reinforcement learning (RL), value-based algorithms learn to associate
each observation with the states and rewards that are likely to be reached from
it. We observe that many self-supervised image pre-training methods bear
similarity to this formulation: learning features that associate crops of
images with those of nearby views, e.g., by taking a different crop or color
augmentation. In this paper, we complete this analogy and explore a method that
directly casts pre-training on unlabeled image data like web crawls and video
frames as an RL problem. We train a general value function in a dynamical
system where an agent transforms an image by changing the view or adding image
augmentations. Learning in this way resembles crop-consistency
self-supervision, but through the reward function, offers a simple lever to
shape feature learning using curated images or weakly labeled captions when
they exist. Our experiments demonstrate improved representations when training
on unlabeled images in the wild, including video data like EpicKitchens, scene
data like COCO, and web-crawl data like CC12M.

</details>


### [190] [Self-Regulating Cars: Automating Traffic Control in Free Flow Road Networks](https://arxiv.org/abs/2506.11973)
*Ankit Bhardwaj,Rohail Asim,Sachin Chauhan,Yasir Zaki,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的自调节车辆协议，通过动态调节车速优化交通流量，无需新增基础设施。


<details>
  <summary>Details</summary>
Motivation: 郊区高速公路等自由流路网因通勤流量增加和基础设施有限而日益拥堵，传统控制机制在此环境中效果不佳。

Method: 结合经典交通流理论、间隙接受模型和微观模拟，构建物理信息强化学习框架，将道路抽象为超级段以捕捉动态流量。

Result: 在PTV Vissim模拟器中测试，该方法提升总吞吐量5%，减少平均延迟13%，降低停车次数3%。

Conclusion: 该方法能实现平滑、抗拥堵的交通流，并适应不同交通模式，展示了可扩展的机器学习驱动交通管理潜力。

Abstract: Free-flow road networks, such as suburban highways, are increasingly
experiencing traffic congestion due to growing commuter inflow and limited
infrastructure. Traditional control mechanisms, such as traffic signals or
local heuristics, are ineffective or infeasible in these high-speed,
signal-free environments. We introduce self-regulating cars, a reinforcement
learning-based traffic control protocol that dynamically modulates vehicle
speeds to optimize throughput and prevent congestion, without requiring new
physical infrastructure. Our approach integrates classical traffic flow theory,
gap acceptance models, and microscopic simulation into a physics-informed RL
framework. By abstracting roads into super-segments, the agent captures
emergent flow dynamics and learns robust speed modulation policies from
instantaneous traffic observations. Evaluated in the high-fidelity PTV Vissim
simulator on a real-world highway network, our method improves total throughput
by 5%, reduces average delay by 13%, and decreases total stops by 3% compared
to the no-control setting. It also achieves smoother, congestion-resistant flow
while generalizing across varied traffic patterns, demonstrating its potential
for scalable, ML-driven traffic management.

</details>


### [191] [Compression Aware Certified Training](https://arxiv.org/abs/2506.11992)
*Changming Xu,Gagandeep Singh*

Main category: cs.LG

TL;DR: CACTUS框架统一了深度神经网络的压缩与认证鲁棒性训练，在保持高效压缩的同时实现高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键、资源受限的环境中，现有方法将压缩和认证鲁棒性视为独立目标，导致效率或安全性妥协。

Method: 提出CACTUS框架，通过联合训练实现压缩和认证鲁棒性的统一，适用于剪枝和量化。

Result: CACTUS在多种数据集和输入规格上实现了剪枝和量化的最先进精度与认证性能。

Conclusion: CACTUS为资源受限环境中的高效且鲁棒的神经网络提供了可行解决方案。

Abstract: Deep neural networks deployed in safety-critical, resource-constrained
environments must balance efficiency and robustness. Existing methods treat
compression and certified robustness as separate goals, compromising either
efficiency or safety. We propose CACTUS (Compression Aware Certified Training
Using network Sets), a general framework for unifying these objectives during
training. CACTUS models maintain high certified accuracy even when compressed.
We apply CACTUS for both pruning and quantization and show that it effectively
trains models which can be efficiently compressed while maintaining high
accuracy and certifiable robustness. CACTUS achieves state-of-the-art accuracy
and certified performance for both pruning and quantization on a variety of
datasets and input specifications.

</details>


### [192] [pLSTM: parallelizable Linear Source Transition Mark networks](https://arxiv.org/abs/2506.11997)
*Korbinian Pöppel,Richard Freinschlag,Thomas Schmied,Wei Lin,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 论文提出了一种并行化的线性源转换标记网络（pLSTM），适用于处理多维数据结构（如DAGs），解决了传统RNN在长距离依赖中的梯度消失/爆炸问题，并在分子图和计算机视觉任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代循环架构（如xLSTM和Mamba）在语言建模中挑战了Transformer，但其结构限制了其在多维数据（如图像或分子图）中的应用。MDRNNs虽适合多维数据，但缺乏并行化能力。本文旨在扩展多维性到线性RNNs，并提出并行化解决方案。

Method: 提出pLSTM，利用源、转换和标记门作用于DAG的线图，实现并行化。通过P模式和D模式解决长距离梯度问题，并在图像任务中验证其有效性。

Result: pLSTM在长距离依赖任务（如箭头指向外推）中表现优于Transformer，并在分子图和计算机视觉基准测试中取得强性能。

Conclusion: pLSTM为处理多维数据提供了一种高效并行化的解决方案，解决了传统RNN的局限性，并在实际任务中展示了优越性。

Abstract: Modern recurrent architectures, such as xLSTM and Mamba, have recently
challenged the Transformer in language modeling. However, their structure
constrains their applicability to sequences only or requires processing
multi-dimensional data structures, such as images or molecular graphs, in a
pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are
well suited for data with a higher level structure, like 2D grids, trees, and
directed acyclic graphs (DAGs). In this work, we extend the notion of
multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source
Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that
act on the line graph of a general DAG. This enables parallelization in analogy
to parallel associative scans and the chunkwise-recurrent form of sequential
linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this
scheme can be efficiently implemented using einsum operations, concatenations,
and padding in logarithmic time. pLSTMs tackle the vanishing/exploding
activation/gradient problem for long distances in DAGs via two distinct modes:
a directed propagation mode (P-mode) and a diffusive distribution mode
(D-mode). To showcase the long-range capabilities of pLSTM, we introduce
arrow-pointing extrapolation as a synthetic computer vision task that contains
long-distance directional information. We demonstrate that pLSTMs generalize
well to larger image sizes, whereas Transformers struggle to extrapolate. On
established molecular graph and computer vision benchmarks, pLSTMs also show
strong performance. Code and Datasets are available at:
https://github.com/ml-jku/plstm_experiments.

</details>


### [193] [An Efficient Compression of Deep Neural Network Checkpoints Based on Prediction and Context Modeling](https://arxiv.org/abs/2506.12000)
*Yuriy Kim,Evgeny Belyaev*

Main category: cs.LG

TL;DR: 提出一种基于预测的压缩方法，结合剪枝和量化，显著减少检查点文件大小，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络训练过程中检查点文件存储占用大的问题，适用于存储受限环境。

Method: 使用基于预测的压缩方法（算术编码），并结合剪枝和量化技术。

Result: 实验显示该方法显著减小文件大小，且恢复后训练几乎无损。

Conclusion: 该方法高效压缩检查点文件，适用于存储受限场景，同时保持模型性能。

Abstract: This paper is dedicated to an efficient compression of weights and optimizer
states (called checkpoints) obtained at different stages during a neural
network training process. First, we propose a prediction-based compression
approach, where values from the previously saved checkpoint are used for
context modeling in arithmetic coding. Second, in order to enhance the
compression performance, we also propose to apply pruning and quantization of
the checkpoint values. Experimental results show that our approach achieves
substantial bit size reduction, while enabling near-lossless training recovery
from restored checkpoints, preserving the model's performance and making it
suitable for storage-limited environments.

</details>


### [194] [SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts](https://arxiv.org/abs/2506.12007)
*Paul Setinek,Gianluca Galletti,Thomas Gross,Dominik Schnürer,Johannes Brandstetter,Werner Zellinger*

Main category: cs.LG

TL;DR: 论文提出了SIMSHIFT数据集，并研究了领域适应方法在神经代理模型中的应用，以解决PDE求解中未见配置的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 神经代理模型在PDE求解中对未见配置（如新材料或结构）性能下降明显，而领域适应技术在视觉和语言处理中已广泛应用。

Method: 引入SIMSHIFT数据集，扩展领域适应方法至神经代理模型，并系统评估其性能。

Result: 实验表明领域适应在模拟任务中具有潜力，但也揭示了分布偏移下神经代理模型的挑战。

Conclusion: 领域适应方法在工业相关场景中具有应用前景，但仍需解决分布偏移下的鲁棒性问题。

Abstract: Neural surrogates for Partial Differential Equations (PDEs) often suffer
significant performance degradation when evaluated on unseen problem
configurations, such as novel material types or structural dimensions.
Meanwhile, Domain Adaptation (DA) techniques have been widely used in vision
and language processing to generalize from limited information about unseen
configurations. In this work, we address this gap through two focused
contributions. First, we introduce SIMSHIFT, a novel benchmark dataset and
evaluation suite composed of four industrial simulation tasks: hot rolling,
sheet metal forming, electric motor design and heatsink design. Second, we
extend established domain adaptation methods to state of the art neural
surrogates and systematically evaluate them. These approaches use parametric
descriptions and ground truth simulations from multiple source configurations,
together with only parametric descriptions from target configurations. The goal
is to accurately predict target simulations without access to ground truth
simulation data. Extensive experiments on SIMSHIFT highlight the challenges of
out of distribution neural surrogate modeling, demonstrate the potential of DA
in simulation, and reveal open problems in achieving robust neural surrogates
under distribution shifts in industrially relevant scenarios. Our codebase is
available at https://github.com/psetinek/simshift

</details>


### [195] [EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction](https://arxiv.org/abs/2506.12015)
*Hsi-Che Lin,Yu-Chu Yu,Kai-Po Chang,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: EMLoC是一种基于模拟器的内存高效微调框架，通过LoRA校正，使模型微调在推理所需的内存预算内完成。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型在微调时内存开销大，限制了普通用户的使用。

Method: EMLoC利用激活感知SVD构建轻量级模拟器，通过LoRA微调，并提出补偿算法校正LoRA模块。

Result: 实验表明EMLoC在多个数据集和模态上优于基线方法，支持在单张24GB GPU上微调38B模型。

Conclusion: EMLoC为个体用户提供了高效、实用的模型适配方案。

Abstract: Open-source foundation models have seen rapid adoption and development,
enabling powerful general-purpose capabilities across diverse domains. However,
fine-tuning large foundation models for domain-specific or personalized tasks
remains prohibitively expensive for most users due to the significant memory
overhead beyond that of inference. We introduce EMLoC, an Emulator-based
Memory-efficient fine-tuning framework with LoRA Correction, which enables
model fine-tuning within the same memory budget required for inference. EMLoC
constructs a task-specific light-weight emulator using activation-aware
singular value decomposition (SVD) on a small downstream calibration set.
Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle
the misalignment between the original model and the compressed emulator, we
propose a novel compensation algorithm to correct the fine-tuned LoRA module,
which thus can be merged into the original model for inference. EMLoC supports
flexible compression ratios and standard training pipelines, making it
adaptable to a wide range of applications. Extensive experiments demonstrate
that EMLoC outperforms other baselines across multiple datasets and modalities.
Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a
single 24GB consumer GPU-bringing efficient and practical model adaptation to
individual users.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [196] [Solving Inverse Problems in Stochastic Self-Organising Systems through Invariant Representations](https://arxiv.org/abs/2506.11796)
*Elias Najarro,Nicolas Bessone,Sebastian Risi*

Main category: nlin.AO

TL;DR: 提出了一种新的逆向建模方法，用于处理可观测空间中的随机性，通过视觉嵌入捕捉感知不变性，从而有效恢复未知因果参数。


<details>
  <summary>Details</summary>
Motivation: 自组织系统通过简单的局部规则生成复杂随机模式，但传统逆向方法难以处理强随机性观测。

Method: 利用视觉嵌入将模式表示映射到不变嵌入空间，无需手工设计目标函数或启发式方法。

Result: 在反应扩散系统和社会隔离的基于代理模型中，方法可靠地恢复了参数，并成功应用于真实生物模式。

Conclusion: 该方法为理论和实验研究复杂随机模式形成的动力学提供了潜在工具。

Abstract: Self-organising systems demonstrate how simple local rules can generate
complex stochastic patterns. Many natural systems rely on such dynamics, making
self-organisation central to understanding natural complexity. A fundamental
challenge in modelling such systems is solving the inverse problem: finding the
unknown causal parameters from macroscopic observations. This task becomes
particularly difficult when observations have a strong stochastic component,
yielding diverse yet equivalent patterns. Traditional inverse methods fail in
this setting, as pixel-wise metrics cannot capture feature similarities between
variable outcomes. In this work, we introduce a novel inverse modelling method
specifically designed to handle stochasticity in the observable space,
leveraging the capacity of visual embeddings to produce robust representations
that capture perceptual invariances. By mapping the pattern representations
onto an invariant embedding space, we can effectively recover unknown causal
parameters without the need for handcrafted objective functions or heuristics.
We evaluate the method on two canonical models--a reaction-diffusion system and
an agent-based model of social segregation--and show that it reliably recovers
parameters despite stochasticity in the outcomes. We further apply the method
to real biological patterns, highlighting its potential as a tool for both
theorists and experimentalists to investigate the dynamics underlying complex
stochastic pattern formation.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [197] [Bubble Dynamics Transformer: Microrheology at Ultra-High Strain Rates](https://arxiv.org/abs/2506.11936)
*Lehu Bu,Zhaohan Yu,Shaoting Lin,Jan N. Fuhg,Jin Yang*

Main category: physics.flu-dyn

TL;DR: 论文提出了一种基于机器学习的微流变学框架，利用激光诱导惯性空化（LIC）和新型Bubble Dynamics Transformer（BDT）神经网络，快速、准确地表征生物材料在超高应变率下的粘弹性特性。


<details>
  <summary>Details</summary>
Motivation: 传统流变学工具在高应变率（>1000 1/s）下受限于加载速度、分辨率或侵入性，无法有效研究软生物材料的力学特性。

Method: 通过超高速成像捕捉LIC事件中气泡半径动态，利用基于物理模拟数据训练的BDT神经网络分析数据，推断材料粘弹性参数。

Result: BDT无需迭代拟合或复杂反演过程，即可实现快速、准确、非接触的材料特性表征。

Conclusion: 该方法为生物医学和材料科学在极端加载条件下的软材料研究提供了重要工具。

Abstract: Laser-induced inertial cavitation (LIC)-where microscale vapor bubbles
nucleate due to a focused high-energy pulsed laser and then violently collapse
under surrounding high local pressures-offers a unique opportunity to
investigate soft biological material mechanics at extremely high strain rates
(>1000 1/s). Traditional rheological tools are often limited in these regimes
by loading speed, resolution, or invasiveness. Here we introduce novel machine
learning (ML) based microrheological frameworks that leverage LIC to
characterize the viscoelastic properties of biological materials at ultra-high
strain rates. We utilize ultra-high-speed imaging to capture time-resolved
bubble radius dynamics during LIC events in various soft viscoelastic
materials. These bubble radius versus time measurements are then analyzed using
a newly developed Bubble Dynamics Transformer (BDT), a neural network trained
on physics-based simulation data. The BDT accurately infers material
viscoelastic parameters, eliminating the need for iterative fitting or complex
inversion processes. This enables fast, accurate, and non-contact
characterization of soft materials under extreme loading conditions, with
significant implications for biomedical applications and materials science.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [198] [Few Single-Qubit Measurements Suffice to Certify Any Quantum State](https://arxiv.org/abs/2506.11355)
*Meghal Gupta,William He,Ryan O'Donnell*

Main category: quant-ph

TL;DR: 本文提出了一种高效的方法，仅需O(n²)单量子比特测量和O(n)副本即可验证任意纯态，解决了Huang等人的主要开放性问题。


<details>
  <summary>Details</summary>
Motivation: 量子信息科学中的核心任务是验证实验室制备的n量子比特态是否接近假设态，但此前未知是否能用次指数级单量子比特测量实现验证。

Method: 通过仅使用O(n²)单量子比特测量和O(n)副本，验证任意纯态。

Result: 证明了所有纯态均可通过该方法高效验证，解决了Huang等人的开放性问题。

Conclusion: 该方法显著提升了量子态验证的效率，为量子信息科学提供了重要工具。

Abstract: A fundamental task in quantum information science is \emph{state
certification}: testing whether a lab-prepared $n$-qubit state is close to a
given hypothesis state. In this work, we show that \emph{every} pure hypothesis
state can be certified using only $O(n^2)$ single-qubit measurements applied to
$O(n)$ copies of the lab state. Prior to our work, it was not known whether
even sub-exponentially many single-qubit measurements could suffice to certify
arbitrary states. This resolves the main open question of Huang, Preskill, and
Soleimanifar (FOCS 2024, QIP 2024).

</details>


### [199] [HQFNN: A Compact Quantum-Fuzzy Neural Network for Accurate Image Classification](https://arxiv.org/abs/2506.11146)
*Jianhong Yao,Yangming Guo*

Main category: quant-ph

TL;DR: HQFNN是一种结合模糊推理和量子电路的创新神经网络，具有高效、鲁棒和可解释性，在图像分类任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在噪声输入和模型解释性上的不足，结合模糊推理的透明性和量子电路的高效性。

Method: 通过量子电路实现模糊推理流程，结合轻量级CNN特征提取器，利用量子态映射和规则层优化分类。

Result: 在标准图像基准测试中，HQFNN表现优于传统方法，且对噪声具有鲁棒性，参数效率高。

Conclusion: HQFNN为视觉任务提供了一种紧凑、可解释且鲁棒的替代方案，为未来量子模糊学习框架奠定了基础。

Abstract: Deep learning vision systems excel at pattern recognition yet falter when
inputs are noisy or the model must explain its own confidence. Fuzzy inference,
with its graded memberships and rule transparency, offers a remedy, while
parameterized quantum circuits can embed features in richly entangled Hilbert
spaces with striking parameter efficiency. Bridging these ideas, this study
introduces a innovative Highly Quantized Fuzzy Neural Network (HQFNN) that
realises the entire fuzzy pipeline inside a shallow quantum circuit and couples
the resulting quantum signal to a lightweight CNN feature extractor. Each image
feature is first mapped to a single qubit membership state through repeated
angle reuploading. Then a compact rule layer refines these amplitudes, and a
clustered CNOT defuzzifier collapses them into one crisp value that is fused
with classical features before classification. Evaluated on standard image
benchmarks, HQFNN consistently surpasses classical, fuzzy enhanced and quantum
only baselines while using several orders of magnitude fewer trainable weights,
and its accuracy degrades only marginally under simulated depolarizing and
amplitude damping noise, evidence of intrinsic robustness. Gate count analysis
further shows that circuit depth grows sublinearly with input dimension,
confirming the model's practicality for larger images. These results position
the model as a compact, interpretable and noise tolerant alternative to
conventional vision backbones and provide a template for future quantum native
fuzzy learning frameworks.

</details>


### [200] [Learning Encodings by Maximizing State Distinguishability: Variational Quantum Error Correction](https://arxiv.org/abs/2506.11552)
*Nico Meyer,Christopher Mutschler,Andreas Maier,Daniel D. Scherer*

Main category: quant-ph

TL;DR: 提出了一种基于可区分性损失函数的变分量子纠错方法（VarQEC），针对特定噪声结构优化纠错码，减少资源开销。


<details>
  <summary>Details</summary>
Motivation: 传统纠错码（如表面码）资源开销大，不适合近期的容错设备，需要更高效的纠错方法。

Method: 通过最大化噪声通道后量子态的可区分性，设计了一种可区分性损失函数，结合变分技术实现资源优化的编码电路。

Result: VarQEC在理论和实践中表现优异，优于标准纠错码，并在IBM和IQM硬件上验证了实用性。

Conclusion: VarQEC为噪声特定优化的量子纠错提供了高效且实用的解决方案。

Abstract: Quantum error correction is crucial for protecting quantum information
against decoherence. Traditional codes like the surface code require
substantial overhead, making them impractical for near-term, early
fault-tolerant devices. We propose a novel objective function for tailoring
error correction codes to specific noise structures by maximizing the
distinguishability between quantum states after a noise channel, ensuring
efficient recovery operations. We formalize this concept with the
distinguishability loss function, serving as a machine learning objective to
discover resource-efficient encoding circuits optimized for given noise
characteristics. We implement this methodology using variational techniques,
termed variational quantum error correction (VarQEC). Our approach yields codes
with desirable theoretical and practical properties and outperforms standard
codes in various scenarios. We also provide proof-of-concept demonstrations on
IBM and IQM hardware devices, highlighting the practical relevance of our
procedure.

</details>


### [201] [Interpretable representation learning of quantum data enabled by probabilistic variational autoencoders](https://arxiv.org/abs/2506.11982)
*Paulin de Schoulepnikoff,Gorka Muñoz-Gil,Hendrik Poulsen Nautrup,Hans J. Briegel*

Main category: quant-ph

TL;DR: 论文提出了一种改进的变分自编码器（VAE）方法，用于从量子数据中提取有意义的物理特征，解决了传统VAE忽略量子数据概率特性的问题。


<details>
  <summary>Details</summary>
Motivation: 量子数据的复杂相关性和内在随机性使得传统VAE难以提取有意义的物理特征，因此需要改进VAE以更好地处理量子数据。

Method: 通过引入能够准确再现量子态的解码器和针对量子数据的概率损失函数，改进VAE以学习有意义的潜在表示。

Result: 在量子自旋模型和实验数据中，改进的VAE成功提取了有意义的物理特征，而传统方法在这些情况下失效。

Conclusion: 改进的VAE为量子系统的无监督和可解释研究提供了有效工具。

Abstract: Interpretable machine learning is rapidly becoming a crucial tool for
scientific discovery. Among existing approaches, variational autoencoders
(VAEs) have shown promise in extracting the hidden physical features of some
input data, with no supervision nor prior knowledge of the system at study.
Yet, the ability of VAEs to create meaningful, interpretable representations
relies on their accurate approximation of the underlying probability
distribution of their input. When dealing with quantum data, VAEs must hence
account for its intrinsic randomness and complex correlations. While VAEs have
been previously applied to quantum data, they have often neglected its
probabilistic nature, hindering the extraction of meaningful physical
descriptors. Here, we demonstrate that two key modifications enable VAEs to
learn physically meaningful latent representations: a decoder capable of
faithfully reproduce quantum states and a probabilistic loss tailored to this
task. Using benchmark quantum spin models, we identify regimes where standard
methods fail while the representations learned by our approach remain
meaningful and interpretable. Applied to experimental data from Rydberg atom
arrays, the model autonomously uncovers the phase structure without access to
prior labels, Hamiltonian details, or knowledge of relevant order parameters,
highlighting its potential as an unsupervised and interpretable tool for the
study of quantum systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [202] [LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation](https://arxiv.org/abs/2506.11476)
*Tom Baker,Javier Nistal*

Main category: cs.SD

TL;DR: 提出了一种轻量级模块化架构，显著减少参数数量，同时保持音频质量和条件控制能力，优于ControlNet。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频扩散模型缺乏精细的时间变化控制，且ControlNet方法内存占用大、灵活性不足。

Method: 采用轻量级模块化架构，减少参数数量，同时匹配ControlNet的音频质量和条件控制能力。

Result: 在客观和主观评估中表现优异，提供更低内存占用和更高灵活性。

Conclusion: 该方法显著提升了训练和部署效率，适用于独立控制的高效实现。

Abstract: Text-to-audio diffusion models produce high-quality and diverse music but
many, if not most, of the SOTA models lack the fine-grained, time-varying
controls essential for music production. ControlNet enables attaching external
controls to a pre-trained generative model by cloning and fine-tuning its
encoder on new conditionings. However, this approach incurs a large memory
footprint and restricts users to a fixed set of controls. We propose a
lightweight, modular architecture that considerably reduces parameter count
while matching ControlNet in audio quality and condition adherence. Our method
offers greater flexibility and significantly lower memory usage, enabling more
efficient training and deployment of independent controls. We conduct extensive
objective and subjective evaluations and provide numerous audio examples on the
accompanying website at https://lightlatentcontrol.github.io

</details>


### [203] [Enabling automatic transcription of child-centered audio recordings from real-world environments](https://arxiv.org/abs/2506.11747)
*Daniil Kocharov,Okko Räsänen*

Main category: cs.SD

TL;DR: 提出一种方法，通过自动检测长音频中可被现代ASR系统可靠转录的片段，实现部分语音的自动准确转录，显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 长音频转录对研究儿童语言发展至关重要，但全面手动标注不可行，而现有ASR技术因音频噪声和复杂性难以直接应用。

Method: 自动检测长音频中可被现代ASR系统可靠转录的片段，仅转录这些部分以提高准确性。

Result: 在四个英语长音频语料库中验证，该方法转录13%的语音时，中位WER为0%，平均WER为18%，优于无过滤的转录（中位WER 52%，平均WER 51%）。自动与手动转录的词频相关性高达0.92-0.98。

Conclusion: 该方法为儿童长音频的自动化语言分析提供了可行方案，显著提升了转录准确性。

Abstract: Longform audio recordings obtained with microphones worn by children-also
known as child-centered daylong recordings-have become a standard method for
studying children's language experiences and their impact on subsequent
language development. Transcripts of longform speech audio would enable rich
analyses at various linguistic levels, yet the massive scale of typical
longform corpora prohibits comprehensive manual annotation. At the same time,
automatic speech recognition (ASR)-based transcription faces significant
challenges due to the noisy, unconstrained nature of real-world audio, and no
existing study has successfully applied ASR to transcribe such data. However,
previous attempts have assumed that ASR must process each longform recording in
its entirety. In this work, we present an approach to automatically detect
those utterances in longform audio that can be reliably transcribed with modern
ASR systems, allowing automatic and relatively accurate transcription of a
notable proportion of all speech in typical longform data. We validate the
approach on four English longform audio corpora, showing that it achieves a
median word error rate (WER) of 0% and a mean WER of 18% when transcribing 13%
of the total speech in the dataset. In contrast, transcribing all speech
without any filtering yields a median WER of 52% and a mean WER of 51%. We also
compare word log-frequencies derived from the automatic transcripts with those
from manual annotations and show that the frequencies correlate at r = 0.92
(Pearson) for all transcribed words and r = 0.98 for words that appear at least
five times in the automatic transcripts. Overall, the work provides a concrete
step toward increasingly detailed automated linguistic analyses of
child-centered longform audio.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [204] [Joint Denoising of Cryo-EM Projection Images using Polar Transformers](https://arxiv.org/abs/2506.11283)
*Joakim Andén,Justus Sagemüller*

Main category: eess.IV

TL;DR: 提出一种基于Transformer的神经网络架构，用于同时聚类、对齐和去噪冷冻电镜图像，显著降低噪声。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在高噪声环境（如冷冻电镜图像）中效果有限，但传统分类平均方法利用冗余信息有效去噪。

Method: 设计基于Transformer的架构，同时实现图像聚类、对齐和去噪。

Result: 在合成数据上，相对均方误差降低45%（SNR=0.03）。

Conclusion: 该架构显著提升了冷冻电镜图像的去噪性能。

Abstract: Deep neural networks~(DNNs) have proven powerful for denoising, but they are
ultimately of limited use in high-noise settings, such as for cryogenic
electron microscopy~(cryo-EM) projection images. In this setting, however,
datasets contain a large number of projections of the same molecule, each taken
from a different viewing direction. This redundancy of information is useful in
traditional denoising techniques known as class averaging methods, where images
are clustered, aligned, and then averaged to reduce the noise level. We present
a neural network architecture based on transformers that extends these class
averaging methods by simultaneously clustering, aligning, and denoising cryo-EM
images. Results on synthetic data show accurate denoising performance using
this architecture, reducing the relative mean squared error (MSE) single-image
DNNs by $45\%$ at a signal-to-noise (SNR) of $0.03$.

</details>


### [205] [Score-based Generative Diffusion Models to Synthesize Full-dose FDG Brain PET from MRI in Epilepsy Patients](https://arxiv.org/abs/2506.11297)
*Jiaqi Wu,Jiahong Ouyang,Farshad Moradi,Mohammad Mehdi Khalighi,Greg Zaharchuk*

Main category: eess.IV

TL;DR: 论文比较了扩散模型和非扩散模型在MRI到PET图像转换任务中的表现，发现扩散模型（SGM-KD）在纯MRI输入时表现最佳，而加入低剂量PET后所有模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 针对癫痫患者，减少FDG PET的辐射剂量需求，探索通过MRI或极低剂量PET生成高质量PET图像的AI方法。

Method: 使用52名受试者的同步PET/MRI数据，比较了两种扩散模型（SGM-KD和SGM-VP）和Transformer-Unet在MRI到PET转换任务中的表现。

Result: SGM-KD在纯MRI输入时表现最优，加入1%低剂量PET后所有模型性能显著提升且表现相近。

Conclusion: 扩散模型在纯MRI到PET转换中潜力巨大，而所有模型均可通过MRI和极低剂量PET准确合成全剂量FDG-PET图像。

Abstract: Fluorodeoxyglucose (FDG) PET to evaluate patients with epilepsy is one of the
most common applications for simultaneous PET/MRI, given the need to image both
brain structure and metabolism, but is suboptimal due to the radiation dose in
this young population. Little work has been done synthesizing diagnostic
quality PET images from MRI data or MRI data with ultralow-dose PET using
advanced generative AI methods, such as diffusion models, with attention to
clinical evaluations tailored for the epilepsy population. Here we compared the
performance of diffusion- and non-diffusion-based deep learning models for the
MRI-to-PET image translation task for epilepsy imaging using simultaneous
PET/MRI in 52 subjects (40 train/2 validate/10 hold-out test). We tested three
different models: 2 score-based generative diffusion models (SGM-Karras
Diffusion [SGM-KD] and SGM-variance preserving [SGM-VP]) and a
Transformer-Unet. We report results on standard image processing metrics as
well as clinically relevant metrics, including congruency measures (Congruence
Index and Congruency Mean Absolute Error) that assess hemispheric metabolic
asymmetry, which is a key part of the clinical analysis of these images. The
SGM-KD produced the best qualitative and quantitative results when synthesizing
PET purely from T1w and T2 FLAIR images with the least mean absolute error in
whole-brain specific uptake value ratio (SUVR) and highest intraclass
correlation coefficient. When 1% low-dose PET images are included in the
inputs, all models improve significantly and are interchangeable for
quantitative performance and visual quality. In summary, SGMs hold great
potential for pure MRI-to-PET translation, while all 3 model types can
synthesize full-dose FDG-PET accurately using MRI and ultralow-dose PET.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [206] [HEIST: A Graph Foundation Model for Spatial Transcriptomics and Proteomics Data](https://arxiv.org/abs/2506.11152)
*Hiren Madhu,João Felipe Rocha,Tinglin Huang,Siddharth Viswanath,Smita Krishnaswamy,Rex Ying*

Main category: q-bio.GN

TL;DR: HEIST是一种基于分层图变换器的空间转录组学基础模型，通过建模细胞邻域图和基因调控网络图，结合跨层次信息传递，显著提升了细胞微环境表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型忽略空间分辨率或基因调控信息，无法捕捉细胞微环境对基因调控的影响。

Method: HEIST使用分层图变换器建模组织为空间细胞邻域图，每个细胞为基因调控网络图，并通过对比学习和掩码自编码目标预训练。

Result: HEIST在22.3M细胞上预训练，显著提升了细胞嵌入的微环境表征能力，并在多个下游任务中达到最优性能。

Conclusion: HEIST通过分层建模和基因调控网络表征，为空间转录组学提供了更强大的分析工具。

Abstract: Single-cell transcriptomics has become a great source for data-driven
insights into biology, enabling the use of advanced deep learning methods to
understand cellular heterogeneity and transcriptional regulation at the
single-cell level. With the advent of spatial transcriptomics data we have the
promise of learning about cells within a tissue context as it provides both
spatial coordinates and transcriptomic readouts. However, existing models
either ignore spatial resolution or the gene regulatory information. Gene
regulation in cells can change depending on microenvironmental cues from
neighboring cells, but existing models neglect gene regulatory patterns with
hierarchical dependencies across levels of abstraction. In order to create
contextualized representations of cells and genes from spatial transcriptomics
data, we introduce HEIST, a hierarchical graph transformer-based foundation
model for spatial transcriptomics and proteomics data. HEIST models tissue as
spatial cellular neighborhood graphs, and each cell is, in turn, modeled as a
gene regulatory network graph. The framework includes a hierarchical graph
transformer that performs cross-level message passing and message passing
within levels. HEIST is pre-trained on 22.3M cells from 124 tissues across 15
organs using spatially-aware contrastive learning and masked auto-encoding
objectives. Unsupervised analysis of HEIST representations of cells, shows that
it effectively encodes the microenvironmental influences in cell embeddings,
enabling the discovery of spatially-informed subpopulations that prior models
fail to differentiate. Further, HEIST achieves state-of-the-art results on four
downstream task such as clinical outcome prediction, cell type annotation, gene
imputation, and spatially-informed cell clustering across multiple
technologies, highlighting the importance of hierarchical modeling and
GRN-based representations.

</details>


### [207] [Brain-wide interpolation and conditioning of gene expression in the human brain using Implicit Neural Representations](https://arxiv.org/abs/2506.11158)
*Xizheng Yu,Justin Torok,Sneha Pandya,Sourav Pal,Vikas Singh,Ashish Raj*

Main category: q-bio.GN

TL;DR: 研究利用隐式神经表示（INR）技术，通过稀疏采样的微阵列基因表达数据生成高分辨率全脑基因空间图谱。


<details>
  <summary>Details</summary>
Motivation: 利用非局部、非线性图像插值和外推算法（如INR）分析空间转录组数据，填补稀疏采样数据的空白。

Method: 获取100个阿尔茨海默病风险基因的基线空间转录谱，通过INR模型生成全脑体素级基因表达图谱。

Result: 实验以Abagen插值结果为基准，验证了INR模型生成高分辨率基因图谱的可行性。

Conclusion: INR技术为稀疏采样数据的空间转录组分析提供了有效工具。

Abstract: In this paper, we study the efficacy and utility of recent advances in
non-local, non-linear image interpolation and extrapolation algorithms,
specifically, ideas based on Implicit Neural Representations (INR), as a tool
for analysis of spatial transcriptomics data. We seek to utilize the microarray
gene expression data sparsely sampled in the healthy human brain, and produce
fully resolved spatial maps of any given gene across the whole brain at a
voxel-level resolution. To do so, we first obtained the 100 top AD risk genes,
whose baseline spatial transcriptional profiles were obtained from the Allen
Human Brain Atlas (AHBA). We adapted Implicit Neural Representation models so
that the pipeline can produce robust voxel-resolution quantitative maps of all
genes. We present a variety of experiments using interpolations obtained from
Abagen as a baseline/reference.

</details>


### [208] [SemanticST: Spatially Informed Semantic Graph Learning for1 Clustering, Integration, and Scalable Analysis of Spatial2 Transcriptomics](https://arxiv.org/abs/2506.11491)
*Roxana Zahedi,Ahmadreza Argha,Nona Farbehi,Ivan Bakhshayeshi,Youqiong Ye,Nigel H. Lovell,Hamid Alinejad-Rokny*

Main category: q-bio.GN

TL;DR: SemanticST是一个基于图深度学习的框架，通过多语义图构建和注意力融合策略，解决了空间转录组学数据分析中的噪声、可扩展性和复杂细胞关系建模问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学分析方法存在噪声数据、可扩展性差和复杂细胞关系建模不足的问题，需要一种更高效、可解释的解决方案。

Method: SemanticST通过构建多语义图（空间邻近性、基因表达相似性和组织域结构），学习解耦嵌入，并使用注意力融合策略生成统一表示。社区感知的最小割损失增强了稀疏数据的鲁棒性。

Result: 在多个平台和组织中，SemanticST在ARI、NMI和轨迹保真度上比现有方法提升了20%。在乳腺癌数据分析中，发现了罕见的临床相关细胞群和非经典EMT程序。

Conclusion: SemanticST为空间转录组学提供了一个可扩展、可解释且生物学基础的分析框架，支持跨组织和疾病的稳健发现，推动了空间分辨组织图谱和精准医学的发展。

Abstract: Spatial transcriptomics (ST) technologies enable gene expression profiling
with spatial resolution, offering unprecedented insights into tissue
organization and disease heterogeneity. However, current analysis methods often
struggle with noisy data, limited scalability, and inadequate modelling of
complex cellular relationships. We present SemanticST, a biologically informed,
graph-based deep learning framework that models diverse cellular contexts
through multi-semantic graph construction. SemanticST builds multiple
context-specific graphs capturing spatial proximity, gene expression
similarity, and tissue domain structure, and learns disentangled embeddings for
each. These are fused using an attention-inspired strategy to yield a unified,
biologically meaningful representation. A community-aware min-cut loss improves
robustness over contrastive learning, particularly in sparse ST data.
SemanticST supports mini-batch training, making it the first graph neural
network scalable to large-scale datasets such as Xenium (500,000 cells).
Benchmarking across four platforms (Visium, Slide-seq, Stereo-seq, Xenium) and
multiple human and mouse tissues shows consistent 20 percentage gains in ARI,
NMI, and trajectory fidelity over DeepST, GraphST, and IRIS. In re-analysis of
breast cancer Xenium data, SemanticST revealed rare and clinically significant
niches, including triple receptor-positive clusters, spatially distinct
DCIS-to-IDC transition zones, and FOXC2 tumour-associated myoepithelial cells,
suggesting non-canonical EMT programs with stem-like features. SemanticST thus
provides a scalable, interpretable, and biologically grounded framework for
spatial transcriptomics analysis, enabling robust discovery across tissue types
and diseases, and paving the way for spatially resolved tissue atlases and
next-generation precision medicine.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [209] [Learning Before Filtering: Real-Time Hardware Learning at the Detector Level](https://arxiv.org/abs/2506.11981)
*Boštjan Maček*

Main category: hep-ex

TL;DR: 本文提出了一种用于实时神经网络训练的数字硬件架构，优化了高吞吐量数据处理，并通过FPGA验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 传统滤波方法难以适应动态或意外的数据特征，机器学习提供了一种替代方案，尤其是在检测器附近进行训练时。

Method: 设计了一种实现无关的数字硬件架构，详细分析了各组件及其性能影响，并通过系统参数化探索了速度、模型复杂度和硬件资源之间的权衡。

Result: FPGA概念验证实现了原位训练，计算精度与传统软件方法相当，当前FPGA可训练约3,500个神经元。

Conclusion: 该架构具有可扩展性和适应性，推动了将学习直接集成到检测系统中的进展，支持极端边缘实时信息处理。

Abstract: Advances in sensor technology and automation have ushered in an era of data
abundance, where the ability to identify and extract relevant information in
real time has become increasingly critical. Traditional filtering approaches,
which depend on a priori knowledge, often struggle to adapt to dynamic or
unanticipated data features. Machine learning offers a compelling
alternative-particularly when training can occur directly at or near the
detector. This paper presents a digital hardware architecture designed for
real-time neural network training, specifically optimized for high-throughput
data ingestion. The design is described in an implementation-independent
manner, with detailed analysis of each architectural component and their
performance implications. Through system parameterization, the study explores
trade-offs between processing speed, model complexity, and hardware resource
utilization. Practical examples illustrate how these parameters affect
applicability across various use cases. A proof-of-concept implementation on an
FPGA demonstrates in-situ training, confirming that computational accuracy is
preserved relative to conventional software-based approaches. Moreover,
resource estimates indicate that current-generation FPGAs can train networks of
approximately 3,500 neurons per chip. The architecture is both scalable and
adaptable, representing a significant advancement toward integrating learning
directly within detector systems and enabling a new class of extreme-edge,
real-time information processing.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [210] [Topology-Aware Virtualization over Inter-Core Connected Neural Processing Units](https://arxiv.org/abs/2506.11446)
*Dahu Feng,Erhu Feng,Dong Du,Pinjie Xu,Yubin Xia,Haibo Chen,Rong Zhao*

Main category: cs.AR

TL;DR: 本文提出vNPU，首个针对互连核心NPU的全面虚拟化设计，通过路由虚拟化、内存虚拟化和最佳拓扑映射，显著提升性能并降低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 互连核心NPU在硬件资源利用上存在不平衡问题，现有虚拟化技术未考虑其硬件拓扑结构，vNPU旨在解决这一问题。

Method: vNPU采用三种新技术：NPU路由虚拟化、NPU内存虚拟化和最佳拓扑映射，优化资源利用和性能。

Result: 实验表明，vNPU在多种ML模型上性能提升2倍，硬件成本仅增加2%。

Conclusion: vNPU为互连核心NPU提供高效虚拟化方案，显著提升资源利用率和性能。

Abstract: With the rapid development of artificial intelligence (AI) applications, an
emerging class of AI accelerators, termed Inter-core Connected Neural
Processing Units (NPU), has been adopted in both cloud and edge computing
environments, like Graphcore IPU, Tenstorrent, etc. Despite their innovative
design, these NPUs often demand substantial hardware resources, leading to
suboptimal resource utilization due to the imbalance of hardware requirements
across various tasks. To address this issue, prior research has explored
virtualization techniques for monolithic NPUs, but has neglected inter-core
connected NPUs with the hardware topology.
  This paper introduces vNPU, the first comprehensive virtualization design for
inter-core connected NPUs, integrating three novel techniques: (1) NPU route
virtualization, which redirects instruction and data flow from virtual NPU
cores to physical ones, creating a virtual topology; (2) NPU memory
virtualization, designed to minimize translation stalls for SRAM-centric and
NoC-equipped NPU cores, thereby maximizing the memory bandwidth; and (3)
Best-effort topology mapping, which determines the optimal mapping from all
candidate virtual topologies, balancing resource utilization with end-to-end
performance. We have developed a prototype of vNPU on both an FPGA platform
(Chipyard+FireSim) and a simulator (DCRA). Evaluation results indicate that,
compared to other virtualization approaches such as unified virtual memory and
MIG, vNPU achieves up to a 2x performance improvement across various ML models,
with only 2% hardware cost.

</details>


### [211] [Real-World Deployment of a Lane Change Prediction Architecture Based on Knowledge Graph Embeddings and Bayesian Inference](https://arxiv.org/abs/2506.11925)
*M. Manzour,Catherine M. Elias,Omar M. Shehata,R. Izquierdo,M. A. Sotelo*

Main category: cs.AR

TL;DR: 论文提出了一种基于知识图谱嵌入和贝叶斯推理的车道变换预测系统，并在实际硬件上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前车道变换预测研究多局限于仿真或数据集，缺乏实际道路部署的验证，本文旨在填补这一空白。

Method: 系统分为感知模块和预测模块，前者提取环境特征并转换为语言类别，后者通过知识图谱嵌入和贝叶斯推理预测目标车辆行为并触发制动动作。

Result: 实验表明，系统能提前3-4秒预测车道变换，为自车提供充足反应时间，确保安全。

Conclusion: 该研究成功将算法应用于实际硬件，验证了车道变换预测系统的实用性和安全性。

Abstract: Research on lane change prediction has gained a lot of momentum in the last
couple of years. However, most research is confined to simulation or results
obtained from datasets, leaving a gap between algorithmic advances and on-road
deployment. This work closes that gap by demonstrating, on real hardware, a
lane-change prediction system based on Knowledge Graph Embeddings (KGEs) and
Bayesian inference. Moreover, the ego-vehicle employs a longitudinal braking
action to ensure the safety of both itself and the surrounding vehicles. Our
architecture consists of two modules: (i) a perception module that senses the
environment, derives input numerical features, and converts them into
linguistic categories; and communicates them to the prediction module; (ii) a
pretrained prediction module that executes a KGE and Bayesian inference model
to anticipate the target vehicle's maneuver and transforms the prediction into
longitudinal braking action. Real-world hardware experimental validation
demonstrates that our prediction system anticipates the target vehicle's lane
change three to four seconds in advance, providing the ego vehicle sufficient
time to react and allowing the target vehicle to make the lane change safely.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [212] [Measuring multi-calibration](https://arxiv.org/abs/2506.11251)
*Ido Guy,Daniel Haimovich,Fridolin Linder,Nastaran Okati,Lorenzo Perini,Niek Tax,Mark Tygert*

Main category: stat.ME

TL;DR: 提出了一种基于Kuiper统计量的新标量度量，用于衡量多校准性，并通过信号噪声比加权子群体贡献。


<details>
  <summary>Details</summary>
Motivation: 实践中预测概率很少完全多校准，需要一种度量来量化与完美多校准的距离。

Method: 基于Kuiper统计量设计新度量，避免分箱或核密度估计的问题，并通过信号噪声比加权子群体贡献。

Result: 新度量在基准数据集上表现良好，忽略信号噪声比会导致度量噪声增加。

Conclusion: 新度量有效衡量多校准性，信号噪声比加权是关键。

Abstract: A suitable scalar metric can help measure multi-calibration, defined as
follows. When the expected values of observed responses are equal to
corresponding predicted probabilities, the probabilistic predictions are known
as "perfectly calibrated." When the predicted probabilities are perfectly
calibrated simultaneously across several subpopulations, the probabilistic
predictions are known as "perfectly multi-calibrated." In practice, predicted
probabilities are seldom perfectly multi-calibrated, so a statistic measuring
the distance from perfect multi-calibration is informative. A recently proposed
metric for calibration, based on the classical Kuiper statistic, is a natural
basis for a new metric of multi-calibration and avoids well-known problems of
metrics based on binning or kernel density estimation. The newly proposed
metric weights the contributions of different subpopulations in proportion to
their signal-to-noise ratios; data analyses' ablations demonstrate that the
metric becomes noisy when omitting the signal-to-noise ratios from the metric.
Numerical examples on benchmark data sets illustrate the new metric.

</details>


### [213] [Bias and Identifiability in the Bounded Confidence Model](https://arxiv.org/abs/2506.11751)
*Claudio Borile,Jacopo Lenti,Valentina Ghidini,Corrado Monti,Gianmarco De Francisci Morales*

Main category: stat.ME

TL;DR: 该论文研究了有界置信模型（BCMs）中两个关键参数的统计估计特性，发现最大似然估计在不同参数上表现不同，并揭示了参数估计中的可识别性问题。


<details>
  <summary>Details</summary>
Motivation: 将有界置信模型与真实数据连接，以理解共识、分裂或极化现象，并测试模型假设。

Method: 使用最大似然估计方法分析有界置信模型的两个关键参数（置信界限和收敛速率）的统计特性。

Result: 置信界限的估计在小样本中存在偏差但一致，而收敛速率的估计则存在持久偏差；参数联合估计在特定区域存在可识别性问题。

Conclusion: 分析似然函数有助于理解意见动力学模型参数估计的局限与可能性，并为模型校准提供形式化保证。

Abstract: Opinion dynamics models such as the bounded confidence models (BCMs) describe
how a population can reach consensus, fragmentation, or polarization, depending
on a few parameters. Connecting such models to real-world data could help
understanding such phenomena, testing model assumptions. To this end,
estimation of model parameters is a key aspect, and maximum likelihood
estimation provides a principled way to tackle it. Here, our goal is to outline
the properties of statistical estimators of the two key BCM parameters: the
confidence bound and the convergence rate. We find that their maximum
likelihood estimators present different characteristics: the one for the
confidence bound presents a small-sample bias but is consistent, while the
estimator of the convergence rate shows a persistent bias. Moreover, the joint
parameter estimation is affected by identifiability issues for specific regions
of the parameter space, as several local maxima are present in the likelihood
function. Our results show how the analysis of the likelihood function is a
fruitful approach for better understanding the pitfalls and possibilities of
estimating the parameters of opinion dynamics models, and more in general,
agent-based models, and for offering formal guarantees for their calibration.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [214] [Teleoperated Driving: a New Challenge for 3D Object Detection in Compressed Point Clouds](https://arxiv.org/abs/2506.11804)
*Filippo Bragato,Michael Neri,Paolo Testolina,Marco Giordani,Federica Battisti*

Main category: cs.CV

TL;DR: 论文研究了如何通过点云数据检测车辆和行人以支持远程驾驶（TD），并评估了压缩算法和目标检测器的性能及其对V2X网络的影响。


<details>
  <summary>Details</summary>
Motivation: 随着互联设备的普及和硬件软件的进步，远程驾驶（TD）成为受益领域之一。本研究旨在通过检测车辆和行人确保TD操作的安全性。

Method: 利用扩展的SELMA数据集（包含3D物体的真实边界框），评估了压缩算法和目标检测器的性能，包括压缩效率、处理时间、检测精度及对V2X网络的影响。

Result: 分析了压缩算法和目标检测器在多种指标下的表现，并评估了其对V2X网络数据速率和延迟的影响。

Conclusion: 研究为远程驾驶中的目标检测和数据处理提供了重要参考，支持了安全TD操作的实现。

Abstract: In recent years, the development of interconnected devices has expanded in
many fields, from infotainment to education and industrial applications. This
trend has been accelerated by the increased number of sensors and accessibility
to powerful hardware and software. One area that significantly benefits from
these advancements is Teleoperated Driving (TD). In this scenario, a controller
drives safely a vehicle from remote leveraging sensors data generated onboard
the vehicle, and exchanged via Vehicle-to-Everything (V2X) communications. In
this work, we tackle the problem of detecting the presence of cars and
pedestrians from point cloud data to enable safe TD operations. More
specifically, we exploit the SELMA dataset, a multimodal, open-source,
synthetic dataset for autonomous driving, that we expanded by including the
ground-truth bounding boxes of 3D objects to support object detection. We
analyze the performance of state-of-the-art compression algorithms and object
detectors under several metrics, including compression efficiency,
(de)compression and inference time, and detection accuracy. Moreover, we
measure the impact of compression and detection on the V2X network in terms of
data rate and latency with respect to 3GPP requirements for TD applications.

</details>


### [215] [Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting](https://arxiv.org/abs/2506.11124)
*Yifei Chen,Ross Greer*

Main category: cs.CV

TL;DR: RefAV框架通过LLM将自然语言查询转换为可执行代码以挖掘自动驾驶场景，但存在运行时错误和参数解释不准确的问题。本文提出两种改进：迭代代码生成机制和专门提示工程，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成代码的运行时错误和复杂空间关系函数参数解释不准确的问题，提升场景挖掘的可靠性和精度。

Method: 引入故障容忍的迭代代码生成机制和专门提示工程，优化LLM的代码生成和空间关系函数应用。

Result: 在Argoverse 2验证集上实验显示性能提升，HOTA-Temporal得分达52.37（Gemini 2.5 Pro）。

Conclusion: 提出的技术显著提升了场景挖掘的可靠性和精度，验证了方法的有效性。

Abstract: Scenario mining from extensive autonomous driving datasets, such as Argoverse
2, is crucial for the development and validation of self-driving systems. The
RefAV framework represents a promising approach by employing Large Language
Models (LLMs) to translate natural-language queries into executable code for
identifying relevant scenarios. However, this method faces challenges,
including runtime errors stemming from LLM-generated code and inaccuracies in
interpreting parameters for functions that describe complex multi-object
spatial relationships. This technical report introduces two key enhancements to
address these limitations: (1) a fault-tolerant iterative code-generation
mechanism that refines code by re-prompting the LLM with error feedback, and
(2) specialized prompt engineering that improves the LLM's comprehension and
correct application of spatial-relationship functions. Experiments on the
Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash,
and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably,
the proposed system achieves a HOTA-Temporal score of 52.37 on the official
test set using Gemini 2.5 Pro. These results underline the efficacy of the
proposed techniques for reliable, high-precision scenario mining.

</details>


### [216] [Gender Fairness of Machine Learning Algorithms for Pain Detection](https://arxiv.org/abs/2506.11132)
*Dylan Green,Yuting Shang,Jiaee Cheong,Yang Liu,Hatice Gunes*

Main category: cs.CV

TL;DR: 论文研究了基于机器学习和深度学习的疼痛检测模型在性别公平性上的表现，发现所有模型均存在性别偏见，尽管Vision Transformer（ViT）在准确性和部分公平性指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 自动化疼痛检测在医疗领域潜力巨大，但对不同人口群体（如性别）的公平性研究不足。

Method: 使用UNBC-McMaster Shoulder Pain Expression Archive Database，比较了传统ML（L SVM、RBF SVM）和DL（CNN、ViT）模型在视觉模态下的表现。

Result: ViT在准确性和部分公平性指标上表现最佳，但所有模型均存在性别偏见。

Conclusion: 研究强调了在自动化医疗系统中平衡准确性与公平性的重要性，需采用公平性感知技术减少偏见。

Abstract: Automated pain detection through machine learning (ML) and deep learning (DL)
algorithms holds significant potential in healthcare, particularly for patients
unable to self-report pain levels. However, the accuracy and fairness of these
algorithms across different demographic groups (e.g., gender) remain
under-researched. This paper investigates the gender fairness of ML and DL
models trained on the UNBC-McMaster Shoulder Pain Expression Archive Database,
evaluating the performance of various models in detecting pain based solely on
the visual modality of participants' facial expressions. We compare traditional
ML algorithms, Linear Support Vector Machine (L SVM) and Radial Basis Function
SVM (RBF SVM), with DL methods, Convolutional Neural Network (CNN) and Vision
Transformer (ViT), using a range of performance and fairness metrics. While ViT
achieved the highest accuracy and a selection of fairness metrics, all models
exhibited gender-based biases. These findings highlight the persistent
trade-off between accuracy and fairness, emphasising the need for
fairness-aware techniques to mitigate biases in automated healthcare systems.

</details>


### [217] [Monocular 3D Hand Pose Estimation with Implicit Camera Alignment](https://arxiv.org/abs/2506.11133)
*Christos Pantazopoulos,Spyridon Thermos,Gerasimos Potamianos*

Main category: cs.CV

TL;DR: 提出了一种从2D关键点估计3D手部关节的优化流程，无需相机参数，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决单目彩色图像中3D手部关节估计的挑战，如深度信息缺失、遮挡和复杂关节结构。

Method: 采用关键点对齐和指尖损失优化流程，避免对相机参数的依赖。

Result: 在EgoDexter和Dexter+Object基准测试中表现优异，对野外图像具有鲁棒性。

Conclusion: 该方法在无需相机参数的情况下实现了与SotA竞争的性能，但2D关键点估计精度对结果敏感。

Abstract: Estimating the 3D hand articulation from a single color image is a
continuously investigated problem with applications in Augmented Reality (AR),
Virtual Reality (VR), Human-Computer Interaction (HCI), and robotics. Apart
from the absence of depth information, occlusions, articulation complexity, and
the need for camera parameters knowledge pose additional challenges. In this
work, we propose an optimization pipeline for estimating the 3D hand
articulation from 2D keypoint input, which includes a keypoint alignment step
and a fingertip loss to overcome the need to know or estimate the camera
parameters. We evaluate our approach on the EgoDexter and Dexter+Object
benchmarks to showcase that our approach performs competitively with the SotA,
while also demonstrating its robustness when processing "in-the-wild" images
without any prior camera knowledge. Our quantitative analysis highlights the
sensitivity of the 2D keypoint estimation accuracy, despite the use of hand
priors. Code is available at https://github.com/cpantazop/HandRepo

</details>


### [218] [FARCLUSS: Fuzzy Adaptive Rebalancing and Contrastive Uncertainty Learning for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.11142)
*Ebenezer Tarubinga,Jenifer Kalafatovich*

Main category: cs.CV

TL;DR: 提出了一种解决半监督语义分割中未标记数据利用不足问题的框架，通过模糊伪标签、不确定性动态加权、自适应类平衡和轻量对比正则化提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在利用未标记数据时存在伪标签利用不足、类不平衡偏差加剧和预测不确定性忽视的问题，需改进。

Method: 框架包含四个组件：模糊伪标签、不确定性动态加权、自适应类平衡和轻量对比正则化。

Result: 在基准测试中表现优于现有方法，显著提升了少数类和模糊区域的分割效果。

Conclusion: 该框架通过将不确定性转化为学习资源，有效解决了半监督语义分割中的关键问题。

Abstract: Semi-supervised semantic segmentation (SSSS) faces persistent challenges in
effectively leveraging unlabeled data, such as ineffective utilization of
pseudo-labels, exacerbation of class imbalance biases, and neglect of
prediction uncertainty. Current approaches often discard uncertain regions
through strict thresholding favouring dominant classes. To address these
limitations, we introduce a holistic framework that transforms uncertainty into
a learning asset through four principal components: (1) fuzzy pseudo-labeling,
which preserves soft class distributions from top-K predictions to enrich
supervision; (2) uncertainty-aware dynamic weighting, that modulate pixel-wise
contributions via entropy-based reliability scores; (3) adaptive class
rebalancing, which dynamically adjust losses to counteract long-tailed class
distributions; and (4) lightweight contrastive regularization, that encourage
compact and discriminative feature embeddings. Extensive experiments on
benchmarks demonstrate that our method outperforms current state-of-the-art
approaches, achieving significant improvements in the segmentation of
under-represented classes and ambiguous regions.

</details>


### [219] [LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs](https://arxiv.org/abs/2506.11148)
*Melvin Wong,Yueming Lyu,Thiago Rios,Stefan Menzel,Yew-Soon Ong*

Main category: cs.CV

TL;DR: LLM-to-Phy3D是一种新型在线3D对象生成方法，通过物理约束优化现有LLM-to-3D模型的输出，提升物理性能和几何新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-to-3D模型缺乏物理知识，导致生成的3D对象脱离现实物理约束，限制了其在工程设计中的应用。

Method: 引入在线黑盒优化循环，结合视觉和物理评估，迭代优化提示词以生成物理合规的3D对象。

Result: 在车辆设计优化中，LLM-to-Phy3D比传统模型提升4.5%至106.7%的物理合规性。

Conclusion: LLM-to-Phy3D在物理AI领域具有广泛潜力，可推动科学和工程应用的生成设计。

Abstract: The emergence of generative artificial intelligence (GenAI) and large
language models (LLMs) has revolutionized the landscape of digital content
creation in different modalities. However, its potential use in Physical AI for
engineering design, where the production of physically viable artifacts is
paramount, remains vastly underexplored. The absence of physical knowledge in
existing LLM-to-3D models often results in outputs detached from real-world
physical constraints. To address this gap, we introduce LLM-to-Phy3D, a
physically conform online 3D object generation that enables existing LLM-to-3D
models to produce physically conforming 3D objects on the fly. LLM-to-Phy3D
introduces a novel online black-box refinement loop that empowers large
language models (LLMs) through synergistic visual and physics-based
evaluations. By delivering directional feedback in an iterative refinement
process, LLM-to-Phy3D actively drives the discovery of prompts that yield 3D
artifacts with enhanced physical performance and greater geometric novelty
relative to reference objects, marking a substantial contribution to AI-driven
generative design. Systematic evaluations of LLM-to-Phy3D, supported by
ablation studies in vehicle design optimization, reveal various LLM
improvements gained by 4.5% to 106.7% in producing physically conform target
domain 3D designs over conventional LLM-to-3D models. The encouraging results
suggest the potential general use of LLM-to-Phy3D in Physical AI for scientific
and engineering applications.

</details>


### [220] [VIBE: Can a VLM Read the Room?](https://arxiv.org/abs/2506.11162)
*Tania Chakraborty,Eylon Caplan,Dan Goldwasser*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLMs）在社交推理中的能力，发现其存在视觉社交-语用推理差距，并提出新任务和数据集进行测试。


<details>
  <summary>Details</summary>
Motivation: 理解人类社交行为（如情绪识别和社交动态）是一个重要但具有挑战性的问题，现有LLMs局限于文本领域，无法捕捉非语言线索的作用。

Method: 提出新任务“视觉社交-语用推理”，构建高质量数据集，并测试多种VLMs的性能。

Result: 发现VLMs在视觉社交-语用推理方面存在局限性。

Conclusion: VLMs在社交推理中仍有改进空间，新任务和数据集为未来研究提供了方向。

Abstract: Understanding human social behavior such as recognizing emotions and the
social dynamics causing them is an important and challenging problem. While
LLMs have made remarkable advances, they are limited to the textual domain and
cannot account for the major role that non-verbal cues play in understanding
social situations. Vision Language Models (VLMs) can potentially account for
this gap, however their ability to make correct inferences over such social
cues has received little attention. In this paper, we explore the capabilities
of VLMs at social reasoning. We identify a previously overlooked limitation in
VLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose
a new task for VLMs: Visual Social-Pragmatic Inference. We construct a high
quality dataset to test the abilities of a VLM for this task and benchmark the
performance of several VLMs on it.

</details>


### [221] [Synthetic Geology -- Structural Geology Meets Deep Learning](https://arxiv.org/abs/2506.11164)
*Simon Ghyselincks,Valeriia Okhmak,Stefano Zampini,George Turkiyyah,David Keyes,Eldad Haber*

Main category: cs.CV

TL;DR: 利用生成式AI和合成数据训练神经网络，从地表地质数据生成高保真三维地下图像，填补地下数据空白，应用于资源勘探和灾害评估。


<details>
  <summary>Details</summary>
Motivation: 解决地下数据稀缺问题，扩展地表地质数据至三维地下区域，支持资源勘探、灾害评估等应用。

Method: 设计合成数据生成器模拟地质活动，训练神经网络生成三维地下图像，结合地表数据和钻孔数据提高保真度。

Result: 生成的三维图像能准确描绘地层、断层等结构，保真度随钻孔数据增加而提高。

Conclusion: 该方法为地下深度学习提供数据支持，未来可通过区域数据微调模型，进一步优化资源勘探等应用。

Abstract: Visualizing the first few kilometers of the Earth's subsurface, a
long-standing challenge gating a virtually inexhaustible list of important
applications, is coming within reach through deep learning. Building on
techniques of generative artificial intelligence applied to voxelated images,
we demonstrate a method that extends surface geological data supplemented by
boreholes to a three-dimensional subsurface region by training a neural
network. The Earth's land area having been extensively mapped for geological
features, the bottleneck of this or any related technique is the availability
of data below the surface. We close this data gap in the development of
subsurface deep learning by designing a synthetic data-generator process that
mimics eons of geological activity such as sediment compaction, volcanic
intrusion, and tectonic dynamics to produce a virtually limitless number of
samples of the near lithosphere. A foundation model trained on such synthetic
data is able to generate a 3D image of the subsurface from a previously unseen
map of surface topography and geology, showing increasing fidelity with
increasing access to borehole data, depicting such structures as layers,
faults, folds, dikes, and sills. We illustrate the early promise of the
combination of a synthetic lithospheric generator with a trained neural network
model using generative flow matching. Ultimately, such models will be
fine-tuned on data from applicable campaigns, such as mineral prospecting in a
given region. Though useful in itself, a regionally fine-tuned models may be
employed not as an end but as a means: as an AI-based regularizer in a more
traditional inverse problem application, in which the objective function
represents the mismatch of additional data with physical models with
applications in resource exploration, hazard assessment, and geotechnical
engineering.

</details>


### [222] [Evaluating BiLSTM and CNN+GRU Approaches for Human Activity Recognition Using WiFi CSI Data](https://arxiv.org/abs/2506.11165)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.CV

TL;DR: 比较BiLSTM和CNN+GRU在WiFi CSI数据集上的性能，发现CNN+GRU在UT-HAR表现更好，BiLSTM在NTU-Fi HAR更优，强调数据集特性和预处理的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索不同深度学习模型在WiFi CSI数据上的性能差异，以优化人类活动识别（HAR）的准确性。

Method: 使用BiLSTM和CNN+GRU模型在UT-HAR和NTU-Fi HAR数据集上进行实验比较。

Result: CNN+GRU在UT-HAR上准确率95.20%，BiLSTM在NTU-Fi HAR上准确率92.05%。

Conclusion: 数据集特性和预处理对模型性能至关重要，模型在医疗和智能家居中有实际应用潜力。

Abstract: This paper compares the performance of BiLSTM and CNN+GRU deep learning
models for Human Activity Recognition (HAR) on two WiFi-based Channel State
Information (CSI) datasets: UT-HAR and NTU-Fi HAR. The findings indicate that
the CNN+GRU model has a higher accuracy on the UT-HAR dataset (95.20%) thanks
to its ability to extract spatial features. In contrast, the BiLSTM model
performs better on the high-resolution NTU-Fi HAR dataset (92.05%) by
extracting long-term temporal dependencies more effectively. The findings
strongly emphasize the critical role of dataset characteristics and
preprocessing techniques in model performance improvement. We also show the
real-world applicability of such models in applications like healthcare and
intelligent home systems, highlighting their potential for unobtrusive activity
recognition.

</details>


### [223] [Towards a general-purpose foundation model for fMRI analysis](https://arxiv.org/abs/2506.11167)
*Cheng Wang,Yu Jiang,Zhihao Peng,Chenxin Li,Changbae Bang,Lin Zhao,Jinglei Lv,Jorge Sepulcre,Carl Yang,Lifang He,Tianming Liu,Daniel Barron,Quanzheng Li,Randy Hirschtick,Byung-Hoon Kim,Xiang Li,Yixuan Yuan*

Main category: cs.CV

TL;DR: NeuroSTORM是一个通用的fMRI分析框架，通过预训练和任务特定提示调优，解决了现有方法的可重复性和迁移性问题。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI分析方法因复杂的预处理和任务特定模型而面临可重复性和迁移性问题，需要一种通用且高效的解决方案。

Method: NeuroSTORM采用Mamba骨干网络和移位扫描策略处理4D fMRI数据，提出空间-时间优化的预训练方法和任务特定提示调优。

Result: 在五个任务中表现优于现有方法，并在多国医院数据中展示了强大的临床实用性。

Conclusion: NeuroSTORM为fMRI临床研究提供了一个标准化、开源的基础模型，提升了可重复性和迁移性。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is essential for studying brain
function and diagnosing neurological disorders, but current analysis methods
face reproducibility and transferability issues due to complex pre-processing
and task-specific models. We introduce NeuroSTORM (Neuroimaging Foundation
Model with Spatial-Temporal Optimized Representation Modeling), a generalizable
framework that directly learns from 4D fMRI volumes and enables efficient
knowledge transfer across diverse applications. NeuroSTORM is pre-trained on
28.65 million fMRI frames (>9,000 hours) from over 50,000 subjects across
multiple centers and ages 5 to 100. Using a Mamba backbone and a shifted
scanning strategy, it efficiently processes full 4D volumes. We also propose a
spatial-temporal optimized pre-training approach and task-specific prompt
tuning to improve transferability. NeuroSTORM outperforms existing methods
across five tasks: age/gender prediction, phenotype prediction, disease
diagnosis, fMRI-to-image retrieval, and task-based fMRI classification. It
demonstrates strong clinical utility on datasets from hospitals in the U.S.,
South Korea, and Australia, achieving top performance in disease diagnosis and
cognitive phenotype prediction. NeuroSTORM provides a standardized, open-source
foundation model to improve reproducibility and transferability in fMRI-based
clinical research.

</details>


### [224] [BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization](https://arxiv.org/abs/2506.11178)
*Nguyen Linh Dan Le,Jing Ren,Ciyuan Peng,Chengyao Xie,Bowen Li,Feng Xia*

Main category: cs.CV

TL;DR: BrainMAP是一种新型多模态图学习框架，用于高效识别神经退行性疾病相关脑区，显著降低计算开销并保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法无法精确定位神经退行性病变的脑区，且多模态脑图模型计算复杂度高，限制了实际应用。

Method: BrainMAP采用图谱驱动过滤提取关键脑子图，并通过跨节点注意力和自适应门控机制融合fMRI和DTI数据。

Result: 实验显示，BrainMAP计算效率提升50%以上，且预测准确性优于现有方法。

Conclusion: BrainMAP为神经退行性疾病的精准识别提供了高效、实用的解决方案。

Abstract: Recent years have seen a surge in research focused on leveraging graph
learning techniques to detect neurodegenerative diseases. However, existing
graph-based approaches typically lack the ability to localize and extract the
specific brain regions driving neurodegenerative pathology within the full
connectome. Additionally, recent works on multimodal brain graph models often
suffer from high computational complexity, limiting their practical use in
resource-constrained devices. In this study, we present BrainMAP, a novel
multimodal graph learning framework designed for precise and computationally
efficient identification of brain regions affected by neurodegenerative
diseases. First, BrainMAP utilizes an atlas-driven filtering approach guided by
the AAL atlas to pinpoint and extract critical brain subgraphs. Unlike recent
state-of-the-art methods, which model the entire brain network, BrainMAP
achieves more than 50% reduction in computational overhead by concentrating on
disease-relevant subgraphs. Second, we employ an advanced multimodal fusion
process comprising cross-node attention to align functional magnetic resonance
imaging (fMRI) and diffusion tensor imaging (DTI) data, coupled with an
adaptive gating mechanism to blend and integrate these modalities dynamically.
Experimental results demonstrate that BrainMAP outperforms state-of-the-art
methods in computational efficiency, without compromising predictive accuracy.

</details>


### [225] [Enhanced Vehicle Speed Detection Considering Lane Recognition Using Drone Videos in California](https://arxiv.org/abs/2506.11239)
*Amirali Ataee Naeini,Ashkan Teymouri,Ghazaleh Jafarsalehi,Michael Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于改进YOLOv11模型的车速检测系统，显著提高了车速检测的准确性，并增加了车道识别和车辆分类功能。


<details>
  <summary>Details</summary>
Motivation: 加州车辆数量增加，交通系统不足和测速摄像头稀疏，需要更有效的车速检测方法，尤其是区分车道和车辆类型以监控HOV车道和执行限速规定。

Method: 研究使用改进的YOLOv11模型，训练了约800张鸟瞰图像，能够识别车辆所在车道并将车辆分为轿车和重型车辆两类。

Result: 改进的YOLOv11模型在车速检测中表现优异，平均绝对误差（MAE）为0.97 mph，均方误差（MSE）为0.94 mph²。

Conclusion: 该模型有效解决了车速检测和分类中的挑战，适用于交通监控和法规执行。

Abstract: The increase in vehicle numbers in California, driven by inadequate
transportation systems and sparse speed cameras, necessitates effective vehicle
speed detection. Detecting vehicle speeds per lane is critical for monitoring
High-Occupancy Vehicle (HOV) lane speeds, distinguishing between cars and heavy
vehicles with differing speed limits, and enforcing lane restrictions for heavy
vehicles. While prior works utilized YOLO (You Only Look Once) for vehicle
speed detection, they often lacked accuracy, failed to identify vehicle lanes,
and offered limited or less practical classification categories. This study
introduces a fine-tuned YOLOv11 model, trained on almost 800 bird's-eye view
images, to enhance vehicle speed detection accuracy which is much higher
compare to the previous works. The proposed system identifies the lane for each
vehicle and classifies vehicles into two categories: cars and heavy vehicles.
Designed to meet the specific requirements of traffic monitoring and
regulation, the model also evaluates the effects of factors such as drone
height, distance of Region of Interest (ROI), and vehicle speed on detection
accuracy and speed measurement. Drone footage collected from Northern
California was used to assess the proposed system. The fine-tuned YOLOv11
achieved its best performance with a mean absolute error (MAE) of 0.97 mph and
mean squared error (MSE) of 0.94 $\text{mph}^2$, demonstrating its efficacy in
addressing challenges in vehicle speed detection and classification.

</details>


### [226] [Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models](https://arxiv.org/abs/2506.11253)
*Yuwen Tan,Boqing Gong*

Main category: cs.CV

TL;DR: 论文提出将数据追踪的机器遗忘提升为针对基础模型的知识追踪遗忘，以满足实际需求并借鉴认知研究。


<details>
  <summary>Details</summary>
Motivation: 基础模型的多样化遗忘需求（如监管、企业用户等）无法通过数据追踪满足，而知识追踪更符合人类遗忘机制。

Method: 提出知识追踪机器遗忘范式，并通过视觉语言基础模型案例说明其实现方式。

Result: 知识追踪遗忘更贴近实际需求，且与人类认知遗忘机制更一致。

Conclusion: 知识追踪机器遗忘是基础模型遗忘的更优解决方案。

Abstract: Machine unlearning removes certain training data points and their influence
on AI models (e.g., when a data owner revokes their decision to allow models to
learn from the data). In this position paper, we propose to lift data-tracing
machine unlearning to knowledge-tracing for foundation models (FMs). We support
this position based on practical needs and insights from cognitive studies.
Practically, tracing data cannot meet the diverse unlearning requests for FMs,
which may be from regulators, enterprise users, product teams, etc., having no
access to FMs' massive training data. Instead, it is convenient for these
parties to issue an unlearning request about the knowledge or capability FMs
(should not) possess. Cognitively, knowledge-tracing unlearning aligns with how
the human brain forgets more closely than tracing individual training data
points. Finally, we provide a concrete case study about a vision-language FM to
illustrate how an unlearner might instantiate the knowledge-tracing machine
unlearning paradigm.

</details>


### [227] [On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving](https://arxiv.org/abs/2506.11472)
*Pedram MohajerAnsari,Amir Salarpour,Michael Kühr,Siyu Huang,Mohammad Hamad,Sebastian Steinhorst,Habeeb Olufowobi,Mert D. Pesé*

Main category: cs.CV

TL;DR: 论文提出了一种名为V2LMs的视觉语言模型，用于提升自动驾驶车辆（AVs）的感知安全性，相比传统深度神经网络（DNNs）具有更强的对抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统DNNs在对抗攻击下表现脆弱，且现有防御机制（如对抗训练）会降低正常精度且无法泛化到未见攻击。

Method: 通过微调视觉语言模型（V2LMs）专门用于AV感知，并评估了两种部署策略：Solo Mode（单任务）和Tandem Mode（多任务）。

Result: 实验显示，DNNs在攻击下性能下降33%-46%，而V2LMs平均仅下降不到8%，Tandem Mode在内存效率上表现更优。

Conclusion: V2LMs为AV感知系统提供了一种更安全、更具鲁棒性的解决方案。

Abstract: Autonomous vehicles (AVs) rely on deep neural networks (DNNs) for critical
tasks such as traffic sign recognition (TSR), automated lane centering (ALC),
and vehicle detection (VD). However, these models are vulnerable to attacks
that can cause misclassifications and compromise safety. Traditional defense
mechanisms, including adversarial training, often degrade benign accuracy and
fail to generalize against unseen attacks. In this work, we introduce Vehicle
Vision Language Models (V2LMs), fine-tuned vision-language models specialized
for AV perception. Our findings demonstrate that V2LMs inherently exhibit
superior robustness against unseen attacks without requiring adversarial
training, maintaining significantly higher accuracy than conventional DNNs
under adversarial conditions. We evaluate two deployment strategies: Solo Mode,
where individual V2LMs handle specific perception tasks, and Tandem Mode, where
a single unified V2LM is fine-tuned for multiple tasks simultaneously.
Experimental results reveal that DNNs suffer performance drops of 33% to 46%
under attacks, whereas V2LMs maintain adversarial accuracy with reductions of
less than 8% on average. The Tandem Mode further offers a memory-efficient
alternative while achieving comparable robustness to Solo Mode. We also explore
integrating V2LMs as parallel components to AV perception to enhance resilience
against adversarial threats. Our results suggest that V2LMs offer a promising
path toward more secure and resilient AV perception systems.

</details>


### [228] [Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs](https://arxiv.org/abs/2506.11515)
*Xiao Xu,Libo Qin,Wanxiang Che,Min-Yen Kan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Manager的轻量级插件，用于增强视觉-语言模型（VLM）的性能，通过自适应聚合不同层次单模态专家的知识，提升跨模态对齐和融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有的BridgeTower模型在利用单模态表示时效率低下，且仅适用于低分辨率数据集。论文旨在解决这些问题，提升模型性能。

Method: 提出ManagerTower，在跨模态层中引入Manager插件，自适应聚合单模态专家知识。同时探索了多模态大语言模型（MLLM）架构的应用。

Result: ManagerTower在4个下游任务中表现优异，LLaVA-OV-Manager在20个数据集上显著提升了零样本性能。

Conclusion: Manager插件和多网格算法协同作用，能捕获更多视觉细节并提升性能，代码和模型已开源。

Abstract: Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance
across various downstream VL tasks. While BridgeTower further enhances
performance by building bridges between encoders, it \textit{(i)} suffers from
ineffective layer-by-layer utilization of unimodal representations,
\textit{(ii)} restricts the flexible exploitation of different levels of
unimodal semantic knowledge, and \textit{(iii)} is limited to the evaluation on
traditional low-resolution datasets only with the Two-Tower VLM architecture.
In this work, we propose Manager, a lightweight, efficient and effective plugin
that adaptively aggregates insights from different levels of pre-trained
unimodal experts to facilitate more comprehensive VL alignment and fusion.
First, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel
VLM that introduces the manager in each cross-modal layer. Whether with or
without VL pre-training, ManagerTower outperforms previous strong baselines and
achieves superior performance on 4 downstream VL tasks. Moreover, we extend our
exploration to the latest Multimodal Large Language Model (MLLM) architecture.
We demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot
performance of LLaVA-OV across different categories of capabilities, images,
and resolutions on 20 downstream datasets, whether the multi-grid algorithm is
enabled or not. In-depth analysis reveals that both our manager and the
multi-grid algorithm can be viewed as a plugin that improves the visual
representation by capturing more diverse visual details from two orthogonal
perspectives (depth and width). Their synergy can mitigate the semantic
ambiguity caused by the multi-grid algorithm and further improve performance.
Code and models are available at https://github.com/LooperXX/ManagerTower.

</details>


### [229] [FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation](https://arxiv.org/abs/2506.11543)
*Zhuguanyu Wu,Shihe Wang,Jiayi Zhang,Jiaxin Chen,Yunhong Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为FIMA-Q的新型PTQ方法，用于解决ViTs在低比特量化中的精度下降问题，通过KL散度与FIM的联系及DPLR-FIM近似方法显著提升了量化精度。


<details>
  <summary>Details</summary>
Motivation: 当前ViTs的后训练量化方法在低比特量化时精度下降显著，传统Hessian近似方法存在局限性。

Method: 提出FIMA-Q方法，基于块重建框架，利用KL散度与FIM的联系快速计算量化损失，并采用DPLR-FIM近似方法优化量化损失。

Result: 在多种ViT架构和公开数据集上的实验表明，FIMA-Q显著提升了量化精度，尤其在低比特量化场景下优于现有方法。

Conclusion: FIMA-Q为ViTs的后训练量化提供了一种高效且高精度的解决方案，尤其在低比特量化中表现突出。

Abstract: Post-training quantization (PTQ) has stood out as a cost-effective and
promising model compression paradigm in recent years, as it avoids
computationally intensive model retraining. Nevertheless, current PTQ methods
for Vision Transformers (ViTs) still suffer from significant accuracy
degradation, especially under low-bit quantization. To address these
shortcomings, we analyze the prevailing Hessian-guided quantization loss, and
uncover certain limitations of conventional Hessian approximations. By
following the block-wise reconstruction framework, we propose a novel PTQ
method for ViTs, dubbed FIMA-Q. Specifically, we firstly establish the
connection between KL divergence and FIM, which enables fast computation of the
quantization loss during reconstruction. We further propose an efficient FIM
approximation method, namely DPLR-FIM, by employing the diagonal plus low-rank
principle, and formulate the ultimate quantization loss. Our extensive
experiments, conducted across various vision tasks with representative
ViT-based architectures on public datasets, demonstrate that our method
substantially promotes the accuracy compared to the state-of-the-art
approaches, especially in the case of low-bit quantization. The source code is
available at https://github.com/ShiheWang/FIMA-Q.

</details>


### [230] [EasyARC: Evaluating Vision Language Models on True Visual Reasoning](https://arxiv.org/abs/2506.11595)
*Mert Unsal,Aylin Akkus*

Main category: cs.CV

TL;DR: 论文提出EasyARC，一个多模态推理基准，结合视觉和文本，用于评估视觉语言模型的真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏视觉与语言复杂交互的真实推理测试，EasyARC填补了这一空白。

Method: 通过程序生成多图像、多步骤推理任务，支持自校正，并设计渐进难度级别。

Result: 评估了当前最优视觉语言模型，并分析了其失败模式。

Conclusion: EasyARC为视觉语言模型的真实推理和测试扩展能力设定了新标准，并开源了数据集和代码。

Abstract: Building on recent advances in language-based reasoning models, we explore
multimodal reasoning that integrates vision and text. Existing multimodal
benchmarks primarily test visual extraction combined with text-based reasoning,
lacking true visual reasoning with more complex interactions between vision and
language. Inspired by the ARC challenge, we introduce EasyARC, a
vision-language benchmark requiring multi-image, multi-step reasoning, and
self-correction. EasyARC is procedurally generated, fully verifiable, and
scalable, making it ideal for reinforcement learning (RL) pipelines. The
generators incorporate progressive difficulty levels, enabling structured
evaluation across task types and complexities. We benchmark state-of-the-art
vision-language models and analyze their failure modes. We argue that EasyARC
sets a new standard for evaluating true reasoning and test-time scaling
capabilities in vision-language models. We open-source our benchmark dataset
and evaluation code.

</details>


### [231] [Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique using Tensor Data and Bayesian Regression](https://arxiv.org/abs/2506.11627)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种新的评估机器学习公平性的方法，专注于肤色数据，避免传统分类，改用概率分布和统计距离度量。


<details>
  <summary>Details</summary>
Motivation: 肤色在计算机视觉中以张量数据表示，不同于传统的分类特征（如性别、种族），现有公平性研究未充分涵盖此类数据。

Method: 将肤色数据转换为概率分布，应用统计距离度量，并通过贝叶斯回归与多项式函数计算颜色距离估计，以减少潜在偏见。

Result: 新方法能够捕捉传统分类无法识别的细粒度公平性差异，并在图像分类任务中实现更公平的肤色处理。

Conclusion: 该方法为肤色公平性评估提供了更灵活和精确的框架，有助于提升机器学习模型的公平性。

Abstract: Fairness is a critical component of Trustworthy AI. In this paper, we focus
on Machine Learning (ML) and the performance of model predictions when dealing
with skin color. Unlike other sensitive attributes, the nature of skin color
differs significantly. In computer vision, skin color is represented as tensor
data rather than categorical values or single numerical points. However, much
of the research on fairness across sensitive groups has focused on categorical
features such as gender and race. This paper introduces a new technique for
evaluating fairness in ML for image classification tasks, specifically without
the use of annotation. To address the limitations of prior work, we handle
tensor data, like skin color, without classifying it rigidly. Instead, we
convert it into probability distributions and apply statistical distance
measures. This novel approach allows us to capture fine-grained nuances in
fairness both within and across what would traditionally be considered distinct
groups. Additionally, we propose an innovative training method to mitigate the
latent biases present in conventional skin tone categorization. This method
leverages color distance estimates calculated through Bayesian regression with
polynomial functions, ensuring a more nuanced and equitable treatment of skin
color in ML models.

</details>


### [232] [DISCO: Mitigating Bias in Deep Learning with Conditional Distance Correlation](https://arxiv.org/abs/2506.11653)
*Emre Kavak,Tom Nuno Wolf,Christian Wachinger*

Main category: cs.CV

TL;DR: 论文提出了一种标准反因果预测模型（SAM）和一种新的正则化策略DISCO，用于解决预测任务中模型依赖无关信号的问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在预测任务中，模型可能依赖无关信号（如光照条件）而非真实因果关系，导致不理想的结果。

Method: 引入SAM模型分析反因果设置中的信息路径，并提出DISCO正则化策略，利用条件距离相关性优化条件独立性。

Result: DISCO在不同偏置缓解实验中表现优异，是传统核方法的有效替代。

Conclusion: SAM和DISCO为解决预测任务中的无关信号依赖问题提供了有效工具。

Abstract: During prediction tasks, models can use any signal they receive to come up
with the final answer - including signals that are causally irrelevant. When
predicting objects from images, for example, the lighting conditions could be
correlated to different targets through selection bias, and an oblivious model
might use these signals as shortcuts to discern between various objects. A
predictor that uses lighting conditions instead of real object-specific details
is obviously undesirable. To address this challenge, we introduce a standard
anti-causal prediction model (SAM) that creates a causal framework for
analyzing the information pathways influencing our predictor in anti-causal
settings. We demonstrate that a classifier satisfying a specific conditional
independence criterion will focus solely on the direct causal path from label
to image, being counterfactually invariant to the remaining variables. Finally,
we propose DISCO, a novel regularization strategy that uses conditional
distance correlation to optimize for conditional independence in regression
tasks. We can show that DISCO achieves competitive results in different bias
mitigation experiments, deeming it a valid alternative to classical
kernel-based methods.

</details>


### [233] [Predicting Patient Survival with Airway Biomarkers using nn-Unet/Radiomics](https://arxiv.org/abs/2506.11677)
*Zacharia Mesbah,Dhruv Jain,Tsiry Mayet,Romain Modzelewski,Romain Herault,Simon Bernard,Sebastien Thureau,Clement Chatelain*

Main category: cs.CV

TL;DR: 该研究通过三阶段方法评估气道影像生物标志物对肺纤维化患者生存的预测意义，包括气道分割、特征提取和分类，取得了较高的评分。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索气道相关影像生物标志物对肺纤维化患者生存结果的预测价值。

Method: 采用三阶段方法：1) 使用nn-Unet分割气道结构；2) 从气管和气道周围提取关键特征；3) 用SVM分类器整合特征。

Result: Task 1分割得分为0.8601，Task 2分类得分为0.7346。

Conclusion: 该方法在气道影像生物标志物分析中表现出较高的预测能力。

Abstract: The primary objective of the AIIB 2023 competition is to evaluate the
predictive significance of airway-related imaging biomarkers in determining the
survival outcomes of patients with lung fibrosis.This study introduces a
comprehensive three-stage approach. Initially, a segmentation network, namely
nn-Unet, is employed to delineate the airway's structural boundaries.
Subsequently, key features are extracted from the radiomic images centered
around the trachea and an enclosing bounding box around the airway. This step
is motivated by the potential presence of critical survival-related insights
within the tracheal region as well as pertinent information encoded in the
structure and dimensions of the airway. Lastly, radiomic features obtained from
the segmented areas are integrated into an SVM classifier. We could obtain an
overall-score of 0.8601 for the segmentation in Task 1 while 0.7346 for the
classification in Task 2.

</details>


### [234] [CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection](https://arxiv.org/abs/2506.11772)
*Byeongchan Lee,John Won,Seunghyun Lee,Jinwoo Shin*

Main category: cs.CV

TL;DR: CLIPFUSION结合判别式和生成式基础模型，通过多模态融合解决异常检测问题，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 异常检测因定义模糊、类型多样及数据稀缺而复杂，需能捕捉多层次特征的模型。

Method: 利用CLIP判别模型捕捉全局特征，扩散生成模型捕捉局部细节，并引入跨注意力图和特征图方法。

Result: 在MVTec-AD和VisA数据集上表现优异，异常分割和分类性能突出。

Conclusion: 多模态多模型融合有效应对异常检测挑战，为实际应用提供可扩展方案。

Abstract: Anomaly detection is a complex problem due to the ambiguity in defining
anomalies, the diversity of anomaly types (e.g., local and global defect), and
the scarcity of training data. As such, it necessitates a comprehensive model
capable of capturing both low-level and high-level features, even with limited
data. To address this, we propose CLIPFUSION, a method that leverages both
discriminative and generative foundation models. Specifically, the CLIP-based
discriminative model excels at capturing global features, while the
diffusion-based generative model effectively captures local details, creating a
synergistic and complementary approach. Notably, we introduce a methodology for
utilizing cross-attention maps and feature maps extracted from diffusion models
specifically for anomaly detection. Experimental results on benchmark datasets
(MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline
methods, achieving outstanding performance in both anomaly segmentation and
classification. We believe that our method underscores the effectiveness of
multi-modal and multi-model fusion in tackling the multifaceted challenges of
anomaly detection, providing a scalable solution for real-world applications.

</details>


### [235] [Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation](https://arxiv.org/abs/2506.11777)
*Divyanshu Mishra,Mohammadreza Salehi,Pramit Saha,Olga Patey,Aris T. Papageorghiou,Yuki M. Asano,J. Alison Noble*

Main category: cs.CV

TL;DR: DISCOVR是一种自监督双分支框架，用于心脏超声视频表示学习，结合聚类和图像编码器，通过语义聚类蒸馏损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决超声心动图领域因细微解剖结构、复杂时间动态和缺乏领域预训练模型而导致的SSL挑战。

Method: 采用双分支框架，结合聚类视频编码器和在线图像编码器，通过语义聚类蒸馏损失连接。

Result: 在六个超声心动图数据集上表现优异，超越现有视频异常检测和视频SSL基线。

Conclusion: DISCOVR通过结合时间动态和细粒度语义理解，显著提升了超声视频表示学习性能。

Abstract: Self-supervised learning (SSL) has achieved major advances in natural images
and video understanding, but challenges remain in domains like echocardiography
(heart ultrasound) due to subtle anatomical structures, complex temporal
dynamics, and the current lack of domain-specific pre-trained models. Existing
SSL approaches such as contrastive, masked modeling, and clustering-based
methods struggle with high intersample similarity, sensitivity to low PSNR
inputs common in ultrasound, or aggressive augmentations that distort
clinically relevant features. We present DISCOVR (Distilled Image Supervision
for Cross Modal Video Representation), a self-supervised dual branch framework
for cardiac ultrasound video representation learning. DISCOVR combines a
clustering-based video encoder that models temporal dynamics with an online
image encoder that extracts fine-grained spatial semantics. These branches are
connected through a semantic cluster distillation loss that transfers
anatomical knowledge from the evolving image encoder to the video encoder,
enabling temporally coherent representations enriched with fine-grained
semantic understanding. Evaluated on six echocardiography datasets spanning
fetal, pediatric, and adult populations, DISCOVR outperforms both specialized
video anomaly detection methods and state-of-the-art video-SSL baselines in
zero-shot and linear probing setups, and achieves superior segmentation
transfer.

</details>


### [236] [Vision-based Lifting of 2D Object Detections for Automated Driving](https://arxiv.org/abs/2506.11839)
*Hendrik Königshof,Kun Li,Christoph Stiller*

Main category: cs.CV

TL;DR: 提出了一种仅使用摄像头将2D检测结果提升为3D检测的管道，作为LiDAR的经济替代方案。


<details>
  <summary>Details</summary>
Motivation: 由于LiDAR成本高，而摄像头价格低廉且广泛可用，研究低成本3D目标检测方法对自动驾驶至关重要。

Method: 利用2D CNN处理点云数据，将现有2D算法结果提升为3D检测，专注于所有道路使用者。

Result: 在KITTI 3D目标检测基准上表现接近现有图像方法，且运行时间仅为三分之一。

Conclusion: 该方法为低成本3D检测提供了可行方案，适用于自动驾驶中的多类目标检测。

Abstract: Image-based 3D object detection is an inevitable part of autonomous driving
because cheap onboard cameras are already available in most modern cars.
Because of the accurate depth information, currently, most state-of-the-art 3D
object detectors heavily rely on LiDAR data. In this paper, we propose a
pipeline which lifts the results of existing vision-based 2D algorithms to 3D
detections using only cameras as a cost-effective alternative to LiDAR. In
contrast to existing approaches, we focus not only on cars but on all types of
road users. To the best of our knowledge, we are the first using a 2D CNN to
process the point cloud for each 2D detection to keep the computational effort
as low as possible. Our evaluation on the challenging KITTI 3D object detection
benchmark shows results comparable to state-of-the-art image-based approaches
while having a runtime of only a third.

</details>


### [237] [How Visual Representations Map to Language Feature Space in Multimodal LLMs](https://arxiv.org/abs/2506.11976)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.CV

TL;DR: 论文提出了一种通过线性适配器连接冻结的视觉和语言模型的方法，研究了视觉与语言表征的对齐机制。


<details>
  <summary>Details</summary>
Motivation: 理解视觉语言模型（VLMs）如何实现视觉与语言表征的对齐，目前机制尚不明确。

Method: 使用冻结的大型语言模型（LLM）和视觉变换器（ViT），仅通过训练线性适配器进行视觉指令调整。

Result: 实验表明，视觉表征逐渐与语言表征对齐，但在早期层存在不匹配。

Conclusion: 当前基于适配器的架构可能未最优支持跨模态表征学习。

Abstract: Effective multimodal reasoning depends on the alignment of visual and
linguistic representations, yet the mechanisms by which vision-language models
(VLMs) achieve this alignment remain poorly understood. We introduce a
methodological framework that deliberately maintains a frozen large language
model (LLM) and a frozen vision transformer (ViT), connected solely by training
a linear adapter during visual instruction tuning. This design is fundamental
to our approach: by keeping the language model frozen, we ensure it maintains
its original language representations without adaptation to visual data.
Consequently, the linear adapter must map visual features directly into the
LLM's existing representational space rather than allowing the language model
to develop specialized visual understanding through fine-tuning. Our
experimental design uniquely enables the use of pre-trained sparse autoencoders
(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned
with the unchanged language model and serve as a snapshot of the learned
language feature-representations. Through systematic analysis of SAE
reconstruction error, sparsity patterns, and feature SAE descriptions, we
reveal the layer-wise progression through which visual representations
gradually align with language feature representations, converging in
middle-to-later layers. This suggests a fundamental misalignment between ViT
outputs and early LLM layers, raising important questions about whether current
adapter-based architectures optimally facilitate cross-modal representation
learning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [238] [Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork](https://arxiv.org/abs/2506.11285)
*Jianhong Wang,Yang Li,Samuel Kaski,Jonathan Lawry*

Main category: cs.MA

TL;DR: 论文提出了一种基于合作博弈论的方法来解决开放多智能体系统中的n智能体临时团队（NAHT）问题，通过Shapley值分配贡献，并提出了Shapley Machine算法。


<details>
  <summary>Details</summary>
Motivation: 开放多智能体系统（如智能电网、群体机器人）中，现有方法缺乏理论严谨性和明确的贡献分配机制，因此需要一种更严谨的解决方案。

Method: 将NAHT建模为合作博弈问题，利用基游戏扩展状态空间，并通过Shapley值分配贡献。提出了一种类似TD(λ)的Shapley Machine算法。

Result: 实验验证了Shapley Machine的有效性，并证明了理论的合理性。

Conclusion: 论文首次将合作博弈论与强化学习概念直接结合，为解决NAHT问题提供了理论支持和实用算法。

Abstract: Open multi-agent systems are increasingly important in modeling real-world
applications, such as smart grids, swarm robotics, etc. In this paper, we aim
to investigate a recently proposed problem for open multi-agent systems,
referred to as n-agent ad hoc teamwork (NAHT), where only a number of agents
are controlled. Existing methods tend to be based on heuristic design and
consequently lack theoretical rigor and ambiguous credit assignment among
agents. To address these limitations, we model and solve NAHT through the lens
of cooperative game theory. More specifically, we first model an open
multi-agent system, characterized by its value, as an instance situated in a
space of cooperative games, generated by a set of basis games. We then extend
this space, along with the state space, to accommodate dynamic scenarios,
thereby characterizing NAHT. Exploiting the justifiable assumption that basis
game values correspond to a sequence of n-step returns with different horizons,
we represent the state values for NAHT in a form similar to $\lambda$-returns.
Furthermore, we derive Shapley values to allocate state values to the
controlled agents, as credits for their contributions to the ad hoc team.
Different from the conventional approach to shaping Shapley values in an
explicit form, we shape Shapley values by fulfilling the three axioms uniquely
describing them, well defined on the extended game space describing NAHT. To
estimate Shapley values in dynamic scenarios, we propose a TD($\lambda$)-like
algorithm. The resulting reinforcement learning (RL) algorithm is referred to
as Shapley Machine. To our best knowledge, this is the first time that the
concepts from cooperative game theory are directly related to RL concepts. In
experiments, we demonstrate the effectiveness of Shapley Machine and verify
reasonableness of our theory.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [239] [5G-Enabled Smart Prosthetic Hand: Connectivity Analysis and Assessment](https://arxiv.org/abs/2506.11729)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: eess.SY

TL;DR: 本文提出了一种边缘连接假肢系统的框架，通过5G或WiFi实现低延迟控制，延迟低于125ms。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够通过边缘服务器实时处理环境信息的假肢系统，探索5G技术在此类应用中的可行性。

Method: 使用配备摄像头的仿生手连接Jetson设备，通过5G或WiFi与边缘服务器通信，处理视频流并反馈环境信息。

Result: 系统控制回路的总延迟低于125ms，验证了5G技术在假肢系统中的可行性。

Conclusion: 该框架首次展示了5G支持的假肢系统的可行性，为未来相关应用提供了技术基础。

Abstract: In this paper, we demonstrate a proof-of-concept implementation of a
framework for the development of edge-connected prosthetic systems. The
framework is composed of a bionic hand equipped with a camera and connected to
a Jetson device that establishes a wireless connection to the edge server,
processing the received video stream and feeding back the inferred information
about the environment. The hand-edge server connection is obtained either
through a direct 5G link, where the edge server also functions as a 5G base
station, or through a WiFi link. We evaluate the latency of closing the control
loop in the system, showing that, in a realistic usage scenario, the
connectivity and computation delays combined are well below 125 ms, which falls
into the natural control range. To the best of our knowledge, this is the first
analysis showcasing the feasibility of a 5G-enabled prosthetic system.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [240] [Demonstration Sidetracks: Categorizing Systematic Non-Optimality in Human Demonstrations](https://arxiv.org/abs/2506.11262)
*Shijie Fang,Hang Yu,Qidi Fang,Reuben M. Aronson,Elaine S. Short*

Main category: cs.RO

TL;DR: 论文研究了非专家演示中的非最优行为，发现这些行为是系统性的，称为“演示偏离”。通过实验识别了四种偏离类型，并指出其对LfD算法改进的重要性。


<details>
  <summary>Details</summary>
Motivation: 人类演示中的不完美行为通常被视为随机噪声，但本文认为这些行为是系统性的，需要更深入的研究以改进LfD算法。

Method: 通过40名参与者的公共空间研究，模拟实验并标注演示数据，识别了四种偏离类型和一种控制模式。

Result: 发现偏离行为在参与者中普遍存在，且其时空分布与任务背景相关；控制模式依赖于控制接口。

Conclusion: 研究强调了改进非最优演示模型的必要性，以缩小实验室训练与实际部署的差距。数据与标注已开源。

Abstract: Learning from Demonstration (LfD) is a popular approach for robots to acquire
new skills, but most LfD methods suffer from imperfections in human
demonstrations. Prior work typically treats these suboptimalities as random
noise. In this paper we study non-optimal behaviors in non-expert
demonstrations and show that they are systematic, forming what we call
demonstration sidetracks. Using a public space study with 40 participants
performing a long-horizon robot task, we recreated the setup in simulation and
annotated all demonstrations. We identify four types of sidetracks
(Exploration, Mistake, Alignment, Pause) and one control pattern (one-dimension
control). Sidetracks appear frequently across participants, and their temporal
and spatial distribution is tied to task context. We also find that users'
control patterns depend on the control interface. These insights point to the
need for better models of suboptimal demonstrations to improve LfD algorithms
and bridge the gap between lab training and real-world deployment. All
demonstrations, infrastructure, and annotations are available at
https://github.com/AABL-Lab/Human-Demonstration-Sidetracks.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [241] [Gradients of unitary optical neural networks using parameter-shift rule](https://arxiv.org/abs/2506.11565)
*Jinzhe Jiang,Yaqian Zhao,Xin Zhang,Chen Li,Yunlong Yu,Hailing Liu*

Main category: cs.ET

TL;DR: 本文探讨了参数偏移规则（PSR）在单元光学神经网络（UONNs）中计算梯度的应用，提出了一种绕过传统反向传播限制的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播在光学神经网络中因物理限制难以实现，需要一种替代方法。

Method: 利用PSR通过参数偏移值计算梯度，结合Mach-Zehnder干涉仪网格的傅里叶级数特性，直接从硬件测量中获取精确梯度。

Result: PSR为光学神经网络提供了一种高效且硬件友好的梯度计算方法，避免了有限差分近似和全光学反向传播的局限性。

Conclusion: PSR为光学计算系统提供了一种潜在的硬件训练策略，推动了光学神经网络的发展。

Abstract: This paper explores the application of the parameter-shift rule (PSR) for
computing gradients in unitary optical neural networks (UONNs). While
backpropagation has been fundamental to training conventional neural networks,
its implementation in optical neural networks faces significant challenges due
to the physical constraints of optical systems. We demonstrate how PSR, which
calculates gradients by evaluating functions at shifted parameter values, can
be effectively adapted for training UONNs constructed from Mach-Zehnder
interferometer meshes. The method leverages the inherent Fourier series nature
of optical interference in these systems to compute exact analytical gradients
directly from hardware measurements. This approach offers a promising
alternative to traditional in silico training methods and circumvents the
limitations of both finite difference approximations and all-optical
backpropagation implementations. We present the theoretical framework and
practical methodology for applying PSR to optimize phase parameters in optical
neural networks, potentially advancing the development of efficient
hardware-based training strategies for optical computing systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [242] [code_transformed: The Influence of Large Language Models on Code](https://arxiv.org/abs/2506.12014)
*Yuliang Xu,Siming Huang,Mingmeng Geng,Yao Wan,Xuanhua Shi,Dongping Chen*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）对代码风格的影响，分析了命名规范、复杂性、可维护性和相似性，发现LLMs显著改变了实际编程风格。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在代码生成能力上的快速发展，研究其对代码风格的影响成为重要问题。

Method: 通过分析2020至2025年间与arXiv论文相关的19,000多个GitHub仓库中的代码，量化了代码风格的演变趋势。

Result: 研究发现LLMs显著影响了代码风格，例如Python中snake_case变量名的比例从2023年Q1的47%上升至2025年Q1的51%。

Conclusion: LLMs对实际编程风格产生了可测量的影响，但难以精确估计其生成或辅助代码的比例。

Abstract: Coding remains one of the most fundamental modes of interaction between
humans and machines. With the rapid advancement of Large Language Models
(LLMs), code generation capabilities have begun to significantly reshape
programming practices. This development prompts a central question: Have LLMs
transformed code style, and how can such transformation be characterized? In
this paper, we present a pioneering study that investigates the impact of LLMs
on code style, with a focus on naming conventions, complexity, maintainability,
and similarity. By analyzing code from over 19,000 GitHub repositories linked
to arXiv papers published between 2020 and 2025, we identify measurable trends
in the evolution of coding style that align with characteristics of
LLM-generated code. For instance, the proportion of snake\_case variable names
in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we
investigate how LLMs approach algorithmic problems by examining their reasoning
processes. Given the diversity of LLMs and usage scenarios, among other
factors, it is difficult or even impossible to precisely estimate the
proportion of code generated or assisted by LLMs. Our experimental results
provide the first large-scale empirical evidence that LLMs affect real-world
programming style.

</details>


### [243] [AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models](https://arxiv.org/abs/2506.11110)
*Jaeho Lee,Atharv Chowdhary*

Main category: cs.CL

TL;DR: AssertBench评估大型语言模型（LLM）在面对用户对同一事实的不同表述时是否保持一致，而非盲目同意用户观点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试关注LLM的事实一致性和修辞鲁棒性，但缺乏对用户表述方向性如何影响模型同意的研究。

Method: 从FEVEROUS数据集中选取证据支持的事实，构建两种表述（用户声称正确或错误），记录模型的同意和推理。

Result: 通过分层结果，隔离表述引起的变异性，衡量模型在用户矛盾表述下坚持事实的能力。

Conclusion: AssertBench旨在量化LLM在面对用户矛盾表述时坚持真相的能力，代码已开源。

Abstract: Recent benchmarks have probed factual consistency and rhetorical robustness
in Large Language Models (LLMs). However, a knowledge gap exists regarding how
directional framing of factually true statements influences model agreement, a
common scenario for LLM users. AssertBench addresses this by sampling
evidence-supported facts from FEVEROUS, a fact verification dataset. For each
(evidence-backed) fact, we construct two framing prompts: one where the user
claims the statement is factually correct, and another where the user claims it
is incorrect. We then record the model's agreement and reasoning. The desired
outcome is that the model asserts itself, maintaining consistent truth
evaluation across both framings, rather than switching its evaluation to agree
with the user. AssertBench isolates framing-induced variability from the
model's underlying factual knowledge by stratifying results based on the
model's accuracy on the same claims when presented neutrally. In doing so, this
benchmark aims to measure an LLM's ability to "stick to its guns" when
presented with contradictory user assertions about the same fact. The complete
source code is available at https://github.com/achowd32/assert-bench.

</details>


### [244] [SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models](https://arxiv.org/abs/2506.11120)
*Hourun Zhu,Chengchao Shen*

Main category: cs.CL

TL;DR: 论文提出了一种在剪枝阶段引入自蒸馏损失的方法，以更准确地获取梯度信息，并专注于多层感知机（MLP）模块的剪枝，显著压缩大型语言模型（LLM）而不明显降低性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）性能强大，但其部署成本高昂。现有基于梯度的剪枝方法因忽略生成能力的关键信息而效果有限。

Method: 在剪枝阶段引入自蒸馏损失，利用原始模型的预测信息，并专注于MLP模块的剪枝。

Result: 实验表明，该方法在零样本基准测试中显著优于现有剪枝方法，并在1B规模的开源LLM中表现优异。

Conclusion: 该方法有效压缩LLM，同时保持高性能，为低成本部署提供了可行方案。

Abstract: In spite of strong performance achieved by LLMs, the costs of their
deployment are unaffordable. For the compression of LLMs, gradient-based
pruning methods present promising effectiveness. However, in these methods, the
gradient computation with one-hot labels ignore the potential predictions on
other words, thus missing key information for generative capability of the
original model. To address this issue, we introduce a self-distillation loss
during the pruning phase (rather than post-training) to fully exploit the
predictions of the original model, thereby obtaining more accurate gradient
information for pruning. Moreover, we find that, compared to attention modules,
the predictions of LLM are less sensitive to multilayer perceptron (MLP)
modules, which take up more than $5 \times$ parameters (LLaMA3.2-1.2B). To this
end, we focus on the pruning of MLP modules, to significantly compress LLM
without obvious performance degradation. Experimental results on extensive
zero-shot benchmarks demonstrate that our method significantly outperforms
existing pruning methods. Furthermore, our method achieves very competitive
performance among 1B-scale open source LLMs. The source code and trained
weights are available at https://github.com/visresearch/SDMPrune.

</details>


### [245] [Large Language Models and Emergence: A Complex Systems Perspective](https://arxiv.org/abs/2506.11135)
*David C. Krakauer,John W. Krakauer,Melanie Mitchell*

Main category: cs.CL

TL;DR: 论文探讨了涌现现象与大型语言模型（LLM）的智能涌现，分析了量化涌现的方法，并讨论了LLM是否具备涌现智能。


<details>
  <summary>Details</summary>
Motivation: 研究涌现现象在复杂系统中的作用，并探讨LLM是否表现出涌现能力及其智能性质。

Method: 回顾了多种量化涌现的方法，并分析LLM的涌现能力。

Result: 讨论了LLM是否具备涌现智能，并总结了相关证据。

Conclusion: 论文提出了对LLM涌现能力的见解，并探讨了未来研究方向。

Abstract: Emergence is a concept in complexity science that describes how many-body
systems manifest novel higher-level properties, properties that can be
described by replacing high-dimensional mechanisms with lower-dimensional
effective variables and theories. This is captured by the idea "more is
different". Intelligence is a consummate emergent property manifesting
increasingly efficient -- cheaper and faster -- uses of emergent capabilities
to solve problems. This is captured by the idea "less is more". In this paper,
we first examine claims that Large Language Models exhibit emergent
capabilities, reviewing several approaches to quantifying emergence, and
secondly ask whether LLMs possess emergent intelligence.

</details>


### [246] [Learning a Continue-Thinking Token for Enhanced Test-Time Scaling](https://arxiv.org/abs/2506.11274)
*Liran Ringel,Elad Tolochinsky,Yaniv Romano*

Main category: cs.CL

TL;DR: 研究发现，通过学习专用的“继续思考”标记（<|continue-thinking|>），可以比固定标记（如“Wait”）更有效地提升语言模型的推理能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过学习专用的“继续思考”标记来触发更长的推理步骤，从而提升模型性能。

Method: 在蒸馏版DeepSeek-R1模型中添加一个学习的“继续思考”标记，仅通过强化学习训练其嵌入，同时冻结模型权重。

Result: 在标准数学基准测试中，学习标记方法比基线模型和固定标记方法表现更好，例如在GSM8K上提升了4.2%。

Conclusion: 学习专用的“继续思考”标记是一种有效的方法，能够显著提升语言模型的推理能力。

Abstract: Test-time scaling has emerged as an effective approach for improving language
model performance by utilizing additional compute at inference time. Recent
studies have shown that overriding end-of-thinking tokens (e.g., replacing
"</think>" with "Wait") can extend reasoning steps and improve accuracy. In
this work, we explore whether a dedicated continue-thinking token can be
learned to trigger extended reasoning. We augment a distilled version of
DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only
its embedding via reinforcement learning while keeping the model weights
frozen. Our experiments show that this learned token achieves improved accuracy
on standard math benchmarks compared to both the baseline model and a test-time
scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In
particular, we observe that in cases where the fixed-token approach enhances
the base model's accuracy, our method achieves a markedly greater improvement.
For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%
absolute improvement in accuracy, whereas our learned-token method achieves a
4.2% improvement over the base model that does not use budget forcing.

</details>


### [247] [Long-Short Alignment for Effective Long-Context Modeling in LLMs](https://arxiv.org/abs/2506.11769)
*Tianqi Du,Haotian Huang,Yifei Wang,Yisen Wang*

Main category: cs.CL

TL;DR: 论文提出了一种新视角，关注模型输出分布的长短对齐性，提出了一种度量指标和正则化方法，以提升大语言模型的长上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文建模中受限于固定上下文窗口，长度泛化能力不足。

Method: 通过合成任务案例研究，提出长短对齐概念，并设计Long-Short Misalignment指标和正则化方法。

Result: 实验验证了方法的有效性，揭示了长短对齐性与长度泛化性能的强相关性。

Conclusion: 研究为提升大语言模型的长上下文建模提供了新思路。

Abstract: Large language models (LLMs) have exhibited impressive performance and
surprising emergent properties. However, their effectiveness remains limited by
the fixed context window of the transformer architecture, posing challenges for
long-context modeling. Among these challenges, length generalization -- the
ability to generalize to sequences longer than those seen during training -- is
a classical and fundamental problem. In this work, we propose a fresh
perspective on length generalization, shifting the focus from the conventional
emphasis on input features such as positional encodings or data structures to
the output distribution of the model. Specifically, through case studies on
synthetic tasks, we highlight the critical role of \textbf{long-short
alignment} -- the consistency of output distributions across sequences of
varying lengths. Extending this insight to natural language tasks, we propose a
metric called Long-Short Misalignment to quantify this phenomenon, uncovering a
strong correlation between the metric and length generalization performance.
Building on these findings, we develop a regularization term that promotes
long-short alignment during training. Extensive experiments validate the
effectiveness of our approach, offering new insights for achieving more
effective long-context modeling in LLMs. Code is available at
https://github.com/PKU-ML/LongShortAlignment.

</details>


### [248] [Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models](https://arxiv.org/abs/2506.11798)
*Maximilian Kreutner,Marlene Lutz,Markus Strohmaier*

Main category: cs.CL

TL;DR: 研究探讨了零样本身份提示是否能准确预测个人投票决策及欧洲群体政策立场，发现其能较好模拟欧洲议会议员投票行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在政治话语中表现出进步左倾偏见，研究旨在验证身份提示是否能预测个体及群体的政治立场。

Method: 使用零样本身份提示，分析其对投票决策和政策立场的预测能力，并评估其对反事实论点、不同提示和生成方法的稳定性。

Result: 研究发现，加权F1分数约为0.793，能较准确地模拟欧洲议会议员的投票行为。

Conclusion: 零样本身份提示可有效预测政治立场，为LLMs在政治模拟中的应用提供了支持。

Abstract: Large Language Models (LLMs) display remarkable capabilities to understand or
even produce political discourse, but have been found to consistently display a
progressive left-leaning bias. At the same time, so-called persona or identity
prompts have been shown to produce LLM behavior that aligns with socioeconomic
groups that the base model is not aligned with. In this work, we analyze
whether zero-shot persona prompting with limited information can accurately
predict individual voting decisions and, by aggregation, accurately predict
positions of European groups on a diverse set of policies. We evaluate if
predictions are stable towards counterfactual arguments, different persona
prompts and generation methods. Finally, we find that we can simulate voting
behavior of Members of the European Parliament reasonably well with a weighted
F1 score of approximately 0.793. Our persona dataset of politicians in the 2024
European Parliament and our code are available at
https://github.com/dess-mannheim/european_parliament_simulation.

</details>


### [249] [Improving Large Language Model Safety with Contrastive Representation Learning](https://arxiv.org/abs/2506.11938)
*Samuel Simko,Mrinmaya Sachan,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出了一种基于对比表示学习（CRL）的防御框架，通过三元组损失和对抗性硬负样本挖掘，增强模型对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受对抗攻击，现有防御方法难以泛化，因此需要更有效的防御策略。

Method: 将模型防御问题转化为对比表示学习问题，使用三元组损失和对抗性硬负样本挖掘来区分良性表示和有害表示。

Result: 实验表明，该方法优于现有基于表示工程的防御方法，提升了对抗输入级和嵌入空间攻击的鲁棒性，且不影响标准性能。

Conclusion: 提出的CRL框架为LLMs提供了一种有效的防御策略，具有广泛的应用潜力。

Abstract: Large Language Models (LLMs) are powerful tools with profound societal
impacts, yet their ability to generate responses to diverse and uncontrolled
inputs leaves them vulnerable to adversarial attacks. While existing defenses
often struggle to generalize across varying attack types, recent advancements
in representation engineering offer promising alternatives. In this work, we
propose a defense framework that formulates model defense as a contrastive
representation learning (CRL) problem. Our method finetunes a model using a
triplet-based loss combined with adversarial hard negative mining to encourage
separation between benign and harmful representations. Our experimental results
across multiple models demonstrate that our approach outperforms prior
representation engineering-based defenses, improving robustness against both
input-level and embedding-space attacks without compromising standard
performance. Our code is available at
https://github.com/samuelsimko/crl-llm-defense

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [250] [Voxel-Level Brain States Prediction Using Swin Transformer](https://arxiv.org/abs/2506.11455)
*Yifei Sun,Daniel Chahine,Qinghao Wen,Tianming Liu,Xiang Li,Yixuan Yuan,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: 该研究提出了一种基于4D Swin Transformer的模型，用于预测人类静息状态下的未来脑活动，展示了高精度和潜在的应用价值。


<details>
  <summary>Details</summary>
Motivation: 理解大脑动态对神经科学和心理健康至关重要，而fMRI提供了测量神经活动的工具。本研究旨在利用fMRI预测未来的静息脑状态。

Method: 采用4D Swin Transformer作为编码器学习时空信息，卷积解码器用于预测与输入fMRI数据相同分辨率的脑状态。

Result: 模型在预测7.2秒静息脑活动时表现出高精度，预测结果与BOLD信号高度相似。

Conclusion: 研究表明Swin Transformer能高效学习人脑时空组织，为减少fMRI扫描时间和开发脑机接口提供了潜力。

Abstract: Understanding brain dynamics is important for neuroscience and mental health.
Functional magnetic resonance imaging (fMRI) enables the measurement of neural
activities through blood-oxygen-level-dependent (BOLD) signals, which represent
brain states. In this study, we aim to predict future human resting brain
states with fMRI. Due to the 3D voxel-wise spatial organization and temporal
dependencies of the fMRI data, we propose a novel architecture which employs a
4D Shifted Window (Swin) Transformer as encoder to efficiently learn
spatio-temporal information and a convolutional decoder to enable brain state
prediction at the same spatial and temporal resolution as the input fMRI data.
We used 100 unrelated subjects from the Human Connectome Project (HCP) for
model training and testing. Our novel model has shown high accuracy when
predicting 7.2s resting-state brain activities based on the prior 23.04s fMRI
time series. The predicted brain states highly resemble BOLD contrast and
dynamics. This work shows promising evidence that the spatiotemporal
organization of the human brain can be learned by a Swin Transformer model, at
high resolution, which provides a potential for reducing the fMRI scan time and
the development of brain-computer interfaces in the future.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [251] [GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant](https://arxiv.org/abs/2506.11781)
*Gaspard Merten,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.HC

TL;DR: GeoPandas-AI通过将LLMs集成到GeoPandas工作流中，简化了地理空间数据分析，降低了使用门槛。


<details>
  <summary>Details</summary>
Motivation: 解决GeoPandas等工具因复杂语法和工作流程对用户的高要求问题。

Method: 设计并实现了一个智能化的GeoDataFrame类，结合LLMs提供对话式界面和代码生成功能。

Result: 开发了开源GeoPandas-AI工具，通过PyPI发布，为地理空间开发提供了新范式。

Conclusion: GeoPandas-AI通过LLMs和对话式界面，为地理空间数据分析提供了更易用的解决方案。

Abstract: Geospatial data analysis plays a crucial role in tackling intricate societal
challenges such as urban planning and climate modeling. However, employing
tools like GeoPandas, a prominent Python library for geospatial data
manipulation, necessitates expertise in complex domain-specific syntax and
workflows. GeoPandas-AI addresses this gap by integrating LLMs directly into
the GeoPandas workflow, transforming the GeoDataFrame class into an
intelligent, stateful class for both data analysis and geospatial code
development. This paper formalizes the design of such a smart class and
provides an open-source implementation of GeoPandas-AI in PyPI package manager.
Through its innovative combination of conversational interfaces and stateful
exploitation of LLMs for code generation and data analysis, GeoPandas-AI
introduces a new paradigm for code-copilots and instantiates it for geospatial
development.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [252] [Automated Treatment Planning for Interstitial HDR Brachytherapy for Locally Advanced Cervical Cancer using Deep Reinforcement Learning](https://arxiv.org/abs/2506.11957)
*Mohammadamin Moradi,Runyu Jiang,Yingzi Liu,Malvern Madondo,Tianming Wu,James J. Sohn,Xiaofeng Yang,Yasmin Hasan,Zhen Tian*

Main category: physics.med-ph

TL;DR: 开发了一种基于强化学习的自动化HDR近距离放射治疗计划框架，显著提高了治疗计划的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 手动治疗计划依赖专家经验，效率低且一致性差，需要自动化解决方案。

Method: 采用两阶段框架：第一阶段使用DQN强化学习代理选择治疗参数，第二阶段用Adam优化器计算停留时间分布。

Result: 在测试患者中，自动化方法平均得分93.89%，优于临床计划的91.86%，同时保持目标覆盖并减少热点。

Conclusion: 该框架成功实现了自动化治疗计划，具有临床意义和实用价值。

Abstract: High-dose-rate (HDR) brachytherapy plays a critical role in the treatment of
locally advanced cervical cancer but remains highly dependent on manual
treatment planning expertise. The objective of this study is to develop a fully
automated HDR brachytherapy planning framework that integrates reinforcement
learning (RL) and dose-based optimization to generate clinically acceptable
treatment plans with improved consistency and efficiency. We propose a
hierarchical two-stage autoplanning framework. In the first stage, a deep
Q-network (DQN)-based RL agent iteratively selects treatment planning
parameters (TPPs), which control the trade-offs between target coverage and
organ-at-risk (OAR) sparing. The agent's state representation includes both
dose-volume histogram (DVH) metrics and current TPP values, while its reward
function incorporates clinical dose objectives and safety constraints,
including D90, V150, V200 for targets, and D2cc for all relevant OARs (bladder,
rectum, sigmoid, small bowel, and large bowel). In the second stage, a
customized Adam-based optimizer computes the corresponding dwell time
distribution for the selected TPPs using a clinically informed loss function.
The framework was evaluated on a cohort of patients with complex applicator
geometries. The proposed framework successfully learned clinically meaningful
TPP adjustments across diverse patient anatomies. For the unseen test patients,
the RL-based automated planning method achieved an average score of 93.89%,
outperforming the clinical plans which averaged 91.86%. These findings are
notable given that score improvements were achieved while maintaining full
target coverage and reducing CTV hot spots in most cases.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [253] [Deep Symmetric Autoencoders from the Eckart-Young-Schmidt Perspective](https://arxiv.org/abs/2506.11641)
*Simone Brivio,Nicola Rares Franco*

Main category: math.NA

TL;DR: 本文分析了对称自编码器的理论表达性，提出了基于EYS定理的初始化策略，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度自编码器在多种应用中表现优异，但其理论基础仍不明确，尤其是与经典投影技术相比。本文旨在填补这一空白，分析对称自编码器的数学特性。

Method: 引入对称自编码器的分类，分析其数学特性，提出基于EYS定理的初始化策略（EYS初始化），并通过数值实验验证。

Result: 研究表明，对称自编码器的重构误差可通过EYS定理理解，EYS初始化策略优于传统方法。

Conclusion: 对称自编码器的理论分析为模型设计和初始化提供了新视角，EYS初始化策略具有实际应用价值。

Abstract: Deep autoencoders have become a fundamental tool in various machine learning
applications, ranging from dimensionality reduction and reduced order modeling
of partial differential equations to anomaly detection and neural machine
translation. Despite their empirical success, a solid theoretical foundation
for their expressiveness remains elusive, particularly when compared to
classical projection-based techniques. In this work, we aim to take a step
forward in this direction by presenting a comprehensive analysis of what we
refer to as symmetric autoencoders, a broad class of deep learning
architectures ubiquitous in the literature. Specifically, we introduce a formal
distinction between different classes of symmetric architectures, analyzing
their strengths and limitations from a mathematical perspective. For instance,
we show that the reconstruction error of symmetric autoencoders with
orthonormality constraints can be understood by leveraging the well-renowned
Eckart-Young-Schmidt (EYS) theorem. As a byproduct of our analysis, we end up
developing the EYS initialization strategy for symmetric autoencoders, which is
based on an iterated application of the Singular Value Decomposition (SVD). To
validate our findings, we conduct a series of numerical experiments where we
benchmark our proposal against conventional deep autoencoders, discussing the
importance of model design and initialization.

</details>


### [254] [Data-driven approaches to inverse problems](https://arxiv.org/abs/2506.11732)
*Carola-Bibiane Schönlieb,Zakhar Shumaylov*

Main category: math.NA

TL;DR: 论文介绍了逆问题的数据驱动方法，包括传统数学建模与现代深度学习技术，并探讨了其理论性质与数值示例。


<details>
  <summary>Details</summary>
Motivation: 逆问题在医学成像、遥感等领域至关重要，但传统方法受限于建模能力与计算效率，数据驱动方法提供了更高效的解决方案。

Method: 论文分为两部分：第一部分介绍逆问题及传统解法；第二部分探讨数据驱动方法，如对抗正则化和可证明收敛的线性即插即用去噪器。

Result: 数据驱动方法在解决逆问题时表现出高精度与计算效率，超越了传统方法。

Conclusion: 论文总结了数据驱动方法的优势，并提出了未来研究方向与开放性问题。

Abstract: Inverse problems are concerned with the reconstruction of unknown physical
quantities using indirect measurements and are fundamental across diverse
fields such as medical imaging, remote sensing, and material sciences. These
problems serve as critical tools for visualizing internal structures beyond
what is visible to the naked eye, enabling quantification, diagnosis,
prediction, and discovery. However, most inverse problems are ill-posed,
necessitating robust mathematical treatment to yield meaningful solutions.
While classical approaches provide mathematically rigorous and computationally
stable solutions, they are constrained by the ability to accurately model
solution properties and implement them efficiently.
  A more recent paradigm considers deriving solutions to inverse problems in a
data-driven manner. Instead of relying on classical mathematical modeling, this
approach utilizes highly over-parameterized models, typically deep neural
networks, which are adapted to specific inverse problems using carefully
selected training data. Current approaches that follow this new paradigm
distinguish themselves through solution accuracy paired with computational
efficiency that was previously inconceivable.
  These notes offer an introduction to this data-driven paradigm for inverse
problems. The first part of these notes will provide an introduction to inverse
problems, discuss classical solution strategies, and present some applications.
The second part will delve into modern data-driven approaches, with a
particular focus on adversarial regularization and provably convergent linear
plug-and-play denoisers. Throughout the presentation of these methodologies,
their theoretical properties will be discussed, and numerical examples will be
provided. The lecture series will conclude with a discussion of open problems
and future perspectives in the field.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [255] [Convergence of physics-informed neural networks modeling time-harmonic wave fields](https://arxiv.org/abs/2506.11395)
*Stefan Schoder,Aneta Furmanová,Viktor Hruška*

Main category: cs.CE

TL;DR: 该论文研究了基于物理信息的神经网络（PINNs）在三维低频室内声学中的应用，探讨了源定义、边界条件数量和复声速模型对收敛行为的影响。


<details>
  <summary>Details</summary>
Motivation: 研究PINNs在解决声波场问题中的潜力，特别是在复杂几何和边界条件下的表现。

Method: 通过改变源定义、边界条件集数量和复声速模型，评估PINN的收敛行为，并与有限元参考模拟进行对比。

Result: 研究表明，每个波长至少需要六个训练点才能确保PINN的准确训练和预测。

Conclusion: 该研究为低频室内声学建模提供了新方法，尤其是在包含吸声材料的情况下。

Abstract: Studying physics-informed neural networks (PINNs) for modeling partial
differential equations to solve the acoustic wave field has produced promising
results for simple geometries in two-dimensional domains. One option is to
compute the time-harmonic wave field using the Helmholtz equation. Compared to
existing numerical models, the physics-informed neural networks forward problem
has to overcome several topics related to the convergence of the optimization
toward the "true" solution. The topics reach from considering the physical
dimensionality (from 2D to 3D), the modeling of realistic sources (from a
self-similar source to a realistic confined point source), the modeling of
sound-hard (Neumann) boundary conditions, and the modeling of the full wave
field by considering the complex solution quantities. Within this contribution,
we study 3D room acoustic cases at low frequency, varying the source definition
and the number of boundary condition sets and using a complex speed of sound
model to account for some degree of absorption. We assess the convergence
behavior by looking at the loss landscape of the PINN architecture, the $L^2$
error compared to a finite element reference simulation for each network
architecture and configuration. The convergence studies showed that at least
six training points per wavelength are necessary for accurate training and
subsequent predictions of the PINN. The developments are part of an initiative
aiming to model the low-frequency behavior of room acoustics, including
absorbers.

</details>


### [256] [CLEAN-MI: A Scalable and Efficient Pipeline for Constructing High-Quality Neurodata in Motor Imagery Paradigm](https://arxiv.org/abs/2506.11830)
*Dingkun Liu,Zhu Chen,Dongrui Wu*

Main category: cs.CE

TL;DR: CLEAN-MI是一个用于构建大规模、高质量运动想象（MI）脑机接口（BCI）数据集的系统化流程，通过频率带滤波、通道模板选择等技术提升数据质量和分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号存在低信噪比、设备异构性和受试者间差异等问题，阻碍了基础模型的开发。

Method: CLEAN-MI整合频率带滤波、通道模板选择、受试者筛选和边缘分布对齐等技术，标准化多源EEG数据。

Result: 在多个公共MI数据集上验证了CLEAN-MI的有效性，显著提升了数据质量和分类性能。

Conclusion: CLEAN-MI为构建高质量MI数据集提供了可扩展且系统化的解决方案。

Abstract: The construction of large-scale, high-quality datasets is a fundamental
prerequisite for developing robust and generalizable foundation models in motor
imagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals
collected from different subjects and devices are often plagued by low
signal-to-noise ratio, heterogeneity in electrode configurations, and
substantial inter-subject variability, posing significant challenges for
effective model training. In this paper, we propose CLEAN-MI, a scalable and
systematic data construction pipeline for constructing large-scale, efficient,
and accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band
filtering, channel template selection, subject screening, and marginal
distribution alignment to systematically filter out irrelevant or low-quality
data and standardize multi-source EEG datasets. We demonstrate the
effectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent
improvements in data quality and classification performance.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [257] [Recursive KalmanNet: Deep Learning-Augmented Kalman Filtering for State Estimation with Consistent Uncertainty Quantification](https://arxiv.org/abs/2506.11639)
*Hassan Mortada,Cyril Falcon,Yanis Kahil,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: Recursive KalmanNet是一种基于卡尔曼滤波的循环神经网络，用于在噪声测量下实现准确的状态估计和误差协方差量化，优于传统卡尔曼滤波和现有深度学习估计器。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的随机动态系统状态估计常因噪声和非高斯条件而偏离卡尔曼滤波的假设，需要数据驱动的滤波技术。

Method: 采用递归Joseph公式传播误差协方差，并优化高斯负对数似然，构建Kalman滤波启发的循环神经网络。

Result: 在非高斯测量白噪声实验中，Recursive KalmanNet表现优于传统卡尔曼滤波和现有深度学习估计器。

Conclusion: Recursive KalmanNet为复杂噪声条件下的状态估计提供了有效解决方案。

Abstract: State estimation in stochastic dynamical systems with noisy measurements is a
challenge. While the Kalman filter is optimal for linear systems with
independent Gaussian white noise, real-world conditions often deviate from
these assumptions, prompting the rise of data-driven filtering techniques. This
paper introduces Recursive KalmanNet, a Kalman-filter-informed recurrent neural
network designed for accurate state estimation with consistent error covariance
quantification. Our approach propagates error covariance using the recursive
Joseph's formula and optimizes the Gaussian negative log-likelihood.
Experiments with non-Gaussian measurement white noise demonstrate that our
model outperforms both the conventional Kalman filter and an existing
state-of-the-art deep learning based estimator.

</details>


### [258] [Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection](https://arxiv.org/abs/2506.11815)
*Tae-Seong Han,Jae-Wook Heo,Hakseung Kim,Cheol-Hui Lee,Hyub Huh,Eue-Keun Choi,Dong-Joo Kim*

Main category: eess.SP

TL;DR: 提出了一种基于扩散模型的ECG噪声量化方法，通过重建异常检测解决标注不一致和传统方法泛化性差的问题，显著提升了噪声量化的性能。


<details>
  <summary>Details</summary>
Motivation: ECG信号常受噪声干扰，影响临床和可穿戴设备的诊断准确性，传统方法存在标注不一致和泛化性差的问题。

Method: 采用扩散模型框架，通过Wasserstein-1距离（$W_1$）比较干净和噪声ECG的重建误差分布，仅需三步反向扩散即可实现噪声量化。

Result: 模型在基准测试中宏平均$W_1$得分为1.308，优于次优方法48%以上，外部验证显示强泛化性。

Conclusion: 该方法提升了临床决策、诊断准确性和实时ECG监测能力，为临床和可穿戴ECG应用提供了支持。

Abstract: Electrocardiography (ECG) signals are often degraded by noise, which
complicates diagnosis in clinical and wearable settings. This study proposes a
diffusion-based framework for ECG noise quantification via reconstruction-based
anomaly detection, addressing annotation inconsistencies and the limited
generalizability of conventional methods. We introduce a distributional
evaluation using the Wasserstein-1 distance ($W_1$), comparing the
reconstruction error distributions between clean and noisy ECGs to mitigate
inconsistent annotations. Our final model achieved robust noise quantification
using only three reverse diffusion steps. The model recorded a macro-average
$W_1$ score of 1.308 across the benchmarks, outperforming the next-best method
by over 48%. External validations demonstrated strong generalizability,
supporting the exclusion of low-quality segments to enhance diagnostic accuracy
and enable timely clinical responses to signal degradation. The proposed method
enhances clinical decision-making, diagnostic accuracy, and real-time ECG
monitoring capabilities, supporting future advancements in clinical and
wearable ECG applications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [259] [Polymorphism Crystal Structure Prediction with Adaptive Space Group Diversity Control](https://arxiv.org/abs/2506.11332)
*Sadman Sadeed Omee,Lai Wei,Sourin Dey,Jianjun Hu*

Main category: cond-mat.mtrl-sci

TL;DR: ParetoCSP2是一种多目标遗传算法，用于预测无机多晶型结构，通过自适应空间群多样性控制技术提高性能。


<details>
  <summary>Details</summary>
Motivation: 理解多晶型稳定性关系、指导合成并发现新材料，减少试错实验。

Method: 结合神经网络原子间势的自适应空间群多样性控制技术，改进种群初始化方法和迭代结构松弛。

Result: 在多晶型预测中表现优异，空间群和结构相似性准确率接近完美，基准测试中优于基线算法。

Conclusion: ParetoCSP2在多晶型预测中显著优于现有方法，代码开源。

Abstract: Crystalline materials can form different structural arrangements (i.e.
polymorphs) with the same chemical composition, exhibiting distinct physical
properties depending on how they were synthesized or the conditions under which
they operate. For example, carbon can exist as graphite (soft, conductive) or
diamond (hard, insulating). Computational methods that can predict these
polymorphs are vital in materials science, which help understand stability
relationships, guide synthesis efforts, and discover new materials with desired
properties without extensive trial-and-error experimentation. However,
effective crystal structure prediction (CSP) algorithms for inorganic polymorph
structures remain limited. We propose ParetoCSP2, a multi-objective genetic
algorithm for polymorphism CSP that incorporates an adaptive space group
diversity control technique, preventing over-representation of any single space
group in the population guided by a neural network interatomic potential. Using
an improved population initialization method and performing iterative structure
relaxation, ParetoCSP2 not only alleviates premature convergence but also
achieves improved convergence speed. Our results show that ParetoCSP2 achieves
excellent performance in polymorphism prediction, including a nearly perfect
space group and structural similarity accuracy for formulas with two polymorphs
but with the same number of unit cell atoms. Evaluated on a benchmark dataset,
it outperforms baseline algorithms by factors of 2.46-8.62 for these accuracies
and improves by 44.8\%-87.04\% across key performance metrics for regular CSP.
Our source code is freely available at
https://github.com/usccolumbia/ParetoCSP2.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [260] [KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis](https://arxiv.org/abs/2506.11612)
*Zhijie Liu,Qiyi Tang,Sen Nie,Shi Wu,Liang Feng Zhang,Yutian Tang*

Main category: cs.CR

TL;DR: KEENHash是一种新型哈希方法，通过LLM生成函数嵌入，实现高效的大规模二进制代码相似性分析。


<details>
  <summary>Details</summary>
Motivation: 传统的函数级匹配方法时间复杂度过高，无法适应大规模场景（如1/n-to-n搜索）。

Method: 使用K-Means和特征哈希将二进制代码压缩为固定长度的程序嵌入。

Result: KEENHash比现有方法快至少215倍，在53亿次相似性评估中仅需395.83秒。

Conclusion: KEENHash在大规模二进制代码相似性分析中表现出色，尤其在恶意软件检测方面优于现有方法。

Abstract: Binary code similarity analysis (BCSA) is a crucial research area in many
fields such as cybersecurity. Specifically, function-level diffing tools are
the most widely used in BCSA: they perform function matching one by one for
evaluating the similarity between binary programs. However, such methods need a
high time complexity, making them unscalable in large-scale scenarios (e.g.,
1/n-to-n search). Towards effective and efficient program-level BCSA, we
propose KEENHash, a novel hashing approach that hashes binaries into
program-level representations through large language model (LLM)-generated
function embeddings. KEENHash condenses a binary into one compact and
fixed-length program embedding using K-Means and Feature Hashing, allowing us
to do effective and efficient large-scale program-level BCSA, surpassing the
previous state-of-the-art methods. The experimental results show that KEENHash
is at least 215 times faster than the state-of-the-art function matching tools
while maintaining effectiveness. Furthermore, in a large-scale scenario with
5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while
these tools will cost at least 56 days. We also evaluate KEENHash on the
program clone search of large-scale BCSA across extensive datasets in 202,305
binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of
them by at least 23.16%, and displays remarkable superiority over them in the
large-scale BCSA security scenario of malware detection.

</details>


### [261] [SecONNds: Secure Outsourced Neural Network Inference on ImageNet](https://arxiv.org/abs/2506.11586)
*Shashank Balla*

Main category: cs.CR

TL;DR: SecONNds是一种针对大规模卷积神经网络的安全推理框架，通过优化计算和通信开销，显著提升了隐私保护的效率。


<details>
  <summary>Details</summary>
Motivation: 解决外包神经网络推理中的隐私问题，现有框架因高计算和通信成本而不实用。

Method: 采用完全布尔化的GMW协议、NTT预处理和GPU加速，提出SecONNds-P确保精确结果。

Result: 在非线性操作中实现17倍加速，通信开销减少，GPU上端到端推理时间为2.8秒。

Conclusion: SecONNds高效且适合资源受限环境，开源可用。

Abstract: The widespread adoption of outsourced neural network inference presents
significant privacy challenges, as sensitive user data is processed on
untrusted remote servers. Secure inference offers a privacy-preserving
solution, but existing frameworks suffer from high computational overhead and
communication costs, rendering them impractical for real-world deployment. We
introduce SecONNds, a non-intrusive secure inference framework optimized for
large ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel
fully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison
-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit
triples generated from Silent Random Oblivious Transfer. Our novel protocol
achieves an online speedup of 17$\times$ in nonlinear operations compared to
state-of-the-art solutions while reducing communication overhead. To further
enhance performance, SecONNds employs Number Theoretic Transform (NTT)
preprocessing and leverages GPU acceleration for homomorphic encryption
operations, resulting in speedups of 1.6$\times$ on CPU and 2.2$\times$ on GPU
for linear operations. We also present SecONNds-P, a bit-exact variant that
ensures verifiable full-precision results in secure computation, matching the
results of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet
model, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s
on CPU, with a total communication of just 420 MiB. SecONNds' efficiency and
reduced computational load make it well-suited for deploying privacy-sensitive
applications in resource-constrained environments. SecONNds is open source and
can be accessed from: https://github.com/shashankballa/SecONNds.

</details>


### [262] [Differential Privacy in Machine Learning: From Symbolic AI to LLMs](https://arxiv.org/abs/2506.11687)
*Francisco Aguilera-Martínez,Fernando Berzal*

Main category: cs.CR

TL;DR: 这篇综述论文探讨了差分隐私（DP）在机器学习中的应用，包括其定义、发展历程、集成方法及实践评估。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型不泄露敏感信息，通过差分隐私框架减少隐私风险。

Method: 回顾差分隐私的基础定义和关键研究进展，分析其在机器学习中的集成方法。

Result: 提供了差分隐私在机器学习中的全面概述，包括现有提案和隐私保护方法。

Conclusion: 差分隐私有助于开发安全、负责任的AI系统，但仍面临对抗性攻击等挑战。

Abstract: Machine learning models should not reveal particular information that is not
otherwise accessible. Differential privacy provides a formal framework to
mitigate privacy risks by ensuring that the inclusion or exclusion of any
single data point does not significantly alter the output of an algorithm, thus
limiting the exposure of private information. This survey paper explores the
foundational definitions of differential privacy, reviews its original
formulations and tracing its evolution through key research contributions. It
then provides an in-depth examination of how DP has been integrated into
machine learning models, analyzing existing proposals and methods to preserve
privacy when training ML models. Finally, it describes how DP-based ML
techniques can be evaluated in practice. %Finally, it discusses the broader
implications of DP, highlighting its potential for public benefit, its
real-world applications, and the challenges it faces, including vulnerabilities
to adversarial attacks. By offering a comprehensive overview of differential
privacy in machine learning, this work aims to contribute to the ongoing
development of secure and responsible AI systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [263] [Complexity of normalized stochastic first-order methods with momentum under heavy-tailed noise](https://arxiv.org/abs/2506.11214)
*Chuan He,Zhaosong Lu,Defeng Sun,Zhanwang Deng*

Main category: math.OC

TL;DR: 提出了具有Polyak动量、多外推动量和递归动量的归一化随机一阶方法，用于解决无约束优化问题，无需依赖问题相关参数，并在弱平滑和重尾噪声条件下改进了复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 解决传统优化方法依赖问题相关参数（如Lipschitz常数）的局限性，并在更弱的假设条件下提升性能。

Method: 采用动态更新的算法参数，结合多种动量技术（Polyak、多外推、递归动量），无需显式问题参数。

Result: 在重尾噪声和弱平均平滑条件下，复杂度结果优于或匹配现有最佳结果。

Conclusion: 数值实验验证了方法的有效性，为无约束优化提供了更实用的解决方案。

Abstract: In this paper, we propose practical normalized stochastic first-order methods
with Polyak momentum, multi-extrapolated momentum, and recursive momentum for
solving unconstrained optimization problems. These methods employ dynamically
updated algorithmic parameters and do not require explicit knowledge of
problem-dependent quantities such as the Lipschitz constant or noise bound. We
establish first-order oracle complexity results for finding approximate
stochastic stationary points under heavy-tailed noise and weakly average
smoothness conditions -- both of which are weaker than the commonly used
bounded variance and mean-squared smoothness assumptions. Our complexity bounds
either improve upon or match the best-known results in the literature.
Numerical experiments are presented to demonstrate the practical effectiveness
of the proposed methods.

</details>


### [264] [Quantum Learning and Estimation for Distribution Networks and Energy Communities Coordination](https://arxiv.org/abs/2506.11730)
*Yingrui Zhuang,Lin Cheng,Yuji Cao,Tongxin Li,Ning Qi,Yan Xu,Yue Chen*

Main category: math.OC

TL;DR: 论文提出了一种量子学习和估计方法，通过量子特性提升电网与能源社区的协调效率，显著提高了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 电网与能源社区的协调面临信息有限和高计算负担的挑战，需要更高效的方法。

Method: 开发了混合量子时间卷积网络-长短期记忆（Q-TCN-LSTM）模型和基于量子振幅估计（QAE）的量子估计方法。

Result: Q-TCN-LSTM模型比传统神经网络准确率提高69.2%，模型大小减少99.75%，计算时间减少93.9%；QAE比蒙特卡洛模拟计算时间减少99.99%。

Conclusion: 量子方法显著提升了电网与能源社区协调的效率和准确性，具有实际应用潜力。

Abstract: Price signals from distribution networks (DNs) guide energy communities (ECs)
to adjust energy usage, enabling effective coordination for reliable power
system operation. However, this coordination faces significant challenges due
to the limited availability of information (i.e., only the aggregated energy
usage of ECs is available to DNs), and the high computational burden of
accounting for uncertainties and the associated risks through numerous
scenarios. To address these challenges, we propose a quantum learning and
estimation approach to enhance coordination between DNs and ECs. Specifically,
leveraging advanced quantum properties such as quantum superposition and
entanglement, we develop a hybrid quantum temporal convolutional network-long
short-term memory (Q-TCN-LSTM) model to establish an end-to-end mapping between
ECs' responses and the price incentives from DNs. Moreover, we develop a
quantum estimation method based on quantum amplitude estimation (QAE) and two
phase-rotation circuits to significantly accelerate the optimization process
under numerous uncertainty scenarios. Numerical experiments demonstrate that,
compared to classical neural networks, the proposed Q-TCN-LSTM model improves
the mapping accuracy by 69.2% while reducing the model size by 99.75% and the
computation time by 93.9%. Compared to classical Monte Carlo simulation, QAE
achieves comparable accuracy with a dramatic reduction in computational time
(up to 99.99%) and requires significantly fewer computational resources.

</details>


### [265] [Convergence of Momentum-Based Optimization Algorithms with Time-Varying Parameters](https://arxiv.org/abs/2506.11904)
*Mathukumalli Vidyasagar*

Main category: math.OC

TL;DR: 本文提出了一种统一的随机优化算法，利用“动量”项，将SHB和SNAG算法作为特例，并允许动量随时间变化。算法在梯度估计条件最宽松的情况下收敛。


<details>
  <summary>Details</summary>
Motivation: 解决随机优化中梯度估计的偏差和方差问题，扩展算法适用范围至零阶方法。

Method: 提出统一算法，结合动量项，允许其随时间变化，并分析收敛条件。

Result: 证明了算法在最宽松梯度条件下的收敛性，并指出文献中某方法的不可行性。

Conclusion: 统一算法具有广泛适用性，为零阶方法提供了理论支持。

Abstract: In this paper, we present a unified algorithm for stochastic optimization
that makes use of a "momentum" term; in other words, the stochastic gradient
depends not only on the current true gradient of the objective function, but
also on the true gradient at the previous iteration. Our formulation includes
the Stochastic Heavy Ball (SHB) and the Stochastic Nesterov Accelerated
Gradient (SNAG) algorithms as special cases. In addition, in our formulation,
the momentum term is allowed to vary as a function of time (i.e., the iteration
counter). The assumptions on the stochastic gradient are the most general in
the literature, in that it can be biased, and have a conditional variance that
grows in an unbounded fashion as a function of time. This last feature is
crucial in order to make the theory applicable to "zero-order" methods, where
the gradient is estimated using just two function evaluations.
  We present a set of sufficient conditions for the convergence of the unified
algorithm. These conditions are natural generalizations of the familiar
Robbins-Monro and Kiefer-Wolfowitz-Blum conditions for standard stochastic
gradient descent. We also analyze another method from the literature for the
SHB algorithm with a time-varying momentum parameter, and show that it is
impracticable.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [266] [Challenges in Automated Processing of Speech from Child Wearables: The Case of Voice Type Classifier](https://arxiv.org/abs/2506.11074)
*Tarek Kunze,Marianne Métais,Hadrien Titeux,Lucas Elbert,Joseph Coffey,Emmanuel Dupoux,Alejandrina Cristia,Marvin Lavechin*

Main category: eess.AS

TL;DR: 论文探讨了儿童佩戴设备记录语音数据的挑战，指出语音技术在处理大量数据时面临障碍，尤其是语音类型分类任务。


<details>
  <summary>Details</summary>
Motivation: 通过儿童佩戴设备记录自然语音环境，为语音科学提供大量数据，但需技术将其转化为有用信息。

Method: 总结了三年实验，改进语音类型分类任务，测试了特征表示、架构和参数搜索。

Result: 特征、架构和参数搜索的改进仅带来边际性能提升，数据相关性和数量更为关键。

Conclusion: 收集数据时需确保权限以共享，数据质量和数量对语音技术进展至关重要。

Abstract: Recordings gathered with child-worn devices promised to revolutionize both
fundamental and applied speech sciences by allowing the effortless capture of
children's naturalistic speech environment and language production. This
promise hinges on speech technologies that can transform the sheer mounds of
data thus collected into usable information. This paper demonstrates several
obstacles blocking progress by summarizing three years' worth of experiments
aimed at improving one fundamental task: Voice Type Classification. Our
experiments suggest that improvements in representation features, architecture,
and parameter search contribute to only marginal gains in performance. More
progress is made by focusing on data relevance and quantity, which highlights
the importance of collecting data with appropriate permissions to allow
sharing.

</details>


### [267] [Fifteen Years of Child-Centered Long-Form Recordings: Promises, Resources, and Remaining Challenges to Validity](https://arxiv.org/abs/2506.11075)
*Loann Peurey,Marvin Lavechin,Tarek Kunze,Manel Khentout,Lucas Gautheron,Emmanuel Dupoux,Alejandrina Cristia*

Main category: eess.AS

TL;DR: 论文总结了使用儿童佩戴设备收集的音频数据在语言研究中的应用，提出了自动化分析的必要性，并探讨了误差来源及改进策略。


<details>
  <summary>Details</summary>
Motivation: 通过长时录音捕捉儿童语言输入和输出的高有效性数据，但数据量大需自动化分析。

Method: 总结现有技术资源，提出误差来源及改进策略。

Result: 指出自动化质量控制的局限性，但提供了实用的数据收集和分析策略。

Conclusion: 尽管完全自动化的质量控制不可行，但研究者可通过改进数据收集和上下文分析提升研究质量。

Abstract: Audio-recordings collected with a child-worn device are a fundamental tool in
child language research. Long-form recordings collected over whole days promise
to capture children's input and production with minimal observer bias, and
therefore high validity. The sheer volume of resulting data necessitates
automated analysis to extract relevant metrics for researchers and clinicians.
This paper summarizes collective knowledge on this technique, providing entry
points to existing resources. We also highlight various sources of error that
threaten the accuracy of automated annotations and the interpretation of
resulting metrics. To address this, we propose potential troubleshooting
metrics to help users assess data quality. While a fully automated quality
control system is not feasible, we outline practical strategies for researchers
to improve data collection and contextualize their analyses.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [268] [Decentralized Uplink Adaptive Compression for Cell-Free MIMO with Limited Fronthaul](https://arxiv.org/abs/2506.11284)
*Zehua Li,Jingjie Wei,Raviraj Adve*

Main category: cs.IT

TL;DR: 本文研究了在有限前传容量下，无小区多输入多输出网络的上行链路压缩问题，提出了一种基于速率的自适应压缩方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于变换矩阵优化，无法适应网络流量和前传限制的动态变化，因此需要一种更灵活的自适应压缩方法。

Method: 提出了一种基于速率的自适应压缩方法，同时确定压缩维度和压缩方式，并利用互信息作为目标函数实现理论网络容量。

Result: 通过信道统计和用户流量密度，展示了如何高效计算全局信道状态信息的侧信息，并在低开销下实现去中心化压缩。

Conclusion: 去中心化的自适应压缩方法在网络性能上与集中式方法相当，同时保持了低信息交换开销。

Abstract: We study the problem of uplink compression for cell-free multi-input
multi-output networks with limited fronthaul capacity. In compress-forward
mode, remote radio heads (RRHs) compress the received signal and forward it to
a central unit for joint processing. While previous work has focused on a
transform-based approach, which optimizes the transform matrix that reduces
signals of high dimension to a static pre-determined lower dimension, we
propose a rate-based approach that simultaneously finds both dimension and
compression adaptively. Our approach accommodates for changes to network
traffic and fronthaul limits. Using mutual information as the objective, we
obtain the theoretical network capacity for adaptive compression and decouple
the expression to enable decentralization. Furthermore, using channel
statistics and user traffic density, we show different approaches to compute an
efficient representation of side information that summarizes global channel
state information and is shared with RRHs to assist compression. While keeping
the information exchange overhead low, our decentralized implementation of
adaptive compression shows competitive overall network performance compared to
a centralized approach.

</details>


### [269] [Black-Box Edge AI Model Selection with Conformal Latency and Accuracy Guarantees](https://arxiv.org/abs/2506.11391)
*Anders E. Kalør,Tomoaki Ohtsuki*

Main category: cs.IT

TL;DR: 本文提出了一种新颖的黑盒模型选择框架，用于6G中可靠的实时无线边缘AI，旨在满足预定义的截止时间违反概率和预期损失要求。


<details>
  <summary>Details</summary>
Motivation: 6G应用（如自动驾驶和机器人技术）需要低延迟和高精度的AI服务，但由于ML模型的黑盒性质、任务复杂性以及无线信道随机性，实现这一目标具有挑战性。

Method: 利用符合风险控制和非参数统计，从不同复杂度和计算时间的黑盒特征提取和推理模型中选择最优组合，提出固定和动态模型选择方案。

Result: 数值结果验证了框架在截止时间约束的图像分类任务中的有效性，同时满足最大误分类概率要求。

Conclusion: 该框架有望为6G提供可靠的实时边缘AI服务。

Abstract: Edge artificial intelligence (AI) will be a central part of 6G, with powerful
edge servers supporting devices in performing machine learning (ML) inference.
However, it is challenging to deliver the latency and accuracy guarantees
required by 6G applications, such as automated driving and robotics. This stems
from the black-box nature of ML models, the complexities of the tasks, and the
interplay between transmitted data quality, chosen inference model, and the
random wireless channel. This paper proposes a novel black-box model selection
framework for reliable real-time wireless edge AI designed to meet predefined
requirements on both deadline violation probability and expected loss.
Leveraging conformal risk control and non-parametric statistics, our framework
intelligently selects the optimal model combination from a collection of
black-box feature-extraction and inference models of varying complexities and
computation times. We present both a fixed (relying on channel statistics) and
a dynamic (channel-adaptive) model selection scheme. Numerical results validate
the framework on a deadline-constrained image classification task while
satisfying a maximum misclassification probability requirement. These results
indicate that the proposed framework has the potential to provide reliable
real-time edge AI services in 6G.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [270] [Deep Learning Model Acceleration and Optimization Strategies for Real-Time Recommendation Systems](https://arxiv.org/abs/2506.11421)
*Junli Shao,Jing Dong,Dingzhou Wang,Kowei Shih,Dannier Li,Chengrui Zhou*

Main category: cs.IR

TL;DR: 论文提出了一种结合模型和系统级优化的方法，显著降低了实时推荐系统的延迟并提高了吞吐量，同时保持了推荐质量。


<details>
  <summary>Details</summary>
Motivation: 随着互联网服务的快速增长，实时推荐系统面临高计算成本和资源瓶颈的挑战，需要在不牺牲推荐质量的情况下减少延迟并提高吞吐量。

Method: 在模型层面，通过轻量级网络设计、结构化剪枝和权重量化减少参数和计算需求；在系统层面，整合异构计算平台和高性能推理库，并设计弹性推理调度和负载均衡机制。

Result: 实验表明，在保持推荐准确性的同时，延迟降低至基线的30%以下，系统吞吐量提高了一倍以上。

Conclusion: 该方法为大规模在线推荐服务的部署提供了实用解决方案。

Abstract: With the rapid growth of Internet services, recommendation systems play a
central role in delivering personalized content. Faced with massive user
requests and complex model architectures, the key challenge for real-time
recommendation systems is how to reduce inference latency and increase system
throughput without sacrificing recommendation quality. This paper addresses the
high computational cost and resource bottlenecks of deep learning models in
real-time settings by proposing a combined set of modeling- and system-level
acceleration and optimization strategies. At the model level, we dramatically
reduce parameter counts and compute requirements through lightweight network
design, structured pruning, and weight quantization. At the system level, we
integrate multiple heterogeneous compute platforms and high-performance
inference libraries, and we design elastic inference scheduling and
load-balancing mechanisms based on real-time load characteristics. Experiments
show that, while maintaining the original recommendation accuracy, our methods
cut latency to less than 30% of the baseline and more than double system
throughput, offering a practical solution for deploying large-scale online
recommendation services.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [271] [Decadal sink-source shifts of forest aboveground carbon since 1988](https://arxiv.org/abs/2506.11879)
*Zhen Qian,Sebastian Bathiany,Teng Liu,Lana L. Blaschke,Hoong Chen Teo,Niklas Boers*

Main category: physics.geo-ph

TL;DR: 全球森林在1988-2021年间总体为碳汇，但热带湿润森林和北方森林逐渐转为碳源，而温带和亚热带森林碳储量增加。


<details>
  <summary>Details</summary>
Motivation: 研究全球森林地上碳（AGC）的长期动态及其碳汇-源转换，以应对气候变化和碳循环的不确定性。

Method: 结合多源卫星观测和概率深度学习模型，估算高空间分辨率的全球森林AGC及其不确定性。

Result: 全球森林30年碳汇6.2 PgC，但热带湿润森林和北方森林转为碳源；热带森林AGC变化与大气CO2增长率负相关增强。

Conclusion: 热带森林AGC在调节陆地碳循环中的作用增强，气候变化可能加剧未受干扰区域的碳损失。

Abstract: As enduring carbon sinks, forest ecosystems are vital to the terrestrial
carbon cycle and help moderate global warming. However, the long-term dynamics
of aboveground carbon (AGC) in forests and their sink-source transitions remain
highly uncertain, owing to changing disturbance regimes and inconsistencies in
observations, data processing, and analysis methods. Here, we derive reliable,
harmonized AGC stocks and fluxes in global forests from 1988 to 2021 at high
spatial resolution by integrating multi-source satellite observations with
probabilistic deep learning models. Our approach simultaneously estimates AGC
and associated uncertainties, showing high reliability across space and time.
We find that, although global forests remained an AGC sink of 6.2 PgC over 30
years, moist tropical forests shifted to a substantial AGC source between 2001
and 2010 and, together with boreal forests, transitioned toward a source in the
2011-2021 period. Temperate, dry tropical and subtropical forests generally
exhibited increasing AGC stocks, although Europe and Australia became sources
after 2011. Regionally, pronounced sink-to-source transitions occurred in
tropical forests over the past three decades. The interannual relationship
between global atmospheric CO2 growth rates and tropical AGC flux variability
became increasingly negative, reaching Pearson's r = -0.63 (p < 0.05) in the
most recent decade. In the Brazilian Amazon, the contribution of deforested
regions to AGC losses declined from 60% in 1989-2000 to 13% in 2011-2021, while
the share from untouched areas increased from 33% to 76%. Our findings suggest
a growing role of tropical forest AGC in modulating variability in the
terrestrial carbon cycle, with anthropogenic climate change potentially
contributing increasingly to AGC changes, particularly in previously untouched
areas.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [272] [Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task](https://arxiv.org/abs/2506.11986)
*Wuzhenghong Wen,Su Pan,yuwei Sun*

Main category: cs.AI

TL;DR: 论文提出了一种基于强化学习的Schema-R1模型，用于提升Text-to-SQL任务中模式链接的推理能力，相比现有方法在过滤准确率上提升了10%。


<details>
  <summary>Details</summary>
Motivation: 当前模式链接模型的微调方法过度依赖死记硬背，缺乏推理能力，主要因为高质量推理样本难以获取。

Method: Schema-R1通过构建高质量推理样本小批量、监督微调冷启动初始化和基于规则的强化学习训练三个步骤实现。

Result: 实验结果表明，该方法显著提升了模式链接模型的推理能力，过滤准确率提高了10%。

Conclusion: Schema-R1通过强化学习有效解决了模式链接中的推理能力不足问题，为Text-to-SQL任务提供了更优解决方案。

Abstract: Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.

</details>


### [273] [OntoGSN: An Ontology for Dynamic Management of Assurance Cases](https://arxiv.org/abs/2506.11023)
*Tomas Bueno Momcilovic,Barbara Gallina,Ingmar Kessler,Dian Balta*

Main category: cs.AI

TL;DR: OntoGSN是一个基于GSN标准的本体和中间件，用于管理保证案例（ACs），提供知识表示和可查询图，支持自动填充、评估和更新。


<details>
  <summary>Details</summary>
Motivation: 管理ACs在动态环境中具有挑战性，现有工具难以维护嵌入的知识，导致开发效率低或虚假信心。

Method: 提出OntoGSN，包括GSN标准的OWL本体、辅助本体和解析器、SPARQL查询库及原型界面。

Result: OntoGSN严格遵循GSN标准，并通过多种评估验证其有效性。

Conclusion: OntoGSN为动态AC管理提供了实用工具，并通过示例展示了其在大型语言模型对抗鲁棒性保证中的应用。

Abstract: Assurance cases (ACs) are a common artifact for building and maintaining
confidence in system properties such as safety or robustness. Constructing an
AC can be challenging, although existing tools provide support in static,
document-centric applications and methods for dynamic contexts (e.g.,
autonomous driving) are emerging. Unfortunately, managing ACs remains a
challenge, since maintaining the embedded knowledge in the face of changes
requires substantial effort, in the process deterring developers - or worse,
producing poorly managed cases that instill false confidence. To address this,
we present OntoGSN: an ontology and supporting middleware for managing ACs in
the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge
representation and a queryable graph that can be automatically populated,
evaluated, and updated. Our contributions include: a 1:1 formalization of the
GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology
and parser for integration with a widely used AC tool; a repository and
documentation of design decisions for OntoGSN maintenance; a SPARQL query
library with automation patterns; and a prototypical interface. The ontology
strictly adheres to the standard's text and has been evaluated according to
FAIR principles, the OOPS framework, competency questions, and community
feedback. The development of other middleware elements is guided by the
community needs and subject to ongoing evaluations. To demonstrate the utility
of our contributions, we illustrate dynamic AC management in an example
involving assurance of adversarial robustness in large language models.

</details>


### [274] [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)
*Lin Sun,Weihong Lin,Jinzhu Wu,Yongfu Zhu,Xiaoqi Jian,Guangxiang Zhao,Change Jia,Linglin Zhang,Sai-er Hu,Yuhan Wu,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 研究发现Deepseek-R1-Distill系列模型的基准评估结果受多种因素影响，波动较大，呼吁建立更严格的评估范式。


<details>
  <summary>Details</summary>
Motivation: 揭示开源推理模型评估结果的不可靠性，提出改进评估方法的必要性。

Method: 通过实证评估Deepseek-R1-Distill系列模型，分析评估条件对结果的影响。

Result: 评估结果因细微条件差异而波动，性能改进难以可靠复现。

Conclusion: 需要更严格的模型性能评估范式以确保结果的可信度。

Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.

</details>


### [275] [Relational GNNs Cannot Learn $C_2$ Features for Planning](https://arxiv.org/abs/2506.11721)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: R-GNNs无法学习由$C_2$特征定义的价值函数，但其他GNN架构可能更适合。


<details>
  <summary>Details</summary>
Motivation: 研究R-GNNs在规划领域中学习价值函数的局限性，并探索更合适的GNN架构。

Method: 通过理论分析，探讨R-GNNs与$C_2$特征的表达能力关系。

Result: R-GNNs无法学习由$C_2$特征定义的价值函数。

Conclusion: 其他GNN架构可能更适合学习此类价值函数。

Abstract: Relational Graph Neural Networks (R-GNNs) are a GNN-based approach for
learning value functions that can generalise to unseen problems from a given
planning domain. R-GNNs were theoretically motivated by the well known
connection between the expressive power of GNNs and $C_2$, first-order logic
with two variables and counting. In the context of planning, $C_2$ features
refer to the set of formulae in $C_2$ with relations defined by the unary and
binary predicates of a planning domain. Some planning domains exhibit optimal
value functions that can be decomposed as arithmetic expressions of $C_2$
features. We show that, contrary to empirical results, R-GNNs cannot learn
value functions defined by $C_2$ features. We also identify prior GNN
architectures for planning that may better learn value functions defined by
$C_2$ features.

</details>


### [276] [Causal Effect Identification in Heterogeneous Environments from Higher-Order Moments](https://arxiv.org/abs/2506.11756)
*Yaroslav Kivva,Sina Akbari,Saber Salehkaleybar,Negar Kiyavash*

Main category: cs.AI

TL;DR: 论文研究了在存在潜在混杂变量的情况下，估计治疗变量对结果的因果效应。通过多环境数据，证明了在目标因果效应不变条件下可识别性，并提出了一种基于矩的估计算法。同时，证明了如果潜在和治疗变量的外生噪声分布均变化，则无法识别。最后，提出了识别变化参数的方法，并通过实验验证了性能。


<details>
  <summary>Details</summary>
Motivation: 研究在潜在混杂变量存在时，如何准确估计因果效应，尤其是在多环境数据下的可识别性和估计方法。

Method: 提出基于矩的估计算法，利用多环境数据中目标因果效应的不变性，识别变化的参数。

Result: 证明了在单一参数变化条件下因果效应的可识别性，并验证了算法的有效性。

Conclusion: 在多环境数据中，通过不变性和参数变化识别，可以有效估计因果效应，但需避免多参数同时变化。

Abstract: We investigate the estimation of the causal effect of a treatment variable on
an outcome in the presence of a latent confounder. We first show that the
causal effect is identifiable under certain conditions when data is available
from multiple environments, provided that the target causal effect remains
invariant across these environments. Secondly, we propose a moment-based
algorithm for estimating the causal effect as long as only a single parameter
of the data-generating mechanism varies across environments -- whether it be
the exogenous noise distribution or the causal relationship between two
variables. Conversely, we prove that identifiability is lost if both exogenous
noise distributions of both the latent and treatment variables vary across
environments. Finally, we propose a procedure to identify which parameter of
the data-generating mechanism has varied across the environments and evaluate
the performance of our proposed methods through experiments on synthetic data.

</details>


### [277] [On the Performance of LLMs for Real Estate Appraisal](https://arxiv.org/abs/2506.11812)
*Margot Geerts,Manon Reusens,Bart Baesens,Seppe vanden Broucke,Jochen De Weerdt*

Main category: cs.AI

TL;DR: 该研究探讨了如何利用大型语言模型（LLMs）通过优化的上下文学习策略生成可解释的房价估计，以缓解房地产市场的信息不对称问题。


<details>
  <summary>Details</summary>
Motivation: 房地产市场对全球经济至关重要，但存在严重的信息不对称问题。研究旨在利用LLMs提供更透明、可解释的房价评估方法。

Method: 研究系统评估了多种LLMs在国际住房数据集上的表现，比较了零样本、少样本、市场报告增强和混合提示技术。

Result: LLMs能够有效利用房屋特征生成有意义的估计，尽管传统机器学习模型在预测准确性上更强，但LLMs提供了更易用和可解释的替代方案。

Conclusion: LLMs在提高房地产评估透明度方面具有潜力，但需注意其局限性，如对价格区间的过度自信和空间推理能力有限。

Abstract: The real estate market is vital to global economies but suffers from
significant information asymmetry. This study examines how Large Language
Models (LLMs) can democratize access to real estate insights by generating
competitive and interpretable house price estimates through optimized
In-Context Learning (ICL) strategies. We systematically evaluate leading LLMs
on diverse international housing datasets, comparing zero-shot, few-shot,
market report-enhanced, and hybrid prompting techniques. Our results show that
LLMs effectively leverage hedonic variables, such as property size and
amenities, to produce meaningful estimates. While traditional machine learning
models remain strong for pure predictive accuracy, LLMs offer a more
accessible, interactive and interpretable alternative. Although
self-explanations require cautious interpretation, we find that LLMs explain
their predictions in agreement with state-of-the-art models, confirming their
trustworthiness. Carefully selected in-context examples based on feature
similarity and geographic proximity, significantly enhance LLM performance, yet
LLMs struggle with overconfidence in price intervals and limited spatial
reasoning. We offer practical guidance for structured prediction tasks through
prompt optimization. Our findings highlight LLMs' potential to improve
transparency in real estate appraisal and provide actionable insights for
stakeholders.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [278] [A Framework for Non-Linear Attention via Modern Hopfield Networks](https://arxiv.org/abs/2506.11043)
*Ahmed Farooq*

Main category: stat.ML

TL;DR: 本文提出了一种基于现代Hopfield网络（MNH）的能量函数，其稳定点对应于Vaswani等人提出的注意力机制，从而统一了两者框架。能量函数的最小值形成“上下文井”，稳定地封装了token之间的上下文关系。非线性注意力机制可以增强Transformer模型在序列建模任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 通过能量函数统一现代Hopfield网络和注意力机制，揭示两者之间的联系，并提出非线性注意力机制以提升Transformer模型的性能。

Method: 提出一种能量函数，其梯度对应于注意力计算，并探讨非线性注意力机制在Transformer模型中的应用。

Result: 能量函数的最小值形成“上下文井”，非线性注意力机制能提升模型对复杂关系的理解、表示学习能力及整体效率。

Conclusion: 本文通过能量函数统一了现代Hopfield网络和注意力机制，并提出非线性注意力机制为Transformer模型提供了更强的建模能力。

Abstract: In this work we propose an energy functional along the lines of Modern
Hopfield Networks (MNH), the stationary points of which correspond to the
attention due to Vaswani et al. [12], thus unifying both frameworks. The minima
of this landscape form "context wells" - stable configurations that encapsulate
the contextual relationships among tokens. A compelling picture emerges: across
$n$ token embeddings an energy landscape is defined whose gradient corresponds
to the attention computation. Non-linear attention mechanisms offer a means to
enhance the capabilities of transformer models for various sequence modeling
tasks by improving the model's understanding of complex relationships, learning
of representations, and overall efficiency and performance. A rough analogy can
be seen via cubic splines which offer a richer representation of non-linear
data where a simpler linear model may be inadequate. This approach can be used
for the introduction of non-linear heads in transformer based models such as
BERT, [6], etc.

</details>


### [279] [Collaborative Prediction: To Join or To Disjoin Datasets](https://arxiv.org/abs/2506.11271)
*Kyung Rok Kim,Yansong Wang,Xiaocheng Li,Guanting Chen*

Main category: stat.ML

TL;DR: 论文研究了如何通过选择高质量数据集来最小化预测模型的总体损失，并提出了一种具有理论保证的实用算法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的兴起，选择高质量数据集以提升机器学习模型性能的需求日益增加，但这一领域仍有部分问题未被充分探索。

Method: 通过利用oracle不等式和数据驱动估计器，提出了一种算法，能够高概率地减少总体损失。

Result: 数值实验表明，该算法在标准线性回归和更广泛的机器学习应用中均有效。

Conclusion: 该研究为数据集选择和合并提供了实用解决方案，并具有理论支持。

Abstract: With the recent rise of generative Artificial Intelligence (AI), the need of
selecting high-quality dataset to improve machine learning models has garnered
increasing attention. However, some part of this topic remains underexplored,
even for simple prediction models. In this work, we study the problem of
developing practical algorithms that select appropriate dataset to minimize
population loss of our prediction model with high probability. Broadly
speaking, we investigate when datasets from different sources can be
effectively merged to enhance the predictive model's performance, and propose a
practical algorithm with theoretical guarantees. By leveraging an oracle
inequality and data-driven estimators, the algorithm reduces population loss
with high probability. Numerical experiments demonstrate its effectiveness in
both standard linear regression and broader machine learning applications. Code
is available at https://github.com/kkrokii/collaborative_prediction.

</details>


### [280] [Fast Bayesian Optimization of Function Networks with Partial Evaluations](https://arxiv.org/abs/2506.11456)
*Poompol Buathong,Peter I. Frazier*

Main category: stat.ML

TL;DR: 提出了一种加速的p-KGFN算法，通过全局蒙特卡洛模拟生成节点候选输入，显著降低计算开销，同时保持查询效率。


<details>
  <summary>Details</summary>
Motivation: 优化昂贵的函数网络评估，特别是在制造和药物发现等应用中，节点可独立评估且成本不同。

Method: 通过全局蒙特卡洛模拟生成节点候选输入，减少嵌套优化的计算开销。

Result: 新方法在保持查询效率的同时，计算速度提升高达16倍。

Conclusion: 加速的p-KGFN算法在计算效率和查询效率之间取得了良好平衡。

Abstract: Bayesian optimization of function networks (BOFN) is a framework for
optimizing expensive-to-evaluate objective functions structured as networks,
where some nodes' outputs serve as inputs for others. Many real-world
applications, such as manufacturing and drug discovery, involve function
networks with additional properties - nodes that can be evaluated independently
and incur varying costs. A recent BOFN variant, p-KGFN, leverages this
structure and enables cost-aware partial evaluations, selectively querying only
a subset of nodes at each iteration. p-KGFN reduces the number of expensive
objective function evaluations needed but has a large computational overhead:
choosing where to evaluate requires optimizing a nested Monte Carlo-based
acquisition function for each node in the network. To address this, we propose
an accelerated p-KGFN algorithm that reduces computational overhead with only a
modest loss in query efficiency. Key to our approach is generation of
node-specific candidate inputs for each node in the network via one inexpensive
global Monte Carlo simulation. Numerical experiments show that our method
maintains competitive query efficiency while achieving up to a 16x speedup over
the original p-KGFN algorithm.

</details>


### [281] [On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiologic boundary conditions](https://arxiv.org/abs/2506.11683)
*Chloe H. Choi,Andrea Zanoni,Daniele E. Schiavazzi,Alison L. Marsden*

Main category: stat.ML

TL;DR: 本文探讨了在心血管建模中通过低保真近似降低贝叶斯参数估计计算成本的方法，比较了五种不同方法，并在心血管模型上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 解决心血管建模中的逆问题计算成本高的问题。

Method: 利用低保真近似构建替代模型，包括直接替代高保真模型、替代高低保真模型间的差异，以及将差异视为随机噪声并估计其分布。

Result: 在分析测试案例和心血管模型中验证了五种方法的准确性和计算成本。

Conclusion: 提出的方法能有效降低计算成本，适用于不同复杂度的心血管模型。

Abstract: Solving inverse problems in cardiovascular modeling is particularly
challenging due to the high computational cost of running high-fidelity
simulations. In this work, we focus on Bayesian parameter estimation and
explore different methods to reduce the computational cost of sampling from the
posterior distribution by leveraging low-fidelity approximations. A common
approach is to construct a surrogate model for the high-fidelity simulation
itself. Another is to build a surrogate for the discrepancy between high- and
low-fidelity models. This discrepancy, which is often easier to approximate, is
modeled with either a fully connected neural network or a nonlinear
dimensionality reduction technique that enables surrogate construction in a
lower-dimensional space. A third possible approach is to treat the discrepancy
between the high-fidelity and surrogate models as random noise and estimate its
distribution using normalizing flows. This allows us to incorporate the
approximation error into the Bayesian inverse problem by modifying the
likelihood function. We validate five different methods which are variations of
the above on analytical test cases by comparing them to posterior distributions
derived solely from high-fidelity models, assessing both accuracy and
computational cost. Finally, we demonstrate our approaches on two
cardiovascular examples of increasing complexity: a lumped-parameter Windkessel
model and a patient-specific three-dimensional anatomy.

</details>


### [282] [Using Deep Operators to Create Spatio-temporal Surrogates for Dynamical Systems under Uncertainty](https://arxiv.org/abs/2506.11761)
*Jichuan Tang,Patrick T. Brewick,Ryan G. McClarren,Christopher Sweet*

Main category: stat.ML

TL;DR: 提出了一种新型的深度算子网络变体FExD，用于时空替代模型，在多自由度动态系统中实现高效准确的多输出响应预测。


<details>
  <summary>Details</summary>
Motivation: 解决现有SciML方法在时空替代模型中的局限性，尤其是在多自由度动态系统中的应用。

Method: 通过增强分支网络的表达能力和扩展主干网络的预测能力，提出FExD模型。

Result: FExD在精度和计算效率上均优于传统DeepONet和时空扩展DeepONet。

Conclusion: FExD为结构动力学应用中的算子学习提供了显著进展。

Abstract: Spatio-temporal data, which consists of responses or measurements gathered at
different times and positions, is ubiquitous across diverse applications of
civil infrastructure. While SciML methods have made significant progress in
tackling the issue of response prediction for individual time histories,
creating a full spatial-temporal surrogate remains a challenge. This study
proposes a novel variant of deep operator networks (DeepONets), namely the
full-field Extended DeepONet (FExD), to serve as a spatial-temporal surrogate
that provides multi-output response predictions for dynamical systems. The
proposed FExD surrogate model effectively learns the full solution operator
across multiple degrees of freedom by enhancing the expressiveness of the
branch network and expanding the predictive capabilities of the trunk network.
The proposed FExD surrogate is deployed to simultaneously capture the dynamics
at several sensing locations along a testbed model of a cable-stayed bridge
subjected to stochastic ground motions. The ensuing response predictions from
the FExD are comprehensively compared against both a vanilla DeepONet and a
modified spatio-temporal Extended DeepONet. The results demonstrate the
proposed FExD can achieve both superior accuracy and computational efficiency,
representing a significant advancement in operator learning for structural
dynamics applications.

</details>


### [283] [Bayesian Optimization with Inexact Acquisition: Is Random Grid Search Sufficient?](https://arxiv.org/abs/2506.11831)
*Hwanwoo Kim,Chong Liu,Yuxin Chen*

Main category: stat.ML

TL;DR: 该论文研究了贝叶斯优化（BO）中获取函数非精确最大化对算法性能的影响，证明了在适当条件下，非精确BO仍能实现次线性累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中获取函数的精确最大化通常难以实现且计算昂贵，因此研究非精确最大化对算法性能的影响具有重要意义。

Method: 定义了获取函数解的不精确性度量，并分析了GP-UCB和GP-TS在非精确解下的累积遗憾界。

Result: 结果表明，在适当条件下，非精确BO算法仍能实现次线性累积遗憾。随机网格搜索被证明为一种高效的计算方法。

Conclusion: 非精确BO算法在理论和数值上均表现出有效性，随机网格搜索是一种实用的获取函数求解方法。

Abstract: Bayesian optimization (BO) is a widely used iterative algorithm for
optimizing black-box functions. Each iteration requires maximizing an
acquisition function, such as the upper confidence bound (UCB) or a sample path
from the Gaussian process (GP) posterior, as in Thompson sampling (TS).
However, finding an exact solution to these maximization problems is often
intractable and computationally expensive. Reflecting such realistic
situations, in this paper, we delve into the effect of inexact maximizers of
the acquisition functions. Defining a measure of inaccuracy in acquisition
solutions, we establish cumulative regret bounds for both GP-UCB and GP-TS
without requiring exact solutions of acquisition function maximization. Our
results show that under appropriate conditions on accumulated inaccuracy,
inexact BO algorithms can still achieve sublinear cumulative regret. Motivated
by such findings, we provide both theoretical justification and numerical
validation for random grid search as an effective and computationally efficient
acquisition function solver.

</details>


### [284] [Learning Overspecified Gaussian Mixtures Exponentially Fast with the EM Algorithm](https://arxiv.org/abs/2506.11850)
*Zhenisbek Assylbekov,Alan Legg,Artur Pak*

Main category: stat.ML

TL;DR: 研究了EM算法在过参数化高斯混合模型中的收敛性，证明了在特定结构下，算法能以指数速度收敛。


<details>
  <summary>Details</summary>
Motivation: 理解EM算法在过参数化模型中的行为，为高维聚类和密度估计提供理论支持。

Method: 利用负对数似然函数的强凸性和Polyak-Łojasiewicz不等式，分析收敛速度。

Result: 在特定配置下，EM算法能以指数速度收敛，并在有限样本中验证了理论结果。

Conclusion: 研究不仅深化了对EM算法的理解，还为实际应用提供了初始化策略和模型设计的指导。

Abstract: We investigate the convergence properties of the EM algorithm when applied to
overspecified Gaussian mixture models -- that is, when the number of components
in the fitted model exceeds that of the true underlying distribution. Focusing
on a structured configuration where the component means are positioned at the
vertices of a regular simplex and the mixture weights satisfy a non-degeneracy
condition, we demonstrate that the population EM algorithm converges
exponentially fast in terms of the Kullback-Leibler (KL) distance. Our analysis
leverages the strong convexity of the negative log-likelihood function in a
neighborhood around the optimum and utilizes the Polyak-{\L}ojasiewicz
inequality to establish that an $\epsilon$-accurate approximation is achievable
in $O(\log(1/\epsilon))$ iterations. Furthermore, we extend these results to a
finite-sample setting by deriving explicit statistical convergence guarantees.
Numerical experiments on synthetic datasets corroborate our theoretical
findings, highlighting the dramatic acceleration in convergence compared to
conventional sublinear rates. This work not only deepens the understanding of
EM's behavior in overspecified settings but also offers practical insights into
initialization strategies and model design for high-dimensional clustering and
density estimation tasks.

</details>


### [285] [How do Probabilistic Graphical Models and Graph Neural Networks Look at Network Data?](https://arxiv.org/abs/2506.11869)
*Michela Lapenna,Caterina De Bacco*

Main category: stat.ML

TL;DR: 比较概率图模型（PGMs）和图神经网络（GNNs）在图数据上的表现，发现PGMs在低维或噪声特征及高异质性图中更优。


<details>
  <summary>Details</summary>
Motivation: 探讨PGMs和GNNs在捕捉网络数据信息时的差异，尤其是在特征处理、噪声鲁棒性和异质性方面的表现。

Method: 通过链接预测任务进行实验，包括特征处理、噪声特征和高异质性图的测试。

Result: PGMs在低维或噪声特征及高异质性图中表现优于GNNs。

Conclusion: PGMs在某些实际场景中更具优势，尤其是在特征质量较差或图异质性较高时。

Abstract: Graphs are a powerful data structure for representing relational data and are
widely used to describe complex real-world systems. Probabilistic Graphical
Models (PGMs) and Graph Neural Networks (GNNs) can both leverage
graph-structured data, but their inherent functioning is different. The
question is how do they compare in capturing the information contained in
networked datasets? We address this objective by solving a link prediction task
and we conduct three main experiments, on both synthetic and real networks: one
focuses on how PGMs and GNNs handle input features, while the other two
investigate their robustness to noisy features and increasing heterophily of
the graph. PGMs do not necessarily require features on nodes, while GNNs cannot
exploit the network edges alone, and the choice of input features matters. We
find that GNNs are outperformed by PGMs when input features are low-dimensional
or noisy, mimicking many real scenarios where node attributes might be scalar
or noisy. Then, we find that PGMs are more robust than GNNs when the
heterophily of the graph is increased. Finally, to assess performance beyond
prediction tasks, we also compare the two frameworks in terms of their
computational complexity and interpretability.

</details>


### [286] [Spectral Estimation with Free Decompression](https://arxiv.org/abs/2506.11994)
*Siavash Ameli,Chris van der Heide,Liam Hodgkinson,Michael W. Mahoney*

Main category: stat.ML

TL;DR: 论文提出了一种基于自由概率理论的“自由解压缩”方法，用于估计无法直接操作的大矩阵的谱。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模增长，直接操作大矩阵变得不切实际，现有方法在矩阵无法直接访问时失效。

Method: 利用自由概率理论，从小子矩阵的谱推断大矩阵的谱。

Result: 方法在合成和真实数据集上有效，能够匹配已知的极限分布和完整谱。

Conclusion: 自由解压缩方法为解决无法直接操作的大矩阵谱估计问题提供了新途径。

Abstract: Computing eigenvalues of very large matrices is a critical task in many
machine learning applications, including the evaluation of log-determinants,
the trace of matrix functions, and other important metrics. As datasets
continue to grow in scale, the corresponding covariance and kernel matrices
become increasingly large, often reaching magnitudes that make their direct
formation impractical or impossible. Existing techniques typically rely on
matrix-vector products, which can provide efficient approximations, if the
matrix spectrum behaves well. However, in settings like distributed learning,
or when the matrix is defined only indirectly, access to the full data set can
be restricted to only very small sub-matrices of the original matrix. In these
cases, the matrix of nominal interest is not even available as an implicit
operator, meaning that even matrix-vector products may not be available. In
such settings, the matrix is "impalpable," in the sense that we have access to
only masked snapshots of it. We draw on principles from free probability theory
to introduce a novel method of "free decompression" to estimate the spectrum of
such matrices. Our method can be used to extrapolate from the empirical
spectral densities of small submatrices to infer the eigenspectrum of extremely
large (impalpable) matrices (that we cannot form or even evaluate with full
matrix-vector products). We demonstrate the effectiveness of this approach
through a series of examples, comparing its performance against known limiting
distributions from random matrix theory in synthetic settings, as well as
applying it to submatrices of real-world datasets, matching them with their
full empirical eigenspectra.

</details>
