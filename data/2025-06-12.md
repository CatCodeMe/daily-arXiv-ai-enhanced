<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 94]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.CL](#cs.CL) [Total: 10]
- [math.OC](#math.OC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.AI](#cs.AI) [Total: 8]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [stat.ML](#stat.ML) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Terabyte-Scale Analytics in the Blink of an Eye](https://arxiv.org/abs/2506.09226)
*Bowen Wu,Wei Cui,Carlo Curino,Matteo Interlandi,Rathijit Sen*

Main category: cs.DB

TL;DR: 论文探讨了在分布式GPU集群上扩展分析性SQL查询的性能潜力，目标是确定性能提升的上限。通过原型设计和实验，展示了至少60倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的普及和GPU集群的强大性能，研究如何利用这些资源优化分布式数据分析，探索性能提升的极限。

Method: 构建了一个原型系统，采用ML/HPC最佳实践（如跨设备数据移动的组通信原语），以最大化性能。

Result: 实验表明，性能提升至少60倍，系统能在极短时间内完成TPC-H 1TB规模的所有22个查询。

Conclusion: GPU集群为分布式数据分析提供了巨大的性能机会，社区应关注这一潜力。

Abstract: For the past two decades, the DB community has devoted substantial research
to take advantage of cheap clusters of machines for distributed data analytics
-- we believe that we are at the beginning of a paradigm shift. The scaling
laws and popularity of AI models lead to the deployment of incredibly powerful
GPU clusters in commercial data centers. Compared to CPU-only solutions, these
clusters deliver impressive improvements in per-node compute, memory bandwidth,
and inter-node interconnect performance. In this paper, we study the problem of
scaling analytical SQL queries on distributed clusters of GPUs, with the stated
goal of establishing an upper bound on the likely performance gains. To do so,
we build a prototype designed to maximize performance by leveraging ML/HPC best
practices, such as group communication primitives for cross-device data
movements. This allows us to conduct thorough performance experimentation to
point our community towards a massive performance opportunity of at least
60$\times$. To make these gains more relatable, before you can blink twice, our
system can run all 22 queries of TPC-H at a 1TB scale factor!

</details>


### [2] [ArcNeural: A Multi-Modal Database for the Gen-AI Era](https://arxiv.org/abs/2506.09467)
*Wu Min,Qiao Yuncong,Yu Tan,Chenghu Yang*

Main category: cs.DB

TL;DR: ArcNeural是一个多模态数据库，专为生成式AI和大语言模型设计，支持图形、向量和文档等多种数据类型，具有存储与计算分离的架构。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据处理中的挑战，为生成式AI时代提供智能、数据驱动的解决方案。

Method: 采用存储与计算分离的架构，结合图形技术、高级向量索引和事务处理，支持实时分析和AI应用。

Result: 实验评估显示ArcNeural在性能和扩展性上优于现有系统。

Conclusion: ArcNeural为结构化与非结构化数据管理提供了通用解决方案，适用于企业级AI应用。

Abstract: ArcNeural introduces a novel multimodal database tailored for the demands of
Generative AI and Large Language Models, enabling efficient management of
diverse data types such as graphs, vectors, and documents. Its storage-compute
separated architecture integrates graph technology, advanced vector indexing,
and transaction processing to support real-time analytics and AI-driven
applications. Key features include a unified storage layer, adaptive edge
collection in MemEngine, and seamless integration of transaction and analytical
processing. Experimental evaluations demonstrate ArcNeural's superior
performance and scalability compared to state-of-the-art systems. This system
bridges structured and unstructured data management, offering a versatile
solution for enterprise-grade AI applications.
  ArcNeural's design addresses the challenges of multimodal data processing,
providing a robust framework for intelligent, data-driven solutions in the Gen
AI era.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: EdgeProfiler是一个快速分析框架，用于评估边缘系统上的轻量级大语言模型（LLMs），通过量化技术和内存约束优化性能。


<details>
  <summary>Details</summary>
Motivation: LLMs的高计算、内存和功耗需求限制了其在边缘环境中的应用，EdgeProfiler旨在解决这一问题。

Method: 框架采用量化技术和内存约束分析轻量级LLMs，包括TinyLLaMA等模型，并通过分析模型估计延迟、FLOPs和能耗。

Result: 4位量化减少内存使用60-70%，精度损失2-5%，推理速度提升2-3倍，能耗降低35-50%。

Conclusion: EdgeProfiler为轻量级LLMs在边缘环境中的高效部署提供了平衡精度、能耗和计算可行性的解决方案。

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [4] [Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism](https://arxiv.org/abs/2506.09242)
*Jonas Latt,Christophe Coreixas*

Main category: cs.DC

TL;DR: 本文介绍了Palabos库的GPU移植版本，采用混合CPU-GPU执行模型，通过现代C++技术实现硬件无关的计算内核，性能接近原生CUDA求解器。


<details>
  <summary>Details</summary>
Motivation: 将Palabos库从CPU移植到GPU，以提升性能并保持代码兼容性。

Method: 采用混合CPU-GPU执行模型，结合面向对象（CPU）和数据导向（GPU）的架构，利用现代C++技术生成硬件无关的计算内核。

Result: 在三维多物理场基准测试中，单GPU性能接近原生CUDA求解器，多GPU测试表现出良好的弱扩展和强扩展性。

Conclusion: GPU移植的Palabos库在保持高性能的同时，实现了代码的渐进式移植和用户自定义组件的开发。

Abstract: This article presents the principles, software architecture, and performance
analysis of the GPU port of the lattice Boltzmann software library Palabos (J.
Latt et al., "Palabos: Parallel lattice Boltzmann solver", Comput. Math. Appl.
81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which
numerical components are selectively assigned to either the CPU or the GPU,
depending on considerations of performance or convenience. This design enables
a progressive porting strategy, allowing most features of the original
CPU-based codebase to be gradually and seamlessly adapted to GPU execution. The
new architecture builds upon two complementary paradigms: a classical
object-oriented structure for CPU execution, and a data-oriented counterpart
for GPUs, which reproduces the modularity of the original code while
eliminating object-oriented overhead detrimental to GPU performance. Central to
this approach is the use of modern C++, including standard parallel algorithms
and template metaprogramming techniques, which permit the generation of
hardware-agnostic computational kernels. This facilitates the development of
user-defined, GPU-accelerated components such as collision operators or
boundary conditions, while preserving compatibility with the existing codebase
and avoiding the need for external libraries or non-standard language
extensions. The correctness and performance of the GPU-enabled Palabos are
demonstrated through a series of three-dimensional multiphysics benchmarks,
including the laminar-turbulent transition in a Taylor-Green vortex, lid-driven
cavity flow, and pore-scale flow in Berea sandstone. Despite the high-level
abstraction of the implementation, the single-GPU performance is similar to
CUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling
across all test cases.

</details>


### [5] [A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO](https://arxiv.org/abs/2506.09275)
*Jonas Svedas,Hannah Watson,Nathan Laubeuf,Diksha Moolchandani,Abubakr Nada,Arjun Singh,Dwaipayan Biswas,James Myers,Debjyoti Bhattacharjee*

Main category: cs.DC

TL;DR: 本文综述了分布式深度神经网络训练模拟器的现状，重点探讨了工作负载表示、模拟基础设施和总拥有成本（TCO）模型三个维度，并总结了现有工具的局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂度的快速增长，分布式DNN训练的系统设计面临可持续性和效率的挑战，模拟器成为解决这一问题的关键工具。

Method: 通过分析工作负载抽象、模拟基础设施和TCO模型，综述了现有模拟框架及其能力、假设和关注点。

Result: 提供了对分布式训练系统设计和评估的结构化概述，总结了现有工具的局限性和未来研究方向。

Conclusion: 本文为分布式训练系统的设计和评估提供了决策支持，并指出了未来的研究挑战。

Abstract: Distributed deep neural networks (DNNs) have become a cornerstone for scaling
machine learning to meet the demands of increasingly complex applications.
However, the rapid growth in model complexity far outpaces CMOS technology
scaling, making sustainable and efficient system design a critical challenge.
Addressing this requires coordinated co-design across software, hardware, and
technology layers. Due to the prohibitive cost and complexity of deploying
full-scale training systems, simulators play a pivotal role in enabling this
design exploration. This survey reviews the landscape of distributed DNN
training simulators, focusing on three major dimensions: workload
representation, simulation infrastructure, and models for total cost of
ownership (TCO) including carbon emissions. It covers how workloads are
abstracted and used in simulation, outlines common workload representation
methods, and includes comprehensive comparison tables covering both simulation
frameworks and TCO/emissions models, detailing their capabilities, assumptions,
and areas of focus. In addition to synthesizing existing tools, the survey
highlights emerging trends, common limitations, and open research challenges
across the stack. By providing a structured overview, this work supports
informed decision-making in the design and evaluation of distributed training
systems.

</details>


### [6] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: TTrace是一个用于检测和定位分布式训练中静默错误的系统，通过对比单设备参考实现和分布式训练的中间张量，有效识别错误。


<details>
  <summary>Details</summary>
Motivation: 分布式训练中的静默错误难以检测和定位，传统调试方法效率低下，尤其是在低精度训练中。

Method: TTrace收集分布式训练的中间张量，与单设备参考实现对比，并提出数学分析以区分错误和浮点舍入误差。

Result: 实验证明TTrace在Megatron-LM框架中检测到11个现有错误和3个新错误，且代码改动少。

Conclusion: TTrace在多种训练场景（包括低精度训练）中表现高效，是解决分布式训练静默错误的有效工具。

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [7] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: ScalableHD是一种针对多核CPU的高通量超维计算推理方法，采用两阶段流水线执行模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统HDC方法在CPU上的推理效率未得到充分研究，ScalableHD填补了这一空白。

Method: 采用两阶段流水线并行化处理，结合内存分块和NUMA感知绑定，针对不同批量大小优化执行。

Result: 在多种任务中实现10倍吞吐量提升，且扩展性良好。

Conclusion: ScalableHD为多核CPU上的高效HDC推理提供了可行方案。

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


### [8] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: 论文提出SLED方法，利用推测解码技术优化边缘计算中的大语言模型推理，通过异构设备协同计算降低延迟和能耗，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 边缘设备内存和功耗限制导致高效推理大语言模型具有挑战性，现有方法需在精度和效率间权衡。

Method: SLED方法利用轻量级边缘设备本地生成候选令牌，边缘服务器批量验证，减少内存占用并支持设备异构性。

Result: 实验表明，SLED显著降低延迟、提高能效并支持更多并发推理会话，且不影响模型精度。

Conclusion: SLED为边缘计算中的大语言模型推理提供了高效且精确的解决方案。

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [9] [Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP](https://arxiv.org/abs/2506.09463)
*Soumyajit Chatterjee,Rahul Utkoor,Uppu Eshwar,Sathya Peri,V. Krishna Nandivada*

Main category: cs.DC

TL;DR: 论文提出了一种新颖的任务调度技术，采用双队列方法高效执行QR分解内核，解决了SLSQP算法中中间结果存储的挑战，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 在并行编程中，任务调度对多核架构至关重要。SLSQP算法中的QR分解需要存储中间结果，而现有DAG方法无法满足这一需求。

Method: 采用双队列调度技术，结合高级C++实现，支持编译器优化并存储中间结果。

Result: 实验评估显示性能显著提升，比SLSQP算法的顺序QR版本快10倍。

Conclusion: 提出的方法有效解决了QR分解中间结果存储问题，显著提升了SLSQP算法的性能。

Abstract: Efficient task scheduling is paramount in parallel programming on multi-core
architectures, where tasks are fundamental computational units. QR
factorization is a critical sub-routine in Sequential Least Squares Quadratic
Programming (SLSQP) for solving non-linear programming (NLP) problems. QR
factorization decomposes a matrix into an orthogonal matrix Q and an upper
triangular matrix R, which are essential for solving systems of linear
equations arising from optimization problems. SLSQP uses an in-place version of
QR factorization, which requires storing intermediate results for the next
steps of the algorithm. Although DAG-based approaches for QR factorization are
prevalent in the literature, they often lack control over the intermediate
kernel results, providing only the final output matrices Q and R. This
limitation is particularly challenging in SLSQP, where intermediate results of
QR factorization are crucial for back-substitution logic at each iteration. Our
work introduces novel scheduling techniques using a two-queue approach to
execute the QR factorization kernel effectively. This approach, implemented in
high-level C++ programming language, facilitates compiler optimizations and
allows storing intermediate results required by back-substitution logic.
Empirical evaluations demonstrate substantial performance gains, including a
10x improvement over the sequential QR version of the SLSQP algorithm.

</details>


### [10] [On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems](https://arxiv.org/abs/2506.09505)
*Dumitrel Loghin,Shuang Liang,Shengwei Liu,Xiong Liu,Pingcheng Ruan,Zhigang Ye*

Main category: cs.DC

TL;DR: 论文分析了ARM服务器在零知识证明（ZKP）生成中的性能表现，发现当前ARM CPU（如AWS Graviton4和GCP Axion）在构建Merkle树时比x86-64服务器慢1.4-1.6倍，主要原因是向量大小和时钟频率较低。但ARM SVE/SVE2 ISA具有潜力，若向量大小提升至512位，性能可能超越x86-64。


<details>
  <summary>Details</summary>
Motivation: 探索ARM服务器是否能在ZKP生成中取代x86-64服务器，以利用其成本优势。

Method: 比较ARM（AWS Graviton4和GCP Axion）与x86-64（AMD EPYC和Intel Xeon）服务器在构建Merkle树时的性能，分析原因。

Result: 当前ARM CPU性能较低（慢1.4-1.6倍），主要受限于较小的向量大小（128位）和较低的时钟频率。

Conclusion: ARM SVE/SVE2 ISA具有潜力，若向量大小提升至512位，可能超越x86-64服务器，同时保持成本优势。

Abstract: Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling
blockchains and bringing Web3 to life. At the same time, ZKP for transactions
running on the Ethereum Virtual Machine require powerful servers with hundreds
of CPU cores. The current zkProver implementation from Polygon is optimized for
x86-64 CPUs by vectorizing key operations, such as Merkle tree building with
Poseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX
and AVX512). With these optimizations, a ZKP for a batch of transactions is
generated in less than two minutes. With the advent of cloud servers with ARM
which are at least 10% cheaper than x86-64 servers and the implementation of
ARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over
their x86-64 counterparts. Unfortunately, our analysis shows that current ARM
CPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web
Services (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X
slower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX
and AVX512, respectively, when building a Merkle tree with over four million
leaves. This low performance is due to (1) smaller vector size in these ARM
CPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the
other hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as
powerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing
the vector size to 512 bits will enable higher performance in ARM CPUs compared
to their x86-64 counterparts while maintaining their price advantage.

</details>


### [11] [Understanding the Performance and Power of LLM Inferencing on Edge Accelerators](https://arxiv.org/abs/2506.09554)
*Mayank Arya,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文评估了在NVIDIA Jetson Orin AGX边缘加速器上运行大型语言模型（LLM）的可行性，研究了不同参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 由于云数据中心无法满足关键任务和隐私敏感应用的需求，本地托管开源LLM成为必要，但边缘设备的性能尚未充分探索。

Method: 在NVIDIA Jetson Orin AGX上测试了四种模型（2.7B至32.8B参数），分析了批次大小、序列长度、量化级别及电源模式对延迟、吞吐量、困惑度和能耗的影响。

Result: 研究发现，序列长度增加会降低吞吐量，量化会使小模型变慢，电源模式对能耗有显著影响。

Conclusion: 研究结果为边缘设备上优化LLM服务提供了实用指导，平衡了效率、速度和资源使用。

Abstract: Large Language Models (LLMs) have demonstrated exceptional benefits to a wide
range of domains, for tasks as diverse as code generation and robot navigation.
While LLMs are usually served from cloud data centers, mission-critical and
privacy-sensitive applications may require local hosting of open LLM models.
Given the large GPU memory footprint needed for LLMs, edge accelerators such as
Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.
However, the feasibility and performance of LLM inference on edge accelerators
is under-explored. This study presents a detailed evaluation of LLM inference
on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B
parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We
investigate the impact of varying batch sizes, sequence lengths, and
quantization levels on latency, throughput, and perplexity, and also explore
various custom power modes on the Orin AGX to perform power and energy
consumption analysis. Our findings offer interesting insights on the trade-offs
between efficiency, inference speed and resource use, e.g., increasing the
sequence length causes a decrease in token throughput and quantization causes
smaller LLMs to be slower. These results can help optimize LLM serving on edge
accelerators for practical applications.

</details>


### [12] [Frosty for partial synchrony](https://arxiv.org/abs/2506.09823)
*Stephen Buttolph,Andrew Lewis-Pye,Kevin Sekniqi*

Main category: cs.DC

TL;DR: 本文探讨如何将Frosty模块修改为适用于部分同步版本的Snowman共识协议。


<details>
  <summary>Details</summary>
Motivation: Frosty模块假设强同步性，而Snowman已修改为部分同步一致性，因此需要调整Frosty以适应部分同步环境。

Method: 修改Frosty模块，使其与部分同步版本的Snowman兼容。

Result: 提出了适用于部分同步Snowman的Frosty改进版本。

Conclusion: 成功展示了如何调整Frosty以支持部分同步Snowman，增强了协议的适应性。

Abstract: Snowman is the consensus protocol used by blockchains on Avalanche. Recent
work has shown both how to augment Snowman with a `liveness' module called
`Frosty' that protects against liveness attacks, and also how to modify Snowman
so as to be consistent in partial synchrony. Since Frosty assumes (a strong
form of) synchrony, the aim of this note is to show how to modify Frosty to
deal with the partially synchronous version of Snowman.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [13] [Tight Paths and Tight Pairs in Weighted Directed Graphs](https://arxiv.org/abs/2506.09966)
*José Luis Balcázar*

Main category: cs.DS

TL;DR: 论文研究了在有向边加权图中寻找紧路径及其简化问题（紧对），这些问题源于数据分析中寻找闭包空间中的基本前驱需求。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决数据分析中闭包空间的基本前驱问题，需要高效的算法支持。

Method: 讨论并比较了多种算法来解决紧路径和紧对问题。

Result: 提出了针对这些问题的多种算法，并进行了比较分析。

Conclusion: 论文总结了不同算法的适用性，为数据分析中的闭包空间问题提供了解决方案。

Abstract: We state the graph-theoretic computational problem of finding tight paths in
a directed, edge-weighted graph, as well as its simplification of finding tight
pairs. These problems are motivated by the need of algorithms that find
so-called basic antecedents in closure spaces, in one specific approach to data
analysis. We discuss and compare several algorithms to approach these problems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [14] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 论文提出UTGenerator和UTBoost框架，通过LLM生成测试用例，解决SWE-Bench中测试用例不足的问题，并发现大量错误补丁。


<details>
  <summary>Details</summary>
Motivation: SWE-Bench中手动编写的测试用例不足，导致生成的补丁可能通过测试但未解决问题。

Method: 引入UTGenerator（基于LLM的测试用例生成器）和UTBoost（测试用例增强框架），自动分析代码库生成测试用例。

Result: 在评估中发现36个任务实例测试用例不足，并识别出345个错误补丁。这些修正影响了SWE-Bench Lite和Verified排行榜的40.9%和24.4%条目，分别导致18和11个排名变化。

Conclusion: UTGenerator和UTBoost能有效提升测试用例质量，减少错误补丁的误判。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [15] [Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study](https://arxiv.org/abs/2506.09370)
*Rohit Mehra,Priyavanshi Pathania,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 论文探讨了通过自动化工具优化软件代码以减少碳排放，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和元宇宙技术的发展，软件系统的广泛使用导致碳排放增加，亟需从可持续性角度优化软件。

Method: 采用自动化工具识别代码中的能源低效模式，并通过重构优化。

Result: 案例研究表明，重构后每用户每月能耗降低了29%。

Conclusion: 自动化工具在提升软件可持续性方面具有显著潜力。

Abstract: Advances in technologies like artificial intelligence and metaverse have led
to a proliferation of software systems in business and everyday life. With this
widespread penetration, the carbon emissions of software are rapidly growing as
well, thereby negatively impacting the long-term sustainability of our
environment. Hence, optimizing software from a sustainability standpoint
becomes more crucial than ever. We believe that the adoption of automated tools
that can identify energy-inefficient patterns in the code and guide appropriate
refactoring can significantly assist in this optimization. In this extended
abstract, we present an industry case study that evaluates the sustainability
impact of refactoring energy-inefficient code patterns identified by automated
software sustainability assessment tools for a large application. Preliminary
results highlight a positive impact on the application's sustainability
post-refactoring, leading to a 29% decrease in per-user per-month energy
consumption.

</details>


### [16] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: 提出将推理深度作为可控资源，优化代码生成模型的设计，以平衡快速直接答案与详细推理之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统代码生成模型未明确管理推理深度，导致在准确性、延迟和成本之间难以取得最佳平衡。

Method: 通过自适应控制推理深度，从合成数据创建到实际部署，优化模型的整个生命周期。

Result: 该方法可以提升监督信号、推动多维度基准测试，并支持成本和安全意识的部署策略。

Conclusion: 将快速与慢速推理视为互补模式，可实现代码生成模型在必要时深入思考，在可能时快速行动。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [17] [Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries](https://arxiv.org/abs/2506.09550)
*Fanpeng Yang,Xu Ma,Shuling Wang,Xiong Xu,Qinxiang Cao,Naijun Zhan,Xiaofeng Li,Bin Gu*

Main category: cs.SE

TL;DR: 提出了一种结合符号执行、大语言模型和形式验证的新框架，用于生成C程序的函数摘要，支持多种抽象级别。


<details>
  <summary>Details</summary>
Motivation: 任务关键型遗留代码缺乏形式化规范，且C程序的复杂特性（如循环、指针别名等）使得自动生成函数摘要具有挑战性。

Method: 结合VST-A的符号执行、大语言模型推断循环不变量，以及Frama-C的形式验证，生成相对最强后条件（RSPs）并构建函数摘要。

Result: 通过实验验证，该方法能够生成完全捕获程序行为的函数摘要，并支持多种抽象级别。

Conclusion: 提出的框架有效解决了C程序函数摘要生成的挑战，适用于形式验证和人类理解的需求。

Abstract: Function summaries, which characterize the behavior of code segments
(typically functions) through preconditions and postconditions, are essential
for understanding, reusing, and verifying software, particularly in
safety-critical domains like aerospace embedded systems. However, these
mission-critical legacy code serving as a valuable reused asset often lacks
formal specifications. It is challenging to automatically generate function
summaries for C programs, due to the existence of complex features such as
loops, nested function calls, pointer aliasing, and so on. Moreover, function
summaries should support multiple abstraction levels to meet diverse
requirements, e.g. precise summaries capturing full functionality for formal
verification and intuitive summaries for human understanding.
  To address these challenges, we first propose a novel framework that combines
symbolic execution, large language models (LLMs), and formal verification to
generate Relatively Strongest Postconditions (RSPs) and build function
summaries that fully capture program behavior. Our approach leverages VST-A's
symbolic execution to precisely track program execution paths and state
transitions, employs LLMs to infer loop invariants based on predefined
templates, and uses Frama-C to guarantee soundness of generated summaries in an
iterative refinement loop. Furthermore, from generated RSPs, we automatically
synthesize strongest non-redundant postconditions expressed within given domain
specific language. We compare our approach with existing work through extensive
experiments.

</details>


### [18] [ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs](https://arxiv.org/abs/2506.09601)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Tao Xiao,Yasutaka Kamei*

Main category: cs.SE

TL;DR: ASTAGEN利用大型语言模型（LLM）自动生成SATD分类法，通过解释驱动的迭代设计，提高了分类一致性，并在短时间内低成本完成。


<details>
  <summary>Details</summary>
Motivation: 传统手动构建SATD分类法耗时、费力且不一致，ASTAGEN旨在通过自动化解决这一问题。

Method: ASTAGEN通过生成SATD评论的简明解释，并迭代更新分类法，利用LLM实现自动化。

Result: ASTAGEN在三个领域的SATD数据集中成功恢复已知分类，如机器学习中的“层配置”，且成本低、时间短。

Conclusion: ASTAGEN支持半自动分类法构建，为未来其他领域的自动分类法生成提供了可能。

Abstract: Technical debt refers to suboptimal code that degrades software quality. When
developers intentionally introduce such debt, it is called self-admitted
technical debt (SATD). Since SATD hinders maintenance, identifying its
categories is key to uncovering quality issues. Traditionally, constructing
such taxonomies requires manually inspecting SATD comments and surrounding
code, which is time-consuming, labor-intensive, and often inconsistent due to
annotator subjectivity. This study presents ASTAGEN, an initial step toward
automating SATD taxonomy generation using large language models (LLMs). Given a
comment and its surrounding code, ASTAGEN first generates a concise explanation
for each SATD comment, then incrementally generates and updates categories to
construct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:
quantum software, smart contracts, and machine learning. It successfully
recovers domain-specific categories reported in prior work, such as Layer
Configuration in machine learning. Compared to a naive use of an LLM, ASTAGEN
produces more consistent category assignments due to its explanation-driven,
iterative design. It also completes taxonomy generation in under two hours and
for less than one USD, even on the largest dataset. These results suggest that
while full automation remains challenging, ASTAGEN is able to support
semi-automated taxonomy construction. Furthermore, our work opens up avenues
for future work, such as automatic taxonomy generation in other areas.

</details>


### [19] [Translating a VDM Model of a Medical Device into Kapture](https://arxiv.org/abs/2506.09636)
*Joe Hare,Leo Freitas,Ken Pierce*

Main category: cs.SE

TL;DR: 论文探讨了使用Kapture工具将VDM模型转换为Kapture模型的效果，评估了工具可用性和挑战，结果表明Kapture适合初学者建模复杂系统。


<details>
  <summary>Details</summary>
Motivation: 随着医疗设备复杂度的增加，需要清晰可验证的软件需求，研究探索了Kapture工具在建模中的适用性。

Method: 使用Kapture工具将现有的VDM模型（CANDO医疗植入物）转换为Kapture模型，评估工具可用性和建模挑战。

Result: Kapture模型覆盖了90%以上的原始VDM模型，并生成匹配的结果轨迹。

Conclusion: Kapture适合初学者建模复杂系统，但VDM到Kapture的转换存在一定困难。

Abstract: As the complexity of safety-critical medical devices increases, so does the
need for clear, verifiable, software requirements. This paper explores the use
of Kapture, a formal modelling tool developed by D-RisQ, to translate an
existing formal VDM model of a medical implant for treating focal epilepsy
called CANDO. The work was undertaken without prior experience in formal
methods. The paper assess Kapture's usability, the challenges of formal
modelling, and the effectiveness of the translated model. The result is a model
in Kapture which covers over 90% of the original VDM model, and produces
matching traces of results. While several issues were encountered during design
and implementation, mainly due to the initial learning curve, this paper
demonstrates that complex systems can be effectively modelled in Kapture by
inexperienced users and highlights some difficulties in translating VDM
specifications to Kapture.

</details>


### [20] [Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead](https://arxiv.org/abs/2506.09683)
*Priyavanshi Pathania,Nikhil Bamby,Rohit Mehra,Samarth Sikand,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文综述了测量软件和AI相关能源及碳排放的方法与工具，提出分类法并比较现有工具，强调社区合作以应对挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件和AI的普及，其能源和碳排放问题日益突出，亟需优化其对环境的影响。

Method: 提出分类法（监测、估算、黑箱方法），比较工具的多维度和组件覆盖范围。

Result: 总结了现有工具的优缺点及挑战，提出组件整合的实践观察。

Conclusion: 呼吁社区合作解决当前挑战，推动可持续发展。

Abstract: The proliferation of software and AI comes with a hidden risk: its growing
energy and carbon footprint. As concerns regarding environmental sustainability
come to the forefront, understanding and optimizing how software impacts the
environment becomes paramount. In this paper, we present a state-of-the-art
review of methods and tools that enable the measurement of software and
AI-related energy and/or carbon emissions. We introduce a taxonomy to
categorize the existing work as Monitoring, Estimation, or Black-Box
approaches. We delve deeper into the tools and compare them across different
dimensions and granularity - for example, whether their measurement encompasses
energy and carbon emissions and the components considered (like CPU, GPU, RAM,
etc.). We present our observations on the practical use (component wise
consolidation of approaches) as well as the challenges that we have identified
across the current state-of-the-art. As we start an initiative to address these
challenges, we emphasize active collaboration across the community in this
important field.

</details>


### [21] [Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies](https://arxiv.org/abs/2506.09938)
*Aaditaa Vashisht,Rekha B S*

Main category: cs.SE

TL;DR: 本文综述了现代事件驱动和微服务架构（如Apache Kafka、Spring Boot、MongoDB和Kubernetes）在零售和金融系统中的关键作用，强调了它们在实时分析、欺诈检测和高可用性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型的加速，零售行业亟需实时、可扩展且弹性的系统来管理金融交易、分析客户行为和优化订单处理。

Method: 通过系统回顾近年来的学术出版物、技术白皮书和行业报告，本研究综合了关键主题和实施策略。

Result: 分析表明，Kafka和Spring Boot支持低延迟的事件驱动应用，MongoDB结合Kubernetes确保库存和交易系统的容错性和高可用性。

Conclusion: 这些发现为行业从业者设计可扩展基础设施、研究混合部署模型提供了宝贵见解，并为教育工作者整合现代系统架构奠定了基础。

Abstract: With the rapid pace of digital transformation, the retail industry is
increasingly depending on real-time, scalable, and resilient systems to manage
financial transactions, analyze customer behavior, and streamline order
processing. This literature review explores how modern event-driven and
microservices-based architectures, particularly those leveraging Apache Kafka,
Spring Boot, MongoDB, and Kubernetes are transforming retail and financial
systems. By systematically reviewing academic publications, technical white
papers, and industry reports from recent years, this study synthesizes key
themes and implementation strategies. The analysis reveals that technologies
like Kafka and Spring Boot are instrumental in building low-latency,
event-driven applications that support real-time analytics and fraud detection,
while MongoDB, when deployed on Kubernetes, ensures fault tolerance and high
availability in inventory and transaction systems. Kubernetes itself plays a
crucial role in automating deployment and scaling of microservices. These
findings provide valuable insights for industry practitioners aiming to design
scalable infrastructures, identify research opportunities in hybrid deployment
models, and offer educators a foundation to integrate modern system
architectures into professional and technical communication training.

</details>


### [22] [Mapping NVD Records to Their VFCs: How Hard is it?](https://arxiv.org/abs/2506.09702)
*Huu Hung Nguyen,Duc Manh Tran,Yiran Cheng,Thanh Le-Cong,Hong Jin Kang,Ratnadira Widyasari,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 该研究探讨了将NVD记录映射到漏洞修复提交（VFCs）的可行性，发现Git引用成功率较高，但仍有大量记录无法映射。


<details>
  <summary>Details</summary>
Motivation: NVD记录与漏洞修复提交之间的显式链接稀疏，导致漏洞分析困难，研究旨在探索映射的可行性。

Method: 通过手动分析NVD引用，发现Git引用成功率较高，随后构建自动化管道，并挖掘外部安全数据库和GitHub仓库以填补空白。

Result: 成功映射了11.3%的NVD记录，但88.7%的记录仍无法映射，Git引用表现最佳。

Conclusion: 研究为漏洞数据集增强和未来自动化安全研究提供了见解，但无Git链接的记录映射仍具挑战性。

Abstract: Mapping National Vulnerability Database (NVD) records to vulnerability-fixing
commits (VFCs) is crucial for vulnerability analysis but challenging due to
sparse explicit links in NVD references.This study explores this mapping's
feasibility through an empirical approach. Manual analysis of NVD references
showed Git references enable over 86% success, while non-Git references achieve
under 14%. Using these findings, we built an automated pipeline extracting
31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,
mainly from Git references. To fill gaps, we mined six external security
databases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,
and GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%
precision. Combining these, we mapped 26,710 unique records (11.3% coverage)
from 7,634 projects, with overlap between NVD and external databases, plus
unique GitHub contributions. Despite success with Git references, 88.7% of
records remain unmapped, highlighting the difficulty without Git links. This
study offers insights for enhancing vulnerability datasets and guiding future
automated security research.

</details>


### [23] [A First Look at Bugs in LLM Inference Engines](https://arxiv.org/abs/2506.09713)
*Mugeng Liu,Siqi Zhong,Weichen Bi,Yixuan Zhang,Zhiyang Chen,Zhenpeng Chen,Xuanzhe Liu,Yun Ma*

Main category: cs.SE

TL;DR: 该论文首次对大型语言模型推理引擎中的错误进行了实证研究，分析了929个真实错误，揭示了症状、根本原因及共性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推理引擎在现代AI基础设施中至关重要，但由于资源需求和跨平台复杂性，它们容易出错，目前缺乏系统性理解。

Method: 通过挖掘5个广泛采用的LLM推理引擎的官方仓库，构建了929个真实错误的全面数据集，并通过开放编码分析症状和根本原因。

Result: 研究发现6种主要错误症状和28种根本原因分类，揭示了错误检测和定位的关键挑战。

Conclusion: 基于研究结果，为研究人员、引擎供应商和应用开发者提出了一系列可操作的改进建议。

Abstract: Large language model-specific inference engines (in short as \emph{LLM
inference engines}) have become a fundamental component of modern AI
infrastructure, enabling the deployment of LLM-powered applications (LLM apps)
across cloud and local devices. Despite their critical role, LLM inference
engines are prone to bugs due to the immense resource demands of LLMs and the
complexities of cross-platform compatibility. However, a systematic
understanding of these bugs remains lacking. To bridge this gap, we present the
first empirical study on bugs in LLM inference engines. We mine official
repositories of 5 widely adopted LLM inference engines, constructing a
comprehensive dataset of 929 real-world bugs. Through a rigorous open coding
process, we analyze these bugs to uncover their symptoms, root causes, and
commonality. Our findings reveal six major bug symptoms and a taxonomy of 28
root causes, shedding light on the key challenges in bug detection and location
within LLM inference engines. Based on these insights, we propose a series of
actionable implications for researchers, inference engine vendors, and LLM app
developers.

</details>


### [24] [Towards Bridging Formal Methods and Human Interpretability](https://arxiv.org/abs/2506.09759)
*Abhijit Paul,Proma Chowdhury,Kazi Sakib*

Main category: cs.SE

TL;DR: 研究探讨了人类对标记转移系统（LTS）设计的理解，提出了7个关键指标，并通过实验验证了其中4个指标最能反映人类理解。


<details>
  <summary>Details</summary>
Motivation: 尽管LTS在模型检查和设计修复中至关重要，但此前缺乏对人类理解LTS设计的研究。

Method: 结合软件工程和图论，提出7个指标，创建148个LTS设计数据集，通过Bradley-Terry模型和Kendall's Tau分析验证。

Result: Albin复杂度、状态空间大小、圈复杂度和冗余度最能反映人类理解。应用Albin复杂度后，理解时间减少39%。

Conclusion: 强调人类因素的指标可提升形式化设计的可解释性。

Abstract: Labeled Transition Systems (LTS) are integral to model checking and design
repair tools. System engineers frequently examine LTS designs during model
checking or design repair to debug, identify inconsistencies, and validate
system behavior. Despite LTS's significance, no prior research has examined
human comprehension of these designs. To address this, we draw on traditional
software engineering and graph theory, identifying 7 key metrics: cyclomatic
complexity, state space size, average branching factor, maximum depth, Albin
complexity, modularity, and redundancy. We created a dataset of 148 LTS
designs, sampling 48 for 324 paired comparisons, and ranked them using the
Bradley-Terry model. Through Kendall's Tau correlation analysis, we found that
Albin complexity ($\tau = 0.444$), state space size ($\tau = 0.420$),
cyclomatic complexity ($\tau = 0.366$), and redundancy ($\tau = 0.315$) most
accurately reflect human comprehension of LTS designs. To showcase the metrics'
utility, we applied the Albin complexity metric within the Fortis design repair
tool, ranking system redesigns. This ranking reduced annotators' comprehension
time by 39\%, suggesting that metrics emphasizing human factors can enhance
formal design interpretability.

</details>


### [25] [variability.dev: Towards an Online Toolbox for Feature Modeling](https://arxiv.org/abs/2506.09845)
*Tobias Heß,Lukas Ostheimer,Tobias Betz,Simon Karrer,Tim Jannik Schmidt,Pierre Coquet,Sean Semmler,Thomas Thüm*

Main category: cs.SE

TL;DR: 本文介绍了正在开发的在线工具箱variability.dev，用于特征建模，重点展示了其协作式特征模型编辑器和在线配置器。


<details>
  <summary>Details</summary>
Motivation: 当前在线特征模型编辑器功能有限、维护不足或需要离线安装，因此需要开发一个功能完善的在线工具。

Method: 基于FeatureIDE库开发协作式特征模型编辑器和在线配置器。

Result: 展示了variability.dev工具箱的预览版，支持协作编辑和在线配置。

Conclusion: variability.dev有望填补当前在线特征建模工具的空白，提供更便捷的协作和配置功能。

Abstract: The emergence of feature models as the default to model the variability in
configurable systems fosters a rich diversity in applications, application
domains, and perspectives. Independent of their domain, modelers require to
open, view, edit, transform, save, and configure models as well as to
collaborate with others. However, at the time of writing, the top five results
when googling ``Online Editor Feature Model'' point to editors that either have
minimal functionality, are unmaintained or defunct, or require an offline
installation, such as FeatureIDE. In this work we present a preview of our
in-development online toolbox for feature modeling, variability.dev. In
particular, we showcase our collaborative feature-model editor and our online
configurator both of which are built on top of the FeatureIDE library.

</details>


### [26] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: 研究发现，现有的利益相关者参与（SHI）实践主要受商业目标驱动，与负责任AI（rAI）的目标存在脱节，需通过干预措施和研究机会加以改进。


<details>
  <summary>Details</summary>
Motivation: 探讨SHI在商业AI开发中的实践与rAI目标之间的差异，以指导未来的干预措施。

Method: 分析56份rAI指南，进行在线调查（n=130）和半结构化访谈（n=10）。

Result: SHI实践主要服务于商业目标（如客户价值、合规），与rAI目标（如权力再分配、风险预测）脱节。

Conclusion: 需提出干预措施和研究机会，以推动SHI实践与rAI目标的对齐。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


### [27] [Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation](https://arxiv.org/abs/2506.09929)
*Scott Schnelle,Francesca Favaro,Laura Fraade-Blanar,David Wichner,Holland Broce,Justin Miranda*

Main category: cs.SE

TL;DR: 本文提出了一种评估自动驾驶系统（ADS）安全案例可信度的方法，重点关注案例中每个主张的支持程度和证据状态。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，确保安全和公众信任需要强有力的保障框架，安全案例成为关键工具。本文旨在评估安全案例的可信度，以支持其整体有效性。

Method: 通过分析安全案例的构建模块（主张、证据等），评估每个主张的程序支持和实施支持，并对证据状态进行独立评估。提供了评分策略和评估指南。

Result: 提出了一个评估框架，包括详细的评分表和评估指南，用于判断安全案例的可信度，并讨论了治理、持续改进和时间安排。

Conclusion: 该方法为安全案例的可信度评估提供了起点，有助于自动驾驶技术的安全整合和社会接受。

Abstract: As Automated Driving Systems (ADS) technology advances, ensuring safety and
public trust requires robust assurance frameworks, with safety cases emerging
as a critical tool toward such a goal. This paper explores an approach to
assess how a safety case is supported by its claims and evidence, toward
establishing credibility for the overall case. Starting from a description of
the building blocks of a safety case (claims, evidence, and optional
format-dependent entries), this paper delves into the assessment of support of
each claim through the provided evidence. Two domains of assessment are
outlined for each claim: procedural support (formalizing process specification)
and implementation support (demonstrating process application). Additionally,
an assessment of evidence status is also undertaken, independently from the
claims support. Scoring strategies and evaluation guidelines are provided,
including detailed scoring tables for claim support and evidence status
assessment. The paper further discusses governance, continual improvement, and
timing considerations for safety case assessments. Reporting of results and
findings is contextualized within its primary use for internal decision-making
on continual improvement efforts. The presented approach builds on state of the
art auditing practices, but specifically tackles the question of judging the
credibility of a safety case. While not conclusive on its own, it provides a
starting point toward a comprehensive "Case Credibility Assessment" (CCA),
starting from the evaluation of the support for each claim (individually and in
aggregate), as well as every piece of evidence provided. By delving into the
technical intricacies of ADS safety cases, this work contributes to the ongoing
discourse on safety assurance and aims to facilitate the responsible
integration of ADS technology into society.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [28] [MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge](https://arxiv.org/abs/2506.09159)
*Antonio Calagna,Yenchia Yu,Paolo Giaccone,Carla Fabiana Chiasserini*

Main category: cs.NI

TL;DR: 该论文提出了一种支持微服务移动性的状态迁移技术框架，解决了迁移过程的实现与编排问题，显著提升了性能并满足了严格的用户体验要求。


<details>
  <summary>Details</summary>
Motivation: 解决边缘网络中微服务移动性的状态迁移问题，同时满足网络和应用KPI目标。

Method: 引入了一种高效实现状态迁移并智能编排迁移过程的框架。

Result: 实验表明，该方案将迁移停机时间减少77%，并成功满足了延迟敏感微服务的严格用户体验要求。

Conclusion: 该框架在配置迁移过程和满足KPI目标方面优于现有技术，适用于无人机自动驾驶和多目标跟踪等实际场景。

Abstract: Stateful migration has emerged as the dominant technology to support
microservice mobility at the network edge while ensuring a satisfying
experience to mobile end users. This work addresses two pivotal challenges,
namely, the implementation and the orchestration of the migration process. We
first introduce a novel framework that efficiently implements stateful
migration and effectively orchestrates the migration process by fulfilling both
network and application KPI targets. Through experimental validation using
realistic microservices, we then show that our solution (i) greatly improves
migration performance, yielding up to 77% decrease of the migration downtime
with respect to the state of the art, and (ii) successfully addresses the
strict user QoE requirements of critical scenarios featuring latency-sensitive
microservices. Further, we consider two practical use cases, featuring,
respectively, a UAV autopilot microservice and a multi-object tracking task,
and demonstrate how our framework outperforms current state-of-the-art
approaches in configuring the migration process and in meeting KPI targets.

</details>


### [29] [Adaptive Bandwidth Sharing for Optimizing QoE of Real-Time Video](https://arxiv.org/abs/2506.09197)
*Sushi Anna George,Vinay Joseph*

Main category: cs.NI

TL;DR: 提出了一种新颖的迭代半静态带宽共享策略，平衡静态和动态共享的优点，优化资源分配并满足实时流量需求。


<details>
  <summary>Details</summary>
Motivation: 提高无线网络中实时流量管理的效率，减少拥塞并改善用户体验（QoE）。

Method: 采用迭代半静态带宽共享策略，最小化运营商间的协调频率，同时优化资源分配。

Result: 理论证明策略的最优性，并通过仿真验证其性能，实现近最优带宽分配且降低开销。

Conclusion: 该策略是实时无线应用的实用解决方案，具有高效性和低开销。

Abstract: The concept of spectrum or bandwidth sharing has gained significant global
attention as a means to enhance the efficiency of real-time traffic management
in wireless networks. Effective bandwidth sharing enables optimal utilization
of available resources, reducing congestion and improving QoE for
delay-sensitive applications such as real-time video transmission. In this
paper, we propose a novel iterative semi-static bandwidth sharing policy that
balances the advantages of both static and dynamic sharing approaches. Our
approach minimizes the frequency of coordination between network operators
while ensuring efficient resource allocation and meeting the stringent QoE
demands of real-time traffic. The proposed policy iteratively optimizes both
the spectrum sharing between operators and the resource allocation for
individual clients. We establish strong theoretical guarantees for the
optimality of the proposed policy and prove that it converges to the optimal
static sharing policy irrespective of initial conditions or fluctuations in
traffic arrival rates. Additionally, we conduct extensive simulations to
evaluate the impact of key system parameters - including step size, hyperperiod
length, and arrival process dynamics - on the performance of our policy. Our
results demonstrate the effectiveness of the proposed approach in achieving
near-optimal bandwidth allocation with reduced overhead, making it a practical
solution for real-time wireless applications.

</details>


### [30] [Age of Information in Unreliable Tandem Queues](https://arxiv.org/abs/2506.09245)
*Muthukrishnan Senthilkumar,Aresh Dadlani,Hina Tabassum*

Main category: cs.NI

TL;DR: 本文提出了一个分析框架，用于研究不可靠节点串联队列系统中的信息年龄（AoI），推导了两种配置下的停留时间分布，并分析了关键参数对AoI的影响。


<details>
  <summary>Details</summary>
Motivation: 实时应用和物联网对信息及时性的需求日益增长，现有AoI模型假设节点完全可靠，无法准确反映节点传输失败的情况。

Method: 使用概率生成函数推导无限缓冲M/M/1串联系统中不可靠节点的停留时间分布，并扩展到M/G/1串联系统，采用补充变量技术。

Result: 数值结果表明，关键系统参数对马尔可夫和非马尔可夫服务时间的不可靠串联队列的平均AoI有显著影响。

Conclusion: 本文为不可靠节点串联队列系统的AoI分析提供了理论框架，揭示了节点不可靠性对数据新鲜度的影响。

Abstract: Stringent demands for timely information delivery, driven by the widespread
adoption of real-time applications and the Internet of Things, have established
the age of information (AoI) as a critical metric for quantifying data
freshness. Existing AoI models often assume multi-hop communication networks
with fully reliable nodes, which may not accurately capture scenarios involving
node transmission failures. This paper presents an analytical framework for two
configurations of tandem queue systems, where status updates generated by a
single sensor are relayed to a destination monitor through unreliable
intermediate nodes. Using the probability generating function, we first derive
the sojourn time distribution for an infinite-buffer M/M/1 tandem system with
two unreliable nodes. We then extend our analysis to an M/G/1 tandem system
with an arbitrary number of unreliable nodes, employing the supplementary
variable technique while assuming that only the first node has an infinite
buffer. Numerical results demonstrate the impact of key system parameters on
the average AoI in unreliable tandem queues with Markovian and non-Markovian
service times.

</details>


### [31] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: 本文提出了一种基于多臂老虎机（MAB）的在线优化框架，用于集成地面与非地面网络（TN-NTN），以平衡网络容量与能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着地面网络部署密度的增加，探索非地面网络（NTN）在减轻地面网络负载和实现能源高效运行方面的潜力。

Method: 采用多臂老虎机（MAB）和Bandit-feedback Constrained Online Mirror Descent（BCOMD）算法，实时优化带宽分配、用户设备关联和宏基站关闭等参数。

Result: 24小时系统级仿真显示，该框架显著减少了高峰时段未满足需求的用户设备比例，并在低流量时段实现了19%的吞吐量增益和5%的能源节省。

Conclusion: 提出的框架在集成TN-NTN架构中表现出色，优于遵循3GPP建议的标准网络设置。

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [32] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: 提出一种生成模型方法，用于实时网络流量预测，处理数据缺失问题，通过张量补全和预训练生成模型实现低秩结构，优化潜在表示以实现实时预测。


<details>
  <summary>Details</summary>
Motivation: 实际场景中网络流量数据常不完整，现有方法假设数据完整，无法直接应用。

Method: 将流量预测建模为张量补全问题，引入预训练生成模型捕捉低秩结构，优化潜在表示而非高维张量。

Result: 在真实数据集上验证，预测误差MAE低于0.002，响应时间100毫秒内。

Conclusion: 该方法有效解决了数据缺失问题，实现了高精度实时预测。

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


### [33] [Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks](https://arxiv.org/abs/2506.09703)
*Huan Lin,Chenguang Zhu,Lianghui Ding,Feng Yang*

Main category: cs.NI

TL;DR: 提出了一种多级损伤感知图学习算法（ML-DAGL），解决无人机群网络中图学习算法的过聚合和非收敛问题，通过多分支损伤注意力模块（MBDA）和扩张图卷积网络（DGCN）优化恢复轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有图学习算法在大规模损伤场景下因拓扑不均匀和稀疏性导致过聚合和非收敛问题，需改进。

Method: 引入MBDA模块生成多跳损伤注意力图（mDAG），结合DGCN生成最优恢复轨迹，确保收敛性。

Result: 仿真显示算法能在大规模损伤下恢复网络连接，恢复时间减少75.94%，拓扑均匀性提升。

Conclusion: ML-DAGL算法有效解决了无人机群网络的连通性恢复问题，性能显著优于现有方法。

Abstract: Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to
address communication network split issues and restore connectivity. However,
existing graph learning-based resilient algorithms face over-aggregation and
non-convergence problems caused by uneven and sparse topology under massive
damage scenarios. To alleviate these problems, we propose a novel Multi-Level
Damage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery
trajectories by mining information from destroyed UAVs. We first introduce a
Multi-Branch Damage Attention (MBDA) module, which forms a sequence of
multi-hop Damage Attentive Graphs (mDAG) with different ranges of receptive
fields. Each mDAG links only remaining and damaged nodes to ensure a more even
degree distribution for mitigating over-aggregation, and utilizes multi-hop
dilation to establish more links for sparse topology enhancement. To resort to
the mDAG, we propose a Dilated Graph Convolution Network (DGCN), which
generates the optimal recovery trajectories with theoretically proven
convergence under massive damage cases. Simulation results show that the
proposed algorithm can guarantee the connectivity restoration under large swarm
and damage scales, while significantly expediting the recovery time by 75.94%
and improving the topology uniformity after recovery.

</details>


### [34] [Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks](https://arxiv.org/abs/2506.09878)
*Ryan Barker*

Main category: cs.NI

TL;DR: 本文对虚拟化无线接入网（RAN）进行了综合分析，涵盖频谱政策、云计算、组织准备等多个领域，提出了5G和6G发展的关键见解。


<details>
  <summary>Details</summary>
Motivation: 当前关于虚拟化RAN的讨论往往将频谱政策、云计算和组织准备割裂开来，缺乏整体视角。本文旨在填补这一空白，为5G和6G的发展提供集成分析。

Method: 通过比较研究（如T-Mobile US和Verizon）、数学建模（频谱容量计算）、技术平台分析（如NVIDIA EGX、Samsung vRAN 3.0）以及案例研究（如国家自动化案例），全面探讨虚拟化RAN的各个方面。

Result: 研究发现，中频段连续频谱在覆盖和用户流失率上优于毫米波部署；256位加密会增加延迟，但可通过内联加密卸载缓解；文化变革错误修正后，自动化周期时间显著缩短。

Conclusion: 本文总结了6G研究的开放挑战（如太赫兹频段、能源中性AI加速器），并为运营商、供应商和研究人员提供了实用建议。

Abstract: Virtualising the Radio Access Network (RAN) is widely touted as the
corner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current
discourse often isolates spectrum policy, cloud engineering and organisational
readiness into silos. This paper delivers an integrated analysis that spans
science, technology, business strategy and culture. I first review
spectrum-auction economics and show-via a comparative study of T-Mobile US and
Verizon-that mid-band contiguity leveraged through software-defined carrier
aggregation outperforms mmWave-centric deployments in both coverage and churn
metrics. I then formalise the technical foundations of virtualised and open
RAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths
and quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute
platforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent
Controllers are examined alongside AI ML pipelines that enable
digital-twin-driven optimisation. A security cost model extends recent O-RAN
measurements to show how 256-bit cipher enforcement adds 35-60 us latency
unless mitigated by inline crypto off-load. Finally, a national automation case
study of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction
once cultural change errors are corrected. I conclude with open research
challenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust
orchestration, offering actionable recommendations for operators, vendors and
researchers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Main category: cs.LG

TL;DR: 论文提出了一种基于Llama 3的抗体-抗原结合亲和力预测模型（LlamaAffinity），在多个评估指标上优于现有方法，并显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统抗体亲和力测量方法耗时且昂贵，人工智能尤其是大语言模型（LLMs）为抗体设计和亲和力预测提供了新途径。

Method: 利用开源Llama 3框架和Observed Antibody Space（OAS）数据库的抗体序列数据，开发了LlamaAffinity模型。

Result: 模型在准确率（0.9640）、F1分数（0.9643）、精确度（0.9702）、召回率（0.9586）和AUC-ROC（0.9936）上表现优异，且训练时间显著缩短（0.46小时）。

Conclusion: LlamaAffinity模型在抗体-抗原亲和力预测中表现出色，为AI驱动的抗体设计提供了高效工具。

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [36] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Main category: cs.LG

TL;DR: FinHEAR是一个多智能体框架，结合人类专业知识和自适应风险感知推理，提升语言模型在金融决策中的表现。


<details>
  <summary>Details</summary>
Motivation: 语言模型在金融决策中缺乏对行为模式（如信息不对称下的专家依赖、损失厌恶敏感性等）的捕捉能力。

Method: FinHEAR通过多智能体框架，结合历史趋势分析、事件解读和专家指导检索，融入行为经济学原理。

Result: 在金融数据集上，FinHEAR在趋势预测和交易任务中表现优于基线模型，准确率和风险调整收益更高。

Conclusion: FinHEAR通过结合人类专业知识和自适应风险感知，显著提升了金融决策的准确性和稳健性。

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [37] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Main category: cs.LG

TL;DR: 论文提出了一种基于用户反馈的奖励微调方法PageLLM，用于优化大型语言模型在整页优化（WPO）任务中的表现，结合页面级和项目级奖励机制，显著提升了实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型（LLMs）在复杂任务如整页优化（WPO）中的微调需要大量人工标注数据，成本高昂。用户反馈虽易获取但噪声多，如何有效利用用户反馈成为关键挑战。

Method: 提出PageLLM方法，采用混合粒度奖励机制，结合页面级奖励（评估整体质量）和项目级奖励（评估关键推荐准确性），以用户反馈为监督信号进行微调。

Result: 在公开和工业数据集上验证，PageLLM优于基线方法，并在超过1000万用户的在线A/B测试中实现0.44%的GMV增长。

Conclusion: PageLLM通过双奖励机制有效利用用户反馈，显著提升了WPO任务的性能，具有实际应用价值。

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [38] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了一种结合LLMs和ML的团队框架，通过四个步骤实现特征转换，提升下游任务性能并减少错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法（传统ML的低效性和LLMs的不稳定性）无法同时解决特征转换中的稳定性和有效性挑战。

Method: 结合LLMs的符号生成和ML的梯度优化，包括四个步骤：高质量样本生成、特征转换序列嵌入与搜索、学生LLM知识蒸馏、LLM-ML解码器团队协作。

Result: 实验显示团队策略在下游任务中性能提升5%，错误减少近半，且框架高效稳健。

Conclusion: 团队框架有效结合LLMs和ML的优势，解决了特征转换中的稳定性和有效性挑战，并展示了LLMs对原始数据的理解能力。

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [39] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Main category: cs.LG

TL;DR: 论文提出了一种基于生物可解释性的脉冲神经网络（SNN）模型，用于决策任务，弥补了传统模型（如DDM和泊松计数器模型）缺乏学习机制的不足。


<details>
  <summary>Details</summary>
Motivation: 传统决策模型（如DDM和泊松计数器模型）缺乏学习机制，且仅适用于参与者已具备类别先验知识的任务。本文旨在通过生物可解释的SNN模型，填补认知模型与生物模型之间的鸿沟。

Method: 提出了一种基于多变量霍克斯过程的SNN模型，并展示了DDM与泊松计数器模型的耦合关系。进一步推导了具有相关噪声的特定DDM模型，并通过在线分类任务验证模型预测。

Result: 证明了DDM可以被脉冲泊松神经元近似，且霍克斯网络中的脉冲神经元可以通过局部学习规则生成特定DDM模型。

Conclusion: 该研究为将生物相关的神经机制整合到认知模型中提供了重要进展，深化了对神经活动与行为关系的理解。

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [40] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Main category: cs.LG

TL;DR: 本文提出了一种增强的异步AdaBoost框架，用于联邦学习（FL），通过自适应通信调度和延迟权重补偿减少同步频率和通信开销，同时保持或提高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在联邦学习中提高通信效率、可扩展性、收敛性和鲁棒性，特别是在计算机视觉、区块链、移动设备、物联网和医疗诊断等五个领域。

Method: 采用自适应通信调度和延迟权重补偿技术，减少同步频率和通信开销，同时评估训练时间、通信开销、收敛迭代和分类准确性等指标。

Result: 实验结果显示，相比基线AdaBoost，训练时间减少20-35%，通信开销减少30-40%，收敛所需的提升轮数显著减少。

Conclusion: 增强的AdaBoost框架在多种联邦学习场景中表现出显著的高效性和鲁棒性，具有广泛的应用潜力。

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [41] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: 提出了一种基于耦合自由能的变分推断优化框架，扩展了变分推断技术以处理耦合指数族的曲率几何，提高了模型的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统变分推断技术在处理重尾分布（如广义Pareto和Student's t分布）时存在局限性，需要一种更通用的框架。

Method: 利用耦合自由能和耦合ELBO，设计了一种耦合变分自编码器（CVAE），通过采样重尾潜在分布及其耦合概率来改进模型。

Result: 在CelebA图像重建任务中，CVAE在5个训练周期后比VAE有3%的性能提升。

Conclusion: 该方法通过耦合几何和概率分布，显著提高了模型的鲁棒性和准确性，尤其在处理重尾数据时表现优异。

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [42] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Main category: cs.LG

TL;DR: 论文提出了一种名为FSR的框架，用于利用LLMs自动生成和优化高性能CUDA程序，显著提升GPU内核的执行效率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用代码生成上表现优异，但在生成硬件特定、架构感知且性能关键的GPU代码方面仍面临挑战。

Method: 提出了FSR框架，联合优化编译、功能正确性和运行时性能，并通过实际GPU内核执行延迟验证。

Result: FSR增强的LLMs不仅能生成正确CUDA代码，还能迭代优化效率，生成的代码执行速度最高可达人工编写的179倍。

Conclusion: 结合LLMs与性能强化的方法，有望自动化GPU编程，适用于硬件特定、架构敏感和性能关键的应用。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [43] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为LwPTV的方法，通过构建显著性分数来修剪任务向量中的冗余参数，从而提升多任务学习在域外数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多任务学习模型合并方法主要关注域内数据集性能，而忽略了域外数据集的表现。本文旨在解决这一问题。

Method: 提出LwPTV方法，通过层级修剪任务向量中的冗余参数，生成掩码向量，仅保留预训练模型参数。

Result: 实验表明，LwPTV显著提升了域外任务性能，同时保持了域内任务的能力。

Conclusion: LwPTV方法灵活且有效，可与其他模型合并方法结合，提升多任务学习的泛化能力。

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [44] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了一种利用生成概率增强奖励模型的方法，通过响应轨迹中的过程一致性改进奖励学习，提升模型在未见响应上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型依赖粗粒度的响应级评分，难以识别与评分真正相关的响应轨迹中的具体部分，导致泛化能力差。

Method: 利用生成概率建立响应轨迹中过程间的奖励一致性，提出轨迹内一致性正则化，强制相邻过程在更高生成概率下保持奖励一致性。

Result: 改进的奖励模型在RewardBench上表现更优，同时提升了DPO对齐策略和最佳N推理验证结果。

Conclusion: 通过引入过程一致性正则化，奖励模型能够更好地学习细粒度信号，提升泛化能力和性能。

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [45] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLMs）中记忆与泛化的关系，发现模型容量与学习模式之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 理解记忆与泛化在LLMs中的关系，探索模型容量如何影响学习行为。

Method: 通过预训练容量受限的Transformer模型，在合成字符级任务中分别测试泛化（算术外推）和记忆（事实回忆）。

Result: 小模型能泛化但无法记忆，大模型能记忆但无法泛化；联合训练时所有模型均无法泛化。

Conclusion: 预训练可能固有地偏向某种学习模式，研究为小语言模型的设计提供了启示。

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [46] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: FLoRIST提出了一种联邦学习框架，通过高效分解本地适配器，平衡通信效率和模型性能，适用于异构客户端。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中LoRA方法在通信效率、模型准确性和计算成本之间的平衡问题。

Method: FLoRIST采用奇异值分解本地适配器，通过中间空间表示信息，并引入可调阈值选择全局低秩适配器。

Result: 在多种数据集和LLM上验证，FLoRIST在通信效率和性能上表现最佳。

Conclusion: FLoRIST为联邦学习中的参数高效微调提供了有效解决方案。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [47] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: FSL-Net是一种神经网络，用于在大规模高维数据中快速准确地定位特征偏移，无需重新训练即可应用于新数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在定位特征偏移时的不准确性和可扩展性问题，特别是在异构数据源和噪声数据中。

Method: 提出FSL-Net，通过训练学习数据集的统计特性，能够泛化到未见过的数据集和偏移。

Result: FSL-Net能够高效且准确地定位特征偏移，适用于大规模高维数据。

Conclusion: FSL-Net为特征偏移定位提供了一种有效且可扩展的解决方案，代码和预训练模型已开源。

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [48] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为UPQ的统一渐进量化框架，用于将指令调优的LLM从FP16量化到INT2，结合了块级后训练量化和蒸馏量化感知训练，首次实现了不依赖专有数据的INT2量化，并在MMLU和IFEval基准上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）规模的迅速扩大，在资源受限设备上部署面临挑战，因此对极低位量化（如2位）的需求增加。此前工作仅针对预训练LLM，未扩展到指令调优模型。

Method: UPQ框架分两步：1）使用块级后训练量化（PTQ）将FP16模型量化为INT4以减少后续INT2量化的误差；2）通过蒸馏量化感知训练（Distill-QAT）最小化广义Jensen-Shannon散度（JSD），使INT2模型输出与FP16一致。

Result: UPQ首次实现了不依赖专有数据的INT2指令调优LLM量化，在MMLU和IFEval基准上达到最优性能。

Conclusion: UPQ为指令调优LLM的极低位量化提供了一种有效方法，填补了此前研究的空白。

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [49] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: 本文分析了去中心化学习中的泛化误差，重点关注数据异质性、模型初始化和随机梯度噪声的影响，并揭示了拜占庭攻击对泛化误差的负面影响。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习的优化误差已被广泛研究，但其泛化误差相对较少被探索。泛化误差对模型在现实应用中的性能至关重要，因此理解其影响因素具有重要意义。

Method: 本文提出了细粒度的泛化误差分析，针对无攻击和拜占庭鲁棒的去中心化学习，考虑了数据异质性和温和假设，而非严格的随机梯度假设。

Result: 研究发现数据异质性、模型初始化和随机梯度噪声对泛化误差有显著影响，拜占庭攻击的负面影响与数据异质性相关而与样本量无关。数值实验验证了理论结果。

Conclusion: 本文填补了去中心化学习泛化误差研究的空白，揭示了关键影响因素，为实际应用提供了理论支持。

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [50] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: MetaTT是一个统一的Tensor Train（TT）适配器框架，用于预训练Transformer的全局低秩微调。与LoRA独立微调每个权重矩阵不同，MetaTT通过共享TT分解所有子模块，显著减少参数数量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有微调方法（如LoRA）参数冗余问题，提出一种更高效的全局低秩微调框架。

Method: 使用共享TT分解Transformer的所有子模块，并通过索引结构轴（如层和矩阵类型）实现参数压缩。

Result: 在标准语言建模基准测试中，MetaTT显著减少参数数量，同时保持与LoRA相似的准确性，甚至优于其他基于张量的方法。

Conclusion: MetaTT通过TT分解和成熟的优化方法，提供了一种高效、可扩展的微调方案，适用于多任务共享适配器。

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [51] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: SyncFed是一个时间感知的联邦学习框架，通过时间同步和时间戳量化客户端更新的陈旧性，提升模型准确性和信息新鲜度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中网络延迟、时钟不同步和客户端更新不一致会损害模型可靠性和收敛性，现有方法缺乏量化陈旧性的机制。

Method: 引入SyncFed框架，利用时间同步和时间戳（基于NTP协议）量化更新陈旧性，并在聚合时应用时间感知的权重。

Result: 实验表明，SyncFed在分布式环境中能稳定模型的时间上下文，相比传统方法提升了准确性和信息新鲜度。

Conclusion: SyncFed通过时间感知机制有效解决了联邦学习中的时间不一致问题，提升了模型性能。

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [52] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM是一个传感器-语言基础模型家族，通过自然语言理解可穿戴传感器数据，解决了传感器数据与语言对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏配对的、丰富注释的传感器-文本描述，传感器数据与语言的对齐和解释仍然具有挑战性。

Method: 采用分层标题生成流程，从传感器数据中提取统计、结构和语义信息，并扩展了多模态预训练架构（如CLIP、CoCa）。

Result: 构建了最大的传感器-语言数据集（59.7百万小时数据，103,000人），在零样本识别、少样本学习和跨模态检索中表现优异。

Conclusion: SensorLM在人类活动分析和医疗保健任务中表现出色，并展示了标签效率、传感器标题生成和零样本泛化等能力。

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [53] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出了一种多阶段方法，结合可验证秘密共享、安全聚合和定制对称私有信息检索方案，以在数据异质性下实现信息论隐私保证和拜占庭弹性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，如何在保护客户端数据隐私的同时抵御拜占庭客户端的攻击是一个基本挑战，尤其是在数据异质性的情况下。

Method: 采用多阶段方法，结合可验证秘密共享、安全聚合和定制对称私有信息检索方案。

Result: 在多种攻击下验证了方案的有效性，并展示了其优于现有技术的性能。同时，通过零阶估计方法降低通信成本。

Conclusion: 该方法在数据异质性下实现了隐私保护和拜占庭弹性，并通过优化通信成本使其具有可扩展性。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [54] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: CodeBrain是一种高效的EEG基础模型，通过双阶段训练解决传统模型在EEG数据中的泛化性问题，结合了TFDual-Tokenizer和EEGSSM方法。


<details>
  <summary>Details</summary>
Motivation: 传统EEG模型因通道配置、序列长度和任务目标的差异而泛化性差，现有EEG基础模型在异构表示和多尺度依赖捕捉上效率不足。

Method: 提出TFDual-Tokenizer独立编码时频域特征，扩展表示空间；EEGSSM结合全局卷积和滑动窗口注意力，建模多尺度依赖。

Result: 在10个公开EEG数据集上验证了CodeBrain的泛化能力，并通过线性探测展示了其性能。

Conclusion: CodeBrain为EEG建模提供了生物学启发和可解释性，为未来神经科学研究奠定了基础。

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [55] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: TRACE是一种通用的多模态检索器，通过将时间序列嵌入与对齐的文本上下文结合，实现细粒度通道级对齐和语义检索，提升下游任务的预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 动态数据（如天气、医疗和能源领域）的普遍性需要有效的时间序列数据解释和检索方法。现有方法缺乏语义基础，难以处理异构模态和多通道信号。

Method: 提出TRACE，通过硬负样本挖掘实现语义检索，支持文本到时间序列和时间序列到文本的跨模态检索。

Result: TRACE在下游预测和分类任务中达到最先进性能，同时作为通用检索器增强时间序列模型。

Conclusion: TRACE不仅是一种强大的独立编码器，还能通过轻量级任务特定调优提升上下文感知表示，具有广泛的应用潜力。

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [56] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Main category: cs.LG

TL;DR: BSA-TNP是一种新型神经过程架构，通过引入KRBlocks和BSA，在保持高精度的同时提升可扩展性，适用于多分辨率学习和时空建模。


<details>
  <summary>Details</summary>
Motivation: 现代神经过程模型在复杂应用中面临精度与可扩展性的权衡，本文旨在证明这种权衡并非必要，并提出一种更高效的架构。

Method: 提出BSA-TNP架构，结合KRBlocks、群不变注意力偏差和内存高效的BSA，支持多分辨率学习和时空建模。

Result: BSA-TNP在精度上优于或匹配现有模型，训练时间更短，支持高维固定效应，并能高效处理大规模数据。

Conclusion: BSA-TNP在精度和可扩展性上取得平衡，适用于复杂应用，为神经过程模型的发展提供了新方向。

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [57] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出了一种新的LLM代理框架，通过上下文学习增强规划能力，利用原子事实增强和递归前瞻搜索，无需微调即可在线改进决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂交互环境中难以适应新信息或高效利用历史经验进行多步推理。

Method: 通过原子事实增强动态提示LLM组件，结合深度受限的前瞻搜索模拟轨迹并评估结果。

Result: 在TextFrozenLake和ALFWorld等任务中表现出更高的性能和适应性。

Conclusion: 该框架通过经验积累优化行为，验证了基于事实抽象和LLM模拟的理论动机。

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [58] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet是一个开源的多模态基准测试和软件生态系统，用于评估和适应视觉、语言和动作领域的模型。


<details>
  <summary>Details</summary>
Motivation: 开发通用代理系统需要结合视觉理解、语言理解和动作生成，但目前缺乏统一的评估标准。

Method: 提出MultiNet基准测试，包括标准化评估协议、开源软件和复合数据集（1.3万亿token）。

Result: MultiNet被用于下游研究，揭示了视觉-语言-动作模型的泛化局限性。

Conclusion: MultiNet为多模态模型的评估和适应提供了重要工具，推动了通用代理系统的发展。

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [59] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Main category: cs.LG

TL;DR: CuriosiTree是一种基于启发式的零样本信息获取策略，通过贪心树搜索平衡信息增益与成本，在临床诊断模拟中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 决策者在信息不足时需选择成本效益高的信息获取方式。

Method: 提出CuriosiTree，采用贪心树搜索评估行动的信息增益与成本。

Result: 在临床诊断模拟中，CuriosiTree能有效整合异构信息源，诊断准确性优于基线策略。

Conclusion: CuriosiTree为LLMs提供了一种高效的信息获取策略，适用于信息不足的决策场景。

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [60] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FNF的骨干网络和DBD架构，用于解决多变量长期时间序列预测中同时捕捉时间依赖性和空间相关性的挑战，无需依赖辅助技术即可实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要借用自然语言处理或计算机视觉的骨干网络（如Transformer），未能充分处理时间序列的独特性质（如周期性），缺乏专门针对时间序列的骨干网络。

Method: 引入FNF作为骨干网络，统一处理局部时域和全局频域信息；DBD架构通过信息瓶颈理论优化梯度流和表示能力。

Result: 在11个公共基准数据集上验证了最先进性能，且无需辅助技术。

Conclusion: 研究表明，设计良好的神经网络架构可以捕捉时间序列的固有特性，可能改变科学和工业应用中的时间序列建模方式。

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [61] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习方法，通过联合考虑多个任务来模仿人类决策，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF方法简化了人类决策过程，未能充分整合多策略，需改进。

Method: 利用无奖励环境中的人类评分推断奖励函数，引入可学习权重平衡分类和回归模型。

Result: 实验表明，该方法优于现有评分RL方法，甚至超越传统RL方法。

Conclusion: 新方法能更好地捕捉人类决策的不确定性，适应性强。

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [62] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Main category: cs.LG

TL;DR: LaDCast是一种全球潜在扩散框架，用于中程集合天气预报，通过潜在空间生成小时级集合预报，性能接近欧洲中期天气预报中心IFS-ENS，并显著减少计算和存储需求。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报和机器学习方法在高精度和高效不确定性量化方面存在挑战，LaDCast旨在解决这些问题。

Method: 使用自动编码器压缩高维ERA5再分析数据，基于Transformer的扩散模型生成潜在更新，结合GeoRoPE、双流注意力机制和正弦时间嵌入。

Result: LaDCast在确定性和概率性预报技能上接近IFS-ENS，且在极端事件（如气旋）轨迹捕捉上表现更优，同时大幅降低计算和存储需求。

Conclusion: LaDCast展示了在潜在空间进行天气预报的可行性，为实时公里级分辨率预报提供了实用路径。

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [63] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Main category: cs.LG

TL;DR: FedRAG是一个用于在集中式和联邦式架构上微调RAG系统的框架，支持先进的微调方法，填补了现有工具的空白。


<details>
  <summary>Details</summary>
Motivation: 解决仅依赖大型语言模型参数记忆的缺点，并通过微调提升RAG系统的性能。

Method: 提出FedRAG框架，支持集中式和联邦式架构的微调，提供简单接口和任务转换功能。

Result: FedRAG填补了现代RAG生态系统中的工具空白，支持先进的微调方法。

Conclusion: FedRAG为RAG系统的微调提供了高效且灵活的解决方案，适用于多种架构。

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [64] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 论文提出了一种新的任务：从离线强化学习数据集中聚类轨迹，每个聚类中心代表生成其轨迹的策略。通过KL散度和策略诱导分布的混合，提出了PG-Kmeans和CAAE两种方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习数据集中的轨迹聚类是一个新任务，旨在通过聚类中心揭示生成轨迹的策略，为离线RL提供更深入的分析工具。

Method: 提出PG-Kmeans（基于行为克隆策略迭代聚类）和CAAE（类似VQ-VAE的框架，通过潜在表示引导聚类）。

Result: 在D4RL数据集和自定义GridWorld环境中验证了方法的有效性，能够将轨迹划分为有意义的聚类。

Conclusion: PG-Kmeans和CAAE为基于策略的轨迹聚类提供了有效框架，具有广泛的应用潜力。

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [65] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 论文提出了一种改进的降阶模型方法mLaSDI，通过多阶段训练自编码器来提升精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统LaSDI方法在复杂或高频场景下难以同时满足数据重建和动态约束，需要改进。

Method: 采用多阶段自编码器训练，每阶段修正前一阶段的误差，提升模型性能。

Result: mLaSDI在预测和重建误差上表现更优，同时减少了训练时间。

Conclusion: mLaSDI是一种高效且精确的降阶模型方法，适用于复杂场景。

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [66] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: 研究了基于Transformer嵌入模型的池化方法设计，提出了一种注意力自适应池化方法，优于传统方法（AvgPool、MaxPool、ClsToken），在信号噪声比波动时表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 主要动机是解决传统池化方法在信号噪声比（SNR）波动时性能下降的问题，特别是在强化学习和视觉应用中。

Method: 通过将池化问题建模为向量量化，提出了一种基于注意力的自适应池化方法，以最小化信号损失。

Result: 在合成数据集和标准基准测试（如关系推理、多智能体强化学习和视觉任务）中验证了该方法的优越性和鲁棒性。

Conclusion: 自适应池化方法在信号噪声比波动时表现更优，为Transformer模型在噪声环境中的应用提供了更稳健的解决方案。

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [67] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于意图的新分类法，用于分析大语言模型（LLM）的遗忘技术，探讨了遗忘的真实性、评估方法的局限性及实际挑战。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘技术的分类忽视了其根本意图，即是否真正移除内部知识或仅抑制其行为效果。本文旨在填补这一空白。

Method: 提出基于意图的新分类法，重新审视遗忘方法的真实性，调查评估策略并指出局限性，探讨实际挑战。

Result: 发现许多移除方法可能仅表现为抑制，评估方法存在不足，实际应用中存在可扩展性和顺序遗忘等挑战。

Conclusion: 本文为理解和推进生成式AI中的遗忘提供了全面框架，支持未来研究和数据隐私政策决策。

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [68] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Main category: cs.LG

TL;DR: 论文提出MindRAG框架，结合LLM和多模态RAG技术，优化工业条件监测（CM）中的故障严重性估计和决策支持，减少误报并提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前CM系统依赖人工分析，存在高误报率和不确定性，MindRAG旨在通过LLM和RAG技术自动化分析，提升决策效率和可解释性。

Method: 提出MindRAG框架，整合多模态RAG和新型向量存储结构，利用现有标注和维护工单作为监督学习标签，处理无标签和噪声数据。

Result: 初步实验表明，MindRAG能有效支持决策，提高报警管理效率，并增强CM系统的可解释性。

Conclusion: MindRAG通过LLM和RAG技术显著改善了CM系统的自动化分析和决策支持能力，具有实际工业应用潜力。

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [69] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Main category: cs.LG

TL;DR: CFMI是一种新的通用缺失数据填补方法，结合了连续归一化流、流匹配和共享条件建模，优于传统和现代填补技术。


<details>
  <summary>Details</summary>
Motivation: 解决传统多重填补方法的计算难题，提供一种更高效且通用的填补方法。

Method: 结合连续归一化流、流匹配和共享条件建模，处理缺失数据。

Result: 在24个数据集的比较中，CFMI表现优于或媲美其他方法，且在时间序列数据填补中计算效率更高。

Conclusion: CFMI是一种适用于各种数据类型和维度的高效填补方法。

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [70] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Main category: cs.LG

TL;DR: 提出了一种基于认知不确定性估计的经验回放优先级方法，以减少噪声对强化学习的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间差分误差的优先级方法容易受噪声干扰，类似于探索中的噪声电视问题。

Method: 利用认知不确定性估计指导回放缓冲区中的转移优先级，减少不可预测随机过程的影响。

Result: 在表格模型和Atari游戏中验证了方法的有效性，性能优于分位数回归深度Q学习基准。

Conclusion: 认知不确定性优先级回放为强化学习代理提供了一条新路径。

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [71] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: G-Sim是一个混合框架，结合LLM驱动的结构设计和严格的实证校准，自动化构建稳健的模拟器，解决现有方法在泛化和准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 在医疗和物流等关键领域，构建可靠的模拟器对政策制定至关重要，但现有方法难以泛化或准确性不足。

Method: G-Sim通过LLM迭代设计模拟器核心组件和因果关系，结合灵活的校准技术（如梯度自由优化或模拟推断）估计参数。

Result: G-Sim生成可靠、因果感知的模拟器，缓解数据效率问题，支持复杂决策的系统级干预。

Conclusion: G-Sim通过结合领域先验和实证证据，为复杂决策提供稳健的模拟器。

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [72] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Main category: cs.LG

TL;DR: 本文提出了一种仅从状态轨迹学习MDP状态表示的框架，无需奖励信号或动作信息，通过学习最小动作距离（MAD）作为环境结构的度量。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖奖励或动作信息，限制了其适用性。本文旨在通过自监督学习MAD，提供一种更通用的状态表示方法。

Method: 提出学习MAD作为状态间距离的度量，构建嵌入空间使嵌入距离对应MAD，支持对称和非对称近似。

Result: 实验表明，该方法能高效学习准确的MAD表示，并在多种环境下显著优于现有方法。

Conclusion: MAD是一种有效的状态表示度量，自监督学习方法在多样环境中表现优越。

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [73] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Main category: cs.LG

TL;DR: 该研究利用自然语言处理技术分析电子健康记录，揭示HIV感染者面临的污名化、社会和行为问题。


<details>
  <summary>Details</summary>
Motivation: 传统问卷方法在评估HIV相关污名化问题时存在局限性，研究旨在通过电子健康记录提供更高效、可扩展的评估方法。

Method: 使用潜在狄利克雷分配（LDA）主题建模方法，结合专家筛选的关键词和雪球策略，分析9,140名HIV感染者的临床记录。

Result: 识别出多个主题，如心理健康与污名、社会支持、医疗资源限制等，并在不同年龄和性别亚组中发现差异。

Conclusion: 通过电子健康记录分析，可高效评估HIV相关污名化问题，改善患者结局。

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [74] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Main category: cs.LG

TL;DR: 论文提出了一种利用约束优化方法（ASP）从时间序列数据中学习因果图结构的方法，解决了因采样频率不匹配导致的信息丢失问题，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决时间序列数据中因采样频率不匹配导致的因果图结构学习困难问题，尤其是信息丢失带来的多解性。

Method: 采用约束优化方法（ASP）推导因果图，结合图理论进一步剪枝可能的解集，提高了准确性和速度。

Result: 在模拟数据和实际脑结构连接数据上验证了方法的优越性，F1分数平均提升12%，且在精度和召回率上达到最优。

Conclusion: 该方法不仅显著优于现有方法，还对不同程度的采样丢失表现出鲁棒性，适用于实际应用。

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [75] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: 论文提出了一种双状态线性注意力机制（DSLA）和自适应蒸馏框架（Serve），以解决长输入下LLMs的计算和内存成本问题，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长输入时面临高昂的计算和内存成本，而现有的次二次方法（如线性注意力）因过度强调近期标记而降低准确性。

Method: 提出DSLA，通过维护两个专用隐藏状态（历史上下文和近期跟踪）来缓解线性注意力的短程偏差；并引入Serve框架，通过自适应蒸馏在推理时逐步替换Transformer层。

Result: 实验表明，Serve在推理速度上比Llama2-7B快2.3倍，比Zamba-7B快3.0倍，同时在下游任务中保持可比性能。

Conclusion: DSLA的双状态设计有效捕捉全局和局部依赖关系，解决了线性注意力中历史标记表示不足的问题。

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [76] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: InstructPro是一种基于自然语言指令设计蛋白质的生成模型，能够根据功能描述和配体公式生成功能一致的蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型依赖稀缺的蛋白质-配体复合物数据，而人类整理的文本描述资源丰富，因此提出利用文本指令指导蛋白质设计。

Method: 提出InstructPro模型家族，包括1B和3B参数版本，并开发了大规模数据集InstructProBench支持训练和评估。

Result: InstructPro在对接成功率和结构偏差上优于基线模型，3B版本平均RMSD降至2.527Å。

Conclusion: InstructPro展示了通过自然语言指令设计功能蛋白质的潜力，尤其在配体结合任务中表现优异。

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [77] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Main category: cs.LG

TL;DR: 论文提出ErrorEraser，一种通过识别和消除数据偏差引起的错误记忆来提升持续学习性能的通用插件。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）需要防止遗忘和促进知识迁移，但现有方法忽略了数据偏差导致的虚假相关性，影响了CL的效果。

Method: ErrorEraser包含两个模块：错误识别（通过无先验学习任务数据的概率密度分布识别偏差样本）和错误擦除（通过调整决策空间消除错误知识）。

Result: 实验表明，ErrorEraser显著减少了数据偏差的负面影响，在三种CL方法中实现了更高的准确率和更低的遗忘率。

Conclusion: ErrorEraser通过消除错误记忆，提升了持续学习在保留和迁移知识方面的性能。

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [78] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: 该论文研究了对抗训练中替代风险与分类风险收敛速率的关系，并提出了替代风险界限。


<details>
  <summary>Details</summary>
Motivation: 探讨对抗训练中替代风险最小化序列是否能够快速收敛到最优分类风险。

Method: 通过理论分析，推导替代风险界限，量化收敛速率。

Result: 提出了替代风险界限，并扩展到标准学习场景中的分布依赖性界限。

Conclusion: 研究为对抗训练中的风险收敛提供了理论支持，并对标准学习场景有独立意义。

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [79] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Main category: cs.LG

TL;DR: 本文综述了基于扩散模型（DMs）的异常检测与生成（ADGDM），探讨了其理论基础、实际应用及协同关系，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 异常检测在多个领域至关重要，但传统方法面临数据稀缺和复杂分布学习的挑战。扩散模型因其强大的数据生成能力，为解决这些问题提供了新思路。

Method: 通过分类ADGDM方法（如异常评分机制、条件策略和架构设计），分析其优缺点，并探讨扩散模型在异常检测与生成中的协同作用。

Result: 扩散模型通过生成异常数据解决了数据稀缺问题，同时检测方法提升了生成质量，两者相互促进。

Conclusion: 本文为研究者提供了扩散模型在异常检测中的创新应用指南，并指出了未来研究方向，如高效架构和与基础模型的整合。

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [80] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为LPO的新方法，通过利用位置数据和信息熵优化GUI交互，显著提升了交互精度。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在空间定位上依赖监督微调方法，但位置感知能力有限，且强化学习等方法难以有效评估位置准确性。

Method: LPO利用信息熵预测信息丰富区域的交互位置，并引入基于物理距离的动态位置奖励函数，结合GRPO支持广泛探索GUI环境。

Result: 实验表明LPO在离线和在线评估中均达到SOTA性能。

Conclusion: LPO通过优化位置偏好显著提升了GUI交互的精确性，代码将开源。

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [81] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Main category: cs.LG

TL;DR: 论文提出扩散蒸馏技术的局限性，发现GAN目标可以克服这些限制，并提出扩散训练可作为生成预训练，通过轻量级GAN微调实现高效一步生成。


<details>
  <summary>Details</summary>
Motivation: 扩散蒸馏技术虽广泛用于降低扩散模型的采样成本，但存在训练时间长和学生模型性能下降的问题。研究旨在探索GAN目标如何解决这些问题。

Method: 通过分析扩散蒸馏的局限性，提出单独使用GAN目标可克服这些限制，并基于扩散训练的生成预训练能力，通过轻量级GAN微调实现一步生成。

Result: 实验表明，仅需0.2M图像即可实现强性能，5M图像时接近SOTA结果。频率域分析解释了扩散训练中获得的一步生成能力。

Conclusion: 扩散训练可作为强大的生成预训练过程，为构建高效一步生成模型提供新视角。

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [82] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为QHNetV2的高效网络，用于预测哈密顿矩阵以加速电子结构计算。通过利用SO(2)局部框架的关系，避免了昂贵的SO(3)张量积，实现了全局SO(3)等变性。


<details>
  <summary>Details</summary>
Motivation: 电子结构计算在物理、化学和材料科学中至关重要，但传统方法计算成本高。论文旨在通过高效网络预测哈密顿矩阵，减少计算负担。

Method: 提出QHNetV2网络，引入高效的SO(2)等变操作，在SO(2)局部框架内完成特征更新和信息传递，避免了SO(3)张量积。

Result: 在QH9和MD17数据集上的实验表明，模型在多种分子结构和轨迹中表现优异，具有强泛化能力。

Conclusion: SO(2)局部框架的操作为电子结构的可扩展和对称感知学习提供了新方向。

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [83] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习（DRL）和遗传算法（GA）的进化增强机制（EAM），通过生成和优化解决方案来提升探索能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决DRL在组合优化问题中探索能力不足和易陷入局部最优的问题，同时弥补GA样本效率低和计算成本高的缺陷。

Method: EAM框架通过DRL生成解决方案，利用GA的遗传操作（如交叉和变异）优化这些方案，并将优化后的方案重新注入策略训练循环。

Result: 在TSP、CVRP等基准问题上，EAM显著提高了解决方案质量和训练效率。

Conclusion: EAM是一种通用且即插即用的框架，能够有效结合DRL和GA的优势，提升组合优化问题的解决效果。

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [84] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Main category: cs.LG

TL;DR: CAPT通过因果感知的后训练方法减少LLMs中的虚假相关性，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在分布外样本上因预训练中的虚假相关性而表现不佳的问题。

Method: 将偏差预测分解为事件估计和事件干预两个无偏步骤，避免额外微调偏差。

Result: 在CLadder和PrOntoQA数据集上，3B规模的CAPT模型仅用100个样本即优于传统SFT和更大LLMs。

Conclusion: CAPT方法有效且样本高效，显著提升LLMs在分布内外任务上的表现。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [85] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对Group SLOPE模型的安全筛选规则，通过识别零系数的不活跃组，显著提升计算效率和内存使用。


<details>
  <summary>Details</summary>
Motivation: Group SLOPE在自适应选择预测变量组时表现良好，但其块不可分组效应导致现有方法无效或低效，计算成本和内存占用高。

Method: 引入了一种安全筛选规则，专门解决块不可分组效应，识别不活跃组并在训练中排除。

Result: 实验证明该方法能有效检测不活跃特征组，显著提升计算效率且不影响准确性。

Conclusion: 提出的筛选规则可安全集成到现有优化算法中，确保结果与原始方法一致，适用于批量和随机算法。

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [86] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: 论文提出了一种名为Stained Glass Transform的方法，通过随机变换LLM的词嵌入，在保护输入数据隐私的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI计算基础设施的高成本和LLM部署的挑战导致企业对数据隐私的担忧，尤其是在共享或多租户环境中。

Method: 引入Stained Glass Transform，一种基于高斯混合模型互信息理论的学习随机变换方法。

Result: 通过理论和实验验证，该方法在保护隐私的同时保持了LLM的性能。

Conclusion: Stained Glass Transform为共享计算环境中的隐私保护提供了一种有效解决方案。

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [87] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出了两种新的损失函数（RG²和RG×），通过泰勒展开Softmax损失函数，解决了大规模对象空间中的计算和扩展性问题，并结合ALS优化方法，显著提升了收敛速度和排名性能。


<details>
  <summary>Details</summary>
Motivation: Softmax损失在大规模对象空间中存在计算开销和扩展性限制，需要更高效的替代方案。

Method: 通过泰勒展开Softmax损失函数，提出RG²和RG×损失函数，并结合ALS优化方法。

Result: 新方法在真实数据集上表现出与Softmax损失相当或更优的排名性能，同时显著加速收敛。

Conclusion: 该框架为相似性学习提供了理论见解和高效工具，适用于需要平衡排名质量和计算效率的任务。

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [88] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Main category: cs.LG

TL;DR: 论文指出了在RL训练中KL散度梯度估计的几个常见陷阱，包括错误地将KL估计作为损失函数以及忽略问题的序列性，并通过实验展示了正确实现方法。


<details>
  <summary>Details</summary>
Motivation: 揭示KL散度梯度估计在RL训练中的常见错误实现，并提出正确方法以避免这些问题。

Method: 通过表格和LLM实验，对比错误和正确的KL梯度实现方式。

Result: 错误实现无法产生期望的KL梯度，而正确方法能有效解决问题。

Conclusion: 论文提供了KL梯度估计的正确实现指南，强调了避免常见陷阱的重要性。

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [89] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [90] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Main category: cs.LG

TL;DR: OKBEs提出了一种新的无奖励MDP方法，通过状态-时间选项核（STOK）直接优化目标完成概率，具有组合性、模块化和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习中奖励最大化与组合性、模块化和可解释性之间的冲突，支持可验证的长时程规划和内在动机。

Method: 使用STOK作为预测映射，通过Chapman-Kolmogorov方程组合，高效表示和计算高维STOK，并记录语义可解释的事件概率。

Result: 实现了灵活的策略合成、跨任务规划表示重用，以及基于内在动机的目标验证。

Conclusion: OKBEs为高维动态世界模型中的可验证规划和内在动机提供了有效支持。

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [91] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 研究基于人类反馈的强化学习，提出一种基于随机探索的元算法，解决偏好查询的计算挑战，并改进查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决在马尔可夫决策过程中从轨迹级偏好比较中学习奖励函数的挑战，同时确保理论保证。

Method: 提出基于随机探索的元算法，并改进为批量收集轨迹对和优化实验设计以选择信息量大的查询。

Result: 在温和的强化学习假设下，证明了遗憾和最终迭代保证，实证表明方法在少量偏好查询下表现优异。

Conclusion: 该方法在计算效率和查询复杂度上具有优势，适用于实际部署中的并行反馈收集。

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [92] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Main category: cs.LG

TL;DR: 本文提出了一种新型网络架构，利用周期性模式提升信号表示的外推性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于坐标的多层感知机（MLP）在训练区域外泛化能力不足的问题，尤其是在信号具有周期性时。

Method: 设计了一种网络架构，从测量数据中提取周期性模式，并利用这些信息表示信号。

Result: 实验表明，该方法在微分方程周期解学习、时间序列插补和外推任务中表现优异。

Conclusion: 所提方法显著提升了周期性信号的外推性能，具有广泛的应用潜力。

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [93] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM是一种多模态过程奖励模型，用于评估复杂推理问题中每一步的奖励分数。通过利用弱和强完成者之间的预测一致性生成高质量标签数据，仅需5000样本即可高效运行，并在多个场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统自动标注方法（如蒙特卡洛估计）生成噪声标签且计算成本高，因此需要高效生成高质量过程标注数据的方法。

Method: 提出利用弱和强完成者的预测一致性作为可靠过程标签的生成标准，并采用ORM初始化和负数据上采样策略提升PRM性能。

Result: 在多个场景和基准测试中表现优异，如WeMath和MathVista分别提升10.2和7.1分，并在VisualProcessBench中超越之前SoTA 3.9 F1分数。

Conclusion: Athena-PRM能高效评估推理步骤正确性，显著提升模型性能，适用于多种应用场景。

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [94] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Main category: cs.LG

TL;DR: STOAT是一种新颖的空间-时间概率因果推断网络框架，用于空间-时间因果时间序列的概率预测，通过结合空间关系矩阵和深度概率模型，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立建模空间和时间动态，忽略了因果驱动的概率预测，限制了预测能力。

Method: STOAT扩展了因果推断方法，引入空间关系矩阵编码区域间依赖关系，并通过深度概率模型估计分布参数。

Result: 在COVID-19数据上的实验表明，STOAT在关键指标上优于现有模型，尤其在空间依赖强的区域表现突出。

Conclusion: STOAT为复杂空间-时间任务（如疫情管理）提供了一个通用框架，结合了因果推断和地理空间概率预测。

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [95] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Main category: cs.LG

TL;DR: MOORL是一种结合离线与在线强化学习的混合框架，通过元策略无缝适应两种数据，提升样本效率和探索能力。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中OOD动作限制性能与泛化的问题，同时避免传统混合方法的高计算复杂度。

Method: 提出MOORL框架，利用元策略结合离线与在线数据，实现高效探索与稳定Q函数学习。

Result: 在28个任务上验证，MOORL优于现有离线与混合RL基线，性能稳定且计算开销低。

Conclusion: MOORL展示了实际应用的潜力，为复杂领域中的强化学习提供了高效解决方案。

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [96] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Main category: cs.LG

TL;DR: 本文研究了基础模型（如ConvNeXt、EVA和BEiT）的校准行为，发现它们在分布内预测中倾向于不自信，导致校准误差较高，但在分布偏移下表现更好。后处理校准技术对分布内数据有效，但在严重分布偏移下效果下降。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在高风险应用中需要可靠的校准，但基础模型的校准特性尚未充分研究。

Method: 通过实证分析基础模型的校准行为，并测试后处理校准技术的效果。

Result: 基础模型在分布内预测中不自信，校准误差高；在分布偏移下校准表现更好。后处理校准在分布内有效，但在严重偏移下效果不佳。

Conclusion: 基础模型的校准行为复杂，挑战了持续改进的传统观点。

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [97] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Main category: cs.LG

TL;DR: 论文提出两种快速随机算法解决低秩张量逼近问题，并开发了一种新的非凸建模框架，用于大规模张量恢复，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有张量恢复方法未考虑张量尺度变化对结构特性的影响，且计算成本高。

Method: 结合Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略，提出随机化算法和非凸建模框架。

Result: 理论证明了近似误差估计的准确性，实验显示方法在大规模张量数据上实用且高效。

Conclusion: 所提方法在计算效率和恢复效果上优于现有技术。

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [98] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Main category: cs.LG

TL;DR: SparseSSM是一种无需训练的剪枝框架，针对状态空间模型（如Mamba）设计，通过近似二阶显著性评分和组件敏感性分析，实现高效剪枝，50%权重剪枝后零精度损失。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法针对注意力块设计，无法处理状态空间模块的时间共享和离散化特性，因此需要一种专门针对状态空间架构的剪枝方法。

Method: 提出SparseSSM框架，结合近似二阶显著性评分（Hessian-trace信息）和组件敏感性分析，支持半结构化和结构化稀疏化。

Result: 在Mamba模型中剪枝50%权重后，零精度损失，达到当前最先进的剪枝效果。

Conclusion: SparseSSM是首个针对状态空间架构的无训练剪枝框架，高效且可扩展，为Mamba类模型的部署提供了新方向。

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [99] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Main category: cs.LG

TL;DR: 提出了一种基于几何（Clifford）代数的等变神经网络架构GLGENN，其参数轻量化且性能优于或匹配基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有等变神经网络在处理伪正交变换时的局限性，并减少过拟合倾向。

Method: 利用几何代数的基本结构和操作，提出权重共享参数化技术。

Result: 在多个基准任务中表现优异，且参数数量显著减少。

Conclusion: GLGENN是一种高效且参数轻量化的等变神经网络架构。

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [100] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Main category: cs.LG

TL;DR: LLMs用于生成合成表格数据时，可能因上下文示例中的统计偏差导致全局数据失真，甚至被恶意利用注入偏见，影响下游分类器的公平性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在数据稀缺场景下生成合成表格数据时，上下文示例中的统计偏差如何传播并影响数据分布，揭示潜在的安全隐患。

Method: 系统分析上下文示例中的统计偏差对合成数据分布的影响，并模拟恶意注入偏见的场景。

Result: 发现即使轻微的上下文偏差也会导致全局数据失真，恶意注入偏见会显著损害下游分类器的公平性。

Conclusion: LLM基于上下文提示的数据生成管道在敏感领域存在新的安全漏洞，需谨慎使用。

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [101] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedVLMBench是首个系统性的联邦学习视觉语言模型（VLM）微调基准，涵盖多种架构、策略和任务，揭示了数据异质性和多任务优化的关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有VLM微调方法依赖集中训练，难以满足隐私敏感领域（如医疗）的需求，且缺乏联邦学习下的全面评估基准。

Method: 提出FedVLMBench，整合两种VLM架构、四种微调策略、五种FL算法、六个多模态数据集，覆盖单任务和多任务场景。

Result: 发现2层MLP连接器与并发调优是FL中编码器VLM的最佳配置，且FL方法对视觉任务的数据异质性更敏感。

Conclusion: FedVLMBench为隐私保护的多模态基础模型联邦训练提供了标准化平台和实证指导。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [102] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Main category: cs.LG

TL;DR: WAFFLE是一种基于小波和傅里叶变换的联邦学习异常客户端检测算法，能在训练前识别恶意客户端，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中异常客户端（如数据分布不具代表性或传感器故障）会显著降低模型性能，但如何在保护隐私的前提下检测这些客户端仍是一个挑战。

Method: 通过小波散射变换（WST）或傅里叶变换生成低维任务无关嵌入，利用轻量级检测器在公共数据集上训练，实现高效检测。

Result: 实验表明，WAFFLE在检测准确性和下游分类性能上优于现有方法，验证了其作为预训练替代在线检测策略的有效性。

Conclusion: WAFFLE是一种高效、隐私保护的异常客户端检测方法，特别适合联邦学习场景。

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [103] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein距离的超图神经网络（WHGNN），通过将节点和超边邻域视为分布，并利用Sliced Wasserstein Pooling进行信息聚合，显著提升了节点分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络多采用两阶段框架，信息聚合方式简单（如均值或求和），无法捕捉高阶统计特性。本文旨在通过引入Wasserstein距离，保留分布的几何特性，从而更有效地建模超图关系。

Method: 提出Wasserstein Hypergraph Neural Network（WHGNN），将节点和超边邻域建模为分布，并使用Sliced Wasserstein Pooling进行信息聚合，以捕捉分布的几何特性。

Result: 实验结果表明，WHGNN在多个真实数据集上的节点分类任务中表现优异，显著优于传统聚合方法。

Conclusion: WHGNN通过引入Wasserstein距离和分布建模，能够更有效地捕捉超图的高阶关系，为超图神经网络提供了新的研究方向。

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [104] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Main category: cs.LG

TL;DR: TRIDENT是一种无需重新训练的通用推理算法，确保LLM输出满足线性时序逻辑约束。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成和分类任务中表现优异，但无法保证输出满足时序约束。

Method: 将LTLf公式编译为DFA，引导受限波束搜索，动态屏蔽违规路径并重新排序。

Result: TRIDENT保证约束满足，并提升输出质量，在图像流分类和文本生成任务中表现优异。

Conclusion: TRIDENT高效且高质量地满足时序约束，优于现有方法。

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [105] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.LG

TL;DR: Auto-Compressing Networks (ACNs) 通过长前馈连接替代短残差连接，实现自动压缩信息，提升表示质量并减少冗余。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络中深度增加带来的计算冗余问题，同时提升表示质量。

Method: 引入ACNs架构，通过长前馈连接实现自动压缩信息，动态优化层间训练模式。

Result: ACNs在噪声鲁棒性、低数据性能、迁移学习和灾难性遗忘方面表现优异，参数减少30-80%且保持精度。

Conclusion: ACNs是一种高效的自适应神经网络架构，能自动调整计算复杂度并学习鲁棒表示。

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [106] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: AtmosMJ挑战了传统观点，证明在标准经纬度网格上也能实现长期稳定的天气预测，无需依赖非标准空间域转换。


<details>
  <summary>Details</summary>
Motivation: 探讨是否能在标准经纬度网格上实现长期稳定的天气预测，而非依赖非标准空间域转换。

Method: 提出AtmosMJ，一种直接在ERA5数据上运行的深度卷积网络，采用新型Gated Residual Fusion机制防止误差累积。

Result: AtmosMJ能稳定预测约500天，10天预测精度与Pangu-Weather和GraphCast相当，且训练成本极低。

Conclusion: 高效架构设计比非标准数据表示更能实现稳定且计算高效的长期天气预测。

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [107] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种多模态图大语言模型（MG-LLM）框架，旨在统一和泛化多模态图数据与任务，并探讨了其关键特性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法泛化到不同多模态图数据与任务，因此需要一种统一框架来解决这一问题。

Method: 提出了MG-LLM框架，包括多模态图数据的统一表示、任务处理能力、上下文学习、自然语言交互和推理等特性。

Result: 总结了多模态图数据集，并提出了实现MG-LLM的关键挑战和未来方向。

Conclusion: MG-LLM有望推动多模态图数据与任务的泛化研究。

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [108] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Main category: cs.LG

TL;DR: 提出了一种基于特征工程原则的认知架构，用于提升机器学习模型监控输出的可解释性，通过三个关键步骤（重构、分解和编译）优化决策过程。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型监控方法输出冗长且难以解释，影响决策效率。

Method: 采用特征工程原则，通过重构、分解和编译三个步骤优化LLM代理的监控输出。

Result: 实验表明，该方法显著提高了监控输出的准确性和可解释性，优于多种基线方法。

Conclusion: 结合特征工程和选择性LLM利用的决策支持系统，能够提供更清晰、可操作的监控洞察。

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [109] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Main category: cs.LG

TL;DR: Load-aware Tram-FL通过引入训练调度机制，优化分散式联邦学习的计算和通信负载，显著减少训练时间并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 在分散式联邦学习中，计算和通信负载的不均衡会导致训练效率低下，因此需要一种机制来优化调度。

Method: 将全局优化问题分解为节点级子问题，引入方差约束以平衡非IID数据分布下的数据利用率，并通过目标函数最小化训练延迟。

Result: 在MNIST和CIFAR-10上的仿真结果表明，Load-aware Tram-FL显著减少了训练时间并加速了收敛。

Conclusion: Load-aware Tram-FL通过优化调度机制，有效提升了分散式联邦学习的效率。

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [110] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Main category: cs.LG

TL;DR: 本文提出了一个统一的对比学习框架，基于正负对嵌入的余弦相似性分析，揭示了在完整批次和小批次设置中的关键问题，并提出了一种辅助损失函数以改进小批次训练的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法缺乏一个系统性的框架来解释广泛的对比损失目标，本文旨在填补这一空白。

Method: 通过分析正负对嵌入的余弦相似性，提出统一框架，并在小批次训练中引入辅助损失以减少负对相似性的方差。

Result: 实验表明，提出的辅助损失在小批次训练中能显著提升对比学习方法的性能。

Conclusion: 本文的统一框架和辅助损失为对比学习提供了新的理论支持和实践改进。

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [111] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 论文提出了一种针对连续依赖数据的对比自监督学习框架，通过硬和软相似性度量改进传统方法，并在时空任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统对比自监督学习方法假设样本间语义独立，不适用于具有复杂相关性的依赖数据（如时空数据）。

Method: 提出依赖感知的对比学习框架，引入硬和软相似性度量，推导出适应样本依赖关系的相似性矩阵和损失函数。

Result: 在UEA和UCR基准测试中分别提升4.17%和2.08%的准确率，在干旱分类任务中ROC-AUC得分提高7%。

Conclusion: 依赖感知的对比学习方法能有效捕捉时空依赖性，优于现有方法。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [112] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Main category: cs.LG

TL;DR: 本文提出了一种针对本地隐私图学习协议的数据投毒攻击，填补了该领域的研究空白，并探讨了防御策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实中的图数据常包含敏感信息，本地隐私图学习协议虽能保护隐私，但可能面临数据投毒攻击的威胁，需研究其安全性和防御措施。

Method: 攻击者通过注入虚假用户并操纵其与真实用户建立链接，发送精心设计的数据以破坏图学习的效用。

Result: 攻击在理论和实验上均被证明有效，现有防御策略效果有限。

Conclusion: 需开发更鲁棒的防御机制以保障隐私图学习的安全性。

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [113] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Main category: cs.LG

TL;DR: 论文提出ProjNCE，一种统一监督与自监督对比学习目标的损失函数，并证明其为有效的互信息下界，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 监督对比学习（SupCon）在互信息视角下的研究较少，本文旨在填补这一空白，提出更灵活的损失函数。

Method: 引入ProjNCE，通过投影函数和负对调整项统一监督与自监督目标，并探索基于质心的投影方法。

Result: ProjNCE在多个数据集和设置下均优于SupCon和标准交叉熵训练。

Conclusion: ProjNCE从互信息解释和投影设计两方面改进了SupCon，为对比学习提供了广泛适用的优化方案。

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [114] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Main category: cs.LG

TL;DR: 本文提出两种基于社会选择理论的度量子集选择方法：位置代表性和位置比例性，并通过理论证明和实际案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM评估中度量子集选择缺乏明确定义的问题，提高效率和可解释性。

Method: 引入位置代表性和位置比例性两种形式化概念，并研究其理论界限和实际应用。

Result: 证明了在最坏情况下满足这些性质所需的最小度量数量，并通过案例研究验证了方法的实用性。

Conclusion: 提出的方法为度量子集选择提供了理论基础和实践指导，适用于LLM评估和其他领域。

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [115] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Main category: cs.LG

TL;DR: 本文探讨了稀疏线性常微分方程（ODE）的可识别性问题，发现与密集矩阵不同，稀疏系统在实际相关稀疏度下存在不可识别性，并提供了概率下界。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性ODE在生物、社会和物理系统中具有实际意义，但其可识别性尚未充分研究。本文旨在填补这一空白。

Method: 通过理论分析和实证研究，本文研究了稀疏线性ODE的可识别性及其在实际估计方法中的表现。

Result: 研究发现稀疏系统在实际相关稀疏度下存在不可识别性，且理论限制无法通过归纳偏置或优化动态解决。

Conclusion: 研究呼吁重新思考数据驱动动态系统建模的预期，并为评估学习到的线性ODE的可信度提供了量化方法。

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [116] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Main category: cs.LG

TL;DR: 论文提出了一种名为WoLA的加权损失方法，用于在联邦学习中解决数据异构性下识别拜占庭梯度的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在隐私保护方面具有吸引力，但存在拜占庭参与者贡献有害梯度的安全威胁。现有方法在异构数据环境下难以区分诚实梯度与拜占庭梯度。

Method: 引入Worker Label Alignment Loss (WoLA)，通过加权损失对齐诚实工作者的梯度，便于识别拜占庭梯度。

Result: WoLA在异构数据环境下显著优于现有方法，并通过理论和实验验证了其有效性。

Conclusion: WoLA为联邦学习中拜占庭攻击的防御提供了新的解决方案，尤其在数据异构性强的场景中表现突出。

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [117] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Main category: cs.LG

TL;DR: 论文提出了一种名为GGC的框架，通过图自动编码器压缩图的节点和特征维度，以提升下游分类任务的性能，并在量子或经典分类器中应用。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络（GNNs）处理大图时的高内存需求和稀疏矩阵操作效率低的问题，以及量子计算（QC）硬件对数据维度的限制。

Method: 使用图自动编码器进行节点和特征维度的压缩，压缩过程以提升下游分类任务性能为目标。

Result: 在Jet Tagging任务中，GGC框架表现优于单独使用自动编码器或经典GNN分类器，并支持在真实数据集上测试新型QGNN结构。

Conclusion: GGC框架有效解决了大图处理和量子硬件限制问题，同时提升了分类性能，为QGNN的实际应用提供了支持。

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [118] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的微波谐振传感器方法，用于根据介电特性分类油样，随机森林分类器准确率达99.41%。


<details>
  <summary>Details</summary>
Motivation: 油的分子组成决定其介电特性，导致传感器谐振频率和振幅变化，可用于分类。

Method: 利用微波谐振传感器非破坏性捕获油样介电特性，提取特征后输入多种机器学习分类器。

Result: 随机森林分类器表现最佳，分类准确率达99.41%。

Conclusion: 该方法高效、紧凑，适用于工业环境中的实时油样分类。

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [119] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文研究了单指标模型的学习问题，提出用球谐函数而非Hermite多项式作为自然基，以捕捉问题的旋转对称性。提出了两种估计器，分别优化样本复杂度或运行时间。


<details>
  <summary>Details</summary>
Motivation: 探索单指标模型学习的统计和计算复杂性，特别是输入分布的旋转对称性对学习的影响。

Method: 提出基于球谐函数的新视角，设计两种估计器：基于张量展开和在线SGD。

Result: 在任意球对称输入分布下，分别实现了最优样本复杂度或最优运行时间，并发现同时优化两者可能不可行。

Conclusion: 球谐函数为单指标模型学习提供了更自然的基，理论不仅复现了高斯输入下的结果，还揭示了新现象。

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [120] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Main category: cs.LG

TL;DR: 提出了一种基于因果表示学习的可解释气候模型模拟器，结合物理信息和贝叶斯滤波，实现了长期稳定的自回归模拟。


<details>
  <summary>Details</summary>
Motivation: 传统气候模型计算成本高，机器学习方法缺乏物理因果关系的整合。

Method: 开发了一种基于因果表示学习的模拟器，结合物理信息和贝叶斯滤波。

Result: 模拟器能准确学习气候动态，并在合成数据集和实际气候模型数据中验证了其各组件的重要性。

Conclusion: 该方法为高效、可解释的气候模拟提供了新途径。

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [121] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: 论文研究了VQVAE在对抗攻击下的防御能力，通过分析其对高信噪比射频数据的攻击抑制效果，发现VQVAE能显著降低攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究VQVAE在对抗攻击下的表现，探索其在数字无线电波形中的防御潜力。

Method: 设计并比较了保留相位和不保留相位的对抗攻击，测试分类器在原始数据和VQVAE重建数据上的准确性。

Result: VQVAE显著降低了攻击的有效性，并在潜在空间分布中发现了有助于检测攻击的特性。

Conclusion: VQVAE在对抗攻击中表现出良好的防御能力，其潜在空间的特性可能有助于攻击检测。

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [122] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Main category: cs.LG

TL;DR: 本文提出了一种名为DNA的可解释强化学习新方法，通过生成多样化的近最优轨迹选项，帮助人类用户理解代理的选择。


<details>
  <summary>Details</summary>
Motivation: 旨在通过提供多样化的轨迹选项，增强强化学习代理的可解释性，并支持人类用户的决策。

Method: 利用奖励塑造和局部改进的Q学习问题，求解具有保证的ε最优性的多样化策略。

Result: 在模拟中成功生成了具有显著差异的策略，验证了方法的有效性。

Conclusion: DNA不仅提升了可解释性，还为强化学习中的探索和自适应规划开辟了新途径。

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [123] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Main category: cs.LG

TL;DR: 论文提出了一种新的隐私攻击方法Apollo，针对机器遗忘（MU）场景，仅需访问遗忘模型的标签输出即可推断数据样本是否被遗忘，攻击效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘（MU）虽能保护隐私和合规性，但也可能增加模型的攻击面。现有攻击方法依赖较弱威胁模型（需访问原始和遗忘模型），限制了实际可行性。

Method: 提出Apollo攻击方法，基于严格威胁模型（仅需访问遗忘模型的标签输出），推断数据样本是否被遗忘。

Result: 实验表明，Apollo在访问权限较少的情况下，仍能高精度推断遗忘样本的成员状态。

Conclusion: Apollo攻击方法在严格威胁模型下有效，揭示了机器遗忘可能带来的新隐私风险。

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [124] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Main category: cs.LG

TL;DR: 论文研究了概率矩阵分解（PMF）的后验分布近似问题，比较了MCMC和VI两种贝叶斯推断方法在MovieLens数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分解无法量化不确定性，而PMF虽然引入概率分布，但后验分布计算困难。

Method: 采用MCMC和VI两种贝叶斯推断方法近似后验分布。

Result: 实验表明，VI收敛更快，而MCMC的后验估计更准确。

Conclusion: VI适合快速收敛场景，MCMC适合高精度需求场景。

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [125] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文提出了一种样本高效的算法，用于在信息不对称和知识转移的背景下，学习系统动态并实现强化学习中的最优策略。


<details>
  <summary>Details</summary>
Motivation: 信息不对称和知识转移是多智能体系统中的常见挑战，尤其是在经济学和社会科学中。本文旨在解决如何在这些复杂环境中高效学习系统动态的问题。

Method: 提出了一种样本高效的算法，通过非独立同分布（non-i.i.d.）动作学习混杂变量，并在强化学习的在线战略交互模型中实现知识转移。

Result: 算法能够以紧密的样本复杂度O(1/ε²)学习ε-最优策略。

Conclusion: 该方法在信息不对称和知识转移的背景下，有效解决了系统动态学习和策略优化问题。

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [126] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: 论文提出了一种名为CLAReps的潜在表示方法，用于解决条件扩散模型（CDMs）在生成任务中因特征纠缠导致的问题，并通过CaDistill方法实现了高效的类别知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型（CDMs）在生成任务中表现出色，但其建模能力会导致类别特征与无关上下文的纠缠，影响特征的鲁棒性和可解释性。

Method: 通过识别CLAReps（保留类别核心信息、去除无关信号的潜在表示），并开发CaDistill方法，利用CLAReps进行特征蒸馏。

Result: 学生模型仅需10%的训练数据即可学习到核心类别知识，表现出更强的对抗鲁棒性和泛化能力。

Conclusion: CDMs不仅可以作为图像生成器，还能作为紧凑、可解释的教师模型，推动鲁棒表示学习。

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [127] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Main category: cs.LG

TL;DR: Multiverse是一种新型生成模型，通过MapReduce范式实现并行生成，性能与主流AR-LLMs相当，且具有更高的效率和扩展性。


<details>
  <summary>Details</summary>
Motivation: 受AR-LLMs中隐式并行性的启发，开发一种原生支持并行生成的模型，以提高效率和性能。

Method: 采用MapReduce范式，分为Map（任务分解）、Process（并行执行）和Reduce（结果合成）三阶段；设计了Multiverse Attention和Multiverse Engine支持并行推理。

Result: Multiverse-32B在3小时微调后，性能与同规模AR-LLMs相当（AIME24 & 25得分54%和46%），效率提升2倍。

Conclusion: Multiverse展示了并行生成模型的潜力，开源生态系统为后续研究提供了完整支持。

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


### [128] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）在描述概率分布与生成样本之间的差距，提出了一种名为Verbalized Rejection Sampling（VRS）的方法，通过自然语言改进采样偏差，并在理论上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs能够准确描述概率分布，但在生成可靠样本方面表现不佳，限制了其在需要随机性的任务中的应用。

Method: 提出了Verbalized Rejection Sampling（VRS），一种基于自然语言的拒绝采样方法，通过提示LLM对样本进行推理和接受/拒绝。

Result: VRS显著减少了采样偏差，理论分析表明其在温和假设下优于直接采样。

Conclusion: 研究表明，经典概率工具可以通过自然语言嵌入LLM工作流，提高可靠性，而无需访问模型内部或复杂的提示工程。

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [129] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Main category: physics.med-ph

TL;DR: 研究探讨了使用强化学习（RL）在高剂量率前列腺近距离放射治疗中自动生成针位和停留时间的可行性，结果显示RL计划与传统方法效果相当或更优，且使用更少的针。


<details>
  <summary>Details</summary>
Motivation: 目前针位放置仅依赖医生经验，RL方法可减少手术时间并确保计划质量的一致性。

Method: 训练RL代理调整针位和停留时间，基于预定义奖励函数优化，使用11例患者数据（1例训练，10例测试）。

Result: RL计划与传统计划在前列腺覆盖和直肠剂量上相似，但前列腺热点和尿道剂量更低，且平均少用2根针。

Conclusion: RL方法可行，能标准化计划、减少临床差异并改善患者结果。

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [130] [Almost-Optimal Local-Search Methods for Sparse Tensor PCA](https://arxiv.org/abs/2506.09959)
*Max Lovig,Conor Sheehan,Konstantinos Tsirkas,Ilias Zadik*

Main category: math.ST

TL;DR: 本文提出了一系列局部搜索方法，旨在填补稀疏张量主成分分析（PCA）中局部计算方法的理论空白，并在多个模型范围内达到已知多项式时间算法的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 局部搜索方法在统计应用中广泛使用，但其理论基础相对不足，尤其是在稀疏张量PCA中，现有研究表明局部计算方法存在性能差距。

Method: 提出包括标准贪婪算法、随机贪婪算法及其变体（如随机阈值变体）的局部搜索框架，通过引入随机高斯阈值来优化算法性能。

Result: 新方法在多个模型范围内成功填补了局部计算方法的性能差距，并超越了现有局部马尔可夫链方法的性能。

Conclusion: 随机阈值的引入为随机贪婪算法的轨迹分析提供了数学基础，可能对其他研究领域也有独立价值。

Abstract: Local-search methods are widely employed in statistical applications, yet
interestingly, their theoretical foundations remain rather underexplored,
compared to other classes of estimators such as low-degree polynomials and
spectral methods. Of note, among the few existing results recent studies have
revealed a significant "local-computational" gap in the context of a
well-studied sparse tensor principal component analysis (PCA), where a broad
class of local Markov chain methods exhibits a notable underperformance
relative to other polynomial-time algorithms. In this work, we propose a series
of local-search methods that provably "close" this gap to the best known
polynomial-time procedures in multiple regimes of the model, including and
going beyond the previously studied regimes in which the broad family of local
Markov chain methods underperforms. Our framework includes: (1) standard greedy
and randomized greedy algorithms applied to the (regularized) posterior of the
model; and (2) novel random-threshold variants, in which the randomized greedy
algorithm accepts a proposed transition if and only if the corresponding change
in the Hamiltonian exceeds a random Gaussian threshold-rather that if and only
if it is positive, as is customary. The introduction of the random thresholds
enables a tight mathematical analysis of the randomized greedy algorithm's
trajectory by crucially breaking the dependencies between the iterations, and
could be of independent interest to the community.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [131] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 本文研究了可学习特征提取前端在ASR系统中的正则化方法，通过音频扰动和STFT域掩码改进性能，缩小与传统方法的差距。


<details>
  <summary>Details</summary>
Motivation: 神经前端在ASR系统中表现不如传统方法，主要原因是过拟合问题，因此需要研究正则化方法。

Method: 研究了音频扰动方法，并提出在STFT域进行掩码的改进版SpecAugment。

Result: 结合两种正则化方法后，可学习特征与传统特征的性能差距显著缩小。

Conclusion: 正则化方法有效提升了可学习特征前端的性能，使其接近传统方法。

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [132] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: 论文研究了多智能体任务分配问题中，异质性团队何时优于同质性团队，并通过奖励设计和多智能体强化学习验证了异质性的优势。


<details>
  <summary>Details</summary>
Motivation: 探讨异质性团队在任务分配中的优势，并研究奖励设计如何促进异质性。

Method: 结合广义聚合算子和多智能体强化学习（MARL），提出Heterogeneous Environment Design（HED）算法。

Result: 理论证明了算子曲率决定异质性优势，HED算法在实验中验证了理论预测。

Conclusion: 研究为理解行为多样性何时带来显著优势提供了理论和实验支持。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [133] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Main category: eess.SY

TL;DR: 本文综述了AI和ML在6G-V2X通信中的最新进展，重点介绍了深度学习、强化学习、生成学习和联邦学习等技术，并探讨了其在资源分配、波束成形、交通管理和安全管理中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，V2X通信需要更智能化的解决方案，AI和ML因其在多个领域的卓越表现成为优化V2X通信的关键技术。然而，缺乏对近期研究的系统性总结，本文旨在填补这一空白。

Method: 本文通过综述近两年的研究，分析了深度学习、强化学习、生成学习和联邦学习等技术在6G-V2X中的应用，并探讨了技术挑战和未来方向。

Result: AI尤其是生成学习在提升6G-V2X系统的性能、适应性和智能化方面表现出显著潜力。

Conclusion: 本文为研究人员、工程师和政策制定者提供了关于AI驱动的6G-V2X生态系统发展的宝贵见解，并指出了未来的研究方向。

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [134] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Main category: cs.IR

TL;DR: 提出了一种基于用户-物品二分图投影的简单有效方法，用于推荐互补产品，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 互补产品推荐能提升用户体验和零售销售额，但用户-物品交互数据稀疏且噪声多，导致推荐困难。

Method: 通过将用户-物品二分图投影为有向加权图，推断互补关系。

Result: 在不同基准测试中，平均性能分别比序列和基于图的推荐方法提升43%和38%。

Conclusion: 该方法简单但有效，显著提升了互补产品推荐的性能。

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [135] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Main category: q-bio.GN

TL;DR: 提出一种概率框架，用于推断未测序病例与已知序列之间的遗传距离，支持基因组数据的不确定性增强。


<details>
  <summary>Details</summary>
Motivation: 病原体基因组数据在空间模型中有用，但测序覆盖率不足限制了其效用。

Method: 基于时间感知的进化距离建模，估计未测序病例与已知序列的遗传距离，无需序列比对或已知传播链。

Result: 应用于高致病性禽流感A/H5病例，支持基因组数据的不确定性增强和时空建模。

Conclusion: 该方法增强了基因组数据在时空建模中的整合能力。

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [136] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: ComfyUI-R1是一个用于自动化生成AI内容工作流的大型推理模型，通过两阶段训练框架（CoT微调和强化学习）显著提升了工作流生成的格式有效性和结构完整性。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的工作流定制需要高度专业知识，学习曲线陡峭，ComfyUI-R1旨在解决这一问题。

Method: 使用4K工作流数据集构建长链推理数据，通过CoT微调和强化学习训练模型，奖励机制确保格式和结构有效性。

Result: 7B参数模型在格式有效性、节点和图形级别F1分数上显著优于GPT-4o和Claude系列。

Conclusion: 长链推理和代码化工作流在AI艺术创作中具有潜力，ComfyUI-R1展示了其复杂工作流合成的优势。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [137] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: RuleReasoner是一种通过强化学习增强的小型推理模型，能够有效处理规则推理任务，并在多样任务和领域中表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决小型推理模型（SRMs）在规则推理任务中的有效性和泛化能力问题，避免依赖大型推理模型（LRMs）。

Method: 提出RuleReasoner方法，结合领域感知的动态采样技术，通过历史奖励更新采样权重，实现灵活的在线学习。

Result: RuleReasoner在分布内（ID）和分布外（OOD）任务中显著优于前沿大型推理模型（如OpenAI-o1），且计算效率更高。

Conclusion: RuleReasoner为小型推理模型提供了一种高效、灵活的规则推理解决方案，具有广泛的应用潜力。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [138] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（Mistral-7B）的配对困惑度方法，用于检测阿尔茨海默病（AD），准确率较现有方法提升3.33%-6.35%，并展示了其可解释性和潜在应用。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）常伴随语言能力下降，现有检测方法的准确性和可解释性不足，需要改进。

Method: 采用指令跟随版本的Mistral-7B大语言模型，扩展配对困惑度方法，并通过微调模型分析语言模式。

Result: 准确率平均提升3.33%和6.35%，且决策边界清晰可解释。模型还学习了AD患者的特殊语言模式。

Conclusion: 该方法显著提升了AD检测的准确性和可解释性，并为模型解释和数据增强提供了新思路。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [139] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: RePO通过利用多样化的回放策略从回放缓冲区中检索离策略样本，显著提升了大型语言模型的优化效率，相比GRPO在性能和计算效率上有明显改进。


<details>
  <summary>Details</summary>
Motivation: GRPO方法因依赖多个同策略输出导致计算成本高且数据效率低，RePO旨在通过引入离策略样本优化策略。

Method: RePO采用多样化的回放策略从回放缓冲区中检索离策略样本，以更广泛的样本集优化策略。

Result: 在七个数学推理基准测试中，RePO显著提升了模型性能，计算成本仅增加15%，但有效优化步骤增加了48%。

Conclusion: RePO通过结合离策略样本显著提升了优化效率和性能，为大型语言模型的强化学习提供了更高效的解决方案。

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [140] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: POET方法通过截断响应长度解决DAAs中的奖励-生成差距，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: DAAs（如DPO和SimPO）在训练目标和生成性能间存在奖励-生成差距，影响模型表现。

Method: 提出POET方法，截断响应至等长，优化DAAs目标，关注前缀标记。

Result: POET在DPO和SimPO上表现提升，AlpacaEval 2得分提高15.6分。

Conclusion: 解决奖励-生成差距对提升DAAs性能至关重要。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [141] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 该研究通过结合计算方法和专家知识，分析了YouTube上自杀行为的表现形式，发现了一些与自杀尝试相关的行为指标，并揭示了分享动机的差异。


<details>
  <summary>Details</summary>
Motivation: 自杀是西方国家的主要死因之一，社交媒体数据为研究自杀行为提供了新视角。

Method: 采用三种互补方法：自下而上的计算分析、混合方法和专家驱动的自上而下分析，对181个有自杀尝试的YouTube频道和134个对照频道进行了研究。

Result: 发现五个与自杀尝试相关的主题，其中两个表现出时间相关性；专家未发现的平台特定指标（YouTube参与度）具有显著意义；分享动机在尝试前后存在差异。

Conclusion: 结合多种方法可以更全面地理解自杀行为，弥合数字行为与临床知识之间的差距。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [142] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: 综述探讨了知识图谱（KGs）与大语言模型（LLMs）的协同作用，分为KG增强LLMs和LLM增强KGs两类，强调可扩展性、计算效率和数据质量，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 通过整合KGs的结构化知识提升LLMs的事实基础和推理能力，同时利用LLMs优化KGs的构建和查询。

Method: 系统分析现有方法，分为KG增强LLMs（提升推理、减少幻觉）和LLM增强KGs（促进KG构建与查询）两类。

Result: 揭示了结构化知识整合的互惠性，并指出当前研究的不足。

Conclusion: 未来研究方向包括神经符号整合、动态KG更新、数据可靠性和伦理问题，为复杂知识任务提供支持。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [143] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: RAPL是一个新颖的框架，通过两阶段标注、模型无关的图转换和基于路径的推理策略，提升了知识图谱问答中的图检索效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有检索增强生成（RAG）方法在知识图谱问答（KGQA）中泛化能力不足的问题。

Method: 提出RAPL框架，包括两阶段标注策略、模型无关的图转换方法和基于路径的推理策略。

Result: RAPL在性能上优于现有方法2.66%-20.34%，并显著缩小了不同LLM推理器之间的性能差距。

Conclusion: RAPL通过结构化检索和推理策略，显著提升了知识图谱问答的检索能力和泛化性。

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [144] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT框架通过Hint-Engineering优化LRM与代码解释器的交互，显著提升数学推理能力并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型（LRM）在复杂数学运算中效率低或准确性不足的问题，同时克服直接结合代码解释器（CI）的技术挑战。

Method: 通过Hint-Engineering合成代码集成推理数据，结合监督微调、拒绝微调和强化学习对模型进行后训练。

Result: 在多个数学推理数据集上，模型性能提升4%至8%，计算开销减少30%至50%。

Conclusion: CoRT框架有效提升了LRM与CI的交互效率，为复杂推理任务提供了实用解决方案。

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [145] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: PersonaLens是一个用于评估任务导向AI助手个性化能力的综合基准，通过用户代理和法官代理揭示当前LLM助手在个性化能力上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能全面评估个性化任务导向助手的复杂性，因此需要开发更全面的评估工具。

Method: 引入PersonaLens基准，包含多样化用户配置和两个LLM代理（用户代理和法官代理），用于模拟对话和评估个性化能力。

Result: 实验显示当前LLM助手在个性化能力上存在显著差异。

Conclusion: PersonaLens为提升对话AI系统的个性化能力提供了重要见解。

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [146] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Main category: math.OC

TL;DR: 本文通过理论和实证评估了不同一阶优化方法在梯度计算存在相对不精确性时的鲁棒性，提出了改进方法并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究梯度计算不精确（如梯度压缩）对优化方法性能的影响，特别是在大规模GPU问题中。

Method: 分析了三种优化方法：恒定步长梯度下降、长步方法和加速方法，提出半启发式缩短因子改进后两种方法。

Result: 加速方法在实际不精确问题中表现比预期更鲁棒，缩短因子显著改善了长步方法的性能。

Conclusion: 所有改进后的方法在不精确环境下均表现良好，具有应用潜力。

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [147] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 本文提出了一种基于Transformer的简单框架，用于运动插值任务，强调数据建模选择对性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 运动插值是动画师的重要工具，但现有机器学习方法依赖复杂模型。本文旨在探索简单模型是否能通过数据优化实现高质量动画。

Method: 采用单一Transformer编码器框架，重点研究数据建模选择（如数据量、姿态表示和速度输入特征）对性能的影响。

Result: 实验表明，增加数据量、优化姿态表示和引入速度特征可显著提升运动插值质量，挑战了模型复杂性决定性能的假设。

Conclusion: 本文证明数据优化比模型复杂性更能提升运动插值质量，为动画领域提供了数据为中心的解决方案。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [148] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: DGS-LRM是首个基于单目视频的动态场景3D高斯点云预测方法，解决了动态场景重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法多限于静态场景，动态场景重建因数据稀缺和3D表示困难而具有挑战性。

Method: 提出大规模合成数据集、可变形3D高斯点云表示及大型Transformer网络，实现实时动态重建。

Result: DGS-LRM在重建质量上媲美优化方法，优于现有预测方法，且适用于长程3D跟踪。

Conclusion: DGS-LRM为动态场景重建提供了高效、高质量的解决方案，适用于实际应用。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [149] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: 本文提出了一种混合量子-经典架构，用于Devanagari手写数字识别，结合CNN和量子电路，在低资源语言环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: Devanagari等区域脚本的手写数字识别对多语言文档数字化、教育工具和文化遗产保护至关重要，但其复杂结构和有限标注数据对传统模型构成挑战。

Method: 采用卷积神经网络（CNN）提取空间特征，结合10量子比特变分量子电路（VQC）进行量子增强分类。

Result: 在Devanagari手写字符数据集（DHCD）上测试，模型准确率达99.80%，损失为0.2893，平均每类F1分数为0.9980，优于经典CNN。

Conclusion: 该工作通过量子叠加和纠缠原理，为区域脚本识别设定了新基准，展示了量子机器学习在低资源语言环境中的潜力。

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [150] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: 该研究利用LSTM和GBC模型预测美元/孟加拉塔卡汇率，LSTM准确率达99.449%，优于传统ARIMA方法，但GBC模型在交易中亏损。


<details>
  <summary>Details</summary>
Motivation: 外汇汇率预测对全球金融市场至关重要，影响贸易和投资。

Method: 使用2018-2023年历史数据，采用LSTM和GBC模型进行预测和交易模拟。

Result: LSTM表现优异（RMSE 0.9858），GBC交易模拟亏损20,653.25美元。

Conclusion: 深度学习在汇率预测中潜力巨大，未来可结合情感分析和实时经济指标提升模型。

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [151] [Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G](https://arxiv.org/abs/2506.09418)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 本文综述了O-RAN在5G部署中的安全挑战，分析了密码学工具的性能，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: O-RAN的模块化和灵活性带来了新的安全挑战，需要研究如何保护其分散接口。

Method: 通过文献综述，分析了13个学术和行业来源，评估了密码学工具的性能及其在O-RAN中的应用。

Result: 研究发现O-RAN接口存在多种漏洞，并提出了AI驱动的动态编排和异常检测解决方案。

Conclusion: 未来研究应关注硬件卸载、跨层密码适应和零信任架构，以支持6G发展。

Abstract: The advent of Open Radio Access Networks (O-RAN) introduces modularity and
flexibility into 5G deployments but also surfaces novel security challenges
across disaggregated interfaces. This literature review synthesizes recent
research across thirteen academic and industry sources, examining
vulnerabilities such as cipher bidding-down attacks, partial encryption
exposure on control/user planes, and performance trade-offs in securing O-RAN
interfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,
AES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,
and adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is
placed on emerging testbeds and AI-driven controllers that facilitate dynamic
orchestration, anomaly detection, and secure configuration. We conclude by
outlining future research directions, including hardware offloading,
cross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN
Alliance security mandates, all of which point toward the need for integrated,
zero-trust architectures in 6G.

</details>


### [152] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: 该论文研究了在生成合成轨迹的深度学习模型中应用差分隐私（DP）对效用-隐私权衡的影响，提出了新的DP机制，并比较了不同模型类型的表现。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据包含敏感信息，差分隐私提供保护，但现有生成模型缺乏正式隐私保证，且依赖真实数据条件信息。

Method: 评估DP-SGD对生成模型效用的影响，提出新的DP机制用于条件生成，并比较扩散模型、VAE和GAN的表现。

Result: DP-SGD显著影响性能，但大数据集下仍保留部分效用；新DP机制提升训练稳定性；扩散模型在无隐私保证时表现最佳，而GAN在DP-SGD下最优。

Conclusion: DP轨迹生成仍具挑战性，正式隐私保证目前仅适用于大数据集和受限场景。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [153] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: TooBadRL是一个系统优化深度强化学习（DRL）后门攻击触发器的框架，通过时间、空间和幅度三个维度提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击依赖简单启发式触发器配置，忽视了触发器优化的潜力。

Method: 提出TooBadRL框架，包括性能感知的自适应冻结机制、基于合作游戏的维度选择和梯度优化的幅度调整。

Result: 在三种主流DRL算法和九个基准任务中，TooBadRL显著提高了攻击成功率，同时最小化正常任务性能下降。

Conclusion: TooBadRL证明了触发器优化在DRL后门攻击中的重要性，为未来研究提供了新方向。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [154] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Main category: math.PR

TL;DR: 递归训练生成模型可能导致模拟概率分布的“崩溃”，但研究表明，根据是否有外部样本源的参与，会出现两种不同的渐近行为。


<details>
  <summary>Details</summary>
Motivation: 探讨递归训练生成模型时概率分布崩溃的现象，并研究外部样本源对渐近行为的影响。

Method: 分析递归训练生成模型的过程，并引入外部样本源进行对比实验。

Result: 发现有无外部样本源会导致两种不同的渐近行为。

Conclusion: 外部样本源的参与显著影响递归训练生成模型的渐近行为，避免崩溃现象。

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [155] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: BemaGANv2是一种基于GAN的高保真音频生成模型，通过改进生成器和判别器架构，实现了更好的长期音频生成效果。


<details>
  <summary>Details</summary>
Motivation: 提升音频生成的高保真度和长期依赖性建模能力。

Method: 生成器采用AMP模块和Snake激活函数，判别器结合MED和MRD架构。

Result: 通过客观和主观评估验证了模型性能，代码和预训练模型已开源。

Conclusion: BemaGANv2在音频生成领域具有显著优势，并提供了详细的实现指南。

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [156] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: Factorized MKL-VC是一种无需训练的改进方法，用于kNN-VC流程，仅需5秒参考音频即可实现高质量的跨语言语音转换。


<details>
  <summary>Details</summary>
Motivation: 解决kNN-VC在短参考音频下内容保留和鲁棒性不足的问题。

Method: 用因子化的最优传输映射替代kNN回归，基于WavLM嵌入子空间和Monge-Kantorovich线性解。

Result: 在LibriSpeech和FLEURS数据集上表现优于kNN-VC，尤其在跨语言语音转换领域接近FACodec性能。

Conclusion: MKL-VC在短参考音频下显著提升性能，适用于跨语言语音转换。

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [157] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 论文提出了一种利用预训练语音-语言模型（PSLMs）和预训练语言模型（PLMs）作为辅助知识源的方法，以提升音频-视觉目标说话人提取（AV-TSE）模型的性能。


<details>
  <summary>Details</summary>
Motivation: 受人类利用语言知识（如语法和语义）辅助语音感知的启发，研究探索了PSLMs和PLMs在AV-TSE中的潜力。

Method: 通过将PSLMs或PLMs的语言约束作为额外的监督信号引入AV-TSE模型，无需在推理阶段增加计算成本。

Result: 该方法显著提升了语音质量和可懂度，并在多语言和视觉线索受损的场景中表现出稳健的性能提升。

Conclusion: 研究表明，利用语言模型作为辅助知识源可以有效增强AV-TSE模型的性能。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [158] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Main category: cs.SD

TL;DR: UmbraTTS是一种基于流匹配的TTS模型，可同时生成语音和环境音频，解决了语音与复杂背景环境融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前TTS技术虽能生成高度自然的语音，但将语音与复杂背景环境结合仍具挑战性。

Method: 提出UmbraTTS模型，通过自监督框架从无标注录音中提取语音、背景音频和文本，实现细粒度控制。

Result: UmbraTTS显著优于现有基线，生成自然、高质量且环境感知的音频。

Conclusion: UmbraTTS为语音与背景环境融合提供了有效解决方案，具有广泛的应用潜力。

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [159] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Symbolic Graph Ranker (SGR)的方法，结合文本和图结构信息，利用大语言模型(LLMs)提升会话搜索的效果。


<details>
  <summary>Details</summary>
Motivation: 当前会话搜索方法侧重于顺序建模或通用图结构表示，忽略了文本语义与图结构的结合。

Method: 通过符号语法规则将会话图转换为文本，并设计自监督任务（如链接预测、节点内容生成等）增强LLMs对图结构的理解。

Result: 在AOL和Tiangong-ST数据集上的实验证明了SGR的优越性。

Conclusion: SGR为传统搜索策略与现代LLMs的结合提供了新方法。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [160] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: BG-HOP是一种生成性先验模型，用于建模3D双手-物体交互，通过扩展单手生成先验解决数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决双手交互数据有限的问题，并建模双手与物体的联合分布。

Method: 扩展现有的单手生成先验模型，生成双手交互数据。

Result: 模型能够生成双手交互动作，并为给定物体合成抓取动作。

Conclusion: BG-HOP为双手交互建模提供了可行方案，代码和模型已公开。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [161] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: AVA-Bench是一个新的基准测试，旨在通过解耦14种原子视觉能力（AVAs）来系统评估视觉基础模型（VFMs），解决了现有VQA基准测试的两个盲点。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准测试存在数据分布不匹配和多能力耦合问题，导致难以准确评估VFMs的视觉能力。

Method: 引入AVA-Bench，通过解耦14种AVAs并在每种能力内匹配训练和测试分布，精准评估VFMs的表现。

Result: AVA-Bench揭示了VFMs的独特“能力指纹”，并发现0.5B LLM在评估效率上优于7B LLM。

Conclusion: AVA-Bench为下一代VFMs的发展提供了全面透明的评估基础。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [162] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: 论文研究了生成式AI模型中偏见的机制，发现偏见变化较小，但对属性分类器敏感，尤其是在连续属性上。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型的广泛使用引发了对其潜在偏见和歧视性结果的担忧，但偏见产生的机制尚不明确。

Method: 训练无条件图像生成模型，采用常用偏见评估框架分析训练与生成分布间的偏见变化。

Result: 实验显示偏见变化较小，但属性分类器的选择对结果敏感，尤其是连续属性。

Conclusion: 需改进标签实践，深入评估框架的局限性，并考虑属性的社会复杂性。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [163] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: PatchGuard是一种基于Vision Transformer的对抗鲁棒异常检测与定位方法，通过引入伪异常样本和定位掩码，显著提升了对抗环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测与定位方法因训练数据限制（仅含正常样本）易受对抗攻击，PatchGuard旨在解决这一问题。

Method: 利用前景感知伪异常样本，结合ViT架构和对抗训练，采用新型损失函数提升模型鲁棒性。

Result: 在工业和医学数据集上，PatchGuard在对抗环境下AD性能提升53.2%，AL提升68.5%，且在非对抗环境下仍具竞争力。

Conclusion: PatchGuard通过伪异常样本和理论指导的对抗训练，显著提升了异常检测与定位的对抗鲁棒性。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [164] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的光流与匹配模型（UFM），通过统一训练数据，使用简单的Transformer架构直接回归(u,v)光流，显著提升了准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在宽基线和光流估计中分别处理密集对应的问题，探索统一训练是否能超越专用方法。

Method: 采用统一的训练数据和简单的Transformer架构，直接回归(u,v)光流，避免了传统粗到细成本体积的复杂性。

Result: UFM比最先进的光流方法（Unimatch）准确率高28%，比密集宽基线匹配器（RoMa）误差低62%，速度快6.7倍。

Conclusion: UFM首次证明统一训练可在两个领域中超越专用方法，为多模态、长距离和实时对应任务开辟了新方向。

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [165] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级且节能的空中应急图像目标检测方案，基于YOLOv4-Tiny模型，通过INT8量化优化，在自定义数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集缺乏无人机视角的应急图像，因此作者创建了一个包含10,820张标注图像的自定义数据集，并优化模型以适应低功耗边缘设备。

Method: 采用YOLOv4-Tiny模型，通过后训练量化至INT8精度，并在自定义数据集上进行训练和评估。

Result: 量化后的YOLOv4-Tiny模型在检测性能上与YOLOv5-small相当，但模型大小减少71%（22.5 MB至6.4 MB），推理速度提升44%。

Conclusion: 量化后的YOLOv4-Tiny模型适合在低功耗边缘设备上实时检测应急场景。

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [166] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出了一种自回归对抗性后训练（AAPT）方法，将预训练的潜在视频扩散模型转化为实时交互式视频生成器，支持单步生成和实时交互。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算密集，无法满足实时和交互式应用的需求。

Method: 采用自回归对抗性训练，单步生成潜在帧，并利用KV缓存提高效率，同时通过学生强制训练减少长视频生成中的误差累积。

Result: 8B模型在单H100上实现24fps的736x416分辨率实时视频生成，或在8xH100上支持1280x720分辨率长达1分钟的视频生成。

Conclusion: AAPT方法在实时视频生成中表现出高效性和交互性，为实际应用提供了可行方案。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [167] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该论文提出了一种文本引导的多人物动作分割方法，并引入了首个相关数据集RHAS133。通过提出的HopaDIFF框架，结合傅里叶条件和注意力机制，显著提升了动作分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单人物固定动作序列，忽略了多人物场景。本文旨在填补这一空白，通过文本描述指定目标人物进行动作分割。

Method: 提出了HopaDIFF框架，结合交叉输入门控注意力xLSTM和傅里叶条件，增强整体-局部长程推理和细粒度控制。

Result: HopaDIFF在RHAS133数据集上取得了最先进的性能。

Conclusion: 该研究为多人物动作分割提供了新方法，并通过实验验证了其有效性。

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [168] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: CINeMA是一种新型框架，用于在低数据环境下创建高分辨率、多模态的胎儿和新生儿脑图谱，显著提高了效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 研究胎儿和新生儿大脑快速发育阶段需要高分辨率脑图谱，但传统方法依赖大量数据，难以应对病理数据稀缺的情况。

Method: CINeMA在潜在空间中操作，避免了计算密集的图像配准，支持基于解剖特征的灵活条件生成。

Result: CINeMA在准确性、效率和多功能性上超越现有方法，支持组织分割、年龄预测等任务，并能生成合成数据。

Conclusion: CINeMA为脑研究提供了强大工具，代码和图谱已开源。

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [169] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 本文提出了一种在推理时添加简单结构的方法，通过分割图像和文本片段，利用VLM匹配并聚合相似度，显著提升了双编码器VLM在组合性任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的双编码器VLM（如CLIP）在组合性任务中表现不佳，而训练方法已有很多研究，推理时技术却较少被关注。

Method: 在推理时，将图像分割为小块，提取文本片段（对象、属性和关系），利用VLM匹配图像块与文本片段，并聚合相似度计算最终得分。

Result: 该方法在不训练的情况下显著提升了VLM在组合性任务中的性能，尤其在属性-对象绑定任务中表现突出。

Conclusion: 推理时技术具有潜力，图像块处理是关键，未来可进一步优化推理时方法。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [170] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: Kvasir-VQA-x1是一个新的大规模胃肠道内窥镜数据集，旨在提升医学视觉问答（MedVQA）的临床复杂性和视觉多样性。


<details>
  <summary>Details</summary>
Motivation: 现有MedVQA数据集缺乏临床复杂性和视觉多样性，限制了临床决策支持系统的发展。

Method: 通过大型语言模型生成159,549个新问答对，并引入视觉增强模拟常见成像伪影。

Result: 数据集支持标准VQA性能和模型鲁棒性评估，为临床场景提供更具挑战性的基准。

Conclusion: Kvasir-VQA-x1旨在加速开发更可靠的多模态AI系统，并遵循FAIR数据原则，为研究社区提供资源。

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [171] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: 论文提出了MVP基准，用于评估视频语言模型的物理世界理解能力，通过最小变化对避免模型依赖表面线索。


<details>
  <summary>Details</summary>
Motivation: 现有基准因依赖表面视觉或文本线索导致评分虚高，需更准确评估模型性能。

Method: 引入MVP基准，包含55K高质量多选题视频QA样本，每样本有最小变化对以消除表面线索依赖。

Result: 人类表现92.9%，最佳开源模型仅40.2%（随机为25%）。

Conclusion: MVP能有效评估模型真实理解能力，避免表面线索干扰。

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


### [172] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: EditInspector是一个基于人类标注的新基准，用于评估文本引导的图像编辑质量，发现当前模型在全面评估编辑和描述变化时表现不佳，并提出了两种新方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，文本引导的图像编辑日益普及，但缺乏一个全面的框架来验证和评估编辑质量。

Method: 引入EditInspector基准，利用人类标注的模板验证编辑，并评估现有模型在多维度的表现，提出两种新方法。

Result: 当前模型在评估编辑时表现不全面且易产生幻觉，新方法在伪影检测和差异描述生成上优于现有模型。

Conclusion: EditInspector为文本引导编辑评估提供了新基准，新方法解决了现有模型的局限性。

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [173] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像修复任务TAIR，专注于同时恢复视觉内容和文本保真度，并提出了一个大规模基准SA-Text和多任务扩散框架TeReDiff，显著提升了文本识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像修复方法在自然图像修复中表现良好，但在文本区域的恢复中常出现文本图像幻觉现象，即生成看似合理但错误的文本模式。

Method: 提出了TAIR任务和SA-Text基准，并设计了TeReDiff框架，将扩散模型的内部特征与文本检测模块结合，通过联合训练提取丰富的文本表示。

Result: 实验表明，TeReDiff在文本识别准确性上显著优于现有方法。

Conclusion: TAIR任务和TeReDiff框架为解决文本图像幻觉问题提供了有效方案，并在实验中验证了其优越性。

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [174] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 论文提出一种元学习方法，通过蒸馏任务相关图像特征生成软提示，提升小规模多模态模型的少样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态模型（LMMs）的上下文学习（ICL）性能不稳定，尤其是在小规模模型中，额外图像信息可能干扰任务表现。

Method: 引入元学习方法，通过注意力映射模块蒸馏任务相关特征生成软提示，结合LLaVA v1.5架构实现少样本任务适应。

Result: 在VL-ICL Bench上的实验表明，该方法优于ICL和其他提示调优方法，尤其在图像扰动下仍能提升视觉问答任务的表现。

Conclusion: 提出的方法有效解决了小规模LMMs在少样本学习中的性能不稳定问题，为任务适应提供了新思路。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [175] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.AI

TL;DR: AIM是一种新型的机器人门控交互模仿学习算法，通过自适应调整干预请求，显著减少人类监督的认知负担。


<details>
  <summary>Details</summary>
Motivation: 当前交互模仿学习方法对人类监督者的认知要求过高，需要一种更高效的干预机制。

Method: AIM利用代理Q函数模拟人类干预规则，根据代理与人类动作的对齐程度动态调整干预请求。

Result: 实验表明，AIM在连续和离散控制任务中显著减少专家监控成本，比基线方法Thrifty-DAgger提升40%的效率。

Conclusion: AIM能有效识别安全关键状态，收集更高质量的专家演示，减少整体数据需求和环境交互。

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [176] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Main category: cs.AI

TL;DR: Shojaee等人（2025）的研究指出大型推理模型（LRMs）在规划谜题上存在“准确性崩溃”，但本文认为其结论源于实验设计缺陷而非模型推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 揭示Shojaee等人研究中实验设计的局限性，重新评估大型推理模型的真实能力。

Method: 分析原研究的实验设计问题，包括输出令牌限制、评估框架缺陷和数学上不可能的问题实例，并通过改进实验设计重新测试模型表现。

Result: 在控制实验设计缺陷后，模型在Tower of Hanoi等任务上表现优异，表明原研究的失败结论不准确。

Conclusion: 实验设计对评估AI推理能力至关重要，需避免误导性结论。

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [177] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni是一个统一的多模态模型，支持图像、文本、音频和视频处理，并在语音和图像生成方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统多模态模型需要单独模型或任务特定调整的问题，提供统一的框架支持多模态输入和生成任务。

Method: 采用专用编码器提取多模态标记，结合MoE架构和新提出的模态特定路由器，集成音频解码器和图像生成模块。

Result: 实验表明Ming-Omni在统一感知和生成任务中表现强大，支持上下文聊天、文本转语音和图像编辑。

Conclusion: Ming-Omni是首个开源支持多模态匹配GPT-4o的模型，代码和权重已公开以促进研究。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [178] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 论文质疑完全自主AI代理的可行性，提出LLM-HAS（基于LLM的人机协作系统），强调人机协作比AI独立工作更可靠和灵活。


<details>
  <summary>Details</summary>
Motivation: 当前自主AI系统在可靠性、透明度和理解人类需求方面存在问题，需要探索更有效的人机协作模式。

Method: 提出LLM-HAS框架，通过人类参与提供指导和保持控制，结合医疗、金融和软件开发案例展示协作优势。

Result: 人机协作系统在复杂任务中表现优于独立AI，同时更具信任度和适应性。

Conclusion: AI发展的目标不应是独立性，而是与人类的协作能力，未来AI应通过增强人类能力实现价值。

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [179] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.AI

TL;DR: DipLLM是一种基于LLM的智能体，通过少量数据微调，在复杂多人游戏《外交》中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量计算资源生成游戏数据，而LLMs提供了一种更高效的替代方案。

Method: DipLLM采用自回归分解框架，将多单位动作分配简化为单位级决策序列，并通过定义均衡策略作为学习目标。

Result: 仅需1.5%的数据即可超越现有Cicero模型的性能。

Conclusion: 微调LLMs在复杂多人游戏中具有巨大潜力。

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [180] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: 提出了一种名为Intent Factored Generation (IFG)的方法，通过分阶段采样（先采样意图，再生成最终响应）来提高语言模型生成样本的多样性，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 当前方法在提高多样性时往往仅停留在词汇层面，导致推理问题探索不足和对话代理重复无趣。

Method: IFG将采样过程分为两个阶段：1) 采样语义密集的意图（如摘要或关键词）；2) 基于原始提示和意图生成最终响应。通过调整温度参数（意图阶段高温度，生成阶段低温度）平衡多样性与一致性。

Result: 在数学和代码任务中提高了pass@k和RLVF性能；结合Direct Preference Optimisation提升了对话多样性；在通用语言建模任务中保持了生成质量的同时提高了多样性。

Conclusion: IFG是一种简单有效的方法，通过调整提示和温度参数即可实现多样性与性能的平衡，适用于多种应用场景。

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [181] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: 论文提出了一种结合互联网视频数据和少量机器人交互数据的自监督学习方法，开发了能够理解、预测和规划物理世界的模型。


<details>
  <summary>Details</summary>
Motivation: 现代AI的主要挑战是通过观察学习和理解世界。本文旨在探索如何利用大规模视频数据和少量交互数据实现这一目标。

Method: 采用自监督方法，预训练无动作的联合嵌入预测架构V-JEPA 2，并结合大语言模型和机器人视频数据训练动作条件世界模型V-JEPA 2-AC。

Result: V-JEPA 2在运动理解和人类动作预测任务中表现优异；V-JEPA 2-AC在机器人规划任务中实现零样本部署，无需任务特定训练。

Conclusion: 研究表明，自监督学习结合大规模数据和少量交互数据可以构建能够规划物理世界的世界模型。

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [182] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: 本文提出了一种基于AI的辅助技术，通过眼动变量生成的图像变换结合迁移学习，简化ASD诊断和管理，提高效率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: ASD患病率上升，现有诊断方法耗时且成本高，亟需更便捷、高效的解决方案。

Method: 结合迁移学习和眼动变量生成的图像变换，开发AI辅助诊断系统。

Result: 实现家庭定期诊断，减少压力，保护隐私，并改善监护人与治疗师间的沟通。

Conclusion: 该方法提供了及时、可访问的诊断方案，同时保护隐私，改善ASD患者的生活质量。

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [183] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: LoREIN是一种新型无监督双先验集成框架，用于加速3D多参数定量MRI重建，结合低秩和连续性先验以提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前重建方法仅依赖单一先验或物理模型，难以解决高度不适定的逆问题，导致结果不理想。

Method: LoREIN结合低秩表示（LRR）和隐式神经表示（INR），利用低秩和连续性先验提升重建精度。

Result: 该方法能高保真重建加权图像，并提高定量参数图的重建准确性。

Conclusion: LoREIN为零样本学习范式提供了广泛潜力，推动了医学影像领域的发展。

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [184] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Main category: cs.SI

TL;DR: 提出了一种新的零模型，用于评估二元交易和序列数据集的结果，通过统计假设检验。该模型比现有模型保留更多数据特性，尤其是二部联合度矩阵。


<details>
  <summary>Details</summary>
Motivation: 现有零模型未能充分保留数据集的关键特性，如二部联合度矩阵和特定路径数量。

Method: 开发了Alice算法套件，基于马尔可夫链蒙特卡洛方法，通过定义状态集和高效操作实现数据采样。

Result: 实验表明Alice混合速度快、扩展性好，且能发现与现有模型不同的显著结果。

Conclusion: 新零模型在保留更多数据特性的同时，提供了更准确的统计假设检验结果。

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [185] [Linking Data Citation to Repository Visibility: An Empirical Study](https://arxiv.org/abs/2506.09530)
*Fakhri Momeni,Janete Saldanha Bach,Brigitte Mathiak,Peter Mutschke*

Main category: cs.DL

TL;DR: 研究探讨了数据集存储库的可见性是否影响数据引用率，发现高可见性存储库与更多数据引用相关，但其他因素如数据集质量也起重要作用。


<details>
  <summary>Details</summary>
Motivation: 数据引用对学术诚信和科学可重复性至关重要，但数据集可见性对引用率的影响尚不明确。

Method: 使用OpenAlex数据和存储库影响指标（如Sistrix可见性指数、h指数和引用指标）分析社会科学和经济学数据集。

Result: 高可见性存储库的数据集引用更多，但引用指标的关联性不一致，其他因素如数据集质量也影响引用。

Conclusion: 可见性虽有助于增加引用，但非唯一因素，数据集质量和学科规范同样重要。

Abstract: In today's data-driven research landscape, dataset visibility and
accessibility play a crucial role in advancing scientific knowledge. At the
same time, data citation is essential for maintaining academic integrity,
acknowledging contributions, validating research outcomes, and fostering
scientific reproducibility. As a critical link, it connects scholarly
publications with the datasets that drive scientific progress. This study
investigates whether repository visibility influences data citation rates. We
hypothesize that repositories with higher visibility, as measured by search
engine metrics, are associated with increased dataset citations. Using OpenAlex
data and repository impact indicators (including the visibility index from
Sistrix, the h-index of repositories, and citation metrics such as mean and
median citations), we analyze datasets in Social Sciences and Economics to
explore their relationship. Our findings suggest that datasets hosted on more
visible web domains tend to receive more citations, with a positive correlation
observed between web domain visibility and dataset citation counts,
particularly for datasets with at least one citation. However, when analyzing
domain-level citation metrics, such as the h-index, mean, and median citations,
the correlations are inconsistent and weaker. While higher visibility domains
tend to host datasets with greater citation impact, the distribution of
citations across datasets varies significantly. These results suggest that
while visibility plays a role in increasing citation counts, it is not the sole
factor influencing dataset citation impact. Other elements, such as dataset
quality, research trends, and disciplinary norms, also contribute significantly
to citation patterns.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [186] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: SkillBlender是一个分层强化学习框架，通过预训练任务无关的原始技能并动态混合它们，实现多样化的人形机器人操作任务，减少任务特定调优。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对每个任务进行繁琐的调优，限制了其通用性和可扩展性，SkillBlender旨在解决这一问题。

Method: SkillBlender预训练目标条件的任务无关原始技能，并动态混合这些技能以完成复杂任务，减少任务特定奖励工程。

Result: 实验表明，SkillBlender显著优于基线方法，行为更准确且可行。

Conclusion: SkillBlender为多样化人形机器人操作任务提供了高效解决方案，代码和基准将开源。

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [187] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: 提出了一种框架，使四足机器人能够利用腿部的敏捷性收集物体，无需额外执行器。


<details>
  <summary>Details</summary>
Motivation: 探索四足机器人在动态物体操作中的潜力，超越静态任务。

Method: 采用分层策略结构，包括两个专家策略（抓取和抛掷、接近物体位置）和一个动态切换的元策略。

Result: 展示了四足机器人腿部在动态物体操作中的有效性。

Conclusion: 该方法扩展了四足机器人腿部的功能，超越了传统运动任务。

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [188] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: 论文提出了一种时间统一的扩散策略（TUDP），通过动作识别能力构建时间统一的去噪过程，解决了现有扩散策略在实时性和动作准确性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的策略在机器人操作中需要大量时间迭代去噪，且时间变化的去噪过程增加了模型训练难度，导致动作准确性不足。

Method: TUDP构建了时间统一的速度场，并引入动作判别分支提供额外信息，通过动作级训练提高去噪准确性。

Result: 在RLBench上，TUDP取得了多视图82.6%和单视图83.8%的最高成功率，且在较少去噪迭代时表现更优。

Conclusion: TUDP通过时间统一的去噪过程和动作级训练，显著提升了机器人动作生成的效率和准确性。

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [189] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出了一种紧密耦合的LiDAR-IMU-腿里程计方法，通过在线学习腿运动学模型，适应特征缺失环境和可变形地形。


<details>
  <summary>Details</summary>
Motivation: 解决在特征缺失环境和可变形地形中里程计估计的鲁棒性问题。

Method: 开发了基于神经网络的腿运动学模型，结合触觉信息，通过因子图联合优化模型训练和里程计估计。

Result: 在沙地和校园等复杂环境中，该方法优于现有技术。

Conclusion: 提出的方法在挑战性环境中表现出色，为机器人导航提供了更可靠的解决方案。

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [190] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的框架，通过预测调整和改进多吸盘末端执行器的选择，显著降低了拣选失败率。


<details>
  <summary>Details</summary>
Motivation: 提升仓库自动化效率，减少拣选失败率，弥补现有启发式方法的不足。

Method: 开发了一个ML框架，优化拣选样本的吸盘选择和变换调整，并在类似亚马逊机器人Robin车队的测试环境中集成评估。

Result: 在200万次拣选中，拣选失败率降低了20%，优于启发式基线方法。

Conclusion: 该框架在大规模仓库自动化场景中表现出高效性和实用性。

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [191] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: Chain-of-Action (CoA) 是一种基于轨迹自回归建模的新型视觉运动策略范式，通过反向推理生成完整轨迹，实现了任务目标的全局到局部约束。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常前向预测下一步动作，而 CoA 通过反向推理和任务目标驱动的动作级思维链（CoT）过程，提供更全局化的动作规划。

Method: CoA 采用自回归结构，首先生成一个稳定的关键帧动作，随后基于关键帧和已预测动作自回归生成后续动作。此外，还设计了连续动作标记表示、动态停止、反向时间集成和多标记预测等技术。

Result: CoA 在 60 个 RLBench 任务和 8 个真实世界操作任务中实现了最先进的性能。

Conclusion: CoA 在保持视觉运动策略灵活性和简单性的同时，提供了强大的空间泛化能力。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [192] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Main category: stat.ML

TL;DR: 本文提出了一种通过分位数回归校准过程奖励模型（PRMs）的方法，并引入实例自适应缩放（IAS）框架，动态调整推理预算，以减少计算成本同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有PRMs校准不佳且常高估成功概率，影响推理效率。

Method: 使用分位数回归校准PRMs输出，并基于校准后的成功概率和置信边界设计IAS框架。

Result: 实验表明校准方法优于基线，IAS在保持准确性的同时显著降低计算成本。

Conclusion: 校准PRMs和IAS框架能有效提升推理效率与成本效益。

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [193] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Main category: stat.ML

TL;DR: 提出了一种结合自注意力机制和贝叶斯滤波的混合跟踪框架，用于在噪声和杂乱场景中高效跟踪多粒子。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯滤波在轨迹假设较少时表现优于Transformer，但Transformer在减少关联可能性方面表现优异，因此结合两者优势以提升跟踪性能。

Method: 通过Transformer编码器推断帧间检测的软关联，修剪假设集，再在贝叶斯滤波框架中进行多粒子跟踪。

Result: 该方法在跟踪精度和对虚假检测的鲁棒性上有所提升，适用于高杂乱场景。

Conclusion: 混合框架结合了自注意力学习和贝叶斯滤波的可靠性，为多粒子跟踪提供了更优解决方案。

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [194] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Main category: stat.ML

TL;DR: 本文提出了一种基于大语言模型（LLM）的CPI预测方法LLM-CPI，利用高频在线文本数据改进CPI预测。


<details>
  <summary>Details</summary>
Motivation: 传统CPI预测方法依赖低频调查数据，而大语言模型的发展为利用高频在线文本数据提供了新机会。

Method: 通过LLM（如ChatGPT和BERT）处理社交媒体文本，提取文本嵌入，构建联合时间序列模型（ARX和VARX）结合CPI数据和文本数据。

Result: 方法在模拟和实际数据中表现出良好的有限样本性能，并提供了预测区间。

Conclusion: LLM-CPI展示了利用在线文本数据改进CPI预测的潜力。

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [195] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: 本文提出了一种针对贝叶斯预测模型设计最优规避攻击的通用方法，研究了两种对抗目标，并提出了基于梯度的新攻击方法。


<details>
  <summary>Details</summary>
Motivation: 对抗性机器学习研究多集中于经典预测模型的弱点，而贝叶斯预测模型的易受攻击性尚未充分探索。

Method: 提出了一种通用方法，设计针对贝叶斯预测模型的最优规避攻击，包括两种对抗目标和基于梯度的攻击方法。

Result: 在不同计算设置下研究了攻击方法的实现和特性。

Conclusion: 本文填补了贝叶斯预测模型对抗攻击研究的空白，为相关领域提供了新的攻击方法。

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [196] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Main category: stat.ML

TL;DR: 论文探讨了深度学习中预测不确定性的缩放规律，发现即使在过参数化模型中，不确定性仍遵循可预测的趋势。


<details>
  <summary>Details</summary>
Motivation: 受深度学习缩放规律的启发，研究预测不确定性是否也存在类似的规律，以解决对贝叶斯方法的质疑。

Method: 通过实验验证预测不确定性的缩放规律，使用近似贝叶斯推断和集成方法。

Result: 实验结果表明，预测不确定性在视觉和语言任务中均存在缩放规律。

Conclusion: 研究证明，即使数据量大，认知不确定性仍不可忽视，支持贝叶斯方法的实用性。

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [197] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Main category: stat.ML

TL;DR: 论文研究了去噪扩散概率模型（DDPMs）对噪声的鲁棒性，并提出了更快的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 探索DDPMs在噪声环境下的表现，并验证其鲁棒性和收敛性。

Method: 通过实验和理论分析，评估DDPMs在噪声评分函数下的性能，并建立Wasserstein-2距离的有限样本保证。

Result: DDPMs对噪声评分估计具有鲁棒性，且收敛速率优于先前结果，接近高斯情况下的最优速率。

Conclusion: DDPMs在噪声环境下表现优异，收敛速率快且接近最优，验证了其在实际应用中的潜力。

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [198] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Main category: stat.ML

TL;DR: 论文提出了一种基于图生成模型（深度学习技术）的新方法，用于模拟离散岩溶网络的复杂性。该方法结合了图循环神经网络（GraphRNN）和图去噪扩散概率模型（G-DDPM），生成具有真实性的岩溶网络。


<details>
  <summary>Details</summary>
Motivation: 由于岩溶网络的复杂性及其与水文地质条件的紧密关联，模拟其形态和特征具有挑战性。

Method: 1. 使用GraphRNN学习岩溶网络的拓扑分布；2. 利用G-DDPM学习节点特征（如空间坐标）。

Result: 生成的子图与实际数据在几何和拓扑指标上表现一致，验证了方法的有效性。

Conclusion: 该方法能够随机模拟不同类型的岩溶网络，为研究物理过程（如流动和传输）提供了实用工具。

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [199] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: CryoSPIRE是一种新型的3D重建框架，通过分层高斯混合模型处理冷冻电镜图像中的非刚性构象灵活性和组成变化，显著提升了复杂实验数据的解析能力。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜（Cryo-EM）在分子生物学中具有革命性意义，但如何建模非刚性构象灵活性和组成变化的分子结构仍是一个挑战。

Method: 提出了一种基于分层高斯混合模型的3D重建框架，结合部分分割的初始过程，以处理构象和组成变化。

Result: CryoSPIRE在复杂实验数据中揭示了具有生物学意义的结构，并在CryoBench基准测试中达到了新的最优性能。

Conclusion: CryoSPIRE为冷冻电镜图像中的异质性建模提供了高效解决方案，推动了该领域的发展。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


### [200] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Main category: q-bio.QM

TL;DR: 利用循环肿瘤DNA（ctDNA）水平和签名理论，结合连续时间马尔可夫模型，提出了一种高效的多癌症早期检测方法。


<details>
  <summary>Details</summary>
Motivation: ctDNA水平监测为癌症早期检测提供了低负担且高准确性的可能，但数据稀疏性是一个挑战。签名理论作为特征提取工具，有望解决这一问题。

Method: 结合连续时间马尔可夫模型分析ctDNA动态，并利用签名理论构建高效检测流程。

Result: 数值实验验证了该方法在克服数据稀疏性问题上的有效性。

Conclusion: 提出的方法为基于血液样本的癌症早期检测提供了新思路，具有实际应用潜力。

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [201] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Main category: cond-mat.mtrl-sci

TL;DR: 利用人工神经网络和合成数据预测PEALD工艺的饱和时间，仅需两次实验即可在10%误差内预测结果，同时模型对表面重组主导的交互判断准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 解决PEALD工艺中因表面重组导致的高深宽比结构内饱和时间过长的问题。

Method: 基于模拟数据训练人工神经网络，预测饱和时间并判断表面重组是否主导。

Result: 仅需两次实验即可预测饱和时间（误差10%），模型判断准确率达99%。

Conclusion: 机器学习为PEALD工艺优化提供了新途径，并可扩展至其他复杂工艺。

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [202] [Not all those who drift are lost: Drift correction and calibration scheduling for the IoT](https://arxiv.org/abs/2506.09186)
*Aaron Hurst,Andrey V. Kalinichev,Klaus Koren,Daniel E. Lucani*

Main category: eess.SP

TL;DR: 提出了一种基于高斯过程回归的概率传感器漂移校正方法，显著降低误差，并优化校准计划。


<details>
  <summary>Details</summary>
Motivation: 传感器漂移和校准机会有限导致数据质量下降，现有方法需大量真实数据且忽略不确定性。

Method: 使用高斯过程回归建模传感器响应，提出不确定性驱动的校准计划优化。

Result: 溶解氧传感器测试中，平均MSE降低20%以上，最高90%；校准优化进一步降低MSE达15.7%。

Conclusion: 该方法有效校正传感器漂移并优化校准，显著提升数据质量。

Abstract: Sensors provide a vital source of data that link digital systems with the
physical world. However, as sensors age, the relationship between what they
measure and what they output changes. This is known as sensor drift and poses a
significant challenge that, combined with limited opportunity for
re-calibration, can severely limit data quality over time. Previous approaches
to drift correction typically require large volumes of ground truth data and do
not consider measurement or prediction uncertainty. In this paper, we propose a
probabilistic sensor drift correction method that takes a fundamental approach
to modelling the sensor response using Gaussian Process Regression. Tested
using dissolved oxygen sensors, our method delivers mean squared error (MSE)
reductions of up to 90% and more than 20% on average. We also propose a novel
uncertainty-driven calibration schedule optimisation approach that builds on
top of drift correction and further reduces MSE by up to 15.7%.

</details>


### [203] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Main category: eess.SP

TL;DR: 提出了一种基于机器学习的SEEG通道排序方法，结合临床选择和计算发现，通过XGBoost和SHAP评分提高癫痫区域识别的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: SEEG信号的人工检查效率低下，需自动化方法辅助识别癫痫区域。

Method: 使用XGBoost分类模型学习发作期通道特征，结合SHAP评分排序通道，并扩展搜索空间以发现更多可疑区域。

Result: 在五名患者数据上验证，结果显示方法在准确性、一致性和可解释性方面表现良好。

Conclusion: 该方法能有效辅助临床决策，提高癫痫区域识别的效率和可靠性。

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [204] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Main category: eess.SP

TL;DR: 该论文扩展了跨通道无标记感知框架，支持更复杂的信号结构（如子空间联合），改进了样本需求量的边界，并在实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多通道信号在跨通道测量中因样本与通道不匹配而难以恢复的问题，特别是在复杂信号结构（如子空间联合）和实际应用（如全脑钙成像）中。

Method: 扩展跨通道无标记感知框架，支持信号位于子空间联合的情况，推导更紧的样本需求量边界，并在全脑钙成像中验证。

Result: 改进了样本需求量的理论边界，支持更广泛的信号类型，并在实际应用中实现了准确的信号重建。

Conclusion: 该框架在复杂信号结构和实际应用中表现出色，为跨通道无标记感知提供了更通用的解决方案。

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>
