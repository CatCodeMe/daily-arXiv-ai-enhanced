<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 124]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 10]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: RADAR是一个用于评估语言模型在表格数据中处理数据异常能力的基准测试，包含2980个查询对，覆盖9个领域和5种数据异常类型。测试显示前沿模型在存在数据异常时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自主数据分析中的应用日益广泛，但其对数据异常（如缺失值、离群值等）的识别和处理能力尚未充分研究，这可能影响分析结论的有效性。

Method: 通过程序化扰动模拟数据异常，构建RADAR基准测试，包含2980个查询对，覆盖多种领域和异常类型，并系统化调整表格大小以评估模型性能。

Result: 前沿模型在无数据异常时表现良好，但在引入数据异常后性能显著下降，暴露其在鲁棒数据分析能力上的不足。

Conclusion: RADAR为提升表格推理能力提供了灵活且可扩展的资源，揭示了语言模型在数据感知推理方面的关键缺陷。

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [2] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: LEANN是一种针对资源受限个人设备的存储高效近似最近邻搜索索引，通过紧凑的图结构和动态重计算策略，显著减少存储开销，同时保持搜索性能。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入搜索在推荐和RAG等应用中的普及，本地设备上存储个人数据的需求增加，但传统索引的高存储开销使其难以实现。

Method: LEANN结合紧凑的图结构和动态重计算策略，优化存储效率。

Result: LEANN将索引大小降至原始数据的5%，存储开销减少50倍，同时在真实问答基准测试中保持90%的top-3召回率，延迟低于2秒。

Conclusion: LEANN为资源受限设备提供了一种高效的嵌入搜索解决方案，显著降低了存储需求，同时保持了搜索质量。

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


### [3] [Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and Design Choices](https://arxiv.org/abs/2506.08671)
*Junfeng Liu,Jiarui Ye,Mengshi Chen,Meng Li,Siqiang Luo*

Main category: cs.DB

TL;DR: 论文对LSM-tree系统中学习索引进行了全面系统评测，总结了8种现有学习索引的工作流程和理论成本，提出了影响性能的关键因素和配置空间，并通过实验揭示了意外发现，提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着数据量增长，高效查询大规模数据库变得更具挑战性，现有研究尝试将学习索引集成到LSM-tree中以提升性能，但缺乏对不同学习索引类型的全面理解。

Method: 总结了8种学习索引的工作流程和理论成本，提出了配置空间（包括索引类型、边界位置和粒度），并在统一平台上实现不同设计进行评测。

Result: 实验发现了一些意外结果，例如为学习索引分配大内存预算时查找性能提升有限，以及学习索引的重新训练开销较小。

Conclusion: 论文填补了学习索引在LSM-tree系统中的研究空白，提供了实用指南，帮助开发者选择和调优学习索引。

Abstract: LSM-tree-based data stores are widely used in industry due to their
exceptional performance. However, as data volumes grow, efficiently querying
large-scale databases becomes increasingly challenging. To address this, recent
studies attempted to integrate learned indexes into LSM-trees to enhance lookup
performance, which has demonstrated promising improvements. Despite this, only
a limited range of learned index types has been considered, and the strengths
and weaknesses of different learned indexes remain unclear, making them
difficult for practical use. To fill this gap, we provide a comprehensive and
systematic benchmark to pursue an in-depth understanding of learned indexes in
LSM-tree systems. In this work, we summarize the workflow of 8 existing learned
indexes and analyze the associated theoretical cost. We also identify several
key factors that significantly influence the performance of learned indexes and
conclude them with a novel configuration space, including various index types,
boundary positions, and granularity. Moreover, we implement different learned
index designs on a unified platform to evaluate across various configurations.
Surprisingly, our experiments reveal several unexpected insights, such as the
marginal lookup enhancement when allocating a large memory budget to learned
indexes and modest retraining overhead of learned indexes. Besides, we also
offer practical guidelines to help developers intelligently select and tune
learned indexes for custom use cases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: PerfTracker是一个在线性能诊断系统，用于大规模模型训练中的性能问题排查，支持硬件和软件层面的问题定位。


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群规模庞大，软硬件交互复杂，传统分布式系统或数据中心网络的故障排查方法难以适用。

Method: PerfTracker通过细粒度在线分析和差异可观测性，诊断硬件（如GPU及其互连）和软件（如Python函数和GPU操作）的性能问题。

Result: PerfTracker已部署在包含约10,000个GPU的生产集群中，成功诊断了多种复杂性能问题。

Conclusion: PerfTracker为大规模模型训练提供了一种高效的在线性能诊断工具。

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


### [5] [Towards Provenance-Aware Earth Observation Workflows: the openEO Case Study](https://arxiv.org/abs/2506.08597)
*H. Omidi,L. Sacco,V. Hutter,G. Irsiegler,M. Claus,M. Schobben,A. Jacob,M. Schramm,S. Fiore*

Main category: cs.DC

TL;DR: 本文提出了一种通过集成数据溯源库yProv4WFs到openEO平台的方法，以改进地球观测（EO）工作流中操作和活动的历史记录。


<details>
  <summary>Details</summary>
Motivation: 地球观测工作流中操作和活动的历史记录对数据溯源至关重要，但目前缺乏统一的解决方案。

Method: 通过将yProv4WFs库集成到openEO平台，记录数据产品的生成、传输和操作过程。

Result: 该方法使研究人员和利益相关者能更好地理解分析工作流中的数据流、依赖关系和转换过程。

Conclusion: 集成数据溯源概念到EO处理链中，显著提升了工作流的透明度和可追溯性。

Abstract: Capturing the history of operations and activities during a computational
workflow is significantly important for Earth Observation (EO). The data
provenance helps to collect the metadata that records the lineage of data
products, providing information about how data are generated, transferred,
manipulated, by whom all these operations are performed and through which
processes, parameters, and datasets. This paper presents an approach to improve
those aspects, by integrating the data provenance library yProv4WFs within
openEO, a platform to let users connect to Earth Observation cloud back-ends in
a simple and unified way. In addition, it is demonstrated how the integration
of data provenance concepts across EO processing chains enables researchers and
stakeholders to better understand the flow, the dependencies, and the
transformations involved in analytical workflows.

</details>


### [6] [Blockchain and Edge Computing Nexus: A Large-scale Systematic Literature Review](https://arxiv.org/abs/2506.08636)
*Zeinab Nezami,Zhuolun Li,Chuhao Qin,Fatemeh Banaie,Rabiya Khalid,Evangelos Pournaras*

Main category: cs.DC

TL;DR: 本文通过系统文献综述，探讨区块链与边缘计算的结合如何推动创新，解决研究挑战，并提出了新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 区块链与边缘计算作为分散计算的两大范式，虽在社会中影响深远，但作为技术和研究领域仍显碎片化。本文旨在揭示两者结合如何促进创新。

Method: 通过收集和分析近6000篇论文，构建包含22个特征和287个属性的分类体系，并采用定量和机器学习方法进行研究。

Result: 研究揭示了区块链与边缘计算的四种交互模式，并发现区块链辅助边缘计算在隐私和安全性方面尤为突出。

Conclusion: 区块链与边缘计算的结合为移动计算等应用提供了创新解决方案，尤其是在隐私和安全领域。

Abstract: Blockchain and edge computing are two instrumental paradigms of decentralized
computation, driving key advancements in Smart Cities applications such as
supply chain, energy and mobility. Despite their unprecedented impact on
society, they remain significantly fragmented as technologies and research
areas, while they share fundamental principles of distributed systems and
domains of applicability. This paper introduces a novel and large-scale
systematic literature review on the nexus of blockchain and edge computing with
the aim to unravel a new understanding of how the interfacing of the two
computing paradigms can boost innovation to provide solutions to timely but
also long-standing research challenges. By collecting almost 6000 papers from 3
databases and putting under scrutiny almost 1000 papers, we build a novel
taxonomy and classification consisting of 22 features with 287 attributes that
we study using quantitative and machine learning methods. They cover a broad
spectrum of technological, design, epistemological and sustainability aspects.
Results reveal 4 distinguishing patterns of interplay between blockchain and
edge computing with key determinants the public (permissionless) vs. private
(permissioned) design, technology and proof of concepts. They also demonstrate
the prevalence of blockchain-assisted edge computing for improving privacy and
security, in particular for mobile computing applications.

</details>


### [7] [Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX](https://arxiv.org/abs/2506.08653)
*Alexander Strack,Christopher Taylor,Dirk Pflüger*

Main category: cs.DC

TL;DR: 本文评估了FFTW库在RISC-V上的并行扩展性能，并与x86-64架构进行了对比，发现RISC-V在双精度2D FFT上的性能差距较大。内存优化在RISC-V上效果不明显，但FFTW与MPI在两种架构上均表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V硬件的发展，高效并行化成为关键。本文旨在评估RISC-V在高性能计算中的潜力，尤其是并行FFT的性能表现。

Method: 使用FFTW库和HPX-FFT库，分别在RISC-V和x86-64架构上测试MPI和OpenMP的并行扩展性能，并分析内存优化的效果。

Result: RISC-V在双精度2D FFT上的性能比x86-64慢8倍。FFTW与MPI在两种架构上均能良好扩展至64核，而OpenMP需要特定规划才能达到类似效果。内存优化在RISC-V上无效。

Conclusion: 本研究为RISC-V在大规模并行应用中的发展提供了初步参考，表明其在并行计算中仍有优化空间。

Abstract: Rapid advancements in RISC-V hardware development shift the focus from
low-level optimizations to higher-level parallelization. Recent RISC-V
processors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with
core counts comparable to the SG2042, make efficient parallelization as crucial
for RISC-V as the more established processors such as x86-64. In this work, we
evaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI
and OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for
different types of FFTW planning. Additionally, we investigate the effect of
memory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the
asynchronous many-task runtime HPX using an FFTW backend. We generally observe
a performance delta between the x86-64 and RISC-V chips of factor eight for
double-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do
not translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64
cores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with
OpenMP requires measured planning on both architectures to achieve good scaling
up to 64 cores. The results of our study mark an early step on the journey to
large-scale parallel applications running on RISC-V.

</details>


### [8] [Synchronization in Anonymous Networks Under Continuous Dynamics](https://arxiv.org/abs/2506.08661)
*Rida Bazzi,Anya Chaturvedi,Andréa W. Richa,Peter Vargas*

Main category: cs.DC

TL;DR: 论文提出了κ-Synchronizer，用于在非同步动态网络中工作，支持连续拓扑变化且无需全局稳定假设。这是首个在弱公平节点激活调度下实现动态网络同步算法模拟的同步器。


<details>
  <summary>Details</summary>
Motivation: 研究如何在无全局时钟、节点匿名、无持久连接等严格限制下，实现动态网络的同步模拟。

Method: 扩展Pull通信模型，为每个节点边端口添加1位多写原子寄存器，提出κ-Synchronizer算法。

Result: κ-Synchronizer在节点内存开销上线性于最大节点度，对数于模拟同步算法的运行时间。

Conclusion: 该同步器扩展了同步器定义，填补了半同步到同步模型在动态网络中的空白，并展示了非平凡应用。

Abstract: We present the $\kappa$-Synchronizer that works in non-synchronous dynamic
networks under minimal assumptions. Our model allows continuous topological
changes without any guarantee of eventual global or partial stabilization and
assumes that nodes are anonymous. This deterministic synchronizer is the first
to enable nodes to simulate a dynamic network synchronous algorithm for
executions in a semi-synchronous dynamic environment under a weakly-fair node
activation scheduler, despite the absence of a global clock, node ids,
persistent connectivity or any assumptions about the edge dynamics (in both the
synchronous and semi-synchronous environments). In summary, we make the
following contributions: (1) we extend the definition of synchronizers to
networks with continuous arbitrary edge dynamics; (2) we present the first
synchronizer from the semi-synchronous to the synchronous model in a network
with continuous arbitrary edge dynamics; and (3) we present non-trivial
applications of the proposed synchronizer to existing algorithms. We assume an
extension of the Pull communication model by adding a single 1-bit multi-writer
atomic register at each edge-port of a node, since we show that the standard
Pull model is not sufficient to allow for non-trivial synchronization in our
scenario. The $\kappa$-Synchronizer operates with memory overhead at the nodes
that is linear on the maximum node degree and logarithmic on the runtime of the
underlying synchronous algorithm being simulated.

</details>


### [9] [Balancing Fixed Number of Nodes Among Multiple Fixed Clusters](https://arxiv.org/abs/2506.08715)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy,Bhuban Padhan*

Main category: cs.DC

TL;DR: 提出了一种动态节点平衡系统，通过实时资源利用率阈值在多集群间动态分配节点，优化资源利用和成本。


<details>
  <summary>Details</summary>
Motivation: 解决固定节点分配导致的资源利用率低下问题。

Method: 引入Node Balancing Cluster Group（NBCG）和动态节点重分配机制，通过阈值参数识别资源利用情况。

Result: 系统优化资源利用和成本，同时确保集群稳定性。

Conclusion: 该方法显著减少资源浪费，提升IBM Cloud的竞争力。

Abstract: Cloud infrastructure users often allocate a fixed number of nodes to
individual container clusters (e.g., Kubernetes, OpenShift), resulting in
underutilization of computing resources due to asynchronous and variable
workload peaks across clusters. This research proposes a novel system and
method for dynamic rebalancing of a fixed total number of nodes among multiple
fixed clusters based on real-time resource utilization thresholds. By
introducing a Node Balancing Cluster Group (NBCG), clusters are grouped and
allowed to dynamically share nodes through a controlled reallocation mechanism,
managed by a Node Balancing Cluster Balancer and a Resizing Rule Engine. The
system identifies overutilized and underutilized clusters using threshold
parameters, and reassigns nodes without incurring additional provisioning
costs. If reallocation causes a violation of utilization thresholds, the system
reverses the operation to maintain cluster stability. The proposed architecture
not only optimizes resource utilization and operational cost but also
introduces a strategic advantage for cloud service providers like IBM Cloud.
Unlike existing solutions, this approach enables intra-account node sharing
across clusters with strict adherence to user-defined constraints and ensures
consistent cluster state management. This invention has the potential to
significantly reduce computing resource waste and position IBM Cloud services
as more efficient and competitive.

</details>


### [10] [Mycelium: A Transformation-Embedded LSM-Tree](https://arxiv.org/abs/2506.08923)
*Holly Casaletto,Jeff Lefevre,Aldrin Montana,Peter Alvaro*

Main category: cs.DC

TL;DR: TE-LSM是一种新型LSM树方法，通过在压缩过程中嵌入数据转换，降低IO成本并提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统LSM树压缩过程成本高，TE-LSM利用压缩过程嵌入其他有用工作以分摊成本。

Method: 提出TE-LSM方法，并在RocksDB基础上构建原型Mycelium，支持跨列族合并和转换接口。

Result: Mycelium在写入性能上仅增加20%开销，读取延迟提升高达425%。

Conclusion: TE-LSM通过嵌入转换优化压缩过程，显著提升性能，为未来数据访问提供更好准备。

Abstract: Compaction is a necessary, but often costly background process in
write-optimized data structures like LSM-trees that reorganizes incoming data
that is sequentially appended to logs. In this paper, we introduce
Transformation-Embedded LSM-trees (TE-LSM), a novel approach that transparently
embeds a variety of data transformations into the compaction process. While
many others have sought to reduce the high cost of compaction, TE-LSMs leverage
the opportunity to embed other useful work to amortize IO costs and
amplification. We illustrate the use of a TE-LSM in Mycelium, our prototype
built on top of RocksDB that extends the compaction process through a
cross-column-family merging mechanism. Mycelium enables seamless integration of
a transformer interface and aims to better prepare data for future accesses
based on access patterns. We use Mycelium to explore three types of
transformations: splitting column groups, converting data formats, and index
building. In addition to providing a cost model analysis, we evaluate
Mycelium's write and read performance using YCSB workloads. Our results show
that Mycelium incurs a 20% write throughput overhead - significantly lower than
the 35% to 60% overhead observed in naive approaches that perform data
transformations outside of compaction-while achieving up to 425% improvements
in read latency compared to RocksDB baseline.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Fair Diversity Maximization with Few Representatives](https://arxiv.org/abs/2506.08110)
*Florian Adriaens,Nikolaj Tatti*

Main category: cs.DS

TL;DR: 论文研究了公平多样性最大化问题，提出了一种基于随机填充分解的算法，显著提高了近似比。


<details>
  <summary>Details</summary>
Motivation: 解决在多样性最大化问题中如何保证每个标签组的公平表示，同时优化多样性。

Method: 算法分为预处理剪枝、分解阶段和分配阶段，确保标签组内点距离远且多样性高。

Result: 算法将近似比提高到√log(m)/(3m)，实验验证了其在大数据集上的有效性。

Conclusion: 提出的算法在理论和实验上均优于现有方法，适用于需要公平多样性的场景。

Abstract: Diversity maximization problem is a well-studied problem where the goal is to
find $k$ diverse items. Fair diversity maximization aims to select a diverse
subset of $k$ items from a large dataset, while requiring that each group of
items be well represented in the output. More formally, given a set of items
with labels, our goal is to find $k$ items that maximize the minimum pairwise
distance in the set, while maintaining that each label is represented within
some budget. In many cases, one is only interested in selecting a handful (say
a constant) number of items from each group. In such scenario we show that a
randomized algorithm based on padded decompositions improves the
state-of-the-art approximation ratio to $\sqrt{\log(m)}/(3m)$, where $m$ is the
number of labels. The algorithms work in several stages: ($i$) a preprocessing
pruning which ensures that points with the same label are far away from each
other, ($ii$) a decomposition phase, where points are randomly placed in
clusters such that there is a feasible solution with maximum one point per
cluster and that any feasible solution will be diverse, $(iii)$ assignment
phase, where clusters are assigned to labels, and a representative point with
the corresponding label is selected from each cluster. We experimentally verify
the effectiveness of our algorithm on large datasets.

</details>


### [12] [Testing Suffixient Sets](https://arxiv.org/abs/2506.08225)
*Davide Cenzato,Francisco Olivares,Nicola Prezza*

Main category: cs.DS

TL;DR: 论文提出了一种基于子采样的前缀数组（PA）压缩技术——后缀集（suffixient sets），并解决了判断给定文本位置子集是否为后缀集及其最小基数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的前缀数组压缩技术需要压缩整个数组，而本文提出了一种基于子采样的新方法，仅存储少量PA条目即可实现模式匹配。

Method: 通过存储少量PA条目，利用文本的随机访问能力，实现二进制搜索模式匹配。

Result: 提供了线性时间算法，用于判断给定子集是否为后缀集及其最小基数。

Conclusion: 后缀集是一种高效的PA压缩技术，其验证和最小化问题可通过线性时间算法解决。

Abstract: Suffixient sets are a novel prefix array (PA) compression technique based on
subsampling PA (rather than compressing the entire array like previous
techniques used to do): by storing very few entries of PA (in fact, a
compressed number of entries), one can prove that pattern matching via binary
search is still possible provided that random access is available on the text.
In this paper, we tackle the problems of determining whether a given subset of
text positions is (1) a suffixient set or (2) a suffixient set of minimum
cardinality. We provide linear-time algorithms solving these problems.

</details>


### [13] [Towards universally optimal sorting algorithms](https://arxiv.org/abs/2506.08261)
*Sandeep Sen*

Main category: cs.DS

TL;DR: 提出了一种新的算法最优性范式，扩展了仅基于输入大小的最坏情况最优性，引入了问题相关参数（包括隐式参数）。重新审视了一些现有排序算法，并提出了一种新的有序性度量，基于分区排序实现了最优算法。


<details>
  <summary>Details</summary>
Motivation: 传统的最优性范式仅基于输入大小，忽略了问题中的其他潜在参数。本文旨在通过引入更全面的参数（包括隐式参数）来改进算法效率的衡量。

Method: 提出了一种新的最优性范式，重新审视了现有排序算法，并基于新的有序性度量设计了一种基于分区排序的最优算法。

Result: 展示了新范式在排序问题中的应用，证明了基于分区排序的算法在新度量下的最优性。

Conclusion: 新范式为算法效率的衡量提供了更全面的视角，有望在更广泛的应用中发挥作用。

Abstract: We formalize a new paradigm for optimality of algorithms, that generalizes
worst-case optimality based only on input-size to problem-dependent parameters
including implicit ones. We re-visit some existing sorting algorithms from this
perspective, and also present a novel measure of sortedness that leads to an
optimal algorithm based on partition sort. This paradigm of measuring
efficiency of algorithms looks promising for further interesting applications
beyond the existing ones.

</details>


### [14] [Optimal Graph Reconstruction by Counting Connected Components in Induced Subgraphs](https://arxiv.org/abs/2506.08405)
*Hadley Black,Arya Mazumdar,Barna Saha,Yinzhan Xu*

Main category: cs.DS

TL;DR: 本文提出了一种基于连通分量数量的新查询模型，用于图重构问题，并证明了自适应查询的上下界，以及非自适应查询的高复杂度。


<details>
  <summary>Details</summary>
Motivation: 图重构问题在多种查询模型下已被广泛研究，但连通分量作为图的基本参数之一，尚未被充分探索。本文旨在填补这一空白。

Method: 通过设计一种新的查询模型，利用子集顶点诱导子图的连通分量数量作为查询响应，分析自适应和非自适应查询的复杂度。

Result: 证明了自适应查询的期望复杂度为Θ(m log n / log m)，非自适应查询则需要Ω(n²)次，即使m=O(n)。同时提出了一种仅需两轮自适应的O(m log n + n log² n)查询算法。

Conclusion: 本文为图重构问题提供了一种新的查询模型，并展示了自适应查询的高效性，同时揭示了非自适应查询的局限性。

Abstract: The graph reconstruction problem has been extensively studied under various
query models. In this paper, we propose a new query model regarding the number
of connected components, which is one of the most basic and fundamental graph
parameters. Formally, we consider the problem of reconstructing an $n$-node
$m$-edge graph with oracle queries of the following form: provided with a
subset of vertices, the oracle returns the number of connected components in
the induced subgraph. We show $\Theta(\frac{m \log n}{\log m})$ queries in
expectation are both sufficient and necessary to adaptively reconstruct the
graph. In contrast, we show that $\Omega(n^2)$ non-adaptive queries are
required, even when $m = O(n)$. We also provide an $O(m\log n + n\log^2 n)$
query algorithm using only two rounds of adaptivity.

</details>


### [15] [Improving Online Bin Covering with Little Advice](https://arxiv.org/abs/2506.09004)
*Andrej Brodnik,Bengt J. Nilsson,Gordana Vujović*

Main category: cs.DS

TL;DR: 论文改进了在线装箱覆盖问题的策略，通过强化分析和微小改进，将竞争比从8/15提升至135/242，同时保持O(log log n)的辅助比特数。


<details>
  <summary>Details</summary>
Motivation: 在线装箱覆盖问题需要高效分配物品以最大化满足条件的箱子数量，现有策略的竞争比有待提高。

Method: 通过强化分析和微小改进现有策略，保持辅助比特数为O(log log n)。

Result: 改进后的策略将竞争比从8/15显著提升至135/242。

Conclusion: 研究证明，通过优化分析和方法，可以在不增加辅助比特数的情况下显著提升竞争比。

Abstract: The online bin covering problem is: given an input sequence of items find a
placement of the items in the maximum number of bins such that the sum of the
items' sizes in each bin is at least~1. Boyar~{\em et~al}.\@~\cite{boyar2021}
present a strategy that with $O(\log \log n)$ bits of advice, where $n$ is the
length of the input sequence, achieves a competitive ratio of
$8/15\approx0.5333\ldots$. We show that with a strengthened analysis and some
minor improvements, the same strategy achieves the significantly improved
competitive ratio of~$135/242\approx0.5578\ldots$, still using $O(\log \log n)$
bits of advice.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing](https://arxiv.org/abs/2506.08055)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 本文通过系统文献综述（SLR）分析了云环境中CI/CD的安全问题，总结了工具、方法和挑战，并指出研究空白。


<details>
  <summary>Details</summary>
Motivation: 随着云环境的普及，网络安全成为关键问题，尤其是在CI/CD流程中，需要系统性地识别研究空白和改进方向。

Method: 通过SLR方法，分析了66篇论文，总结了CI/CD在云环境中的安全工具、方法和挑战。

Result: 发现Harbor、SonarQube等工具的应用，以及镜像篡改、未授权访问等安全挑战，揭示了研究空白。

Conclusion: 需要进一步研究以改进云环境中CI/CD的安全解决方案。

Abstract: As cloud environments become widespread, cybersecurity has emerged as a top
priority across areas such as networks, communication, data privacy, response
times, and availability. Various sectors, including industries, healthcare, and
government, have recently faced cyberattacks targeting their computing systems.
Ensuring secure app deployment in cloud environments requires substantial
effort. With the growing interest in cloud security, conducting a systematic
literature review (SLR) is critical to identifying research gaps. Continuous
Software Engineering, which includes continuous integration (CI), delivery
(CDE), and deployment (CD), is essential for software development and
deployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,
and challenges related to the security of CI/CD in the cloud. We addressed key
aspects of cloud security and CI/CD and reported on tools such as Harbor,
SonarQube, and GitHub Actions. Challenges such as image manipulation,
unauthorised access, and weak authentication were highlighted. The review also
uncovered research gaps in how tools and practices address these security
issues in CI/CD pipelines, revealing a need for further study to improve
cloud-based security solutions.

</details>


### [17] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 研究提出了一种基于指标的架构模型，用于管理和评估机器学习赋能系统（MLES）的复杂性，旨在支持架构决策。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂性如何影响MLES，并寻求有效管理方法。

Method: 扩展参考架构以描述MLES并收集指标，作为构建指标模型的第一步。

Result: 提出了一个初步的扩展架构框架。

Conclusion: 该研究为MLES的复杂性管理提供了初步工具和方向。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [18] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLMs）在程序最坏情况执行中的符号约束分析能力，通过SMT约束求解和专门设计的数据集提升模型性能。实验表明，3B规模的LLM能够通过强化学习方法恢复算法的最坏行为约束。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂符号推理任务中的潜力，尤其是程序最坏情况执行的符号约束分析，以连接LLMs与符号推理方法。

Method: 定义并解决最坏情况符号约束分析问题，通过SMT约束求解和专门设计的符号约束数据集进行模型微调。

Result: 实验证明3B规模的LLM（WARP-1.0-3B）在符号约束分析任务中表现优于规模匹配甚至更大的基线模型。

Conclusion: LLMs具备深度符号推理能力，支持神经网络学习与形式化方法的更紧密集成。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [19] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: Repeton是一个开源框架，利用LLMs进行精确的代码操作，通过分步的补丁测试流程提升复杂软件工程任务的精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成和理解方面表现强大，但在复杂软件工程任务中精度低且可解释性有限。

Method: Repeton采用结构化的补丁测试流程，迭代诊断问题、提出代码变更并通过自动化测试验证每个补丁，同时利用轻量级启发式方法和开发工具。

Result: 在SWE-bench Lite基准测试中，Repeton在补丁有效性和可解释性方面优于RAG-based方法。

Conclusion: 通过模块化和可验证的阶段分解软件工程任务，Repeton为可扩展且透明的自主调试提供了实用路径。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [20] [MBTModelGenerator: A software tool for reverse engineering of Model-based Testing (MBT) models from clickstream data of web applications](https://arxiv.org/abs/2506.08179)
*Sasidhar Matta,Vahid Garousi*

Main category: cs.SE

TL;DR: 开发了一个开源工具，通过用户点击流数据自动生成基于模型的测试（MBT）模型，减少手动建模的负担。


<details>
  <summary>Details</summary>
Motivation: 自动化测试中创建测试模型和套件仍然费时费力，希望通过利用用户实际行为数据降低MBT的采用门槛。

Method: 工具捕获UI事件，将其转换为状态转移模型，并导出为GraphWalker兼容格式，实现无需手动建模的测试执行。

Result: 工具成功生成MBT模型并支持即时测试执行，减少了前期建模的依赖。

Conclusion: 该工具通过开源方式提供，有效降低了MBT的采用难度，并展示了实际应用潜力。

Abstract: Automated testing has become a standard practice in software engineering, yet
the creation of test models and suites remains labor-intensive. To reduce this
effort, we developed an open-source tool that automatically generates
Model-Based Testing (MBT) models from clickstream data collected during user
interaction with web applications. The tool captures UI events, transforms them
into state-transition models, and exports the result in a format compatible
with the GraphWalker MBT tool. This enables immediate test execution without
the need for manual model creation. The approach lowers the barrier to MBT
adoption by leveraging actual usage behavior and reducing the reliance on
upfront modeling. This technical report documents the system requirements,
design decisions, implementation details, testing process, and empirical
evaluation of the tool, which is publicly available as open-source.

</details>


### [21] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: 该论文首次系统研究了软件工程代理（SWE代理）的行为，通过执行轨迹分析提出了决策路径的分类法，并深入研究了影响代理成功的三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，SWE代理在自动化软件任务中表现出强大能力，但其内部决策机制尚不清晰，研究这些机制有助于提升代理的可靠性和效率。

Method: 通过执行轨迹分析，提出决策路径分类法，并深入研究三个核心组件：错误定位、补丁生成和测试生成。同时进行大规模代码克隆分析和定性研究。

Result: 研究发现测试生成对补丁生成的成功有显著影响，并揭示了代理生成补丁与开发者编写补丁在结构和风格上的差异。

Conclusion: 研究结果为代理设计提供了新见解，有助于开发更高效且更符合人类开发实践的代理。

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [22] [Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis](https://arxiv.org/abs/2506.08561)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: PriceSleuth是一种结合大型语言模型（LLM）和静态分析的方法，用于主动检测DeFi智能合约中的价格操纵攻击。


<details>
  <summary>Details</summary>
Motivation: 随着DeFi协议的普及，价格操纵攻击频发，攻击者通过操纵代币价格非法获利，亟需一种有效的检测方法。

Method: PriceSleuth通过识别价格计算逻辑、依赖分析和传播分析，结合LLM定位潜在的价格操纵代码。

Result: 初步实验结果验证了PriceSleuth的有效性。

Conclusion: PriceSleuth为DeFi安全提供了新思路，未来研究将进一步优化其性能。

Abstract: An increasing number of DeFi protocols are gaining popularity, facilitating
transactions among multiple anonymous users. State Manipulation is one of the
notorious attacks in DeFi smart contracts, with price variable being the most
commonly exploited state variable-attackers manipulate token prices to gain
illicit profits. In this paper, we propose PriceSleuth, a novel method that
leverages the Large Language Model (LLM) and static analysis to detect Price
Manipulation (PM) attacks proactively. PriceSleuth firstly identifies core
logic function related to price calculation in DeFi contracts. Then it guides
LLM to locate the price calculation code statements. Secondly, PriceSleuth
performs backward dependency analysis of price variables, instructing LLM in
detecting potential price manipulation. Finally, PriceSleuth utilizes
propagation analysis of price variables to assist LLM in detecting whether
these variables are maliciously exploited. We presented preliminary
experimental results to substantiate the effectiveness of PriceSleuth . And we
outline future research directions for PriceSleuth.

</details>


### [23] [Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification](https://arxiv.org/abs/2506.08581)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: 该研究评估了Sentence-BERT在多标签代码注释分类任务中的表现，旨在最大化分类性能的同时控制推理效率。通过微调Sentence-BERT模型并结合不同分类头，研究发现较大模型在F1分数上表现更好，但较小模型在效率上更优。最终实现了F1分数提升（+0.0346）与效率损失（运行时间+1.4倍，GFLOPS+2.1倍）的平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在多标签代码注释分类任务中，如何在保持高效率的同时最大化分类性能。

Method: 方法包括使用13,216条标记的注释句子数据集，对Sentence-BERT模型进行微调，并结合不同分类头进行分类。

Result: 结果显示较大模型在F1分数上优于较小模型，但较小模型在运行时间和GFLOPS上效率更高。最终实现了性能与效率的平衡。

Conclusion: 结论表明，通过权衡模型大小，可以在多标签代码注释分类任务中实现性能与效率的平衡。

Abstract: This work evaluates Sentence-BERT for a multi-label code comment
classification task seeking to maximize the classification performance while
controlling efficiency constraints during inference. Using a dataset of 13,216
labeled comment sentences, Sentence-BERT models are fine-tuned and combined
with different classification heads to recognize comment types. While larger
models outperform smaller ones in terms of F1, the latter offer outstanding
efficiency, both in runtime and GFLOPS. As result, a balance between a
reasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x
in runtime and +2.1x in GFLOPS) is reached.

</details>


### [24] [RE-oriented Model Development with LLM Support and Deduction-based Verification](https://arxiv.org/abs/2506.08606)
*Radoslaw Klimek*

Main category: cs.SE

TL;DR: 提出一个结合UML图、大语言模型（LLMs）和形式化验证的框架，以提升需求工程（RE）阶段的质量。


<details>
  <summary>Details</summary>
Motivation: 需求工程阶段对软件开发至关重要，但传统方法可能效率不足，需要更高效的工具支持。

Method: 结合UML图、LLMs和逻辑推理引擎，生成行为模型并自动转换为形式化逻辑规范，再通过形式化验证确保需求完整性。

Result: 框架能够自动生成程序骨架，简化从设计到实现的过渡。

Conclusion: 该框架显著提升了需求工程阶段的效率和质量，为软件开发提供了更高效的工具支持。

Abstract: The requirements engineering (RE) phase is pivotal in developing high-quality
software. Integrating advanced modelling techniques with large language models
(LLMs) and formal verification in a logical style can significantly enhance
this process. We propose a comprehensive framework that focuses on specific
Unified Modelling Language (UML) diagrams for preliminary system development.
This framework offers visualisations at various modelling stages and seamlessly
integrates large language models and logical reasoning engines. The behavioural
models generated with the assistance of LLMs are automatically translated into
formal logical specifications. Deductive formal verification ensures that
logical requirements and interrelations between software artefacts are
thoroughly addressed. Ultimately, the framework facilitates the automatic
generation of program skeletons, streamlining the transition from design to
implementation.

</details>


### [25] [Logic Mining from Process Logs: Towards Automated Specification and Verification](https://arxiv.org/abs/2506.08628)
*Radoslaw Klimek,Julia Witek*

Main category: cs.SE

TL;DR: 本文提出了一种从工作流挖掘发现的流程模型中生成逻辑规范的方法，结合模式翻译与自动推理技术，并在通用和实际事件日志上评估其效果。


<details>
  <summary>Details</summary>
Motivation: 自动化生成逻辑规范对于复杂系统的行为模型分析至关重要，可减少人工构建的时间和错误。

Method: 采用模式翻译与自动推理技术，从流程模型生成逻辑规范，并使用自动定理证明器验证其性质。

Result: 方法在通用和实际事件日志上表现良好，支持其在现实场景中的应用。

Conclusion: 该方法适用于实际环境，并有望整合到实证软件工程实践中。

Abstract: Logical specifications play a key role in the formal analysis of behavioural
models. Automating the derivation of such specifications is particularly
valuable in complex systems, where manual construction is time-consuming and
error-prone. This article presents an approach for generating logical
specifications from process models discovered via workflow mining, combining
pattern-based translation with automated reasoning techniques. In contrast to
earlier work, we evaluate the method on both general-purpose and real-case
event logs, enabling a broader empirical assessment. The study examines the
impact of data quality, particularly noise, on the structure and testability of
generated specifications. Using automated theorem provers, we validate a
variety of logical properties, including satisfiability, internal consistency,
and alignment with predefined requirements. The results support the
applicability of the approach in realistic settings and its potential
integration into empirical software engineering practices.

</details>


### [26] [Proceedings of the 23rd International Overture Workshop](https://arxiv.org/abs/2506.08680)
*Hugo Daniel Macedo,Ken Pierce*

Main category: cs.SE

TL;DR: 第23届国际Overture研讨会论文集，聚焦VDM和Overture技术的最新进展。


<details>
  <summary>Details</summary>
Motivation: 推动VDM和Overture技术在系统开发中的应用，促进学术界与工业界的合作。

Method: 通过研讨会分享VDM相关工具（如Overture、Crescendo）和建模语言（如VDM-SL、VDM++）的最新发展。

Result: 展示了VDM/Overture在静态/动态分析、测试生成、模型检查等领域的技术进展。

Conclusion: VDM和Overture技术持续发展，为复杂系统建模与分析提供了强大支持。

Abstract: This volume contains the papers presented at the 23rd International Overture
Workshop, held on the 11th of June 2025. This event was the latest in a series
of workshops around the Vienna Development Method (VDM), the open-source
project Overture, and related tools and formalisms. VDM is one of the longest
established formal methods for systems development. A lively community of
researchers and practitioners has grown up in academia and industry has grown
around the modelling languages (VDM-SL, VDM++, VDM-RT, CML) and tools
(VDMTools, Overture, Crescendo, Symphony, the INTO-CPS chain, and ViennaTalk).
Together, these provide a platform for work on modelling and analysis
technology that includes static and dynamic analysis, test generation,
execution support, and model checking. This workshop provided updates on the
emerging technology of VDM/Overture, including collaboration infrastructure,
collaborative modelling and co-simulation for Cyber-Physical Systems.

</details>


### [27] [Causality-aware Safety Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.08688)
*Wenbing Tang,Mingfei Cheng,Renzhi Wang,Yuan Zhou,Chengwei Liu,Yang Liu,Zuohua Ding*

Main category: cs.SE

TL;DR: 本文提出了一种名为Causal-Fuzzer的新型因果关系感知模糊测试技术，用于高效全面地测试自动驾驶系统（ADS）。通过构建因果图来建模输入场景、ADS运动命令和系统违规之间的复杂关系，并基于此生成关键测试场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常关注单个多样性指标，而忽略了这些元素之间的复杂相互关系，导致测试覆盖不足。本文旨在通过因果关系建模解决这一问题。

Method: Causal-Fuzzer通过构建因果图来建模多样性的相互关系，并提出基于因果关系的反馈机制和突变策略，以优先处理对系统行为有更高因果影响的元素。

Result: 在工业级ADS Apollo上的实验表明，Causal-Fuzzer在识别更多违规、提高测试充分性和检测效率方面显著优于现有方法。

Conclusion: Causal-Fuzzer通过因果关系建模和优化测试策略，显著提升了自动驾驶系统的测试效果和效率。

Abstract: Simulation-based testing is essential for evaluating the safety of Autonomous
Driving Systems (ADSs). Comprehensive evaluation requires testing across
diverse scenarios that can trigger various types of violations under different
conditions. While existing methods typically focus on individual diversity
metrics, such as input scenarios, ADS-generated motion commands, and system
violations, they often fail to capture the complex interrelationships among
these elements. This oversight leads to gaps in testing coverage, potentially
missing critical issues in the ADS under evaluation. However, quantifying these
interrelationships presents a significant challenge. In this paper, we propose
a novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient
and comprehensive testing of ADSs by exploring causally diverse scenarios. The
core of Causal-Fuzzer is constructing a causal graph to model the
interrelationships among the diversities of input scenarios, ADS motion
commands, and system violations. Then the causal graph will guide the process
of critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a
causality-based feedback mechanism that quantifies the combined diversity of
test scenarios by assessing whether they activate new causal relationships, and
(2) a causality-driven mutation strategy that prioritizes mutations on input
scenario elements with higher causal impact on ego action changes and violation
occurrence, rather than treating all elements equally. We evaluated
Causal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our
empirical results demonstrate that Causal-Fuzzer significantly outperforms
existing methods in (1) identifying a greater diversity of violations, (2)
providing enhanced testing sufficiency with improved coverage of causal
relationships, and (3) achieving greater efficiency in detecting the first
critical scenarios.

</details>


### [28] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 研究探讨了AI生成代码的可持续性问题，发现主流工具（如ChatGPT、BARD、Copilot）生成的代码普遍不符合绿色编码实践，需进一步研究改进。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的普及，其代码生成能力被广泛应用，但缺乏对其可持续性（如资源利用和环境友好性）的研究。

Method: 通过分析三种流行生成式AI工具（ChatGPT、BARD、Copilot）生成的代码，评估其是否符合可持续编码实践。

Result: 结果显示这些工具生成的代码普遍不符合绿色编码标准，存在资源浪费和环境不友好的问题。

Conclusion: 研究强调需深入探讨AI生成代码的可持续性，并制定有效的改进策略。

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [29] [Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development](https://arxiv.org/abs/2506.08812)
*Priyavanshi Pathania,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 论文提出构建标准化的可持续性代码弱点知识库，并初步探索其方法，强调现有知识不能直接重用于可持续性领域。


<details>
  <summary>Details</summary>
Motivation: 气候危机下，开发可持续软件系统需优化资源利用并减少环境影响，但目前缺乏标准化知识库支持相关工具开发。

Method: 提出构建可持续性代码弱点的标准知识库，并通过初步实验验证其必要性。

Result: 实验表明现有软件弱点知识不能直接重用于可持续性领域，需进一步探索。

Conclusion: 呼吁开发标准化知识库以支持可持续性软件优化工具，并强调该领域的进一步研究重要性。

Abstract: With the climate crisis looming, engineering sustainable software systems
become crucial to optimize resource utilization, minimize environmental impact,
and foster a greener, more resilient digital ecosystem. For developers, getting
access to automated tools that analyze code and suggest sustainabilityrelated
optimizations becomes extremely important from a learning and implementation
perspective. However, there is currently a dearth of such tools due to the lack
of standardized knowledge, which serves as the foundation of these tools. In
this paper, we motivate the need for the development of a standard knowledge
base of commonly occurring sustainability weaknesses in code, and propose an
initial way of doing that. Furthermore, through preliminary experiments, we
demonstrate why existing knowledge regarding software weaknesses cannot be
re-tagged "as is" to sustainability without significant due diligence, thereby
urging further explorations in this ecologically significant domain.

</details>


### [30] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文研究了代码审查中的偏差现象，提出了一种检测方法并验证了其对机器学习模型的影响。


<details>
  <summary>Details</summary>
Motivation: 工业中的合并请求（MR）流程常偏离标准化审查过程，导致分析偏差和机器学习模型效果下降。

Method: 识别了七种偏差类别，并提出了一种少样本学习检测方法。

Result: 排除偏差后，机器学习模型在53.33%的情况下性能提升（最高2.25倍），特征重要性显著变化。

Conclusion: 研究为优化代码审查提供了分类、检测方法和实证支持，有助于提升分析可靠性。

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


### [31] [AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation](https://arxiv.org/abs/2506.08980)
*Kaifeng He,Mingwei Liu,Chong Wang,Zike Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文提出了一种名为AdaDec的自适应解码框架，通过基于不确定性的暂停-重排机制，显著提升了代码生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究发现，代码生成中的许多错误源于高不确定性步骤中的排名错误，而标准解码策略（如贪婪搜索和束搜索）未能有效处理这些模式。

Method: AdaDec框架结合了基于香农熵的暂停-重排机制，学习模型特定的不确定性阈值，并在高不确定性时应用前瞻性重排策略。

Result: 在HumanEval和MBPP基准测试中，AdaDec将Pass@1准确率提高了15.5%，优于或匹配束搜索，同时降低了计算成本和延迟。

Conclusion: AdaDec展示了不确定性感知的自适应解码在提升LLM代码生成可靠性和效率方面的潜力。

Abstract: Code generation with large language models (LLMs) is highly sensitive to
token selection during decoding, particularly at uncertain decision points that
influence program logic. While standard strategies like greedy and beam search
treat all tokens uniformly, they overlook code-specific uncertainty patterns,
leading to suboptimal performance. This paper presents an empirical study
revealing that many generation errors stem from ranking mistakes at
high-uncertainty steps, where the correct token is present but not top-ranked.
  Motivated by these findings, we propose AdaDec, an uncertainty-guided
adaptive decoding framework that integrates a token-level pause-then-rerank
mechanism driven by token uncertainty (Shannon entropy). AdaDec learns
model-specific uncertainty thresholds and applies a lookahead-based reranking
strategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks
show that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding,
outperforms or matches beam search, and reduces computational cost and latency
through efficient, selective pausing. Our results highlight the promise of
uncertainty-aware adaptive decoding for improving the reliability and
efficiency of LLM-based code generation.

</details>


### [32] [Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models](https://arxiv.org/abs/2506.09002)
*Bei Chu,Yang Feng,Kui Liu,Hange Shi,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: PALM利用大型语言模型（LLMs）和程序分析生成高覆盖率的单元测试，显著提升测试覆盖率，效果接近人工水平。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成单元测试时难以处理复杂程序单元，导致覆盖率低。LLMs虽有潜力，但固定提示导致编译成功率和覆盖率不高。

Method: PALM通过程序分析识别分支条件，结合路径约束和上下文信息构建提示，指导LLMs生成测试用例。

Result: 在10个Rust项目中，PALM在2-3小时内显著提升覆盖率（最高超50%），平均覆盖率达75.77%，接近人工水平（71.30%）。91个提交测试中80个被接受。

Conclusion: PALM展示了LLMs与程序分析结合的潜力，为自动化测试开辟了新方向。

Abstract: Unit testing is essential for ensuring software reliability and correctness.
Classic Search-Based Software Testing (SBST) methods and concolic
execution-based approaches for generating unit tests often fail to achieve high
coverage due to difficulties in handling complex program units, such as
branching conditions and external dependencies. Recent work has increasingly
utilized large language models (LLMs) to generate test cases, improving the
quality of test generation by providing better context and correcting errors in
the model's output. However, these methods rely on fixed prompts, resulting in
relatively low compilation success rates and coverage. This paper presents
PALM, an approach that leverages large language models (LLMs) to enhance the
generation of high-coverage unit tests. PALM performs program analysis to
identify branching conditions within functions, which are then combined into
path constraints. These constraints and relevant contextual information are
used to construct prompts that guide the LLMs in generating unit tests. We
implement the approach and evaluate it in 10 open-source Rust crates.
Experimental results show that within just two or three hours, PALM can
significantly improves test coverage compared to classic methods, with
increases in overall project coverage exceeding 50% in some instances and its
generated tests achieving an average coverage of 75.77%, comparable to human
effort (71.30%), highlighting the potential of LLMs in automated test
generation. We submitted 91 PALM-generated unit tests targeting new code. Of
these submissions, 80 were accepted, 5 were rejected, and 6 remain pending
review. The results demonstrate the effectiveness of integrating program
analysis with AI and open new avenues for future research in automated software
testing.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [33] [Congestion-Aware Path Selection for Load Balancing in AI Clusters](https://arxiv.org/abs/2506.08132)
*Erfan Nosrati,Majid Ghaderi*

Main category: cs.NI

TL;DR: Hopper是一种专为AI集群中RDMA流量优化的负载均衡技术，无需专用硬件或交换机修改，通过动态路径切换减少拥塞，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型机器学习模型的分布式训练依赖于高效的GPU互联网络，现有负载均衡技术不适用于RDMA流量或需要专用硬件。

Method: Hopper在主机层面运行，实时监测路径拥塞并动态切换流量，同时控制路径切换时机以减少乱序数据包。

Result: 实验表明，Hopper将平均和尾部流完成时间分别降低20%和14%，优于现有主机负载均衡技术。

Conclusion: Hopper为AI集群中的RDMA流量提供了一种高效、无需硬件修改的负载均衡解决方案。

Abstract: Fast training of large machine learning models requires distributed training
on AI clusters consisting of thousands of GPUs. The efficiency of distributed
training crucially depends on the efficiency of the network interconnecting
GPUs in the cluster. These networks are commonly built using RDMA following a
Clos-like datacenter topology. To efficiently utilize the network bandwidth,
load balancing is employed to distribute traffic across multiple redundant
paths. While there exists numerous techniques for load-balancing in traditional
datacenters, these are often either optimized for TCP traffic or require
specialized network hardware, thus limiting their utility in AI clusters.
  This paper presents the design and evaluation of Hopper, a new load-balancing
technique optimized for RDMA traffic in AI clusters. Operating entirely at the
host level, Hopper requires no specialized hardware or modifications to network
switches. It continuously monitors the current path for congestion and
dynamically switches traffic to a less congested path when congestion is
detected. Furthermore, it incorporates a lightweight mechanism to identify
alternative paths and carefully controls the timing of path switching to
prevent excessive out-of-order packets.
  We evaluated Hopper using ns-3 simulations and a testbed implementation. Our
evaluations show that Hopper reduces the average and 99-percentile tail flow
completion time by up to 20% and 14%, respectively, compared to
state-of-the-art host-based load balancing techniques.

</details>


### [34] [5G Aero: A Prototyping Platform for Evaluating Aerial 5G Communications](https://arxiv.org/abs/2506.08386)
*Matteo Bordin,Madhukara S. Holla,Sakthivel Velumani,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文介绍了5G Aero无人机，优化了5G连接性，满足3GPP标准，实验证明其在视距和非视距条件下均能实现高吞吐量、低延迟通信。


<details>
  <summary>Details</summary>
Motivation: 确保无人机在5G网络中实现可靠、高吞吐量和低延迟通信是一个关键但尚未充分探索的问题。

Method: 在室内环境中进行实验，评估无人机在视距和非视距条件下的通信性能。

Result: 5G Aero满足3GPP标准，通信延迟低，且5G模块对飞行时间影响极小（仅1%）。

Conclusion: 研究表明5G网络能有效支持无人机高级操作，为未来优化提供了方向。

Abstract: The application of small-factor, 5G-enabled Unmanned Aerial Vehicles (UAVs)
has recently gained significant interest in various aerial and Industry 4.0
applications. However, ensuring reliable, high-throughput, and low-latency 5G
communication in aerial applications remains a critical and underexplored
problem. This paper presents the 5th generation (5G) Aero, a compact UAV
optimized for 5G connectivity, aimed at fulfilling stringent 3rd Generation
Partnership Project (3GPP) requirements. We conduct a set of experiments in an
indoor environment, evaluating the UAV's ability to establish high-throughput,
low-latency communications in both Line-of-Sight (LoS) and Non-Line-of-Sight
(NLoS) conditions. Our findings demonstrate that the 5G Aero meets the required
3GPP standards for Command and Control (C2) packets latency in both LoS and
NLoS, and video latency in LoS communications and it maintains acceptable
latency levels for video transmission in NLoS conditions. Additionally, we show
that the 5G module installed on the UAV introduces a negligible 1% decrease in
flight time, showing that 5G technologies can be integrated into commercial
off-the-shelf UAVs with minimal impact on battery lifetime. This paper
contributes to the literature by demonstrating the practical capabilities of
current 5G networks to support advanced UAV operations in telecommunications,
offering insights into potential enhancements and optimizations for UAV
performance in 5G networks

</details>


### [35] [Aerial Shepherds: Enabling Hierarchical Localization in Heterogeneous MAV Swarms](https://arxiv.org/abs/2506.08408)
*Haoyang Wang,Jingao Xu,Chenyu Zhao,Yuhan Cheng,Xuecheng Chen,Chaopeng Hong,Xiao-Ping Zhang,Yunhao Liu,Xinlei Chen*

Main category: cs.NI

TL;DR: TransformLoc框架将高级微型飞行器（AMAVs）转化为移动定位基础设施，为低成本、资源受限的基础微型飞行器（BMAVs）提供实时高精度定位，显著提升定位性能和导航成功率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏低成本、高精度且实时的定位解决方案，尤其是针对轻量级BMAVs。通过将AMAVs转化为移动定位基础设施，可以填补这一空白。

Method: 设计了错误感知的联合位置估计模型和基于相似性的自适应分组调度策略，动态分配AMAVs资源。

Result: 定位性能提升68%，导航成功率提高60%，并通过鲁棒性和消融实验验证了设计的优越性。

Conclusion: TransformLoc为大规模异构MAV群提供了一种协作、自适应且经济高效的定位系统。

Abstract: A heterogeneous micro aerial vehicles (MAV) swarm consists of
resource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but
cost-effective basic MAVs (BMAVs), offering opportunities in diverse fields.
Accurate and real-time localization is crucial for MAV swarms, but current
practices lack a low-cost, high-precision, and real-time solution, especially
for lightweight BMAVs. We find an opportunity to accomplish the task by
transforming AMAVs into mobile localization infrastructures for BMAVs. However,
translating this insight into a practical system is challenging due to issues
in estimating locations with diverse and unknown localization errors of BMAVs,
and allocating resources of AMAVs considering interconnected influential
factors. This work introduces TransformLoc, a new framework that transforms
AMAVs into mobile localization infrastructures, specifically designed for
low-cost and resource-constrained BMAVs. We design an error-aware joint
location estimation model to perform intermittent joint estimation for BMAVs
and introduce a similarity-instructed adaptive grouping-scheduling strategy to
allocate resources of AMAVs dynamically. TransformLoc achieves a collaborative,
adaptive, and cost-effective localization system suitable for large-scale
heterogeneous MAV swarms. We implement and validate TransformLoc on industrial
drones. Results show it outperforms all baselines by up to 68\% in localization
performance, improving navigation success rates by 60\%. Extensive robustness
and ablation experiments further highlight the superiority of its design.

</details>


### [36] [Deep Reinforcement Learning-Based RAN Slicing with Efficient Inter-Slice Isolation in Tactical Wireless Networks](https://arxiv.org/abs/2506.09039)
*Abderrahime Filali,Diala Naboulsi,Georges Kaddoum*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度强化学习（DRL）的RAN切片机制，用于在战术网络中实现带宽共享与切片隔离的平衡，并通过O-RAN架构实现高效管理。


<details>
  <summary>Details</summary>
Motivation: 战术网络（TNs）需要动态带宽切片策略以应对带宽稀缺问题，但现有策略在服务质量（QoS）隔离方面存在不足。

Method: 采用两阶段带宽分配机制：第一阶段为RAN切片分配带宽，第二阶段为每个切片的用户分配带宽，均基于DRL算法。

Result: 开发了三种基于不同DRL算法的实现，并通过多参数评估验证了其性能优于基线方法。

Conclusion: 提出的机制在O-RAN架构中有效平衡了带宽共享与切片隔离，为战术网络提供了创新解决方案。

Abstract: The next generation of tactical networks (TNs) is poised to further leverage
the key enablers of 5G and beyond 5G (B5G) technology, such as radio access
network (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple
architectural options and opportunities for a wide range of innovative
applications. RAN slicing and the O-RAN paradigm are considered game changers
in TNs, where the former makes it possible to tailor user services to users
requirements, and the latter brings openness and intelligence to the management
of the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing
strategy. Although this type of strategy ensures efficient bandwidth
utilization, it compromises RAN slicing isolation in terms of quality of
service (QoS) performance. To deal with this challenge, we propose a deep
reinforcement learning (DRL)-based RAN slicing mechanism that achieves a
trade-off between efficient RAN bandwidth sharing and appropriate inter- and
intra-slice isolation. The proposed mechanism performs bandwidth allocation in
two stages. In the first stage, the bandwidth is allocated to the RAN slices.
In the second stage, each slice partitions its bandwidth among its associated
users. In both stages, the slicing operation is constrained by several
considerations related to improving the QoS of slices and users that in turn
foster inter- and intra-slice isolation. The proposed RAN slicing mechanism is
based on DRL algorithms to perform the bandwidth sharing operation in each
stage. We propose to deploy the mechanism in an O-RAN architecture and describe
the O-RAN functional blocks and the main DRL model lifecycle management phases
involved. We also develop three different implementations of the proposed
mechanism, each based on a different DRL algorithm, and evaluate their
performance against multiple baselines across various parameters.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: KVmix是一种新型的混合精度量化方法，用于解决LLM推理中KV Cache的高内存需求问题。它通过梯度重要性分析和动态优化策略，实现了内存压缩与推理速度的提升。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的KV Cache内存需求过高，限制了其在资源受限平台上的部署。现有方法无法动态分配精度或优化长上下文任务中的关键KV对。

Method: KVmix利用梯度重要性分析评估KV投影矩阵对模型损失的影响，实现分层位宽分配。同时，动态优化策略保留重要KV对的全精度，压缩不重要的部分。

Result: 在Llama和Mistral等LLM上，KVmix实现了接近无损的推理性能（Key 2.19bit，Value 2.38bit），内存压缩4.9倍，推理速度提升5.3倍。

Conclusion: KVmix通过动态混合精度量化和长上下文优化，显著降低了KV Cache的内存需求，同时保持了高推理性能。

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [38] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 半监督方法将难民统计数据从行政边界分解到0.5度网格单元，结合多种数据源实现高精度空间分布。


<details>
  <summary>Details</summary>
Motivation: 解决传统区域和国家统计数据中难民分布模式不清晰的问题。

Method: 整合UNHCR注册数据、Google建筑足迹和OpenStreetMap位置坐标，使用标签传播算法生成高粒度难民统计数据。

Result: 平均准确率达92.9%，覆盖1000多万难民观测数据，揭示局部位移模式。

Conclusion: 高分辨率数据集为深入理解难民位移驱动因素提供了基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [39] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: BUOT模型通过双层不平衡最优传输框架，同时表征样本级和类级关系，解决了部分域适应中的异常类识别问题。


<details>
  <summary>Details</summary>
Motivation: 部分域适应问题需要对齐跨域样本并识别异常类，但现有加权框架仅关注样本级关系，导致对聚类结构探索不足。

Method: 提出Bi-level Unbalanced Optimal Transport (BUOT)模型，结合样本级和类级传输，通过协作机制提供结构信息和判别信息。

Result: 在基准数据集上的实验验证了BUOT的竞争力。

Conclusion: BUOT通过双层传输框架有效解决了部分域适应中的异常类识别问题，提升了知识转移的准确性。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [40] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: 提出了一种基于知识迁移的通用流场预测框架，结合POD降维和LLM微调，显著降低计算成本并提升跨条件迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统CFD方法计算成本高和现有深度学习模型跨条件迁移能力有限的问题。

Method: 集成POD降维与LLM微调策略，设计流体动力学文本模板以增强预测性能。

Result: 在少样本学习场景中优于传统Transformer模型，预测时间从小时级降至秒级，精度超90%。

Conclusion: 该框架为快速流体动力学预测开辟了新方向，适用于气动优化和流动控制等领域。

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [41] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: MBPO提出了一种新的偏好学习框架，通过生成硬负样本和在线验证奖励，解决LMM中的模态不平衡问题，提升性能并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在推理中存在模态不平衡问题，语言先验偏见压倒视觉输入，限制了泛化能力并导致幻觉。现有偏好优化方法未有效抑制LLM内部偏见，且依赖离线数据。

Method: MBPO通过对抗性扰动生成硬负样本构建离线偏好数据集，并利用闭端任务生成在线验证奖励，结合GRPO训练模型。

Result: 实验表明，MBPO能显著提升LMM在视觉语言任务中的表现，并有效减少幻觉。

Conclusion: MBPO通过模态平衡偏好优化，解决了LMM的模态不平衡问题，为下游任务提供了更强的泛化能力。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [42] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [43] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: 研究发现，简单的k-NN基线方法在网络流量分类任务中表现优异，甚至超越复杂模型，原因是数据集中存在大量冗余样本，导致性能评估失真。


<details>
  <summary>Details</summary>
Motivation: 探究为何简单的k-NN方法在网络流量分类中表现优异，并揭示数据集冗余对模型性能的影响。

Method: 在12个数据集和15个分类任务中评估k-NN基线方法，分析数据冗余现象及其对性能的影响。

Result: 发现多数数据集包含超过50%的冗余样本，导致性能评估失真，且复杂模型可能因冗余数据而表现不佳。

Conclusion: 提出新的任务定义和评估方向，以适应网络流量分类的独特需求，避免因数据冗余导致的性能误判。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [44] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: 论文探讨了在预训练中使用低精度表示（如MX格式）提升GPU效率的方法，并提出了一种改进的舍入模式以避免模型发散。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保持精度的同时，通过低精度表示提升GPU效率，并解决MX格式在实际应用中可能导致模型发散的问题。

Method: 提出了一种改进的舍入模式（round-to-infinity）用于计算缩放因子，以稳定MXFP8格式的预训练。

Result: 使用改进的舍入模式成功在15T tokens上预训练了一个8B参数的LLM模型。

Conclusion: 改进的舍入模式能够有效解决MX格式在预训练中的发散问题，为低精度预训练提供了实用方案。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [45] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: ST-GraphNet是一种时空图神经网络框架，用于建模和预测自动驾驶车辆（AV）碰撞严重性，通过细粒度和区域聚合的空间图实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 研究AV碰撞严重性的时空动态对提升城市交通安全和基础设施规划至关重要。

Method: 构建两种图表示：细粒度图（节点为单个碰撞事件）和粗粒度图（基于H3空间单元的聚合）。节点包含多模态数据（语义、空间、时间属性），并评估多种GNN架构。

Result: ST-GraphNet在粗粒度H3图上使用DSTGCN，测试准确率达97.74%，显著优于细粒度模型（64.7%）。

Conclusion: 空间聚合、动态消息传递和多模态特征整合能有效捕捉AV碰撞严重性的复杂时空模式。

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [46] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: 该论文研究共识聚类问题，首次结合公平性要求，提出了一种常数因子近似算法，并探讨了如何最小化修改现有聚类以实现公平性。


<details>
  <summary>Details</summary>
Motivation: 共识聚类是机器学习和数据分析中的基础任务，但现有方法未考虑公平性。论文旨在解决这一问题，确保每个聚类中保护群体的比例均衡。

Method: 通过研究公平聚类问题，提出了一种最优算法（适用于等比例群体）和近似算法（适用于不等比例群体），并证明了问题的NP-hard性质。

Result: 论文提供了常数因子近似算法，并在不同群体比例下实现了高效求解。同时，证明了问题在不等比例群体下的NP-hard性。

Conclusion: 该研究填补了公平共识聚类的空白，其成果对其他公平聚类问题具有广泛意义。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [47] [GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity](https://arxiv.org/abs/2210.16402)
*Artavazd Maranjyan,Mher Safaryan,Peter Richtárik*

Main category: cs.LG

TL;DR: 论文提出了一种改进的分布式优化算法GradSkip，通过减少不重要数据的本地训练步骤，保持通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有ProxSkip方法要求所有客户端在每轮通信中执行相同数量本地训练步骤的限制。

Method: 重新设计ProxSkip方法，允许数据重要性较低的客户端减少本地训练步骤，同时保持通信加速特性。

Result: GradSkip在相同假设下线性收敛，通信复杂度不变，且本地梯度步骤可减少。

Conclusion: GradSkip及其扩展GradSkip+在理论和实验中均表现优异，并能涵盖文献中多种相关方法。

Abstract: We study a class of distributed optimization algorithms that aim to alleviate
high communication costs by allowing clients to perform multiple local
gradient-type training steps before communication. In a recent breakthrough,
Mishchenko et al. (2022) proved that local training, when properly executed,
leads to provable communication acceleration, and this holds in the strongly
convex regime without relying on any data similarity assumptions. However,
their ProxSkip method requires all clients to take the same number of local
training steps in each communication round. We propose a redesign of the
ProxSkip method, allowing clients with ``less important'' data to get away with
fewer local training steps without impacting the overall communication
complexity of the method. In particular, we prove that our modified method,
GradSkip, converges linearly under the same assumptions and has the same
accelerated communication complexity, while the number of local gradient steps
can be reduced relative to a local condition number. We further generalize our
method by extending the randomness of probabilistic alternations to arbitrary
unbiased compression operators and by considering a generic proximable
regularizer. This generalization, which we call GradSkip+, recovers several
related methods in the literature as special cases. Finally, we present an
empirical study on carefully designed toy problems that confirm our theoretical
claims.

</details>


### [48] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: STAMImputer提出了一种基于时空注意力专家混合网络的交通数据填补方法，解决了块缺失数据特征提取和静态图结构灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在块缺失数据场景下特征提取效果不佳，且静态图结构无法适应非平稳交通数据的分布变化。

Method: 采用专家混合框架捕获时空特征及其权重，设计低秩引导采样图注意力机制动态平衡路网相关性。

Result: 在四个交通数据集上实验表明，STAMImputer性能显著优于现有方法。

Conclusion: STAMImputer有效解决了交通数据填补中的关键问题，性能优越。

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [49] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: 论文证明，在理想假设下，通过推理时技术（如上下文学习）可以近似监督微调（SFT）的能力，无需修改模型参数，并扩展到实际有限上下文和部分数据集的情况。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）计算成本高，研究如何在推理时通过上下文学习（ICL）近似其能力，以高效部署大型语言模型。

Method: 在理想假设下，通过理论分析证明ICL可以近似SFT能力，并扩展到有限上下文和部分数据集的实际场景。

Result: 对于文本生成和线性分类任务，给出了所需数据集大小的理论界限，证明了ICL的可行性。

Conclusion: 研究为大型语言模型的高效部署提供了理论基础，实际技术（如检索增强生成）可将理论应用于现实场景。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [50] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL是一种新的联邦学习框架，通过两种正则化策略解决非独立同分布数据导致的性能下降问题，无需依赖全局模型。


<details>
  <summary>Details</summary>
Motivation: 非独立同分布数据导致联邦学习性能下降，传统方法计算成本高或适应性差。

Method: UniVarFL采用分类器方差正则化和超球面均匀性正则化，模拟IID训练动态。

Result: 在多个基准数据集上，UniVarFL在准确性上优于现有方法。

Conclusion: UniVarFL是一种高效、可扩展的解决方案，适用于资源受限的实际联邦学习部署。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [51] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: FairDICE是首个离线多目标强化学习框架，直接优化非线性福利目标，无需显式偏好权重或穷举搜索。


<details>
  <summary>Details</summary>
Motivation: 线性标量化在多目标强化学习中无法捕捉公平导向目标（如Nash社会福利或最大最小公平），而现有方法缺乏离线设置下的统一解决方案。

Method: FairDICE利用分布校正估计，联合优化福利最大化和分布正则化，实现稳定且样本高效的学习。

Result: 在多个离线基准测试中，FairDICE表现出优于现有基线的公平感知性能。

Conclusion: FairDICE为离线多目标强化学习中的非线性福利优化提供了有效的统一框架。

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [52] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: 提出了一种基于随机神经网络的联邦学习方法（Federated stochastic neural networks），用于解决本地数据中的潜在噪声问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的潜在噪声（如测量误差或人为错误）可能影响模型性能，需要一种方法估计真实数据状态并量化噪声。

Method: 在联邦学习框架中使用随机神经网络作为本地模型，以估计数据真实状态并量化噪声。

Result: 数值实验表明，该方法在处理非独立同分布数据时表现良好。

Conclusion: Federated stochastic neural networks 能有效解决联邦学习中的数据噪声问题，提升模型性能。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [53] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: 提出了一种轻量级、快速的随机向量功能链接网络Lite-RVFL，用于在线学习中适应概念漂移，无需检测或重新训练。


<details>
  <summary>Details</summary>
Motivation: 数据分布随时间变化（概念漂移）对在线学习方法可靠性构成挑战，现有方法计算成本高且不适用于实时应用。

Method: Lite-RVFL通过引入新的目标函数，为新样本分配指数增加的权重，强调近期数据并实现快速适应。

Result: 理论分析和实验验证表明，Lite-RVFL能高效适应概念漂移，并捕捉时间模式。

Conclusion: Lite-RVFL是一种高效、轻量的解决方案，适用于实时应用中的概念漂移问题。

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [54] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: HASFL框架通过自适应控制批量大小和模型分割，解决了分片联邦学习中因设备异构性导致的性能问题，显著提升了训练效率和收敛性。


<details>
  <summary>Details</summary>
Motivation: 分片联邦学习（SFL）在边缘设备上实现分层模型分割，但现有方法因设备异构性导致性能下降。

Method: 提出HASFL框架，通过自适应控制批量大小和模型分割，平衡通信-计算延迟与训练收敛性。

Result: 实验验证HASFL在多种数据集上优于现有方法。

Conclusion: HASFL有效解决了异构边缘网络中的性能问题，具有显著优势。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [55] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: Info-Coevolution是一种新颖的框架，通过在线选择性标注实现模型与数据的协同进化，减少标注和训练成本。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据的持续增长对高效数据集构建和训练提出了挑战，传统方法存在数据冗余和效率低下的问题。

Method: 提出Info-Coevolution框架，利用任务特定模型选择性标注和整合在线数据，实现无偏见的协同进化。

Result: 在ImageNet-1K等数据集上，标注和训练成本减少32%，且无需调整比例即可自动计算节省比例。结合半监督学习，标注比例可进一步降至50%。

Conclusion: Info-Coevolution显著提升了数据效率和训练效果，为机器学习中的数据集优化提供了新思路。

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [56] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: 本文比较了多种预训练时间序列基础模型（TSFMs）与传统统计和机器学习方法在电力价格预测（EPF）中的表现，发现TSFMs表现与传统方法相当，但双季节性MSTL模型在一致性和性能上更优。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测对电力现货市场交易决策至关重要，但现有生成式人工智能（GenAI）和预训练大语言模型（LLMs）在EPF中的效果尚不明确，因此需要对比评估。

Method: 使用德国、法国、荷兰、奥地利和比利时的2024年日前拍卖（DAA）电价数据，对多种TSFMs（如Chronos-Bolt、Time-MoE等）与传统方法进行每日预测比较。

Result: Chronos-Bolt和Time-MoE在TSFMs中表现最佳，与传统方法相当；但双季节性MSTL模型在各国和评估指标中表现一致且最优，无TSFM能显著超越它。

Conclusion: 尽管TSFMs在EPF中表现良好，但双季节性MSTL模型因其一致性和稳定性成为更优选择。

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [57] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo是一个RL框架，通过改进长度奖励设计，提升语言模型推理效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升推理效率时往往牺牲准确性，Bingo旨在解决这一问题。

Method: Bingo引入显著性感知长度奖励和动态长度奖励，逐步减少无关标记并优化效率。

Result: 实验表明，Bingo在多个基准测试中优于基线方法，实现了准确性与效率的良好平衡。

Conclusion: Bingo展示了通过显式训练提升语言模型推理效率的潜力。

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [58] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: 论文提出了一种名为NONA的回归层，通过神经网络注意力机制和学习的注意力掩码方案，实现了k-NN回归算法的可微代理，提升了回归任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统算法（如k-NN）在结合神经网络的监督微调（SFT）后表现优于SFT模型本身，但因其不可微分和独特优化需求，难以直接融入SFT。

Method: 引入NONA回归层，利用注意力机制和学习的注意力掩码方案，作为k-NN回归的可微代理。

Result: 在多个非结构化数据集上，NONA在回归任务中表现优于密集层预测和基于SFT嵌入的k-NN。

Conclusion: NONA为传统算法融入SFT提供了可行方案，显著提升了回归任务的性能。

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [59] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT是一个自动收集高质量科学发现编码任务的管道，构建了AutoSDT-5K数据集，并训练了AutoSDT-Coder模型，显著提升了数据驱动发现的性能。


<details>
  <summary>Details</summary>
Motivation: 解决科学发现中高质量数据稀缺的问题，加速AI在科学发现中的应用。

Method: 利用LLM的编码能力和参数知识，自动搜索、筛选和合成任务与代码，构建AutoSDT-5K数据集。

Result: AutoSDT-5K包含5,404个任务，专家验证93%任务有效，92.2%代码正确；AutoSDT-Coder模型在多个基准测试中表现优异。

Conclusion: AutoSDT有效解决了数据稀缺问题，提升了AI在科学发现中的性能，接近GPT-4o水平。

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [60] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了一种高效的公平谱聚类方法（Fair SC），通过将问题转化为凸差函数（DC）框架，并采用变量增强策略和ADMM算法，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决决策算法中的公平性问题，确保每个聚类中人口群体的比例与总体一致。

Method: 将公平谱聚类问题转化为DC框架，引入变量增强策略，并采用ADMM算法解决子问题。

Result: 实验表明，该方法在合成和真实数据集上均有效，计算速度显著优于现有方法。

Conclusion: 该方法为公平聚类在实际应用中的推广提供了重要进展。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [61] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: 提出了一种新框架，用于识别无闭合形式本构方程的非均质材料力学性能，结合神经网络和物理驱动方法，展示了其鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理非均质材料的力学性能识别，尤其是缺乏闭合形式本构方程的情况。

Method: 使用神经网络结合傅里叶特征逼近应变场，基于NODE框架发现本构方程，并通过超网络处理非均质性。

Result: 数值实验表明，该方法在噪声和非均质性条件下仍能有效识别材料性能。

Conclusion: 该方法为传统逆向方法提供了通用且鲁棒的替代方案。

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [62] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: 论文探讨了生成式AI（如LLMs）对能源和环境的影响，提出了一种基于现有基准的框架R-ICE，用于估算推理碳排放，以支持可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速普及对能源和环境造成负担，现有工具在监测和估算碳排放方面存在不足，需要更高效、非侵入性的解决方案。

Method: 利用现有LLM基准和相关数据点，开发框架R-ICE，估算提示级推理碳排放，克服现有工具的高误差和侵入性问题。

Result: 验证结果表明，基于基准的建模在推理排放估算方面潜力巨大，支持动态LLM路由和碳核算等新兴用例。

Conclusion: R-ICE框架为碳排放估算提供了实用且非侵入性的方法，值得科学界进一步探索。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [63] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 论文提出了一种新的遗忘学习问题分层建模方法，通过双层优化（Bi-Level UnleaRning, BLUR）算法，优先解决遗忘问题，同时保持模型性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLMs）在训练后能遗忘特定知识或能力，以满足数据法规和伦理要求，同时避免性能下降。

Method: 提出分层建模方法，将遗忘问题（优先）和保留问题（次要）分开，采用双层优化（BLUR）算法，下层最小化遗忘损失，上层保持模型效用。

Result: BLUR算法在多种遗忘任务、模型和指标上均优于现有方法。

Conclusion: 分层建模和BLUR算法有效解决了遗忘学习中的性能下降问题，为实际应用提供了可靠方案。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [64] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文提出了一种基于遗传算法的个性化决策树构建方法，适用于联邦学习中的分类和回归任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习主要关注基于梯度的参数模型，非参数模型如决策树研究较少，且现有方法受限于差分隐私，仅适用于分类任务和分类数据。

Method: 利用遗传算法构建个性化决策树，支持分类和数值数据，适用于分类和回归任务。

Result: 实验表明，该方法优于仅基于本地数据训练的决策树和基准算法。

Conclusion: 提出的方法扩展了联邦学习中决策树的应用范围，支持更多数据类型和任务。

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [65] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 本文探讨了相关噪声机制在差分隐私（DP）中的设计与分析，重点研究了其在AI和机器学习模型私有训练中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统DP机制在随机梯度下降（SGD）学习算法的每一步注入独立噪声以保护训练数据的隐私，但研究表明引入（反）相关噪声可通过在后续步骤中抵消部分噪声，显著改善隐私-效用权衡。

Method: 研究采用相关噪声机制（如矩阵机制、因子分解机制和DP-FTRL），并将其应用于学习算法中。

Result: 这些机制在实践中具有重要影响，已在全球范围内实现工业级部署。

Conclusion: 相关噪声机制为差分隐私提供了更优的隐私-效用权衡，并在实际应用中展现出巨大潜力。

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [66] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的残余应力生成器（RSG），通过有限测量数据推断全场应力分布，显著减少实验工作量。


<details>
  <summary>Details</summary>
Motivation: 残余应力影响组件性能，但全场表征实验成本高，需一种高效方法。

Method: 构建多样化参数模拟数据集，基于U-Net架构训练ML模型，并通过超参数调优优化性能。

Result: 模型在模拟和实验数据上均表现优异，预测精度高且泛化能力强。

Conclusion: RSG方法可行，能从有限测量中全面理解残余应力分布，大幅降低实验需求。

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [67] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: 本文通过计算复杂性理论分析了集成模型的解释性挑战，揭示了基模型数量、大小和类型对解释性的影响。


<details>
  <summary>Details</summary>
Motivation: 集成模型（如提升树）通常被视为黑箱，缺乏对其解释性的严格数学理解。本文旨在填补这一空白。

Method: 应用计算复杂性理论，研究不同集成配置下生成解释的挑战。

Result: 发现解释性受基模型数量、大小和类型影响，例如小规模决策树集成可高效解释，而线性模型集成即使数量少也难解释。

Conclusion: 研究为理解集成模型解释性提供了更坚实的基础，强调了计算复杂性视角的重要性。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [68] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Mondrian是一种基于Transformer的算子学习方法，通过域分解和子域注意力机制，解决了高分辨率多尺度PDE建模中的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在建模高分辨率、多尺度PDE时，由于注意力机制的二次计算成本和与离散化的耦合，难以扩展。

Method: Mondrian将域分解为不重叠的子域，在每个子域内使用神经算子，并通过软最大内积计算子域间的注意力。支持分层窗口和邻域注意力。

Result: 在Allen-Cahn和Navier-Stokes PDE上表现优异，支持分辨率扩展而无需重新训练。

Conclusion: 域分解注意力为可扩展和通用的神经算子提供了潜力。

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [69] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: 研究了编码器-解码器自回归Transformer模型在自动驾驶领域中的联合运动预测和规划任务上的经验性扩展规律，发现模型性能随计算预算呈幂律增长，并探讨了训练和推理时的最优扩展策略。


<details>
  <summary>Details</summary>
Motivation: 探索自动驾驶领域中运动预测和规划模型的扩展规律，以优化模型性能并解决数据稀缺问题。

Method: 使用50万小时驾驶数据集，分析模型性能与计算预算的关系，研究模型参数与数据量的最优扩展比例，并评估推理时的计算效率。

Result: 模型性能随计算预算呈幂律增长，训练计算最优模型时，模型参数需以1.5倍于数据量的速度扩展；推理时，小模型通过采样和聚类可媲美大模型，直至交叉点。

Conclusion: 优化训练和推理的扩展策略是提升自动驾驶模型性能的关键，同时利用其他车辆的通用驾驶数据可缓解机器人数据稀缺问题。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [70] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 论文提出一个评估LLM提取临床数据质量的综合框架，包括性能基准测试、自动验证和复制分析，以提升数据可靠性和公平性。


<details>
  <summary>Details</summary>
Motivation: LLM在临床数据提取中虽高效但存在可靠性、准确性和公平性问题，现有框架未能完全解决这些问题。

Method: 提出多维框架，结合性能基准测试、自动验证和复制分析，支持偏差评估。

Result: 框架能识别需改进变量、系统性检测潜在错误，并确认数据适用性。

Conclusion: 该框架为LLM提取的RWD提供了严格透明的评估方法，推动行业标准并支持AI证据的可靠使用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [71] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: 本文探讨了随机数据增强的局限性，并提出了一种简单方法来解决其导致的特征扭曲问题，从而提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随机数据增强成本低但效果有限，而定向增强虽有效但成本高。本文旨在改进随机增强的泛化效果。

Method: 通过解决随机增强中的特征扭曲问题（类似灾难性遗忘），提出了一种简单方法。

Result: 该方法在单源域泛化（sDG）基准测试中表现出强大的泛化性能。

Conclusion: 改进后的随机增强方法在提升泛化效果方面具有潜力，且成本较低。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [72] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: 提出了一种基于信号时序逻辑（STL）的框架，用于评估和改进大型语言模型（LLM）在数学推理任务中的逐步置信度，从而提供更可靠的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: LLM在数学推理任务中常产生高置信度但错误的输出，尤其在教育等领域可能带来风险，用户难以评估推理步骤的正确性。

Method: 通过STL定义时间约束，计算鲁棒性评分作为置信度估计，并引入不确定性重塑策略以确保推理轨迹的平滑性、单调性和因果一致性。

Result: 实验表明，该方法显著改进了校准指标，提供了比传统方法更可靠的不确定性估计。

Conclusion: 提出的框架能有效提升LLM在数学推理中的置信度校准和不确定性估计的可靠性。

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [73] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: 提出一种零参数方法，通过在损失函数中添加近似等变性约束，简化了等变神经网络的实现，并在多个数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有等变方法计算复杂度高、参数多且架构固定，需要一种更简单高效的方法。

Method: 在潜在表示中为有限群添加近似等变性约束，作为损失函数的附加项，并让网络学习潜在空间的群表示。

Result: 实验表明，网络倾向于学习正则表示，且该方法在多个数据集上性能接近或优于现有方法，参数更少。

Conclusion: 提出的零参数方法简单高效，适用于多种任务，且性能优越。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [74] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: SHIELD提出了一种结合超网络和区间算术的新方法，同时解决灾难性遗忘和对抗攻击问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络存在灾难性遗忘和对输入数据扰动的脆弱性，目前缺乏能同时解决这两个问题的模型。

Method: SHIELD通过超网络生成任务特定的目标模型权重，并利用区间算术为输入数据提供严格的安全保证。

Result: SHIELD能够动态生成子任务网络，同时确保输入数据在区间范围内的安全性。

Conclusion: SHIELD在不牺牲网络适应性的情况下增强了安全性，解决了持续学习中的安全问题。

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [75] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: HC-RLHF是一种结合高置信度安全保证与最大化帮助性的语言模型对齐方法，通过分解人类偏好为帮助性和无害性，并采用两步优化过程确保安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言模型对齐中常将安全性与帮助性对立，导致敏感领域出现不可接受的响应，因此需要一种能提供高置信度安全保证的方法。

Method: HC-RLHF将人类偏好分解为帮助性和无害性，分别训练奖励模型和成本模型，通过两步优化（悲观约束优化和安全测试）确保安全性。

Result: 实验表明，HC-RLHF能以高概率生成安全模型，并在无害性和帮助性上优于现有方法。

Conclusion: HC-RLHF提供了一种可靠的语言模型对齐方法，能在高置信度下保证安全性，同时提升模型性能。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [76] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: LIES框架通过固定神经网络架构和优化策略，生成稀疏且准确的符号表达式，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法在可扩展性和符号一致性上存在不足，LIES旨在解决这些问题。

Method: LIES使用固定神经网络架构（包含对数、恒等、指数和正弦激活函数），通过过采样和定制损失函数优化，并结合剪枝策略提取紧凑公式。

Result: 实验表明LIES在符号回归基准测试中表现优于基线方法，生成稀疏且准确的公式。

Conclusion: LIES框架通过其设计组件有效提升了符号回归的性能和可解释性。

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [77] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 提出了一种同时优化神经网络结构和权重的新方法，通过连续潜在空间嵌入架构和参数信息，实现高效模型设计。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络设计依赖手动试错或离散化的架构搜索与权重优化，效率低下且耗时。

Method: 训练一个多尺度自编码器，将架构和参数信息嵌入连续潜在空间，通过梯度下降联合优化结构和权重。

Result: 在合成回归任务中，该方法成功发现性能优异的稀疏紧凑神经网络。

Conclusion: 该方法为神经网络设计提供了一种高效且连续的优化途径。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [78] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: UDEs结合神经网络与物理微分方程，用于智能电网中电池动态建模，解决传统方法泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 智能电网中节点电池动态建模因太阳能输入随机性和家庭负载变化而困难，传统方法难以捕捉未建模动态。

Method: 提出基于UDE的方法，将神经残差嵌入物理启发的电池ODE中，学习节点特定电池动态。

Result: 实验显示UDE与真实电池轨迹高度吻合，收敛平滑，长期预测稳定。

Conclusion: UDE方法适用于分散式能源网络电池建模，对实时控制和优化有广泛意义。

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [79] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: 本文系统评估了12种特征缩放技术对14种机器学习算法和16个数据集的影响，发现集成方法对缩放不敏感，而其他模型（如逻辑回归、SVM等）性能显著依赖缩放选择。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对特征缩放技术的全面评估，尤其是对不同机器学习算法和任务的影响。

Method: 使用12种缩放技术，评估14种算法在16个数据集上的表现，分析预测性能和计算成本。

Result: 集成方法（如随机森林、XGBoost等）对缩放不敏感，而逻辑回归、SVM等模型性能显著依赖缩放选择。

Conclusion: 研究为实践者提供了模型特定的特征缩放选择指南，并公开了所有代码和结果以确保透明性和可重复性。

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [80] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: ECON是一种基于贝叶斯纳什均衡的多LLM协调框架，通过分层强化学习实现高效推理，显著降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多代理框架中计算成本高和缺乏收敛保证的问题。

Method: 将多LLM协调建模为不完全信息博弈，通过贝叶斯纳什均衡实现分布式推理与集中输出的结合。

Result: ECON在六个基准测试中平均性能提升11.2%，并具有可扩展性。

Conclusion: ECON为构建更强大的多LLM系统提供了高效且可扩展的解决方案。

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [81] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: AR-Bench是一个新基准，用于评估大型语言模型（LLM）的主动推理能力，发现当前模型在此方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估被动推理，缺乏对主动推理的系统性研究。

Method: 设计了AR-Bench，包含三类任务（侦探案例、情境谜题和猜数字），模拟真实场景并测试推理能力。

Result: 当代LLM在主动推理上表现较差，即使采用高级策略也仅略有改善。

Conclusion: 需发展新方法（如交互学习、实时反馈）以提升主动推理能力。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [82] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: 论文提出了一种名为H$^2$GFM的新框架，旨在统一处理同质和异质文本属性图（TAGs），通过上下文编码和自适应图变换器（CGT）提升图基础模型（GFM）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质文本属性图（HoTAGs），而忽略了异质文本属性图（HeTAGs）。为了增强GFM的能力和应用范围，需要一种能够同时处理这两种图的统一模型。

Method: H$^2$GFM框架通过统一的文本空间投影多样化的元关系，并采用上下文编码捕捉空间和高阶语义关系。提出了一种上下文自适应图变换器（CGT）来生成鲁棒的节点表示，并通过混合CGT专家捕捉结构模式的异质性。

Result: 在广泛的HoTAGs和HeTAGs以及多种学习场景下的实验证明了模型的有效性。

Conclusion: H$^2$GFM成功扩展了GFM的应用范围，为处理异质图提供了有效的解决方案。

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [83] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: 论文提出了一种可学习的时空位置编码方法L-STEP，解决了现有位置编码在适应性、动态性和计算效率上的不足，并在多个数据集和任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有图深度学习中的位置编码方法存在适应性差、无法处理动态图信息以及计算效率低的问题，因此需要一种更有效和高效的方法。

Method: 提出L-STEP模型，通过可学习的时空位置编码方案，结合MLP的表示能力，替代传统Transformer的注意力机制，降低了计算复杂度。

Result: 在13个经典数据集和10种算法上验证了L-STEP的优越性，特别是在时空链路预测任务中表现突出，并在TGB基准测试中取得领先性能。

Conclusion: L-STEP是一种高效且有效的位置编码方法，能够适应复杂动态图数据，并在性能和计算效率上优于现有方法。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [84] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文解释了离散扩散模型中掩码扩散表现优异的原因，并提出了一种新的方法（SCUD）来改进离散扩散模型，使其在图像、文本和蛋白质数据上表现优于掩码扩散。


<details>
  <summary>Details</summary>
Motivation: 研究离散扩散模型中掩码扩散表现优异的原因，并探索如何将这一优势推广到其他离散扩散模型中。

Method: 提出了一种名为SCUD的方法，将已知的跳跃时间分布嵌入到离散扩散模型中，从而改进模型性能。

Result: SCUD方法在图像、文本和蛋白质数据上的表现优于传统的掩码扩散模型。

Conclusion: 通过将跳跃时间分布嵌入离散扩散模型，可以显著提升模型性能，SCUD方法为离散扩散模型提供了新的改进方向。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [85] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: 本文系统综述了图提示（graph prompting）的最新进展，包括图预训练方法、主流图提示技术及其应用，并讨论了该领域的开放挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 图学习模型在大规模图数据中表现优异，但如何高效适应下游任务仍具挑战性。图提示作为一种新兴方法，通过学习可训练提示而不改变预训练模型，提供了解决方案。

Method: 1. 介绍图预训练方法作为基础；2. 综述主流图提示技术，包括可训练提示的设计；3. 总结图提示在各领域的实际应用。

Result: 图提示在多种任务中展现出潜力，但仍需解决设计优化和泛化性等挑战。

Conclusion: 图提示是图学习领域的重要方向，未来需进一步探索其理论基础和实际应用潜力。

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [86] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: 本文提出了一种简化的理论框架，用于分析DDPM中VP-SDE的Euler--Maruyama离散化误差，收敛速率为$\mathcal{O}(1/T^{1/2})$，并证明离散噪声可替代高斯噪声。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型离散化误差分析依赖复杂概率工具，本文旨在简化分析并探索离散噪声的实用性。

Method: 利用Grönwall不等式推导收敛速率，并实验验证离散噪声的效果。

Result: 误差按预测比例缩放，离散噪声与高斯噪声效果相当，噪声缩放错误会降低性能。

Conclusion: 本文通过简化分析和离散噪声替代，在扩散生成模型中实现了理论严谨性与实际效率的统一。

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [87] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: 提出了一种优化框架，通过将控制权转移给参数化策略，形成自治动力系统，从而简化优化过程，避免直接使用动态规划和强化学习的复杂方法。


<details>
  <summary>Details</summary>
Motivation: 旨在简化策略优化过程，避免传统动态规划和强化学习的复杂性，同时统一处理策略参数和其他系统参数。

Method: 在自治系统层面推导出更简单的算法，证明其与策略梯度、自然梯度等方法等价，并扩展至行为克隆、系统辨识等领域。

Result: 算法能够计算与策略梯度、自然梯度等相同的量，并适用于多种任务，如生成AI模型的调优。

Conclusion: 该框架为策略优化提供了更简单且通用的方法，适用于广泛的应用场景。

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [88] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了一种针对关系数据学习的差分隐私框架，解决了传统DP-SGD在关系学习中因实体多参与和耦合采样导致的隐私保护难题。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域中，保护关系数据中实体的隐私至关重要，但直接应用DP-SGD存在高敏感性和隐私放大分析不适用的问题。

Method: 通过严格的敏感性分析和自适应梯度裁剪方案，结合扩展的隐私放大结果，开发了一种专为关系数据设计的DP-SGD变体。

Result: 实验表明，该方法在文本属性网络数据上实现了良好的隐私-效用平衡。

Conclusion: 该框架为关系学习提供了具有严格实体级差分隐私保证的解决方案。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [89] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct是一种新颖的优化算法，通过根据激活方差调整学习率，提升神经元输出的稳定性，从而改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统激活正则化方法存在局限性，AdaAct旨在通过神经元级别的自适应提升训练稳定性和泛化能力。

Method: AdaAct通过调整学习率以适应激活方差，结合了Adam的快速收敛和SGD的强泛化能力。

Result: 在CIFAR和ImageNet等标准图像分类基准测试中，AdaAct表现出色，性能优于其他先进方法。

Conclusion: AdaAct在保持高效执行的同时，成功平衡了收敛速度和泛化能力，是一种有效的优化算法。

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [90] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: NysAct是一种可扩展的一阶梯度预处理方法，平衡了一阶和二阶优化方法，通过近似激活协方差矩阵显著降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 自适应梯度方法计算高效但泛化能力差，二阶方法泛化能力强但计算成本高，需要一种折中方案。

Method: 使用特征值偏移的Nystrom方法近似激活协方差矩阵作为预处理矩阵。

Result: NysAct在测试准确率上优于一阶和二阶方法，且计算资源需求显著低于二阶方法。

Conclusion: NysAct在计算效率和泛化能力之间取得了良好平衡，是一种高效的优化方法。

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [91] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: AlphaFold数据库（AFDB）的结构数据存在系统性几何偏差，与实验数据（PDB）的构象多样性不符。作者提出了一种去偏自编码器（DeSAE），通过重构自然构象来消除偏差，显著提升了逆折叠等任务的性能。


<details>
  <summary>Details</summary>
Motivation: AFDB的结构数据虽然覆盖广且精度高，但其几何偏差限制了在敏感任务（如逆折叠）中的应用。实验数据（PDB）更具多样性和物理真实性，但AFDB的偏差需要通过方法消除。

Method: 提出DeSAE模型，通过从故意损坏的主链几何中重构自然构象，学习更稳健的结构流形。

Result: DeSAE处理后的AFDB结构显著提升了逆折叠任务在多个基准测试中的性能。

Conclusion: AFDB的系统性偏差对结构学习任务有重要影响，DeSAE提供了一种有效的去偏框架，提升了任务性能。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [92] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: 论文提出DPSDP算法，通过强化学习优化LLM的多轮答案迭代，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有验证与改进方法反馈空间受限且缺乏协调训练，导致性能不佳。

Method: 将多轮改进建模为马尔可夫决策过程，提出DPSDP算法，通过直接偏好学习训练actor-critic系统。

Result: 在MATH 500基准上，五轮改进将准确率从58.2%提升至63.2%。

Conclusion: DPSDP在多轮协作和分布外泛化中表现出优势。

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [93] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: 论文通过分析IoT-23数据集，比较了多种机器学习技术，发现结合不平衡处理技术和集成方法（如gcForest）能显著提升恶意流量检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）网络的快速扩展，实时检测恶意流量成为关键网络安全挑战。

Method: 使用IoT-23数据集，采用三种重采样策略处理类别不平衡，比较多种机器学习技术。

Result: 结合不平衡处理技术和集成方法（如gcForest）的检测性能优于传统方法。

Conclusion: 该研究为IoT环境开发更智能、高效的自动化威胁检测系统提供了重要贡献。

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [94] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: 论文提出了一种新的框架Reinforcement-Learned Teachers (RLTs)，通过训练专注于下游蒸馏效果的教师模型，避免了强化学习的探索挑战。RLTs通过提供详细解释来指导学生，并在实践中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练推理语言模型时依赖于初始探索能力，且模型主要用于蒸馏学生模型而非直接部署。因此，需要一种更高效的框架来优化蒸馏效果。

Method: 引入RLTs，其输入问题和解决方案，任务是生成详细解释以指导学生。训练时使用密集奖励，通过测试学生对解释的理解来优化模型。

Result: 7B规模的RLT在竞争和研究生级任务中表现优于现有蒸馏和冷启动方法，且能有效训练更大规模的学生模型并适应分布外任务。

Conclusion: RLTs框架显著提升了强化学习推理的效率与可重用性，为未来研究提供了新方向。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [95] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度学习和数据增强的方法，用于检测气旋快速增强事件，解决了数据集类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 气旋快速增强是一种极端事件，数据集类别不平衡且影响因素复杂，传统机器学习方法难以处理。

Method: 采用深度学习、集成学习和数据增强框架，生成空间坐标和风速数据以解决类别不平衡问题，并使用深度学习模型进行分类。

Result: 数据增强显著提升了快速增强事件的检测效果，空间坐标作为输入特征至关重要。

Conclusion: 该方法为时空数据中极端事件的合成数据生成提供了新思路。

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [96] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种基于模糊集的集合表示学习方法FUSE，用于解决分类扩展问题，相比现有方法提升高达23%。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将集合建模为向量或几何对象，但这些方法在集合操作下不封闭，无法有效处理不确定性。

Method: 提出FUSE框架，通过模糊集的体积近似实现集合表示学习，满足所有集合操作且高效。

Result: 在分类扩展任务中，FUSE相比基线方法提升高达23%。

Conclusion: FUSE是首次尝试理解并高效计算模糊集嵌入的工作，具有显著优势。

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [97] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习与无监督异常生成方法的新框架SGDA，用于三相电机的智能诊断，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法依赖签名分析，虽为标准实践，但结合先进ML技术可进一步提升性能。

Method: 提出SGDA框架，结合ML算法与无监督异常生成方法，直接在健康电流信号的频域合成物理合理的故障。

Result: 该方法在诊断准确性和可靠性上表现优异，具有广泛的工业应用潜力。

Conclusion: SGDA为电机诊断领域提供了高效、鲁棒的解决方案，具有实际应用价值。

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [98] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 论文研究了数据重用如何改善线性回归中的扩展规律，通过多轮随机梯度下降（SGD）在数据受限情况下实现更优的测试误差。


<details>
  <summary>Details</summary>
Motivation: 当前神经扩展规律在数据耗尽时可能不可持续，因此探索数据重用是否能优化扩展规律。

Method: 使用多轮SGD训练M维线性模型，假设数据协方差和真实参数具有特定幂律谱。

Result: 多轮SGD的测试误差为Θ(M^{1-b} + L^{(1-b)/a})，优于单轮SGD的Θ(M^{1-b} + N^{(1-b)/a})。

Conclusion: 数据重用（L>N）在数据受限情况下能改善扩展规律，数值模拟验证了理论结果。

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [99] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种新方法SQOG，通过平滑贝尔曼算子（SBO）在凸包及其邻域（CHN）内提升Q函数泛化能力，解决了离线强化学习中Q值高估问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，分布偏移导致Q值对分布外（OOD）动作的高估，现有方法因过度保守限制了Q函数泛化。

Method: 提出平滑贝尔曼算子（SBO），在CHN内通过平滑OOD Q值与邻近样本Q值更新Q值。

Result: SQOG在D4RL基准测试中表现优于现有方法，计算效率更高。

Conclusion: SQOG通过提升Q函数泛化能力，有效解决了Q值高估问题，性能显著提升。

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [100] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: GALA框架通过动态调整学习率，基于梯度对齐和局部曲率估计，无需繁琐的超参数调优，提升优化器性能。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型中，学习率的精细调优对优化器性能至关重要，但传统方法需大量网格搜索，效率低下。

Method: 提出GALA框架，通过跟踪连续梯度的对齐性和局部曲率估计，动态调整学习率，并将其建模为一维在线学习问题。

Result: GALA与SGD或Adam结合时，在广泛初始学习率下表现稳健，无需调优即可取得竞争性性能。

Conclusion: GALA提供了一种自适应学习率调整方法，显著减少超参数调优需求，适用于非凸平滑场景。

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [101] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 本文通过实证研究证明，即使在现实的联邦学习环境中，客户数据仍可能被有效重构，并提出FedLeak方法以解决梯度匹配问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）因其隐私保护能力备受关注，但梯度泄漏攻击（GLAs）的存在引发了隐私风险争议。本文旨在填补这一关键空白，证明即使在现实FL环境中，数据重构仍可行。

Method: 提出FedLeak方法，引入部分梯度匹配和梯度正则化技术，并制定基于文献和行业实践的实际评估协议。

Result: 在现实FL环境中，FedLeak仍能实现高保真数据重构，揭示了FL系统的重大漏洞。

Conclusion: FL系统存在显著隐私风险，亟需更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [102] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [103] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: TAWM通过显式结合时间动态，利用时间步长Δ𝑡训练，提升模型性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统模型固定时间步长采样，无法适应系统动态变化，TAWM旨在解决这一问题。

Method: TAWM基于信息论，通过多样化Δ𝑡训练，学习高低频任务动态。

Result: TAWM在多种控制任务中表现优于传统模型，且数据效率更高。

Conclusion: TAWM的时间感知设计显著提升了模型性能，适用于动态变化任务。

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [104] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY是一种基于模型的离线强化学习算法，通过生成目标域的合成数据来解决源域和目标域动态不匹配的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法在源域和目标域动态不匹配时表现受限，无法充分利用目标域数据。

Method: MOBODY通过学习共享潜在表示生成目标域合成数据，并结合Q加权行为克隆损失稳定训练。

Result: 在MuJoCo基准测试中，MOBODY显著优于现有基线方法，尤其在复杂场景中表现突出。

Conclusion: MOBODY通过模型生成和潜在表示学习有效解决了动态不匹配问题，提升了离线强化学习的性能。

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [105] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: Reinforced RCSL通过引入in-distribution optimal return-to-go机制，解决了RCSL缺乏stitching属性的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: RCSL在离线强化学习中虽简单稳定，但缺乏stitching属性，性能受限于数据集质量。

Method: 提出Reinforced RCSL框架，引入in-distribution optimal return-to-go机制，避免复杂回报增强技术。

Result: 理论分析和实验结果表明，Reinforced RCSL显著优于标准RCSL。

Conclusion: Reinforced RCSL是一种有效改进RCSL的方法，具有理论和实践优势。

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [106] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAC的高效二阶优化方法，通过近似Kronecker因子降低计算负担，优于KFAC等方法。


<details>
  <summary>Details</summary>
Motivation: KFAC等二阶优化方法虽然收敛性好，但计算成本高，因此需要更高效的优化方法。

Method: 分析Kronecker因子（激活和预激活梯度），提出高效近似方法，并应用于注意力层的FIM。

Result: MAC在准确性、训练时间和内存使用上优于KFAC及其他先进方法。

Conclusion: MAC是一种高效且收敛性好的优化方法，适用于多种网络架构和数据集。

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [107] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种名为AsFT的安全微调方法，通过正则化项抑制有害方向的更新，显著减少有害行为并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在微调过程中容易受到安全风险的影响，少量恶意或无害数据可能破坏模型的安全性。

Method: 基于对齐方向的概念，提出AsFT方法，通过正则化项将微调约束在安全范围内。

Result: 实验表明，AsFT优于Safe LoRA，减少有害行为7.60%，提升性能3.44%，且在不同实验设置下表现稳健。

Conclusion: AsFT通过锚定对齐方向，有效保护模型安全性，同时提升性能，为安全微调提供了可靠方法。

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [108] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种基于热力学原理的潜在空间动力学识别框架（tLaSDI），用于参数化非线性动力系统的降阶建模，结合自编码器和参数化GENERIC神经网络，实现了高效学习并保持热力学特性。


<details>
  <summary>Details</summary>
Motivation: 解决参数化非线性动力系统的高效建模问题，同时确保热力学原理（如自由能守恒和熵生成）在参数空间中得到保持。

Method: 结合自编码器降维与参数化GENERIC神经网络（pGFINNs），并采用物理信息主动学习策略，通过残差误差指标自适应采样训练数据。

Result: 在Burgers方程和1D/1V Vlasov-Poisson方程上实现了高达3,528倍的加速，相对误差1-3%，训练和推理成本显著降低。

Conclusion: tLaSDI框架不仅高效且准确，还能揭示系统的热力学行为，为物理空间动力学提供深入见解。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [109] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 提出了一种新的抽象-细化技术，用于高效计算神经网络预测的可证明充分解释。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释方法依赖启发式且缺乏形式化保证，而可证明的解释方法计算效率低。

Method: 通过构建简化网络抽象原始网络，逐步细化网络规模直至收敛，以加速验证过程。

Result: 实验表明该方法提高了获取可证明充分解释的效率，并提供了细粒度的网络预测解释。

Conclusion: 该方法显著提升了可证明解释的计算效率，同时支持多层次的网络预测解释。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [110] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: 论文提出了SHAMs和DiffGradCAM，用于评估和改进CAM方法的鲁棒性，特别是在对抗条件下。


<details>
  <summary>Details</summary>
Motivation: CAM及其变体（如GradCAM）在解释CNN预测时存在漏洞，易受对抗性操纵（如被动欺骗）影响。

Method: 提出SHAMs作为对抗性基准，并开发DiffGradCAM，一种轻量级且对比性的CAM方法，以解决被动欺骗问题。

Result: SHAMs和DiffGradCAM在多类任务中验证了其有效性，DiffGradCAM在非对抗情况下与标准CAM方法表现一致。

Conclusion: SHAMs和DiffGradCAM为基于显著性的解释方法提供了新的鲁棒性评估和改进框架。

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [111] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: ML4CFD竞赛展示了机器学习在流体动力学模拟中的潜力，部分模型表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在科学计算中准确性、泛化性和物理一致性不足的问题。

Method: 组织ML4CFD竞赛，基于OpenFOAM生成数据集，评估模型的多项指标。

Result: 顶级模型在综合指标上优于OpenFOAM求解器。

Conclusion: 机器学习替代模型在特定条件下可超越传统方法，需进一步优化评估框架。

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [112] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: 论文探讨了在异常大学习率下梯度下降（GD）的动态变化，发现学习率在特定范围内时，GD会从纯利用型转向探索-利用平衡态，且训练时间最短。


<details>
  <summary>Details</summary>
Motivation: 研究传统GD算法在大学习率下的动态行为，探索如何通过调整学习率加速神经网络训练。

Method: 通过分析GD轨迹的最大Lyapunov指数，表征其对初始条件的敏感依赖性，并在MNIST分类任务中验证。

Result: 在学习率特定范围内，GD表现出探索-利用平衡态，训练时间最短，且结果适用于多种任务和架构。

Conclusion: 瞬态混沌动态在神经网络训练中具有建设性作用，可通过调整学习率优化训练效率。

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [113] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: EMNAS通过遗传算法优化自动驾驶强化学习的神经网络架构，提升奖励并减少模型大小，同时采用并行化和师生方法加速搜索和优化。实验表明其性能优于手动设计模型。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶强化学习优化神经网络架构，提升性能并减少模型复杂度。

Method: 使用遗传算法自动化网络设计，结合并行化和师生方法加速搜索和优化。

Result: EMNAS在实验中表现优于手动设计模型，奖励更高且参数更少。

Conclusion: EMNAS为自动驾驶强化学习提供了高效且可扩展的网络优化方法，适用于实际场景。

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [114] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm是首个专为自动化通信系统设计而开发的推理LLM，通过两阶段训练策略（SFT和C-ReMax）和开源数据集CSFRC，显著优于现有大型专有LLM。


<details>
  <summary>Details</summary>
Motivation: 现有通用LLM缺乏通信系统设计所需的专业领域知识和推理能力，亟需专用模型填补这一空白。

Method: 采用两阶段训练：1）使用带CoT数据的SFT提炼领域知识；2）基于ReMax的C-ReMax算法培养高级建模能力。

Result: 实验表明，DeepForm在多样化场景中表现卓越，显著优于大型专有LLM。

Conclusion: DeepForm为通信系统设计提供了首个专用LLM，未来将开源资源以推动研究。

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [115] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型（LLMs）的“真理几何”方法在任务间缺乏可迁移性，线性分类器在不同任务中表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs的可靠性问题，验证“真理几何”方法是否具有跨任务适用性。

Method: 通过训练线性分类器分析LLMs的激活向量，并测试其在跨任务中的表现。

Result: 发现“真理几何”方法任务依赖性高，跨任务迁移效果差，分类器支持集几乎不重叠。

Conclusion: 当前方法难以实现跨任务的可靠性评估，需探索更有效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [116] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: SLEEPYLAND是一个开源睡眠分期评估框架，旨在解决模型评估、泛化、偏见和标注差异等问题。其包含大量ID和OOD数据，并推出SOMNUS集成模型，性能优于现有方法和人类评分。


<details>
  <summary>Details</summary>
Motivation: 临床采用深度学习进行自动睡眠分期的进展受限，主要由于模型评估不公平、数据集泛化能力差、模型偏见和人类标注差异等问题。

Method: 提出SLEEPYLAND框架，包含大量ID和OOD数据，并开发SOMNUS集成模型，通过软投票结合多种架构和通道配置。

Result: SOMNUS在24个数据集中表现稳健，性能优于现有方法和人类评分，并能预测标注不一致区域。

Conclusion: SLEEPYLAND和SOMNUS为睡眠分期提供了标准化评估和高效解决方案，但模型偏见仍需进一步研究。

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [117] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: 提出了一种基于生成式人工智能的深度学习模型，用于提升污水系统上下文预测的准确性，通过扩散模型处理多元时间序列数据，并结合保形推理技术确保预测区间的统计可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决污水系统在极端天气条件下预测准确性不足的问题，提升模型的鲁棒性和可靠性。

Method: 开发扩散模型处理多元时间序列数据，结合保形推理技术校准预测区间。

Result: 在真实污水系统数据上的测试表明，模型在极端天气条件下仍能保持高准确性和可靠性。

Conclusion: 该模型为污水系统的上下文预测提供了一种可靠且高效的解决方案。

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [118] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: Transformer模型通过端到端深度学习学习符号计算，CALT库为非专家提供训练模型的工具。


<details>
  <summary>Details</summary>
Motivation: 探索符号计算的可学习性，并开发工具以降低深度学习门槛。

Method: 使用Transformer模型训练符号计算的序列到序列函数。

Result: 开发了CALT库，支持非专家训练符号计算模型。

Conclusion: 符号计算与深度学习的结合为研究开辟新方向，CALT库助力非专家参与。

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [119] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: PBFM是一种新的生成框架，通过显式嵌入物理约束（PDE残差和代数关系）到流匹配目标中，提升了复杂系统行为的建模精度。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法（如扩散模型和流匹配）通常隐式学习物理规律，缺乏显式物理约束，限制了建模精度。

Method: 提出PBFM框架，结合流匹配损失和物理残差损失，并引入时间展开训练策略，无需超参数调优。

Result: 在三个PDE问题上，PBFM的物理残差精度比FM高8倍，且在分布精度上优于现有算法。

Conclusion: PBFM为物理和工程应用中的代理建模、不确定性量化和加速仿真提供了高效且理论完备的框架。

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [120] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: 论文提出了一种名为CASE的高效样本选择方法，通过将示例选择问题建模为多臂老虎机问题，显著减少了LLM的调用次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习范式中，示例选择对构建有效提示至关重要，但现有方法在样本效率和计算成本上存在不足。

Method: 将示例选择问题建模为多臂老虎机问题，提出CASE方法，通过维护一个“挑战者”臂短列表，减少样本复杂性和LLM调用次数。

Result: CASE方法在运行时实现了7倍加速，LLM调用次数减少了87%，且性能未受影响。

Conclusion: CASE是一种高效且样本节约的示例选择方法，适用于上下文学习任务。

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [121] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: HSG-12M是首个大规模空间多重图数据集，包含静态和动态的哈密顿谱图，用于几何感知图学习。


<details>
  <summary>Details</summary>
Motivation: 现有图基准假设边是非空间且简单的，忽略了物理上不同的路径。HSG-12M旨在保留几何上不同的轨迹作为独立边。

Method: 通过Poly2Graph流水线将一维晶体哈密顿量映射为谱图，生成多样化的物理基础拓扑。

Result: HSG-12M包含1160万静态和510万动态谱图，覆盖1401个特征多项式类。基准测试显示多边几何学习的新挑战。

Conclusion: HSG-12M为几何感知图学习奠定基础，并建立了多项式、向量和矩阵的通用拓扑指纹。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [122] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: TiViT框架将时间序列转换为图像，利用预训练的视觉变换器（ViT）提升时间序列分类性能，并在标准基准测试中达到最优表现。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在医疗和工业中至关重要，但公开时间序列数据集的稀缺限制了时间序列基础模型（TSFM）的发展。

Method: 提出Time Vision Transformer（TiViT），将时间序列转化为图像，利用预训练的ViT模型提取特征。理论分析表明，ViT的2D分块可增加标签相关标记并降低样本复杂度。

Result: TiViT在标准时间序列分类基准测试中表现最优，并发现中间层高内在维度的表示最有效。与TSFM表示空间结合可进一步提升性能。

Conclusion: TiViT展示了在非视觉领域复用视觉表示的新方向，为时间序列分类提供了高效解决方案。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [123] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: DICE框架用于解决策略诱导的稳态分布与目标分布不匹配问题，但近期改进方法削弱了其离线策略评估（OPE）能力。本文提出新方法，通过半梯度DICE实现OPE和约束RL。


<details>
  <summary>Details</summary>
Motivation: 解决DICE框架在离线约束强化学习中因半梯度优化导致的成本估计失败问题。

Method: 提出一种新方法，通过半梯度DICE实现准确的成本估计和约束RL。

Result: 在离线约束RL基准DSRL上达到最优性能。

Conclusion: 新方法成功解决了DICE框架的局限性，实现了准确的成本估计和约束RL。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [124] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: RP-KrossFuse方法通过随机投影和Kronecker积将跨模态嵌入与单模态嵌入结合，既保留了跨模态对齐能力，又提升了单模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态嵌入在单模态任务上表现不如专用单模态嵌入，而单模态嵌入缺乏跨模态对齐能力，因此需要一种统一方法。

Method: 提出RP-KrossFuse，利用随机投影和Kronecker积融合跨模态与单模态嵌入的相似性得分，支持高效核空间操作。

Result: 实验表明，RP-KrossFuse在单模态任务上表现优异，同时保持了跨模态对齐能力。

Conclusion: RP-KrossFuse成功弥合了跨模态与单模态嵌入之间的性能差距。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [125] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: JoFormer是一种基于旅程的Transformer架构，通过可学习的方向变换表示相对位置，在语言建模任务中表现优于RoFormer。


<details>
  <summary>Details</summary>
Motivation: Transformer在序列建模中表现出色，但如何有效融入位置信息仍是一个挑战。JoFormer旨在通过非交换代数扩展和泛化现有方法。

Method: JoFormer通过可学习的方向变换表示相对位置，并推导出注意力机制，涵盖旋转变换等标准方法。

Result: 在Tiny Shakespeare任务中，JoFormer实现了更低的困惑度和更快的收敛速度。

Conclusion: JoFormer为Transformer架构提供了一种原则性的位置信息整合方法，展示了其潜力。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [126] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: ChannelTokenFormer是一种基于Transformer的预测模型，旨在解决多变量时间序列数据中的通道依赖性、采样异步性和缺失值问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列数据通常存在通道依赖性、采样异步性和缺失值问题，而现有模型往往基于过于简化的假设，无法应对这些挑战。

Method: 提出ChannelTokenFormer，通过灵活的架构显式捕捉跨通道交互、适应通道异步采样并有效处理缺失值。

Result: 在三个基准数据集和一个真实工业数据集上的实验表明，ChannelTokenFormer在复杂现实条件下具有优越的鲁棒性和准确性。

Conclusion: ChannelTokenFormer为多变量时间序列预测提供了一种更接近现实场景的解决方案。

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [127] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: 论文提出了一种针对变分自编码器图像压缩的额外微调训练步骤，通过在推理阶段使用量化潜在表示重新训练部分网络，以解决量化噪声建模不足的问题，从而提升编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练过程中难以正确建模量化噪声，导致网络性能次优，尤其是在熵约束量化器（如Trellis-Coded Quantization）中，量化潜在表示的相互依赖性使得近似方法效果不佳。

Method: 在常规端到端训练后，对网络部分进行基于推理阶段量化潜在表示的微调训练，以更准确地建模量化噪声。

Result: 实验表明，该方法在均匀标量量化和熵约束量化中均能带来额外的编码增益，且不增加推理复杂度。在Kodak和TecNick测试集上分别实现了1%-2%和最高2.2%的比特率节省。

Conclusion: 通过微调训练步骤，能够有效提升变分自编码器图像压缩的性能，尤其是在熵约束量化场景下。

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [128] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出一种通过LLM生成蓝图增强SLM推理能力的框架，并解决提示敏感性问题。


<details>
  <summary>Details</summary>
Motivation: SLM因容量限制导致推理能力不足且对提示变化敏感，需轻量级解决方案。

Method: 利用LLM生成结构化蓝图指导SLM推理，并集成提示模板搜索机制。

Result: 在数学、编程和逻辑推理任务中显著提升SLM性能，无需增加模型规模或训练。

Conclusion: 该框架为资源受限环境提供轻量级解决方案，有效增强SLM推理能力。

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [129] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: 本文提出了一种基于重要性采样的方法（IS-DAAs）来解决直接对齐算法（DAAs）中的过优化问题，通过引入重要性比率并限制其最大值，有效降低了方差并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 直接对齐算法（如DPO）在训练过程中容易出现过优化问题，导致模型偏离参考策略，性能下降。本文旨在解决这一问题。

Method: 提出IS-DAAs方法，通过重要性比率调整目标函数，并限制重要性比率的最高值以避免高方差问题。

Result: 实验表明，IS-DAAs能有效缓解过优化问题，尤其在低正则化强度下表现优于其他方法。

Conclusion: IS-DAAs是一种有效的解决方案，能够提升直接对齐算法的性能，并公开了实现代码。

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [130] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分自编码器（VAE）的模型VAE-LF，用于高效表示和补充高维不完整（HDI）电力负荷监测（PLM）数据，提升电力负荷预测（PLF）性能。


<details>
  <summary>Details</summary>
Motivation: 智能电网中高维不完整的PLM数据对PLF模型性能提出了挑战，需要一种高效的数据表示和补充方法。

Method: VAE-LF通过编码器-解码器结构学习数据的低维潜在表示，将HDI PLM数据分块输入模型并生成补充数据。

Result: 在UK-DALE数据集上，VAE-LF在5%和10%稀疏度测试中表现优于基准模型，RMSE和MAE显著降低，尤其在低稀疏度数据上表现突出。

Conclusion: VAE-LF为智能电网中的电力负荷管理提供了一种高效的数据补充解决方案。

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [131] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: RRP是一种新的强化学习探索策略，通过向环境奖励添加零均值噪声增强策略多样性，提升探索效果。


<details>
  <summary>Details</summary>
Motivation: 探索策略在强化学习中至关重要，但现有方法如动作扰动策略（如ε-greedy）仍有改进空间。RRP旨在通过奖励扰动进一步提升探索效果。

Method: RRP通过向环境奖励添加零均值噪声，增强策略多样性，同时兼容现有动作扰动策略。

Result: 实验表明，RRP显著提升了PPO和SAC算法的性能，提高了样本效率并帮助逃离局部最优。

Conclusion: RRP是一种轻量级、通用的探索策略，可无缝集成到现有RL算法中，为探索和奖励塑形提供了理论联系。

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [132] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: 提出一种多视图、多输出的GNN模型，结合政府检查评分和众包报告数据预测城市事件的真实状态，并在纽约市案例中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 政府检查评分稀疏且众包报告存在偏差，需结合两者预测城市事件的真实状态。

Method: 使用多视图、多输出的GNN模型，整合政府评分和众包报告数据。

Result: 模型在真实和半合成数据上优于仅使用单一数据源的模型，并量化了众包报告的偏差。

Conclusion: 该方法适用于处理异构、稀疏和偏差数据的潜在状态预测问题。

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [133] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: 论文提出了一种适用于稀疏性和非独立同分布权重的深度神经网络稳定性定理，扩展了初始化方案的理论基础。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络中梯度爆炸或消失的问题，特别是针对稀疏性和非独立同分布权重的网络模型。

Method: 利用随机矩阵理论的最新进展，建立了一个适用于更广泛网络模型的谱稳定性定理。

Result: 为具有结构化和依赖随机性的现代神经网络提供了严格的谱稳定性保证。

Conclusion: 扩展了初始化方案的理论适用范围，为更复杂的网络模型提供了稳定性保障。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [134] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Main category: cs.LG

TL;DR: 论文提出了一套设计模式，用于构建具有可证明抗提示注入攻击能力的AI代理，分析了其效用与安全的权衡，并通过案例研究展示了实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI代理能力增强，其安全性成为关键挑战，尤其是提示注入攻击对工具访问或敏感信息处理的威胁。

Method: 提出了一套原则性设计模式，系统分析其效用与安全的权衡，并通过案例研究验证。

Result: 设计模式为AI代理提供了可证明的抗提示注入攻击能力。

Conclusion: 该研究为构建安全的AI代理提供了实用且可验证的设计方案。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [135] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 该研究利用世界银行的公开合成数据集IMAGIC-500，为社会经济数据中的缺失值填补提供了全面的基准测试，评估了多种填补方法的性能。


<details>
  <summary>Details</summary>
Motivation: 社会经济数据因隐私保护难以公开共享，导致缺失值填补方法缺乏系统评估基准。研究旨在填补这一空白。

Method: 使用IMAGIC-500数据集，在不同缺失机制（MCAR、MAR、MNAR）和缺失比例（10%至50%）下评估多种填补方法。

Result: 研究揭示了统计、传统机器学习和深度学习填补方法的优缺点，并展示了其对下游预测任务的影响。

Conclusion: IMAGIC-500数据集和基准测试有助于开发鲁棒的填补算法，促进社会科学研究的可重复性。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [136] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: 论文提出了一种敏捷强化学习（aRL）方法，用于解决边缘计算中软实时应用任务调度的复杂性和动态性问题。通过智能探索和动作屏蔽，aRL能够快速适应并收敛，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 边缘计算环境中任务调度的复杂性、动态性以及传统启发式和元启发式算法的局限性，促使研究者探索强化学习算法。然而，强化学习的学习时间长和全局动作空间大的问题需要解决。

Method: 提出敏捷强化学习（aRL），结合智能探索和动作屏蔽技术，减少无关动作的随机探索，提升RL-agent的预测性和适应性。

Result: 实验表明，aRL在命中率和收敛速度上优于基线方法，能够快速适应动态环境。

Conclusion: aRL是一种适用于边缘计算中软实时应用任务调度的有效方法，具有快速适应和高性能的优势。

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [137] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: 论文提出了一种名为SG-GNN的图神经网络架构，通过构建具有更高标签同质性的替代图结构，解决了GNN在异质性数据上的性能问题。


<details>
  <summary>Details</summary>
Motivation: GNN通常假设同质性并依赖局部消息传递，但在异质性数据（连接节点标签不同）上表现不佳。

Method: 通过链接具有相似结构属性的节点创建替代图结构，提出SG-GNN架构，自适应学习原始图与新结构图的权重。

Result: 在多个异质性基准数据集上，SG-GNN实现了最先进或极具竞争力的性能。

Conclusion: 利用结构信息指导GNN能显著提升性能，特别是在异质性数据上。

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [138] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: 论文探讨了数据填补技术在水分配网络监测中的应用，比较了多种填补方法的效果。


<details>
  <summary>Details</summary>
Motivation: 智能水表数据存在缺失，影响运营决策和效率，填补数据能提升数据质量。

Method: 比较了k-最近邻、MissForest、Transformer和循环神经网络等填补方法。

Result: 有效的数据填补显著提升了水消耗数据的准确性和可靠性。

Conclusion: 填补技术可改善漏损检测和预测性维护等应用的决策质量。

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [139] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: InfoDPCCA是一种动态概率CCA框架，用于提取两个相互依赖序列的共享潜在表示，平衡压缩与预测能力，并提升可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从高维序列数据中提取有意义的潜在表示是机器学习中的关键挑战，尤其在自然科学和工程领域。

Method: 提出信息论目标，学习共享和独立的潜在组件，采用两步训练方案和残差连接机制。

Result: 在合成和医学fMRI数据实验中表现优异。

Conclusion: InfoDPCCA是一种有效的表示学习工具，代码已开源。

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [140] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: SeerAttention-R是一个稀疏注意力框架，专为推理模型的长解码设计，保留了自蒸馏门控机制学习注意力稀疏性的特点，并移除了查询池化以适应自回归解码。


<details>
  <summary>Details</summary>
Motivation: 为推理模型的长解码提供高效的稀疏注意力解决方案，同时保持灵活性和易集成性。

Method: 通过轻量级门控插件实现稀疏注意力，无需修改预训练模型参数，支持自回归解码。

Result: 在AIME基准测试中，仅用0.4B tokens训练，SeerAttention-R在4K token预算下保持接近无损的推理准确性，稀疏注意力块大小为64/128时速度提升显著。

Conclusion: SeerAttention-R是一种高效、灵活的稀疏注意力框架，适用于长解码任务，并展示了显著的性能优势。

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [141] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 论文提出了一种基于流匹配的概率模型InFOM，用于预测智能体在长期未来可能访问的状态，并通过潜在变量捕捉用户意图，显著提升了强化学习的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在机器学习中已广泛应用，但在强化学习（RL）中仍面临长期依赖的挑战。本文旨在通过建模状态占用和用户意图，提升RL的样本效率和鲁棒性。

Method: 提出了一种基于流匹配的概率模型InFOM，通过潜在变量捕捉用户意图，并结合广义策略改进进行适应。

Result: 在36个基于状态和4个基于图像的基准任务中，InFOM实现了1.8倍的中位数回报提升和36%的成功率提升。

Conclusion: InFOM通过建模长期状态占用和用户意图，显著提升了强化学习预训练的效果，为RL领域的大规模预训练提供了新思路。

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [142] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: 论文提出了一种多实验方程学习方法（ME-EQL），通过两种方法（OAT ME-EQL和ES ME-EQL）从ABM数据中学习连续模型，显著降低了参数恢复的误差。


<details>
  <summary>Details</summary>
Motivation: 解决传统方程学习方法（EQL）需要大量模拟且泛化性不足的问题。

Method: 提出了两种ME-EQL方法：OAT ME-EQL（逐个参数集学习并插值）和ES ME-EQL（构建统一模型库）。

Result: 两种方法显著降低了参数恢复误差，OAT ME-EQL在参数空间泛化性更好。

Conclusion: 多实验方程学习可提升复杂生物系统模型的泛化性和可解释性。

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [143] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: 论文提出了一种新的局部特征重要性方法LMDI+，用于解决现有方法（如LIME和TreeSHAP）在解释树模型预测时的不足，并在12个真实数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域中，树模型的预测需要可解释性以确保信任。现有的局部特征重要性方法依赖近似和扰动，忽略了模型内部结构，导致不稳定。

Method: 通过扩展全局MDI+方法，提出局部MDI+（LMDI+），利用决策树与线性模型在节点基础上的等价性，实现样本特定的特征重要性分析。

Result: LMDI+在识别实例特定信号特征上优于LIME和TreeSHAP，下游任务性能平均提升10%，且在不同随机森林拟合中表现更稳定。

Conclusion: LMDI+不仅提升了局部可解释性，还支持反事实识别和同质子组发现，为树模型的解释提供了更可靠的工具。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [144] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: BioLangFusion通过整合预训练的DNA、mRNA和蛋白质语言模型，生成统一的分子表示，在五种分子属性预测任务中表现优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 受分子生物学中心法则（基因到转录本到蛋白质的信息流）启发，通过生物意义明确的密码子级别对齐模态嵌入，确保跨模态直接对应。

Method: 研究三种标准融合技术：密码子级别嵌入拼接、熵正则化注意力池化和跨模态多头注意力，每种技术为结合模态特定信号提供不同归纳偏置。

Result: BioLangFusion在五种分子属性预测任务中优于单模态基线，表明即使简单的预训练模型融合也能以最小开销捕获互补的多组学信息。

Conclusion: BioLangFusion展示了预训练模型简单融合的有效性，为多模态分子表示提供了一种低开销的解决方案。

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [145] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: KARMA是一种新型时间序列预测模型，通过自适应时间通道分解（ATCD）和混合频率-时间分解（HFTD）模块动态提取趋势和季节性成分，结合多尺度Mamba-based KarmaBlock高效处理全局和局部信息，显著提升了预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分解方法固定且单一，无法挖掘复杂动态特征；Transformer模型因计算复杂度高难以处理长序列和复杂动态关系。

Method: 提出KARMA模型，包含ATCD模块动态提取趋势和季节性成分，HFTD模块进一步分解为频域和时域，结合多尺度KarmaBlock协调处理全局和局部信息。

Result: 在八个真实数据集上的实验表明，KARMA在预测精度和计算效率上显著优于主流基线方法。

Conclusion: KARMA通过动态分解和高效信息处理，解决了传统方法和Transformer模型的局限性，为多变量长期时间序列预测提供了有效解决方案。

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [146] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习（DRL）中环境状态扰动的对抗攻击与防御，提出了一种新的防御框架BAT，显著提升了代理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注环境状态扰动，而这是实际场景中的常见问题，因此需要提高DRL代理的鲁棒性。

Method: 首先提出一种非目标攻击方法作为校准对手，然后提出BAT框架，结合监督学习和对抗训练来增强代理。

Result: 实验证明主流代理在环境状态扰动下脆弱，而BAT能显著提升鲁棒性。

Conclusion: BAT框架有效解决了环境状态扰动问题，优于现有鲁棒强化学习算法。

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [147] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: 提出了一种数据增强与扩展框架，通过偏好细化和多级直接偏好优化（M-DPO），在小样本数据上训练生成奖励模型，性能媲美大规模数据集训练的模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如直接偏好优化DPO）在样本配对和数据多样性上效率低下，限制了生成奖励模型的训练效果。

Method: 采用Chain-of-Thought（CoT）采样揭示高质量偏好关系，结合基于困惑度的评分机制和M-DPO，捕捉样本间更细粒度的偏好差异。

Result: 实验表明，该方法显著提升了数据效率和模型性能，小样本训练的奖励模型性能与大规模数据集训练的模型相当。

Conclusion: 数据高效策略在奖励模型优化中潜力巨大，为低资源RLHF应用提供了可靠解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [148] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: 论文提出了一种新数据集和模型TimeFlex，旨在揭示时间序列特性与模型性能之间的明确联系。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在时间序列预测中表现优异，但缺乏对数据特性与模型架构匹配性的系统研究。

Method: 使用高斯过程生成具有明确特性的数据集，并开发模块化模型TimeFlex以处理多样时间动态。

Result: TimeFlex与现有先进模型对比，展示了在不同时间序列条件下的性能表现。

Conclusion: 研究为时间序列特性与模型选择提供了更清晰的指导。

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [149] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: 论文研究了三种神经网络架构（Transformer、GCN和LSTM）在命题逻辑任务中的泛化能力，发现其对未见模式（尤其是涉及否定的组合）的泛化能力有限，表明需要更强的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否能学习和表示符号规则，特别是在命题逻辑任务中的泛化能力。

Method: 使用平衡扩展的数据集，评估三种架构（Transformer、GCN、LSTM）在生成逻辑公式满足分配任务中的表现，重点关注未见操作符组合的泛化能力。

Result: 所有模型在训练分布内表现良好，但对未见模式（尤其是涉及否定的组合）泛化能力差，Transformer需引入结构偏置才能正确应用否定。

Conclusion: 标准架构在系统表示逻辑运算符方面存在局限性，需要更强的归纳偏置以支持基于规则的稳健推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [150] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: 本文研究了TabPFNv2的微调策略及其内部机制变化，发现全微调是最优方法，并揭示了其成功的原因在于改进的相似性度量。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过微调优化TabPFNv2在表格数据上的性能，并理解其内部机制的变化。

Method: 系统评估多种微调策略，分析微调对TabPFNv2内部机制的影响，类比检索增强模型。

Result: 全微调在时间和效果上最优，微调后模型能更准确地衡量相似性，提升性能，但在某些数据集上表现不稳定。

Conclusion: 全微调是TabPFNv2的最佳实践，但其适用性依赖于数据集特性。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [151] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 论文提出了一种自我感知的弱点驱动问题合成框架（SwS），用于增强强化学习中的问题生成效率，提升模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有问题合成方法效率低，且未考虑模型能力，导致生成的问题对强化学习效果有限。

Method: 通过识别模型在训练中的失败案例，提取核心概念并合成新问题，针对性增强模型弱点。

Result: 在不依赖外部知识蒸馏的情况下，SwS框架在8个主流推理基准上平均提升了7B和32B模型的性能10.0%和7.7%。

Conclusion: SwS框架通过自我识别和解决弱点，显著提升了模型在复杂推理任务中的泛化能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [152] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 论文提出了一种新框架BranchSBM，用于学习分支的Schrödinger桥，解决了现有方法无法捕捉多路径演化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如流匹配和Schrödinger桥匹配）只能建模单一路径，无法处理从共同起源到多个不同结果的分支演化。

Method: BranchSBM通过参数化多个时间依赖的速度场和增长过程，实现了对多终端分布的建模。

Result: BranchSBM不仅表达能力更强，而且在多路径导航、细胞命运分叉建模和细胞响应模拟等任务中表现出色。

Conclusion: BranchSBM为解决多路径演化问题提供了有效工具，扩展了生成建模的应用范围。

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [153] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种新的重要性分数外推框架，仅需少量数据训练即可预测整个数据集的样本重要性，解决了现有剪枝技术需完整训练的低效问题。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集训练机器学习模型计算成本高昂，现有数据剪枝技术需完整训练才能识别冗余样本，效率低下。

Method: 引入重要性分数外推框架，基于少量数据训练（如k近邻和图神经网络）预测全数据集样本重要性。

Result: 在两种先进剪枝方法、四种数据集和三种训练范式下验证了方法的有效性。

Conclusion: 分数外推是扩展昂贵计算任务（如剪枝、数据归因）的有前景方向。

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [154] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: SPEED是一种自适应在线RL课程，通过选择性选择中等难度的训练示例，显著提高了大型语言模型的训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统均匀采样方法在训练大型语言模型时计算成本高，效率低。

Method: 提出SPEED方法，选择性采样中等难度的提示，优化梯度估计器的信噪比。

Result: 实验表明，SPEED实现了2至6倍的训练加速，且不影响准确性。

Conclusion: SPEED无需手动调整，可无缝集成到标准RL算法中，显著提升训练效率。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [155] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Edit Flows是一种非自回归模型，通过编辑操作（插入、删除、替换）实现灵活的可变长度序列生成，优于自回归和掩码模型。


<details>
  <summary>Details</summary>
Motivation: 解决非自回归模型在生成可变长度序列时的局限性，使其更贴近序列数据的结构。

Method: 利用连续时间马尔可夫链在序列空间上定义编辑操作，并通过扩展状态空间和辅助变量实现高效训练。

Result: 在图像描述生成任务中优于自回归和掩码模型，在文本和代码生成中显著优于掩码构造。

Conclusion: Edit Flows提供了一种灵活且高效的序列生成方法，为非自回归模型开辟了新方向。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [156] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种通过测试时扩展（test-time scaling）提升LLM推理能力的方法，重点在于训练模型进行上下文探索（in-context exploration），以实现性能的外推（extrapolation）。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在外推性能上表现不佳，作者希望通过训练LLM进行上下文探索来改善这一现象。

Method: 提出了e3方法，包括三个关键部分：1）利用LLM在验证和生成等任务上的不对称能力进行链式操作；2）利用错误轨迹的“负梯度”增强强化学习中的探索；3）通过特定设计的课程学习将任务难度与训练token预算耦合。

Result: e3方法训练的1.7B模型在AIME'25和HMMT'25上表现最佳，并能外推到2倍训练token预算，同时提升了pass@1和pass@k分数。

Conclusion: 通过上下文探索训练LLM可以有效提升其推理能力，并实现性能的外推。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [157] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: FZOO是一种快速零阶优化器，通过减少收敛所需的前向传递次数，显著提高了内存效率，同时保持了与Adam相当的速度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调中GPU内存瓶颈问题，同时避免零阶优化器收敛速度慢的缺点。

Method: 采用批量单侧估计和基于损失标准差的步长调整，结合Rademacher随机向量扰动和CUDA并行处理。

Result: FZOO在多种模型和任务中表现优于MeZO，准确率平均提高3%，前向传递次数减少3倍。

Conclusion: FZOO实现了单GPU高效微调，为内存高效预训练提供了方向。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [158] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 论文提出了一种可视化方法，用于补充Performative Prediction的理论研究，并引入了一个新的扩展场景。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要从理论角度研究Performative Prediction，缺乏实际可视化工具来辅助理解。

Method: 提出了一种解耦风险可视化方法，用于分析模型参数和数据参数的损失景观，并扩展了Performative Prediction的场景。

Result: 通过可视化方法揭示了新的性质，并验证了现有算法在更现实条件下的表现。

Conclusion: 可视化方法为Performative Prediction提供了实践洞察，扩展场景更贴近现实应用。

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [159] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: Agentic Neural Network (ANN) 是一个多智能体协作框架，通过神经网络的层级结构动态优化任务分解与协作，显著提升准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统依赖静态配置，限制了复杂任务的灵活性和效率。

Method: ANN 采用两阶段优化策略：前向阶段动态分解任务并构建协作团队，后向阶段通过反馈优化协作。

Result: 在四个基准数据集上，ANN 表现优于现有方法，准确性和适应性显著提升。

Conclusion: ANN 提供了一个可扩展、数据驱动的多智能体框架，结合了 LLMs 的协作能力和神经网络的效率。

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [160] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: 任务向量通过线性组合原始演示形成单一可重用表示，加速上下文学习推理。线性组合猜想得到理论与实证支持，揭示了其在低秩映射中的有效性及高秩映射中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管任务向量在经验上成功，但其出现和功能的原理尚不明确，本研究旨在揭示其机制。

Method: 提出线性组合猜想，通过损失景观分析和实证验证任务向量的形成与功能。

Result: 任务向量在低秩映射中有效，但在高秩映射中失败；通过注入多个任务向量可增强效果。

Conclusion: 研究深化了对任务向量及上下文学习机制的理解，为改进任务向量提供了方向。

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [161] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 提出了一种基于物理信息的神经模拟器，利用Kelvinlet先验改进软组织变形模拟，实现高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 快速准确的软组织变形模拟对手术机器人和医学培训至关重要。

Method: 将Kelvinlet先验集成到神经模拟器中，结合大规模FEM模拟，改进神经网络预测。

Result: 实现了高保真度的实时软组织模拟，适用于手术操作。

Conclusion: Kelvinlet增强学习是一种高效策略，适用于手术中的实时物理感知软组织模拟。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [162] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Main category: stat.ML

TL;DR: TelePiT是一种新型深度学习架构，通过多尺度物理和遥相关建模提升全球S2S预报能力。


<details>
  <summary>Details</summary>
Motivation: S2S预报因大气系统的混沌性和多尺度复杂交互而极具挑战性，现有方法未能明确建模关键物理过程和遥相关。

Method: TelePiT包含三个核心组件：球谐嵌入、多尺度物理神经ODE和遥相关感知Transformer。

Result: 实验表明，TelePiT显著优于现有数据驱动方法和数值天气预报系统，如2米温度RMSE降低57.7%。

Conclusion: TelePiT通过物理和遥相关建模，显著提升了S2S预报的准确性。

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [163] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Main category: stat.ML

TL;DR: 本文提出了一种基于Wasserstein距离的集成聚合方法WWAggr，用于提升高维变化点检测的性能，解决了传统聚合方法的不足和决策阈值选择问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据流中的变化点检测（CPD）面临数据模式复杂性和常见假设违反的挑战，现有深度神经网络检测器性能仍有提升空间。集成方法虽能提升鲁棒性，但传统聚合技术（如平均）未能充分利用问题特性。

Method: 提出WWAggr方法，基于Wasserstein距离进行任务特定的集成聚合，适用于多种深度CPD模型集成。

Result: WWAggr方法在多种集成模型中表现优异，并解决了CPD中决策阈值选择的长期问题。

Conclusion: WWAggr为高维CPD提供了一种高效且通用的解决方案，显著提升了检测性能。

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [164] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Main category: stat.ML

TL;DR: 论文提出了一种在多臂老虎机设置下识别受可行性约束的帕累托集的方法，其算法在固定置信度识别中表现优于传统方法，并证明了其样本复杂度的近优性。


<details>
  <summary>Details</summary>
Motivation: 解决在多目标优化中识别帕累托集时面临的可行性约束问题，特别是在未知均值的情况下。

Method: 提出一种新算法，结合固定置信度识别，优于传统的两阶段方法和竞速类算法。

Result: 算法在样本复杂度上接近理论下界，并通过实验验证了其优越性。

Conclusion: 该方法在多目标优化中具有高效性和理论保障，适用于实际应用。

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [165] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Main category: stat.ML

TL;DR: 提出了一种基于条件深度度量的新型模型无关不确定性量化算法，用于定义预测区域，适用于可分离希尔伯特空间中的预测变量和响应变量。


<details>
  <summary>Details</summary>
Motivation: 深度度量在复杂随机对象中的应用广泛，但将其整合到回归建模中以提供预测区域的研究较少。

Method: 结合条件核均值嵌入和集成深度度量，并引入保形预测变体以增强实用性。

Result: 算法在模拟研究和实际应用中表现良好，具有快速收敛率和一致性保证。

Conclusion: 该方法在理论和实践中均表现出色，适用于功能数据和高维场景。

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [166] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 论文研究了在类别不平衡数据上训练中心随机森林（CRF）的理论问题，提出了一种基于重要性采样（IS）的去偏估计器IS-ICRF，证明了其中心极限定理（CLT）和方差降低的优势。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡数据在分类任务中常见，现有方法通过重平衡数据集训练分类器，但缺乏理论分析。本文旨在填补这一空白，尤其是针对CRF的理论研究。

Method: 通过理论分析CRF在无限情况下的CLT，提出基于重要性采样的去偏估计器IS-ICRF，并分析其性能。

Result: 证明了IS-ICRF满足CLT且在高度不平衡情况下方差降低，实验验证了结果对Breiman随机森林的有效性。

Conclusion: 理论分析和实验结果表明，在重平衡数据集上训练随机森林（并去偏）优于直接使用原始数据。

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [167] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: stat.ML

TL;DR: 提出了一种无需标签的高效概念漂移检测算法，基于统计过程控制，适用于大规模数据集，并在计算受限下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型广泛应用于自动化决策，但概念漂移的早期检测对模型性能至关重要，尤其是在标签延迟的场景下。

Method: 采用经典统计过程控制方法，设计无需标签的概念漂移检测算法，并引入新框架处理已有检测后的漂移场景。

Result: 在计算受限条件下，新方法统计功效优于现有方法，数值模拟验证了其有效性。

Conclusion: 该算法为无标签环境下的概念漂移检测提供了灵活高效的解决方案，适用于大规模数据场景。

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [168] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 提出了一种基于CLIP框架的多模态模型，用于从大规模蛋白质数据集中检索结构和语义相似的蛋白质，以辅助功能注释。


<details>
  <summary>Details</summary>
Motivation: 利用视觉-语言模型（VLMs）的最新进展，解决蛋白质结构功能注释的挑战。

Method: 采用对比学习框架对齐3D蛋白质结构与功能注释，并构建了一个包含约20万蛋白质-描述对的数据集。

Result: 模型在PDB和EMDB数据集上展示了良好的零样本检索性能。

Conclusion: 多模态基础模型在蛋白质结构-功能理解方面具有潜力。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [169] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: 提出了一种名为Re4MPC的多模型运动规划方法，通过结合非线性模型预测控制（NMPC）和深度强化学习（DRL），提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的高自由度机器人运动规划方法计算成本高，难以应用于实际场景。

Method: Re4MPC通过动态选择NMPC的模型、成本和约束条件，结合DRL框架学习决策策略。

Result: 实验表明，Re4MPC比传统NMPC方法计算效率更高，成功率更高。

Conclusion: Re4MPC为高自由度机器人提供了一种高效的运动规划解决方案。

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [170] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: 论文提出了一种新的线性广义Bradley-Terry模型，结合扩散先验，确保单调性，并在有限数据集上提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于比较的偏好学习模型（如大型语言模型）缺乏单调性保证，而唯一被证明单调的广义Bradley-Terry模型无法泛化到未比较数据。

Method: 提出线性广义Bradley-Terry模型与扩散先验结合，并确定嵌入条件的充分条件以保证单调性。

Result: 实验表明，单调性并非普遍保证，新模型在有限数据集上提高了准确性。

Conclusion: 新模型在保证单调性的同时，提升了泛化能力和准确性。

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [171] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Main category: math.OC

TL;DR: MOSS是一个多目标优化框架，用于构建稳定的决策规则集，结合了稀疏性、准确性和稳定性三个关键标准。


<details>
  <summary>Details</summary>
Motivation: 解决决策规则集中准确性、稀疏性和稳定性之间的权衡问题。

Method: 开发了一种专门的切割平面算法，快速计算Pareto前沿，适用于大规模问题。

Result: MOSS在预测性能和稳定性上优于现有规则集成方法。

Conclusion: MOSS为实践者提供了一个高效的工具，用于评估和选择最优的决策规则集。

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [172] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Main category: math.OC

TL;DR: 提出了一种连续策略-值迭代算法，通过Langevin型动态同时更新随机控制问题的值函数近似和最优控制。


<details>
  <summary>Details</summary>
Motivation: 解决熵正则化松弛控制问题和经典控制问题中的无限时域优化，结合机器学习的分布采样和非凸学习技术。

Method: 利用Langevin型随机微分方程进行连续更新，同时优化值函数和最优控制。

Result: 在Hamiltonian单调性条件下，证明了策略改进和收敛到最优控制。

Conclusion: 该方法为随机控制问题提供了高效的优化框架，适用于机器学习的非凸优化技术。

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [173] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [174] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Main category: math.OC

TL;DR: 论文提出了一种利用优化问题几何结构的方法，通过降维映射改善优化问题的条件，加速梯度下降收敛。


<details>
  <summary>Details</summary>
Motivation: 高维优化问题的解集常形成光滑流形，利用这种结构可以加速优化过程。

Method: 引入降维映射框架，分析其对优化问题曲率的影响。

Result: 设计良好的降维映射能改善问题条件，理论上加速梯度下降收敛。

Conclusion: 该框架为利用结构信息加速优化提供了理论解释，统一了多种场景。

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [175] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Main category: math.OC

TL;DR: 本文提出了一种新的迭代硬阈值算法，用于解决稀疏优化问题中的混合约束问题，并提供了全局收敛保证。


<details>
  <summary>Details</summary>
Motivation: 稀疏优化中，硬约束（如ℓ0伪范数）能提供可控的稀疏性，但实际应用中常需额外约束。现有方法通常需要闭式投影或仅提供局部收敛保证，无法满足全局收敛需求。

Method: 提出了一种新的迭代硬阈值算法，采用两步连续投影算子处理混合约束，并通过稀疏性与次优性的权衡提供全局收敛保证。

Result: 在确定性、随机和零阶设置下，算法在受限强凸/平滑假设下实现了目标值的全局收敛。

Conclusion: 本文填补了稀疏优化中混合约束问题的空白，并通过新的证明技术改进了现有结果，尤其在零阶情况下优于最新研究。

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [176] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Main category: stat.ME

TL;DR: 论文介绍了R包midr，用于实现最大解释分解（MID），通过最小化黑盒模型预测函数与低阶加性表示之间的平方误差，提供可解释的全局替代模型。


<details>
  <summary>Details</summary>
Motivation: 在需要模型和预测可解释性的领域，采用适当的可解释机器学习（IML）和可解释人工智能（XAI）方法至关重要。

Method: 提出MID方法，通过功能分解生成黑盒模型的低阶加性表示，并开发R包midr实现该方法。

Result: midr包能够构建具有高级分析能力的全局替代模型，帮助理解黑盒模型。

Conclusion: MID和midr为黑盒模型提供了有效的解释工具，适用于需要高解释性的应用场景。

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [177] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Main category: cs.CY

TL;DR: 该研究探讨了机器学习方法在预测学生学业表现中的应用，发现多层感知器分类器（MLPC）在测试集上表现最佳，准确率达86.46%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用学生的行为、学术和人口统计数据，通过机器学习方法预测学业表现，以验证神经网络的潜力。

Method: 使用多层感知器分类器（MLPC）等经典机器学习模型，结合特征选择方法和多种评估方式，并与现有文献对比。

Result: MLPC在测试集上最高准确率为86.46%，10折交叉验证中测试集平均准确率为79.58%，训练集为99.65%。

Conclusion: MLPC的优异表现表明神经网络是高效的数据模型，特征选择方法对性能提升至关重要，可解释性方法有助于验证模型。

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [178] [Power Domain Sparse Dimensional Constellation Multiple Access (PD-SDCMA) for Enabled Flexible PONs](https://arxiv.org/abs/2506.08053)
*Yuhao Lian,Xiao Han,Xinmao Deng*

Main category: cs.ET

TL;DR: 提出了一种新型传输技术PD-SDCMA，通过高维信号空间中的稀疏叠加，提升下一代光纤接入系统的容量和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统OMA-PON难以满足下一代PON对高谱效和灵活性的需求。

Method: 采用信号空间维度选择策略（S2D-strategy），将低维星座稀疏叠加至高维空间以减少多用户干扰。

Result: 在25公里单模光纤系统中，PD-SDCMA比PD-NOMA和3D-NOMA支持更多用户并显著降低误码率。

Conclusion: PD-SDCMA为灵活PON的演进提供了高效低成本解决方案。

Abstract: With the commercial deployment of 5G and the in-depth research of 6G, the
demand for high-speed data services in the next-generation fiber optic access
systems is growing increasingly. Passive optical networks (PONs) have become a
research hotspot due to their characteristics of low loss, high bandwidth, and
low cost. However, the traditional orthogonal multiple access (OMA-PON) has
difficulty meeting the requirements of the next-generation PON for high
spectral efficiency and flexibility. In this paper, a novel transmission
technology, namely power-domain sparse dimension constellation multiple access
(PD-SDCMA), is proposed for the first time. Through the signal space dimension
selection strategy (S2D-strategy) in the high-dimensional signal space, the
low-dimensional constellation is sparsely superimposed into the
high-dimensional space, thereby reducing multi-user interference and enhancing
the system capacity. PD-SDCMA supports higher-order modulation formats and more
access groups, and is also compatible with the existing orthogonal frequency
division multiplexing (OFDM) architecture. The simulation results show that in
a 25 km single-mode fiber system, compared with PD-NOMA and 3D-NOMA, PD-SDCMA
can support more users and significantly reduce BER. This technology provides
an efficient and low-cost solution for the evolution of Flexible PONs.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [179] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Main category: physics.med-ph

TL;DR: 论文提出了一种基于联邦学习的CBCT-to-sCT合成方法，解决了多中心数据共享的限制，并验证了其跨中心的泛化能力。


<details>
  <summary>Details</summary>
Motivation: CBCT在图像引导放疗中存在噪声、软组织对比度低和伪影等问题，传统方法受限于机构异质性和数据隐私法规。

Method: 采用跨机构水平联邦学习框架（FedSynthCT），结合条件生成对抗网络，在多中心数据上协作训练。

Result: 联邦模型在多个中心表现出良好的泛化能力，MAE为64.38-85.90 HU，SSIM为0.882-0.922，PSNR为32.86-34.91 dB，外部验证结果也表现优异。

Conclusion: 联邦学习为CBCT-to-sCT合成提供了可行的隐私保护解决方案，支持跨机构协作建模，无需集中数据共享或特定站点微调。

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [180] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 研究发现，指令调优的多模态大语言模型（MLLMs）在视频和音频任务中显著优于非指令调优模型，且其层次结构与大脑功能区域对齐。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在评估多模态刺激下MLLMs与大脑对齐的空白，尤其是基于指令调优模型的研究。

Method: 利用13种视频任务指令和6种视频、2种音频指令调优MLLMs，测量其预测自然电影观看时神经活动的程度。

Result: 指令调优视频MLLMs比非指令调优多模态和单模态模型分别高出15%和20%，且任务表征清晰分离。

Conclusion: 任务指令显著提升MLLMs与大脑活动的对齐，为研究大脑多模态信息处理提供了新方向。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [181] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Main category: q-bio.QM

TL;DR: 通过系统研究八种分子特征表示类型，结合AutoML技术，提升Caco-2渗透性预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: Caco-2渗透性是早期药物发现中预测口服吸收的关键指标，需提高计算预测的准确性和效率。

Method: 使用2D/3D描述符、结构指纹和深度学习嵌入，结合AutoML技术，评估不同表示类型在TDC和OCHEM数据集上的表现。

Result: PaDEL、Mordred和RDKit描述符表现最佳，AutoML模型CaliciBoost的MAE性能最优，3D描述符使MAE降低15.73%。

Conclusion: AutoML方法在ADMET建模中有效，为数据有限的任务提供了特征选择指导。

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [182] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Main category: q-bio.QM

TL;DR: Protriever是一个端到端的可微分框架，用于检索同源蛋白序列并同时训练目标任务，相比传统MSA方法更快且性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于MSA的同源序列检索方法计算成本高，且难以处理高度分化的序列或复杂的插入删除模式，同时与下游建模目标无关。

Method: Protriever框架通过端到端学习，结合高效向量搜索，实现了同源序列检索与目标任务的联合训练。

Result: 在蛋白质适应性预测任务中，Protriever性能优于基于MSA的序列模型，且速度快两个数量级。

Conclusion: Protriever提供了一种可扩展的替代方案，适用于不同检索策略和蛋白质数据库，具有架构和任务无关的灵活性。

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [183] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: 论文通过系统比较自回归解码和条件流匹配两种建模范式，揭示了它们在文本到音乐生成中的优缺点。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音乐生成模型的多样性使得公平评估和设计选择的影响难以确定，本文专注于建模范式的影响。

Method: 使用相同的数据集、训练配置和类似的主干架构，对比自回归解码和条件流匹配两种范式。

Result: 研究揭示了两种范式在生成质量、推理鲁棒性、可扩展性等方面的不同表现。

Conclusion: 研究结果为未来文本到音乐生成系统的设计和训练提供了实用指导。

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [184] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: 论文提出了一种将RDF知识图谱与GNN结合的方法，充分利用其语义信息，显著提升了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量RDF知识图谱，但其丰富的语义信息在GNN推荐系统中尚未被充分利用。

Method: 提出了一种综合集成RDF知识图谱与GNN的方法，利用其拓扑和内容信息，并评估了不同GNN的性能。

Result: 实验表明，利用RDF知识图谱的语义信息显著提升了推荐系统的性能。

Conclusion: 该方法为基于GNN的推荐系统在Linked Open Data云中的应用奠定了基础。

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [185] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 论文分析了t-SNE和UMAP在可视化分析中的误用现象，探讨了原因并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: t-SNE和UMAP的投影常被误用于研究簇间关系，尽管它们不能真实反映簇间距离。论文旨在揭示误用原因并提出预防措施。

Method: 通过文献综述（114篇论文）和访谈研究，验证误用普遍性并分析背后原因。

Result: 误用主要源于可视化分析中关于t-SNE和UMAP正确使用的讨论不足。

Conclusion: 提出未来方向和具体行动，以促进降维技术的合理使用。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [186] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 在NXP MCXN947微控制器上实现的关键词检测系统，结合MFCC和CNN，通过量化感知训练优化，NPU加速59倍，模型大小30.58 KB，准确率97.06%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的设备上实现实时语音交互。

Method: 使用MFCC特征提取和CNN分类器，通过量化感知训练优化模型。

Result: NPU加速59倍，模型大小30.58 KB，准确率97.06%。

Conclusion: 证明了在嵌入式平台上实现高效、低功耗语音接口的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [187] [Qymera: Simulating Quantum Circuits using RDBMS](https://arxiv.org/abs/2506.08759)
*Tim Littau,Rihan Hai*

Main category: quant-ph

TL;DR: Qymera利用关系数据库管理系统（RDBMS）进行量子电路模拟，将电路转换为SQL查询，支持多种电路输入方式，并提供性能比较框架。


<details>
  <summary>Details</summary>
Motivation: 量子电路模拟对量子计算至关重要，但现有方法可能效率不足或灵活性不够。Qymera旨在通过RDBMS提供高效且灵活的模拟方案。

Method: 将量子电路转换为SQL查询，利用RDBMS原生执行量子操作，支持图形化电路构建器和代码接口。

Result: Qymera展示了基于SQL的端到端执行能力，能够无缝集成经典工作流，适用于开发、基准测试和教育。

Conclusion: Qymera为量子计算和数据管理提供了一种高效、灵活的模拟工具，具有广泛的应用潜力。

Abstract: Quantum circuit simulation is crucial for quantum computing such as
validating quantum algorithms. We present Qymera, a system that repurposes
relational database management systems (RDBMSs) for simulation by translating
circuits into SQL queries, allowing quantum operations to run natively within
an RDBMS. Qymera supports a wide range of quantum circuits, offering a
graphical circuit builder and code-based interfaces to input circuits. With a
benchmarking framework, Qymera facilitates comparison of RDBMS-based simulation
against state-of-the-art simulation methods. Our demonstration showcases
Qymera's end-to-end SQL-based execution, seamless integration with classical
workflows, and its utility for development, benchmarking, and education in
quantum computing and data management.

</details>


### [188] [Optimizing Sparse SYK](https://arxiv.org/abs/2506.09037)
*Matthew Ding,Robbie King,Bobak T. Kiani,Eric R. Anschuetz*

Main category: quant-ph

TL;DR: 研究了稀疏SYK模型中量子与经典算法的复杂性差异，证明了在特定稀疏条件下量子算法仍能有效近似基态能量，而经典高斯态方法存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨SYK模型在稀疏化条件下的量子与经典算法复杂性差异，以理解其鲁棒性。

Method: 分析了稀疏SYK模型（$p \in [\Theta(1/n^3),1]$），证明了Hastings--O'Donnell量子算法在$p\geq\Omega(\log n/n)$时的有效性，并给出了经典高斯态方法的近似限制。

Result: 量子算法在$p\geq\Omega(\log n/n)$时仍能实现常数因子近似，而经典高斯态方法的近似因子为$O(\sqrt{\log n/pn})$。

Conclusion: 稀疏SYK模型中，量子算法在特定条件下优于经典方法，扩展了Hastings--O'Donnell的结果。

Abstract: Finding the ground state of strongly-interacting fermionic systems is often
the prerequisite for fully understanding both quantum chemistry and condensed
matter systems. The Sachdev--Ye--Kitaev (SYK) model is a representative example
of such a system; it is particularly interesting not only due to the existence
of efficient quantum algorithms preparing approximations to the ground state
such as Hastings--O'Donnell (STOC 2022), but also known no-go results for many
classical ansatzes in preparing low-energy states. However, this
quantum-classical separation is known to \emph{not} persist when the SYK model
is sufficiently sparsified, i.e., when terms in the model are discarded with
probability $1-p$, where $p=\Theta(1/n^3)$ and $n$ is the system size. This
raises the question of how robust the quantum and classical complexities of the
SYK model are to sparsification.
  In this work we initiate the study of the sparse SYK model where $p \in
[\Theta(1/n^3),1]$. We show there indeed exists a certain robustness of
sparsification. First, we prove that the quantum algorithm of
Hastings--O'Donnell for $p=1$ still achieves a constant-factor approximation to
the ground energy when $p\geq\Omega(\log n/n)$. Additionally, we prove that
with high probability, Gaussian states cannot achieve better than a
$O(\sqrt{\log n/pn})$-factor approximation to the true ground state energy of
sparse SYK. This is done through a general classical circuit complexity
lower-bound of $\Omega(pn^3)$ for any quantum state achieving a constant-factor
approximation. Combined, these show a provable separation between classical
algorithms outputting Gaussian states and efficient quantum algorithms for the
goal of finding approximate sparse SYK ground states when $p \geq \Omega(\log
n/n)$, extending the analogous $p=1$ result of Hastings--O'Donnell.

</details>


### [189] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Main category: quant-ph

TL;DR: 该论文提出了一种基于量子退火（QA）解决高阶组合优化问题的方法，通过使用ReLU基函数将目标函数转化为二次多项式形式，从而适用于QA。


<details>
  <summary>Details</summary>
Motivation: 传统二次化方法在处理涉及机器学习的复杂问题时效率低下，因为强非线性和密集交互使得常规方法难以应用。

Method: 利用ReLU基函数对目标函数建模，将其转化为等效的二次多项式形式，并结合QA设计了一种新的黑盒优化方案。

Result: 通过数值和解析验证了方法的可行性，并展示了其在机器学习代理回归器中的应用。

Conclusion: 该方法为高阶组合优化问题提供了一种有效的解决方案，扩展了QA在复杂问题中的适用性。

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [190] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Main category: quant-ph

TL;DR: 探讨量子机器学习中对抗鲁棒性与泛化性的关系，提出基于Lipschitz界的正则化训练方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子机器学习中对抗鲁棒性与泛化性的相互作用，填补现有文献的空白。

Method: 利用Lipschitz界量化鲁棒性和泛化性，提出基于模型参数的正则化训练方法。

Result: 理论结果在时间序列分析中得到验证，展示了其实际应用价值。

Conclusion: 强调可训练数据编码策略的重要性，为量子模型的鲁棒性和泛化性提供了新思路。

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [191] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Main category: quant-ph

TL;DR: 提出了一种基于神经网络的算法（NQES），用于高效计算量子多体系统的低激发态，无需显式正交化，适用于高维系统。


<details>
  <summary>Details</summary>
Motivation: 强相互作用量子多体系统的激发态计算具有基础重要性，但由于希尔伯特空间维度的指数增长，传统方法面临挑战。

Method: NQES算法通过神经网络同时输出多个低激发态，适用于高维系统，并在Haldane-Shastry模型和二维Wigner晶体中验证。

Result: 算法成功计算了多激发态及其可观测量，揭示了长程相互作用系统的能隙标度和关联特征，与实验观测一致。

Conclusion: NQES算法为量子多体系统激发态计算提供了可扩展且高效的解决方案，有望应用于量子设备基准测试等领域。

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [192] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 论文提出了一种叠加参数化量子电路，通过结合翻转量子随机存取存储器和重复直到成功协议，提升了量子机器学习的表达能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习方法依赖线性酉操作和共享可训练参数，限制了表达能力和可扩展性，无法与经典深度网络的多层非线性架构相比。

Method: 引入叠加参数化量子电路，通过振幅变换和后选择实现多项式激活函数，并并行训练多个参数集。

Result: 数值实验显示，在1D阶跃函数回归中，误差降低了三个数量级；在2D星形分类任务中，准确率提升至81.4%，方差降低三倍。

Conclusion: 叠加参数化量子电路为硬件高效的深度量子电路提供了新途径，能够学习复杂决策边界。

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [193] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种基于多目标核学习的工作流程，用于从高分辨率成像数据中推断微观结构规则，揭示了畴壁构型和局部切换动力学的关系。


<details>
  <summary>Details</summary>
Motivation: 传统的手动或基于网格的光谱测量方法无法系统性地探索铁电极化切换对复杂微观结构的依赖。

Method: 采用多目标核学习工作流程，结合自动化压电力显微镜实验，分析畴壁构型和缺陷分布对极化反转的影响。

Result: 揭示了特定畴壁几何形状和缺陷分布如何调控极化反转，并将抽象的奖励函数映射到物理可解释的描述符上。

Conclusion: 该方法不仅实现了高通量主动学习，还为微观结构控制切换现象提供了机制性见解，具有广泛适用性。

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [194] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 显微镜数据管理存在标准化不足的问题，机器学习与显微镜社区之间存在鸿沟，黑客马拉松通过合作开发解决方案来弥合这一差距。


<details>
  <summary>Details</summary>
Motivation: 显微镜数据虽丰富但格式不一致，机器学习在实时分析中的应用潜力未被充分挖掘，需要社区协作推动标准化。

Method: 通过黑客马拉松促进机器学习与显微镜专家的合作，开发标准化代码、基准数据集和数字孪生显微镜。

Result: 生成了基准数据集和数字孪生显微镜，相关代码已开源。

Conclusion: 黑客马拉松是弥合机器学习与显微镜社区差距的有效方式，为未来标准化工作奠定了基础。

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [195] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Main category: physics.geo-ph

TL;DR: 论文提出了一种名为TS-PIELM的高效物理信息机器学习方法，用于改进传统PINN在土壤固结分析中的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在土壤固结分析中的精度和效率不足，需要改进以成为有竞争力的替代方法。

Method: 提出TS-PIELM框架，将固结过程分为多个时间区间，使用单层前馈极限学习机（ELM）近似解，并通过线性方程组直接计算输出层权重。

Result: TS-PIELM的计算效率和精度分别比PINN提高1000倍和100倍（一维案例）。

Conclusion: PIML在计算岩土工程中具有强大潜力，TS-PIELM是一个高效且精确的工具。

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [196] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: TIP-Search是一个时间可预测的推理调度框架，用于在不确定工作负载下进行实时市场预测。它动态选择深度学习模型，以最大化预测准确性并满足任务截止时间。


<details>
  <summary>Details</summary>
Motivation: 高频金融系统对延迟有严格要求，需要一种能够在不确定工作负载下动态选择模型的方法。

Method: 离线分析延迟和泛化性能，在线进行任务感知选择，无需显式输入域标签。

Result: 在三个真实订单簿数据集上测试，TIP-Search比静态基线准确率提高8.5%，且100%满足截止时间。

Conclusion: TIP-Search在不确定环境下实现了高效的低延迟金融推理。

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [197] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Main category: cs.AI

TL;DR: SafeCoT是一个轻量级、可解释的框架，通过基于规则的链式思维监督改进视觉语言模型（VLMs）的拒绝行为，减少过度拒绝并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 确保视觉语言模型（VLMs）在高风险或模糊场景中生成安全且适当的响应是一个关键挑战。

Method: SafeCoT利用基于规则的链式思维（CoT）监督，通过最小化监督帮助模型推理安全风险并做出上下文感知的拒绝。

Result: 实验表明，SafeCoT显著减少了过度拒绝并提升了泛化能力，即使在有限训练数据下也表现优异。

Conclusion: SafeCoT为对齐VLM与安全关键目标提供了一种可扩展的解决方案。

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [198] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL是一个联邦领域泛化框架，通过锐度引导的梯度对齐优化解决长尾分布和优化冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长尾类别分布和冲突优化目标下表现不佳，FedTAIL旨在解决这些问题。

Method: 结合梯度一致性正则化、类别感知锐度最小化和曲率感知动态加权，增强条件分布对齐。

Result: 在标准领域泛化基准测试中表现优异，尤其在领域偏移和标签不平衡情况下。

Conclusion: FedTAIL在集中式和联邦设置下均有效，验证了其方法的优越性。

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [199] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识图谱（KGs）与大语言模型（LLMs）的新方法，以提升基于知识的因果发现能力，解决了现有LLM方法结果不稳定、不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于观测数据的因果发现方法存在局限性，而基于知识的因果发现（如利用变量元数据）是一种有潜力的替代方案。然而，现有LLM方法的结果不稳定，影响了可靠性。

Method: 通过知识图谱识别信息丰富的基于元路径的子图，并利用学习排序模型优化子图选择，将排名靠前的子图融入零样本提示中，提升LLM的因果推理能力。

Result: 在生物医学和开放领域数据集上的实验表明，该方法在F1分数上比大多数基线方法高出44.4分，且适用于多种LLM和KG。

Conclusion: 结合KGs和LLMs的方法显著提升了基于知识的因果发现的性能和稳定性，为复杂系统中的因果推理提供了可靠工具。

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [200] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 论文提出了一种基于数据驱动的交通模拟器，用于更真实地模拟交叉路口的驾驶行为，并通过交通工程相关指标评估生成轨迹预测模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的交通模拟器难以真实反映驾驶行为，而交叉路口在安全和效率方面至关重要，因此需要开发数据驱动的模拟器。

Method: 提出了一个多头部自注意力轨迹预测模型，结合信号信息，并通过模拟闭环流程评估模型。

Result: 新模型在交通工程相关指标上优于先前模型。

Conclusion: 研究为数据驱动的交通模拟器提供了评估框架和改进模型，有助于更真实地模拟交叉路口行为。

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [201] [ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference](https://arxiv.org/abs/2506.08838)
*Yuchong Xie,Wenhui Zhang,Dongdong She*

Main category: cs.CR

TL;DR: ZTaint-Havoc是一种高效的黑盒污点推断方法，通过改进传统的havoc变异方案，实现零额外执行开销，显著提升模糊测试的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统污点分析方法存在可扩展性问题，而现有的黑盒污点推断方法（如FTI）运行时开销较大。本文旨在提出一种轻量级且高效的污点推断方法。

Method: 通过计算模型分析havoc变异模式，提出ZTaint-Havoc方法，结合有效的变异算法利用识别到的热点字节。

Result: 在AFL++中实现ZTaint-Havoc，FuzzBench和UniBench上的覆盖率分别提升33.71%和51.12%，平均提升2.97%和6.12%。

Conclusion: ZTaint-Havoc是一种高效的污点推断方法，显著提升了模糊测试的性能，且运行时开销极低。

Abstract: Fuzzing is a widely used technique for discovering software vulnerabilities,
but identifying hot bytes that influence program behavior remains challenging.
Traditional taint analysis can track such bytes white-box, but suffers from
scalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box
alternative, yet typically incurs significant runtime overhead due to extra
program executions. We observe that the commonly used havoc mutation scheme in
fuzzing can be adapted for lightweight FTI with zero extra executions. We
present a computational model of havoc mode, demonstrating that it can perform
FTI while generating new test cases. Building on this, we propose ZTaint-Havoc,
a novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on
FuzzBench). We further design an effective mutation algorithm utilizing the
identified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,
implemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and
51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in
24-hour fuzzing campaigns.

</details>


### [202] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Main category: cs.CR

TL;DR: 分析了两个开源深度强化学习代理在CAGE Challenge 2中的表现，通过简化状态和动作空间以及跟踪关键事件，揭示了防御和攻击代理的细粒度行为。


<details>
  <summary>Details</summary>
Motivation: 研究深度强化学习代理在模拟网络防御中的表现，以提高其可解释性和有效性。

Method: 简化复杂状态和动作空间，跟踪关键事件，分析渗透和清除事件的模式，评估动作的有效性。

Result: 防御代理通常在主机被利用后1-2个时间步内清除渗透；某些动作的无效性高达40%-99%；诱饵服务可阻止高达94%的特权访问攻击。

Conclusion: 研究揭示了代理行为的细节，并提出了改进建议，CAGE Challenge 4已部分解决了这些问题。

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


### [203] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Main category: cs.CR

TL;DR: ReAgent是一种针对大型语言模型（LLM）代理后门攻击的新型防御方法，通过检测用户指令、代理规划和执行之间的一致性来识别后门。


<details>
  <summary>Details</summary>
Motivation: LLM代理在训练和微调过程中面临后门攻击的安全风险，可能导致恶意操作。

Method: ReAgent采用两级方法：执行层面验证代理思维与行动的一致性；规划层面通过代理重建指令检查一致性。

Result: ReAgent显著降低攻击成功率，例如在数据库操作任务中攻击成功率降低90%。

Conclusion: ReAgent展示了利用被攻击代理自身来缓解后门风险的潜力。

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [204] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: DCIDA是一种分布式电路逆向设计框架，通过联合训练的采样策略生成近最优设计，显著降低了设计误差。


<details>
  <summary>Details</summary>
Motivation: 现有设计方法在处理非可微评估、多变拓扑和近连续布局空间时存在局限性，DCIDA旨在解决这些问题。

Method: DCIDA通过联合训练的采样策略生成设计决策，并利用注入式映射将原始设计动作转换为物理表示。

Result: 实验表明，DCIDA的Transformer策略网络在复杂传输函数情况下显著优于现有方法。

Conclusion: DCIDA为分布式电路设计提供了一种高效且灵活的解决方案。

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [205] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: 论文提出了一种基于CNN和MLP的替代模型，用于近似2D壁域中参与性气体的辐射热传递解，以减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了降低数值模拟的计算成本，同时适应CNN架构的输入需求。

Method: 使用CNN和MLP构建替代模型，并与传统求解器ICARUS2D的性能进行比较。

Result: CNN和MLP均显著加速计算且误差在工业可接受范围内，CNN在精度和稳定性上优于MLP。

Conclusion: CNN在替代模型中表现更优，适用于工业应用。

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [206] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: MasHost是一个基于强化学习的框架，用于自主和查询自适应的多智能体系统设计，通过图搜索和概率采样机制优化智能体角色和交互。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统构建方法依赖人工规则，限制了自主能力并引入人为偏见，需要更自主的解决方案。

Method: 将多智能体系统构建建模为图搜索问题，通过概率采样机制联合采样智能体角色和交互，并提出分层相对策略优化（HRPO）实现多目标优化。

Result: 在六个基准测试中，MasHost表现优于现有基线，验证了其有效性、效率和结构合理性。

Conclusion: MasHost是首个基于强化学习的自主多智能体系统图构建框架，具有显著优势。

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [207] [Excluding an induced wheel minor in graphs without large induced stars](https://arxiv.org/abs/2506.08829)
*Mujin Choi,Claire Hilaire,Martin Milanič,Sebastian Wiederrecht*

Main category: math.CO

TL;DR: 论文研究了Dallard等人的猜想，证明了当H是k-轮时猜想成立，并提出了相关算法。


<details>
  <summary>Details</summary>
Motivation: 验证Dallard等人关于K_{1,d}-free图和无H诱导子图的树独立数有界的猜想。

Method: 使用广义的bramble概念与树独立数结合，证明H为k-轮时猜想成立。

Result: 证明了猜想在H为k-轮时成立，并提供了多项式时间算法。

Conclusion: 研究结果为K_{1,d}-free图和无大轮诱导子图的NP难问题提供了可解性。

Abstract: We study a conjecture due to Dallard, Krnc, Kwon, Milani\v{c}, Munaro,
\v{S}torgel, and Wiederrecht stating that for any positive integer $d$ and any
planar graph $H$, the class of all $K_{1,d}$-free graphs without $H$ as an
induced minor has bounded tree-independence number. A $k$-wheel is the graph
obtained from a cycle of length $k$ by adding a vertex adjacent to all vertices
of the cycle. We show that the conjecture of Dallard et al. is true when $H$ is
a $k$-wheel for any $k\geq 3$. Our proof uses a generalization of the concept
of brambles to tree-independence number. As a consequence of our main result,
several important $\mathsf{NP}$-hard problems such as Maximum Independent Set
are tractable on $K_{1,d}$-free graphs without large induced wheel minors.
Moreover, for fixed $d$ and $k$, we provide a polynomial-time algorithm that,
given a $K_{1,d}$-free graph $G$ as input, finds an induced minor model of a
$k$-wheel in $G$ if one exists.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [208] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Main category: cs.IT

TL;DR: 研究了在多用户MIMO-OFDM系统中使用混合波束成形和毫米波信道的调度问题，目标是最大化比例公平（PF），提出了基于贪婪算法、排序算法和机器学习的方法，并分析了性能与复杂度的权衡。


<details>
  <summary>Details</summary>
Motivation: 在混合波束成形系统中，由于复用增益有限，改进调度对提升频谱效率和长期性能至关重要，尤其是在毫米波信道下。

Method: 采用两时间尺度协议：长时尺度为用户分配模拟波束，短时尺度进行用户调度和数字预编码设计，提出了贪婪算法、排序算法和机器学习方法。

Result: 数值结果展示了不同方法在性能与复杂度之间的权衡，表明方法选择需根据具体场景需求。

Conclusion: 研究表明，在混合波束成形系统中，调度方法的选择应结合实际场景的性能和复杂度需求。

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [209] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: 论文介绍了EDINET-Bench，一个开源日本金融基准数据集，用于评估大语言模型（LLM）在金融任务中的表现，填补了日本金融数据稀缺的空白。


<details>
  <summary>Details</summary>
Motivation: 日本金融数据稀缺阻碍了金融分析领域的学术创新和LLM的发展，需要公开可用的研究资源。

Method: 通过从日本EDINET下载过去10年的年报，自动标注任务标签，构建EDINET-Bench数据集。

Result: 实验表明，即使是先进的LLM在欺诈检测和盈利预测等任务中表现仅略优于逻辑回归。

Conclusion: LLM在金融领域的应用面临挑战，需领域特定适应，公开数据集和代码以促进未来研究。

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [210] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: 论文提出了一种非正交矩阵分解方法$D$-decomposition，通过最小化正则化Frobenius损失实现，支持控制秩、稀疏性和条件数。相比传统方法（如LU或SVD），其计算复杂度更低，且在稀疏和噪声条件下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分解方法（如LU或SVD）在稀疏性和噪声条件下表现不佳，需要一种更灵活且高效的非正交分解方法。

Method: 提出$D$-decomposition，形式为$A \approx P D Q$，通过交替最小化正则化Frobenius损失实现，支持控制秩、稀疏性和条件数。

Result: 实验表明，该方法在MovieLens、MNIST、Olivetti Faces和基因表达矩阵上优于截断SVD、CUR和非负矩阵分解，尤其在稀疏和噪声条件下。

Conclusion: $D$-decomposition是一种高效且灵活的矩阵分解方法，适用于稀疏和噪声数据，具有实际应用潜力。

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [211] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: sparseGeoHOPCA是一种新的稀疏高阶主成分分析框架，通过几何视角优化高维张量分解，显著提升计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决高维和不平衡数据场景中稀疏高阶主成分分析的计算复杂性和可解释性问题。

Method: 通过展开张量并转化为结构化二元线性优化问题，将非凸稀疏目标转化为可处理的几何形式。

Result: 理论证明几何子问题与原问题的等价性，计算复杂度线性增长，实验显示在合成数据、分类和图像重建中表现优异。

Conclusion: sparseGeoHOPCA在计算效率和性能上具有显著优势，适用于高维数据和多场景应用。

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [212] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Main category: physics.soc-ph

TL;DR: 论文提出了一种基于马尔可夫决策过程（MDP）的实时级联停电缓解方法，通过强化学习优化决策，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源渗透率增加，面临级联停电风险上升的问题，需快速且复杂的实时决策。

Method: 将影响图扩展为MDP模型，结合强化学习（特别是策略梯度算法）进行决策优化，设计了保守动作和奖励机制。

Result: 在IEEE 14和118总线系统上验证，主动断开线路可有效降低级联风险，并识别出关键线路。

Conclusion: 该方法能快速收敛且保守决策，为电力系统实时级联缓解提供了有效工具。

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [213] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究探讨了使用现代解码器LLM作为文本编码器在文本到图像扩散模型中的效果，发现多层平均嵌入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型仍使用过时的T5和CLIP作为文本编码器，研究旨在探索现代LLM的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，分析12种文本编码器，研究嵌入提取方法、LLM变体和模型大小的影响。

Result: 多层归一化平均嵌入显著提升复杂提示的对齐效果，多数LLM表现优于基线T5模型。

Conclusion: 现代LLM作为文本编码器在文本到图像生成中具有优势，尤其是多层嵌入方法。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [214] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一种音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频。它通过统一的训练方法，在保持通用性的同时，生成质量优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 视频的感染力依赖于音频和视觉的和谐结合，但现有方法要么忽略音频，要么局限于特定领域。Mirage旨在解决这一问题。

Method: 采用自注意力机制的音频到视频生成模型，支持从零训练或基于现有权重训练，无需特定架构或损失组件。

Result: Mirage生成的视频在主观质量上优于其他方法，尤其在语音驱动的视频生成中表现突出。

Conclusion: Mirage为音频到视频生成提供了通用且高质量的解决方案，未来可扩展至更多应用场景。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [215] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 论文提出了一种多样性引导的MLP压缩方法（DGMR），显著减少大型视觉Transformer的参数和计算量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型Transformer模型参数过多导致计算和内存成本高昂，研究发现MLP模块占用了大部分参数。

Method: 采用Gram-Schmidt权重剪枝策略消除MLP隐藏层的冗余神经元，同时保留权重多样性以在蒸馏过程中恢复性能。

Result: 实验表明，DGMR方法在多个大型视觉Transformer上实现了57%以上的参数和FLOPs减少，且性能损失极小。

Conclusion: DGMR方法高效压缩模型，显著降低资源需求，适用于大规模视觉Transformer。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [216] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: Landsat-Bench是一套基于Landsat影像的基准测试，包含三个数据集，用于评估地理空间基础模型（GFM）的性能。


<details>
  <summary>Details</summary>
Motivation: Landsat数据缺乏标准化基准，限制了基于Landsat的地理空间基础模型的发展。

Method: 通过EuroSAT-L、BigEarthNet-L和LC100-L三个数据集建立基准，并使用SSL4EO-L预训练的GFM进行评估。

Result: SSL4EO-L预训练的GFM在下游任务中表现优于ImageNet，性能提升达4% OA和5.1% mAP。

Conclusion: Landsat-Bench为Landsat数据提供了标准化评估方法，并验证了SSL4EO-L预训练模型的有效性。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [217] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一个实时处理未校准视频流并重建动态3D场景的框架，解决了现有方法在实时性、动态建模和长期稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理未校准输入、动态场景建模和长期稳定性，StreamSplat旨在解决这些问题。

Method: 提出静态编码器中的概率采样机制和动态解码器中的双向变形场，实现高效动态建模。

Result: 在静态和动态基准测试中表现优于现有方法，支持任意长度视频流的在线重建。

Conclusion: StreamSplat在重建质量和动态建模上表现优异，适用于实时应用。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [218] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEE的统一方法，用于解决不完全监督的隐蔽物体分割（ISCOS）问题，通过结合SAM模型生成伪标签和混合粒度特征分组模块，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 隐蔽物体分割任务面临不完全标注数据和物体与背景相似性高的双重挑战，现有方法难以有效解决。

Method: 提出SEE框架，利用SAM生成伪标签，并通过伪标签生成、存储和监督策略优化训练；设计混合粒度特征分组模块提升分割一致性。

Result: 实验表明SEE在多个ISCOS任务中达到最优性能，并可作为即插即用方案提升现有模型。

Conclusion: SEE为ISCOS任务提供了一种高效且通用的解决方案，具有广泛的应用潜力。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [219] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 提出了一种基于Fast AutoAugment的数据增强方法，显著提升了小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 小目标检测性能远低于大目标，是计算机视觉中的重要挑战。

Method: 使用Fast AutoAugment快速找到最优数据增强策略。

Result: 在DOTA数据集上实现了20%的性能提升。

Conclusion: 该方法有效解决了小目标检测性能退化问题。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [220] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA是一种高效的医学视觉-语言对齐方法，通过适应预训练的视觉模型，显著提升了检索和零样本分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的跨模态对比学习方法在视觉表示能力上表现不佳，而多模态掩码建模方法虽视觉表示强，但跨模态匹配效果差。ALTA旨在解决这一矛盾。

Method: ALTA通过适应预训练的视觉模型（来自掩码记录建模），结合时间多视图放射图像输入，提升视觉-语言对齐效果。

Result: ALTA在文本到图像和图像到文本检索任务中分别比最佳对比方法高出4%和6%的绝对准确率。

Conclusion: ALTA不仅高效（仅需8%可训练参数和少量计算资源），还提升了视觉和语言理解能力。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [221] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 论文提出了一种名为DIsoN的去中心化OOD检测框架，能够在无法共享数据的情况下通过交换模型参数实现训练数据与测试数据的比较，并在医学影像数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如医学影像）部署ML模型时，需检测训练数据中未见的输入（OOD），但现有方法要么丢弃训练数据，要么假设数据集中存储，这在现实中难以实现。

Method: 提出Isolation Network框架，通过二元分类任务量化测试样本与训练数据的分离难度；进一步提出DIsoN，支持在无法共享数据时通过交换模型参数进行比较。

Result: 在四个医学影像数据集上的12个OOD检测任务中，DIsoN表现优于现有方法，同时保护数据隐私。

Conclusion: DIsoN为ML开发者提供了一种新的服务模式，即远程安全利用训练数据进行OOD检测。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [222] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单可插拔正则化器，用于改进基于扩散的生成模型，无需正样本对或额外参数。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习进展独立发展。本文旨在通过Dispersive Loss填补这一差距。

Method: 提出Dispersive Loss，鼓励隐空间中的表示分散，类似于对比自监督学习，但无需正样本对。

Result: 在ImageNet数据集上评估，显示对多种模型的性能均有提升。

Conclusion: Dispersive Loss为生成模型与表示学习之间的桥梁提供了简单有效的解决方案。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [223] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: 论文通过符号回归方法，构建了描述重子物理对物质功率谱影响的解析近似函数，并提供了不确定性预测。


<details>
  <summary>Details</summary>
Motivation: 重子物理对宇宙物质分布的影响是当前和未来宇宙学调查的关键系统误差来源，需要简单且物理合理的参数化方法。

Method: 使用符号回归从CAMELS流体动力学模拟中提取重子物理影响的解析近似函数，并分析其不确定性。

Result: 得到的近似函数误差与样本方差相当，且在小尺度上误差增加，但整体表现与数值模拟器相当。

Conclusion: 解析形式的函数可直接解释参数变化的影响，并用于区分不同重子物理模型，代码已公开。

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [224] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 本文探讨了不同数值精度和数据并行化策略对LLM训练速度和模型精度的影响，旨在支持低资源环境下的领域适应。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLM）成本高昂，且易受主流文化和价值观影响。领域适应虽能改善模型与多样文化的对齐，但计算成本仍是障碍，尤其是资源有限的研究团队。

Method: 评估不同数值精度和数据并行化策略对训练速度和模型精度的影响。

Result: 研究结果为关注能源效率、可访问性或硬件限制的场景提供了实用参考。

Conclusion: 通过优化数值精度和并行化策略，可在低资源环境下更高效地实现领域适应。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [225] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: 论文提出了一种基于自然语言推理（NLI）的合规检测方法EXCLAIM，用于解决保证案例中的复杂性和数据不足问题，并通过生成保证案例和引入新指标验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决保证案例中法律和技术文本的复杂性、模型解释需求以及数据不足的挑战。

Method: 利用自然语言推理（NLI）将保证案例的声明-论据-证据结构转化为多跳推理，生成保证案例并引入覆盖率和结构一致性指标。

Result: 通过GDPR要求的案例研究验证了生成保证案例在多跳推理任务中的有效性。

Conclusion: 基于NLI的方法在自动化监管合规过程中具有潜力。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [226] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的狼人杀游戏系统，结合优化的TTS模型，旨在提升用户体验和兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理能力的提升，传统方法（如微调或提示工程）可能变得多余，因此需要更简洁高效的解决方案。

Method: 采用优化的TTS模型与多种LLM结合，简化系统设计。

Result: 系统提升了用户参与度和兼容性，同时减少了对额外组件的依赖。

Conclusion: 随着LLM推理能力的增强，未来类似系统可以进一步简化，无需依赖复杂组件。

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [227] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 该论文提出了一种基于注意力层和多语言数据集的仇恨言论检测方法，显著提升了英语、西班牙语和乌尔都语的检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的仇恨言论威胁在线安全和包容性，但乌尔都语的仇恨言论检测研究较少，尤其是基于翻译的方法。

Method: 使用注意力层增强特征提取，结合GPT-3.5 Turbo和Qwen 2.5 72B等模型，并对比传统机器学习方法（如SVM）。

Result: 在多语言数据集上，模型性能显著提升，英语、西班牙语和乌尔都语的F1分数分别达到0.87、0.85和0.81。

Conclusion: 该框架为多语言仇恨言论检测提供了有效解决方案，有助于构建更安全的数字社区。

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [228] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 论文提出了mSTEB基准，用于评估LLMs在低资源语言上的表现，发现高资源与低资源语言之间存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言缺乏标准化评估基准的问题。

Method: 引入mSTEB基准，覆盖语言识别、文本分类、问答和翻译任务，评估多种LLMs。

Result: 高资源与低资源语言（尤其是非洲和美洲/大洋洲语言）性能差距显著。

Conclusion: 需要更多投资以解决低资源语言在LLMs中的代表性不足问题。

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [229] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: Olica是一种无需重新训练的结构化剪枝框架，通过正交分解和线性校准压缩LLMs，保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs剪枝方法需要大量计算和数据资源重新训练，成本高昂。

Method: 利用PCA处理多头注意力层的矩阵乘积，快速分解降低复杂度；线性校准解决FFN层剪枝误差。

Result: Olica在数据使用、GPU内存和运行时间上高效，性能优于基准测试。

Conclusion: Olica提供了一种高效且无需重新训练的LLMs剪枝解决方案。

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [230] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: 本文提出了RAG中知识冲突的新分类法，并开发了CONFLICTS基准测试，用于评估LLMs在解决冲突时的表现。实验表明，LLMs在冲突解决上仍有不足，但通过提示其显式推理可显著改善。


<details>
  <summary>Details</summary>
Motivation: RAG中检索到的信息可能存在冲突，但目前缺乏对冲突类型及模型应对行为的系统研究。

Method: 提出知识冲突的分类法，构建CONFLICTS基准测试，并通过实验评估LLMs的表现。

Result: LLMs在解决冲突时表现不佳，但显式推理提示能显著提升其响应质量。

Conclusion: 未来研究需进一步改进LLMs在知识冲突中的表现，显式推理是一种有效方法。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [231] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: 论文介绍了CoMuMDR语料库，用于多模态多领域代码混合（印地语和英语）对话的篇章解析，并测试了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有篇章解析数据集局限于单领域英语对话，缺乏多模态和多语言混合的语料库，限制了NLU应用的发展。

Method: 构建了包含音频和文本的代码混合（印地语和英语）语料库CoMuMDR，标注了九种篇章关系，并测试了多种现有模型。

Result: 现有模型在CoMuMDR上表现不佳，突显了多领域代码混合语料库的挑战。

Conclusion: 需要开发更强大的模型以应对多模态、多领域和代码混合的现实场景。

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [232] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 论文研究了文本编码器在细粒度实体或事件识别上的局限性，提出了中文评估数据集CapRetrieval，并通过数据生成策略优化编码器性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器在细粒度语义匹配上表现不佳，导致密集检索失败，需探索改进方法。

Method: 引入CapRetrieval数据集，评估编码器性能，提出数据生成策略进行微调。

Result: 微调后的编码器在CapRetrieval上表现最佳，但发现粒度困境问题。

Conclusion: 研究揭示了文本编码器的细粒度语义匹配挑战，并公开了数据集和模型。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [233] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: TableDreamer是一个渐进式、弱点引导的数据合成框架，用于解决LLM在表格指令调优中数据多样性和效率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM数据合成方法在表格理解任务中无法充分探索输入空间，且忽视目标LLM的弱点，导致数据多样性和效率不足。

Method: 首先生成多样化的种子数据，然后基于新识别的弱点数据迭代探索输入空间，最终用于目标LLM的微调。

Result: 在10个表格基准测试中，TableDreamer将Llama3.1-8B-instruct的平均准确率提升11.62%，优于现有方法。

Conclusion: TableDreamer通过弱点引导的数据合成显著提升了表格指令调优的效果，且数据效率更高。

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [234] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO是一种基于训练策略置信度的偏好学习方法，专注于优化对偏好影响最大的token，无需额外模型或计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DPO）对所有token进行均匀调整，而ConfPO通过针对性优化关键token，提高对齐质量并避免过优化。

Method: ConfPO仅依赖训练策略的置信度识别偏好关键token，并高效利用KL散度预算进行优化。

Result: 在AlpacaEval 2和Arena-Hard等基准测试中，ConfPO表现优于均匀调整方法，且无额外计算开销。

Conclusion: ConfPO是一种简单、轻量且无需模型的方法，显著提升了LLM的对齐效果。

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [235] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: 本文介绍了一种针对核应用领域的特定领域大型语言模型，基于公开的Essential CANDU教材构建，采用紧凑的Transformer架构，在单GPU上训练以保护核操作中的敏感数据。模型在捕获专业核词汇方面表现良好，但生成文本有时缺乏语法连贯性。


<details>
  <summary>Details</summary>
Motivation: 为满足核领域对数据保密性和网络安全的高标准，开发一种专用于核应用的语言模型，避免依赖外部数据源。

Method: 基于Transformer架构，使用Essential CANDU教材作为训练数据，在单GPU上训练，专注于核领域内容。

Result: 模型能够捕获专业核词汇，但生成文本的语法连贯性有待提升。初步验证了内部开发语言模型的可行性。

Conclusion: 未来需扩展数据集、优化预处理和指令微调，以提升模型在核领域的准确性，并评估其在实际应用中的准备度。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [236] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: 提出了一种基于函数调用的大型语言模型（LLM）替代传统自然语言转SQL（NL-to-SQL）的方法，以提高核电站数据查询的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统NL-to-SQL方法在核电站等关键系统中存在风险，如生成的SQL查询难以验证，且复杂数据库结构增加了不准确性。

Method: 通过预定义一组经过审核的专用函数，封装已验证的SQL逻辑，避免直接生成SQL查询，确保查询的准确性和安全性。

Result: 研究表明，基于函数的方法在准确性和可维护性上优于直接NL-to-SQL生成。

Conclusion: 该方法平衡了用户易用性与操作安全性，为关键系统提供了可靠的数据检索框架。

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [237] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 论文揭示了LLM对抗性攻击的几何盲点，提出了ALKALI基准和GRACE防御框架，显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前防御措施无法快速适应LLM对抗性威胁的增长，尤其是对抗性提示利用潜在伪装绕过表面防御。

Method: 提出ALKALI基准评估21种LLM，引入GRACE框架通过几何表示增强防御，并开发AVQI指标量化潜在对齐失败。

Result: GRACE框架将攻击成功率降低39%，AVQI指标揭示了模型内部安全编码的几何特性。

Conclusion: 论文揭示了LLM的潜在伪装漏洞，并提出GRACE和AVQI作为有效的防御和评估工具。

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [238] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND是一种基于超网络的知识传播方法，通过元学习修改梯度以促进知识传播，显著提升了多跳问题的回答能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术无法支持基于注入知识的推理，需要一种方法促进知识传播。

Method: 采用超网络方法，扩展MEND的元目标，修改梯度以支持多跳问题回答。

Result: 在RippleEdit数据集上表现优异，多跳问题准确率提升近2倍；在Controlled RippleEdit数据集上仍优于现有方法，但性能差距减小。

Conclusion: PropMEND在知识传播方面表现突出，但在未见实体关系上的泛化能力有待提升，未来需进一步研究。

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [239] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: 论文展示了如何通过强化学习和内存优化技术，在单块普通游戏GPU（RTX 3080 Ti）上训练出性能优异的数学推理模型，挑战了高性能AI研究需要大规模基础设施的范式。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学推理等任务上表现优异，但其训练通常需要高昂的计算资源。本文旨在探索如何在资源受限的环境中高效训练高性能模型。

Method: 通过结合强化学习和内存优化技术，在单块RTX 3080 Ti（16GB内存）上训练了一个1.5B参数的数学推理模型。

Result: 该模型在数学推理基准测试中表现优于或与更大规模的模型相当，证明了资源受限环境下高性能AI研究的可行性。

Conclusion: 研究结果表明，高性能数学推理模型无需依赖大规模基础设施，为AI研究的普及提供了新思路。

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [240] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文提出了一种新的探测技术，能够从预训练语言模型的嵌入中高精度解码数值，证明模型在预训练后能精确表示数字，并发现这种精确性与算术错误相关。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在算术运算中容易出错，现有方法未能有效探测数值嵌入的结构，因此需要更有效的探测技术。

Method: 提出了一种新型探测技术，能够解码输入嵌入中的数值，并验证其准确性。

Result: 新方法在多种开源语言模型中实现了近乎完美的数值解码精度，证明模型能精确表示数字。

Conclusion: 嵌入的精确性与算术错误相关，通过调整嵌入结构可以缓解这些错误。

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [241] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: Router-R1是一个基于强化学习的框架，通过动态路由和聚合多个LLM来处理复杂任务，优化性能和成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器通常仅进行单轮一对一映射，无法充分利用多个LLM的互补优势。

Method: Router-R1将多LLM路由和聚合建模为序列决策过程，利用LLM的推理能力动态调用模型，并通过轻量级规则奖励引导学习。

Result: 在七个通用和多跳QA基准测试中，Router-R1优于多个基线，表现出卓越的性能和成本管理能力。

Conclusion: Router-R1通过强化学习实现了性能和成本的优化，同时具备强大的泛化能力。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [242] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Main category: astro-ph.IM

TL;DR: 论文研究了扩散薛定谔桥（DSB）模型在天体物理动力学中的应用，提出Astro-DSB模型，提升了预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决巨分子云（GMCs）中恒星形成的观测逆预测问题，探索扩散模型在天体物理中的应用潜力。

Method: 提出Astro-DSB模型，基于成对域假设，通过物理模拟和真实观测数据验证其性能。

Result: Astro-DSB在可解释性、学习效率和预测性能上优于传统方法，且在分布外测试中表现更优。

Conclusion: 研究扩展了扩散模型的应用范围，为未来物理感知生成模型的发展提供了方向。

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>
