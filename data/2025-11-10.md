<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 49]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [An Efficient Proximity Graph-based Approach to Table Union Search](https://arxiv.org/abs/2511.05082)
*Yiming Xie,Hua Dai,Mingfeng Jiang,Pengyue Li,zhengkai Zhang,Bohan Li*

Main category: cs.DB

TL;DR: 提出PGTUS方法解决表格联合搜索中的效率问题，通过多阶段流水线结合新颖的优化策略和过滤策略，在保持召回率的同时实现3.6-6.0倍的加速。


<details>
  <summary>Details</summary>
Motivation: 多向量模型在表格联合搜索中虽然检索质量优越，但由于依赖二分图最大匹配计算联合性分数，面临比单向量模型更严重的效率挑战。

Method: 采用多阶段流水线，结合新颖的优化策略、基于多对一二分匹配的过滤策略，以及增强的剪枝策略来修剪候选集。

Result: 在六个基准数据集上的广泛实验表明，该方法相比现有方法实现了3.6-6.0倍的加速，同时保持了可比的召回率。

Conclusion: PGTUS方法有效解决了表格联合搜索中的效率瓶颈问题，在保持检索质量的同时显著提升了搜索效率。

Abstract: Neural embedding models are extensively employed in the table union search
problem, which aims to find semantically compatible tables that can be merged
with a given query table. In particular, multi-vector models, which represent a
table as a vector set (typically one vector per column), have been demonstrated
to achieve superior retrieval quality by capturing fine-grained semantic
alignments. However, this problem faces more severe efficiency challenges than
the single-vector problem due to the inherent dependency on bipartite graph
maximum matching to compute unionability scores. Therefore, this paper proposes
an efficient Proximity Graph-based Table Union Search (PGTUS) approach. PGTUS
employs a multi-stage pipeline that combines a novel refinement strategy, a
filtering strategy based on many-to-one bipartite matching. Besides, we propose
an enhanced pruning strategy to prune the candidate set, which further improve
the search efficiency. Extensive experiments on six benchmark datasets
demonstrate that our approach achieves 3.6-6.0X speedup over existing
approaches while maintaining comparable recall rates.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette是一个C++17库，旨在解决大型面向对象C++代码库在异构平台（如GPU）上硬件加速的挑战，通过解耦数据布局与接口描述，提供灵活、高效、可移植的数据结构定义。


<details>
  <summary>Details</summary>
Motivation: 适应大型面向对象C++代码库进行硬件加速非常困难，特别是在针对GPU等异构平台时，需要解决数据布局、内存管理和跨设备数据传输等挑战。

Method: 使用C++17编译时抽象，解耦数据布局与接口描述，支持多种内存管理策略，提供高效的数据传输和转换，同时允许接口通过任意函数进行增强。

Result: 通过基于CUDA的案例研究证明，Marionette能够以最小的运行时开销实现高效灵活的数据结构管理，保持与现有代码的兼容性。

Conclusion: Marionette为C++代码库在异构平台上的硬件加速提供了一种有效的解决方案，通过编译时抽象实现了高性能和灵活性，支持从简单到复杂的各种使用场景。

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [3] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: 本文提出了一种基于RISC-V GPU的混合HDC-CNN加速器设计，通过定制GPU指令优化超维计算操作，实现了高达56.2倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络能耗高，而超维计算(HDC)虽然轻量但精度较低。混合HDC-CNN加速器面临泛化性和可编程性差的问题，RISC-V架构为定制GPU设计提供了新机遇。

Method: 设计并实现了四种针对HDC操作优化的定制GPU指令，在RISC-V GPU平台上处理混合HDC-CNN工作负载。

Result: 微基准测试显示性能提升最高达56.2倍，证明了RISC-V GPU在高性能能效计算方面的潜力。

Conclusion: 基于RISC-V的定制GPU指令能够有效加速HDC操作，为混合HDC-CNN工作负载提供高效处理方案。

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


### [4] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: 本文提出了一种结合在线遥测参数和硬件性能计数器来评估GPU工作负载应力的方法，用于预测GPU可靠性问题。


<details>
  <summary>Details</summary>
Motivation: GPU在持续工作负载下会产生显著应力，可能导致组件故障和计算错误，因此需要准确评估工作负载应力以预测可靠性问题。

Method: 结合在线遥测参数和硬件性能计数器，通过测量吞吐量、指令数量和停顿事件来评估GPU资源使用效率。

Result: 实验结果表明，通过结合遥测数据和性能计数器可以准确估计并行工作负载在GPU上产生的应力。

Conclusion: 所提出的方法能够有效评估GPU工作负载应力，为可靠性预测提供了实用工具。

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [5] [Improved Additive Approximation Algorithms for APSP](https://arxiv.org/abs/2511.04775)
*Ce Jin,Yael Kirkpatrick,Michał Stawarz,Virginia Vassilevska Williams*

Main category: cs.DS

TL;DR: 本文改进了无向无权图中全对最短路径(APSP)问题的近似算法，针对+2、+4和+6近似分别达到了O(n^2.2255)、O(n^2.1462)和O(n^2.1026)的时间复杂度，优于现有最佳结果。


<details>
  <summary>Details</summary>
Motivation: 全对最短路径是理论计算机科学中的基础问题，近年来许多工作试图通过快速矩阵乘法等代数工具改进Dor等人[SICOMP'01]的原始算法。本文旨在进一步提升这些近似算法的性能。

Method: 采用一种简单技术将输入图分解为少量常数直径的簇和低度数顶点的剩余部分，仅使用标准快速矩阵乘法获得改进，而不依赖有界差(min,+)乘积算法。

Result: 对于+2近似APSP，将时间复杂度从O(n^2.259)改进到O(n^2.2255)；对于+4和+6近似，分别从O(n^2.155)和O(n^2.103)改进到O(n^2.1462)和O(n^2.1026)。

Conclusion: 本文提出的基于图分解的简单技术能够有效改进APSP近似算法的性能，该方法在最短路径问题研究中可能具有独立价值。

Abstract: The All-Pairs Shortest Paths (APSP) is a foundational problem in theoretical
computer science. Approximating APSP in undirected unweighted graphs has been
studied for many years, beginning with the work of Dor, Halperin and Zwick
[SICOMP'01]. Many recent works have attempted to improve these original
algorithms using the algebraic tools of fast matrix multiplication. We improve
on these results for the following problems.
  For $+2$-approximate APSP, the state-of-the-art algorithm runs in
$O(n^{2.259})$ time [D\"urr, IPL 2023; Deng, Kirkpatrick, Rong, Vassilevska
Williams, and Zhong, ICALP 2022]. We give an improved algorithm in
$O(n^{2.2255})$ time.
  For $+4$ and $+6$-approximate APSP, we achieve time complexities
$O(n^{2.1462})$ and $O(n^{2.1026})$ respectively, improving the previous
$O(n^{2.155})$ and $O(n^{2.103})$ achieved by [Saha and Ye, SODA 2024].
  In contrast to previous works, we do not use the big hammer of
bounded-difference $(\min,+)$-product algorithms. Instead, our algorithms are
based on a simple technique that decomposes the input graph into a small number
of clusters of constant diameter and a remainder of low degree vertices, which
could be of independent interest in the study of shortest paths problems. We
then use only standard fast matrix multiplication to obtain our improvements.

</details>


### [6] [Optimal Parallel Basis Finding in Graphic and Related Matroids](https://arxiv.org/abs/2511.04826)
*Sanjeev Khanna,Aaron Putterman,Junkai Song*

Main category: cs.DS

TL;DR: 该论文解决了在独立性预言访问下寻找图形拟阵基的并行复杂度问题，给出了一个确定性算法，使用O(log m)自适应轮次和每轮poly(m)非自适应查询来找到生成森林，并给出了匹配的下界。


<details>
  <summary>Details</summary>
Motivation: Karp、Upfal和Wigderson在1985年提出了寻找生成森林的问题，他们建立了两种算法，但存在一个关键问题：是否能够同时实现多对数轮次和多项式查询次数。

Method: 提出了一个确定性算法框架，使用O(log m)自适应轮次，每轮执行poly(m)次非自适应查询来寻找生成森林。

Result: 算法在O(log m)轮次内返回生成森林，并证明了这是最优的，因为任何算法（即使是随机化的）使用poly(m)查询每轮都需要Ω(log m)轮次。

Conclusion: 该工作完全刻画了图形拟阵的自适应轮复杂度，解决了这个长期存在的问题，并且框架也适用于满足平滑电路计数性质的二元拟阵。

Abstract: We study the parallel complexity of finding a basis of a graphic matroid
under independence-oracle access. Karp, Upfal, and Wigderson (FOCS 1985, JCSS
1988) initiated the study of this problem and established two algorithms for
finding a spanning forest: one running in $O(\log m)$ rounds with
$m^{\Theta(\log m)}$ queries, and another, for any $d \in \mathbb{Z}^+$,
running in $O(m^{2/d})$ rounds with $\Theta(m^d)$ queries. A key open question
they posed was whether one could simultaneously achieve polylogarithmic rounds
and polynomially many queries. We give a deterministic algorithm that uses
$O(\log m)$ adaptive rounds and $\mathrm{poly}(m)$ non-adaptive queries per
round to return a spanning forest on $m$ edges, and complement this result with
a matching $\Omega(\log m)$ lower bound for any (even randomized) algorithm
with $\mathrm{poly}(m)$ queries per round. Thus, the adaptive round complexity
for graphic matroids is characterized exactly, settling this long-standing
problem. Beyond graphs, we show that our framework also yields an $O(\log
m)$-round, $\mathrm{poly}(m)$-query algorithm for any binary matroid satisfying
a smooth circuit counting property, implying, among others, an optimal $O(\log
m)$-round parallel algorithms for finding bases of cographic matroids.

</details>


### [7] [Tight Bounds for Sampling q-Colorings via Coupling from the Past](https://arxiv.org/abs/2511.04982)
*Tianxing Ding,Hongyang Liu,Yitong Yin,Can Zhou*

Main category: cs.DS

TL;DR: 本文建立了图着色问题中基于边界链的CFTP算法的渐近紧阈值，证明了所有满足标准收缩性质的此类算法需要q≥2.5Δ，并提出了一个达到该最优阈值的有效CFTP算法。


<details>
  <summary>Details</summary>
Motivation: CFTP是完美采样的经典方法，但在图着色问题中，现有基于边界链的CFTP算法在q值要求上仍有改进空间。本文旨在找到此类算法的渐近紧阈值。

Method: 通过设计最优的边界链，构建了一个高效的CFTP算法，证明其在q≥(2.5+o(1))Δ条件下有效。

Result: 证明了基于边界链的CFTP算法需要q≥2.5Δ的必要条件，并实现了达到该最优阈值的算法。

Conclusion: 本文确定了图着色问题中基于边界链的CFTP算法的渐近紧阈值为q≥2.5Δ，为完美采样算法设计提供了理论指导。

Abstract: The Coupling from the Past (CFTP) paradigm is a canonical method for perfect
sampling. For uniform sampling of proper $q$-colorings in graphs with maximum
degree $\Delta$, the bounding chains of Huber (STOC 1998) provide a systematic
framework for efficiently implementing CFTP algorithms within the classical
regime $q \ge (1 + o(1))\Delta^2$. This was subsequently improved to $q >
3\Delta$ by Bhandari and Chakraborty (STOC 2020) and to $q \ge (8/3 +
o(1))\Delta$ by Jain, Sah, and Sawhney (STOC 2021).
  In this work, we establish the asymptotically tight threshold for
bounding-chain-based CFTP algorithms for graph colorings. We prove a lower
bound showing that all such algorithms satisfying the standard contraction
property require $q \ge 2.5\Delta$, and we present an efficient CFTP algorithm
that achieves this asymptotically optimal threshold $q \ge (2.5 + o(1))\Delta$
via an optimal design of bounding chains.

</details>


### [8] [Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations](https://arxiv.org/abs/2511.05295)
*Jon Kleinberg,Fan Wei*

Main category: cs.DS

TL;DR: 本文研究了语言生成极限框架，证明了最佳可实现下界密度为1/2的紧界，并将结果推广到部分枚举设置，同时重新审视了Gold-Angluin语言识别模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的成功激发了语言生成和学习的形式理论研究，需要理解在极限情况下语言生成的可行性及其性能界限。

Method: 采用语言生成极限框架，研究对抗方从未知语言中枚举字符串时算法的生成能力；引入部分枚举模型，分析在仅揭示无限子集情况下的生成性能；重新审视Gold-Angluin语言识别模型。

Result: 证明最佳可实现下界密度为1/2的紧界；在部分枚举设置中，算法输出密度至少为α/2（其中α是揭示子集在语言中的密度）；给出了语言识别在极限下可行的特征化条件。

Conclusion: 语言生成在极限情况下是可行的，存在1/2的密度界限；部分枚举设置下的结果推广了这一界限；Angluin的特征化条件等价于适当拓扑空间具有T_D分离性质。

Abstract: The success of large language models (LLMs) has motivated formal theories of
language generation and learning. We study the framework of \emph{language
generation in the limit}, where an adversary enumerates strings from an unknown
language $K$ drawn from a countable class, and an algorithm must generate
unseen strings from $K$. Prior work showed that generation is always possible,
and that some algorithms achieve positive lower density, revealing a
\emph{validity--breadth} trade-off between correctness and coverage. We resolve
a main open question in this line, proving a tight bound of $1/2$ on the best
achievable lower density. We then strengthen the model to allow \emph{partial
enumeration}, where the adversary reveals only an infinite subset $C \subseteq
K$. We show that generation in the limit remains achievable, and if $C$ has
lower density $\alpha$ in $K$, the algorithm's output achieves density at least
$\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the
partial-information setting, where the generator must recover within a factor
$1/2$ of the revealed subset's density. We further revisit the classical
Gold--Angluin model of \emph{language identification} under partial
enumeration. We characterize when identification in the limit is possible --
when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ -- and in
the process give a new topological formulation of Angluin's characterization,
showing that her condition is precisely equivalent to an appropriate
topological space having the $T_D$ separation property.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Agentic Refactoring: An Empirical Study of AI Coding Agents](https://arxiv.org/abs/2511.04824)
*Kosei Horikawa,Hao Li,Yutaro Kashiwa,Bram Adams,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对AI代理在真实开源Java项目中进行大规模重构的实证研究，分析了15,451个重构实例，发现代理重构主要关注低层次、一致性导向的编辑，与人类重构相比更注重局部改进而非高层次设计变更。


<details>
  <summary>Details</summary>
Motivation: AI代理编码工具正在改变软件工程格局，但缺乏对代理重构在实践中如何应用、与人类重构比较以及其对代码质量影响的实证理解。

Method: 使用AIDev数据集，分析12,256个pull请求和14,988个提交中的15,451个重构实例，进行大规模实证研究。

Result: 重构是这种开发范式中常见且有意向的活动（26.1%的提交明确针对重构），代理重构主要关注低层次编辑（如变量类型更改11.8%、参数重命名10.4%），动机主要是可维护性（52.5%）和可读性（28.1%），在结构指标上产生小而显著的改进。

Conclusion: AI代理重构在实践中是常见且有效的活动，但与人类重构相比更注重局部改进，在代码质量指标上产生积极但有限的改进。

Abstract: Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are
transforming the software engineering landscape. These AI-powered systems
function as autonomous teammates capable of planning and executing complex
development tasks. Agents have become active participants in refactoring, a
cornerstone of sustainable software development aimed at improving internal
code quality without altering observable behavior. Despite their increasing
adoption, there is a critical lack of empirical understanding regarding how
agentic refactoring is utilized in practice, how it compares to human-driven
refactoring, and what impact it has on code quality. To address this empirical
gap, we present a large-scale study of AI agent-generated refactorings in
real-world open-source Java projects, analyzing 15,451 refactoring instances
across 12,256 pull requests and 14,988 commits derived from the AIDev dataset.
Our empirical analysis shows that refactoring is a common and intentional
activity in this development paradigm, with agents explicitly targeting
refactoring in 26.1% of commits. Analysis of refactoring types reveals that
agentic efforts are dominated by low-level, consistency-oriented edits, such as
Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable
(8.5%), reflecting a preference for localized improvements over the high-level
design changes common in human refactoring. Additionally, the motivations
behind agentic refactoring focus overwhelmingly on internal quality concerns,
with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative
evaluation of code quality metrics shows that agentic refactoring yields small
but statistically significant improvements in structural metrics, particularly
for medium-level changes, reducing class size and complexity (e.g., Class LOC
median $\Delta$ = -15.25).

</details>


### [10] [Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach](https://arxiv.org/abs/2511.04849)
*Quang-Dung Nguyen,Tri-Dung Tran,Thanh-Hieu Chu,Hoang-Loc Tran,Xiangwei Cheng,Dirk Slama*

Main category: cs.SE

TL;DR: 使用系统提示和少量样本提示策略，无需训练即可优化大型语言模型在软件定义车辆代码生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆(SDV)的发展需要高效代码生成工具，但专有模型架构的限制阻碍了LLM在特定任务中的应用。

Method: 采用高级提示工程技术设计系统提示结构，通过少量样本提示策略引导LLM生成SDV代码，无需模型训练。

Result: 实验表明，使用少量样本提示策略的模型在定量指标上表现最佳，能更好地调整LLM输出以匹配预期结果。

Conclusion: 提示工程是优化LLM在SDV代码生成任务中的有效方法，无需访问模型架构或进行训练即可显著提升性能。

Abstract: The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in
the automotive industry, where software now plays a pivotal role in defining
vehicle functionality, enabling rapid innovation of modern vehicles. Developing
SDV-specific applications demands advanced tools to streamline code generation
and improve development efficiency. In recent years, general-purpose large
language models (LLMs) have demonstrated transformative potential across
domains. Still, restricted access to proprietary model architectures hinders
their adaption to specific tasks like SDV code generation. In this study, we
propose using prompts, a common and basic strategy to interact with LLMs and
redirect their responses. Using only system prompts with an appropriate and
efficient prompt structure designed using advanced prompt engineering
techniques, LLMs can be crafted without requiring a training session or access
to their base design. This research investigates the extensive experiments on
different models by applying various prompting techniques, including bare
models, using a benchmark specifically created to evaluate LLMs' performance in
generating SDV code. The results reveal that the model with a few-shot
prompting strategy outperforms the others in adjusting the LLM answers to match
the expected outcomes based on quantitative metrics.

</details>


### [11] [What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers](https://arxiv.org/abs/2511.04986)
*Mohammadreza Saeidi,Ethan Thoma,Raula Gaikovina Kula,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 研究调查了npm生态系统中维护者对bug报告的响应情况，发现维护者总体上响应积极(中位数70%)，但某些bug因贡献实践、依赖约束和库特定标准等原因未被解决。


<details>
  <summary>Details</summary>
Motivation: npm等生态系统依赖链中的bug可能传播影响下游，假设维护者可能不总是修复bug，特别是当他们认为超出责任范围时。

Method: 采用混合方法分析30,340个bug报告，挖掘仓库问题数据并进行定性开放编码分析未解决bug报告的原因。

Result: 维护者通常响应积极，项目级响应中位数为70%(IQR:55%-89%)，反映了他们对下游开发者的支持承诺。

Conclusion: 提出了bug未解决原因的分类法，包括贡献实践、依赖约束和库特定标准。理解维护者行为有助于促进更健壮和响应迅速的开源生态系统。

Abstract: Background: Widespread use of third-party libraries makes ecosystems like
Node Package Manager (npm) critical to modern software development. However,
this interconnected chain of dependencies also creates challenges: bugs in one
library can propagate downstream, potentially impacting many other libraries
that rely on it. We hypothesize that maintainers may not always decide to fix a
bug, especially if the maintainer decides it falls out of their responsibility
within the chain of dependencies. Aims: To confirm this hypothesis, we
investigate the responsiveness of 30,340 bug reports across 500 of the most
depended-upon npm packages. Method: We adopt a mixed-method approach to mine
repository issue data and perform qualitative open coding to analyze reasons
behind unaddressed bug reports. Results: Our findings show that maintainers are
generally responsive, with a median project-level responsiveness of 70% (IQR:
55%-89%), reflecting their commitment to support downstream developers.
Conclusions: We present a taxonomy of the reasons some bugs remain unresolved.
The taxonomy includes contribution practices, dependency constraints, and
library-specific standards as reasons for not being responsive. Understanding
maintainer behavior can inform practices that promote a more robust and
responsive open-source ecosystem that benefits the entire community.

</details>


### [12] [Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model](https://arxiv.org/abs/2511.05165)
*Ahmad Hatahet,Christoph Knieke,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出了一种结合逆向工程和大语言模型的半自动化方法，从源代码生成软件架构描述文档，解决传统文档缺失、过时或与实现不符的问题。


<details>
  <summary>Details</summary>
Motivation: 软件架构描述文档在实践中常常缺失、过时或与系统实际实现不一致，导致开发者需要直接从源代码获取架构信息，这个过程耗时且增加认知负担，影响新开发者上手和系统长期维护。

Method: 整合逆向工程技术和LLM，通过提取完整组件图、使用提示工程筛选架构重要元素（核心组件），以及基于代码逻辑生成状态机图来恢复静态和行为架构视图。

Result: 该方法能够生成可扩展且可维护的架构视图表示，显著减少对人工专家参与的依赖，并能准确表示复杂软件行为，特别是在通过少样本提示注入领域知识时。

Conclusion: 该方法为显著减少人工工作量、增强系统理解和长期可维护性提供了可行路径，展示了LLM在软件架构文档生成中的强大潜力。

Abstract: Software Architecture Descriptions (SADs) are essential for managing the
inherent complexity of modern software systems. They enable high-level
architectural reasoning, guide design decisions, and facilitate effective
communication among diverse stakeholders. However, in practice, SADs are often
missing, outdated, or poorly aligned with the system's actual implementation.
Consequently, developers are compelled to derive architectural insights
directly from source code-a time-intensive process that increases cognitive
load, slows new developer onboarding, and contributes to the gradual
degradation of clarity over the system's lifetime. To address these issues, we
propose a semi-automated generation of SADs from source code by integrating
reverse engineering (RE) techniques with a Large Language Model (LLM). Our
approach recovers both static and behavioral architectural views by extracting
a comprehensive component diagram, filtering architecturally significant
elements (core components) via prompt engineering, and generating state machine
diagrams to model component behavior based on underlying code logic with
few-shots prompting. This resulting views representation offer a scalable and
maintainable alternative to traditional manual architectural documentation.
This methodology, demonstrated using C++ examples, highlights the potent
capability of LLMs to: 1) abstract the component diagram, thereby reducing the
reliance on human expert involvement, and 2) accurately represent complex
software behaviors, especially when enriched with domain-specific knowledge
through few-shot prompting. These findings suggest a viable path toward
significantly reducing manual effort while enhancing system understanding and
long-term maintainability.

</details>


### [13] [CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits](https://arxiv.org/abs/2511.05205)
*Huimin Hu,Michael Pradel*

Main category: cs.SE

TL;DR: CodeMapper是一个解决代码映射问题的方法，能够在不同提交之间找到对应代码区域，不依赖特定编程语言或程序元素。


<details>
  <summary>Details</summary>
Motivation: 现有技术如git diff无法专注于开发者选择的特定代码区域，而其他技术又局限于特定编程语言和代码元素，因此需要一种通用的代码映射方法。

Method: CodeMapper采用两阶段方法：1) 通过分析差异、检测代码移动和搜索特定代码片段来计算候选区域；2) 通过计算相似度选择最可能的目标区域。

Result: 在四个数据集上的评估显示，CodeMapper在71.0%-94.5%的情况下正确识别预期目标区域，比最佳基线方法提高了1.5-58.8个百分点。

Conclusion: CodeMapper提供了一种独立于编程语言和程序元素的代码映射解决方案，显著优于现有方法。

Abstract: During software evolution, developers commonly face the problem of mapping a
specific code region from one commit to another. For example, they may want to
determine how the condition of an if-statement, a specific line in a
configuration file, or the definition of a function changes. We call this the
code mapping problem. Existing techniques, such as git diff, address this
problem only insufficiently because they show all changes made to a file
instead of focusing on a code region of the developer's choice. Other
techniques focus on specific code elements and programming languages (e.g.,
methods in Java), limiting their applicability. This paper introduces
CodeMapper, an approach to address the code mapping problem in a way that is
independent of specific program elements and programming languages. Given a
code region in one commit, CodeMapper finds the corresponding region in another
commit. The approach consists of two phases: (i) computing candidate regions by
analyzing diffs, detecting code movements, and searching for specific code
fragments, and (ii) selecting the most likely target region by calculating
similarities. Our evaluation applies CodeMapper to four datasets, including two
new hand-annotated datasets containing code region pairs in ten popular
programming languages. CodeMapper correctly identifies the expected target
region in 71.0%--94.5% of all cases, improving over the best available
baselines by 1.5--58.8 absolute percent points.

</details>


### [14] [Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2511.05297)
*Mohammed Hilel,Yannis Karmim,Jean De Bodinat,Reda Sarehane,Antoine Gillon*

Main category: cs.SE

TL;DR: 提出基于图检索增强生成的框架，自动将企业Web应用转换为状态-动作知识图谱，使LLM能够生成基于实际软件界面的可靠指导


<details>
  <summary>Details</summary>
Motivation: 传统数字采用平台需要大量人工构建维护指南，而直接使用LLM作为虚拟助手容易产生幻觉且不可靠，同时生产级LLM无法微调

Method: 开发图基检索增强生成框架，自动提取和结构化软件界面为状态-动作知识图谱，设计基于图的检索流程

Result: 框架与RAKAM和Lemon Learning合作开发，已集成到生产DAP工作流中，解决了LLM幻觉问题

Conclusion: 该框架能够为企业软件提供基于实际界面的可靠指导，讨论了工业用例中的可扩展性、鲁棒性和部署经验

Abstract: Digital Adoption Platforms (DAPs) have become essential tools for helping
employees navigate complex enterprise software such as CRM, ERP, or HRMS
systems. Companies like LemonLearning have shown how digital guidance can
reduce training costs and accelerate onboarding. However, building and
maintaining these interactive guides still requires extensive manual effort.
Leveraging Large Language Models as virtual assistants is an appealing
alternative, yet without a structured understanding of the target software,
LLMs often hallucinate and produce unreliable answers. Moreover, most
production-grade LLMs are black-box APIs, making fine-tuning impractical due to
the lack of access to model weights. In this work, we introduce a Graph-based
Retrieval-Augmented Generation framework that automatically converts enterprise
web applications into state-action knowledge graphs, enabling LLMs to generate
grounded and context-aware assistance. The framework was co-developed with the
AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the
engineering pipeline that extracts and structures software interfaces, the
design of the graph-based retrieval process, and the integration of our
approach into production DAP workflows. Finally, we discuss scalability,
robustness, and deployment lessons learned from industrial use cases.

</details>


### [15] [Code Review Automation using Retrieval Augmented Generation](https://arxiv.org/abs/2511.05302)
*Qianru Meng,Xiao Zhang,Zhaochen Ren,Joost Visser*

Main category: cs.SE

TL;DR: 提出了RARe方法，结合检索和生成技术，利用RAG框架将外部领域知识融入代码审查过程，显著提升了自动代码审查的质量。


<details>
  <summary>Details</summary>
Motivation: 代码审查对软件质量至关重要但劳动密集，现有自动代码审查方法存在生成评论偏离重点或过于笼统的问题，需要结合检索和生成方法的优势。

Method: 使用密集检索器从代码库中选择最相关的评论，然后利用神经生成器结合LLMs的上下文学习能力生成最终审查意见。

Result: 在两个基准数据集上优于现有最先进方法，分别达到12.32和12.96的BLEU-4分数，并通过人工评估和案例研究验证了其有效性。

Conclusion: RARe方法通过结合检索和生成技术，有效提升了自动代码审查的质量和实用性，展示了RAG框架在代码审查任务中的价值。

Abstract: Code review is essential for maintaining software quality but is
labor-intensive. Automated code review generation offers a promising solution
to this challenge. Both deep learning-based generative techniques and
retrieval-based methods have demonstrated strong performance in this task.
However, despite these advancements, there are still some limitations where
generated reviews can be either off-point or overly general. To address these
issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages
Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative
methods, explicitly incorporating external domain knowledge into the code
review process. RARe uses a dense retriever to select the most relevant reviews
from the codebase, which then enrich the input for a neural generator,
utilizing the contextual learning capacity of large language models (LLMs), to
produce the final review. RARe outperforms state-of-the-art methods on two
benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.
Its effectiveness is further validated through a detailed human evaluation and
a case study using an interpretability tool, demonstrating its practical
utility and reliability.

</details>


### [16] [SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models](https://arxiv.org/abs/2511.05459)
*Jingxuan Xu,Ken Deng,Weihao Li,Songwei Yu,Huaixi Tang,Haoyang Huang,Zhiyi Lai,Zizheng Zhan,Yanan Wu,Chenchen Zhang,Kepeng Lei,Yifan Yao,Xinping Lei,Wenqiang Zhu,Zongxian Feng,Han Li,Junqi Xiong,Dailin Li,Zuchen Gao,Kun Wu,Wen Xiang,Ziqi Zhan,Yuanxing Zhang,Wuxuan Gong,Ziyuan Gao,Guanxiang Wang,Yirong Xue,Xiaojiang Zhang,Jinghui Wang,Huiming Wang,Wenhao Zhuang,Zhaoxiang Zhang,Yuqun Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.SE

TL;DR: SWE-Compass是一个全面的软件工程基准测试，涵盖8种任务类型、8种编程场景和10种编程语言，包含2000个来自真实GitHub拉取请求的高质量实例，用于评估LLM在真实软件开发环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准存在任务覆盖范围窄、语言偏见严重、与真实开发者工作流程不够匹配的问题，主要关注算法问题或Python错误修复，忽略了软件工程的关键维度。

Method: 构建SWE-Compass基准，统一异构代码相关评估到结构化且与生产环境对齐的框架中，通过系统筛选和验证从真实GitHub拉取请求中精选2000个高质量实例，在两种代理框架(SWE-Agent和Claude Code)下评估10个最先进的LLM。

Result: 揭示了不同任务类型、语言和场景之间的难度层次结构，为诊断和改进LLM在代理编码能力方面提供了严格且可复现的基础。

Conclusion: SWE-Compass通过将评估与真实开发者实践对齐，为LLM在软件工程领域的评估提供了更全面、更贴近实际的基准框架。

Abstract: Evaluating large language models (LLMs) for software engineering has been
limited by narrow task coverage, language bias, and insufficient alignment with
real-world developer workflows. Existing benchmarks often focus on algorithmic
problems or Python-centric bug fixing, leaving critical dimensions of software
engineering underexplored. To address these gaps, we introduce SWE-Compass1, a
comprehensive benchmark that unifies heterogeneous code-related evaluations
into a structured and production-aligned framework. SWE-Compass spans 8 task
types, 8 programming scenarios, and 10 programming languages, with 2000
high-quality instances curated from authentic GitHub pull requests and refined
through systematic filtering and validation. We benchmark ten state-of-the-art
LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear
hierarchy of difficulty across task types, languages, and scenarios. Moreover,
by aligning evaluation with real-world developer practices, SWE-Compass
provides a rigorous and reproducible foundation for diagnosing and advancing
agentic coding capabilities in large language models.

</details>


### [17] [A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 本文提出MetaCompress框架，通过蜕变测试系统评估代码语言模型知识蒸馏中的行为保真度，发现传统精度评估无法捕捉师生模型间的深层行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有代码语言模型压缩技术主要依赖精度评估，但无法深入评估学生模型是否真正模仿教师模型的行为和内部表示，存在行为保真度不足的问题。

Method: 提出MetaCompress蜕变测试框架，通过一组行为保持的蜕变关系系统比较师生模型的输出，评估行为保真度。

Result: 实验显示学生模型在对抗攻击下性能下降高达285%，MetaCompress能识别高达62%的行为差异，远超传统精度评估的检测能力。

Conclusion: 知识蒸馏流程中需要行为保真度评估，MetaCompress为测试压缩代码语言模型提供了实用框架。

Abstract: Transformer-based language models of code have achieved state-of-the-art
performance across a wide range of software analytics tasks, but their
practical deployment remains limited due to high computational costs, slow
inference speeds, and significant environmental impact. To address these
challenges, recent research has increasingly explored knowledge distillation as
a method for compressing a large language model of code (the teacher) into a
smaller model (the student) while maintaining performance. However, the degree
to which a student model deeply mimics the predictive behavior and internal
representations of its teacher remains largely unexplored, as current
accuracy-based evaluation provides only a surface-level view of model quality
and often fails to capture more profound discrepancies in behavioral fidelity
between the teacher and student models. To address this gap, we empirically
show that the student model often fails to deeply mimic the teacher model,
resulting in up to 285% greater performance drop under adversarial attacks,
which is not captured by traditional accuracy-based evaluation. Therefore, we
propose MetaCompress, a metamorphic testing framework that systematically
evaluates behavioral fidelity by comparing the outputs of teacher and student
models under a set of behavior-preserving metamorphic relations. We evaluate
MetaCompress on two widely studied tasks, using compressed versions of popular
language models of code, obtained via three different knowledge distillation
techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress
identifies up to 62% behavioral discrepancies in student models, underscoring
the need for behavioral fidelity evaluation within the knowledge distillation
pipeline and establishing MetaCompress as a practical framework for testing
compressed language models of code derived through knowledge distillation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems](https://arxiv.org/abs/2511.05022)
*Charles Melvin,N. Rich Nguyen*

Main category: cs.NI

TL;DR: PriorityFresh是一种语义优先、以可操作性为导向的缓存策略，专为离线紧急预警系统设计，在AWARE系统模拟环境中优化受限连接下的警报保留和推送。


<details>
  <summary>Details</summary>
Motivation: 设计一种在连接受限环境下能够优先保留和推送最具可操作性警报的缓存策略，以提升离线紧急预警系统的效能。

Method: 在AWARE系统模拟环境中实现PriorityFresh策略，该策略基于语义和可操作性优先级来优化警报缓存；同时使用独立的优先级预测模型生成真实警报序列用于实验。

Result: 实验表明PriorityFresh在保持效率的同时，显著提升了以可操作性为导向的性能表现。

Conclusion: PriorityFresh策略能够有效提升离线紧急预警系统的警报可操作性，且不影响系统效率，为受限连接环境下的预警系统提供了有效的缓存解决方案。

Abstract: PriorityFresh is a semantic, actionability-first caching policy designed for
offline emergency warning systems. Within the AWARE system's simulation
environment, PriorityFresh optimizes which alerts to retain and surface under
constrained connectivity. Experiments indicate improved actionability-first
performance without harming efficiency. A separate Priority Forecasting model
is used only to synthesize realistic alert sequences for controlled experiments
and does not influence caching or push decisions.

</details>


### [19] [Cross-link RTS/CTS for MLO mm-Wave WLANs](https://arxiv.org/abs/2511.05027)
*Zhuoling Chen,Yi Zhong,Martin Haenggi*

Main category: cs.NI

TL;DR: 提出了跨链路RTS/CTS机制解决毫米波Wi-Fi中的隐藏终端问题，并建立了广义RTS/CTS硬核过程模型来分析空间收发关系，揭示了链路可靠性与网络吞吐量之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 毫米波Wi-Fi中的定向RTS/CTS机制无法完美解决隐藏终端问题，需要开发更有效的机制来改善链路质量。

Method: 提出跨链路RTS/CTS机制，并引入广义RTS/CTS硬核过程(G-HCP)来建模空间收发关系，推导了强度、平均干扰、成功概率近似和隐藏节点期望数等分析表达式。

Result: 跨链路RTS/CTS机制以降低网络吞吐量为代价确保更高的链路质量，而定向RTS/CTS则牺牲链路质量来获得更高的吞吐量。

Conclusion: 研究揭示了链路可靠性与网络吞吐量之间的基本权衡，为下一代WLAN标准中RTS/CTS机制的选择和优化提供了关键见解。

Abstract: The directional RTS/CTS mechanism of mm-wave Wi-Fi hardly resolves the hidden
terminal problem perfectly.This paper proposes cross-link RTS/CTS under
multi-link operation (MLO) to address this problem and introduces a novel point
process, named the generalized RTS/CTS hard-core process (G-HCP), to model the
spatial transceiver relationships under the RTS/CTS mechanism, including the
directional case and the omnidirectional case.Analytical expressions are
derived for the intensity, the mean interference, an approximation of the
success probability, and the expected number of hidden nodes for the
directional RTS/CTS mechanism.Theoretical and numerical results demonstrate the
performance difference between two RTS/CTS mechanisms.The cross-link RTS/CTS
mechanism ensures higher link quality at the cost of reduced network
throughput.In contrast, the directional RTS/CTS sacrifices the link quality for
higher throughput.Our study reveals a fundamental trade-off between link
reliability and network throughput, providing critical insights into the
selection and optimization of RTS/CTS mechanisms in next-generation WLAN
standards.

</details>


### [20] [Improving Injection-Throttling Mechanisms for Congestion Control for Data-center and Supercomputer Interconnects](https://arxiv.org/abs/2511.05149)
*Cristina Olmedilla,Jesus Escudero-Sahuquillo,Pedro J. Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.NI

TL;DR: 本文重新审视了DCQCN拥塞控制机制，改进了其闭环设计，通过更准确的拥塞检测、信令和注入节流来减少控制流量开销，避免对非拥塞流的不必要节流。


<details>
  <summary>Details</summary>
Motivation: 随着超级计算机和数据中心的高速发展，互连网络成为关键组件。DCQCN作为流行的拥塞控制技术，其核心的拥塞检测、通知和反应机制在现有高性能互连网络中已显不足，需要改进以适应现代网络拥塞动态。

Method: 重新设计DCQCN的闭环机制，改进拥塞检测精度，优化信令机制，并完善注入节流策略。

Result: 提出的改进机制减少了控制流量开销，避免了非拥塞流的不必要节流，提升了网络性能。

Conclusion: 通过对DCQCN机制的重新设计和优化，能够更好地应对现代高性能互连网络中的拥塞问题，提升整体系统性能。

Abstract: Over the past decade, Supercomputers and Data centers have evolved
dramatically to cope with the increasing performance requirements of
applications and services, such as scientific computing, generative AI, social
networks or cloud services. This evolution have led these systems to
incorporate high-speed networks using faster links, end nodes using multiple
and dedicated accelerators, or a advancements in memory technologies to bridge
the memory bottleneck. The interconnection network is a key element in these
systems and it must be thoroughly designed so it is not the bottleneck of the
entire system, bearing in mind the countless communication operations that
generate current applications and services. Congestion is serious threat that
spoils the interconnection network performance, and its effects are even more
dramatic when looking at the traffic dynamics and bottlenecks generated by the
communication operations mentioned above. In this vein, numerous congestion
control (CC) techniques have been developed to address congestion negative
effects. One popular example is Data Center Quantized Congestion Notification
(DCQCN), which allows congestion detection at network switch buffers, then
marking congesting packets and notifying about congestion to the sources, which
finally apply injection throttling of those packets contributing to congestion.
While DCQCN has been widely studied and improved, its main principles for
congestion detection, notification and reaction remain largely unchanged, which
is an important shortcoming considering congestion dynamics in current
high-performance interconnection networks. In this paper, we revisit the DCQCN
closed-loop mechanism and refine its design to leverage a more accurate
congestion detection, signaling, and injection throttling, reducing control
traffic overhead and avoiding unnecessary throttling of non-congesting flows.

</details>


### [21] [EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environme](https://arxiv.org/abs/2511.05238)
*Peide Li,Liu Cao,Lyutianyang Zhang,Dongyu Wei,Ye Hu,Qipeng Xie*

Main category: cs.NI

TL;DR: EPFL-REMNet是一种高效的个性化联邦学习框架，用于构建6G异构无线电环境的高保真数字孪生，通过共享主干+轻量个性化头的模型设计，在非IID数据条件下显著提升了精度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决标准联邦学习在6G异构无线电环境地图构建中，面对非独立同分布数据时出现的性能下降和通信效率低下的问题。

Method: 采用"共享主干+轻量个性化头"模型架构，仅传输压缩的共享主干，客户端本地维护个性化头，在90个客户端的地理分区数据上构建三种不同复杂度的非IID场景。

Result: 在所有非IID设置下，相比标准FedAvg和最新方法，EPFL-REMNet同时实现了更高的数字孪生保真度和更低的上行开销，显著减少了数据集间的性能差异，提升了长尾客户端的本地地图精度。

Conclusion: EPFL-REMNet框架有效提升了6G异构无线电环境数字孪生的整体完整性，在非IID条件下实现了精度和通信效率的双重优化。

Abstract: Radio Environment Map (REM) is transitioning from 5G homogeneous environments
to B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL),
a natural fit for this distributed task, struggles with performance degradation
in accuracy and communication efficiency under the non-independent and
identically distributed (Non-IID) data conditions inherent to these new
environments. This paper proposes EPFL-REMNet, an efficient personalized
federated framework for constructing a high-fidelity digital twin of the 6G
heterogeneous radio environment. The proposed EPFL-REMNet employs a"shared
backbone + lightweight personalized head" model, where only the compressed
shared backbone is transmitted between the server and clients, while each
client's personalized head is maintained locally. We tested EPFL-REMNet by
constructing three distinct Non-IID scenarios (light, medium, and heavy) based
on radio environment complexity, with data geographically partitioned across 90
clients. Experimental results demonstrate that EPFL-REMNet simultaneously
achieves higher digital twin fidelity (accuracy) and lower uplink overhead
across all Non-IID settings compared to standard FedAvg and recent
state-of-the-art methods. Particularly, it significantly reduces performance
disparities across datasets and improves local map accuracy for long-tail
clients, enhancing the overall integrity of digital twin.

</details>


### [22] [A Formal Model for Path Set Attribute Calculation in Network Systems](https://arxiv.org/abs/2511.05334)
*Giovanni Fiaschi,Carlo Vitucci,Thomas Westerbäck,Daniel Sundmark,Thomas Nolte*

Main category: cs.NI

TL;DR: 本文提出了一种数学方法来评估路径集合，强调路径特性分析的重要性，特别是在多路径场景下。


<details>
  <summary>Details</summary>
Motivation: 在图论和网络应用中，路径选择需要对备选方案进行评分。虽然已有研究对单一路径进行了全面评估，但对路径集合的分析缺乏同等深度。路径特性强烈依赖于所考虑的性质，这在多路径分析中尤为关键。

Method: 提出了一种数学方法，定义了一个功能模型，该模型能够很好地表征一般形式下的路径集合。该模型可以将特定属性置于上下文中进行分析。

Result: 展示了功能模型如何将特定属性置于上下文中进行表征，为路径集合分析提供了数学基础。

Conclusion: 该方法为路径集合的特性分析提供了理论基础，强调了在多路径场景中基于属性进行路径表征的重要性。

Abstract: In graph theory and its practical networking applications, e.g.,
telecommunications and transportation, the problem of finding paths has
particular importance. Selecting paths requires giving scores to the
alternative solutions to drive a choice. While previous studies have provided
comprehensive evaluation of single-path solutions, the same level of detail is
lacking when considering sets of paths. This paper emphasizes that the path
characterization strongly depends on the properties under consideration. While
property-based characterization is also valid for single paths, it becomes
crucial to analyse multiple path sets. From the above consideration, this paper
proposes a mathematical approach, defining a functional model that lends itself
well to characterizing the path set in its general formulation. The paper shows
how the functional model contextualizes specific attributes.

</details>


### [23] [To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger](https://arxiv.org/abs/2511.05362)
*Lucian Trestioreanu,Flaviene Scheidt,Wazen Shbair,Jerome Francois,Damien Magoni,Radu State*

Main category: cs.NI

TL;DR: 评估XRP账本网络中Squelching机制在真实环境下的效果，并与基于命名数据网络和gossip方法的替代方案进行比较


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术采用率大幅增长，底层P2P网络需要相应扩展。现有研究主要关注比特币、以太坊等主流PoW区块链，而共识验证型区块链（如XRP账本）的通信效率问题尚未充分研究

Method: 使用真实可控测试床和XRPL生产网络，定量评估Squelching机制的性能，并与基于命名数据网络和gossip方法的替代方案进行对比

Result: 未在摘要中明确说明具体结果

Conclusion: 通过真实环境测试评估Squelching机制的有效性，为XRP账本网络的可扩展性提供实证依据

Abstract: With the large increase in the adoption of blockchain technologies, their
underlying peer-to-peer networks must also scale with the demand. In this
context, previous works highlighted the importance of ensuring efficient and
resilient communication for the underlying consensus and replication
mechanisms. However, they were mainly focused on mainstream,
Proof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.
  In this paper, the problem is investigated in the context of
consensus-validation based blockchains, like the XRP Ledger. The latter relies
on a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to
have a good scalability in regards to transaction throughput. However, it is
known that significant increases in the size of the XRP Ledger network would be
challenging to achieve. The main reason is the flooding mechanism used to
disseminate the messages related to the consensus protocol, which creates many
duplicates in the network. Squelching is a recent solution proposed for
limiting this duplication, however, it was never evaluated quantitatively in
real-life scenarios involving the XRPL production network. In this paper, our
aim is to assess this mechanism using a real-life controllable testbed and the
XRPL production network, to assess its benefit and compare it to alternative
solutions relying on Named Data Networking and on a gossip-based approach.

</details>


### [24] [Scanning the IPv6 Internet Using Subnet-Router Anycast Probing](https://arxiv.org/abs/2511.05423)
*Maynard Koch,Raphael Hiesgen,Marcin Nawrocki,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 本文提出使用主动子网路由器任播（SRA）探测技术来探索IPv6地址空间，相比随机探测能发现更多路由器IP地址，且受ICMP速率限制影响更小。


<details>
  <summary>Details</summary>
Motivation: IPv6地址空间巨大，识别活跃IPv6地址具有挑战性。现有方法包括命中列表、新探测技术和AI生成目标列表，但SRA探测方法尚未充分利用。

Method: 应用主动子网路由器任播（SRA）探测技术，并与先前方法获得的活跃IPv6节点列表以及随机探测进行比较。

Result: SRA探测平均比随机探测多发现10%的路由器IP地址，且受ICMP速率限制影响更小。相比直接定位路由器地址，SRA探测发现80%更多的地址。

Conclusion: SRA探测是IPv6测量工具箱的重要补充，能显著提高结果稳定性。同时发现某些主动扫描可能对当前IPv6部署造成有害条件，已开始与网络运营商合作修复。

Abstract: Identifying active IPv6 addresses is challenging. Various methods emerged to
master the measurement challenge in this huge address space, including
hitlists, new probing techniques, and AI-generated target lists. In this paper,
we apply active Subnet-Router anycast (SRA) probing, a commonly unused method
to explore the IPv6 address space. We compare our results with lists of active
IPv6 nodes obtained from prior methods and with random probing. Our findings
indicate that probing an SRA address reveals on average 10% more router IP
addresses than random probing and is far less affected by ICMP rate limiting.
Compared to targeting router addresses directly, SRA probing discovers 80% more
addresses. We conclude that SRA probing is an important addition to the IPv6
measurement toolbox and may improve the stability of results significantly. We
also find evidence that some active scans can cause harmful conditions in
current IPv6 deployments, which we started to fix in collaboration with network
operators.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV缓存管理策略在接近或超过模型训练上下文窗口时会严重降低LLM生成质量，位置编码完整性的破坏是主要问题，保持连续上下文块的简单策略优于复杂的位置破坏性策略。


<details>
  <summary>Details</summary>
Motivation: 研究KV缓存在状态化多轮场景中无限增长带来的挑战，特别是位置编码完整性对LLM生成质量的影响。

Method: 使用状态化基准测试框架进行实证分析，比较不同KV缓存驱逐策略，包括高保留率策略和保持连续上下文块的策略。

Result: 当累积KV缓存接近或超过模型训练上下文窗口时，LLM生成质量急剧下降；保持位置连续性的简单策略比复杂策略产生更连贯的生成结果。

Conclusion: KV缓存驱逐技术应尊重架构限制、保持位置结构，将"缓存健康"视为超越单纯大小的整体概念。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [26] [Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2511.04718)
*Yue Xun,Jiaxing Xu,Wenbo Gao,Chen Yang,Shujun Wang*

Main category: cs.LG

TL;DR: 提出了一种新的自适应频率连接网络框架，通过自适应级联分解学习任务相关的频率子带，并构建统一的功能连接网络来捕捉跨频段交互，在脑疾病诊断中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有静息态fMRI模型大多忽略神经元振荡的多频特性，将BOLD信号视为单一时间序列处理，这限制了诊断敏感性和特异性。神经疾病通常在特定频段表现出异常，但现有方法依赖预定义频带，无法捕捉个体差异和疾病特异性改变。

Method: 提出自适应级联分解学习每个脑区的任务相关频率子带，频率耦合连接学习捕捉频段内和跨频段的精细交互，构建统一功能网络。该网络通过统一GCN中的新型消息传递机制生成精炼节点表示用于诊断预测。

Result: 在ADNI和ABIDE数据集上的实验结果表明，该方法优于现有方法。

Conclusion: 该框架能够有效学习任务相关的频率子带并捕捉跨频段交互，为脑疾病诊断提供了更敏感和特异的方法。

Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders
and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els
largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial
fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they
often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade
Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism
within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.

</details>


### [27] [AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2511.04722)
*Qianyang Li,Xingjun Zhang,Peng Tao,Shaoxun Wang,Yancheng Pan,Jia Wei*

Main category: cs.LG

TL;DR: AWEMixer是一种自适应小波增强混合网络，通过频率路由器和相干门控融合块，在时频域实现精确的时序预测，在IoT环境中有效解决长期时间序列预测问题。


<details>
  <summary>Details</summary>
Motivation: IoT环境中传感器信号的非平稳性和多尺度特性使长期时间序列预测面临挑战，传统方法局限于时域操作，而傅里叶变换获得的全局频率信息被视为平稳信号，会模糊瞬态事件的时间模式。

Method: 提出AWEMixer网络，包含两个创新组件：1）频率路由器利用快速傅里叶变换获得的全局周期性模式自适应加权局部小波子带；2）相干门控融合块通过交叉注意力和门控机制实现突出频率特征与多尺度时间表示的选择性集成。

Result: 在七个公共基准测试中验证了模型的有效性，相比基于Transformer和MLP的最新模型，在长序列时间序列预测中持续实现性能提升。

Conclusion: AWEMixer模型能够实现精确的时频定位，同时对噪声保持鲁棒性，在IoT环境中的长期时间序列预测任务中表现出色。

Abstract: Forecasting long-term time series in IoT environments remains a significant
challenge due to the non-stationary and multi-scale characteristics of sensor
signals. Furthermore, error accumulation causes a decrease in forecast quality
when predicting further into the future. Traditional methods are restricted to
operate in time-domain, while the global frequency information achieved by
Fourier transform would be regarded as stationary signals leading to blur the
temporal patterns of transient events. We propose AWEMixer, an Adaptive
Wavelet-Enhanced Mixer Network including two innovative components: 1) a
Frequency Router designs to utilize the global periodicity pattern achieved by
Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a
Coherent Gated Fusion Block to achieve selective integration of prominent
frequency features with multi-scale temporal representation through
cross-attention and gating mechanism, which realizes accurate time-frequency
localization while remaining robust to noise. Seven public benchmarks validate
that our model is more effective than recent state-of-the-art models.
Specifically, our model consistently achieves performance improvement compared
with transformer-based and MLP-based state-of-the-art models in long-sequence
time series forecasting. Code is available at
https://github.com/hit636/AWEMixer

</details>


### [28] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: 提出了一种结合TCN和增强型TFT的新框架，用于工业系统的剩余使用寿命预测，通过多时间窗口方法提高适应性，在基准数据集上平均RMSE降低5.5%。


<details>
  <summary>Details</summary>
Motivation: 现有RUL预测模型难以捕捉细粒度时间依赖关系，且无法动态优先处理关键特征，需要更稳健的预测方法。

Method: 集成时间卷积网络(TCN)进行局部时间特征提取，结合由Bi-LSTM编码器-解码器增强的改进型时间融合变换器(TFT)，采用多时间窗口方法。

Result: 在基准数据集上的广泛评估表明，该模型平均RMSE降低高达5.5%，相比现有最优方法具有更高的预测精度。

Conclusion: 该框架弥补了当前方法的不足，提升了工业预测系统的有效性，凸显了先进时间序列变换器在RUL预测中的潜力。

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [29] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: 本文提出了一种传感器引导的GLISp扩展方法，通过整合可测量的传感器数据到偏好学习循环中，结合主观反馈与定量传感器信息，提高了校准效率和最终解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好的优化方法（如GLISp）将系统视为黑盒，忽略了信息丰富的传感器测量数据，这限制了校准效率。

Method: 引入传感器引导的正则化GLISp扩展，通过物理信息假设函数和最小二乘正则化项将可测量描述符整合到偏好学习循环中。

Result: 在分析基准和人在环车辆悬架调谐任务上的数值评估显示，相比基线GLISp，该方法具有更快的收敛速度和更优的最终解决方案。

Conclusion: 所提出的传感器引导方法成功地将灰盒结构注入偏好学习，结合了主观反馈与定量传感器信息，同时保持了基于偏好的搜索的灵活性。

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [30] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: 本文研究了在数据稀缺和分布偏移情况下，知识蒸馏如何诱导和加速模型的grokking现象，以及在联合分布和持续预训练场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究在数据稀缺（训练样本低于临界阈值）和分布偏移的实际场景下，如何通过知识蒸馏来诱导和加速模型的grokking现象，解决部署模型在新分布下数据有限时的适应问题。

Method: 使用知识蒸馏方法，从已在分布p1上grokked的模型蒸馏到新分布p2；研究在联合分布(p1,p2)上的训练；以及在持续预训练设置中从p1到p2的过渡。

Result: 知识蒸馏可以在数据低于临界阈值时诱导和加速grokking；在联合分布训练中，从个体分布grokked模型蒸馏能够实现泛化；在持续预训练中，知识蒸馏加速泛化并减轻灾难性遗忘，仅需10%数据即可获得强性能。

Conclusion: 知识蒸馏在低数据和分布演化设置中对于实现泛化具有核心作用，为知识转移下的grokking机制提供了新见解。

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [31] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow是一个将PyTorch稀疏模型转换为融合稀疏数据流图的编译器，支持跨表达式融合、并行化等优化，针对可重构数据流架构，通过设计空间探索发现全融合并非总是最优策略。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模扩大，稀疏计算和专用数据流硬件成为提高效率的重要方案，但缺乏支持通用跨表达式稀疏操作融合的编译器。

Method: 开发FuseFlow编译器，支持跨内核融合、并行化、数据流排序和稀疏分块等优化，使用周期精确的数据流模拟器进行微架构分析。

Result: 在四个真实世界机器学习应用中，发现全融合并非总是最优，融合粒度取决于模型本身；在GPT-3 BigBird块稀疏注意力上实现约2.7倍加速。

Conclusion: FuseFlow为稀疏模型提供了有效的融合策略探索工具，揭示了融合粒度的重要性，并提供了识别次优配置的启发式方法。

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [32] [SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices](https://arxiv.org/abs/2511.04774)
*Liu Jiang,Zerui Bao,Shiqi Sheng,Di Zhu*

Main category: cs.LG

TL;DR: 提出了一种针对云工作负载的指令预取设计，通过压缩条目、分层元数据存储和在线ML控制器来减少片上状态并提高效率


<details>
  <summary>Details</summary>
Motivation: 大规模网络服务依赖深度软件栈和微服务编排，这会增加指令占用空间并造成前端停顿，从而推高尾延迟和能耗

Method: 基于纠缠指令预取器(EIP)，引入压缩条目(36位捕获8个目标)、分层元数据存储(片上仅保留L1常驻和频繁查询条目)，以及轻量级在线ML控制器(使用上下文特征和bandit调整阈值评估预取收益)

Result: 在数据中心应用中，该方法在保持EIP类似加速效果的同时减少了片上状态，提高了网络服务在ML时代的效率

Conclusion: 该设计为云工作负载提供了一种高效的指令预取解决方案，特别适合SLO驱动和自优化系统

Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice
orchestration, which increase instruction footprints and create frontend stalls
that inflate tail latency and energy. We revisit instruction prefetching for
these cloud workloads and present a design that aligns with SLO driven and self
optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we
introduce a Compressed Entry that captures up to eight destinations around a
base using 36 bits by exploiting spatial clustering, and a Hierarchical
Metadata Storage scheme that keeps only L1 resident and frequently queried
entries on chip while virtualizing bulk metadata into lower levels. We further
add a lightweight Online ML Controller that scores prefetch profitability using
context features and a bandit adjusted threshold. On data center applications,
our approach preserves EIP like speedups with smaller on chip state and
improves efficiency for networked services in the ML era.

</details>


### [33] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: 提出CNODE框架，使用神经ODE建模帕金森病脑形态变化的连续时间过程，通过患者特异性初始时间和进展速度实现个体化轨迹对齐，在PPMI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病具有异质性和演变的脑形态模式，现有方法难以处理不规则稀疏的MRI数据，且难以捕捉个体异质性（如发病时间、进展速度和症状严重程度的差异）。

Method: CNODE框架使用神经ODE模型将脑形态变化建模为连续时间过程，联合学习患者特异性初始时间和进展速度，将个体轨迹对齐到共享进展轨迹中。

Result: 在PPMI数据集上的实验结果显示，该方法在预测纵向帕金森病进展方面优于最先进的基线方法。

Conclusion: CNODE能够有效建模帕金森病的连续个体化进展轨迹，解决了现有方法在处理不规则数据和个体异质性方面的局限性。

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [34] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: 该论文提出将表示学习与因果推断相结合，利用多模态数据（观察性和扰动性数据）进行因果发现和表示学习，以解决生物医学中的基本问题。


<details>
  <summary>Details</summary>
Motivation: 表示学习在预测任务中很成功，但在因果任务（如预测干预效果）中表现不佳，因此需要将表示学习与因果推断相结合。多模态数据的可用性为这一结合提供了机会。

Method: 提出了一个统计和计算框架，用于因果结构和表示学习，包括如何利用观察性和扰动性数据进行因果发现、如何使用多模态视图学习因果变量，以及如何设计最优扰动。

Result: 论文概述了一个框架，但未提供具体的实验结果。

Conclusion: 表示学习与因果推断的结合是解决生物医学中因果问题的有前景的方向，多模态数据为这一结合提供了重要机会。

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [35] [No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models](https://arxiv.org/abs/2511.05179)
*Ragini Gupta,Naman Raina,Bo Chen,Li Chen,Claudiu Danilov,Josh Eckhardt,Keyshla Bernard,Klara Nahrstedt*

Main category: cs.LG

TL;DR: 本研究系统评估了不同预测模型在时空数据中的表现，发现STGNN在稀疏传感器部署和中等采样率下表现最佳，而TSFM在高频采样时表现优异但受限于空间覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有边缘数据过滤和部署技术忽略了采样频率和空间覆盖变化对下游模型性能的影响，需要系统研究不同模型架构在时空数据中的表现差异。

Method: 使用真实世界无线传感器网络温度数据，系统研究经典模型(VAR)、神经网络(GRU、Transformer)、时空图神经网络(STGNNs)和时间序列基础模型(TSFMs)在不同空间传感器节点密度和采样间隔下的表现。

Result: STGNN在传感器部署稀疏且采样率中等时最有效，通过编码图结构利用空间相关性补偿有限覆盖；TSFM在高频时表现有竞争力但在空间覆盖减少时性能下降；多变量TSFM Moirai通过学习跨传感器依赖关系优于所有模型。

Conclusion: 研究结果为构建高效的时空系统预测管道提供了可行见解，所有代码和数据集已开源以确保可复现性。

Abstract: Modern IoT deployments for environmental sensing produce high volume
spatiotemporal data to support downstream tasks such as forecasting, typically
powered by machine learning models. While existing filtering and strategic
deployment techniques optimize collected data volume at the edge, they overlook
how variations in sampling frequencies and spatial coverage affect downstream
model performance. In many forecasting models, incorporating data from
additional sensors denoise predictions by providing broader spatial contexts.
This interplay between sampling frequency, spatial coverage and different
forecasting model architectures remain underexplored. This work presents a
systematic study of forecasting models - classical models (VAR), neural
networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),
and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under
varying spatial sensor nodes density and sampling intervals using real-world
temperature data in a wireless sensor network. Our results show that STGNNs are
effective when sensor deployments are sparse and sampling rate is moderate,
leveraging spatial correlations via encoded graph structure to compensate for
limited coverage. In contrast, TSFMs perform competitively at high frequencies
but degrade when spatial coverage from neighboring sensors is reduced.
Crucially, the multivariate TSFM Moirai outperforms all models by natively
learning cross-sensor dependencies. These findings offer actionable insights
for building efficient forecasting pipelines in spatio-temporal systems. All
code for model configurations, training, dataset, and logs are open-sourced for
reproducibility:
https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models

</details>


### [36] [DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing](https://arxiv.org/abs/2511.04791)
*Lei Gao,Chaoyi Jiang,Hossein Entezari Zarch,Daniel Wong,Murali Annavaram*

Main category: cs.LG

TL;DR: DuetServe是一个统一的LLM服务框架，通过在单个GPU内实现解耦级别的隔离，动态激活SM级GPU空间复用，在预测到TBT降级时解耦预填充和解码执行，从而提高吞吐量同时保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统要么在共享GPU上聚合预填充和解码阶段导致干扰和TBT降级，要么在GPU间解耦两个阶段但造成资源浪费和KV缓存传输开销。需要一种能在单个GPU内提供解耦级别隔离的解决方案。

Method: DuetServe默认在聚合模式下运行，当预测到TBT降级时动态激活SM级GPU空间复用。核心思想是通过细粒度的自适应SM分区仅在需要时解耦预填充和解码执行。包含注意力感知屋顶模型预测迭代延迟、分区优化器选择最优SM分割、无中断执行引擎消除CPU-GPU同步开销。

Result: 评估显示DuetServe相比最先进框架将总吞吐量提高了1.3倍，同时保持低生成延迟。

Conclusion: DuetServe通过动态SM分区在单个GPU内实现了预填充和解码阶段的解耦级别隔离，在提高吞吐量的同时满足了延迟SLO要求，解决了现有方法的干扰和资源浪费问题。

Abstract: Modern LLM serving systems must sustain high throughput while meeting strict
latency SLOs across two distinct inference phases: compute-intensive prefill
and memory-bound decode phases. Existing approaches either (1) aggregate both
phases on shared GPUs, leading to interference between prefill and decode
phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two
phases across GPUs, improving latency but wasting resources through duplicated
models and KV cache transfers. We present DuetServe, a unified LLM serving
framework that achieves disaggregation-level isolation within a single GPU.
DuetServe operates in aggregated mode by default and dynamically activates
SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key
idea is to decouple prefill and decode execution only when needed through
fine-grained, adaptive SM partitioning that provides phase isolation only when
contention threatens latency service level objectives (SLOs). DuetServe
integrates (1) an attention-aware roofline model to forecast iteration latency,
(2) a partitioning optimizer that selects the optimal SM split to maximize
throughput under TBT constraints, and (3) an interruption-free execution engine
that eliminates CPU-GPU synchronization overhead. Evaluations show that
DuetServe improves total throughput by up to 1.3x while maintaining low
generation latency compared to state-of-the-art frameworks.

</details>


### [37] [Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator](https://arxiv.org/abs/2511.04804)
*Chaymae Yahyati,Ismail Lamaakal,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SiFEN是一种基于有限元方法的神经网络，使用分片多项式在学习的单纯形网格上表示函数，具有显式局部性、可控平滑度和缓存友好的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 提出一种紧凑、可解释且理论基础的替代方案，以替代密集MLP和边缘样条网络，提供更好的校准和推理延迟。

Method: 使用度数为m的Bernstein-Bezier多项式与轻量可逆扭曲配对，通过形状正则化、半离散OT覆盖和可微分边翻转进行端到端训练。

Result: 在合成逼近任务、表格回归/分类以及作为紧凑CNN的头部时，SiFEN在匹配参数预算下表现优于或等同于MLP和KAN，改善了校准并降低了推理延迟。

Conclusion: SiFEN是一种紧凑、可解释且理论基础的神经网络架构，特别适合需要局部性和可控平滑度的应用场景。

Abstract: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial
predictor that represents f: R^d -> R^k as a globally C^r finite-element field
on a learned simplicial mesh in an optionally warped input space. Each query
activates exactly one simplex and at most d+1 basis functions via barycentric
coordinates, yielding explicit locality, controllable smoothness, and
cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with
a light invertible warp and trains end-to-end with shape regularization,
semi-discrete OT coverage, and differentiable edge flips. Under standard
shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic
FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic
approximation tasks, tabular regression/classification, and as a drop-in head
on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter
budgets, improves calibration (lower ECE/Brier), and reduces inference latency
due to geometric locality. These properties make SiFEN a compact,
interpretable, and theoretically grounded alternative to dense MLPs and
edge-spline networks.

</details>


### [38] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: PuzzleMoE是一种无需训练的MoE模型压缩方法，通过稀疏专家合并和位压缩编码实现高效推理，在50%压缩率下保持准确度并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型由于存储所有专家参数导致的高内存开销问题，特别是在专家数量增加时，现有方法在高压缩率下存在性能下降。

Method: 1. 稀疏专家合并：识别权重冗余和专业化，使用双掩码捕获共享和专家特定参数
2. 位压缩编码：重用未充分利用的指数位，避免存储二进制掩码和符号的开销

Result: 压缩MoE模型达50%同时保持各任务准确度，在MMLU上比现有方法提升16.7%，推理速度提升1.28倍

Conclusion: PuzzleMoE提供了一种高效且准确的MoE压缩方案，解决了高内存开销问题，实现了压缩与性能的良好平衡。

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [39] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 本文分析了自编码器在数据流形上的拓扑限制和能力，并描述了在动力系统中使用自编码器的方法。


<details>
  <summary>Details</summary>
Motivation: 研究自编码器在数据流形上的拓扑特性，探索其在表示学习和动力系统建模中的理论基础。

Method: 通过拓扑学方法分析自编码器的编码器-解码器对在数据流形上的行为，特别关注round-trip映射与恒等映射的接近程度。

Result: 揭示了自编码器在数据流形表示中的固有拓扑限制和可能性，并展示了其在动力系统不变流形建模中的应用能力。

Conclusion: 自编码器的拓扑特性对其表示能力有重要影响，理解这些特性有助于设计更有效的自编码器架构，特别是在动力系统建模中。

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [40] [Sharp Minima Can Generalize: A Loss Landscape Perspective On Data](https://arxiv.org/abs/2511.04808)
*Raymond Fan,Bryce Sandlund,Lin Myat Ko*

Main category: cs.LG

TL;DR: 本文挑战了深度学习中的体积假设，发现增加训练数据会改变损失景观，使原本体积小但泛化好的极小值变得相对更大，从而更容易被找到。


<details>
  <summary>Details</summary>
Motivation: 体积假设认为深度学习有效是因为平坦极小值体积大易被找到，但这无法解释大数据集在泛化中的作用。本文旨在探究数据量如何影响极小值体积与泛化能力的关系。

Method: 通过在不同训练数据量下测量极小值的体积，分析损失景观的变化，研究极小值体积与泛化性能之间的关系。

Result: 发现存在体积小但泛化好的尖锐极小值，但由于体积小难以被找到。增加数据会使这些泛化好的极小值相对体积变大。

Conclusion: 大数据集的作用不仅是提供更多训练样本，更重要的是改变损失景观，使泛化好的极小值变得更容易被优化算法找到。

Abstract: The volume hypothesis suggests deep learning is effective because it is
likely to find flat minima due to their large volumes, and flat minima
generalize well. This picture does not explain the role of large datasets in
generalization. Measuring minima volumes under varying amounts of training data
reveals sharp minima which generalize well exist, but are unlikely to be found
due to their small volumes. Increasing data changes the loss landscape, such
that previously small generalizing minima become (relatively) large.

</details>


### [41] [A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification](https://arxiv.org/abs/2511.04814)
*Sebastian Ojeda,Rafael Velasquez,Nicolás Aparicio,Juanita Puentes,Paula Cárdenas,Nicolás Andrade,Gabriel González,Sergio Rincón,Carolina Muñoz-Camargo,Pablo Arbeláez*

Main category: cs.LG

TL;DR: ESCAPE是一个标准化的抗菌肽评估框架，整合了来自27个验证库的80,000多个肽序列，提供了多标签功能分类和新的最先进预测模型。


<details>
  <summary>Details</summary>
Motivation: 解决抗菌肽研究中数据集碎片化、注释不一致和缺乏标准化基准的问题，以促进计算方法和新候选肽的发现。

Method: 构建ESCAPE数据集，整合多个来源的肽序列，建立生物一致的多标签层次结构，并开发基于transformer的模型，结合序列和结构信息预测肽的多功能活性。

Result: 该方法在平均精度均值上相对于第二佳方法实现了2.56%的相对改进，建立了新的最先进多标签肽分类方法。

Conclusion: ESCAPE提供了一个全面且可复现的评估框架，推动了AI驱动的抗菌肽研究发展。

Abstract: Antimicrobial peptides have emerged as promising molecules to combat
antimicrobial resistance. However, fragmented datasets, inconsistent
annotations, and the lack of standardized benchmarks hinder computational
approaches and slow down the discovery of new candidates. To address these
challenges, we present the Expanded Standardized Collection for Antimicrobial
Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000
peptides from 27 validated repositories. Our dataset separates antimicrobial
peptides from negative sequences and incorporates their functional annotations
into a biologically coherent multilabel hierarchy, capturing activities across
antibacterial, antifungal, antiviral, and antiparasitic classes. Building on
ESCAPE, we propose a transformer-based model that leverages sequence and
structural information to predict multiple functional activities of peptides.
Our method achieves up to a 2.56% relative average improvement in mean Average
Precision over the second-best method adapted for this task, establishing a new
state-of-the-art multilabel peptide classification. ESCAPE provides a
comprehensive and reproducible evaluation framework to advance AI-driven
antimicrobial peptide research.

</details>


### [42] [Persistent reachability homology in machine learning applications](https://arxiv.org/abs/2511.04825)
*Luigi Caputi,Nicholas Meadows,Henri Riihimäki*

Main category: cs.LG

TL;DR: 本文研究了持久可达性同调（PRH）在有向图数据中的应用，特别是在癫痫检测这一神经科学问题中的网络分类任务表现。PRH相比传统的基于有向旗复形（DPH）的持久同调方法，通过考虑有向图的凝聚而使用更小的图进行计算，在分类任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索持久可达性同调（PRH）在有向图数据分析中的有效性，特别是在癫痫检测这一重要神经科学问题中，解决传统基于有向旗复形（DPH）的方法可能存在的计算效率问题。

Method: 使用持久可达性同调（PRH）分析有向图数据，该方法考虑有向图在持久过滤中的凝聚，从而从更小的有向图进行计算。使用Betti曲线及其积分作为拓扑特征，并在支持向量机上实现分类管道。

Result: PRH在癫痫检测的分类任务中表现优于传统的DPH方法，证明了其在网络分类任务中的有效性。

Conclusion: 持久可达性同调（PRH）是一种有效的有向图数据分析方法，在癫痫检测等神经科学应用中具有优越的分类性能，且计算效率更高。

Abstract: We explore the recently introduced persistent reachability homology (PRH) of
digraph data, i.e. data in the form of directed graphs. In particular, we study
the effectiveness of PRH in network classification task in a key neuroscience
problem: epilepsy detection. PRH is a variation of the persistent homology of
digraphs, more traditionally based on the directed flag complex (DPH). A main
advantage of PRH is that it considers the condensations of the digraphs
appearing in the persistent filtration and thus is computed from smaller
digraphs. We compare the effectiveness of PRH to that of DPH and we show that
PRH outperforms DPH in the classification task. We use the Betti curves and
their integrals as topological features and implement our pipeline on support
vector machine.

</details>


### [43] [Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.04834)
*Jiwoo Shin,Byeonghu Na,Mina Kang,Wonhyeok Choi,Il-chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种简单有效的方法来解决文本到图像生成模型中的有害内容防御问题，通过用概念反转获得的隐式负嵌入替换训练自由方法中的负提示，实现了两种防御方法的兼容性提升。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型防御方法包括微调模型和训练自由引导方法，但将这两种正交方法结合使用时往往导致防御性能下降，表明两种范式之间存在关键的不兼容性。

Method: 提出用概念反转获得的隐式负嵌入替换训练自由方法中的负提示，该方法无需修改现有方法，可轻松集成到现有流程中。

Result: 在裸露和暴力基准测试中验证了方法的有效性，显示防御成功率持续提升，同时保持了输入提示的核心语义。

Conclusion: 该方法成功解决了两种防御范式之间的不兼容问题，实现了更好的有害内容防御效果。

Abstract: Recent advances in text-to-image generative models have raised concerns about
their potential to produce harmful content when provided with malicious input
text prompts. To address this issue, two main approaches have emerged: (1)
fine-tuning the model to unlearn harmful concepts and (2) training-free
guidance methods that leverage negative prompts. However, we observe that
combining these two orthogonal approaches often leads to marginal or even
degraded defense performance. This observation indicates a critical
incompatibility between two paradigms, which hinders their combined
effectiveness. In this work, we address this issue by proposing a conceptually
simple yet experimentally robust method: replacing the negative prompts used in
training-free methods with implicit negative embeddings obtained through
concept inversion. Our method requires no modification to either approach and
can be easily integrated into existing pipelines. We experimentally validate
its effectiveness on nudity and violence benchmarks, demonstrating consistent
improvements in defense success rate while preserving the core semantics of
input prompts.

</details>


### [44] [SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression](https://arxiv.org/abs/2511.04838)
*Brenda Nogueira,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: SPECTRA是一个光谱目标感知的图增强框架，通过在光谱域生成真实的分子图来解决分子属性预测中稀有化合物预测性能差的问题。


<details>
  <summary>Details</summary>
Motivation: 在分子属性预测中，最有价值的化合物（如高效力）通常占据目标空间的稀疏区域。标准图神经网络优化平均误差，在这些不常见但关键的情况下表现不佳，现有的过采样方法经常扭曲分子拓扑结构。

Method: SPECTRA框架：(i)从SMILES重建多属性分子图；(ii)通过(Fused)Gromov-Wasserstein耦合对齐分子对以获得节点对应关系；(iii)在稳定共享基中插值拉普拉斯特征值、特征向量和节点特征；(iv)重建边以合成具有插值目标的物理合理中间体。采用基于标签核密度估计的稀有感知预算方案，将增强集中在数据稀缺区域。

Result: 在基准测试中，SPECTRA持续改善相关目标范围内的误差，同时保持竞争力的整体MAE，并产生可解释的合成分子，其结构反映了底层的光谱几何。

Conclusion: 结果表明，光谱、几何感知的增强是解决不平衡分子属性回归问题的有效且高效策略。

Abstract: In molecular property prediction, the most valuable compounds (e.g., high
potency) often occupy sparse regions of the target space. Standard Graph Neural
Networks (GNNs) commonly optimize for the average error, underperforming on
these uncommon but critical cases, with existing oversampling methods often
distorting molecular topology. In this paper, we introduce SPECTRA, a Spectral
Target-Aware graph augmentation framework that generates realistic molecular
graphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute
molecular graphs from SMILES; (ii) aligns molecule pairs via (Fused)
Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates
Laplacian eigenvalues, eigenvectors and node features in a stable share-basis;
and (iv) reconstructs edges to synthesize physically plausible intermediates
with interpolated targets. A rarity-aware budgeting scheme, derived from a
kernel density estimation of labels, concentrates augmentation where data are
scarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions,
SPECTRA densifies underrepresented regions without degrading global accuracy.
On benchmarks, SPECTRA consistently improves error in relevant target ranges
while maintaining competitive overall MAE, and yields interpretable synthetic
molecules whose structure reflects the underlying spectral geometry. Our
results demonstrate that spectral, geometry-aware augmentation is an effective
and efficient strategy for imbalanced molecular property regression.

</details>


### [45] [Sublinear iterations can suffice even for DDPMs](https://arxiv.org/abs/2511.04844)
*Matthew S. Zhang,Stephen Huan,Jerry Huang,Nicholas M. Boffi,Sitan Chen,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出了一种名为DDRaM的SDE积分器，通过使用随机中点来更好地近似SDE，实现了DDPM采样的首个次线性复杂度界限，仅需O(√d)次评分评估即可确保收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的DDPM分析主要关注指数欧拉离散化，其收敛保证通常至少线性依赖于维度或初始Fisher信息。本文受对数凹采样工作的启发，旨在开发更高效的SDE积分器。

Method: 提出了去噪扩散随机中点方法(DDRaM)，利用随机中点来改进SDE近似。使用新开发的"移位组合规则"分析框架，在适当的平滑性假设下分析该算法的离散化特性。

Result: DDRaM实现了次线性复杂度界限，仅需O(√d)次评分评估即可确保收敛，这是纯DDPM采样的首个次线性复杂度界限。先前获得此类界限的工作需要修改采样器，偏离实际使用方式。

Conclusion: 实验验证表明DDRaM在实际预训练图像合成模型中表现良好，为DDPM采样提供了更高效的算法选择。

Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs)
have shown remarkable success in real-world sample generation tasks. Prior
analyses of DDPMs have been focused on the exponential Euler discretization,
showing guarantees that generally depend at least linearly on the dimension or
initial Fisher information. Inspired by works in log-concave sampling (Shen and
Lee, 2019), we analyze an integrator -- the denoising diffusion randomized
midpoint method (DDRaM) -- that leverages an additional randomized midpoint to
better approximate the SDE. Using a recently-developed analytic framework
called the "shifted composition rule", we show that this algorithm enjoys
favorable discretization properties under appropriate smoothness assumptions,
with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure
convergence. This is the first sublinear complexity bound for pure DDPM
sampling -- prior works which obtained such bounds worked instead with
ODE-based sampling and had to make modifications to the sampler which deviate
from how they are used in practice. We also provide experimental validation of
the advantages of our method, showing that it performs well in practice with
pre-trained image synthesis models.

</details>


### [46] [Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches](https://arxiv.org/abs/2511.04845)
*Jingchen Bi,Rodrigo Mesa-Arango*

Main category: cs.LG

TL;DR: 使用机器学习模型分析美国消费者对具有创新运输证书食品的偏好，发现安全性和能源证书最受重视


<details>
  <summary>Details</summary>
Motivation: 基于先前研究发现运输因素在消费者食品购买选择中具有显著影响，需要具体识别消费者重视的运输属性

Method: 采用机器学习模型，提出五种创新运输证书（运输模式、物联网、安全措施、能源来源、必须到达日期），并进行偏好实验控制产品和决策者因素

Result: 研究发现消费者明显倾向于运输领域的安全和能源证书，同时分析了价格、产品类型、证书和决策者因素对购买选择的影响

Conclusion: 研究为改进食品供应链系统提供了数据驱动的建议

Abstract: This paper utilizes a machine learning model to estimate the consumer's
behavior for food products with innovative transportation certificates in the
U.S. Building on previous research that examined demand for food products with
supply chain traceability using stated preference analysis, transportation
factors were identified as significant in consumer food purchasing choices.
Consequently, a second experiment was conducted to pinpoint the specific
transportation attributes valued by consumers. A machine learning model was
applied, and five innovative certificates related to transportation were
proposed: Transportation Mode, Internet of Things (IoT), Safety measures,
Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also
incorporated product-specific and decision-maker factors for control purposes.
The findings reveal a notable inclination toward safety and energy certificates
within the transportation domain of the U.S. food supply chain. Additionally,
the study examined the influence of price, product type, certificates, and
decision-maker factors on purchasing choices. Ultimately, the study offers
data-driven recommendations for improving food supply chain systems.

</details>


### [47] [Grounded Test-Time Adaptation for LLM Agents](https://arxiv.org/abs/2511.04847)
*Arthur Chen,Zuxin Liu,Jianguo Zhang,Akshara Prabhakar,Zhiwei Liu,Shelby Heinecke,Silvio Savarese,Victor Zhong,Caiming Xiong*

Main category: cs.LG

TL;DR: 提出了两种互补策略来提升LLM智能体在新环境中的泛化能力：在线分布适应和部署时动态基础化，显著提高了在复杂环境中的成功率。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在未见过的复杂环境（如新网站或函数集）中表现不佳，主要由于预训练与测试条件不匹配，存在语法误解和语义误解两种失败模式。

Method: 1. 在线分布适应：学习轻量级适应向量来偏置模型输出分布，快速对齐环境响应格式；2. 部署时动态基础化：通过角色驱动探索阶段系统性地探测和学习环境的因果动态，构建非参数世界模型。

Result: 在函数调用和网页导航等多样化智能体基准测试中，两种策略都表现出有效性且计算成本最小。在WebArena多站点分割上，动态基础化方法将成功率从2%提升到23%。

Conclusion: 动态基础化在复杂环境中特别有效，为构建更通用和强大的LLM智能体提供了稳健路径。

Abstract: Large language model (LLM)-based agents struggle to generalize to novel and
complex environments, such as unseen websites or new sets of functions, due to
a fundamental mismatch between their pre-training and test-time conditions.
This challenge stems from two distinct failure modes: a syntactic
misunderstanding of environment-specific components like observation formats,
and a semantic misunderstanding of state-transition dynamics, which are only
revealed at test time. To address these issues, we propose two distinct and
complementary strategies for adapting LLM agents by leveraging
environment-specific information available during deployment. First, an online
distributional adaptation method parameterizes environmental nuances by
learning a lightweight adaptation vector that biases the model's output
distribution, enabling rapid alignment with an environment response format.
Second, a deployment-time dynamics grounding method employs a persona-driven
exploration phase to systematically probe and learn the environment's causal
dynamics before task execution, equipping the agent with a nonparametric world
model. We evaluate these strategies across diverse agentic benchmarks,
including function calling and web navigation. Our empirical results show the
effectiveness of both strategies across all benchmarks with minimal
computational cost. We find that dynamics grounding is particularly effective
in complex environments where unpredictable dynamics pose a major obstacle,
demonstrating a robust path toward more generalizable and capable LLM-based
agents. For example, on the WebArena multi-site split, this method increases
the agent's success rate from 2% to 23%.

</details>


### [48] [SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion](https://arxiv.org/abs/2511.04854)
*Alvaro Prat,Leo Zhang,Charlotte M. Deane,Yee Whye Teh,Garrett M. Morris*

Main category: cs.LG

TL;DR: SigmaDock是一种基于SE(3)黎曼扩散模型的分子对接方法，通过将配体分解为刚性片段并在结合口袋中重新组装，实现了最先进的对接性能。


<details>
  <summary>Details</summary>
Motivation: 解决生成式分子对接方法中存在的化学不合理输出、泛化性差和计算成本高的问题。

Method: 提出新颖的片段化方案，将配体分解为刚性片段，然后使用SE(3)黎曼扩散模型学习在结合口袋中重新组装这些片段。

Result: 在PoseBusters数据集上达到79.9%的Top-1成功率，远超近期深度学习方法(12.7-30.8%)，并且首次超越经典物理对接方法。

Conclusion: SigmaDock在分子建模的可靠性和可行性方面实现了重大突破，是第一个在PB训练-测试分割下超越经典物理对接的深度学习方法。

Abstract: Determining the binding pose of a ligand to a protein, known as molecular
docking, is a fundamental task in drug discovery. Generative approaches promise
faster, improved, and more diverse pose sampling than physics-based methods,
but are often hindered by chemically implausible outputs, poor
generalisability, and high computational cost. To address these challenges, we
introduce a novel fragmentation scheme, leveraging inductive biases from
structural chemistry, to decompose ligands into rigid-body fragments. Building
on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion
model that generates poses by learning to reassemble these rigid bodies within
the binding pocket. By operating at the level of fragments in SE(3), SigmaDock
exploits well-established geometric priors while avoiding overly complex
diffusion processes and unstable training dynamics. Experimentally, we show
SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates
(RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8%
reported by recent deep learning approaches, whilst demonstrating consistent
generalisation to unseen proteins. SigmaDock is the first deep learning
approach to surpass classical physics-based docking under the PB train-test
split, marking a significant leap forward in the reliability and feasibility of
deep learning for molecular modelling.

</details>


### [49] [Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2511.04856)
*Thore Gerlach,Michael Schenk,Verena Kain*

Main category: cs.LG

TL;DR: 提出理论基础的连续半量子玻尔兹曼机(CSQBMs)，支持连续动作强化学习，通过结合可见单元的指数族先验和隐藏单元的量子玻尔兹曼分布，构建混合量子-经典模型，减少量子比特需求同时保持强表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决连续控制中不稳定性问题，开发能够支持连续动作的量子强化学习框架，同时降低量子资源需求。

Method: 结合指数族先验和量子玻尔兹曼分布构建混合量子-经典模型，利用解析梯度计算实现与Actor-Critic算法的直接集成，提出用CSQBM分布采样替代全局最大化的连续Q学习框架。

Result: 开发出理论基础的CSQBMs模型，能够计算连续变量的解析梯度，构建了稳定的连续控制强化学习框架。

Conclusion: CSQBMs为连续动作强化学习提供了可行的量子-经典混合解决方案，在减少量子资源需求的同时保持了模型的表达能力，解决了连续控制中的稳定性问题。

Abstract: We introduce theoretically grounded Continuous Semi-Quantum Boltzmann
Machines (CSQBMs) that supports continuous-action reinforcement learning. By
combining exponential-family priors over visible units with quantum Boltzmann
distributions over hidden units, CSQBMs yield a hybrid quantum-classical model
that reduces qubit requirements while retaining strong expressiveness.
Crucially, gradients with respect to continuous variables can be computed
analytically, enabling direct integration into Actor-Critic algorithms.
Building on this, we propose a continuous Q-learning framework that replaces
global maximization by efficient sampling from the CSQBM distribution, thereby
overcoming instability issues in continuous control.

</details>


### [50] [FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting](https://arxiv.org/abs/2511.04865)
*Esha Sharma,Lauren Davis,Julie Ivy,Min Chi*

Main category: cs.LG

TL;DR: FoodRL是一个基于强化学习的元学习框架，用于预测食品银行的实物捐赠，通过动态加权不同预测模型来应对波动性和概念漂移，在灾害期间表现优异。


<details>
  <summary>Details</summary>
Motivation: 食品银行对缓解粮食不安全至关重要，但传统预测模型难以准确预测高度波动的实物捐赠，特别是在季节性变化和自然灾害（如飓风、野火）导致的不可预测波动和概念漂移情况下。

Method: 提出FoodRL框架，使用强化学习进行元学习，基于近期性能和上下文信息对不同的预测模型进行聚类和动态加权。

Result: 在两个结构不同的美国食品银行（受野火影响的西海岸大型区域食品银行和受飓风影响的东海岸州级食品银行）的多年度数据评估中，FoodRL始终优于基线方法，特别是在中断或下降期间。

Conclusion: FoodRL通过提供更可靠和自适应的预测，每年可促进相当于额外170万餐的食品重新分配，展示了其在社会影响和人道主义供应链自适应集成学习方面的巨大潜力。

Abstract: Food banks are crucial for alleviating food insecurity, but their
effectiveness hinges on accurately forecasting highly volatile in-kind
donations to ensure equitable and efficient resource distribution. Traditional
forecasting models often fail to maintain consistent accuracy due to
unpredictable fluctuations and concept drift driven by seasonal variations and
natural disasters such as hurricanes in the Southeastern U.S. and wildfires in
the West Coast. To address these challenges, we propose FoodRL, a novel
reinforcement learning (RL) based metalearning framework that clusters and
dynamically weights diverse forecasting models based on recent performance and
contextual information. Evaluated on multi-year data from two structurally
distinct U.S. food banks-one large regional West Coast food bank affected by
wildfires and another state-level East Coast food bank consistently impacted by
hurricanes, FoodRL consistently outperforms baseline methods, particularly
during periods of disruption or decline. By delivering more reliable and
adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent
to 1.7 million additional meals annually, demonstrating its significant
potential for social impact as well as adaptive ensemble learning for
humanitarian supply chains.

</details>


### [51] [Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning](https://arxiv.org/abs/2511.04883)
*Di Chen,Jia Li,Michael Zhang*

Main category: cs.LG

TL;DR: 研究表明，在混合自动驾驶交通系统中，即使驾驶代理（包括自动驾驶车辆和人类驾驶车辆）出于自身利益行动，也能通过深度强化学习实现集体理性，从而为所有参与者带来益处。


<details>
  <summary>Details</summary>
Motivation: 探讨在混合自动驾驶交通中，当所有驾驶代理（包括自动驾驶车辆和人类驾驶车辆）都出于自身利益行动时，是否仍能为整个交通系统带来益处。

Method: 使用深度强化学习训练驾驶代理，采用简单的奖励设计，不直接包含系统级目标，研究集体理性是否能在各种场景中自然涌现。

Result: 集体理性在各种场景中持续涌现，表明这一特性具有鲁棒性。研究还提出了解释集体理性在微观动态环境中涌现的机制，并通过模拟证据进行了验证。

Conclusion: 研究表明可以利用先进学习方法（如联邦学习）在混合自动驾驶系统中实现自利驾驶代理之间的集体合作，为交通系统优化提供了新思路。

Abstract: Autonomous vehicles (AVs) are expected to be commercially available in the
near future, leading to mixed autonomy traffic consisting of both AVs and
human-driven vehicles (HVs). Although numerous studies have shown that AVs can
be deployed to benefit the overall traffic system performance by incorporating
system-level goals into their decision making, it is not clear whether the
benefits still exist when agents act out of self-interest -- a trait common to
all driving agents, both human and autonomous. This study aims to understand
whether self-interested AVs can bring benefits to all driving agents in mixed
autonomy traffic systems. The research is centered on the concept of collective
rationality (CR). This concept, originating from game theory and behavioral
economics, means that driving agents may cooperate collectively even when
pursuing individual interests. Our recent research has proven the existence of
CR in an analytical game-theoretical model and empirically in mixed
human-driven traffic. In this paper, we demonstrate that CR can be attained
among driving agents trained using deep reinforcement learning (DRL) with a
simple reward design. We examine the extent to which self-interested traffic
agents can achieve CR without directly incorporating system-level objectives.
Results show that CR consistently emerges in various scenarios, which indicates
the robustness of this property. We also postulate a mechanism to explain the
emergence of CR in the microscopic and dynamic environment and verify it based
on simulation evidence. This research suggests the possibility of leveraging
advanced learning methods (such as federated learning) to achieve collective
cooperation among self-interested driving agents in mixed-autonomy systems.

</details>


### [52] [You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models](https://arxiv.org/abs/2511.04902)
*Shuvendu Roy,Hossein Hajimirsadeghi,Mengyao Zhai,Golnoosh Samei*

Main category: cs.LG

TL;DR: 本文系统研究了无标签强化学习方法在不同规模模型（0.5B-7B参数）上的表现，发现该方法严重依赖基础模型的推理能力，对较弱模型效果不佳。作者提出基于课程学习的改进方法，通过渐进式引入难题和屏蔽无多数共识的rollout来提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索无标签强化学习方法在推理能力有限的小型基础模型上的泛化能力，解决现有方法对模型推理能力依赖过强的问题。

Method: 提出基于课程学习的无标签强化学习方法：1）在训练过程中渐进式引入更困难的问题；2）屏蔽训练过程中无多数共识的rollout；3）建立数据筛选流程生成预定义难度的样本。

Result: 改进方法在所有模型规模和推理能力上都表现出持续提升，为资源受限模型推理能力的自举提供了更稳健的无监督强化学习路径。

Conclusion: 无标签强化学习的效果高度依赖基础模型的推理能力，通过课程学习和数据筛选可以显著提升该方法在各种规模模型上的表现，为资源受限环境下的推理能力提升提供了可行方案。

Abstract: Recent advances in large language models have demonstrated the promise of
unsupervised reinforcement learning (RL) methods for enhancing reasoning
capabilities without external supervision. However, the generalizability of
these label-free RL approaches to smaller base models with limited reasoning
capabilities remains unexplored. In this work, we systematically investigate
the performance of label-free RL methods across different model sizes and
reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals
critical limitations: label-free RL is highly dependent on the base model's
pre-existing reasoning capability, with performance often degrading below
baseline levels for weaker models. We find that smaller models fail to generate
sufficiently long or diverse chain-of-thought reasoning to enable effective
self-reflection, and that training data difficulty plays a crucial role in
determining success. To address these challenges, we propose a simple yet
effective method for label-free RL that utilizes curriculum learning to
progressively introduce harder problems during training and mask no-majority
rollouts during training. Additionally, we introduce a data curation pipeline
to generate samples with predefined difficulty. Our approach demonstrates
consistent improvements across all model sizes and reasoning capabilities,
providing a path toward more robust unsupervised RL that can bootstrap
reasoning abilities in resource-constrained models. We make our code available
at https://github.com/BorealisAI/CuMa

</details>


### [53] [Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale](https://arxiv.org/abs/2511.04904)
*Bassel Al Omari,Michael Matthews,Alexander Rutherford,Jakob Nicolaus Foerster*

Main category: cs.LG

TL;DR: 提出了Craftax-MA和Craftax-Coop两个多智能体强化学习基准环境，前者扩展了Craftax支持多智能体，后者引入异质智能体、交易等机制，需要复杂合作。基于JAX实现，训练速度快，能评估长期依赖和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MARL基准主要针对短期挑战，无法充分评估长期依赖和泛化能力，需要更具挑战性的基准来推动MARL研究发展。

Method: 扩展Craftax环境支持多智能体（Craftax-MA），并进一步引入异质智能体、交易等合作机制（Craftax-Coop），基于JAX实现高效训练。

Result: 现有算法在该基准中面临长期信用分配、探索和合作等关键挑战，训练250M环境交互可在1小时内完成。

Conclusion: Craftax基准环境为MARL研究提供了更具挑战性的测试平台，有潜力推动长期研究发展。

Abstract: Progress in multi-agent reinforcement learning (MARL) requires challenging
benchmarks that assess the limits of current methods. However, existing
benchmarks often target narrow short-horizon challenges that do not adequately
stress the long-term dependencies and generalization capabilities inherent in
many multi-agent systems. To address this, we first present
\textit{Craftax-MA}: an extension of the popular open-ended RL environment,
Craftax, that supports multiple agents and evaluates a wide range of general
abilities within a single environment. Written in JAX, \textit{Craftax-MA} is
exceptionally fast with a training run using 250 million environment
interactions completing in under an hour. To provide a more compelling
challenge for MARL, we also present \textit{Craftax-Coop}, an extension
introducing heterogeneous agents, trading and more mechanics that require
complex cooperation among agents for success. We provide analysis demonstrating
that existing algorithms struggle with key challenges in this benchmark,
including long-horizon credit assignment, exploration and cooperation, and
argue for its potential to drive long-term research in MARL.

</details>


### [54] [Efficient Swap Multicalibration of Elicitable Properties](https://arxiv.org/abs/2511.04907)
*Lunjia Hu,Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: 本文提出了针对可引出属性的多校准泛化，从群组成员函数扩展到任意有界假设类，并引入更强的交换多校准概念。作者设计了一种oracle高效算法，在给定在线不可知学习器的情况下，以高概率实现T^{1/(r+1)}的ℓ_r-交换多校准误差。


<details>
  <summary>Details</summary>
Motivation: 现有工作[NR23]建立了多校准与属性引出性之间的深刻联系，但其在线算法效率低下。本文旨在为任意有界假设类和可引出属性设计更高效的oracle高效算法，并改进现有误差界限。

Method: 将多校准从群组成员函数泛化到任意有界假设类，引入交换多校准概念。提出基于在线不可知学习器的oracle高效算法，利用假设类的有界顺序Rademacher复杂度。

Result: 对于r≥2，算法以高概率实现T^{1/(r+1)}的ℓ_r-交换多校准误差。特别地，当r=2时，实现了T^{1/3}的ℓ_2-交换多校准误差，显著改进了现有界限。

Conclusion: 本文完全解决了[GJRR24]中关于oracle高效算法能否实现√T ℓ_2-均值多校准误差的开放问题，给出了强烈肯定的答案，并为多校准理论提供了更强大的算法保证。

Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that
demands that the predictions of a predictor are correct conditional on
themselves and membership in a collection of potentially overlapping subgroups
of a population. The work of [NR23] established a surprising connection between
multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and
property elicitation: a property $\Gamma$ can be multicalibrated if and only if
it is elicitable, where elicitability is the notion that the true property
value of a distribution can be obtained by solving a regression problem over
the distribution. In the online setting, [NR23] proposed an inefficient
algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a
hypothesis class of group membership functions and an elicitable property
$\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property
$\Gamma$ from group membership functions to arbitrary bounded hypothesis
classes and introduce a stronger notion -- swap multicalibration, following
[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when
given access to an online agnostic learner, achieves $T^{1/(r+1)}$
$\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a
hypothesis class with bounded sequential Rademacher complexity and an
elicitable property $\Gamma$. For the special case of $r=2$, this implies an
oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap
multicalibration error, which significantly improves on the previously
established bounds for the problem [NR23, GMS25, LSS25a], and completely
resolves an open question raised in [GJRR24] on the possibility of an
oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean
multicalibration error by answering it in a strongly affirmative sense.

</details>


### [55] [A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates](https://arxiv.org/abs/2511.04909)
*Paula Rodriguez-Diaz,Kirk Bansak Elisabeth Paulson*

Main category: cs.LG

TL;DR: 提出了Dual-Guided Loss (DGL)，一种简单可扩展的决策导向学习方法，通过利用下游问题的对偶变量来减少对求解器的依赖，在组合选择问题中实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有的决策导向学习方法需要频繁调用求解器，计算成本高昂，难以扩展到大规模问题。需要一种既能保持决策对齐又能减少求解器依赖的方法。

Method: 利用下游组合选择问题的对偶变量构建DGL损失函数，通过周期性求解下游问题来解耦优化和梯度更新，在刷新间隔期间使用对偶调整目标进行训练。

Result: DGL在两种问题类别上与最先进的DFL方法性能相当或更优，同时显著减少求解器调用次数和训练时间，训练成本接近标准监督学习。

Conclusion: DGL提供了一种可扩展的决策导向学习框架，在保持强决策对齐的同时大幅降低了计算成本，适用于匹配、背包、最短路径等组合选择问题。

Abstract: Many real-world decisions are made under uncertainty by solving optimization
problems using predicted quantities. This predict-then-optimize paradigm has
motivated decision-focused learning, which trains models with awareness of how
the optimizer uses predictions, improving the performance of downstream
decisions. Despite its promise, scaling is challenging: state-of-the-art
methods either differentiate through a solver or rely on task-specific
surrogates, both of which require frequent and expensive calls to an optimizer,
often a combinatorial one. In this paper, we leverage dual variables from the
downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a
simple, scalable objective that preserves decision alignment while reducing
solver dependence. We construct DGL specifically for combinatorial selection
problems with natural one-of-many constraints, such as matching, knapsack, and
shortest path. Our approach (a) decouples optimization from gradient updates by
solving the downstream problem only periodically; (b) between refreshes, trains
on dual-adjusted targets using simple differentiable surrogate losses; and (c)
as refreshes become less frequent, drives training cost toward standard
supervised learning while retaining strong decision alignment. We prove that
DGL has asymptotically diminishing decision regret, analyze runtime complexity,
and show on two problem classes that DGL matches or exceeds state-of-the-art
DFL methods while using far fewer solver calls and substantially less training
time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.

</details>


### [56] [Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application](https://arxiv.org/abs/2511.04918)
*A. Ganapathi Rao,Sathish Krishna Anumula,Aditya Kumar Singh,Renukhadevi M,Y. Jeevan Nagendra Kumar,Tammineni Rama Tulasi*

Main category: cs.LG

TL;DR: 本文研究了机器学习算法与传统统计模型的集成方法，展示了混合模型在预测准确性、鲁棒性和可解释性方面的显著改进


<details>
  <summary>Details</summary>
Motivation: 探索机器学习与传统统计建模的新颖集成方式，改变数据分析、预测分析和决策制定的方法

Method: 研究机器学习与统计模型的连接，展示现代ML算法如何丰富传统模型，通过混合模型提升性能

Result: 混合模型在预测准确性、鲁棒性和可解释性方面实现了重大改进

Conclusion: 机器学习与传统统计模型的集成能够显著提升模型的性能、扩展性、灵活性和鲁棒性

Abstract: It involves the completely novel ways of integrating ML algorithms with
traditional statistical modelling that has changed the way we analyze data, do
predictive analytics or make decisions in the fields of the data. In this
paper, we study some ML and statistical model connections to understand ways in
which some modern ML algorithms help 'enrich' conventional models; we
demonstrate how new algorithms improve performance, scale, flexibility and
robustness of the traditional models. It shows that the hybrid models are of
great improvement in predictive accuracy, robustness, and interpretability

</details>


### [57] [Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding](https://arxiv.org/abs/2511.04934)
*Hadi Reisizadeh,Jiajun Ruan,Yiwei Chen,Soumyadeep Pal,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 现有的大语言模型遗忘方法在概率解码下无法实现真正的知识遗忘，敏感信息会可靠地重新出现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的遗忘对于法规合规和构建道德生成AI系统至关重要，需要避免产生私人、有毒、非法或受版权保护的内容。

Method: 引入leak@k元评估指标，量化在k个样本生成中被遗忘知识重新出现的可能性，并在TOFU、MUSE和WMDP三个基准上进行大规模系统研究。

Result: 发现知识泄露在方法和任务中持续存在，当前最先进的遗忘技术仅提供有限的遗忘效果。

Conclusion: 当前的大语言模型遗忘方法无法实现真正的知识移除，迫切需要更鲁棒的遗忘方法。

Abstract: Unlearning in large language models (LLMs) is critical for regulatory
compliance and for building ethical generative AI systems that avoid producing
private, toxic, illegal, or copyrighted content. Despite rapid progress, in
this work we show that \textit{almost all} existing unlearning methods fail to
achieve true forgetting in practice. Specifically, while evaluations of these
`unlearned' models under deterministic (greedy) decoding often suggest
successful knowledge removal using standard benchmarks (as has been done in the
literature), we show that sensitive information reliably resurfaces when models
are sampled with standard probabilistic decoding. To rigorously capture this
vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric
that quantifies the likelihood of forgotten knowledge reappearing when
generating $k$ samples from the model under realistic decoding strategies.
Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the
first large-scale, systematic study of unlearning reliability using our newly
defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge
leakage persists across methods and tasks, underscoring that current
state-of-the-art unlearning techniques provide only limited forgetting and
highlighting the urgent need for more robust approaches to LLM unlearning.

</details>


### [58] [Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression](https://arxiv.org/abs/2511.04937)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: 本文研究了EM算法在两分量混合线性回归中的结构特性、摆线轨迹和非渐近收敛保证，特别是在未知混合权重和回归参数的情况下。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究在已知平衡权重和高信噪比情况下建立了EM算法的全局收敛性，但在完全未知参数设置下的理论行为仍不清楚，其轨迹和收敛阶数尚未完全表征。

Method: 推导了在所有信噪比情况下未知混合权重和回归参数的2MLR的显式EM更新表达式，分析了其结构特性和摆线轨迹。在无噪声情况下证明了回归参数轨迹形成摆线，在高信噪比情况下量化了与摆线轨迹的偏差。

Result: 轨迹分析揭示了收敛阶数：当EM估计与真实值几乎正交时呈线性收敛，当估计与真实值夹角较小时呈二次收敛。建立了有限样本水平上任意初始化的收敛保证。

Conclusion: 这项工作为分析混合线性回归中的EM算法提供了一个新颖的基于轨迹的框架。

Abstract: This work investigates the structural properties, cycloid trajectories, and
non-asymptotic convergence guarantees of the Expectation-Maximization (EM)
algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing
weights and regression parameters. Recent studies have established global
convergence for 2MLR with known balanced weights and super-linear convergence
in noiseless and high signal-to-noise ratio (SNR) regimes. However, the
theoretical behavior of EM in the fully unknown setting remains unclear, with
its trajectory and convergence order not yet fully characterized. We derive
explicit EM update expressions for 2MLR with unknown mixing weights and
regression parameters across all SNR regimes and analyze their structural
properties and cycloid trajectories. In the noiseless case, we prove that the
trajectory of the regression parameters in EM iterations traces a cycloid by
establishing a recurrence relation for the sub-optimality angle, while in high
SNR regimes we quantify its discrepancy from the cycloid trajectory. The
trajectory-based analysis reveals the order of convergence: linear when the EM
estimate is nearly orthogonal to the ground truth, and quadratic when the angle
between the estimate and ground truth is small at the population level. Our
analysis establishes non-asymptotic guarantees by sharpening bounds on
statistical errors between finite-sample and population EM updates, relating
EM's statistical accuracy to the sub-optimality angle, and proving convergence
with arbitrary initialization at the finite-sample level. This work provides a
novel trajectory-based framework for analyzing EM in Mixed Linear Regression.

</details>


### [59] [Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2511.04971)
*Esha Chowdhury*

Main category: cs.LG

TL;DR: 本研究提出了一种结合机器学习和混合深度学习的方法，用于预测糖尿病患者的心血管疾病风险，其中XGBoost和LSTM模型达到了最高的0.9050准确率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病与心血管疾病之间存在强关联，且糖尿病患病率不断增长，需要开发有效的CVD风险预测模型来改善医疗决策。

Method: 使用BRFSS数据集，经过数据预处理和PCA特征提取，实施了多种ML模型（DT、RF、KNN、SVM、AdaBoost、XGBoost）和DL模型（ANN、DNN、RNN、CNN、LSTM、BiLSTM、GRU）以及CNN与LSTM/BiLSTM/GRU的混合模型。

Result: XGBoost和LSTM模型均达到最高准确率0.9050，部分模型实现了完美的召回率（1.00），高准确率和F1分数证明了这些模型的有效性。

Conclusion: 机器学习和深度学习模型在预测糖尿病患者CVD风险方面表现出色，能够自动化并增强临床决策，有潜力改善个性化风险管理和预防策略。

Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for
healthcare institutions. This study addresses the growing prevalence of
diabetes and its strong link to heart disease by proposing an efficient CVD
risk prediction model for diabetic patients using machine learning (ML) and
hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by
removing duplicates, handling missing values, identifying categorical and
numerical features, and applying Principal Component Analysis (PCA) for feature
extraction. Several ML models, including Decision Trees (DT), Random Forest
(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and
XGBoost, were implemented, with XGBoost achieving the highest accuracy of
0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep
Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and
Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,
BiLSTM, and GRU, were also explored. Some of these models achieved perfect
recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.
Our research highlights the effectiveness of ML and DL models in predicting CVD
risk among diabetic patients, automating and enhancing clinical
decision-making. High accuracy and F1 scores demonstrate these models'
potential to improve personalized risk management and preventive strategies.

</details>


### [60] [Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces](https://arxiv.org/abs/2511.04973)
*Siyuan Li,Yifan Sun,Lei Cheng,Lewen Wang,Yang Liu,Weiqing Liu,Jianlong Li,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: FAR-TS是一个用于多变量时间序列生成的框架，通过解耦因子化和自回归Transformer在离散量化潜空间上实现快速、可控的任意长度序列生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的时间序列生成方法速度慢且仅限于固定长度窗口，需要一种更高效灵活的生成方法。

Method: 将时间序列分解为捕获静态跨通道相关性的数据自适应基和时序系数，后者被向量量化为离散标记，然后使用LLaMA风格的自回归Transformer建模这些标记序列。

Result: FAR-TS比Diffusion-TS快几个数量级，同时保持了跨通道相关性和可解释的潜空间，实现了高质量灵活的时间序列合成。

Conclusion: FAR-TS通过简化的设计实现了快速、可控的任意长度时间序列生成，在保持数据质量的同时显著提升了生成效率。

Abstract: Generative models for multivariate time series are essential for data
augmentation, simulation, and privacy preservation, yet current
state-of-the-art diffusion-based approaches are slow and limited to
fixed-length windows. We propose FAR-TS, a simple yet effective framework that
combines disentangled factorization with an autoregressive Transformer over a
discrete, quantized latent space to generate time series. Each time series is
decomposed into a data-adaptive basis that captures static cross-channel
correlations and temporal coefficients that are vector-quantized into discrete
tokens. A LLaMA-style autoregressive Transformer then models these token
sequences, enabling fast and controllable generation of sequences with
arbitrary length. Owing to its streamlined design, FAR-TS achieves
orders-of-magnitude faster generation than Diffusion-TS while preserving
cross-channel correlations and an interpretable latent space, enabling
high-quality and flexible time series synthesis.

</details>


### [61] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: 提出了一种可扩展的ROC-SVM变体，通过不完全U统计量和低秩核近似大幅降低计算复杂度，同时保持与原方法相当的AUC性能。


<details>
  <summary>Details</summary>
Motivation: 原始ROC-SVM虽然能直接最大化AUC且在处理类别不平衡时表现优异，但其O(n²)的计算复杂度限制了实际应用。

Method: 使用不完全U统计量减少计算复杂度，并通过低秩核近似扩展到非线性分类，实现再生核希尔伯特空间中的高效训练。

Result: 理论分析建立了误差界证明近似合理性，实证结果显示在合成和真实数据集上，新方法能达到与原ROC-SVM相当的AUC性能，同时训练时间大幅减少。

Conclusion: 提出的可扩展ROC-SVM变体在保持性能的同时显著提高了计算效率，使其更适合实际应用。

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [62] [Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk](https://arxiv.org/abs/2511.04980)
*Rongbin Ye,Jiaqi Chen*

Main category: cs.LG

TL;DR: 该论文提出一个五维框架来评估复杂机器学习模型在金融风险建模中的可解释性，证明通过SHAP和LIME等解释技术可以在保持高性能的同时满足监管要求。


<details>
  <summary>Details</summary>
Motivation: 解决金融行业在风险建模中面临的挑战：平衡先进机器学习模型的预测能力与监管机构要求的可解释性之间的冲突。

Method: 应用LIME和SHAP解释框架于不同模型，并提出包含固有可解释性、全局解释、局部解释、一致性和复杂度的五维评估框架。

Result: 研究表明，具有更好预测能力的复杂模型通过SHAP和LIME可以达到与传统模型相同的可解释性水平。

Conclusion: 通过现代解释技术，可以在受监管的金融环境中应用复杂的高性能ML模型，并提供结构化方法来评估模型性能与可解释性之间的权衡。

Abstract: The financial industry faces a significant challenge modeling and risk
portfolios: balancing the predictability of advanced machine learning models,
neural network models, and explainability required by regulatory entities (such
as Office of the Comptroller of the Currency, Consumer Financial Protection
Bureau). This paper intends to fill the gap in the application between these
"black box" models and explainability frameworks, such as LIME and SHAP.
Authors elaborate on the application of these frameworks on different models
and demonstrates the more complex models with better prediction powers could be
applied and reach the same level of the explainability, using SHAP and LIME.
Beyond the comparison and discussion of performances, this paper proposes a
novel five dimensional framework evaluating Inherent Interpretability, Global
Explanations, Local Explanations, Consistency, and Complexity to offer a
nuanced method for assessing and comparing model explainability beyond simple
accuracy metrics. This research demonstrates the feasibility of employing
sophisticated, high performing ML models in regulated financial environments by
utilizing modern explainability techniques and provides a structured approach
to evaluate the crucial trade offs between model performance and
interpretability.

</details>


### [63] [Deep Progressive Training: scaling up depth capacity of zero/one-layer models](https://arxiv.org/abs/2511.04981)
*Zhiqi Bu*

Main category: cs.LG

TL;DR: 提出了零/一层渐进训练方法，在GPT2上可节省约80%计算量或加速5倍，同时达到与60层模型几乎相同的性能


<details>
  <summary>Details</summary>
Motivation: 模型深度在深度学习中是一把双刃剑：更深的模型能获得更高准确率但需要更高计算成本。渐进训练通过在训练过程中逐步扩展模型容量来有效降低计算成本

Method: 从优化理论和特征学习的角度研究深度扩展，提出零/一层渐进训练方法，关注新层初始化、超参数迁移、学习率调度和模型扩展时机

Result: 在GPT2上的实验表明，该方法可节省约80%计算量或实现5倍加速，同时达到与60层7B参数模型几乎相同的损失

Conclusion: 渐进训练是实现计算效率与性能平衡的有效策略，零/一层渐进训练提供了最优的计算与损失权衡

Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve
higher accuracy but require higher computational cost. To efficiently train
models at scale, an effective strategy is the progressive training, which
scales up model capacity during training, hence significantly reducing
computation with little to none performance degradation. In this work, we study
the depth expansion of large models through the lens of optimization theory and
feature learning, offering insights on the initialization of new layers,
hyperparameter transfer, learning rate schedule, and timing of model expansion.
Specifically, we propose zero/one-layer progressive training for the optimal
tradeoff between computation and loss. For example, zero/one-layer progressive
training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate
$\approx 5\times$ while achieving almost the same loss, compared to to a fully
trained 60-layer model with 7B parameters.

</details>


### [64] [Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding](https://arxiv.org/abs/2511.04984)
*Xinheng He,Yijia Zhang,Haowei Lin,Xingang Peng,Xiangzhe Kong,Mingyu Li,Jianzhu Ma*

Main category: cs.LG

TL;DR: Peptide2Mol是一个E(3)等变图神经网络扩散模型，通过参考原始肽结合剂及其周围蛋白口袋环境来生成小分子，在非自回归生成任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大多数AI驱动的药物设计方法忽略了内源性蛋白与肽相互作用的重要性，可能导致次优分子设计。

Method: 使用E(3)-等变图神经网络扩散模型，基于原始肽结合剂和蛋白口袋环境生成小分子，支持部分扩散过程进行分子优化和肽模拟设计。

Result: 模型不仅实现了最先进的非自回归生成性能，还生成了与原始肽结合剂相似的分子。

Conclusion: Peptide2Mol是一个有效的深度生成模型，可用于从蛋白结合口袋生成和优化生物活性小分子。

Abstract: Structure-based drug design has seen significant advancements with the
integration of artificial intelligence (AI), particularly in the generation of
hit and lead compounds. However, most AI-driven approaches neglect the
importance of endogenous protein interactions with peptides, which may result
in suboptimal molecule designs. In this work, we present Peptide2Mol, an
E(3)-equivariant graph neural network diffusion model that generates small
molecules by referencing both the original peptide binders and their
surrounding protein pocket environments. Trained on large datasets and
leveraging sophisticated modeling techniques, Peptide2Mol not only achieves
state-of-the-art performance in non-autoregressive generative tasks, but also
produces molecules with similarity to the original peptide binder.
Additionally, the model allows for molecule optimization and peptidomimetic
design through a partial diffusion process. Our results highlight Peptide2Mol
as an effective deep generative model for generating and optimizing bioactive
small molecules from protein binding pockets.

</details>


### [65] [Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models](https://arxiv.org/abs/2511.04988)
*Runsheng Ren,Jing Li,Yanxiu Li,Shixun Huang,Jun Shen,Wanqing Li,John Le,Sheng Wang*

Main category: cs.LG

TL;DR: 提出一个综合混合框架，结合结构断点检测、小波信号去噪和深度学习模型，显著提升碳价格预测精度。


<details>
  <summary>Details</summary>
Motivation: 碳价格预测对能源市场决策至关重要，但面临结构断点和高频噪声的挑战。现有研究缺乏系统性评估，限制了模型的鲁棒性和泛化能力。

Method: 集成Bai-Perron、ICSS和PELT算法进行结构断点检测，使用小波信号去噪，结合LSTM、GRU和TCN三种深度学习模型构建混合框架。

Result: PELT-WT-TCN模型表现最佳，相比最先进基线模型，RMSE降低22.35%，MAE降低18.63%；相比原始LSTM，RMSE降低70.55%，MAE降低74.42%。

Conclusion: 将结构意识和多尺度分解集成到深度学习架构中，可显著提升碳价格预测及其他非平稳金融时间序列的准确性和可解释性。

Abstract: Accurately forecasting carbon prices is essential for informed energy market
decision-making, guiding sustainable energy planning, and supporting effective
decarbonization strategies. However, it remains challenging due to structural
breaks and high-frequency noise caused by frequent policy interventions and
market shocks. Existing studies, including the most recent baseline approaches,
have attempted to incorporate breakpoints but often treat denoising and
modeling as separate processes and lack systematic evaluation across advanced
deep learning architectures, limiting the robustness and the generalization
capability. To address these gaps, this paper proposes a comprehensive hybrid
framework that integrates structural break detection (Bai-Perron, ICSS, and
PELT algorithms), wavelet signal denoising, and three state-of-the-art deep
learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot
prices from 2007 to 2024 and exogenous features such as energy prices and
policy indicators, the framework constructs univariate and multivariate
datasets for comparative evaluation. Experimental results demonstrate that our
proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing
forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the
state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by
70.55% in RMSE and 74.42% in MAE compared to the original LSTM without
decomposition from the same baseline study. These findings underscore the value
of integrating structural awareness and multiscale decomposition into deep
learning architectures to enhance accuracy and interpretability in carbon price
forecasting and other nonstationary financial time series.

</details>


### [66] [BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records](https://arxiv.org/abs/2511.04998)
*Daniel S. Lee,Mayra S. Haedo-Cruz,Chen Jiang,Oshin Miranda,LiRong Wang*

Main category: cs.LG

TL;DR: 提出了BiPETE模型，结合旋转位置嵌入和正弦嵌入来处理电子健康记录中的时序依赖问题，在抑郁和PTSD队列中显著提升了酒精和物质使用障碍风险的预测性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在电子健康记录疾病风险预测中表现出潜力，但由于就诊间隔不规则和缺乏统一结构，建模时序依赖仍具挑战。

Method: 开发了Bi-Positional Embedding Transformer Encoder (BiPETE)，整合旋转位置嵌入编码相对就诊时间，正弦嵌入保留就诊顺序，在两个心理健康队列上训练预测ASUD风险。

Result: BiPETE在抑郁和PTSD队列中分别将AUPRC提升了34%和50%，消融研究证实了双重位置编码策略的有效性。

Conclusion: 研究提供了一个实用且可解释的电子健康记录疾病风险预测框架，能够实现强性能，并通过归因方法识别了与ASUD风险相关的关键临床特征。

Abstract: Transformer-based deep learning models have shown promise for disease risk
prediction using electronic health records(EHRs), but modeling temporal
dependencies remains a key challenge due to irregular visit intervals and lack
of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder
or BiPETE for single-disease prediction, which integrates rotary positional
embeddings to encode relative visit timing and sinusoidal embeddings to
preserve visit order. Without relying on large-scale pretraining, BiPETE is
trained on EHR data from two mental health cohorts-depressive disorder and
post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and
substance use disorders (ASUD). BiPETE outperforms baseline models, improving
the area under the precision-recall curve (AUPRC) by 34% and 50% in the
depression and PTSD cohorts, respectively. An ablation study further confirms
the effectiveness of the dual positional encoding strategy. We apply the
Integrated Gradients method to interpret model predictions, identifying key
clinical features associated with ASUD risk and protection, such as abnormal
inflammatory, hematologic, and metabolic markers, as well as specific
medications and comorbidities. Overall, these key clinical features identified
by the attribution methods contribute to a deeper understanding of the risk
assessment process and offer valuable clues for mitigating potential risks. In
summary, our study presents a practical and interpretable framework for disease
risk prediction using EHR data, which can achieve strong performance.

</details>


### [67] [Multi-agent Coordination via Flow Matching](https://arxiv.org/abs/2511.05005)
*Dongsu Lee,Daehee Lee,Amy Zhang*

Main category: cs.LG

TL;DR: MAC-Flow是一个简单但表达能力强的多智能体协调框架，通过流式表示学习联合行为，并将其蒸馏为分散式单步策略，在保持协调性能的同时实现快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有效协调的两个要求之间存在权衡：扩散方法能捕捉复杂协调但计算慢，高斯策略方法推理快但处理多智能体交互脆弱。需要解决这种性能与计算成本的权衡。

Method: 首先学习基于流的联合行为表示，然后将其蒸馏为分散式单步策略，这些策略在保持协调的同时支持快速执行。

Result: 在4个基准测试（12个环境和34个数据集）上，MAC-Flow缓解了性能与计算成本的权衡，推理速度比基于扩散的MARL方法快约14.5倍，同时保持良好性能，推理速度与先前基于高斯策略的离线MARL方法相当。

Conclusion: MAC-Flow成功解决了多智能体协调中性能与计算效率的权衡问题，提供了一种既表达能力强又计算高效的解决方案。

Abstract: This work presents MAC-Flow, a simple yet expressive framework for
multi-agent coordination. We argue that requirements of effective coordination
are twofold: (i) a rich representation of the diverse joint behaviors present
in offline data and (ii) the ability to act efficiently in real time. However,
prior approaches often sacrifice one for the other, i.e., denoising
diffusion-based solutions capture complex coordination but are computationally
slow, while Gaussian policy-based solutions are fast but brittle in handling
multi-agent interaction. MAC-Flow addresses this trade-off by first learning a
flow-based representation of joint behaviors, and then distilling it into
decentralized one-step policies that preserve coordination while enabling fast
execution. Across four different benchmarks, including $12$ environments and
$34$ datasets, MAC-Flow alleviates the trade-off between performance and
computational cost, specifically achieving about $\boldsymbol{\times14.5}$
faster inference compared to diffusion-based MARL methods, while maintaining
good performance. At the same time, its inference speed is similar to that of
prior Gaussian policy-based offline multi-agent reinforcement learning (MARL)
methods.

</details>


### [68] [OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data](https://arxiv.org/abs/2511.05028)
*Dongjin Park,Hasung Yeo,Joon-Woo Lee*

Main category: cs.LG

TL;DR: OvA-LP是一个在联邦微调中从源头抑制局部漂移的简约框架，通过线性探测和一对多头设计，在非IID数据下保持95.9%的IID准确率，远优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调在异构客户端分布下因局部漂移导致的系统性偏差和方差放大问题，现有方法在极端非IID条件下表现脆弱。

Method: 结合冻结编码器上的线性探测与一对多头，采用两阶段程序，保持预训练特征几何并解耦逻辑以防止漂移放大机制。

Result: 在CIFAR-100的100个客户端上，OvA-LP在非IID条件下保持95.9%的IID准确率，而PFPT和FFT-MoE分别只有10.1%和34.5%。在对称和非对称标签噪声下也保持韧性。

Conclusion: OvA-LP为异构环境下的鲁棒联邦微调提供了原则性和高效的基础。

Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data
but remains fragile under heterogeneous client distributions due to local
drift, i.e., client-level update divergences that induce systematic bias and
amplified variance in the global model. Existing aggregation and
personalization methods largely correct drift post hoc, which proves brittle
under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework
that is, to our knowledge, the first explicitly designed to suppress drift at
its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing
on a frozen encoder with a one-vs-all head and a simple two-stage procedure,
preserving pretrained feature geometry and decoupling logits to prevent the
mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over
shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of
its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%
(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains
resilience under both symmetric and asymmetric label noise. In addition,
precomputing encoder features makes per-round cost nearly independent of
encoder size. Together, these results demonstrate that OvA-LP provides a
principled and efficient basis for robust FFT under heterogeneity.

</details>


### [69] [Usando LLMs para Programar Jogos de Tabuleiro e Variações](https://arxiv.org/abs/2511.05114)
*Álvaro Guglielmin Becker,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 测试三种大型语言模型（Claude、DeepSeek和ChatGPT）创建棋盘游戏代码及变体的能力


<details>
  <summary>Details</summary>
Motivation: 利用LLMs高效生成代码的能力来加速棋盘游戏程序的开发过程

Method: 提出方法测试三种LLMs创建棋盘游戏代码和现有游戏变体的能力

Result: 

Conclusion: 

Abstract: Creating programs to represent board games can be a time-consuming task.
Large Language Models (LLMs) arise as appealing tools to expedite this process,
given their capacity to efficiently generate code from simple contextual
information. In this work, we propose a method to test how capable three LLMs
(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as
new variants of existing games.

</details>


### [70] [QuAnTS: Question Answering on Time Series](https://arxiv.org/abs/2511.05124)
*Felix Divo,Maurice Kraus,Anh Q. Nguyen,Hao Xue,Imran Razzak,Flora D. Salim,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: 提出了一个新颖的时间序列问答数据集QuAnTS，专注于人体运动的骨架轨迹数据，填补了时间序列问答研究的空白。


<details>
  <summary>Details</summary>
Motivation: 文本信息可以补充数值时间序列的密度，改善与时间序列模型的交互，但当前问答研究主要关注视觉和文本，时间序列领域缺乏关注。

Method: 创建了QuAnTS数据集，包含关于人体运动骨架轨迹的各种问答对，并通过广泛实验验证数据集的质量和完整性。

Result: 评估了现有和新提出的基线模型，为时间序列问答的深入研究奠定了基础，并提供了人类表现作为实用性的参考标准。

Conclusion: 希望促进通过文本与时间序列模型交互的研究，实现更好的决策支持和更透明的系统。

Abstract: Text offers intuitive access to information. This can, in particular,
complement the density of numerical time series, thereby allowing improved
interactions with time series models to enhance accessibility and
decision-making. While the creation of question-answering datasets and models
has recently seen remarkable growth, most research focuses on question
answering (QA) on vision and text, with time series receiving minute attention.
To bridge this gap, we propose a challenging novel time series QA (TSQA)
dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we
pose a wide variety of questions and answers about human motion in the form of
tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is
well-formed and comprehensive through extensive experiments. Thoroughly
evaluating existing and newly proposed baselines then lays the groundwork for a
deeper exploration of TSQA using QuAnTS. Additionally, we provide human
performances as a key reference for gauging the practical usability of such
models. We hope to encourage future research on interacting with time series
models through text, enabling better decision-making and more transparent
systems.

</details>


### [71] [DL101 Neural Network Outputs and Loss Functions](https://arxiv.org/abs/2511.05131)
*Fernando Berzal*

Main category: cs.LG

TL;DR: 该技术报告分析了神经网络输出层激活函数（线性、sigmoid、ReLU、softmax）与损失函数（MSE、MAE、交叉熵）之间的统计联系，揭示了损失函数选择等价于假设特定输出概率分布，并与广义线性模型相关联。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络输出层激活函数与损失函数之间的统计联系，为选择合适的损失函数提供理论依据，并揭示其与最大似然估计和广义线性模型的深层关系。

Method: 通过数学分析常见激活函数（线性、sigmoid、ReLU、softmax）和损失函数（MSE、MAE、交叉熵）的统计特性，建立它们与最大似然估计和广义线性模型的联系。

Result: 发现损失函数选择等价于假设特定输出概率分布，不同激活函数对应不同的统计分布假设，为损失函数选择提供了统计理论基础。

Conclusion: 神经网络输出层激活函数和损失函数的选择具有深刻的统计意义，与最大似然估计和广义线性模型紧密相关，为深度学习模型的统计解释提供了理论框架。

Abstract: The loss function used to train a neural network is strongly connected to its
output layer from a statistical point of view. This technical report analyzes
common activation functions for a neural network output layer, like linear,
sigmoid, ReLU, and softmax, detailing their mathematical properties and their
appropriate use cases. A strong statistical justification exists for the
selection of the suitable loss function for training a deep learning model.
This report connects common loss functions such as Mean Squared Error (MSE),
Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical
principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss
function is equivalent to assuming a specific probability distribution for the
model output, highlighting the link between these functions and the Generalized
Linear Models (GLMs) that underlie network output layers. Additional scenarios
of practical interest are also considered, such as alternative output
encodings, constrained outputs, and distributions with heavy tails.

</details>


### [72] [Consecutive Preferential Bayesian Optimization](https://arxiv.org/abs/2511.05163)
*Aras Erarslan,Carlos Sevilla Salcedo,Ville Tanskanen,Anni Nisov,Eero Päiväkumpu,Heikki Aisala,Kaisu Honkapää,Arto Klami,Petrus Mikkola*

Main category: cs.LG

TL;DR: 提出Consecutive Preferential Bayesian Optimization方法，在偏好优化中考虑生产和评估成本，通过约束比较使用先前生成的候选来降低生产成本，并引入Just-Noticeable Difference阈值处理感知模糊性。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好优化方法忽略了生成候选解的生产成本，且未考虑评估者感知模糊性，导致在成本高昂或存在感知模糊的场景中效率低下。

Method: 扩展偏好优化框架，显式考虑生产和评估成本；约束比较使用历史候选解；引入JND阈值构建概率偏好模型处理感知模糊性；采用信息论获取策略选择最具信息量的新配置。

Result: 在高生产成本或存在感知模糊反馈的设置中，实验证明该方法显著提高了优化准确性。

Conclusion: 所提出的方法能有效降低偏好优化的生产和评估成本，并在存在感知模糊的情况下保持高精度，为实际应用提供了更实用的解决方案。

Abstract: Preferential Bayesian optimization allows optimization of objectives that are
either expensive or difficult to measure directly, by relying on a minimal
number of comparative evaluations done by a human expert. Generating candidate
solutions for evaluation is also often expensive, but this cost is ignored by
existing methods. We generalize preference-based optimization to explicitly
account for production and evaluation costs with Consecutive Preferential
Bayesian Optimization, reducing production cost by constraining comparisons to
involve previously generated candidates. We also account for the perceptual
ambiguity of the oracle providing the feedback by incorporating a
Just-Noticeable Difference threshold into a probabilistic preference model to
capture indifference to small utility differences. We adapt an
information-theoretic acquisition strategy to this setting, selecting new
configurations that are most informative about the unknown optimum under a
preference model accounting for the perceptual ambiguity. We empirically
demonstrate a notable increase in accuracy in setups with high production costs
or with indifference feedback.

</details>


### [73] [Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy](https://arxiv.org/abs/2511.05169)
*Simon Baur,Tristan Ruhwedel,Ekin Böke,Zuzanna Kobus,Gergana Lishkova,Christoph Wetz,Holger Amthauer,Christoph Roderburg,Frank Tacke,Julian M. Rogasch,Wojciech Samek,Henning Jann,Jackie Ma,Johannes Eschrich*

Main category: cs.LG

TL;DR: 本研究评估了多种模型（实验室指标、影像学和多模态深度学习）用于预测PRRT治疗患者的无进展生存期，发现多模态融合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: PRRT治疗转移性神经内分泌肿瘤时，只有部分患者能获得长期疾病控制，预测无进展生存期有助于个体化治疗规划。

Method: 回顾性单中心研究，纳入116例接受177Lu-DOTATOC治疗的转移性NET患者，收集临床特征、实验室值和治疗前SR-PET/CT影像，训练7种模型分类低/高PFS组。

Result: 多模态融合模型（实验室值+SR-PET+CT+预训练CT分支）表现最佳（AUROC 0.72±0.01），优于单模态模型。

Conclusion: 结合SR-PET、CT和实验室生物标志物的多模态深度学习在PRRT后PFS预测中优于单模态方法，经外部验证后可能支持风险适应随访策略。

Abstract: Peptide receptor radionuclide therapy (PRRT) is an established treatment for
metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs
only in a subset of patients. Predicting progression-free survival (PFS) could
support individualized treatment planning. This study evaluates laboratory,
imaging, and multimodal deep learning models for PFS prediction in PRRT-treated
patients. In this retrospective, single-center study 116 patients with
metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical
characteristics, laboratory values, and pretherapeutic somatostatin receptor
positron emission tomography/computed tomographies (SR-PET/CT) were collected.
Seven models were trained to classify low- vs. high-PFS groups, including
unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches.
Explainability was evaluated by feature importance analysis and gradient maps.
Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1
year). Groups were similar in most characteristics, except for higher baseline
chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT
cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only
on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal
three-dimensional convolutional neural networks using SR-PET or CT performed
worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion
model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch
- achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01).
Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers
outperformed unimodal approaches for PFS prediction after PRRT. Upon external
validation, such models may support risk-adapted follow-up strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型（LLM）的智能体在汽车行业预测性维护（PdM）数据清洗管道中的应用潜力，特别是在处理维护日志中的六种常见噪声类型方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 汽车行业预测性维护面临经济约束、数据集可用性有限和专业人才短缺等挑战，而LLM的进展为克服这些障碍提供了机会，加速PdM从研究向工业实践的过渡。

Method: 研究评估了LLM智能体在维护日志数据清洗任务中的表现，重点关注六种不同类型的噪声处理，包括拼写错误、缺失字段、近似重复条目和错误日期等。

Result: 研究发现LLM在处理通用清洗任务方面表现有效，为未来工业应用提供了有前景的基础。尽管领域特定错误仍然具有挑战性，但这些结果显示了通过专门训练和增强智能体能力进一步改进的潜力。

Conclusion: LLM智能体在预测性维护数据清洗方面展现出显著潜力，特别是在处理通用噪声类型时表现良好。虽然领域特定问题仍需解决，但通过专门化训练和能力增强，有望实现进一步改进。

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [75] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: UA-Code-Bench是一个用于评估语言模型在乌克兰语代码生成和竞争性编程问题解决能力的新基准，包含500个难度不同的问题，测试显示即使是顶级模型也只能解决一半问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多关注从英语翻译的任务或仅评估简单的语言理解能力，缺乏对低资源语言中代码生成能力的全面评估。

Method: 使用Eolymp平台的500个问题，分布在5个难度级别，通过13个领先的专有和开源模型生成Python解决方案，使用一次性提示，在专用环境中进行隐藏测试评估代码正确性。

Result: 即使是表现最好的模型（如OpenAI o3和GPT-5）也只能解决一半的问题，突显了在低资源自然语言中进行代码生成的挑战。

Conclusion: 竞争性编程基准在评估大型语言模型方面具有重要价值，特别是在代表性不足的语言中，为多语言代码生成和推理增强模型的未来研究铺平了道路。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [76] [Awesome graph parameters](https://arxiv.org/abs/2511.05285)
*Kenny Bešter Štorgel,Clément Dallard,Vadim Lozin,Martin Milanič,Viktor Zamaraev*

Main category: math.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: For a graph $G$, we denote by $\alpha(G)$ the size of a maximum independent
set and by $\omega(G)$ the size of a maximum clique in $G$. Our paper lies on
the edge of two lines of research, related to $\alpha$ and $\omega$,
respectively. One of them studies $\alpha$-variants of graph parameters, such
as $\alpha$-treewidth or $\alpha$-degeneracy. The second line deals with graph
classes where some parameters are bounded by a function of $\omega(G)$. A
famous example of this type is the family of $\chi$-bounded classes, where the
chromatic number $\chi(G)$ is bounded by a function of $\omega(G)$.
  A Ramsey-type argument implies that if the $\alpha$-variant of a graph
parameter $\rho$ is bounded by a constant in a class $\mathcal{G}$, then $\rho$
is bounded by a function of $\omega$ in $\mathcal{G}$. If the reverse
implication also holds, we say that $\rho$ is awesome. Otherwise, we say that
$\rho$ is awful. In the present paper, we identify a number of awesome and
awful graph parameters, derive some algorithmic applications of awesomeness,
and propose a number of open problems related to these notions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [77] [The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective](https://arxiv.org/abs/2511.04946)
*Lei Chen,Erci Xu,Yiming Sun,Shengyu Fan,Xianglong Deng,Guiming Shi,Guang Fan,Liang Kong,Yilan Zhu,Shoumeng Yan,Mingzhe Zhang*

Main category: cs.CR

TL;DR: 分析存储I/O对全同态加密(FHE)应用性能的影响，发现存储I/O会显著降低ASIC和GPU的性能表现


<details>
  <summary>Details</summary>
Motivation: 全同态加密允许在加密数据上执行计算，增强用户隐私，但部署FHE应用时的I/O挑战研究不足

Method: 分析存储I/O对FHE应用性能的影响，总结现状中的关键经验教训

Result: 存储I/O可使ASIC性能下降高达357倍，GPU性能下降高达22倍

Conclusion: 存储I/O是FHE应用部署中的重要瓶颈，需要进一步研究和优化

Abstract: Fully Homomorphic Encryption (FHE) allows computations to be performed on
encrypted data, significantly enhancing user privacy. However, the I/O
challenges associated with deploying FHE applications remains understudied. We
analyze the impact of storage I/O on the performance of FHE applications and
summarize key lessons from the status quo. Key results include that storage I/O
can degrade the performance of ASICs by as much as 357$\times$ and reduce GPUs
performance by up to 22$\times$.

</details>


### [78] [Bit-Flipping Attack Exploration and Countermeasure in 5G Network](https://arxiv.org/abs/2511.04882)
*Joon Kim,Chengwei Duan,Sandip Ray*

Main category: cs.CR

TL;DR: 本文研究了5G系统对位翻转攻击的脆弱性，这是一种完整性攻击，攻击者无需解密即可修改加密消息的特定字段。研究在OpenAirInterface平台上验证了攻击的现实可行性，并提出了一种基于密钥流的洗牌防御机制来缓解此类攻击。


<details>
  <summary>Details</summary>
Motivation: 5G通信技术因其高数据速率和低延迟等独特优势而广泛应用于各种应用。然而，现有研究大多关注优化效率和性能，安全考虑未得到同等重视，可能留下关键漏洞未被探索。

Method: 在遵循3GPP技术规范的开源5G平台OpenAirInterface上，对位翻转攻击的现实可行性和影响进行严格测试。攻击者拦截5G网络流量，无需解密即可修改加密消息的特定字段。

Result: 研究发现当前5G加密机制下位翻转攻击是可行的，攻击者无需知道明文，仅需了解某些字段的语义含义或位置即可进行针对性修改。

Conclusion: 5G安全需要增强以更好地保护网络层传输过程中的数据不被篡改。提出的基于密钥流的洗牌防御机制在不增加通信开销的情况下提高了操纵特定加密字段的难度。

Abstract: 5G communication technology has become a vital component in a wide range of
applications due to its unique advantages such as high data rate and low
latency. While much of the existing research has focused on optimizing its
efficiency and performance, security considerations have not received
comparable attention, potentially leaving critical vulnerabilities unexplored.
In this work, we investigate the vulnerability of 5G systems to bit-flipping
attacks, which is an integrity attack where an adversary intercepts 5G network
traffic and modifies specific fields of an encrypted message without
decryption, thus mutating the message while remaining valid to the receiver.
Notably, these attacks do not require the attacker to know the plaintext, and
only the semantic meaning or position of certain fields would be enough to
effect targeted modifications. We conduct our analysis on OpenAirInterface
(OAI), an open-source 5G platform that follows the 3GPP Technical
Specifications, to rigorously test the real-world feasibility and impact of
bit-flipping attacks under current 5G encryption mechanisms. Finally, we
propose a keystream-based shuffling defense mechanism to mitigate the effect of
such attacks by raising the difficulty of manipulating specific encrypted
fields, while introducing no additional communication overhead compared to the
NAS Integrity Algorithm (NIA) in 5G. Our findings reveal that enhancements to
5G security are needed to better protect against attacks that alter data during
transmission at the network level.

</details>


### [79] [A Secured Intent-Based Networking (sIBN) with Data-Driven Time-Aware Intrusion Detection](https://arxiv.org/abs/2511.05133)
*Urslla Uchechi Izuazu,Mounir Bensalem,Admela Jukan*

Main category: cs.CR

TL;DR: 提出了一种安全的基于意图的网络(sIBN)系统，使用机器学习模型进行网络行为异常检测，以保护用户意图免受中间人攻击篡改。


<details>
  <summary>Details</summary>
Motivation: 基于意图的网络(IBN)隐含信任意图数据的完整性，这使其容易受到中间人攻击，攻击者可在意图执行前拦截并篡改，导致网络配置恶意操作。

Method: 开发了意图入侵检测系统，使用机器学习模型进行网络行为异常检测，结合原始行为指标和新设计的时间感知特征，通过随机搜索交叉验证技术优化模型超参数。

Result: 基于真实数据集的数值结果表明，sIBN在二元和多分类任务中均取得最佳性能，同时保持低错误率。

Conclusion: 提出的sIBN系统能有效检测意图篡改，解决了IBN系统中数据完整性假设的安全漏洞问题。

Abstract: While Intent-Based Networking (IBN) promises operational efficiency through
autonomous and abstraction-driven network management, a critical unaddressed
issue lies in IBN's implicit trust in the integrity of intent ingested by the
network. This inherent assumption of data reliability creates a blind spot
exploitable by Man-in-the-Middle (MitM) attacks, where an adversary intercepts
and alters intent before it is enacted, compelling the network to orchestrate
malicious configurations. This study proposes a secured IBN (sIBN) system with
data driven intrusion detection method designed to secure legitimate user
intent from adversarial tampering. The proposed intent intrusion detection
system uses a ML model applied for network behavioral anomaly detection to
reveal temporal patterns of intent tampering. This is achieved by leveraging a
set of original behavioral metrics and newly engineered time-aware features,
with the model's hyperparameters fine-tuned through the randomized search
cross-validation (RSCV) technique. Numerical results based on real-world data
sets, show the effectiveness of sIBN, achieving the best performance across
standard evaluation metrics, in both binary and multi classification tasks,
while maintaining low error rates.

</details>


### [80] [SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks](https://arxiv.org/abs/2511.05156)
*Azhar Hussain Mozumder,M. John Basha,Chayapathi A. R*

Main category: cs.CR

TL;DR: SmartSecChain-SDN是一个结合机器学习入侵检测、区块链日志存储和应用感知优先级的SDN安全平台，通过多种ML算法实时检测网络入侵，使用Hyperledger Fabric确保日志不可篡改，并基于应用类型实现QoS流量控制。


<details>
  <summary>Details</summary>
Motivation: 随着传统网络向SDN转型，需要更安全的网络环境和智能流量控制方法，以应对日益复杂的网络安全威胁和多样化的应用需求。

Method: 采用多种机器学习算法（随机森林、XGBoost、CatBoost、CNN-BiLSTM）进行实时入侵检测，使用Hyperledger Fabric区块链存储IDS日志，基于应用类型实现QoS优先级控制，通过Mininet模拟SDN环境进行原型验证。

Result: 使用InSDN数据集测试表明，该框架能有效识别各类网络攻击，在资源受限条件下实现高效的带宽分配，证明了其检测精度高、误报率低的特性。

Conclusion: SmartSecChain-SDN为SDN系统提供了全面的安全保护和性能增强方案，为下一代可编程网络的网络安全、合规性和管理提供了创新且可扩展的解决方案。

Abstract: With more and more existing networks being transformed to Software-Defined
Networking (SDN), they need to be more secure and demand smarter ways of
traffic control. This work, SmartSecChain-SDN, is a platform that combines
machine learning based intrusion detection, blockchain-based storage of logs,
and application-awareness-based priority in SDN networks. To detect network
intrusions in a real-time, precision and low-false positives setup, the
framework utilizes the application of advanced machine learning algorithms,
namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is
based on the Hyperledger Fabric, which is a permissioned blockchain technology,
to provide secure, scalable, and privacy-preserving storage and, thus,
guarantee that the Intrusion Detection System (IDS) records cannot be altered
and can be analyzed comprehensively. The system also has Quality of Service
(QoS) rules and traffic shaping based on applications, which enables
prioritization of critical services, such as VoIP, video conferencing, and
business applications, as well as de-prioritization of non-essential traffic,
such as downloads and updates. Mininet can simulate real-time SDN scenarios
because it is used to prototype whole architectures. It is also compatible with
controllers OpenDaylight and Ryu. It has tested the framework using the InSDN
dataset and proved that it can identify different kinds of cyberattacks and
handle bandwidth allocation efficiently under circumstances of resource
constraints. SmartSecChain-SDN comprehensively addresses SDN system protection,
securing and enhancing. The proposed study offers an innovative, extensible way
to improve cybersecurity, regulatory compliance, and the administration of
next-generation programmable networks.

</details>


### [81] [Chasing One-day Vulnerabilities Across Open Source Forks](https://arxiv.org/abs/2511.05097)
*Romain Lefeuvre,Charly Reux,Stefano Zacchiroli,Olivier Barais,Benoit Combemale*

Main category: cs.CR

TL;DR: 提出了一种检测分叉仓库中一日漏洞的新方法，通过提交级别的漏洞信息传播和自动化影响分析，识别未合并修复的分叉项目。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞分析方法缺乏提交级别的粒度，无法跟踪跨分叉的漏洞引入和修复，可能导致一日漏洞未被检测。

Method: 利用Software Heritage档案中的公共代码全局图，在提交级别传播漏洞信息并执行自动化影响分析。

Result: 从7162个包含漏洞提交的仓库中识别出220万个分叉，经过严格过滤发现356个漏洞-分叉对，手动评估65对发现3个高严重性漏洞。

Conclusion: 该方法能够有效检测分叉项目中未修复的漏洞，证明了其影响力和适用性。

Abstract: Tracking vulnerabilities inherited from third-party open-source components is
a well-known challenge, often addressed by tracing the threads of dependency
information. However, vulnerabilities can also propagate through forking: a
repository forked after the introduction of a vulnerability, but before it is
patched, may remain vulnerable in the fork well after being fixed in the
original project. Current approaches for vulnerability analysis lack the
commit-level granularity needed to track vulnerability introductions and fixes
across forks, potentially leaving one-day vulnerabilities undetected. This
paper presents a novel approach to help developers identify one-day
vulnerabilities in forked repositories. Leveraging the global graph of public
code, as captured by the Software Heritage archive, the approach propagates
vulnerability information at the commit level and performs automated impact
analysis. This enables automatic detection of forked projects that have not
incorporated fixes, leaving them potentially vulnerable. Starting from 7162
repositories that, according to OSV, include vulnerable commits in their
development histories, we identify 2.2 M forks, containing at least one
vulnerable commit. Then we perform a strict filtering, allowing us to find 356
___vulnerability, fork___ pairs impacting active and popular GitHub forks, we
manually evaluate 65 pairs, finding 3 high-severity vulnerabilities,
demonstrating the impact and applicability of this approach.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [82] [Pixi: Unified Software Development and Distribution for Robotics and AI](https://arxiv.org/abs/2511.04827)
*Tobias Fischer,Wolf Vollprecht,Bas Zalmstra,Ruben Arts,Tim de Jager,Alejandro Fontan,Adam D Hines,Michael Milford,Silvio Traversaro,Daniel Claes,Scarlett Raine*

Main category: cs.RO

TL;DR: Pixi是一个统一的包管理框架，通过项目级锁文件捕获精确依赖状态，实现跨平台的比特级可重现性，解决了机器人学研究中的可重现性危机。


<details>
  <summary>Details</summary>
Motivation: 机器人学研究面临严重的可重现性危机，70%的算法无法被独立团队重现，多语言、硬件-软件工具链碎片化导致依赖地狱问题，阻碍了研究部署和协作。

Method: 开发Pixi统一包管理框架，使用高性能SAT求解器进行依赖解析，集成conda-forge和PyPI生态系统，通过项目级锁文件确保依赖状态精确性。

Result: Pixi依赖解析速度比同类工具快10倍，自2023年以来已被5300多个项目采用，将设置时间从数小时缩短到数分钟，降低了全球研究者的技术门槛。

Conclusion: Pixi通过提供可扩展、可重现的协作研究基础设施，加速了机器人和人工智能领域的进展。

Abstract: The reproducibility crisis in scientific computing constrains robotics
research. Existing studies reveal that up to 70% of robotics algorithms cannot
be reproduced by independent teams, while many others fail to reach deployment
because creating shareable software environments remains prohibitively complex.
These challenges stem from fragmented, multi-language, and hardware-software
toolchains that lead to dependency hell. We present Pixi, a unified
package-management framework that addresses these issues by capturing exact
dependency states in project-level lockfiles, ensuring bit-for-bit
reproducibility across platforms. Its high-performance SAT solver achieves up
to 10x faster dependency resolution than comparable tools, while integration of
the conda-forge and PyPI ecosystems removes the need for multiple managers.
Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours
to minutes and lowers technical barriers for researchers worldwide. By enabling
scalable, reproducible, collaborative research infrastructure, Pixi accelerates
progress in robotics and AI.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [83] [Reconstructing Riemannian Metrics From Random Geometric Graphs](https://arxiv.org/abs/2511.05434)
*Han Huang,Pakawut Jiradilok,Elchanan Mossel*

Main category: math.PR

TL;DR: 本文研究了从黎曼流形上采样的稀疏随机几何图中重建点间黎曼距离的问题，相比之前基于欧氏距离的工作，在稀疏图设置下取得了更强的结果。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要研究从嵌入流形的随机几何图中恢复欧氏距离，而本文考虑更自然的黎曼度量设置，旨在从稀疏图中高效重建黎曼距离。

Method: 提出了一种高效的算法，从平均度为n^{1/2}polylog(n)的稀疏随机几何图中重建黎曼距离，运行时间为O(n^2 polylog(n))。

Result: 算法在稀疏图设置下成功重建黎曼距离，误差接近体积估计的下界，且运行时间与输入图大小匹配（忽略对数因子）。

Conclusion: 在黎曼度量设置下，可以从稀疏随机几何图中高效且准确地重建点间距离，结果优于之前的欧氏距离方法。

Abstract: Random geometric graphs are random graph models defined on metric measure
spaces. A random geometric graph is generated by first sampling points from a
metric space and then connecting each pair of sampled points independently with
a probability that depends on their distance.
  In recent work of Huang, Jiradilok, and Mossel~\cite{HJM24}, the authors
study the problem of reconstructing an embedded manifold form a random
geometric graph sampled from the manifold, where edge probabilities depend
monotonically on the Euclidean distance between the embedded points. They show
that, under mild regularity assumptions on the manifold, the sampling measure,
and the connection probability function, it is possible to recover the pairwise
Euclidean distances of the embedded sampled points up to a vanishing error as
the number of vertices grows.
  In this work we consider a similar and arguably more natural problem where
the metric is the Riemannian metric on the manifold. Again points are sampled
from the manifold and a random graph is generated where the connection
probability is monotone in the Riemannian distance. Perhaps surprisingly we
obtain stronger results in this setup.
  Unlike the previous work that only considered dense graph we provide
reconstruction algorithms from sparse graphs with average degree $n^{1/2}{\rm
polylog}(n)$, where $n$ denotes the number of vertices. Our algorithm is also a
more efficient algorithm for distance reconstruction with improved error
bounds. The running times of the algorithm is
  $O(n^2\,{\rm polylog}(n))$ which up to polylog factor matches the size of the
input graph.
  Our distance error also nearly matches the volumetric lower bounds for
distance estimation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [84] [CUNQA: a Distributed Quantum Computing emulator for HPC](https://arxiv.org/abs/2511.05209)
*Jorge Vázquez-Pérez,Daniel Expósito-Patiño,Marta Losada,Álvaro Carballido,Andrés Gómez,Tomás F. Pena*

Main category: quant-ph

TL;DR: CUNQA是一个开源的分布式量子计算模拟器，专为高性能计算环境设计，用于在真实分布式量子计算实现之前进行测试、评估和研究。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机向多量子处理单元架构发展，以及量子计算机作为加速器集成到高性能计算环境中的趋势，需要工具来模拟分布式量子计算在HPC环境中的运行。

Method: 开发了CUNQA模拟器，实现了三种分布式量子计算模型：无通信、经典通信和量子通信，并使用量子相位估计算法进行演示和分析。

Result: CUNQA是首个在HPC环境中模拟三种分布式量子计算方案的专用工具，能够处理编程考虑、模拟细节和实现具体问题。

Conclusion: CUNQA为分布式量子计算在高性能计算环境中的研究和开发提供了重要的模拟平台，填补了该领域的工具空白。

Abstract: The challenge of scaling quantum computers to gain computational power is
expected to lead to architectures with multiple connected quantum processing
units (QPUs), commonly referred to as Distributed Quantum Computing (DQC). In
parallel, there is a growing momentum toward treating quantum computers as
accelerators, integrating them into the heterogeneous architectures of
high-performance computing (HPC) environments. This work combines these two
foreseeable futures in CUNQA, an open-source DQC emulator designed for HPC
environments that allows testing, evaluating and studying DQC in HPC before it
even becomes real. It implements the three DQC models of no-communication,
classical-communication and quantum-communication; which will be examined in
this work. Addressing programming considerations, explaining emulation and
simulation details, and delving into the specifics of the implementation will
be part of the effort. The well-known Quantum Phase Estimation (QPE) algorithm
is used to demonstrate and analyze the emulation of the models. To the best of
our knowledge, CUNQA is the first tool designed to emulate the three DQC
schemes in an HPC environment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [85] [Story Arena: A Multi-Agent Environment for Envisioning the Future of Software Engineering](https://arxiv.org/abs/2511.05410)
*Justin D. Weisz,Michael Muller,Kush R. Varshney*

Main category: cs.HC

TL;DR: 使用多AI代理系统构建Story Arena，让持有不同观点的AI代理讨论软件工程的未来，并合作创作设计小说《信任之码》，探讨人机协作中的信任、理解等主题。


<details>
  <summary>Details</summary>
Motivation: 通过让AI自身讨论AI对软件工程的影响，以新颖的方式理解这一主题，探索人机协作的未来可能性。

Method: 构建多代理"编剧室"Story Arena，让持有不同立场观点的AI代理进行对话，形成共同愿景，并基于此协作创作设计小说。

Result: 创作出短篇小说《信任之码》，深入探讨了人类理解、信任、内容所有权、增强vs替代、不确定未来等人机共创主题。

Conclusion: 通过AI代理间的对话和协作创作，提供了一种理解AI对软件工程影响的创新视角，展示了人机协作叙事的潜力。

Abstract: What better way to understand the impact of AI on software engineering than
to ask AI itself? We constructed Story Arena, a multi-agent "writer's room" in
which multiple AI agents, independently imbued with a position statement on the
future of software engineering, converse with each other to develop a shared
vision. They then use this shared vision to collaboratively construct a design
fiction that depicts this vision in narrative form. We present "The Code of
Trust," a short fiction that investigates themes of human comprehension, trust,
content ownership, augmentation vs. replacement, and uncertain futures in
human-AI co-creation.

</details>
