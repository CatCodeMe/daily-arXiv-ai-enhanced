<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 63]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.CV](#cs.CV) [Total: 4]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.MA](#cs.MA) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.CR](#cs.CR) [Total: 9]
- [eess.IV](#eess.IV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories](https://arxiv.org/abs/2601.00002)
*Tarek Al Mustafa*

Main category: cs.DB

TL;DR: 该论文提出语义单元（SUs）作为知识图谱中语义显著的有名子图，旨在增强用户认知互操作性，并应用于生物多样性研究的知识图谱构建。


<details>
  <summary>Details</summary>
Motivation: 知识图谱在生态学和生物多样性研究中具有巨大潜力，但SPARQL查询语言对许多用户群体来说难以使用，且用户需求与知识图谱技术需求之间存在根本性脱节，导致知识图谱中的许多陈述对终端用户没有语义意义。

Method: 1) 提出语义单元（SUs）作为知识图谱中有语义意义的命名子图；2) 从德国生物多样性探索研究平台的出版物和数据集元数据构建知识图谱；3) 首次在知识图谱上实现语义单元；4) 使用大语言模型从标题和摘要中提取结构化元数据类别；5) 使用嵌入模型通过潜在信息丰富元数据。

Result: 实现了语义单元在知识图谱上的首次实现，研究了语义单元如何影响知识图谱查询，并展示了两种任务实现：使用LLMs提取结构化元数据类别，以及使用嵌入模型丰富元数据，支持创建结构化且符合FAIR原则的元数据。

Conclusion: 语义单元是解决知识图谱用户交互挑战的有效方法，能够增强认知互操作性，同时结合LLMs和嵌入模型的技术可以支持结构化、FAIR元数据的创建，为生物多样性研究提供计算机科学视角的贡献。

Abstract: Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata.

</details>


### [2] [Database Theory in Action: Yannakakis' Algorithm](https://arxiv.org/abs/2601.00098)
*Paraschos Koutris,Stijn Vansummeren,Qichen Wang,Yisu Remy Wang,Xiangyao Yu*

Main category: cs.DB

TL;DR: 本文简要综述了使Yannakakis算法更实用的最新进展，包括效率和实现便利性方面，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Yannakakis算法在理论上是无环连接的最优算法，但由于实际性能不佳而未被广泛采用。本文旨在探讨如何使该算法在实际应用中更实用。

Method: 采用文献综述方法，简要调查了使Yannakakis算法更实用的最新技术进展，包括算法效率和实现便利性两个维度。

Result: 综述了近期在优化Yannakakis算法实用性的研究成果，展示了该算法在实际应用中性能改进的可能性。

Conclusion: 虽然Yannakakis算法理论最优但实际应用受限，近期研究已取得进展使其更实用，但仍需进一步研究解决剩余挑战。

Abstract: Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research.

</details>


### [3] [Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More](https://arxiv.org/abs/2601.00208)
*David Lomet,Rui Wang*

Main category: cs.DB

TL;DR: 提出一种名为notices的新型无锁技术，通过delta记录更新显著减少浪费工作，解决B树索引维护中的线程切换/停滞问题


<details>
  <summary>Details</summary>
Motivation: 键值存储中线程切换/停滞的高成本是性能瓶颈，主要源于资源争用。传统基于锁的方法通过阻塞线程处理争用，而无锁技术虽然能避免阻塞，但如果需要执行浪费工作则收益有限

Method: 提出notices无锁方法，利用delta记录更新技术，显著减少浪费工作。该方法专门解决B树索引维护问题，避免线程切换或停滞

Result: notices方法能够有效解决B树索引维护中的并发访问问题，避免传统锁机制导致的线程阻塞，同时减少无锁技术中的浪费工作

Conclusion: notices作为一种新的无锁方法，通过delta记录更新显著减少浪费工作，有效解决B树索引维护中的线程切换/停滞问题，为键值存储提供高性能解决方案

Abstract: A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed.

</details>


### [4] [Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches](https://arxiv.org/abs/2601.00304)
*Mouna Ammar,Marvin Hofer,Erhard Rahm*

Main category: cs.DB

TL;DR: 本文对结合图数据和时序数据的现有系统进行了全面综述，将其分为四类架构，分析各系统如何满足不同需求并展示实现特性，帮助读者理解当前选项和权衡。


<details>
  <summary>Details</summary>
Motivation: 随着图数据和时序数据在现实应用中的重要性日益增加，需要系统能够统一处理这两种数据类型。然而，现有系统在架构、集成程度、成熟度和开放性等方面存在差异，缺乏系统的分类和分析框架来指导选择。

Method: 对现有系统进行系统性综述，将其分为四种架构类别，分析各系统如何满足不同需求并展示实现特性，重点关注跨模型集成程度、成熟度和开放性等关键维度。

Result: 提出了一个分类框架，将现有系统分为四类架构，分析了各类系统的特点和适用场景，识别了不同系统在跨模型集成、成熟度、开放性等方面的权衡。

Conclusion: 该综述为读者提供了理解和评估当前图与时序数据结合系统的框架，帮助根据具体需求选择合适的系统，并指出了未来研究和发展方向。

Abstract: We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness.

</details>


### [5] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: KELP是一种基于进化分组树的新型实时日志解析器，能动态适应生产环境变化，相比传统静态模板方法更稳健，并在新设计的复杂基准测试中保持高精度和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有在线日志解析器基于静态模板模型，无法适应生产环境的动态变化，导致模式漂移时解析管道静默失效，造成警报丢失和运维负担。

Method: 提出KELP解析器，采用新型数据结构"进化分组树"，将模板发现视为连续在线聚类过程，树结构随日志到达而动态演化，节点根据频率分布进行分裂、合并和根节点重评估。

Result: KELP在反映现代生产系统结构模糊性的新基准测试中保持高精度，而传统启发式方法失败，且不牺牲吞吐量。

Conclusion: KELP通过进化分组树实现了对生产环境动态变化的适应性，解决了传统静态模板解析器的脆弱性问题，为实时日志分析提供了更稳健的解决方案。

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


### [6] [DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data](https://arxiv.org/abs/2601.00695)
*Chuanyi Lv,Huan Li,Dingyu Yang,Zhongle Xie,Lu Chen,Christian S. Jensen*

Main category: cs.DB

TL;DR: DeXOR：一种新颖的浮点数流压缩框架，通过十进制XOR编码最长公共前缀和后缀，实现高压缩比和快速解压


<details>
  <summary>Details</summary>
Motivation: 随着流式浮点数应用日益广泛，需要高效压缩方案来利用连续数值的相似性，同时应对高精度值或缺乏平滑性的极端情况

Method: 1) 十进制XOR过程编码十进制空间的最长公共前缀和后缀；2) 带容错舍入的缩放截断；3) 针对十进制XOR优化的比特管理策略；4) 鲁棒的异常处理器管理浮点数指数

Result: 在22个数据集评估中，DeXOR超越现有最优方案，压缩比提高15%，解压速度加快20%，同时保持有竞争力的压缩速度，在极端条件下表现稳健

Conclusion: DeXOR框架通过创新的十进制XOR编码和容错机制，实现了浮点数流的高效压缩，在压缩比、解压速度和鲁棒性方面均优于现有方法

Abstract: With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

TL;DR: 该论文结合Serverless计算和MapReduce模型，通过实验寻找执行词频统计任务时最优的Map和Reduce函数数量配置，以提高执行效率和减少时间开销。


<details>
  <summary>Details</summary>
Motivation: 随着高性能、高效率计算需求的增长，云计算的Serverless计算成为研究热点，而MapReduce作为流行的大数据处理模型已在各领域广泛应用。论文受到Serverless框架的高并发性和MapReduce编程模型的鲁棒性启发，希望结合两者来减少词频统计任务的执行时间并提高效率。

Method: 论文采用基于Serverless计算平台的MapReduce编程模型，通过大量实验来寻找特定任务中最优的Map函数和Reduce函数数量配置。针对相同的工作负载，研究不同Map和Reduce函数数量对执行时间和效率的影响。

Result: 实验结果表明，随着Map函数和Reduce函数数量的增加，执行时间会减少，程序整体效率会以不同速率提高。论文发现了针对特定任务的最优Map和Reduce函数数量配置。

Conclusion: 该研究发现的Map和Reduce函数最优数量配置可以帮助企业和程序员找到最优解决方案，提高大数据处理任务的执行效率。

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [8] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

TL;DR: Revati是一个时间扭曲仿真器，通过直接执行真实服务系统代码实现性能建模，无需物理GPU，速度比真实GPU执行快5-17倍，预测误差小于5%。


<details>
  <summary>Details</summary>
Motivation: 部署LLM需要测试数百种服务配置，但每个配置在GPU集群上评估需要数小时且成本高昂。离散事件仿真器虽然更快更便宜，但需要重新实现服务系统的控制逻辑，随着框架演进负担加重。

Method: Revati是一个时间扭曲仿真器，通过拦截CUDA API调用来虚拟化设备管理，允许服务框架在没有物理GPU的情况下运行。系统不执行GPU内核，而是进行时间跳跃——通过预测的内核持续时间快速推进虚拟时间。提出了一个协调协议来同步分布式进程中的这些跳跃，同时保持因果关系。

Result: 在vLLM和SGLang上，Revati在多个模型和并行配置下实现了小于5%的预测误差，同时运行速度比真实GPU执行快5-17倍。

Conclusion: Revati通过直接执行真实服务系统代码实现高性能建模，解决了传统仿真方法需要重新实现控制逻辑的问题，为LLM服务配置评估提供了高效准确的解决方案。

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [9] [Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure](https://arxiv.org/abs/2601.00530)
*Ravi Teja Pagidoju*

Main category: cs.DC

TL;DR: 本文通过开源基准测试代码，系统比较了零售POS工作负载在Google Cloud Platform和Microsoft Azure上的性能表现，发现GCP响应时间快23.0%，而Azure成本效率高71.9%。


<details>
  <summary>Details</summary>
Motivation: 零售业数字化转型加速了云端POS系统的采用，但缺乏针对零售工作负载的平台性能实证研究，特别是小型零售商和研究人员可用的透明评估方法。

Method: 使用免费层云资源，通过实时API端点和开源基准测试代码，建立系统可重复的POS工作负载部署比较方法，测量响应延迟、吞吐量、可扩展性等关键性能指标，并根据实际资源使用和当前公有云定价估算运营成本。

Result: GCP在基准负载下实现23.0%更快的响应时间，而Azure在稳态操作中显示71.9%更高的成本效率。所有表格和图表均直接从代码输出生成，确保实验数据与报告结果一致。

Conclusion: 本研究建立了强大的零售云应用开源基准测试方法，首次提供了跨领先云平台的POS系统特有工作负载的全面代码驱动比较，为商家考虑云端POS实施提供了有用框架。

Abstract: Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.

</details>


### [10] [FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding](https://arxiv.org/abs/2601.00644)
*Yuchen Li,Rui Kong,Zhonghao Lyu,Qiyang Li,Xinran Chen,Hengyi Cai,Lingyong Yan,Shuaiqiang Wang,Jiashu Zhao,Guangxu Zhu,Linghe Kong,Guihai Chen,Haoyi Xiong,Dawei Yin*

Main category: cs.DC

TL;DR: FlexSpec是一个面向边缘-云协同推理的高效通信框架，通过共享主干架构让单一静态边缘草稿模型兼容多种云端目标模型，并采用信道感知自适应推测机制动态调整草稿长度，显著降低通信开销和维护成本。


<details>
  <summary>Details</summary>
Motivation: 在移动和边缘计算环境中部署大语言模型面临设备资源有限、无线带宽稀缺和模型频繁更新的挑战。现有的边缘-云协同推理框架中，推测解码技术虽然能降低延迟，但依赖于边缘和云端模型的紧密耦合，导致频繁的模型同步带来过高的通信开销，限制了在边缘环境中的可扩展性。

Method: FlexSpec采用共享主干架构设计，使单个静态的边缘侧草稿模型能够与一系列演化的云端目标模型保持兼容，从而解耦边缘部署与云端模型更新。此外，开发了信道感知自适应推测机制，根据实时信道状态信息和设备能量预算动态调整推测草稿长度。

Result: 大量实验表明，FlexSpec在推理效率方面优于传统的推测解码方法，显著减少了通信和维护成本，同时适应变化的无线条件和异构设备约束。

Conclusion: FlexSpec通过解耦边缘与云端模型更新，并引入自适应推测机制，为演化中的边缘-云系统提供了一个通信高效的协同推理框架，解决了现有推测解码方法在边缘环境中的可扩展性限制问题。

Abstract: Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [11] [Bounds on Longest Simple Cycles in Weighted Directed Graphs via Optimum Cycle Means](https://arxiv.org/abs/2601.00094)
*Ali Dasdan*

Main category: cs.DS

TL;DR: 利用最优环均值（最小和最大环均值）为有向图中最长简单环问题提供严格代数界和启发式近似，适用于分支定界算法剪枝和精确估计


<details>
  <summary>Details</summary>
Motivation: 寻找有向图中最长简单环是NP难问题，在计算生物学、调度和网络分析中有重要应用。现有多项式时间近似算法仅适用于受限图类，一般界要么宽松要么计算代价高。

Method: 利用可在强多项式时间内计算的最优环均值（最小和最大环均值），推导最长简单环权重和长度的严格代数界和启发式近似。分析这些均值统计量与最长环性质之间的代数关系，并提供最短环的对偶结果。

Result: 在ISCAS基准电路上的实验评估显示：严格代数下界通常较宽松（中位数低于真实值85-93%），而启发式近似的中位数误差仅为6-14%。同时观察到最大权重和最大长度环经常重合，表明长环倾向于积累大权重。

Conclusion: 严格界限为分支定界算法提供多项式时间可计算的剪枝约束，而启发式近似为目标值提供精确估计，在计算效率和准确性之间实现权衡。

Abstract: The problem of finding the longest simple cycle in a directed graph is NP-hard, with critical applications in computational biology, scheduling, and network analysis. While polynomial-time approximation algorithms exist for restricted graph classes, general bounds remain loose or computationally expensive. In this paper, we exploit optimum cycle means (minimum and maximum cycle means), which are computable in strongly polynomial time, to derive both strict algebraic bounds and heuristic approximations for the weight and length of the longest simple cycle. We rigorously analyze the algebraic relationships between these mean statistics and the properties of longest cycles, and present dual results for shortest cycles. While the strict bounds provide polynomial-time computable constraints suitable for pruning search spaces in branch-and-bound algorithms, our proposed heuristic approximations offer precise estimates for the objective value. Experimental evaluation on ISCAS benchmark circuits demonstrates this trade-off: while the strict algebraic lower bounds are often loose (median 85--93% below true values), the heuristic approximations achieve median errors of only 6--14%. We also observe that maximum weight and maximum length cycles frequently coincide, suggesting that long cycles tend to accumulate large weights.

</details>


### [12] [Efficient Algorithms for Adversarially Robust Approximate Nearest Neighbor Search](https://arxiv.org/abs/2601.00272)
*Alexandr Andoni,Themistoklis Haris,Esty Kelman,Krzysztof Onak*

Main category: cs.DS

TL;DR: 该论文研究了自适应对抗环境下近似最近邻搜索问题，针对高维和低维两种场景提出了新算法，结合了公平性、差分隐私和LSH技术来对抗控制数据集和查询序列的强敌手。


<details>
  <summary>Details</summary>
Motivation: 研究在强大自适应敌手控制下的近似最近邻搜索问题，该敌手既能控制数据集又能控制查询序列。传统ANN算法在这种对抗环境下容易受到攻击，需要新的安全保证机制。

Method: 1. 高维场景(d=ω(√Q))：建立自适应安全与公平性的联系，利用公平ANN搜索隐藏内部随机性；使用差分隐私机制在LSH数据结构上实现鲁棒决策；提出同心环LSH构造打破√n查询时间障碍
2. 低维场景(d=O(√Q))：提出专门算法提供"forall"保证，确保每个查询都正确；引入新的度量覆盖构造，改进汉明空间和ℓ_p空间的ANN方法

Result: 1. 高维场景：通过结合公平性和差分隐私技术，实现了对抗自适应敌手的ANN搜索，打破了√n查询时间障碍
2. 低维场景：实现了强"forall"保证，在汉明空间和ℓ_p空间改进了现有ANN结果
3. 提出了一种鲁棒释放底层算法实例时序信息的新方法

Conclusion: 该论文通过创新性地结合公平性、差分隐私和LSH技术，为自适应对抗环境下的ANN问题提供了有效的解决方案，在高维和低维场景都取得了理论上的突破，改进了现有结果并打破了查询时间障碍。

Abstract: We study the Approximate Nearest Neighbor (ANN) problem under a powerful adaptive adversary that controls both the dataset and a sequence of $Q$ queries.
  Primarily, for the high-dimensional regime of $d = ω(\sqrt{Q})$, we introduce a sequence of algorithms with progressively stronger guarantees. We first establish a novel connection between adaptive security and \textit{fairness}, leveraging fair ANN search to hide internal randomness from the adversary with information-theoretic guarantees. To achieve data-independent performance, we then reduce the search problem to a robust decision primitive, solved using a differentially private mechanism on a Locality-Sensitive Hashing (LSH) data structure. This approach, however, faces an inherent $\sqrt{n}$ query time barrier. To break the barrier, we propose a novel concentric-annuli LSH construction that synthesizes these fairness and differential privacy techniques. The analysis introduces a new method for robustly releasing timing information from the underlying algorithm instances and, as a corollary, also improves existing results for fair ANN.
  In addition, for the low-dimensional regime $d = O(\sqrt{Q})$, we propose specialized algorithms that provide a strong ``for-all'' guarantee: correctness on \textit{every} possible query with high probability. We introduce novel metric covering constructions that simplify and improve prior approaches for ANN in Hamming and $\ell_p$ spaces.

</details>


### [13] [Deterministic Coreset for Lp Subspace](https://arxiv.org/abs/2601.00361)
*Rachit Chhaya,Anirban Dasgupta,Dan Feldman,Supratim Shit*

Main category: cs.DS

TL;DR: 提出首个迭代算法构建确定性ℓ_p子空间嵌入的ε-coreset，移除对数因子，达到最优大小


<details>
  <summary>Details</summary>
Motivation: 解决ℓ_p子空间嵌入coreset构造中长期存在的对数因子问题，提供确定性保证的迭代算法

Method: 设计迭代算法，每轮确保维护集的损失在原数据集损失上下界内，通过有界损失实现确定性ℓ_p子空间嵌入保证

Result: 算法在O(poly(n,d,ε⁻¹))时间内返回大小为O(d^{max{1,p/2}}/ε²)的确定性ε-coreset，移除对数因子达到下界最优

Conclusion: 首次实现确定性ℓ_p子空间嵌入coreset的最优构造，解决长期开放问题，可用于ℓ_p回归问题的确定性近似求解

Abstract: We introduce the first iterative algorithm for constructing a $\varepsilon$-coreset that guarantees deterministic $\ell_p$ subspace embedding for any $p \in [1,\infty)$ and any $\varepsilon > 0$. For a given full rank matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$ where $n \gg d$, $\mathbf{X}' \in \mathbb{R}^{m \times d}$ is an $(\varepsilon,\ell_p)$-subspace embedding of $\mathbf{X}$, if for every $\mathbf{q} \in \mathbb{R}^d$, $(1-\varepsilon)\|\mathbf{Xq}\|_{p}^{p} \leq \|\mathbf{X'q}\|_{p}^{p} \leq (1+\varepsilon)\|\mathbf{Xq}\|_{p}^{p}$. Specifically, in this paper, $\mathbf{X}'$ is a weighted subset of rows of $\mathbf{X}$ which is commonly known in the literature as a coreset. In every iteration, the algorithm ensures that the loss on the maintained set is upper and lower bounded by the loss on the original dataset with appropriate scalings. So, unlike typical coreset guarantees, due to bounded loss, our coreset gives a deterministic guarantee for the $\ell_p$ subspace embedding. For an error parameter $\varepsilon$, our algorithm takes $O(\mathrm{poly}(n,d,\varepsilon^{-1}))$ time and returns a deterministic $\varepsilon$-coreset, for $\ell_p$ subspace embedding whose size is $O\left(\frac{d^{\max\{1,p/2\}}}{\varepsilon^{2}}\right)$. Here, we remove the $\log$ factors in the coreset size, which had been a long-standing open problem. Our coresets are optimal as they are tight with the lower bound. As an application, our coreset can also be used for approximately solving the $\ell_p$ regression problem in a deterministic manner.

</details>


### [14] [Mind the Gap. Doubling Constant Parametrization of Weighted Problems: TSP, Max-Cut, and More](https://arxiv.org/abs/2601.00768)
*Mihail Stoian*

Main category: cs.DS

TL;DR: 提出一种新方法，将无权重问题的算法重新用于有权重问题，通过Freiman定理将权重转换为多项式有界整数，避免伪多项式因子


<details>
  <summary>Details</summary>
Motivation: 有权重NP难问题通常比无权重版本更难解决，现有方法通过多项式嵌入引入伪多项式因子，导致处理任意权重实例时不实用

Method: 使用Randolph和Węgrzycki的构造性Freiman定理将输入权重转换为多项式有界整数，然后应用多项式嵌入，开发元算法

Result: 当输入权重集合具有小倍增性质时，TSP、Weighted Max-Cut、Edge-Weighted k-Clique等问题的复杂度与其无权重版本成比例

Conclusion: 提出了一种有效的方法将无权重算法重新用于有权重问题，特别适用于权重集合具有小倍增性质的场景

Abstract: Despite much research, hard weighted problems still resist super-polynomial improvements over their textbook solution. On the other hand, the unweighted versions of these problems have recently witnessed the sought-after speedups. Currently, the only way to repurpose the algorithm of the unweighted version for the weighted version is to employ a polynomial embedding of the input weights. This, however, introduces a pseudo-polynomial factor into the running time, which becomes impractical for arbitrarily weighted instances.
  In this paper, we introduce a new way to repurpose the algorithm of the unweighted problem. Specifically, we show that the time complexity of several well-known NP-hard problems operating over the $(\min, +)$ and $(\max, +)$ semirings, such as TSP, Weighted Max-Cut, and Edge-Weighted $k$-Clique, is proportional to that of their unweighted versions when the set of input weights has small doubling. We achieve this by a meta-algorithm that converts the input weights into polynomially bounded integers using the recent constructive Freiman's theorem by Randolph and Węgrzycki [ESA 2024] before applying the polynomial embedding.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Understanding Security Risks of AI Agents' Dependency Updates](https://arxiv.org/abs/2601.00205)
*Tanmay Singla,Berk Çakar,Paschal C. Amusuo,James C. Davis*

Main category: cs.SE

TL;DR: AI编程代理在依赖项变更中比人类更频繁选择已知漏洞版本（2.46% vs 1.64%），且修复难度更大，导致净漏洞增加而非减少


<details>
  <summary>Details</summary>
Motivation: 随着AI编程代理通过PR修改软件，需要了解其依赖决策是否引入独特安全风险，以改进软件供应链安全

Method: 研究117,062个依赖项变更，涵盖7个生态系统，比较AI代理和人类作者PR中的依赖决策安全影响

Result: AI代理选择已知漏洞版本频率更高，修复更困难（36.8%需主版本升级vs人类12.9%），净漏洞增加98个，而人类减少1,316个

Conclusion: 需要在PR时进行漏洞筛查，并建立注册表感知的防护机制，使AI驱动的依赖更新更安全

Abstract: Package dependencies are a critical control point in modern software supply chains. Dependency changes can substantially alter a project's security posture. As AI coding agents increasingly modify software via pull requests, it is unclear whether their dependency decisions introduce distinct security risks.
  We study 117,062 dependency changes from agent- and human-authored pull requests across seven ecosystems. Agents select known-vulnerable versions more often than humans (2.46% vs. 1.64%), and their vulnerable selections are more disruptive to remediate, with 36.8% requiring major-version upgrades compared to 12.9% for humans, despite patched alternatives existing in most cases. At the aggregate level, agent-driven dependency work yields a net vulnerability increase of 98, whereas human-authored work yields a net reduction of 1,316. These findings motivate pull-request-time vulnerability screening and registry-aware guardrails to make agent-driven dependency updates safer.

</details>


### [16] [Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities](https://arxiv.org/abs/2601.00235)
*Victor Wen,Zedong Peng*

Main category: cs.SE

TL;DR: 开发了一个先进的Log4j扫描工具，通过评估软件的实际可利用性来减少误报，集成到GitHub Actions中实现自动化持续扫描


<details>
  <summary>Details</summary>
Motivation: Log4Shell漏洞披露后，仍有大量下载包含易受攻击的包，现有检测工具主要关注Log4j版本识别，导致大量误报，因为它们不检查软件是否真的可被恶意利用

Method: 开发先进的Log4j扫描工具，首先识别漏洞，然后提供针对性的缓解建议和即时反馈，通过GitHub Actions实现自动化持续扫描能力

Result: 评估了28个开源软件项目的不同版本，在140次扫描样本中达到91.4%的准确率，GitHub Action实现已在GitHub市场可用

Conclusion: 该工具为检测和缓解开源项目中的漏洞提供了可靠方法，通过集成到现有开发工作流中实现实时监控和快速响应潜在威胁

Abstract: Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.

</details>


### [17] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

TL;DR: 本研究比较了三种LLM驱动的软件漏洞检测方法：RAG、SFT和双代理框架，发现RAG方法在准确率和F1分数上表现最佳，强调了领域知识增强对实际应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展为自动化软件漏洞检测提供了新机遇，这是保护现代代码库的关键任务。本研究旨在比较不同LLM技术在漏洞检测中的有效性。

Method: 研究评估了三种方法：1）检索增强生成（RAG），整合互联网和MITRE CWE数据库的外部领域知识；2）监督微调（SFT），使用参数高效的QLoRA适配器；3）双代理LLM框架，其中第二个代理审计和优化第一个代理的输出。使用从Big-Vul和GitHub真实代码库中整理的包含五个关键CWE类别的数据集进行评估。

Result: RAG方法取得了最高的整体准确率（0.86）和F1分数（0.85）。SFT方法也表现出色。双代理系统在提高推理透明度和错误缓解方面显示出潜力，同时减少了资源开销。

Conclusion: 研究结果表明，整合领域专业知识机制显著增强了LLM在实际漏洞检测任务中的适用性，RAG方法因其上下文增强能力而表现最佳。

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [18] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: InlineCoder：一种新颖的仓库级代码生成框架，通过将未完成函数内联到其调用图中，将复杂的仓库理解任务转化为更简单的函数级编码任务。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码生成方法（如RAG或基于上下文的函数选择）主要依赖表面相似性，难以捕捉仓库级语义中丰富的依赖关系，导致在理解整个仓库和跨函数、类、模块的复杂依赖推理方面表现不足。

Method: InlineCoder首先根据函数签名生成一个称为"锚点"的草稿完成，近似下游依赖并支持基于困惑度的置信度估计。然后进行双向内联过程：(1)上游内联：将锚点嵌入到其调用者中以捕捉多样化使用场景；(2)下游检索：将锚点的被调用者集成到提示中以提供精确的依赖上下文。最终结合草稿完成、上游和下游视角的丰富上下文为LLM提供全面的仓库视图。

Result: 论文未提供具体实验结果，但方法理论上能够通过将仓库级理解转化为函数级任务，结合双向依赖分析，为LLM提供更全面的上下文，从而提升仓库级代码生成的质量。

Conclusion: InlineCoder通过创新的双向内联方法，将复杂的仓库级代码生成任务转化为更易处理的函数级任务，能够更好地捕捉仓库中的依赖关系，为仓库级代码生成提供了新的解决方案。

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [19] [On Plagiarism and Software Plagiarism](https://arxiv.org/abs/2601.00429)
*Rares Folea,Emil Slusanschi*

Main category: cs.SE

TL;DR: 本文探讨软件相似性自动检测的复杂性，介绍开源软件Project Martial用于检测代码相似性，综述现有反软件抄袭方法，并基于可用工件分类检测挑战。


<details>
  <summary>Details</summary>
Motivation: 软件相似性检测面临数字工件的独特挑战，现有反软件抄袭方法需要系统梳理，学术界和法律界对软件版权侵权的理解需要整合，以开发有效的检测工具。

Method: 1) 分析学术界和法律界现有反软件抄袭方法；2) 基于可用工件分类检测挑战；3) 综述文献中的检测技术（指纹识别、软件胎记、代码嵌入等）；4) 在Project Martial中应用部分技术。

Result: 介绍了开源软件Project Martial作为代码相似性检测解决方案，系统梳理了软件相似性检测的挑战分类和技术方法，为实际应用提供了技术框架。

Conclusion: 软件相似性检测是复杂但重要的问题，Project Martial提供了一个开源解决方案框架，整合了多种检测技术，有助于应对软件抄袭和版权侵权问题。

Abstract: This paper explores the complexities of automatic detection of software similarities, in relation to the unique challenges of digital artifacts, and introduces Project Martial, an open-source software solution for detecting code similarity. This research enumerates some of the existing approaches to counter software plagiarism by examining both the academia and legal landscape, including notable lawsuits and court rulings that have shaped the understanding of software copyright infringements in commercial applications. Furthermore, we categorize the classes of detection challenges based on the available artifacts, and we provide a survey of the previously studied techniques in the literature, including solutions based on fingerprinting, software birthmarks, or code embeddings, and exemplify how a subset of them can be applied in the context of Project Martial.

</details>


### [20] [DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis](https://arxiv.org/abs/2601.00469)
*Negin Ayoughi,David Dewar,Shiva Nejati,Mehrdad Sabetzadeh*

Main category: cs.SE

TL;DR: EXEOS是一个基于LLM的方法，能从自然语言描述生成AMPL模型和Python代码，并通过求解器反馈迭代优化。研究表明，对于数学优化领域，AMPL在LLM生成中与Python相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程（MDE）虽然提供抽象和分析严谨性，但在许多领域的工业采用受到模型开发和维护成本的限制。LLMs可以通过从自然语言描述直接生成模型来帮助改变这种成本平衡。然而，对于领域特定语言（DSL），LLM生成的模型可能不如主流语言（如Python）生成的代码准确，因为后者在LLM训练语料库中占主导地位。

Method: 提出了EXEOS方法：基于LLM从自然语言问题描述生成AMPL模型和Python代码，并利用求解器反馈进行迭代优化。使用公开优化数据集和工业合作伙伴Kinaxis的实际供应链案例进行评估。

Result: 在两个LLM家族的消融研究中显示，AMPL在可执行性和正确性方面与Python相当，有时甚至更好。EXEOS中的设计选择提高了生成规范的质量。

Conclusion: 对于数学优化领域，AMPL这种DSL在LLM生成中表现出与Python相当的竞争力，挑战了主流语言在LLM生成中必然更优的假设。EXEOS方法通过迭代优化有效提高了生成模型的质量。

Abstract: Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.

</details>


### [21] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

TL;DR: 提出了首个多智能体框架来自动化协调重命名，通过范围推断、计划执行和复制三个智能体协作，在保持开发者控制权的同时显著减轻协调重命名的负担。


<details>
  <summary>Details</summary>
Motivation: 协调重命名是软件开发中频繁但具有挑战性的任务，需要跨多个文件和上下文手动传播重命名重构，既繁琐又容易出错。现有启发式方法产生过多误报，而普通大语言模型由于上下文限制和无法与重构工具交互，只能提供不完整的建议。

Method: 设计了首个多智能体框架：范围推断智能体将开发者的初始重构线索转化为明确的自然语言声明范围；计划执行智能体使用该范围作为严格计划，识别需要重构的程序元素，并通过调用IDE的可信重构API安全执行更改；复制智能体指导项目范围内的搜索。

Result: 通过对100个开源项目的609K次提交进行形成性研究，并调查了205名开发者，展示了该框架的有效性。AI智能体能够扩展开发者的推理和行动能力，而不是取代人类参与。

Conclusion: 该多智能体框架成功自动化了协调重命名任务，在保持开发者控制权的同时显著减轻了他们的负担，展示了AI智能体如何与开发者协同工作以扩展其能力。

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [22] [STELLAR: A Search-Based Testing Framework for Large Language Model Applications](https://arxiv.org/abs/2601.00497)
*Lev Sorokin,Ivan Vasilev,Ken E. Friedl,Andrea Stocco*

Main category: cs.SE

TL;DR: STELLAR是一个基于搜索的自动化测试框架，用于发现LLM应用中导致不当响应的文本输入，通过进化优化探索特征组合，比现有方法多暴露2.5-4.3倍的故障。


<details>
  <summary>Details</summary>
Motivation: LLM应用在客服、教育、出行等领域广泛应用，但容易产生不准确、虚构或有害的响应。其高维输入空间使得系统测试特别困难，需要自动化方法来发现导致不当响应的输入。

Method: 将测试生成建模为优化问题，将输入空间离散化为风格、内容和扰动特征，使用进化优化动态探索更可能暴露故障的特征组合，而不是依赖提示优化或覆盖率启发式方法。

Result: 在三个LLM对话问答系统上评估：安全基准测试（公共和专有LLM）、开源和工业级检索增强的车内场所推荐系统。STELLAR比现有基线方法多暴露2.5-4.3倍的故障。

Conclusion: STELLAR框架通过进化优化方法有效发现LLM应用中的故障，显著优于现有测试方法，为LLM系统的可靠性测试提供了有效解决方案。

Abstract: Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.

</details>


### [23] [SEMODS: A Validated Dataset of Open-Source Software Engineering Models](https://arxiv.org/abs/2601.00635)
*Alexandra González,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: SEMODS是一个包含3,427个Hugging Face模型的软件工程专用数据集，通过自动收集和人工标注结合LLM辅助验证构建，将模型与软件开发周期任务关联，支持数据分析、模型发现、基准测试和模型适配等应用。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件工程中的应用日益广泛，需要专门的模型目录来识别适合SE任务的模型。Hugging Face上有数百万个模型且持续增长，手动识别SE模型不可行，因此需要构建专门的SE模型数据集。

Method: 从Hugging Face自动提取模型数据，结合人工标注和大语言模型辅助进行严格验证，构建包含3,427个模型的SE专用数据集。数据集将模型与软件开发生命周期中的任务和活动关联，提供标准化的评估结果表示。

Result: 创建了SEMODS数据集，包含3,427个经过验证的SE模型，支持多种应用场景：数据分析、模型发现、基准测试和模型适配。数据集提供了模型与SE任务的系统化关联。

Conclusion: SEMODS填补了SE领域专用模型目录的空白，为AI在软件工程中的应用提供了系统化的模型资源，支持SE社区更有效地发现、评估和适配AI模型。

Abstract: Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.

</details>


### [24] [Early-Stage Prediction of Review Effort in AI-Generated Pull Requests](https://arxiv.org/abs/2601.00753)
*Dao Sy Duy Minh,Huynh Trung Kiet,Tran Chi Nguyen,Nguyen Lam Phu Quy,Phu Hoa Pham,Nguyen Dinh Ha Duong,Truong Bao Tran*

Main category: cs.SE

TL;DR: AI代理生成的PR存在两极分化模式：28.3%即时合并，其余常陷入迭代审查循环。提出基于静态结构特征的Circuit Breaker模型，可提前预测高审查成本PR，AUC达0.957。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理从代码补全工具转变为能大规模提交PR的完整团队成员，软件维护者面临新挑战：不仅要审查代码，还要管理与非人类贡献者的复杂交互循环。需要预测哪些AI生成的PR会在开始人类交互前就消耗过多审查精力。

Method: 分析AIDev数据集中33,707个AI生成的PR和2,807个仓库，发现两极行为模式。提出Circuit Breaker分类模型，使用静态结构特征（而非语义文本特征）预测高审查成本PR，采用LightGBM模型在时间分割上评估。

Result: 发现28.3%的PR在1分钟内合并（即时合并），其余常陷入迭代审查循环。Circuit Breaker模型AUC达0.957，在20%审查预算下能拦截69%的总审查工作量。语义特征（TF-IDF、CodeBERT）预测价值可忽略。

Conclusion: AI辅助代码审查的主流假设受到挑战：审查负担由AI修改的内容结构决定，而非其语义描述。需要结构性治理机制来管理人类-AI协作，实现零延迟治理。

Abstract: As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?
  Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).
  We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.
  Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [25] [Delay-Tolerant Networking for Tsunami Evacuation on the Small Island of Hachijojima: A Study of Epidemic and Prophet Routing](https://arxiv.org/abs/2601.00109)
*Keiya Kawano,Milena Radenkovic*

Main category: cs.NI

TL;DR: 评估两种延迟容忍网络路由方案在海啸疏散场景中的多标准性能特征


<details>
  <summary>Details</summary>
Motivation: 海啸灾害对沿海和岛屿社区构成严重威胁。大地震发生时，通信基础设施受损，传统信息共享渠道失效，而人们携带的移动设备可以形成延迟容忍网络进行信息传递。

Method: 使用日本八丈岛海啸疏散场景作为案例研究，评估两种DTN路由方案的多标准性能特征。

Result: 论文评估了两种路由方案在极端通信中断情况下的性能，但具体结果未在摘要中说明。

Conclusion: 在传统通信基础设施失效时，移动设备形成的延迟容忍网络可以作为海啸疏散中的替代通信方案。

Abstract: Tsunami disasters pose a serious and recurring threat to coastal and island communities. When a large earthquake occurs, people are forced to make evacuation decisions under extreme time pressure, often at the same time as the communication infrastructure is damaged or completely lost. In such circumstances, the familiar channels for sharing information - cellular networks, the internet, and even landlines - can no longer be relied upon. What typically remains are the mobile devices that evacuees carry with them. These devices can form Delay Tolerant Networks (DTNs), in which messages are forwarded opportunistically whenever people come into contact. To explore this, we evaluate multi-criteria performance characteristics of two DTN routing schemes in a pre-tsunami evacuation scenario for the island of Hachijojima, Japan use case.

</details>


### [26] [CTMap: LLM-Enabled Connectivity-Aware Path Planning in Millimeter-Wave Digital Twin Networks](https://arxiv.org/abs/2601.00110)
*Md Salik Parwez,Sai Teja Srivillibhutturu,Sai Venkat Reddy Kopparthi,Asfiya Misba,Debashri Roy,Habeeb Olufowobi,Charles Kim*

Main category: cs.NI

TL;DR: CTMAP是一个基于大语言模型的数字孪生框架，用于毫米波无线网络中的连接感知路径导航，通过数字孪生模拟和LLM推理优化信号强度而非传统距离指标。


<details>
  <summary>Details</summary>
Motivation: 传统导航工具只优化距离、时间或成本，忽略了密集城市环境中信号阻塞导致的网络连接质量下降问题，特别是在毫米波网络中信号易受阻挡。

Method: 使用OpenStreetMap、Blender和NVIDIA Sionna的射线追踪引擎构建毫米波网络的数字孪生，模拟真实接收信号强度地图；采用改进的Dijkstra算法生成最大化累积RSS的最优路径；基于GPT-4进行指令调优推理。

Result: CTMAP相比最短距离基线实现了高达10倍的累积信号强度提升，同时保持高路径有效性；支持语义化路径查询并返回用户可理解的连接优化路径。

Conclusion: 数字孪生模拟与LLM推理的结合为智能、可解释、连接驱动的导航建立了可扩展基础，推动了AI赋能的6G移动系统设计。

Abstract: In this paper, we present \textit{CTMAP}, a large language model (LLM) empowered digital twin framework for connectivity-aware route navigation in millimeter-wave (mmWave) wireless networks. Conventional navigation tools optimize only distance, time, or cost, overlooking network connectivity degradation caused by signal blockage in dense urban environments. The proposed framework constructs a digital twin of the physical mmWave network using OpenStreetMap, Blender, and NVIDIA Sionna's ray-tracing engine to simulate realistic received signal strength (RSS) maps. A modified Dijkstra algorithm then generates optimal routes that maximize cumulative RSS, forming the training data for instruction-tuned GPT-4-based reasoning. This integration enables semantic route queries such as ``find the strongest-signal path'' and returns connectivity-optimized paths that are interpretable by users and adaptable to real-time environmental updates. Experimental results demonstrate that CTMAP achieves up to a tenfold improvement in cumulative signal strength compared to shortest-distance baselines, while maintaining high path validity. The synergy of digital twin simulation and LLM reasoning establishes a scalable foundation for intelligent, interpretable, and connectivity-driven navigation, advancing the design of AI-empowered 6G mobility systems.

</details>


### [27] [A-FC: An Activity-Based Delay Tolerant Routing Protocol for Improving Future School Campus Emergency Communications](https://arxiv.org/abs/2601.00148)
*Chengjun Jiang,Milena Radenkovic*

Main category: cs.NI

TL;DR: 提出基于活动的首次接触协议，利用校园人员社会角色提升灾后通信可靠性


<details>
  <summary>Details</summary>
Motivation: 校园应急通信系统对台风等灾害下的学生安全至关重要，传统DTN协议在高延迟、低投递率场景下表现不佳

Method: 提出活动型首次接触协议，利用现实社会角色，强制将消息上传给高度活跃的"工作人员节点"以克服网络分区

Result: 在福州一中拓扑的真实评估场景中，A-FC协议显著优于基线，达到约68%的消息投递概率，平均延迟降至4311秒，平均跳数仅1.68

Conclusion: 该协议为校园灾害响应建立了一个低成本、高可靠性的备用通信模型

Abstract: School Campus emergency communication systems are vital for safeguarding student safety during sudden disasters such as typhoons, which frequently cause widespread paralysis of communication infrastructure. Traditional Delay-Tolerant Network (DTN) protocols, such as Direct Delivery and First Contact, struggle to maintain reliable connections in such scenarios due to high latency and low delivery rates. This paper proposes the Activity-based First Contact (A-FC) protocol, an innovative routing scheme that leverages real-world social roles to overcome network partitioning by mandatorily uploading messages to highly active "staff nodes". We constructed a real-world evaluation scenario based on the topology of Fuzhou No. 1 Middle School. Simulation results demonstrate that the A-FC protocol significantly outperforms baseline protocols, achieving approximately 68% message delivery probability and reducing average delay to 4311 seconds. With an average hop count of merely 1.68, this protocol establishes a low-cost, highly reliable backup communication model for school campus disaster response.

</details>


### [28] [Multi-Satellite NOMA-Irregular Repetition Slotted ALOHA for IoT Networks](https://arxiv.org/abs/2601.00341)
*Estefanía Recayte,Carla Amatetti*

Main category: cs.NI

TL;DR: 论文评估了在卫星网络中采用多接收器对IoT节点使用NOMA-IRSA协议共享信道的影响，发现即使只增加一个额外卫星接收器也能显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着5G向6G过渡，IoT设备数量激增，需要整合非地面网络(NTN)来满足高容量需求。但卫星覆盖范围广会导致大量用户同时接入信道，增加碰撞概率。同时，巨型星座部署使得地面用户能同时看到多个卫星，实现接收器分集。

Method: 在IoT节点使用非正交多址接入(NOMA)不规则重复时隙ALOHA(IRSA)协议共享信道的场景下，评估多接收器的影响。考虑卫星信道损伤，推导系统性能下界作为网络行为快速评估工具，并分析网络设计参数间的权衡。

Result: 推导出系统性能下界，识别了网络设计参数在丢包率和能量效率方面的权衡。特别值得注意的是，即使只增加一个额外卫星作为接收器，也能显著提升整体系统性能。

Conclusion: 多接收器配置在卫星IoT网络中具有重要价值，通过接收器分集能有效提升系统性能。论文提出的性能下界为网络设计提供了快速评估工具，揭示了网络参数优化的关键权衡。

Abstract: As the transition from 5G to 6G unfolds, a substantial increase in Internet of Things (IoT) devices is expected, enabling seamless and pervasive connectivity across various applications. Accommodating this surge and meeting the high capacity demands will necessitate the integration of NonTerrestrial Networks (NTNs). However, the extensive coverage area of satellites, relative to terrestrial receivers, will lead to a high density of users attempting to access the channel at the same time, increasing the collision probability. In turn, the deployment of mega constellations make it possible for ground users to be in visibility of more than one satellite at the same time, enabling receiver diversity. Therefore, in this paper, we evaluate the impact of multi-receivers in scenarios where IoT nodes share the channel following a non-orthogonal multiple access (NOMA)irregular repetition slotted ALOHA (IRSA) protocol. Considering the impairments of satellite channels, we derive a lower bound of system performance, serving as a fast tool for initial evaluation of network behavior. Additionally, we identify the trade-offs inherent to the network design parameters, with a focus on packet loss rate and energy efficiency. Notably, in the visibility of only one extra satellite as receiver yields significant gains in overall system performance.

</details>


### [29] [MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Observability](https://arxiv.org/abs/2601.00481)
*Tie Ma,Yixi Chen,Vaastav Anand,Alessandro Cornacchia,Amândio R. Faustino,Guanheng Liu,Shan Zhang,Hongbin Luo,Suhaib A. Fahmy,Zafar A. Qazi,Marco Canini*

Main category: cs.NI

TL;DR: MAESTRO是一个用于评估基于LLM的多智能体系统（MAS）的测试、可靠性和可观测性的评估套件，通过标准化配置、统一接口和框架无关的执行跟踪来系统评估智能体系统。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多智能体系统（MAS）的快速发展，缺乏系统化的评估框架来测试其可靠性、可观测性和性能。现有评估方法分散且缺乏标准化，难以对不同的MAS架构进行公平比较和分析。

Method: MAESTRO通过统一接口标准化MAS配置和执行，支持集成原生和第三方MAS（通过示例仓库和轻量适配器），导出框架无关的执行跟踪和系统级信号（延迟、成本、故障等）。使用12个代表性MAS实例化，在重复运行、后端模型和工具配置上进行受控实验。

Result: 研究发现：1）MAS执行在结构上稳定但在时间上可变，导致运行间性能和可靠性存在显著差异；2）MAS架构是资源概况、可重现性以及成本-延迟-准确性权衡的主导因素，通常超过后端模型或工具设置的变化影响。

Conclusion: MAESTRO能够实现系统化评估，并为设计和优化智能体系统提供实证指导，强调了MAS架构在系统性能中的关键作用。

Abstract: We present MAESTRO, an evaluation suite for the testing, reliability, and observability of LLM-based MAS. MAESTRO standardizes MAS configuration and execution through a unified interface, supports integrating both native and third-party MAS via a repository of examples and lightweight adapters, and exports framework-agnostic execution traces together with system-level signals (e.g., latency, cost, and failures). We instantiate MAESTRO with 12 representative MAS spanning popular agentic frameworks and interaction patterns, and conduct controlled experiments across repeated runs, backend models, and tool configurations. Our case studies show that MAS executions can be structurally stable yet temporally variable, leading to substantial run-to-run variance in performance and reliability. We further find that MAS architecture is the dominant driver of resource profiles, reproducibility, and cost-latency-accuracy trade-off, often outweighing changes in backend models or tool settings. Overall, MAESTRO enables systematic evaluation and provides empirical guidance for designing and optimizing agentic systems.

</details>


### [30] [Scheduling for TWDM-EPON-Based Fronthaul Without a Dedicated Registration Wavelength](https://arxiv.org/abs/2601.00661)
*Akash Kumar,Sourav Dutta,Goutam Das*

Main category: cs.NI

TL;DR: 提出一种TWDM EPON前传网络调度框架，支持周期性注册而无需额外波长信道，相比专用注册波长方案可支持更多RU数量


<details>
  <summary>Details</summary>
Motivation: C-RAN架构需要满足严格延迟和抖动要求的前传系统。EPON因成本效益和基础设施兼容性成为有前景的解决方案，但传统EPON注册过程会中断数据传输，违反eCPRI要求。ITU-T建议使用专用波长信道进行注册，但这导致带宽利用率低下。

Method: 提出一种新颖的时波分复用(TWDM) EPON前传调度框架，能够在无需额外波长信道的情况下实现周期性注册。该方法通过智能调度机制协调数据传输和注册过程。

Result: 性能评估显示，相比使用专用注册波长的基线方案，所提方法在给定波长信道数量下，支持的无线单元(RU)数量最多可增加71%。

Conclusion: 该调度框架有效解决了EPON前传系统中注册过程与数据传输冲突的问题，提高了波长利用率，支持更多RU连接，满足C-RAN的延迟和抖动要求。

Abstract: The adoption of Centralized Radio Access Network (C-RAN) architectures requires fronthaul systems capable of carrying large volumes of radio data while meeting stringent delay and jitter requirements. Ethernet Passive Optical Networks (EPONs) have emerged as a promising fronthaul solution due to their cost efficiency and compatibility with existing infrastructure. However, the traditional registration process for EPON systems halts the ongoing data transmissions during the registration period, thereby violating the enhanced Common Public Radio Interface (eCPRI) delay and jitter requirements. This limitation has been acknowledged by the ITU-T, which recommends the use of a dedicated wavelength channel for registration, leading to inefficient bandwidth utilization. In this paper, we propose a novel scheduling framework for a Time and Wavelength Division Multiplexed (TWDM) EPON-based fronthaul that enables periodic registration without wasting an additional wavelength channel. Performance evaluation demonstrates that the proposed method achieves up to a 71\% increase in the number of Radio Units (RUs) supported for a given number of wavelength channels, compared to a baseline scheme employing a dedicated registration wavelength.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的性能，发现最佳检测器高度依赖于训练数据中故障样本的总数量。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业系统（如质量控制和预测性维护）中具有应用潜力，但面临极端类别不平衡的挑战，主要由于训练过程中故障数据的可用性有限。需要评估异常检测算法在真实工程约束下的性能。

Method: 使用基于超球面异常分布的合成数据集（2D和10D），对14种异常检测器进行基准测试。训练数据集的异常率在0.05%到20%之间，训练规模在1,000到10,000之间（测试数据集大小为40,000），以评估性能和泛化误差。

Result: 发现最佳检测器高度依赖于训练数据中故障样本的总数量：当故障样本少于20个时，无监督方法（kNN/LOF）占优；当有30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。在只有两个特征时，半监督方法没有显著优势，但在十个特征时改进明显。研究还突显了较小数据集上异常检测方法的泛化性能下降。

Conclusion: 该研究为工业环境中部署异常检测提供了实用见解，强调了故障样本数量对检测器选择的关键影响，并指出在不同特征维度下半监督方法的有效性差异。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [32] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 该研究将Yahtzee游戏建模为MDP，使用多种策略梯度方法训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优性能的241.78分。


<details>
  <summary>Details</summary>
Motivation: Yahtzee作为具有随机性、组合结构和延迟奖励的经典骰子游戏，是中等规模强化学习的理想基准。虽然单人游戏可通过动态规划求解，但多人游戏难以处理，需要近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程，使用REINFORCE、A2C和PPO等策略梯度方法训练自博弈智能体，采用共享主干的多头网络架构，并对特征编码、动作编码、架构、回报估计器和熵正则化进行消融研究。

Result: 在固定训练预算下，A2C表现最稳健，中位得分241.78分（接近最优DP得分254.59的5%以内），上区奖励和Yahtzee达成率分别为24.9%和34.1%。REINFORCE和PPO对超参数敏感且未能达到接近最优性能。

Conclusion: A2C在Yahtzee游戏中表现出色，但所有模型都难以学习上区奖励策略，过度依赖四条策略，突显了长期信用分配和探索的持续挑战。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [33] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文提出一种针对LLM模型组合技术的攻击方法：通过设计单个"破坏性token"，在捐赠模型中功能正常，但在移植到基础模型后重构为恶意特征，破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM生态系统中模型组合技术（如权重合并、推测解码、词汇扩展）的普及，不同模型家族间的tokenizer移植成为关键前提。作者发现这一互操作性步骤存在供应链漏洞，可能被恶意利用。

Method: 利用系数重用的几何特性，通过双目标优化问题形式化攻击，使用稀疏求解器实例化。攻击是免训练的，通过谱模仿规避异常检测，同时保持结构持久性以抵抗微调和权重合并。

Result: 成功设计出单个"破坏性token"，在捐赠模型中功能正常（与正常行为统计上无差异），但在移植到基础模型后可靠地重构为高显著性的恶意特征，破坏基础模型的生成能力。

Conclusion: tokenizer移植这一关键互操作性步骤存在隐藏风险，可能被用于供应链攻击，破坏模块化AI组合流程的安全性。攻击具有持久性和隐蔽性，凸显了模型组合技术中的安全挑战。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [34] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: IMBWatch是一个基于时空图神经网络的框架，用于大规模检测非法按摩院，通过分析在线广告、营业执照和评论等开源情报构建动态图，有效识别人口贩运和剥削网络。


<details>
  <summary>Details</summary>
Motivation: 非法按摩院（IMBs）以合法健康服务为掩护，从事人口贩运、性剥削和强迫劳动等犯罪活动。传统检测方法（如社区举报和监管检查）反应迟缓且效果有限，难以揭示犯罪网络的整体运作模式。

Method: IMBWatch构建时空图神经网络（ST-GNN），从开源情报（在线广告、营业执照、众包评论）创建动态图。节点代表企业、别名、电话号码、位置等异构实体，边捕捉时空和关系模式（如共址、电话重复使用、广告同步）。结合图卷积操作和时间注意力机制，建模IMB网络在时空上的演化。

Result: 在美国多个城市的真实数据集上，IMBWatch优于基线模型，获得了更高的准确率和F1分数。除了性能提升，该框架还提供更好的可解释性，为主动干预提供可操作的见解。

Conclusion: IMBWatch是一个可扩展、可适应其他非法领域的框架，已发布匿名数据和开源代码以支持可重复研究。该框架能够有效检测IMB网络，支持针对人口贩运的主动干预措施。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [35] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出一种渐近置信度的最佳臂识别框架，通过放松严格误差控制要求，在长时域场景中实现更优样本复杂度，支持非参数分布和协变量利用。


<details>
  <summary>Details</summary>
Motivation: 现有BAI方法在实践中有局限：严格误差控制需要使用宽松的尾界不等式或参数限制，难以处理弱信号、高显著性要求和实验后推断等现实场景。

Method: 引入渐近误差控制框架，要求误差控制相对于最小样本量渐近有效；开发新的渐近随时有效置信序列，设计BAI算法，灵活纳入协变量进行方差缩减。

Result: 在温和收敛假设下，提供样本复杂度的渐近界，最坏情况样本复杂度与高斯BAI在已知方差下的最佳情况样本复杂度匹配；实验显示能减少平均样本复杂度同时保持误差控制。

Conclusion: 提出的渐近框架克服了传统BAI方法的局限性，在非参数设置中实现近似误差控制，在长时域场景中提供更优性能，适用于现实世界的弱信号和高显著性要求场景。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [36] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO：将提示工程重构为序列决策问题，通过贝叶斯优化自适应选择指令，解决LLM方程发现中的指令脆弱性问题


<details>
  <summary>Details</summary>
Motivation: LLM在方程发现中表现出潜力，但输出对提示措辞高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解

Method: 提出NeuroSymBO方法，将提示工程重构为序列决策问题。维护离散推理策略库，使用贝叶斯优化基于数值反馈在每个步骤选择最优指令

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现更高的恢复率和更简洁的解决方案

Conclusion: 自适应提示选择能有效解决LLM方程发现中的指令脆弱性问题，提升模型性能和解决方案质量

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [37] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下显著提升语义保真度


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义意义保持的挑战，传统信道编码（如LDPC、Reed-Solomon）无法实现细粒度语义保护

Method: 采用强化学习框架，通过自适应重复编码实现按维度不等错误保护，使用复合语义失真度量平衡全局嵌入相似性和实体级保护

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 代码结构必须与语义粒度对齐，智能分配的简单重复编码能实现细粒度语义保护，适用于边缘计算和物联网等带宽稀缺但语义保真度关键的场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [38] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个几何强化学习框架，用于未知环境中的同时导航与建图，通过局部能量景观编码可达性和障碍约束，无需构建全局地图。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图的挑战性问题，传统方法需要构建全局地图或设计复杂的多智能体策略，而本方法旨在仅依赖局部感官观测实现高效导航。

Method: 将路径导航和建图建模为动态最短路径搜索与发现过程，使用受控哈密顿优化：感官输入转化为局部能量景观编码可达性、障碍屏障和变形约束，策略通过更新哈密顿量分阶段演化。

Result: 在2D导航任务中评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM保持安全距离，泛化到未见布局，通过局部能量优化而非广泛全局建图实现高质量导航。

Conclusion: 通过更新哈密顿量的几何强化学习能够实现高质量导航，仅需最小化探索和局部能量优化，无需构建全局地图，为未知环境中的同时导航与建图提供了有效解决方案。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [39] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 研究非马尔可夫状态和成本过程下线性函数逼近的强化学习方法，包括策略评估和Q学习的收敛性分析，并应用于部分可观测MDP


<details>
  <summary>Details</summary>
Motivation: 传统强化学习通常假设马尔可夫过程，但在实际应用中状态和成本过程可能是非马尔可夫的。本文旨在研究在非马尔可夫环境下，使用线性函数逼近的强化学习方法的收敛性

Method: 1. 分析策略评估方法在非马尔可夫过程下的收敛性，证明在适当的遍历性条件下算法收敛；2. 研究Q学习与线性函数逼近，证明当基函数基于量化映射选择时的收敛性；3. 将结果应用于部分可观测MDP，使用有限记忆变量作为状态表示

Result: 1. 策略评估方法在非马尔可夫过程下收敛，极限对应于正交投影和辅助马尔可夫决策过程贝尔曼算子的联合算子的不动点；2. Q学习在特定基函数选择下可收敛；3. 为部分可观测MDP的学习算法推导了明确的误差界

Conclusion: 本文为非马尔可夫环境下线性函数逼近的强化学习提供了理论保证，证明了在适当条件下策略评估和特定Q学习算法的收敛性，并将结果应用于部分可观测MDP，为实际应用提供了理论支持

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [40] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大型模型在联邦学习框架下的定制化方法，回顾了多种定制技术，并首次将前缀调优应用于联邦学习环境，验证了其可行性、竞争性性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型模型的发展，如何在保护数据隐私的联邦学习框架下实现模型定制化成为重要挑战。需要探索将各种大型模型定制技术适配到联邦学习环境中，并验证其可行性。

Method: 1. 系统回顾了全微调、高效微调、提示工程、前缀调优、知识蒸馏和检索增强生成等大型模型定制技术；2. 探讨了这些技术在联邦学习框架下的实现方式；3. 首次在联邦学习环境中实验了前缀调优方法，并与三种其他联邦定制方法进行比较。

Result: 联邦前缀调优实验验证了其在联邦学习环境中的可行性，性能接近集中式方法。与其他三种联邦定制方法相比，表现出竞争性性能、满意的效率和一致的鲁棒性。

Conclusion: 大型模型可以在联邦学习框架下有效定制，前缀调优等方法是可行的解决方案。该研究为在保护数据隐私的同时实现大型模型个性化定制提供了技术路径。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [41] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是主要预测因子，但模型对极端严重事故预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，通过随机搜索交叉验证优化XGBoost分类器，并采用类别加权处理类别不平衡问题。

Result: 模型整体准确率78%，对主要类别（严重程度2）的精确率和召回率达到87%。特征重要性分析显示时间、地理位置、能见度、温度和风速是最强预测因子，但降水和能见度预测能力有限。

Conclusion: 研究为交通管理提供了实证基础，但数据集以中等严重程度事故为主限制了模型对极端情况的预测能力，未来需要改进采样策略、特征工程和外部数据整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [42] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种使用纯强化学习梯度进行决策变换器在线微调的新方法，解决了传统基于监督学习的在线DT方法中后见回报重标记与重要性采样RL算法不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但扩展到在线设置时，现有方法仍主要依赖监督序列建模目标。研究发现后见回报重标记这一在线DT标准组件与基于重要性采样的RL算法（如GRPO）存在根本性不兼容，导致训练不稳定。

Method: 将GRPO适配到决策变换器，并引入关键改进：1）子轨迹优化以改善信用分配；2）序列级似然目标以增强稳定性和效率；3）主动采样以鼓励在不确定区域进行探索。

Result: 实验表明，所提方法在多个基准测试中超越了现有在线DT基线，实现了新的最先进性能，证明了纯RL在线微调对决策变换器的有效性。

Conclusion: 通过解决后见回报重标记与重要性采样RL算法的不兼容问题，本文成功实现了决策变换器的纯强化学习梯度在线微调，为序列决策建模提供了更有效的在线学习方法。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [43] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: 提出Sequential RC架构，将大储层分解为多个小储层串联，在保持长期依赖的同时降低计算成本，相比LSTM和标准RNN在高维时空系统预测中表现更优


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中存在梯度训练和内存瓶颈问题，传统Reservoir Computing虽然通过固定循环层和凸读出优化缓解了这些问题，但仍面临输入维度扩展性差的问题

Method: 提出Sequential Reservoir Computing架构，将单个大储层分解为一系列小型相互连接的储层，这种设计在保持长期时间依赖性的同时减少了内存和计算成本

Result: 在低维混沌系统（Lorenz63）和高维物理模拟（2D涡度和浅水方程）中，Sequential RC相比LSTM和标准RNN基线：预测有效时间延长15-25%，误差指标（SSIM、RMSE）降低20-30%，训练成本降低高达三个数量级

Conclusion: Sequential RC在保持传统RC简单高效的同时，实现了对高维动力系统的优越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [44] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 机器学习模型利用常规电子健康记录数据，在预测肝硬化的早期风险方面显著优于传统FIB-4评分，可实现更早、更准确的风险分层。


<details>
  <summary>Details</summary>
Motivation: 开发能够利用常规电子健康记录数据早期预测肝硬化风险的机器学习模型，以改善临床风险分层和预防管理。

Method: 回顾性队列研究，使用大型学术医疗系统的去标识化EHR数据。识别脂肪肝患者并分为肝硬化和非肝硬化队列。构建预测场景，从观察窗口聚合人口统计学、诊断、实验室结果、生命体征和共病指数。训练XGBoost模型用于1年、2年和3年预测，并与FIB-4评分进行性能比较。

Result: XGBoost模型在所有预测时间窗口均优于FIB-4：1年预测AUC为0.81 vs 0.71，2年预测0.73 vs 0.63，3年预测0.69 vs 0.57。随着预测时间延长，性能优势持续存在，表明早期风险识别能力更强。

Conclusion: 基于常规EHR数据的机器学习模型在肝硬化早期预测方面显著优于传统FIB-4评分，可作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [45] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE是一个异构MoE联邦学习框架，通过自适应选择专家子集来解决资源受限客户端的LLM微调问题，提高训练效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的大语言模型微调面临计算资源受限问题，而MoE模型虽然计算高效，但在FL环境中存在三个关键挑战：专家选择缺乏可靠指标、异构计算资源限制、客户端特定专家子集导致聚合困难。

Method: 提出HFedMoE框架：1) 基于专家对微调性能的贡献识别专家重要性；2) 从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3) 设计稀疏感知模型聚合策略，加权聚合活跃专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确性和收敛速度方面优于现有最先进的基准方法。

Conclusion: HFedMoE成功解决了MoE在联邦学习环境中的关键挑战，为资源受限客户端提供了高效的大语言模型微调解决方案，在保持隐私的同时实现了计算效率和性能的平衡。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [46] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN半监督学习方法，仅需1-3%标注数据即可实现蚊子神经元尖峰信号分类，用于检测寨卡病毒、登革热病毒感染，准确率达99.93%，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒主要传播媒介，人工分类神经元尖峰模式耗时耗力。现有深度学习方法需要完全标注的数据集和高度预处理的信号，难以在实际场景大规模应用。需要解决标注数据稀缺问题。

Method: 提出半监督Swin启发式GAN架构（SSI-GAN），包含基于Transformer的生成器和Swin启发的移位窗口判别器。使用多头自注意力模型在平面窗口式Transformer判别器中学习稀疏高频尖峰特征。仅需1-3%标注数据，训练超过1500万个尖峰样本，使用贝叶斯Optuna框架优化超参数，五折蒙特卡洛交叉验证验证鲁棒性。

Result: SSI-GAN在感染后第三天仅用3%标注数据达到99.93%分类准确率，在所有感染阶段仅需1%监督即保持高准确率。相比标准监督方法，在相同性能水平下减少97-99%人工标注工作量。移位窗口Transformer设计大幅超越所有基线方法，在尖峰神经元感染分类中创下新纪录。

Conclusion: SSI-GAN通过半监督学习和Transformer架构有效解决了神经元尖峰信号分类中标注数据稀缺问题，大幅降低了实际应用中的标注成本，为蚊子病毒神经嗜性检测提供了高效可行的解决方案。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [47] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出一个资源高效的、以数据为中心的心律失常检测框架，通过特征工程使复杂数据线性可分，实现超轻量级模型（8.54KB）和实时分类（0.46μs延迟）。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测。现有深度学习方法计算开销过大，不适合资源受限的边缘设备。

Method: 采用以数据为中心的方法，优先特征工程而非模型复杂度。整合时频小波分解和图论结构描述符（如PageRank中心性），构建混合特征空间，再通过互信息和递归消除进行特征选择，使用可解释的超轻量级线性分类器。

Result: 在MIT-BIH和INCART数据集上达到98.44%诊断准确率，模型大小仅8.54KB，分类推理延迟0.46μs，每搏处理管道52ms，实现实时操作。相比压缩模型KD-Light（25KB，96.32%准确率）有数量级效率提升。

Conclusion: 该框架通过优化特征工程使复杂心律失常数据线性可分，实现了超高效的心律失常检测系统，为无电池心脏传感器提供了可行方案，显著优于现有压缩模型。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [48] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 该论文提出了一种利用未标记网络数据增强生成模型归因的方法，通过约束优化提升对未知生成器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，能够识别特定生成模型产生的图像。现有方法在已知生成器上表现良好，但难以泛化到未见或新发布的生成器。

Method: 使用CLIP特征和线性分类器建立基线，然后提出约束优化方法：利用未标记的网络数据（可能包含真实图像、未知生成器输出或目标模型样本），鼓励将野生样本分类为非目标，同时约束在标记数据上的性能保持高水平。

Result: 实验结果显示，加入野生数据显著提升了在挑战性未见生成器上的归因性能，证明未标记数据可以有效增强开放世界设置下的AI生成内容归因。

Conclusion: 利用未标记网络数据可以有效提升生成模型归因的泛化能力，特别是在面对未知和新发布生成器时，为开放世界环境下的AI生成内容溯源提供了有效解决方案。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [49] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 提出对抗性图提示（AGP）框架，将对抗学习融入图提示中，实现鲁棒的图神经网络微调，解决现有参数高效微调方法对图拓扑和节点特征噪声的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在适应预训练GNN模型到下游任务时，对图拓扑和节点属性的各种噪声和攻击表现出显著脆弱性，需要提升鲁棒性。

Method: 提出对抗性图提示（AGP）框架：1）将AGP建模为min-max优化问题，采用交替优化方案；2）内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声；3）外层最小化学习最优节点提示来对抗噪声；4）理论证明可处理图拓扑和节点噪声。

Result: 在多个基准任务上的广泛实验验证了AGP相比最先进方法的鲁棒性和有效性，能够增强各种预训练GNN模型在下游任务中的鲁棒性。

Conclusion: AGP是首个将对抗学习融入图提示的框架，通过min-max优化和交替训练方案，实现了对图拓扑和节点噪声的鲁棒微调，具有通用性和理论保证。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [50] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: GRIT是一种动态、曲率感知的LoRA优化方法，通过K-FAC预条件梯度、定期重投影到Fisher主特征方向、自适应调整有效秩，在减少46%可训练参数的同时达到或超越LoRA/QLoRA性能


<details>
  <summary>Details</summary>
Motivation: 现有LoRA和QLoRA方法主要存在几何不可知问题：它们在固定、随机方向的低秩子空间中优化，使用一阶下降，基本忽略了局部损失曲率。这会增加有效更新预算并放大弱约束方向的漂移。

Method: GRIT方法：1) 使用K-FAC作为自然梯度代理在秩空间中对梯度进行预条件处理；2) 定期将低秩基重投影到主导Fisher特征方向以抑制漂移；3) 根据谱自适应调整有效秩，使容量集中在信号所在位置。

Result: 在LLaMA骨干上的指令跟随、理解和推理基准测试中，GRIT匹配或超越了LoRA和QLoRA，同时平均减少46%的可训练参数（不同任务中减少25-80%），在不同提示风格和数据混合下没有实际质量损失。

Conclusion: GRIT通过曲率感知的优化机制，显著减少了参数高效微调中的漂移问题，在更新与保留之间实现了更好的权衡，优于其他PEFT优化器基线。

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [51] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: 宽L2正则化网络中监督学习本质上是压缩的，核秩受类别数限制，SGD噪声也是低秩的，这与自监督学习的高秩表示形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究宽L2正则化网络中特征学习的理论，探索监督学习的内在压缩特性，统一确定性和随机性视角下的对齐机制。

Method: 提出理论框架，推导核ODE预测"水填充"谱演化，证明稳定稳态下核秩受类别数限制，分析SGD噪声的低秩特性。

Result: 监督学习的核秩受类别数C限制，SGD噪声也是O(C)低秩的，动态被限制在任务相关子空间，与自监督学习的高秩表示形成鲜明对比。

Conclusion: 监督学习本质上是压缩的，受任务相关性的约束，而自监督学习产生高秩、扩张的表示，这统一了确定性和随机性视角下的对齐理论。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [52] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，实现通信高效且保护隐私的共享奖励学习。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，直接共享数据学习统一奖励函数不切实际，因为存在动态差异、隐私约束和通信带宽限制。需要一种能处理异构环境且保护隐私的联邦学习方法。

Method: 每个客户端先在本地进行轻量级最大熵逆强化学习，然后将得到的奖励函数通过Wasserstein重心进行融合，考虑其底层几何结构。

Result: 证明这种重心融合方法比传统联邦学习中的参数平均方法能产生更准确的全局奖励估计，提供了通信高效且能泛化到异构智能体和环境的框架。

Conclusion: 提出了一种基于最优传输的联邦逆强化学习框架，能够有效处理多智能体系统中的异构性和隐私约束，实现通信高效的共享奖励学习。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [53] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 该论文提出了基于国际象棋战术的量子基准测试QKRD，用于评估QAOA算法在结构化问题上的表现，发现约束保持混频器和预热策略能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有QAOA基准测试主要使用随机实例（如MaxCut、TSP、SAT），这些实例缺乏语义结构和人类可解释性，无法反映真实世界问题的约束特性，限制了算法在实际应用中的性能评估。

Method: 提出量子王车易位支配问题（QKRD），这是一个基于国际象棋战术位置的NISQ规模基准测试，包含5000个结构化实例，具有独热约束、空间局部性和10-40量子比特规模。该基准结合了人类可解释的覆盖度指标和针对经典启发式算法的内在验证。

Result: 约束保持混频器（XY、domain-wall）比标准混频器收敛快约13步；预热策略减少45步收敛时间，能量改进超过d=8；CVaR优化产生负面结果（能量更差，无覆盖度优势）。QAOA比贪婪启发式算法表现好12.6%，比随机选择好80.1%。

Conclusion: 结构化基准测试能揭示问题感知的QAOA技术在随机实例中被掩盖的优势，QKRD为可重复的NISQ算法研究提供了有价值的评估工具。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [54] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种无需姿态控制系统的纳米卫星电源故障检测方法，使用神经网络模拟正常状态，结合多种机器学习算法进行故障分类


<details>
  <summary>Details</summary>
Motivation: 纳米卫星在低地球轨道运行时，其电源系统容易因压力耐受性、发射压力和环境因素出现故障，特别是光伏子系统的线间故障和开路、DC-DC转换器的短路和IGBT开路、地面电池调节器故障等常见问题

Method: 首先基于神经网络建立无故障系统模型，以太阳辐射和太阳能板表面温度为输入，电流和负载为输出；然后使用神经网络分类器通过故障模式和类型进行诊断；同时采用PCA分类、决策树和KNN等其他机器学习方法进行故障分类

Result: 开发了一种无需姿态控制系统的纳米卫星电源故障检测方法，能够有效诊断光伏子系统、DC-DC转换器和地面电池调节器的各类故障

Conclusion: 该方法为纳米卫星电源系统提供了一种有效的故障检测方案，特别是在缺乏姿态控制系统的情况下，通过机器学习技术实现了可靠的故障诊断

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [55] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 该论文提出结合光流和三种深度学习模型（监督CNN、预训练CNN特征提取器、分层极限学习机）的方法，用于无人机非静态摄像头视频中的人体检测，在UCF-ARG数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识，对光照变化、相机抖动等动态事件敏感，且任务依赖性高。需要更便宜、更自动化的特征学习方法。

Method: 结合光流和三种深度学习模型：监督CNN（S-CNN）、预训练CNN特征提取器、分层极限学习机（H-ELM），在UCF-ARG无人机数据集上训练和测试。

Result: 预训练CNN平均准确率98.09%，S-CNN（softmax）95.6%，S-CNN（SVM）91.7%，H-ELM 95.9%。H-ELM在CPU上训练445秒，S-CNN在GPU上训练770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务中表现成功，预训练CNN效果最佳，H-ELM在计算效率上有优势。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [56] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: DDL提出了一种广义的残差连接，通过可学习的几何变换（Delta算子）调制恒等映射，实现身份映射、正交投影和几何反射之间的动态插值，从而增强网络建模复杂状态转换的能力。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络的恒等连接虽然缓解了梯度消失问题，但施加了严格的加法归纳偏置，限制了网络建模复杂状态转换的能力。需要一种更灵活的机制来增强网络的表达能力。

Method: 提出Deep Delta Learning (DDL)架构，用可学习的、数据依赖的几何变换（Delta算子）替代标准残差连接。Delta算子是对单位矩阵的秩-1扰动，由反射方向向量k(X)和门控标量β(X)参数化，能动态插值身份映射、正交投影和几何反射。

Result: DDL能够显式控制层间转移算子的谱，在保持门控残差架构稳定训练特性的同时，增强网络建模复杂非单调动态的能力。通过同步秩-1注入重新构建残差更新，门控β(X)作为动态步长同时控制旧信息擦除和新特征写入。

Conclusion: DDL成功推广了标准残差连接，通过几何变换调制机制增强了深度网络的表达能力，在保持训练稳定性的同时，能够建模更复杂的动态状态转换。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [57] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化来增强流匹配模型的人类偏好对齐，通过合并低熵步骤形成高熵SDE采样步骤，提高探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多去噪步骤优化时面临稀疏和模糊的奖励信号问题，高熵步骤能实现更高效有效的探索，而低熵步骤则导致无区别的轨迹。

Method: 提出E-GRPO（熵感知组相对策略优化），合并连续低熵步骤形成高熵SDE采样步骤，在其他步骤应用ODE采样，并引入多步组归一化优势函数。

Result: 在不同奖励设置下的实验结果证明了该方法的有效性。

Conclusion: 通过熵感知的SDE采样步骤合并和组相对优势计算，能够更有效地对齐人类偏好，提高强化学习在流匹配模型中的性能。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [58] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 大规模比较评估16种可解释机器学习方法在216个表格数据集上的表现，发现性能具有明显层次结构且高度依赖数据集特征，为实践者提供可解释性与预测性能平衡的指导。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融、法律等高风险领域的广泛应用，模型可解释性和问责制日益重要。尽管可解释ML受到关注，但对表格数据固有可解释模型的系统性评估仍然相对缺乏，且通常只关注聚合性能结果。

Method: 对16种固有可解释方法进行大规模比较评估，包括经典线性模型、决策树以及EBMs、符号回归、GOSDT等新方法。研究涵盖216个真实世界表格数据集，不仅进行聚合排名，还根据数据集结构特征（维度、样本量、线性度、类别不平衡）分层分析性能，并评估训练时间和受控分布偏移下的鲁棒性。

Result: 结果显示清晰的性能层次结构，特别是在回归任务中，EBMs始终表现出强大的预测准确性。同时，性能高度依赖上下文：SR和IGANNs在非线性场景中表现特别好，而GOSDT模型对类别不平衡表现出明显的敏感性。

Conclusion: 这些发现为寻求可解释性与预测性能平衡的实践者提供了实用指导，并有助于对表格数据可解释建模的更深入实证理解。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [59] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）可作为异常检测的通用骨干网络，在零样本推理、全模型适应和参数高效微调（PEFT）策略下均优于任务特定基线，特别是在类别不平衡严重时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法大多需要大量任务特定训练，本文探索在大规模异构数据上预训练的时间序列基础模型是否可作为异常检测的通用骨干网络，实现更高效和可扩展的检测。

Method: 通过系统实验比较三种策略：零样本推理、全模型适应和参数高效微调（PEFT），包括LoRA、OFT和HRA等方法，在多基准测试上评估时间序列基础模型的异常检测能力。

Result: TSFMs在AUC-PR和VUS-PR指标上显著优于任务特定基线，特别是在类别不平衡严重时表现更佳。PEFT方法不仅降低计算成本，在多数情况下还能匹配或超越全微调性能。

Conclusion: 时间序列基础模型可作为异常检测的通用骨干网络，即使是为预测任务预训练的模型也能高效适应异常检测，为可扩展和高效的时间序列异常检测提供了有前景的解决方案。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [60] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型（CCBMs），支持三种粒度的模型编辑（概念-标签级、概念级、数据级），无需重新训练即可实现高效模型维护。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型主要针对静态场景，而实际应用中需要持续维护模型：删除错误/敏感数据（遗忘）、纠正错误标注概念、纳入新样本（增量学习）。如何在不重新训练的情况下实现高效可编辑的CBMs是重要挑战。

Method: 提出可控概念瓶颈模型（CCBMs），基于影响函数推导出数学上严格的闭式近似解，支持三种粒度编辑：概念-标签级（纠正概念与标签关联）、概念级（修正概念本身）、数据级（数据删除和添加）。

Result: 实验结果表明CCBMs在效率和适应性方面表现优异，验证了其在实现动态可信CBMs方面的实用价值。

Conclusion: CCBMs为概念瓶颈模型提供了高效的可编辑性，解决了实际部署中模型持续维护的挑战，使CBMs能够适应动态环境并保持可信性。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [61] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: TGE：一种用于离线观察模仿学习（LfO）的轨迹级生成嵌入方法，通过时间扩散模型的潜在空间估计专家状态密度来构建密集平滑的替代奖励，有效处理专家演示稀缺且离线数据与专家行为分布差异大的问题。


<details>
  <summary>Details</summary>
Motivation: 离线观察模仿学习面临两个主要挑战：专家演示稀缺，以及可用的离线次优数据与专家行为分布差异大。现有的分布匹配方法在这种场景下效果不佳，因为它们施加了严格的支持约束并依赖脆弱的一步模型，难以从不完美的数据中提取有用信号。

Method: 提出TGE（轨迹级生成嵌入）方法：1）在离线轨迹数据上训练时间扩散模型；2）利用学习到的扩散嵌入的平滑几何结构，在潜在空间中估计专家状态密度；3）构建密集、平滑的替代奖励，捕捉长时程时间动态；4）有效弥合不相交支持之间的差距，即使在离线数据与专家分布差异大时也能提供稳健的学习信号。

Result: 在D4RL运动和控制基准测试中，TGE方法始终匹配或优于先前的离线LfO方法，证明了其在处理专家演示稀缺且离线数据分布与专家差异大的场景下的有效性。

Conclusion: TGE通过轨迹级生成嵌入方法，利用时间扩散模型的潜在空间几何特性，成功解决了离线观察模仿学习中专家数据稀缺和离线数据分布差异大的挑战，为这一领域提供了更稳健的解决方案。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [62] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 该论文证明了在残差网络上使用分层SGD可以高效学习一类层次化模型，这类模型超越了先前可学习模型的深度限制，并提出了人类"教师"提供分层标签可能解释深度学习成功的原因。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习为何能成功学习复杂函数，特别是探索是否存在一类模型既能被深度学习高效学习，又能超越先前已知可学习模型的深度限制。同时，试图解释人类"教师"提供分层标签的现象如何与深度学习的学习机制相联系。

Method: 提出一类层次化模型，假设存在未知的标签层次结构 L₁ ⊆ L₂ ⊆ ... ⊆ Lᵣ = [n]，其中L₁中的标签是输入的简单函数，而i>1时，Lᵢ中的标签是更简单标签的简单函数。在残差网络上使用分层SGD来学习这类模型。

Result: 证明了这类层次化模型可以被深度学习算法高效学习，且达到了高效可学习性的深度极限：存在需要多项式深度才能表达的模型，而先前模型只能由对数深度电路计算。同时形式化了教师部分了解其内部逻辑的简化模型，展示了层次结构如何促进高效可学习性。

Conclusion: 层次化模型的易学性可能为理解深度学习提供基础。这类模型不仅自然契合深度学习擅长的领域，而且人类"教师"提供分层标签的现象支持了层次结构天然可用的假设。通过提供细粒度标签，教师有效地揭示了大脑内部算法的"提示"或"片段"。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [63] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失在MoE模型中无法有效促进专家多样性，反而增加权重空间重叠，对性能影响不一致，不适合用于MoE多样性优化


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE专家专业化中的作用，探索正交性损失是否能有效促进专家多样性

Method: 在MoE模型中应用正交性损失，通过7种正则化强度进行实验，分析权重空间重叠和激活空间重叠的变化

Result: 正交性损失失败：权重空间重叠增加114%，激活空间重叠保持高位(~0.6)，性能影响不一致（WikiText-103微降0.9%，TinyStories微升0.9%，PTB结果高度波动），权重与激活正交性无显著相关(r=-0.293, p=0.523)

Conclusion: 权重空间正则化既无法实现其几何目标，也不能可靠提升性能，不适合用于MoE多样性优化

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [64] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 该研究开发了一种基于1D UNet的数据增强方法（AugUNet1D），用于自动标记脑电图中的棘波放电（SWD），相比传统方法和现有算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）记录中事件的手动标记非常耗时，特别是对于持续数周至数月的连续记录。棘波放电（SWD）作为失神发作的电生理标志，通常需要手动标记。虽然已有研究使用机器学习自动分割和分类EEG信号，但仍有改进空间。

Method: 研究比较了14种机器学习分类器在961小时C3H/HeJ小鼠EEG记录（包含22,637个标记SWD）上的性能，发现1D UNet表现最佳。通过数据增强改进1D UNet，其中缩放增强效果最好，最终开发了AugUNet1D模型，并与近期发表的"Twin Peaks"时频算法方法进行比较。

Result: AugUNet1D在SWD标记任务中表现优于"Twin Peaks"算法，检测到的事件特征更接近手动标记的SWD。研究将预训练和未训练的AugUNet1D模型公开供其他用户使用。

Conclusion: AugUNet1D是一种有效的自动标记EEG中SWD事件的方法，通过数据增强显著提升了1D UNet的性能，为EEG分析提供了高效的工具，减轻了手动标记的工作负担。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [65] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 提出了一种基于图结构的多用户上下文赌博机框架，将图拉普拉斯正则化与核方法结合，设计了具有理论保证的探索算法。


<details>
  <summary>Details</summary>
Motivation: 研究多用户上下文赌博机问题，用户通过图结构关联，奖励函数既呈现非线性特性又具有图同质性。现有方法难以同时处理非线性行为和用户间的图结构关系。

Method: 引入一个原则性的联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一多用户RKHS中的平方范数，显式推导其再生核，将图拉普拉斯与基础臂核融合。设计了LK-GP-UCB和LK-GP-TS算法，利用高斯过程后验进行探索。

Result: 提供了高概率遗憾界，其缩放与多用户核的有效维度相关，而非用户数量或环境维度。实验表明，在非线性设置中优于强线性和非图感知基线，即使在真实奖励为线性时也保持竞争力。

Conclusion: 提出了一个统一、理论扎实且实用的框架，将拉普拉斯正则化与核化赌博机结合，用于结构化探索，为图结构多用户学习问题提供了新的解决方案。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [66] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 论文通过比较无自注意力Transformer架构（神经链）与离散动力系统的类比，分析了标准数值离散化与物理信息神经网络（PINN）在求解Burgers和Eikonal方程时的差异，发现两者获得相同动力学知识但路径不同。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索无自注意力Transformer架构（神经链）与离散动力系统之间的类比关系，比较传统数值离散化方法与物理信息神经网络（PINN）在求解偏微分方程时的表现差异，理解PINN的工作原理和局限性。

Method: 方法包括：1）建立无自注意力Transformer（神经链）与离散化神经积分/偏微分方程动力系统的类比；2）对粘性和非粘性Burgers方程及Eikonal方程进行标准数值离散化（也表示为神经链形式）；3）使用PINN进行学习求解；4）比较两种方法的数值解。

Result: 研究发现：1）标准数值离散化和PINN学习提供了获取系统动力学相同知识的两种不同路径；2）PINN学习通过随机矩阵进行，这些矩阵与有限差分方法的高度结构化矩阵没有直接关系；3）可接受解的随机矩阵数量远多于唯一的三对角矩阵形式；4）PINN需要更多参数，导致物理透明度（可解释性）降低和训练成本增加。

Conclusion: 结论是：对于一维动态问题，PINN虽然能获得与标准数值方法相同的结果，但代价是参数更多、可解释性更差、训练成本更高。然而，这些结果仅针对一维问题，不排除PINN和机器学习在高维问题中可能提供更好的策略。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [67] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: 研究发现小型语言模型（7-9B参数）存在严重的"正确但推理错误"现象，50-69%的正确答案包含根本性推理缺陷，标准准确率指标无法检测。论文提出推理完整性评分(RIS)作为过程评估指标，发现RAG能显著改善推理完整性，而元认知干预在小模型上反而有害。


<details>
  <summary>Details</summary>
Motivation: 部署小型语言模型作为自主代理需要信任其推理过程，而不仅仅是输出结果。当前存在可靠性危机：许多正确答案包含根本性推理缺陷，这种现象无法通过标准准确率指标检测，对实际部署构成严重威胁。

Method: 通过分析10,734个推理轨迹（涉及三个模型和多样化任务），引入推理完整性评分(RIS)作为过程评估指标。研究对比了检索增强生成(RAG)和元认知干预的效果，并进行机制分析。最后训练了一个神经分类器来验证推理完整性。

Result: 发现50-69%的正确答案包含根本性推理缺陷。RAG能显著改善推理完整性（Cohen's d=0.23-0.93），而元认知干预在小模型上反而损害性能（d=-0.14到-0.33）。机制分析显示RAG通过外部证据基础减少7.6%的错误，而元认知在模型容量不足时会放大混乱。训练的神经分类器达到0.86 F1分数和100倍加速。

Conclusion: 仅依赖准确率评估模型是危险的，因为模型可能基于完全错误的推理得出正确答案。过程验证对于可信代理部署至关重要，论文提出的RIS指标和验证方法为小型语言模型的实际应用提供了重要工具。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [68] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一种用于检测LLM智能体多步行动计划异常的Siamese循环自编码器，通过对比学习和重构的混合损失函数，能同时检测任务轨迹对齐和序列结构问题，比现有方法快17-27倍。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适用于LLM智能体生成的多步行动计划：均值池化嵌入会稀释异常步骤，而仅对比方法忽略序列结构。标准的无监督方法在预训练嵌入上F1分数不超过0.69。

Method: 提出Trajectory Guard，一种Siamese循环自编码器，采用混合损失函数，通过对比学习联合学习任务-轨迹对齐，通过重构学习序列有效性。这种双重目标能统一检测"错误的任务计划"和"畸形计划结构"。

Result: 在合成扰动和真实世界失败（安全审计RAS-Eval和多智能体系统Who&When）的基准测试中，在平衡集上达到0.88-0.94的F1分数，在不平衡外部基准上达到0.86-0.92的召回率。推理延迟32ms，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能有效检测LLM智能体行动计划的异常，实现实时安全验证，适用于生产部署，解决了现有方法在序列异常检测上的不足。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [69] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: 提出SAFN模型，一种可解释的深度学习框架，用于帕金森病的多模态数据融合分析，在PPMI数据集上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病具有异质性表现，需要整合生物和临床标志物。现有模型在可解释性、类别不平衡和多模态数据融合方面存在局限。

Method: 提出Class-Weighted Sparse-Attention Fusion Network (SAFN)，使用模态特定编码器和对称交叉注意力机制融合MRI皮层厚度、体积测量、临床评估和人口统计学数据，通过稀疏约束注意力门控层动态选择信息模态，采用类别平衡焦点损失处理数据不平衡。

Result: 在PPMI数据集703名参与者（570名PD，133名健康对照）上，使用五折交叉验证，SAFN达到0.98±0.02的准确率和1.00±0.00的PR-AUC，优于现有基线方法。可解释性分析显示约60%预测权重分配给临床评估，符合运动障碍学会诊断原则。

Conclusion: SAFN为神经退行性疾病的计算分析提供了一个可重复、透明的多模态建模范式，具有临床一致的可解释性。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [70] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: 该论文研究了LSTM模型压缩，通过逐步减少隐藏单元数量（从128到16），在保持预测精度的同时显著减小模型大小。实验显示64单元模型比128单元模型更小（73%）、更准确（47%）。


<details>
  <summary>Details</summary>
Motivation: 标准LSTM神经网络在零售业销售预测中虽然准确，但计算资源需求大，对中小型零售业构成挑战。需要探索模型压缩方法，在保持预测精度的同时减少计算需求。

Method: 使用Kaggle Store Item Demand Forecasting数据集（913,000条日销售记录），逐步减少LSTM隐藏单元数量（从128到16），研究模型大小与预测精度之间的权衡关系。

Result: 64单元LSTM模型在保持相同精度水平的同时，模型大小从280KB减小到76KB（减少73%），平均绝对百分比误差（MAPE）从23.6%改善到12.4%（提升47%）。

Conclusion: 更大的模型不一定获得更好的结果，通过适当的模型压缩可以在显著减小模型大小的同时提高预测精度，这对计算资源有限的中小型零售业特别有价值。

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [71] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 基于扩散模型的云原生架构，可自动生成店铺专属货架图，将设计时间从30小时缩短至0.5小时，成本降低97.5%


<details>
  <summary>Details</summary>
Motivation: 传统货架图设计耗时巨大（平均30小时/复杂布局），需要自动化解决方案来提升零售空间优化效率

Method: 采用云原生架构，结合AWS云端模型训练和边缘部署实时推理；使用扩散模型学习多零售点成功货架布局，通过改进损失函数整合零售特定约束

Result: 设计时间减少98.3%（30→0.5小时），约束满足率达94.4%；成本降低97.5%，投资回收期4.4个月；架构线性扩展，支持10,000并发请求

Conclusion: 生成式AI在自动化零售空间优化方面具有可行性，云原生扩散模型架构能显著提升货架图设计效率和经济效益

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [72] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于非平衡随机动力学的熵触发重训练框架，通过Fokker-Planck方程建模数据漂移，利用KL散度的时间导数进行熵平衡分解，实现标签无关的重训练触发策略。


<details>
  <summary>Details</summary>
Motivation: 现有漂移检测方法缺乏动力学原理解释，无法平衡重训练频率与操作成本。需要一种基于原理的框架来指导重训练策略。

Method: 将部署时数据漂移建模为Fokker-Planck方程控制的概率流，使用时间演化的KL散度量模型-数据不匹配，通过熵平衡分解得到非负熵产生项，以此触发重训练。

Result: 在受控非平稳分类实验中，熵触发重训练达到与高频重训练相当的预测性能，同时将重训练事件减少一个数量级（相比每日和基于标签的策略）。

Conclusion: 基于非平衡动力学的熵触发重训练框架提供了一种原理性方法，能够响应累积的不匹配而非延迟的性能崩溃，有效平衡性能与操作成本。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [73] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 论文提出区分两种对抗样本类型：利用非鲁棒特征的和不利用非鲁棒特征的，并设计指标量化对抗扰动对非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然解释了对抗攻击的普遍性，但忽略了不直接利用这些特征的对抗样本。需要区分这两种对抗弱点类型，以更全面地评估对抗鲁棒性。

Method: 提出基于集成学习的指标，用于衡量对抗扰动对非鲁棒特征的操纵程度，并用该指标分析攻击生成的对抗样本组成。

Result: 新视角能够重新审视多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练与标准训练之间的鲁棒性差距。

Conclusion: 区分两种对抗样本类型对于全面评估对抗鲁棒性至关重要，提出的指标为分析对抗样本组成提供了新工具，有助于更深入理解对抗弱点。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [74] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: 提出一种自定义损失函数（CLF）来减少深度学习训练中的随机性影响，提高模型的可复现性和可靠性


<details>
  <summary>Details</summary>
Motivation: 当前深度学习训练方法缺乏确保跨运行一致性和鲁棒性的机制，即使在受控条件下，模型准确率也存在显著变异性，这影响了模型的可复现性和可靠性

Method: 提出自定义损失函数（CLF），通过微调参数来平衡预测准确率和训练稳定性，减少对权重初始化和数据洗牌等随机因素的敏感性

Result: 在图像分类和时间序列预测的多种架构上进行广泛实验，证明CLF能显著提高训练鲁棒性，同时不牺牲预测性能

Conclusion: CLF是一种有效且高效的策略，可用于开发更稳定、可靠和可信赖的神经网络

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [75] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习预测骑行时间，结合路线拓扑特征和运动员体能状态，相比传统物理模型更实用


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时间预测需要大量参数（如空气阻力系数、实时风速），对业余骑手不实用，需要更简单有效的方法

Method: 采用机器学习方法，结合路线拓扑特征和运动员当前体能状态（通过训练负荷指标CTL、ATL等），使用Lasso回归模型，在单运动员数据集(N=96次骑行)上进行N-of-1研究设计

Result: Lasso回归模型（拓扑+体能特征）达到MAE=6.60分钟，R²=0.922；加入体能指标后误差比仅用拓扑特征降低14%（MAE从7.66分钟降至6.60分钟）

Conclusion: 机器学习方法能有效预测骑行时间，体能状态对自定节奏的骑行表现有显著影响，渐进式检查点预测支持动态比赛规划

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [76] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 基于图神经网络的强化学习框架，用于优化城市出租车热点预测，结合实时交通数据减少乘客等待时间和司机行驶距离。


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共事件等动态因素的影响，无法实现实时供需匹配优化。

Method: 将城市道路网络建模为图结构（节点为交叉口，边为路段），使用GNN编码时空依赖关系，结合Q-learning智能体推荐最优出租车热点位置，奖励机制同时优化乘客等待时间、司机行驶距离和拥堵避免。

Result: 在模拟的德里出租车数据集上实验表明，相比基线随机选择方法，该模型将乘客等待时间减少约56%，行驶距离减少38%。

Conclusion: 该交通感知的图强化学习框架能有效优化出租车供需匹配，可扩展到多模式交通系统，并集成到智慧城市平台实现实时城市移动性优化。

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [77] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一个针对非单调γ-弱DR-子模函数在向下闭凸体上最大化的近似算法，当γ=1时恢复0.401近似比，γ<1时性能优雅下降


<details>
  <summary>Details</summary>
Motivation: 研究约束条件下的子模目标最大化是机器学习和优化的基本问题。需要处理非单调、非负的γ-弱DR-子模函数在向下闭凸体上的最大化问题，特别是在γ<1时的性能保证需要改进。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，形成简单有效的处理非单调性的方法。

Result: 算法保证平滑依赖于γ：γ=1时恢复0.401近似比，γ<1时性能优雅下降，改进了先前在相同约束下γ-弱DR-子模最大化的边界。

Conclusion: 该方法为向下闭凸体上的非单调γ-弱DR-子模最大化提供了最先进的性能保证，通过连续贪婪和双贪婪的结合有效处理了非单调性。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [78] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个轻量级基准测试，用于量化LLM在简洁理想提示上的过度生成问题，通过测量超出基准答案的额外字符数来评估模型的冗余回答倾向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为通用助手时，经常对简单请求给出不必要的冗长回答，包含冗余解释、模棱两可的表述和模板化内容，这会增加认知负担和推理成本。先前研究表明基于偏好的后训练和LLM评估会导致系统性长度偏差，即更长的回答即使质量相同也会获得更高评分。

Method: 引入YapBench基准测试，包含300多个英语提示，涵盖三种简洁理想场景：需要简短澄清的模糊输入、有简短稳定答案的封闭式事实问题、以及只需单行代码的技术任务。主要指标YapScore测量响应超出基准答案的字符数，YapIndex则是类别级别中位数YapScore的均匀加权平均值。

Result: 评估76个助手LLM发现，中位数额外长度存在数量级差异，并观察到特定类别的失败模式：在模糊输入上填充无意义内容，在单行技术请求上添加不必要的解释或格式化开销。

Conclusion: YapBench提供了一个量化LLM过度生成问题的标准化框架，揭示了不同模型在简洁回答能力上的显著差异，有助于跟踪和改进LLM的简洁性表现。基准测试和实时排行榜已公开发布。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [79] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG测量特征重要性，并提出最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法缺乏结构化领域知识的有效整合，且特征重要性测量中的OOD问题影响模型可靠性。

Method: 1) 将特征重要性层次编码为DAG；2) 使用Temporal Integrated Gradients测量特征重要性；3) 提出Optimal Path Oracle解决TIG计算中的OOD问题；4) 采用双目标优化框架平衡可解释性和准确性。

Result: 理论分析证明收敛性和对mini-batch噪声的鲁棒性；在时间序列数据上的实验表明IGBO能有效实施DAG约束，精度损失最小，优于标准正则化基线。

Conclusion: IGBO为训练可解释模型提供了有效框架，通过结构化领域知识整合和OOD问题解决，在保持模型性能的同时增强可解释性。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [80] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Avatar Forcing框架，通过扩散强制建模实时用户-头像交互，实现低延迟（约500ms）的互动头像生成，并引入无标签学习的直接偏好优化方法。


<details>
  <summary>Details</summary>
Motivation: 当前说话头像生成模型缺乏真正的互动感，主要生成单向响应而缺乏情感参与。需要解决两个关键挑战：在因果约束下实时生成运动，以及无需额外标注数据学习富有表现力的反应。

Method: 提出Avatar Forcing框架，通过扩散强制建模实时用户-头像交互，处理多模态输入（用户音频和动作）。引入直接偏好优化方法，利用丢弃用户条件构建的合成负样本来实现无标签学习。

Result: 框架实现低延迟实时交互（约500ms），相比基线加速6.8倍。生成的响应性和表现力强的头像运动在超过80%的对比中优于基线。

Conclusion: Avatar Forcing框架成功解决了交互式头像生成的关键挑战，实现了低延迟的实时互动和富有表现力的反应，为虚拟通信和内容创作提供了更自然的交互体验。

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [81] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: IRPO是一种新的强化学习框架，通过将Bradley-Terry模型融入GRPO，用点式评分替代成对比较，解决了生成奖励模型在RL训练中的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 生成奖励模型(GRMs)因其可解释性、推理时扩展性和通过强化学习进行优化的潜力而受到关注，但广泛使用的成对GRMs在与GRPO等RL算法结合时存在计算瓶颈：1) 成对比较需要O(n²)时间复杂度；2) 重复采样或额外思维链推理带来计算开销。

Method: 提出Intergroup Relative Preference Optimization (IRPO)，将Bradley-Terry模型融入Group Relative Policy Optimization (GRPO)框架，为每个响应生成点式评分，从而在RL训练中高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRMs中的最先进性能，性能与当前领先的成对GRMs相当。在训练后评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过点式评分方法有效解决了成对GRMs的计算瓶颈问题，在保持可解释性和细粒度奖励信号的同时，实现了高效且性能优越的强化学习训练框架。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [82] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR：一个针对电信领域票务故障排除的端到端系统，集成领域特定排序和生成模型，自动化分类、检索和生成任务，显著提升故障排除的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信领域的票务故障排除高度复杂且耗时，需要专家解释票务内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。

Method: 提出TeleDoCTR系统，集成领域特定排序和生成模型，自动化故障排除工作流：1) 票务分类到相应专家团队；2) 检索上下文和语义相似的历史票务；3) 生成详细故障分析报告（问题、根本原因、解决方案）。

Result: 在真实电信基础设施数据集上评估，TeleDoCTR优于现有最先进方法，显著提升故障排除过程的准确性和效率。

Conclusion: TeleDoCTR通过自动化关键故障排除步骤，有效解决了电信领域票务故障排除的复杂性和效率问题，为端到端票务解决提供了实用解决方案。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [83] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE：一个轻量级框架，通过添加基于群体的探索层来增强强化学习，在具有非平稳奖励或高维策略的挑战性任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂环境中探索不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上添加了一个紧凑的群体探索层。它将策略动作与粒子驱动建议相结合，每个粒子代表在动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应地调节探索。

Result: 在简单基准测试中仅有轻微改进（如CartPole-v1上+0.7%），但在更具挑战性的任务中表现出显著提升：LunarLander-v3上+46%，Hopper-v4上+22%，同时在Walker2d和Ant上保持稳定。在非平稳奖励变化下，ARISE表现出明显的鲁棒性优势，在CartPole上比PPO高出+75分，在LunarLander上也有相应改进。

Conclusion: ARISE提供了一个简单、架构无关的途径，可以在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的强化学习智能体。消融研究证实群体组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [84] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出贝叶斯逆博弈框架，通过变分自编码器和可微纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，相比最大似然估计减少不确定性，实现更安全的决策。


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能处理不确定性、结合多模态观测数据的贝叶斯推理方法。

Method: 提出贝叶斯逆博弈框架：使用结构化变分自编码器，嵌入可微纳什博弈求解器，在交互数据集上训练。无需智能体真实目标标签，能生成隐藏目标的后验分布样本，支持多模态观测融合。

Result: 框架成功学习先验和后验分布，相比基于最大似然估计的逆博弈方法提高推理质量，实现更安全的下游决策而不牺牲效率。多模态推理在轨迹信息不足时进一步减少不确定性。

Conclusion: 贝叶斯逆博弈方法能有效量化不确定性，通过多模态观测融合提高推理鲁棒性，为自主决策提供更安全的规划基础，特别是在信息有限或不确定的场景中。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [85] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: BSAT使用B样条自适应分词器解决长时序预测问题，通过高曲率区域放置token实现高效压缩，结合L-RoPE混合位置编码，在内存受限场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长时序预测中存在自注意力二次复杂度和均匀分块与数据语义结构不匹配的问题，需要更高效的自适应方法。

Method: 提出B样条自适应分词器(BSAT)，通过拟合B样条自适应分割时序，在高曲率区域放置token；采用L-RoPE混合位置编码，结合可学习位置编码和层间可学习基的旋转位置嵌入。

Result: 在多个公开基准测试中表现优异，在高压缩率下保持强大性能，特别适合内存受限的应用场景。

Conclusion: BSAT提供了一种参数自由的自适应时序分割方法，通过高效压缩和灵活的位置编码机制，为长时序预测提供了内存友好的解决方案。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [86] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文老虎机问题动态选择计算步骤的精度配置，平衡精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中，混合精度数值方法可以显著提高计算效率，但需要手动调优精度配置。现有方法缺乏自动化、自适应的精度选择机制，无法根据具体计算特征动态调整精度。

Method: 将精度调优问题建模为上下文老虎机问题，使用离散化状态空间和增量动作值估计。采用Q表映射离散化特征（如近似条件数和矩阵范数）到精度配置动作，通过epsilon-greedy策略优化多目标奖励函数，平衡精度和计算成本。

Result: 在线性系统求解的迭代精化应用中，该框架能有效选择精度配置，在保持与双精度基线相当的精度同时，显著降低计算成本。框架对未见数据集具有良好的泛化能力。

Conclusion: 这是首个基于强化学习的精度自动调优工作，验证了在未见数据集上的有效性。该框架可推广到其他数值算法，推动了科学计算中混合精度数值方法的发展。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [87] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: STAC算法通过分布评论家网络建模时间回报不确定性，利用时间性随机不确定性而非认知不确定性来缩放悲观偏差，解决了评论家网络高估问题，提高了计算效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 离策略演员-评论家方法虽然样本效率高，但评论家网络倾向于系统性高估价值估计。现有方法使用集成来量化认知不确定性以引入悲观偏差，但这种方法计算成本高。

Method: 提出STAC算法：1) 使用单一分布评论家网络建模时间回报不确定性（包括随机转移、奖励和政策变化）；2) 利用时间性随机不确定性而非认知不确定性来缩放TD更新中的悲观偏差；3) 对评论家和演员网络应用dropout进行正则化。

Result: 仅基于分布评论家的悲观偏差就足以缓解高估问题，并在随机环境中自然导致风险规避行为。引入dropout进一步提高了训练稳定性和性能。STAC使用单一分布评论家网络实现了改进的计算效率。

Conclusion: STAC通过利用时间性随机不确定性而非认知不确定性来缩放悲观偏差，有效解决了评论家网络高估问题，同时保持了计算效率，在随机环境中表现出风险规避特性。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [88] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文提出Distributional Creative Reasoning (DCR)框架，分析现有LLM推理方法（STaR、GRPO、DPO）如何导致推理路径分布坍缩，损害创造性，并提供防止坍缩的理论和实践方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理管道依赖引导式推理循环，主要优化正确性，但会导致模型推理路径分布坍缩，降低语义熵，损害创造性问题解决能力。

Method: 提出Distributional Creative Reasoning (DCR)框架，将训练视为通过解决方案轨迹概率测度的梯度流，统一了STaR、GRPO、DPO等多种方法。

Result: 1) 多样性衰减定理：描述基于正确性的目标如何导致STaR、GRPO、DPO的不同多样性衰减模式；2) 确保收敛到稳定且多样策略的设计；3) 实用的防坍缩方案。

Conclusion: DCR为LLM提供了首个既保持正确性又保持创造性的原则性方案，解决了推理路径分布坍缩问题。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [89] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的足球角球防守评估框架，通过追踪数据推断盯人与区域防守任务，实现无标签的防守贡献评估和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调移动，现有价值模型主要评估有球行为，反事实方法缺乏战术背景，需要更精确的防守评估框架。

Method: 针对角球场景开发协变量依赖隐马尔可夫模型(CDHMM)，从球员追踪数据推断时间分辨的盯人和区域防守任务分配，提出防守贡献归因框架和角色条件幽灵模型进行反事实分析。

Result: 该模型能够无标签地推断防守任务分配，提供可解释的防守贡献评估，相比基于"平均"行为的传统方法，能更好地结合战术背景进行反事实分析。

Conclusion: CDHMM框架为足球角球防守提供了更精确的无球防守评估方法，通过角色条件反事实分析实现了基于战术背景的防守贡献量化，提升了防守评估的准确性和可解释性。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [90] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，结合在线重置机制防止码本崩溃，使用KV-LoRA高效利用压缩记忆表示，将记忆库大小减少到最强基线的0.3%同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，持续学习需要更新模型而不遗忘旧知识。现有记忆增强方法面临记忆库随数据流不断增长的瓶颈，需要更高效的记忆压缩方案。

Method: 提出MBC模型：1) 通过码本优化策略在线压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在注意力层使用Key-Value低秩适应(KV-LoRA)高效利用压缩记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过码本压缩和KV-LoRA实现了高效的持续学习，解决了记忆库无限增长的问题，为大规模数据流下的LLM持续学习提供了实用解决方案。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [91] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 提出一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化问题


<details>
  <summary>Details</summary>
Motivation: 传统方法处理分类变量优化时存在局限：得分函数估计器无偏但噪声大，连续松弛方法有偏且依赖温度参数

Method: 引入基于扩散的软重参数化，利用高斯噪声过程下的去噪器闭式解，实现无需训练即可反向传播的扩散采样器

Result: 在多个基准测试中，该方法展现出有竞争力或改进的优化性能

Conclusion: 扩散基重参数化为分类分布优化提供了有效的新方法，平衡了偏差和方差问题

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [92] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: FedHypeVAE：基于超网络的差分隐私联邦数据合成框架，通过条件VAE架构和客户端感知解码器解决非IID数据异质性问题，提供形式化隐私保护


<details>
  <summary>Details</summary>
Motivation: 现有联邦数据共享方法在非IID客户端异质性下表现不佳，且缺乏对梯度泄漏的形式化保护，需要一种既能个性化生成又能保证隐私的解决方案

Method: 基于条件VAE架构，用超网络生成客户端感知的解码器和类条件先验，采用差分隐私优化超网络，结合局部MMD对齐和Lipschitz正则化增强稳定性

Result: FedHypeVAE在非IID条件下实现了稳定的分布对齐，支持领域无关合成和可控多领域覆盖，统一了生成器层面的个性化、隐私保护和分布对齐

Conclusion: 该框架为联邦设置下的隐私保护数据合成建立了原则性基础，通过超网络驱动架构解决了异质性、隐私和分布对齐的关键挑战

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


### [93] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过分析注意力矩阵的谱特征来检测大型语言模型中的有效数学推理，在多个模型上达到85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要训练数据或微调，而本文旨在开发一种无需训练、基于注意力模式谱分析的方法来检测数学推理的有效性，为AI安全监控和幻觉检测提供新工具。

Method: 将注意力矩阵视为动态图的邻接矩阵，提取四个可解释的谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，通过统计分析这些指标在有效和无效数学证明中的差异。

Result: 在7个来自4个不同架构家族的Transformer模型上实验，效果量达到Cohen's d=3.30（p<10^{-116}），分类准确率85.0-95.6%，校准阈值在完整数据集上达到93-95%。发现该方法检测的是逻辑一致性而非编译器接受度。

Conclusion: 谱图分析为推理验证提供了原则性框架，具有即时应用于幻觉检测和AI安全监控的潜力。注意力机制设计会影响哪些谱特征捕捉推理有效性，如Mistral-7B的滑动窗口注意力将判别信号从HFER转移到后期层的平滑度。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [94] [Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction](https://arxiv.org/abs/2601.00146)
*Vikram Seenivasan,Srinath Saikrishnan,Andrew Lizarraga,Jonathan Soriano,Bernie Boscoe,Tuan Do*

Main category: astro-ph.IM

TL;DR: 使用LoRA技术结合不同星系成像数据集改进CNN红移估计，相比传统迁移学习方法减少约2.5倍偏差和2.2倍散射


<details>
  <summary>Details</summary>
Motivation: 星系红移估计对宇宙学研究至关重要，但存在数据质量与数量的权衡：测光红移数据量大但精度较低，光谱红移精度高但数据稀缺且获取成本高。需要一种方法能有效结合两类数据的优势。

Method: 采用低秩适应(LoRA)技术，先在测光红移数据集上训练基础CNN模型，然后在光谱红移数据集上使用LoRA进行微调。LoRA通过添加适配器网络调整模型权重，避免完全重新训练。

Result: LoRA模型相比传统迁移学习方法表现更优：偏差减少约2.5倍，散射减少约2.2倍。在组合数据集上完全重新训练可获得更好泛化性能，但计算成本更高。

Conclusion: LoRA在天体物理回归任务中提供了一种介于完全重新训练和不重新训练之间的有效折中方案，特别适用于数据稀疏任务，能够利用现有预训练模型。

Abstract: In this work, we demonstrate how Low-Rank Adaptation (LoRA) can be used to combine different galaxy imaging datasets to improve redshift estimation with CNN models for cosmology. LoRA is an established technique for large language models that adds adapter networks to adjust model weights and biases to efficiently fine-tune large base models without retraining. We train a base model using a photometric redshift ground truth dataset, which contains broad galaxy types but is less accurate. We then fine-tune using LoRA on a spectroscopic redshift ground truth dataset. These redshifts are more accurate but limited to bright galaxies and take orders of magnitude more time to obtain, so are less available for large surveys. Ideally, the combination of the two datasets would yield more accurate models that generalize well. The LoRA model performs better than a traditional transfer learning method, with $\sim2.5\times$ less bias and $\sim$2.2$\times$ less scatter. Retraining the model on a combined dataset yields a model that generalizes better than LoRA but at a cost of greater computation time. Our work shows that LoRA is useful for fine-tuning regression models in astrophysics by providing a middle ground between full retraining and no retraining. LoRA shows potential in allowing us to leverage existing pretrained astrophysical models, especially for data sparse tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统，通过语言模型驱动的智能体实现自主故障隔离、诊断、恢复和知识积累。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算连续体系统（DCCS）整合了从物联网设备到云基础设施的异构计算资源，其固有的复杂性、移动性和动态操作条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个层次：遏制、诊断、元认知和知识。使用语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上评估显示，ReCiSt能在数十秒内完成自愈，智能体CPU使用率最低为10%。结果还展示了系统克服不确定性的分析深度和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功地将生物自愈原理应用于分布式计算系统，实现了自主故障管理，为复杂动态计算环境提供了有效的弹性解决方案。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [96] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 使用微调大语言模型进行尼日利亚皮钦语抑郁症自动筛查，GPT-4.1在准确性和文化适应性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统PHQ-9问卷在高收入国家验证，但存在语言文化障碍，需要适应尼日利亚皮钦语和520多种本地语言的筛查工具

Method: 收集432个尼日利亚年轻人皮钦语音频回答，进行转录、预处理和标注（语义标记、俚语解释、PHQ-9严重程度评分），微调三种LLM（Phi-3-mini-4k-instruct、Gemma-3-4B-it、GPT-4.1），评估定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适应性）性能

Result: GPT-4.1表现最佳，PHQ-9严重程度评分预测准确率达94.5%，在文化适应性、清晰度和上下文相关性方面也最优

Conclusion: AI介导的抑郁症筛查可为尼日利亚服务不足社区提供解决方案，为在语言多样、资源有限环境中部署对话式心理健康工具奠定基础

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [97] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 提出多算法方法解决最后一公里包裹配送中的工作量平衡问题，通过距离和工作量考虑优化包裹分配，确保每位配送员完成相似工作量


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的包裹分配方法效率低下，导致配送员间工作量分布不均衡，需要优化系统以改善配送时间并实现完整的工作量平衡

Method: 采用多算法方法，包括不同版本的k-means、进化算法、基于k-means初始化的递归分配（不同问题编码）以及混合进化集成算法，结合距离和工作量考虑优化包裹分配

Result: 在西班牙Azuqueca de Henares的实际最后一公里包裹配送系统中验证了所提方法的性能

Conclusion: 提出的多算法方法能有效解决最后一公里配送中的工作量平衡问题，确保配送员间工作量均衡分配

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [98] [On the Error Floor Evaluation of NOMA-Irregular Repetition Slotted ALOHA](https://arxiv.org/abs/2601.00317)
*Estefanía Recayte*

Main category: cs.ET

TL;DR: 提出了一种用于NOMA-IRSA方案在错误平层区域包丢失率的简单而紧密的解析近似方法


<details>
  <summary>Details</summary>
Motivation: 在物联网场景中，需要分析非正交多址接入(NOMA)与不规则重复时隙ALOHA(IRSA)结合方案的包丢失率，特别是在有限长度区域和错误平层区域的性能分析

Method: 用户随机选择基于设计的度分布的副本数量和预定功率级别，接收端执行连续干扰消除(SIC)，推导出有限长度区域的包丢失率解析表达式

Result: 推导的包丢失率表达式能够快速评估，通过蒙特卡洛仿真验证了其准确性，在不同信道负载（包括超出低负载区域）下都表现出良好的匹配度

Conclusion: 提出的解析近似方法简单而准确，为NOMA-IRSA方案在错误平层区域的性能分析提供了有效的理论工具

Abstract: In this work, we provide a simple yet tight analytical approximation of the packet loss rate in the error floor region for a non-orthogonal multiple access (NOMA)-based irregular repetition slotted ALOHA (IRSA) scheme. Considering an Internet of Things (IoT) scenario, users randomly select both the number of replicas based on a designed degree distribution and the transmission power from predetermined levels, while successive interference cancellation (SIC) is performed at the receiver. Our derived packet loss rate expression in the finite length regime is promptly evaluated. Its accuracy is validated through Monte-Carlo simulations, demonstrating a strong match across channel loads, including those beyond the low load regime

</details>


### [99] [Two-Step Interference Cancellation for Energy Saving in Irregular Repetition Slotted ALOHA](https://arxiv.org/abs/2601.00343)
*Estefanía Recayte,Leonardo Badia,Andrea Munari*

Main category: cs.ET

TL;DR: 提出一种改进的IRSA协议，通过中间解码和成功节点提前终止传输来减少不必要的传输，从而降低能耗，特别适用于低负载场景。


<details>
  <summary>Details</summary>
Motivation: 现有IRSA文献大多关注高负载渐近性能，而在低负载场景下，许多传输不会发生碰撞而是冗余的，造成能量浪费。需要一种机制来避免不必要的传输以降低能耗。

Method: 修改IRSA协议，引入中间解码和早期传输终止机制：节点在成功解码后立即停止传输，避免继续发送冗余数据包。同时建立有限帧长和低负载下的能耗与成功概率模型。

Result: 通过分析和仿真验证，所提技术能在保持对标准ALOHA性能优势的同时，显著降低IRSA能耗。例如在10%负载下可实现33%的节能，且不影响吞吐量。

Conclusion: 提出的改进IRSA协议通过中间解码和早期传输终止机制，有效减少了低负载下的冗余传输，实现了显著的节能效果，同时保持了协议的性能优势。

Abstract: We evaluate a modification of irregular repetition slotted ALOHA (IRSA) involving intermediate decoding and early transmission termination by some nodes, upon their decoding success. This is meant to avoid unnecessary transmissions, thereby reducing energy consumption. We expect this to be particularly useful at low loads, where most transmissions can be avoided as they do not often result in a collision and are therefore redundant. To validate this proposal, we observe that most of the literature related to IRSA considers an asymptotic heavily loaded regime; thus, we also present a model of energy consumption and success probability for frames of limited length and low offered loads. Thanks to our analysis, also confirmed by simulation, we are able to show that the proposed technique is able to reduce IRSA energy consumption by minimizing transmissions, while preserving performance gains over standard ALOHA. For example, we are able to get a 33% energy saving at offered loads around 10% without affecting throughput.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [100] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 通过噪声优化解决文本到图像生成中的模式崩溃问题，提升生成多样性和质量


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型存在严重的模式崩溃问题，即相同文本提示下生成的图像缺乏多样性。虽然已有工作尝试通过引导机制或生成大量候选再筛选的方法来解决，但本文探索了不同的方向。

Method: 提出简单的噪声优化目标来缓解模式崩溃，同时保持基础模型的保真度。分析噪声的频率特性，并展示具有不同频率分布的替代噪声初始化可以改善优化和搜索过程。

Result: 实验证明，噪声优化方法在生成质量和多样性方面都取得了优越的结果。

Conclusion: 噪声优化是解决文本到图像生成中模式崩溃问题的有效方法，能够在不损害模型保真度的前提下显著提升生成多样性。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [101] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出跨模态数据增强框架，结合CycleGAN和YOLOv8，通过可见光PCB图像生成伪红外数据，解决红外数据稀缺问题，提升PCB缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 红外数据在PCB缺陷检测中稀缺，限制了检测模型的训练效果。传统方法需要成对监督数据，而实际应用中难以获取足够的红外图像。

Method: 使用CycleGAN进行无配对图像转换，将丰富的可见光PCB图像映射到红外域，生成高质量伪红外样本。然后采用异构训练策略，融合伪红外数据和有限真实红外数据训练轻量级YOLOv8检测器。

Result: 该方法在低数据条件下有效增强特征学习。增强后的检测器显著优于仅使用有限真实数据训练的模型，性能接近全监督训练的基准，证明了伪红外合成作为工业检测鲁棒增强策略的有效性。

Conclusion: 提出的跨模态数据增强框架通过生成伪红外数据成功解决了红外数据稀缺问题，为工业检测提供了一种有效的低数据训练策略，在PCB缺陷检测中表现出色。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [102] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: HyperPriv-EPN：一种基于超图的特权学习框架，利用术后文本特权信息提升术前室管膜瘤预后预测，无需推理时文本输入


<details>
  <summary>Details</summary>
Motivation: 室管膜瘤术前预后对治疗规划至关重要，但MRI缺乏术后手术报告的语义洞察。现有多模态方法在推理时无法利用这些不可用的特权文本数据。

Method: 提出HyperPriv-EPN框架，采用Severed Graph策略，使用共享编码器处理教师图（含术后特权信息）和学生图（仅术前数据），通过双流蒸馏让学生图从视觉特征中"幻觉"语义社区结构。

Result: 在311名患者的多中心队列验证中，达到最先进的诊断准确率和生存分层性能，有效将专家知识转移到术前设置。

Conclusion: 该框架解锁了历史术后数据的价值，可在无需推理时文本的情况下指导新患者诊断，实现了从特权信息到术前场景的知识迁移。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [103] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 本文提出了两种用于左心室分割的新型深度学习架构LNU-Net和IBU-Net，基于层归一化和实例-批量归一化改进U-Net，在短轴电影MRI图像上取得了优于现有方法的分割性能。


<details>
  <summary>Details</summary>
Motivation: 左心室分割对于心脏图像的临床量化和诊断至关重要，需要更准确的分割方法来支持临床决策。

Method: 提出了两种基于U-Net的改进架构：LNU-Net在每个卷积块中应用层归一化；IBU-Net在第一个卷积块中结合实例和批量归一化，并将结果传递到下一层。方法还结合了仿射变换和弹性变形进行图像数据处理。

Result: 在包含45名患者805张MRI图像的数据集上评估，实验结果表明提出的方法在Dice系数和平均垂直距离指标上优于其他最先进方法。

Conclusion: LNU-Net和IBU-Net是有效的左心室分割方法，通过归一化技术的改进提升了分割性能，为心脏图像分析提供了更好的工具。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [104] [Cuffless, calibration-free hemodynamic monitoring with physics-informed machine learning models](https://arxiv.org/abs/2601.00081)
*Henry Crandall,Tyler Schuessler,Filip Bělík,Albert Fabregas,Barry M. Stults,Alexandra Boyadzhiev,Huanan Zhang,Jim S. Wu,Aylin R. Rodan,Stephen P. Juraschek,Ramakrishna Mukkamala,Alfred K. Cheung,Stavros G. Drakos,Christel Hohenegger,Braxton Osting,Benjamin Sanchez*

Main category: physics.med-ph

TL;DR: 开发基于生物电阻抗的智能手表，通过物理信息神经网络实现无袖带血压和血流速度监测，解决了现有技术缺乏理论基础的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有无袖带血压监测设备（如脉搏波分析、脉搏波到达时间）缺乏理论基础，容易受到生理和实验干扰，影响准确性和临床实用性，需要更可靠的技术。

Method: 开发具有实时生物电阻抗传感的智能手表，通过多尺度分析和计算建模框架阐明生物电阻抗与血压的物理关系，使用融合流体动力学原理的信号标记物理信息神经网络进行无校准血压和血流速度估计。

Result: 在健康人群（休息和运动后）、高血压患者和心血管疾病患者中成功测试，涵盖门诊和重症监护环境，证明了生物电阻抗技术用于无袖带血压和血流速度监测的可行性。

Conclusion: 生物电阻抗技术能够解决现有无袖带技术的局限性，为连续心血管健康监测提供理论基础支撑的可靠方法，具有临床转化潜力。

Abstract: Wearable technologies have the potential to transform ambulatory and at-home hemodynamic monitoring by providing continuous assessments of cardiovascular health metrics and guiding clinical management. However, existing cuffless wearable devices for blood pressure (BP) monitoring often rely on methods lacking theoretical foundations, such as pulse wave analysis or pulse arrival time, making them vulnerable to physiological and experimental confounders that undermine their accuracy and clinical utility. Here, we developed a smartwatch device with real-time electrical bioimpedance (BioZ) sensing for cuffless hemodynamic monitoring. We elucidate the biophysical relationship between BioZ and BP via a multiscale analytical and computational modeling framework, and identify physiological, anatomical, and experimental parameters that influence the pulsatile BioZ signal at the wrist. A signal-tagged physics-informed neural network incorporating fluid dynamics principles enables calibration-free estimation of BP and radial and axial blood velocity. We successfully tested our approach with healthy individuals at rest and after physical activity including physical and autonomic challenges, and with patients with hypertension and cardiovascular disease in outpatient and intensive care settings. Our findings demonstrate the feasibility of BioZ technology for cuffless BP and blood velocity monitoring, addressing critical limitations of existing cuffless technologies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [105] [Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing](https://arxiv.org/abs/2601.00020)
*Nikhil Garg,Anxiong Song,Niklas Plessnig,Nathan Savoia,Laura Bégon-Lours*

Main category: cs.NE

TL;DR: 该研究展示了在铁电忆阻器上部署脉冲神经网络用于自适应EEG运动想象解码，通过设备感知训练和权重转移策略实现个性化脑机接口。


<details>
  <summary>Details</summary>
Motivation: EEG脑机接口面临非平稳神经信号跨会话和个体变化的挑战，限制了通用模型的泛化能力，需要资源受限平台上的自适应个性化学习。忆阻器硬件为部署后适应提供了有前景的基底，但实际实现受到权重分辨率有限、设备变异、非线性编程动态和有限耐久性等挑战。

Method: 1) 制造、表征和建模铁电突触器件；2) 在两种部署策略下评估卷积-循环SNN架构：设备感知训练和软件训练权重转移后的低开销设备重调；3) 引入设备感知权重更新策略，梯度更新在数字端累积，仅在超过阈值时转换为离散编程事件，模拟非线性状态依赖编程动态并减少编程频率。

Result: 两种部署策略都实现了与最先进软件SNN相当的分类性能。通过仅重训练最终网络层实现的主体特定迁移学习提高了分类准确率。铁电硬件支持SNN中稳健、低开销的自适应。

Conclusion: 可编程铁电硬件能够支持脉冲神经网络中稳健、低开销的自适应，为神经信号的个性化神经形态处理开辟了实用路径。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.

</details>


### [106] [Modern Neuromorphic AI: From Intra-Token to Inter-Token Processing](https://arxiv.org/abs/2601.00245)
*Osvaldo Simeone*

Main category: cs.NE

TL;DR: 该论文探讨了人工智能能效挑战与神经形态计算原理的关联，分析了神经形态模型、状态空间模型和Transformer架构在"token内处理"与"token间处理"上的联系，并综述了相关训练方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展带来了巨大的能源消耗问题，这促使研究者重新关注神经形态计算原理。神经形态计算通过离散稀疏激活、循环动力学和非线性反馈等机制，能够实现类似大脑的高效计算，为解决AI能效挑战提供了新思路。

Method: 论文通过"token内处理"和"token间处理"的视角，系统分析了神经形态模型、状态空间模型和Transformer架构之间的联系。早期神经形态AI主要基于脉冲神经网络进行token内处理（如图像像素转换），而近期研究则探索如何利用神经形态原理设计高效的token间处理方法，包括状态空间动力学和稀疏自注意力机制。

Result: 论文建立了现代AI架构（如量化激活、状态空间动力学、稀疏注意力）与神经形态原理之间的明确联系，展示了神经形态计算原则如何为高效AI设计提供理论基础，并系统综述了从基于并行卷积处理的代理梯度到基于强化学习机制的局部学习规则等多种训练方法。

Conclusion: 神经形态计算原理为解决AI能效挑战提供了有前景的方向。通过token内处理和token间处理的框架，可以更好地理解神经形态模型与现代AI架构的关联。未来的高效AI系统将越来越多地融入神经形态设计原则，而多样化的训练方法为这些模型的优化提供了技术基础。

Abstract: The rapid growth of artificial intelligence (AI) has brought novel data processing and generative capabilities but also escalating energy requirements. This challenge motivates renewed interest in neuromorphic computing principles, which promise brain-like efficiency through discrete and sparse activations, recurrent dynamics, and non-linear feedback. In fact, modern AI architectures increasingly embody neuromorphic principles through heavily quantized activations, state-space dynamics, and sparse attention mechanisms. This paper elaborates on the connections between neuromorphic models, state-space models, and transformer architectures through the lens of the distinction between intra-token processing and inter-token processing. Most early work on neuromorphic AI was based on spiking neural networks (SNNs) for intra-token processing, i.e., for transformations involving multiple channels, or features, of the same vector input, such as the pixels of an image. In contrast, more recent research has explored how neuromorphic principles can be leveraged to design efficient inter-token processing methods, which selectively combine different information elements depending on their contextual relevance. Implementing associative memorization mechanisms, these approaches leverage state-space dynamics or sparse self-attention. Along with a systematic presentation of modern neuromorphic AI models through the lens of intra-token and inter-token processing, training methodologies for neuromorphic AI models are also reviewed. These range from surrogate gradients leveraging parallel convolutional processing to local learning rules based on reinforcement learning mechanisms.

</details>


### [107] [RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient Long-Context Transformers](https://arxiv.org/abs/2601.00426)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: RMAAT是一种受星形胶质细胞启发的Transformer架构，通过递归内存增强和自适应压缩机制，在保持竞争力的准确率的同时显著提升长序列处理的计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的自注意力机制具有二次复杂度，限制了其在长序列上的应用。作者从生物学的星形胶质细胞（对记忆和突触调节至关重要的胶质细胞）中获取计算原理，作为传统架构修改的补充方法，以开发高效的自注意力机制。

Method: 提出RMAAT架构：1）采用基于片段的递归处理策略，通过持久内存token传播上下文信息；2）基于模拟星形胶质细胞长时程可塑性（LTP）的自适应压缩机制调节这些token；3）片段内注意力使用受星形胶质细胞短时程可塑性（STP）启发的线性复杂度机制；4）使用专门为递归网络设计的AMRB算法进行内存高效训练。

Result: 在Long Range Arena基准测试中，RMAAT展现出竞争力的准确率，同时在计算和内存效率方面有显著提升，表明星形胶质细胞启发的动态机制有潜力应用于可扩展的序列模型。

Conclusion: 将星形胶质细胞启发的动态机制整合到序列模型中具有潜力，能够开发出既准确又计算高效的Transformer变体，为长序列处理提供了新的生物学启发的解决方案。

Abstract: The quadratic complexity of self-attention mechanism presents a significant impediment to applying Transformer models to long sequences. This work explores computational principles derived from astrocytes-glial cells critical for biological memory and synaptic modulation-as a complementary approach to conventional architectural modifications for efficient self-attention. We introduce the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), an architecture integrating abstracted astrocyte functionalities. RMAAT employs a recurrent, segment-based processing strategy where persistent memory tokens propagate contextual information. An adaptive compression mechanism, governed by a novel retention factor derived from simulated astrocyte long-term plasticity (LTP), modulates these tokens. Attention within segments utilizes an efficient, linear-complexity mechanism inspired by astrocyte short-term plasticity (STP). Training is performed using Astrocytic Memory Replay Backpropagation (AMRB), a novel algorithm designed for memory efficiency in recurrent networks. Evaluations on the Long Range Arena (LRA) benchmark demonstrate RMAAT's competitive accuracy and substantial improvements in computational and memory efficiency, indicating the potential of incorporating astrocyte-inspired dynamics into scalable sequence models.

</details>


### [108] [Three factor delay learning rules for spiking neural networks](https://arxiv.org/abs/2601.00668)
*Luke Vassallo,Nima Taherinejad*

Main category: cs.NE

TL;DR: 该论文提出了一种在线学习突触和轴突延迟的脉冲神经网络方法，通过三因素学习规则同时优化延迟参数，显著提升了时序模式识别性能，同时大幅减少了模型规模和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统SNN的可学习参数主要局限于突触权重，对时序模式识别贡献有限。现有延迟学习方法依赖大型网络和离线学习，不适合资源受限环境下的实时操作。需要一种能够在资源受限设备上在线学习延迟参数的方法。

Method: 在基于LIF的前馈和循环SNN中引入突触和轴突延迟，提出三因素学习规则在线同时学习延迟参数。使用高斯替代函数专门用于资格迹计算，结合自上而下的误差信号确定参数更新。

Result: 引入延迟使准确率比仅有权重的基线提升高达20%；在相似参数数量下，联合学习权重和延迟使准确率提升高达14%。在SHD语音识别数据集上，达到与离线反向传播方法相似的准确率。相比SOTA方法，模型规模减少6.6倍，推理延迟降低67%，仅损失2.4%的分类准确率。

Conclusion: 该方法通过在线学习延迟参数显著提升了SNN的时序模式识别能力，同时大幅降低了模型复杂度和计算需求，有利于设计功耗和面积受限的神经形态处理器，支持设备端学习和降低内存需求。

Abstract: Spiking Neural Networks (SNNs) are dynamical systems that operate on spatiotemporal data, yet their learnable parameters are often limited to synaptic weights, contributing little to temporal pattern recognition. Learnable parameters that delay spike times can improve classification performance in temporal tasks, but existing methods rely on large networks and offline learning, making them unsuitable for real-time operation in resource-constrained environments. In this paper, we introduce synaptic and axonal delays to leaky integrate and fire (LIF)-based feedforward and recurrent SNNs, and propose three-factor learning rules to simultaneously learn delay parameters online. We employ a smooth Gaussian surrogate to approximate spike derivatives exclusively for the eligibility trace calculation, and together with a top-down error signal determine parameter updates. Our experiments show that incorporating delays improves accuracy by up to 20% over a weights-only baseline, and for networks with similar parameter counts, jointly learning weights and delays yields up to 14% higher accuracy. On the SHD speech recognition dataset, our method achieves similar accuracy to offline backpropagation-based approaches. Compared to state-of-the-art methods, it reduces model size by 6.6x and inference latency by 67%, with only a 2.4% drop in classification accuracy. Our findings benefit the design of power and area-constrained neuromorphic processors by enabling on-device learning and lowering memory requirements.

</details>


### [109] [QSLM: A Performance- and Memory-aware Quantization Framework with Tiered Search Strategy for Spike-driven Language Models](https://arxiv.org/abs/2601.00679)
*Rachmad Vidya Wicaksana Putra,Pasindu Wickramasinghe,Muhammad Shafique*

Main category: cs.NE

TL;DR: QSLM：一个自动量化框架，用于压缩脉冲驱动语言模型（SLM）的内存占用，同时满足性能和内存约束


<details>
  <summary>Details</summary>
Motivation: 虽然脉冲驱动语言模型（SLM）能显著降低处理功耗，但其内存占用仍然过大，不适合资源受限的嵌入式设备。手动量化方法虽然有效但耗时耗力，缺乏可扩展性。

Method: QSLM首先识别网络架构层次和层对量化的敏感性，然后采用分层量化策略（全局、块、模块级），利用多目标性能-内存权衡函数选择最终量化设置。

Result: QSLM将内存占用减少高达86.5%，功耗降低高达20%，在SST-2数据集上保持84.4%的情感分类准确率，在WikiText-2数据集上获得23.2的困惑度分数，接近原始非量化模型。

Conclusion: QSLM框架成功实现了对SLM的自动量化压缩，在显著减少内存占用和功耗的同时保持了高性能，为资源受限设备部署语言模型提供了可行方案。

Abstract: Large Language Models (LLMs) have been emerging as prominent AI models for solving many natural language tasks due to their high performance (e.g., accuracy) and capabilities in generating high-quality responses to the given inputs. However, their large computational cost, huge memory footprints, and high processing power/energy make it challenging for their embedded deployments. Amid several tinyLLMs, recent works have proposed spike-driven language models (SLMs) for significantly reducing the processing power/energy of LLMs. However, their memory footprints still remain too large for low-cost and resource-constrained embedded devices. Manual quantization approach may effectively compress SLM memory footprints, but it requires a huge design time and compute power to find the quantization setting for each network, hence making this approach not-scalable for handling different networks, performance requirements, and memory budgets. To bridge this gap, we propose QSLM, a novel framework that performs automated quantization for compressing pre-trained SLMs, while meeting the performance and memory constraints. To achieve this, QSLM first identifies the hierarchy of the given network architecture and the sensitivity of network layers under quantization, then employs a tiered quantization strategy (e.g., global-, block-, and module-level quantization) while leveraging a multi-objective performance-and-memory trade-off function to select the final quantization setting. Experimental results indicate that our QSLM reduces memory footprint by up to 86.5%, reduces power consumption by up to 20%, maintains high performance across different tasks (i.e., by up to 84.4% accuracy of sentiment classification on the SST-2 dataset and perplexity score of 23.2 for text generation on the WikiText-2 dataset) close to the original non-quantized model while meeting the performance and memory constraints.

</details>


### [110] [Cost Optimization in Production Line Using Genetic Algorithm](https://arxiv.org/abs/2601.00689)
*Alireza Rezaee*

Main category: cs.NE

TL;DR: 使用遗传算法解决生产流水线中的成本最优任务调度问题，比较了两种染色体编码策略（基于工位和基于任务），发现基于任务的编码在复杂约束下表现更好。


<details>
  <summary>Details</summary>
Motivation: 生产流水线中的任务调度是一个组合优化问题，存在复杂的约束条件（任务持续时间、执行成本、优先级约束、工位容量限制）。传统的基于梯度或解析方法难以处理这类非可微成本函数和复杂约束的组合调度问题，因此需要探索遗传算法等进化计算方法。

Method: 采用遗传算法框架，研究两种染色体编码策略：1) 基于工位的表示（使用JGAP库和SuperGene有效性检查）；2) 基于任务的表示（基因直接编码工位分配）。为每种编码调整标准遗传算子（交叉、变异、选择和替换）以保持可行性并驱动种群向低成本调度进化。在三种优先级结构（紧密耦合、松散耦合、非耦合）上进行实验验证。

Result: 实验结果表明，基于任务的编码比基于工位的编码具有更平滑的收敛性和更可靠的成本最小化效果，特别是在有效调度数量较大时。基于任务的编码在复杂约束和非可微成本函数环境下表现更优。

Conclusion: 遗传算法特别适合处理具有复杂约束和不可微成本函数的组合调度问题。基于任务的染色体编码策略在成本最优任务调度中表现优于基于工位的编码，为生产流水线调度提供了有效的解决方案。

Abstract: This paper presents a genetic algorithm (GA) approach to cost-optimal task scheduling in a production line. The system consists of a set of serial processing tasks, each with a given duration, unit execution cost, and precedence constraints, which must be assigned to an unlimited number of stations subject to a per-station duration bound. The objective is to minimize the total production cost, modeled as a station-wise function of task costs and the duration bound, while strictly satisfying all prerequisite and capacity constraints. Two chromosome encoding strategies are investigated: a station-based representation implemented using the JGAP library with SuperGene validity checks, and a task-based representation in which genes encode station assignments directly. For each encoding, standard GA operators (crossover, mutation, selection, and replacement) are adapted to preserve feasibility and drive the population toward lower-cost schedules. Experimental results on three classes of precedence structures-tightly coupled, loosely coupled, and uncoupled-demonstrate that the task-based encoding yields smoother convergence and more reliable cost minimization than the station-based encoding, particularly when the number of valid schedules is large. The study highlights the advantages of GA over gradient-based and analytical methods for combinatorial scheduling problems, especially in the presence of complex constraints and non-differentiable cost landscapes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [111] [μACP: A Formal Calculus for Expressive, Resource-Constrained Agent Communication](https://arxiv.org/abs/2601.00219)
*Arnab Mallick,Indraveni Chebolu*

Main category: cs.MA

TL;DR: μACP是一个形式化演算，用于在明确资源约束下实现表达性智能体通信，通过最小四动词基编码FIPA协议，建立信息论界限，并在边缘计算环境中实现高效协调。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通信协议存在两难：FIPA-ACL等协议语义丰富但资源消耗大，不适合受限环境；轻量级IoT协议效率高但表达能力不足。需要一种能兼顾语义表达性和可证明效率的统一框架。

Method: 提出μACP形式演算，建立资源约束智能体通信（RCAC）模型，证明{PING, TELL, ASK, OBSERVE}四动词基足以编码有限状态FIPA协议，建立消息复杂度的紧信息论界限，实现部分同步和崩溃故障下的共识。

Result: 形式验证（TLA⁺模型检查和Coq机械化不变量）证明安全性和有界性，大规模系统仿真显示中位数端到端消息延迟34ms（95百分位104ms），在严重资源约束下优于现有智能体和IoT协议。

Conclusion: μACP统一了语义表达性和可证明效率，为下一代资源受限多智能体系统提供了严格基础，实现了表达性通信与资源效率的调和。

Abstract: Agent communication remains a foundational problem in multi-agent systems: protocols such as FIPA-ACL guarantee semantic richness but are intractable for constrained environments, while lightweight IoT protocols achieve efficiency at the expense of expressiveness. This paper presents $μ$ACP, a formal calculus for expressive agent communication under explicit resource bounds. We formalize the Resource-Constrained Agent Communication (RCAC) model, prove that a minimal four-verb basis \textit{\{PING, TELL, ASK, OBSERVE\}} is suffices to encode finite-state FIPA protocols, and establish tight information-theoretic bounds on message complexity. We further show that $μ$ACP can implement standard consensus under partial synchrony and crash faults, yielding a constructive coordination framework for edge-native agents. Formal verification in TLA$^{+}$ (model checking) and Coq (mechanized invariants) establishes safety and boundedness, and supports liveness under modeled assumptions. Large-scale system simulations confirm ACP achieves a median end-to-end message latency of 34 ms (95th percentile 104 ms) at scale, outperforming prior agent and IoT protocols under severe resource constraints. The main contribution is a unified calculus that reconciles semantic expressiveness with provable efficiency, providing a rigorous foundation for the next generation of resource-constrained multi-agent systems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [112] [Sparse FEONet: A Low-Cost, Memory-Efficient Operator Network via Finite-Element Local Sparsity for Parametric PDEs](https://arxiv.org/abs/2601.00672)
*Seungchan Ko,Jiyeon Kim,Dongwook Shin*

Main category: math.NA

TL;DR: 提出稀疏网络架构的FEONet改进版本，通过利用有限元结构降低计算成本，同时保持精度


<details>
  <summary>Details</summary>
Motivation: 原始FEONet在处理大规模问题时，随着单元数量增加，计算成本上升且精度可能下降，需要改进以应对大规模问题

Method: 提出基于有限元结构的新稀疏网络架构，通过稀疏连接减少计算复杂度

Result: 稀疏网络在计算成本和效率方面有显著提升，同时保持可比的精度，并通过理论分析验证了有效性和稳定性

Conclusion: 提出的稀疏架构成功解决了FEONet在大规模问题中的计算挑战，为参数化PDE求解提供了更高效的算子学习方法

Abstract: In this paper, we study the finite element operator network (FEONet), an operator-learning method for parametric problems, originally introduced in J. Y. Lee, S. Ko, and Y. Hong, Finite Element Operator Network for Solving Elliptic-Type Parametric PDEs, SIAM J. Sci. Comput., 47(2), C501-C528, 2025. FEONet realizes the parameter-to-solution map on a finite element space and admits a training procedure that does not require training data, while exhibiting high accuracy and robustness across a broad class of problems. However, its computational cost increases and accuracy may deteriorate as the number of elements grows, posing notable challenges for large-scale problems. In this paper, we propose a new sparse network architecture motivated by the structure of the finite elements to address this issue. Throughout extensive numerical experiments, we show that the proposed sparse network achieves substantial improvements in computational cost and efficiency while maintaining comparable accuracy. We also establish theoretical results demonstrating that the sparse architecture can approximate the target operator effectively and provide a stability analysis ensuring reliable training and prediction.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [113] [Solving nonlinear subsonic compressible flow in infinite domain via multi-stage neural networks](https://arxiv.org/abs/2601.00342)
*Xuehui Qian,Hongkai Tao,Yongji Wang*

Main category: physics.flu-dyn

TL;DR: 使用物理信息神经网络(PINNs)解决无限域非线性可压缩势流方程，通过坐标变换和多阶段方法实现高精度求解，量化了传统有限域和线性化方法的误差。


<details>
  <summary>Details</summary>
Motivation: 传统方法在模拟亚音速可压缩流时存在局限性：线性化方程或有限截断域会引入不可忽略的误差，限制了在实际航空设计中的应用。需要一种能够处理无限域非线性方程的高精度方法。

Method: 提出基于物理信息神经网络(PINNs)的新框架，通过坐标变换处理无限域问题，将物理渐近约束嵌入网络架构，并采用多阶段PINN(MS-PINN)方法迭代最小化残差，实现接近机器精度的求解精度。

Result: 在圆形和椭圆形几何体上验证了框架的有效性，与传统有限域和线性化解进行比较。结果表明，在较高马赫数下，域截断和线性化会引入显著误差，而新框架能够提供高保真度的计算流体动力学解决方案。

Conclusion: 该PINNs框架是计算流体动力学中一个稳健、高保真的工具，能够准确求解无限域非线性可压缩势流方程，为飞机设计提供了更可靠的模拟方法。

Abstract: In aerodynamics, accurately modeling subsonic compressible flow over airfoils is critical for aircraft design. However, solving the governing nonlinear perturbation velocity potential equation presents computational challenges. Traditional approaches often rely on linearized equations or finite, truncated domains, which introduce non-negligible errors and limit applicability in real-world scenarios. In this study, we propose a novel framework utilizing Physics-Informed Neural Networks (PINNs) to solve the full nonlinear compressible potential equation in an unbounded (infinite) domain. We address the unbounded-domain and convergence challenges inherent in standard PINNs by incorporating a coordinate transformation and embedding physical asymptotic constraints directly into the network architecture. Furthermore, we employ a Multi-Stage PINN (MS-PINN) approach to iteratively minimize residuals, achieving solution accuracy approaching machine precision. We validate this framework by simulating flow over circular and elliptical geometries, comparing our results against traditional finite-domain and linearized solutions. Our findings quantify the noticeable discrepancies introduced by domain truncation and linearization, particularly at higher Mach numbers, and demonstrate that this new framework is a robust, high-fidelity tool for computational fluid dynamics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [114] [Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes](https://arxiv.org/abs/2601.00012)
*Shahar Ain Kedem,Itamar Zimerman,Eliya Nachmani*

Main category: eess.SP

TL;DR: 提出一种受NeRF启发的EEG信号处理方法，将脑电信号表示为连续神经场，实现任意时空分辨率的信号渲染和电极数据模拟


<details>
  <summary>Details</summary>
Motivation: EEG数据面临长度可变、信噪比低、个体差异大、时间漂移等挑战，现有深度学习方法难以有效处理，需要开发新的建模方法

Method: 借鉴NeRF思想，将EEG电极类比为不同视角的图像，训练神经网络学习连续神经活动表示，生成固定大小的权重向量编码整个信号

Result: 方法能连续可视化任意分辨率的脑活动，重建原始EEG信号，有效模拟不存在电极的数据，提升标准EEG处理网络的性能

Conclusion: NeRF启发的EEG表示方法为解决EEG建模挑战提供了新思路，实现了连续时空表示和信号增强，有望改进脑电信号处理

Abstract: Electroencephalography (EEG) data present unique modeling challenges because recordings vary in length, exhibit very low signal to noise ratios, differ significantly across participants, drift over time within sessions, and are rarely available in large and clean datasets. Consequently, developing deep learning methods that can effectively process EEG signals remains an open and important research problem. To tackle this problem, this work presents a new method inspired by Neural Radiance Fields (NeRF). In computer vision, NeRF techniques train a neural network to memorize the appearance of a 3D scene and then uses its learned parameters to render and edit the scene from any viewpoint. We draw an analogy between the discrete images captured from different viewpoints used to learn a continuous 3D scene in NeRF, and EEG electrodes positioned at different locations on the scalp, which are used to infer the underlying representation of continuous neural activity. Building on this connection, we show that a neural network can be trained on a single EEG sample in a NeRF style manner to produce a fixed size and informative weight vector that encodes the entire signal. Moreover, via this representation we can render the EEG signal at previously unseen time steps and spatial electrode positions. We demonstrate that this approach enables continuous visualization of brain activity at any desired resolution, including ultra high resolution, and reconstruction of raw EEG signals. Finally, our empirical analysis shows that this method can effectively simulate nonexistent electrodes data in EEG recordings, allowing the reconstructed signal to be fed into standard EEG processing networks to improve performance.

</details>


### [115] [Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI](https://arxiv.org/abs/2601.00014)
*Eran Zvuloni,Ronit Almog,Michael Glikson,Shany Brimer Biton,Ilan Green,Izhar Laufer,Offer Amir,Joachim A. Behar*

Main category: eess.SP

TL;DR: 使用深度学习模型DeepHHF分析24小时单导联心电图数据，可在5年内预测心衰风险，准确率达AUC 0.80，优于30秒片段模型和临床评分


<details>
  <summary>Details</summary>
Motivation: 心衰影响11.8%的65岁以上成年人，降低生活质量和寿命。预防心衰可减少发病率和死亡率。需要一种非侵入性、廉价且广泛可及的心衰风险预测工具。

Method: 使用Technion-Leumit Holter ECG数据集（69,663条记录，47,729名患者，20年数据）。开发深度学习模型DeepHHF，训练于24小时单导联心电图数据，并与30秒片段模型和临床评分进行比较。

Result: DeepHHF的AUC达到0.80，优于其他方法。高风险个体住院或死亡风险增加两倍。可解释性分析显示模型关注心律失常和心脏异常，关键注意力集中在上午8点至下午3点之间。

Conclusion: 深度学习建模24小时连续心电图数据可行，能捕捉阵发性事件和昼夜节律变化，对可靠风险预测至关重要。AI应用于单导联Holter心电图具有非侵入性、廉价和广泛可及性，是心衰风险预测的有前景工具。

Abstract: Heart failure (HF) affects 11.8% of adults aged 65 and older, reducing quality of life and longevity. Preventing HF can reduce morbidity and mortality. We hypothesized that artificial intelligence (AI) applied to 24-hour single-lead electrocardiogram (ECG) data could predict the risk of HF within five years. To research this, the Technion-Leumit Holter ECG (TLHE) dataset, including 69,663 recordings from 47,729 patients, collected over 20 years was used. Our deep learning model, DeepHHF, trained on 24-hour ECG recordings, achieved an area under the receiver operating characteristic curve of 0.80 that outperformed a model using 30-second segments and a clinical score. High-risk individuals identified by DeepHHF had a two-fold chance of hospitalization or death incidents. Explainability analysis showed DeepHHF focused on arrhythmias and heart abnormalities, with key attention between 8 AM and 3 PM. This study highlights the feasibility of deep learning to model 24-hour continuous ECG data, capturing paroxysmal events and circadian variations essential for reliable risk prediction. Artificial intelligence applied to single-lead Holter ECG is non-invasive, inexpensive, and widely accessible, making it a promising tool for HF risk prediction.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [116] [Group Cross-Correlations with Faintly Constrained Filters](https://arxiv.org/abs/2601.00045)
*Benedikt Fluhr*

Main category: math.DS

TL;DR: 提出更宽松的群互相关概念，解决非紧稳定子群的不兼容问题，推广到非传递群作用并放宽幺模性假设


<details>
  <summary>Details</summary>
Motivation: 现有群互相关概念中的滤波器约束过于严格，导致无法处理具有非紧稳定子群的群作用，需要更一般化的理论框架

Method: 重新定义群互相关概念，放宽滤波器约束条件，允许非传递群作用，并减弱幺模性假设要求

Result: 建立了更一般的群互相关理论，解决了非紧稳定子群的不兼容问题，推广了现有结果的应用范围

Conclusion: 新框架扩展了群互相关理论的应用范围，为处理更一般的群作用提供了数学基础

Abstract: We provide a notion of group cross-correlations, where the associated filter is not as tightly constrained as in the previous literature. This resolves an incompatibility previous constraints have for group actions with non-compact stabilizers. Moreover, we generalize previous results to group actions that are not necessarily transitive, and we weaken the common assumption of unimodularity.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [117] [StockBot 2.0: Vanilla LSTMs Outperform Transformer-based Forecasting for Stock Prices](https://arxiv.org/abs/2601.00197)
*Shaswat Mohanty*

Main category: cs.CE

TL;DR: 研究发现，在金融时间序列预测中，经过精心构建的普通LSTM模型在默认超参数设置下，比基于注意力机制和Transformer的模型表现更好，具有更高的预测准确性和更稳定的买卖决策。


<details>
  <summary>Details</summary>
Motivation: 金融市场的准确预测面临长期挑战，包括复杂的时间依赖性、非线性动态和高波动性。研究旨在系统评估现代时间序列预测模型在金融领域的表现，特别是比较注意力机制、卷积和循环神经网络模型。

Method: 基于之前的循环神经网络框架，提出了增强的StockBot架构，在统一的实验设置下系统评估基于注意力的模型、卷积模型和循环时间序列预测模型。使用共同的默认超参数集进行训练。

Result: 实证评估显示，精心构建的普通LSTM模型在预测准确性和买卖决策稳定性方面表现最优，超越了基于注意力和Transformer启发的模型。这表明循环序列模型在金融时间序列预测中具有鲁棒性和数据效率。

Conclusion: 在缺乏广泛超参数调优或数据有限的情况下，循环序列模型（特别是LSTM）在金融时间序列预测中表现更优。这些结果强调了架构归纳偏置在数据受限的市场预测任务中的重要性。

Abstract: Accurate forecasting of financial markets remains a long-standing challenge due to complex temporal and often latent dependencies, non-linear dynamics, and high volatility. Building on our earlier recurrent neural network framework, we present an enhanced StockBot architecture that systematically evaluates modern attention-based, convolutional, and recurrent time-series forecasting models within a unified experimental setting. While attention-based and transformer-inspired models offer increased modeling flexibility, extensive empirical evaluation reveals that a carefully constructed vanilla LSTM consistently achieves superior predictive accuracy and more stable buy/sell decision-making when trained under a common set of default hyperparameters. These results highlight the robustness and data efficiency of recurrent sequence models for financial time-series forecasting, particularly in the absence of extensive hyperparameter tuning or the availability of sufficient data when discretized to single-day intervals. Additionally, these results underscore the importance of architectural inductive bias in data-limited market prediction tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [118] [Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback](https://arxiv.org/abs/2601.00224)
*Yan Sun,Ming Cai,Stanley Kok*

Main category: cs.CL

TL;DR: 论文提出Q*和Feedback+两种验证技术，通过反向翻译、语义匹配和执行反馈来提升LLM助手在商业分析中的准确性和可靠性，减少错误率和任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 当前对话式商业分析系统缺乏内置验证机制，用户需要手动验证可能错误的输出，这影响了企业工作流程中LLM助手的可靠性和实用性。

Method: 提出Q*（反向翻译和语义匹配）和Feedback+（执行反馈引导代码优化）两种互补验证技术，嵌入生成器-判别器框架，将验证责任从用户转移到系统。

Result: 在Spider、Bird和GSM8K三个基准数据集上的评估显示，Q*和Feedback+都能显著降低错误率和任务完成时间。

Conclusion: 该研究为构建更可靠的企业级GenAI系统提供了一个设计导向的框架，同时识别了反向翻译作为关键瓶颈，为未来改进指明了方向。

Abstract: As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.

</details>


### [119] [Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations](https://arxiv.org/abs/2601.00282)
*Qianli Wang,Nils Feldhus,Pepa Atanasova,Fedor Splitt,Simon Ostermann,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化对LLM自解释能力的影响：量化通常导致自解释质量适度下降（最高4.4%）和忠实度下降（最高2.38%），但不会破坏量化作为模型压缩技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于加速大型语言模型推理和部署，但其对自解释（模型为自身输出生成的解释）的影响尚未被探索。自解释需要模型推理自身的决策过程，这种能力可能对量化特别敏感。由于自解释在高风险应用中越来越依赖，理解量化是否会降低自解释质量和忠实度至关重要。

Method: 研究两种自解释类型：自然语言解释和反事实示例，使用三种常见量化技术在不同比特宽度下对LLM进行量化。通过用户研究评估量化对自解释连贯性和可信度的影响。

Result: 量化通常导致自解释质量适度下降（最高4.4%）和忠实度下降（最高2.38%）。用户研究表明量化降低了自解释的连贯性和可信度（最高8.5%）。相比小模型，大模型在自解释质量方面对量化的韧性有限，但在保持忠实度方面更好。没有量化技术能在任务准确性、自解释质量和忠实度三个方面都表现优异。

Conclusion: 量化对自解释的影响因上下文而异，建议针对特定用例验证自解释质量，特别是对更敏感的自然语言解释。尽管如此，自解释质量和忠实度的相对较小恶化不会破坏量化作为模型压缩技术的有效性。

Abstract: Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\%) and faithfulness (up to 2.38\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.

</details>


### [120] [BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics](https://arxiv.org/abs/2601.00366)
*Taj Gillin,Adam Lalani,Kenneth Zhang,Marcel Mateos Salles*

Main category: cs.CL

TL;DR: BERT-JEPA (BEPA) 将JEPA训练目标加入BERT风格模型，解决[CLS]嵌入空间坍缩问题，创建语言无关空间，提升多语言基准性能


<details>
  <summary>Details</summary>
Motivation: JEPA作为新型自监督训练技术在多个领域展现潜力，但BERT风格模型的[CLS]嵌入空间存在坍缩问题，需要改进以创建更好的语言无关表示空间

Method: 在BERT风格模型中添加JEPA训练目标，形成BERT-JEPA (BEPA)训练范式，通过联合嵌入预测架构解决[CLS]嵌入空间坍缩问题

Result: BERT-JEPA在多语言基准测试中表现出性能提升，成功将[CLS]嵌入空间转变为语言无关空间

Conclusion: JEPA训练目标能有效增强BERT风格模型的多语言表示能力，为解决嵌入空间坍缩问题提供新思路

Abstract: Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.

</details>


### [121] [Noise-Aware Named Entity Recognition for Historical VET Documents](https://arxiv.org/abs/2601.00488)
*Alexander M. Esser,Jens Dörpinghaus*

Main category: cs.CL

TL;DR: 该论文提出了一种针对职业教育与培训领域历史数字化文档的鲁棒命名实体识别方法，通过噪声感知训练、合成OCR错误注入和迁移学习来应对OCR噪声问题。


<details>
  <summary>Details</summary>
Motivation: 职业教育与培训领域的历史数字化文档存在OCR噪声问题，这影响了命名实体识别的准确性。目前缺乏专门针对该领域多实体类型识别的方法，需要开发能够处理噪声的鲁棒NER系统。

Method: 采用噪声感知训练，通过合成注入OCR错误，结合迁移学习和多阶段微调。系统比较了三种互补策略：在噪声数据、干净数据和人工数据上训练。该方法专门针对德语文档设计，但可扩展到任意语言。

Result: 实验结果表明，领域特定和噪声感知的微调显著提高了在噪声条件下的鲁棒性和准确性。这是首批能够识别职业教育与培训文档中多种实体类型的方法之一。

Conclusion: 提出的方法有效解决了职业教育与培训领域历史文档的NER问题，提供了公开可用的代码，支持在领域特定上下文中的可重复噪声感知NER研究。

Abstract: This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [122] [From Consensus to Chaos: A Vulnerability Assessment of the RAFT Algorithm](https://arxiv.org/abs/2601.00273)
*Tamer Afifi,Abdelfatah Hegazy,Ehab Abousaif*

Main category: cs.CR

TL;DR: 对RAFT分布式共识协议进行系统性安全分析，发现其易受消息重放和伪造攻击，提出基于密码学、认证消息验证和新鲜度检查的安全增强方案。


<details>
  <summary>Details</summary>
Motivation: RAFT协议虽然以简单、可靠、高效著称，但其安全属性未被充分认识，实现中存在多种攻击漏洞，可能导致数据不一致性，需要系统性安全分析。

Method: 通过模拟场景分析RAFT协议的消息传递机制漏洞，特别是消息重放和伪造攻击的可行性，识别设计中的关键弱点。

Result: 发现恶意攻击者可以利用协议的消息传递机制重新引入旧消息，破坏共识过程并导致数据不一致，验证了这些攻击的实际可行性。

Conclusion: 提出基于密码学、认证消息验证和新鲜度检查的新方法，为增强RAFT实现的安全性提供框架，指导开发更具弹性的分布式系统。

Abstract: In recent decades, the RAFT distributed consensus algorithm has become a main pillar of the distributed systems ecosystem, ensuring data consistency and fault tolerance across multiple nodes. Although the fact that RAFT is well known for its simplicity, reliability, and efficiency, its security properties are not fully recognized, leaving implementations vulnerable to different kinds of attacks and threats, which can transform the RAFT harmony of consensus into a chaos of data inconsistency. This paper presents a systematic security analysis of the RAFT protocol, with a specific focus on its susceptibility to security threats such as message replay attacks and message forgery attacks. Examined how a malicious actor can exploit the protocol's message-passing mechanism to reintroduce old messages, disrupting the consensus process and leading to data inconsistency. The practical feasibility of these attacks is examined through simulated scenarios, and the key weaknesses in RAFT's design that enable them are identified. To address these vulnerabilities, a novel approach based on cryptography, authenticated message verification, and freshness check is proposed. This proposed solution provides a framework for enhancing the security of the RAFT implementations and guiding the development of more resilient distributed systems.

</details>


### [123] [Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution](https://arxiv.org/abs/2601.00418)
*Prajwal Panth,Sahaj Raj Malla*

Main category: cs.CR

TL;DR: CPPDD是一个轻量级、后设置自主的隐私保护数据分发框架，通过双层保护机制实现安全的多客户端数据聚合，具有线性可扩展性和高效性。


<details>
  <summary>Details</summary>
Motivation: 解决安全多方计算中存在的可扩展性、信任最小化和可验证性问题，特别是在受监管和资源受限环境中，需要一种轻量级且能抵抗合谋攻击的隐私保护数据聚合方案。

Method: 采用双层保护机制：每客户端仿射掩码和优先级驱动的顺序共识锁定。通过步骤校验和（sigma_S）和数据校验和（sigma_D）实现去中心化完整性验证，支持标量、向量和矩阵负载，具有O(N*D)计算和通信复杂度。

Result: 在MNIST数据集上的实验显示，框架可线性扩展到N=500个客户端，每客户端计算时间低于毫秒级，实现100%恶意偏差检测和精确数据恢复，计算量比MPC和HE基线低3-4个数量级。

Conclusion: CPPDD框架为安全投票、联盟联邦学习、区块链托管和地理信息能力建设等应用提供了原子协作能力，填补了可扩展性、信任最小化和可验证多方计算的关键空白。

Abstract: We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.

</details>


### [124] [Security in the Age of AI Teammates: An Empirical Study of Agentic Pull Requests on GitHub](https://arxiv.org/abs/2601.00477)
*Mohammed Latif Siddiq,Xinye Zhao,Vinicius Carvalho Lopes,Beatrice Casey,Joanna C. S. Santos*

Main category: cs.CR

TL;DR: 对33,000多个AI代理编写的GitHub PR进行大规模实证分析，发现安全相关PR占约4%，主要涉及测试、文档、配置等安全加固活动而非漏洞修复，相比非安全PR合并率更低、审核时间更长，PR复杂度比安全主题更能预测拒绝


<details>
  <summary>Details</summary>
Motivation: 随着自主编码代理在软件工程中作为AI队友被广泛部署，独立编写修改生产代码的PR，需要系统研究这些代理如何实际影响软件安全、安全相关贡献如何被评审和接受，以及哪些信号与PR拒绝相关

Method: 使用AIDev数据集对33,000多个GitHub PR进行大规模实证分析，通过关键词过滤识别安全相关PR并手动验证（确认1,293个），分析流行度、接受结果和审核延迟，应用定性开放编码识别安全相关行动和意图，检查评审元数据识别PR拒绝的早期信号

Result: 安全相关AI代理PR占代理活动的约4%，主要执行支持性安全加固活动（测试、文档、配置、错误处理改进），相比非安全PR合并率更低、审核时间更长，PR拒绝更与复杂性和冗长度相关而非明确的安全主题

Conclusion: 自主编码代理在软件安全中扮演重要角色，主要进行安全加固而非漏洞修复，人类对安全相关PR审查更严格，PR拒绝主要受复杂性和冗长度影响而非安全主题本身

Abstract: Autonomous coding agents are increasingly deployed as AI teammates in modern software engineering, independently authoring pull requests (PRs) that modify production code at scale. This study aims to systematically characterize how autonomous coding agents contribute to software security in practice, how these security-related contributions are reviewed and accepted, and which observable signals are associated with PR rejection. We conduct a large-scale empirical analysis of agent-authored PRs using the AIDev dataset, comprising of over 33,000 curated PRs from popular GitHub repositories. Security-relevant PRs are identified using a keyword filtering strategy, followed by manual validation, resulting in 1,293 confirmed security-related agentic-PRs. We then analyze prevalence, acceptance outcomes, and review latency across autonomous agents, programming ecosystems, and types of code changes. Moreover, we apply qualitative open coding to identify recurring security-related actions and underlying intents, and examine review metadata to identify early signals associated with PR rejection. Security-related Agentic-PRs constitute a meaningful share of agent activity (approximately 4\%). Rather than focusing solely on narrow vulnerability fixes, agents most frequently perform supportive security hardening activities, including testing, documentation, configuration, and improved error handling. Compared to non-security PRs, security-related Agentic-PRs exhibit lower merge rates and longer review latency, reflecting heightened human scrutiny, with variation across agents and programming ecosystems. PR rejection is more strongly associated with PR complexity and verbosity than with explicit security topics.

</details>


### [125] [NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion](https://arxiv.org/abs/2601.00389)
*Muhammad Bilal,Omer Tariq,Hasan Ahmed*

Main category: cs.CR

TL;DR: NOS-Gate：一种用于独立网关的流式入侵检测系统，通过轻量级两状态单元处理加密流量元数据，在低CPU和延迟预算下实现高效检测与缓解。


<details>
  <summary>Details</summary>
Motivation: 加密流量中的时序和突发模式可能泄露信息，自适应攻击者可利用这些信息绕过检测。独立消费者网关需要在仅使用元数据、且CPU和延迟预算有限的情况下，对流式加密流量进行入侵检测。

Method: 提出NOS-Gate系统，为每个流实例化基于网络优化脉冲(NOS)动力学的轻量级两状态单元。系统对固定长度元数据特征窗口进行评分，在K-of-M持续规则下触发可逆缓解措施，暂时降低流在加权公平队列(WFQ)中的权重。

Result: 在时序控制规避攻击下评估，在0.1%误报率下，NOS-Gate达到0.952的事件召回率，优于基线方法的0.857。在门控机制下，显著降低p99.9排队延迟和p99.9附带延迟，平均每流窗口评分成本约2.09微秒。

Conclusion: NOS-Gate为独立网关提供了一种高效的流式入侵检测解决方案，能够在仅使用加密流量元数据、低计算开销的情况下有效检测和缓解攻击，同时保持网络服务质量。

Abstract: Timing and burst patterns can leak through encryption, and an adaptive adversary can exploit them. This undermines metadata-only detection in a stand-alone consumer gateway. Therefore, consumer gateways need streaming intrusion detection on encrypted traffic using metadata only, under tight CPU and latency budgets. We present a streaming IDS for stand-alone gateways that instantiates a lightweight two-state unit derived from Network-Optimised Spiking (NOS) dynamics per flow, named NOS-Gate. NOS-Gate scores fixed-length windows of metadata features and, under a $K$-of-$M$ persistence rule, triggers a reversible mitigation that temporarily reduces the flow's weight under weighted fair queueing (WFQ). We evaluate NOS-Gate under timing-controlled evasion using an executable 'worlds' benchmark that specifies benign device processes, auditable attacker budgets, contention structure, and packet-level WFQ replay to quantify queue impact. All methods are calibrated label-free via burn-in quantile thresholding. Across multiple reproducible worlds and malicious episodes, at an achieved $0.1%$ false-positive operating point, NOS-Gate attains 0.952 incident recall versus 0.857 for the best baseline in these runs. Under gating, it reduces p99.9 queueing delay and p99.9 collateral delay with a mean scoring cost of ~ 2.09 μs per flow-window on CPU.

</details>


### [126] [Towards Understanding and Characterizing Vulnerabilities in Intelligent Connected Vehicles through Real-World Exploits](https://arxiv.org/abs/2601.00627)
*Yuelin Wang,Yuqiao Ning,Yanbang Sun,Xiaofei Xie,Zhihua Xie,Yang Chen,Zhen Guo,Shihao Xue,Junjie Wang,Sen Chen*

Main category: cs.CR

TL;DR: 首次大规模实证研究智能网联汽车漏洞，通过分析649个可利用漏洞评估现有分类体系，发现新漏洞位置和类型，提供数据驱动的安全洞见。


<details>
  <summary>Details</summary>
Motivation: 现有智能网联汽车安全研究大多关注特定子系统，缺乏系统性理解；且依赖主观分析（如调查访谈），理论发现与实际攻击存在差距。

Method: 分析现有ICV安全文献总结漏洞位置和类型分类体系；收集649个可利用漏洞（592个来自2023-2024年8次匿名杯竞赛，57个来自研究人员日常提交）；评估分类体系覆盖度，识别差距。

Result: 发现1个新漏洞位置和13个新漏洞类型；将漏洞分为6种威胁类型（如隐私数据泄露）和4个风险等级；分析了参赛者技能和涉及的ICV类型；数据集已公开。

Conclusion: 本研究提供了首个数据驱动的ICV漏洞全面分析，为研究人员、行业从业者和政策制定者提供可操作的洞见，支持未来研究。

Abstract: Intelligent Connected Vehicles (ICVs) are a core component of modern transportation systems, and their security is crucial as it directly relates to user safety. Despite prior research, most existing studies focus only on specific sub-components of ICVs due to their inherent complexity. As a result, there is a lack of systematic understanding of ICV vulnerabilities. Moreover, much of the current literature relies on human subjective analysis, such as surveys and interviews, which tends to be high-level and unvalidated, leaving a significant gap between theoretical findings and real-world attacks. To address this issue, we conducted the first large-scale empirical study on ICV vulnerabilities. We began by analyzing existing ICV security literature and summarizing the prevailing taxonomies in terms of vulnerability locations and types. To evaluate their real-world relevance, we collected a total of 649 exploitable vulnerabilities, including 592 from eight ICV vulnerability discovery competitions, Anonymous Cup, between January 2023 and April 2024, covering 48 different vehicles. The remaining 57 vulnerabilities were submitted daily by researchers. Based on this dataset, we assessed the coverage of existing taxonomies and identified several gaps, discovering one new vulnerability location and 13 new vulnerability types. We further categorized these vulnerabilities into 6 threat types (e.g., privacy data breach) and 4 risk levels (ranging from low to critical) and analyzed participants' skills and the types of ICVs involved in the competitions. This study provides a comprehensive and data-driven analysis of ICV vulnerabilities, offering actionable insights for researchers, industry practitioners, and policymakers. To support future research, we have made our vulnerability dataset publicly available.

</details>


### [127] [Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing](https://arxiv.org/abs/2601.00042)
*Manish Bhatt,Adrian Wood,Idan Habler,Ammar Al-Kahfah*

Main category: cs.CR

TL;DR: 研究评估GPT-4o-mini工具使用安全性的方法学发现：随机种子方差对结果影响最大（8倍差异），多种子平均可降低方差；奖励塑造会损害性能；简单状态签名优于复杂签名；集成方法提供攻击类型多样性


<details>
  <summary>Details</summary>
Motivation: 尽管经过安全训练，生产级LLM代理的工具使用能力仍需安全测试。需要评估安全测试方法学的可靠性，特别是随机种子方差、算法参数、奖励塑造等因素对测试结果的影响

Method: 采用Go-Explore方法，对GPT-4o-mini进行28次实验运行，涵盖六个研究问题。比较不同随机种子、算法参数、奖励塑造策略、状态签名复杂度，以及单代理与集成方法的差异

Result: 随机种子方差主导算法参数影响，导致8倍结果差异；单种子比较不可靠，多种子平均显著降低方差。奖励塑造在94%运行中导致探索崩溃或产生18个未经验证的误报。简单状态签名优于复杂签名。集成方法提供攻击类型多样性，单代理优化特定攻击类型的覆盖率

Conclusion: 安全测试中，随机种子方差和针对性领域知识可能比算法复杂度更重要。多种子平均可提高结果可靠性，应避免奖励塑造，使用简单状态签名，并根据测试目标选择单代理或集成方法

Abstract: Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.

</details>


### [128] [Rectifying Adversarial Examples Using Their Vulnerabilities](https://arxiv.org/abs/2601.00270)
*Fumiya Morimoto,Ryuto Morita,Satoshi Ono*

Main category: cs.CR

TL;DR: 提出一种对抗样本修正方法，通过重新攻击对抗样本来估计原始正确标签，无需参数调整或预训练，能处理多种攻击类型。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本防御方法主要关注检测而非修正，但某些应用（如自动驾驶交通标志识别）需要正确恢复原始类别。现有方法难以有效修正黑盒攻击或目标攻击产生的对抗样本。

Method: 基于重新攻击对抗样本的思路，将对抗样本移出决策边界以预测正确标签。仅以对抗样本为输入，无需参数调整或预训练，能处理白盒、黑盒和目标攻击。

Result: 该方法在各种攻击方法（包括目标攻击和黑盒攻击）下均表现稳定，在对抗多种攻击的稳定性方面优于传统修正和输入变换方法。

Conclusion: 提出的对抗样本修正方法能有效处理多种攻击类型，无需复杂调整，为需要恢复原始类别的安全关键应用提供了实用解决方案。

Abstract: Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.

</details>


### [129] [Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing](https://arxiv.org/abs/2601.00384)
*Md Mahbub Hasan,Marcus Sternhagen,Krishna Chandra Roy*

Main category: cs.CR

TL;DR: 该研究针对3D打印系统的网络安全威胁，通过中间人攻击拦截并篡改G代码文件，导致结构缺陷但外观正常的打印件，并提出基于Transformer编码器和对比学习的无监督入侵检测系统来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 增材制造在航空航天、汽车和医疗等关键领域的快速应用带来了新的网络安全威胁，特别是在CAD设计和机器执行层之间的接口处。传统的切片软件和运行时接口无法检测到这些隐蔽的攻击，导致结构缺陷但外观正常的打印件，可能造成严重的安全隐患。

Method: 研究采用多层中间人攻击威胁模型，针对Creality K1 Max和Ender 3两种FDM系统进行攻击实验。防御方面提出无监督入侵检测系统，使用冻结的Transformer编码器提取系统行为语义表示，通过对比学习训练投影头获得异常敏感嵌入，最后采用基于聚类的方法和自注意力自编码器进行分类。

Result: 实验结果表明，该方法能够有效区分正常和受攻击的执行过程，成功检测到中间人攻击导致的G代码篡改，防御了多种隐蔽的破坏场景。

Conclusion: 增材制造系统面临严重的网络安全威胁，特别是中间人攻击可能导致结构缺陷但外观正常的打印件。提出的基于Transformer和对比学习的无监督入侵检测系统能够有效防御此类攻击，为3D打印系统的安全提供了重要保障。

Abstract: Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.

</details>


### [130] [Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback](https://arxiv.org/abs/2601.00509)
*Vidyut Sriram,Sawan Pandita,Achintya Lakshmanan,Aneesh Shamraj,Suman Saha*

Main category: cs.CR

TL;DR: 论文提出了一种检索增强的多工具修复工作流，通过编译器诊断、安全扫描和符号执行等工具，让代码生成LLM迭代修复自身输出中的安全漏洞和错误，显著提升了代码的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码常常存在安全漏洞、逻辑不一致和编译错误。虽然已有研究表明结构化反馈、静态分析、检索增强和执行优化对LLM有帮助，但需要更系统的方法来提升生成代码的质量和安全性。

Method: 提出检索增强的多工具修复工作流：1) 使用单个代码生成LLM迭代优化输出；2) 利用编译器诊断、CodeQL安全扫描和KLEE符号执行提供反馈；3) 通过轻量级嵌入模型进行语义检索，获取先前成功的修复示例（特别是安全相关示例）来指导生成。

Result: 在DeepSeek-Coder-1.3B和CodeLlama-7B生成的3,242个程序数据集上评估：DeepSeek的安全漏洞减少96%；CodeLlama的关键安全缺陷率从58.55%降至22.19%，表明即使对"顽固"模型，工具辅助的自我修复也有效。

Conclusion: 工具辅助的自我修复工作流能显著提升LLM生成代码的鲁棒性和安全性，特别是通过多工具反馈和检索增强机制，即使对较大模型也能有效减少安全漏洞。

Abstract: Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on "stubborn" models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [131] [Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging](https://arxiv.org/abs/2601.00041)
*Fatemeh Hosseinabadi,Mohammad Mojtaba Rohani*

Main category: eess.IV

TL;DR: 使用ResNetRS、RegNet和EfficientNetV2三种CNN架构进行迁移学习，对儿科胸部X光片进行肺炎自动分类，RegNet表现最佳（准确率92.4%，敏感性90.1%）。


<details>
  <summary>Details</summary>
Motivation: 儿科肺炎是全球儿童发病率和死亡率的主要原因，及时准确诊断至关重要，但常受限于放射学专业知识和儿科影像的生理及程序复杂性。

Method: 从公开数据集中提取1,000张儿科胸部X光图像进行预处理和二元分类标注，使用预训练的ImageNet权重对ResNetRS、RegNet和EfficientNetV2三种CNN架构进行微调。

Result: RegNet获得最高分类性能（准确率92.4%，敏感性90.1%），其次是ResNetRS（准确率91.9%，敏感性89.3%）和EfficientNetV2（准确率88.5%，敏感性88.1%）。

Conclusion: 基于CNN的迁移学习方法在儿科肺炎自动分类中表现出色，RegNet架构特别适合此任务，为儿科放射学诊断提供了有效的辅助工具。

Abstract: Pediatric pneumonia remains a leading cause of morbidity and mortality in children worldwide. Timely and accurate diagnosis is critical but often challenged by limited radiological expertise and the physiological and procedural complexity of pediatric imaging. This study investigates the performance of state-of-the-art convolutional neural network (CNN) architectures ResNetRS, RegNet, and EfficientNetV2 using transfer learning for the automated classification of pediatric chest Xray images as either pneumonia or normal.A curated subset of 1,000 chest X-ray images was extracted from a publicly available dataset originally comprising 5,856 pediatric images. All images were preprocessed and labeled for binary classification. Each model was fine-tuned using pretrained ImageNet weights and evaluated based on accuracy and sensitivity. RegNet achieved the highest classification performance with an accuracy of 92.4 and a sensitivity of 90.1, followed by ResNetRS (accuracy: 91.9, sensitivity: 89.3) and EfficientNetV2 (accuracy: 88.5, sensitivity: 88.1).

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [132] [Neural Minimum Weight Perfect Matching for Quantum Error Codes](https://arxiv.org/abs/2601.00242)
*Yotam Peled,David Zenati,Eliya Nachmani*

Main category: quant-ph

TL;DR: 提出名为NMWPM的数据驱动解码器，结合图神经网络和Transformer预测动态边权重，通过代理损失函数实现端到端优化，显著降低逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 量子纠错是实现量子计算潜力的关键。传统MWPM解码器依赖固定边权重，无法充分利用局部症状特征和长程依赖关系，限制了纠错性能。

Method: 提出神经最小权重完美匹配解码器，采用混合架构：图神经网络提取局部症状特征，Transformer捕获长程全局依赖，预测MWPM解码器的动态边权重。通过设计代理损失函数解决MWPM不可微问题，实现端到端训练。

Result: 相比标准基线方法，NMWPM解码器在逻辑错误率上实现了显著降低，证明了神经网络预测能力与经典匹配算法结构结合的混合解码器优势。

Conclusion: 结合神经网络预测能力和经典匹配算法结构的混合解码器在量子纠错中表现出优越性能，为数据驱动的量子纠错解码器设计提供了新思路。

Abstract: Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [133] [Impact of Clustering on the Observability and Controllability of Complex Networks](https://arxiv.org/abs/2601.00221)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: 聚类对复杂无标度网络可观测性与可控性的影响：密集聚类网络需要更少的驱动和观测节点


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性和互联性的增加，研究复杂网络（特别是无标度网络）变得日益重要。本文旨在探索聚类如何影响复杂网络的可观测性和可控性，这对于优化社交网络、智能交通系统等应用中的网络设计具有重要意义。

Method: 首先量化网络可观测性/可控性需求，然后通过蒙特卡洛模拟和不同案例研究，展示聚类如何影响这些指标。将网络特性置于结构化系统理论框架下进行分析。

Result: 研究发现密集聚类网络需要更少的驱动和观测节点，因为聚类内部的信息传播更有效。这为资源受限设置下的传感器/执行器布置提供了实用见解。

Conclusion: 这项工作增进了对网络可观测性/可控性的理解，并提出了通过改变网络结构和聚类来改进这些特性的技术。研究结果为优化网络设计提供了理论基础，特别是在需要减少控制资源的应用中。

Abstract: The increasing complexity and interconnectedness of systems across various fields have led to a growing interest in studying complex networks, particularly Scale-Free (SF) networks, which best model real-world systems. This paper investigates the influence of clustering on the observability and controllability of complex SF networks, framing these characteristics in the context of structured systems theory. In this paper, we show that densely clustered networks require fewer driver and observer nodes due to better information propagation within clusters. This relationship is of interest for optimizing network design in applications such as social networks and intelligent transportation systems. We first quantify the network observability/controllability requirements, and then, through Monte-Carlo simulations and different case studies, we show how clustering affects these metrics. Our findings offer practical insights into reducing control and observer nodes for sensor/actuator placement, particularly in resource-constrained setups. This work contributes to the understanding of network observability/controllability and presents techniques for improving these features through alterations in network structure and clustering.

</details>


### [134] [Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective](https://arxiv.org/abs/2601.00257)
*Aly Sabri Abdalla,Vuk Marojevic*

Main category: eess.SY

TL;DR: 提出基于O-RAN的低空经济框架，通过语义感知rApp和强化学习xApp实现无人机集群实时轨迹规划


<details>
  <summary>Details</summary>
Motivation: 低空经济应用（如无人机物流、应急响应）在复杂信号受限环境中面临实时性、鲁棒性和上下文感知的编排挑战，现有AI技术缺乏针对LAE任务的专门集成

Method: 采用O-RAN架构，通过解耦的RAN架构、开放接口和RAN智能控制器（RICs）实现闭环AI优化。设计语义感知rApp作为地形解释器，为强化学习xApp提供语义指导，实现LAE集群节点的实时轨迹规划

Result: 评估了所提架构的可行性和性能，调查了可用于LAE研究的无人机测试平台能力，并提出了关键研究挑战和标准化需求

Conclusion: O-RAN启用的LAE框架能够促进闭环、AI优化的关键任务LAE操作，为低空经济应用提供了新的解决方案

Abstract: Despite the growing interest in low-altitude economy (LAE) applications, including UAV-based logistics and emergency response, fundamental challenges remain in orchestrating such missions over complex, signal-constrained environments. These include the absence of real-time, resilient, and context-aware orchestration of aerial nodes with limited integration of artificial intelligence (AI) specialized for LAE missions. This paper introduces an open radio access network (O-RAN)-enabled LAE framework that leverages seamless coordination between the disaggregated RAN architecture, open interfaces, and RAN intelligent controllers (RICs) to facilitate closed-loop, AI-optimized, and mission-critical LAE operations. We evaluate the feasibility and performance of the proposed architecture via a semantic-aware rApp that acts as a terrain interpreter, offering semantic guidance to a reinforcement learning-enabled xApp, which performs real-time trajectory planning for LAE swarm nodes. We survey the capabilities of UAV testbeds that can be leveraged for LAE research, and present critical research challenges and standardization needs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [Reinforcement learning with timed constraints for robotics motion planning](https://arxiv.org/abs/2601.00087)
*Zhaoan Wang,Junchao Li,Mahdi Mohammad,Shaoping Xiao*

Main category: cs.RO

TL;DR: 提出基于自动机的强化学习框架，在MDP和POMDP中合成满足MITL时序约束的策略，通过Timed-LDGBA转换和时间产品模型实现Q学习，在网格世界和服务机器人场景中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 动态不确定环境中的机器人系统需要满足复杂时序约束的规划器，但MITL与强化学习的结合面临随机动态和部分可观测性的挑战。

Method: 将MITL公式转换为Timed-LDGBA自动机，与底层决策过程同步构建时间产品模型，设计简单但表达性强的奖励结构，使用Q-learning学习策略。

Result: 在三个仿真研究中验证：5×5网格世界MDP、10×10网格世界POMDP和办公室服务机器人场景，框架能学习满足严格时间约束的策略，可扩展到更大状态空间，在部分可观测环境中保持有效。

Conclusion: 提出的统一框架能可靠合成满足MITL时序约束的策略，适用于时间关键和不确定环境中的机器人规划，展示了在随机动态和部分可观测性下的鲁棒性。

Abstract: Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \times 5$ grid-world formulated as an MDP, a $10 \times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [136] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: 开发主动学习框架，通过贝叶斯线性回归和自适应采样提升参数化降阶模型的稳定性和精度


<details>
  <summary>Details</summary>
Motivation: 数据驱动的降阶模型对训练数据质量敏感，需要智能选择训练参数来获得最优参数化ROM，作为数字孪生的虚拟资产基础

Method: 采用算子推断方法，建立概率化参数化算子推断框架，使用贝叶斯线性回归，利用预测不确定性设计顺序自适应采样方案

Result: 在多个非线性参数化偏微分方程系统上的数值实验表明，自适应采样策略在相同计算预算下比随机采样产生更稳定和准确的ROM

Conclusion: 提出的主动学习框架能有效提升参数化降阶模型的性能，为数字孪生应用提供更可靠的虚拟资产基础

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [137] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

TL;DR: 提出KRCD方法，用于非线性单环境观测数据中的未观测混杂因子检测，基于核回归比较，无需线性假设或多环境数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要线性假设或多个异质环境数据，限制了在非线性单环境设置中的应用。需要一种能在非线性观测数据中检测未观测混杂因子的方法。

Method: 提出核回归混杂因子检测(KRCD)，利用再生核希尔伯特空间建模复杂依赖关系。通过比较标准和高阶核回归，推导检验统计量，显著偏离零值表示存在未观测混杂。

Result: 理论证明：无限样本下回归系数相等当且仅当无未观测混杂；有限样本差异收敛于零均值高斯分布。在合成基准和Twins数据集上，KRCD优于现有基线且计算效率更高。

Conclusion: KRCD填补了非线性单环境观测数据中未观测混杂因子检测的空白，提供理论保证和实际有效性，为可靠因果推断提供了新工具。

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [138] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

TL;DR: 提出生成式条件缺失值插补网络(GCMI)，通过链式方程的多重插补框架增强鲁棒性，在MCAR和MAR机制下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 数据集中的缺失值插补在统计分析中至关重要，需要开发更有效的方法来处理缺失数据问题

Method: 1. 提出生成式条件缺失值插补网络(GCMI)的理论框架；2. 通过链式方程方法集成多重插补框架，增强模型稳定性和插补性能

Result: 在模拟实验和基准数据集上的实证评估表明，该方法在MCAR和MAR机制下优于其他主流插补技术

Conclusion: GCMI方法具有实际应用价值，可作为统计数据分析领域的先进工具

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [139] [Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures](https://arxiv.org/abs/2601.00067)
*Merritt P. R. Losert,Dario Denora,Barnaby van Straaten,Michael Chan,Stefan D. Oosterhout,Lucas Stehouwer,Giordano Scappucci,Menno Veldhorst,Justyna P. Zwolak*

Main category: cond-mat.mes-hall

TL;DR: 开发自动化协议，利用机器学习、图像处理和对象检测从电荷稳定图中提取量子点器件的电容特性，无需人工标注


<details>
  <summary>Details</summary>
Motivation: 随着量子点自旋量子比特向更大、更复杂的器件架构发展，快速、自动化的器件表征和数据分析工具变得至关重要。手动解释电荷稳定图中的特征耗时、易错且难以规模化。

Method: 集成机器学习、图像处理和对象检测技术，自动识别和跟踪电荷稳定图中的电荷跃迁线，无需人工标注。使用实验数据验证，包括应变锗单量子阱（平面）和应变锗双量子阱（双层）量子点器件。

Result: 该方法能够从大量电荷稳定图中统计估计物理相关量，如相对杠杆臂和电容耦合。特别展示了双层锗异质结构中更复杂的跃迁线（包括层间隧穿和垂直堆叠量子点的不同加载线）的自动分析能力。

Conclusion: 该协议能够快速提取量子点器件的非平凡有用信息，为大规模量子点器件表征提供了自动化解决方案。

Abstract: As quantum dot (QD)-based spin qubits advance toward larger, more complex device architectures, rapid, automated device characterization and data analysis tools become critical. The orientation and spacing of transition lines in a charge stability diagram (CSD) contain a fingerprint of a QD device's capacitive environment, making these measurements useful tools for device characterization. However, manually interpreting these features is time-consuming, error-prone, and impractical at scale. Here, we present an automated protocol for extracting underlying capacitive properties from CSDs. Our method integrates machine learning, image processing, and object detection to identify and track charge transitions across large datasets without manual labeling. We demonstrate this method using experimentally measured data from a strained-germanium single-quantum-well (planar) and a strained-germanium double-quantum-well (bilayer) QD device. Unlike for planar QD devices, CSDs in bilayer germanium heterostructure exhibit a larger set of transitions, including interlayer tunneling and distinct loading lines for the vertically stacked QDs, making them a powerful testbed for automation methods. By analyzing the properties of many CSDs, we can statistically estimate physically relevant quantities, like relative lever arms and capacitive couplings. Thus, our protocol enables rapid extraction of useful, nontrivial information about QD devices.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [140] [Interpretable Machine Learning for Quantum-Informed Property Predictions in Artificial Sensing Materials](https://arxiv.org/abs/2601.00503)
*Li Chen,Leonardo Medrano Sandonas,Shirong Huang,Alexander Croy,Gianaurelio Cuniberti*

Main category: physics.chem-ph

TL;DR: MORE-ML是一个结合量子力学性质数据和机器学习方法的计算框架，用于预测体味挥发组传感相关性质，为人工传感材料设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 数字传感面临挑战，需要开发可持续方法将定制电子鼻扩展到复杂的体味挥发组分析。现有方法在分子水平计算与实际应用之间存在差距。

Method: 开发MORE-ML框架，整合电子鼻分子构建块的量子力学性质数据与机器学习方法。扩展MORE-Q数据集为MORE-QX，采样更大的构象空间，计算电子结合特征。使用构建块的电子描述符作为树基机器学习模型的输入来预测结合特征。

Result: 分析显示构建块的量子力学性质与结合特征之间相关性弱。CatBoost模型表现最佳，尤其在未见化合物的可转移性方面。可解释AI方法识别出影响结合特征预测的关键量子力学性质。

Conclusion: MORE-ML结合量子力学洞察与机器学习，为体味挥发组传感中的分子受体提供机理理解和理性设计原则，为分析复杂气味混合物的人工传感材料奠定基础。

Abstract: Digital sensing faces challenges in developing sustainable methods to extend the applicability of customized e-noses to complex body odor volatilome (BOV). To address this challenge, we developed MORE-ML, a computational framework that integrates quantum-mechanical (QM) property data of e-nose molecular building blocks with machine learning (ML) methods to predict sensing-relevant properties. Within this framework, we expanded our previous dataset, MORE-Q, to MORE-QX by sampling a larger conformational space of interactions between BOV molecules and mucin-derived receptors. This dataset provides extensive electronic binding features (BFs) computed upon BOV adsorption. Analysis of MORE-QX property space revealed weak correlations between QM properties of building blocks and resulting BFs. Leveraging this observation, we defined electronic descriptors of building blocks as inputs for tree-based ML models to predict BFs. Benchmarking showed CatBoost models outperform alternatives, especially in transferability to unseen compounds. Explainable AI methods further highlighted which QM properties most influence BF predictions. Collectively, MORE-ML combines QM insights with ML to provide mechanistic understanding and rational design principles for molecular receptors in BOV sensing. This approach establishes a foundation for advancing artificial sensing materials capable of analyzing complex odor mixtures, bridging the gap between molecular-level computations and practical e-nose applications.

</details>


### [141] [AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules](https://arxiv.org/abs/2601.00581)
*Stephen E. Farr,Stefan Doerr,Antonio Mirarchi,Francesc Sabanes Zariquiey,Gianni De Fabritiis*

Main category: physics.chem-ph

TL;DR: AceFF是一种针对小分子药物发现优化的预训练机器学习原子间势能模型，通过改进的TensorNet2架构和全面的药物类化合物数据集训练，在保持DFT级别精度的同时实现高通量推理速度。


<details>
  <summary>Details</summary>
Motivation: 机器学习原子间势能模型（MLIPs）虽然作为密度泛函理论（DFT）的高效替代方案出现，但在不同化学空间中的泛化能力仍然有限。特别是在药物发现领域，需要能够处理药物类化合物并支持关键药用化学元素的通用力场。

Method: 采用改进的TensorNet2架构，在全面的药物类化合物数据集上进行训练。该模型支持所有关键药用化学元素（H、B、C、N、O、F、Si、P、S、Cl、Br、I），并专门训练以处理带电状态。

Result: AceFF在复杂扭转能量扫描、分子动力学轨迹、批量最小化以及力和能量精度等严格基准测试中验证，为有机分子建立了新的最先进水平。模型在保持DFT级别精度的同时实现了高通量推理速度。

Conclusion: AceFF为小分子药物发现提供了一个平衡精度与速度的机器学习原子间势能模型，通过全面的基准验证证明了其在有机分子领域的优越性能，模型权重和推理代码已公开可用。

Abstract: We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.

</details>
