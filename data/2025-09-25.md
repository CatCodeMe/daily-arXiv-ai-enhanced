<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.NI](#cs.NI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 97]
- [math.OC](#math.OC) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [eess.IV](#eess.IV) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 6]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.SD](#cs.SD) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 9]
- [eess.SP](#eess.SP) [Total: 22]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [About the Multi-Head Linear Restricted Chase Termination](https://arxiv.org/abs/2509.19400)
*Lukas Gerlach,Lucas Larroque,Jerzy Marcinkowski,Piotr Ostropolski-Nalewaja*

Main category: cs.DB

TL;DR: 该论文证明了对于线性多头规则，所有实例的受限追逐终止问题是可判定的。


<details>
  <summary>Details</summary>
Motivation: 在数据库理论中，追逐算法是普遍存在的，但对于存在规则（即元组生成依赖），其终止性无法保证，甚至在一般情况下是不可判定的。受限追逐的终止问题尤其困难，因为规则应用的顺序会影响结果。对于许多表现良好的规则类（如线性或保护多头规则），受限追逐终止的可判定性仍然是一个开放问题。

Method: 作者通过理论分析和技术证明，展示了在多头线性规则的情况下，所有实例的受限追逐终止问题是可判定的。

Result: 研究结果表明，对于线性多头规则，可以确定受限追逐是否会在所有实例上终止。

Conclusion: 这项工作在解决受限追逐终止问题的可判定性方面迈出了重要一步，特别是对于线性多头规则类，填补了该领域的一个空白。

Abstract: The chase is a ubiquitous algorithm in database theory. However, for
existential rules (aka tuple-generating dependencies), its termination is not
guaranteed, and even undecidable in general. The problem of termination becomes
particularly difficult for the restricted (or standard) chase, for which the
order of rule application matters. Thus, decidability of restricted chase
termination is still open for many well-behaved classes such as linear or
guarded multi-headed rules. We make a step forward by showing that
all-instances restricted chase termination is decidable in the linear
multi-headed case.

</details>


### [2] [STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases](https://arxiv.org/abs/2509.19508)
*Mounica Maddela,Lingjue Xie,Daniel Preotiuc-Pietro,Mausam*

Main category: cs.DB

TL;DR: STARQA是第一个针对专业领域数据库的复杂分析推理问答数据集，提出Text2SQLCode方法将SQL和Python结合使用，相比纯SQL方法表现更好，但对现有LLM仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有文本转SQL基准的问题复杂度受限于查询语言表达能力，缺乏专门针对需要聚合分析、时间序列分析等复杂分析推理问题的数据集。

Method: 提出Text2SQLCode方法，将任务分解为SQL负责数据获取，Python负责推理计算，结合两种语言的优势。

Result: SQL和Python结合的方法比单独使用SQL表现更好，但STARQA数据集对现有最先进的LLM仍然相当具有挑战性。

Conclusion: 识别并结合SQL和Python的能力是有益的，但复杂分析推理任务仍然是现有技术的挑战。

Abstract: Semantic parsing methods for converting text to SQL queries enable question
answering over structured data and can greatly benefit analysts who routinely
perform complex analytics on vast data stored in specialized relational
databases. Although several benchmarks measure the abilities of text to SQL,
the complexity of their questions is inherently limited by the level of
expressiveness in query languages and none focus explicitly on questions
involving complex analytical reasoning which require operations such as
calculations over aggregate analytics, time series analysis or scenario
understanding. In this paper, we introduce STARQA, the first public
human-created dataset of complex analytical reasoning questions and answers on
three specialized-domain databases. In addition to generating SQL directly
using LLMs, we evaluate a novel approach (Text2SQLCode) that decomposes the
task into a combination of SQL and Python: SQL is responsible for data
fetching, and Python more naturally performs reasoning. Our results demonstrate
that identifying and combining the abilities of SQL and Python is beneficial
compared to using SQL alone, yet the dataset still remains quite challenging
for the existing state-of-the-art LLMs.

</details>


### [3] [Gamma Acyclicity, Annotated Relations, and Consistency Witness Functions](https://arxiv.org/abs/2509.19621)
*Albert Atserias,Phokion G. Kolaitis*

Main category: cs.DB

TL;DR: 本文研究了gamma-无环数据库模式与带注释关系之间的语义性质，发现在具有传输性质的正交换幺半群注释下，gamma-无环模式的优良性质可以扩展到带注释关系。


<details>
  <summary>Details</summary>
Motivation: 在关系数据库理论早期发现"无环"数据库模式具有良好语义性质，已有三种无环性概念被研究。最近关于alpha-无环模式与带注释关系的研究成果促使研究beta-无环和gamma-无环模式在带注释关系中的扩展。

Method: 通过分析gamma-无环模式的定义特性，结合带注释关系的数学框架，研究在正交换幺半群注释下的一致性见证函数概念，探讨标准连接操作的本质性质。

Result: 发现gamma-无环模式的语义性质可以扩展到带注释关系，前提是注释来自具有传输性质的正交换幺半群。揭示了标准连接操作的核心性质是作为两个一致关系的一致性见证。

Conclusion: gamma-无环数据库模式在带注释关系设置下仍保持其优良语义特性，通过一致性见证函数的概念可以更好地理解连接操作的本质作用，为数据库理论提供了新的理论框架。

Abstract: During the early days of relational database theory it was realized that
"acyclic" database schemas possess a number of desirable semantic properties.
In fact, three different notions of "acyclicity" were identified and
extensively investigated during the 1980s, namely, alpha-acyclicity,
beta-acyclicity, and gamma-acyclicity. Much more recently, the study of
alpha-acyclicity was extended to annotated relations, where the annotations are
values from some positive commutative monoid. The recent results about
alpha-acyclic schemas and annotated relations give rise to results about
beta-acyclic schemas and annotated relations, since a schema is beta-acyclic if
and only if every sub-schema of it is alpha-acyclic. Here, we study
gamma-acyclic schemas and annotated relations. Our main finding is that the
desirable semantic properties of gamma-acyclic schemas extend to annotated
relations, provided the annotations come from a positive commutative monoid
that has the transportation property. Furthermore, the results reported here
shed light on the role of the join of two standard relations, Specifically, our
results reveal that the only relevant property of the join of two standard
relations is that it is a witness to the consistency of the two relations,
provided that these two relations are consistent. For the more abstract setting
of annotated relations, this property of the standard join is captured by the
notion of a consistency witness function, a notion which we systematically
investigate in this work.

</details>


### [4] [ARCADE: A Real-Time Data System for Hybrid and Continuous Query Processing across Diverse Data Modalities](https://arxiv.org/abs/2509.19757)
*Jingyi Yang,Songsong Mo,Jiachen Shi,Zihao Yu,Kunhao Shi,Xuchen Ding,Gao Cong*

Main category: cs.DB

TL;DR: ARCADE是一个实时数据系统，支持多模态数据的高吞吐量摄入和混合连续查询处理，性能优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 多模态数据的爆炸式增长超出了现有数据库系统的能力，现有系统缺乏高效的摄入和连续查询能力，或在支持混合分析方面不足。

Method: ARCADE引入了基于LSM存储的统一磁盘二级索引（支持向量、空间和文本数据）、基于成本的混合查询优化器，以及用于高效连续查询的增量物化视图框架。

Result: 基于开源RocksDB存储和MySQL查询引擎构建的ARCADE，在读取密集型工作负载上性能提升达7.4倍，写入密集型工作负载上提升1.4倍。

Conclusion: ARCADE系统有效解决了多模态数据实时处理的挑战，为高吞吐量摄入和混合连续查询提供了高效解决方案。

Abstract: The explosive growth of multimodal data - spanning text, image, video,
spatial, and relational modalities, coupled with the need for real-time
semantic search and retrieval over these data - has outpaced the capabilities
of existing multimodal and real-time database systems, which either lack
efficient ingestion and continuous query capability, or fall short in
supporting expressive hybrid analytics. We introduce ARCADE, a real-time data
system that efficiently supports high-throughput ingestion and expressive
hybrid and continuous query processing across diverse data types. ARCADE
introduces unified disk-based secondary index on LSM-based storage for vector,
spatial, and text data modalities, a comprehensive cost-based query optimizer
for hybrid queries, and an incremental materialized view framework for
efficient continuous queries. Built on open-source RocksDB storage and MySQL
query engine, ARCADE outperforms leading multimodal data systems by up to 7.4x
on read-heavy and 1.4x on write-heavy workloads.

</details>


### [5] [Output-Sensitive Evaluation of Acyclic Conjunctive Regular Path Queries](https://arxiv.org/abs/2509.20204)
*Mahmoud Abo Khamis,Alexandru-Mihai Hurjui,Ahmet Kara,Dan Olteanu,Dan Suciu,Zilu Tian*

Main category: cs.DB

TL;DR: 提出首个针对无环CRPQ的输出敏感算法，复杂度取决于输入图大小和查询输出大小，不依赖于查询中正则表达式的输出大小。


<details>
  <summary>Details</summary>
Motivation: 现有CRPQ评估算法复杂度依赖于正则表达式的输出大小，而实际查询输出可能远小于此，需要更高效的输出敏感算法。

Method: 采用两阶段方法：第一阶段将查询收缩为自由连接无环CRPQ，通过移除绑定变量（正则表达式组合或提升为自由变量）；第二阶段评估收缩后的查询并投影掉提升的绑定变量列。

Result: 算法在查询输出小于最坏情况输出大小或正则表达式输出较大时，复杂度低于现有最优方法。

Conclusion: 提出的输出敏感算法通过查询收缩和校准输出计算，显著提高了无环CRPQ的评估效率。

Abstract: Conjunctive Regular Path Queries, or CRPQs for short, are an essential
construct in graph query languages. In this paper, we propose the first
output-sensitive algorithm for evaluating acyclic CRPQs. It is output-sensitive
in the sense that its complexity is a function of the sizes of the input graph
and of the query output. In particular, it does not depend on the output sizes
of the regular expressions that appear in the query, as these sizes can be much
larger than the query output size.
  Our algorithm proceeds in two stages. In the first stage, it contracts the
given query into a free-connex acyclic one such that the output of the original
query can be obtained from the output of the contracted one. This contraction
removes bound variables by composing regular expressions or by promoting bound
variables to free ones. The minimum necessary number of promoted bound
variables gives the contraction width, which is a novel parameter specific to
CRPQs. In the second stage, our algorithm evaluates the free-connex acyclic
CRPQ and projects away the columns of the promoted bound variables. It ensures
output-sensitivity by computing the calibrated outputs of the regular
expressions appearing in the free-connex acyclic CRPQ in time proportional to
their sizes.
  Our algorithm has lower complexity than the state-of-the-art approaches for
problem instances where (i) the query output is asymptotically smaller than the
worst-case output size or (ii) the largest output size of any of the regular
expression in the query.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera](https://arxiv.org/abs/2509.19478)
*Ziwei Wang,Cong Wu,Paolo Tasca*

Main category: cs.DC

TL;DR: 本文提出了一种用于Hedera区块链的混合分片解决方案，通过将网络划分为本地和全局委员会来实现高效跨分片交易，显著降低了存储和通信开销，提高了可扩展性和容错能力。


<details>
  <summary>Details</summary>
Motivation: 解决区块链网络面临的可扩展性挑战，使Hedera能够实现更高的交易吞吐量、降低延迟并优化资源使用。

Method: 探索各种学术和工业分片技术，基于这些见解提出混合分片方案：将网络划分为本地和全局委员会，支持高效跨分片交易，并通过动态重新配置确保安全性。

Result: 分析显示存储和通信开销显著降低，可扩展性得到改善，容错能力增强，证明了将分片集成到Hedera架构中的可行性和优势。

Conclusion: 混合分片解决方案为Hedera区块链提供了有效的可扩展性提升途径，通过创新的网络分区和动态配置机制实现了性能和安全性的平衡。

Abstract: Sharding has emerged as a critical solution to address the scalability
challenges faced by blockchain networks, enabling them to achieve higher
transaction throughput, reduced latency, and optimized resource usage. This
paper investigates the advancements, methodologies, and adoption potential of
sharding in the context of Hedera, a distributed ledger technology known for
its unique Gossip about Gossip protocol and asynchronous Byzantine Fault
Tolerance (ABFT). We explore various academic and industrial sharding
techniques, emphasizing their benefits and trade-offs. Building on these
insights, we propose a hybrid sharding solution for Hedera that partitions the
network into local and global committees, facilitating efficient cross-shard
transactions and ensuring robust security through dynamic reconfiguration. Our
analysis highlights significant reductions in storage and communication
overhead, improved scalability, and enhanced fault tolerance, demonstrating the
feasibility and advantages of integrating sharding into Hedera's architecture.

</details>


### [7] [To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions](https://arxiv.org/abs/2509.19532)
*Flavio Castro,Weijian Zheng,Joaquin Chung,Ian Foster,Rajkumar Kettimuthu*

Main category: cs.DC

TL;DR: 本文提出了一个定量框架和流式速度评分来评估远程高性能计算资源是否比本地替代方案能提供更及时的数据处理，通过考虑数据生成率、传输效率、远程处理能力和文件I/O开销等关键参数来计算总处理完成时间。


<details>
  <summary>Details</summary>
Motivation: 现代科学仪器生成数据的速度越来越超过本地计算能力，而基于文件的传输方式在时间敏感的分析和实验引导中变得不切实际。实时流式框架承诺降低延迟并提高系统效率，但缺乏评估其可行性的原则性方法。

Method: 引入一个定量框架和流式速度评分模型，该模型结合数据生成率、传输效率、远程处理能力和文件I/O开销等关键参数，计算总处理完成时间，并识别流式处理有益的操作区间。

Result: 测量结果显示，在高数据速率下，流式处理可以实现比基于文件的方法低97%的端到端完成时间，但最坏情况下的拥塞可能使传输时间增加一个数量级以上。

Conclusion: 流式处理在高数据速率场景下具有显著优势，但尾部延迟在流式可行性决策中至关重要，需要综合考虑网络拥塞等因素。

Abstract: Modern scientific instruments generate data at rates that increasingly exceed
local compute capabilities and, when paired with the staging and I/O overheads
of file-based transfers, also render file-based use of remote HPC resources
impractical for time-sensitive analysis and experimental steering. Real-time
streaming frameworks promise to reduce latency and improve system efficiency,
but lack a principled way to assess their feasibility. In this work, we
introduce a quantitative framework and an accompanying Streaming Speed Score to
evaluate whether remote high-performance computing (HPC) resources can provide
timely data processing compared to local alternatives. Our model incorporates
key parameters including data generation rate, transfer efficiency, remote
processing power, and file input/output overhead to compute total processing
completion time and identify operational regimes where streaming is beneficial.
We motivate our methodology with use cases from facilities such as APS, FRIB,
LCLS-II, and the LHC, and validate our approach through an illustrative case
study based on LCLS-II data. Our measurements show that streaming can achieve
up to 97% lower end-to-end completion time than file-based methods under high
data rates, while worst-case congestion can increase transfer times by over an
order of magnitude, underscoring the importance of tail latency in streaming
feasibility decisions.

</details>


### [8] [A Survey of Recent Advancements in Secure Peer-to-Peer Networks](https://arxiv.org/abs/2509.19539)
*Raj Patel,Umesh Biswas,Surya Kodipaka,Will Carroll,Preston Peranich,Maxwell Young*

Main category: cs.DC

TL;DR: 本文对P2P网络安全的最新理论进展进行了更新综述，重点关注经典威胁（如Sybil攻击和路由攻击）的解决方案，并探讨了机器学习、社交网络和动态系统等新兴趋势带来的新挑战和新方法。


<details>
  <summary>Details</summary>
Motivation: P2P网络是现代计算的基石，其安全研究是一个活跃领域。虽然已有许多具有强安全保证的防御方案被提出，但最近的综述已有十多年历史，需要更新以反映最新进展。

Method: 通过文献综述方法，系统回顾和分析近年来P2P网络安全的理论进展，特别关注经典威胁的解决方案和新兴趋势带来的挑战。

Result: 评估了各种解决方案的优缺点，识别了机器学习、社交网络和动态系统等新兴趋势对P2P网络安全的影响，并提出了相应的新解决方案。

Conclusion: 论文总结了当前P2P网络安全研究的现状，指出了现有解决方案的局限性，并为未来研究提出了方向性建议。

Abstract: Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their
security is an active area of research. Many defenses with strong security
guarantees have been proposed; however, the most-recent survey is over a decade
old. This paper delivers an updated review of recent theoretical advances that
address classic threats, such as the Sybil and routing attacks, while
highlighting how emerging trends -- such as machine learning, social networks,
and dynamic systems -- pose new challenges and drive novel solutions. We
evaluate the strengths and weaknesses of these solutions and suggest directions
for future research.

</details>


### [9] [Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE](https://arxiv.org/abs/2509.19701)
*Akash Poptani,Alireza Khadem,Scott Mahlke,Jonah Miller,Joshua Dolence,Reetuparna Das*

Main category: cs.DC

TL;DR: 本文分析了Parthenon（一种块结构自适应网格细化基准测试）在CPU-GPU系统上的性能表现，发现较小的网格块和较深的AMR级别会降低GPU性能，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算模拟对自适应网格细化（AMR）的依赖增加，需要了解其在CPU-GPU异构系统上的性能特征，特别是针对美国能源部即将推出的异构超级计算机。

Method: 通过详细的性能分析，识别了GPU性能瓶颈，包括通信开销、串行开销、GPU利用率低下、低占用率和内存访问瓶颈，并分析了等级可扩展性和内存约束。

Result: 研究发现较小的网格块和较深的AMR级别会显著降低GPU性能，主要原因是通信增加、串行开销和GPU利用效率低下。

Conclusion: 提出了优化建议以提高GPU吞吐量和减少内存占用，这些见解可为未来在异构超级计算机上的AMR部署提供指导。

Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce
compute and memory demands while maintaining accuracy. This work analyzes the
performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.
We show that smaller mesh blocks and deeper AMR levels degrade GPU performance
due to increased communication, serial overheads, and inefficient GPU
utilization. Through detailed profiling, we identify inefficiencies, low
occupancy, and memory access bottlenecks. We further analyze rank scalability
and memory constraints, and propose optimizations to improve GPU throughput and
reduce memory footprint. Our insights can inform future AMR deployments on
Department of Energy's upcoming heterogeneous supercomputers.

</details>


### [10] [Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference](https://arxiv.org/abs/2509.19729)
*Haoyu Chen,Xue Li,Kun Qian,Yu Guan,Jin Zhao,Xin Wang*

Main category: cs.DC

TL;DR: Gyges通过自适应调整并行策略来优化LLM服务中的上下文长度动态变化处理，提升吞吐量1.75x-6.57x


<details>
  <summary>Details</summary>
Motivation: 解决LLM服务中上下文长度动态变化处理的效率问题，平衡并行策略带来的吞吐量损失

Method: 提出跨实例并行转换(Gyges)，包括页面友好的KV缓存布局、专用权重填充和转换感知调度器

Result: 在真实世界trace评估中，相比现有最优方案，吞吐量提升1.75倍到6.57倍

Conclusion: Gyges通过自适应并行策略转换有效解决了LLM服务中的动态请求处理问题

Abstract: Efficiently processing the dynamics of requests, especially the context
length variance, is important in Large Language Model (LLM) serving scenarios.
However, there is an intrinsic trade-off: while leveraging parallelism
strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to
accommodate larger context lengths, it inevitably results in degraded overall
throughput. In this paper, we propose Cross-Instance Parallelism Transformation
(Gyges), which adaptively adjusts the parallelism strategies of running
instances to align with the dynamics of incoming requests. We design (1) a
page-friendly, header-centric layout to accelerate KV cache transformations;
(2) dedicated weight padding to accelerate model weight transformations; and
(3) a transformation-aware scheduler to cooperatively schedule requests and
parallelism transformations, optimizing the overall performance. Evaluations
using real-world traces show that Gyges improves throughput by 1.75x-6.57x
compared to state-of-the-art solutions.

</details>


### [11] [BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens](https://arxiv.org/abs/2509.19836)
*Ao Sun,Weilin Zhao,Xu Han,Cheng Yang,Zhiyuan Liu,Chuan Shi,Maosong sun*

Main category: cs.DC

TL;DR: BurstEngine是一个高效训练LLM长序列数据的框架，通过BurstAttention降低通信成本，结合序列级选择性检查点和负载平衡优化，在超过100万token的极长序列训练中实现1.2倍加速和更低内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Tensor Parallelism和Context Parallelism在处理长序列数据时模型FLOPs利用率低，特别是在序列长度超过100万token且GPU数量增加时表现不佳。

Method: 提出BurstAttention优化分布式注意力机制，利用拓扑感知环形通信和细粒度通信计算重叠；引入序列级选择性检查点；融合语言建模头与损失函数；实现各种注意力掩码的负载平衡优化。

Result: BurstEngine在训练超过100万token的极长序列时，相比最先进的基线方法实现了1.2倍的加速，内存开销显著降低。

Conclusion: BurstEngine通过综合优化有效解决了长序列训练中的效率和内存问题，为大规模语言模型的长序列训练提供了高效解决方案。

Abstract: Existing methods for training LLMs on long-sequence data, such as Tensor
Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as
sequence lengths and number of GPUs increase, especially when sequence lengths
exceed 1M tokens. To address these challenges, we propose BurstEngine, an
efficient framework designed to train LLMs on long-sequence data. BurstEngine
introduces BurstAttention, an optimized distributed attention with lower
communication cost than RingAttention. BurstAttention leverages topology-aware
ring communication to fully utilize network bandwidth and incorporates
fine-grained communication-computation overlap. Furthermore, BurstEngine
introduces sequence-level selective checkpointing and fuses the language
modeling head with the loss function to reduce memory cost. Additionally,
BurstEngine introduces workload balance optimization for various types of
attention masking. By integrating these optimizations, BurstEngine achieves a
$1.2\times$ speedup with much lower memory overhead than the state-of-the-art
baselines when training LLMs on extremely long sequences of over 1M tokens. We
have made our code publicly available on GitHub:
https://github.com/thunlp/BurstEngine.

</details>


### [12] [Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models](https://arxiv.org/abs/2509.20160)
*Prashanthi S. K.,Sai Anuroop Kesanapalli,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文对边缘设备上的DNN训练进行了系统性研究，分析了不同Jetson设备在训练参数变化下的性能表现，并建立了预测模型来优化训练时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管DNN在边缘推理方面应用广泛，但边缘设备上的DNN训练研究较少。随着联邦学习和GPU加速边缘设备的发展，需要对边缘训练进行全面表征。

Method: 在三种Jetson设备上对三种DNN模型-数据集组合进行训练实验，改变I/O流水线、存储介质、批次大小和功率模式等参数，分析其对CPU/GPU利用率、训练时间和能耗的影响。

Result: 研究揭示了资源间的相互依赖关系和反直觉的发现，帮助量化已知经验。建立了简单模型来预测不同功率模式下的训练时间和能耗。

Conclusion: 该研究有助于优化边缘训练性能，在受限设备上权衡时间和能耗，并为未来联邦学习研究奠定基础。

Abstract: Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.

</details>


### [13] [Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators](https://arxiv.org/abs/2509.20189)
*Prashanthi S. K.,Kunal Kumar Sahoo,Amartya Ranjan Saikia,Pranav Gupta,Atharva Vinay Joshi,Priyanshu Pansari,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文开发了Jetson Orin AGX的时间屋顶线和能量屋顶线模型，结合DNN推理的分析模型，揭示了边缘加速器功率模式的独特见解，并应用这些方法优化DNN推理的延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 边缘加速器如Nvidia Jetson在计算连续体中变得越来越重要，但缺乏对其功率模式性能行为的原理性研究。现有方法主要预测功率和延迟，但未深入理解其工作原理。

Method: 开发了Jetson Orin AGX的时间屋顶线和新型能量屋顶线模型，结合DNN推理的计算（FLOP）和内存访问（字节）分析模型，从第一原理分析不同功率模式。

Result: 揭示了独特的见解：默认MAXN功率模式并非最节能；时间效率在所有功率模式下都意味着能量效率。通过功率模式调优，实现了高达15%的能耗降低，推理时间仅有最小退化。

Conclusion: 屋顶线模型为理解边缘加速器性能提供了原理性框架，功率模式调优可显著优化DNN推理的能耗和延迟性能，该方法也可扩展到DNN训练场景。

Abstract: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the
computing continuum, and are often used for DNN inferencing and training.
Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power
envelope and offer $1000$s of power modes to customize CPU, GPU and memory
frequencies. Their widely varying power--performance trade-offs can be
exploited for energy and power-constrained deployments. While data-driven
methods to predict the power and latency of DNN workloads for edge devices
exist, there is a lack of principled study to understand why edge accelerators
and their power modes perform the way they do. We develop a time roofline and a
novel energy roofline model for the Jetson Orin AGX for diverse power modes,
and couple it with an analytical model of the compute (FLOP) and memory access
(bytes) for DNN inference workloads to analyze them from first principles.
These reveal unique, sometimes counter-intuitive, insights into the power and
performance behavior of DNN workloads on edge accelerators, e.g., the default
power mode MAXN is not the most energy efficient and time efficiency implies
energy efficiency for all power modes. We also extend our analytical roofline
models to DNN training. Finally, we apply these methods to tune the power mode
(and hence the roofline) of the edge device to optimize the latency and energy
for DNN inference, with up to $15\%$ lower energy and minimal degradation in
inference time.

</details>


### [14] [Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators](https://arxiv.org/abs/2509.20205)
*Prashanthi S. K.,Saisamarth Taluri,Pranav Gupta,Amartya Ranjan Saikia,Kunal Kumar Sahoo,Atharva Vinay Joshi,Lakshya Karwa,Kedar Dhule,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出了一种智能时间切片方法GMD和ALS，用于在Jetson等边缘设备上优化并发DNN训练和推理的性能，在有限的分析成本下实现功率-性能目标。


<details>
  <summary>Details</summary>
Motivation: GPU加速边缘设备的普及和隐私关注度的提升，使得在边缘设备上进行并发DNN训练和推理变得重要。但边缘加速器如Jetson不支持原生GPU共享，且提供数千种功率模式，需要仔细的时间共享来满足功率-性能目标。

Method: 设计了Fulcrum调度器，包含GMD（高效多维梯度下降搜索，仅分析15种功率模式）和ALS（主动学习技术，识别可重用的帕累托最优功率模式，分析50-150种功率模式），通过优化问题来交错训练和推理小批量，决定设备功率模式和推理小批量大小。

Result: 在15个DNN工作负载的273,000+配置中评估，ALS和GMD优于简单和复杂基线方法。解决方案在97%以上的运行中满足延迟和功率预算，平均吞吐量接近最优值的7%以内。

Conclusion: 提出的智能时间切片方法能够有效管理边缘设备上的并发DNN工作负载，在有限的分析成本下实现接近最优的性能。

Abstract: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the
rise in privacy concerns are placing an emphasis on concurrent DNN training and
inferencing on edge devices. Inference and training have different computing
and QoS goals. But edge accelerators like Jetson do not support native GPU
sharing and expose 1000s of power modes. This requires careful time-sharing of
concurrent workloads to meet power--performance goals, while limiting costly
profiling. In this paper, we design an intelligent time-slicing approach for
concurrent DNN training and inferencing on Jetsons. We formulate an
optimization problem to interleave training and inferencing minibatches, and
decide the device power mode and inference minibatch size, while maximizing the
training throughput and staying within latency and power budgets, with modest
profiling costs. We propose GMD, an efficient multi-dimensional gradient
descent search which profiles just $15$ power modes; and ALS, an Active
Learning technique which identifies reusable Pareto-optimal power modes, but
profiles $50$--$150$ power modes. We evaluate these within our Fulcrum
scheduler for $273,000+$ configurations across $15$ DNN workloads. We also
evaluate our strategies on dynamic arrival inference and concurrent inferences.
ALS and GMD outperform simpler and more complex baselines with larger-scale
profiling. Their solutions satisfy the latency and power budget for $>97\%$ of
our runs, and on average are within $7\%$ of the optimal throughput.

</details>


### [15] [An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications](https://arxiv.org/abs/2509.20223)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.DC

TL;DR: 本文对联邦学习在自动驾驶车辆应用中的安全性进行实证分析，特别关注在存在网络攻击的情况下，使用安全聚合技术和多方计算来保护交通图像数据集（如LISA交通灯数据集）的模型训练。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然为自动驾驶车辆提供了分布式学习和数据隐私保护的优势，但仍容易受到各种网络攻击，特别是投毒攻击和推理攻击。需要更强大的安全措施来有效缓解潜在威胁。

Method: 使用各种安全聚合技术和多方计算对交通图像数据集进行实证分析，评估这些技术在存在不同类型网络攻击时的防护效果。多方计算作为最先进的安全机制，通过多种安全协议为边缘自动驾驶车辆的本地模型更新提供标准隐私保护。

Result: 研究表明，攻击者的存在可能误导自动驾驶车辆学习模型，导致交通灯错误分类，产生有害影响。需要验证各种安全联邦学习聚合技术和多方计算在面对网络威胁时的弹性表现。

Conclusion: 这项实证研究探索了各种安全联邦学习聚合技术和多方计算在训练和推理阶段保护自动驾驶车辆应用免受各种网络威胁的弹性能力，为提升联邦学习在自动驾驶领域的安全性提供了重要参考。

Abstract: Federated Learning lends itself as a promising paradigm in enabling
distributed learning for autonomous vehicles applications and ensuring data
privacy while enhancing and refining predictive model performance through
collaborative training on edge client vehicles. However, it remains vulnerable
to various categories of cyber-attacks, necessitating more robust security
measures to effectively mitigate potential threats. Poisoning attacks and
inference attacks are commonly initiated within the federated learning
environment to compromise secure system performance. Secure aggregation can
limit the disclosure of sensitive information from outsider and insider
attackers of the federated learning environment. In this study, our aim is to
conduct an empirical analysis on the transportation image dataset (e.g., LISA
traffic light) using various secure aggregation techniques and multiparty
computation in the presence of diverse categories of cyber-attacks. Multiparty
computation serves as a state-of-the-art security mechanism, offering standard
privacy for secure aggregation of edge autonomous vehicles local model updates
through various security protocols. The presence of adversaries can mislead the
autonomous vehicle learning model, leading to the misclassification of traffic
lights, and resulting in detrimental impacts. This empirical study explores the
resilience of various secure federated learning aggregation techniques and
multiparty computation in safeguarding autonomous vehicle applications against
various cyber threats during both training and inference times.

</details>


### [16] [xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture](https://arxiv.org/abs/2509.20340)
*Liubov Kurafeeva,Alan Subedi,Ryan Hartung,Michael Fay,Avhishek Biswas,Shantenu Jha,Ozgur O. Kilic,Chandra Krintz,Andre Merzky,Douglas Thain,Mehmet C. Vuran,Rich Wolski*

Main category: cs.DC

TL;DR: xGFabric是一个端到端系统，通过私有5G网络将传感器网络与高性能计算设施耦合，实现实时数字农业模拟


<details>
  <summary>Details</summary>
Motivation: 数字农业中的柑橘保护屏研究需要将分布式传感器网络与集中式高性能计算设施耦合，以进行环境条件建模和实时干预指导

Method: 开发xGFabric原型系统，通过5G网络切片将远程传感器连接到HPC系统，利用私有5G网络的低延迟、高吞吐量和可靠性特性

Result: 成功实现了传感器网络与HPC设施的实时耦合，支持柑橘保护屏研究的数字农业模拟

Conclusion: 私有5G网络为解决边缘传感器网络与HPC模拟的近实时耦合挑战提供了新的能力，xGFabric系统验证了这一方法的可行性

Abstract: Advanced scientific applications require coupling distributed sensor networks
with centralized high-performance computing facilities. Citrus Under Protective
Screening (CUPS) exemplifies this need in digital agriculture, where citrus
research facilities are instrumented with numerous sensors monitoring
environmental conditions and detecting protective screening damage. CUPS
demands access to computational fluid dynamics codes for modeling environmental
conditions and guiding real-time interventions like water application or
robotic repairs. These computing domains have contrasting properties: sensor
networks provide low-performance, limited-capacity, unreliable data access,
while high-performance facilities offer enormous computing power through
high-latency batch processing. Private 5G networks present novel capabilities
addressing this challenge by providing low latency, high throughput, and
reliability necessary for near-real-time coupling of edge sensor networks with
HPC simulations. This work presents xGFabric, an end-to-end system coupling
sensor networks with HPC facilities through Private 5G networks. The prototype
connects remote sensors via 5G network slicing to HPC systems, enabling
real-time digital agriculture simulation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [17] [A Note on Fine-Grained Quantum Reductions for Linear Algebraic Problems](https://arxiv.org/abs/2509.19528)
*Kyle Doney,Cameron Musco*

Main category: cs.DS

TL;DR: 本文证明在量子计算中，矩阵乘法与多个线性代数问题（如行列式、矩阵迹等）的计算复杂度本质上是等价的。任何对这些问题的改进都会带来更快的量子矩阵乘法算法。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算中线性代数问题的复杂度关系，探索矩阵乘法与其他核心线性代数操作之间的等价性，为量子算法设计提供理论指导。

Method: 首先利用Bernstein-Vazirani算法将矩阵乘法量子归约到计算tr(ABC)，然后将tr(ABC)进一步归约到其他线性代数问题。

Result: 发现矩阵乘法与det(A)、tr(A³)、tr(A⁻¹)等问题的量子计算复杂度等价，当前最优算法都需要Θ(n^ω)时间（ω≈2.37）。

Conclusion: 任何突破现有矩阵乘法指数ω的改进都会直接带来更快的量子矩阵乘法算法，这为量子线性代数算法的未来发展提供了重要方向。

Abstract: We observe that any $T(n)$ time algorithm (quantum or classical) for several
central linear algebraic problems, such as computing $\det(A)$, $tr(A^3)$, or
$tr(A^{-1})$ for an $n \times n$ integer matrix $A$, yields a $O(T(n)) + \tilde
O(n^2)$ time \textit{quantum algorithm} for $n \times n$ matrix-matrix
multiplication. That is, on quantum computers, the complexity of these problems
is essentially equivalent to that of matrix multiplication. Our results follow
by first observing that the Bernstein-Vazirani algorithm gives a direct quantum
reduction from matrix multiplication to computing $tr(ABC)$ for $n \times n$
inputs $A,B,C$. We can then reduce $tr(ABC)$ to each of our problems of
interest.
  For the above problems, and many others in linear algebra, their fastest
known algorithms require $\Theta(n^\omega)$ time, where $\omega \approx 2.37$
is the current exponent of fast matrix multiplication. Our finding shows that
any improvements beyond this barrier would lead to faster quantum algorithms
for matrix multiplication. Our results complement existing reductions from
matrix multiplication in algebraic circuits [BCS13], and reductions that work
for standard classical algorithms, but are not tight -- i.e., which roughly
show that an $O(n^{3-\delta})$ time algorithm for the problem yields an
$O(n^{3-\delta/3})$ matrix multiplication algorithm [WW10].

</details>


### [18] [A Better-Than-$5/4$-Approximation for Two-Edge Connectivity](https://arxiv.org/abs/2509.19655)
*Felix Hommelsheim,Alexander Lindermayr,Zhenwei Liu*

Main category: cs.DS

TL;DR: 本文提出了一种突破5/4近似比的2-边连通生成子图问题(2ECSS)的近似算法，达到了(5/4 - η)的近似比，其中η ≥ 10^{-6}。


<details>
  <summary>Details</summary>
Motivation: 2ECSS问题是生存网络设计中的基础问题，之前最好的近似比是5/4，但该界限在许多技术中都是紧的，不清楚是否能被改进。

Method: 采用两种互补算法：一种针对少量4-环的情况，一种针对大量4-环的情况。使用彩色桥覆盖、富顶点和分支粘合路径等新技术来克服突破5/4近似比的技术障碍。

Result: 成功突破了5/4的近似比界限，达到了(5/4 - η)的近似比，其中η是一个大于等于10^{-6}的常数。

Conclusion: 这是首次突破2ECSS问题5/4近似比界限的工作，为后续研究开辟了新的方向。

Abstract: The 2-Edge-Connected Spanning Subgraph Problem (2ECSS) is a fundamental
problem in survivable network design. Given an undirected $2$-edge-connected
graph, the goal is to find a $2$-edge-connected spanning subgraph with the
minimum number of edges; a graph is 2-edge-connected if it is connected after
the removal of any single edge. 2ECSS is APX-hard and has been extensively
studied in the context of approximation algorithms. Very recently, Bosch-Calvo,
Garg, Grandoni, Hommelsheim, Jabal Ameli, and Lindermayr showed the currently
best-known approximation ratio of $\frac{5}{4}$ [STOC 2025]. This factor is
tight for many of their techniques and arguments, and it was not clear whether
$\frac{5}{4}$ can be improved.
  We break this natural barrier and present a $(\frac{5}{4} -
\eta)$-approximation algorithm, for some constant $\eta \geq 10^{-6}$. On a
high level, we follow the approach of previous works: take a triangle-free
$2$-edge cover and transform it into a 2-edge-connected spanning subgraph by
adding only a few additional edges. For $\geq \frac{5}{4}$-approximations, one
can heavily exploit that a $4$-cycle in the 2-edge cover can ``buy'' one
additional edge. This enables simple and nice techniques, but immediately fails
for our improved approximation ratio. To overcome this, we design two
complementary algorithms that perform well for different scenarios: one for few
$4$-cycles and one for many $4$-cycles. Besides this, there appear more
obstructions when breaching $\frac54$, which we surpass via new techniques such
as colorful bridge covering, rich vertices, and branching gluing paths.

</details>


### [19] [Non-Clairvoyant Scheduling with Progress Bars](https://arxiv.org/abs/2509.19662)
*Ziyad Benomar,Romain Cosson,Alexander Lindermayr,Jens Schlöter*

Main category: cs.DS

TL;DR: 该论文研究非预知调度问题，引入进度条反馈机制，设计了针对对抗性和随机性进度条的算法，并在学习增强调度和算法组合方面取得新突破。


<details>
  <summary>Details</summary>
Motivation: 传统非预知调度缺乏作业处理时间的先验知识，作者希望通过引入进度条形式的连续反馈来改进调度性能，这是学习增强算法框架下的自然扩展。

Method: 设计了针对对抗性和随机性进度条的新调度算法，提出了调度算法组合的一般方法，并建立了随机进度条模型作为传统最坏情况模型的更乐观替代方案。

Result: 在对抗性情况下获得了强竞争性界限，意外地改进了基于作业大小预测的学习增强调度保证，在随机设置下提出了渐近最优调度算法。

Conclusion: 进度条反馈为非预知调度提供了有效的连续信息源，所提出的算法和方法在理论和实际应用中都显示出显著优势，为调度问题提供了新的解决思路。

Abstract: In non-clairvoyant scheduling, the goal is to minimize the total job
completion time without prior knowledge of individual job processing times.
This classical online optimization problem has recently gained attention
through the framework of learning-augmented algorithms. We introduce a natural
setting in which the scheduler receives continuous feedback in the form of
progress bars: estimates of the fraction of each job completed over time. We
design new algorithms for both adversarial and stochastic progress bars and
prove strong competitive bounds. Our results in the adversarial case
surprisingly induce improved guarantees for learning-augmented scheduling with
job size predictions. We also introduce a general method for combining
scheduling algorithms, yielding further insights in scheduling with
predictions. Finally, we propose a stochastic model of progress bars as a more
optimistic alternative to conventional worst-case models, and present an
asymptotically optimal scheduling algorithm in this setting.

</details>


### [20] [SS-GUMAP, SL-GUMAP, SSSL-GUMAP: Fast UMAP Algorithms for Large Graph Drawing](https://arxiv.org/abs/2509.19703)
*Amyra Meidiana,Seok-Hee Hong*

Main category: cs.DS

TL;DR: 本文提出了三种基于UMAP的快速图绘制算法，通过谱稀疏化和部分BFS等技术显著降低了计算复杂度，实现了比传统方法更快的运行速度且保持相似的绘图质量。


<details>
  <summary>Details</summary>
Motivation: UMAP是一种流行的降维算法，但直接应用于图绘制时存在O(nm)时间复杂度的全对最短路径计算问题，无法扩展到大型图的可视化。

Method: 提出了三种算法：(1) SS-GUMAP使用谱稀疏化保留图重要属性，将复杂度降至O(n² log n)；(2) SSL-GUMAP使用部分BFS将kNN图计算降至线性时间，通过边采样将成本优化降至亚线性时间；(3) SSSL-GUMAP结合两种方法实现O(n)总体运行时间。

Result: 实验显示SS-GUMAP比GUMAP快28%，质量指标相似；SL-GUMAP和SSSL-GUMAP快80%以上，质量指标差异小于15%。与tsNET相比，GUMAP快90%，邻域保持相似，质量指标平均好10%。

Conclusion: 提出的快速UMAP图绘制算法在保持绘图质量的同时显著提高了效率，验证了UMAP在图绘制中的有效性。

Abstract: UMAP is a popular neighborhood-preserving dimension reduction (DR) algorithm.
However, its application for graph drawing has not been evaluated. Moreover, a
naive application of UMAP to graph drawing would include O(nm) time all-pair
shortest path computation, which is not scalable to visualizing large graphs.
  In this paper, we present fast UMAP-based for graph drawing. Specifically, we
present three fast UMAP-based algorithms for graph drawing: (1) The SS-GUMAP
algorithm utilizes spectral sparsification to compute a subgraph G' preserving
important properties of a graph G, reducing the O(nm) component of the runtime
to O(n^2 log n) runtime; (2) The SSL-GUMAP algorithm reduces the kNN (k-Nearest
Neighbors) graph computation from $O(n \log n)$ time to linear time using
partial BFS (Breadth First Search), and the cost optimization runtime from O(n)
time to sublinear time using edge sampling; (3) The SSSL-GUMAP algorithm
combines both approaches, for an overall O(n) runtime.
  Experiments demonstrate that SS-GUMAP runs 28% faster than GUMAP, a naive
application of UMAP to graph drawing, with similar quality metrics, while
SL-GUMAP and SSSL-GUMAP run over 80% faster than GUMAP with less than 15%
difference on average for all quality metrics.
  We also present an evaluation of GUMAP to tsNET, a graph layout based on the
popular DR algorithm t-SNE. GUMAP runs 90% faster than tsNET with similar
neighborhood preservation and, on average, 10% better on quality metrics such
as stress, edge crossing, and shape-based metrics, validating the effectiveness
of UMAP for graph drawing.

</details>


### [21] [Geometric Interpretation of 3-SAT and Phase Transition](https://arxiv.org/abs/2509.19740)
*Frederic Gillet*

Main category: cs.DS

TL;DR: 将3-SAT问题解释为体积填充问题，并利用该方法探索SAT/UNSAT相变


<details>
  <summary>Details</summary>
Motivation: 传统的3-SAT问题分析方法存在局限性，需要新的几何视角来理解其相变行为

Method: 将3-SAT问题重新表述为高维空间中的体积填充问题，通过几何方法分析约束满足

Result: 该方法为理解3-SAT问题的SAT/UNSAT相变提供了新的几何解释和量化工具

Conclusion: 体积填充视角为组合优化问题的相变研究开辟了新的途径，具有重要的理论价值

Abstract: Interpretation of 3-SAT as a volume filling problem, and its use to explore
the SAT/UNSAT phase transition.

</details>


### [22] [BH-tsNET, FIt-tsNET, L-tsNET: Fast tsNET Algorithms for Large Graph Drawing](https://arxiv.org/abs/2509.19785)
*Amyra Meidiana,Seok-Hee Hong,Kwan-Liu Ma*

Main category: cs.DS

TL;DR: 本文提出了三种快速算法（BH-tsNET、FIt-tsNET和L-tsNET）来显著降低tsNET图绘制算法的时间复杂度，从O(nm)降至O(n log n)和O(n)，同时保持相似的绘图质量。


<details>
  <summary>Details</summary>
Motivation: tsNET算法使用t-SNE进行高质量图绘制，但时间复杂度较高（O(nm)），限制了其在大规模图上的应用。需要降低计算复杂度以提高效率。

Method: 通过优化三个关键组件：高维概率计算（C0）、KL散度梯度计算（C1）和熵计算（C2）。BH-tsNET使用部分BFS和四叉树方法；FIt-tsNET结合插值方法；L-tsNET使用FFT加速插值方法。

Result: 实验表明，三种算法分别比原始tsNET快93.5%、96%和98.6%，在邻域保持、应力、边交叉和形状度量等质量指标上表现相似。

Conclusion: 提出的快速算法显著提高了tsNET的效率，使其能够处理更大规模的图数据，同时保持高质量的图绘制效果。

Abstract: The tsNET algorithm utilizes t-SNE to compute high-quality graph drawings,
preserving the neighborhood and clustering structure. We present three fast
algorithms for reducing the time complexity of tsNET algorithm from O(nm) time
to O(n log n) time and O(n) time. To reduce the runtime of tsNET, there are
three components that need to be reduced: (C0) computation of high-dimensional
probabilities, (C1) computation of KL divergence gradient, and (C2) entropy
computation. Specifically, we reduce the overall runtime of tsNET, integrating
our new fast approaches for C0 and C2 with fast t-SNE algorithms for C1. We
first present O(n log n)-time BH-tsNET, based on (C0) new O(n)-time partial
BFS-based high-dimensional probability computation and (C2) new O(n log n)-time
quadtree-based entropy computation, integrated with (C1) O(n log n)-time
quadtree-based KL divergence computation of BH-SNE. We next present faster O(n
log n)-time FIt-tsNET, using (C0) O(n)-time partial BFS-based high-dimensional
probability computation and (C2) quadtree-based O(n log n)-time entropy
computation, integrated with (C1) O(n)-time interpolation-based KL divergence
computation of FIt-SNE. Finally, we present the O(n)-time L-tsNET, integrating
(C2) new O(n)-time FFT-accelerated interpolation-based entropy computation with
(C0) O(n)-time partial BFS-based high-dimensional probability computation, and
(C1) O(n)-time interpolation-based KL divergence computation of FIt-SNE.
Extensive experiments using benchmark data sets confirm that BH-tsNET,
FIt-tsNET, and L-tsNET outperform tsNET, running 93.5%, 96%, and 98.6% faster
while computing similar quality drawings in terms of quality metrics
(neighborhood preservation, stress, edge crossing, and shape-based metrics) and
visual comparison. We also present a comparison between our algorithms and
DRGraph, another dimension reduction-based graph drawing algorithm.

</details>


### [23] [Stealing From the Dragon's Hoard: Online Unbounded Knapsack With Removal](https://arxiv.org/abs/2509.19914)
*Matthias Gehnen,Moritz Stocker*

Main category: cs.DS

TL;DR: 本文提出了在线无界背包移除问题，这是在线背包问题的一个变体，允许物品无限次打包且可随时无成本移除。作者为一般情况提供了竞争比为1.6911的确定性算法，并给出了1.5877的下界。在比例设置下，确定性算法可达3/2竞争比，随机算法竞争比在6/5到4/3之间。


<details>
  <summary>Details</summary>
Motivation: 研究在线背包问题的一个自然变体，探索在允许物品移除和无限次打包的情况下，是否能够设计出具有竞争性的在线算法。

Method: 提出了在线无界背包移除问题的数学模型，设计了确定性算法来分析一般情况和比例情况下的竞争比，并给出了相应的上下界证明。

Result: 在一般设置下，确定性算法的竞争比为1.6911，下界为1.5877；在比例设置下，确定性算法可达3/2竞争比，随机算法竞争比在6/5到4/3之间。

Conclusion: 这是少数几个允许确定性算法具有竞争性的自然在线背包变体之一，为在线优化问题提供了新的理论结果和算法设计思路。

Abstract: We introduce the Online Unbounded Knapsack Problem with Removal, a variation
of the well-known Online Knapsack Problem. Items, each with a weight and value,
arrive online and an algorithm must decide on whether or not to pack them into
a knapsack with a fixed weight limit. An item may be packed an arbitrary number
of times and items may be removed from the knapsack at any time without cost.
The goal is to maximize the total value of items packed, while respecting a
weight limit. We show that this is one of the very few natural online knapsack
variants that allow for competitive deterministic algorithms in the general
setting, by providing an algorithm with competitivity 1.6911. We complement
this with a lower bound of 1.5877.
  We also analyze the proportional setting, where the weight and value of any
single item agree, and show that deterministic algorithms can be exactly
3/2-competitive. Lastly, we give lower and upper bounds of 6/5 and 4/3 on the
competitivity of randomized algorithms in this setting.

</details>


### [24] [Ads that Stick: Near-Optimal Ad Optimization through Psychological Behavior Models](https://arxiv.org/abs/2509.20304)
*Kailash Gopal Darmasubramanian,Akash Pareek,Arindam Khan,Arpit Agarwal*

Main category: cs.DS

TL;DR: 本文提出了一种基于心理学原理的广告投放优化算法，通过建模用户兴趣变化来最大化广告效果，相比传统均匀投放策略有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有广告调度策略依赖简单启发式方法（如均匀间隔和频率上限），忽视了用户长期兴趣的心理机制。本文旨在基于心理学原理优化广告投放时机和频率。

Method: 基于三个关键心理学原理建模用户兴趣变化：单纯曝光效应、享乐适应和操作性条件反射。开发了一种准线性时间算法来生成近似最优的广告投放计划。

Result: 算法输出与最优计划的性能差异呈指数级小，实验结果表明该策略在各种自然设置下显著优于基线方法，证明均匀间隔等简单启发式方法是次优的。

Conclusion: 该研究提供了关于最优广告投放的重要见解，表明考虑心理学效应的调度策略能显著提升广告效果，最优广告数量可通过简单线性搜索确定。

Abstract: Optimizing the timing and frequency of ads is a central problem in digital
advertising, with significant economic consequences. Existing scheduling
policies rely on simple heuristics, such as uniform spacing and frequency caps,
that overlook long-term user interest. However, it is well-known that users'
long-term interest and engagement result from the interplay of several
psychological effects (Curmei, Haupt, Recht, Hadfield-Menell, ACM CRS, 2022).
  In this work, we model change in user interest upon showing ads based on
three key psychological principles: mere exposure, hedonic adaptation, and
operant conditioning. The first two effects are modeled using a concave
function of user interest with repeated exposure, while the third effect is
modeled using a temporal decay function, which explains the decline in user
interest due to overexposure. Under our psychological behavior model, we ask
the following question: Given a continuous time interval $T$, how many ads
should be shown, and at what times, to maximize the user interest towards the
ads?
  Towards answering this question, we first show that, if the number of
displayed ads is fixed, then the optimal ad-schedule only depends on the
operant conditioning function. Our main result is a quasi-linear time algorithm
that outputs a near-optimal ad-schedule, i.e., the difference in the
performance of our schedule and the optimal schedule is exponentially small.
Our algorithm leads to significant insights about optimal ad placement and
shows that simple heuristics such as uniform spacing are sub-optimal under many
natural settings. The optimal number of ads to display, which also depends on
the mere exposure and hedonistic adaptation functions, can be found through a
simple linear search given the above algorithm. We further support our findings
with experimental results, demonstrating that our strategy outperforms various
baselines.

</details>


### [25] [Testable algorithms for approximately counting edges and triangles in sublinear time and space](https://arxiv.org/abs/2509.20351)
*Talya Eden,Ronitt Rubinfeld,Arsen Vasilyan*

Main category: cs.DS

TL;DR: 本文提出了一种无需事先知道图树宽度的三角形计数算法，能够自适应地估计图结构并保证结果的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的三角形计数算法在假设图树宽度有界时效率很高，但当这个假设被违反时，结果不再可靠。需要一种无需先验知识且能自适应图结构的鲁棒算法。

Method: 通过尝试一系列候选树宽度值，使用可测试算法框架，确保错误候选值不会导致错误估计。算法会检测错误候选并继续尝试新候选，直到接受正确候选。

Result: 算法在时间O*(m·α(G)/t + m/t^{2/3})内计算(1±ε)-近似，其中α(G)是图树宽度，t是三角形数量。该算法适用于任何图，无需先验知识。

Conclusion: 该方法在保持已知树宽度边界时的高效性的同时，确保了对抗误导建议的鲁棒性，并通过下界证明这种开销是不可避免的。

Abstract: We consider the fundamental problems of approximately counting the numbers of
edges and triangles in a graph in sublinear time. Previous algorithms for these
tasks are significantly more efficient under a promise that the arboricity of
the graph is bounded by some parameter $\overline{\alpha}$. However, when this
promise is violated, the estimates given by these algorithms are no longer
guaranteed to be correct.
  For the triangle counting task, we give an algorithm that requires no promise
on the input graph $G$, and computes a $(1\pm \epsilon)$-approximation for the
number of triangles $t$ in $G$ in time $O^*\left( \frac{m\cdot \alpha(G)}{t} +
\frac{m}{t^{2/3}}
  \right)$, where $\alpha(G)$ is the arboricity of the graph. The algorithm can
be used on any graph $G$ (no prior knowledge the arboricity $\alpha(G)$ is
required), and the algorithm adapts its run-time on the fly based on the graph
$G$.
  We accomplish this by trying a sequence of candidate values $\tilde{\alpha}$
for $\alpha(G)$ and using a novel algorithm in the framework of testable
algorithms. This ensures that wrong candidates $\tilde{\alpha}$ cannot lead to
incorrect estimates: as long as the advice is incorrect, the algorithm detects
it and continues with a new candidate. Once the algorithm accepts the
candidate, its output is guaranteed to be correct with high probability.
  We prove that this approach preserves - up to an additive overhead - the
dramatic efficiency gains obtainable when good arboricity bounds are known in
advance, while ensuring robustness against misleading advice. We further
complement this result with a lower bound, showing that such an overhead is
unavoidable whenever the advice may be faulty.
  We further demonstrate implications of our results for triangle counting in
the streaming model.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [26] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust是一个编译器，能自动插入flush和fence操作，确保持久内存代码没有缺失flush和fence的bug。


<details>
  <summary>Details</summary>
Motivation: 持久内存和CXL共享内存需要在崩溃后保持内容，但正确使用flush和fence操作很困难。现有工具需要bug暴露的测试用例，无法确保没有缺失flush的bug。

Method: PMRobust采用新颖的静态分析，针对新分配的对象进行优化，自动插入必要的flush和fence操作。

Result: 在持久内存库和多个持久内存数据结构上评估，相对于手动放置flush和fence操作的基准测试，几何平均开销仅为0.26%。

Conclusion: PMRobust能有效自动确保持久内存代码的正确性，且性能开销极小。

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [27] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: 该论文提出了一个将推理能力强大的大型语言模型（LLMs）与AFL++模糊测试工具集成的开源微服务框架，以解决传统模糊测试工具在语义推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的模糊测试工具虽然能有效探索代码路径，但缺乏语义推理能力，无法处理复杂的协议逻辑和字段间依赖关系。而具有推理能力的LLMs可以利用预训练知识理解输入格式和约束条件，但缺乏监督微调的真实数据，因此需要探索基于提示的少样本学习方法。

Method: 开发了一个开源微服务框架，将推理LLMs与AFL++集成在Google的FuzzBench平台上，解决了LLMs和模糊测试工具在异步执行和硬件需求（GPU vs CPU）方面的差异。

Result: 实验评估了四个研究问题，发现Deepseek模型表现最佳，突变效果更多取决于提示复杂度和模型选择而非样本数量。响应延迟和吞吐量瓶颈是主要挑战。

Conclusion: 推理LLMs可以显著提升模糊测试的突变质量，但需要解决性能瓶颈问题。Deepseek模型在提示条件下表现最优，为未来工作提供了方向。

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [28] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLMs）能否从源代码自动恢复用户故事，并评估了不同提示策略对输出质量的影响。


<details>
  <summary>Details</summary>
Motivation: 在敏捷开发中，用户故事至关重要，但在遗留系统和文档不完善的系统中常常缺失或过时。

Method: 使用1,750个标注的C++代码片段，评估了五种最先进的LLMs在六种提示策略下的表现。

Result: 所有模型在不超过200行代码的情况下平均F1分数达到0.8。最小的8B模型通过单个示例提示即可达到70B模型的性能，而链式思考提示仅对较大模型有边际提升。

Conclusion: LLMs能够有效从源代码恢复用户故事，简单的示例提示比复杂的推理策略更有效，特别是对于较小模型。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [29] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文评估了四种最先进的Fill-in-the-Middle大语言模型在生成Java测试断言消息方面的能力，发现Codestral-22B表现最佳，但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 断言消息能显著提升单元测试的可读性，但开发者和自动化测试工具经常省略它们。目前缺乏对大语言模型生成断言消息能力的系统评估。

Method: 使用包含216个Java测试方法的数据集，评估了Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B和StarCoder四种FIM LLM模型，采用类人评估方法进行质量评分。

Result: Codestral-22B获得最高质量分2.76/5，而人工编写的消息为3.24。包含描述性测试注释可将Codestral性能提升至2.97。所有模型都能复制开发者偏好的语言模式。

Conclusion: 研究为推进自动化、上下文感知的断言消息生成提供了重要基础，指出了现有模型和传统文本评估指标在捕捉多样化断言消息结构方面的局限性。

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [30] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 企业级AI辅助软件开发工具的真实评估：300名工程师使用内部AI平台(DeputyDev)一年，PR审查周期减少31.8%，代码生产量增加28%


<details>
  <summary>Details</summary>
Motivation: 评估AI工具在企业软件开发环境中的实际效果，提供生产环境的实证数据而非受控基准测试

Method: 纵向研究，300名工程师使用结合代码生成和自动审查功能的内部AI平台(DeputyDev)，进行为期一年的队列分析

Result: PR审查周期减少31.8%，开发者满意度85%，使用率从4%增长到83%，顶级采用者代码生产量增加61%，整体代码交付量增加28%

Conclusion: AI工具在企业软件开发中具有变革潜力，但实际部署面临挑战，实证研究为AI集成提供了重要参考

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [31] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen提出了一种协调多智能体架构，通过数据驱动的桥接语言选择机制，显著提升多编程语言代码生成质量


<details>
  <summary>Details</summary>
Motivation: 当前LLM在不同编程语言上的代码生成能力差异很大，特别是对于训练数据较少的语言如Rust、Perl、OCaml等。现有方法往往孤立处理每种语言，未能充分利用跨语言的知识共享和模式复用

Method: 采用协调多智能体架构，集成中间表示、代码生成、翻译和自动修复。核心创新是基于经验推导的转移矩阵选择最佳桥接语言，进行早期输出验证和迭代错误修正

Result: 实验显示XL-CoGen比最强的微调基线提升13个百分点，比现有单语言多智能体方法提升高达30个百分点。消融研究证实兼容性引导的桥接策略显著优于基于LLM的启发式方法

Conclusion: 该研究证明了累积跨语言知识转移的价值，为多语言代码生成提供了有效解决方案

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [32] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: 本文提出了神经网络物料清单（NNBOM）的概念，用于分析神经网络软件的演化趋势，填补了传统SBOM和概念性AIBOM在NN软件分析方面的不足。


<details>
  <summary>Details</summary>
Motivation: 神经网络软件快速发展，但缺乏有效的演化分析工具。传统SBOM不适用于NN软件，而概念性AIBOM缺乏实际实现。

Method: 从55,997个PyTorch GitHub仓库构建大规模NNBOM数据库，分析软件规模、组件重用和跨领域依赖的演化趋势。

Result: 建立了全面的NNBOM数据集，并开发了两个原型应用来展示分析的实际价值。

Conclusion: NNBOM为神经网络软件的演化分析提供了有效工具，有助于开发者和维护者把握长期趋势。

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [33] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym是一个针对视觉游戏开发的多模态代码生成基准测试，包含2,219个高质量样本，旨在弥补当前代码大语言模型在算法问题解决与实际游戏开发需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型基准测试主要关注单模态编程任务，忽略了游戏开发中关键的视觉美学、可玩性和用户参与度等指标，无法满足实际游戏开发需求。

Method: 采用基于聚类的筛选方法从真实世界代码库中提取100个主题簇的样本，建立多模态评估框架，使用完整的UI沙盒环境进行自动化视觉代码合成。

Result: V-GameGym能够有效连接代码生成准确性与实际游戏开发工作流程，为视觉编程和交互元素生成提供可量化的质量指标。

Conclusion: 该基准测试填补了代码LLM在视觉游戏开发评估方面的空白，为实际部署提供了更全面的评估标准。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [34] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型进行数据增强的方法来解决需求追踪中的数据稀缺问题，通过提示模板生成需求到代码的追踪链接，并优化追踪模型的编码器，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化需求追踪方法受到训练数据稀缺和需求与代码之间语义鸿沟的限制，需要解决数据不足的问题以提高追踪准确性。

Method: 使用四种大型语言模型（Gemini 1.5 Pro、Claude 3、GPT-3.5、GPT-4）和零样本/少样本提示模板进行数据增强，同时优化追踪模型的编码器组件以适应增强数据。

Result: 实验结果表明，该方法显著提升了模型性能，F1分数最高提升了28.59%，证明了方法的有效性。

Conclusion: 基于LLM的数据增强方法能有效解决需求追踪中的数据稀缺问题，具有实际应用潜力，四种提示模板和LLM的比较分析为后续研究提供了参考。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [35] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: 本文评估了大型语言模型在生成Web API集成代码方面的能力，发现现有开源模型在生成API调用代码时存在显著挑战，错误率较高。


<details>
  <summary>Details</summary>
Motivation: API集成是数字基础设施的核心，但编写正确的API调用代码具有挑战性。虽然LLMs在软件开发中越来越流行，但它们在生成Web API集成代码方面的有效性尚未得到充分探索。

Method: 提出了一个数据集和评估流程，用于评估LLMs生成Web API调用代码的能力。对多个开源LLMs进行了实验。

Result: 实验结果显示，生成API调用存在重大挑战，包括产生虚构的端点、错误的参数使用等错误。评估的开源模型中没有一个能够解决超过40%的任务。

Conclusion: 当前的开源LLMs在生成Web API集成代码方面表现不佳，需要进一步改进模型能力来解决这一重要但具有挑战性的任务。

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [36] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: 本文提出VCD-RNK模型，通过语义对齐方法解决LLM在Verilog生成中缺乏领域知识的问题，避免传统方法中计算密集的测试执行过程。


<details>
  <summary>Details</summary>
Motivation: LLM在Verilog代码生成中面临领域知识不足的挑战，硬件工程师需要一个可信赖的解决方案而非多个不确定的候选方案。

Method: 将问题建模为需求与Verilog实现之间的语义对齐问题，提出VCD-RNK判别器模型，通过代码语义分析、测试用例生成和功能正确性评估三个维度进行专家知识蒸馏。

Result: VCD-RNK在推理过程中显式模拟上述推理过程，有效避免了现有方法中计算密集的测试执行。

Conclusion: VCD-RNK为Verilog代码生成提供了一个高效的重排序解决方案，解决了LLM在硬件设计领域的可信度问题。

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [37] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: 该论文提出了一种基于零知识证明(ZKP)的方法，用于在保护机密性的同时实现业务过程的可验证执行。通过将zkVM集成到BPM引擎中，支持链式可验证计算，并以产品碳足迹为例展示了如何在不暴露敏感信息的情况下证明和验证过程完整性。


<details>
  <summary>Details</summary>
Motivation: 解决跨组织业务流程中确保过程完整性而不泄露机密业务信息的挑战。传统方法难以在验证过程正确性的同时保护商业敏感数据。

Method: 将零知识证明虚拟机(zkVM)集成到业务流程管理引擎中，通过系统架构和原型实现支持链式可验证计算。采用证明组合技术，并在产品碳足迹场景中建模顺序足迹活动。

Result: 实验评估表明，该方法能够在给定机密性约束下自动化过程验证，评估了不同ZKP证明变体在过程模型中的证明和验证效率。

Conclusion: 零知识证明技术可以有效地集成到BPM生命周期中，实现业务流程的可验证执行同时保护商业机密信息，为跨组织协作提供了新的隐私保护解决方案。

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [38] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: 本文提出了一种结合输入生成和输出检查的协议测试新方法，使用I/O语法完整规范协议语法和语义，并通过FANDANGO框架实现测试生成、模拟对象和测试预言功能。


<details>
  <summary>Details</summary>
Motivation: 协议测试面临两个基本问题：需要生成语法语义正确且多样化的输入，以及需要检查输出正确性的预言机制。现有工具无法同时解决这两个问题。

Method: 引入I/O语法完整规范协议的消息、状态和交互，基于FANDANGO框架实现多功能测试工具，支持用户定义约束和k路径引导系统化覆盖。

Result: 在DNS、FTP、SMTP等协议上的评估表明，I/O语法能正确完整地规范高级协议特性，系统化覆盖比随机方法能更快覆盖输入和响应空间。

Conclusion: I/O语法为协议测试提供了统一的规范框架，结合系统化覆盖策略，显著提升了测试效率和效果。

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [39] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: 该研究调查了GitHub Copilot对开发者活动和感知生产力的实际影响，发现在大型公共部门组织中，Copilot用户的活动水平在工具引入前后没有显著变化，但存在客观指标与主观生产力感知之间的差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解生成式AI工具GitHub Copilot在真实工作环境中的实际影响，特别是在大型公共部门敏捷组织中的效果，填补了现有研究中对实际生产力影响的实证空白。

Method: 采用混合方法案例研究，分析NAV IT组织中703个GitHub仓库的26,317个非合并提交，比较25名Copilot用户和14名非用户的活动指标，并结合调查问卷和13次访谈进行综合分析。

Result: Copilot用户在使用工具前后在提交活动指标上没有显著统计变化，尽管观察到轻微增加；但用户群体在Copilot引入前就比非用户更活跃；存在客观活动指标与主观生产力感知之间的不一致。

Conclusion: GitHub Copilot的使用并未显著改变开发者的提交活动模式，但用户的主观生产力感知与客观指标存在差异，表明需要更全面的评估方法来衡量AI工具的实际影响。

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [40] [Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question](https://arxiv.org/abs/2509.19337)
*Stefanos Bakirtzis,Paul Almasan,José Suárez-Varela,Gabriel O. Ferreira,Michail Kalntis,André Felipe Zanella,Ian Wassell,Andra Lutu*

Main category: cs.NI

TL;DR: 本文通过大规模真实世界数据评估了可微分光线追踪与传统深度学习模型在无线电传播建模中的性能对比，发现深度学习模型在准确性和适应性方面优于可微分光线追踪方法。


<details>
  <summary>Details</summary>
Motivation: 可微分光线追踪技术虽然理论上具有速度和数据学习优势，但缺乏在实际生产级网络中的实验验证，无法为移动网络运营商和研究社区提供明确的适用性指导。

Method: 使用来自主要移动网络运营商的真实世界数据，覆盖13个城市和超过10,000个天线，同时部署可微分光线追踪和深度学习模型进行无线电覆盖仿真。

Result: 可微分光线追踪在大规模真实数据下泛化能力不足，不适合实时应用；而深度学习模型在城郊乡村等各种部署场景中表现出更高的准确性和更快的适应能力，准确度提升可达3dB。

Conclusion: 深度学习模型在实际应用中优于可微分光线追踪方法，为无线生态系统和未来研究提供了重要参考。

Abstract: Differentiable ray tracing has recently challenged the status quo in radio
propagation modelling and digital twinning. Promising unprecedented speed and
the ability to learn from real-world data, it offers a real alternative to
conventional deep learning (DL) models. However, no experimental evaluation on
production-grade networks has yet validated its assumed scalability or
practical benefits. This leaves mobile network operators (MNOs) and the
research community without clear guidance on its applicability. In this paper,
we fill this gap by employing both differentiable ray tracing and DL models to
emulate radio coverage using extensive real-world data collected from the
network of a major MNO, covering 13 cities and more than 10,000 antennas. Our
results show that, while differentiable ray-tracing simulators have contributed
to reducing the efficiency-accuracy gap, they struggle to generalize from
real-world data at a large scale, and they remain unsuitable for real-time
applications. In contrast, DL models demonstrate higher accuracy and faster
adaptation than differentiable ray-tracing simulators across urban, suburban,
and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental
results aim to provide timely insights into a fundamental open question with
direct implications on the wireless ecosystem and future research.

</details>


### [41] [Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks](https://arxiv.org/abs/2509.19341)
*Yang Fu,Peng Qin,Yueyue Zhang,Yifei Wang*

Main category: cs.NI

TL;DR: 提出了一种细粒度的AI模型缓存和下载系统，利用参数可重用性来优化6G网络中边缘节点的模型存储和传输效率。


<details>
  <summary>Details</summary>
Motivation: 解决6G网络中AI模型下载面临的挑战：大模型尺寸导致边缘缓存存储容量受限，以及异构模型在无线信道上的并发传输困难。

Method: 基于参数可重用性原理，选择性缓存模型参数块；采用协调多点广播传输可重用参数块；提出分布式多智能体学习框架优化缓存、迁移和波束成形；使用数据增强方法提高样本效率。

Result: 理论分析和仿真实验验证了所提学习框架的优越收敛性能。

Conclusion: 该系统能有效减少模型下载延迟，提高下行频谱利用率，为6G网络中的按需AI模型下载提供了可行解决方案。

Abstract: 6G networks are envisioned to support on-demand AI model downloading to
accommodate diverse inference requirements of end users. By proactively caching
models at edge nodes, users can retrieve the requested models with low latency
for on-device AI inference. However, the substantial size of contemporary AI
models poses significant challenges for edge caching under limited storage
capacity, as well as for the concurrent delivery of heterogeneous models over
wireless channels. To address these challenges, we propose a fine-grained AI
model caching and downloading system that exploits parameter reusability,
stemming from the common practice of fine-tuning task-specific models from a
shared pre-trained model with frozen parameters. This system selectively caches
model parameter blocks (PBs) at edge nodes, eliminating redundant storage of
reusable parameters across different cached models. Additionally, it
incorporates coordinated multipoint (CoMP) broadcasting to simultaneously
deliver reusable PBs to multiple users, thereby enhancing downlink spectrum
utilization. Under this arrangement, we formulate a model downloading delay
minimization problem to jointly optimize PB caching, migration (among edge
nodes), and broadcasting beamforming. To tackle this intractable problem, we
develop a distributed multi-agent learning framework that enables edge nodes to
explicitly learn mutual influence among their actions, thereby facilitating
cooperation. Furthermore, a data augmentation approach is proposed to
adaptively generate synthetic training samples through a predictive model,
boosting sample efficiency and accelerating policy learning. Both theoretical
analysis and simulation experiments validate the superior convergence
performance of the proposed learning framework.

</details>


### [42] [TinyAC: Bringing Autonomic Computing Principles to Resource-Constrained Systems](https://arxiv.org/abs/2509.19350)
*Wojciech Kalka,Ruitao Xue,Kamil Faber,Aleksander Slominski,Devki Jha,Rajiv Ranjan,Tomasz Szydlo*

Main category: cs.NI

TL;DR: 本文探讨了自主计算在物联网设备中的应用问题与挑战，提出了一种结合自下而上智能（TinyML和设备端学习）与自上而下指导（LLMs）的混合方法，旨在开发可扩展且可解释的智能自适应微型系统。


<details>
  <summary>Details</summary>
Motivation: 自主计算是开发深度网络边缘智能自适应自管理系统的有前景方法，但将其应用于物联网设备时面临特定问题和挑战。

Method: 提出混合方法：结合自下而上智能（TinyML和设备端学习）与自上而下指导（大型语言模型），实现可扩展且可解释的智能自适应微型系统开发。

Result: 识别了TinyAC系统需要自适应特性来处理运行中可能出现的问题，并指出了现有挑战和研究方向。

Conclusion: 通过混合方法可以开发出更智能、自适应的物联网系统，但仍需解决现有挑战并探索未来研究方向。

Abstract: Autonomic Computing (AC) is a promising approach for developing intelligent
and adaptive self-management systems at the deep network edge. In this paper,
we present the problems and challenges related to the use of AC for IoT
devices. Our proposed hybrid approach bridges bottom-up intelligence (TinyML
and on-device learning) and top-down guidance (LLMs) to achieve a scalable and
explainable approach for developing intelligent and adaptive self-management
tiny systems. Moreover, we argue that TinyAC systems require self-adaptive
features to handle problems that may occur during their operation. Finally, we
identify gaps, discuss existing challenges and future research directions.

</details>


### [43] [A User-to-User Resource Reselling Game in Open RAN with Buffer Rollover](https://arxiv.org/abs/2509.19392)
*Ruide Cao,Marie Siew,David Yau*

Main category: cs.NI

TL;DR: 提出了一种基于博弈论的O-RAN网络中用户间PRB资源转售模型，通过建模未满足需求的跨时隙传递和用户缓冲区状态，证明了纳什均衡的存在性和唯一性，并设计了收敛到均衡的迭代竞价机制。


<details>
  <summary>Details</summary>
Motivation: O-RAN框架的灵活性为未使用物理资源块（PRBs）的用户间转售提供了机会，以提高频谱效率并满足用户动态异构的服务需求。

Method: 构建了用户间的战略博弈模型，考虑未满足需求的跨时隙传递和用户缓冲区状态与PRB购买的关系，提出了收敛到纳什均衡的迭代竞价机制。

Result: 仿真实验表明，该方法相比无转售机制，数据丢失减少30.5%，频谱资源浪费减少50.7%，并显著提高了社会福利。

Conclusion: 该博弈论模型和竞价机制有效提升了O-RAN网络中的资源利用效率和服务质量，为动态资源分配提供了可行方案。

Abstract: The development of the Open RAN (O-RAN) framework helps enable network
slicing through its virtualization, interoperability, and flexibility. To
improve spectral efficiency and better meet users' dynamic and heterogeneous
service demands, O-RAN's flexibility further presents an opportunity for
resource reselling of unused physical resource blocks (PRBs) across users. In
this work, we propose a novel game-based user-to-user PRB reselling model in
the O-RAN setting, which models the carryover of unmet demand across time
slots, along with how users' internal buffer states relate to any PRBs
purchased. We formulate the interplay between the users as a strategic game,
with each participant aiming to maximize their own payoffs, and we prove the
existence and uniqueness of the Nash equilibrium (NE) in the game. We
furthermore propose an iterative bidding mechanism that converges to this NE.
Extensive simulations show that our best approach reduces data loss by 30.5%
and spectrum resource wastage by 50.7% while significantly improving social
welfare, compared to its absence.

</details>


### [44] [FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks](https://arxiv.org/abs/2509.19398)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: FedOC是一个多服务器联邦学习框架，利用重叠区域的客户端作为中继节点和动态模型选择器，实现边缘服务器间的模型共享和间接数据融合，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 多服务器联邦学习中，不同边缘服务器覆盖区域可能存在重叠，重叠区域的客户端可以访问多个边缘服务器模型。传统方法未能充分利用这一特性，FedOC旨在通过重叠客户端实现边缘服务器间的有效模型共享。

Method: 1) 将重叠客户端分为中继重叠客户端(ROCs)和普通重叠客户端(NOCs)；2) ROCs实时转发边缘模型促进服务器间模型共享；3) NOCs根据模型交付时间动态选择初始模型进行本地训练；4) 每轮训练包含本地训练、模型聚合、ROC中继转发和二次聚合。

Result: 实验结果表明，FedOC相比现有方法在性能上取得了显著提升，特别适合延迟敏感的边缘计算环境。

Conclusion: FedOC通过充分利用重叠客户端实现了边缘服务器间的去中心化模型传播，有效加速了训练过程，为多服务器联邦学习提供了创新解决方案。

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to
mitigate communication bottlenecks of single-server FL. We focus on a typical
multi-server FL architecture, where the regions covered by different edge
servers (ESs) may overlap. A key observation of this architecture is that
clients located in the overlapping areas can access edge models from multiple
ESs. Building on this insight, we propose FedOC (Federated learning with
Overlapping Clients), a novel framework designed to fully exploit the potential
of these overlapping clients. In FedOC, overlapping clients could serve dual
roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models
between neighboring ESs in real time to facilitate model sharing among
different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically
select their initial model for local training based on the edge model delivery
time, which enables indirect data fusion among different regions of ESs. The
overall FedOC workflow proceeds as follows: in every round, each client trains
local model based on the earliest received edge model and transmits to the
respective ESs for model aggregation. Then each ES transmits the aggregated
edge model to neighboring ESs through ROC relaying. Upon receiving the relayed
models, each ES performs a second aggregation and subsequently broadcasts the
updated model to covered clients. The existence of ROCs enables the model of
each ES to be disseminated to the other ESs in a decentralized manner, which
indirectly achieves intercell model and speeding up the training process,
making it well-suited for latency-sensitive edge environments. Extensive
experimental results show remarkable performance gains of our scheme compared
to existing methods.

</details>


### [45] [Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation](https://arxiv.org/abs/2509.19405)
*Tony Chahoud,Lorenzo Mario Amorosa,Riccardo Marini,Luca De Nardis*

Main category: cs.NI

TL;DR: 提出了一种轻量级模块化移动数据增强框架，通过合成空间位置和无线电特征来增强基于多小区指纹的定位性能，特别适用于稀疏测量区域


<details>
  <summary>Details</summary>
Motivation: 解决蜂窝网络中室外定位因测量数据稀疏、异构且现场勘测成本高而导致精度受限的问题

Method: 使用核密度估计(KDE)建模空间分布生成地理连贯的合成位置，结合K近邻(KNN)方法生成增强的每小区无线电指纹，采用训练免费、可解释的模块化架构

Result: 在真实MDT数据集上的实验表明，KDE-KNN增强方法能持续提升定位性能，在稀疏采样或结构复杂区域效果最显著，但存在区域依赖的饱和效应

Conclusion: 该框架为运营商提供了一种实用、低复杂度的路径，可利用现有移动数据轨迹增强定位服务

Abstract: Accurate outdoor positioning in cellular networks is hindered by sparse,
heterogeneous measurement collections and the high cost of exhaustive site
surveys. This paper introduces a lightweight, modular mobile data augmentation
framework designed to enhance multi-cell fingerprinting-based positioning using
operator-collected minimization of drive test (MDT) records. The proposed
approach decouples spatial and radio-feature synthesis: kernel density
estimation (KDE) models the empirical spatial distribution to generate
geographically coherent synthetic locations, while a k-nearest-neighbor
(KNN)-based block produces augmented per-cell radio fingerprints. The
architecture is intentionally training-free, interpretable, and suitable for
distributed or on-premise operator deployments, supporting privacy-aware
workflows. We both validate each augmentation module independently and assess
its end-to-end impact on fingerprinting-based positioning using a real-world
MDT dataset provided by an Italian mobile network operator across diverse urban
and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation
consistently improves positioning performance, with the largest benefits in
sparsely sampled or structurally complex regions; we also observe
region-dependent saturation effects as augmentation increases. The framework
offers a practical, low-complexity path to enhance operator positioning
services using existing mobile data traces.

</details>


### [46] [Poster: ChatIYP: Enabling Natural Language Access to the Internet Yellow Pages Database](https://arxiv.org/abs/2509.19411)
*Vasilis Andritsoudis,Pavlos Sermpezis,Ilias Dimitriadis,Athena Vakali*

Main category: cs.NI

TL;DR: ChatIYP是一个基于检索增强生成(RAG)的系统，允许用户通过自然语言查询Internet Yellow Pages(IYP)，解决了传统查询需要Cypher语言和IYP模式知识的问题。


<details>
  <summary>Details</summary>
Motivation: IYP整合了互联网路由信息，但查询需要掌握Cypher语言和精确的IYP模式，限制了非专家用户的使用。

Method: 提出ChatIYP系统，采用领域特定的检索增强生成(RAG)技术，将自然语言问题转换为IYP查询。

Result: 评估显示在简单查询上表现良好，同时指出了改进方向，并为选择更适合IYP查询AI代理的评估指标提供了见解。

Conclusion: ChatIYP成功实现了通过自然语言访问IYP知识库的目标，为非专家用户提供了更友好的查询界面。

Abstract: The Internet Yellow Pages (IYP) aggregates information from multiple sources
about Internet routing into a unified, graph-based knowledge base. However,
querying it requires knowledge of the Cypher language and the exact IYP schema,
thus limiting usability for non-experts. In this paper, we propose ChatIYP, a
domain-specific Retrieval-Augmented Generation (RAG) system that enables users
to query IYP through natural language questions. Our evaluation demonstrates
solid performance on simple queries, as well as directions for improvement, and
provides insights for selecting evaluation metrics that are better fit for IYP
querying AI agents.

</details>


### [47] [Where 6G Stands Today: Evolution, Enablers, and Research Gaps](https://arxiv.org/abs/2509.19646)
*Salma Tika,Abdelkrim Haqiq,Essaid Sabir,Elmahdi Driouch*

Main category: cs.NI

TL;DR: 本文对6G移动通信系统进行了全面概述，包括其严格需求、关键技术、应用场景及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着5G的全球部署，产业界和学术界开始构想6G，以满足日益增长的先进数字化社会需求。5G虽然相比LTE有显著进步，但在超高可靠性、无缝自动化和无处不在覆盖等方面可能仍显不足。

Method: 论文通过分析6G的主要严格需求，重点介绍了太赫兹通信、智能反射面、大规模MIMO和AI驱动网络等关键技术，并列举了6G的应用场景。

Result: 6G有望实现高度智能、自动化和超可靠的通信系统，能够处理大量连接设备。

Conclusion: 论文最后概述了实现6G承诺必须解决的潜在挑战。

Abstract: As the fifth-generation (5G) mobile communication system continues its global
deployment, both industry and academia have started conceptualizing the 6th
generation (6G) to address the growing need for a progressively advanced and
digital society. Even while 5G offers considerable advancements over LTE, it
could struggle to be sufficient to meet all of the requirements, including
ultra-high reliability, seamless automation, and ubiquitous coverage. In
response, 6G is supposed to bring out a highly intelligent, automated, and
ultra-reliable communication system that can handle a vast number of connected
devices. This paper offers a comprehensive overview of 6G, beginning with its
main stringent requirements while focusing on key enabling technologies such as
terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO
and AI-driven networking that will shape the 6G networks. Furthermore, the
paper lists various 6G applications and usage scenarios that will benefit from
these advancements. At the end, we outline the potential challenges that must
be addressed to achieve the 6G promises.

</details>


### [48] [RIS-assisted Data Collection and Wireless Power Transfer in Low-altitude Wireless Networks](https://arxiv.org/abs/2509.19651)
*Wenwen Xie,Geng Sun,Jiahui Li,Jiacheng Wang,Yinqiu Liu,Dusit Niyato,Dong In Kim,Shiwen Mao*

Main category: cs.NI

TL;DR: 该论文提出了一种基于RIS辅助的无人机数据收集和无线能量传输系统，采用深度强化学习方法优化RIS相位、无人机轨迹等参数，以最小化信息年龄和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决低空无线网络中物联网设备面临的能量约束和低信道质量问题，确保在偏远地区及时收集数据。

Method: 提出交替优化改进参数化深度Q网络（AO-IPDQN）方法，先使用交替优化方法优化RIS相位，再用改进的参数化深度Q网络处理混合动作空间。

Result: 仿真结果表明，AO-IPDQN方法在各种仿真场景下相对于多种对比方法表现出优异的性能。

Conclusion: RIS辅助的无人机系统结合深度强化学习优化策略，能有效提升低空无线网络中物联网设备的数据收集效率和能量利用效率。

Abstract: Low-altitude wireless networks (LAWNs) have become effective solutions for
collecting data from low-power Internet-of-Things devices (IoTDs) in remote
areas with limited communication infrastructure. However, some outdoor IoTDs
deployed in such areas face both energy constraints and low-channel quality
challenges, making it challenging to ensure timely data collection from these
IoTDs in LAWNs. In this work, we investigate a reconfigurable intelligent
surface (RIS)-assisted uncrewed aerial vehicle (UAV)-enabled data collection
and wireless power transfer system in LAWN. Specifically, IoTDs first harvest
energy from a low-altitude UAV, and then upload their data to the UAV by
applying the time division multiple access (TDMA) protocol, supported by an RIS
to improve the channel quality. To maintain satisfactory data freshness of the
IoTDs and save energy for an energy-constrained UAV, we aim to minimize the age
of information (AoI) and energy consumption of the UAV by jointly optimizing
the RIS phase shits, UAV trajectory, charging time allocation, and binary IoTD
scheduling. We propose a deep reinforcement learning (DRL)-based approach,
namely the alternating optimization-improved parameterized deep Q-network
(AO-IPDQN). Specifically, considering that RIS typically contains a large
number of reflecting elements, we first adopt an alternating optimization (AO)
method to optimize the RIS phase shifts to reduce the dimension of the action
space. Then, we propose the improved parameterized deep Q-network (IPDQN)
method to deal with the hybrid action space. Simulation results indicate that
AO-IPDQN approach achieves excellent performance relative to multiple
comparison methods across various simulation scenarios.

</details>


### [49] [Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement](https://arxiv.org/abs/2509.19669)
*Yifan Wang,Minzhao Lyu,Vijay Sivaraman*

Main category: cs.NI

TL;DR: 本文提出了一种通过分析网络流量来实时测量云游戏用户体验的方法，包括游戏标题和玩家活动阶段等上下文因素。该方法能在游戏启动前5秒内识别游戏标题，并持续评估玩家活动状态。


<details>
  <summary>Details</summary>
Motivation: 随着云游戏市场的增长，网络运营商需要创建可盈利的保障服务来动态配置网络资源。但缺乏准确测量云游戏用户体验的方法，无法评估资源配置效果。基本的带宽和帧率指标不足以单独衡量体验质量。

Method: 通过分析网络流量来实时测量云游戏体验，包括识别游戏标题和玩家活动阶段（活跃、被动或空闲）。该方法能在游戏启动5秒内分类游戏标题，并持续监测玩家活动状态。

Result: 在托管NVIDIA云游戏服务器的ISP中部署该方法，分析了三个月内数十万个云游戏流会话，揭示了带宽消耗和体验水平对游戏情境的依赖性。

Conclusion: 该方法为网络运营商提供了实时测量云游戏用户体验的有效工具，能够更好地理解游戏情境对网络资源需求和用户体验的影响。

Abstract: To tap into the growing market of cloud gaming, whereby game graphics is
rendered in the cloud and streamed back to the user as a video feed, network
operators are creating monetizable assurance services that dynamically
provision network resources. However, without accurately measuring cloud gaming
user experience, they cannot assess the effectiveness of their provisioning
methods. Basic measures such as bandwidth and frame rate by themselves do not
suffice, and can only be interpreted in the context of the game played and the
player activity within the game. This paper equips the network operator with a
method to obtain a real-time measure of cloud gaming experience by analyzing
network traffic, including contextual factors such as the game title and player
activity stage. Our method is able to classify the game title within the first
five seconds of game launch, and continuously assess the player activity stage
as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud
gaming servers for the region. We provide insights from hundreds of thousands
of cloud game streaming sessions over a three-month period into the dependence
of bandwidth consumption and experience level on the gameplay contexts.

</details>


### [50] [SPARQ: An Optimization Framework for the Distribution of AI-Intensive Applications under Non-Linear Delay Constraints](https://arxiv.org/abs/2509.19913)
*Pietro Spadaccino,Paolo Di Lorenzo,Sergio Barbarossa,Antonia M. Tulino,Jaime Llorca*

Main category: cs.NI

TL;DR: 该论文提出了SPARQ算法，用于在边缘-云基础设施上对分布式AI应用进行队列延迟感知的编排，解决了传统模型无法捕捉AI密集型工作负载中延迟与资源使用非线性关系的问题。


<details>
  <summary>Details</summary>
Motivation: 下一代实时计算密集型应用（如扩展现实、多用户游戏等）包含异构AI功能，具有不同的资源需求和严格的延迟约束。现有模型无法准确捕捉AI密集型工作负载中延迟与资源使用的非线性关系。

Method: 扩展云网络流优化框架，引入两种执行模型（GR和SR），采用M/M/1和M/G/1队列动态表示资源使用。开发SPARQ迭代近似算法，将非凸优化问题分解为两个凸子问题。

Result: 仿真结果表明，SPARQ不仅更准确地表示系统延迟，而且在资源效率和成本-延迟权衡方面显著优于现有最先进方法。

Conclusion: SPARQ算法成功解决了AI应用编排中的非线性延迟约束问题，为边缘-云基础设施上的分布式AI应用提供了更高效的资源管理和延迟控制方案。

Abstract: Next-generation real-time compute-intensive applications, such as extended
reality, multi-user gaming, and autonomous transportation, are increasingly
composed of heterogeneous AI-intensive functions with diverse resource
requirements and stringent latency constraints. While recent advances have
enabled very efficient algorithms for joint service placement, routing, and
resource allocation for increasingly complex applications, current models fail
to capture the non-linear relationship between delay and resource usage that
becomes especially relevant in AI-intensive workloads. In this paper, we extend
the cloud network flow optimization framework to support queuing-delay-aware
orchestration of distributed AI applications over edge-cloud infrastructures.
We introduce two execution models, Guaranteed-Resource (GR) and Shared-Resource
(SR), that more accurately capture how computation and communication delays
emerge from system-level resource constraints. These models incorporate M/M/1
and M/G/1 queue dynamics to represent dedicated and shared resource usage,
respectively. The resulting optimization problem is non-convex due to the
non-linear delay terms. To overcome this, we develop SPARQ, an iterative
approximation algorithm that decomposes the problem into two convex
sub-problems, enabling joint optimization of service placement, routing, and
resource allocation under nonlinear delay constraints. Simulation results
demonstrate that the SPARQ not only offers a more faithful representation of
system delays, but also substantially improves resource efficiency and the
overall cost-delay tradeoff compared to existing state-of-the-art methods.

</details>


### [51] [A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network](https://arxiv.org/abs/2509.20068)
*Bilal Dalgic,Betul Sen,Muge Erel-Ozcevik*

Main category: cs.NI

TL;DR: 提出了一种基于SDN和数字孪生的新型IIoT短时异常检测框架，通过GPU加速的LightGBM模型实现高效异常检测


<details>
  <summary>Details</summary>
Motivation: 当前文献缺乏SDN数字孪生的实现细节和面向IIoT威胁的时态智能模型训练方法，需要解决IIoT环境的安全监控和动态控制需求

Method: 使用SDN数字孪生框架，结合综合数据集和时态特征标注，评估多种机器学习模型，提出SD-TWIN异常检测算法

Result: GPU加速的LightGBM模型表现最佳，实现了高召回率和强分类性能的平衡

Conclusion: SD-TWIN框架为IIoT环境提供了有效的短时异常检测解决方案，SDN数字孪生与机器学习结合具有良好应用前景

Abstract: Secure monitoring and dynamic control in an IIoT environment are major
requirements for current development goals. We believe that dynamic, secure
monitoring of the IIoT environment can be achieved through integration with the
Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current
literature lacks implementation details for SDN-based DT and time-aware
intelligent model training for short-term anomaly detection against IIoT
threats. Therefore, we have proposed a novel framework for short-term anomaly
detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware
labeling of features, and a comprehensive evaluation of various machine
learning models, we propose a novel SD-TWIN-based anomaly detection algorithm.
According to the performance of a new real-time SD-TWIN deployment, the GPU-
accelerated LightGBM model is particularly effective, achieving a balance of
high recall and strong classification performance.

</details>


### [52] [Can LLMs Forecast Internet Traffic from Social Media?](https://arxiv.org/abs/2509.20123)
*Jonatan Langlet,Mariano Scazzariello,Flavio Luciani,Marta Burocchi,Dejan Kostić,Marco Chiesa*

Main category: cs.NI

TL;DR: 该论文提出了一种利用公共讨论信号（如新闻标题、论坛和社交媒体）来预测互联网流量异常峰值的社会技术系统，以补充传统基于历史流量模式的预测方法。


<details>
  <summary>Details</summary>
Motivation: 社会事件（如名人去世、软件发布、体育赛事）会引发互联网流量激增，但现有预测系统仅依赖常规流量模式，无法捕捉这些关键异常。

Method: 开发了一个概念验证系统，自动抓取在线讨论，推断现实世界事件，进行语义聚类和丰富，并将其与主要互联网交换点的流量测量数据关联。

Result: 原型系统在抓取适量在线讨论后，成功预测了56-92%的社会驱动流量峰值。

Conclusion: 这种方法为跨域预测、调度、需求预期和社会知情决策开辟了新的研究机会。

Abstract: Societal events shape the Internet's behavior. The death of a prominent
public figure, a software launch, or a major sports match can trigger sudden
demand surges that overwhelm peering points and content delivery networks.
Although these events fall outside regular traffic patterns, forecasting
systems still rely solely on those patterns and therefore miss these critical
anomalies.
  Thus, we argue for socio-technical systems that supplement technical
measurements with an active understanding of the underlying drivers, including
how events and collective behavior shape digital demands. We propose traffic
forecasting using signals from public discourse, such as headlines, forums, and
social media, as early demand indicators.
  To validate our intuition, we present a proof-of-concept system that
autonomously scrapes online discussions, infers real-world events, clusters and
enriches them semantically, and correlates them with traffic measurements at a
major Internet Exchange Point. This prototype predicted between 56-92% of
society-driven traffic spikes after scraping a moderate amount of online
discussions.
  We believe this approach opens new research opportunities in cross-domain
forecasting, scheduling, demand anticipation, and society-informed decision
making.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning](https://arxiv.org/abs/2509.19305)
*Yifu Luo,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于扩散模型的强化学习框架WFDiffuser，通过引入小波变换和短时傅里叶变换来解决现有方法在频率域上的频率偏移问题，从而提升轨迹稳定性和决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散概率模型的离线强化学习方法主要关注时域特征而忽略了频域特征，导致频率偏移和性能下降。作者观察到纯时域方法会无意中引起频域低频分量的偏移，造成轨迹不稳定。

Method: 提出了WFDiffuser框架，集成了离散小波变换将轨迹分解为低频和高频分量，使用短时傅里叶变换提取频域特征，并通过交叉注意力机制促进跨频率交互。

Result: 在D4RL基准测试上的大量实验结果表明，WFDiffuser有效缓解了频率偏移问题，产生了更平滑、更稳定的轨迹，决策性能优于现有方法。

Conclusion: 从频域视角研究强化学习问题具有重要价值，WFDiffuser通过频域特征建模有效提升了扩散模型在离线强化学习中的性能。

Abstract: Diffusion probability models have shown significant promise in offline
reinforcement learning by directly modeling trajectory sequences. However,
existing approaches primarily focus on time-domain features while overlooking
frequency-domain features, leading to frequency shift and degraded performance
according to our observation. In this paper, we investigate the RL problem from
a new perspective of the frequency domain. We first observe that
time-domain-only approaches inadvertently introduce shifts in the low-frequency
components of the frequency domain, which results in trajectory instability and
degraded performance. To address this issue, we propose Wavelet Fourier
Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates
Discrete Wavelet Transform to decompose trajectories into low- and
high-frequency components. To further enhance diffusion modeling for each
component, WFDiffuser employs Short-Time Fourier Transform and cross attention
mechanisms to extract frequency-domain features and facilitate cross-frequency
interaction. Extensive experiment results on the D4RL benchmark demonstrate
that WFDiffuser effectively mitigates frequency shift, leading to smoother,
more stable trajectories and improved decision-making performance over existing
methods.

</details>


### [54] [Anti-Money Laundering Systems Using Deep Learning](https://arxiv.org/abs/2509.19359)
*Mashkhal Abdalwahid Sidiq,Yimamu Kirubel Wondaferew*

Main category: cs.LG

TL;DR: 本文提出使用深度学习方法和中心性算法来改进反洗钱系统，通过图卷积网络分析金融交易网络结构，以克服传统规则系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统反洗钱系统存在高误报率且无法检测复杂洗钱模式的缺陷，需要更智能的解决方案来提升检测效果。

Method: 采用深度学习技术，特别是图卷积网络模型，结合度中心性、接近中心性、介数中心性和PageRank等中心性算法来分析金融交易网络结构。

Result: 新的GCN模型实现显示出实用性和优越性，能够更好地分析连接结构数据，在金融环境中有效识别可疑活动。

Conclusion: 深度学习与中心性算法的结合为反洗钱系统提供了有前景的改进方向，能够显著提升检测复杂洗钱模式的能力。

Abstract: In this paper, we focused on using deep learning methods for detecting money
laundering in financial transaction networks, in order to demonstrate that it
can be used as a complement or instead of the more commonly used rule-based
systems and conventional Anti-Money Laundering (AML) systems. The paper
explores the pivotal role played by Anti-Money Laundering (AML) activities in
the global financial industry. It underscores the drawbacks of conventional AML
systems, which exhibit high rates of false positives and lack the
sophistication to uncover intricate money laundering schemes. To tackle these
challenges, the paper proposes an advanced AML system that capitalizes on link
analysis using deep learning techniques. At the heart of this system lies the
utilization of centrality algorithms like Degree Centrality, Closeness
Centrality, Betweenness Centrality, and PageRank. These algorithms enhance the
system's capability to identify suspicious activities by examining the
influence and interconnections within networks of financial transactions. The
significance of Anti-Money Laundering (AML) efforts within the global financial
sector is discussed in this paper. It highlights the limitations of traditional
AML systems. The results showed the practicality and superiority of the new
implementation of the GCN model, which is a preferable method for connectively
structured data, meaning that a transaction or account is analyzed in the
context of its financial environment. In addition, the paper delves into the
prospects of Anti-Money Laundering (AML) efforts, proposing the integration of
emerging technologies such as deep learning and centrality algorithms. This
integration holds promise for enhancing the effectiveness of AML systems by
refining their capabilities.

</details>


### [55] [DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models](https://arxiv.org/abs/2509.19362)
*Benedikt W. Hosp*

Main category: cs.LG

TL;DR: DeepACTIF是一种轻量级、架构感知的特征归因方法，专为时间序列模型设计，通过利用LSTM内部激活来高效估计特征重要性，显著优于SHAP、IG等传统方法。


<details>
  <summary>Details</summary>
Motivation: 标准特征归因方法（如Integrated Gradients和SHAP）计算密集，不适合实时应用，特别是在医疗、生物识别等时间序列领域需要高效可解释性解决方案。

Method: 提出DeepACTIF方法，针对LSTM网络设计逆加权聚合方案，强调跨时间步激活的稳定性和幅度，实现轻量级特征重要性估计。

Result: 在三个生物识别凝视数据集上的评估显示，DeepACTIF在严重特征缩减（前10%特征）下保持预测性能，在准确性和统计鲁棒性上显著优于SHAP、IG和DeepLIFT，计算时间和内存使用量降低数个数量级。

Conclusion: DeepACTIF是边缘设备（如移动XR头显或嵌入式健康监测器）实时可解释性的可行解决方案，平衡了计算效率和模型性能。

Abstract: Feature attribution is essential for interpreting deep learning models,
particularly in time-series domains such as healthcare, biometrics, and
human-AI interaction. However, standard attribution methods, such as Integrated
Gradients or SHAP, are computationally intensive and not well-suited for
real-time applications. We present DeepACTIF, a lightweight and
architecture-aware feature attribution method that leverages internal
activations of sequence models to estimate feature importance efficiently.
Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation
scheme that emphasises stability and magnitude of activations across time
steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF
not only preserves predictive performance under severe feature reduction (top
10% of features) but also significantly outperforms established methods,
including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical
robustness. Using Wilcoxon signed-rank tests and effect size analysis, we
demonstrate that DeepACTIF yields more informative feature rankings with
significantly lower error across all top-k conditions (10 - 40%). Our
experiments demonstrate that DeepACTIF not only reduces computation time and
memory usage by orders of magnitude but also preserves model accuracy when
using only top-ranked features. That makes DeepACTIF a viable solution for
real-time interpretability on edge devices such as mobile XR headsets or
embedded health monitors.

</details>


### [56] [Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System](https://arxiv.org/abs/2509.19363)
*Zhuqi Wang,Qinghe Zhang,Zhuopei Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种基于增强ANFIS的混合分析方法用于信用卡欺诈检测，通过多分辨率小波分解和时间注意力机制改进传统ANFIS框架，在RMSE指标上比传统方法降低了17.8%。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈对美国家庭财务状况构成严重威胁，导致家庭经济行为发生不可预测的变化，需要更有效的检测方法。

Method: 使用增强ANFIS框架，结合多分辨率小波分解模块和时间注意力机制。对历史交易数据和宏观经济指标进行离散小波变换生成局部经济冲击信号，然后输入基于Takagi-Sugeno模糊规则的深度模糊规则库。

Result: 实验结果表明，与局部神经模糊模型和传统LSTM模型相比，RMSE降低了17.8%。

Conclusion: 该方法通过集成模糊规则激活与小波基选择和时间相关性权重，有效提升了欺诈检测的准确性和长期时间依赖关系的捕捉能力。

Abstract: Credit card fraud is assuming growing proportions as a major threat to the
financial position of American household, leading to unpredictable changes in
household economic behavior. To solve this problem, in this paper, a new hybrid
analysis method is presented by using the Enhanced ANFIS. The model proposes
several advances of the conventional ANFIS framework and employs a
multi-resolution wavelet decomposition module and a temporal attention
mechanism. The model performs discrete wavelet transformations on historical
transaction data and macroeconomic indicators to generate localized economic
shock signals. The transformed features are then fed into a deep fuzzy rule
library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian
membership functions. The model proposes a temporal attention encoder that
adaptively assigns weights to multi-scale economic behavior patterns,
increasing the effectiveness of relevance assessment in the fuzzy inference
stage and enhancing the capture of long-term temporal dependencies and
anomalies caused by fraudulent activities. The proposed method differs from
classical ANFIS which has fixed input-output relations since it integrates
fuzzy rule activation with the wavelet basis selection and the temporal
correlation weights via a modular training procedure. Experimental results show
that the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and
conventional LSTM models.

</details>


### [57] [Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data](https://arxiv.org/abs/2509.19366)
*Buhe Li,Berkay Kaplan,Maksym Lazirko,Aleksandr Kogan*

Main category: cs.LG

TL;DR: 本研究比较了多种无监督异常检测方法在美国卫生与公众服务部支出数据上的效果，发现混合方法能提高异常检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统审计方法在大规模政府数据集上可能不足，需要更高效准确的异常检测技术来改进审计质量和效率。

Method: 使用HBOS、稳健PCA、MCD和KNN等无监督异常检测算法，通过数据准备、算法实现和性能评估（精确率、召回率、F1分数）来识别联邦支出模式中的异常。

Result: 结果表明，结合多种检测策略的混合方法能增强复杂财务数据中异常识别的鲁棒性和准确性。

Conclusion: 该研究为审计分析领域提供了各种异常检测模型的比较效果见解，展示了无监督学习技术在改进审计质量和效率方面的潜力，对审计师、政策制定者和研究人员具有重要价值。

Abstract: This study investigates the effectiveness of unsupervised outlier detection
methods in audit analytics, utilizing USA spending data from the U.S.
Department of Health and Human Services (DHHS) as a case example. We employ and
compare multiple outlier detection algorithms, including Histogram-based
Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum
Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify
anomalies in federal spending patterns. The research addresses the growing need
for efficient and accurate anomaly detection in large-scale governmental
datasets, where traditional auditing methods may fall short. Our methodology
involves data preparation, algorithm implementation, and performance evaluation
using precision, recall, and F1 scores. Results indicate that a hybrid
approach, combining multiple detection strategies, enhances the robustness and
accuracy of outlier identification in complex financial data. This study
contributes to the field of audit analytics by providing insights into the
comparative effectiveness of various outlier detection models and demonstrating
the potential of unsupervised learning techniques in improving audit quality
and efficiency. The findings have implications for auditors, policymakers, and
researchers seeking to leverage advanced analytics in governmental financial
oversight and risk management.

</details>


### [58] [Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution](https://arxiv.org/abs/2509.19372)
*Zuzanna Dubanowska,Maciej Żelaszczyk,Michał Brzozowski,Paolo Mandica,Michał Karpowicz*

Main category: cs.LG

TL;DR: 当前最先进的幻觉检测方法在RAGTruth数据集上的性能主要由数据中的伪相关性驱动，实际效果与监督线性探针相当，且需要大量超参数调优，无法实现分布外泛化。


<details>
  <summary>Details</summary>
Motivation: 批判性评估当前SOTA幻觉检测方法的真实效能，揭示其性能可能受到数据伪相关性的影响。

Method: 通过控制伪相关性效应，比较SOTA方法与监督线性探针的性能，并分析不同方法的超参数调优需求和分布外泛化能力。

Result: 控制伪相关性后，SOTA方法性能与监督线性探针相当，所有方法在分布外泛化上都接近随机水平。

Conclusion: 提出幻觉检测及其评估的指导原则，强调当前方法在分布外泛化方面的局限性。

Abstract: We critically assess the efficacy of the current SOTA in hallucination
detection and find that its performance on the RAGTruth dataset is largely
driven by a spurious correlation with data. Controlling for this effect,
state-of-the-art performs no better than supervised linear probes, while
requiring extensive hyperparameter tuning across datasets. Out-of-distribution
generalization is currently out of reach, with all of the analyzed methods
performing close to random. We propose a set of guidelines for hallucination
detection and its evaluation.

</details>


### [59] [Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation](https://arxiv.org/abs/2509.19375)
*Mridul Sharma,Adeetya Patel,Zaneta D' Souza,Samira Abbasgholizadeh Rahimi,Siva Reddy,Sreenath Madathil*

Main category: cs.LG

TL;DR: 该论文提出了一种基于近似贝叶斯计算（ABC）的方法，用于改进大型语言模型在不确定性表达方面的表现，特别是在临床诊断等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在表达不确定性方面存在困难，现有方法如模型logits和概率估计往往产生过度自信且校准效果差的估计，这在临床诊断等安全关键领域带来了可靠性挑战。

Method: 采用近似贝叶斯计算（ABC）方法，将LLMs视为随机模拟器，通过似然无关的贝叶斯推断来推断预测概率的后验分布。

Result: 在两个临床相关基准测试中，ABC方法相比标准基线方法将准确率提高了46.9%，Brier分数降低了74.4%，并通过预期校准误差和预测熵指标显示出更好的校准效果。

Conclusion: ABC方法能够有效改善LLMs的不确定性表达，为高风险领域的可靠部署提供了更好的解决方案。

Abstract: Despite their widespread applications, Large Language Models (LLMs) often
struggle to express uncertainty, posing a challenge for reliable deployment in
high stakes and safety critical domains like clinical diagnostics. Existing
standard baseline methods such as model logits and elicited probabilities
produce overconfident and poorly calibrated estimates. In this work, we propose
Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference,
based approach that treats LLMs as a stochastic simulator to infer posterior
distributions over predictive probabilities. We evaluate our ABC approach on
two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset
and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to
standard baselines, our approach improves accuracy by up to 46.9\%, reduces
Brier scores by 74.4\%, and enhances calibration as measured by Expected
Calibration Error (ECE) and predictive entropy.

</details>


### [60] [Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection](https://arxiv.org/abs/2509.19376)
*Matthew Grofsky*

Main category: cs.LG

TL;DR: 论文提出两种方法解决RAG系统中的时间失效问题：一种简单的新近性先验方法在新鲜度任务上达到1.00准确率，而另一种基于聚类的主题演化启发式方法表现不佳（F1分数0.08），表明趋势检测需要超越简单启发式的方法。


<details>
  <summary>Details</summary>
Motivation: 解决RAG（检索增强生成）系统中存在的时间失效问题，特别是在网络安全数据上的应用。

Method: 使用两种方法：1）简单的新近性先验方法；2）基于聚类的主题演化启发式方法。

Result: 新近性先验方法在新鲜度任务上表现优异（准确率1.00），而聚类启发式方法在趋势检测任务上表现很差（F1分数0.08）。

Conclusion: 趋势检测任务需要比简单启发式方法更复杂的方法，而新近性先验方法在处理时间相关任务时效果显著。

Abstract: We address temporal failures in RAG systems using two methods on
cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on
freshness tasks. In contrast, a clustering heuristic for topic evolution failed
(0.08 F1-score), showing trend detection requires methods beyond simple
heuristics.

</details>


### [61] [OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC](https://arxiv.org/abs/2509.19396)
*Sahil Tyagi,Andrei Cozma,Olivera Kotevska,Feiyi Wang*

Main category: cs.LG

TL;DR: OmniFed是一个模块化的联邦学习框架，通过解耦配置、编排、通信和训练逻辑，支持配置驱动的原型设计和代码级定制，提供多种拓扑结构、通信协议、训练算法以及隐私保护和压缩策略。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘计算和高性能计算中至关重要，但现有框架在异构环境部署、隐私保护和系统定制方面存在挑战。OmniFed旨在通过模块化架构解决这些问题。

Method: 采用解耦架构设计，支持配置驱动的原型开发，提供可扩展的插件系统，包括多种拓扑结构、混合通信协议、训练算法，以及差分隐私、同态加密等隐私保护机制。

Result: 通过评估多种模型和算法，OmniFed在性能指标上表现良好，能够有效统一拓扑配置、混合协议通信和可插拔模块，简化异构环境中的联邦学习部署。

Conclusion: OmniFed通过其模块化和可扩展的设计，为联邦学习在边缘和HPC环境中的部署提供了一个高效、灵活且安全的解决方案。

Abstract: Federated Learning (FL) is critical for edge and High Performance Computing
(HPC) where data is not centralized and privacy is crucial. We present OmniFed,
a modular framework designed around decoupling and clear separation of concerns
for configuration, orchestration, communication, and training logic. Its
architecture supports configuration-driven prototyping and code-level
override-what-you-need customization. We also support different topologies,
mixed communication protocols within a single deployment, and popular training
algorithms. It also offers optional privacy mechanisms including Differential
Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well
as compression strategies. These capabilities are exposed through well-defined
extension points, allowing users to customize topology and orchestration,
learning logic, and privacy/compression plugins, all while preserving the
integrity of the core system. We evaluate multiple models and algorithms to
measure various performance metrics. By unifying topology configuration,
mixed-protocol communication, and pluggable modules in one stack, OmniFed
streamlines FL deployment across heterogeneous environments. Github repository
is available at https://github.com/at-aaims/OmniFed.

</details>


### [62] [Learning from Observation: A Survey of Recent Advances](https://arxiv.org/abs/2509.19379)
*Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 本文提出了一个学习从观察（LfO）的框架，用于调查和分类现有的LfO方法，并探讨了与离线强化学习、基于模型的强化学习和分层强化学习等相关领域的联系。


<details>
  <summary>Details</summary>
Motivation: 模仿学习算法通常需要专家演示中的状态和动作信息，但在实际应用中获取专家动作可能不切实际。因此，仅使用专家状态访问信息的状态模仿学习（SOIL）或学习从观察（LfO）方法受到关注。

Method: 提出了一个LfO框架，用于调查和分类现有LfO方法，包括轨迹构建、假设和算法设计选择。

Result: 通过框架对现有LfO方法进行了系统分类，并识别了与相关领域的联系。

Conclusion: 利用该框架识别了开放问题，并提出了未来的研究方向。

Abstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent
by mimicking an expert's behavior without requiring a reward function. IL
algorithms often necessitate access to state and action information from expert
demonstrations. Although expert actions can provide detailed guidance,
requiring such action information may prove impractical for real-world
applications where expert actions are difficult to obtain. To address this
limitation, the concept of learning from observation (LfO) or state-only
imitation learning (SOIL) has recently gained attention, wherein the imitator
only has access to expert state visitation information. In this paper, we
present a framework for LfO and use it to survey and classify existing LfO
methods in terms of their trajectory construction, assumptions and algorithm's
design choices. This survey also draws connections between several related
fields like offline RL, model-based RL and hierarchical RL. Finally, we use our
framework to identify open problems and suggest future research directions.

</details>


### [63] [TensLoRA: Tensor Alternatives for Low-Rank Adaptation](https://arxiv.org/abs/2509.19391)
*Axel Marmoret,Reda Bensaid,Jonathan Lys,Vincent Gripon,François Leduc-Primeau*

Main category: cs.LG

TL;DR: TensLoRA是一个统一的框架，将LoRA更新聚合为高阶张量，建模广泛的基于张量的低秩适应方法，支持模态特定的压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法将注意力投影中的可训练低秩矩阵视为独立，缺乏系统化的张量框架来联合建模这些适应。

Method: 通过将LoRA更新聚合为高阶张量，构建统一的张量框架，支持模态特定的压缩率配置。

Result: 在视觉和语言基准测试中，张量构建直接影响性能，有时在相似参数数量下优于标准LoRA。

Conclusion: TensLoRA提供了一个系统化的张量框架，能够根据模态和任务需求定制参数预算，性能优于现有方法。

Abstract: Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers
by adding trainable low-rank matrices to attention projections. While
effective, these matrices are considered independent for each attention
projection (Query, Key, and Value) and each layer. Recent extensions have
considered joint, tensor-based adaptations, but only in limited forms and
without a systematic framework. We introduce TensLoRA, a unified framework that
aggregates LoRA updates into higher-order tensors and models a broad family of
tensor-based low-rank adaptations. Our formulation generalizes existing
tensor-based methods and enables mode-specific compression rates, allowing
parameter budgets to be tailored according to the modality and task.
Experiments on vision and language benchmarks reveal that the tensor
construction directly impacts performance, sometimes better than standard LoRA
under similar parameter counts.

</details>


### [64] [Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute](https://arxiv.org/abs/2509.20241)
*Felipe Oviedo,Fiodar Kazhamiaka,Esha Choukse,Allen Kim,Amy Luers,Melanie Nakagawa,Ricardo Bianchini,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: 本文提出了一种自下而上的方法来估算大规模LLM系统每次查询的能耗，发现前沿模型每次查询的中位能耗为0.34 Wh，而非生产环境估算可能高估4-20倍。通过模型、服务平台和硬件层面的优化，可实现8-20倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 随着AI推理规模达到数十亿查询，以及新兴推理和代理工作流增加token需求，可靠的每次查询能耗估算对于容量规划、排放核算和效率优先级排序变得越来越重要。许多公开估算不一致且高估能耗，因为它们基于有限基准测试进行外推，未能反映规模化可实现的效率增益。

Method: 引入基于token吞吐量的自下而上方法，估算大规模LLM系统的每次查询能耗。在H100节点上运行模型，考虑GPU利用率和PUE约束，对前沿规模模型（>2000亿参数）进行估算。

Result: 前沿模型每次查询的中位能耗为0.34 Wh（IQR：0.18-0.67）。非生产环境估算可能高估能耗4-20倍。当典型查询的token数量增加15倍时，中位能耗上升13倍至4.32 Wh。通过针对性效率干预，可实现8-20倍的能耗降低。

Conclusion: 在推理token需求增长的背景下，针对效率优化将带来最大的全舰队节省。数据中心可以通过效率增益来缓和能耗增长，类似于互联网和云建设期间的历史经验。部署10亿次查询的基线日能耗为0.8 GWh/天，通过效率干预可降至0.9 GWh/天，与同等规模的网络搜索能耗相当。

Abstract: As AI inference scales to billions of queries and emerging reasoning and
agentic workflows increase token demand, reliable estimates of per-query energy
use are increasingly important for capacity planning, emissions accounting, and
efficiency prioritization. Many public estimates are inconsistent and overstate
energy use, because they extrapolate from limited benchmarks and fail to
reflect efficiency gains achievable at scale. In this perspective, we introduce
a bottom-up methodology to estimate the per-query energy of large-scale LLM
systems based on token throughput. For models running on an H100 node under
realistic workloads, GPU utilization and PUE constraints, we estimate a median
energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200
billion parameters). These results are consistent with measurements using
production-scale configurations and show that non-production estimates and
assumptions can overstate energy use by 4-20x. Extending to test-time scaling
scenarios with 15x more tokens per typical query, the median energy rises 13x
to 4.32 Wh, indicating that targeting efficiency in this regime will deliver
the largest fleet-wide savings. We quantify achievable efficiency gains at the
model, serving platform, and hardware levels, finding individual median
reductions of 1.5-3.5x in energy per query, while combined advances can
plausibly deliver 8-20x reductions. To illustrate the system-level impact, we
estimate the baseline daily energy use of a deployment serving 1 billion
queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8
GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,
similar to the energy footprint of web search at that scale. This echoes how
data centers historically tempered energy growth through efficiency gains
during the internet and cloud build-up.

</details>


### [65] [TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding](https://arxiv.org/abs/2509.19406)
*Kuiye Ding,Fanda Fan,Chunyi Hou,Zheya Wang,Lei Wang,Zhengxin Yang,Jianfeng Zhan*

Main category: cs.LG

TL;DR: TimeMosaic是一个多变量时间序列预测框架，通过自适应补丁嵌入和分段解码来解决时间异质性问题，在多个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的方法采用固定长度分割，忽略了局部时间动态的异质性和预测的解码异质性，导致信息密集区域细节丢失、稳定段冗余，无法捕捉短期和长期预测的不同复杂性。

Method: TimeMosaic采用自适应补丁嵌入根据局部信息密度动态调整粒度，平衡模式重用与结构清晰度，同时保持时间连续性；引入分段解码将每个预测视界视为相关子任务，适应特定视界的难度和信息需求。

Result: 在基准数据集上的广泛评估表明，TimeMosaic相比现有方法有持续改进，在包含3210亿观测值的大规模语料库上训练后，性能可与最先进的时间序列预测模型相媲美。

Conclusion: TimeMosaic通过解决时间异质性问题，在多变量时间序列预测任务中取得了显著改进，证明了自适应粒度调整和分段解码策略的有效性。

Abstract: Multivariate time series forecasting is essential in domains such as finance,
transportation, climate, and energy. However, existing patch-based methods
typically adopt fixed-length segmentation, overlooking the heterogeneity of
local temporal dynamics and the decoding heterogeneity of forecasting. Such
designs lose details in information-dense regions, introduce redundancy in
stable segments, and fail to capture the distinct complexities of short-term
and long-term horizons. We propose TimeMosaic, a forecasting framework that
aims to address temporal heterogeneity. TimeMosaic employs adaptive patch
embedding to dynamically adjust granularity according to local information
density, balancing motif reuse with structural clarity while preserving
temporal continuity. In addition, it introduces segment-wise decoding that
treats each prediction horizon as a related subtask and adapts to
horizon-specific difficulty and information requirements, rather than applying
a single uniform decoder. Extensive evaluations on benchmark datasets
demonstrate that TimeMosaic delivers consistent improvements over existing
methods, and our model trained on the large-scale corpus with 321 billion
observations achieves performance competitive with state-of-the-art TSFMs.

</details>


### [66] [Enhancing Credit Default Prediction Using Boruta Feature Selection and DBSCAN Algorithm with Different Resampling Techniques](https://arxiv.org/abs/2509.19408)
*Obu-Amoah Ampomah,Edmund Agyemang,Kofi Acheampong,Louis Agyekum*

Main category: cs.LG

TL;DR: 本研究比较了三种处理信用违约数据类别不平衡问题的技术（SMOTE、SMOTE-Tomek、ADASYN），结合多种机器学习模型和特征选择方法，发现Boruta+DBSCAN+SMOTE-Tomek+GBM组合在信用违约预测中表现最优。


<details>
  <summary>Details</summary>
Motivation: 信用违约数据集通常存在严重的类别不平衡问题（违约者比例远低于非违约者），这会影响机器学习模型的预测性能。研究旨在找到最有效的技术组合来解决这一问题。

Method: 使用真实信用违约数据集，首先在不进行重采样的情况下建立基线模型，然后比较三种重采样技术（SMOTE、SMOTE-Tomek、ADASYN）与多种分类器（Naive Bayes、KNN、XGBoost、AdaBoost、GBM、Light GBM）的组合效果，结合Boruta特征选择和DBSCAN异常检测。

Result: Boruta+DBSCAN+SMOTE-Tomek+GBM组合取得了最佳性能：F1-score 82.56%、G-mean 82.98%、ROC-AUC 90.90%、PR-AUC 91.85%，显著优于其他模型组合。

Conclusion: 该研究为构建更稳健的信用违约预测系统奠定了基础，证明结合特征选择、异常检测和合适的重采样技术可以显著提升不平衡数据集的预测性能。

Abstract: This study examines credit default prediction by comparing three techniques,
namely SMOTE, SMOTE-Tomek, and ADASYN, that are commonly used to address the
class imbalance problem in credit default situations. Recognizing that credit
default datasets are typically skewed, with defaulters comprising a much
smaller proportion than non-defaulters, we began our analysis by evaluating
machine learning (ML) models on the imbalanced data without any resampling to
establish baseline performance. These baseline results provide a reference
point for understanding the impact of subsequent balancing methods. In addition
to traditional classifiers such as Naive Bayes and K-Nearest Neighbors (KNN),
our study also explores the suitability of advanced ensemble boosting
algorithms, including Extreme Gradient Boosting (XGBoost), AdaBoost, Gradient
Boosting Machines (GBM), and Light GBM for credit default prediction using
Boruta feature selection and DBSCAN-based outlier detection, both before and
after resampling. A real-world credit default data set sourced from the
University of Cleveland ML Repository was used to build ML classifiers, and
their performances were tested. The criteria chosen to measure model
performance are the area under the receiver operating characteristic curve
(ROC-AUC), area under the precision-recall curve (PR-AUC), G-mean, and
F1-scores. The results from this empirical study indicate that the
Boruta+DBSCAN+SMOTE-Tomek+GBM classifier outperformed the other ML models
(F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%) in a credit
default context. The findings establish a foundation for future progress in
creating more resilient and adaptive credit default systems, which will be
essential as credit-based transactions continue to rise worldwide.

</details>


### [67] [Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2509.19417)
*Andreas Lebedev,Abhinav Das,Sven Pappert,Stephan Schlüter*

Main category: cs.LG

TL;DR: 该研究比较了统计和深度学习模型在电力价格概率预测中的不确定性量化方法，发现LEAR模型表现良好，DDNN模型通过结合数据和模型不确定性得到改善，而保形预测方法能最好地捕捉不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的概率预测模型未能充分捕捉数据、模型和分布选择带来的全面不确定性，需要系统评估不同不确定性量化方法在电力价格预测中的效果。

Method: 使用深度分布神经网络（DDNNs）结合集成方法、蒙特卡洛dropout和保形预测来处理模型不确定性；同时考虑LASSO估计的自回归（LEAR）方法结合分位数回归平均（QRA）、GARCH和保形预测。

Result: LEAR模型在概率预测方面表现良好，DDNNs通过结合数据和模型不确定性改善了预测性能，保形预测方法能最好地捕捉不确定性。

Conclusion: 所有模型都表现出竞争力，但相对性能取决于点预测和概率预测的度量标准选择。

Abstract: Precise probabilistic forecasts are fundamental for energy risk management,
and there is a wide range of both statistical and machine learning models for
this purpose. Inherent to these probabilistic models is some form of
uncertainty quantification. However, most models do not capture the full extent
of uncertainty, which arises not only from the data itself but also from model
and distributional choices. In this study, we examine uncertainty
quantification in state-of-the-art statistical and deep learning probabilistic
forecasting models for electricity price forecasting in the German market. In
particular, we consider deep distributional neural networks (DDNNs) and augment
them with an ensemble approach, Monte Carlo (MC) dropout, and conformal
prediction to account for model uncertainty. Additionally, we consider the
LASSO-estimated autoregressive (LEAR) approach combined with quantile
regression averaging (QRA), generalized autoregressive conditional
heteroskedasticity (GARCH), and conformal prediction. Across a range of
performance metrics, we find that the LEAR-based models perform well in terms
of probabilistic forecasting, irrespective of the uncertainty quantification
method. Furthermore, we find that DDNNs benefit from incorporating both data
and model uncertainty, improving both point and probabilistic forecasting.
Uncertainty itself appears to be best captured by the models using conformal
prediction. Overall, our extensive study shows that all models under
consideration perform competitively. However, their relative performance
depends on the choice of metrics for point and probabilistic forecasting.

</details>


### [68] [Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems](https://arxiv.org/abs/2509.19419)
*Birk Torpmann-Hagen,Pål Halvorsen,Michael A. Riegler,Dag Johansen*

Main category: cs.LG

TL;DR: 该论文提出了一种新的深度学习方法验证、评估和风险评估框架，通过建模分布偏移概率和网络正确性条件概率，构建二叉树结构来提供更准确的性能估计和风险评估。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在基准测试中表现优异，但在实际部署中由于对输入数据的微小分布偏移敏感而表现不佳，而传统评估方法很少考虑这些偏移，导致性能指标被高估。

Method: 通过估计分布外检测器的输出概率来建模运行时分布偏移的发生概率，结合网络正确性的条件概率构建二叉树结构，通过遍历该树计算可信和精确的网络准确率估计。

Result: 在五个数据集上的评估显示，该方法显著优于传统评估方法，准确率估计误差通常在0.01到0.1之间。在医学分割基准测试中，通过将成本与树节点关联，展示了风险评估的潜力。

Conclusion: 该方法为深度学习的可靠性和可信度提供了稳健框架，特别是在安全关键应用中，通过提供更准确的性能估计和可操作的风险评估来改进系统部署。

Abstract: Despite achieving excellent performance on benchmarks, deep neural networks
often underperform in real-world deployment due to sensitivity to minor, often
imperceptible shifts in input data, known as distributional shifts. These
shifts are common in practical scenarios but are rarely accounted for during
evaluation, leading to inflated performance metrics. To address this gap, we
propose a novel methodology for the verification, evaluation, and risk
assessment of deep learning systems. Our approach explicitly models the
incidence of distributional shifts at runtime by estimating their probability
from outputs of out-of-distribution detectors. We combine these estimates with
conditional probabilities of network correctness, structuring them in a binary
tree. By traversing this tree, we can compute credible and precise estimates of
network accuracy. We assess our approach on five different datasets, with which
we simulate deployment conditions characterized by differing frequencies of
distributional shift. Our approach consistently outperforms conventional
evaluation, with accuracy estimation errors typically ranging between 0.01 and
0.1. We further showcase the potential of our approach on a medical
segmentation benchmark, wherein we apply our methods towards risk assessment by
associating costs with tree nodes, informing cost-benefit analyses and
value-judgments. Ultimately, our approach offers a robust framework for
improving the reliability and trustworthiness of deep learning systems,
particularly in safety-critical applications, by providing more accurate
performance estimates and actionable risk assessments.

</details>


### [69] [A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models](https://arxiv.org/abs/2509.19465)
*Kin G. Olivares,Malcolm Wolff,Tatiana Konstantinova,Shankar Ramasubramanian,Andrew Gordon Wilson,Andres Potapczynski,Willa Potosnak,Mengfei Cao,Boris Oreshkin,Dmitry Efimov*

Main category: cs.LG

TL;DR: 本文分析了跨频迁移学习(CFTL)在时间序列预测中的基准测试问题，发现当前评估方法存在多个缺陷，并通过大规模实证研究证明传统统计模型仍优于现有基础预测模型(FFMs)。


<details>
  <summary>Details</summary>
Motivation: 当前CFTL基准测试存在严重问题：过度依赖小规模评估数据集、样本量处理不当、报告次优统计模型、以及未能考虑预训练与测试数据集重叠的风险。

Method: 重新实现广泛采用的神经预测网络并适配CFTL设置；仅在专有和合成数据上进行预训练，避免测试泄漏；在15个大型多样的公共预测竞赛数据集上进行评估。

Result: 统计模型的准确性经常被低估。统计模型及其集成在sCRPS上比现有FFMs高出8.2%以上，在MASE上高出20%以上。但合成数据集预训练确实能将FFM准确性提高7%。

Conclusion: 虽然CFTL框架有潜力，但当前FFMs在预测准确性上仍不如传统统计方法，需要更严谨的基准测试和评估方法。

Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework
for curating large-scale time series datasets to pre-train foundation
forecasting models (FFMs). Although CFTL has shown promise, current
benchmarking practices fall short of accurately assessing its performance. This
shortcoming stems from many factors: an over-reliance on small-scale evaluation
datasets; inadequate treatment of sample size when computing summary
statistics; reporting of suboptimal statistical models; and failing to account
for non-negligible risks of overlap between pre-training and test datasets. To
address these limitations, we introduce a unified reimplementation of
widely-adopted neural forecasting networks, adapting them for the CFTL setup;
we pre-train only on proprietary and synthetic data, being careful to prevent
test leakage; and we evaluate on 15 large, diverse public forecast competition
datasets. Our empirical analysis reveals that statistical models' accuracy is
frequently underreported. Notably, we confirm that statistical models and their
ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and
by more than 20% MASE, across datasets. However, we also find that synthetic
dataset pre-training does improve the accuracy of a FFM by 7% percent.

</details>


### [70] [THINNs: Thermodynamically Informed Neural Networks](https://arxiv.org/abs/2509.19467)
*Javier Castro,Benjamin Gess*

Main category: cs.LG

TL;DR: 提出了THINNs方法，这是一种基于热力学一致性的PINNs扩展，通过物理信息化的惩罚项选择来近似非平衡波动系统的PDE解。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs中的惩罚项选择往往是启发式的，缺乏物理基础。针对非平衡波动系统，需要一种与底层波动结构一致的惩罚策略。

Method: 基于大偏差原理，提出了一种物理信息化的惩罚项选择方法，惩罚的是概率较小的偏差而非任意偏差，从而构建了热力学一致的THINNs框架。

Result: 建立了分析性的后验估计，并通过经验比较验证了THINNs相对于传统惩罚策略的优越性。

Conclusion: THINNs提供了一种物理上更合理的PINNs扩展，为处理非平衡波动系统提供了新的有效工具。

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
aiming to approximate solutions of PDEs by training neural networks to minimize
the residual of the equation. Focusing on non-equilibrium fluctuating systems,
we propose a physically informed choice of penalization that is consistent with
the underlying fluctuation structure, as characterized by a large deviations
principle. This approach yields a novel formulation of PINNs in which the
penalty term is chosen to penalize improbable deviations, rather than being
selected heuristically. The resulting thermodynamically consistent extension of
PINNs, termed THINNs, is subsequently analyzed by establishing analytical a
posteriori estimates, and providing empirical comparisons to established
penalization strategies.

</details>


### [71] [Transformer Modeling for Both Scalability and Performance in Multivariate Time Series](https://arxiv.org/abs/2509.19471)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: DELTAformer通过引入委托令牌注意力机制，在多变量时间序列中实现线性扩展并提升性能，解决了传统transformer在变量数量增加时的可扩展性瓶颈和噪声积累问题。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中变量数量增加导致transformer模型可扩展性瓶颈，同时无差别的变量间混合会造成噪声积累和性能下降，特别是在信号稀疏和变量异质性的场景下。

Method: 提出DELTAformer方法，使用委托令牌约束变量间建模，委托令牌作为隐式正则化器，选择性传播变量间信息，同时进行完整的时序建模。

Result: DELTAformer在变量数量上实现线性扩展，性能超越标准transformer，在多个基准测试中达到最先进水平，在噪声环境中表现更优。

Conclusion: 通过针对多变量时间序列的领域特定挑战进行模型设计，DELTAformer能够同时实现线性扩展和性能提升，优于传统的二次复杂度transformer。

Abstract: Variable count is among the main scalability bottlenecks for transformer
modeling in multivariate time series (MTS) data. On top of this, a growing
consensus in the field points to indiscriminate inter-variable mixing as a
potential source of noise-accumulation and performance degradation. This is
likely exacerbated by sparsity of informative signals characteristic of many
MTS systems coupled with representational misalignment stemming from
indiscriminate information mixing between (heterogeneous) variables. While
scalability and performance are often seen as competing interests in
transformer design, we show that both can be improved simultaneously in MTS by
strategically constraining the representational capacity of inter-variable
mixing. Our proposed method, transformer with Delegate Token Attention
(DELTAformer), constrains inter-variable modeling through what we call delegate
tokens which are then used to perform full, unconstrained, inter-temporal
modeling. Delegate tokens act as an implicit regularizer that forces the model
to be highly selective about what inter-variable information is allowed to
propagate through the network. Our results show that DELTAformer scales
linearly with variable-count while actually outperforming standard
transformers, achieving state-of-the-art performance across benchmarks and
baselines. In addition, DELTAformer can focus on relevant signals better than
standard transformers in noisy MTS environments and overall exhibit superior
noise-resilience. Overall, results across various experiments confirm that by
aligning our model design to leverage domain-specific challenges in MTS to our
advantage, DELTAformer can simultaneously achieve linear scaling while actually
improving its performance against standard, quadratic transformers.

</details>


### [72] [Constraint-Reduced MILP with Local Outlier Factor Modeling for Plausible Counterfactual Explanations in Credit Approval](https://arxiv.org/abs/2509.19504)
*Trung Nguyen Thanh,Huyen Giang Thi Thu,Tai Le Quy,Ha-Bang Ban*

Main category: cs.LG

TL;DR: 本文提出了一个改进的混合整数线性规划（MILP）公式，显著减少了反事实解释中局部离群因子（LOF）目标组件的约束数量，从而提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的合理反事实解释方法虽然通过考虑数据分布特征提高了现实性，但其优化模型引入了大量约束，导致计算成本高昂。

Method: 重新审视DACE框架，提出精炼的MILP公式，减少LOF目标组件的约束数量，并将该方法应用于带有标准缩放器的线性SVM分类器。

Result: 实验结果显示，该方法在保持解释质量的同时实现了更快的求解时间。

Conclusion: 这些结果表明在反事实解释和数据科学应用中更高效的LOF建模具有广阔前景。

Abstract: Counterfactual explanation (CE) is a widely used post-hoc method that
provides individuals with actionable changes to alter an unfavorable prediction
from a machine learning model. Plausible CE methods improve realism by
considering data distribution characteristics, but their optimization models
introduce a large number of constraints, leading to high computational cost. In
this work, we revisit the DACE framework and propose a refined Mixed-Integer
Linear Programming (MILP) formulation that significantly reduces the number of
constraints in the local outlier factor (LOF) objective component. We also
apply the method to a linear SVM classifier with standard scaler. The
experimental results show that our approach achieves faster solving times while
maintaining explanation quality. These results demonstrate the promise of more
efficient LOF modeling in counterfactual explanation and data science
applications.

</details>


### [73] [Frame-based Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2509.19506)
*Mohan Guo,Cong Liu,Patrick Forré*

Main category: cs.LG

TL;DR: 本文提出了一种基于框架的扩散范式，通过三种变体（GFD、LFD、IFD）实现确定性E(3)-等变性，在QM9数据集上取得了最先进性能，同时保持了高采样效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有分子生成方法在严格等变性与可扩展性之间的权衡问题，提出一种既能保持物理对称性又具有灵活性的新范式。

Method: 基于框架的扩散范式，包括全局框架扩散（GFD）、局部框架扩散（LFD）和不变框架扩散（IFD）三种变体，结合EdgeDiT（具有边缘感知注意力的扩散Transformer）增强表达能力。

Result: 在QM9数据集上，GFD与EdgeDiT结合实现了-137.97（标准尺度）和-141.85（双尺度）的测试NLL，原子稳定性98.98%，分子稳定性90.51%，采样速度比EDM快近2倍。

Conclusion: 基于框架的扩散为分子生成提供了一个可扩展、灵活且物理基础扎实的范式，强调了全局结构保持的关键作用。

Abstract: Recent methods for molecular generation face a trade-off: they either enforce
strict equivariance with costly architectures or relax it to gain scalability
and flexibility. We propose a frame-based diffusion paradigm that achieves
deterministic E(3)-equivariance while decoupling symmetry handling from the
backbone. Building on this paradigm, we investigate three variants: Global
Frame Diffusion (GFD), which assigns a shared molecular frame; Local Frame
Diffusion (LFD), which constructs node-specific frames and benefits from
additional alignment constraints; and Invariant Frame Diffusion (IFD), which
relies on pre-canonicalized invariant representations. To enhance expressivity,
we further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention.
  On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance,
with a test NLL of -137.97 at standard scale and -141.85 at double scale,
alongside atom stability of 98.98%, and molecular stability of 90.51%. These
results surpass all equivariant baselines while maintaining high validity and
uniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study
establishes frame-based diffusion as a scalable, flexible, and physically
grounded paradigm for molecular generation, highlighting the critical role of
global structure preservation.

</details>


### [74] [Metriplectic Conditional Flow Matching for Dissipative Dynamics](https://arxiv.org/abs/2509.19526)
*Ali Baheri,Lars Lindemann*

Main category: cs.LG

TL;DR: MCFM是一种学习耗散动力学的方法，通过构建保守-耗散分裂的向量场和结构保持采样器，避免违反第一原理，确保长期滚动的稳定性。


<details>
  <summary>Details</summary>
Motivation: 神经代理模型在长期滚动中常常注入能量导致不稳定，MCFM旨在学习耗散动力学而不违反物理第一原理。

Method: 使用条件流匹配在短转换上进行训练，避免长滚动伴随；推理时采用Strang-prox方案交替进行辛更新和近端度量步骤，确保离散能量衰减。

Result: 在受控机械基准测试中，MCFM产生的相图更接近真实情况，能量增加和正能量率事件显著少于同等表达能力的无约束神经流，同时匹配终端分布拟合。

Conclusion: MCFM通过结构保持的方法实现了稳定的长期滚动，在保持物理原理的同时提高了模型性能。

Abstract: Metriplectic conditional flow matching (MCFM) learns dissipative dynamics
without violating first principles. Neural surrogates often inject energy and
destabilize long-horizon rollouts; MCFM instead builds the
conservative-dissipative split into both the vector field and a structure
preserving sampler. MCFM trains via conditional flow matching on short
transitions, avoiding long rollout adjoints. In inference, a Strang-prox scheme
alternates a symplectic update with a proximal metric step, ensuring discrete
energy decay; an optional projection enforces strict decay when a trusted
energy is available. We provide continuous and discrete time guarantees linking
this parameterization and sampler to conservation, monotonic dissipation, and
stable rollouts. On a controlled mechanical benchmark, MCFM yields phase
portraits closer to ground truth and markedly fewer energy-increase and
positive energy rate events than an equally expressive unconstrained neural
flow, while matching terminal distributional fit.

</details>


### [75] [DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions](https://arxiv.org/abs/2509.19538)
*Zongyue Li,Xiao Han,Yusong Li,Niklas Strauss,Matthias Schubert*

Main category: cs.LG

TL;DR: DAWM是一种基于扩散的世界模型，通过生成状态-奖励轨迹并结合逆动力学模型来推断动作，为离线强化学习提供完整的合成转换数据。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散世界模型通常不直接生成动作，限制了与基于一步时序差分学习的标准离线RL算法的兼容性。而联合建模状态、奖励和动作的方法在实践中往往导致训练复杂度增加和性能下降。

Method: 提出DAWM模型，生成基于当前状态、动作和回报的未来状态-奖励轨迹，同时使用逆动力学模型进行高效的动作推断。这种模块化设计产生适合一步TD学习的完整合成转换。

Result: 实验表明，TD3BC和IQL等保守离线RL算法在使用这些增强轨迹训练时表现显著提升，在D4RL基准测试的多个任务中持续优于先前的扩散基线方法。

Conclusion: DAWM通过模块化设计有效解决了扩散世界模型与标准离线RL算法的兼容性问题，提供了计算高效的训练方法。

Abstract: Diffusion-based world models have demonstrated strong capabilities in
synthesizing realistic long-horizon trajectories for offline reinforcement
learning (RL). However, many existing methods do not directly generate actions
alongside states and rewards, limiting their compatibility with standard
value-based offline RL algorithms that rely on one-step temporal difference
(TD) learning. While prior work has explored joint modeling of states, rewards,
and actions to address this issue, such formulations often lead to increased
training complexity and reduced performance in practice. We propose
\textbf{DAWM}, a diffusion-based world model that generates future state-reward
trajectories conditioned on the current state, action, and return-to-go, paired
with an inverse dynamics model (IDM) for efficient action inference. This
modular design produces complete synthetic transitions suitable for one-step
TD-based offline RL, enabling effective and computationally efficient training.
Empirically, we show that conservative offline RL algorithms such as TD3BC and
IQL benefit significantly from training on these augmented trajectories,
consistently outperforming prior diffusion-based baselines across multiple
tasks in the D4RL benchmark.

</details>


### [76] [Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks](https://arxiv.org/abs/2509.19554)
*Yi Ren*

Main category: cs.LG

TL;DR: 该论文提出了一种基于力分析启发的深度学习模型学习过程分析框架，将训练样本间的相互影响分解为相似性和更新强度两个部分，用于解释模型在不同实际系统中的行为。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型如何随时间学习，特别是训练过程中一个训练样本如何影响另一个样本的学习过程，类似于分析力如何移动物体。

Method: 采用力分析启发的框架，将训练样本间的相互影响分解为相似性和更新强度两个维度，并将该方法应用于各种学习任务中。

Result: 该框架成功解释了为什么某些样本具有非平凡的学习路径、为什么某些LLM微调方法有效（或无效）、以及为什么更简单、结构化的模式更容易被学习。同时发现了改进模型训练的新策略。

Conclusion: 虽然该方法仍在发展中，但为系统解释模型行为提供了一种新的分析视角，有助于深入理解深度学习模型的学习机制。

Abstract: This thesis explores how deep learning models learn over time, using ideas
inspired by force analysis. Specifically, we zoom in on the model's training
procedure to see how one training example affects another during learning, like
analyzing how forces move objects. We break this influence into two parts: how
similar the two examples are, and how strong the updating force is. This
framework helps us understand a wide range of the model's behaviors in
different real systems. For example, it explains why certain examples have
non-trivial learning paths, why (and why not) some LLM finetuning methods work,
and why simpler, more structured patterns tend to be learned more easily. We
apply this approach to various learning tasks and uncover new strategies for
improving model training. While the method is still developing, it offers a new
way to interpret models' behaviors systematically.

</details>


### [77] [A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery](https://arxiv.org/abs/2509.19586)
*Alexander Ho,Sukyeong Lee,Francis T. F. Tsai*

Main category: cs.LG

TL;DR: FragAtlas-62M是一个专门的基础模型，基于最大的片段数据集训练，在ZINC-22片段子集上训练，包含6200万个分子，实现了前所未有的片段化学空间覆盖。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够生成化学有效片段并覆盖广泛化学空间的专门基础模型，以加速药物发现和化学研究。

Method: 使用GPT-2架构（4270万参数）在ZINC-22片段子集上训练，包含6200万个分子，验证了12个描述符和三种指纹方法。

Result: 模型生成99.90%化学有效片段，生成片段与训练分布高度匹配（所有效应大小<0.4），保留了53.6%已知ZINC片段，同时产生22%具有实际相关性的新结构。

Conclusion: FragAtlas-62M是一个有效的片段生成模型，发布了训练代码、预处理数据、文档和模型权重以促进采用。

Abstract: We introduce FragAtlas-62M, a specialized foundation model trained on the
largest fragment dataset to date. Built on the complete ZINC-22 fragment subset
comprising over 62 million molecules, it achieves unprecedented coverage of
fragment chemical space. Our GPT-2 based model (42.7M parameters) generates
99.90% chemically valid fragments. Validation across 12 descriptors and three
fingerprint methods shows generated fragments closely match the training
distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC
fragments while producing 22% novel structures with practical relevance. We
release FragAtlas-62M with training code, preprocessed data, documentation, and
model weights to accelerate adoption.

</details>


### [78] [Modular Machine Learning with Applications to Genetic Circuit Composition](https://arxiv.org/abs/2509.19601)
*Jichi Wang,Eduardo D. Sontag,Domitilla Del Vecchio*

Main category: cs.LG

TL;DR: 提出了一种模块化学习框架，利用系统的组合结构先验知识来识别模块的输入/输出函数，并减少所需训练数据量。


<details>
  <summary>Details</summary>
Motivation: 在合成生物学等应用中，虽然模块的输入/输出函数和信号未知，但系统的组合架构知识可以显著减少学习系统输入/输出映射所需的训练数据量。

Method: 引入模块可识别性概念，允许从系统输入/输出数据的子集中恢复模块的输入/输出函数，并提供理论保证。使用考虑组合结构的神经网络来学习模块函数。

Result: 计算研究表明，考虑组合结构的神经网络能够学习模块的输入/输出函数，并在训练集分布之外的输入上预测系统输出；而忽略结构的神经网络则无法做到这一点。

Conclusion: 该框架通过减少实验数据需求和允许模块识别，为合成生物电路和一般多模块系统的设计提供了便利。

Abstract: In several applications, including in synthetic biology, one often has
input/output data on a system composed of many modules, and although the
modules' input/output functions and signals may be unknown, knowledge of the
composition architecture can significantly reduce the amount of training data
required to learn the system's input/output mapping. Learning the modules'
input/output functions is also necessary for designing new systems from
different composition architectures. Here, we propose a modular learning
framework, which incorporates prior knowledge of the system's compositional
structure to (a) identify the composing modules' input/output functions from
the system's input/output data and (b) achieve this by using a reduced amount
of data compared to what would be required without knowledge of the
compositional structure. To achieve this, we introduce the notion of modular
identifiability, which allows recovery of modules' input/output functions from
a subset of the system's input/output data, and provide theoretical guarantees
on a class of systems motivated by genetic circuits. We demonstrate the theory
on computational studies showing that a neural network (NNET) that accounts for
the compositional structure can learn the composing modules' input/output
functions and predict the system's output on inputs outside of the training set
distribution. By contrast, a neural network that is agnostic of the structure
is unable to predict on inputs that fall outside of the training set
distribution. By reducing the need for experimental data and allowing module
identification, this framework offers the potential to ease the design of
synthetic biological circuits and of multi-module systems more generally.

</details>


### [79] [Improved Therapeutic Antibody Reformatting through Multimodal Machine Learning](https://arxiv.org/abs/2509.19604)
*Jiayi Xin,Aniruddh Raghu,Nick Bhattacharya,Adam Carr,Melanie Montgomery,Hunter Elliott*

Main category: cs.LG

TL;DR: 开发机器学习框架预测抗体重新格式化成功率，结合序列和结构信息，在真实数据集上验证效果优于大型预训练蛋白质语言模型


<details>
  <summary>Details</summary>
Motivation: 现代抗体设计需要组合多个功能域，但单个域的功能和稳定性在新格式中无法保证，整个分子可能无法合成，需要预测重新格式化的成功率

Method: 开发结合抗体序列和结构背景的机器学习框架，采用反映实际部署场景的评估协议，使用多模态表示方法

Result: 在真实抗体重新格式化数据集上，大型预训练蛋白质语言模型表现不如简单的领域定制多模态表示，特别是在最具挑战性的'新抗体、无数据'场景中

Conclusion: 多模态模型在预测抗体重新格式化成功率方面表现优异，能够优先选择有希望的候选物，减少实验浪费

Abstract: Modern therapeutic antibody design often involves composing multi-part
assemblages of individual functional domains, each of which may be derived from
a different source or engineered independently. While these complex formats can
expand disease applicability and improve safety, they present a significant
engineering challenge: the function and stability of individual domains are not
guaranteed in the novel format, and the entire molecule may no longer be
synthesizable. To address these challenges, we develop a machine learning
framework to predict "reformatting success" -- whether converting an antibody
from one format to another will succeed or not. Our framework incorporates both
antibody sequence and structural context, incorporating an evaluation protocol
that reflects realistic deployment scenarios. In experiments on a real-world
antibody reformatting dataset, we find the surprising result that large
pretrained protein language models (PLMs) fail to outperform simple,
domain-tailored, multimodal representations. This is particularly evident in
the most difficult evaluation setting, where we test model generalization to a
new starting antibody. In this challenging "new antibody, no data" scenario,
our best multimodal model achieves high predictive accuracy, enabling
prioritization of promising candidates and reducing wasted experimental effort.

</details>


### [80] [Adaptive von Mises-Fisher Likelihood Loss for Supervised Deep Time Series Hashing](https://arxiv.org/abs/2509.19625)
*Juan Manuel Perez,Kevin Garcia,Brooklyn Berry,Dongjin Song,Yifeng Gao*

Main category: cs.LG

TL;DR: 本文提出了一种基于von Mises-Fisher分布的深度哈希方法，用于时间序列索引，通过将数据映射到高维球面空间来减少信息损失，并优化类间分离度。


<details>
  <summary>Details</summary>
Motivation: 传统深度哈希方法在将实值表示转换为二进制码时会导致显著信息损失，需要一种更有效的方法来保持语义信息。

Method: 使用von Mises-Fisher分布建模每个数据类，将数据映射到M维超球面空间，设计损失函数最大化不同vMF分布之间的分离度。

Result: 实验结果表明该方法优于现有基线方法。

Conclusion: 提出的vMF哈希损失能够有效减少信息损失，提高时间序列索引的性能。

Abstract: Indexing time series by creating compact binary representations is a
fundamental task in time series data mining. Recently, deep learning-based
hashing methods have proven effective for indexing time series based on
semantic meaning rather than just raw similarity. The purpose of deep hashing
is to map samples with the same semantic meaning to identical binary hash
codes, enabling more efficient search and retrieval. Unlike other supervised
representation learning methods, supervised deep hashing requires a
discretization step to convert real-valued representations into binary codes,
but this can induce significant information loss. In this paper, we propose a
von Mises-Fisher (vMF) hashing loss. The proposed deep hashing model maps data
to an M-dimensional hyperspherical space to effectively reduce information loss
and models each data class as points following distinct vMF distributions. The
designed loss aims to maximize the separation between each modeled vMF
distribution to provide a better way to maximize the margin between each
semantically different data sample. Experimental results show that our method
outperforms existing baselines. The implementation is publicly available at
https://github.com/jmpq97/vmf-hashing

</details>


### [81] [Mamba Modulation: On the Length Generalization of Mamba](https://arxiv.org/abs/2509.19633)
*Peng Lu,Jerry Huang,Qiuhao Zeng,Xinyu Wang,Boxing Wang,Philippe Langlais,Yufei Cui*

Main category: cs.LG

TL;DR: Mamba模型在处理超出预训练长度上下文时性能显著下降，本文通过分析状态转移矩阵的频谱特性，提出频谱缩放方法来解决长度泛化问题。


<details>
  <summary>Details</summary>
Motivation: Mamba作为替代Transformer的状态空间模型，在语言建模任务中表现出色，但在处理长于预训练长度的上下文时性能急剧下降，这限制了其实际应用。

Method: 通过分析状态转移矩阵A的频谱特性，发现状态收敛行为与长度扩展的关系，提出对预训练Mamba模型应用频谱缩放的方法，选择性调制各层A矩阵的频谱。

Result: 频谱缩放方法能显著改善Mamba在长上下文场景下的性能，特别是在仅调制Δt参数失败的情况下仍能有效工作。

Conclusion: 本文为具有结构化转移矩阵的状态空间模型提供了更好的长度泛化途径，通过频谱分析揭示了Mamba长度敏感性的根本原因。

Abstract: The quadratic complexity of the attention mechanism in Transformer models has
motivated the development of alternative architectures with sub-quadratic
scaling, such as state-space models. Among these, Mamba has emerged as a
leading architecture, achieving state-of-the-art results across a range of
language modeling tasks. However, Mamba's performance significantly
deteriorates when applied to contexts longer than those seen during
pre-training, revealing a sharp sensitivity to context length extension.
Through detailed analysis, we attribute this limitation to the
out-of-distribution behaviour of its state-space dynamics, particularly within
the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent
works which attribute this sensitivity to the vanished accumulation of
discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a
connection between state convergence behavior as the input length approaches
infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a
well-founded explanation of its role in length extension. Next, to overcome
this challenge, we propose an approach that applies spectrum scaling to
pre-trained Mamba models to enable robust long-context generalization by
selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We
show that this can significantly improve performance in settings where simply
modulating $\Delta_t$ fails, validating our insights and providing avenues for
better length generalization of state-space models with structured transition
matrices.

</details>


### [82] [TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation](https://arxiv.org/abs/2509.19638)
*MohammadReza EskandariNasab,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: TIMED是一个统一的生成框架，用于合成高质量的时间序列数据，通过结合扩散模型、监督网络和对抗训练来建模时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 时间序列合成在预测和异常检测等领域至关重要，但真实数据往往稀缺、嘈杂或收集成本高。与静态数据生成不同，时间序列需要同时建模观测值的边际分布和支配序列动态的条件时间依赖性。

Method: TIMED框架集成了去噪扩散概率模型（DDPM）捕捉全局结构，监督网络通过教师强制学习自回归依赖关系，Wasserstein批评器提供对抗反馈确保时间平滑性和保真度，并加入最大均值差异（MMD）损失来对齐特征空间中的真实和合成分布。

Result: 在多个多元时间序列基准测试中，TIMED生成的序列比最先进的生成模型更加真实且时间上更连贯。

Conclusion: TIMED通过统一的生成框架有效捕捉了时间序列数据的无条件和条件方面，在时间序列合成任务上表现出色。

Abstract: Generating high-quality synthetic time series is a fundamental yet
challenging task across domains such as forecasting and anomaly detection,
where real data can be scarce, noisy, or costly to collect. Unlike static data
generation, synthesizing time series requires modeling both the marginal
distribution of observations and the conditional temporal dependencies that
govern sequential dynamics. We propose TIMED, a unified generative framework
that integrates a denoising diffusion probabilistic model (DDPM) to capture
global structure via a forward-reverse diffusion process, a supervisor network
trained with teacher forcing to learn autoregressive dependencies through
next-step prediction, and a Wasserstein critic that provides adversarial
feedback to ensure temporal smoothness and fidelity. To further align the real
and synthetic distributions in feature space, TIMED incorporates a Maximum Mean
Discrepancy (MMD) loss, promoting both diversity and sample quality. All
components are built using masked attention architectures optimized for
sequence modeling and are trained jointly to effectively capture both
unconditional and conditional aspects of time series data. Experimental results
across diverse multivariate time series benchmarks demonstrate that TIMED
generates more realistic and temporally coherent sequences than
state-of-the-art generative models.

</details>


### [83] [Toward Scalable and Structured Global Station Weather Forecasting](https://arxiv.org/abs/2509.19648)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Yun Cheng,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的空间结构化注意力块，用于全球站点天气预报，通过将空间图划分为子图并分别学习子图内和子图间的空间相关性，解决了现有方法忽略空间相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列预测方法在进行大规模全球站点预测时，往往忽略或单向建模空间相关性，这与全球天气系统的内在性质相矛盾，限制了预测性能。

Method: 提出空间结构化注意力块，将空间图划分为子图，通过子图内注意力学习局部空间相关性，通过子图间注意力进行子图表示的消息传递，考虑空间邻近性和全局相关性。基于该块构建多尺度时空预测模型，逐步扩展子图尺度。

Result: 实验结果表明，该模型在低运行成本下，相比时间序列预测基线方法，性能提升可达16.8%。

Conclusion: 所提出的模型既具有可扩展性，又能产生结构化的空间相关性，且易于实现，为全球站点天气预报提供了有效的解决方案。

Abstract: Global Station Weather Forecasting (GSWF) is a key meteorological research
area, critical to energy, aviation, and agriculture. Existing time series
forecasting methods often ignore or unidirectionally model spatial correlation
when conducting large-scale global station forecasting. This contradicts the
intrinsic nature underlying observations of the global weather system, limiting
forecast performance. To address this, we propose a novel Spatial Structured
Attention Block in this paper. It partitions the spatial graph into a set of
subgraphs and instantiates Intra-subgraph Attention to learn local spatial
correlation within each subgraph, and aggregates nodes into subgraph
representations for message passing among the subgraphs via Inter-subgraph
Attention -- considering both spatial proximity and global correlation.
Building on this block, we develop a multiscale spatiotemporal forecasting
model by progressively expanding subgraph scales. The resulting model is both
scalable and able to produce structured spatial correlation, and meanwhile, it
is easy to implement. The experimental results show that it can achieve
performance improvements up to 16.8% over time series forecasting baselines at
low running costs.

</details>


### [84] [Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification](https://arxiv.org/abs/2509.19654)
*Kevin Garcia,Cassandra Garza,Brooklyn Berry,Yifeng Gao*

Main category: cs.LG

TL;DR: 本文提出了一种基于符号袋表示的自监督学习框架，用于处理数字健康领域时间序列数据中的分布偏移问题，特别是在人类行为变化导致的数据偏移场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 数字健康领域的时间序列数据通常包含大量噪声、概念漂移，且存在显著的数据分布偏移（特别是由不同人类行为引起），这给训练通用性强的深度学习模型带来了挑战。

Method: 提出了一种自监督对比学习框架，该框架结合了符号袋表示方法。符号袋表示对数据扭曲、位置偏移和噪声具有不敏感性，能够指导深度学习获得对这些数据偏移具有抵抗力的表示。

Result: 实验证明，在存在显著数据偏移的情况下，所提出的方法能够实现明显更好的性能。

Conclusion: 基于符号袋表示的自监督学习框架能够有效应对数字健康时间序列数据中的分布偏移问题，为处理此类具有挑战性的数据提供了有前景的解决方案。

Abstract: The surge in the significance of time series in digital health domains
necessitates advanced methodologies for extracting meaningful patterns and
representations. Self-supervised contrastive learning has emerged as a
promising approach for learning directly from raw data. However, time series
data in digital health is known to be highly noisy, inherently involves concept
drifting, and poses a challenge for training a generalizable deep learning
model. In this paper, we specifically focus on data distribution shift caused
by different human behaviors and propose a self-supervised learning framework
that is aware of the bag-of-symbol representation. The bag-of-symbol
representation is known for its insensitivity to data warping, location shifts,
and noise existed in time series data, making it potentially pivotal in guiding
deep learning to acquire a representation resistant to such data shifting. We
demonstrate that the proposed method can achieve significantly better
performance where significant data shifting exists.

</details>


### [85] [Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion](https://arxiv.org/abs/2509.19661)
*Puning Zhao,Zhikun Zhang,Bo Sun,Li Shen,Liang Zhang,Shaowei Wang,Zhe Liu*

Main category: cs.LG

TL;DR: 提出了一种基于小波展开的本地差分隐私分布估计方法，特别针对数值数据，通过优先估计低阶小波系数来防止概率质量被错误分配到远离真实值的位置。


<details>
  <summary>Details</summary>
Motivation: 现有的本地差分隐私分布估计方法主要针对分类数据，在转移到数值数据时由于评估指标不同而效果不佳，需要防止概率质量被错误分配到远离真实值的位置。

Method: 使用小波展开表示样本分布，在本地差分隐私条件下估计小波系数，优先估计低阶系数以确保宏观层面的准确估计。

Result: 理论分析证明了方法的有效性，实验表明在小波展开方法在Wasserstein和KS距离上显著优于现有解决方案。

Conclusion: 基于小波展开的方法为数值数据的本地差分隐私分布估计提供了有效的解决方案，能够防止概率质量被错误分配，并在多个评估指标上表现出优越性能。

Abstract: Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.

</details>


### [86] [Revisiting Performance Claims for Chest X-Ray Models Using Clinical Context](https://arxiv.org/abs/2509.19671)
*Andrew Wang,Jiashuo Zhang,Michael Oberst*

Main category: cs.LG

TL;DR: 该论文提出使用临床背景（出院摘要）来更全面地评估胸部X光诊断模型，发现当前模型在预测试概率低的情况下表现最佳，而在预测试概率高时表现较差，表明模型可能依赖临床背景作为捷径而非真正的诊断信号。


<details>
  <summary>Details</summary>
Motivation: 现有的胸部X光数据集上的机器学习模型在平均性能上表现良好，但这不足以证明其临床实用性。论文旨在通过临床背景提供更全面的模型评估。

Method: 利用每个胸部X光之前的出院摘要，推导出每个标签的“预测试概率”，作为临床医生在解读X光时可用背景知识的代理指标。通过这一指标分析模型在不同预测试概率下的表现。

Result: 研究发现，对于多个诊断标签，模型在预测试概率极低的情况下表现最佳，而在预测试概率较高时表现显著较差。此外，在平衡测试集上，模型性能急剧下降，表明其诊断能力可能主要依赖于推断临床背景而非真正的诊断信号。

Conclusion: 使用临床笔记中的背景信息进行分析是更严格和细粒度评估临床视觉模型的有前景的方向，有助于揭示模型是否真正具备诊断能力。

Abstract: Public healthcare datasets of Chest X-Rays (CXRs) have long been a popular
benchmark for developing computer vision models in healthcare. However, strong
average-case performance of machine learning (ML) models on these datasets is
insufficient to certify their clinical utility. In this paper, we use clinical
context, as captured by prior discharge summaries, to provide a more holistic
evaluation of current ``state-of-the-art'' models for the task of CXR
diagnosis. Using discharge summaries recorded prior to each CXR, we derive a
``prior'' or ``pre-test'' probability of each CXR label, as a proxy for
existing contextual knowledge available to clinicians when interpreting CXRs.
Using this measure, we demonstrate two key findings: First, for several
diagnostic labels, CXR models tend to perform best on cases where the pre-test
probability is very low, and substantially worse on cases where the pre-test
probability is higher. Second, we use pre-test probability to assess whether
strong average-case performance reflects true diagnostic signal, rather than an
ability to infer the pre-test probability as a shortcut. We find that
performance drops sharply on a balanced test set where this shortcut does not
exist, which may indicate that much of the apparent diagnostic power derives
from inferring this clinical context. We argue that this style of analysis,
using context derived from clinical notes, is a promising direction for more
rigorous and fine-grained evaluation of clinical vision models.

</details>


### [87] [C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning](https://arxiv.org/abs/2509.19674)
*Kunlun Xu,Yibo Feng,Jiangmeng Li,Yongsheng Qi,Jiahuan Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种名为C²Prompt的新方法，通过增强客户端间类级别知识一致性来解决联邦持续学习中的时空遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的联邦持续学习方法存在类级别知识一致性问题，包括客户端间类内分布差距和类间知识混淆，这会加剧时空遗忘。

Method: 提出类感知客户端知识交互方法C²Prompt，包含局部类分布补偿机制(LCDC)减少类内分布差异，以及类感知提示聚合方案(CPA)缓解类间知识混淆。

Result: 在多个联邦持续学习基准测试上的广泛实验表明，C²Prompt达到了最先进的性能。

Conclusion: C²Prompt通过显式增强提示通信中的类级别知识一致性，有效缓解了联邦持续学习中的时空遗忘问题。

Abstract: Federated continual learning (FCL) tackles scenarios of learning from
continuously emerging task data across distributed clients, where the key
challenge lies in addressing both temporal forgetting over time and spatial
forgetting simultaneously. Recently, prompt-based FCL methods have shown
advanced performance through task-wise prompt communication.In this study, we
underscore that the existing prompt-based FCL methods are prone to class-wise
knowledge coherence between prompts across clients. The class-wise knowledge
coherence includes two aspects: (1) intra-class distribution gap across
clients, which degrades the learned semantics across prompts, (2) inter-prompt
class-wise relevance, which highlights cross-class knowledge confusion. During
prompt communication, insufficient class-wise coherence exacerbates knowledge
conflicts among new prompts and induces interference with old prompts,
intensifying both spatial and temporal forgetting. To address these issues, we
propose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method
that explicitly enhances class-wise knowledge coherence during prompt
communication. Specifically, a local class distribution compensation mechanism
(LCDC) is introduced to reduce intra-class distribution disparities across
clients, thereby reinforcing intra-class knowledge consistency. Additionally, a
class-aware prompt aggregation scheme (CPA) is designed to alleviate
inter-class knowledge confusion by selectively strengthening class-relevant
knowledge aggregation. Extensive experiments on multiple FCL benchmarks
demonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our
source code is available at
https://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt

</details>


### [88] [A Unified Noise-Curvature View of Loss of Trainability](https://arxiv.org/abs/2509.19698)
*Gunbir Singh Baveja,Mark Schmidt*

Main category: cs.LG

TL;DR: 本文分析了持续学习中训练能力丧失(LoT)问题，发现单一指标不可靠，提出了基于梯度噪声和曲率波动的预测阈值方法，并开发了分层调度器来稳定训练。


<details>
  <summary>Details</summary>
Motivation: 持续学习中随着任务演变，梯度步长不再带来改进，导致准确率停滞或下降，即使有足够容量和监督。本文旨在解决Adam优化器中的训练能力丧失问题。

Method: 引入两个互补标准：批大小感知的梯度噪声边界和曲率波动控制边界，结合成每层预测阈值。基于此构建简单的每层调度器，控制每层的有效步长在安全限制内。

Result: 该方法在CReLU、Wasserstein正则化和L2权重衰减等场景下稳定了训练并提高了准确率，学习率轨迹与典型衰减模式一致。

Conclusion: 提出的分层预测阈值方法能有效预测和防止训练能力丧失，为持续学习中的优化稳定性提供了可靠解决方案。

Abstract: Loss of trainability (LoT) in continual learning occurs when gradient steps
no longer yield improvement as tasks evolve, so accuracy stalls or degrades
despite adequate capacity and supervision. We analyze LoT incurred with Adam
through an optimization lens and find that single indicators such as Hessian
rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios,
and unit-sign entropy are not reliable predictors. Instead we introduce two
complementary criteria: a batch-size-aware gradient-noise bound and a curvature
volatility-controlled bound that combine into a per-layer predictive threshold
that anticipates trainability behavior. Using this threshold, we build a simple
per-layer scheduler that keeps each layers effective step below a safe limit,
stabilizing training and improving accuracy across concatenated ReLU (CReLU),
Wasserstein regularization, and L2 weight decay, with learned learning-rate
trajectories that mirror canonical decay.

</details>


### [89] [Linear Transformers Implicitly Discover Unified Numerical Algorithms](https://arxiv.org/abs/2509.19702)
*Patrick Lutz,Aditya Gangrade,Hadi Daneshmand,Venkatesh Saligrama*

Main category: cs.LG

TL;DR: 训练线性注意力变换器完成数百万个掩码块矩阵补全任务，模型仅通过输入-输出对和均方误差损失学习，无需任何手工设计的迭代或提示。研究发现模型隐式发现了统一的、资源自适应的迭代求解器，适用于预测、估计和Nyström外推等多种计算场景。


<details>
  <summary>Details</summary>
Motivation: 探索变换器模型在仅通过输入-输出对学习的情况下，是否能够隐式发现数学问题的求解算法，特别是矩阵补全和相关计算任务的统一求解方法。

Method: 使用线性注意力变换器在数百万个掩码块矩阵补全任务上进行训练，任务包括标量预测目标和Nyström外推的未见核切片。模型仅接触输入-输出对和均方误差损失，没有提供任何正规方程或手工迭代方法。

Result: 训练后通过代数展开发现，模型在三种不同计算机制（完全可见性、秩限制更新和分布式计算）下都产生了相同的无参数更新规则。该规则在完整批次问题上实现二阶收敛，降低分布式迭代复杂度，并在秩限制注意力下保持准确性。

Conclusion: 变换器仅通过补全缺失块的任务训练，就隐式发现了一个统一的、资源自适应的迭代求解器，覆盖预测、估计和Nyström外推等多个领域，这突显了上下文学习的强大能力。

Abstract: We train a linear attention transformer on millions of masked-block matrix
completion tasks: each prompt is masked low-rank matrix whose missing block may
be (i) a scalar prediction target or (ii) an unseen kernel slice of Nystr\"om
extrapolation. The model sees only input-output pairs and a mean-squared loss;
it is given no normal equations, no handcrafted iterations, and no hint that
the tasks are related. Surprisingly, after training, algebraic unrolling
reveals the same parameter-free update rule across three distinct computational
regimes (full visibility, rank-limited updates, and distributed computation).
We prove that this rule achieves second-order convergence on full-batch
problems, cuts distributed iteration complexity, and remains accurate with
rank-limited attention. Thus, a transformer trained solely to patch missing
blocks implicitly discovers a unified, resource-adaptive iterative solver
spanning prediction, estimation, and Nystr\"om extrapolation, highlighting a
powerful capability of in-context learning.

</details>


### [90] [Causal Machine Learning for Surgical Interventions](https://arxiv.org/abs/2509.19705)
*J. Ben Tamo,Nishant S. Chouhan,Micky C. Nnamdi,Yining Yuan,Shreya S. Chivilkar,Wenqi Shi,Steven W. Hwang,B. Randall Brenn,May D. Wang*

Main category: cs.LG

TL;DR: 提出了一个多任务元学习框架X-MultiTask，用于估计个性化治疗效果（ITE），在脊柱融合术和脊柱侧弯矫正手术中表现出色。


<details>
  <summary>Details</summary>
Motivation: 外科决策复杂，需要理解患者特征、干预措施和结果之间的因果关系。传统统计方法在处理复杂异构数据时存在局限，难以准确估计个性化治疗效果。

Method: 开发了多任务元学习框架X-MultiTask，将每个手术决策建模为独立任务，同时学习跨任务的共享表示。在训练目标中加入了逆概率加权（IPW）以增强因果有效性。

Result: 在两个数据集上评估：脊柱融合数据集（1,017名患者）显示前路组平均AUC最高（0.84），后路组保持竞争力（0.77）；AIS数据集（368名患者）在所有领域均表现出优越性能，NN-PEHE和ATE误差最低。

Conclusion: X-MultiTask通过提供稳健的患者特异性因果估计，为推进个性化外科护理和改善患者预后提供了有力工具。

Abstract: Surgical decision-making is complex and requires understanding causal
relationships between patient characteristics, interventions, and outcomes. In
high-stakes settings like spinal fusion or scoliosis correction, accurate
estimation of individualized treatment effects (ITEs) remains limited due to
the reliance on traditional statistical methods that struggle with complex,
heterogeneous data. In this study, we develop a multi-task meta-learning
framework, X-MultiTask, for ITE estimation that models each surgical decision
(e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct
task while learning shared representations across tasks. To strengthen causal
validity, we incorporate the inverse probability weighting (IPW) into the
training objective. We evaluate our approach on two datasets: (1) a public
spinal fusion dataset (1,017 patients) to assess the effect of anterior vs.
posterior approaches on complication severity; and (2) a private AIS dataset
(368 patients) to analyze the impact of posterior spinal fusion (PSF) vs.
non-surgical management on patient-reported outcomes (PROs). Our model achieves
the highest average AUC (0.84) in the anterior group and maintains competitive
performance in the posterior group (0.77). It outperforms baselines in
treatment effect estimation with the lowest overall $\epsilon_{\text{NN-PEHE}}$
(0.2778) and $\epsilon_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs
in AIS, X-MultiTask consistently shows superior performance across all domains,
with $\epsilon_{\text{NN-PEHE}}$ = 0.2551 and $\epsilon_{\text{ATE}}$ = 0.0902.
By providing robust, patient-specific causal estimates, X-MultiTask offers a
powerful tool to advance personalized surgical care and improve patient
outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.

</details>


### [91] [Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods](https://arxiv.org/abs/2509.19750)
*Kainat*

Main category: cs.LG

TL;DR: 本研究提出了一种基于BERT回归模型的新型无创动脉血压预测方法，利用语音信号的声学特征来预测血压水平，避免了传统袖带测量的不适感。


<details>
  <summary>Details</summary>
Motivation: 动脉血压是心血管健康的重要指标，传统袖带测量方法存在白大衣高血压和隐匿性高血压等问题，需要一种更舒适、准确的监测方法。

Method: 采用基于BERT的回归模型，通过分析语音信号的声学特征来建立与血压水平的关联，使用95名参与者的语音数据集进行模型微调。

Result: 模型表现出色，收缩压平均绝对误差为13.6 mmHg（R²=0.99），舒张压平均绝对误差为12.4 mmHg（R²=0.94），训练和验证损失分析显示有效学习且过拟合最小。

Conclusion: 深度学习与语音分析结合为血压监测提供了可行的替代方案，在远程医疗和健康监测领域具有重要应用前景，能够改善患者护理和心血管健康管理。

Abstract: This research presents a novel method for noninvasive arterial blood pressure
ABP prediction using speech signals employing a BERT based regression model
Arterial blood pressure is a vital indicator of cardiovascular health and
accurate monitoring is essential in preventing hypertension related
complications Traditional cuff based methods often yield inconsistent results
due to factors like whitecoat and masked hypertension Our approach leverages
the acoustic characteristics of speech capturing voice features to establish
correlations with blood pressure levels Utilizing advanced deep learning
techniques we analyze speech signals to extract relevant patterns enabling real
time monitoring without the discomfort of conventional methods In our study we
employed a dataset comprising recordings from 95 participants ensuring diverse
representation The BERT model was fine tuned on extracted features from speech
leading to impressive performance metrics achieving a mean absolute error MAE
of 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood
pressure DBP with R scores of 099 and 094 respectively These results indicate
the models robustness in accurately predicting blood pressure levels
Furthermore the training and validation loss analysis demonstrates effective
learning and minimal overfitting Our findings suggest that integrating deep
learning with speech analysis presents a viable alternative for blood pressure
monitoring paving the way for improved applications in telemedicine and remote
health monitoring By providing a user friendly and accurate method for blood
pressure assessment this research has significant implications for enhancing
patient care and proactive management of cardiovascular health

</details>


### [92] [Frictional Q-Learning](https://arxiv.org/abs/2509.19771)
*Hyunwoo Kim,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: 该论文提出了Frictional Q-learning算法，通过类比经典力学中的静摩擦力与离策略强化学习中的外推误差，约束智能体行为以避免漂移到不支持的动动作区域。


<details>
  <summary>Details</summary>
Motivation: 解决离策略强化学习中的外推误差问题，防止策略漂移到经验回放缓冲区中不支持的动动作空间。

Method: 基于批量约束强化学习，通过物理摩擦力类比设计约束条件，限制智能体动作空间使其接近回放缓冲区中的行为模式，同时保持与正交动作空间流形的距离。

Result: 算法在标准连续控制基准测试中表现出稳健的训练过程和具有竞争力的性能。

Conclusion: Frictional Q-learning通过物理启发的约束机制有效解决了外推误差问题，同时保持了算法的简洁性和直观的物理解释。

Abstract: We draw an analogy between static friction in classical mechanics and
extrapolation error in off-policy RL, and use it to formulate a constraint that
prevents the policy from drifting toward unsupported actions. In this study, we
present Frictional Q-learning, a deep reinforcement learning algorithm for
continuous control, which extends batch-constrained reinforcement learning. Our
algorithm constrains the agent's action space to encourage behavior similar to
that in the replay buffer, while maintaining a distance from the manifold of
the orthonormal action space. The constraint preserves the simplicity of
batch-constrained, and provides an intuitive physical interpretation of
extrapolation error. Empirically, we further demonstrate that our algorithm is
robustly trained and achieves competitive performance across standard
continuous control benchmarks.

</details>


### [93] [Sobolev acceleration for neural networks](https://arxiv.org/abs/2509.19773)
*Jong Kwon Oh,Hanbaek Lyu,Hwijae Son*

Main category: cs.LG

TL;DR: 本文提出了首个严格的理论框架，证明Sobolev训练能加速ReLU网络的收敛。通过学生-教师框架和浅层架构，推导了群体梯度和Hessian的精确公式，量化了损失景观条件改善和梯度流收敛速率的提升。


<details>
  <summary>Details</summary>
Motivation: Sobolev训练将目标导数整合到损失函数中，相比传统L²训练能加速收敛并改善泛化能力，但其底层机制尚未完全理解。

Method: 在Gaussian输入和浅层架构的学生-教师框架下，推导群体梯度和Hessian的精确公式，分析损失景观条件和梯度流收敛速率。

Result: 理论分析表明Sobolev训练显著改善了损失景观的条件数，加速了梯度流收敛。数值实验验证了理论发现，并证明Sobolev训练的优势可扩展到现代深度学习任务。

Conclusion: Sobolev训练通过改善损失景观的几何特性来加速神经网络收敛，为理解该方法的有效性提供了首个严格的理论基础。

Abstract: Sobolev training, which integrates target derivatives into the loss
functions, has been shown to accelerate convergence and improve generalization
compared to conventional $L^2$ training. However, the underlying mechanisms of
this training method remain only partially understood. In this work, we present
the first rigorous theoretical framework proving that Sobolev training
accelerates the convergence of Rectified Linear Unit (ReLU) networks. Under a
student-teacher framework with Gaussian inputs and shallow architectures, we
derive exact formulas for population gradients and Hessians, and quantify the
improvements in conditioning of the loss landscape and gradient-flow
convergence rates. Extensive numerical experiments validate our theoretical
findings and show that the benefits of Sobolev training extend to modern deep
learning tasks.

</details>


### [94] [PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection](https://arxiv.org/abs/2509.19774)
*Xiaocheng Fang,Jiarui Jin,Haoyu Wang,Che Liu,Jieyi Cai,Guangkun Nie,Jun Li,Hongyan Li,Shenda Hong*

Main category: cs.LG

TL;DR: PPGFlowECG是一个两阶段框架，通过CardioAlign编码器在共享潜在空间中对齐PPG和ECG信号，并使用潜在整流流生成高保真度的ECG信号，解决PPG到ECG转换的挑战。


<details>
  <summary>Details</summary>
Motivation: ECG是心脏监测的金标准但设备专业，PPG可连续监测但缺乏电生理信息。生成模型可将PPG转换为临床有价值的ECG信号，但现有方法面临生理语义不对齐和高维信号建模复杂性的挑战。

Method: 提出PPGFlowECG框架：1）使用CardioAlign编码器在共享潜在空间对齐PPG和ECG；2）采用潜在整流流生成高保真ECG。在包含1000万+配对样本的MCMED临床数据集上进行实验。

Result: 方法在PPG到ECG转换和心血管疾病检测方面表现有效。心脏病专家评估确认合成ECG达到高保真度并提高诊断可靠性。

Conclusion: 该方法在真实世界心血管筛查中具有潜力，首次在大型临床数据集MCMED上验证了PPG到ECG转换的有效性。

Abstract: In clinical practice, electrocardiography (ECG) remains the gold standard for
cardiac monitoring, providing crucial insights for diagnosing a wide range of
cardiovascular diseases (CVDs). However, its reliance on specialized equipment
and trained personnel limits feasibility for continuous routine monitoring.
Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks
definitive electrophysiological information, preventing conclusive diagnosis.
Generative models present a promising approach to translate PPG into clinically
valuable ECG signals, yet current methods face substantial challenges,
including the misalignment of physiological semantics in generative models and
the complexity of modeling in high-dimensional signals. To this end, we propose
PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent
space via the CardioAlign Encoder and employs latent rectified flow to generate
ECGs with high fidelity and interpretability. To the best of our knowledge,
this is the first study to experiment on MCMED, a newly released clinical-grade
dataset comprising over 10 million paired PPG-ECG samples from more than
118,000 emergency department visits with expert-labeled cardiovascular disease
annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG
translation and cardiovascular disease detection. Moreover, cardiologist-led
evaluations confirm that the synthesized ECGs achieve high fidelity and improve
diagnostic reliability, underscoring our method's potential for real-world
cardiovascular screening.

</details>


### [95] [Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference](https://arxiv.org/abs/2509.19781)
*Ziyi Han,Xutong Liu,Ruiting Zhou,Xiangxiang Dai,John C. S. Lui*

Main category: cs.LG

TL;DR: 提出了一种名为Tanbr的树结构自适应神经赌博路由方法，用于解决稀疏专家混合模型在线推理中的效率和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合模型在在线推理中面临挑战，包括模型规模大、专家路由复杂，以及在边缘网络中资源受限的问题。此外，在线推理时任务信息通常不可用，导致任务级路由容易出错。

Method: Tanbr通过估计历史数据中的任务分布来指导任务感知的专家合并，使用二叉树逐步划分合并权重空间并生成更精细的候选权重，然后应用神经赌博学习从合并权重到模型性能的非线性映射，以决定最优的专家合并。

Result: Tanbr实现了O(√T log(T))的次线性遗憾界，实验表明它能将推理延迟降低至少45%，内存使用减少高达25%，同时保持高准确率。

Conclusion: Tanbr方法有效解决了稀疏专家混合模型在线推理的挑战，在保持性能的同时显著提升了效率。

Abstract: Sparse Mixture of Experts (SMoE) has become a preferred architecture for
scaling Transformer capacity without increasing computational cost, as it
activates only a small subset of experts for each input. However, deploying
such an approach for \textit{online inference} remains challenging due to the
large size of a full SMoE model and the complexity of expert routing,
especially in resource-constrained edge networks. Moreover, during the online
inference, task information is often unavailable, making the task-level routing
error-prone. In this work, we propose a novel tree-structured adaptive neural
bandit router, \texttt{Tanbr}, to enable efficient and reliable online MoE
inference. Instead of relying on explicit task tags, \texttt{Tanbr} estimates
the task distribution over time from historical data and uses it to guide
task-aware expert merging within a given pre-trained MoE. To handle the large
continuous space of merging weights, \texttt{Tanbr} employs a binary tree to
progressively partition the space and generate finer candidate weights. It then
applies a neural bandit to learn the non-linear mapping from merging weight to
model performance and decides optimal expert merging. We prove that
\texttt{Tanbr} achieves a sublinear regret bound of {\small
$\mathcal{O}(\sqrt{T} \log(T))$} over {\small $T$} rounds, despite operating
over a continuous decision space, matching regret bounds compared to existing
methods. Extensive experiments show that \texttt{Tanbr} reduces inference
latency by at least {\small $45\%$} and memory usage by up to {\small $25\%$},
while maintaining a high accuracy compared to many state-of-the-art methods.

</details>


### [96] [RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving](https://arxiv.org/abs/2509.19789)
*Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim*

Main category: cs.LG

TL;DR: RDAR是一种学习每个智能体相关性的策略，通过识别哪些智能体可以从预训练行为模型的输入中排除，从而减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶员每次只关注少数几个智能体，而自动驾驶系统需要处理复杂场景中的所有智能体，无论其是否影响决策。现有注意力机制计算复杂度高，需要更高效的方法。

Method: 将掩码过程建模为马尔可夫决策过程，动作由表示智能体选择的二元掩码组成。通过识别可排除的智能体来学习每个智能体的相关性。

Result: 在大规模驾驶数据集上评估，RDAR能够学习准确的数值相关性度量，在保持可比驾驶性能的同时显著减少处理的智能体数量。

Conclusion: RDAR提供了一种有效的方法来减少自动驾驶系统的计算负担，同时保持安全性和性能。

Abstract: Human drivers focus only on a handful of agents at any one time. On the other
hand, autonomous driving systems process complex scenes with numerous agents,
regardless of whether they are pedestrians on a crosswalk or vehicles parked on
the side of the road. While attention mechanisms offer an implicit way to
reduce the input to the elements that affect decisions, existing attention
mechanisms for capturing agent interactions are quadratic, and generally
computationally expensive. We propose RDAR, a strategy to learn per-agent
relevance -- how much each agent influences the behavior of the controlled
vehicle -- by identifying which agents can be excluded from the input to a
pre-trained behavior model. We formulate the masking procedure as a Markov
Decision Process where the action consists of a binary mask indicating agent
selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate
its ability to learn an accurate numerical measure of relevance by achieving
comparable driving performance, in terms of overall progress, safety and
performance, while processing significantly fewer agents compared to a state of
the art behavior model.

</details>


### [97] [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2509.19803)
*Guochao Jiang,Wenfeng Feng,Guofeng Quan,Chuzhan Hao,Yuewei Zhang,Guohua Liu,Hao Wang*

Main category: cs.LG

TL;DR: VCRL是一个基于奖励方差动态控制训练样本难度的课程强化学习框架，旨在解决现有基于rollout的强化学习方法未能考虑LLM对不同难度样本学习能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于rollout的强化学习方法（如GRPO、DAPO、GSPO等）没有显式考虑LLM对不同难度样本的学习能力，这与人类从易到难的数学推理认知过程相悖。研究发现rollout组奖励的方差部分反映了当前样本对LLM的难度。

Method: 提出VCRL框架，基于组奖励方差动态控制训练样本难度。样本过易或过难时方差较低，中等难度样本方差较高，据此构建从易到难的课程学习策略。

Result: 在五个数学基准测试和两个模型上的实验表明，VCRL优于当前的LLM强化学习基线方法。

Conclusion: VCRL通过考虑样本难度差异，有效提升了LLM在数学推理任务上的性能，验证了课程强化学习框架的有效性。

Abstract: Policy-based reinforcement learning currently plays an important role in
improving LLMs on mathematical reasoning tasks. However, existing rollout-based
reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly
consider LLMs' learning ability for samples of different difficulty levels,
which is contrary to the human cognitive process of mathematical reasoning
tasks from easy to difficult. Intuitively, we find that the variance of the
rollout group's reward in RLVR partly reflects the difficulty of the current
sample for LLMs. Samples that are too easy or too difficult have a lower
variance, while samples with moderate difficulty have a higher variance. Based
on this, we propose VCRL, a curriculum reinforcement learning framework that
dynamically controls the difficulty of training samples based on the variance
of group rewards. Experiments on five mathematical benchmarks and two models
reveal the advantages of VCRL over the current LLM RL baselines.

</details>


### [98] [An Efficient Conditional Score-based Filter for High Dimensional Nonlinear Filtering Problems](https://arxiv.org/abs/2509.19816)
*Zhijun Zeng,Weiye Gan,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 提出CSF算法，利用集合变换器编码器和条件扩散模型实现高效后验采样，无需重复训练


<details>
  <summary>Details</summary>
Motivation: 高维非线性滤波仍是挑战，现有基于分数的扩散模型需要重复训练来跟踪演化先验，这在实践中不可行

Method: 将先验建模和后验采样解耦为离线和在线阶段，使用集合变换器编码器和条件扩散模型

Result: 在基准问题上实验表明CSF在不同非线性滤波场景下具有优越的准确性、鲁棒性和效率

Conclusion: CSF实现了可扩展的基于分数的滤波，为高维非线性滤波问题提供了实用解决方案

Abstract: In many engineering and applied science domains, high-dimensional nonlinear
filtering is still a challenging problem. Recent advances in score-based
diffusion models offer a promising alternative for posterior sampling but
require repeated retraining to track evolving priors, which is impractical in
high dimensions. In this work, we propose the Conditional Score-based Filter
(CSF), a novel algorithm that leverages a set-transformer encoder and a
conditional diffusion model to achieve efficient and accurate posterior
sampling without retraining. By decoupling prior modeling and posterior
sampling into offline and online stages, CSF enables scalable score-based
filtering across diverse nonlinear systems. Extensive experiments on benchmark
problems show that CSF achieves superior accuracy, robustness, and efficiency
across diverse nonlinear filtering scenarios.

</details>


### [99] [On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators](https://arxiv.org/abs/2509.19830)
*Wei Liu,Eleni Chatzi,Zhilu Lai*

Main category: cs.LG

TL;DR: 本文为Kolmogorov-Arnold Networks (KANs)建立了理论收敛保证，证明其在B样条表示下能够达到极小极大最优收敛率，并为B样条节点数选择提供了指导。


<details>
  <summary>Details</summary>
Motivation: KANs提供了一种结构化且可解释的多变量函数逼近框架，但缺乏理论收敛保证。本文旨在填补这一理论空白。

Method: 使用B样条表示单变量变换，通过加性或混合加性-乘性聚合构建KANs，并分析其理论收敛性质。

Result: 证明了加性和混合KANs在Sobolev空间光滑度r的函数上达到极小极大最优收敛率O(n^{-2r/(2r+1)})，并通过仿真验证了理论结果。

Conclusion: 研究结果为KANs在非参数回归中的应用提供了理论基础，表明其作为现有方法的结构化替代方案具有潜力。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable
framework for multivariate function approximation by composing univariate
transformations through additive or multiplicative aggregation. This paper
establishes theoretical convergence guarantees for KANs when the univariate
components are represented by B-splines. We prove that both additive and hybrid
additive-multiplicative KANs attain the minimax-optimal convergence rate
$O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We
further derive guidelines for selecting the optimal number of knots in the
B-splines. The theory is supported by simulation studies that confirm the
predicted convergence rates. These results provide a theoretical foundation for
using KANs in nonparametric regression and highlight their potential as a
structured alternative to existing methods.

</details>


### [100] [BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management](https://arxiv.org/abs/2509.19846)
*Kevin Bradley Dsouza,Enoch Ofosu,Daniel Chukwuemeka Amaogu,Jérôme Pigeon,Richard Boudreault,Pooneh Maghoul,Juan Moreno-Cruz,Yuri Leonenko*

Main category: cs.LG

TL;DR: BoreaRL是首个用于气候适应性北方森林管理的多目标强化学习环境，通过物理基础模拟器揭示碳目标比永久冻土保护目标更容易优化，并展示了不同管理策略的差异。


<details>
  <summary>Details</summary>
Motivation: 北方森林储存大量碳，但优化森林管理以同时实现碳固存和永久冻土保护存在复杂权衡，现有工具无法充分解决这一问题。

Method: 开发BoreaRL多目标强化学习环境，包含能量、碳和水通量的物理基础模拟器，支持站点特定和通用两种训练范式，评估多目标RL算法。

Result: 发现碳目标优化明显比永久冻土保护目标容易，标准偏好条件方法在通用设置中完全失败，而简单的课程学习方法通过策略性选择训练片段获得更好性能。

Conclusion: 当前多目标RL方法在稳健的气候适应性森林管理方面仍面临挑战，BoreaRL为开发更有效方法提供了有价值的基准平台。

Abstract: Boreal forests store 30-40% of terrestrial carbon, much in climate-vulnerable
permafrost soils, making their management critical for climate mitigation.
However, optimizing forest management for both carbon sequestration and
permafrost preservation presents complex trade-offs that current tools cannot
adequately address. We introduce $\textbf{BoreaRL}$, the first multi-objective
reinforcement learning environment for climate-adaptive boreal forest
management, featuring a physically-grounded simulator of coupled energy,
carbon, and water fluxes. BoreaRL supports two training paradigms:
site-specific mode for controlled studies and generalist mode for learning
robust policies under environmental stochasticity. Through evaluation of
multi-objective RL algorithms, we reveal a fundamental asymmetry in learning
difficulty: carbon objectives are significantly easier to optimize than thaw
(permafrost preservation) objectives, with thaw-focused policies showing
minimal learning progress across both paradigms. In generalist settings,
standard preference-conditioned approaches fail entirely, while a naive
curriculum learning approach achieves superior performance by strategically
selecting training episodes. Analysis of learned strategies reveals distinct
management philosophies, where carbon-focused policies favor aggressive
high-density coniferous stands, while effective multi-objective policies
balance species composition and density to protect permafrost while maintaining
carbon gains. Our results demonstrate that robust climate-adaptive forest
management remains challenging for current MORL methods, establishing BoreaRL
as a valuable benchmark for developing more effective approaches. We
open-source BoreaRL to accelerate research in multi-objective RL for climate
applications.

</details>


### [101] [Analyzing Generalization in Pre-Trained Symbolic Regression](https://arxiv.org/abs/2509.19849)
*Henrik Voigt,Paul Kahlmeyer,Kai Lawonn,Michael Habeck,Joachim Giesen*

Main category: cs.LG

TL;DR: 本文对预训练Transformer符号回归模型的泛化能力进行系统评估，发现在分布内表现良好但在分布外场景下性能显著下降，存在严重的泛化差距。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在符号回归中表现出色，但其成功严重依赖预训练数据，对于预训练分布之外的泛化能力尚未得到充分探索。

Method: 通过系统实证研究，在预训练分布内和一系列分布外挑战场景下，评估多个最先进预训练Transformer符号回归方法的性能。

Result: 预训练模型在分布内表现良好，但在分布外场景下性能持续下降，显示出显著的泛化差距。

Conclusion: 泛化差距是实践应用中的关键障碍，严重限制了预训练方法在现实世界应用中的实用性。

Abstract: Symbolic regression algorithms search a space of mathematical expressions for
formulas that explain given data. Transformer-based models have emerged as a
promising, scalable approach shifting the expensive combinatorial search to a
large-scale pre-training phase. However, the success of these models is
critically dependent on their pre-training data. Their ability to generalize to
problems outside of this pre-training distribution remains largely unexplored.
In this work, we conduct a systematic empirical study to evaluate the
generalization capabilities of pre-trained, transformer-based symbolic
regression. We rigorously test performance both within the pre-training
distribution and on a series of out-of-distribution challenges for several
state of the art approaches. Our findings reveal a significant dichotomy: while
pre-trained models perform well in-distribution, the performance consistently
degrades in out-of-distribution scenarios. We conclude that this generalization
gap is a critical barrier for practitioners, as it severely limits the
practical use of pre-trained approaches for real-world applications.

</details>


### [102] [Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach](https://arxiv.org/abs/2509.19856)
*Samir Brahim Belhaouari,Yunis Carreon Kahalan,Humaira Shaffique,Ismael Belhaouari,Ashhadul Islam*

Main category: cs.LG

TL;DR: 该论文提出了一种区分边界数据和核心数据的方法，通过边界数据过采样和核心感知缩减技术，在不平衡分类任务中显著提升F1分数并实现高达90%的数据压缩。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在不平衡分类任务中常因无法区分决策边界附近的关键实例和分布核心的冗余样本而效果受限，需要系统性的数据区分方法。

Method: 提出边界数据过采样方法和核心感知缩减方法，通过识别和区分边界数据与核心数据来优化数据集。

Result: 边界数据过采样在96%的数据集上提升F1分数达10%，核心感知缩减方法压缩数据集达90%同时保持准确率，比原始数据集效率提升10倍。

Conclusion: 该方法可扩展到文本、多模态和自监督学习场景，为数据高效学习提供新途径，推动下一代AI发展。

Abstract: The effectiveness of machine learning models, particularly in unbalanced
classification tasks, is often hindered by the failure to differentiate between
critical instances near the decision boundary and redundant samples
concentrated in the core of the data distribution. In this paper, we propose a
method to systematically identify and differentiate between these two types of
data. Through extensive experiments on multiple benchmark datasets, we show
that the boundary data oversampling method improves the F1 score by up to 10\%
on 96\% of the datasets, whereas our core-aware reduction method compresses
datasets up to 90\% while preserving their accuracy, making it 10 times more
powerful than the original dataset. Beyond imbalanced classification, our
method has broader implications for efficient model training, particularly in
computationally expensive domains such as Large Language Model (LLM) training.
By prioritizing high-quality, decision-relevant data, our approach can be
extended to text, multimodal, and self-supervised learning scenarios, offering
a pathway to faster convergence, improved generalization, and significant
computational savings. This work paves the way for future research in
data-efficient learning, where intelligent sampling replaces brute-force
expansion, driving the next generation of AI advancements. Our code is
available as a Python package at https://pypi.org/project/adaptive-resampling/ .

</details>


### [103] [Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials](https://arxiv.org/abs/2509.19877)
*Shi Yin,Zujian Dai,Xinyang Pan,Lixin He*

Main category: cs.LG

TL;DR: NextHAM是一种用于材料电子结构哈密顿量预测的神经E(3)对称性和表达性校正方法，通过引入零阶哈密顿量、对称性Transformer架构和双空间训练目标，在Materials-HAM-SOC数据集上实现了优异的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统DFT方法计算成本高，而现有深度学习方法在原子类型多样、结构模式复杂和高维哈密顿量预测方面面临泛化性能挑战，需要开发更通用高效的预测方法。

Method: 1) 引入零阶哈密顿量作为神经回归模型的输入描述符和输出初始估计；2) 设计具有严格E(3)对称性的Transformer架构；3) 提出实空间和倒空间双训练目标防止误差放大和"幽灵态"出现。

Result: 在包含17,000个材料结构的Materials-HAM-SOC数据集上，NextHAM在哈密顿量和能带结构预测方面表现出优异的准确性和计算效率。

Conclusion: NextHAM通过方法论创新和大规模数据集构建，推进了哈密顿量预测的通用深度学习范式，为材料电子结构计算提供了高效准确的解决方案。

Abstract: Deep learning methods for electronic-structure Hamiltonian prediction has
offered significant computational efficiency advantages over traditional DFT
methods, yet the diversity of atomic types, structural patterns, and the
high-dimensional complexity of Hamiltonians pose substantial challenges to the
generalization performance. In this work, we contribute on both the methodology
and dataset sides to advance universal deep learning paradigm for Hamiltonian
prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and
expressive correction method for efficient and generalizable materials
electronic-structure Hamiltonian prediction. First, we introduce the
zeroth-step Hamiltonians, which can be efficiently constructed by the initial
charge density of DFT, as informative descriptors of neural regression model in
the input level and initial estimates of the target Hamiltonian in the output
level, so that the regression model directly predicts the correction terms to
the target ground truths, thereby significantly simplifying the input-output
mapping for learning. Second, we present a neural Transformer architecture with
strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian
prediction. Third, we propose a novel training objective to ensure the accuracy
performance of Hamiltonians in both real space and reciprocal space, preventing
error amplification and the occurrence of "ghost states" caused by the large
condition number of the overlap matrix. On the dataset side, we curate a
high-quality broad-coverage large benchmark, namely Materials-HAM-SOC,
comprising 17,000 material structures spanning 68 elements from six rows of the
periodic table and explicitly incorporating SOC effects. Experimental results
on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and
efficiency in predicting Hamiltonians and band structures.

</details>


### [104] [MCGrad:: Multicalibration at Web Scale](https://arxiv.org/abs/2509.19884)
*Lorenzo Perini,Daniel Haimovich,Fridolin Linder,Niek Tax,Dima Karamshuk,Milan Vojnovic,Nastaran Okati,Pavlos Athanasios Apostolopoulos*

Main category: cs.LG

TL;DR: MCGrad是一种新颖且可扩展的多重校准算法，旨在解决现有多重校准方法在工业应用中面临的挑战，如需要手动指定子组、不可扩展性以及可能损害其他模型性能指标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多重校准方法在工业应用中受到限制，主要因为需要手动指定子组、缺乏可扩展性，并且可能损害如对数损失和精确率-召回率曲线下面积等其他模型性能指标。MCGrad旨在克服这些限制。

Method: MCGrad是一种多重校准算法，不需要显式指定受保护组，具有可扩展性，并且通常能改善其他机器学习评估指标而不是损害它们。

Result: MCGrad已在Meta投入生产，并成为数百个生产模型的一部分。在公共数据集和生产部署中均展示了良好的结果。

Conclusion: MCGrad作为一种无需手动指定子组、可扩展且能改善其他性能指标的多重校准算法，在工业应用中具有重要价值，并已在Meta的大规模生产环境中得到验证。

Abstract: We propose MCGrad, a novel and scalable multicalibration algorithm.
Multicalibration - calibration in sub-groups of the data - is an important
property for the performance of machine learning-based systems. Existing
multicalibration methods have thus far received limited traction in industry.
We argue that this is because existing methods (1) require such subgroups to be
manually specified, which ML practitioners often struggle with, (2) are not
scalable, or (3) may harm other notions of model performance such as log loss
and Area Under the Precision-Recall Curve (PRAUC). MCGrad does not require
explicit specification of protected groups, is scalable, and often improves
other ML evaluation metrics instead of harming them. MCGrad has been in
production at Meta, and is now part of hundreds of production models. We
present results from these deployments as well as results on public datasets.

</details>


### [105] [Towards Self-Supervised Foundation Models for Critical Care Time Series](https://arxiv.org/abs/2509.19885)
*Katja Naasunnguaq Jagd,Rachael DeVries,Ole Winther*

Main category: cs.LG

TL;DR: 提出了一个基于双向Transformer的早期预训练基础模型，用于重症监护时间序列数据，在小数据集上表现优于监督基线


<details>
  <summary>Details</summary>
Motivation: 医疗领域特定基础模型发展迅速，但重症监护时间序列基础模型因数据集规模有限而相对未被充分探索

Method: 使用双向Transformer架构，在合并的电子健康记录数据集上进行预训练，然后在独立数据集上进行微调用于死亡率预测

Result: 模型在死亡率预测任务上优于监督基线，特别是在小数据集（<5,000样本）上表现突出

Conclusion: 自监督基础模型在重症监护时间序列领域具有潜力，可支持资源有限环境下的通用和稳健临床应用

Abstract: Domain-specific foundation models for healthcare have expanded rapidly in
recent years, yet foundation models for critical care time series remain
relatively underexplored due to the limited size and availability of datasets.
In this work, we introduce an early-stage pre-trained foundation model for
critical care time-series based on the Bi-Axial Transformer (BAT), trained on
pooled electronic health record datasets. We demonstrate effective transfer
learning by fine-tuning the model on a dataset distinct from the training
sources for mortality prediction, where it outperforms supervised baselines,
particularly for small datasets ($<5,000$). These contributions highlight the
potential of self-supervised foundation models for critical care times series
to support generalizable and robust clinical applications in resource-limited
settings.

</details>


### [106] [PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning](https://arxiv.org/abs/2509.19894)
*Xueliang Zhao,Wei Wu,Jian Guan,Zhuocheng Gong,Lingpeng Kong*

Main category: cs.LG

TL;DR: PromptCoT 2.0是一个可扩展的框架，通过期望最大化(EM)循环迭代优化推理过程，生成更困难、更多样化的训练问题，显著提升大语言模型在数学和编程推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有高质量训练问题稀缺：人工标注成本高且有限，现有合成语料库往往过于简单或狭窄。需要一种可扩展的方法来生成更困难、更多样化的问题以提升模型推理能力。

Method: 基于PromptCoT 1.0，用期望最大化(EM)循环替代手工启发式方法，迭代优化推理过程来指导提示构建。支持两种后训练机制：自我对弈和监督微调。

Result: 在Qwen3-30B模型上实现SOTA性能：AIME 24/25和HMMT 25分别提升4.4、4.8和5.3分，LiveCodeBench v5/v6提升6.1和5.0分，Codeforces提升35 Elo。Qwen2.5-7B仅使用合成提示训练达到73.1(AIME 24)、65.6(AIME 25)和53.4(LiveCodeBench v5)的准确率。

Conclusion: 提示合成成为扩展推理能力的新维度，PromptCoT 2.0为未来开源模型提供了可扩展的基础，证明合成数据可以超越人类或混合数据训练的效果。

Abstract: Large language models (LLMs) are evolving from conversational systems into
strong reasoners for tasks such as Olympiad mathematics and competitive
programming. While scaling parameters and test-time computation has driven
progress, a key bottleneck is the lack of high-quality training problems:
human-curated datasets are costly and limited, while existing synthetic corpora
are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales
into prompt synthesis increases problem difficulty. Building on this, we
present PromptCoT 2.0, a scalable framework that replaces hand-crafted
heuristics with an expectation-maximization (EM) loop, where rationales are
iteratively refined to guide prompt construction. This produces problems that
are both harder and more diverse than prior corpora. The synthetic prompts
support two post-training regimes: (1) Self-Play, where strong models improve
autonomously via verifiable feedback without stronger teachers; and (2)
Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled
traces. Extensive experiments demonstrate the effectiveness of this approach.
In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new
state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME
24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on
Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts
boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5),
surpassing models trained on human or hybrid data. Analyses further confirm
that PromptCoT 2.0 yields fundamentally harder and distributionally distinct
problems. These results establish prompt synthesis as a new axis for scaling
reasoning and position PromptCoT 2.0 as a scalable foundation for future
open-source models. The implementation is available at
https://github.com/inclusionAI/PromptCoT.

</details>


### [107] [Pure Exploration via Frank-Wolfe Self-Play](https://arxiv.org/abs/2509.19901)
*Xinyu Liu,Chao Qin,Wei You*

Main category: cs.LG

TL;DR: 本文提出Frank-Wolfe Self-Play (FWSP)方法，用于解决结构化随机多臂老虎机中的纯探索问题，通过双人零和博弈框架实现高效的假设检验。


<details>
  <summary>Details</summary>
Motivation: 在结构化随机多臂老虎机中，传统的渐近分析需要解决一个极大极小优化问题，但结构约束会引入尖锐的病理特性，如非唯一最优解、最优设计在最佳臂上质量为零等，这给算法设计和分析带来挑战。

Method: 通过允许怀疑者采用混合策略，将问题重新表述为凹-凸鞍点问题，提出FWSP方法：一种无投影、无正则化、无调参的方法，其单热更新与老虎机采样范式相匹配。使用微分包含论证和Lyapunov函数分析收敛性。

Result: 理论分析证明在线性老虎机的最佳臂识别任务中，游戏值能够收敛到最优值，数值实验验证了对偶间隙趋于零。

Conclusion: FWSP方法能够有效处理结构化约束带来的病理特性，实现全局收敛到最优游戏值，为结构化随机多臂老虎机的纯探索问题提供了有效的解决方案。

Abstract: We study pure exploration in structured stochastic multi-armed bandits,
aiming to efficiently identify the correct hypothesis from a finite set of
alternatives. For a broad class of tasks, asymptotic analyses reduce to a
maximin optimization that admits a two-player zero-sum game interpretation
between an experimenter and a skeptic: the experimenter allocates measurements
to rule out alternatives while the skeptic proposes alternatives. We
reformulate the game by allowing the skeptic to adopt a mixed strategy,
yielding a concave-convex saddle-point problem. This viewpoint leads to
Frank-Wolfe Self-Play (FWSP): a projection-free, regularization-free,
tuning-free method whose one-hot updates on both sides match the bandit
sampling paradigm. However, structural constraints introduce sharp pathologies
that complicate algorithm design and analysis: our linear-bandit case study
exhibits nonunique optima, optimal designs with zero mass on the best arm,
bilinear objectives, and nonsmoothness at the boundary. We address these
challenges via a differential-inclusion argument, proving convergence of the
game value for best-arm identification in linear bandits. Our analysis proceeds
through a continuous-time limit: a differential inclusion with a Lyapunov
function that decays exponentially, implying a vanishing duality gap and
convergence to the optimal value. Although Lyapunov analysis requires
differentiability of the objective, which is not guaranteed on the boundary, we
show that along continuous trajectories the algorithm steers away from
pathological nonsmooth points and achieves uniform global convergence to the
optimal game value. We then embed the discrete-time updates into a perturbed
flow and show that the discrete game value also converges. Building on FWSP, we
further propose a learning algorithm based on posterior sampling. Numerical
experiments demonstrate a vanishing duality gap.

</details>


### [108] [Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation](https://arxiv.org/abs/2509.19903)
*Songtao Li,Zhenyu Liao,Tianqi Hou,Ting Gao*

Main category: cs.LG

TL;DR: LIRF是一种解决少样本生成问题的新方法，通过构建几何结构保持的潜在空间和迭代生成-校正-增强循环，有效克服过拟合和模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少样本生成中存在过拟合、模式崩溃问题，且微调大模型会继承偏见并忽略潜在空间的几何结构。需要一种能够保持数据流形几何特性的新方法。

Method: 使用带有流形保持损失的自动编码器建立稳定潜在空间，然后通过迭代生成-校正-增强循环，利用几何校正算子将样本拉向数据流形同时保持多样性。

Result: LIRF在AFHQ-Cat数据集上生成了一致的高分辨率图像，收敛定理证明了生成流形与真实数据流形之间Hausdorff距离的可预测减小。消融研究证实了流形保持潜在空间和收缩校正机制的关键作用。

Conclusion: LIRF为数据稀缺的生成建模提供了一个理论上有基础且实践高效的解决方案，成功解决了少样本生成中的关键挑战。

Abstract: Few-shot generation, the synthesis of high-quality and diverse samples from
limited training data, remains a significant challenge in generative modeling.
Existing methods trained from scratch often fail to overcome overfitting and
mode collapse, and fine-tuning large models can inherit biases while neglecting
the crucial geometric structure of the latent space. To address these
limitations, we introduce Latent Iterative Refinement Flow (LIRF), a novel
approach that reframes few-shot generation as the progressive densification of
geometrically structured manifold. LIRF establishes a stable latent space using
an autoencoder trained with our novel \textbf{manifold-preservation loss}
$L_{\text{manifold}}$. This loss ensures that the latent space maintains the
geometric and semantic correspondence of the input data. Building on this, we
propose an iterative generate-correct-augment cycle. Within this cycle,
candidate samples are refined by a geometric \textbf{correction operator}, a
provably contractive mapping that pulls samples toward the data manifold while
preserving diversity. We also provide the \textbf{Convergence Theorem}
demonstrating a predictable decrease in Hausdorff distance between generated
and true data manifold. We also demonstrate the framework's scalability by
generating coherent, high-resolution images on AFHQ-Cat. Ablation studies
confirm that both the manifold-preserving latent space and the contractive
correction mechanism are critical components of this success. Ultimately, LIRF
provides a solution for data-scarce generative modeling that is not only
theoretically grounded but also highly effective in practice.

</details>


### [109] [On the Fragility of Contribution Score Computation in Federated Learning](https://arxiv.org/abs/2509.19921)
*Balazs Pejo,Marcell Frank,Krisztian Varga,Peter Veliczky*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中贡献评估机制的脆弱性，揭示了贡献分数容易受到架构敏感性和恶意操纵两种因素的显著扭曲。


<details>
  <summary>Details</summary>
Motivation: 贡献评估是联邦学习确保公平性和激励参与的关键机制，但其可靠性尚未得到充分研究。本文旨在揭示贡献分数评估的脆弱性，为构建更稳健的评估方案提供基础。

Method: 通过Flower框架在多种数据集和模型架构上进行广泛实验，分析不同模型聚合方法对贡献分数的影响，并研究投毒攻击下恶意参与者如何策略性操纵模型更新来扭曲贡献评估。

Result: 实验结果表明，聚合方法的选择和攻击者的存在都是扭曲贡献分数的有效途径，高级聚合技术（如处理不可靠或多样化客户端的方法）会无意中显著改变最终分数。

Conclusion: 联邦学习中的贡献评估机制存在严重脆弱性，迫切需要开发更稳健的评估方案来应对架构敏感性和恶意操纵带来的挑战。

Abstract: This paper investigates the fragility of contribution evaluation in federated
learning, a critical mechanism for ensuring fairness and incentivizing
participation. We argue that contribution scores are susceptible to significant
distortions from two fundamental perspectives: architectural sensitivity and
intentional manipulation. First, we explore how different model aggregation
methods impact these scores. While most research assumes a basic averaging
approach, we demonstrate that advanced techniques, including those designed to
handle unreliable or diverse clients, can unintentionally yet significantly
alter the final scores. Second, we explore vulnerabilities posed by poisoning
attacks, where malicious participants strategically manipulate their model
updates to inflate their own contribution scores or reduce the importance of
other participants. Through extensive experiments across diverse datasets and
model architectures, implemented within the Flower framework, we rigorously
show that both the choice of aggregation method and the presence of attackers
are potent vectors for distorting contribution scores, highlighting a critical
need for more robust evaluation schemes.

</details>


### [110] [Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches](https://arxiv.org/abs/2509.19924)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: 本文研究了基础模型在强化学习稀疏奖励环境中的零样本探索能力，发现视觉语言模型（VLM）虽然能从视觉输入推断高级目标，但在精确低级控制方面存在"知-行差距"，并提出了一个混合框架来改善早期样本效率。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型（特别是LLMs和VLMs）在经典RL基准测试中作为零样本探索代理的能力，特别是在稀疏奖励设置下的表现。

Method: 在多臂老虎机、Gridworlds和稀疏奖励Atari游戏中测试LLMs和VLMs的零样本探索能力，并研究了一个简单的在线混合框架来分析VLM指导的潜力。

Result: VLMs能够从视觉输入推断高级目标，但在精确低级控制方面表现不佳。在理想化设置中，VLM指导能显著提高早期样本效率。

Conclusion: 基础模型更适合用于指导探索而非端到端控制，VLM指导在改善早期样本效率方面具有潜力，但需要解决"知-行差距"问题。

Abstract: Exploration in reinforcement learning (RL) remains challenging, particularly
in sparse-reward settings. While foundation models possess strong semantic
priors, their capabilities as zero-shot exploration agents in classic RL
benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed
bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our
investigation reveals a key limitation: while VLMs can infer high-level
objectives from visual input, they consistently fail at precise low-level
control: the "knowing-doing gap". To analyze a potential bridge for this gap,
we investigate a simple on-policy hybrid framework in a controlled, best-case
scenario. Our results in this idealized setting show that VLM guidance can
significantly improve early-stage sample efficiency, providing a clear analysis
of the potential and constraints of using foundation models to guide
exploration rather than for end-to-end control.

</details>


### [111] [MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection](https://arxiv.org/abs/2509.19926)
*Jana Sweidan,Mounim A. El-Yacoubi,Nasredine Semmar*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的方法，通过提示大型语言模型从语音转录本中检测阿尔茨海默病。研究在ADReSS数据集上评估了两种变体：基于MMSE代理的提示和推理增强提示，均达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 探索无需训练的提示方法在阿尔茨海默病检测中的应用，特别是研究零样本和少样本提示的效果，并首次将概率与MMSE评分关联，提高可解释性。

Method: 使用ADReSS数据集，研究零样本和少样本提示。提出两种方法：(i) MMSE-代理提示：少样本示例携带基于MMSE评分的概率；(ii) 推理增强提示：使用多模态LLM生成包含推理的少样本示例池。

Result: MMSE-代理提示达到0.82准确率和0.86 AUC；推理增强提示达到0.82准确率和0.83 AUC，均创造了该数据集上的最佳提示性能。

Conclusion: 提示方法在阿尔茨海默病检测中有效，特别是将概率与MMSE评分关联和多模态构造提高了方法的可解释性和性能。

Abstract: Prompting large language models is a training-free method for detecting
Alzheimer's disease from speech transcripts. Using the ADReSS dataset, we
revisit zero-shot prompting and study few-shot prompting with a class-balanced
protocol using nested interleave and a strict schema, sweeping up to 20
examples per class. We evaluate two variants achieving state-of-the-art
prompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a
probability anchored to Mini-Mental State Examination bands via a deterministic
mapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii)
Reasoning-augmented Prompting: few-shot examples pool is generated with a
multimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript,
and MMSE to output a reasoning and MMSE-aligned probability; evaluation remains
transcript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this
is the first ADReSS study to anchor elicited probabilities to MMSE and to use
multimodal construction to improve interpretability.

</details>


### [112] [TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees](https://arxiv.org/abs/2509.19927)
*Emmanouil Panagiotou,Benoît Ronval,Arjun Roy,Ludwig Bothmann,Bernd Bischl,Siegfried Nijssen,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: TABFAIRGDT是一种使用自回归决策树生成公平合成表格数据的新方法，通过软叶重采样技术减少偏见，在保持预测性能的同时实现更好的公平性-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型经常从训练数据中继承偏见，生成模型是缓解数据层面偏见的有前景方法。现有方法大多依赖深度架构，但简单模型在处理表格数据时可能更有效。

Method: 提出TABFAIRGDT方法，使用自回归决策树生成公平合成数据，采用软叶重采样技术调整决策树输出来减少偏见。该方法是非参数的，能有效处理混合特征类型的复杂关系。

Result: 在基准公平性数据集上评估显示，TABFAIRGDT优于最先进的深度生成模型，实现了更好的公平性-效用权衡和更高的合成数据质量。相比最快基线平均提速72%，能在标准CPU上1秒内生成中等规模数据集的公平合成数据。

Conclusion: TABFAIRGDT是一种轻量级、高效且CPU兼容的解决方案，无需数据预处理，非常适合现实世界中公平性敏感的应用场景。

Abstract: Ensuring fairness in machine learning remains a significant challenge, as
models often inherit biases from their training data. Generative models have
recently emerged as a promising approach to mitigate bias at the data level
while preserving utility. However, many rely on deep architectures, despite
evidence that simpler models can be highly effective for tabular data. In this
work, we introduce TABFAIRGDT, a novel method for generating fair synthetic
tabular data using autoregressive decision trees. To enforce fairness, we
propose a soft leaf resampling technique that adjusts decision tree outputs to
reduce bias while preserving predictive performance. Our approach is
non-parametric, effectively capturing complex relationships between mixed
feature types, without relying on assumptions about the underlying data
distributions. We evaluate TABFAIRGDT on benchmark fairness datasets and
demonstrate that it outperforms state-of-the-art (SOTA) deep generative models,
achieving better fairness-utility trade-off for downstream tasks, as well as
higher synthetic data quality. Moreover, our method is lightweight, highly
efficient, and CPU-compatible, requiring no data pre-processing. Remarkably,
TABFAIRGDT achieves a 72% average speedup over the fastest SOTA baseline across
various dataset sizes, and can generate fair synthetic data for medium-sized
datasets (10 features, 10K samples) in just one second on a standard CPU,
making it an ideal solution for real-world fairness-sensitive applications.

</details>


### [113] [How deep is your network? Deep vs. shallow learning of transfer operators](https://arxiv.org/abs/2509.19930)
*Mohammad Tabish,Benedict Leimkuhler,Stefan Klus*

Main category: cs.LG

TL;DR: 提出了一种名为RaNNDy的随机神经网络方法，用于从数据中学习转移算子及其谱分解。该方法通过随机选择隐藏层权重、仅训练输出层，显著减少训练时间和资源消耗，同时避免深度学习的常见问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在训练转移算子时存在训练时间长、资源消耗大、对超参数敏感以及收敛速度慢等问题。RaNNDy旨在解决这些问题，提供一种更高效且稳定的学习框架。

Method: 使用随机神经网络，隐藏层权重随机选择，仅训练输出层。该方法能够计算输出层的闭式解，直接表示算子的特征函数，并通过集成学习估计谱特性的不确定性。

Result: RaNNDy在保持精度的同时显著减少了训练时间和资源消耗。该方法适用于多种动态算子，包括Koopman算子、Perron-Frobenius算子和Schrödinger算子，并在随机动力系统、蛋白质折叠过程和量子谐振子等数值示例中验证了其有效性。

Conclusion: RaNNDy提供了一种高效、稳定的学习转移算子及其谱分解的方法，具有快速训练、低资源消耗和避免超参数敏感性的优势，但在某些情况下可能存在局限性。

Abstract: We propose a randomized neural network approach called RaNNDy for learning
transfer operators and their spectral decompositions from data. The weights of
the hidden layers of the neural network are randomly selected and only the
output layer is trained. The main advantage is that without a noticeable
reduction in accuracy, this approach significantly reduces the training time
and resources while avoiding common problems associated with deep learning such
as sensitivity to hyperparameters and slow convergence. Additionally, the
proposed framework allows us to compute a closed-form solution for the output
layer which directly represents the eigenfunctions of the operator. Moreover,
it is possible to estimate uncertainties associated with the computed spectral
properties via ensemble learning. We present results for different dynamical
operators, including Koopman and Perron-Frobenius operators, which have
important applications in analyzing the behavior of complex dynamical systems,
and the Schr\"odinger operator. The numerical examples, which highlight the
strengths but also weaknesses of the proposed framework, include several
stochastic dynamical systems, protein folding processes, and the quantum
harmonic oscillator.

</details>


### [114] [Learnable Sampler Distillation for Discrete Diffusion Models](https://arxiv.org/abs/2509.19962)
*Feiyang Fu,Tongxian Guo,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: 提出LSD和LSD+方法，通过可学习采样器蒸馏技术加速离散扩散模型采样，在减少采样步数的同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型采样效率低，需要大量采样步数，而增大步长会放大解码误差和离散化误差，导致生成质量显著下降

Method: 使用蒸馏方法，让少步数的学生采样器学习与多步数教师采样器的中间分数轨迹对齐，通过优化可学习采样系数自适应调整采样动态；LSD+还学习非均匀时间调度

Result: 在文本生成、图像生成和合成任务上的实验表明，该方法优于现有采样器，用更少采样步数实现更高采样质量

Conclusion: LSD和LSD+方法有效解决了离散扩散模型采样效率问题，为实际应用提供了可行的加速方案

Abstract: Discrete diffusion models (DDMs) have shown powerful generation ability for
discrete data modalities like text and molecules. However, their practical
application is hindered by inefficient sampling, requiring a large number of
sampling steps. Accelerating DDMs by using larger step sizes typically
introduces significant problems in generation quality, as it amplifies the
impact of both the compounding decoding error due to factorized predictions and
discretization error from numerical approximations, leading to a significant
decrease in sampling quality. To address these challenges, we propose learnable
sampler distillation (LSD), a novel approach to train fast and high-fidelity
samplers for DDMs. LSD employs a distillation approach where a student sampler
with a few steps learns to align its intermediate score trajectory with that of
a high-quality teacher sampler with numerous steps. This alignment is achieved
by optimizing learnable sampler coefficients that adaptively adjust sampling
dynamics. Additionally, we further propose LSD+, which also learns time
schedules that allocate steps non-uniformly. Experiments across text
generation, image generation, and synthetic tasks demonstrate that our proposed
approaches outperform existing samplers for DDMs, achieving substantially
higher sampling quality with significantly fewer sampling steps. Our code is
available at
\href{https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}.

</details>


### [115] [From Samples to Scenarios: A New Paradigm for Probabilistic Forecasting](https://arxiv.org/abs/2509.19975)
*Xilin Dai,Zhijian Xu,Wanxu Cai,Qiang Xu*

Main category: cs.LG

TL;DR: 提出了概率场景新范式，用{场景,概率}对替代传统采样方法，避免了蒙特卡洛近似的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统概率时间序列预测模型依赖采样方法存在固有缺陷：缺乏显式概率、覆盖不足、计算成本高。

Method: 提出TimePrism模型，仅包含三个并行线性层，通过直接生成有限个{场景,概率}对来学习表示可能的场景及其对应概率。

Result: TimePrism在5个基准数据集上，两个评估指标中获得了10项中的9项最先进结果。

Conclusion: 概率场景范式展示了超越采样的预测研究潜力，通过重新定义学习目标，从建模连续概率空间转向学习表示可能场景及其概率。

Abstract: Most state-of-the-art probabilistic time series forecasting models rely on
sampling to represent future uncertainty. However, this paradigm suffers from
inherent limitations, such as lacking explicit probabilities, inadequate
coverage, and high computational costs. In this work, we introduce
\textbf{Probabilistic Scenarios}, an alternative paradigm designed to address
the limitations of sampling. It operates by directly producing a finite set of
\{Scenario, Probability\} pairs, thus avoiding Monte Carlo-like approximation.
To validate this paradigm, we propose \textbf{TimePrism}, a simple model
composed of only three parallel linear layers. Surprisingly, TimePrism achieves
9 out of 10 state-of-the-art results across five benchmark datasets on two
metrics. The effectiveness of our paradigm comes from a fundamental reframing
of the learning objective. Instead of modeling an entire continuous probability
space, the model learns to represent a set of plausible scenarios and
corresponding probabilities. Our work demonstrates the potential of the
Probabilistic Scenarios paradigm, opening a promising research direction in
forecasting beyond sampling.

</details>


### [116] [Faster Than SVD, Smarter Than SGD: The OPLoRA Alternating Update](https://arxiv.org/abs/2509.19977)
*Abdulla Jasem Almansoori,Maria Ivanova,Andrey Veprikov,Aleksandr Beznosikov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: OPLoRA是一种内存高效的优化器，通过将LoRA优化转化为可解释的子问题并使用交替最小二乘法更新，在1-2步内就能接近截断SVD的性能，同时显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA通过低秩更新大幅减少了可训练参数和内存使用，但与完整训练（SVDLoRA）之间仍存在性能差距，表明LoRA方法还有改进空间。

Method: 将LoRA优化转化为可解释的子问题，使用交替最小二乘法进行高效更新，1-2步交替更新即可接近截断SVD效果。支持动量机制，内存预算仅为LoRA参数的3倍。

Result: 在线性任务、MNIST、CIFAR-100和RoBERTa-base（MNLI）上，OPLoRA始终接近SVDLoRA的性能，同时使用显著更少的内存。

Conclusion: OPLoRA成功缩小了LoRA微调与完整训练之间的性能差距，提供了一种内存高效且性能优越的优化方案。

Abstract: Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank
updates on top of frozen weights, dramatically reducing trainable parameters
and memory. However, there is still a gap between full training with low-rank
projections (SVDLoRA) and LoRA fine-tuning, indicating that LoRA steps can be
further improved. In this study, we propose OPLoRA, a memory-efficient
optimizer that closes this gap by casting LoRA optimization as an interpretable
sub-problem and solving it efficiently with alternating least squares updates,
where 1-2 alternating steps are empirically found to be sufficient to closely
match truncated SVD without ever forming the full matrix. We also retrieve the
recently proposed preconditioning methods for LoRA as a special case. OPLoRA
supports momentum by maintaining a low-rank estimate using the same subroutine
(LoRSum) for computing the step, with a memory budget of 3 times the number of
LoRA parameters (i.e., same as Adam). We also propose an experimental scaled
variant that uses the K-FAC metric, which could be of interest. Across a linear
task, MNIST, CIFAR-100, and RoBERTa-base (MNLI), OPLoRA consistently approaches
SVDLoRA's performance using significantly less memory.

</details>


### [117] [RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis](https://arxiv.org/abs/2509.19980)
*Haolin Li,Tianjie Dai,Zhe Chen,Siyuan Du,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.LG

TL;DR: 提出了RAD框架，通过显式注入外部知识来增强多模态医学诊断模型，在四个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI医学研究主要依赖模型参数中隐式编码的知识，忽视了不同下游任务所需的特定知识，无法满足临床诊断的严格指南要求。

Method: RAD框架包含三个关键机制：从多个医学来源检索和精炼疾病中心知识、指南增强的对比损失约束多模态特征与指南知识的潜在距离、以及使用指南作为查询的双Transformer解码器来引导跨模态融合。

Result: 在四个不同解剖结构的数据集上进行了广泛评估，证明了RAD的泛化能力，达到了最先进的性能水平，并使模型能更精确地关注异常区域和关键指标。

Conclusion: RAD框架能够确保基于证据的可信诊断，与临床诊断工作流程保持一致，并为多模态诊断模型的可解释性提供了量化评估标准。

Abstract: Clinical diagnosis is a highly specialized discipline requiring both domain
expertise and strict adherence to rigorous guidelines. While current AI-driven
medical research predominantly focuses on knowledge graphs or natural text
pretraining paradigms to incorporate medical knowledge, these approaches
primarily rely on implicitly encoded knowledge within model parameters,
neglecting task-specific knowledge required by diverse downstream tasks. To
address this limitation, we propose Retrieval-Augmented Diagnosis (RAD), a
novel framework that explicitly injects external knowledge into multimodal
models directly on downstream tasks. Specifically, RAD operates through three
key mechanisms: retrieval and refinement of disease-centered knowledge from
multiple medical sources, a guideline-enhanced contrastive loss that constrains
the latent distance between multi-modal features and guideline knowledge, and
the dual transformer decoder that employs guidelines as queries to steer
cross-modal fusion, aligning the models with clinical diagnostic workflows from
guideline acquisition to feature extraction and decision-making. Moreover,
recognizing the lack of quantitative evaluation of interpretability for
multimodal diagnostic models, we introduce a set of criteria to assess the
interpretability from both image and text perspectives. Extensive evaluations
across four datasets with different anatomies demonstrate RAD's
generalizability, achieving state-of-the-art performance. Furthermore, RAD
enables the model to concentrate more precisely on abnormal regions and
critical indicators, ensuring evidence-based, trustworthy diagnosis. Our code
is available at https://github.com/tdlhl/RAD.

</details>


### [118] [Pi-Transformer: A Physics-informed Attention Mechanism for Time Series Anomaly Detection](https://arxiv.org/abs/2509.19985)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: Pi-Transformer是一种物理信息Transformer模型，通过双注意力路径（数据驱动序列注意力和平滑演化的先验注意力）来检测多元时间序列中的异常，特别擅长检测时序和相位破坏异常。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中的异常通常源于时间上下文和跨通道协调，而非孤立离群点。现有方法往往忽略了物理先验知识，导致检测性能受限。

Method: 提出双注意力机制：数据驱动序列注意力和编码时间不变量的先验注意力。训练时结合重构目标和散度项，鼓励两种注意力一致但保持差异。推理时融合对齐加权重构信号和失配信号。

Result: 在五个基准数据集（SMD、MSL、SMAP、SWaT和PSM）上达到最先进或极具竞争力的F1分数，在时序和相位破坏异常检测方面表现突出。

Conclusion: 将物理信息先验嵌入注意力机制为复杂多元系统中的异常检测提供了一种校准且鲁棒的方法。

Abstract: Anomalies in multivariate time series often arise from temporal context and
cross-channel coordination rather than isolated outliers. We present
Pi-Transformer, a physics-informed transformer with two attention pathways: a
data-driven series attention and a smoothly evolving prior attention that
encodes temporal invariants such as scale-related self-similarity and phase
synchrony. The prior acts as a stable reference that calibrates reconstruction
error. During training, we pair a reconstruction objective with a divergence
term that encourages agreement between the two attentions while keeping them
meaningfully distinct; the prior is regularised to evolve smoothly and is
lightly distilled towards dataset-level statistics. At inference, the model
combines an alignment-weighted reconstruction signal (Energy) with a mismatch
signal that highlights timing and phase disruptions, and fuses them into a
single score for detection. Across five benchmarks (SMD, MSL, SMAP, SWaT, and
PSM), Pi-Transformer achieves state-of-the-art or highly competitive F1, with
particular strength on timing and phase-breaking anomalies. Case analyses show
complementary behaviour of the two streams and interpretable detections around
regime changes. Embedding physics-informed priors into attention yields a
calibrated and robust approach to anomaly detection in complex multivariate
systems. Code is publicly available at this GitHub
repository\footnote{https://github.com/sepehr-m/Pi-Transformer}.

</details>


### [119] [Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation](https://arxiv.org/abs/2509.20008)
*Raphael Simon,Pieter Libin,Wim Mees*

Main category: cs.LG

TL;DR: 该论文研究了在渗透测试中应用强化学习解决部分可观测性问题，通过比较PPO算法的不同变体在主机网络中的表现，发现历史聚合方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 渗透测试作为网络安全攻击模拟，面临部分可观测性挑战，这破坏了马尔可夫决策过程的马尔可夫性质。研究旨在开发更鲁棒和可迁移的策略，以应对现实世界复杂环境。

Method: 使用vanilla PPO作为基线，比较了多种PPO变体，包括帧堆叠、历史信息增强观测、循环神经网络和基于Transformer的架构，在不同规模主机网络上进行系统性实证分析。

Result: 研究发现历史聚合方法比其他方法收敛速度快三倍，通过手动检查学习策略发现了超越定量结果的清晰区别和洞见。

Conclusion: 渗透测试任务极大受益于历史聚合方法，这为开发适用于现实世界复杂环境的可靠强化学习策略提供了重要指导。

Abstract: Penetration testing, the simulation of cyberattacks to identify security
vulnerabilities, presents a sequential decision-making problem well-suited for
reinforcement learning (RL) automation. Like many applications of RL to
real-world problems, partial observability presents a major challenge, as it
invalidates the Markov property present in Markov Decision Processes (MDPs).
Partially Observable MDPs require history aggregation or belief state
estimation to learn successful policies. We investigate stochastic, partially
observable penetration testing scenarios over host networks of varying size,
aiming to better reflect real-world complexity through more challenging and
representative benchmarks. This approach leads to the development of more
robust and transferable policies, which are crucial for ensuring reliable
performance across diverse and unpredictable real-world environments. Using
vanilla Proximal Policy Optimization (PPO) as a baseline, we compare a
selection of PPO variants designed to mitigate partial observability, including
frame-stacking, augmenting observations with historical information, and
employing recurrent or transformer-based architectures. We conduct a systematic
empirical analysis of these algorithms across different host network sizes. We
find that this task greatly benefits from history aggregation. Converging three
times faster than other approaches. Manual inspection of the learned policies
by the algorithms reveals clear distinctions and provides insights that go
beyond quantitative results.

</details>


### [120] [Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations](https://arxiv.org/abs/2509.20048)
*Rami Zewail*

Main category: cs.LG

TL;DR: 提出了一种新颖的混合框架DACL，结合扩散模型和监督对比学习，用于生物信号的鲁棒表示学习。该方法在PhysioNet 2017 ECG数据集上取得了0.7815的AUROC。


<details>
  <summary>Details</summary>
Motivation: 生物信号表示学习面临数据增强设计的挑战，传统方法难以捕捉生理数据的复杂变化。

Method: DACL框架在轻量级VAE创建的潜空间上运行，利用扩散前向过程作为数据增强技术生成噪声视图，使用U-Net风格编码器和监督对比目标学习表示。

Result: 在PhysioNet 2017 ECG数据集上获得0.7815的竞争性AUROC。

Conclusion: 这项工作建立了一个新的表示学习范式，利用扩散过程驱动对比目标，创建具有强类别可分性的噪声不变嵌入。

Abstract: Learning robust representations for biosignals is often hampered by the
challenge of designing effective data augmentations.Traditional methods can
fail to capture the complex variations inherent in physiological data. Within
this context, we propose a novel hybrid framework, Diffusion-Augmented
Contrastive Learning (DACL), that fuses concepts from diffusion models and
supervised contrastive learning. The DACL framework operates on a latent space
created by a lightweight Variational Autoencoder (VAE) trained on our novel
Scattering Transformer (ST) features [12]. It utilizes the diffusion forward
process as a principled data augmentation technique to generate multiple noisy
views of these latent embeddings. A U-Net style encoder is then trained with a
supervised contrastive objective to learn a representation that balances class
discrimination with robustness to noise across various diffusion time steps. We
evaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,
achieving a competitive AUROC of 0.7815. This work establishes a new paradigm
for representation learning by using the diffusion process itself to drive the
contrastive objective, creating noise-invariant embeddings that demonstrate a
strong foundation for class separability.

</details>


### [121] [One Filters All: A Generalist Filter for State Estimation](https://arxiv.org/abs/2509.20051)
*Shiqi Liu,Wenhan Cao,Chang Liu,Zeyu He,Tianyi Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: LLM-Filter是一个利用大型语言模型进行状态估计的通用滤波框架，通过将噪声观测与文本原型嵌入，在动态系统中实现状态估计。


<details>
  <summary>Details</summary>
Motivation: 动态系统中的隐藏状态估计（最优滤波）是科学和工程领域的长期问题，需要更有效的解决方案。

Method: 设计LLM-Filter框架，利用预训练LLM的推理知识，通过System-as-Prompt（SaP）提示结构使LLM理解估计任务，实现模态对齐。

Result: 在经典动态系统实验中，LLM-Filter优于最先进的学习方法，展现出卓越的泛化能力，能在变化或未见环境中准确执行滤波任务，且精度随模型规模和训练时间增加而提升。

Conclusion: LLM-Filter显示出作为滤波基础模型的潜力，其缩放定律行为表明随着模型规模扩大性能会进一步提升。

Abstract: Estimating hidden states in dynamical systems, also known as optimal
filtering, is a long-standing problem in various fields of science and
engineering. In this paper, we introduce a general filtering framework,
\textbf{LLM-Filter}, which leverages large language models (LLMs) for state
estimation by embedding noisy observations with text prototypes. In various
experiments for classical dynamical systems, we find that first, state
estimation can significantly benefit from the reasoning knowledge embedded in
pre-trained LLMs. By achieving proper modality alignment with the frozen LLM,
LLM-Filter outperforms the state-of-the-art learning-based approaches. Second,
we carefully design the prompt structure, System-as-Prompt (SaP), incorporating
task instructions that enable the LLM to understand the estimation tasks.
Guided by these prompts, LLM-Filter exhibits exceptional generalization,
capable of performing filtering tasks accurately in changed or even unseen
environments. We further observe a scaling-law behavior in LLM-Filter, where
accuracy improves with larger model sizes and longer training times. These
findings make LLM-Filter a promising foundation model of filtering.

</details>


### [122] [You Only Measure Once: On Designing Single-Shot Quantum Machine Learning Models](https://arxiv.org/abs/2509.20090)
*Chen-Yu Liu,Leonardo Placidi,Kuan-Cheng Chen,Samuel Yen-Chi Chen,Gabriel Matos*

Main category: cs.LG

TL;DR: Yomo是一种量子机器学习方法，通过单次测量实现准确推理，显著降低测量成本和推理时间


<details>
  <summary>Details</summary>
Motivation: 传统量子机器学习模型依赖大量重复测量，导致推理成本高、时间长，而量子硬件访问通常按测量次数计费

Method: 用概率聚合机制替代Pauli期望值输出，引入鼓励锐利预测的损失函数

Result: 理论分析显示Yomo避免了基于期望值模型的测量缩放限制，在MNIST和CIFAR-10实验中优于基线方法

Conclusion: Yomo通过实现准确单次测量推理，大幅降低了QML部署的财务和计算成本，促进了QML的实际应用

Abstract: Quantum machine learning (QML) models conventionally rely on repeated
measurements (shots) of observables to obtain reliable predictions. This
dependence on large shot budgets leads to high inference cost and time
overhead, which is particularly problematic as quantum hardware access is
typically priced proportionally to the number of shots. In this work we propose
You Only Measure Once (Yomo), a simple yet effective design that achieves
accurate inference with dramatically fewer measurements, down to the
single-shot regime. Yomo replaces Pauli expectation-value outputs with a
probability aggregation mechanism and introduces loss functions that encourage
sharp predictions. Our theoretical analysis shows that Yomo avoids the
shot-scaling limitations inherent to expectation-based models, and our
experiments on MNIST and CIFAR-10 confirm that Yomo consistently outperforms
baselines across different shot budgets and under simulations with depolarizing
channels. By enabling accurate single-shot inference, Yomo substantially
reduces the financial and computational costs of deploying QML, thereby
lowering the barrier to practical adoption of QML.

</details>


### [123] [Incomplete Data, Complete Dynamics: A Diffusion Approach](https://arxiv.org/abs/2509.20098)
*Zihan Zhou,Chenguang Wang,Hongyi Ye,Yongtao Guan,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的框架，用于从非完整训练样本中学习物理系统，通过将样本划分为观察上下文和未观察查询部分，训练条件扩散模型来重建缺失数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界观测数据通常不完整且采样不规则，现有数据驱动方法面临重大挑战，需要能够处理任意观测模式的准确插补方法。

Method: 设计分割策略将每个样本划分为观察上下文和未观察查询组件，训练条件扩散模型基于可用上下文重建缺失的查询部分，无需完整数据监督。

Result: 理论分析表明该方法在温和正则条件下渐近收敛到真实完整生成过程；实证显示在流体流动和天气系统等基准测试中显著优于现有基线，在有限和不规则观测条件下表现尤其突出。

Conclusion: 该方法为学习和插补部分观测的物理动力学提供了一种理论上有原则的有效方法。

Abstract: Learning physical dynamics from data is a fundamental challenge in machine
learning and scientific modeling. Real-world observational data are inherently
incomplete and irregularly sampled, posing significant challenges for existing
data-driven approaches. In this work, we propose a principled diffusion-based
framework for learning physical systems from incomplete training samples. To
this end, our method strategically partitions each such sample into observed
context and unobserved query components through a carefully designed splitting
strategy, then trains a conditional diffusion model to reconstruct the missing
query portions given available contexts. This formulation enables accurate
imputation across arbitrary observation patterns without requiring complete
data supervision. Specifically, we provide theoretical analysis demonstrating
that our diffusion training paradigm on incomplete data achieves asymptotic
convergence to the true complete generative process under mild regularity
conditions. Empirically, we show that our method significantly outperforms
existing baselines on synthetic and real-world physical dynamics benchmarks,
including fluid flows and weather systems, with particularly strong performance
in limited and irregular observation regimes. These results demonstrate the
effectiveness of our theoretically principled approach for learning and
imputing partially observed dynamics.

</details>


### [124] [Discovering Association Rules in High-Dimensional Small Tabular Data](https://arxiv.org/abs/2509.20113)
*Erkan Karabulut,Daniel Daza,Paul Groth,Victoria Degeler*

Main category: cs.LG

TL;DR: 本文针对高维表格数据中的关联规则挖掘（ARM）问题，提出了基于表格基础模型的Aerial+微调方法，解决了高维低数据场景下的规则爆炸和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统ARM方法在高维设置下存在规则爆炸和计算开销大的问题，而神经符号方法如Aerial+虽然能处理高维数据，但在低数据量情况下性能下降。本文旨在解决高维低数据场景下的ARM挑战。

Method: 提出了两种基于表格基础模型的Aerial+微调方法，专门针对高维低数据设置（如生物医学领域的基因表达数据，约18k特征和50个样本）进行优化。

Result: 实验表明，Aerial+在五个真实数据集上比最先进的算法和神经符号基线方法扩展性好1-2个数量级，提出的微调方法显著提高了规则质量。

Conclusion: 所提出的方法有效解决了高维低数据场景下的ARM问题，为生物医学等领域的知识发现和可解释机器学习提供了实用解决方案。

Abstract: Association Rule Mining (ARM) aims to discover patterns between features in
datasets in the form of propositional rules, supporting both knowledge
discovery and interpretable machine learning in high-stakes decision-making.
However, in high-dimensional settings, rule explosion and computational
overhead render popular algorithmic approaches impractical without effective
search space reduction, challenges that propagate to downstream tasks.
Neurosymbolic methods, such as Aerial+, have recently been proposed to address
the rule explosion in ARM. While they tackle the high dimensionality of the
data, they also inherit limitations of neural networks, particularly reduced
performance in low-data regimes.
  This paper makes three key contributions to association rule discovery in
high-dimensional tabular data. First, we empirically show that Aerial+ scales
one to two orders of magnitude better than state-of-the-art algorithmic and
neurosymbolic baselines across five real-world datasets. Second, we introduce
the novel problem of ARM in high-dimensional, low-data settings, such as gene
expression data from the biomedicine domain with around 18k features and 50
samples. Third, we propose two fine-tuning approaches to Aerial+ using tabular
foundation models. Our proposed approaches are shown to significantly improve
rule quality on five real-world datasets, demonstrating their effectiveness in
low-data, high-dimensional scenarios.

</details>


### [125] [Beyond Slater's Condition in Online CMDPs with Stochastic and Adversarial Constraints](https://arxiv.org/abs/2509.20114)
*Francesco Emanuele Stradi,Eleonora Fidelia Chiefari,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 本文提出了一种新的在线约束马尔可夫决策过程算法，在随机和对抗性约束下都显著改进了现有最佳算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的最佳算法在处理约束马尔可夫决策过程时存在局限性，特别是在没有严格可行解的情况下表现不佳。本文旨在开发一个更强大的算法，能够在随机和对抗性约束环境下都取得更好的性能。

Method: 提出了一种新颖的在线算法，能够在随机约束下实现次线性遗憾和约束违反，且不依赖Slater条件。在对抗性约束下，算法确保次线性约束违反，并实现相对于无约束最优解的次线性α-遗憾。

Result: 在随机约束下达到O(√T)的遗憾和约束违反，在对抗性约束下实现次线性约束违反和α-遗憾。通过合成实验验证了算法的实际有效性。

Conclusion: 该算法在随机和对抗性约束环境下都显著优于现有最佳算法，特别是在处理无严格可行解的情况时表现出色，为约束马尔可夫决策过程提供了更强大的解决方案。

Abstract: We study \emph{online episodic Constrained Markov Decision Processes} (CMDPs)
under both stochastic and adversarial constraints. We provide a novel algorithm
whose guarantees greatly improve those of the state-of-the-art
best-of-both-worlds algorithm introduced by Stradi et al. (2025). In the
stochastic regime, \emph{i.e.}, when the constraints are sampled from fixed but
unknown distributions, our method achieves $\widetilde{\mathcal{O}}(\sqrt{T})$
regret and constraint violation without relying on Slater's condition, thereby
handling settings where no strictly feasible solution exists. Moreover, we
provide guarantees on the stronger notion of \emph{positive} constraint
violation, which does not allow to recover from large violation in the early
episodes by playing strictly safe policies. In the adversarial regime,
\emph{i.e.}, when the constraints may change arbitrarily between episodes, our
algorithm ensures sublinear constraint violation without Slater's condition,
and achieves sublinear $\alpha$-regret with respect to the \emph{unconstrained}
optimum, where $\alpha$ is a suitably defined multiplicative approximation
factor. We further validate our results through synthetic experiments, showing
the practical effectiveness of our algorithm.

</details>


### [126] [Probability Signature: Bridging Data Semantics and Embedding Structure in Language Models](https://arxiv.org/abs/2509.20124)
*Junjie Yao,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 本文通过数据分布解释语言模型嵌入空间的结构形成机制，提出概率签名反映语义关系，并通过实验和理论分析揭示这些签名对嵌入结构的显著影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型的嵌入空间被认为能捕捉语义关系，但其结构形成机制尚不清楚。本文旨在理解数据分布如何指导嵌入结构的形成。

Method: 提出概率签名反映语义关系，在复合加法任务上使用线性模型和前馈网络进行实验，结合梯度流动力学理论分析，并推广到在Pile语料子集上训练Qwen2.5架构的大型语言模型。

Result: 概率签名与嵌入结构高度一致，特别是在捕捉嵌入之间的强成对相似性方面。实验证实数据分布对嵌入结构形成有显著影响。

Conclusion: 揭示了数据分布指导嵌入结构形成的机制，建立了嵌入组织与语义模式关系的新理解，为理解语言模型内部表示提供了新视角。

Abstract: The embedding space of language models is widely believed to capture the
semantic relationships; for instance, embeddings of digits often exhibit an
ordered structure that corresponds to their natural sequence. However, the
mechanisms driving the formation of such structures remain poorly understood.
In this work, we interpret the embedding structures via the data distribution.
We propose a set of probability signatures that reflect the semantic
relationships among tokens. Through experiments on the composite addition tasks
using the linear model and feedforward network, combined with theoretical
analysis of gradient flow dynamics, we reveal that these probability signatures
significantly influence the embedding structures. We further generalize our
analysis to large language models (LLMs) by training the Qwen2.5 architecture
on the subsets of the Pile corpus. Our results show that the probability
signatures are faithfully aligned with the embedding structures, particularly
in capturing strong pairwise similarities among embeddings. Our work uncovers
the mechanism of how data distribution guides the formation of embedding
structures, establishing a novel understanding of the relationship between
embedding organization and semantic patterns.

</details>


### [127] [Generative Model Inversion Through the Lens of the Manifold Hypothesis](https://arxiv.org/abs/2509.20177)
*Xiong Peng,Bo Han,Fengfei Yu,Tongliang Liu,Feng Liu,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 本文分析了生成式模型反演攻击的有效性原因，发现关键在于损失梯度与生成器流形的对齐程度，并提出新的训练目标和训练无关方法来增强这种对齐，从而改进反演攻击效果。


<details>
  <summary>Details</summary>
Motivation: 探索生成式模型反演攻击成功的原因，特别是为什么生成对抗网络能够产生高质量的重建结果。通过分析梯度发现，生成式反演通过将梯度投影到生成器流形的切空间来隐式去噪。

Method: 1) 分析反演损失相对于合成输入的梯度特性；2) 提出假设：当模型损失梯度与生成器流形更对齐时，模型更容易受到反演攻击；3) 设计新的训练目标来显式促进这种对齐；4) 提出训练无关的方法在反演过程中增强梯度-流形对齐。

Result: 实证测量显示，标准监督训练模型中损失梯度往往与数据流形存在较大角度偏差。验证了假设的正确性，新方法相比最先进的生成式反演攻击取得了持续改进。

Conclusion: 模型对反演攻击的脆弱性与损失梯度和生成器流形的对齐程度直接相关。通过促进这种对齐，可以显著提高反演攻击的效果，这为理解和防御模型反演攻击提供了新视角。

Abstract: Model inversion attacks (MIAs) aim to reconstruct class-representative
samples from trained models. Recent generative MIAs utilize generative
adversarial networks to learn image priors that guide the inversion process,
yielding reconstructions with high visual quality and strong fidelity to the
private training data. To explore the reason behind their effectiveness, we
begin by examining the gradients of inversion loss with respect to synthetic
inputs, and find that these gradients are surprisingly noisy. Further analysis
reveals that generative inversion implicitly denoises these gradients by
projecting them onto the tangent space of the generator manifold, filtering out
off-manifold components while preserving informative directions aligned with
the manifold. Our empirical measurements show that, in models trained with
standard supervision, loss gradients often exhibit large angular deviations
from the data manifold, indicating poor alignment with class-relevant
directions. This observation motivates our central hypothesis: models become
more vulnerable to MIAs when their loss gradients align more closely with the
generator manifold. We validate this hypothesis by designing a novel training
objective that explicitly promotes such alignment. Building on this insight, we
further introduce a training-free approach to enhance gradient-manifold
alignment during inversion, leading to consistent improvements over
state-of-the-art generative MIAs.

</details>


### [128] [An Improved Time Series Anomaly Detection by Applying Structural Similarity](https://arxiv.org/abs/2509.20184)
*Tiejun Wang,Rui Wang,Xudong Mou,Mengyuan Ma,Tianyu Wo,Renyu Yang,Xudong Liu*

Main category: cs.LG

TL;DR: StrAD是一种新颖的结构增强异常检测方法，通过将时间序列中的结构信息融入重构模型的优化目标，提高对复杂模式异常检测的能力。


<details>
  <summary>Details</summary>
Motivation: 由于异常标签稀缺且人工标注成本高，基于重构的无监督方法受到关注，但现有方法仅依赖逐点距离度量，忽略了时间序列的结构特征，难以处理复杂的模式异常。

Method: StrAD在重构模型的优化目标中融入趋势、季节性和形状等结构信息，确保原始数据与重构数据在结构特征上对齐，保持全局波动和局部特征的一致性。该方法可插入任何基于重构的方法中。

Result: 实验结果表明，StrAD在五个真实世界异常检测数据集上提高了最先进重构模型的性能。

Conclusion: StrAD通过结构感知的优化目标机制，增强了对点异常和模式异常的检测敏感性，是一种有效的异常检测增强方法。

Abstract: Effective anomaly detection in time series is pivotal for modern industrial
applications and financial systems. Due to the scarcity of anomaly labels and
the high cost of manual labeling, reconstruction-based unsupervised approaches
have garnered considerable attention. However, accurate anomaly detection
remains an unsettled challenge, since the optimization objectives of
reconstruction-based methods merely rely on point-by-point distance measures,
ignoring the potential structural characteristics of time series and thus
failing to tackle complex pattern-wise anomalies. In this paper, we propose
StrAD, a novel structure-enhanced anomaly detection approach to enrich the
optimization objective by incorporating structural information hidden in the
time series and steering the data reconstruction procedure to better capture
such structural features. StrAD accommodates the trend, seasonality, and shape
in the optimization objective of the reconstruction model to learn latent
structural characteristics and capture the intrinsic pattern variation of time
series. The proposed structure-aware optimization objective mechanism can
assure the alignment between the original data and the reconstructed data in
terms of structural features, thereby keeping consistency in global fluctuation
and local characteristics. The mechanism is pluggable and applicable to any
reconstruction-based methods, enhancing the model sensitivity to both
point-wise anomalies and pattern-wise anomalies. Experimental results show that
StrAD improves the performance of state-of-the-art reconstruction-based models
across five real-world anomaly detection datasets.

</details>


### [129] [FairEquityFL -- A Fair and Equitable Client Selection in Federated Learning for Heterogeneous IoV Networks](https://arxiv.org/abs/2509.20193)
*Fahmida Islam,Adnan Mahmood,Noorain Mukhtiar,Kasun Eranda Wijethilake,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 本文提出了FairEquityFL框架，旨在解决联邦学习在动态异构车联网环境中客户端选择过程的公平性问题，通过引入采样均衡器和异常检测机制来确保所有客户端获得平等的参与机会。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习框架在车联网应用中缺乏对客户端选择公平性的考虑，特别是在动态异构环境中，需要确保所有客户端都有平等的机会参与训练过程。

Method: 设计了FairEquityFL框架，包含采样均衡器模块来保证公平选择，以及异常检测机制来识别恶意客户端。选择器负责监控和控制客户端的参与，并暂时暂停可疑客户端的参与。

Result: 在FEMNIST数据集上的仿真结果表明，FairEquityFL在性能上显著优于基线模型。

Conclusion: FairEquityFL框架有效解决了联邦学习在车联网环境中的公平性问题，为动态异构环境下的联邦学习应用提供了可行的解决方案。

Abstract: Federated Learning (FL) has been extensively employed for a number of
applications in machine learning, i.e., primarily owing to its privacy
preserving nature and efficiency in mitigating the communication overhead.
Internet of Vehicles (IoV) is one of the promising applications, wherein FL can
be utilized to train a model more efficiently. Since only a subset of the
clients can participate in each FL training round, challenges arise pertinent
to fairness in the client selection process. Over the years, a number of
researchers from both academia and industry have proposed numerous FL
frameworks. However, to the best of our knowledge, none of them have employed
fairness for FL-based client selection in a dynamic and heterogeneous IoV
environment. Accordingly, in this paper, we envisage a FairEquityFL framework
to ensure an equitable opportunity for all the clients to participate in the FL
training process. In particular, we have introduced a sampling equalizer module
within the selector component for ensuring fairness in terms of fair
collaboration opportunity for all the clients in the client selection process.
The selector is additionally responsible for both monitoring and controlling
the clients' participation in each FL training round. Moreover, an outlier
detection mechanism is enforced for identifying malicious clients based on the
model performance in terms of considerable fluctuation in either accuracy or
loss minimization. The selector flags suspicious clients and temporarily
suspend such clients from participating in the FL training process. We further
evaluate the performance of FairEquityFL on a publicly available dataset,
FEMNIST. Our simulation results depict that FairEquityFL outperforms baseline
models to a considerable extent.

</details>


### [130] [Staying on the Manifold: Geometry-Aware Noise Injection](https://arxiv.org/abs/2509.20201)
*Albert Kjøller Jacobsen,Johanna Marie Gegenfurtner,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 提出几何感知的输入噪声方法，考虑数据流形结构，通过切空间投影和测地线映射来增强模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统方法只在输入空间添加环境噪声，没有考虑数据的低维流形结构，限制了噪声正则化的效果

Method: 将环境高斯噪声投影到流形切空间，然后通过测地线映射到流形上；还考虑了沿流形随机步进的布朗运动噪声

Result: 在高度弯曲流形上显著改善泛化能力和超参数选择鲁棒性，在简单流形上至少与无噪声训练相当

Conclusion: 几何感知噪声框架能有效利用数据流形结构，提升深度学习模型的泛化性能，并可扩展到学习到的数据流形

Abstract: It has been shown that perturbing the input during training implicitly
regularises the gradient of the learnt function, leading to smoother models and
enhancing generalisation. However, previous research mostly considered the
addition of ambient noise in the input space, without considering the
underlying structure of the data. In this work, we propose several methods of
adding geometry-aware input noise that accounts for the lower dimensional
manifold the input space inhabits. We start by projecting ambient Gaussian
noise onto the tangent space of the manifold. In a second step, the noise
sample is mapped on the manifold via the associated geodesic curve. We also
consider Brownian motion noise, which moves in random steps along the manifold.
We show that geometry-aware noise leads to improved generalization and
robustness to hyperparameter selection on highly curved manifolds, while
performing at least as well as training without noise on simpler manifolds. Our
proposed framework extends to learned data manifolds.

</details>


### [131] [Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference](https://arxiv.org/abs/2509.20211)
*Álvaro Parafita,Tomas Garriga,Axel Brando,Francisco J. Cazorla*

Main category: cs.LG

TL;DR: 提出了estimand-agnostic方法使do-SHAP在复杂图上可行，开发了加速算法和解释不可访问数据生成过程的方法


<details>
  <summary>Details</summary>
Motivation: SHAP方法忽略了问题的因果结构，而do-SHAP虽然采用干预查询但依赖estimands限制了实际应用

Method: 使用estimand-agnostic方法从单一模型估计任何可识别查询，开发加速计算算法和解释不可访问DGP的方法

Result: 在估计和计算性能上表现良好，在两个真实数据集上验证了获得可靠解释的潜力

Conclusion: 该方法使do-SHAP在复杂因果图上可行，为获得可靠解释提供了有效解决方案

Abstract: Among explainability techniques, SHAP stands out as one of the most popular,
but often overlooks the causal structure of the problem. In response, do-SHAP
employs interventional queries, but its reliance on estimands hinders its
practical application. To address this problem, we propose the use of
estimand-agnostic approaches, which allow for the estimation of any
identifiable query from a single model, making do-SHAP feasible on complex
graphs. We also develop a novel algorithm to significantly accelerate its
computation at a negligible cost, as well as a method to explain inaccessible
Data Generating Processes. We demonstrate the estimation and computational
performance of our approach, and validate it on two real-world datasets,
highlighting its potential in obtaining reliable explanations.

</details>


### [132] [Time-adaptive HénonNets for separable Hamiltonian systems](https://arxiv.org/abs/2509.20212)
*Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 本文提出了一种新的神经网络架构T-HénonNets，用于学习时间自适应的辛积分器，能够处理不规则采样的测量数据，并扩展到非自治哈密顿系统。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法如SympNets和HénonNets需要固定步长的训练数据，无法处理不规则采样的测量数据。本文旨在为HénonNets开发类似TSympNets的时间自适应扩展。

Method: 提出T-HénonNets神经网络架构，该架构具有辛结构设计，能够处理自适应时间步长，并扩展到非自治哈密顿系统。提供了可分离哈密顿系统的通用逼近定理。

Result: 通过数值实验验证了所提出架构的理论逼近能力，展示了其在处理不规则采样数据方面的有效性。

Conclusion: T-HénonNets成功扩展了HénonNets的时间自适应能力，能够处理自适应时间步长和非自治系统，但对于非可分离哈密顿系统的处理仍存在挑战。

Abstract: Measurement data is often sampled irregularly, i.e., not on equidistant time
grids. This is also true for Hamiltonian systems. However, existing machine
learning methods, which learn symplectic integrators, such as SympNets [1] and
H\'enonNets [2] still require training data generated by fixed step sizes. To
learn time-adaptive symplectic integrators, an extension to SympNets called
TSympNets is introduced in [3]. The aim of this work is to do a similar
extension for H\'enonNets. We propose a novel neural network architecture
called T-H\'enonNets, which is symplectic by design and can handle adaptive
time steps. We also extend the T-H\'enonNet architecture to non-autonomous
Hamiltonian systems. Additionally, we provide universal approximation theorems
for both new architectures for separable Hamiltonian systems and discuss why it
is difficult to handle non-separable Hamiltonian systems with the proposed
methods. To investigate these theoretical approximation capabilities, we
perform different numerical experiments.

</details>


### [133] [Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment](https://arxiv.org/abs/2509.20214)
*Deokjae Lee,Hyun Oh Song*

Main category: cs.LG

TL;DR: 本文提出Q-Palette，一种用于大语言模型权重后训练量化的分数位量化器集合，通过理论推导最优比特分配并实现混合方案量化框架，显著降低量化误差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重分布存在重尾异常值，传统量化方法效果不佳，需要将权重转换为近似高斯分布以减少量化误差。

Method: 推导高斯化权重的最优比特分配理论，设计包含多种分数位量化器的Q-Palette工具集，并提出混合方案量化框架联合优化量化器选择和层融合决策。

Result: Q-Palette实现了接近高斯失真率界限的量化性能，在内存受限的小批量推理场景中显著提升效率。

Conclusion: 分数位量化器对于实现最优量化性能至关重要，Q-Palette为LLM权重量化提供了高效实用的解决方案。

Abstract: We study weight-only post-training quantization (PTQ), which quantizes the
weights of a large language model (LLM) without retraining, using little or no
calibration data. Weight-only PTQ is crucial for reducing the memory footprint
and latency of LLM inference, especially in memory-bound, small-batch inference
scenarios, such as personalized inference on edge devices. Despite its
importance, irregular weight distributions with heavy-tailed outliers in LLMs
complicate quantization, recently motivating rotation-based methods that
transform weights into near-Gaussian distributions, which are more regular with
fewer outliers, thereby reducing quantization error. In this work, we first
derive the information-theoretically optimal bit allocation for Gaussianized
weights under given bit budgets, revealing that fine-grained fractional-bit
quantizers approaching the Gaussian distortion-rate bound are essential to
achieve near-optimal quantization performance. To bridge this theoretical
insight and practical implementation, we introduce Q-Palette, a versatile
collection of fractional-bit quantizers that range from trellis-coded
quantizers offering near-optimal distortion to simpler vector and scalar
quantizers optimized for faster inference, all efficiently implemented with
optimized CUDA kernels across various bitwidths. Furthermore, leveraging
Q-Palette as a foundational component, we propose a novel mixed-scheme
quantization framework, jointly optimizing quantizer choices and layer fusion
decisions given resource constraints. The code is available at
https://github.com/snu-mllab/Q-Palette.

</details>


### [134] [Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](https://arxiv.org/abs/2509.20230)
*Wenhan Wu,Zheyuan Liu,Chongyang Gao,Ren Wang,Kaize Ding*

Main category: cs.LG

TL;DR: 当前LLM遗忘方法存在安全漏洞，遗忘的信息可通过再学习攻击恢复。作者提出StableUN框架，通过邻域感知优化寻找更稳定的参数区域，提高遗忘鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统遗忘方法在损失景观中驱动模型参数趋向尖锐最小值，这些不稳定区域使得遗忘知识容易被再学习攻击恢复，暴露了明显遗忘与实际知识移除之间的鲁棒性差距。

Method: 提出StableUN双层反馈引导优化框架，通过邻域感知优化寻找稳定参数区域，整合遗忘反馈（使用对抗扰动探测参数邻域）和记忆反馈（保持模型效用），通过梯度投影对齐两个目标。

Result: 在WMDP和MUSE基准测试中，该方法对再学习和越狱攻击表现出显著更强的鲁棒性，同时保持竞争力的效用性能。

Conclusion: StableUN解决了传统遗忘方法的安全漏洞，通过寻找稳定参数区域实现了更鲁棒的遗忘效果，为LLM安全遗忘提供了有效解决方案。

Abstract: Current LLM unlearning methods face a critical security vulnerability that
undermines their fundamental purpose: while they appear to successfully remove
sensitive or harmful knowledge, this ``forgotten" information remains
precariously recoverable through relearning attacks. We identify that the root
cause is that conventional methods optimizing the forgetting loss at individual
data points will drive model parameters toward sharp minima in the loss
landscape. In these unstable regions, even minimal parameter perturbations can
drastically alter the model's behaviors. Consequently, relearning attacks
exploit this vulnerability by using just a few fine-tuning samples to navigate
the steep gradients surrounding these unstable regions, thereby rapidly
recovering knowledge that was supposedly erased. This exposes a critical
robustness gap between apparent unlearning and actual knowledge removal. To
address this issue, we propose StableUN, a bi-level feedback-guided
optimization framework that explicitly seeks more stable parameter regions via
neighborhood-aware optimization. It integrates forgetting feedback, which uses
adversarial perturbations to probe parameter neighborhoods, with remembering
feedback to preserve model utility, aligning the two objectives through
gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that
our method is significantly more robust against both relearning and
jailbreaking attacks while maintaining competitive utility performance.

</details>


### [135] [A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification](https://arxiv.org/abs/2509.20240)
*Xin An,Ruijie Li,Qiao Ning,Hui Li,Qian Ma,Shikai Guo*

Main category: cs.LG

TL;DR: HGMamba-ncRNA是一个基于HyperGraphMamba的多通道自适应模型，通过整合ncRNA的序列、二级结构和表达特征来提升非编码RNA分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征提取深度和多模态融合方面存在局限，需要更准确的非编码RNA分类方法用于功能注释和疾病诊断。

Method: 使用并行多尺度卷积和LSTM架构处理序列，多尺度图变换器处理二级结构，切比雪夫多项式基KAN网络处理表达特征，最后通过HyperGraphMamba进行多模态特征自适应对齐和融合。

Result: 在三个公共数据集上的实验表明，HGMamba-ncRNA在准确率等指标上持续优于现有最先进方法。

Conclusion: 该模型具有鲁棒性、有效性和强可迁移性，为复杂的ncRNA功能分类提供了新颖可靠的策略。

Abstract: Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and
the pathogenesis of various diseases. Accurate classification of ncRNAs is
essential for functional annotation and disease diagnosis. To address existing
limitations in feature extraction depth and multimodal fusion, we propose
HGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which
integrates sequence, secondary structure, and optionally available expression
features of ncRNAs to enhance classification performance. Specifically, the
sequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM
architecture (MKC-L) to capture both local patterns and long-range dependencies
of nucleotides. The structure modality employs a multi-scale graph transformer
(MSGraphTransformer) to represent the multi-level topological characteristics
of ncRNA secondary structures. The expression modality utilizes a Chebyshev
Polynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and
interpret high-dimensional expression profiles. Finally, by incorporating
virtual nodes to facilitate efficient and comprehensive multimodal interaction,
HyperGraphMamba is proposed to adaptively align and integrate multichannel
heterogeneous modality features. Experiments conducted on three public datasets
demonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art
methods in terms of accuracy and other metrics. Extensive empirical studies
further confirm the model's robustness, effectiveness, and strong
transferability, offering a novel and reliable strategy for complex ncRNA
functional classification. Code and datasets are available at
https://anonymous.4open.science/r/HGMamba-ncRNA-94D0.

</details>


### [136] [Dynamic Lagging for Time-Series Forecasting in E-Commerce Finance: Mitigating Information Loss with A Hybrid ML Architecture](https://arxiv.org/abs/2509.20244)
*Abhishek Sharma,Anat Parush,Sumit Wadhwa,Amihai Savir,Anne Guinard,Prateek Srivastava*

Main category: cs.LG

TL;DR: 提出一种混合预测框架，结合动态滞后特征工程和自适应滚动窗口表示，解决电商金融领域因发票时间不规律、付款延迟和用户行为差异导致的预测挑战。


<details>
  <summary>Details</summary>
Motivation: 电商金融领域的预测面临发票时间不规律、付款延迟、用户行为差异、数据稀疏和历史窗口短等挑战，传统时间序列方法和深度学习模型在这些条件下效果不佳。

Method: 采用混合框架，集成动态滞后特征工程、自适应滚动窗口表示、经典统计模型和集成学习器，结合发票级行为建模、结构化滞后支持和自定义稳定性感知损失函数。

Result: 实验结果显示，与基线模型相比，平均绝对百分比误差（MAPE）降低约5%，带来显著财务节省，同时提高了季度预测稳定性和特征目标相关性。

Conclusion: 结合结构化滞后、发票级闭合建模和行为洞察，能显著提升稀疏金融时间序列预测的准确性。

Abstract: Accurate forecasting in the e-commerce finance domain is particularly
challenging due to irregular invoice schedules, payment deferrals, and
user-specific behavioral variability. These factors, combined with sparse
datasets and short historical windows, limit the effectiveness of conventional
time-series methods. While deep learning and Transformer-based models have
shown promise in other domains, their performance deteriorates under partial
observability and limited historical data. To address these challenges, we
propose a hybrid forecasting framework that integrates dynamic lagged feature
engineering and adaptive rolling-window representations with classical
statistical models and ensemble learners. Our approach explicitly incorporates
invoice-level behavioral modeling, structured lag of support data, and custom
stability-aware loss functions, enabling robust forecasts in sparse and
irregular financial settings. Empirical results demonstrate an approximate 5%
reduction in MAPE compared to baseline models, translating into substantial
financial savings. Furthermore, the framework enhances forecast stability over
quarterly horizons and strengthens feature target correlation by capturing both
short- and long-term patterns, leveraging user profile attributes, and
simulating upcoming invoice behaviors. These findings underscore the value of
combining structured lagging, invoice-level closure modeling, and behavioral
insights to advance predictive accuracy in sparse financial time-series
forecasting.

</details>


### [137] [Failure Modes of Maximum Entropy RLHF](https://arxiv.org/abs/2509.20265)
*Ömer Veysel Çağatan,Barış Akgün*

Main category: cs.LG

TL;DR: 本文揭示了Simple Preference Optimization（SimPO）是带有长度归一化温度的最大熵强化学习的理论推导，并探讨了最大熵RL在在线RLHF设置中的表现差异。研究发现最大熵RL在在线设置中会出现过优化和不稳定的KL动态，而SimPO在离线设置中表现良好。


<details>
  <summary>Details</summary>
Motivation: 研究SimPO在离线偏好优化中的优异表现是否能在在线RLHF设置中通过最大熵RL实现，以及探索参考无关方法在在线和离线偏好学习中的不同挑战。

Method: 通过理论推导将SimPO与最大熵强化学习建立联系，并在在线RLHF设置中进行实验，比较最大熵RL与KL约束方法的训练稳定性。

Result: 最大熵RL在在线设置中持续表现出过优化和不稳定的KL动态，即使在极低学习率下也无法避免奖励黑客行为；而KL约束方法能够保持稳定训练。

Conclusion: SimPO在离线设置中成功而最大熵RL在在线设置中失败的原因可能与参考无关方法在在线和离线偏好学习中的不同挑战有关，需要进一步研究解释这种差异。

Abstract: In this paper, we show that Simple Preference Optimization (SimPO) can be
derived as Maximum Entropy Reinforcement Learning with length-normalized
temperature, providing a theoretical foundation for this reference-free method.
Motivated by SimPO's strong performance in offline preference optimization, we
investigate whether Maximum Entropy RL can achieve similar results in online
RLHF settings. Our experiments find that Maximum Entropy RL consistently
exhibits overoptimization and unstable KL dynamics, even at very low learning
rates. Unlike KL-constrained methods that maintain stable training, entropy
regularization fails to prevent reward hacking and appears to correlate with
overoptimization. Lastly, we discuss possible explanations for why SimPO
succeeds in offline settings while Maximum Entropy RL struggles in online
scenarios. Our findings suggest that reference-free approaches may face
distinct challenges when applied to online or offline preference learning.

</details>


### [138] [Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation](https://arxiv.org/abs/2509.20269)
*Matteo Cardoni,Sam Leroux*

Main category: cs.LG

TL;DR: 提出了一种结合反向传播和预测编码的混合训练方法，用于在资源受限的边缘设备上实现高效的在线领域自适应


<details>
  <summary>Details</summary>
Motivation: 现实环境中输入数据分布会发生变化（如传感器漂移、光照变化），单一静态模型无法满足需求，需要持续模型自适应

Method: 先使用反向传播进行离线训练获得高性能初始模型，然后使用预测编码进行在线自适应，利用反向传播的鲁棒性和预测编码的计算效率优势

Result: 在MNIST和CIFAR-10数据集上的实验表明，该混合策略能够有效适应数据分布变化，同时降低计算开销

Conclusion: 该方法为动态环境中保持模型性能提供了有前景的解决方案，特别适合资源受限的边缘设备或神经形态加速器

Abstract: As deep neural networks are increasingly deployed in dynamic, real-world
environments, relying on a single static model is often insufficient. Changes
in input data distributions caused by sensor drift or lighting variations
necessitate continual model adaptation. In this paper, we propose a hybrid
training methodology that enables efficient on-device domain adaptation by
combining the strengths of Backpropagation and Predictive Coding. The method
begins with a deep neural network trained offline using Backpropagation to
achieve high initial performance. Subsequently, Predictive Coding is employed
for online adaptation, allowing the model to recover accuracy lost due to
shifts in the input data distribution. This approach leverages the robustness
of Backpropagation for initial representation learning and the computational
efficiency of Predictive Coding for continual learning, making it particularly
well-suited for resource-constrained edge devices or future neuromorphic
accelerators. Experimental results on the MNIST and CIFAR-10 datasets
demonstrate that this hybrid strategy enables effective adaptation with a
reduced computational overhead, offering a promising solution for maintaining
model performance in dynamic environments.

</details>


### [139] [Extended Low-Rank Approximation Accelerates Learning of Elastic Response in Heterogeneous Materials](https://arxiv.org/abs/2509.20276)
*Prabhat Karmakar,Sayan Gupta,Ilaksh Adlakha*

Main category: cs.LG

TL;DR: 本文提出了扩展低秩近似（xLRA）框架，使用张量分解方法高效预测多孔微结构中的局部弹性响应，仅需少量数据训练即可实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 预测异质材料微观结构如何控制力学响应对于优化设计和性能至关重要，但由于微观结构特征的高维复杂性，传统物理模拟计算成本过高，需要开发高效的计算工具来学习结构-性能关系。

Method: 采用规范多线性张量分解（canonical polyadic tensor decomposition），通过自适应引入更高秩项，将高维微观结构信息高效映射到局部弹性响应。

Result: xLRA仅需最大秩为4即可准确预测多孔微结构中的局部弹性应变场，在仅使用5%数据集训练时仍能实现准确预测，计算效率比现有方法高6个数量级。

Conclusion: xLRA提供了一个高效框架，能够从微观结构预测弹性响应，实现了结构-性能关系的可扩展映射，在预测精度、泛化能力和计算效率方面均优于现有方法。

Abstract: Predicting how the microstructure governs the mechanical response of
heterogeneous materials is essential for optimizing design and performance. Yet
this task remains difficult due to the complex, high dimensional nature of
microstructural features. Relying on physics based simulations to probe the
microstructural space is computationally prohibitive. This motivates the
development of computational tools to efficiently learn structure property
linkages governing mechanical behavior. While contemporary data driven
approaches offer new possibilities, they often require large datasets. To
address this challenge, this work presents the Extended Low Rank Approximation
(xLRA), a framework that employs canonical polyadic tensor decomposition. It
efficiently maps high dimensional microstructural information to the local
elastic response by adaptively incorporating higher rank terms. xLRA accurately
predicts the local elastic strain fields in porous microstructures, requiring a
maximum rank of only 4. The compact formulation of xLRA achieves accurate
predictions when trained on just 5% of the dataset, demonstrating significant
data efficiency. Moreover, xLRA proves transferability by delivering results
across representative material systems, including two phase composites and
single and dual phase polycrystals. Despite being compact, xLRA retains
essential microstructural details, enabling accurate predictions on unseen
microstructures. Benchmarking shows that xLRA outperforms contemporary methods
in predictive accuracy, generalizability, and computational efficiency, while
requiring 6 orders of magnitude fewer floating point operations. In summary,
xLRA provides an efficient framework for predicting the elastic response from
microstructures, enabling scalable mapping of structure property linkages.

</details>


### [140] [PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction](https://arxiv.org/abs/2509.20290)
*Dayu Tan,Jing Chen,Xiaoping Zhou,Yansen Su,Chunhou Zheng*

Main category: cs.LG

TL;DR: 本研究提出了PGCLODA框架，使用提示引导的图对比学习来预测寡肽与传染病之间的关联，在多个评估指标上优于现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 传染病对公共卫生构成严重威胁，而寡肽因其结构简单、生物利用度高和耐药性低而成为有前景的抗感染候选药物，但专门预测寡肽与传染病关联的计算模型仍然稀缺。

Method: 构建包含寡肽、微生物和疾病的三方图，采用提示引导的图增强策略生成有意义的对比视图，使用GCN和Transformer的双编码器架构捕获局部和全局特征，最后通过MLP分类器进行预测。

Result: 在基准数据集上的实验表明，PGCLODA在AUROC、AUPRC和准确率方面一致优于现有最佳模型，消融实验和超参数研究验证了各模块的贡献。

Conclusion: 该框架为机制驱动的发现和基于寡肽的药物开发提供了有价值的见解，案例研究进一步验证了其泛化能力和发现生物学相关新关联的潜力。

Abstract: Infectious diseases continue to pose a serious threat to public health,
underscoring the urgent need for effective computational approaches to screen
novel anti-infective agents. Oligopeptides have emerged as promising candidates
in antimicrobial research due to their structural simplicity, high
bioavailability, and low susceptibility to resistance. Despite their potential,
computational models specifically designed to predict associations between
oligopeptides and infectious diseases remain scarce. This study introduces a
prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover
potential associations. A tripartite graph is constructed with oligopeptides,
microbes, and diseases as nodes, incorporating both structural and semantic
information. To preserve critical regions during contrastive learning, a
prompt-guided graph augmentation strategy is employed to generate meaningful
paired views. A dual encoder architecture, integrating Graph Convolutional
Network (GCN) and Transformer, is used to jointly capture local and global
features. The fused embeddings are subsequently input into a multilayer
perceptron (MLP) classifier for final prediction. Experimental results on a
benchmark dataset indicate that PGCLODA consistently outperforms
state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and
hyperparameter studies confirm the contribution of each module. Case studies
further validate the generalization ability of PGCLODA and its potential to
uncover novel, biologically relevant associations. These findings offer
valuable insights for mechanism-driven discovery and oligopeptide-based drug
development. The source code of PGCLODA is available online at
https://github.com/jjnlcode/PGCLODA.

</details>


### [141] [When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity](https://arxiv.org/abs/2509.20293)
*Benjamin Feuer,Chiung-Yi Tseng,Astitwa Sarthak Lathe,Oussama Elachqar,John P Dickerson*

Main category: cs.LG

TL;DR: 本文分析了LLM评判基准存在的问题，提出了两种诊断机制来量化基准排名中的噪声和不确定性，并揭示了当前流行评判模型存在的严重模式不一致和因子崩溃问题。


<details>
  <summary>Details</summary>
Motivation: LLM评判基准在评估复杂模型行为时被广泛使用，但其设计存在传统基于真实标签基准所没有的失效模式。缺乏严格目标和可验证构造会导致基准排名产生高置信度但实际上主要是噪声的结果。

Method: 引入两种诊断机制：1）模式遵循度量化评判者整体裁决中有多少能被显式评估模式解释；2）心理测量有效性通过聚合内部一致性和区分效度信号来量化基准运行中的不可约不确定性。

Result: 在Arena-Hard Auto基准上应用这些工具发现：DeepSeek-R1-32B的未解释方差超过90%，大多数标准因子相关性高于0.93，ELO风格聚合会掩盖真实的排名不确定性。

Conclusion: 研究结果揭示了削弱有效性的设计失败，并提供了构建更好范围、可靠性感知的LLM评判基准的可操作原则。

Abstract: LLM-judged benchmarks are increasingly used to evaluate complex model
behaviors, yet their design introduces failure modes absent in conventional
ground-truth based benchmarks. We argue that without tight objectives and
verifiable constructions, benchmark rankings can produce high-confidence
rankings that are in fact largely noise. We introduce two mechanisms to
diagnose these issues. Schematic adherence quantifies how much of a judge's
overall verdict is explained by the explicit evaluation schema, revealing
unexplained variance when judges deviate from their own rubric. Psychometric
validity aggregates internal consistency and discriminant validity signals to
quantify irreducible uncertainty in any benchmarking run. Applying these tools
to Arena-Hard Auto, we find severe schema incoherence and factor collapse
across popular judges: for example, unexplained variance exceeding 90 percent
for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We
also show that the ELO-style aggregation used by Arena-Hard Auto collapses and
masks genuine ranking uncertainty. Our results highlight design failures that
undermine validity and offer actionable principles for building better-scoped,
reliability-aware LLM-judged benchmarks. We release our code at
https://anonymous.4open.science/r/judgment-to-noise-947D/README.md

</details>


### [142] [Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels](https://arxiv.org/abs/2509.20294)
*Dongming Huang,Zhifan Li,Yicheng Li,Qian Lin*

Main category: cs.LG

TL;DR: 本文提出了有效跨度维度（ESD）这一新的复杂度度量，用于分析数据驱动核学习场景下的谱算法性能，证明了ESD与最小化超额风险的关系，并建立了自适应特征学习与泛化改进的理论联系。


<details>
  <summary>Details</summary>
Motivation: 传统固定核理论无法充分解释数据驱动核学习场景下的泛化性能，需要一种能够同时考虑信号、谱和噪声水平的统一框架来分析谱算法的泛化行为。

Method: 引入有效跨度维度（ESD）作为对齐敏感的复杂度度量，分析序列模型、线性模型和RKHS回归中的谱算法性能，并通过梯度流分析ESD的减少机制。

Result: 证明了当ESD不超过K时，最小化超额风险按σ²K缩放；梯度流能够降低ESD，从而改善泛化性能；数值实验支持理论结果。

Conclusion: ESD框架为超越传统固定核理论提供了新的泛化视角，建立了自适应特征学习与谱算法泛化改进的理论基础，具有广泛的适用性。

Abstract: We study spectral algorithms in the setting where kernels are learned from
data. We introduce the effective span dimension (ESD), an alignment-sensitive
complexity measure that depends jointly on the signal, spectrum, and noise
level $\sigma^2$. The ESD is well-defined for arbitrary kernels and signals
without requiring eigen-decay conditions or source conditions. We prove that
for sequence models whose ESD is at most $K$, the minimax excess risk scales as
$\sigma^2 K$. Furthermore, we analyze over-parameterized gradient flow and
prove that it can reduce the ESD. This finding establishes a connection between
adaptive feature learning and provable improvements in generalization of
spectral algorithms. We demonstrate the generality of the ESD framework by
extending it to linear models and RKHS regression, and we support the theory
with numerical experiments. This framework provides a novel perspective on
generalization beyond traditional fixed-kernel theories.

</details>


### [143] [Graph Variate Neural Networks](https://arxiv.org/abs/2509.20311)
*Om Roy,Yashar Moshfeghi,Keith Smith*

Main category: cs.LG

TL;DR: 本文提出了图变量神经网络（GVNNs），这是一种基于图变量信号分析（GVSA）框架的新型图神经网络层，能够有效建模动态演化的时空信号，通过结合稳定的长期支持和瞬时数据驱动交互来捕捉动态统计依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络（GNNs）通常假设存在固定的底层图结构，但在许多实际应用中，这种结构可能不存在或需要独立推导。而多通道数据可以构建随时间演化的功能网络，需要一种能够处理动态时空信号的统一框架。

Method: 基于GVSA框架和图信号处理工具，GVNNs层将时空信号与信号相关的连接张量进行卷积，该张量结合了稳定的长期支持和瞬时的数据驱动交互。这种方法无需使用临时的滑动窗口，并实现了线性复杂度的有效实现。

Result: 在预测基准测试中，GVNNs持续优于强大的基于图的基线方法，并与广泛使用的序列模型（如LSTMs和Transformers）具有竞争力。在EEG运动想象分类任务中，GVNNs实现了高准确率，展示了其在脑机接口应用中的潜力。

Conclusion: GVNNs提供了一种有效的框架来处理动态时空信号，通过信号依赖的连接张量捕捉动态统计依赖性，在多个基准测试中表现出色，特别是在脑机接口等应用中具有重要价值。

Abstract: Modelling dynamically evolving spatio-temporal signals is a prominent
challenge in the Graph Neural Network (GNN) literature. Notably, GNNs assume an
existing underlying graph structure. While this underlying structure may not
always exist or is derived independently from the signal, a temporally evolving
functional network can always be constructed from multi-channel data. Graph
Variate Signal Analysis (GVSA) defines a unified framework consisting of a
network tensor of instantaneous connectivity profiles against a stable support
usually constructed from the signal itself. Building on GVSA and tools from
graph signal processing, we introduce Graph-Variate Neural Networks (GVNNs):
layers that convolve spatio-temporal signals with a signal-dependent
connectivity tensor combining a stable long-term support with instantaneous,
data-driven interactions. This design captures dynamic statistical
interdependencies at each time step without ad hoc sliding windows and admits
an efficient implementation with linear complexity in sequence length. Across
forecasting benchmarks, GVNNs consistently outperform strong graph-based
baselines and are competitive with widely used sequence models such as LSTMs
and Transformers. On EEG motor-imagery classification, GVNNs achieve strong
accuracy highlighting their potential for brain-computer interface
applications.

</details>


### [144] [A Recovery Guarantee for Sparse Neural Networks](https://arxiv.org/abs/2509.20323)
*Sara Fridovich-Keil,Mert Pilanci*

Main category: cs.LG

TL;DR: 本文证明了ReLU神经网络稀疏恢复的第一个保证，提出了一种简单的迭代硬阈值算法，能够使用线性内存精确恢复稀疏网络权重。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏神经网络权重的结构特性，解决现有方法内存效率低的问题，为稀疏网络恢复提供理论保证。

Method: 使用迭代硬阈值算法，针对两层标量输出网络，在稀疏网络权重的结构特性条件下进行恢复。

Result: 理论证明算法能够精确恢复稀疏权重，实验验证在稀疏植入MLP、MNIST分类和隐式神经表示等任务上表现优异，性能优于基于迭代幅度剪枝的高性能基线。

Conclusion: 提出的算法在保证恢复精度的同时显著提高了内存效率，为稀疏神经网络的实际应用提供了可行的解决方案。

Abstract: We prove the first guarantees of sparse recovery for ReLU neural networks,
where the sparse network weights constitute the signal to be recovered.
Specifically, we study structural properties of the sparse network weights for
two-layer, scalar-output networks under which a simple iterative hard
thresholding algorithm recovers these weights exactly, using memory that grows
linearly in the number of nonzero weights. We validate this theoretical result
with simple experiments on recovery of sparse planted MLPs, MNIST
classification, and implicit neural representations. Experimentally, we find
performance that is competitive with, and often exceeds, a high-performing but
memory-inefficient baseline based on iterative magnitude pruning.

</details>


### [145] [Video models are zero-shot learners and reasoners](https://arxiv.org/abs/2509.20328)
*Thaddäus Wiedemer,Yuxuan Li,Paul Vicol,Shixiang Shane Gu,Nick Matarese,Kevin Swersky,Been Kim,Priyank Jaini,Robert Geirhos*

Main category: cs.LG

TL;DR: Veo 3视频模型展示了类似大型语言模型的零样本能力，能够在未明确训练的情况下完成多种视觉任务，表明视频模型正在向通用视觉基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 探索视频模型是否能够像大型语言模型那样发展出通用的视觉理解能力，而不仅仅是特定任务的专用模型。

Method: 使用Veo 3视频模型，通过简单的生成模型训练范式（大规模生成模型+网络规模数据训练），测试其在各种未训练任务上的表现。

Result: Veo 3能够零样本解决物体分割、边缘检测、图像编辑、物理属性理解、物体功能识别、工具使用模拟等多种任务，并展现出早期的视觉推理能力。

Conclusion: 视频模型正朝着统一、通用的视觉基础模型方向发展，其涌现的零样本能力表明这一技术路径的可行性。

Abstract: The remarkable zero-shot capabilities of Large Language Models (LLMs) have
propelled natural language processing from task-specific models to unified,
generalist foundation models. This transformation emerged from simple
primitives: large, generative models trained on web-scale data. Curiously, the
same primitives apply to today's generative video models. Could video models be
on a trajectory towards general-purpose vision understanding, much like LLMs
developed general-purpose language understanding? We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more. These abilities
to perceive, model, and manipulate the visual world enable early forms of
visual reasoning like maze and symmetry solving. Veo's emergent zero-shot
capabilities indicate that video models are on a path to becoming unified,
generalist vision foundation models.

</details>


### [146] [Feature Dynamics as Implicit Data Augmentation: A Depth-Decomposed View on Deep Neural Network Generalization](https://arxiv.org/abs/2509.20334)
*Tianyu Ruan,Kuo Gai,Shihua Zhang*

Main category: cs.LG

TL;DR: 该论文通过研究深度网络内部特征的时间演化，发现了一种时间一致性现象，即早期检查点的浅层特征与后期深层特征结合时预测保持稳定，这种稳定性作为一种隐式结构化增强支持泛化。


<details>
  <summary>Details</summary>
Motivation: 研究深度网络泛化能力好的根本原因，区别于经典泛化理论，通过分析内部特征演化来理解泛化机制。

Method: 分析深度网络训练过程中不同时间点的特征组合，研究预测稳定性，并在未见数据和损坏数据上测试时间一致性，同时使用统计测试分析SGD噪声的各向异性。

Result: 发现时间一致性现象，这种稳定性不是简单的收敛伪影，而是支持泛化的结构化增强；时间一致性在语义结构被破坏时崩溃；SGD注入与少数主方向对齐的各向异性噪声。

Conclusion: 研究提出了将特征动态与泛化联系起来的概念视角，为未来开发测量时间特征演化的实用替代指标指明了方向。

Abstract: Why do deep networks generalize well? In contrast to classical generalization
theory, we approach this fundamental question by examining not only inputs and
outputs, but the evolution of internal features. Our study suggests a
phenomenon of temporal consistency that predictions remain stable when shallow
features from earlier checkpoints combine with deeper features from later ones.
This stability is not a trivial convergence artifact. It acts as a form of
implicit, structured augmentation that supports generalization. We show that
temporal consistency extends to unseen and corrupted data, but collapses when
semantic structure is destroyed (e.g., random labels). Statistical tests
further reveal that SGD injects anisotropic noise aligned with a few principal
directions, reinforcing its role as a source of structured variability.
Together, these findings suggest a conceptual perspective that links feature
dynamics to generalization, pointing toward future work on practical surrogates
for measuring temporal feature evolution.

</details>


### [147] [Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing](https://arxiv.org/abs/2509.20336)
*Xinnan Dai,Chung-Hsiang Lo,Kai Guo,Shenglai Zeng,Dongsheng Luo,Jiliang Tang*

Main category: cs.LG

TL;DR: 本文通过电路追踪框架分析解码器Transformer在图推理任务中的内部机制，识别出token合并和结构记忆两个核心机制，并量化分析图密度和模型规模对这些机制的影响。


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs在图推理任务上表现出色，但其内部机制尚未被充分探索。本文旨在从基础和解码器Transformer出发，统一解释这些推理过程机制。

Method: 使用电路追踪框架对基础解码器Transformer进行可视化推理追踪，识别出图推理中的核心机制，并量化分析这些行为如何受图密度和模型规模影响。

Result: 发现图推理中存在两个核心机制：token合并和结构记忆，这些机制支撑了路径推理和子结构提取任务。量化分析显示图密度和模型规模对这些机制有显著影响。

Conclusion: 本研究为理解解码器Transformer中的结构推理提供了一个统一的解释性框架。

Abstract: Transformer-based LLMs demonstrate strong performance on graph reasoning
tasks, yet their internal mechanisms remain underexplored. To uncover these
reasoning process mechanisms in a fundamental and unified view, we set the
basic decoder-only transformers and explain them using the circuit-tracer
framework. Through this lens, we visualize reasoning traces and identify two
core mechanisms in graph reasoning: token merging and structural memorization,
which underlie both path reasoning and substructure extraction tasks. We
further quantify these behaviors and analyze how they are influenced by graph
density and model size. Our study provides a unified interpretability framework
for understanding structural reasoning in decoder-only Transformers.

</details>


### [148] [Spatio-Temporal Directed Graph Learning for Account Takeover Fraud Detection](https://arxiv.org/abs/2509.20339)
*Mohsen Nayebi Kerdabadi,William Andrew Byron,Xin Sun,Amirfarrokh Iranitalab*

Main category: cs.LG

TL;DR: ATLAS是一个用于账户盗用检测的框架，通过将ATO检测重新定义为时空节点分类问题，在时间有向会话图上进行建模，显著提升了检测性能并减少了用户摩擦。


<details>
  <summary>Details</summary>
Motivation: 传统的基于XGBoost等表格梯度提升决策树的方法独立评分会话，忽略了在线活动的关联性和时间结构特征，无法有效检测协调攻击和"欺诈环"。

Method: ATLAS框架将ATO检测重新定义为时空节点分类问题，构建时间有向会话图，通过共享标识符（账户、设备、IP）连接实体，并使用时窗和最近性约束调节连通性，实现因果、时间尊重的消息传递和延迟感知标签传播。

Result: 在Capital One的高风险数字产品上，ATLAS实现了6.38%的AUC提升，客户摩擦减少了50%以上，在提高欺诈捕获率的同时降低了用户摩擦。

Conclusion: ATLAS通过图神经网络方法有效利用了会话间的时空关系，显著提升了账户盗用检测性能，为ATO欺诈检测提供了新的有效解决方案。

Abstract: Account Takeover (ATO) fraud poses a significant challenge in consumer
banking, requiring high recall under strict latency while minimizing friction
for legitimate users. Production systems typically rely on tabular
gradient-boosted decision trees (e.g., XGBoost) that score sessions
independently, overlooking the relational and temporal structure of online
activity that characterizes coordinated attacks and "fraud rings." We introduce
ATLAS (Account Takeover Learning Across Spatio-Temporal Directed Graph), a
framework that reformulates ATO detection as spatio-temporal node
classification on a time-respecting directed session graph. ATLAS links
entities via shared identifiers (account, device, IP) and regulates
connectivity with time-window and recency constraints, enabling causal,
time-respecting message passing and latency-aware label propagation that uses
only labels available at scoring time, non-anticipative and leakage-free. We
operationalize ATLAS with inductive GraphSAGE variants trained via neighbor
sampling, at scale on a sessions graph with more than 100M nodes and around 1B
edges. On a high-risk digital product at Capital One, ATLAS delivers 6.38
percent AUC improvement and more than 50 percent reduction in customer
friction, improving fraud capture while reducing user friction.

</details>


### [149] [Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing](https://arxiv.org/abs/2509.20349)
*Ramona Rubini,Siavash Khodakarami,Aniruddha Bora,George Em Karniadakis,Michele Dassisti*

Main category: cs.LG

TL;DR: 该论文提出了过程信息预测（PIF）模型，用于制药冻干过程中的温度预测，通过整合过程信息的轨迹先验来提升深度学习模型的物理一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在复杂物理系统的时间序列预测中表现出色，但由于物理不一致性和鲁棒性问题，在受监管环境中的可靠性受限。

Method: 研究了从经典模型（ARIMA、ETS）到现代深度学习架构（KANs）的多种模型，比较了三种整合过程信息轨迹先验的损失函数：固定权重损失、动态不确定性损失和残差注意力机制。

Result: PIF模型在准确性、物理合理性和噪声鲁棒性方面优于纯数据驱动模型，并在迁移学习场景中表现出良好的泛化能力。

Conclusion: 这项工作为制药制造关键应用开发可靠且可泛化的预测解决方案提供了路线图。

Abstract: Accurate time-series forecasting for complex physical systems is the backbone
of modern industrial monitoring and control. While deep learning models excel
at capturing complex dynamics, currently, their deployment is limited due to
physical inconsistency and robustness, hence constraining their reliability in
regulated environments. We introduce process-informed forecasting (PIF) models
for temperature in pharmaceutical lyophilization. We investigate a wide range
of models, from classical ones such as Autoregressive Integrated Moving Average
Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning
architectures, including Kolmogorov-Arnold Networks (KANs). We compare three
different loss function formulations that integrate a process-informed
trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a
Residual-Based Attention (RBA) mechanism. We evaluate all models not only for
accuracy and physical consistency but also for robustness to sensor noise.
Furthermore, we test the practical generalizability of the best model in a
transfer learning scenario on a new process. Our results show that PIF models
outperform their data-driven counterparts in terms of accuracy, physical
plausibility and noise resilience. This work provides a roadmap for developing
reliable and generalizable forecasting solutions for critical applications in
the pharmaceutical manufacturing landscape.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [150] [ALNS for Tugboat Scheduling in Inland Waterway](https://arxiv.org/abs/2509.19718)
*Zihang Ma*

Main category: math.OC

TL;DR: 本文针对单拖船可拖曳多艘驳船并执行多次往返运输的驳船调度问题，提出了混合整数规划模型和自适应大邻域搜索算法相结合的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决实际规模驳船调度问题中单拖船多驳船运输的优化挑战，传统数学规划方法难以应对大规模实例。

Method: 建立混合整数规划模型，并设计自适应大邻域搜索算法结合解码数学模型来求解大规模问题实例。

Result: 自适应大邻域搜索算法在大规模实例中表现优于强化后的数学规划模型。

Conclusion: 提出的ALNS算法能有效解决实际规模的驳船调度问题，为类似运输优化问题提供了可行的解决方案。

Abstract: This paper focuses on the barges shipping problem, also known as the tugboats
scheduling problem, within the context of a scenario where a single tugboat has
the capacity to tow multiple barges and conduct multiple trips in a
drop-and-pull mode during a daily work shift. The problem is mathematically
formalized as mixed-integer programming models. To tackle real-world-sized
problem instances, an adaptive large neighborhood search (ALNS) algorithm
integrated with a decoding mathematical model is proposed. When applied to
large-scale instances, the ALNS algorithm showcases performance superiority
over the strengthened mathematical model.

</details>


### [151] [Efficient Online Large-Margin Classification via Dual Certificates](https://arxiv.org/abs/2509.19670)
*Nam Ho-Nguyen,Fatma Kılınç-Karzan,Ellie Nguyen,Lingqing Shen*

Main category: math.OC

TL;DR: 本文提出了一种基于离线最大间隔问题几何洞察的在线分类算法，具有平移不变性，在理论分析和实际性能上均优于经典感知机算法。


<details>
  <summary>Details</summary>
Motivation: 经典在线分类算法如感知机虽然高效且对线性可分数据有有限错误保证，但未能充分利用分类问题的几何结构。本文旨在通过离线最大间隔问题的对偶形式获得几何洞察，设计更优的在线算法。

Method: 通过研究离线最大间隔问题的对偶形式获得几何洞察，设计具有平移不变性的在线分类算法。该算法继承了离线问题的平移不变特性，这在性能分析中起核心作用。

Result: 理论分析显示改进的错误和间隔边界仅依赖于平移不变量，在有利设置下提供比现有算法更强的保证。在特定参数范围内，新算法最多每序列犯两个错误，而感知机可能犯任意多错误。数值实验表明新算法计算效率与现有在线算法相当，但准确率显著更优。

Conclusion: 基于几何洞察设计的平移不变在线分类算法在理论和实际性能上均优于传统方法，特别在有利条件下能大幅减少分类错误。

Abstract: Online classification is a central problem in optimization, statistical
learning and data science. Classical algorithms such as the perceptron offer
efficient updates and finite mistake guarantees on linearly separable data, but
they do not exploit the underlying geometric structure of the classification
problem. We study the offline maximum margin problem through its dual
formulation and use the resulting geometric insights to design a principled and
efficient algorithm for the online setting. A key feature of our method is its
translation invariance, inherited from the offline formulation, which plays a
central role in its performance analysis. Our theoretical analysis yields
improved mistake and margin bounds that depend only on translation-invariant
quantities, offering stronger guarantees than existing algorithms under the
same assumptions in favorable settings. In particular, we identify a parameter
regime where our algorithm makes at most two mistakes per sequence, whereas the
perceptron can be forced to make arbitrarily many mistakes. Our numerical study
on real data further demonstrates that our method matches the computational
efficiency of existing online algorithms, while significantly outperforming
them in accuracy.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [152] [The Platonic Universe: Do Foundation Models See the Same Sky?](https://arxiv.org/abs/2509.19453)
*UniverseTBD,:,Kshitij Duraphe,Michael J. Smith,Shashwat Sourav,John F. Wu*

Main category: astro-ph.IM

TL;DR: 本文通过测量不同数据类型的基模型之间的表示收敛性来测试天文学中的柏拉图表示假说，发现表示对齐随着模型容量增加而增强，支持向共享星系天体物理学表示的收敛。


<details>
  <summary>Details</summary>
Motivation: 测试柏拉图表示假说在天文学中的适用性，探索不同基础模型是否能够收敛到共享的星系天体物理学表示。

Method: 使用JWST、HSC、Legacy Survey和DESI的光谱和成像观测数据，通过视觉变换器、自监督模型和天文学专用架构，采用互k近邻分析比较表示。

Result: 观察到一致的缩放规律：表示对齐通常随着模型容量的增加而增强，支持向共享星系天体物理学表示的收敛。

Conclusion: 天文学基础模型可以利用预训练的通用架构，从而受益于更广泛机器学习社区已有的计算投入。

Abstract: We test the Platonic Representation Hypothesis (PRH) in astronomy by
measuring representational convergence across a range of foundation models
trained on different data types. Using spectroscopic and imaging observations
from JWST, HSC, Legacy Survey, and DESI, we compare representations from vision
transformers, self-supervised models, and astronomy-specific architectures via
mutual $k$-nearest neighbour analysis. We observe consistent scaling:
representational alignment generally increases with model capacity across our
tested architectures, supporting convergence toward a shared representation of
galaxy astrophysics. Our results suggest that astronomical foundation models
can use pre-trained general-purpose architectures, allowing us to capitalise on
the broader machine learning community's already-spent computational
investment.

</details>


### [153] [Deep learning for exoplanet detection and characterization by direct imaging at high contrast](https://arxiv.org/abs/2509.20310)
*Théo Bodrito,Olivier Flasseur,Julien Mairal,Jean Ponce,Maud Langlois,Anne-Marie Lagrange*

Main category: astro-ph.IM

TL;DR: 提出一种用于高对比度多变量图像序列的多尺度统计模型，通过可学习架构融合多颗恒星的观测数据，显著提高系外行星成像的检测灵敏度和测量精度。


<details>
  <summary>Details</summary>
Motivation: 系外行星成像面临高角分辨率和高对比度的挑战，需要解决图像序列中的噪声成分问题。

Method: 开发多尺度统计模型，集成到可学习架构中，利用问题的物理特性，以检测信噪比最优的方式融合多颗恒星的观测数据。

Result: 应用于VLT/SPHERE仪器数据，该方法显著提高了检测灵敏度以及天体测量和光度测量的准确性。

Conclusion: 该方法为系外行星成像提供了一种有效的解决方案，通过统计建模和机器学习技术改善了高对比度成像的性能。

Abstract: Exoplanet imaging is a major challenge in astrophysics due to the need for
high angular resolution and high contrast. We present a multi-scale statistical
model for the nuisance component corrupting multivariate image series at high
contrast. Integrated into a learnable architecture, it leverages the physics of
the problem and enables the fusion of multiple observations of the same star in
a way that is optimal in terms of detection signal-to-noise ratio. Applied to
data from the VLT/SPHERE instrument, the method significantly improves the
detection sensitivity and the accuracy of astrometric and photometric
estimation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [154] [CollaPipe: Adaptive Segment-Optimized Pipeline Parallelism for Collaborative LLM Training in Heterogeneous Edge Networks](https://arxiv.org/abs/2509.19855)
*Jiewei Chen,Xiumei Deng,Zehui Xiong,Shaoyong Guo,Xuesong Qiu,Ping Wang,Dusit Niyato*

Main category: eess.SY

TL;DR: CollaPipe是一个混合分布式学习框架，结合协作流水线并行和联邦聚合，用于移动边缘计算网络中基于Transformer的大语言模型训练，显著提升计算效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算网络中多智能体协作的需求日益增长，但大语言模型训练面临计算量大、端到端延迟高和模型泛化能力有限等挑战。

Method: 提出CollaPipe框架：将编码器自适应分区部署在移动设备上进行流水线并行训练，解码器部署在边缘服务器处理生成任务，通过联邦聚合进行全局模型更新。设计了DSSDA算法进行动态段调度和资源分配。

Result: 实验表明，CollaPipe计算效率提升15.09%，端到端延迟降低48.98%，单设备内存使用减少一半以上，支持异构动态通信环境中的在线学习。

Conclusion: CollaPipe有效解决了移动边缘环境中大语言模型训练的挑战，为智能移动应用提供了高效的分布式学习解决方案。

Abstract: The increasing demand for intelligent mobile applications has made
multi-agent collaboration with Transformer-based large language models (LLMs)
essential in mobile edge computing (MEC) networks. However, training LLMs in
such environments remains challenging due to heavy computation, high end-to-end
latency, and limited model generalization. We introduce CollaPipe, a hybrid
distributed learning framework that integrates collaborative pipeline
parallelism with federated aggregation to support self-evolving intelligent
networks. In CollaPipe, the encoder part is adaptively partitioned into
variable-sized segments and deployed across mobile devices for
pipeline-parallel training, while the decoder is deployed on edge servers to
handle generative tasks. Then we perform global model update via federated
aggregation. To enhance training efficiency, we formulate a joint optimization
problem that adaptively allocates model segments, micro-batches, bandwidth, and
transmission power. We derive and use a closed-form convergence bound to design
an Dynamic Segment Scheduling and Resource Allocation (DSSDA) algorithm based
on Lyapunov optimization, ensuring system stability under long-term
constraints. Extensive experiments on downstream tasks with Transformer and
BERT models show that CollaPipe improves computation efficiency by up to
15.09%, reduces end-to-end latency by at least 48.98%, and cuts single device
memory usage by more than half, enabling online learning in heterogeneous and
dynamic communication environments.

</details>


### [155] [Modeling and Control of Deep Sign-Definite Dynamics with Application to Hybrid Powertrain Control](https://arxiv.org/abs/2509.19869)
*Teruki Kato,Ryotaro Shima,Kenji Kashima*

Main category: eess.SY

TL;DR: 该论文提出了一种通过符号约束来确保深度学习模型物理一致性和凸性的方法，从而改善复杂系统的预测精度和控制性能。


<details>
  <summary>Details</summary>
Motivation: 标准深度学习模型在复杂大系统应用中往往无法保证物理结构一致性，且由于非凸性导致控制输入不连续，这限制了深度学习在控制领域的应用。

Method: 引入符号约束（Jacobian矩阵元素的符号限制）来统一单调性、正性和符号确定性，开发了满足这些约束的精确线性化深度模型，并将模型预测控制表述为凸二次规划问题。

Result: 在两水箱系统和混合动力系统上的实验表明，该方法相比现有方法提高了预测精度，并产生了更平滑的控制输入。

Conclusion: 所提出的符号约束方法能够有效确保深度学习模型的物理一致性，同时保持控制问题的凸性，为深度学习在控制系统中的应用提供了可行方案。

Abstract: Deep learning is increasingly used for complex, large-scale systems where
first-principles modeling is difficult. However, standard deep learning models
often fail to enforce physical structure or preserve convexity in downstream
control, leading to physically inconsistent predictions and discontinuous
inputs owing to nonconvexity. We introduce sign constraints--sign restrictions
on Jacobian entries--that unify monotonicity, positivity, and
sign-definiteness; additionally, we develop model-construction methods that
enforce them, together with a control-synthesis procedure. In particular, we
design exactly linearizable deep models satisfying these constraints and
formulate model predictive control as a convex quadratic program, which yields
a unique optimizer and a Lipschitz continuous control law. On a two-tank system
and a hybrid powertrain, the proposed approach improves prediction accuracy and
produces smoother control inputs than existing methods.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [156] [Quantum Harmonic Analysis and the Structure in Data: Augmentation](https://arxiv.org/abs/2509.19474)
*Monika Doerfler,Franz Luef,Henry McNulty*

Main category: math.FA

TL;DR: 本文研究了数据增强对高维数据集主成分平滑度的影响，使用量子调和分析工具证明了增强数据集的算子特征函数位于调制空间M^1(R^d)，保证了平滑性和连续性。


<details>
  <summary>Details</summary>
Motivation: 探索数据增强如何影响高维数据集主成分的平滑特性，为流形学习和特征提取算法提供理论指导。

Method: 使用量子调和分析工具，分析增强数据集的算子特征函数在调制空间M^1(R^d)中的性质，并通过合成数据和音频数据的数值实验验证理论结果。

Result: 理论分析表明增强数据集的算子特征函数具有平滑性和连续性，数值实验证实了这一发现。

Conclusion: 研究结果表明，流形学习和特征提取算法可以从系统化和有指导的数据增强原则中受益。

Abstract: In this short note, we study the impact of data augmentation on the
smoothness of principal components of high-dimensional datasets. Using tools
from quantum harmonic analysis, we show that eigenfunctions of operators
corresponding to augmented data sets lie in the modulation space
$M^1(\mathbb{R}^d)$, guaranteeing smoothness and continuity. Numerical examples
on synthetic and audio data confirm the theoretical findings. While interesting
in itself, the results suggest that manifold learning and feature extraction
algorithms can benefit from systematic and informed augmentation principles.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [157] [Anchored Langevin Algorithms](https://arxiv.org/abs/2509.19455)
*Mert Gurbuzbalaban,Hoang M. Nguyen,Xicheng Zhang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 本文提出了锚定朗之万动力学方法，解决了传统朗之万算法无法处理非可微目标函数和重尾分布的局限性，通过引入光滑参考势能和乘性缩放技术，建立了非渐近收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统朗之万算法（如ULA）虽然在高维和大规模数据采样中广泛应用，但存在两个关键限制：需要可微的对数密度函数（无法处理非可微目标），以及无法有效采样重尾分布。

Method: 提出锚定朗之万动力学方法，用光滑参考势能替代原始势能，并通过乘性缩放修改朗之万扩散过程。同时提供了基于朗之万扩散随机时间变化的等效公式。

Result: 在2-Wasserstein距离下建立了到目标分布的非渐近收敛保证，并通过数值实验验证了理论结果和方法的实际性能。

Conclusion: 锚定朗之万动力学为处理非可微目标函数和特定类型重尾分布提供了统一框架，扩展了朗之万算法在机器学习采样中的应用范围。

Abstract: Standard first-order Langevin algorithms such as the unadjusted Langevin
algorithm (ULA) are obtained by discretizing the Langevin diffusion and are
widely used for sampling in machine learning because they scale to high
dimensions and large datasets. However, they face two key limitations: (i) they
require differentiable log-densities, excluding targets with non-differentiable
components; and (ii) they generally fail to sample heavy-tailed targets. We
propose anchored Langevin dynamics, a unified approach that accommodates
non-differentiable targets and certain classes of heavy-tailed distributions.
The method replaces the original potential with a smooth reference potential
and modifies the Langevin diffusion via multiplicative scaling. We establish
non-asymptotic guarantees in the 2-Wasserstein distance to the target
distribution and provide an equivalent formulation derived via a random time
change of the Langevin diffusion. We provide numerical experiments to
illustrate the theory and practical performance of our proposed approach.

</details>


### [158] [Stochastic Path Planning in Correlated Obstacle Fields](https://arxiv.org/abs/2509.19559)
*Li Zhou,Elvan Ceyhan*

Main category: stat.ML

TL;DR: 提出了SCOS问题，一种具有空间相关障碍物的导航设置，开发了基于高斯随机场的贝叶斯信念更新方法，并提出两阶段学习框架来寻找最优遍历策略


<details>
  <summary>Details</summary>
Motivation: 解决在具有空间相关障碍物、不确定阻塞状态、噪声传感器和昂贵识别成本的环境中的导航挑战

Method: 使用高斯随机场建模空间相关性，开发贝叶斯信念更新，提出离线学习鲁棒基础策略和在线滚动策略的两阶段学习框架，支持蒙特卡洛点估计和分布强化学习

Result: 理论证明了相关感知更新的优势和后验采样下的收敛性，在不同障碍密度和传感器能力下的综合实证评估显示相对于基线方法的持续性能提升

Conclusion: 该框架能够有效处理具有对抗性中断或聚集自然危险的环境中的导航问题

Abstract: We introduce the Stochastic Correlated Obstacle Scene (SCOS) problem, a
navigation setting with spatially correlated obstacles of uncertain blockage
status, realistically constrained sensors that provide noisy readings and
costly disambiguation. Modeling the spatial correlation with Gaussian Random
Field (GRF), we develop Bayesian belief updates that refine blockage
probabilities, and use the posteriors to reduce search space for efficiency. To
find the optimal traversal policy, we propose a novel two-stage learning
framework. An offline phase learns a robust base policy via optimistic policy
iteration augmented with information bonus to encourage exploration in
informative regions, followed by an online rollout policy with periodic base
updates via a Bayesian mechanism for information adaptation. This framework
supports both Monte Carlo point estimation and distributional reinforcement
learning (RL) to learn full cost distributions, leading to stronger uncertainty
quantification. We establish theoretical benefits of correlation-aware updating
and convergence property under posterior sampling. Comprehensive empirical
evaluations across varying obstacle densities, sensor capabilities demonstrate
consistent performance gains over baselines. This framework addresses
navigation challenges in environments with adversarial interruptions or
clustered natural hazards.

</details>


### [159] [MAGIC: Multi-task Gaussian process for joint imputation and classification in healthcare time series](https://arxiv.org/abs/2509.19577)
*Dohyun Ku,Catherine D. Chong,Visar Berisha,Todd J. Schwedt,Jing Li*

Main category: stat.ML

TL;DR: MAGIC是一个统一的多任务高斯过程框架，用于同时处理时间序列中的缺失值插补和标签预测，在医疗应用中表现出优越的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列分析面临时间不对齐和数据稀疏两大挑战，传统两步法（先插补后预测）效果有限，需要更高效的统一解决方案。

Method: 结合分层多任务高斯过程和函数逻辑回归，使用泰勒展开近似处理难解似然分量，通过EM算法和块坐标优化进行参数估计。

Result: 在两个医疗应用（轻度脑外伤后头痛改善预测和ICU入院48小时内死亡率预测）中，MAGIC均优于现有方法。

Conclusion: MAGIC能够利用有限样本生成实时准确的预测，有助于早期临床评估和治疗规划，为医疗决策提供支持。

Abstract: Time series analysis has emerged as an important tool for improving patient
diagnosis and management in healthcare applications. However, these
applications commonly face two critical challenges: time misalignment and data
sparsity. Traditional approaches address these issues through a two-step
process of imputation followed by prediction. We propose MAGIC (Multi-tAsk
Gaussian Process for Imputation and Classification), a novel unified framework
that simultaneously performs class-informed missing value imputation and label
prediction within a hierarchical multi-task Gaussian process coupled with
functional logistic regression. To handle intractable likelihood components,
MAGIC employs Taylor expansion approximations with bounded error analysis, and
parameter estimation is performed using EM algorithm with block coordinate
optimization supported by convergence analysis. We validate MAGIC through two
healthcare applications: prediction of post-traumatic headache improvement
following mild traumatic brain injury and prediction of in-hospital mortality
within 48 hours after ICU admission. In both applications, MAGIC achieves
superior predictive accuracy compared to existing methods. The ability to
generate real-time and accurate predictions with limited samples facilitates
early clinical assessment and treatment planning, enabling healthcare providers
to make more informed treatment decisions.

</details>


### [160] [Diffusion and Flow-based Copulas: Forgetting and Remembering Dependencies](https://arxiv.org/abs/2509.19707)
*David Huk,Theodoros Damoulas*

Main category: stat.ML

TL;DR: 本文提出了基于扩散和流原理的Copula建模方法，通过设计两个逐步忘记变量间依赖关系的过程来构建有效的Copula模型，在复杂高维依赖建模方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有Copula模型在处理多模态和高维依赖时受到限制性假设和扩展性差的制约，需要更强大的建模方法

Method: 设计了两个过程：一个逐步忘记变量间依赖关系但保持维度分布不变的过程，以及学习记忆这些被遗忘依赖关系的方法。第一个实例专注于直接密度估计，第二个专注于高效采样

Result: 实验表明，该方法在科学数据集和图像数据上建模复杂高维依赖关系时，性能优于最先进的Copula方法

Conclusion: 该工作增强了Copula模型的表示能力，为在更大规模和更具挑战性领域中的应用铺平了道路

Abstract: Copulas are a fundamental tool for modelling multivariate dependencies in
data, forming the method of choice in diverse fields and applications. However,
the adoption of existing models for multimodal and high-dimensional
dependencies is hindered by restrictive assumptions and poor scaling. In this
work, we present methods for modelling copulas based on the principles of
diffusions and flows. We design two processes that progressively forget
inter-variable dependencies while leaving dimension-wise distributions
unaffected, provably defining valid copulas at all times. We show how to obtain
copula models by learning to remember the forgotten dependencies from each
process, theoretically recovering the true copula at optimality. The first
instantiation of our framework focuses on direct density estimation, while the
second specialises in expedient sampling. Empirically, we demonstrate the
superior performance of our proposed methods over state-of-the-art copula
approaches in modelling complex and high-dimensional dependencies from
scientific datasets and images. Our work enhances the representational power of
copula models, empowering applications and paving the way for their adoption on
larger scales and more challenging domains.

</details>


### [161] [Convex Regression with a Penalty](https://arxiv.org/abs/2509.19788)
*Eunji Lim*

Main category: stat.ML

TL;DR: 提出一种新的凸回归函数估计方法，通过最小化次梯度惩罚并限制平方误差上界来避免边界过拟合问题。该方法能直接从数据估计参数，并证明了估计量的一致性和收敛速率。


<details>
  <summary>Details</summary>
Motivation: 传统的凸回归函数估计方法在边界区域容易过拟合，这在现实应用中带来显著挑战。需要一种能够避免这种过拟合的新估计方法。

Method: 通过最小化次梯度惩罚，同时施加平方误差的上界约束来估计凸回归函数。关键创新是上界参数可以直接从数据中估计。

Result: 证明了所提估计量及其次梯度在定义域上的一致几乎必然收敛性，并推导了收敛速率。通过单服务器队列等待时间估计验证了方法的有效性。

Conclusion: 新方法成功解决了凸回归估计中的边界过拟合问题，具有理论保证和实际应用价值。

Abstract: A common way to estimate an unknown convex regression function $f_0: \Omega
\subset \mathbb{R}^d \rightarrow \mathbb{R}$ from a set of $n$ noisy
observations is to fit a convex function that minimizes the sum of squared
errors. However, this estimator is known for its tendency to overfit near the
boundary of $\Omega$, posing significant challenges in real-world applications.
In this paper, we introduce a new estimator of $f_0$ that avoids this
overfitting by minimizing a penalty on the subgradient while enforcing an upper
bound $s_n$ on the sum of squared errors. The key advantage of this method is
that $s_n$ can be directly estimated from the data. We establish the uniform
almost sure consistency of the proposed estimator and its subgradient over
$\Omega$ as $n \rightarrow \infty$ and derive convergence rates. The
effectiveness of our estimator is illustrated through its application to
estimating waiting times in a single-server queue.

</details>


### [162] [High-Dimensional Statistical Process Control via Manifold Fitting and Learning](https://arxiv.org/abs/2509.19820)
*Burak I. Tas,Enrique del Castillo*

Main category: stat.ML

TL;DR: 本文提出了两种基于流形的高维动态工业过程统计过程控制方法：流形拟合和流形学习，前者通过精确拟合数据流形并监测偏离，后者通过降维后监控嵌入观测，实验表明流形拟合方法性能更优。


<details>
  <summary>Details</summary>
Motivation: 解决高维动态工业过程的统计过程控制问题，传统线性降维方法在处理非线性流形数据时效果有限，需要开发能够有效处理非线性低维流形结构的监控方法。

Method: 1）流形拟合方法：使用先进流形拟合技术精确近似数据所在流形，采用新型无标量分布控制图监测流形偏离；2）流形学习方法：类似线性降维SPC技术，先将数据嵌入低维空间再监控嵌入观测。两种方法都能控制I类错误概率。

Result: 在合成过程和Tennessee Eastman过程上的大量数值实验表明，概念更简单的流形拟合方法性能与经典低维流形监控方法相当甚至更优。在实际电气换向器图像数据集上成功检测表面异常，验证了方法的实用性。

Conclusion: 流形拟合方法为高维动态工业过程的统计过程控制提供了有效解决方案，其简单概念和优越性能使其在实际应用中具有重要价值，特别是在处理非线性流形结构数据时表现出色。

Abstract: We address the Statistical Process Control (SPC) of high-dimensional, dynamic
industrial processes from two complementary perspectives: manifold fitting and
manifold learning, both of which assume data lies on an underlying nonlinear,
lower dimensional space. We propose two distinct monitoring frameworks for
online or 'phase II' Statistical Process Control (SPC). The first method
leverages state-of-the-art techniques in manifold fitting to accurately
approximate the manifold where the data resides within the ambient
high-dimensional space. It then monitors deviations from this manifold using a
novel scalar distribution-free control chart. In contrast, the second method
adopts a more traditional approach, akin to those used in linear dimensionality
reduction SPC techniques, by first embedding the data into a lower-dimensional
space before monitoring the embedded observations. We prove how both methods
provide a controllable Type I error probability, after which they are
contrasted for their corresponding fault detection ability. Extensive numerical
experiments on a synthetic process and on a replicated Tennessee Eastman
Process show that the conceptually simpler manifold-fitting approach achieves
performance competitive with, and sometimes superior to, the more classical
lower-dimensional manifold monitoring methods. In addition, we demonstrate the
practical applicability of the proposed manifold-fitting approach by
successfully detecting surface anomalies in a real image dataset of electrical
commutators.

</details>


### [163] [Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later](https://arxiv.org/abs/2509.19929)
*Arnaud Vadeboncoeur,Gregory Duthé,Mark Girolami,Eleni Chatzi*

Main category: stat.ML

TL;DR: GABI是一个基于几何自动编码器的贝叶斯反演框架，用于从少量噪声观测中恢复物理系统的全场信息，特别适用于复杂可变几何形状的工程系统。


<details>
  <summary>Details</summary>
Motivation: 工程系统通常具有复杂多变的几何形状，这阻碍了标准贝叶斯不确定性量化方法的应用。需要一种能够处理几何变化且不依赖PDE控制方程的先验学习方法。

Method: 采用"先学习后观测"范式，从不同几何形状系统的大数据集中学习几何感知的生成模型作为条件先验。使用近似贝叶斯计算采样实现高效GPU计算，框架架构无关。

Result: 在多个测试案例中表现良好：预测精度与确定性监督学习方法相当；不确定性量化校准良好且在复杂几何问题上具有鲁棒性；提供独立于观测过程的通用基础模型。

Conclusion: GABI提供了一个灵活、几何感知的"一次训练、随处使用"的基础模型框架，能够有效处理复杂几何形状下的贝叶斯反演问题，且不依赖特定观测过程。

Abstract: Uncertainty Quantification (UQ) is paramount for inference in engineering
applications. A common inference task is to recover full-field information of
physical systems from a small number of noisy observations, a usually highly
ill-posed problem. Critically, engineering systems often have complicated and
variable geometries prohibiting the use of standard Bayesian UQ. In this work,
we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework
for learning geometry-aware generative models of physical responses that serve
as highly informative geometry-conditioned priors for Bayesian inversion.
Following a ''learn first, observe later'' paradigm, GABI distills information
from large datasets of systems with varying geometries, without requiring
knowledge of governing PDEs, boundary conditions, or observation processes,
into a rich latent prior. At inference time, this prior is seamlessly combined
with the likelihood of the specific observation process, yielding a
geometry-adapted posterior distribution. Our proposed framework is architecture
agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling
yields an efficient implementation that utilizes modern GPU hardware. We test
our method on: steady-state heat over rectangular domains; Reynold-Averaged
Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source
localization on 3D car bodies; RANS airflow over terrain. We find: the
predictive accuracy to be comparable to deterministic supervised learning
approaches in the restricted setting where supervised learning is applicable;
UQ to be well calibrated and robust on challenging problems with complex
geometries. The method provides a flexible geometry-aware
train-once-use-anywhere foundation model which is independent of any particular
observation process.

</details>


### [164] [BioBO: Biology-informed Bayesian Optimization for Perturbation Design](https://arxiv.org/abs/2509.19988)
*Yanke Li,Tianyu Cui,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: stat.ML

TL;DR: BioBO是一种将贝叶斯优化与生物先验知识结合的方法，通过整合多模态基因嵌入和富集分析，提高基因组扰动实验的设计效率。


<details>
  <summary>Details</summary>
Motivation: 当前贝叶斯优化方法在基因组扰动实验中未能充分利用领域特定的生物先验知识，导致搜索效率不高。

Method: BioBO将贝叶斯优化与多模态基因嵌入和富集分析相结合，在代理模型和获取策略中融入生物学基础先验，偏向有前景的基因同时保持探索能力。

Result: 在公共基准测试中，BioBO将标记效率提高了25-40%，比传统贝叶斯优化更有效地识别出最佳扰动。

Conclusion: BioBO不仅提高了扰动实验的效率，还通过富集分析提供了通路层面的解释，增强了方法的可解释性。

Abstract: Efficient design of genomic perturbation experiments is crucial for
accelerating drug discovery and therapeutic target identification, yet
exhaustive perturbation of the human genome remains infeasible due to the vast
search space of potential genetic interactions and experimental constraints.
Bayesian optimization (BO) has emerged as a powerful framework for selecting
informative interventions, but existing approaches often fail to exploit
domain-specific biological prior knowledge. We propose Biology-Informed
Bayesian Optimization (BioBO), a method that integrates Bayesian optimization
with multimodal gene embeddings and enrichment analysis, a widely used tool for
gene prioritization in biology, to enhance surrogate modeling and acquisition
strategies. BioBO combines biologically grounded priors with acquisition
functions in a principled framework, which biases the search toward promising
genes while maintaining the ability to explore uncertain regions. Through
experiments on established public benchmarks and datasets, we demonstrate that
BioBO improves labeling efficiency by 25-40%, and consistently outperforms
conventional BO by identifying top-performing perturbations more effectively.
Moreover, by incorporating enrichment analysis, BioBO yields pathway-level
explanations for selected perturbations, offering mechanistic interpretability
that links designs to biologically coherent regulatory circuits.

</details>


### [165] [First-Extinction Law for Resampling Processes](https://arxiv.org/abs/2509.20101)
*Matteo Benati,Alessandro Londei,Denise Lanzieri,Vittorio Loreto*

Main category: stat.ML

TL;DR: 本文提出了一种计算重采样过程中首次灭绝时间的闭式解，将多项式更新建模为独立零漂移平方根扩散过程，取代了之前指数复杂度的计算方法。


<details>
  <summary>Details</summary>
Motivation: 重采样过程的灭绝时间计算通常难以处理，因为现有公式的计算复杂度随状态数呈指数增长（2^M）。需要找到更高效的计算方法。

Method: 将多项式更新处理为独立的零漂移平方根扩散过程，推导出首次灭绝时间的闭式规律，将计算复杂度从指数级降低到线性级。

Result: 证明该方法与Wright-Fisher结果完全一致，并通过大量模拟验证了准确性。在简单自训练设置中成功预测了模型崩溃的发生时间。

Conclusion: 这些结果表明了重采样灭绝动力学的统一视角，为相关研究提供了更高效的计算工具。

Abstract: Extinction times in resampling processes are fundamental yet often
intractable, as previous formulas scale as $2^M$ with the number of states $M$
present in the initial probability distribution. We solve this by treating
multinomial updates as independent square-root diffusions of zero drift,
yielding a closed-form law for the first-extinction time. We prove that the
mean coincides exactly with the Wright-Fisher result of Baxter et al., thereby
replacing exponential-cost evaluations with a linear-cost expression, and we
validate this result through extensive simulations. Finally, we demonstrate
predictive power for model collapse in a simple self-training setup: the onset
of collapse coincides with the resampling-driven first-extinction time computed
from the model's initial stationary distribution. These results hint to a
unified view of resampling extinction dynamics.

</details>


### [166] [Error Propagation in Dynamic Programming: From Stochastic Control to Option Pricing](https://arxiv.org/abs/2509.20239)
*Andrea Della Vecchia,Damir Filipović*

Main category: stat.ML

TL;DR: 本文研究了离散时间随机最优控制的理论和方法基础，在动态规划框架下结合非参数回归和蒙特卡洛子采样来估计价值函数，并分析了误差在时间上的反向传播，最后应用于美式期权定价。


<details>
  <summary>Details</summary>
Motivation: 随机最优控制问题在金融等领域有重要应用，但现有文献对价值函数估计误差在时间上的反向传播分析相对不足，需要建立更系统的理论框架。

Method: 在动态规划框架下，使用再生核希尔伯特空间中的核岭回归进行非参数回归，结合蒙特卡洛采样估计继续价值，提出误差分解方法并分析误差的时间传播。

Result: 建立了价值函数估计的误差控制理论，分析了误差从到期日到初始时刻的反向传播规律，为随机最优控制问题提供了理论保证。

Conclusion: 该方法为离散时间随机最优控制问题提供了系统的理论分析框架，特别在误差传播分析方面有创新，并成功应用于美式期权定价问题。

Abstract: This paper investigates theoretical and methodological foundations for
stochastic optimal control (SOC) in discrete time. We start formulating the
control problem in a general dynamic programming framework, introducing the
mathematical structure needed for a detailed convergence analysis. The
associate value function is estimated through a sequence of approximations
combining nonparametric regression methods and Monte Carlo subsampling. The
regression step is performed within reproducing kernel Hilbert spaces (RKHSs),
exploiting the classical KRR algorithm, while Monte Carlo sampling methods are
introduced to estimate the continuation value. To assess the accuracy of our
value function estimator, we propose a natural error decomposition and
rigorously control the resulting error terms at each time step. We then analyze
how this error propagates backward in time-from maturity to the initial stage-a
relatively underexplored aspect of the SOC literature. Finally, we illustrate
how our analysis naturally applies to a key financial application: the pricing
of American options.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [167] [BALANCE: Bitrate-Adaptive Limit-Aware Netcast Content Enhancement Utilizing QUBO and Quantum Annealing](https://arxiv.org/abs/2509.19616)
*Animesh Rajpurohit,Michael Kelley,Wei Wang,Krishna Murthy Kattiyan Ramamoorthy*

Main category: eess.IV

TL;DR: BALANCE是一种基于量子框架的视频流优化方法，通过智能预选视频片段来在数据限制下提升流媒体质量体验


<details>
  <summary>Details</summary>
Motivation: 在数据流量限制日益严格的时代，如何在用户定义的数据上限内优化视频流质量是一个重要挑战

Method: 使用BALANCE量子框架，基于视觉复杂度和预期数据消耗智能预选视频片段，采用VMAF指标提升QoE，通过QUBO问题框架比较Slack变量方法和动态惩罚方法(DPA)进行比特率分配

Result: 与传统ABR流媒体的比特率阶梯相比，在同等数据约束下显著改善了QoE，DPA方法在数据限制增加时比Slack变量方法表现更好，提供更有效和最优的解决方案

Conclusion: 这种新的量子方法显著提升了有限数据计划用户的流媒体满意度

Abstract: In an era of increasing data cap constraints, optimizing video streaming
quality while adhering to user-defined data caps remains a significant
challenge. This paper introduces Bitrate-Adaptive Limit-Aware Netcast Content
Enhancement (BALANCE), a novel Quantum framework aimed at addressing this
issue. BALANCE intelligently pre-selects video segments based on visual
complexity and anticipated data consumption, utilizing the Video Multimethod
Assessment Fusion (VMAF) metric to enhance Quality of Experience (QoE). We
compare our method against traditional bitrate ladders used in Adaptive Bitrate
(ABR) streaming, demonstrating a notable improvement in QoE under equivalent
data constraints. We compare the Slack variable approach with the Dynamic
Penalization Approach (DPA) by framing the bitrate allocation problem through
Quadratic Unconstrained Binary Optimization (QUBO) to effectively enforce data
limits. Our results indicate that the DPA consistently outperforms the Slack
Variable Method, delivering more valid and optimal solutions as data limits
increase. This new quantum approach significantly enhances streaming
satisfaction for users with limited data plans.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [168] [The Pareto Frontier of Resilient Jet Tagging](https://arxiv.org/abs/2509.19431)
*Rikab Gambhir,Matt LeBlanc,Yuanchen Zhou*

Main category: hep-ph

TL;DR: 该论文探讨了在强子喷注分类中，仅依赖单一性能指标（如准确率、AUC或拒绝率）可能导致模型依赖性强、缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前高能对撞机物理中，基于喷注成分运动学信息的分类器设计往往只追求最佳性能指标，但这可能导致架构对特定模型过度依赖，带来分析中的不确定性和偏差。

Method: 通过探索不同网络架构在性能指标和鲁棒性之间的权衡关系，分析高指标但低鲁棒性网络的潜在后果。

Result: 研究表明，仅关注单一性能指标的分类器可能在模型依赖性方面表现较差，影响分析的可靠性。

Conclusion: 在强子喷注分类器设计中，需要平衡性能指标和鲁棒性，避免过度依赖单一指标导致的模型偏差问题。

Abstract: Classifying hadronic jets using their constituents' kinematic information is
a critical task in modern high-energy collider physics. Often, classifiers are
designed by targeting the best performance using metrics such as accuracy, AUC,
or rejection rates. However, the use of a single metric can lead to the use of
architectures that are more model-dependent than competitive alternatives,
leading to potential uncertainty and bias in analysis. We explore such
trade-offs and demonstrate the consequences of using networks with high
performance metrics but low resilience.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [169] [Supercomputing for High-speed Avoidance and Reactive Planning in Robots](https://arxiv.org/abs/2509.19486)
*Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: SHARP是一个概念验证研究，展示了高性能计算（HPC）如何实现机器人控制的毫秒级响应。通过将轨迹规划任务卸载到HPC集群，在7自由度机械臂躲避高速泡沫弹的测试中，实现了22.9毫秒（本地）和30.0毫秒（远程）的平均规划延迟，成功率分别达到84%和88%。


<details>
  <summary>Details</summary>
Motivation: 现代机器人在人机共享工作空间中对反应性要求越来越高，但机载处理器受限于尺寸、功耗和成本。HPC卸载提供了大规模并行计算能力，但其在实时机器人应用中的可行性因网络延迟和抖动而不确定。

Method: 使用MPI在本地和远程HPC集群上实现并行化多目标A*搜索算法，评估7自由度机械臂躲避高速泡沫弹的应力测试场景。

Result: 系统实现了平均规划延迟22.9毫秒（本地）和30.0毫秒（远程，约300公里外），躲避成功率分别为84%和88%。当往返延迟保持在数十毫秒范围内时，HPC侧计算不再是瓶颈。

Conclusion: SHARP证明了HPC卸载是动态环境中实现可靠、反应性机器人的可行途径，提出了混合控制架构：低级反射保留在机载确保安全，而突发性高吞吐量规划任务卸载到HPC实现可扩展性。

Abstract: This paper presents SHARP (Supercomputing for High-speed Avoidance and
Reactive Planning), a proof-of-concept study demonstrating how high-performance
computing (HPC) can enable millisecond-scale responsiveness in robotic control.
While modern robots face increasing demands for reactivity in human--robot
shared workspaces, onboard processors are constrained by size, power, and cost.
Offloading to HPC offers massive parallelism for trajectory planning, but its
feasibility for real-time robotics remains uncertain due to network latency and
jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator
must dodge high-speed foam projectiles. Using a parallelized multi-goal A*
search implemented with MPI on both local and remote HPC clusters, the system
achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300
km away), with avoidance success rates of 84% and 88%, respectively. These
results show that when round-trip latency remains within the
tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,
enabling avoidance well below human reaction times. The SHARP results motivate
hybrid control architectures: low-level reflexes remain onboard for safety,
while bursty, high-throughput planning tasks are offloaded to HPC for
scalability. By reporting per-stage timing and success rates, this study
provides a reproducible template for assessing real-time feasibility of
HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable
pathway toward dependable, reactive robots in dynamic environments.

</details>


### [170] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: HUNT是一个实时框架，将无人机的高速穿越、目标获取和跟踪统一在相对导航框架中，能够在没有全局定位和感知受限的条件下实现自主搜索救援


<details>
  <summary>Details</summary>
Motivation: 解决无人机在未知非结构化环境中高速穿越和跟踪目标的双重挑战，特别是在感知受限和没有全局定位的情况下

Method: 基于机载即时观测数据（姿态、高度、速度）定义导航目标，使用统一的感知-控制管道，在搜索阶段实现反应式高速飞行，检测到目标后无缝切换到跟踪模式

Result: 在密集森林、集装箱场地和搜索救援场景中的户外实验表明，该框架在全局方法失效的情况下仍能实现鲁棒自主性

Conclusion: HUNT框架成功地将穿越、获取和跟踪统一在单一相对公式中，为搜索救援操作提供了有效的自主解决方案

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [171] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: ROPA是一种用于双手操作模仿学习的离线数据增强方法，通过微调Stable Diffusion合成第三人称RGB和RGB-D观察数据，并生成对应的关节空间动作标签。


<details>
  <summary>Details</summary>
Motivation: 收集多样且精确的真实世界双手操作演示数据成本高、耗时多，限制了可扩展性。现有数据增强方法主要针对手腕相机设置或仅生成新图像而无配对动作，缺乏针对第三人称RGB-D训练的动作标签增强。

Method: 通过微调Stable Diffusion合成第三人称RGB和RGB-D观察数据，同时生成对应的关节空间动作标签，并采用约束优化确保双手操作中夹爪与物体接触的物理一致性。

Result: 在5个模拟任务和3个真实世界任务中进行了评估，2625次模拟试验和300次真实世界试验表明ROPA优于基线方法和消融实验。

Conclusion: ROPA在第三人称双手操作的RGB和RGB-D数据增强方面显示出良好的可扩展性潜力。

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [172] [Self-evolved Imitation Learning in Simulated World](https://arxiv.org/abs/2509.19460)
*Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu*

Main category: cs.RO

TL;DR: SEIL是一个自进化模仿学习框架，通过模拟器交互逐步改进少样本模型，使用双级增强和轻量级选择器来减少对大规模专家演示的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习中大规模专家演示数据收集成本高、劳动密集的问题，特别是在多任务通用智能体训练中需要大量监督数据。

Method: 提出SEIL框架：1）模型在模拟器中尝试任务并收集成功轨迹作为新演示；2）采用双级增强：模型级使用EMA模型协作，环境级引入初始位置变化；3）使用轻量级选择器筛选互补信息轨迹。

Result: 在LIBERO基准测试中，SEIL在少样本模仿学习场景下达到了新的最先进性能，用更少的训练样本实现了有竞争力的表现。

Conclusion: SEIL框架有效减少了模仿学习对大规模专家演示的依赖，通过自进化机制和智能轨迹选择实现了高效的少样本学习。

Abstract: Imitation learning has been a trend recently, yet training a generalist agent
across multiple tasks still requires large-scale expert demonstrations, which
are costly and labor-intensive to collect. To address the challenge of limited
supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework
that progressively improves a few-shot model through simulator interactions.
The model first attempts tasksin the simulator, from which successful
trajectories are collected as new demonstrations for iterative refinement. To
enhance the diversity of these demonstrations, SEIL employs dual-level
augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model
to collaborate with the primary model, and (ii) Environment-level, introducing
slight variations in initial object positions. We further introduce a
lightweight selector that filters complementary and informative trajectories
from the generated pool to ensure demonstration quality. These curated samples
enable the model to achieve competitive performance with far fewer training
examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves
a new state-of-the-art performance in few-shot imitation learning scenarios.
Code is available at https://github.com/Jasper-aaa/SEIL.git.

</details>


### [173] [OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](https://arxiv.org/abs/2509.19480)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 提出了OmniVLA训练框架，通过多模态目标条件化实现机器人导航，支持2D位姿、第一人称图像和自然语言等多种目标模态的灵活组合。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航策略通常只针对单一模态进行训练，限制了在真实场景中的适应性，而人类能够灵活解释和组合不同形式的目标规范。

Method: 采用高容量视觉-语言-动作（VLA）骨干网络，通过随机模态融合策略训练三种主要目标模态（2D位姿、第一人称图像、自然语言）及其组合。

Result: OmniVLA模型在未见环境中表现出强泛化能力，对稀缺模态具有鲁棒性，能够遵循新的自然语言指令，在各项模态上均优于专业基线模型。

Conclusion: OmniVLA为构建广泛泛化和灵活的导航策略迈出了重要一步，为构建全模态机器人基础模型提供了可扩展的路径。

Abstract: Humans can flexibly interpret and compose different goal specifications, such
as language instructions, spatial coordinates, or visual references, when
navigating to a destination. In contrast, most existing robotic navigation
policies are trained on a single modality, limiting their adaptability to
real-world scenarios where different forms of goal specification are natural
and complementary. In this work, we present a training framework for robotic
foundation models that enables omni-modal goal conditioning for vision-based
navigation. Our approach leverages a high-capacity vision-language-action (VLA)
backbone and trains with three primary goal modalities: 2D poses, egocentric
images, and natural language, as well as their combinations, through a
randomized modality fusion strategy. This design not only expands the pool of
usable datasets but also encourages the policy to develop richer geometric,
semantic, and visual representations. The resulting model, OmniVLA, achieves
strong generalization to unseen environments, robustness to scarce modalities,
and the ability to follow novel natural language instructions. We demonstrate
that OmniVLA outperforms specialist baselines across modalities and offers a
flexible foundation for fine-tuning to new modalities and tasks. We believe
OmniVLA provides a step toward broadly generalizable and flexible navigation
policies, and a scalable path for building omni-modal robotic foundation
models. We present videos showcasing OmniVLA performance and will release its
checkpoints and training code on our project page.

</details>


### [174] [AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](https://arxiv.org/abs/2509.19555)
*Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出了一种约束参数化的潜在安全过滤器，能够在运行时根据用户指定的安全约束进行自适应调整，解决了传统安全过滤器在部署过程中约束固定不变的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在安全过滤器方法假设安全约束是预先已知且在部署过程中保持固定的，这限制了安全过滤器在不同场景下的适应性。本文旨在开发一种能够在运行时根据任意安全约束进行自适应的安全过滤器。

Method: 通过将安全约束定义为基于约束图像编码的条件，使用潜在空间相似性度量来参数化安全过滤器。通过保形校准来对齐故障相似性概念，控制系统接近约束表示的程度。整个参数化安全过滤器在世界模型的想象中训练，将模型看到的任何图像视为潜在的测试时约束。

Result: 在基于视觉的控制任务仿真和硬件实验中，该方法能够在运行时通过条件化用户指定的约束图像编码来适应不同安全约束，同时不牺牲性能。

Conclusion: 提出的约束参数化潜在安全过滤器方法实现了对任意安全约束的运行时自适应，为复杂视觉控制任务提供了灵活且安全可靠的解决方案。

Abstract: Recent works have shown that foundational safe control methods, such as
Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space
of world models. While this enables the synthesis of latent safety filters for
hard-to-model vision-based tasks, they assume that the safety constraint is
known a priori and remains fixed during deployment, limiting the safety
filter's adaptability across scenarios. To address this, we propose
constraint-parameterized latent safety filters that can adapt to user-specified
safety constraints at runtime. Our key idea is to define safety constraints by
conditioning on an encoding of an image that represents a constraint, using a
latent-space similarity measure. The notion of similarity to failure is aligned
in a principled way through conformal calibration, which controls how closely
the system may approach the constraint representation. The parameterized safety
filter is trained entirely within the world model's imagination, treating any
image seen by the model as a potential test-time constraint, thereby enabling
runtime adaptation to arbitrary safety constraints. In simulation and hardware
experiments on vision-based control tasks with a Franka manipulator, we show
that our method adapts at runtime by conditioning on the encoding of
user-specified constraint images, without sacrificing performance. Video
results can be found on https://any-safe.github.io

</details>


### [175] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: EgoBridge是一个统一的协同训练框架，通过域适应方法对齐人类和机器人数据的策略潜在空间，解决了视觉外观、传感器模态和运动学方面的领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 利用以自我为中心的人类经验数据来扩展机器人操作的端到端模仿学习，但人类和机器人之间存在显著的领域差距，阻碍了知识迁移。

Method: 使用最优传输（OT）度量联合策略潜在特征和动作的差异，学习既能在人类和机器人领域对齐，又能保留策略学习关键的动作相关信息的观察表示。

Result: 在三个真实世界的单臂和双手操作任务中，EgoBridge比人类增强的跨具身基线实现了44%的绝对策略成功率提升，并且能够泛化到仅在人类数据中见过的新物体、场景和任务。

Conclusion: EgoBridge框架有效解决了人类-机器人领域差距问题，显著提升了模仿学习的性能，并展示了良好的泛化能力。

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [176] [Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization](https://arxiv.org/abs/2509.19688)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: cs.RO

TL;DR: 提出一种基于学习生成运动规划器的形式化安全验证方法，通过小型神经跟踪控制器稳定GMP生成的参考轨迹，然后对闭环动力学应用神经网络验证，从而在保持GMP表达能力的同时实现可验证性。


<details>
  <summary>Details</summary>
Motivation: 生成运动规划器(GMPs)相比传统规划器具有优势，但由于神经网络验证工具只能处理几百个神经元，而GMPs通常包含数百万参数，因此验证其输出安全性和动态可行性很困难。

Method: 通过从GMP采样参考轨迹，用小型神经跟踪控制器进行稳定，然后对闭环系统应用神经网络验证，构建已验证的GMP参考轨迹库，在线部署时模仿原始GMP分布以确保安全。

Result: 在多种规划器(包括扩散模型、流匹配和视觉语言模型)上评估，在仿真(地面机器人和四旋翼)和硬件(差速驱动机器人)上均提高了安全性。

Conclusion: 该方法能够在保持GMP表达能力的同时实现形式化安全验证，无需重新训练即可提高安全性，适用于各种类型的生成运动规划器。

Abstract: We present a method for formal safety verification of learning-based
generative motion planners. Generative motion planners (GMPs) offer advantages
over traditional planners, but verifying the safety and dynamic feasibility of
their outputs is difficult since neural network verification (NNV) tools scale
only to a few hundred neurons, while GMPs often contain millions. To preserve
GMP expressiveness while enabling verification, our key insight is to imitate
the GMP by stabilizing references sampled from the GMP with a small neural
tracking controller and then applying NNV to the closed-loop dynamics. This
yields reachable sets that rigorously certify closed-loop safety, while the
controller enforces dynamic feasibility. Building on this, we construct a
library of verified GMP references and deploy them online in a way that
imitates the original GMP distribution whenever it is safe to do so, improving
safety without retraining. We evaluate across diverse planners, including
diffusion, flow matching, and vision-language models, improving safety in
simulation (on ground robots and quadcopters) and on hardware
(differential-drive robot).

</details>


### [177] [Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.19696)
*Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner*

Main category: cs.RO

TL;DR: 提出了Diffusion-Based Impedance Learning框架，结合学习方法和阻抗控制，通过Transformer-based Diffusion模型重建模拟零力轨迹，实现实时扭矩控制和自主刚度适应。


<details>
  <summary>Details</summary>
Motivation: 学习方法擅长运动生成但不适合物理交互，阻抗控制需要任务感知调参。结合两者优势，实现物理AI的融合。

Method: 使用基于Transformer的扩散模型，通过交叉注意力处理外部力矩，重建模拟零力轨迹。引入SLERP-based四元数噪声调度器保证旋转几何一致性。通过能量估计器更新刚度和阻尼参数。

Result: 在KUKA LBR iiwa机器人上实现亚毫米位置精度和亚度旋转精度。在跑酷场景中平滑穿越力/速度限制，在圆柱、方形和星形孔插入任务中达到30/30成功率。

Conclusion: 这是迈向物理AI的重要一步，融合了基于模型的物理交互控制和基于学习的轨迹生成方法。

Abstract: Learning methods excel at motion generation in the information domain but are
not primarily designed for physical interaction in the energy domain. Impedance
Control shapes physical interaction but requires task-aware tuning by selecting
feasible impedance parameters. We present Diffusion-Based Impedance Learning, a
framework that combines both domains. A Transformer-based Diffusion Model with
cross-attention to external wrenches reconstructs a simulated Zero-Force
Trajectory (sZFT). This captures both translational and rotational task-space
behavior. For rotations, we introduce a novel SLERP-based quaternion noise
scheduler that ensures geometric consistency. The reconstructed sZFT is then
passed to an energy-based estimator that updates stiffness and damping
parameters. A directional rule is applied that reduces impedance along non task
axes while preserving rigidity along task directions. Training data were
collected for a parkour scenario and robotic-assisted therapy tasks using
teleoperation with Apple Vision Pro. With only tens of thousands of samples,
the model achieved sub-millimeter positional accuracy and sub-degree rotational
accuracy. Its compact model size enabled real-time torque control and
autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller
achieved smooth parkour traversal within force and velocity limits and 30/30
success rates for cylindrical, square, and star peg insertions without any
peg-specific demonstrations in the training data set. All code for the
Transformer-based Diffusion Model, the robot controller, and the Apple Vision
Pro telemanipulation framework is publicly available. These results mark an
important step towards Physical AI, fusing model-based control for physical
interaction with learning-based methods for trajectory generation.

</details>


### [178] [VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation](https://arxiv.org/abs/2509.20322)
*Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu*

Main category: cs.RO

TL;DR: VisualMimic是一个视觉模拟到现实的框架，将自我中心视觉与分层全身控制相结合，实现人形机器人在非结构化环境中的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖外部运动捕捉系统，要么无法在不同任务间泛化，需要一种能够整合自我中心感知和全身控制的新方法。

Method: 结合任务无关的低级关键点跟踪器（通过师生方案从人类运动数据训练）和任务特定的高级策略（从视觉和本体感受输入生成关键点命令），通过注入噪声和基于人类运动统计的动作裁剪来确保稳定训练。

Result: 实现了从模拟到真实人形机器人的零样本迁移，完成了多种运动操作任务（如箱子搬运、推物、足球运球和踢球），并在室外环境中表现出鲁棒泛化能力。

Conclusion: VisualMimic框架成功地将视觉感知与分层控制相结合，为人形机器人在非结构化环境中的运动操作任务提供了有效的解决方案。

Abstract: Humanoid loco-manipulation in unstructured environments demands tight
integration of egocentric perception and whole-body control. However, existing
approaches either depend on external motion capture systems or fail to
generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real
framework that unifies egocentric vision with hierarchical whole-body control
for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint
tracker -- trained from human motion data via a teacher-student scheme -- with
a task-specific high-level policy that generates keypoint commands from visual
and proprioceptive input. To ensure stable training, we inject noise into the
low-level policy and clip high-level actions using human motion statistics.
VisualMimic enables zero-shot transfer of visuomotor policies trained in
simulation to real humanoid robots, accomplishing a wide range of
loco-manipulation tasks such as box lifting, pushing, football dribbling, and
kicking. Beyond controlled laboratory settings, our policies also generalize
robustly to outdoor environments. Videos are available at:
https://visualmimic.github.io .

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [179] [A Deep Dive into the Impact of Solar Storms on LEO Satellite Networks](https://arxiv.org/abs/2509.19647)
*Eunju Kang,Alagappan Ramanathan,Sangeetha Abdu Jyothi*

Main category: astro-ph.EP

TL;DR: 本文深入研究了太阳风暴对低地球轨道（LEO）卫星通信的影响，揭示了网络性能退化的空间异质性，发现不同轨道和卫星受影响程度不均，并指出卫星自主机动可能是RTT持续增加的原因。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注太阳风暴对LEO卫星网络的影响，但这些研究仅提供粗粒度的性能洞察，未能捕捉到网络中断的细微时空模式。本文旨在通过更精细的分析揭示太阳风暴影响的空间异质性。

Method: 通过将大气阻力增加的影响定位到单个卫星和轨道层面，分析不同地理区域和卫星定位在风暴期间的性能退化差异。

Result: 研究发现：（i）并非所有卫星轨道同等脆弱；（ii）在同一轨道内，不同卫星根据其相对于地磁条件的位置受到不成比例的影响；（iii）卫星自主机动可能是RTT持续增加的原因。

Conclusion: 研究揭示了LEO卫星星座中先前被忽视的脆弱性模式，强调需要更自适应、区域感知的缓解策略来应对空间天气引起的网络中断。

Abstract: Low Earth Orbit (LEO) satellite networks are an important part of the global
communication infrastructure today. Despite ongoing efforts to improve their
resilience, they remain vulnerable to component damage and deorbiting under
harsh space weather conditions. Prior work identified a modest but noticeable
impact on LEO satellite network performance during solar storms, typically
manifesting as an immediate rise in packet loss and a sustained increase in
round-trip time (RTT). However, these studies offer only coarse-grained
insights and do not capture the nuanced spatial and temporal patterns of
disruption across the LEO network.
  In this paper, we conduct a deep dive into the impact of solar storms on LEO
satellite communications. By localizing the impact of increased atmospheric
drag at the level of individual satellites and orbits, we reveal significant
heterogeneity in how different parts of the network are affected. We find that
the degree of performance degradation varies significantly across geographic
regions, depending on satellite positioning during the storm. Specifically, we
find that (i) not all satellite orbits are equally vulnerable, (ii) within a
given orbit, certain satellites experience disproportionate impact depending on
their position relative to geomagnetic conditions, and (iii) autonomous
maneuvering of satellites might be a cause of the sustained increase in RTT.
Our findings uncover previously overlooked patterns of vulnerability in LEO
satellite constellations and highlight the need for more adaptive, region-aware
mitigation strategies to address space weather-induced network disruptions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [180] [Examining the robustness of Physics-Informed Neural Networks to noise for Inverse Problems](https://arxiv.org/abs/2509.20191)
*Aleksandra Jekic,Afroditi Natsaridou,Signe Riemer-Sørensen,Helge Langseth,Odd Erik Gundersen*

Main category: physics.comp-ph

TL;DR: 该论文比较了物理信息神经网络(PINNs)与传统有限元方法在求解偏微分方程逆问题上的性能表现，发现虽然PINNs需要较少的人力和专业知识，但在计算时间和精度方面仍逊于传统方法。


<details>
  <summary>Details</summary>
Motivation: PINNs作为机器学习方法在求解PDEs方面被广泛认为不如传统方法，但声称在解决逆问题和处理噪声数据方面有潜力。研究旨在验证PINNs在逆问题上的实际表现。

Method: 在一系列难度递增的流体力学问题上，对比PINNs与有限元方法结合数值优化器的性能，测试包含有噪声和无噪声两种情况。

Result: PINNs在逆问题求解上被传统方法超越，但随着维度增加和数据量增多，两者差距有所减小。同时识别了PINNs训练过程中的常见失败模式。

Conclusion: PINNs虽然在易用性上有优势，但在逆问题求解性能上仍需改进，特别是在处理训练失败问题上需要解决，才能与传统方法竞争。

Abstract: Approximating solutions to partial differential equations (PDEs) is
fundamental for the modeling of dynamical systems in science and engineering.
Physics-informed neural networks (PINNs) are a recent machine learning-based
approach, for which many properties and limitations remain unknown. PINNs are
widely accepted as inferior to traditional methods for solving PDEs, such as
the finite element method, both with regard to computation time and accuracy.
However, PINNs are commonly claimed to show promise in solving inverse problems
and handling noisy or incomplete data. We compare the performance of PINNs in
solving inverse problems with that of a traditional approach using the finite
element method combined with a numerical optimizer. The models are tested on a
series of increasingly difficult fluid mechanics problems, with and without
noise. We find that while PINNs may require less human effort and specialized
knowledge, they are outperformed by the traditional approach. However, the
difference appears to decrease with higher dimensions and more data. We
identify common failures during training to be addressed if the performance of
PINNs on noisy inverse problems is to become more competitive.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [181] [Choose Your Battles: Distributed Learning Over Multiple Tug of War Games](https://arxiv.org/abs/2509.20147)
*Siddharth Chandak,Ilai Bistritz,Nicholas Bambos*

Main category: cs.GT

TL;DR: 本文提出了一种分布式算法Meta Tug-of-Peace，用于解决多玩家参与多个同时进行的拔河游戏（Meta-ToW）的均衡问题，通过简单的随机逼近和1位通信实现收敛。


<details>
  <summary>Details</summary>
Motivation: 解决多玩家在多游戏环境中的分布式决策问题，适用于功率控制、分布式任务分配和传感器网络激活等场景，需要满足服务质量要求。

Method: 使用Meta Tug-of-Peace算法，结合随机逼近进行动作更新，通过不频繁的1位通信实现游戏切换决策。

Result: 算法在Meta-ToW游戏中收敛到满足玩家目标服务质量奖励向量的均衡点。

Conclusion: 模拟验证了算法在功率控制、任务分配和传感器网络等场景的有效性，证明了其收敛性和实用性。

Abstract: Consider N players and K games taking place simultaneously. Each of these
games is modeled as a Tug-of-War (ToW) game where increasing the action of one
player decreases the reward for all other players. Each player participates in
only one game at any given time. At each time step, a player decides the game
in which they wish to participate in and the action they take in that game.
Their reward depends on the actions of all players that are in the same game.
This system of K games is termed `Meta Tug-of-War' (Meta-ToW) game. These games
can model scenarios such as power control, distributed task allocation, and
activation in sensor networks. We propose the Meta Tug-of-Peace algorithm, a
distributed algorithm where the action updates are done using a simple
stochastic approximation algorithm, and the decision to switch games is made
using an infrequent 1-bit communication between the players. We prove that in
Meta-ToW games, our algorithm converges to an equilibrium that satisfies a
target Quality of Service reward vector for the players. We then demonstrate
the efficacy of our algorithm through simulations for the scenarios mentioned
above.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [182] [Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees](https://arxiv.org/abs/2509.20345)
*Meshi Bashari,Yonghoon Lee,Roy Maor Lotan,Edgar Dobriban,Yaniv Romano*

Main category: stat.ME

TL;DR: 提出了GESPI框架，通过结合合成数据和真实数据来安全提升统计推断的样本效率，自适应地根据合成数据质量调整使用策略。


<details>
  <summary>Details</summary>
Motivation: 高质量合成数据的快速扩散为统计推断带来了机遇和挑战，需要一种能够安全利用合成数据提升效率的方法。

Method: 开发了通用合成驱动的推断框架，可包装任何统计推断过程，根据合成数据质量自适应选择使用合成数据或仅使用真实数据。

Result: 方法误差保持在用户指定范围内，无需对合成数据做分布假设，且误差随合成数据质量提升而降低。

Conclusion: GESPI框架能够灵活应用于多种推断任务，在有限标注数据的挑战性任务中表现出显著优势。

Abstract: The rapid proliferation of high-quality synthetic data -- generated by
advanced AI models or collected as auxiliary data from related tasks --
presents both opportunities and challenges for statistical inference. This
paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that
wraps around any statistical inference procedure to safely enhance sample
efficiency by combining synthetic and real data. Our framework leverages
high-quality synthetic data to boost statistical power, yet adaptively defaults
to the standard inference method using only real data when synthetic data is of
low quality. The error of our method remains below a user-specified bound
without any distributional assumptions on the synthetic data, and decreases as
the quality of the synthetic data improves. This flexibility enables seamless
integration with conformal prediction, risk control, hypothesis testing, and
multiple testing procedures, all without modifying the base inference method.
We demonstrate the benefits of our method on challenging tasks with limited
labeled data, including AlphaFold protein structure prediction, and comparing
large reasoning models on complex math problems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [183] [Efficient $\varepsilon$-approximate minimum-entropy couplings](https://arxiv.org/abs/2509.19598)
*Spencer Compton*

Main category: cs.IT

TL;DR: 本文设计了一种多项式时间近似方案（PTAS），用于计算离散概率分布的最小熵耦合问题，对于常数m的情况，能够在多项式时间内获得接近最优解的近似解。


<details>
  <summary>Details</summary>
Motivation: 最小熵耦合问题是NP难问题，之前的最佳多项式时间算法只能保证H(ALG) ≤ H(OPT) + c的近似比，其中c约为0.53（m=2）和1.22（一般m）。主要开放问题是该问题是否APX难，或者是否存在PTAS。

Method: 设计了一种算法，在运行时间n^O(poly(1/ε)·exp(m))内产生熵为H(ALG) ≤ H(OPT) + ε的耦合，表明对于常数m存在PTAS。

Result: 证明了对于常数m，最小熵耦合问题存在多项式时间近似方案（PTAS），这是该问题近似算法研究的重要突破。

Conclusion: 该工作解决了最小熵耦合问题近似算法研究中的一个主要开放问题，为常数m的情况提供了PTAS，推进了该问题的计算复杂性理解。

Abstract: Given $m \ge 2$ discrete probability distributions over $n$ states each, the
minimum-entropy coupling is the minimum-entropy joint distribution whose
marginals are the same as the input distributions. Computing the
minimum-entropy coupling is NP-hard, but there has been significant progress in
designing approximation algorithms; prior to this work, the best known
polynomial-time algorithms attain guarantees of the form $H(\operatorname{ALG})
\le H(\operatorname{OPT}) + c$, where $c \approx 0.53$ for $m=2$, and $c
\approx 1.22$ for general $m$ [CKQGK '23].
  A main open question is whether this task is APX-hard, or whether there
exists a polynomial-time approximation scheme (PTAS). In this work, we design
an algorithm that produces a coupling with entropy $H(\operatorname{ALG}) \le
H(\operatorname{OPT}) + \varepsilon$ in running time
$n^{O(\operatorname{poly}(1/\varepsilon) \cdot \operatorname{exp}(m) )}$:
showing a PTAS exists for constant $m$.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [184] [Discovery of Sustainable Refrigerants through Physics-Informed RL Fine-Tuning of Sequence Models](https://arxiv.org/abs/2509.19588)
*Adrien Goldszal,Diego Calanzone,Vincent Taboga,Pierre-Luc Bacon*

Main category: physics.chem-ph

TL;DR: Refgen是一个结合机器学习和物理基础的生成式管道，用于发现新型环保制冷剂，解决了当前制冷剂数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 当前空调系统中使用的氢氟碳化合物等制冷剂是强效温室气体，正在被逐步淘汰。现有制冷剂数据稀缺（仅约300种），限制了纯数据驱动方法的有效性。

Method: Refgen整合了机器学习与物理基础归纳偏置，包括分子生成微调、关键性质预测模型、状态方程、热化学多项式以及完整的蒸汽压缩循环模拟。通过强化学习在热力学约束下进行微调。

Result: 该方法能够有效利用稀缺数据，实现超越已知化合物集合的新型制冷剂发现。

Conclusion: 通过将物理学嵌入学习过程，Refgen能够平衡效率、安全性和环境影响，为环保制冷剂的发现提供了有效途径。

Abstract: Most refrigerants currently used in air-conditioning systems, such as
hydrofluorocarbons, are potent greenhouse gases and are being phased down.
Large-scale molecular screening has been applied to the search for
alternatives, but in practice only about 300 refrigerants are known, and only a
few additional candidates have been suggested without experimental validation.
This scarcity of reliable data limits the effectiveness of purely data-driven
methods. We present Refgen, a generative pipeline that integrates machine
learning with physics-grounded inductive biases. Alongside fine-tuning for
valid molecular generation, Refgen incorporates predictive models for critical
properties, equations of state, thermochemical polynomials, and full vapor
compression cycle simulations. These models enable reinforcement learning
fine-tuning under thermodynamic constraints, enforcing consistency and guiding
discovery toward molecules that balance efficiency, safety, and environmental
impact. By embedding physics into the learning process, Refgen leverages scarce
data effectively and enables de novo refrigerant discovery beyond the known set
of compounds.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [185] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: 本文为einsum符号提供了理论基础，定义了einsum语言并证明了张量表达式的重要等价规则


<details>
  <summary>Details</summary>
Motivation: einsum符号虽然在实践中被广泛使用，但缺乏统一的理论基础，限制了形式化推理和系统优化的机会

Method: 讨论张量表达式的术语，提供einsum语言的形式化定义，基于此定义形式化并证明张量表达式的重要等价规则

Result: 建立了einsum符号的理论基础，证明了重要的等价规则，并展示了这些规则在实际应用中的相关性

Conclusion: 这项工作为einsum符号提供了坚实的理论基础，有助于形式化推理和系统优化

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [186] [Projective Kolmogorov Arnold Neural Networks (P-KANs): Entropy-Driven Functional Space Discovery for Interpretable Machine Learning](https://arxiv.org/abs/2509.20049)
*Alastair Poole,Stig McArthur,Saravan Kumar*

Main category: cs.NE

TL;DR: 提出Projective Kolmogorov-Arnold Networks (P-KANs)，通过熵最小化技术优化KANs的边函数表示，解决现有KANs在高维样条参数空间中的冗余问题，实现参数压缩和泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 当前KANs实现存在高维样条参数空间冗余问题，导致模型容易过拟合且泛化能力差。需要一种方法在保持灵活性的同时引导边函数发现朝向可解释的功能表示。

Method: 引入基于信号分析和稀疏字典学习的熵最小化技术，通过"引力"项引导边函数收敛到最优功能表示。利用投影系数的熵分析识别最优表示，将边函数压缩到低参数投影空间（傅里叶、切比雪夫、贝塞尔等）。

Result: P-KANs在多个领域表现优异，实现高达80%的参数减少，保持表示能力，显著提高对噪声的鲁棒性，并成功应用于工业自动纤维铺放预测。

Conclusion: P-KANs能够自动发现混合功能表示，不同边收敛到不同最优空间，为科学机器学习应用提供压缩优势和增强的可解释性。

Abstract: Kolmogorov-Arnold Networks (KANs) relocate learnable nonlinearities from
nodes to edges, demonstrating remarkable capabilities in scientific machine
learning and interpretable modeling. However, current KAN implementations
suffer from fundamental inefficiencies due to redundancy in high-dimensional
spline parameter spaces, where numerous distinct parameterisations yield
functionally equivalent behaviors. This redundancy manifests as a "nuisance
space" in the model's Jacobian, leading to susceptibility to overfitting and
poor generalization. We introduce Projective Kolmogorov-Arnold Networks
(P-KANs), a novel training framework that guides edge function discovery
towards interpretable functional representations through entropy-minimisation
techniques from signal analysis and sparse dictionary learning. Rather than
constraining functions to predetermined spaces, our approach maintains spline
space flexibility while introducing "gravitational" terms that encourage
convergence towards optimal functional representations. Our key insight
recognizes that optimal representations can be identified through entropy
analysis of projection coefficients, compressing edge functions to
lower-parameter projective spaces (Fourier, Chebyshev, Bessel). P-KANs
demonstrate superior performance across multiple domains, achieving up to 80%
parameter reduction while maintaining representational capacity, significantly
improved robustness to noise compared to standard KANs, and successful
application to industrial automated fiber placement prediction. Our approach
enables automatic discovery of mixed functional representations where different
edges converge to different optimal spaces, providing both compression benefits
and enhanced interpretability for scientific machine learning applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [187] [FusedANN: Convexified Hybrid ANN via Attribute-Vector Fusion](https://arxiv.org/abs/2509.19767)
*Alireza Heidari,Wei Zhang,Ying Xiong*

Main category: cs.IR

TL;DR: FusedANN是一个几何框架，通过拉格朗日松弛将属性过滤提升为ANN优化约束，创建凸融合空间，将硬过滤器转化为连续加权惩罚，实现高效近似搜索。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用需要结合向量相似性和属性过滤的混合查询，现有解决方案在召回率、速度和灵活性之间存在权衡，依赖不可扩展的索引技巧。

Method: 通过基于Transformer的凸化方法联合嵌入属性和向量，将硬过滤器转化为连续加权惩罚，保持top-k语义的同时实现高效近似搜索。

Result: FusedANN在标准混合基准测试中消除了脆弱的过滤阶段，实现了优越的召回率-延迟权衡，比最先进的混合和图基系统吞吐量提高3倍，召回率更好。

Conclusion: FusedANN在符号约束和向量相似性之间建立了原则性、可扩展和可验证的桥梁，为大规模、混合和动态NLP/ML工作负载解锁了新一代过滤检索系统。

Abstract: Vector search powers transformers technology, but real-world use demands
hybrid queries that combine vector similarity with attribute filters (e.g.,
"top document in category X, from 2023"). Current solutions trade off recall,
speed, and flexibility, relying on fragile index hacks that don't scale. We
introduce FusedANN (Fused Attribute-Vector Nearest Neighbor), a geometric
framework that elevates filtering to ANN optimization constraints and
introduces a convex fused space via a Lagrangian-like relaxation. Our method
jointly embeds attributes and vectors through transformer-based
convexification, turning hard filters into continuous, weighted penalties that
preserve top-k semantics while enabling efficient approximate search. We prove
that FusedANN reduces to exact filtering under high selectivity, gracefully
relaxes to semantically nearest attributes when exact matches are insufficient,
and preserves downstream ANN alpha-approximation guarantees. Empirically,
FusedANN improves query throughput by eliminating brittle filtering stages,
achieving superior recall-latency tradeoffs on standard hybrid benchmarks
without specialized index hacks, delivering up to 3 times higher throughput and
better recall than state-of-the-art hybrid and graph-based systems.
Theoretically, we provide explicit error bounds and parameter selection rules
that make FusedANN practical for production. This establishes a principled,
scalable, and verifiable bridge between symbolic constraints and vector
similarity, unlocking a new generation of filtered retrieval systems for large,
hybrid, and dynamic NLP/ML workloads.

</details>


### [188] [AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims on Social Media with Dual Encoders and Neural Re-Ranking](https://arxiv.org/abs/2509.19509)
*Cem Ashbaugh,Leon Baumgärtner,Tim Gress,Nikita Sidorov,Daniel Werner*

Main category: cs.IR

TL;DR: 该论文提出了一种两阶段检索管道，用于将社交媒体上的隐式科学声明链接到原始出版物，在CLEF-2025 CheckThat! Lab中排名第二，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的科学声明与原始出版物链接对于基于证据的事实核查和学术讨论至关重要，但受到词汇稀疏性、短查询和领域特定语言的阻碍。

Method: 采用两阶段检索管道：第一阶段使用基于E5-large的双编码器，通过批次内和挖掘的困难负样本进行微调，并增强分块标记化和丰富文档元数据；第二阶段使用SciBERT交叉编码器进行神经重排序。

Result: 优化后的稀疏检索基线(BM25)在黄金标签盲测集上达到MRR@5 = 0.5025，神经表示将性能提升至MRR@5 = 0.6174，完整管道进一步改善至MRR@5 = 0.6828。

Conclusion: 将密集检索与神经重排序器结合，为推文到研究匹配提供了强大高效的解决方案，并为未来证据检索管道提供了实用蓝图。

Abstract: Linking implicit scientific claims made on social media to their original
publications is crucial for evidence-based fact-checking and scholarly
discourse, yet it is hindered by lexical sparsity, very short queries, and
domain-specific language. Team AIRwaves ranked second in Subtask 4b of the
CLEF-2025 CheckThat! Lab with an evidence-retrieval approach that markedly
outperforms the competition baseline. The optimized sparse-retrieval
baseline(BM25) achieves MRR@5 = 0.5025 on the gold label blind test set. To
surpass this baseline, a two-stage retrieval pipeline is introduced: (i) a
first stage that uses a dual encoder based on E5-large, fine-tuned using
in-batch and mined hard negatives and enhanced through chunked tokenization and
rich document metadata; and (ii) a neural re-ranking stage using a SciBERT
cross-encoder. Replacing purely lexical matching with neural representations
lifts performance to MRR@5 = 0.6174, and the complete pipeline further improves
to MRR@5 = 0.6828. The findings demonstrate that coupling dense retrieval with
neural re-rankers delivers a powerful and efficient solution for tweet-to-study
matching and provides a practical blueprint for future evidence-retrieval
pipelines.

</details>


### [189] [Intelligent Algorithm Selection for Recommender Systems: Meta-Learning via in-depth algorithm feature engineering](https://arxiv.org/abs/2509.20134)
*Jarne Mathi Decker*

Main category: cs.IR

TL;DR: 该论文研究了在推荐系统中通过添加算法特征来改进元学习算法选择的效果，发现虽然算法特征能提高Top-1选择准确率，但用户特征在整体性能中仍占主导地位。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中的算法选择问题，传统元学习方法将不同算法视为等效的"黑盒"选择，忽略了算法本身的特性差异。

Method: 构建全面的算法特征集（包括静态代码指标、抽象语法树属性、行为性能特征和高层概念特征），在五个数据集上评估两种元学习器：仅使用用户特征的基线和同时使用用户与算法特征的模型。

Result: 添加算法特征的元学习器平均NDCG@10为0.143，比单最佳算法基线提高11.7%，但与仅使用用户特征的元学习器相比没有显著改进。算法特征提高了Top-1选择准确率16.1%，但降低了Top-3准确率10.7%。

Conclusion: 在推荐系统的个性化算法选择任务中，用户特征的预测能力占主导地位。虽然算法特征能提高选择精度，但要利用其潜力提升整体性能仍面临挑战。

Abstract: The "No Free Lunch" theorem dictates that no single recommender algorithm is
optimal for all users, creating a significant Algorithm Selection Problem.
Standard meta-learning approaches aim to solve this by selecting an algorithm
based on user features, but treat the fundamentally diverse algorithms
themselves as equivalent, "black-box" choices. This thesis investigates the
impact of overcoming this limitation by engineering a comprehensive feature set
to explicitly characterize the algorithms themselves. We combine static code
metrics, Abstract Syntax Tree properties, behavioral performance landmarks, and
high-level conceptual features. We evaluate two meta-learners across five
datasets: a baseline using only user features and our proposed model using both
user and algorithm features. Our results show that the meta-learner augmented
with algorithm features achieves an average NDCG@10 of 0.143, a statistically
significant improvement of 11.7% over the Single Best Algorithm baseline
(0.128). However, we found that the inclusion of algorithm features did not
lead to an improvement in overall NDCG@10 over the meta learner using only user
features (0.144). While adding algorithm features to the meta-learner did
improve its Top-1 selection accuracy (+16.1%), this was counterbalanced by
leading to a lower Top-3 accuracy (-10.7%). We conclude that for the per-user
algorithm selection task in recommender systems, the predictive power of user
features is overwhelmingly dominant. While algorithm features improve selection
precision, unlocking their potential to boost overall performance remains a
non-trivial challenge.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [190] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种在低代码/无代码环境中集成元认知层的架构模式，通过预测任务失败并主动发起人工交接来提高自主代理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 自主代理在低代码/无代码环境中的非确定性特性导致可靠性问题，如陷入循环、生成错误输出或遇到不可恢复的故障，这会降低用户信任度。

Method: 引入一个次级元认知层来监控主代理，基于延迟过长或重复动作等触发条件预测任务失败，并主动发起人工交接，向用户提供代理的"思考过程"和失败原因。

Result: 原型系统的实证分析表明，该方法显著提高了整体任务成功率，但计算开销有所增加。

Conclusion: 将人工交接重新定义为增强系统韧性、改善用户体验和建立信任的核心设计特征，而非失败的标志，并讨论了该方法的实践和伦理影响及未来研究方向。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [191] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: 本文提出了评估感知强化学习（EvA-RL）框架，通过在训练策略时同时最小化评估误差，使策略既高效又易于评估。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在策略评估时面临高方差（数据有限、长时任务）和高偏差（支持度不均、环境模型不准确）的问题，这些挑战源于标准RL范式在策略学习时未显式考虑评估需求。

Method: 设计EvA-RL框架，训练策略同时最大化期望回报和最小化给定价值预测方案下的评估误差；进一步扩展方法，共同学习评估条件状态价值预测器以缓解评估准确性与策略性能的权衡。

Result: 在离散和连续动作领域的实证结果表明，EvA-RL能显著降低评估误差，同时保持有竞争力的回报。

Conclusion: 这项工作为将可靠评估作为训练过程中首要原则的新型RL方法奠定了基础。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [192] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 该论文提出了计算认知负荷的正式理论，通过ICE基准测试发现大语言模型在认知负荷下性能显著下降，表明认知负荷是导致推理失败的关键因素。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在静态基准测试中表现出色，但在动态、信息丰富的环境中存在脆弱性，计算限制对认知负荷下推理的影响尚未被充分理解。

Method: 引入计算认知负荷的正式理论，设计ICE基准测试系统性地操纵上下文饱和和注意力残留两个负荷因素，在200个多跳推理问题上进行综合研究。

Result: 小型开源模型在所有条件下准确率为0%，而Gemini-2.0-Flash-001在控制条件下达到85%准确率，但在上下文饱和下性能显著下降。

Conclusion: 认知负荷是导致推理失败的关键因素，动态的认知感知压力测试对于评估先进AI系统的真实韧性和安全性至关重要。

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [193] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: 本文提出了一个将AI评估视为推理的框架，强调基准测试分数不是简单的测量而是推断，需要从能力理论出发来估计真实性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型评估的可靠性受到质疑，基准测试分数往往被当作简单测量，但实际上它们是基于能力理论的推断。作者希望建立更可靠和可信的AI能力评估方法。

Method: 提出了评估作为推理的原则性框架：从能力理论出发，推导出估计方法。特别针对敏感性扰动问题，引入了考虑不确定性的方法，包括一个显著降低样本复杂度的自适应算法。

Result: 该框架为通过基准测试获得更可靠和可信的AI能力估计奠定了基础，解决了评估中的敏感性扰动等关键挑战。

Conclusion: 将评估视为推理的视角有助于建立更科学的AI能力评估体系，这一方法借鉴了心理测量学等领域的经验，但在AI评估中尚未普及。

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [194] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL是一个统一的框架，通过标准化gym环境和模拟用户来训练和评估以用户为中心的智能体能力，研究发现奖励塑造和用户模拟选择对多轮交互效果至关重要。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练智能体模型方面显示出潜力，但智能体的最终价值在于协助用户的能力，而用户交互的多样性和动态性带来了挑战。

Method: 提出UserRL框架，系统性地变化回合级奖励分配和轨迹级分数计算，使用GRPO算法分析不同公式对学习的影响，在Qwen3模型上进行实验。

Result: 三个关键发现：(i)SFT冷启动对解锁初始交互能力和持续RL改进至关重要；(ii)精心设计的轨迹评分能产生更高效的多轮交互；(iii)虽然GPT-4o等强模拟用户有助于训练，但开源模拟器是成本效益高且可迁移的选择。

Conclusion: 奖励塑造和用户模拟选择的精心设计与模型规模同等重要，UserRL为开发稳健的用户中心智能体模型提供了实用路径。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [195] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: 该研究通过真实硬件部署探索协同车道变换预测，分享了实施和测试中的实践经验，重点讨论了实际挑战和限制。


<details>
  <summary>Details</summary>
Motivation: 现有车道变换预测研究大多基于仿真环境或预录数据集，依赖简化假设，而真实世界部署较少且实践经验不足。

Method: 在混合交通环境中进行真实硬件部署，实施协同车道变换预测系统。

Result: 识别了系统实施中的瓶颈、可靠性问题和操作约束等实际挑战。

Conclusion: 通过记录这些实践经验，为类似系统开发提供指导。

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [196] [No Quantum Advantage in Decoded Quantum Interferometry for MaxCut](https://arxiv.org/abs/2509.19966)
*Ojas Parekh*

Main category: quant-ph

TL;DR: Decoded Quantum Interferometry (DQI) 是一种近似离散优化问题的框架，但研究发现其在MaxCut问题上获得非平凡渐进近似保证的实例实际上可以在经典多项式时间内精确求解。


<details>
  <summary>Details</summary>
Motivation: 研究DQI框架在MaxCut问题上的表现，分析其近似保证的实际意义和局限性。

Method: 通过简化的DQI阐述，使用基础图论而非编码理论来解释算法，并分析其在MaxCut实例上的性能。

Result: 发现DQI在MaxCut问题上获得非平凡渐进近似保证的实例实际上可以在经典多项式时间内精确求解。

Conclusion: DQI在MaxCut问题上的近似保证存在局限性，其能够有效处理的实例类别实际上可以通过更简单的经典算法精确解决。

Abstract: Decoded Quantum Interferometry (DQI) is a framework for approximating special
kinds of discrete optimization problems that relies on problem structure in a
way that sets it apart from other classical or quantum approaches. We show that
the instances of MaxCut on which DQI attains a nontrivial asymptotic
approximation guarantee are solvable exactly in classical polynomial time. We
include a streamlined exposition of DQI tailored for MaxCut that relies on
elementary graph theory instead of coding theory to motivate and explain the
algorithm.

</details>


### [197] [Dequantization and Hardness of Spectral Sum Estimation](https://arxiv.org/abs/2509.20183)
*Roman Edenhofer,Atsuya Hasegawa,François Le Gall*

Main category: quant-ph

TL;DR: 该论文提出了矩阵谱和估计的新去量子化方法和硬度结果，特别是针对对数行列式的计算。文章展示了经典算法在特定参数范围内对量子算法的指数级改进，并证明了某些谱和估计问题的DQC1完全性。


<details>
  <summary>Details</summary>
Motivation: 量子算法在矩阵谱和估计方面显示出显著优势，特别是对于稀疏、条件良好的正定矩阵的对数行列式计算。本文旨在开发相应的经典算法来匹配量子算法的性能，并研究这些问题的计算复杂性边界。

Method: 提出了简单的去量子化技术，保留了维度上的多对数依赖关系。经典算法运行时间为polylog(N)·s^O(√κlogκ/ε)。同时通过DQC1完全性结果和BQP硬度分析来建立复杂性下界。

Result: 经典算法在特定参数范围内实现了对先前经典算法的指数级改进。证明了特定谱和估计问题的DQC1完全性，解决了Cade和Montanaro提出的关于Schatten-p范数估计复杂性的开放问题。

Conclusion: 该工作为矩阵谱和估计提供了新的经典算法和复杂性理论结果，在量子计算和经典计算之间建立了重要的桥梁，并为相关问题的计算复杂性提供了新的见解。

Abstract: We give new dequantization and hardness results for estimating spectral sums
of matrices, such as the log-determinant. Recent quantum algorithms have
demonstrated that the logarithm of the determinant of sparse, well-conditioned,
positive matrices can be approximated to $\varepsilon$-relative accuracy in
time polylogarithmic in the dimension $N$, specifically in time
$\mathrm{poly}(\mathrm{log}(N), s, \kappa, 1/\varepsilon)$, where $s$ is the
sparsity and $\kappa$ the condition number of the input matrix. We provide a
simple dequantization of these techniques that preserves the polylogarithmic
dependence on the dimension. Our classical algorithm runs in time
$\mathrm{polylog}(N)\cdot s^{O(\sqrt{\kappa}\log \kappa/\varepsilon)}$ which
constitutes an exponential improvement over previous classical algorithms in
certain parameter regimes.
  We complement our classical upper bound with $\mathsf{DQC1}$-completeness
results for estimating specific spectral sums such as the trace of the inverse
and the trace of matrix powers for log-local Hamiltonians, with parameter
scalings analogous to those of known quantum algorithms. Assuming
$\mathsf{BPP}\subsetneq\mathsf{DQC1}$, this rules out classical algorithms with
the same scalings. It also resolves a main open problem of Cade and Montanaro
(TQC 2018) concerning the complexity of Schatten-$p$ norm estimation. We
further analyze a block-encoding input model, where instead of a classical
description of a sparse matrix, we are given a block-encoding of it. We show
$\mathsf{DQC}1$-completeness in a very general way in this model for estimating
$\mathrm{tr}[f(A)]$ whenever $f$ and $f^{-1}$ are sufficiently smooth.
  We conclude our work with $\mathsf{BQP}$-hardness and
$\mathsf{PP}$-completeness results for high-accuracy log-determinant
estimation.

</details>


### [198] [InterQnet: A Heterogeneous Full-Stack Approach to Co-designing Scalable Quantum Networks](https://arxiv.org/abs/2509.19503)
*Joaquin Chung,Daniel Dilley,Ely Eastman,Alvin Gonzales,Kara Hokenstad,Md Shariful Islam,Varun Jorapur,Joseph Petrullo,Andy C. Y. Li,Bikun Li,Vasileios Niaouris,Anirudh Ramesh,Ansh Singal,Caitao Zhan,Michael Bishof,Eric Chitambar,Jacob P. Covey,Alan Dibos,Xu Han,Liang Jiang,Prem Kumar,Jeffrey Larson,Zain H. Saleem,Rajkumar Kettimuthu*

Main category: quant-ph

TL;DR: InterQnet是一个多学科项目，旨在通过改进设备、错误处理和网络架构来解决量子通信的可扩展性挑战，包括InterQnet-Achieve和InterQnet-Scale两个策略。


<details>
  <summary>Details</summary>
Motivation: 量子通信技术从理论发展到实际应用，但可扩展性成为主要挑战，包括节点数量和异质性、节点间距离、应用多样性和用户需求规模等问题。

Method: 采用双管齐下策略：InterQnet-Achieve专注于构建第一代量子中继器并集成错误缓解方案和集中式自动化网络控制系统；InterQnet-Scale通过开发前瞻性量子网络设备模型、高级纠错方案和纠缠协议来研究可扩展量子网络的架构选择。

Result: 论文报告了当前在实现可扩展性目标方面的进展，但具体结果未在摘要中详细说明。

Conclusion: InterQnet项目通过综合方法推进可扩展量子通信，为解决量子网络的可扩展性挑战提供了系统性的解决方案。

Abstract: Quantum communications have progressed significantly, moving from a
theoretical concept to small-scale experiments to recent metropolitan-scale
demonstrations. As the technology matures, it is expected to revolutionize
quantum computing in much the same way that classical networks revolutionized
classical computing. Quantum communications will also enable breakthroughs in
quantum sensing, metrology, and other areas. However, scalability has emerged
as a major challenge, particularly in terms of the number and heterogeneity of
nodes, the distances between nodes, the diversity of applications, and the
scale of user demand. This paper describes InterQnet, a multidisciplinary
project that advances scalable quantum communications through a comprehensive
approach that improves devices, error handling, and network architecture.
InterQnet has a two-pronged strategy to address scalability challenges:
InterQnet-Achieve focuses on practical realizations of heterogeneous quantum
networks by building and then integrating first-generation quantum repeaters
with error mitigation schemes and centralized automated network control
systems. The resulting system will enable quantum communications between two
heterogeneous quantum platforms through a third type of platform operating as a
repeater node. InterQnet-Scale focuses on a systems study of architectural
choices for scalable quantum networks by developing forward-looking models of
quantum network devices, advanced error correction schemes, and entanglement
protocols. Here we report our current progress toward achieving our scalability
goals.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [199] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: 提出了一种用于非铺装道路和越野环境的自动驾驶感知系统CMSNet，能够在恶劣条件下实时分割可通行地面和障碍物，并发布了包含近12,000张图像的新数据集Kamino。


<details>
  <summary>Details</summary>
Motivation: 为露天矿场和发展中国家的自动驾驶系统提供低延迟的智能感知能力，解决非均匀地形和恶劣条件下的导航问题。

Method: 提出可配置模块化分割网络（CMSNet）框架，使用TensorRT、C++和CUDA进行实时推理优化，并在新的Kamino数据集上进行训练和验证。

Result: 系统能够在夜间、雨天、灰尘等恶劣条件下有效分割可通行区域和障碍物，实现实时语义分割。

Conclusion: CMSNet框架在非铺装道路和越野环境中表现出良好的性能，为自动驾驶在复杂地形中的应用提供了有效解决方案。

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [200] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: 该研究使用机器学习方法检测高分辨率遥感数据中的云和云阴影，比较了传统方法和深度学习架构的性能，发现深度学习模型（特别是UNet和SCAN）在云和云阴影检测方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 有效的云和云阴影检测是准确获取大气甲烷或其他痕量气体浓度的关键前提，尤其对于MethaneSAT和MethaneAIR任务。云和云阴影会偏置甲烷反演结果并影响排放量化。

Method: 部署和评估了传统方法（迭代逻辑回归ILR和多层感知器MLP）以及先进的深度学习架构（UNet和光谱通道注意力网络SCAN）。

Result: 传统方法在空间一致性和边界定义方面存在困难，而深度学习模型显著提高了检测质量：UNet在保持空间结构方面表现最佳，SCAN在捕捉精细边界细节方面表现优异。SCAN在MethaneSAT数据上超越了UNet。

Conclusion: 深度学习架构为云和云阴影筛查提供了稳健、可扩展的解决方案，有助于增强现有和下一代高光谱任务的甲烷排放量化能力。

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [201] [Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture](https://arxiv.org/abs/2509.19997)
*Nico Schulthess,Ender Konukoglu*

Main category: cs.CV

TL;DR: 该论文提出使用DINOv2基础模型的嵌入特征，结合狄利克雷过程混合模型进行医学图像的无监督异常检测，避免了传统内存库方法在大数据集上的计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统基于内存库的异常检测方法在大型医学数据集上计算负担过重，需要一种更高效的建模方法来处理大规模医学图像数据。

Method: 使用DINOv2预训练模型提取特征，采用狄利克雷过程混合模型（DPMM）建模正常特征的分布，通过比较特征与混合成分中心的相似度作为异常得分函数。

Result: 实验表明该方法在医学图像异常检测基准上达到有竞争力的性能，同时推理时间至少减少一半，且归一化DINOv2特征与解剖结构更匹配。

Conclusion: 基于DPMM的DINOv2嵌入特征为医学图像异常检测提供了高效且有效的解决方案，即使DINOv2是在自然图像上训练的，也能很好地适应医学图像任务。

Abstract: In this work, we leverage informative embeddings from foundational models for
unsupervised anomaly detection in medical imaging. For small datasets, a
memory-bank of normative features can directly be used for anomaly detection
which has been demonstrated recently. However, this is unsuitable for large
medical datasets as the computational burden increases substantially.
Therefore, we propose to model the distribution of normative DINOv2 embeddings
with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model
that automatically adjusts the number of mixture components to the data at
hand. Rather than using a memory bank, we use the similarity between the
component centers and the embeddings as anomaly score function to create a
coarse anomaly segmentation mask. Our experiments show that through DPMM
embeddings of DINOv2, despite being trained on natural images, achieve very
competitive anomaly detection performance on medical imaging benchmarks and can
do this while at least halving the computation time at inference. Our analysis
further indicates that normalized DINOv2 embeddings are generally more aligned
with anatomical structures than unnormalized features, even in the presence of
anomalies, making them great representations for anomaly detection. The code is
available at https://github.com/NicoSchulthess/anomalydino-dpmm.

</details>


### [202] [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
*Somraj Gautam,Nachiketa Purohit,Gaurav Harit*

Main category: cs.CV

TL;DR: 本文提出了一种结合不确定性和多样性策略的主动学习方法，用于表格检测任务中的高效数据标注，在有限标注预算下显著优于随机采样方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中高效数据标注是一个关键挑战，特别是在需要大量标注数据的物体检测任务中。主动学习通过选择最有信息量的样本来最小化标注成本，但传统方法主要依赖不确定性选择，而结合多样性策略可以进一步提升采样效率。

Method: 提出了一种结合不确定性和多样性策略的主动学习方法，确保选择具有代表性的样本来提升模型泛化能力。在TableBank-LaTeX和TableBank-Word两个基准数据集上，使用CascadeTabNet和YOLOv9等先进的表格检测架构进行评估。

Result: 主动学习基于样本选择的方法显著优于随机采样，在有限标注预算下减少了标注工作量，同时保持了与全监督模型相当的性能。在相同标注预算下获得了更高的mAP分数。

Conclusion: 该方法证明了主动学习在表格检测任务中的有效性，通过智能样本选择策略实现了标注效率的显著提升，为数据标注成本高昂的应用场景提供了实用解决方案。

Abstract: Efficient data annotation remains a critical challenge in machine learning,
particularly for object detection tasks requiring extensive labeled data.
Active learning (AL) has emerged as a promising solution to minimize annotation
costs by selecting the most informative samples. While traditional AL
approaches primarily rely on uncertainty-based selection, recent advances
suggest that incorporating diversity-based strategies can enhance sampling
efficiency in object detection tasks. Our approach ensures the selection of
representative examples that improve model generalization. We evaluate our
method on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using
state-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our
results demonstrate that AL-based example selection significantly outperforms
random sampling, reducing annotation effort given a limited budget while
maintaining comparable performance to fully supervised models. Our method
achieves higher mAP scores within the same annotation budget.

</details>


### [203] [Predictive Quality Assessment for Mobile Secure Graphics](https://arxiv.org/abs/2509.20028)
*Cas Steigstra,Sergey Milyaev,Shaodi You*

Main category: cs.CV

TL;DR: 本文提出了一种预测性框架来解决智能手机图像采集质量差导致的图形验证可靠性问题，通过轻量级模型预测视频帧的质量分数，以确定其是否适合资源密集型的验证任务。


<details>
  <summary>Details</summary>
Motivation: 传统感知图像质量评估方法无法有效解决智能手机采集高熵图形验证码时的可靠性问题，导致高误拒率，形成'可靠性差距'。

Method: 引入预测性框架估计帧对下游验证任务的效用，提出轻量级模型预测质量分数，使用重构的FNMR和ISRR指标在32,000+图像数据集上验证，并进行跨域分析。

Result: 在105款智能手机的大规模数据集上验证有效，跨域分析发现：冻结的ImageNet预训练网络的轻量级探针比完全微调模型对未见打印技术具有更好的泛化能力。

Conclusion: 对于物理制造引起的域偏移，冻结的通用骨干网络比完全微调更鲁棒，后者可能过度拟合源域伪影，这为实际应用中的泛化提供了重要洞见。

Abstract: The reliability of secure graphic verification, a key anti-counterfeiting
tool, is undermined by poor image acquisition on smartphones. Uncontrolled user
captures of these high-entropy patterns cause high false rejection rates,
creating a significant 'reliability gap'. To bridge this gap, we depart from
traditional perceptual IQA and introduce a framework that predictively
estimates a frame's utility for the downstream verification task. We propose a
lightweight model to predict a quality score for a video frame, determining its
suitability for a resource-intensive oracle model. Our framework is validated
using re-contextualized FNMR and ISRR metrics on a large-scale dataset of
32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis
on graphics from different industrial printing presses reveals a key finding: a
lightweight probe on a frozen, ImageNet-pretrained network generalizes better
to an unseen printing technology than a fully fine-tuned model. This provides a
key insight for real-world generalization: for domain shifts from physical
manufacturing, a frozen general-purpose backbone can be more robust than full
fine-tuning, which can overfit to source-domain artifacts.

</details>


### [204] [Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2509.20107)
*JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出了一种新颖的高光谱适配器，利用预训练的视觉基础模型有效学习高光谱数据，在自动驾驶数据集上实现了最先进的语义分割性能


<details>
  <summary>Details</summary>
Motivation: 当前的高光谱语义分割方法性能不佳，因为它们依赖于为RGB输入优化的架构和学习框架。高光谱成像具有丰富的空间-光谱信息，可以在复杂环境中实现鲁棒的机器人感知

Method: 提出了一种包含光谱变换器和频谱感知空间先验模块的架构，以及模态感知交互块，通过专门的提取和注入机制有效整合高光谱表示和冻结的视觉Transformer特征

Result: 在三个基准自动驾驶数据集上的广泛评估表明，该架构在使用高光谱输入时实现了最先进的语义分割性能，优于基于视觉和高光谱的分割方法

Conclusion: 该高光谱适配器架构能够有效利用预训练视觉基础模型来处理高光谱数据，在复杂环境下展现出优越的语义分割能力

Abstract: Hyperspectral imaging (HSI) captures spatial information along with dense
spectral measurements across numerous narrow wavelength bands. This rich
spectral content has the potential to facilitate robust robotic perception,
particularly in environments with complex material compositions, varying
illumination, or other visually challenging conditions. However, current HSI
semantic segmentation methods underperform due to their reliance on
architectures and learning frameworks optimized for RGB inputs. In this work,
we propose a novel hyperspectral adapter that leverages pretrained vision
foundation models to effectively learn from hyperspectral data. Our
architecture incorporates a spectral transformer and a spectrum-aware spatial
prior module to extract rich spatial-spectral features. Additionally, we
introduce a modality-aware interaction block that facilitates effective
integration of hyperspectral representations and frozen vision Transformer
features through dedicated extraction and injection mechanisms. Extensive
evaluations on three benchmark autonomous driving datasets demonstrate that our
architecture achieves state-of-the-art semantic segmentation performance while
directly using HSI inputs, outperforming both vision-based and hyperspectral
segmentation methods. We make the code available at
https://hyperspectraladapter.cs.uni-freiburg.de.

</details>


### [205] [Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2509.20196)
*Dehong Kong,Sifan Yu,Siyuan Liang,Jiawei Liang,Jianhou Gan,Aishan Liu,Wenqi Ren*

Main category: cs.CV

TL;DR: 本文提出了首个针对视觉语言建模自动驾驶系统的通用伪装攻击框架UCA，通过在特征空间操作生成物理可实现的伪装纹理，解决了现有攻击方法难以直接迁移到VLM-AD系统的问题。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法存在明显挑战：物理对抗攻击主要针对视觉模块，难以直接迁移到VLM-AD系统；针对VLM-AD的攻击主要集中在数字层面。需要开发能够在物理世界部署的有效攻击方法。

Method: UCA框架在特征空间操作，引入特征分歧损失最大化干净图像和对抗图像的表征差异。采用多尺度学习策略和调整采样比例，增强对现实场景中尺度和视角变化的适应性。

Result: 实验表明UCA能在各种VLM-AD模型和驾驶场景中诱导错误的驾驶指令，显著超越现有最先进攻击方法（3-P指标提升30%）。在多样化视角和动态条件下表现出强大的攻击鲁棒性。

Conclusion: UCA是首个针对VLM-AD的通用伪装攻击框架，具有物理可实现性和强泛化能力，显示出实际部署的高潜力，为VLM-AD系统的安全性研究提供了重要参考。

Abstract: Visual language modeling for automated driving is emerging as a promising
research direction with substantial improvements in multimodal reasoning
capabilities. Despite its advanced reasoning abilities, VLM-AD remains
vulnerable to serious security threats from adversarial attacks, which involve
misleading model decisions through carefully crafted perturbations. Existing
attacks have obvious challenges: 1) Physical adversarial attacks primarily
target vision modules. They are difficult to directly transfer to VLM-AD
systems because they typically attack low-level perceptual components. 2)
Adversarial attacks against VLM-AD have largely concentrated on the digital
level. To address these challenges, we propose the first Universal Camouflage
Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on
optimizing the logit layer, UCA operates in the feature space to generate
physically realizable camouflage textures that exhibit strong generalization
across different user commands and model architectures. Motivated by the
observed vulnerability of encoder and projection layers in VLM-AD, UCA
introduces a feature divergence loss (FDL) that maximizes the representational
discrepancy between clean and adversarial images. In addition, UCA incorporates
a multi-scale learning strategy and adjusts the sampling ratio to enhance its
adaptability to changes in scale and viewpoint diversity in real-world
scenarios, thereby improving training stability. Extensive experiments
demonstrate that UCA can induce incorrect driving commands across various
VLM-AD models and driving scenarios, significantly surpassing existing
state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore,
UCA exhibits strong attack robustness under diverse viewpoints and dynamic
conditions, indicating high potential for practical deployment.

</details>


### [206] [ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression](https://arxiv.org/abs/2509.20234)
*Tom Burgert,Oliver Stoll,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 本文重新审视了CNN具有纹理偏见的假设，提出了一个领域无关的框架来量化特征依赖，发现CNN主要依赖局部形状特征而非纹理，且不同领域的模型表现出不同的特征依赖模式。


<details>
  <summary>Details</summary>
Motivation: 重新检验Geirhos等人关于CNN具有纹理偏见的假设，指出其冲突实验的局限性，旨在更准确地量化神经网络对形状、纹理和颜色特征的依赖程度。

Method: 提出了一个领域无关的框架，通过系统性地抑制形状、纹理和颜色线索来量化特征依赖，避免强制选择冲突的混淆因素，并在计算机视觉、医学影像和遥感等多个领域进行评估。

Result: 发现CNN并非固有纹理偏见，而是主要依赖局部形状特征；现代训练策略或架构（如ConvNeXt、ViTs）可以显著减轻这种依赖；不同领域的模型表现出系统性的特征依赖差异。

Conclusion: CNN的特征依赖模式因领域而异，计算机视觉模型优先考虑形状，医学影像模型强调颜色，遥感模型更依赖纹理，这挑战了CNN固有纹理偏见的普遍假设。

Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance towards texture. Code is available at
https://github.com/tomburgert/feature-reliance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [207] [Thinking While Listening: Simple Test Time Scaling For Audio Classification](https://arxiv.org/abs/2509.19676)
*Prateek Verma,Mert Pilanci*

Main category: cs.SD

TL;DR: 提出了一个让神经网络模型在听日常声音时“边听边思考”的框架，通过推理能力提升音频分类性能，展示了在两种设置下都能提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 受大型语言模型推理能力进步的启发，探索如何将思考机制融入音频分类流程，以及如何设计支持思考和测试时扩展的新架构。

Method: 采用两种方法：将思考机制整合到现有音频分类流程中实现类别空间推理；设计全新架构支持思考和测试时扩展。利用测试时扩展技术，通过轻量级方法（仅重训练冻结小模型的嵌入矩阵）超越大型文本推理模型。

Result: 模型在两种设置下都表现出分类准确率的提升，随着采样轨迹数量增加获得持续增益。轻量级方法（仅重训练GPT-2嵌入矩阵）超越了数十亿参数文本推理模型的性能。

Conclusion: 思考机制能有效提升音频分类性能，轻量级方法可以超越大型文本推理模型，测试时扩展技术能带来持续的性能增益。

Abstract: We propose a framework that enables neural models to "think while listening"
to everyday sounds, thereby enhancing audio classification performance.
Motivated by recent advances in the reasoning capabilities of large language
models, we address two central questions: (i) how can thinking be incorporated
into existing audio classification pipelines to enable reasoning in the
category space and improve performance, and (ii) can a new architecture be
designed from the ground up to support both thinking and test-time scaling? We
demonstrate that in both settings, our models exhibit improved classification
accuracy. Leveraging test-time scaling, we observe consistent gains as the
number of sampled traces increases. Furthermore, we evaluate two open-source
reasoning models, GPT-OSS-20B and Qwen3-14B, showing that while such models are
capable of zero-shot reasoning, a lightweight approach--retraining only the
embedding matrix of a frozen, smaller model like GPT-2--can surpass the
performance of billion-parameter text-based reasoning models.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [208] [Graph-based Neural Space Weather Forecasting](https://arxiv.org/abs/2509.19605)
*Daniel Holmberg,Ivan Zaitsev,Markku Alho,Ioanna Bouri,Fanni Franssila,Haewon Jeong,Minna Palmroth,Teemu Roos*

Main category: physics.space-ph

TL;DR: 使用基于图的神经模拟器对Vlasiator数据进行训练，实现快速确定性预报和生成式集合预报，为空间天气预测系统增加不确定性量化能力


<details>
  <summary>Details</summary>
Motivation: 准确的太空天气预报对于保护日益数字化的基础设施至关重要，混合Vlasov模型虽然物理真实性高但计算成本大，无法实时使用

Method: 基于图的神经模拟器，利用Vlasiator数据进行自回归训练，预测上游太阳风驱动的近地空间条件

Result: 实现了快速确定性预报，并通过生成模型产生集合预报来捕捉预报不确定性

Conclusion: 机器学习为现有空间天气预测系统增加了不确定性量化能力，使混合Vlasov模拟可用于实际业务

Abstract: Accurate space weather forecasting is crucial for protecting our increasingly
digital infrastructure. Hybrid-Vlasov models, like Vlasiator, offer physical
realism beyond that of current operational systems, but are too computationally
expensive for real-time use. We introduce a graph-based neural emulator trained
on Vlasiator data to autoregressively predict near-Earth space conditions
driven by an upstream solar wind. We show how to achieve both fast
deterministic forecasts and, by using a generative model, produce ensembles to
capture forecast uncertainty. This work demonstrates that machine learning
offers a way to add uncertainty quantification capability to existing space
weather prediction systems, and make hybrid-Vlasov simulation tractable for
operational use.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [209] [The Impact of Structural Changes on Learning Capacity in the Fly Olfactory Neural Circuit](https://arxiv.org/abs/2509.19351)
*Katherine Xie,Gabriel Koch Ocker*

Main category: q-bio.NC

TL;DR: 本研究通过构建果蝇蘑菇体神经回路模型，探究了Kenyon细胞（KC）与蘑菇体输出神经元（MBON）连接结构对气味分类能力的影响，发现KC发育成熟度和连接数量对学习能力至关重要。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注投射神经元（PN）到Kenyon细胞（KC）的连接，本研究旨在探究KC到MBON神经回路的结构扰动和连接变化如何影响MBON区分气味类别的能力。

Method: 构建包含PN、KC和MBON连接性的神经网络模型，使用10个人工输入类别训练模型，进行KC消融、突触修剪和PN-KC回路重连实验。

Result: 发现具有较少突触前KCs的MBON在气味分类任务中表现较差；发育成熟的KC消融对MBON学习能力的负面影响更大；随机和定向修剪实验结果与消融实验一致。

Conclusion: 研究深化了对嗅觉神经可塑性的理解，为理解学习和记忆机制提供了重要线索，在人工智能和神经退行性疾病治疗方面具有潜在应用价值。

Abstract: The Drosophila mushroom body (MB) is known to be involved in olfactory
learning and memory; the synaptic plasticity of the Kenyon cell (KC) to
mushroom body output neuron (MBON) synapses plays a key role in the learning
process. Previous research has focused on projection neuron (PN) to Kenyon cell
(KC) connectivity within the MB; we examine how perturbations to the mushroom
body circuit structure and changes in connectivity, specifically within the KC
to mushroom body output neuron (MBON) neural circuit, affect the MBONs' ability
to distinguish between odor classes. We constructed a neural network that
incorporates the connectivity between PNs, KCs, and MBONs. To train our model,
we generated ten artificial input classes, which represent the projection
neuron activity in response to different odors. We collected data on the number
of KC-to-MBON connections, MBON error rates, and KC-to-MBON synaptic weights,
among other metrics. We observed that MBONs with very few presynaptic KCs
consistently performed worse than others in the odor classification task. The
developmental types of KCs also played a significant role in each MBON's
output. We performed random and targeted KC ablation and observed that ablating
developmentally mature KCs had a greater negative impact on MBONs' learning
capacity than ablating immature KCs. Random and targeted pruning of KC-MBON
synaptic connections yielded results largely consistent with the ablation
experiments. To further explore the various types of KCs, we also performed
rewiring experiments in the PN to KC circuit. Our study furthers our
understanding of olfactory neuroplasticity and provides important clues to
understanding learning and memory in general. Understanding how the olfactory
circuits process and learn can also have potential applications in artificial
intelligence and treatments for neurodegenerative diseases.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [210] [Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs](https://arxiv.org/abs/2509.20208)
*Parker Glenn,Alfy Samuel,Daben Liu*

Main category: cs.CL

TL;DR: 该论文研究了在声明式查询语言中集成LLM驱动的操作符，提出了一种高效的方法来确保LLM函数在SQL查询中的类型正确性，相比现有方法在准确性和延迟方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前方法使用多个LLM后处理调用来确保生成输出与数据库值的对齐，这引入了性能瓶颈。研究旨在解决LLM函数在数据库查询语言中的类型对齐问题。

Method: 研究了不同规模开源语言模型在基于SQL的查询语言中解析和执行函数的能力，发现小型语言模型在混合数据源上作为函数执行器表现优异。提出了一种高效的解决方案来强制执行LLM函数的类型正确性。

Result: 在多跳问答数据集上实现了7%的准确率提升，相比可比解决方案延迟改善了53%。

Conclusion: 小型语言模型可以作为混合数据源的高效函数执行器，提出的类型强制执行方法在保持查询性能的同时显著提升了LLM函数的准确性。

Abstract: Integrating LLM powered operators in declarative query languages allows for
the combination of cheap and interpretable functions with powerful,
generalizable language model reasoning. However, in order to benefit from the
optimized execution of a database query language like SQL, generated outputs
must align with the rules enforced by both type checkers and database contents.
Current approaches address this challenge with orchestrations consisting of
many LLM-based post-processing calls to ensure alignment between generated
outputs and database values, introducing performance bottlenecks. We perform a
study on the ability of various sized open-source language models to both parse
and execute functions within a query language based on SQL, showing that small
language models can excel as function executors over hybrid data sources. Then,
we propose an efficient solution to enforce the well-typedness of LLM
functions, demonstrating 7% accuracy improvement on a multi-hop question
answering dataset with 53% improvement in latency over comparable solutions. We
make our implementation available at https://github.com/parkervg/blendsql

</details>


### [211] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS是一个评估LLMs数学能力的基准，涵盖12个核心技能维度，分为知识理解、问题解决与沟通、元技能与创造力三个领域，通过分类问题和设计任务来构建模型数学能力的细粒度可解释画像。


<details>
  <summary>Details</summary>
Motivation: 现有的数学评估基准往往只关注最终答案的正确性，而忽略了模型背后的数学思维过程和核心技能。GAUSS旨在提供更全面、细粒度和可解释的数学能力评估方法。

Method: 将数学问题按认知技能分类，设计能够隔离特定能力的任务，构建包含12个技能维度的评估框架，分为三个主要领域。

Result: 通过GAUSS基准对GPT-5-thinking进行评估，揭示了其在数学能力方面的优势和不足，以及与o4-mini-high模型的差异。

Conclusion: 基于技能的多维评估方法能够更准确地反映模型的底层数学智能，为模型能力评估提供了新的视角和价值。

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [212] [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
*Robert Tjarko Lange,Yuki Imajuku,Edoardo Cetin*

Main category: cs.CL

TL;DR: ShinkaEvolve是一个开源框架，利用大型语言模型作为变异算子进行科学发现，通过三项关键技术实现样本高效性和解决方案质量的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代码进化方法存在样本效率低下（需要数千个样本）和闭源限制的问题，阻碍了广泛采用和扩展。

Method: 引入三项关键技术：平衡探索与利用的父代采样技术、代码新颖性拒绝采样用于高效搜索空间探索、基于多臂老虎机的LLM集成选择策略。

Result: 在多样化任务中表现优异，仅用150个样本发现新的最优圆填充解决方案，设计高性能AIME数学推理代理系统，改进ALE-Bench竞争编程解决方案，发现新颖的专家混合负载平衡损失函数。

Conclusion: ShinkaEvolve通过开源可访问性和成本效益，实现了跨多样化计算问题的开放式发现民主化，展示了广泛的适用性和卓越的样本效率。

Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large
language models (LLMs) to advance scientific discovery with state-of-the-art
performance and unprecedented efficiency. Recent advances in scaling inference
time compute of LLMs have enabled significant progress in generalized
scientific discovery. These approaches rely on evolutionary agentic harnesses
that leverage LLMs as mutation operators to generate candidate solutions.
However, current code evolution methods suffer from critical limitations: they
are sample inefficient, requiring thousands of samples to identify effective
solutions, and remain closed-source, hindering broad adoption and extension.
ShinkaEvolve addresses these limitations, introducing three key innovations: a
parent sampling technique balancing exploration and exploitation, code novelty
rejection-sampling for efficient search space exploration, and a bandit-based
LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,
demonstrating consistent improvements in sample efficiency and solution
quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution
using only 150 samples, designs high-performing agentic harnesses for AIME
mathematical reasoning tasks, identifies improvements to ALE-Bench competitive
programming solutions, and discovers novel mixture-of-expert load balancing
loss functions that illuminate the space of optimization strategies. Our
results demonstrate that ShinkaEvolve achieves broad applicability with
exceptional sample efficiency. By providing open-source accessibility and
cost-efficiency, this work democratizes open-ended discovery across diverse
computational problems.

</details>


### [213] [LLM-Assisted Topic Reduction for BERTopic on Social Media Data](https://arxiv.org/abs/2509.19365)
*Wannes Janssens,Matthias Bogaert,Dirk Van den Poel*

Main category: cs.CL

TL;DR: 提出了一种结合BERTopic和大型语言模型的框架，用于改进社交媒体数据中的主题建模，通过LLM进行主题归并来减少重叠主题数量。


<details>
  <summary>Details</summary>
Motivation: BERTopic在处理社交媒体等噪声数据时会产生过多重叠主题，而纯LLM方法计算开销大且难以扩展。

Method: 先用BERTopic生成初始主题集并构建表示，然后用LLM迭代识别和合并语义相似的主题。

Result: 在三个Twitter/X数据集和四种LLM上评估，该方法在提升主题多样性和一致性方面优于基线方法。

Conclusion: 该框架有效改进了社交媒体主题建模，但对数据集特性和初始参数选择有一定敏感性。

Abstract: The BERTopic framework leverages transformer embeddings and hierarchical
clustering to extract latent topics from unstructured text corpora. While
effective, it often struggles with social media data, which tends to be noisy
and sparse, resulting in an excessive number of overlapping topics. Recent work
explored the use of large language models for end-to-end topic modelling.
However, these approaches typically require significant computational overhead,
limiting their scalability in big data contexts. In this work, we propose a
framework that combines BERTopic for topic generation with large language
models for topic reduction. The method first generates an initial set of topics
and constructs a representation for each. These representations are then
provided as input to the language model, which iteratively identifies and
merges semantically similar topics. We evaluate the approach across three
Twitter/X datasets and four different language models. Our method outperforms
the baseline approach in enhancing topic diversity and, in many cases,
coherence, with some sensitivity to dataset characteristics and initial
parameter selection.

</details>


### [214] [Confidence Calibration in Large Language Model-Based Entity Matching](https://arxiv.org/abs/2509.19557)
*Iris Kamsteeg,Juan Cardenas-Cartagena,Floris van Beers,Gineke ten Holt,Tsegaye Misikir Tashu,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 该研究探索了大语言模型与实体匹配中置信度校准的交叉领域，通过实证研究比较了RoBERTa基线置信度与使用温度缩放、蒙特卡洛dropout和集成方法校准后的置信度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决RoBERTa模型在实体匹配任务中表现出的轻微过度自信问题，探索有效的置信度校准方法来提高模型可靠性。

Method: 使用Abt-Buy、DBLP-ACM、iTunes-Amazon和Company数据集，比较了温度缩放、蒙特卡洛dropout和集成方法三种校准技术对RoBERTa模型置信度的改进效果。

Result: 研究发现RoBERTa模型存在轻微过度自信，预期校准误差得分在0.0043到0.0552之间。温度缩放方法能有效缓解此问题，可将预期校准误差降低高达23.83%。

Conclusion: 温度缩放是缓解RoBERTa模型在实体匹配任务中过度自信问题的有效方法，显著提高了模型的置信度校准性能。

Abstract: This research aims to explore the intersection of Large Language Models and
confidence calibration in Entity Matching. To this end, we perform an empirical
study to compare baseline RoBERTa confidences for an Entity Matching task
against confidences that are calibrated using Temperature Scaling, Monte Carlo
Dropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company
datasets. The findings indicate that the proposed modified RoBERTa model
exhibits a slight overconfidence, with Expected Calibration Error scores
ranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence
can be mitigated using Temperature Scaling, reducing Expected Calibration Error
scores by up to 23.83%.

</details>


### [215] [Uncertainty in Semantic Language Modeling with PIXELS](https://arxiv.org/abs/2509.19563)
*Stefania Radu,Marco Zullich,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本文分析了基于像素的语言模型在18种语言和7种文字系统中的不确定性量化表现，发现这类模型在重建图像块时低估不确定性，且不确定性受文字系统影响，拉丁文字显示较低不确定性。


<details>
  <summary>Details</summary>
Motivation: 基于像素的语言模型旨在解决语言建模中的词汇瓶颈问题，但不确定性量化这一挑战仍未解决。本文旨在分析这类模型在多种语言和文字系统中的不确定性和置信度表现。

Method: 采用多种方法进行分析，包括蒙特卡洛Dropout、Transformer注意力机制和集成学习，在3个语义挑战性任务中评估18种语言和7种文字系统。

Result: 结果显示基于像素的模型在重建图像块时低估不确定性；不确定性受文字系统影响，拉丁文字语言显示较低不确定性；集成学习在命名实体识别和问答任务中表现更好。

Conclusion: 基于像素的语言模型在不确定性量化方面存在低估问题，文字系统是影响不确定性的重要因素，集成学习方法在跨语言任务中具有更好的性能表现。

Abstract: Pixel-based language models aim to solve the vocabulary bottleneck problem in
language modeling, but the challenge of uncertainty quantification remains
open. The novelty of this work consists of analysing uncertainty and confidence
in pixel-based language models across 18 languages and 7 scripts, all part of 3
semantically challenging tasks. This is achieved through several methods such
as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The
results suggest that pixel-based models underestimate uncertainty when
reconstructing patches. The uncertainty is also influenced by the script, with
Latin languages displaying lower uncertainty. The findings on ensemble learning
show better performance when applying hyperparameter tuning during the named
entity recognition and question-answering tasks across 16 languages.

</details>


### [216] [Thinking Augmented Pre-training](https://arxiv.org/abs/2509.20186)
*Liang Wang,Nan Yang,Shaohan Huang,Li Dong,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种通过思维轨迹增强文本数据来提高大型语言模型训练数据效率的简单可扩展方法，称为思维增强预训练（TPT）。该方法在多种训练配置下实验表明，可将LLM预训练的数据效率提高3倍。


<details>
  <summary>Details</summary>
Motivation: 随着LLM预训练计算量的快速增长，高质量数据的可用性仍然有限。某些高质量标记由于底层逻辑复杂而难以学习，因此需要最大化可用数据的效用。

Method: 提出TPT方法，通过自动生成的思维轨迹来增强文本数据。这种增强有效增加了训练数据量，并通过逐步推理和分解使高质量标记更易学习。

Result: 实验结果表明，该方法显著提高了不同模型规模和系列的LLM性能。对于30亿参数模型，在多个具有挑战性的推理基准上，训练后性能提升超过10%。

Conclusion: TPT是一种通用的方法学，能够有效提高LLM预训练的数据效率，特别是在高质量数据稀缺的情况下表现突出。

Abstract: This paper introduces a simple and scalable approach to improve the data
efficiency of large language model (LLM) training by augmenting existing text
data with thinking trajectories. The compute for pre-training LLMs has been
growing at an unprecedented rate, while the availability of high-quality data
remains limited. Consequently, maximizing the utility of available data
constitutes a significant research challenge. A primary impediment is that
certain high-quality tokens are difficult to learn given a fixed model
capacity, as the underlying rationale for a single token can be exceptionally
complex and deep. To address this issue, we propose Thinking augmented
Pre-Training (TPT), a universal methodology that augments text with
automatically generated thinking trajectories. Such augmentation effectively
increases the volume of the training data and makes high-quality tokens more
learnable through step-by-step reasoning and decomposition. We apply TPT across
diverse training configurations up to $100$B tokens, encompassing pre-training
with both constrained and abundant data, as well as mid-training from strong
open-source checkpoints. Experimental results indicate that our method
substantially improves the performance of LLMs across various model sizes and
families. Notably, TPT enhances the data efficiency of LLM pre-training by a
factor of $3$. For a $3$B parameter model, it improves the post-training
performance by over $10\%$ on several challenging reasoning benchmarks.

</details>


### [217] [Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation](https://arxiv.org/abs/2509.20287)
*Behzad Shayegh,Jan-Thorsten Peter,David Vilar,Tobias Domhan,Juraj Juraska,Markus Freitag,Lili Mou*

Main category: cs.CL

TL;DR: 本文研究了机器翻译中充分性与流畅性之间的权衡关系，分析了当前评估指标的偏向性，并提出了在元评估中控制这种偏向的方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译评估指标普遍偏向充分性而非流畅性，且标准的WMT元评估方法存在偏向充分性导向指标的偏差，需要理解这种权衡关系及其对指标排名的影响。

Method: 通过分析评估指标在充分性-流畅性权衡中的位置，研究元评估层面的权衡关系，并提出一种在元评估中合成翻译系统的方法来控制偏差。

Result: 发现当前指标更偏向充分性，其得分与翻译充分性的相关性更强；元评估层面也存在同样权衡，WMT元评估偏向充分性导向指标；这种偏差部分源于元评估数据集中系统的组成。

Conclusion: 充分性与流畅性之间的权衡在机器翻译评估中具有重要意义，需要在元评估中理解并控制这种权衡带来的偏向性，以确保评估指标的公平性。

Abstract: We investigate the tradeoff between adequacy and fluency in machine
translation. We show the severity of this tradeoff at the evaluation level and
analyze where popular metrics fall within it. Essentially, current metrics
generally lean toward adequacy, meaning that their scores correlate more
strongly with the adequacy of translations than with fluency. More importantly,
we find that this tradeoff also persists at the meta-evaluation level, and that
the standard WMT meta-evaluation favors adequacy-oriented metrics over
fluency-oriented ones. We show that this bias is partially attributed to the
composition of the systems included in the meta-evaluation datasets. To control
this bias, we propose a method that synthesizes translation systems in
meta-evaluation. Our findings highlight the importance of understanding this
tradeoff in meta-evaluation and its impact on metric rankings.

</details>


### [218] [Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning](https://arxiv.org/abs/2509.20315)
*T. O. Abiola,K. D. Abiodun,O. E. Olumide,O. O. Adebanji,O. Hiram Calvo,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文提出了一种基于主动学习和Transformer模型的多语言希望语音检测框架，在英语、西班牙语、德语和乌尔都语数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 希望语音（鼓励性和乐观的语言）在促进网络积极讨论中发挥重要作用，但在多语言和低资源环境下的检测仍然具有挑战性。

Method: 使用主动学习方法和基于Transformer的模型（包括mBERT和XLM-RoBERTa）构建多语言希望语音检测框架。

Result: Transformer模型显著优于传统基线方法，XLM-RoBERTa达到最高准确率，主动学习策略在小规模标注数据集上也能保持强劲性能。

Conclusion: 研究表明，将多语言Transformer与数据高效训练策略相结合对于希望语音检测非常有效。

Abstract: Hope speech language that fosters encouragement and optimism plays a vital
role in promoting positive discourse online. However, its detection remains
challenging, especially in multilingual and low-resource settings. This paper
presents a multilingual framework for hope speech detection using an active
learning approach and transformer-based models, including mBERT and
XLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,
German, and Urdu, including benchmark test sets from recent shared tasks. Our
results show that transformer models significantly outperform traditional
baselines, with XLM-RoBERTa achieving the highest overall accuracy.
Furthermore, our active learning strategy maintained strong performance even
with small annotated datasets. This study highlights the effectiveness of
combining multilingual transformers with data-efficient training strategies for
hope speech detection.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [219] [A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks](https://arxiv.org/abs/2509.19306)
*Jingyi Wang,Zhongyuan Zhao,Qingtian Wang,Zexu Li,Yue Wang,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: 该论文提出了一种基于在线学习的优化方法，用于在异构无线网络中优化联邦微调，通过动态切换LoRA模块来应对设备异构性和传输不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 边缘智能需要低延迟和泛在服务，但无线网络中的设备异构性和资源约束对联邦微调性能构成威胁。

Method: 提出基于切换的联邦微调框架，设备动态切换LoRA模块；推导推理风险差距上界；将非凸混合整数规划问题分解为模型切换、功率控制和带宽分配子问题；开发具有多项式计算复杂度的在线优化算法。

Result: 在SST-2和QNLI数据集上的仿真结果表明，该方法在测试准确率和能量效率方面取得了性能提升。

Conclusion: 所提出的在线优化方法有效解决了异构无线网络中联邦微调的挑战，提高了边缘智能的性能和效率。

Abstract: Edge intelligence has emerged as a promising strategy to deliver low-latency
and ubiquitous services for mobile devices. Recent advances in fine-tuning
mechanisms of foundation models have enabled edge intelligence by integrating
low-rank adaptation (LoRA) with federated learning. However, in wireless
networks, the device heterogeneity and resource constraints on edge devices
pose great threats to the performance of federated fine-tuning. To tackle these
issues, we propose to optimize federated fine-tuning in heterogenous wireless
networks via online learning. First, the framework of switching-based federated
fine-tuning in wireless networks is provided. The edge devices switches to LoRA
modules dynamically for federated fine-tuning with base station to jointly
mitigate the impact of device heterogeneity and transmission unreliability.
Second, a tractable upper bound on the inference risk gap is derived based on
theoretical analysis. To improve the generalization capability, we formulate a
non-convex mixed-integer programming problem with long-term constraints, and
decouple it into model switching, transmit power control, and bandwidth
allocation subproblems. An online optimization algorithm is developed to solve
the problems with polynomial computational complexity. Finally, the simulation
results on the SST-2 and QNLI data sets demonstrate the performance gains in
test accuracy and energy efficiency.

</details>


### [220] [Joint Channel Estimation and Computation Offloading in Fluid Antenna-assisted MEC Networks](https://arxiv.org/abs/2509.19340)
*Ying Ju,Mingdong Li,Haoyu Wang,Lei Liu,Youyang Qu,Mianxiong Dong,Victor C. M. Leung,Chau Yuen*

Main category: eess.SP

TL;DR: 本文提出了一种流体天线辅助的移动边缘计算卸载框架，通过信息瓶颈度量增强信道压缩感知和分层双决斗多智能体算法来解决系统延迟最小化问题。


<details>
  <summary>Details</summary>
Motivation: 流体天线能够动态调整端口位置，提供空间分集和频谱效率优势，这对移动边缘计算系统特别有价值。但面临信道估计复杂度和联合优化问题的非凸性两大挑战。

Method: 1. 提出IBM-CCS信道估计方法，将信息相关性融入感知过程；2. 提出HiTDMA卸载方案，利用博弈论降低功率控制变量维度，通过分层结构解耦用户侧和基站侧的优化任务。

Result: 数值结果表明，所提方案显著降低系统延迟并提升卸载性能，优于基准方法。IBM-CCS在不同端口密度下表现出优越的准确性和鲁棒性。

Conclusion: 流体天线辅助的MEC卸载框架通过创新的信道估计和优化算法，有效解决了系统延迟最小化问题，为不完美CSI下的高效通信提供了解决方案。

Abstract: With the emergence of fluid antenna (FA) in wireless communications, the
capability to dynamically adjust port positions offers substantial benefits in
spatial diversity and spectrum efficiency, which are particularly valuable for
mobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC
offloading framework to minimize system delay. This framework faces two severe
challenges, which are the complexity of channel estimation due to dynamic port
configuration and the inherent non-convexity of the joint optimization problem.
Firstly, we propose Information Bottleneck Metric-enhanced Channel Compressed
Sensing (IBM-CCS), which advances FA channel estimation by integrating
information relevance into the sensing process and capturing key features of FA
channels effectively. Secondly, to address the non-convex and high-dimensional
optimization problem in FA-assisted MEC systems, which includes FA port
selection, beamforming, power control, and resource allocation, we propose a
game theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA)
based offloading scheme, where the hierarchical structure effectively decouples
and coordinates the optimization tasks between the user side and the base
station side. Crucially, the game theory effectively reduces the dimensionality
of power control variables, allowing deep reinforcement learning (DRL) agents
to achieve improved optimization efficiency. Numerical results confirm that the
proposed scheme significantly reduces system delay and enhances offloading
performance, outperforming benchmarks. Additionally, the IBM-CCS channel
estimation demonstrates superior accuracy and robustness under varying port
densities, contributing to efficient communication under imperfect CSI.

</details>


### [221] [Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors](https://arxiv.org/abs/2509.20059)
*Koki Kanzaki,Koya Sato*

Main category: eess.SP

TL;DR: 本文提出了一种针对位置信息存在突发性误差环境的高精度无线电地图构建方法，通过将定位误差建模为可调参数嵌入边缘对数似然函数，实现无线电地图构建中的位置不确定性事后校准。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图构建方法大多假设传感过程中位置信息无噪声，但实际中基于设备的定位系统（如GNSS）会产生数米至数十米的定位误差，忽略这些误差会导致无线电地图精度显著下降。

Method: 引入一个新颖框架，将定位误差与无线电传播的空间相关性一起建模，通过将这些误差作为可调参数嵌入边缘对数似然函数中，实现位置不确定性的事后校准。

Result: 基于实际人类移动数据的数值结果表明，所提方法能将RMSE退化限制在约0.25-0.29 dB，而基线方法的性能损失超过1 dB。

Conclusion: 该方法能有效处理定位误差对无线电地图构建的影响，在存在位置不确定性的环境中保持较高的地图精度。

Abstract: This paper proposes a high-accuracy radio map construction method tailored
for environments where location information is affected by bursty errors. Radio
maps are an effective tool for visualizing wireless environments. Although
extensive research has been conducted on accurate radio map construction, most
existing approaches assume noise-free location information during sensing. In
practice, however, positioning errors ranging from a few to several tens of
meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring
such errors during inference can lead to significant degradation in radio map
accuracy. This study highlights that these errors often tend to be biased when
using mobile devices as sensors. We introduce a novel framework that models
these errors together with spatial correlation in radio propagation by
embedding them as tunable parameters in the marginal log-likelihood function.
This enables ex-post calibration of location uncertainty during radio map
construction. Numerical results based on practical human mobility data
demonstrate that the proposed method can limit RMSE degradation to
approximately 0.25-0.29 dB, compared with Gaussian process regression using
noise-free location data, whereas baseline methods suffer performance losses
exceeding 1 dB.

</details>


### [222] [Graph-Based Spatio-temporal Attention and Multi-Scale Fusion for Clinically Interpretable, High-Fidelity Fetal ECG Extraction](https://arxiv.org/abs/2509.19308)
*Chang Wang,Ming Zhu,Shahram Latifi,Buddhadeb Dawn,Shengjie Zhai*

Main category: eess.SP

TL;DR: FHNet是一个深度学习框架，结合图神经网络和多尺度增强transformer，用于从腹部ECG信号中提取干净的胎儿ECG信号，在低信噪比条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病是最常见的新生儿异常，需要早期检测。但胎儿ECG信号在腹部ECG中常被母体ECG和噪声掩盖，传统方法在低信噪比条件下效果不佳。

Method: 提出FetalHealthNet框架，集成图神经网络和多尺度增强transformer，动态建模导联间的时空相关性，提取干净的胎儿ECG信号。

Result: 在基准aECG数据集上，FHNet显著优于LSTM、标准transformer和最先进模型，R2>0.99，RMSE=0.015，即使在严重噪声条件下也表现优异。

Conclusion: FHNet展示了AI驱动建模在胎儿监测和早期CHD筛查方面的潜力，支持模型透明度和临床信任，代表了下一代生物医学信号处理的变革性影响。

Abstract: Congenital Heart Disease (CHD) is the most common neonatal anomaly,
highlighting the urgent need for early detection to improve outcomes. Yet,
fetal ECG (fECG) signals in abdominal ECG (aECG) are often masked by maternal
ECG and noise, challenging conventional methods under low signal-to-noise ratio
(SNR) conditions. We propose FetalHealthNet (FHNet), a deep learning framework
that integrates Graph Neural Networks with a multi-scale enhanced transformer
to dynamically model spatiotemporal inter-lead correlations and extract clean
fECG signals. On benchmark aECG datasets, FHNet consistently outperforms long
short-term memory (LSTM) models, standard transformers, and state-of-the-art
models, achieving R2>0.99 and RMSE = 0.015 even under severe noise.
Interpretability analyses highlight physiologically meaningful temporal and
lead contributions, supporting model transparency and clinical trust. FHNet
illustrates the potential of AI-driven modeling to advance fetal monitoring and
enable early CHD screening, underscoring the transformative impact of
next-generation biomedical signal processing.

</details>


### [223] [E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion](https://arxiv.org/abs/2509.19312)
*Minghui Wu,Zhen Gao*

Main category: eess.SP

TL;DR: 该论文提出了一种端到端的上行-下行CSI融合预编码网络，通过联合建模下行CSI参考信号设计、CSI反馈和基站预编码，有效利用SRS信息和UE反馈来提升大规模MIMO系统的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统虽然能提供高频谱效率，但高维下行信道状态信息(CSI)使得实时信道获取和预编码变得复杂。传统方法难以充分利用上行参考信号和下行反馈信息。

Method: 构建基于MAXIM架构的投影网络，设计下行CSI-RS；UE压缩反馈CSI观测；BS端采用反馈驱动和SRS驱动的双分支预编码网络，通过融合网络生成最终预编码器；采用三阶段训练策略优化频谱效率。

Result: 仿真结果表明，该方法能有效利用SRS信息和UE反馈，性能显著优于传统基线方法。

Conclusion: 提出的端到端融合预编码网络为大规模MIMO系统提供了一种高效的解决方案，通过联合优化多个模块实现了性能的显著提升。

Abstract: Massive multiple-input multiple-output (MIMO) promises high spectral
efficiency but also leads to high-dimensional downlink channel state
information (CSI), which complicates real-time channel acquisition and
precoding. To address this, we propose an end-to-end (E2E) uplink-downlink CSI
fusion precoding network that jointly models downlink CSI reference signal
(CSI-RS) design, CSI feedback, and base-station (BS) precoding within a single
E2E neural architecture. Concretely, a projection network built on the MAXIM
architecture takes uplink sounding reference signals (SRS) as input and outputs
frequency-, beam-, and port-domain projection matrices for designing downlink
CSI-RS. User equipment (UE) then compresses/quantizes the resulting CSI-RS
observations and feeds back a compact representation. At the base station (BS),
two complementary branches produce candidate precoders: one is a feedback-only
precoding network driven by quantized downlink observations, and the other is
an SRS-only precoding network driven by uplink SRS. These candidate precoders
are subsequently combined by a fusion precoding network to yield the final
transmit precoder. All the modules are trained with a
spectral-efficiency-oriented loss under a three-stage schedule. Simulation
results show that the proposed approach effectively harnesses both SRS-derived
information and UE feedback, achieving markedly better performance than
conventional baselines.

</details>


### [224] [STL-FFT-STFT-TCN-LSTM: An Effective Wave Height High Accuracy Prediction Model Fusing Time-Frequency Domain Features](https://arxiv.org/abs/2509.19313)
*Huipeng Liu,Zhichao Zhu,Yuan Zhou,Changlu Li*

Main category: eess.SP

TL;DR: 本文提出了一种结合STL-FFT-STFT-TCN-LSTM的混合模型，用于精确预测有效波高，以解决波浪能预测中的非线性、突变、多尺度周期性和高频噪声等问题。


<details>
  <summary>Details</summary>
Motivation: 随着传统能源消耗加剧及其环境负面影响日益显著，波浪能因其高能量密度、稳定性、分布广泛和环境友好性而成为可再生能源家族中极具前景的成员。其发展的关键在于精确预测有效波高，但波浪能信号存在强非线性、突变、多尺度周期性、数据稀疏性和高频噪声干扰等挑战，且物理模型计算成本极高。

Method: 本研究提出了一种混合模型STL-FFT-STFT-TCN-LSTM，结合了季节性趋势分解（STL）、快速傅里叶变换（FFT）、短时傅里叶变换（STFT）、时间卷积网络（TCN）和长短期记忆网络（LSTM）技术，旨在优化多尺度特征融合、捕捉极端波高，并解决高频噪声和周期信号问题。

Result: 使用NOAA Station 41008和41047的2019年至2022年每小时数据进行实验，结果显示，与其他单一模型和混合模型相比，STL-FFT-STFT-TCN-LSTM模型在捕捉极端波高和抑制高频噪声方面预测精度显著更高，MAE降低15.8%-40.5%，SMAPE降低8.3%-20.3%，R增加1.31%-2.9%。消融实验也证明了各组件步骤的不可或缺性。

Conclusion: 该模型在多尺度特征融合方面表现出优越性，验证了其在有效波高预测中的高效性和准确性。

Abstract: As the consumption of traditional energy sources intensifies and their
adverse environmental impacts become more pronounced, wave energy stands out as
a highly promising member of the renewable energy family due to its high energy
density, stability, widespread distribution, and environmental friendliness.
The key to its development lies in the precise prediction of Significant Wave
Height (WVHT). However, wave energy signals exhibit strong nonlinearity, abrupt
changes, multi-scale periodicity, data sparsity, and high-frequency noise
interference; additionally, physical models for wave energy prediction incur
extremely high computational costs. To address these challenges, this study
proposes a hybrid model combining STL-FFT-STFT-TCN-LSTM. This model exploits
the Seasonal-Trend Decomposition Procedure based on Loess (STL), Fast Fourier
Transform (FFT), Short-Time Fourier Transform (STFT), Temporal Convolutional
Network (TCN), and Long Short-Term Memory (LSTM) technologies. The model aims
to optimize multi-scale feature fusion, capture extreme wave heights, and
address issues related to high-frequency noise and periodic signals, thereby
achieving efficient and accurate prediction of significant wave height.
Experiments were conducted using hourly data from NOAA Station 41008 and 41047
spanning 2019 to 2022. The results showed that compared with other single
models and hybrid models, the STL-FFT-STFT-TCN-LSTM model achieved
significantly higher prediction accuracy in capturing extreme wave heights and
suppressing high-frequency noise, with MAE reduced by 15.8\%-40.5\%, SMAPE
reduced by 8.3\%-20.3\%, and R increased by 1.31\%-2.9\%; in ablation
experiments, the model also demonstrated the indispensability of each component
step, validating its superiority in multi-scale feature fusion.

</details>


### [225] [Advancing Few-Shot Pediatric Arrhythmia Classification with a Novel Contrastive Loss and Multimodal Learning](https://arxiv.org/abs/2509.19315)
*Yiqiao Chen,Zijian Huang,Zhenghui Feng*

Main category: eess.SP

TL;DR: 提出了一种多模态深度学习框架，结合ECG和IEGM信号，用于儿科心律失常自动分类，解决了类别不平衡和少样本问题，在Leipzig Heart Center数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 儿科心律失常是导致残疾和心源性猝死的主要风险因素，但由于类别不平衡、少样本类别和复杂信号特征，其自动分类面临挑战，限制了早期筛查和临床干预的效率与可靠性。

Method: 提出多模态端到端深度学习框架：双分支卷积编码器处理ECG和IEGM信号，语义注意力实现跨模态特征对齐，轻量级Transformer编码器建模全局依赖关系，并引入自适应全局类感知对比损失函数（AGCACL）增强类内紧凑性和类间可分离性。

Result: 在Leipzig Heart Center儿科/先天性ECG+IEGM数据集上取得最佳性能：Top-1准确率97.76%，宏精确率94.08%，宏召回率91.97%，宏F1分数92.97%，宏F2分数92.36%，相比最强基线分别提升13.64、15.96、19.82和19.44个百分点。

Conclusion: 该框架显著提高了少数心律失常类别的可检测性和鲁棒性，为儿科和先天性心脏病人群的心律筛查、术前评估和术后随访提供了潜在的临床价值。

Abstract: Pediatric arrhythmias are a major risk factor for disability and sudden
cardiac death, yet their automated classification remains challenging due to
class imbalance, few-shot categories, and complex signal characteristics, which
severely limit the efficiency and reliability of early screening and clinical
intervention. To address this problem, we propose a multimodal end-to-end deep
learning framework that combines dual-branch convolutional encoders for ECG and
IEGM, semantic attention for cross-modal feature alignment, and a lightweight
Transformer encoder for global dependency modeling. In addition, we introduce a
new contrastive loss fucntion named Adaptive Global Class-Aware Contrastive
Loss (AGCACL) to enhance intra-class compactness and inter-class separability
through class prototypes and a global similarity matrix. To the best of our
knowledge, this is the first systematic study based on the Leipzig Heart Center
pediatric/congenital ECG+IEGM dataset, for which we also provide a complete and
reproducible preprocessing pipeline. Experimental results demonstrate that the
proposed method achieves the overall best performance on this dataset,
including 97.76\% Top-1 Accuracy, 94.08\% Macro Precision, 91.97\% Macro
Recall, 92.97\% Macro F1, and 92.36\% Macro F2, with improvements of +13.64,
+15.96, +19.82, and +19.44 percentage points over the strongest baseline in
Macro Precision/Recall/F1/F2, respectively. These findings indicate that the
framework significantly improves the detectability and robustness for minority
arrhythmia classes, offering potential clinical value for rhythm screening,
pre-procedural assessment, and postoperative follow-up in pediatric and
congenital heart disease populations.

</details>


### [226] [Electric Vehicle Identification from Behind Smart Meter Data](https://arxiv.org/abs/2509.19316)
*Ammar Kamoona,Hui Song,Ali Moradi Amani,Mahdi Jalili,Xinghuo Yu,Peter McTaggart*

Main category: eess.SP

TL;DR: 本文提出了一种基于异常检测的无监督学习方法，用于从智能电表低频数据中识别电动汽车充电负荷，无需先验的EV充电配置文件知识。


<details>
  <summary>Details</summary>
Motivation: 当电动汽车在电表后充电时，充电负荷被视为用户负载的一部分，而非由配电网络运营商单独测量。配电网络运营商需要了解其网络中的电动汽车存在情况，以便更好地规划和管理配电网。

Method: 提出了一种深度时间卷积编码解码网络，采用基于异常检测技术的无监督学习方法来识别EV充电负荷，仅需非EV用户的实际功耗数据。

Result: 该方法应用于澳大利亚维多利亚州家庭的智能电表数据，在识别拥有EV的家庭方面表现出优越性能。

Conclusion: 该方法为配电网络运营商提供了一种无需先验知识的有效EV充电负荷识别方案，有助于电网的智能决策和可靠性管理。

Abstract: Electric vehicle (EV) charging loads identification from behind smart meter
recordings is an indispensable aspect that enables effective decision-making
for energy distributors to reach an informed and intelligent decision about the
power grid's reliability. When EV charging happens behind the meter (BTM), the
charging occurs on the customer side of the meter, which measures the overall
electricity consumption. In other words, the charging of the EV is considered
part of the customer's load and not separately measured by the Distribution
Network Operators (DNOs). DNOs require complete knowledge about the EV presence
in their network. Identifying the EV charging demand is essential to better
plan and manage the distribution grid. Unlike supervised methods, this paper
addresses the problem of EV charging load identification in a non-nonintrusive
manner from low-frequency smart meter using an unsupervised learning approach
based on anomaly detection technique. Our approach does not require prior
knowledge of EV charging profiles. It only requires real power consumption data
of non-EV users, which are abundant in practice. We propose a deep temporal
convolution encoding decoding (TAE) network. The TAE is applied to power
consumption from smart BTM from Victorian households in Australia, and the TAE
shows superior performance in identifying households with EVs.

</details>


### [227] [Human Activity Recognition Based on Electrocardiogram Data Only](https://arxiv.org/abs/2509.19328)
*Sina Montazeri,Waltenegus Dargie,Yunhe Feng,Kewei Sha*

Main category: eess.SP

TL;DR: 本文首次证明仅使用心电图（ECG）就能在六种不同活动中实现稳健的活动识别，超越了以往工作的范围。设计了三种新的深度学习模型，在54名受试者的数据上测试，所有模型对已知受试者的准确率超过94%，CNNTransformer混合模型对未知受试者的准确率达到72%。


<details>
  <summary>Details</summary>
Motivation: 传统活动识别依赖惯性测量单元（IMU），资源密集且需要校准。虽然已探索基于心电图的方法，但这些方法通常作为IMU的补充或仅限于广泛类别分类。本文旨在推进该领域，首次实现仅使用ECG的稳健活动识别。

Method: 设计并评估了三种新的深度学习模型：1）带有Squeeze-and-Excitation块的CNN分类器，用于通道级特征重新校准；2）带有扩张卷积的ResNet分类器，用于多尺度时间依赖性捕获；3）新颖的CNNTransformer混合模型，结合卷积特征提取和注意力机制，用于长程时间关系建模。

Result: 在54名受试者的六种活动数据上测试，所有三种模型对已知受试者的准确率超过94%，CNNTransformer混合模型对未知受试者的准确率达到72%，这一结果可以通过增加训练人群进一步改善。

Conclusion: 本研究首次成功实现了仅使用ECG的多项身体活动分类，为开发能够同时进行心脏监测和活动识别而无需额外运动传感器的下一代可穿戴设备提供了重要潜力。

Abstract: Human activity recognition is critical for applications such as early
intervention and health analytics. Traditional activity recognition relies on
inertial measurement units (IMUs), which are resource intensive and require
calibration. Although electrocardiogram (ECG)-based methods have been explored,
these have typically served as supplements to IMUs or have been limited to
broad categorical classification such as fall detection or active vs. inactive
in daily activities. In this paper, we advance the field by demonstrating, for
the first time, robust recognition of activity only with ECG in six distinct
activities, which is beyond the scope of previous work. We design and evaluate
three new deep learning models, including a CNN classifier with
Squeeze-and-Excitation blocks for channel-wise feature recalibration, a ResNet
classifier with dilated convolutions for multiscale temporal dependency
capture, and a novel CNNTransformer hybrid combining convolutional feature
extraction with attention mechanisms for long-range temporal relationship
modeling. Tested on data from 54 subjects for six activities, all three models
achieve over 94% accuracy for seen subjects, while CNNTransformer hybrid
reaching the best accuracy of 72% for unseen subjects, a result that can be
further improved by increasing the training population. This study demonstrates
the first successful ECG-only activity classification in multiple physical
activities, offering significant potential for developing next-generation
wearables capable of simultaneous cardiac monitoring and activity recognition
without additional motion sensors.

</details>


### [228] [LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition](https://arxiv.org/abs/2509.19330)
*Zejun Liu,Yunshan Chen,Chengxi Xie,Huan Liu*

Main category: eess.SP

TL;DR: 本文介绍了LibEMER，一个用于EEG多模态情感识别的开源评估框架，旨在解决该领域缺乏开源实现、标准化基准和深入讨论的问题。


<details>
  <summary>Details</summary>
Motivation: EEG多模态情感识别领域存在三个关键问题：缺乏开源实现、缺乏标准化透明基准、缺乏对主要挑战和研究方向的深入讨论。

Method: 开发了LibEMER框架，提供可复现的PyTorch实现、标准化数据预处理、模型实现和实验设置协议。

Result: 该框架在三个广泛使用的公共数据集和两个学习任务上实现了无偏性能评估。

Conclusion: LibEMER为EEG多模态情感识别研究提供了统一的评估平台，有助于推动该领域的公平比较和进一步发展。

Abstract: EEG-based multimodal emotion recognition(EMER) has gained significant
attention and witnessed notable advancements, the inherent complexity of human
neural systems has motivated substantial efforts toward multimodal approaches.
However, this field currently suffers from three critical limitations: (i) the
absence of open-source implementations. (ii) the lack of standardized and
transparent benchmarks for fair performance analysis. (iii) in-depth discussion
regarding main challenges and promising research directions is a notable
scarcity. To address these challenges, we introduce LibEMER, a unified
evaluation framework that provides fully reproducible PyTorch implementations
of curated deep learning methods alongside standardized protocols for data
preprocessing, model realization, and experimental setups. This framework
enables unbiased performance assessment on three widely-used public datasets
across two learning tasks. The open-source library is publicly accessible at:
https://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384

</details>


### [229] [Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention](https://arxiv.org/abs/2509.19331)
*Enhao Huang,Zhiyu Zhang,Tianxiang Xu,Chunshu Xia,Kaichun Hu,Yuchen Yang,Tongtong Pan,Dong Dong,Zhan Qin*

Main category: eess.SP

TL;DR: Holographic Transformer将波干涉原理引入自注意力机制，通过相对相位调制相互作用，在复数信号处理中保持振幅和相位的一致性，在PolSAR图像分类和无线信道预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数深度学习模型将注意力视为实值相关性，忽略了复数信号中同时包含振幅和相位信息的干涉效应，导致对复数信号建模不完整。

Method: 提出全息注意力机制，通过相对相位调制相互作用并相干叠加值；采用双头解码器同时重构输入和预测任务输出，防止相位崩溃；证明全息注意力实现了离散干涉算子并在线性混合下保持相位一致性。

Result: 在PolSAR图像分类和无线信道预测实验中取得优异性能：高分类准确率和F1分数，低回归误差，对相位扰动具有更强的鲁棒性。

Conclusion: 在注意力机制中强制执行物理一致性能够带来复数学习中的泛化改进，为相干信号建模提供了一个统一的基于物理的框架。

Abstract: Complex-valued signals encode both amplitude and phase, yet most deep models
treat attention as real-valued correlation, overlooking interference effects.
We introduce the Holographic Transformer, a physics-inspired architecture that
incorporates wave interference principles into self-attention. Holographic
attention modulates interactions by relative phase and coherently superimposes
values, ensuring consistency between amplitude and phase. A dual-headed decoder
simultaneously reconstructs the input and predicts task outputs, preventing
phase collapse when losses prioritize magnitude over phase. We demonstrate that
holographic attention implements a discrete interference operator and maintains
phase consistency under linear mixing. Experiments on PolSAR image
classification and wireless channel prediction show strong performance,
achieving high classification accuracy and F1 scores, low regression error, and
increased robustness to phase perturbations. These results highlight that
enforcing physical consistency in attention leads to generalizable improvements
in complex-valued learning and provides a unified, physics-based framework for
coherent signal modeling. The code is available at
https://github.com/EonHao/Holographic-Transformers.

</details>


### [230] [A Spatio-Temporal Feature Fusion EEG Virtual Channel Signal Generation Network and Its Application in Anxiety Assessment](https://arxiv.org/abs/2509.19334)
*Shangqing Yuan,Wenshuang Zhai,Shengwen Guo*

Main category: eess.SP

TL;DR: 该研究提出了一种基于时空特征融合的EEG虚拟通道信号生成网络，旨在解决便携式EEG设备通道有限的问题，通过4个额叶通道生成13个重要脑区的虚拟EEG信号。


<details>
  <summary>Details</summary>
Motivation: 解决便携式EEG设备通道数量有限导致信息采集不足的问题，提高EEG信号的信息完整性和应用价值。

Method: 采用二维卷积神经网络架构，包含并行的时间域和空间域特征提取模块，以及特征融合模块，使用PRED+CT数据库的119名受试者多通道EEG信号进行验证。

Result: 生成的虚拟通道EEG信号与原始真实信号的平均相关系数为0.6724，平均绝对误差为3.9470；结合原始信号用于焦虑分类时显著提升了机器学习算法的性能。

Conclusion: 该网络生成的虚拟EEG信号与真实信号具有高度一致性，能有效缓解便携式EEG设备信息采集不足的问题，并增强机器学习算法的分类性能。

Abstract: To address the issue of limited channels and insufficient information
collection in portable EEG devices, this study explores an EEG virtual channel
signal generation network using a novel spatio-temporal feature fusion
strategy. Based on the EEG signals from four frontal lobe channels, the network
aims to generate virtual channel EEG signals for other 13 important brain
regions. The architecture of the network is a two-dimensional convolutional
neural network and it includes a parallel module for temporal and spatial
domain feature extraction, followed by a feature fusion module. The public
PRED+CT database, which includes multi-channel EEG signals from 119 subjects,
was selected to verify the constructed network. The results showed that the
average correlation coefficient between the generated virtual channel EEG
signals and the original real signals was 0.6724, with an average absolute
error of 3.9470. Furthermore, the 13 virtual channel EEG signals were combined
with the original EEG signals of four brain regions and then used for anxiety
classification with a support vector machine. The results indicate that the
virtual EEG signals generated by the constructed network not only have a high
degree of consistency with the real channel EEG signals but also significantly
enhance the performance of machine learning algorithms for anxiety
classification. This study effectively alleviates the problem of insufficient
information acquisition by portable EEG devices with few channels.

</details>


### [231] [A Measurement Report Data-Driven Framework for Localized Statistical Channel Modeling](https://arxiv.org/abs/2509.19342)
*Xinyu Qin,Ye Xue,Qi Yan,Shutao Zhang,Bingsheng Peng,Tsung-Hui Chang*

Main category: eess.SP

TL;DR: 本文提出了一种基于测量报告（MR）数据的本地化统计信道建模框架，通过半监督超图神经网络解决MR数据位置缺失问题，并采用联合网格构建和信道APS估计方法提升复杂环境下的建模鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有本地化统计信道建模方法严重依赖高成本的驱测数据，且空间覆盖有限。本文旨在利用低成本、广泛采集的MR数据来解决这一问题。

Method: 框架包含两个核心模块：1）MR定位模块使用基于超图神经网络的半监督方法进行位置提取；2）联合网格构建和信道APS估计模块通过聚类和改进的稀疏恢复方法交替优化网格划分和APS估计。

Result: 在真实世界MR数据集上的实验表明，该框架在定位和信道建模方面具有优越的性能和鲁棒性。

Conclusion: 所提出的MR数据驱动框架为本地化统计信道建模提供了一种低成本、高效的解决方案，特别适用于复杂环境下的网络优化。

Abstract: Localized statistical channel modeling (LSCM) is crucial for effective
performance evaluation in digital twin-assisted network optimization. Solely
relying on the multi-beam reference signal receiving power (RSRP), LSCM aims to
model the localized statistical propagation environment by estimating the
channel angular power spectrum (APS). However, existing methods rely heavily on
drive test data with high collection costs and limited spatial coverage. In
this paper, we propose a measurement report (MR) data-driven framework for
LSCM, exploiting the low-cost and extensive collection of MR data. The
framework comprises two novel modules. The MR localization module addresses the
issue of missing locations in MR data by introducing a semi-supervised method
based on hypergraph neural networks, which exploits multi-modal information via
distance-aware hypergraph modeling and hypergraph convolution for location
extraction. To enhance the computational efficiency and solution robustness,
LSCM operates at the grid level. Compared to independently constructing
geographically uniform grids and estimating channel APS, the joint grid
construction and channel APS estimation module enhances robustness in complex
environments with spatially non-uniform data by exploiting their correlation.
This module alternately optimizes grid partitioning and APS estimation using
clustering and improved sparse recovery for the ill-conditioned measurement
matrix and incomplete observations. Through comprehensive experiments on a
real-world MR dataset, we demonstrate the superior performance and robustness
of our framework in localization and channel modeling.

</details>


### [232] [Low-Cost Sensor Fusion Framework for Organic Substance Classification and Quality Control Using Classification Methods](https://arxiv.org/abs/2509.19367)
*Borhan Uddin Chowdhury,Damian Valles,Md Raf E Ul Shougat*

Main category: eess.SP

TL;DR: 基于Arduino Mega 2560微控制器平台开发了一个传感器融合框架，用于快速无损分类和有机物质质量控制，通过机器学习模型实现了93-94%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 开发低成本、非破坏性的有机物质分类和质量控制方法，利用商业传感器和机器学习技术替代传统昂贵的分析技术。

Method: 使用Arduino Mega 2560平台配备三个商业环境/气体传感器，收集10种有机物质的数据，进行相关性分析和特征选择，应用PCA/LDA降维，训练SVM、决策树、随机森林、ANN和集成投票分类器。

Result: 优化后的随机森林、集成分类器和人工神经网络在测试集上达到93-94%的准确率。

Conclusion: 基于Arduino的低成本多传感器平台结合先进的机器学习和相关性驱动的特征工程，能够可靠地识别和控制有机化合物的质量。

Abstract: We present a sensor-fusion framework for rapid, non-destructive
classification and quality control of organic substances, built on a standard
Arduino Mega 2560 microcontroller platform equipped with three commercial
environmental and gas sensors. All data used in this study were generated
in-house: sensor outputs for ten distinct classes - including fresh and expired
samples of apple juice, onion, garlic, and ginger, as well as cinnamon and
cardamom - were systematically collected and labeled using this hardware setup,
resulting in a unique, application-specific dataset. Correlation analysis was
employed as part of the preprocessing pipeline for feature selection. After
preprocessing and dimensionality reduction (PCA/LDA), multiple supervised
learning models - including Support Vector Machine (SVM), Decision Tree (DT),
and Random Forest (RF), each with hyperparameter tuning, as well as an
Artificial Neural Network (ANN) and an ensemble voting classifier - were
trained and cross-validated on the collected dataset. The best-performing
models, including tuned Random Forest, ensemble, and ANN, achieved test
accuracies in the 93 to 94 percent range. These results demonstrate that
low-cost, multisensory platforms based on the Arduino Mega 2560, combined with
advanced machine learning and correlation-driven feature engineering, enable
reliable identification and quality control of organic compounds.

</details>


### [233] [Short-Term Regional Electricity Demand Forecasting in Argentina Using LSTM Networks](https://arxiv.org/abs/2509.19374)
*Oscar A. Oviedo*

Main category: eess.SP

TL;DR: 开发基于LSTM网络的深度学习模型，用于预测阿根廷科尔多瓦的短期小时电力需求，整合历史消费数据和外部变量，实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 为电网运营商提供优化的规划和控制策略，需要准确预测短期电力需求，特别是在考虑季节性模式和极端消费事件的情况下。

Method: 使用LSTM网络，整合历史消费数据、气候因素、时间周期和人口统计等外部变量，进行模型设计和超参数优化，并辅以随机森林回归的可解释性研究和每日需求极值时间预测评估。

Result: 模型预测精度高，平均绝对百分比误差为3.20%，决定系数为0.95；在预测每日需求极值时间方面，超过三分之二的测试日达到精确小时准确度，90%以上的情况在±1小时内。

Conclusion: 所提出的框架不仅具有高预测准确性，还具有操作相关性，为电网运营商在不同需求场景下提供了有价值的见解。

Abstract: This study presents the development and optimization of a deep learning model
based on Long Short-Term Memory (LSTM) networks to predict short-term hourly
electricity demand in C\'ordoba, Argentina. Integrating historical consumption
data with exogenous variables (climatic factors, temporal cycles, and
demographic statistics), the model achieved high predictive precision, with a
mean absolute percentage error of 3.20\% and a determination coefficient of
0.95. The inclusion of periodic temporal encodings and weather variables proved
crucial to capture seasonal patterns and extreme consumption events, enhancing
the robustness and generalizability of the model. In addition to the design and
hyperparameter optimization of the LSTM architecture, two complementary
analyses were carried out: (i) an interpretability study using Random Forest
regression to quantify the relative importance of exogenous drivers, and (ii)
an evaluation of model performance in predicting the timing of daily demand
maxima and minima, achieving exact-hour accuracy in more than two-thirds of the
test days and within abs(1) hour in over 90\% of cases. Together, these results
highlight both the predictive accuracy and operational relevance of the
proposed framework, providing valuable insights for grid operators seeking
optimized planning and control strategies under diverse demand scenarios.

</details>


### [234] [Neural Network Based Framework for Passive Intermodulation Cancellation in MIMO Systems](https://arxiv.org/abs/2509.19382)
*Xiaolong Li,Zhi-qin John Xu,Peiting You,Yifei Zhu*

Main category: eess.SP

TL;DR: 提出一种基于深度学习的轻量级PIM消除框架，使用深度可分离卷积和扩张卷积来高效捕获天线和子载波间的非线性依赖关系，在MIMO实验环境中实现了29dB的平均功率误差抑制。


<details>
  <summary>Details</summary>
Motivation: 5G及未来通信系统中，无源互调(PIM)成为MIMO-OFDM系统自干扰的关键来源，传统非线性模型方法计算复杂且扩展性有限。

Method: 采用深度可分离卷积和扩张卷积构建轻量级神经网络，结合循环学习率调度和梯度裁剪来提升收敛性能，仅需11k可训练参数。

Result: 在受控MIMO实验环境中，该方法有效抑制了三阶无源互调失真，达到29dB的平均功率误差抑制效果。

Conclusion: 紧凑的神经网络架构在未来无线通信系统中具有可扩展干扰抑制的潜力。

Abstract: Passive intermodulation (PIM) has emerged as a critical source of
self-interference in modern MIMO-OFDM systems, especially under the stringent
requirements of 5G and beyond. Conventional cancellation methods often rely on
complex nonlinear models with limited scalability and high computational cost.
In this work, we propose a lightweight deep learning framework for PIM
cancellation that leverages depthwise separable convolutions and dilated
convolutions to efficiently capture nonlinear dependencies across antennas and
subcarriers. To further enhance convergence, we adopt a cyclic learning rate
schedule and gradient clipping. In a controlled MIMO experimental setup, the
method effectively suppresses third-order passive intermodulation (PIM)
distortion, achieving up to 29dB of average power error (APE) with only 11k
trainable parameters. These results highlight the potential of compact neural
architectures for scalable interference mitigation in future wireless
communication systems.

</details>


### [235] [Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations](https://arxiv.org/abs/2509.19384)
*Hongyuan Shi,Yilin Zhai,Ping Dong,Zaijin You,Chao Zhan,Qing Wang*

Main category: eess.SP

TL;DR: AUWave是一种混合深度学习框架，用于从稀疏浮标观测数据重建高分辨率区域有效波高场，通过多尺度U-Net和自注意力机制提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 从稀疏不均匀的浮标观测数据重建高分辨率区域有效波高场是海洋监测和风险评估的核心挑战，现有方法在稀疏采样条件下性能有限。

Method: 提出AUWave混合深度学习框架，结合站点序列编码器（MLP）和多尺度U-Net，增强瓶颈自注意力层，通过贝叶斯超参数搜索优化模型参数。

Result: 在夏威夷区域的NDBC浮标观测和ERA5再分析数据上，AUWave达到最小验证损失0.043285，在数据丰富配置下显著优于基线方法，误差分布呈现右偏特征。

Conclusion: AUWave为数据同化提供了可扩展的高分辨率先验，误差分析和站点消融实验为观测网络设计提供了实用指导，在最小但非平凡的空间锚定条件下表现出色。

Abstract: Reconstructing high-resolution regional significant wave height fields from
sparse and uneven buoy observations remains a core challenge for ocean
monitoring and risk-aware operations. We introduce AUWave, a hybrid deep
learning framework that fuses a station-wise sequence encoder (MLP) with a
multi-scale U-Net enhanced by a bottleneck self-attention layer to recover
32$\times$32 regional SWH fields. A systematic Bayesian hyperparameter search
with Optuna identifies the learning rate as the dominant driver of
generalization, followed by the scheduler decay and the latent dimension. Using
NDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave
attains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE
distribution. Spatial errors are lowest near observation sites and increase
with distance, reflecting identifiability limits under sparse sampling.
Sensitivity experiments show that AUWave consistently outperforms a
representative baseline in data-richer configurations, while the baseline is
only marginally competitive in the most underdetermined single-buoy cases. The
architecture's multi-scale and attention components translate into accuracy
gains when minimal but non-trivial spatial anchoring is available. Error maps
and buoy ablations reveal key anchor stations whose removal disproportionately
degrades performance, offering actionable guidance for network design. AUWave
provides a scalable pathway for gap filling, high-resolution priors for data
assimilation, and contingency reconstruction.

</details>


### [236] [A Statistical Mixture-of-Experts Framework for EMG Artifact Removal in EEG: Empirical Insights and a Proof-of-Concept Application](https://arxiv.org/abs/2509.19385)
*Benjamin J. Choi,Griffin Milsap,Clara A. Scholl,Francesco Tenore,Mattson Ogg*

Main category: eess.SP

TL;DR: 本文提出了一种基于混合专家(MoE)框架的EEG信号去噪算法，专门针对高噪声环境下的EMG伪影去除问题，在EEGdenoiseNet数据集上取得了竞争性的整体性能和优越的高噪声场景下界性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于神经网络的EEG去噪方法在高噪声环境下表现不佳，需要改进EMG伪影去除算法以提升神经接口的信号质量。

Method: 采用混合专家(MoE)框架，结合三个新的统计洞察：EMG伪影可分类、窄SNR范围专家专业化、基于相关性的目标函数。模型包含CNN和RNN神经网络。

Result: 在67名受试者的EEGdenoiseNet数据集上测试，MoE去噪模型在整体性能上与SOTA算法竞争，在高噪声设置下表现出优越的下界性能。

Conclusion: MoE框架在EMG伪影去除方面具有潜力，特别是在高噪声环境下，为开发更有效的神经接口提供了前景。

Abstract: Effective control of neural interfaces is limited by poor signal quality.
While neural network-based electroencephalography (EEG) denoising methods for
electromyogenic (EMG) artifacts have improved in recent years, current
state-of-the-art (SOTA) models perform suboptimally in settings with high
noise. To address the shortcomings of current machine learning (ML)-based
denoising algorithms, we present a signal filtration algorithm driven by a new
mixture-of-experts (MoE) framework. Our algorithm leverages three new
statistical insights into the EEG-EMG denoising problem: (1) EMG artifacts can
be partitioned into quantifiable subtypes to aid downstream MoE classification,
(2) local experts trained on narrower signal-to-noise ratio (SNR) ranges can
achieve performance increases through specialization, and (3) correlation-based
objective functions, in conjunction with rescaling algorithms, can enable
faster convergence in a neural network-based denoising context. We empirically
demonstrate these three insights into EMG artifact removal and use our findings
to create a new downstream MoE denoising algorithm consisting of convolutional
(CNN) and recurrent (RNN) neural networks. We tested all results on a major
benchmark dataset (EEGdenoiseNet) collected from 67 subjects. We found that our
MoE denoising model achieved competitive overall performance with SOTA ML
denoising algorithms and superior lower bound performance in high noise
settings. These preliminary results highlight the promise of our MoE framework
for enabling advances in EMG artifact removal for EEG processing, especially in
high noise settings. Further research and development will be necessary to
assess our MoE framework on a wider range of real-world test cases and explore
its downstream potential to unlock more effective neural interfaces.

</details>


### [237] [Hybrid Pipeline SWD Detection in Long-Term EEG Signals](https://arxiv.org/abs/2509.19387)
*Antonio Quintero Rincon,Nicolas Masino,Veronica Marsico,Hadj Batatia*

Main category: eess.SP

TL;DR: 提出了一种轻量级混合管道，结合分析特征和浅层人工神经网络，用于在长期单极脑电图中准确检测棘慢波放电（SWD），实现高灵敏度和特异性。


<details>
  <summary>Details</summary>
Motivation: 棘慢波放电是失神癫痫的脑电图标志，但在多日记录中手动识别既耗时又容易出错，需要自动化解决方案。

Method: 使用双边移动平均滤波器抑制正常背景活动的高频成分，然后计算残差信号的均值和标准差作为特征向量，输入单隐藏层人工神经网络进行分类。

Result: 在12名患者的780个通道上评估，正确检测384个事件（灵敏度98%），特异性96.2%，总体准确率97.2%。

Conclusion: 正态分布描述符与小型神经网络结合为扩展脑电图记录中的自动化SWD筛查提供了有效且计算成本低的解决方案。

Abstract: Spike-and-wave discharges (SWDs) are the electroencephalographic hallmark of
absence epilepsy, yet their manual identification in multi-day recordings
remains labour-intensive and error-prone. We present a lightweight hybrid
pipeline that couples analytical features with a shallow artificial neural
network (ANN) for accurate, patient-specific SWD detection in long-term,
monopolar EEG. A two-sided moving-average (MA) filter first suppresses the
high-frequency components of normal background activity. The residual signal is
then summarised by the mean and the standard deviation of its normally
distributed samples, yielding a compact, two-dimensional feature vector for
every 20s window. These features are fed to a single-hidden-layer ANN trained
via back-propagation to classify each window as SWD or non-SWD. The method was
evaluated on 780 channels sampled at 256 Hz from 12 patients, comprising 392
annotated SWD events. It correctly detected 384 events (sensitivity: 98%) while
achieving a specificity of 96.2 % and an overall accuracy of 97.2%. Because
feature extraction is analytic, and the classifier is small, the pipeline runs
in real-time and requires no manual threshold tuning. These results indicate
that normal-distribution descriptors combined with a modest ANN provide an
effective and computationally inexpensive solution for automated SWD screening
in extended EEG recordings.

</details>


### [238] [Self-Alignment Learning to Improve Myocardial Infarction Detection from Single-Lead ECG](https://arxiv.org/abs/2509.19397)
*Jiarui Jin,Xiaocheng Fang,Haoyu Wang,Jun Li,Che Liu,Donglin Xie,Hongyan Li,Shenda Hong*

Main category: eess.SP

TL;DR: SelfMIS是一种简单有效的对齐学习框架，通过直接对齐单导联和多导联心电图的潜在空间表示，提高了从单导联心电图检测心肌梗死的性能。


<details>
  <summary>Details</summary>
Motivation: 单导联心电图由于空间信息有限，检测心肌梗死具有挑战性。传统方法通过信号级生成将单导联转换为多导联心电图，但存在潜在空间差距，影响诊断性能。

Method: 提出SelfMIS框架，采用自切割策略将多导联心电图与对应的单导联片段配对，直接在潜在空间中对齐，使单导联编码器能够从局部信号推断全局心脏上下文。

Result: SelfMIS在九种心肌梗死类型上均优于基线模型，同时保持更简单的架构和更低的计算开销。

Conclusion: 直接潜在空间对齐是有效的，能够显著提升单导联心电图心肌梗死检测性能。

Abstract: Myocardial infarction is a critical manifestation of coronary artery disease,
yet detecting it from single-lead electrocardiogram (ECG) remains challenging
due to limited spatial information. An intuitive idea is to convert single-lead
into multiple-lead ECG for classification by pre-trained models, but generative
methods optimized at the signal level in most cases leave a large latent space
gap, ultimately degrading diagnostic performance. This naturally raises the
question of whether latent space alignment could help. However, most prior ECG
alignment methods focus on learning transformation invariance, which mismatches
the goal of single-lead detection. To address this issue, we propose SelfMIS, a
simple yet effective alignment learning framework to improve myocardial
infarction detection from single-lead ECG. Discarding manual data
augmentations, SelfMIS employs a self-cutting strategy to pair multiple-lead
ECG with their corresponding single-lead segments and directly align them in
the latent space. This design shifts the learning objective from pursuing
transformation invariance to enriching the single-lead representation,
explicitly driving the single-lead ECG encoder to learn a representation
capable of inferring global cardiac context from the local signal.
Experimentally, SelfMIS achieves superior performance over baseline models
across nine myocardial infarction types while maintaining a simpler
architecture and lower computational overhead, thereby substantiating the
efficacy of direct latent space alignment. Our code and checkpoint will be
publicly available after acceptance.

</details>


### [239] [SpellerSSL: Self-Supervised Learning with P300 Aggregation for Speller BCIs](https://arxiv.org/abs/2509.19401)
*Jiazhen Hong,Geoff Mackellar,Soheila Ghane*

Main category: eess.SP

TL;DR: SpellerSSL框架结合自监督学习和P300聚合技术，解决EEG P300拼写器的低信噪比、泛化能力差和校准耗时三大挑战，在公开数据集上达到94%的字符识别率和21.86 bits/min的信息传输率。


<details>
  <summary>Details</summary>
Motivation: 解决基于EEG的P300拼写器脑机接口面临的三个主要挑战：低信噪比、泛化能力差和耗时的校准过程。

Method: 提出SpellerSSL框架：1）采用P300聚合策略增强信噪比；2）使用定制的1D U-Net骨干网络，在跨域和域内EEG数据上进行预训练；3）通过轻量级ERP-Head分类器进行微调，适应特定被试数据。

Result: 在II-B公开数据集上，域内自监督学习达到94%的字符识别率（仅需7次重复）和21.86 bits/min的最高信息传输率。域内自监督学习结合P300聚合可将所需校准数据量减少60%，同时保持可比的字符识别率。

Conclusion: 这是首个将自监督学习应用于P300拼写器的研究，展示了其在提高拼写器BCI效率和泛化能力方面的潜力，为P300拼写器BCI的EEG基础模型铺平了道路。

Abstract: Electroencephalogram (EEG)-based P300 speller brain-computer interfaces
(BCIs) face three main challenges: low signal-to-noise ratio (SNR), poor
generalization, and time-consuming calibration. We propose SpellerSSL, a
framework that combines self-supervised learning (SSL) with P300 aggregation to
address these issues. First, we introduce an aggregation strategy to enhance
SNR. Second, to achieve generalization in training, we employ a customized 1D
U-Net backbone and pretrain the model on both cross-domain and in-domain EEG
data. The pretrained model is subsequently fine-tuned with a lightweight
ERP-Head classifier for P300 detection, which adapts the learned
representations to subject-specific data. Our evaluations on calibration time
demonstrate that combining the aggregation strategy with SSL significantly
reduces the calibration burden per subject and improves robustness across
subjects. Experimental results show that SSL learns effective EEG
representations in both in-domain and cross-domain, with in-domain achieving a
state-of-the-art character recognition rate of 94% with only 7 repetitions and
the highest information transfer rate (ITR) of 21.86 bits/min on the public
II-B dataset. Moreover, in-domain SSL with P300 aggregation reduces the
required calibration size by 60% while maintaining a comparable character
recognition rate. To the best of our knowledge, this is the first study to
apply SSL to P300 spellers, highlighting its potential to improve both
efficiency and generalization in speller BCIs and paving the way toward an EEG
foundation model for P300 speller BCIs.

</details>


### [240] [Online Adaptation via Dual-Stage Alignment and Self-Supervision for Fast-Calibration Brain-Computer Interfaces](https://arxiv.org/abs/2509.19403)
*Sheng-Bin Duan,Jian-Long Hao,Tian-Yu Xiang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Zeng-Guang Hou*

Main category: eess.SP

TL;DR: 该研究提出了一种基于双阶段对齐和自监督的在线适应算法，用于解决脑机接口系统中个体脑电活动差异的问题，能够在单次在线试验中更新解码器，显著提高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）脑机接口系统在实际应用中面临个体脑活动差异的挑战，这限制了系统的在线应用效果。为了克服这一限制，需要开发能够快速适应新用户的算法。

Method: 提出双阶段对齐和自监督的在线适应算法：首先在EEG数据空间进行欧几里得对齐，然后在表示空间更新批归一化统计量；设计自监督损失函数，使用解码器生成的软伪标签作为未知真实标签的代理，并通过香农熵校准以促进自监督训练。

Result: 在五个公共数据集和七种解码器上的实验表明，该算法能够无缝集成到不同BCI范式和解码器架构中。每次迭代仅需一个在线试验即可更新解码器，在稳态视觉诱发电位（SSVEP）上平均准确率提升4.9%，在运动想象任务上提升3.6%。

Conclusion: 该算法支持快速校准操作，在脑机接口应用中具有巨大潜力，能够有效解决个体差异问题，提高BCI系统的在线应用性能。

Abstract: Individual differences in brain activity hinder the online application of
electroencephalogram (EEG)-based brain computer interface (BCI) systems. To
overcome this limitation, this study proposes an online adaptation algorithm
for unseen subjects via dual-stage alignment and self-supervision. The
alignment process begins by applying Euclidean alignment in the EEG data space
and then updates batch normalization statistics in the representation space.
Moreover, a self-supervised loss is designed to update the decoder. The loss is
computed by soft pseudo-labels derived from the decoder as a proxy for the
unknown ground truth, and is calibrated by Shannon entropy to facilitate
self-supervised training. Experiments across five public datasets and seven
decoders show the proposed algorithm can be integrated seamlessly regardless of
BCI paradigm and decoder architecture. In each iteration, the decoder is
updated with a single online trial, which yields average accuracy gains of 4.9%
on steady-state visual evoked potentials (SSVEP) and 3.6% on motor imagery.
These results support fast-calibration operation and show that the proposed
algorithm has great potential for BCI applications.

</details>
