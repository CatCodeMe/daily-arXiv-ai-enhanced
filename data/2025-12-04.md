<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 8]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.RO](#cs.RO) [Total: 7]
- [stat.ML](#stat.ML) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.IT](#cs.IT) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 8]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases](https://arxiv.org/abs/2512.03278)
*Michael Theologitis,Dan Suciu*

Main category: cs.DB

TL;DR: Thucy是首个跨数据库、跨表格的多智能体声明验证系统，能在关系型数据库中自动发现、检查并推理验证声明，提供SQL查询作为证据，在TabFact基准上达到94.3%准确率，超越之前最佳5.6个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前验证系统主要局限于小型单表数据库，无法处理真实世界中跨多个数据库和表格的复杂声明验证需求。随着LLM和智能体AI的进步，构建能够自动验证结构化数据声明的系统已成为可能。

Method: 开发了Thucy系统，采用多智能体架构，能够自主发现、检查并推理所有可用关系型数据库。系统对底层数据源完全不可知，支持跨数据库、跨表格的声明验证，并提供支持验证结论的具体SQL查询作为证据。

Result: 在TabFact数据集（结构化数据事实验证的标准基准）上，Thucy达到94.3%的准确率，比之前最佳结果88.7%提升了5.6个百分点，创下新的state-of-the-art。

Conclusion: Thucy展示了跨数据库声明验证的可行性，通过多智能体架构和透明SQL查询证据，为真实世界中的事实验证提供了更强大的解决方案，填补了技术可能性与实际演示之间的差距。

Abstract: In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\unicode{x2014}$typically a few hundred rows$\unicode{x2014}$that conveniently fit within an LLM's context window.
  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\unicode{x2014}$the standard benchmark for fact verification over structured data$\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).

</details>


### [2] [Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams](https://arxiv.org/abs/2512.03389)
*Shu Chen,Deepti Raghavan,Uğur Çetintemel*

Main category: cs.DB

TL;DR: Continuous Prompts (CPs) 框架将LLM推理引入连续流处理，通过扩展RAG到流式场景、定义连续语义算子，并采用动态优化框架平衡准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM框架是状态无关且一次性的，无法满足对非结构化数据流进行持久、语义感知计算的需求，限制了其在长期运行分析中的应用。

Method: 1. 提出Continuous Prompts框架，将RAG扩展到流式设置；2. 定义连续语义算子；3. 实现多种方法（主要基于LLM，也有嵌入变体）；4. 研究LLM中心优化（元组批处理和算子融合）；5. 开发动态优化框架，使用轻量级影子执行和成本感知多目标贝叶斯优化。

Result: 在VectraFlow流处理系统中实现CPs，通过算子级微基准测试和真实数据集上的流式管道验证，系统能够适应工作负载动态变化，平衡准确性与效率权衡，并维持对演化中非结构化流的持久语义查询。

Conclusion: Continuous Prompts框架成功将LLM推理引入连续流处理，通过动态优化机制有效管理准确性与效率的权衡，为非结构化数据流的持久语义分析提供了可行解决方案。

Abstract: Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.
  Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.
  We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.

</details>


### [3] [Enterprise Data Science Platform: A Unified Architecture for Federated Data Access](https://arxiv.org/abs/2512.03401)
*Ryoto Miyamoto,Akira Kasuga*

Main category: cs.DB

TL;DR: EDSP是一个基于数据湖仓架构的企业数据科学平台，采用"一次写入，随处读取"原则，通过联邦数据访问解决多查询引擎环境中的数据孤岛问题，减少数据复制和厂商锁定。


<details>
  <summary>Details</summary>
Motivation: 组织在不同部门采用不同数据分析平台时面临数据共享困难，传统数据仓库需要数据复制到特定厂商存储中，导致数据不一致、成本增加和厂商锁定问题。

Method: 提出企业数据科学平台（EDSP），基于数据湖仓架构，采用四层架构：数据准备层、数据存储层、访问接口层和查询引擎层，实现联邦数据访问。

Result: 实验证明主要云数据仓库和编程环境可直接查询EDSP管理的数据集，生产部署确认了跨多查询引擎的互操作性，相比传统方法减少33-44%操作步骤，查询延迟最多增加2.6倍但仍保持秒级响应。

Conclusion: EDSP为多查询引擎环境中的数据孤岛问题提供了实用的设计指南，通过集中数据管理和联邦访问消除数据复制和厂商锁定，在保持实用性能的同时简化数据共享流程。

Abstract: Organizations struggle to share data across departments that have adopted different data analytics platforms. If n datasets must serve m environments, up to n*m replicas can emerge, increasing inconsistency and cost. Traditional warehouses copy data into vendor-specific stores; cross-platform access is hard. This study proposes the Enterprise Data Science Platform (EDSP), which builds on data lakehouse architecture and follows a Write-Once, Read-Anywhere principle. EDSP enables federated data access for multi-query engine environments, targeting data science workloads with periodic data updates and query response times ranging from seconds to minutes. By providing centralized data management with federated access from multiple query engines to the same data sources, EDSP eliminates data duplication and vendor lock-in inherent in traditional data warehouses. The platform employs a four-layer architecture: Data Preparation, Data Store, Access Interface, and Query Engines. This design enforces separation of concerns and reduces the need for data migration when integrating additional analytical environments. Experimental results demonstrate that major cloud data warehouses and programming environments can directly query EDSP-managed datasets. We implemented and deployed EDSP in production, confirming interoperability across multiple query engines. For data sharing across different analytical environments, EDSP achieves a 33-44% reduction in operational steps compared with conventional approaches requiring data migration. Although query latency may increase by up to a factor of 2.6 compared with native tables, end-to-end completion times remain on the order of seconds, maintaining practical performance for analytical use cases. Based on our production experience, EDSP provides practical design guidelines for addressing the data-silo problem in multi-query engine environments.

</details>


### [4] [ExOAR: Expert-Guided Object and Activity Recognition from Textual Data](https://arxiv.org/abs/2512.03790)
*Iris Beerepoot,Vinicius Stein Dani,Xixi Lu*

Main category: cs.DB

TL;DR: ExOAR是一个结合大语言模型和人工验证的交互式方法，用于从非结构化文本中提取对象和活动信息，以支持对象中心流程挖掘。


<details>
  <summary>Details</summary>
Motivation: 对象中心流程挖掘需要结构化数据，但从非结构化文本中提取这些数据仍然是一个挑战。现有方法难以有效识别文本中的对象和活动信息。

Method: ExOAR采用交互式方法，结合大语言模型和人工验证。系统引导用户通过连续阶段：LLM基于上下文输入（如用户职业）和文本数据生成候选对象类型、活动和对象实例，用户审查并完善这些建议后再进入下一阶段。

Result: 通过演示验证和实际Active Window Tracking数据评估，ExOAR能有效弥合非结构化文本数据与对象中心流程分析所需的结构化日志之间的差距，同时保持灵活性和人工监督。

Conclusion: ExOAR提供了一种实用的解决方案，能够从非结构化文本中提取结构化语义信息，支持对象中心流程分析，同时通过人工监督确保质量。

Abstract: Object-centric process mining requires structured data, but extracting it from unstructured text remains a challenge. We introduce ExOAR (Expert-Guided Object and Activity Recognition), an interactive method that combines large language models (LLMs) with human verification to identify objects and activities from textual data. ExOAR guides users through consecutive stages in which an LLM generates candidate object types, activities, and object instances based on contextual input, such as a user's profession, and textual data. Users review and refine these suggestions before proceeding to the next stage. Implemented as a practical tool, ExOAR is initially validated through a demonstration and then evaluated with real-world Active Window Tracking data from five users. Our results show that ExOAR can effectively bridge the gap between unstructured textual data and the structured log with clear semantics needed for object-centric process analysis, while it maintains flexibility and human oversight.

</details>


### [5] [IBM Multilevel Process Mining vs de facto Object-Centric Process Mining approaches](https://arxiv.org/abs/2512.03906)
*Alberto Ronzoni,Anina Antony,Anjana M R,Francesca De Leo,Jesna Jose,Mattia Freda,Nandini Narayanankutty,Rafflesia Khan,Raji RV,Thomas Diacci*

Main category: cs.DB

TL;DR: 本文比较了对象中心过程挖掘和IBM多级过程挖掘两种方法，并基于比较开发了结合两者优势的组织挖掘新功能


<details>
  <summary>Details</summary>
Motivation: 过程挖掘领域正在向对象中心过程挖掘发展，IBM开发了多级过程挖掘方法，需要比较这两种方法的优缺点，以指导产品演进

Method: 对对象中心过程挖掘和IBM多级过程挖掘进行描述和比较分析，基于比较开发组织挖掘新功能，并通过示例演示

Result: 开发了IBM过程挖掘产品的新组织挖掘功能，结合了两种方法的优势，展示了这种创新方法的潜力

Conclusion: 通过比较两种过程挖掘方法，IBM成功开发了创新的组织挖掘功能，为过程挖掘领域提供了新的方法论

Abstract: The academic evolution of process mining is moving toward object centric process mining, marking a significant shift in how processes are modeled and analyzed. IBM has developed its own distinctive approach called Multilevel Process Mining. This paper provides a description of the two approaches and presents a comparative analysis of their respective advantages and limitations. IBM leveraged this comparison to drive the evolution of IBM Process Mining product, creating the new Organizational Mining feature, an innovation that combines the best of the two approaches. Demonstrate the potential of this novel, innovative and distinct methodology with an example.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity](https://arxiv.org/abs/2512.03416)
*Ruiqi Lai,Hongrui Liu,Chengzhi Lu,Zonghao Liu,Siyu Cao,Siyang Shao,Yixin Zhang,Luo Mai,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: TokenScale是一个用于解耦LLM服务的自动扩缩框架，通过Token Velocity指标和Convertible Decoders创新，显著提升SLO达成率和降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM服务预填充/解码解耦架构虽然提高了资源利用率，但难以应对突发工作负载。传统自动扩缩策略基于GPU利用率或请求数等滞后指标，对负载峰值反应慢，导致TTFT和TPOT SLO违规以及过度配置成本高。

Method: 提出两个创新：1) Token Velocity指标，统一预填充、网络和解码阶段的工作速率，作为系统背压的领先指标实现主动扩缩；2) Convertible Decoders，允许解码GPU在流量峰值时动态执行预填充任务，创建快速响应缓冲区吸收突发流量。

Result: 在GPU集群和生产环境trace上的评估显示，TokenScale将SLO达成率从50-88%提升到80-96%，相比DistServe、BlitzScale和AIBrix等先进系统降低成本4-14%。

Conclusion: 通过结合预测性指标和灵活系统设计，TokenScale显著提升了解耦LLM服务基础设施的性能和效率。

Abstract: The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.

</details>


### [7] [Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks](https://arxiv.org/abs/2512.03487)
*Zhen Wang,Bin Lin,Qiang,Ye*

Main category: cs.DC

TL;DR: 提出一种面向空天海一体化网络的双边缘辅助计算卸载与资源分配方案，利用无人机和低轨卫星为海上自主船舶提供并行计算服务，通过联合优化降低系统能耗


<details>
  <summary>Details</summary>
Motivation: 空天海一体化网络中海事自主船舶的计算需求日益增长，需要高效的计算卸载方案来满足低延迟要求，同时降低系统能耗

Method: 采用双边缘计算架构（无人机+低轨卫星），通过多接入方式并行处理计算任务，使用交替优化和分层方法分解原始优化问题，联合优化卸载模式、卸载量和计算资源分配

Result: 仿真验证表明，所提方案在能耗和延迟性能上优于基准算法，实现了空天海一体化网络的高效计算服务

Conclusion: 双边缘辅助的计算卸载与资源分配方案能有效降低空天海一体化网络的能耗，满足海事自主船舶的低延迟计算需求，为未来海洋物联网应用提供可行解决方案

Abstract: In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.

</details>


### [8] [Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas](https://arxiv.org/abs/2512.03565)
*Luis Gall,Samuel James Newcome,Fabio Alexander Gratl,Markus Mühlhäußer,Manish Kumar Mishra,Hans-Joachim Bungartz*

Main category: cs.DC

TL;DR: 该论文研究了在粒子模拟库AutoPas中，通过SIMD向量化技术优化分子间成对力计算的方法，重点关注粒子值加载到向量寄存器的顺序对性能和能耗的影响。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟可以帮助科学家在原子尺度上获取物理过程的宝贵见解。先前研究表明，最优的MD算法可能在运行时发生变化，因此需要研究模拟特定参数（如粒子密度）和邻居识别算法的影响，并扩展AutoPas的动态调优机制。

Method: 探索了多种SIMD向量化技术来改进分子间的成对力计算，重点关注粒子值加载到向量寄存器的顺序优化。扩展了AutoPas的动态调优机制，使其能够在运行时选择最优的向量化顺序，并研究了粒子密度和邻居识别算法等参数的影响。

Result: 基准测试表明，在运行时考虑不同的粒子相互作用顺序，相比AutoPas先前的方法，可以显著提高力计算的性能。

Conclusion: 通过优化粒子值加载到向量寄存器的顺序，并利用动态调优机制在运行时选择最优的向量化策略，可以显著提升分子动力学模拟中力计算的性能，特别是在考虑粒子密度和邻居识别算法等模拟特定参数的情况下。

Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.

</details>


### [9] [FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management](https://arxiv.org/abs/2512.03644)
*Bohan Zhao,Yuanhong Wang,Chenglin Liu,Jiagi Pan,Guang Yang,Ruitao Liu,Tingrui Zhang,Kai Luo,Wei Xu*

Main category: cs.DC

TL;DR: FFTrainer利用网络剩余带宽快速保存和加载训练状态，避免回滚并加速恢复，将恢复时间减少98%，GPU利用率损失降低68%


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型集群规模扩大，节点故障、漫长恢复时间和庞大检查点降低了训练效率。异步检查点要么触发代价高昂的回滚，要么增加过多开销。

Method: FFTrainer利用网络剩余容量快速保存和加载训练状态，防止回滚并加速恢复过程。

Result: 相比现有检查点方法，FFTrainer将恢复时间减少高达98%，GPU利用率损失降低高达68%，且不影响正常训练。

Conclusion: FFTrainer通过有效利用网络资源，为大规模LLM训练提供了高效且鲁棒的解决方案。

Abstract: Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.

</details>


### [10] [On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs](https://arxiv.org/abs/2512.03697)
*Rafael Ravedutti Lucio Machado,Jan Eitzinger,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 该论文分析了在Fritz和Alex HPC集群上使用合成基准测试和Gromacs软件包进行能效分析时遇到的挑战，提出了最佳实践建议。


<details>
  <summary>Details</summary>
Motivation: 研究高性能计算集群中能效分析的挑战，特别是在使用合成基准测试和实际应用（如Gromacs）时，识别测量和分析过程中的陷阱。

Method: 在Intel Ice Lake和Sapphire Rapids CPU以及Nvidia A40和A100 GPU上使用MPI并行化进行实验，利用Likwid和Nvidia性能分析工具收集指标和测量数据。

Result: 展示了在不同硬件配置上的能效测量结果，揭示了实验和分析过程中遇到的具体挑战和陷阱。

Conclusion: 总结了能效分析中的关键挑战，并为未来的能效研究提出了最佳实践建议。

Abstract: This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.

</details>


### [11] [A Chronological Analysis of the Evolution of SmartNICs](https://arxiv.org/abs/2512.04054)
*Olasupo Ajayi,Ryan Grant*

Main category: cs.DC

TL;DR: 本文对智能网卡（SNICs）进行了15年（2010-2024）的系统分析，涵盖其演进历程、制造商、应用场景和领域，基于370篇文献的时序分析。


<details>
  <summary>Details</summary>
Motivation: 随着互联网设备的普及和对更快网络访问需求的增长，传统网卡已演变为智能网卡（SNICs），但关于SNICs的确切用途和适用性仍存在持续争议，特别是随着加速器的集成，这些争议更加复杂。需要系统分析SNICs的发展历程和应用现状。

Method: 采用时序分析方法，对过去15年（2010-2024）发表的370篇相关文献进行系统分析，从演进历程、制造商、使用案例和应用领域等多个维度研究SNICs。

Result: 通过对370篇文献的分析，揭示了SNICs的演进轨迹、主要制造商格局、实际应用场景和具体应用领域，为理解SNICs的发展现状提供了实证基础。

Conclusion: 智能网卡（SNICs）作为现代互联网的关键使能技术，经历了从传统网卡到集成加速器的智能设备的演进，其应用场景和领域正在不断扩展，但对其确切用途和适用性的讨论仍在继续，需要更多研究来明确其最佳实践和应用边界。

Abstract: Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.

</details>


### [12] [Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods](https://arxiv.org/abs/2512.03825)
*Aingeru Ramos,Jose A Pascual,Javier Navaridas,Ivan Coluzza*

Main category: cs.DC

TL;DR: 本文提出了一种并行化的Metropolis-Hastings并行回火算法，使用OpenMP和CUDA分别在CPU和GPU上实现并行化，显著提升了采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统的马尔可夫链蒙特卡洛方法在处理复杂构型空间时采样精度不足，而并行回火技术虽然提高了精度但计算成本显著增加。需要开发并行化方案来抵消这种计算开销，使MCMC/PT技术能更有效地运行并研究更大规模的模型。

Method: 开发了Metropolis-Hastings并行回火算法的并行实现，使用OpenMP在现代CPU上进行并行化，使用CUDA在GPU上进行并行化。通过多个副本并行运行并定期交换状态来改进采样精度。

Result: 使用OpenMP（48核）实现了最大52倍的加速，使用CUDA版本实现了986倍的加速。这些结果也为未来量子实现的相同算法提供了基准比较基础。

Conclusion: 提出的并行实现显著提升了Metropolis-Hastings并行回火算法的计算效率，为研究更大规模的物理/化学模型提供了有效工具，并为未来量子实现建立了性能基准。

Abstract: Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.

</details>


### [13] [OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference](https://arxiv.org/abs/2512.03927)
*Liujianfu Wang,Yuyang Du,Yuchen Pan,Soung Chang Liew,Jiacheng Liu,Kexin Chen*

Main category: cs.DC

TL;DR: OD-MoE：一种无需专家缓存的分布式MoE推理框架，通过按需加载专家参数，在边缘设备上实现高效推理


<details>
  <summary>Details</summary>
Motivation: MoE模型在内存受限的边缘设备上部署面临挑战，现有专家卸载方案中GPU内存利用率不足，需要更高效的解决方案

Method: 1）在分布式边缘节点上并行化专家加载与计算；2）使用超精确的模拟预测器提前多层预测专家激活

Result: 达到99.94%的专家激活预测准确率，解码速度达到完全GPU缓存部署的75%，仅使用1/3的GPU内存

Conclusion: OD-MoE消除了专家缓存需求，使MoE推理能在小于1GB GPU内存的边缘节点上运行，为低成本IoT设备部署MoE模型铺平道路

Abstract: Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [14] [On the Complexity of the Ordered Covering Problem in Distance Geometry](https://arxiv.org/abs/2512.03124)
*Michael Souza,Júlio Araújo,John Kesley Costa,Carlile Lavor*

Main category: cs.DS

TL;DR: 本文证明了有序覆盖问题（OCP）是NP完全问题，通过从强NP完全的3-划分问题进行多项式时间归约，为实际中使用的启发式方法提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 在可离散化分子距离几何问题（DMDGP）中，有序覆盖问题（OCP）的剪枝边排序对SBBU算法性能有重要影响。虽然已有贪婪启发式方法显著优于原始排序，但寻找最优解的计算复杂度问题仍未解决。

Method: 通过从强NP完全的3-划分问题到OCP的多项式时间归约来证明NP完全性。归约构造了一个严格的预算约束，使得最优解恰好对应有效的3-划分。

Result: 证明了OCP是NP完全问题，这为最优边排序建立了计算障碍，并为实践中使用的启发式方法提供了理论依据。

Conclusion: OCP的NP完全性证明了寻找最优边排序在计算上是困难的，这解释了为什么实践中需要使用启发式方法，并为未来研究提供了理论基础。

Abstract: The Ordered Covering Problem (OCP) arises in the context of the Discretizable Molecular Distance Geometry Problem (DMDGP), where the ordering of pruning edges significantly impacts the performance of the SBBU algorithm for protein structure determination. In recent work, Souza et al. (2023) formalized OCP as a hypergraph covering problem with ordered, exponential costs, and proposed a greedy heuristic that outperforms the original SBBU ordering by orders of magnitude. However, the computational complexity of finding optimal solutions remained open. In this paper, we prove that OCP is NP-complete through a polynomial-time reduction from the strongly NP-complete 3-Partition problem. Our reduction constructs a tight budget that forces optimal solutions to correspond exactly to valid 3-partitions. This result establishes a computational barrier for optimal edge ordering and provides theoretical justification for the heuristic approaches currently used in practice.

</details>


### [15] [Complexity of Local Search for CSPs Parameterized by Constraint Difference](https://arxiv.org/abs/2512.03275)
*Aditya Anand,Vincent Cohen-Addad,Tommaso d'Orsi,Anupam Gupta,Euiwoong Lee,Debmalya Panigrahi,Sijin Peng*

Main category: cs.DS

TL;DR: 本文研究了局部搜索的参数化复杂度，提出了一个通用模型：给定当前解P和目标解S*，目标是在参数化时间f(k)·n^O(1)内找到与S*一样好的可行解，其中k=|PΔS*|。该模型推广了经典参数化优化问题，并应用于布尔对称CSPs，给出了完整的参数化复杂度分类。


<details>
  <summary>Details</summary>
Motivation: 研究局部搜索的参数化复杂度，建立一个通用框架来统一处理各种参数化优化问题。传统参数化问题通常考虑从全集U中移除最少元素使其可行（对应P=U），本文将其推广到任意当前解P的情况，从而涵盖更广泛的局部搜索场景。

Method: 提出一个通用参数化局部搜索模型：输入包括当前解P和问题实例，目标是找到与最优解S*质量相当的可行解，参数k定义为当前解与目标解之间的对称差|PΔS*|。将该模型应用于布尔对称约束满足问题（CSPs），其中U是约束集合，可行子集U'存在满足所有约束的变量赋值。通过分析不同对称谓词的特性，对布尔对称CSPs进行完整的参数化复杂度分类。

Result: 给出了布尔对称CSPs参数化局部搜索问题的完整复杂度分类。对于某些对称谓词，问题在FPT（固定参数可解）中；对于其他谓词，问题为W[1]-hard或para-NP-hard。分类结果揭示了对称CSPs局部搜索问题的计算复杂性边界。

Conclusion: 本文建立了一个统一的参数化局部搜索框架，推广了经典参数化优化问题。通过对布尔对称CSPs的完整分类，揭示了这类问题在参数化复杂度理论中的精细结构，为理解局部搜索算法的计算复杂性提供了理论基础。

Abstract: In this paper, we study the parameterized complexity of local search, whose goal is to find a good nearby solution from the given current solution. Formally, given an optimization problem where the goal is to find the largest feasible subset $S$ of a universe $U$, the new input consists of a current solution $P$ (not necessarily feasible) as well as an ordinary input for the problem.
  Given the existence of a feasible solution $S^*$, the goal is to find a feasible solution as good as $S^*$ in parameterized time $f(k) \cdot n^{O(1)}$, where $k$ denotes the distance $|PΔS^*|$. This model generalizes numerous classical parameterized optimization problems whose parameter $k$ is the minimum number of elements removed from $U$ to make it feasible, which corresponds to the case $P = U$.
  We apply this model to widely studied Constraint Satisfaction Problems (CSPs), where $U$ is the set of constraints, and a subset $U'$ of constraints is feasible if there is an assignment to the variables satisfying all constraints in $U'$. We give a complete characterization of the parameterized complexity of all boolean-alphabet symmetric CSPs, where the predicate's acceptance depends on the number of true literals.

</details>


### [16] [Fast approximate $\ell$-center clustering in high dimensional spaces](https://arxiv.org/abs/2512.03304)
*Mirosław Kowaluk,Andrzej Lingas,Mia Persson*

Main category: cs.DS

TL;DR: 本文提出了一种基于随机降维的高效近似算法，用于解决高维欧几里得空间和汉明空间中的ℓ-中心聚类和最小直径ℓ-聚类问题，显著提升了现有算法的运行速度。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中，ℓ-中心聚类和最小直径ℓ-聚类问题的现有近似算法运行时间严重依赖于维度大小，当ℓ和维度都超过对数级别时，算法效率低下。需要设计更高效的近似算法来解决这一瓶颈。

Method: 主要工具是随机降维技术。首先提出一种通用方法，减少高维欧几里得空间中ℓ-中心问题算法运行时间对维度大小的依赖。利用该方法，为欧几里得和汉明空间中的ℓ-中心聚类和最小直径ℓ-聚类问题提供(2+ε)-近似算法。还将该方法应用于最近的高维欧几里得空间中ℓ-中心聚类问题的快速近似算法。最后，对包含z个异常值的ℓ-中心聚类问题的O(1)-近似方法进行加速。

Result: 当ℓ和维度都超过对数级别时，提出的(2+ε)-近似算法比已知的2-近似算法显著更快。通过随机降维技术成功减少了算法运行时间对维度大小的依赖，为高维空间中的聚类问题提供了更高效的解决方案。

Conclusion: 随机降维是解决高维空间中ℓ-中心聚类和最小直径ℓ-聚类问题的有效工具，能够显著提升算法效率，特别是在ℓ和维度都较大的情况下。该方法不仅适用于基本聚类问题，还能扩展到包含异常值的更一般化问题。

Abstract: We study the design of efficient approximation algorithms for the
  $\ell$-center clustering and minimum-diameter $\ell$-clustering
  problems in high dimensional Euclidean and Hamming spaces. Our main
  tool is randomized dimension reduction. First, we present a general
  method of reducing the dependency of the running time of a
  hypothetical algorithm for the $\ell$-center problem in a high
  dimensional Euclidean space on the dimension size. Utilizing in
  part this method, we provide $(2+ε)$- approximation
  algorithms for the $\ell$-center clustering and minimum-diameter
  $\ell$-clustering problems in Euclidean and Hamming spaces that are
  substantially faster than the known $2$-approximation ones when both
  $\ell$ and the dimension are super-logarithmic. Next, we apply the
  general method to the recent fast approximation algorithms with
  higher approximation guarantees for the $\ell$-center clustering
  problem in a high dimensional Euclidean space. Finally, we provide a
  speed-up of the known $O(1)$-approximation method for the
  generalization of the $\ell$-center clustering problem to include
  $z$ outliers (i.e., $z$ input points can be ignored while computing
  the maximum distance of an input point to a center) in high
  dimensional Euclidean and Hamming spaces.

</details>


### [17] [Singing a MIS](https://arxiv.org/abs/2512.03311)
*Sandy Irani,Michael Luby*

Main category: cs.DS

TL;DR: 提出了一种称为"歌唱模型"的广播模型，其中代理对网络结构和邻居一无所知，但可以发出多个音符。给出了一个简单自然的协议，代理通过音符数量竞争，在O(log(n))时间内收敛到MIS。


<details>
  <summary>Details</summary>
Motivation: 现有beeping模型只能发出单一频率声音，限制了表达能力。需要一种更通用的广播模型，使代理能在对网络结构完全无知的情况下进行更复杂的通信，特别是在动态网络和异步环境中。

Method: 提出歌唱模型作为beeping模型的泛化，代理可以发出多个音符。设计了一个简单协议：代理通过歌唱的音符数量与邻居竞争，音符数量反映代理的"强度"。协议在异步、动态网络中工作，代理可以故障。

Result: 协议以高概率在O(log(n))时间内收敛到最大独立集(MIS)。这是首个在网络无知模型下为动态网络提供对数时间MIS收敛的协议。

Conclusion: 歌唱模型比beeping模型更具表达力，提出的协议简单有效，能在异步动态网络中快速收敛到MIS，为网络无知环境下的分布式计算提供了新工具。

Abstract: We introduce a broadcast model called the singing model, where agents are oblivious of the size and structure of the communication network, even their immediate neighborhood. Agents can sing multiple notes which are heard by their neighbors. The model is a generalization of the beeping model, where agents can only emit sound at a single frequency. We give a simple and natural protocol where agents compete with their neighbors and their strength is reflected in the number of notes they sing. It converges in $O(log(n))$ time with high probability, where $n$ is the number of agents in the network. The protocol works in an asynchronous model where rounds vary in length and have different start times. It works with completely dynamic networks where agents can be faulty. The protocol is the first to converge to an MIS in logarithmic time for dynamic networks in a network oblivious model.

</details>


### [18] [Comparative algorithm performance evaluation and prediction for the maximum clique problem using instance space analysis](https://arxiv.org/abs/2512.03419)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.DS

TL;DR: 使用实例空间分析(ISA)方法系统分析最大团问题的实例空间，评估和预测包括精确、启发式和GNN算法在内的SOTA算法性能，在真实图数据集上构建性能预测模型。


<details>
  <summary>Details</summary>
Motivation: 最大团问题作为经典的组合优化问题，虽然已有多种算法解决方案，但对问题实例的系统分析仍然不足。需要系统分析实例空间并评估不同算法的性能表现。

Method: 采用实例空间分析(ISA)方法，使用TWITTER、COLLAB和IMDB-BINARY基准数据集，提取33个通用特征和2个问题特定特征（包括谱特性），构建复合性能度量（包含解质量和运行时间），评估MOMC、Gurobi、CliSAT等算法。

Result: MOMC算法在约74.7%的实例空间中表现最优，Gurobi和CliSAT分别在13.8%和11%的实例空间中表现最好。基于ISA的性能预测模型在BHOSLIB和DIMACS测试集上，top-1和top-2预测准确率分别达到88%和97%。

Conclusion: ISA方法能有效分析最大团问题的实例空间并预测算法性能，MOMC在大多数实例中表现最优，基于ISA的预测模型具有很高的准确性，为算法选择提供了实用工具。

Abstract: The maximum clique problem, a well-known graph-based combinatorial optimization problem, has been addressed through various algorithmic approaches, though systematic analyses of the problem instances remain sparse. This study employs the instance space analysis (ISA) methodology to systematically analyze the instance space of this problem and assess & predict the performance of state-of-the-art (SOTA) algorithms, including exact, heuristic, and graph neural network (GNN)-based methods. A dataset was compiled using graph instances from TWITTER, COLLAB and IMDB-BINARY benchmarks commonly used in graph machine learning research. A set of 33 generic and 2 problem-specific polynomial-time-computable graph-based features, including several spectral properties, was employed for the ISA. A composite performance mea- sure incorporating both solution quality and algorithm runtime was utilized. The comparative analysis demonstrated that the exact algorithm Mixed Order Maximum Clique (MOMC) exhib- ited superior performance across approximately 74.7% of the instance space constituted by the compiled dataset. Gurobi & CliSAT accounted for superior performance in 13.8% and 11% of the instance space, respectively. The ISA-based algorithm performance prediction model run on 34 challenging test instances compiled from the BHOSLIB and DIMACS datasets yielded top-1 and top-2 best performing algorithm prediction accuracies of 88% and 97%, respectively.

</details>


### [19] [Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity](https://arxiv.org/abs/2512.03718)
*Robert Ganian,Hung P. Hoang,Simon Wietheger*

Main category: cs.DS

TL;DR: 本文研究了公平均值聚类问题的计算复杂性，证明了即使在高度受限情况下也不存在固定参数算法，但通过三种规避策略（添加约束、固定参数近似、树状矩阵参数化）可以获得可处理性结果。


<details>
  <summary>Details</summary>
Motivation: 研究公平均值聚类问题的计算复杂性，该问题可等价表述为通过修改最多k个值将彩色矩阵编辑为具有少量不同颜色平衡行的矩阵。虽然该问题在公平忽略和公平设置下都是NP难的，但在前者中已知存在固定参数算法。本文旨在探索公平设置下的算法可能性。

Method: 首先证明了即使在高度受限的公平均值聚类实例中也不存在类似固定参数算法。然后通过三种规避策略获得完整的复杂性图谱：1) 对问题实例施加额外约束；2) 固定参数近似；3) 针对树状矩阵的替代参数化。

Result: 排除了公平设置下类似固定参数算法的存在性，但通过三种规避策略获得了可处理性结果，为公平均值聚类问题提供了完整的复杂性分类。

Conclusion: 公平均值聚类问题在公平设置下比传统设置更具挑战性，但通过适当的规避策略（添加约束、近似算法或替代参数化）可以获得可行的算法解决方案。

Abstract: We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.

</details>


### [20] [Robust Algorithms for Path and Cycle Problems in Geometric Intersection Graphs](https://arxiv.org/abs/2512.03843)
*Malory Marin,Jean-Florent Raymond,Rémi Watrigant*

Main category: cs.DS

TL;DR: 提出λ-linked partition新工具，在R^d中相似尺寸fat对象交集图上设计鲁棒亚指数算法，解决了Hamiltonian Path/Cycle的ETH紧算法问题，并扩展到参数化Long Path算法。


<details>
  <summary>Details</summary>
Motivation: 解决几何交集图上Hamiltonian Path和Hamiltonian Cycle的ETH紧算法开放问题，设计鲁棒的亚指数算法，扩展参数化算法到任意固定维度。

Method: 引入λ-linked partition作为新工具，将顶点集划分为高度连通的组，可在多项式时间计算且无需几何表示。结合低树宽模式覆盖定理，设计随机化鲁棒算法。

Result: 获得Hamiltonian Path/Cycle在R^d中相似尺寸fat对象交集图上的首个ETH紧算法，时间2^{O(n^{1-1/d})}。参数化Long Path算法时间2^{O(k^{1-1/d}log^2 k)} n^{O(1)}。

Conclusion: λ-linked partition是设计几何交集图鲁棒亚指数算法的有效工具，解决了de Berg等人的开放问题，完成了ETH紧算法视角下的研究，结构结果具有独立价值。

Abstract: We study the design of robust subexponential algorithms for classical connectivity problems on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$. In this setting, each vertex corresponds to a geometric object, and two vertices are adjacent if and only if their objects intersect. We introduce a new tool for designing such algorithms, which we call a $λ$-linked partition. This is a partition of the vertex set into groups of highly connected vertices. Crucially, such a partition can be computed in polynomial time and does not require access to the geometric representation of the graph. We apply this framework to problems related to paths and cycles in graphs. First, we obtain the first robust ETH-tight algorithms for Hamiltonian Path and Hamiltonian Cycle, running in time $2^{O(n^{1-1/d})}$ on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$. This resolves an open problem of de Berg et al. [STOC 2018] and completes the study of these problems on geometric intersection graphs from the viewpoint of ETH-tight exact algorithms. We further extend our approach to the parameterized setting and design the first robust subexponential parameterized algorithm for Long Path in any fixed dimension $d$. More precisely, we obtain a randomized robust algorithm running in time $2^{O(k^{1-1/d}\log^2 k)}\, n^{O(1)}$ on intersection graphs of similarly sized fat objects in $\mathbb{R}^d$, where $k$ is the natural parameter. Besides $λ$-linked partitions, our algorithm also relies on a low-treewidth pattern covering theorem that we establish for geometric intersection graphs, which may be viewed as a refinement of a result of Marx-Pilipczuk [ESA 2017]. This structural result may be of independent interest.

</details>


### [21] [Aggregating maximal cliques in real-world graphs](https://arxiv.org/abs/2512.03960)
*Noga Alon,Sabyasachi Basu,Shweta Jain,Haim Kaplan,Jakub Łącki,Blair D. Sullivan*

Main category: cs.DS

TL;DR: 提出ρ-稠密聚合器方法，用少量高密度簇替代枚举所有极大团，显著降低计算复杂度和输出冗余


<details>
  <summary>Details</summary>
Motivation: 极大团枚举存在计算不可行性和高度冗余输出的问题，需要更高效的图结构摘要方法

Method: 引入ρ-稠密聚合器概念，识别边密度至少为ρ的小规模簇集合，这些簇共同包含所有极大团

Result: 证明所有图都存在亚指数大小的聚合器，对有界退化图实现近线性时间和规模，实际测试显示6-300倍加速

Conclusion: ρ-稠密聚合器提供了一种高效且简洁的图结构摘要方法，在理论和实践上都有显著优势

Abstract: Maximal clique enumeration is a fundamental graph mining task, but its utility is often limited by computational intractability and highly redundant output. To address these challenges, we introduce \emph{$ρ$-dense aggregators}, a novel approach that succinctly captures maximal clique structure. Instead of listing all cliques, we identify a small collection of clusters with edge density at least $ρ$ that collectively contain every maximal clique.
  In contrast to maximal clique enumeration, we prove that for all $ρ< 1$, every graph admits a $ρ$-dense aggregator of \emph{sub-exponential} size, $n^{O(\log_{1/ρ}n)}$, and provide an algorithm achieving this bound. For graphs with bounded degeneracy, a typical characteristic of real-world networks, our algorithm runs in near-linear time and produces near-linear size aggregators. We also establish a matching lower bound on aggregator size, proving our results are essentially tight. In an empirical evaluation on real-world networks, we demonstrate significant practical benefits for the use of aggregators: our algorithm is consistently faster than the state-of-the-art clique enumeration algorithm, with median speedups over $6\times$ for $ρ=0.1$ (and over $300\times$ in an extreme case), while delivering a much more concise structural summary.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks](https://arxiv.org/abs/2512.03262)
*Songwen Zhao,Danqing Wang,Kexun Zhang,Jiaxuan Luo,Zhuo Li,Lei Li*

Main category: cs.SE

TL;DR: 论文提出SU S VI B E S基准测试，评估LLM编程代理在真实软件工程任务中的安全性，发现即使功能正确的代码也存在严重安全漏洞，引发对vibe coding在生产环境中部署的担忧。


<details>
  <summary>Details</summary>
Motivation: 随着vibe coding（人类工程师指导LLM代理完成复杂编码任务）的普及，需要评估其输出在生产环境中的安全性，特别是在安全敏感应用中。

Method: 构建包含200个真实开源项目特征请求任务的SU S VI B E S基准测试，这些任务在人类程序员实现时导致了漏洞。使用前沿模型评估多个广泛使用的编码代理，并测试初步安全策略（如添加漏洞提示）的有效性。

Result: 所有编码代理在软件安全方面表现都很差。虽然SWE-Agent with Claude 4 Sonnet的61%解决方案功能正确，但只有10.5%是安全的。初步安全策略无法缓解这些安全问题。

Conclusion: 研究结果对vibe coding的广泛采用，特别是在安全敏感应用中，提出了严重关切，表明当前LLM编程代理生成安全代码的能力存在重大缺陷。

Abstract: Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.

</details>


### [23] [Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization](https://arxiv.org/abs/2512.03421)
*Hexiang Xu,Hengyuan Liu,Yonghao Wu,Xiaolan Kang,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 该研究评估了LLM在故障定位中的表现，发现具有推理能力的高级模型（如OpenAI o3和DeepSeekR1）在Codeflaws、Condefects和BugT数据集上表现优异，但计算成本高且在处理复杂问题时准确性下降。


<details>
  <summary>Details</summary>
Motivation: 传统故障定位方法（如SBFL和MBFL）缺乏代码上下文理解能力，对新手程序员不够有效。LLM具有理解程序语法和语义的能力，有望克服这些限制，为新手提供更好的调试帮助。

Method: 使用Codeflaws、Condefects和BugT三个数据集评估6个闭源和7个开源LLM。BugT是新构建的数据集，专门用于缓解数据泄露问题。评估包括不同难度级别的故障定位任务。

Result: 具有推理能力的高级模型（OpenAI o3、DeepSeekR1）在最小提示工程下获得最高准确性。无推理能力的模型（如GPT-4）需要精心设计的提示。LLM在简单故障定位中表现良好，但随着问题难度增加准确性下降。过度推理和计算成本是主要挑战。

Conclusion: LLM在提高调试效率方面具有潜力，特别适合新手程序员辅助。但需要在推理能力和计算效率方面进一步改进才能实现实际应用。

Abstract: Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.

</details>


### [24] [Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate](https://arxiv.org/abs/2512.03815)
*Shayan Ghasemnezhad,Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith,Heanh Sok*

Main category: cs.SE

TL;DR: Causify Dev系统提出了一种介于monorepo和multi-repo之间的混合方法，通过可运行目录实现模块化、一致性和可扩展性


<details>
  <summary>Details</summary>
Motivation: 现代软件系统对传统代码库组织策略提出了挑战：monorepo提供一致性但存在可扩展性问题，multi-repo提供模块化但面临协调和依赖管理困难。需要找到两者之间的平衡方案。

Method: 提出Causify Dev系统，核心概念是"可运行目录"——自包含、独立可执行的单元，具有自己的开发、测试和部署生命周期。通过统一轻量级环境、共享辅助工具和基于Docker的容器化工作流程支持。

Result: 可运行目录实现了一致性设置、隔离依赖和高效的CI/CD流程。Causify Dev方法在monorepo和multi-repo策略之间提供了实用的中间地带。

Conclusion: Causify Dev方法改善了不断增长、复杂代码库的可靠性和可维护性，为软件系统组织提供了更好的平衡方案。

Abstract: Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.

</details>


### [25] [A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software](https://arxiv.org/abs/2512.03868)
*Shree Hari Bittugondanahalli Indra Kumar,Lilia Rodrigues Sampaio,André Martin,Andrey Brito,Christof Fetzer*

Main category: cs.SE

TL;DR: 研究开源项目中依赖漏洞的严重性、持久性和分布情况，发现关键漏洞平均需要超过一年才能修复，且大多数漏洞来自传递依赖。


<details>
  <summary>Details</summary>
Motivation: 开源库广泛使用但可能引入安全漏洞（如Log4Shell），随着开源库使用增加，理解并解决这些依赖漏洞变得至关重要。SCA工具能帮助分析项目依赖，但需要更全面的研究来了解漏洞的普遍性和修复速度。

Method: 使用VODA SCA工具爬取2013-2023年间超过1000个开源项目（约5万个版本），涵盖Java、Python、Rust、Go、Ruby、PHP和JavaScript等多种语言。分析库版本、依赖深度、已知漏洞及其在软件开发周期中的演变。

Result: 数据集比先前研究更大更多样化，提供了更好的洞察力和结果泛化性。发现大多数编程语言中，漏洞依赖是传递性的，关键漏洞平均需要超过一年才能修复。还研究了漏洞与项目指标（团队规模、贡献者数量、活动频率和发布周期）的关联。

Conclusion: 开源项目中的依赖漏洞问题严重且持久，特别是传递依赖带来的安全风险。需要更有效的漏洞修复机制和依赖管理策略来改善软件供应链安全。

Abstract: Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.

</details>


### [26] [Tunable Automation in Automated Program Verification](https://arxiv.org/abs/2512.03926)
*Alexander Y. Bai,Chris Hawblitzel,Andrea Lattuada*

Main category: cs.SE

TL;DR: 该论文提出了一种细粒度控制量化事实可用性的机制，允许开发者在SMT验证工具中根据上下文选择不同的自动化级别，平衡自动化程度与验证性能。


<details>
  <summary>Details</summary>
Motivation: 基于SMT求解器的自动化验证工具在处理量化实例化时面临自动化与性能之间的根本性矛盾：激进的量化实例化提供更多自动化但验证时间长，保守的实例化响应快但需要更多手动证明提示。

Method: 提出了一种机制，能够在验证上下文中对量化事实的可用性进行细粒度控制，允许开发者选择性调整自动化级别。库作者可以提供不同的预定义自动化级别，同时让最终用户能够在模块、函数或证明上下文级别进一步自定义量化可用性。

Result: 在基于Rust的验证工具Verus中实现了该技术，并在多个公开可用的代码库上进行了评估。实证分析展示了自动化与性能的权衡，并证明选择性量化管理使开发者能够在不同上下文中选择适当的自动化级别。

Conclusion: 通过提供细粒度的量化事实控制机制，该研究解决了SMT验证工具中自动化与性能的权衡问题，使开发者能够根据具体验证上下文灵活选择适当的自动化策略。

Abstract: Automated verification tools based on SMT solvers have made significant progress in verifying complex software systems. However, these tools face a fundamental tension between automation and performance when dealing with quantifier instantiation -- the primary source of incompleteness and verification slowdown in SMT-based verifiers. Tools choose between aggressive quantifier instantiation that provides more automation but longer verification times, or conservative instantiation that responds quickly but may require more manual proof hints.
  We present a mechanism that enables fine-grained control over the availability of quantified facts in verification contexts, allowing developers to selectively tune the level of automation. Our approach lets library authors provide different pre-defined automation levels while giving end-users the ability to further customize quantifier availability at the module, function, or proof context level.
  We implement our techniques in Verus, a Rust-based verification tool, and evaluate them on multiple openly available codebases. Our empirical analysis demonstrates the automation-performance tradeoff and that selective quantifier management enables developers to select the appropriate level of automation in different contexts.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [27] [Mobility Induced Sensitivity of UAV based Nodes to Jamming in Private 5G Airfield Networks An Experimental Study](https://arxiv.org/abs/2512.03536)
*Pavlo Mykytyn,Ronald Chitauro,Onur Yener,Peter Langendoerfer*

Main category: cs.NI

TL;DR: 该研究通过实验评估了在定向SDR干扰攻击下，5G机场专用网络中无人机UE节点的性能表现，分析了移动性对信号质量和网络稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 评估5G机场专用网络在定向干扰攻击下的鲁棒性，特别是针对无人机用户设备（UE）节点，了解移动性参数如何影响网络性能和安全稳定性。

Method: 使用搭载QualiPoc Android UE的四旋翼无人机，在受控的定向SDR干扰环境下进行实验，测量不同飞行速度、高度和移动模式下的物理层和网络层指标。

Result: 实验记录了CQI、MCS、RSRP、SINR、BLER、Net PDSCH吞吐量和RLF等关键指标，揭示了无人机UE节点移动性水平对链路稳定性和信号退化的依赖关系。

Conclusion: 在5G机场专用网络中，无人机UE节点的自主和自动操作期间的移动性水平会显著影响链路稳定性和信号质量，这对网络设计和安全防护具有重要意义。

Abstract: This work presents an experimental performance evaluation of a private 5G airfield network under controlled directional SDR jamming attacks targeting UAV-based UE nodes. Using a QualiPoc Android UE, mounted as a payload on a quadcopter UAV, we conducted a series of experiments to evaluate signal degradation, handover performance, and ser-vice stability in the presence of constant directional jamming. The conducted experiments aimed to examine the effects of varying travel speeds, altitudes, and moving patterns of a UAV-based UE to record and analyze the key physical-layer and network-layer metrics such as CQI, MCS, RSRP, SINR, BLER, Net PDSCH Throughput and RLF. The re-sults of this work describe the link stability and signal degradation dependencies, caused by the level of mobility of the UAV-based UE nodes during autonomous and automatic operation in private 5G Airfield networks

</details>


### [28] [Performance Evaluation of Parallel Wi-Fi Redundancy with Deferral Techniques](https://arxiv.org/abs/2512.03569)
*Gianluca Cena,Pietro Chiavassa,Stefano Scanzio*

Main category: cs.NI

TL;DR: 评估Wi-Fi在工业环境中通过并行冗余技术提升实时控制可靠性的可行性


<details>
  <summary>Details</summary>
Motivation: Wi-Fi在工业环境中应用广泛，但传统上被认为不适合实时控制系统。需要评估如何通过技术改进提升其可靠性以满足实时控制需求。

Method: 采用并行冗余技术，特别是延迟并行冗余，通过真实环境采集的大数据集进行定量分析，评估多项与软实时应用相关的性能指标。

Result: 延迟并行冗余在有限频谱消耗成本下，显著改善了最坏情况传输延迟，为无线连接在控制回路中的应用提供了实际可行性。

Conclusion: 并行冗余技术可以有效提升Wi-Fi在工业实时控制系统中的可靠性，特别是在控制回路包含无线连接时具有实用价值。

Abstract: Wireless communication is increasingly used in industrial environments, since it supports mobility of interconnected devices. Among the transmission technologies operating in unlicensed bands available to this purpose, Wi-Fi is certainly one of the most interesting, because of its high performance and the relatively low deployment costs. Unfortunately, its dependability is often deemed unsuitable for real-time control systems. In this paper, the use of parallel redundancy is evaluated from a quantitative viewpoint, by considering a number of performance indices that are relevant for soft real-time applications. Analysis is carried out on a large dataset acquired from a real setup, to provide realistic insights on the advantages this kind of approaches can provide. As will be seen, deferred parallel redundancy provides clear advantages in terms of the worst-case transmission latency, at limited costs concerning the amount of consumed spectrum. Hence, it can be practically exploited every time a wireless connection is included in a control loop.

</details>


### [29] [Machine Learning to Predict Slot Usage in TSCH Wireless Sensor Networks](https://arxiv.org/abs/2512.03570)
*Stefano Scanzio,Gabriele Formis,Tullio Facchinetti,Gianluca Cena*

Main category: cs.NI

TL;DR: 该论文提出使用机器学习学习TSCH协议网络的流量模式，通过预测无传输时段让节点进入深度睡眠状态，从而显著提高无线传感器网络的能效。


<details>
  <summary>Details</summary>
Motivation: 工业无线传感器网络需要超低功耗和确定性操作。TSCH协议能满足这两个条件，但仍有进一步节能的空间。通过机器学习预测流量模式，可以在无传输时段让节点进入深度睡眠，进一步提高能效。

Method: 使用机器学习模型学习基于TSCH协议的流量模式，分析不同网络层级（典型树形拓扑）的预测能力。在模拟数据上应用这些算法，基于精确的无线传感器节点建模进行评估。

Result: 机器学习模型在不同网络层级表现出良好的预测能力，但随着接近树形拓扑的根节点，预测能力会下降。模拟结果表明，所研究的算法能够显著降低TSCH网络的功耗。

Conclusion: 机器学习可以有效地学习TSCH网络的流量模式，通过预测无传输时段让节点进入深度睡眠状态，从而大幅提高无线传感器网络的能源效率，特别是在工业应用中。

Abstract: Wireless sensor networks (WSNs) are employed across a wide range of industrial applications where ultra-low power consumption is a critical prerequisite. At the same time, these systems must maintain a certain level of determinism to ensure reliable and predictable operation. In this view, time slotted channel hopping (TSCH) is a communication technology that meets both conditions, making it an attractive option for its usage in industrial WSNs. This work proposes the use of machine learning to learn the traffic pattern generated in networks based on the TSCH protocol, in order to turn nodes into a deep sleep state when no transmission is planned and thus to improve the energy efficiency of the WSN. The ability of machine learning models to make good predictions at different network levels in a typical tree network topology was analyzed in depth, showing how their capabilities degrade while approaching the root of the tree. The application of these models on simulated data based on an accurate modeling of wireless sensor nodes indicates that the investigated algorithms can be suitably used to further and substantially reduce the power consumption of a TSCH network.

</details>


### [30] [Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks](https://arxiv.org/abs/2512.03722)
*Lingyi Cai,Wenjie Fu,Yuxi Huang,Ruichen Zhang,Yinqiu Liu,Jiawen Kang,Zehui Xiong,Tao Jiang,Dusit Niyato,Xianbin Wang,Shiwen Mao,Xuemin Shen*

Main category: cs.NI

TL;DR: 本文是关于LLM增强RL在无线网络中的综合教程，提出了LLM在RL中的四种关键角色分类，并探讨了其在无线网络应用中的具体案例和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在无线网络动态环境中存在泛化能力差、学习反馈有限、可解释性不足和样本效率低等局限性，而大语言模型在知识泛化、上下文推理和交互生成方面具有卓越能力，有望增强传统RL的性能。

Method: 提出一个分类法将LLM在RL中的角色分为四种关键功能：状态感知器、奖励设计器、决策制定器和生成器，并回顾现有研究探索LLM如何增强RL管道的各个阶段。

Result: 通过案例研究展示了如何在低空经济网络、车载网络和空天地一体化网络中设计和应用LLM增强的RL，为无线网络优化提供了新的解决方案。

Conclusion: LLM增强的RL为无线网络优化提供了有前景的方向，未来需要进一步探索其在无线网络中的发展和应用潜力。

Abstract: Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 提出一个结合物理知识与机器学习的计算框架，用于开发钢材的物理信息连续冷却转变（CCT）模型，该模型能高效生成CCT图并准确预测相变温度和相分类。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在材料科学中已用于新化合物发现和制造过程优化，但将其应用于钢铁等复杂工业材料仍具挑战性。主要障碍在于难以准确捕捉化学成分、加工参数与微观结构/性能之间的复杂关系。

Method: 开发了一个结合物理洞察与机器学习的计算框架，构建了物理信息连续冷却转变（CCT）模型。该模型在4,100个CCT图的数据集上进行训练，并采用文献和实验数据进行验证。

Result: 模型计算效率高，能在5秒内生成包含100条冷却曲线的完整CCT图。在合金钢中表现出强泛化能力：所有相的相分类F1分数均超过88%；相变温度回归中，除贝氏体（MAE为27°C）外，所有相的MAE均低于20°C。

Conclusion: 该框架可通过添加通用和定制ML模型扩展为通用的热处理数字孪生平台。与补充模拟工具和针对性实验集成，将进一步支持加速材料设计工作流程。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [32] [Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation](https://arxiv.org/abs/2512.03053)
*Andrew S. Cassidy,Guillaume Garreau,Jay Sivagnaname,Mike Grassi,Bernard Brezzo,John V. Arthur,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 提出一种利用LLM作为无损编码器/解码器的方法，通过源域到目标域的转换和反向重构来检测LLM生成的错误，显著提高硬件设计生产力


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码生成中存在的幻觉和遗漏问题，特别是在硬件设计领域，需要确保生成的代码逻辑正确性

Method: 使用LLM作为无损编码器将源域数据（如逻辑条件表）转换为目标域数据（如HDL代码），再作为无损解码器重构回源域数据，通过比较原始和重构数据来验证正确性

Result: 成功生成二维网络芯片路由器的完整HDL代码（13个单元，1500-2000行代码），通过七种不同LLM验证，能够检测错误生成的逻辑并帮助开发者发现设计规范错误

Conclusion: 该方法能有效缓解LLM的幻觉和遗漏问题，显著提高硬件设计生产力，不仅确认正确生成的逻辑，还能检测错误并辅助发现设计规范问题

Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.

</details>


### [33] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: 提出一种面向绿色AI的自适应层冻结策略，在联邦学习中减少能耗和计算负载，同时保持MRI到CT转换的性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能促进医疗平等，但其高资源需求会排除计算基础设施有限的机构，加剧医疗不平等。需要开发节能的联邦学习方法

Method: 提出自适应层冻结策略：基于轮次间编码器权重的相对差异选择性冻结编码器权重，采用基于耐心的机制确保仅在更新持续最小时才冻结。使用CodeCarbon库跟踪能耗和碳排放

Result: 相比非冻结方法，训练时间、总能耗和CO2eq排放减少达23%。MRI-to-CT转换性能基本保持，MAE仅有小变化。5种架构中3种无统计显著差异，2种有统计显著改进

Conclusion: 该工作符合促进临床需求与气候、社会、经济可持续性并重的研究范式，为新的联邦学习评估框架奠定基础，推动AI驱动医疗中的隐私、公平和正义

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [34] [Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins](https://arxiv.org/abs/2512.03055)
*Xiaowu Sun,Thabo Mahendiran,Ortal Senouf,Denise Auberson,Bernard De Bruyne,Stephane Fournier,Olivier Muller,Pascal Frossard,Emmanuel Abbe,Dorina Thanou*

Main category: cs.LG

TL;DR: PINS-CAD：基于物理信息自监督学习的框架，通过20万合成冠状动脉数字孪生预训练图神经网络预测血流动力学，无需CFD或标注数据，在临床数据微调后能预测心血管事件并生成可解释生物标志物。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，冠状动脉疾病（CAD）是最常见形式，需要早期风险预测。传统3D冠状动脉数字孪生分析依赖计算流体动力学（CFD），计算量大难以扩展；数据驱动方法受限于标注数据稀缺且缺乏生理学先验知识。

Method: 提出PINS-CAD物理信息自监督学习框架：1）使用20万合成冠状动脉数字孪生预训练图神经网络；2）通过1D Navier-Stokes方程和压力降定律指导预测压力和血流；3）无需CFD或标注数据；4）在FAME2研究635名患者临床数据上微调。

Result: 1）预测未来心血管事件的AUC达0.73，优于临床风险评分和数据驱动基线；2）物理信息预训练提升样本效率并产生生理学有意义的表示；3）生成空间分辨的压力和血流储备分数曲线，提供可解释生物标志物。

Conclusion: PINS-CAD通过将物理先验嵌入几何深度学习，将常规血管造影转化为无需模拟、具有生理感知的框架，实现可扩展的预防性心脏病学。

Abstract: Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.

</details>


### [35] [A Multi-Agent, Policy-Gradient approach to Network Routing](https://arxiv.org/abs/2512.03211)
*Nigel Tao,Jonathan Baxter,Lex Weaver*

Main category: cs.LG

TL;DR: OLPOMDP算法成功应用于模拟网络路由，多个分布式路由器代理在无显式通信情况下学习协作行为，避免个体最优但群体有害的行为，通过奖励塑形显著提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 网络路由是一个分布式决策问题，具有自然的数值性能度量（如数据包从源到目的地的平均时间）。需要探索如何在无显式通信的情况下让多个分布式代理（路由器）学习协作行为，避免个体最优但群体有害的行为。

Method: 采用OLPOMDP（一种策略梯度强化学习算法）应用于模拟网络路由，通过奖励塑形（显式惩罚某些次优行为模式）来改进学习过程。

Result: OLPOMDP在多种网络模型下成功应用于模拟网络路由，多个分布式代理学会了协作行为，避免了有害的个体行为，且奖励塑形显著提高了收敛速度。

Conclusion: OLPOMDP算法能够有效解决网络路由中的分布式协作问题，奖励塑形是提升学习效率的关键技术，证明了强化学习在网络路由优化中的潜力。

Abstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.

</details>


### [36] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: Delta Sampling (DS) 是一种无需训练数据、在推理时实现不同架构基础模型间知识迁移的方法，通过利用模型预测差异来指导新基础模型的去噪过程。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生态系统中的适配组件（如LoRA、ControlNet等）与特定基础模型紧密耦合，当基础模型升级（如从SD 1.x到2.x）时，由于参数和架构的显著变化，这些适配组件难以重用，需要重新训练。

Method: Delta Sampling (DS) 在推理时操作，通过计算基础模型在适配前后的预测差异（delta），然后将这个delta用于指导新基础模型的去噪过程，实现知识迁移，无需访问原始训练数据。

Result: 在不同版本的Stable Diffusion上评估显示，DS在各种采样策略下都能一致地改善期望效果（如视觉风格、语义概念和结构）的创建，证明其作为即插即用知识迁移机制的有效性。

Conclusion: DS为扩散模型生态系统提供了一种有效的、无需重新训练的跨架构知识迁移方法，解决了适配组件与特定基础模型耦合的问题，促进了模型升级时的知识重用。

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [37] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR: 研究预训练Transformer中token的动态特性，分析其连续时间极限的动力学系统，提出改进Transformer架构的简单方法


<details>
  <summary>Details</summary>
Motivation: 探索预训练Transformer模型中token的动态特性，理解token随时间相互靠近或远离的行为模式，以及这些行为如何影响模型性能

Method: 分析预训练模型的连续时间极限动力学系统，研究解的渐近行为，基于模型参数提供token收敛到零或发散到无穷的充分条件，并研究绝对位置编码和旋转位置编码对动态机制的影响

Result: 发现收敛场景对模型性能有负面影响，提出了针对绝对位置编码和旋转位置编码的简单Transformer架构改进方法，以减轻收敛行为

Conclusion: 研究为改进Transformer模型提供了理论基础和设计原则，提出的简单架构改进能有效缓解收敛行为对性能的负面影响

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [38] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出一种安全的分层深度强化学习框架，用于解决多源不确定性下的电动公交车充电调度问题，通过双重行动者-评论家多智能体近端策略优化拉格朗日算法实现成本最小化和安全运行。


<details>
  <summary>Details</summary>
Motivation: 电动公交车与光伏等可再生能源整合是实现低碳公共交通的重要途径，但在实际运行中面临光伏发电不确定性、动态电价、可变行驶时间和有限充电设施等多源不确定性，需要优化充电调度以最小化运营成本同时确保电池不耗尽的安全运行。

Method: 将问题建模为带约束的马尔可夫决策过程，提出DAC-MAPPO-Lagrangian分层深度强化学习算法：高层采用集中式PPO-Lagrangian学习安全充电桩分配策略，低层采用MAPPO-Lagrangian在集中训练分散执行范式下学习分散式充电功率决策。

Result: 基于真实数据的实验表明，所提方法在成本最小化和安全合规性方面优于现有基线方法，同时保持了快速的收敛速度。

Conclusion: 提出的安全分层深度强化学习框架能有效解决多源不确定性下的电动公交车充电调度问题，为可持续低碳公共交通提供了实用的优化解决方案。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [39] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 大规模工业框架利用数百个实验数据估计异质性处理效应，发现潜在用户特征并稳定估计处理效应，应用于广告影响力和敏感性，在线测试显示效果显著提升


<details>
  <summary>Details</summary>
Motivation: 传统方法假设处理效应对所有用户相同，但实际存在异质性。需要大规模框架从海量实验数据中估计异质性处理效应，发现之前无法测量的潜在用户特征

Method: 结合数百个实验数据，构建大规模工业框架，包括实验选择、基础学习器设计和增量训练等核心组件

Result: 框架能够发现潜在用户特征，产生稳定的处理效应估计。应用于用户对广告的影响力和敏感性，在线A/B测试显示使用影响力分数进行定向投放，关键业务指标提升超过通常认为显著水平的6倍

Conclusion: 该大规模框架成功估计异质性处理效应，发现新用户特征，并在实际应用中显著提升广告效果，验证了异质性处理效应模型在工业场景中的价值

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [40] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 提出两种基于物理启发的SVD LLM压缩改进方法：FermiGrad（通过费米函数将离散奇异值截断松弛为连续优化，确定全局最优层秩）和PivGa（利用参数化中的规范自由度对低秩因子进行无损压缩）


<details>
  <summary>Details</summary>
Motivation: 大语言模型对计算资源需求很高，低秩分解（如SVD）是LLM压缩的有前景方法，但存在层秩选择和参数冗余等实际障碍

Method: 1. FermiGrad：使用费米函数将离散奇异值截断松弛为连续优化问题，通过梯度下降确定全局最优层秩；2. PivGa：利用低秩因子参数化中的规范自由度，对低秩因子进行额外的无损压缩

Result: 论文提出了两种改进SVD LLM压缩的物理启发方法，解决了层秩选择和参数冗余问题

Conclusion: 通过FermiGrad和PivGa两种方法，改进了基于SVD的LLM压缩技术，使其更实用有效

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [41] [Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors](https://arxiv.org/abs/2512.03287)
*Dario Fenoglio,Mohan Li,Davide Casnici,Matias Laporte,Shkurta Gashi,Silvia Santini,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 提出多频联邦学习用于头戴设备的人体活动识别，解决隐私问题和设备采样频率差异问题


<details>
  <summary>Details</summary>
Motivation: 传统人体活动识别需要集中式用户数据，存在隐私风险；不同设备采样频率不同，需要跨设备联合学习

Method: 采用多频联邦学习方法，支持隐私保护的机器学习，允许不同采样频率的设备共同学习模型

Result: 在两个数据集上相比频率特定方法有改进，展示了多频联邦学习在人体活动识别任务中的潜力

Conclusion: 多频联邦学习为头戴设备的人体活动识别提供了隐私保护和跨设备学习的有效解决方案，代码已开源

Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.

</details>


### [42] [Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning](https://arxiv.org/abs/2512.03065)
*Nihir Chadderwala*

Main category: cs.LG

TL;DR: 提出结合AWS Strands Agents与Thompson Sampling上下文多臂老虎机的框架，让AI代理通过用户反馈学习最优决策策略，在生命科学领域提升15-30%用户满意度


<details>
  <summary>Details</summary>
Motivation: 生命科学领域的生成式AI代理面临关键挑战：如何为从简单事实问题到复杂机制推理的多样化查询确定最优方法。传统方法依赖固定规则或昂贵的标注训练数据，都无法适应变化条件或用户偏好。

Method: 结合AWS Strands Agents与Thompson Sampling上下文多臂老虎机，让AI代理仅从用户反馈中学习最优决策策略。系统优化三个关键维度：生成策略选择（直接生成vs思维链）、工具选择（文献搜索、药物数据库等）和领域路由（药理学、分子生物学、临床专家）。

Result: 在生命科学查询的实证评估中，相比随机基线实现了15-30%的用户满意度提升，在20-30个查询后出现清晰的学习模式。

Conclusion: 该方法无需真实标签，能持续适应用户偏好，为代理AI系统中的探索-利用困境提供了原则性解决方案。

Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.

</details>


### [43] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 使用预拓扑学建模能源消费档案，开发多准则分层分类算法，通过Python库实现，用于优化大规模分布式建筑能耗管理。


<details>
  <summary>Details</summary>
Motivation: 对数千栋建筑进行逐案深度审计需要大量时间、资金和专业人员，因此需要开发自动化方法来建立有效的能耗管理推荐系统。

Method: 使用预拓扑学建模站点消费档案，开发基于预拓扑空间特性的多准则分层分类算法，并实现为Python库。

Result: 在2D空间点数据集上能识别基于位置和大小的聚类；在生成的时间序列数据集上使用皮尔逊相关系数获得ARI=1的完美聚类效果；在400个真实能耗站点的消费时间序列上验证。

Conclusion: 预拓扑学和多准则分层分类算法为大规模分布式建筑能耗管理提供了有效的自动化解决方案，能够识别消费模式聚类并支持优化管理决策。

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [44] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 提出一种基于预拓扑空间的混合数据聚类方法，用于处理大数据环境下的异构数据聚类挑战


<details>
  <summary>Details</summary>
Motivation: 大数据时代带来了数据量、速度和多样性的挑战，传统聚类方法主要针对同质数据集，难以有效处理包含数值和分类变量的混合数据，需要专门的方法来应对这种复杂性

Method: 基于预拓扑空间的聚类方法，该方法能够处理混合数据类型，并提供层次化和可解释的聚类结果

Result: 通过与经典数值聚类算法和现有预拓扑方法进行基准测试，评估了所提方法的性能和有效性

Conclusion: 预拓扑空间方法为大数据环境下的混合数据聚类提供了有效的解决方案，能够生成结构化和可解释的聚类结果，支持决策制定

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [45] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 提出基于预拓扑学的新聚类算法，无需降维处理混合数据，使用析取范式构建可定制逻辑规则，实现可解释的层次聚类。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理混合数据时通常需要降维，这会损失数据完整性。现有方法在聚类可解释性方面存在不足，需要一种能直接处理原始混合数据且能提供清晰解释的聚类方法。

Method: 基于预拓扑学框架，利用析取范式构建可定制的逻辑规则和可调超参数，支持用户定义的层次聚类结构，直接从原始数据构建聚类而不需要降维。

Result: 通过层次树状图分析和聚类指标比较，该方法在混合数据聚类中表现出优越性能，能准确且可解释地划分聚类，保持数据完整性，并展现出良好的鲁棒性。

Conclusion: 该算法通过避免传统降维技术并创新性地使用逻辑规则，显著提升了混合数据聚类的可解释性和性能，为聚类可解释性研究做出了重要贡献。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [46] [Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information](https://arxiv.org/abs/2512.03074)
*Mahdi Tavassoli Kejani,Fadi Dornaika,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一种模型无关的公平性正则化框架，用于处理图神经网络中敏感属性部分可用的现实场景，在保持分类性能的同时显著降低偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平性GNN方法假设敏感属性在训练时完全可用，但这在实际中因隐私和数据收集限制难以实现。需要解决敏感属性部分可用场景下的公平性问题。

Method: 提出模型无关的公平性正则化框架，将机会均等和统计奇偶性作为可微正则化项整合到目标函数中，适用于敏感属性部分可用的现实场景。

Result: 在五个真实世界基准数据集上的实验表明，该方法在关键公平性指标上显著降低偏见，同时保持竞争力的节点分类性能，在公平性-准确性权衡方面优于基线模型。

Conclusion: 该框架有效解决了敏感属性部分可用场景下的GNN公平性问题，实现了良好的公平性-准确性权衡，为实际应用提供了实用解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.

</details>


### [47] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 本文提出将倾斜风险（tilted risk）应用于流匹配（Flow Matching），通过log-exponential变换改进标准均方误差损失，以更好地捕捉数据流形中的高阶统计信息和几何结构。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配（FM）使用均方误差损失训练速度场，这会将到达同一时空点的所有速度目标压缩为单一条件均值，忽略了高阶条件信息（方差、偏度、多模态）。这些高阶信息编码了数据流形的精细几何结构和少数分支，对于理解数据分布至关重要。

Method: 将标准的风险敏感（log-exponential）变换应用于条件FM损失，得到倾斜风险损失。该损失是每个时空点上有意义的条件熵FM目标的上界。通过对此条件熵目标的梯度进行小阶展开，得到两个可解释的一阶修正项：FM残差的协方差预处理，以及偏好不对称或稀有分支的偏尾项。

Result: 在专门设计用于探测模糊性和尾部的合成数据上，风险敏感损失在统计指标上优于标准整流FM，并能更忠实地恢复几何结构。

Conclusion: 倾斜风险损失为流匹配提供了一种改进的优化框架，能够更好地捕捉数据分布的高阶统计特性和几何结构，特别是在处理模糊性和稀有事件时表现出优势。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [48] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: ALARM是一个基于多模态大语言模型的视觉异常检测框架，集成了不确定性量化、推理链、自我反思和模型集成等技术，在复杂环境中实现可靠决策。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，视觉异常通常是高度上下文相关且模糊的，因此不确定性量化成为MLLM-based VAD系统成功的关键能力。

Method: ALARM框架将不确定性量化与质量保证技术（推理链、自我反思、MLLM集成）相结合，基于严格的概率推理流程和计算过程设计。

Result: 在真实世界智能家居基准数据和伤口图像分类数据上的广泛实证评估显示，ALARM具有优越性能，并在不同领域具有通用适用性。

Conclusion: ALARM框架通过集成不确定性量化与多种质量保证技术，为复杂环境中的视觉异常检测提供了可靠决策支持，具有跨领域的通用应用潜力。

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [49] [Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration](https://arxiv.org/abs/2512.03102)
*Yiwei Shi,Hongnan Ma,Mengyue Yang,Cunjia Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 提出扩散驱动的贝叶斯探索框架，解决早期状态估计错误导致的"后验支持不变性"问题，在危险气体定位任务中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 在应急响应等高风险应用中，基于有限或偏差信息的早期状态估计可能与现实严重不符，导致灾难性后果。传统自举粒子滤波器存在"平稳性诱导的后验支持不变性"问题，使得初始先验排除的区域永远无法探索，即使新证据与当前信念矛盾也无法修正

Method: 提出扩散驱动的贝叶斯探索框架，通过熵正则化采样和协方差缩放扩散扩展后验支持，使用Metropolis-Hastings检查验证提议并保持推理对意外证据的自适应性

Result: 在危险气体定位任务中，当先验正确时与强化学习和规划基线相当；在先验失准时显著优于传统SMC扰动和基于RL的方法，并提供理论保证证明DEPF能解决S-PSI问题同时保持统计严谨性

Conclusion: 提出的扩散驱动贝叶斯探索框架能够原则性地实时修正早期状态估计错误，解决后验支持不变性问题，在高风险应用中具有重要价值

Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.

</details>


### [50] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: ECLIPSE框架通过结合语义熵估计和困惑度分解来检测LLM幻觉，将幻觉视为模型语义熵与可用证据容量之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会产生流畅但无根据的回答（幻觉），这限制了其在高风险领域的部署。需要一种机制来检测这些幻觉，确保回答的可靠性。

Method: 提出ECLIPSE框架：1) 通过多样本聚类进行熵估计；2) 新颖的困惑度分解，测量模型如何使用检索到的证据；3) 证明在温和条件下，熵-容量目标是严格凸的，具有唯一稳定最优解。

Result: 在受控金融问答数据集上，ECLIPSE达到ROC AUC 0.89和平均精度0.90，显著优于仅使用语义熵的基线（AUC 0.50）。当使用缺乏token级对数概率的Claude-3-Haiku时，AUC降至0.59，系数幅度下降95%，表明ECLIPSE依赖于校准的token级不确定性。

Conclusion: ECLIPSE是一个有效的幻觉检测机制，其效果依赖于token级对数概率。困惑度分解特征具有最大的学习系数，证实证据利用是幻觉检测的核心。该研究是受控机制研究，跨领域和自然发生幻觉的验证是未来工作。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [51] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: e-valuator：将任意黑盒验证器分数转换为具有可证明误报率控制的决策规则的方法，用于在线监控AI代理轨迹


<details>
  <summary>Details</summary>
Motivation: 当前AI代理验证器（如LLM评判器和过程奖励模型）的启发式评分缺乏正确性保证，无法可靠判断代理轨迹是否成功，需要统计上有效的决策方法

Method: 将成功与失败轨迹的区分问题构建为顺序假设检验问题，基于e-过程工具开发顺序假设检验，在代理轨迹的每一步都保持统计有效性

Result: 在六个数据集和三个代理上的实验表明，e-valuator相比其他策略具有更高的统计功效和更好的误报率控制，还能快速终止问题轨迹以节省token

Conclusion: e-valuator提供了一个轻量级、模型无关的框架，将验证器启发式方法转换为具有统计保证的决策规则，使能更可靠的代理系统部署

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [52] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: SISR框架同时解决Shapley值的两个核心问题：通过单调变换恢复可加性，并通过L0稀疏约束实现高效高维解释。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值面临两个主要挑战：1) 现实世界中的收益函数往往违反可加性假设（非高斯分布、重尾、特征依赖等），导致归因失真；2) 在高维场景中，先计算密集Shapley值再阈值化的方法计算成本过高且可能不一致。

Method: 提出Sparse Isotonic Shapley Regression (SISR)框架，同时学习单调变换来恢复可加性，并施加L0稀疏约束。优化算法使用Pool-Adjacent-Violators进行高效等渗回归，以及归一化硬阈值进行支持选择。

Result: SISR能在多种场景下恢复真实变换，在高噪声下仍能实现强支持恢复。实验表明，SISR能稳定不同收益方案下的归因，正确过滤无关特征，而标准Shapley值则出现严重的排序和符号失真。

Conclusion: SISR通过统一非线性变换估计和稀疏性追求，推进了非线性可解释性的前沿，提供了一个理论上有依据且实用的归因框架。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [53] [Temporal Graph Neural Networks for Early Anomaly Detection and Performance Prediction via PV System Monitoring Data](https://arxiv.org/abs/2512.03114)
*Srijani Mukherjee,Laurent Vuillon,Liliane Bou Nassif,Stéphanie Giroux-Julien,Hervé Pabiou,Denys Dutykh,Ionnasis Tsanakas*

Main category: cs.LG

TL;DR: 提出一种基于时序图神经网络的方法，利用环境参数预测光伏系统输出功率并检测异常


<details>
  <summary>Details</summary>
Motivation: 随着光伏系统快速增长，需要先进的性能监控和异常检测方法来确保系统最优运行

Method: 使用时序图神经网络，基于关键参数（辐照度、模块温度、环境温度）之间的图结构时序关系来预测电力输出

Result: 基于法国里昂屋顶光伏设施的实际数据（包括功率测量和气象参数）进行验证

Conclusion: 该方法为光伏系统性能监控和异常检测提供了新颖有效的解决方案

Abstract: The rapid growth of solar photovoltaic (PV) systems necessitates advanced methods for performance monitoring and anomaly detection to ensure optimal operation. In this study, we propose a novel approach leveraging Temporal Graph Neural Network (Temporal GNN) to predict solar PV output power and detect anomalies using environmental and operational parameters. The proposed model utilizes graph-based temporal relationships among key PV system parameters, including irradiance, module and ambient temperature to predict electrical power output. This study is based on data collected from an outdoor facility located on a rooftop in Lyon (France) including power measurements from a PV module and meteorological parameters.

</details>


### [54] [Real-Time Structural Health Monitoring with Bayesian Neural Networks: Distinguishing Aleatoric and Epistemic Uncertainty for Digital Twin Frameworks](https://arxiv.org/abs/2512.03115)
*Hanbin Cho,Jecheon Yu,Hyeonbin Moon,Jiyoung Yoon,Junhyeong Lee,Giyoung Kim,Jinhyoung Park,Seunghwa Ryu*

Main category: cs.LG

TL;DR: 提出结合PCA、贝叶斯神经网络和哈密顿蒙特卡洛的集成SHM框架，从稀疏应变测量重建全场应变分布并量化不确定性，在CFRP试件上验证了高精度重建和实时不确定性场生成。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测需要可靠的全场应变分析和不确定性量化，但现有方法难以同时获得空间分辨的偶然和认知不确定性，限制了可信决策制定。

Method: 集成框架结合主成分分析(PCA)、贝叶斯神经网络(BNN)和哈密顿蒙特卡洛(HMC)推理，将稀疏应变计测量映射到主要PCA模式，重建全场应变分布并量化不确定性。

Result: 在碳纤维增强聚合物(CFRP)试件的循环四点弯曲试验中验证，实现高精度应变场重建(R平方值>0.9)，同时生成实时不确定性场，能处理裂纹引起的应变奇异性。

Conclusion: 该框架通过提供互补的偶然和认知不确定性场，支持局部诊断和可靠决策，推动了结构健康监测向可信数字孪生部署和风险感知结构诊断的发展。

Abstract: Reliable real-time analysis of sensor data is essential for structural health monitoring (SHM) of high-value assets, yet a major challenge is to obtain spatially resolved full-field aleatoric and epistemic uncertainties for trustworthy decision-making. We present an integrated SHM framework that combines principal component analysis (PCA), a Bayesian neural network (BNN), and Hamiltonian Monte Carlo (HMC) inference, mapping sparse strain gauge measurements onto leading PCA modes to reconstruct full-field strain distributions with uncertainty quantification. The framework was validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with varying crack lengths, achieving accurate strain field reconstruction (R squared value > 0.9) while simultaneously producing real-time uncertainty fields. A key contribution is that the BNN yields robust full-field strain reconstructions from noisy experimental data with crack-induced strain singularities, while also providing explicit representations of two complementary uncertainty fields. Considered jointly in full-field form, the aleatoric and epistemic uncertainty fields make it possible to diagnose at a local level, whether low-confidence regions are driven by data-inherent issues or by model-related limitations, thereby supporting reliable decision-making. Collectively, the results demonstrate that the proposed framework advances SHM toward trustworthy digital twin deployment and risk-aware structural diagnostics.

</details>


### [55] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: 本文提出MoDE架构，通过模态解耦专家来解决统一多模态生成模型中的模态间遗忘问题，有效缓解梯度冲突并保持预训练能力。


<details>
  <summary>Details</summary>
Motivation: 统一多模态生成模型在持续学习新任务时存在严重的灾难性遗忘问题，包括模态内遗忘和模态间遗忘。虽然模态内遗忘已有研究，但模态间遗忘尚未被充分探索，本文首次识别并理论解释了这一现象。

Method: 提出模态解耦专家架构，通过隔离模态特定更新来缓解梯度冲突，并利用知识蒸馏防止灾难性遗忘和保持预训练能力。该方法轻量且可扩展，与之前保持模态耦合的方法不同。

Result: 在多个基准测试中，MoDE显著缓解了模态间和模态内遗忘，在统一多模态生成设置中优于之前的持续学习基线方法。

Conclusion: MoDE通过模态解耦有效解决了统一多模态生成模型中的模态间遗忘问题，为多模态持续学习提供了新的解决方案。

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [56] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: ChefNMR：基于原子扩散模型的端到端框架，仅从1D NMR谱和化学式直接预测未知分子结构，在天然产物结构解析中达到超过65%的准确率


<details>
  <summary>Details</summary>
Motivation: NMR谱解析是确定小分子结构的关键技术，但传统方法耗时且需要大量领域专业知识。特别是在天然产物和临床治疗药物发现中，自动化结构解析是一个重大挑战

Method: 将结构解析构建为条件生成问题，采用基于非等变transformer架构的原子扩散模型。创建了包含111,000多个天然产物的模拟1D NMR谱数据集

Result: ChefNMR在具有挑战性的天然产物化合物结构预测中达到了超过65%的准确率，超越了现有方法

Conclusion: 该工作在小分子结构解析自动化方面迈出了重要一步，展示了深度学习在加速分子发现方面的潜力

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [57] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: 提出基于VQ-VAE的无监督病毒变异检测框架，用于废水基因组监测，无需参考基因组或变异标签，在SARS-CoV-2废水测序数据上实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 废水基因组监测面临高测序噪声、低病毒覆盖率、片段化读取和缺乏标注变异注释等计算挑战，传统基于参考的变异检测方法难以处理新突变且计算资源需求大。

Method: 使用向量量化变分自编码器（VQ-VAE）从k-mer标记化序列中学习基因组模式的离散码本，无需参考基因组或变异标签。扩展基础VQ-VAE架构，加入掩码重建预训练以增强对缺失数据的鲁棒性，以及对比学习以获得高判别性嵌入。

Result: 在约10万条SARS-CoV-2废水测序数据上，VQ-VAE达到99.52%的平均标记级准确率和56.33%的精确序列匹配率，码本利用率为19.73%（512个码中101个活跃）。对比微调显著改善聚类效果：64维嵌入的轮廓系数提升35%（0.31到0.42），128维嵌入提升42%（0.31到0.44）。

Conclusion: 该无参考框架为基因组监测提供了可扩展、可解释的方法，可直接应用于公共卫生监测，展示了嵌入维度对变异判别能力的重要影响。

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [58] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 论文提出交错推理(IR)方法，让语言模型在思考过程中交替输出中间结果，替代传统的"先思考后回答"模式，减少用户感知延迟并允许早期干预。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型长时间思考而不给用户任何提示，导致用户无法判断推理是否正确或及时纠正错误前提，浪费用户时间且体验不佳。相比之下，人类对话中会进行轻量级、增量的确认行为来确保双方理解一致。

Method: 提出交错推理(IR)方法，模型在思考过程中交替输出中间响应；进一步提出Plantain（计划-思考-答案交错）方法，首先输出明确的逐步执行计划，然后进行思考和回答的交错。

Result: Plantain在多个数学推理和编程基准测试中实现了约6%的pass@1提升，同时将首次响应时间相对于"先思考后回答"基线减少了60%以上。

Conclusion: 交错推理方法能够显著改善用户体验，减少感知延迟，同时提高任务性能，允许用户早期干预和反馈，使语言模型更接近人类对话的交互模式。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [59] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: FiRE/FiRE.1是用于单细胞RNA测序数据中罕见细胞亚群检测的草图算法，Enhash是用于流数据概念漂移检测的高效集成学习器


<details>
  <summary>Details</summary>
Motivation: 需要快速识别大规模单细胞RNA测序数据中的罕见细胞亚群，以及高效检测流数据中的概念漂移问题

Method: FiRE/FiRE.1使用草图技术进行异常检测，Enhash采用投影哈希的集成学习方法

Result: FiRE/FiRE.1在检测罕见细胞亚群方面优于现有技术，Enhash在各种漂移类型中表现出优异的时间和精度性能

Conclusion: 提出的两种算法分别在单细胞RNA测序分析和流数据概念漂移检测中表现出色，具有实际应用价值

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [60] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出几种改进算法，用于在无限时域设置中学习带记忆的策略，并在大型POMDP问题上进行测试。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在处理部分可观测环境时受到关注，但在需要记忆的情况下效果不佳，需要改进算法来学习带记忆的策略。

Method: 开发了多种改进算法：当环境模型已知时直接学习，否则通过模拟学习，在无限时域设置中学习带记忆的策略。

Result: 在大型POMDP问题上进行测试，包括噪声机器人导航和多智能体问题，展示了算法的有效性。

Conclusion: 提出的改进算法能够有效学习带记忆的策略，扩展了策略梯度方法在部分可观测环境中的应用范围。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [61] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0生物声学基础模型在海洋哺乳动物分类任务上通过few-shot迁移学习表现出色，优于其他预训练模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Perch 2.0在训练数据中几乎不包含海洋哺乳动物音频，但研究者希望评估其在海洋哺乳动物和水下音频任务上的few-shot迁移学习能力。

Method: 使用Perch 2.0生成的嵌入进行线性探测，并与多个开源生物声学模型（包括Perch 1.0、SurfPerch、AVES-bio等）进行性能比较。

Result: Perch 2.0的嵌入在few-shot迁移学习中表现一致优异，在大多数任务上优于其他嵌入模型。

Conclusion: Perch 2.0是开发海洋哺乳动物分类线性分类器的推荐选择，特别是在标记样本有限的情况下。

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [62] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK框架通过三阶段方法：1) 生成多样解并用验证器评估，2) 用验证输出训练生成式过程奖励模型，3) 将PRM作为RL奖励信号，在数学推理任务上超越基于真实答案的方法。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型(PRMs)需要昂贵的步骤级标注或真实参考答案，限制了其应用。需要开发无需真实答案参考的RL训练方法。

Method: 三阶段框架：1) 生成器产生多样解，验证器通过并行缩放(自一致性)和顺序缩放(元批判)评估；2) 用验证输出作为合成数据训练生成式PRMs；3) 将PRM作为RL奖励信号，加入格式约束防止奖励攻击。

Result: 在ProcessBench上达到67.5 F1，优于参考指导训练的66.4和GPT-4o的61.9。在六个数学推理基准上平均准确率47.4%，优于基于真实答案的RLVR(43.9%)。

Conclusion: SPARK实现了无需真实答案参考的RL训练，性能超越基于真实答案的方法，为缺乏可验证答案或真实参考的领域开辟了新可能。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [63] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型在事实回忆任务上表现下降，主要原因是实体表示形成过晚，无法有效利用LLM已有的知识回忆机制。


<details>
  <summary>Details</summary>
Motivation: 许多视觉语言模型相比其LLM主干在事实回忆性能上有所下降，需要研究多模态微调在将LLM机制扩展到视觉输入方面的有效性。

Method: 对14个不同架构、规模和训练设置的VLM进行基准测试，使用归因修补、激活修补和探测技术分析性能差异，并尝试两种性能恢复方法。

Result: 14个模型中有11个表现出事实回忆性能下降；性能差的VLM实体表示形成过晚，无法利用LLM已有的知识回忆电路；通过修补实体表示或使用思维链提示可以恢复性能。

Conclusion: 早期实体表示形成的速度是VLM能否有效利用预训练LLM机制的关键因素，机制分析可以解释多模态对齐中的系统性失败。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [64] [BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark](https://arxiv.org/abs/2512.03280)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Matthew C. Jones,Faez Ahmed*

Main category: cs.LG

TL;DR: 提出了BlendedNet++数据集，包含12,000+混合翼体飞机几何形状的CFD模拟结果，用于气动代理模型基准测试和逆向设计研究。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习气动代理模型面临大规模、高分辨率数据集稀缺的问题，限制了点预测精度和可重复逆向设计的进展。

Method: 构建包含12,490个CFD模拟结果的数据集，提供积分系数和密集表面场数据。建立前向代理模型基准测试（6种模型架构）和基于条件扩散模型的逆向设计任务。

Result: 创建了统一的前向和逆向协议基准，包含多模型基线，支持不同架构和优化范式的公平可重复比较。

Conclusion: BlendedNet++将为场级气动学和逆向设计提供可重复研究的基础，数据集和代码将在接受后发布。

Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.

</details>


### [65] [ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics](https://arxiv.org/abs/2512.03290)
*Julian Evan Chrisnanto,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: 提出ASPEN架构，通过自适应谱层解决PINNs在刚性、多尺度非线性系统上的频谱偏差问题，成功求解复杂Ginzburg-Landau方程。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理刚性、多尺度、非线性偏微分方程时存在频谱偏差问题，无法有效表示高频分量，导致求解失败。

Method: 提出ASPEN架构，在网络输入阶段集成自适应谱层和可学习的傅里叶特征，使模型能在训练过程中动态调整谱基，高效学习所需的频率内容。

Result: ASPEN成功求解了复杂Ginzburg-Landau方程，预测解与高分辨率基准解视觉上无法区分，中位物理残差低至5.10×10^-3，正确捕捉了自由能快速弛豫和畴壁前沿长期稳定性等物理特性。

Conclusion: 通过引入自适应谱基，ASPEN为传统PINNs无法处理的复杂动力系统提供了鲁棒且物理一致的求解器，为机器学习在挑战性物理领域的应用开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.

</details>


### [66] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 该研究将自适应共形推理（ACI）与深度切换状态空间模型结合，为非平稳时间序列提供分布无关的不确定性量化，确保有限样本下的边际覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的体制转换经常破坏平稳性，使得校准的不确定性与点预测准确性同等重要。需要为体制切换预测提供分布无关的不确定性量化方法。

Method: 将深度切换状态空间模型与自适应共形推理（ACI）及其聚合变体（AgACI）耦合。引入统一的共形包装器，可应用于S4、MC-Dropout GRU、稀疏高斯过程和变点局部模型等强序列基线，在线生成预测区间。

Result: 在合成和真实数据集上，共形化预测器实现了接近名义覆盖率的性能，具有竞争力的准确性，并且通常提高了区间效率。

Conclusion: 通过结合深度切换模型与自适应共形推理，可以在非平稳性和模型误设条件下，为时间序列预测提供具有有限样本边际保证的在线预测区间，同时保持预测准确性。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [67] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: HydroDCM是一个用于跨水库流入预测的可扩展领域泛化框架，通过空间元数据构建伪域标签指导对抗学习，在推理时通过轻量级条件层进行位置特定适应。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在水库流入预测中表现良好，但应用于不同水库时性能下降，存在领域偏移问题。传统领域泛化技术难以处理水文系统中每个水库具有独特流入模式且空间元数据具有间接但重要影响的情况。

Method: 使用水库的空间元数据构建伪域标签，指导对抗学习提取不变时间特征。在推理时，通过轻量级条件层根据目标水库的元数据调整这些特征，实现领域泛化的不变性与位置特定适应的平衡。

Result: 在科罗拉多河上游流域30个真实水库上的实验表明，该方法在多域条件下显著优于最先进的领域泛化基线方法，且保持计算高效。

Conclusion: HydroDCM通过结合对抗学习和元数据条件适应，有效解决了水文系统中多域条件下的领域泛化问题，为跨水库流入预测提供了可扩展的解决方案。

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [68] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: RTFM提出了一种对抗性训练框架，通过参数化合成数据生成器来强调对模型具有挑战性的数据集，从而提升表格基础模型的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型（TFMs）主要关注设计高质量的数据生成先验来提升预训练性能，但缺乏针对模型弱点的针对性训练。作者发现参数化生成器分布可以实现对抗性鲁棒性视角，通过强调模型难以处理的数据集来提升性能。

Method: 提出RTFM（Robust Tabular Foundation Models）框架：1）引入最优性差距度量，计算TFM性能与XGBoost、CatBoost、随机森林等强基线最佳可达性能的差异；2）参数化生成器分布，在训练过程中自适应生成对模型特别具有挑战性的数据集；3）采用对抗性训练方法，使用少于10万个额外合成数据集。

Result: 在TabPFN V2分类器上应用RTFM，相比原始TabPFN和其他基线算法，平均归一化AUC提升高达6%，仅需不到10万个额外合成数据集。

Conclusion: RTFM展示了仅使用合成数据进行针对性对抗训练和微调表格基础模型的新方向，通过强调具有挑战性的数据集来提升模型鲁棒性和性能。

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [69] [Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates](https://arxiv.org/abs/2512.03309)
*Aniruddha Bora,Shixuan Zhang,Khemraj Shukla,Bryce Harrop,George Em. Karniadakis,L. Ruby Leung*

Main category: cs.LG

TL;DR: 提出基于算子学习的在线偏差校正框架，通过U-Net架构学习模型状态到偏差校正趋势的映射，在E3SM运行时进行在线校正，提高长期模拟稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化的偏差校正方法在模型自由运行时效果有限，需要一种能够在模型集成过程中持续校正偏差的方法，提高地球系统模型的预测能力。

Method: 开发了两种算子架构：Inception U-Net (IUNet)和多尺度网络(M&M)，基于U-Net骨干网络，结合多种上采样和感受野，捕捉多尺度非线性特征。使用E3SM模拟数据训练，学习从瞬时模型状态到偏差校正趋势的映射。

Result: 两种架构在离线测试中均优于标准U-Net基线；在线混合E3SM运行中，M&M在变量和垂直层次上提供最一致的偏差减少；ML增强配置在多年代模拟中保持稳定且计算可行。

Conclusion: 该框架强调长期稳定性、可移植性和更新频率限制，展示了表达性ML算子在学习和结构化跨尺度关系以及改造传统ESM方面的实用性，为可扩展混合建模提供了实用途径。

Abstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.

</details>


### [70] [Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs](https://arxiv.org/abs/2512.03324)
*Ngoc Bui,Shubham Sharma,Simran Lamba,Saumitra Mishra,Rex Ying*

Main category: cs.LG

TL;DR: TRIM-KV：一种通过学习令牌内在重要性来优化KV缓存的轻量级方法，使用保留门预测令牌保留分数，在内存受限时淘汰低分令牌，在多种长上下文任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 长序列LLM推理中，自注意力的二次计算成本和不断增长的KV缓存成为核心瓶颈。现有方法如量化、卸载或启发式KV淘汰要么协调成本高，要么依赖不可靠的注意力重要性代理。

Method: 提出TRIM-KV方法，通过轻量级保留门在令牌创建时学习其内在重要性。每个门预测一个随时间衰减的保留分数，反映令牌对特定层和头的长期效用。当内存预算超限时淘汰低分令牌，确保缓存始终包含最关键令牌。通过蒸馏冻结LLM结合容量损失进行高效训练，仅需微调门且推理开销可忽略。

Result: 在数学推理（GSM8K、MATH-500、AIME24）、程序生成（LongProc）、对话长记忆基准（LongMemEval）和长上下文理解（LongBench和SCBench）中，TRIM-KV始终优于强淘汰和可学习检索基线，尤其在低内存情况下。在某些设置中甚至超过全缓存模型，表明选择性保留可作为正则化形式，抑制无信息令牌的噪声。

Conclusion: TRIM-KV通过学习令牌保留分数有效管理KV缓存，不仅提升效率，还提供层和头特定角色的洞察，为LLM可解释性开辟新路径。保留分数与人类直觉一致，自然恢复汇令牌、滑动窗口和要点压缩等启发式方法。

Abstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.

</details>


### [71] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: SAFLe框架通过引入结构化头部和稀疏分组嵌入，实现非线性表达能力，同时保持单轮聚合优势，在联邦学习中平衡了通信效率和异构数据性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信开销高和异构数据性能下降两大挑战。现有方法存在权衡：分析式联邦学习（AFL）虽然单轮且对数据分布不变，但仅限于线性模型；非线性方法如DeepAFL恢复准确性但牺牲了单轮优势。

Method: 提出SAFLe框架，通过引入结构化头部（分桶特征）和稀疏分组嵌入实现非线性表达能力。关键创新是证明这种非线性架构在数学上等价于高维线性回归，从而可以利用AFL的单轮不变聚合定律。

Result: SAFLe在分析式联邦学习中建立了新的最先进水平，在所有基准测试中显著优于线性AFL和多轮DeepAFL的准确性，为联邦视觉提供了高效可扩展的解决方案。

Conclusion: SAFLe成功打破了传统联邦学习的权衡，通过数学等价性将非线性表达能力与单轮聚合优势相结合，为联邦学习提供了既高效又准确的新范式。

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [72] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出首个确定性广告拍卖中的离策略评估框架，通过重新利用出价景观模型来近似倾向得分，使SNIPS等稳定估计器能够用于反事实评估，在工业平台测试中达到92%的平均方向准确率。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试消耗大量工程资源且存在收入损失风险，而传统离策略评估方法在确定性拍卖环境中不适用，因为最高出价者获胜导致非获胜广告的曝光概率为零。

Method: 重新利用出价景观模型来近似倾向得分，推导稳健的近似倾向得分，从而能够使用自归一化逆倾向评分等稳定估计器进行反事实评估。

Result: 在AuctionNet模拟基准和大型工业平台2周在线A/B测试中验证，方法在CTR预测上达到92%的平均方向准确率，显著优于参数化基线。

Conclusion: 这是首个经过验证的确定性拍卖环境中可靠离策略评估的实用框架，为昂贵且有风险的在线实验提供了高效替代方案。

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [73] [A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning](https://arxiv.org/abs/2512.03363)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出A2G框架，通过几何增益和QoS增益联合调节量子联邦学习中的异构客户端聚合，提升噪声和异构条件下的性能


<details>
  <summary>Details</summary>
Motivation: 量子增强的异构经典网络中的联邦学习面临客户端质量不均、量子隐形传态保真度随机、设备不稳定以及局部与全局模型几何不匹配等挑战，传统基于欧几里得拓扑和均匀通信可靠性的聚合规则不适用于新兴量子联邦系统

Method: 提出A2G（自适应双增益聚合）框架，包含几何增益调节几何混合，QoS增益基于隐形传态保真度、延迟和不稳定性调节客户端重要性，开发A2G更新规则

Result: 在量子经典混合测试平台上实验证明，在异构和噪声条件下提高了稳定性和准确性，A2G可退化为FedAvg、QoS感知平均和流形聚合等特例

Conclusion: A2G框架有效解决了量子联邦学习中的异构和噪声挑战，为量子增强网络中的联邦学习提供了鲁棒的聚合方案

Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

</details>


### [74] [MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems](https://arxiv.org/abs/2512.03375)
*Mahdi Arab Loodaricheh,Mohammad Hossein Manshaei,Anita Raja*

Main category: cs.LG

TL;DR: MAGE-ID是一个基于扩散的多模态攻击生成框架，用于入侵检测系统数据增强，通过联合训练Transformer和CNN编码器实现表格特征与图像特征的协同生成。


<details>
  <summary>Details</summary>
Motivation: 现代入侵检测系统面临异构网络流量、不断演变的网络威胁以及良性流量与攻击流量之间严重数据不平衡的挑战。现有生成方法局限于单模态，无法捕捉跨域依赖关系。

Method: 提出MAGE-ID框架，将表格流量特征与其转换后的图像通过统一潜在先验耦合。联合训练基于Transformer和CNN的变分编码器，配合EDM风格去噪器，实现平衡且连贯的多模态合成。

Result: 在CIC-IDS-2017和NSL-KDD数据集上的评估显示，MAGE-ID在保真度、多样性和下游检测性能方面显著优于TabSyn和TabDDPM方法。

Conclusion: MAGE-ID证明了多模态数据增强在入侵检测系统中的有效性，能够生成平衡且连贯的多模态合成数据，提升检测性能。

Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.

</details>


### [75] [UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs](https://arxiv.org/abs/2512.03383)
*Hung-Yueh Chiang,Chi-Chih Chang,Yu-Chen Lu,Chien-Yu Lin,Kai-Chiang Wu,Mohamed S. Abdelfattah,Diana Marculescu*

Main category: cs.LG

TL;DR: UniQL是一个统一的边缘LLM后训练量化和低秩压缩框架，支持在设备上配置高达35%的剪枝率，实现4-5.7倍内存减少和2.7-3.4倍吞吐提升，精度损失在5%以内。


<details>
  <summary>Details</summary>
Motivation: 在移动平台上部署大语言模型面临内存有限和计算资源共享的挑战，资源可用性受设备当前工作负载影响，增加了模型部署的不确定性。

Method: 提出UniQL统一框架，集成量化和低秩压缩，支持Transformers、SSMs和混合模型。引入高效结构化权重排序方法（加速20倍）、量化感知SVD、SSM状态感知权重排序、剪枝模型的融合RoPE内核。在云端单次工作流中完成权重排序、微调和量化，支持设备端可配置剪枝率。

Result: 量化和剪枝模型实现4-5.7倍内存减少和2.7-3.4倍token吞吐提升，在15%剪枝率下精度损失控制在原始模型的5%以内，支持Transformers（Llama3、Qwen2.5）、SSMs（Mamba2）和混合模型（Nemotron-H、Bamba-v2）。

Conclusion: UniQL为边缘LLM部署提供了一个高效统一的压缩框架，显著减少内存占用并提升推理速度，同时保持模型精度，支持多种模型架构和可配置的剪枝策略。

Abstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.

</details>


### [76] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出一种基于隐式正则化的免调参框架，用于解决多测量向量（MMV）中的联合稀疏信号恢复问题，无需先验知识或参数调整。


<details>
  <summary>Details</summary>
Motivation: 传统MMV方法（如M-OMP、M-FOCUSS）需要仔细的参数调整或对信号稀疏度和噪声方差的先验知识，这在实际应用中存在局限性。

Method: 通过过参数化引入隐式正则化，将估计矩阵重新参数化为因子，分离共享行支持与个体向量条目。对标准最小二乘目标应用梯度下降，优化动态自然促进期望的行稀疏结构。

Result: 理论证明：在足够小且平衡的初始化下，优化动态呈现"动量效应"，使真实支持中的行范数增长远快于其他行，保证解轨迹收敛到理想的行稀疏解。实证结果表明性能与现有方法相当。

Conclusion: 提出的免调参框架通过隐式正则化有效解决MMV问题，无需先验信息或参数调整，为联合稀疏信号恢复提供了实用解决方案。

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [77] [VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing](https://arxiv.org/abs/2512.03394)
*Hamed Poursiami,Shay Snyder,Guojing Cong,Thomas Potok,Maryam Parsa*

Main category: cs.LG

TL;DR: VS-Graph：基于向量符号架构的图分类框架，通过尖峰扩散机制和关联消息传递，在保持高维向量空间计算的同时，实现了与GNN竞争的性能，训练速度提升高达450倍。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在图分类任务中表现出色，但计算成本高，限制了其在资源受限设备上的可扩展性和部署。超维计算（HDC）提供了轻量级的脑启发替代方案，但现有HDC方法通常难以匹配GNN的预测性能。

Method: 提出VS-Graph框架：1）尖峰扩散机制用于拓扑驱动的节点识别；2）关联消息传递方案用于在高维向量空间内进行多跳邻域聚合。无需基于梯度的优化或反向传播。

Result: 在MUTAG和DD等标准基准测试中，比先前的HDC基线性能提升4-5%，与多个现代GNN基线性能相当或更好，训练速度提升高达450倍。即使在超向量维度降至D=128时仍保持高精度。

Conclusion: VS-Graph缩小了HDC效率与消息传递表达能力之间的差距，展示了在边缘和神经形态硬件上实现超高效执行的潜力，为资源受限环境下的图学习提供了有前景的解决方案。

Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.

</details>


### [78] [Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value](https://arxiv.org/abs/2512.03399)
*Joe Edelman,Tan Zhi-Xuan,Ryan Lowe,Oliver Klingefjord,Vincent Wang-Mascianica,Matija Franklin,Ryan Othniel Kearns,Ellie Hain,Atrisha Sarkar,Michiel Bakker,Fazl Barez,David Duvenaud,Jakob Foerster,Iason Gabriel,Joseph Gubbels,Bryce Goodman,Andreas Haupt,Jobst Heitzig,Julian Jara-Ettinger,Atoosa Kasirzadeh,James Ravi Kirkpatrick,Andrew Koh,W. Bradley Knox,Philipp Koralus,Joel Lehman,Sydney Levine,Samuele Marro,Manon Revel,Toby Shorin,Morgan Sutherland,Michael Henry Tessler,Ivan Vendrov,James Wilken-Smith*

Main category: cs.LG

TL;DR: 论文提出"全栈对齐"概念，认为仅对齐AI系统与操作者意图不足，需同时对齐塑造AI的机构与人类价值观，并建议使用"厚价值模型"来有效表示价值观。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在局限：即使AI完美对齐其操作组织的意图，若该组织目标与其他机构或个人目标不一致，仍可能导致不良社会结果。现有价值观表示方法（如效用函数、偏好排序、非结构化文本）难以有效区分价值观与其他信号、支持规范性推理、建模集体利益。

Method: 提出"厚价值模型"方法，结构化表示价值观和规范，使系统能够：1) 区分持久价值观与短暂偏好；2) 建模个体选择的社会嵌入性；3) 进行规范性推理，将价值观应用于新领域。在五个领域展示该方法：AI价值管理、规范能力智能体、双赢谈判系统、意义保持经济机制、民主监管机构。

Result: 论文展示了厚价值模型在五个应用领域的可行性，表明该方法能够解决当前价值观表示方法的局限性，支持更有效的AI对齐和机构对齐。

Conclusion: 需要"全栈对齐"——同时对齐AI系统和塑造它们的机构与人类价值观，而厚价值模型是实现这一目标的关键方法，能够支持区分价值观与其他信号、规范性推理和集体利益建模，且无需强加特定的个体或集体繁荣愿景。

Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

</details>


### [79] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: 研究显示，在Transformer中显式训练世界模型能提升状态表示的线性可解码性和因果可操控性，从而增强强化学习后训练的效果，特别是在复杂任务状态上。


<details>
  <summary>Details</summary>
Motivation: 探索显式世界建模目标如何影响Transformer在不同训练阶段的内在表示和下游能力，特别是世界模型质量对强化学习后训练性能的影响。

Method: 使用2x2x2魔方作为测试环境，比较标准下一标记预测与两种显式世界建模策略：(i)状态预测预训练，(ii)状态预测+下一标记联合目标，然后应用GRPO进行后训练，通过线性探测和因果干预评估表示质量。

Result: 显式世界建模产生更线性可解码和因果可操控的状态表示，改进的状态表示显著提升GRPO性能，特别是在更难的魔方状态上。

Conclusion: 锐化状态表示可以提升序列规划任务后训练的有效性，显式世界建模是改善Transformer表示质量和下游任务性能的有前景方法。

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [80] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出GaussDetect-LiNGAM，一种新的双变量因果发现方法，通过利用前向模型噪声高斯性与反向回归残差独立性之间的等价性，消除了显式高斯性检验的需求。


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法依赖脆弱且对样本敏感的高斯性检验，这在实际应用中限制了其可靠性和实用性。需要一种更稳健的方法来增强因果推断的效率和实际适用性。

Method: 基于线性、无环性和外生性假设，证明了前向模型噪声的高斯性等价于反向模型中回归量与残差的独立性。利用这一理论洞察，用稳健的基于核的独立性检验替代了传统的高斯性检验。

Result: 实验验证了所提出的等价性，表明GaussDetect-LiNGAM在不同噪声类型和样本大小下保持高一致性，同时减少了每个决策的检验次数（TPD）。

Conclusion: 该方法提高了因果推断的效率和实际适用性，使LiNGAM在现实场景中更加可靠和易于应用，为因果发现提供了更稳健的解决方案。

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [81] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: 研究发现，在模型经历"顿悟"（grokking）阶段后应用机器遗忘方法，相比在顿悟前应用，能获得更高效的遗忘、更少的副作用和更稳定的更新。


<details>
  <summary>Details</summary>
Motivation: 探索顿悟现象（训练数据拟合后出现的延迟泛化）是否有助于机器遗忘任务，即在不完全重新训练的情况下移除特定数据的影响。

Method: 在视觉（CNNs/ResNets在CIFAR、SVHN、ImageNet）和语言（transformer在TOFU风格设置）任务上，比较在顿悟前后应用标准遗忘方法的效果。分析特征和曲率变化。

Result: 从顿悟后的检查点开始遗忘，能实现：(1) 更高效的遗忘（达到目标遗忘水平需要更少的更新），(2) 更少的附带损害（在保留数据和测试数据上的性能下降更小），(3) 跨种子的更新更稳定。顿悟后模型学习到更模块化的表示，遗忘和保留子集之间的梯度对齐减少。

Conclusion: 模型何时被训练（顿悟前vs后）是影响遗忘效果的独立因素，为改进现有遗忘方法提供了实用方案，无需改变算法本身。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [82] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 提出一个用于金融情感分析的端到端深度学习框架，通过跨模态注意力机制整合时效性意见和流行性意见两种模态，在837家公司数据集上达到83.5%准确率，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以有效整合多样化的意见模态并捕捉模态间的细粒度交互。时效性意见（近期市场更新）和流行性意见（趋势性集体情感）虽然都是文本数据，但代表不同的信息渠道，需要专门的方法来融合这些不同模态的信息。

Method: 提出端到端深度学习框架：1) 使用BERT（中文wwm-ext版本）进行特征嵌入；2) 设计金融多头跨注意力机制（FMHCA）促进两种意见模态间的信息交换；3) 通过Transformer层优化处理特征；4) 使用多模态因子双线性池化进行特征融合；5) 分类为负面、中性和正面情感。

Result: 在覆盖837家公司的综合数据集上进行广泛实验，该方法达到83.5%的准确率，显著优于包括BERT+Transformer在内的基线方法，提升幅度达21%。

Conclusion: 该框架通过有效整合时效性和流行性两种金融意见模态，展示了支持更准确金融决策和风险管理的潜力，证明了跨模态注意力机制在金融情感分析中的有效性。

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [83] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: BEBMS（贝叶斯事件模型）在疾病亚型分析中比SuStaIn表现更优，对模型误设更稳健，在阿尔茨海默病数据上结果更符合科学共识。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病在不同患者中进展方式存在差异，通常存在少量亚型。SuStaIn模型被广泛用于识别疾病亚型，但其稳健性需要评估。本文旨在开发更稳健的贝叶斯亚型事件模型，并与SuStaIn进行比较。

Method: 开发了贝叶斯事件模型（BEBMS），在多种合成数据实验中与SuStaIn进行比较，实验包含不同水平的模型误设。同时将两种方法应用于真实阿尔茨海默病数据集。

Result: BEBMS在排序、分期和亚型分配任务中显著优于SuStaIn。在阿尔茨海默病数据上，BEBMS的结果更符合该疾病进展的科学共识。

Conclusion: BEBMS作为SuStaIn的贝叶斯变体，在疾病亚型分析中表现更稳健、更准确，为理解疾病异质性提供了改进工具。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [84] [SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening](https://arxiv.org/abs/2512.03471)
*Ian Henriques,Lynda Elhassar,Sarvesh Relekar,Denis Walrave,Shayan Hassantabar,Vishu Ghanakota,Adel Laoui,Mahmoud Aich,Rafia Tir,Mohamed Zerguine,Samir Louafi,Moncef Kimouche,Emmanuel Cosson,Niraj K Jha*

Main category: cs.LG

TL;DR: SweetDeep：基于三星智能手表生理数据的轻量级神经网络，在自由生活条件下实现82.5%的2型糖尿病检测准确率


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病全球发病率上升，需要可扩展且经济有效的筛查方法。现有诊断依赖侵入性生化检测，成本高。虽然消费级可穿戴设备为机器学习疾病检测提供了可能，但先前研究局限于受控环境。

Method: 开发SweetDeep紧凑神经网络，使用欧盟和中东地区285名参与者的生理和人口统计数据，通过三星Galaxy Watch 7在自由生活条件下收集6天数据。每人每天提供多个2分钟传感器记录，总计约20条记录。模型参数少于3000个。

Result: 在三折交叉验证下，患者级准确率达82.5%（宏F1 82.1%，敏感性79.7%，特异性84.6%），预期校准误差5.5%。对低置信度预测（<10%）允许弃权后，剩余患者准确率提升至84.5%。

Conclusion: 工程化特征与轻量级架构结合，可在真实世界可穿戴设备环境中实现准确、快速且可泛化的2型糖尿病检测。

Abstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.

</details>


### [85] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: JPM是一个概率框架，用于从横断面数据中推断混合神经退行性疾病的联合进展，将单疾病轨迹视为部分排序，并构建联合进展的先验分布。


<details>
  <summary>Details</summary>
Motivation: 标准事件模型假设个体只有单一疾病，但神经退行性疾病中混合病理很常见，需要能够处理多种疾病同时进展的模型。

Method: 提出联合进展模型(JPM)，将单疾病轨迹视为部分排序，构建联合进展先验。研究了四种变体：Pairwise、Bradley-Terry、Plackett-Luce和Mallows，并分析了校准性、分离性和锐度三个特性。

Result: 所有变体都具有校准性，且几乎达到完美分离；锐度因变体而异，可通过输入部分排序的简单特征预测。在合成实验中，JPM比强基线SA-EBM提高了约21%的排序准确性。在NACC数据中，Mallows变体和基线模型的结果与AD和VaD混合病理进展的先前文献更一致。

Conclusion: JPM为从横断面数据推断混合神经退行性疾病进展提供了一个有效的概率框架，能够处理多种疾病同时进展的复杂情况，相比单一疾病假设的模型有显著改进。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [86] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: ATHENA是一个自主代理框架，用于管理端到端的计算研究生命周期，通过知识驱动的诊断过程（HENA循环）实现超人类性能，在科学计算和科学机器学习中达到10^-14的验证误差。


<details>
  <summary>Details</summary>
Motivation: 解决科学计算和科学机器学习中理论概念化与计算实现之间的鸿沟，这是一个主要的瓶颈。当前标准自动化方法在处理复杂科学问题时存在局限性，需要更智能的系统来加速科学发现。

Method: 引入ATHENA框架，其核心是HENA循环（知识驱动的诊断过程，建模为上下文多臂老虎机问题）。系统作为在线学习器分析先前试验，从组合空间中选择结构"动作"，通过专家蓝图指导（如通用逼近、物理信息约束），将动作转换为可执行代码以生成科学奖励。结合混合符号-数值工作流和人类在环协作。

Result: 在科学计算中，自主识别数学对称性获得精确解析解，或推导稳定数值求解器；在科学机器学习中，处理病态公式并解决多物理问题。框架达到超人类性能，验证误差达到10^-14。人类在环干预可将结果提高一个数量级。

Conclusion: ATHENA实现了从实现机制到方法创新的范式转变，加速科学发现。它超越了标准自动化，通过知识驱动的自主学习和人类协作，解决了科学计算和科学机器学习中的核心挑战。

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [87] [Modal Logical Neural Networks](https://arxiv.org/abs/2512.03491)
*Antonin Sulc*

Main category: cs.LG

TL;DR: 提出Modal Logical Neural Networks (MLNNs)，一种将深度学习与模态逻辑形式语义结合的神经符号框架，能够进行必然性和可能性推理。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够将深度学习与形式逻辑推理结合起来的框架，特别是能够处理模态逻辑（必然性和可能性）的神经符号系统，以提高模型的逻辑一致性和可解释性。

Method: 基于Kripke语义，引入专门处理模态运算符□和◇的神经元，这些神经元在一组可能世界上操作。框架具有高度灵活性：世界间的可达关系可以由用户固定以强制执行已知规则，或者作为归纳特征由神经网络参数化学习。

Result: 在四个案例研究中展示了MLNNs的应用：语法护栏、未知的公理检测、多智能体认知信任、以及自然语言谈判中的建设性欺骗检测。实验表明，强制执行或学习可达关系可以提高逻辑一致性和可解释性，而不改变底层任务架构。

Conclusion: MLNNs提供了一个可微分的端到端神经符号框架，能够同时从数据中学习逻辑系统的关系结构并在该结构内进行演绎推理，增强了深度学习模型的逻辑一致性和可解释性。

Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.

</details>


### [88] [Physics-Driven Learning Framework for Tomographic Tactile Sensing](https://arxiv.org/abs/2512.03512)
*Xuanxuan Yang,Xiuyang Zhang,Haofeng Chen,Gang Ma,Xiaojie Wang*

Main category: cs.LG

TL;DR: PhyDNN：一种物理驱动的深度学习框架，通过将EIT前向模型嵌入学习目标，提高触觉传感中电导率重建的准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 电阻抗断层扫描（EIT）因其布线简单和形状灵活而适用于大面积触觉传感，但其非线性逆问题常导致严重伪影和不准确的接触重建。

Method: 提出PhyDNN框架，将EIT前向模型直接嵌入学习目标，联合最小化预测与真实电导率图的差异，并强制与正向偏微分方程的一致性。设计了可微分的前向算子网络来近似非线性EIT响应，实现快速物理引导训练。

Result: 在16电极软传感器上的仿真和实际触觉实验表明，PhyDNN在重建接触形状、位置和压力分布方面始终优于NOSER、TV和标准DNN，产生更少伪影、更清晰边界和更高指标分数。

Conclusion: PhyDNN通过结合物理模型与深度学习，提高了EIT触觉传感的重建质量，展示了高质量断层扫描触觉传感的有效性。

Abstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.

</details>


### [89] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 提出自适应稀疏感知框架，结合变分自编码器先验和强化学习进行顺序测量选择，优于传统压缩感知、最优传感器放置和基于生成模型的方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知使用通用基和随机测量，效率和质量受限；最优传感器放置基于历史数据但使用固定线性基，无法适应非线性或样本特异性变化；基于生成模型的压缩感知使用深度生成先验但仍采用次优随机采样。

Method: 提出自适应稀疏感知框架，将变分自编码器先验与强化学习相结合，通过强化学习顺序选择测量点，利用VAE提供生成先验指导采样过程。

Result: 实验表明该方法在稀疏测量下的重建质量优于传统压缩感知、最优传感器放置和基于生成模型的重建方法。

Conclusion: 通过结合深度生成先验和自适应顺序采样，实现了更高效的稀疏感知和更高质量的重建，解决了传统方法在采样效率和适应性方面的局限性。

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [90] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: DLC是一种用于类增量学习的插件扩展范式，通过部署额外的LoRA组件，在基础模型上注入任务特定的残差，并使用轻量级加权单元聚合表示，显著提升准确率且参数效率高。


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法存在遗忘问题或稳定性-可塑性困境，而扩展方法虽然准确率更高但参数增加显著。需要一种既高效又参数友好的增量学习方案。

Method: 将基于回放或蒸馏训练的特征提取器作为基础模型，为每个任务使用LoRA在深层注入任务特定残差。推理时聚合带残差的表示，并引入轻量级加权单元来缓解非目标LoRA插件的干扰。

Result: 在ImageNet-100上，仅使用标准ResNet-18 4%的参数，DLC实现了8%的准确率提升，表现出卓越的效率。在固定内存预算下也能超越最先进方法。

Conclusion: DLC作为一种即插即用的增强方法，通过LoRA组件高效扩展基础方法，在类增量学习中实现了高准确率和参数效率的平衡。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [91] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 本文提出扩散模型再学习攻击(DiMRA)可逆转基于微调的机器遗忘方法，并提出了通过记忆替代数据的遗忘方法(DiMUM)来增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的机器遗忘方法存在安全漏洞，当前基于微调的遗忘方法可能被攻击逆转，导致敏感内容重新生成，需要更鲁棒的解决方案。

Method: 1. DiMRA攻击：在无先验知识的情况下，在辅助数据集上优化已遗忘的扩散模型，逆转遗忘效果；2. DiMUM防御：通过记忆替代数据/特征来替换目标遗忘内容，而非单纯遗忘。

Result: DiMRA成功逆转了最先进的基于微调的扩散模型遗忘方法；DiMUM在保持生成性能的同时，显著增强了对DiMRA攻击的鲁棒性。

Conclusion: 当前基于微调的扩散模型遗忘方法存在严重安全漏洞，DiMUM通过记忆替代策略提供了更鲁棒的遗忘解决方案，能有效防御再学习攻击。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [92] [When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate](https://arxiv.org/abs/2512.03578)
*Florent Forest,Amaury Wei,Olga Fink*

Main category: cs.LG

TL;DR: MAGNETS是一个用于时间序列外生回归的固有可解释神经网络架构，通过学习无需标注的人类可理解概念，提供透明决策过程。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列外生回归模型虽然预测性能强，但通常是黑盒模型，难以理解驱动决策的时间模式。后验可解释性技术产生粗糙、嘈杂或不稳定的解释，而固有可解释方法需要概念标注、无法捕捉特征交互、缺乏复杂时间模式表达能力，且难以扩展到高维多元数据。

Method: 提出MAGNETS（Mask-and-AGgregate NEtwork for Time Series）架构，通过学习一组紧凑的人类可理解概念，每个概念对应基于掩码的选定输入特征聚合，明确揭示哪些特征驱动预测以及何时在序列中起作用。预测通过透明加法结构组合这些学习到的概念。

Result: MAGNETS能够学习无需标注的人类可理解概念，明确显示驱动预测的特征及其在时间序列中的重要性，提供清晰的模型决策过程洞察。

Conclusion: MAGNETS解决了现有可解释时间序列模型的局限性，提供了一种固有可解释的神经网络架构，能够在保持预测准确性的同时提供透明决策过程，适用于医疗、金融、环境监测和工程等领域。

Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.

</details>


### [93] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 该论文为高斯分布下的最优传输和Gromov-Wasserstein对齐提供了全面的闭式解，解决了未中心化高斯在希尔伯特空间中的对齐问题，并扩展到多边际OT和IGW重心计算。


<details>
  <summary>Details</summary>
Motivation: 最优传输和Gromov-Wasserstein对齐在数据科学中应用广泛，但计算成本高。现有方法主要针对高斯分布和二次成本，但仍有未解决的问题，如未中心化高斯在希尔伯特空间中的对齐问题。

Method: 1. 为希尔伯特空间中未中心化高斯分布的IGW对齐提供闭式表达式（包含酉算子的二次优化）；2. 当至少一个高斯分布中心化时，提供完全闭式解；3. 为中心化高斯的IGW重心提供解析解；4. 将高斯多边际OT简化为可处理优化问题，并提出基于秩缺陷约束的高效算法。

Result: 1. 解决了未中心化高斯在希尔伯特空间中的IGW对齐问题；2. 为中心化情况提供完全闭式解；3. 实现了IGW重心的解析计算；4. 开发了高效的多边际OT算法；5. 在知识蒸馏和异构聚类任务中验证了方法的实用性。

Conclusion: 该工作填补了高斯分布下OT和GW对齐的理论空白，提供了更广泛适用的闭式解和高效算法，为大规模应用提供了理论基础和实用工具。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [94] [Federated Learning and Trajectory Compression for Enhanced AIS Coverage](https://arxiv.org/abs/2512.03584)
*Thomas Gräupl,Andreas Reisenbauer,Marcel Hecko,Anil Rasouli,Anita Graser,Melitta Dragaschnig,Axel Weissenfeld,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.LG

TL;DR: VesselEdge系统利用联邦学习和带宽受限轨迹压缩技术，将船只转化为移动传感器，通过扩展AIS覆盖范围来增强海上态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 海上态势感知受限于AIS（自动识别系统）覆盖范围不足，特别是在偏远海域。传统方法难以实现实时异常检测和高效数据传输，特别是在低带宽连接环境下。

Method: 系统结合联邦学习（M3fed模型）和带宽受限轨迹压缩（BWC-DR-A算法），将船只转化为移动传感器，优先传输异常数据，实现实时异常检测和高效数据传输。

Result: 初步结果显示VesselEdge能有效提高AIS覆盖范围和海上态势感知能力，特别是在使用历史数据的情况下表现出良好效果。

Conclusion: VesselEdge系统通过联邦学习和轨迹压缩技术，成功扩展了AIS覆盖范围，增强了海上实时态势感知能力，为低带宽环境下的海事监控提供了有效解决方案。

Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.

</details>


### [95] [Observation-driven correction of numerical weather prediction for marine winds](https://arxiv.org/abs/2512.03606)
*Matteo Peduto,Qidong Yang,Jonathan Giezendanner,Devis Tuia,Sherrie Wang*

Main category: cs.LG

TL;DR: 基于Transformer的深度学习架构，通过同化现场观测数据来校正全球天气预报系统(GFS)输出，显著提升海洋风速预测精度


<details>
  <summary>Details</summary>
Motivation: 海洋风速预测对航行安全和能源运营至关重要，但海洋观测数据稀疏、异构且随时间变化，导致预测困难。现有数值天气预报模型存在系统性误差，需要利用观测数据进行校正

Method: 提出基于Transformer的深度学习架构：1) 通过掩码和基于集合的注意力机制处理不规则和时间变化的观测集；2) 通过交叉注意力将预测条件化于最近的观测-预报对；3) 使用循环时间嵌入和坐标感知位置表示，实现任意空间坐标的单次推理

Result: 在大西洋区域使用ICOADS观测数据评估，模型在所有48小时预报时效内均降低了GFS 10米风速RMSE：1小时时效改善45%，48小时时效改善13%。空间分析显示在海岸线和航运路线等观测密集区域改进最显著

Conclusion: 该模型提供了一种实用、低延迟的后处理方案，通过校正系统性预报误差来补充数值天气预报，能够处理异构观测平台并同时生成站点特定预测和流域尺度网格产品

Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.

</details>


### [96] [CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion](https://arxiv.org/abs/2512.03610)
*Julius Lenz*

Main category: cs.LG

TL;DR: CoGraM是一种用于联邦学习和分布式学习的神经网络合并方法，通过多阶段、上下文敏感、基于损失的迭代优化，在层、神经元和权重级别进行合并，相比传统方法显著提升合并网络性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习和分布式学习中，无需重新训练就能合并神经网络是关键需求。现有方法如权重平均或Fisher合并通常会导致精度损失，并且在不同随机种子下表现不稳定，需要更鲁棒的合并方法。

Method: CoGraM是一种多阶段、上下文敏感的基于损失的迭代优化方法，在层、神经元和权重级别进行操作。它通过损失差异和阈值对齐决策，并通过回滚机制防止有害更新。

Result: CoGraM能够显著改善合并后的网络性能，解决了Fisher等传统方法的弱点，提供更稳定和准确的神经网络合并。

Conclusion: CoGraM为联邦学习和分布式学习中的神经网络合并问题提供了一个有效的解决方案，通过精细的多级优化机制克服了现有方法的局限性。

Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.

</details>


### [97] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: 使用视觉语言模型从视频编码的网格天气数据直接生成航运预报文本，展示了在气象服务中应用多模态基础模型的新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型具有潜力，但在气象产品和服务生成方面的应用仍处于起步阶段。需要探索新方法来加速气象企业的生产效率和创新。

Method: 采用视觉语言模型，直接从视频编码的网格天气数据生成航运预报文本。

Result: 早期结果显示，该方法在提高生产效率和促进服务创新方面具有可扩展的技术机会。

Conclusion: 这项研究为气象企业及其他领域展示了多模态基础模型在增强生产效率和推动服务创新方面的潜力。

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [98] [Conditional updates of neural network weights for increased out of training performance](https://arxiv.org/abs/2512.03653)
*Jan Saynisch-Wagner,Saran Rajendran Sari*

Main category: cs.LG

TL;DR: 提出一种增强神经网络在训练数据与应用数据分布不同时性能的方法，通过重训练获取权重异常，建立预测因子与权重异常的回归关系，并外推权重到应用数据


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在训练数据与应用数据不相似时的性能问题，包括分布外问题、模式变化和机制转换等场景

Method: 三步骤方法：1) 对训练数据集的合理子集进行重训练，记录权重异常；2) 选择合理预测因子，建立预测因子与权重异常的回归关系；3) 将权重外推到应用数据，从而外推神经网络

Result: 在气候科学的三个用例中展示了该方法，成功实现了神经网络在时间、空间和跨域的外推

Conclusion: 该方法能有效增强神经网络在训练数据与应用数据分布不同时的性能，在气候科学领域具有实际应用价值

Abstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.

</details>


### [99] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 提出统一深度学习框架，结合循环时间编码与LSTM-CNN混合架构，提升多步能源预测精度


<details>
  <summary>Details</summary>
Motivation: 准确的电力消费预测对需求管理和智能电网运营至关重要，需要同时捕捉长期季节效应和短期局部模式

Method: 使用正弦余弦编码系统转换日历属性以保留周期结构，采用LSTM、CNN和针对每个预测时域的MLP回归器元学习器组成的集成模型

Result: 在所有七个预测时域上均取得一致改进，混合模型的RMSE和MAE均低于单个架构和现有方法

Conclusion: 循环时间表示与互补深度学习结构的结合具有显著优势，这是首个在统一短期能源预测框架中联合评估时间编码、日历特征和混合集成架构的工作

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [100] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: DSAS是一个动态缩放激活导向框架，通过自适应调节导向强度，只在需要时进行干预，在毒性缓解和模型效用间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有激活导向方法对所有输入统一应用干预，在不必要导向时会降低模型性能。需要一种能自适应判断何时需要导向的方法。

Method: DSAS将"何时导向"与"如何导向"解耦，通过计算上下文相关的缩放因子，动态调节现有导向方法的强度。可以端到端联合优化导向函数。

Result: DSAS与现有导向方法结合能改善帕累托前沿，在毒性缓解和效用保持间取得更好权衡。应用于文本到图像扩散模型也能有效调节特定概念。

Conclusion: DSAS提供了一种通用、高效的动态导向框架，计算开销小，同时提高了可解释性，能识别哪些标记需要导向及导向强度。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [101] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 该论文提出了一种特征感知的时间调制机制，通过调整特征表示的时间上下文来缓解表格数据中的时间分布偏移问题，平衡模型的泛化性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据的特征与标签关系随时间不断演变，静态模型假设固定映射以保证泛化性，而自适应模型可能过度拟合瞬态模式，这造成了鲁棒性与适应性之间的困境。

Method: 提出特征感知的时间调制机制，通过条件化特征表示的时间上下文，调制统计属性如尺度和偏度，从而对齐跨时间阶段的特征语义。

Result: 基准评估验证了该方法在处理表格数据时间偏移方面的有效性，实现了轻量级但强大的适应性，有效平衡了泛化性和适应性。

Conclusion: 通过分析特征语义演变和特征转换策略，提出的特征感知时间调制机制能够有效缓解表格数据中的概念漂移问题，为时间分布偏移提供了实用的解决方案。

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [102] [Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns](https://arxiv.org/abs/2512.03696)
*Mohammad Doost,Mohammad Manthouri*

Main category: cs.LG

TL;DR: 提出QTGNN框架，结合量子嵌入、变分图卷积和拓扑数据分析，用于大规模金融网络中的欺诈交易检测，在NISQ设备上实现稳定训练和可解释决策。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法难以捕捉金融交易网络中的复杂动态和结构异常，需要结合量子计算、图神经网络和拓扑数据分析来提升检测性能。

Method: 1. 量子数据嵌入与纠缠增强；2. 变分量子图卷积与非线形动态；3. 高阶拓扑不变量提取；4. 混合量子-经典异常学习与自适应优化；5. 基于拓扑归因的可解释决策。

Result: 在PaySim和Elliptic金融数据集上的模拟实验显示，QTGNN在ROC-AUC、精确度和误报率等指标上优于经典和量子基线方法，消融研究验证了各组件贡献。

Conclusion: QTGNN为金融欺诈检测提供了理论可靠、可解释且实用的解决方案，桥接了量子机器学习、图论和拓扑分析，适用于NISQ硬件并具有良好的可扩展性。

Abstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.

</details>


### [103] [Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm](https://arxiv.org/abs/2512.03744)
*Xuhui Lin,Qiuchen Lu*

Main category: cs.LG

TL;DR: 提出物理约束哈密顿学习算法，结合结构不可逆检测和能量景观重建，识别城市交通系统在极端天气后的隐藏结构损伤，解决传统方法无法区分的"虚假恢复"问题。


<details>
  <summary>Details</summary>
Motivation: 当前城市交通系统韧性评估方法依赖表面恢复指标，无法检测隐藏的结构损伤，导致无法区分真正的恢复和"虚假恢复"（交通指标正常化但系统动力学永久退化）。

Method: 开发物理约束哈密顿学习算法，包括：提取低维状态表示、通过物理约束优化识别准哈密顿结构、通过能量景观比较量化结构变化。

Result: 分析伦敦2021年极端降雨事件，发现表面指标完全恢复，但算法检测到64.8%的结构损伤被传统监测方法遗漏。

Conclusion: 该框架为主动结构风险评估提供工具，使基础设施投资能够基于真实的系统健康状况而非误导性的表面指标。

Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.

</details>


### [104] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 研究发现近60个科学模型（涵盖字符串、图、3D原子和蛋白质等模态）在化学系统中表现出高度对齐的表示学习，表明基础模型学习物理现实的共同底层表示，但当前模型仍受训练数据和归纳偏置限制。


<details>
  <summary>Details</summary>
Motivation: 理解不同模态和架构的机器学习模型是否学习相似的内部表示对于构建可靠泛化的科学基础模型至关重要。虽然语言和视觉领域已观察到表示收敛，但在科学领域尚未系统探索。

Method: 分析了近60个科学模型的表示学习，涵盖字符串、图、3D原子和蛋白质等不同模态，研究了它们在各种化学系统中的表示对齐情况，并考察了模型在不同输入条件下的表示行为。

Result: 1) 不同数据集训练的模型对小分子具有高度相似的表示；2) 机器学习原子间势能模型在性能提升时在表示空间收敛；3) 发现两种不同机制：在训练相似输入上，高性能模型紧密对齐，弱模型在表示空间发散到局部最优；在完全不同结构上，几乎所有模型都坍缩到低信息表示。

Conclusion: 表示对齐可作为科学基础模型通用性的定量基准。当前模型仍受训练数据和归纳偏置限制，尚未编码真正通用的结构。该工作可追踪物质通用表示的出现，并为跨模态、物质领域和科学任务选择最佳表示迁移模型提供指导。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [105] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 提出条件轨迹编码器，联合学习空间和运动表示，通过几何特征保持起点依赖的不对称性，量化城市形态导致的认知不平等。


<details>
  <summary>Details</summary>
Motivation: 当前城市轨迹分析方法存在方法碎片化：轨迹学习捕捉运动模式但忽略空间上下文，空间嵌入方法编码街道网络但忽略时间动态。存在三个主要差距：(1)缺乏整合时空表示的联合训练，(2)起点不可知的处理忽略导航中的方向不对称性，(3)过度依赖辅助数据而非城市空间的基本几何特性。

Method: 引入条件轨迹编码器，联合学习空间和运动表示，使用几何特征保持起点依赖的不对称性。框架将城市导航分解为共享认知模式和起点特定的空间叙事。采用双向LSTM处理可见性比和曲率特征，以可学习的起点嵌入为条件，通过对比学习将表示分解为共享城市模式和起点特定特征。

Result: 在六个合成城市和北京西城区的真实世界验证表明，城市形态创造了系统性的认知不平等。该方法为城市规划者提供了评估体验公平性的定量工具，为建筑师提供了布局决策认知影响的见解，并为导航系统实现了起点感知分析。

Conclusion: 提出的条件轨迹编码器框架成功解决了当前轨迹分析方法的局限性，通过联合时空表示学习和几何特征利用，实现了对城市导航中认知不对称性的量化测量，为城市规划、建筑设计和导航系统提供了重要的分析工具。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [106] [Deep Unfolding: Recent Developments, Theory, and Design Guidelines](https://arxiv.org/abs/2512.03768)
*Nir Shlezinger,Santiago Segarra,Yi Zhang,Dvir Avrahami,Zohar Davidov,Tirza Routtenberg,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 深度展开（Deep Unfolding）是一种将迭代优化算法转化为结构化可训练机器学习架构的框架，旨在结合传统优化方法的理论保证与机器学习的数据驱动能力。


<details>
  <summary>Details</summary>
Motivation: 传统迭代优化算法虽然具有可解释性和理论保证，但通常依赖替代目标函数、需要仔细的超参数调整且计算延迟较大。而机器学习虽然具备强大的数据驱动建模能力，但缺乏优化驱动推理所需的结构、透明性和效率。深度展开旨在桥接这两个范式。

Method: 深度展开通过系统地将迭代优化算法转化为结构化、可训练的机器学习架构。文章介绍了四种代表性的设计范式，并讨论了由其迭代性质产生的独特训练方案。

Result: 文章综述了深度展开的理论进展，包括收敛性和泛化保证的建立，并通过比较定性和实证研究说明了其在复杂性、可解释性和鲁棒性方面的权衡。

Conclusion: 深度展开为优化和机器学习提供了一个有前景的桥梁框架，结合了传统优化的理论优势与机器学习的数据驱动能力，在信号处理等领域具有重要应用价值。

Abstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.

</details>


### [107] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 基于智能手机和智能手表传感器数据，使用机器学习方法将数字痕迹转化为不同身体活动的似然比，用于法医调查中的活动识别和重建。


<details>
  <summary>Details</summary>
Motivation: 智能手机和智能手表在日常生活中的普及提供了丰富的用户行为信息，其内置运动传感器产生的数字痕迹为法医调查人员了解个人身体活动提供了机会。

Method: 提出基于机器学习的方法，将数字痕迹转化为不同身体活动的似然比（LRs），使用新的数据集NFI_FARED（包含四种iPhone的19种活动标记数据），并扩展到同时分析多个活动或活动组，创建活动时间线。

Result: 在171种可能的活动中，该方法能够为167种活动配对生成有效的似然比系统，成功区分不同活动类型。数据集和代码已公开以促进进一步研究。

Conclusion: 该方法能够有效利用智能设备传感器数据进行法医调查中的活动识别，为调查的早期和后期阶段提供支持，公开的数据集和代码有助于推动该领域研究。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [108] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 提出基于过程挖掘的两阶段临床路径建模方法，利用合规性检查扩展临床路径知识库，在COVID-19并发症数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 临床路径的手工建模困难且难以反映实际最佳实践，特别是针对不同疾病变异或组合的情况。需要自动化方法来扩展临床路径知识库。

Method: 两阶段过程挖掘方法：第一阶段从历史数据中提取治疗过程模型；第二阶段将新数据与参考模型进行合规性检查，基于检查结果扩展知识库，为新的疾病变异或组合创建更具体的模型。

Result: 在Synthea基准数据集（模拟SARS-CoV-2感染及COVID-19并发症）上验证，方法能够以足够精度扩展临床路径知识库，AUC峰值达到95.62%，同时保持67.11%的弧度简单性。

Conclusion: 提出的两阶段过程挖掘方法能够有效扩展临床路径知识库，为不同疾病变异和组合提供更精确的治疗模型，有助于改善医疗护理质量。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [109] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 基于EfficientNet的轻量级ECG分类模型EfficientECG，通过跨注意力机制融合多导联和多特征（性别、年龄），实现高精度心电图分析


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）是一种快速、无创且信息丰富的诊断信号，但现有ECG模型误诊率高，医疗工作者负担重。需要开发准确、快速且能自动提取特征的深度学习模型来改善ECG分析。

Method: 1. 提出EfficientECG：基于EfficientNet的轻量级ECG分类模型，能有效处理高频长序列多导联ECG数据；2. 提出跨注意力特征融合模型：在EfficientECG基础上，通过跨注意力机制融合多导联ECG数据及多特征（性别、年龄等）。

Result: 在代表性ECG数据集上的评估验证了模型在精度、多特征融合和轻量化方面的优越性，优于现有最先进方法。

Conclusion: 提出的EfficientECG及跨注意力特征融合模型能准确、快速分析ECG数据，有效降低医疗工作者负担，为ECG诊断提供了新的深度学习解决方案。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [110] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文系统研究了深度强化学习在动态算法配置中的应用，针对(1+(λ,λ))-GA算法在OneMax问题上的种群规模参数控制，揭示了DDQN和PPO的局限性，并提出自适应奖励偏移机制解决探索不足问题。


<details>
  <summary>Details</summary>
Motivation: 动态算法配置(DAC)旨在为参数化优化算法寻找高效的控制策略。虽然强化学习(RL)因其决策鲁棒性被广泛应用于算法配置，但将RL应用于DAC面临挑战且需要大量领域专业知识。本文旨在通过系统研究深度RL算法在DAC中的表现，揭示其根本问题并提供解决方案。

Method: 以控制(1+(λ,λ))-GA算法在OneMax实例上的种群规模参数为案例，系统分析DDQN和PPO两种深度RL算法。针对发现的探索不足问题，提出自适应奖励偏移机制，利用奖励分布统计增强DDQN探索能力；针对规划视野覆盖问题，采用无折扣学习解决DDQN的问题，并分析PPO的方差问题。

Result: 研究发现DDQN和PPO在DAC中存在两个根本挑战：可扩展性退化和学习不稳定性。自适应奖励偏移机制能有效解决探索不足问题，无需实例特定超参数调优且在不同问题规模下保持一致性。无折扣学习能解决DDQN的规划视野覆盖问题，而PPO存在根本性方差问题。配备自适应奖励偏移的DDQN能达到与理论推导策略相当的性能，样本效率大幅提升，优于先前DAC方法数个数量级。

Conclusion: 深度RL算法在DAC应用中面临探索不足和规划视野覆盖两大挑战。自适应奖励偏移机制和无折扣学习能有效解决这些问题，使DDQN在DAC任务中表现出色。PPO由于方差问题需要替代算法设计。研究为RL在DAC中的应用提供了系统分析和实用解决方案。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [111] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出一种基于logprobs统计测试的低成本LLM API监控方法，仅需单个token输出即可检测微小模型变化，比现有方法敏感1000倍且成本低1000倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM API审计方法成本过高，无法定期监控广泛可用的LLM API，导致模型更新在实践中缺乏监控，影响下游应用可靠性和研究可复现性

Method: 基于LLM对数概率(logprobs)的统计测试方法，仅需请求单个token输出，通过分析每个token logprob的平均值来检测模型变化

Result: 该方法能检测小至一次微调步骤的模型变化，比现有方法更敏感，同时成本降低1000倍；引入TinyChange基准来衡量审计方法对微小实际模型变化的敏感性

Conclusion: logprobs虽然通常是非确定性的，但仍可作为低成本连续监控LLM API的基础，提供了一种实用且高效的模型变化检测方案

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [112] [Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission](https://arxiv.org/abs/2512.03819)
*Junlin Chang,Yubo Han,Hnag Yue,John S Thompson,Rongke Liu*

Main category: cs.LG

TL;DR: 提出基于深度联合信源信道编码的3D点云语义无线传输框架，通过预测接收端语义正交特征池的组合权重实现紧凑表示和鲁棒重建，在带宽受限场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度传感器的普及，点云获取门槛降低，但点云数据的无线传输面临带宽限制和信道噪声的挑战。现有方法在高带宽下表现良好，但在带宽受限场景下性能下降明显，需要更高效的语义传输方案。

Method: 1) 基于DeepJSCC构建语义无线传输框架；2) 发送端预测接收端语义正交特征池的组合权重而非原始特征；3) 使用折叠式解码器将2D网格变形为3D点云；4) 采用Chamfer距离和正交正则化进行训练。

Result: 在ModelNet40数据集上评估，高带宽下性能与SEPT相当，带宽受限场景下明显优于SEPT，PSNR和Chamfer距离均有持续改善。消融实验验证了正交化和折叠先验的有效性。

Conclusion: 提出的语义传输框架通过正交特征池和折叠解码器实现了高效的点云传输，在带宽受限场景下表现出色，为3D点云的无线传输提供了有效的解决方案。

Abstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.

</details>


### [113] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO：结合条件风险理论与分布价值建模的新RL框架，用于LLM后训练，在噪声监督下平衡鲁棒性与泛化性


<details>
  <summary>Details</summary>
Motivation: 现实世界部署中常遇到噪声或不完整的监督信号，现有方法（如RFQI、CQL、PPO、GRPO）要么忽视泛化性，要么产生过于保守的策略，在不同场景下表现不稳定

Method: DVPO结合条件风险理论与分布价值建模，学习token级别的价值分布提供细粒度监督，应用非对称风险正则化：压缩下尾以抑制噪声负偏差，扩展上尾以保持探索多样性

Result: 在多轮对话、数学推理和科学问答的广泛实验中，DVPO在噪声监督下持续优于PPO、GRPO和基于鲁棒Bellman的PPO

Conclusion: DVPO展示了在现实世界LLM后训练中的潜力，能更好地平衡鲁棒性与泛化性

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [114] [Scalable Decision Focused Learning via Online Trainable Surrogates](https://arxiv.org/abs/2512.03861)
*Gaetano Signorelli,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出一种基于无偏估计的代理损失函数加速决策聚焦学习，在保持解质量的同时减少昂贵的内部求解器调用。


<details>
  <summary>Details</summary>
Motivation: 决策支持系统需要解决复杂的优化问题，传统训练的估计器会导致次优解。决策聚焦学习使用实际决策成本作为损失函数，但训练时计算代价高，可扩展性差。

Method: 提出一种加速方法，用高效的代理损失函数替代昂贵的损失函数评估。该代理基于无偏估计，减少虚假局部最优风险，并能提供局部置信度信息，必要时可切换回备选方法。代理设计为黑盒设置，可补偿优化模型简化并考虑成本计算中的追索行动。

Result: 该方法显著减少了昂贵的内部求解器调用次数，同时解质量与现有最先进技术相当。

Conclusion: 提出的基于无偏估计的代理损失函数方法有效解决了决策聚焦学习的可扩展性问题，在保持解质量的同时大幅降低计算成本。

Abstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.

</details>


### [115] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: 研究比较了智能加工中AI模型的能耗，发现超维计算(HDC)在保持精度的同时，能耗比传统模型降低200倍（训练）和175-1000倍（推理），训练时间减少200倍，推理时间减少300-600倍。


<details>
  <summary>Details</summary>
Motivation: 智能制造虽能提高效率和降低能耗，但AI模型的高能耗可能抵消这些优势。需要寻找既能保持精度又能大幅降低能耗的AI解决方案。

Method: 采用原位传感预测智能加工中的几何质量，比较常见AI模型的能耗、精度和速度。引入超维计算(HDC)作为替代方案，与传统模型进行对比分析。

Result: HDC在精度上与传统模型相当，但能耗显著降低：训练能耗降低200倍，推理能耗降低175-1000倍。训练时间减少200倍，推理时间减少300-600倍。

Conclusion: HDC展示了在智能制造中实现高能效AI的巨大潜力，能够在保持精度的同时大幅降低能耗和计算时间，为可持续智能制造提供可行方案。

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [116] [Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models](https://arxiv.org/abs/2512.03882)
*Haidong Kang,Wei Wu,Hanling Wang*

Main category: cs.LG

TL;DR: 本文提出ACraft方法，利用大语言模型自动为少样本类增量学习(FSCIL)生成最优攻击方法，显著超越人工设计的攻击方法


<details>
  <summary>Details</summary>
Motivation: FSCIL是一个更具现实性和挑战性的持续学习范式，但之前的研究主要关注有效的FSCIL方法，较少关注其安全问题。本文旨在全面研究攻击对FSCIL的影响，并发现现有攻击方法要么无法有效攻击基类，要么依赖大量专家知识导致高成本

Method: 提出ACraft方法：1) 利用大语言模型自动发现针对FSCIL的最优攻击方法，无需人工专家；2) 引入基于近端策略优化的强化学习来优化学习，通过建立正反馈使LLM在下一代生成更好的攻击方法

Result: 在主流基准测试上，ACraft显著降低了最先进FSCIL方法的性能，大幅超越人工专家设计的攻击方法，同时保持最低的攻击成本

Conclusion: 本文首次全面研究FSCIL的安全问题，提出的ACraft方法能有效自动生成针对FSCIL的攻击，揭示了FSCIL系统在面对自动生成攻击时的脆弱性，为未来安全研究提供了新方向

Abstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.

</details>


### [117] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 本文为模糊单纯集提供了一个统一的概率理论框架，将其解释为单纯集上概率测度的边际分布，揭示了UMAP中模糊权重的生成模型来源，并展示了如何基于该框架推导新的降维方法。


<details>
  <summary>Details</summary>
Motivation: 模糊单纯集在降维和流形学习中（特别是UMAP中）变得重要，但其代数拓扑定义缺乏清晰的概率解释，脱离了这些领域常用的理论框架。本文旨在建立模糊单纯集的概率理论基础。

Method: 引入一个框架，将模糊单纯集解释为单纯集上概率测度的边际分布。具体展示了UMAP的模糊权重源于一个生成模型，该模型在随机尺度上采样Vietoris-Rips滤过，产生成对距离的累积分布函数。该框架还将模糊单纯集连接到面偏序集上的概率模型。

Result: 该框架为模糊单纯集提供了统一的概率理论基础，阐明了UMAP在该框架中的角色，恢复了通过底层单纯集的布尔运算得到的标准t-范数和t-余范数，并澄清了Kullback-Leibler散度与模糊交叉熵之间的关系。

Conclusion: 概率视角为模糊单纯集提供了统一的理论基础，阐明了UMAP在该框架中的作用，并使得能够系统性地推导新的降维方法。作者通过使用Čech滤过和三重采样的UMAP泛化示例展示了新方法的推导潜力。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [118] [Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations](https://arxiv.org/abs/2512.03923)
*Xiang Rao,Yina Liu,Yuxuan Shen*

Main category: cs.LG

TL;DR: 提出了一种结合量子电路和经典神经网络的混合物理信息神经网络（QCPINN），用于解决油藏渗流偏微分方程，相比传统PINNs具有更高的参数效率和更强的非线性拟合能力。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法存在网格依赖误差和计算成本高的问题，经典物理信息神经网络在参数效率、高维表达和强非线性拟合方面存在瓶颈。需要一种更高效的求解油藏渗流PDE的方法。

Method: 提出离散变量电路量子-经典物理信息神经网络（DV-QCPINN），将经典预处理/后处理网络与离散变量量子核心集成，利用量子叠加和纠缠增强高维特征映射，同时嵌入物理约束确保解的一致性。测试了三种量子电路拓扑结构：级联、交叉网格和交替拓扑。

Result: QCPINNs比经典PINNs使用更少参数实现高预测精度。在非均质单相流和两相BL方程模拟中，交替拓扑表现最佳；在考虑吸附的对流-扩散-吸附耦合组分流中，级联拓扑表现最优。验证了QCPINN在油藏工程应用中的可行性。

Conclusion: 该工作首次将QCPINN应用于三种典型油藏渗流模型，验证了量子计算在油藏工程中的实际应用潜力，为量子计算研究与油气工程工业实践之间搭建了桥梁。

Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.

</details>


### [119] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: DiVAE是一种轻量级VAE正则化器，通过将VAE先验概率与数据估计密度对齐来改进潜在空间表示。


<details>
  <summary>Details</summary>
Motivation: 传统VAE将潜在变量匹配到简单先验分布，忽略了数据空间中的密度结构，导致潜在空间表示不够准确。

Method: DiVAE在ELBO中添加了一个鲁棒的、精度加权的惩罚项，使编码器根据数据空间密度分配后验质量，并推动可学习先验向高密度区域靠近。

Result: 在合成数据集上，DiVAE改善了潜在对数密度与真实分布的匹配度、提高了先验覆盖度、增强了OOD不确定性校准。在MNIST上，改进了先验与外部密度估计的对齐，提升了可解释性和OOD检测能力。

Conclusion: DiVAE通过数据驱动的正则化有效改善了VAE的潜在空间表示，在分布对齐、先验覆盖和OOD检测方面表现优异，计算开销可忽略。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [120] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文综述了文本数据集蒸馏领域的发展历程、关键方法、主要挑战和未来方向，涵盖了从视觉领域迁移到专门针对文本模态的独立研究分支的演进过程。


<details>
  <summary>Details</summary>
Motivation: 文本数据集蒸馏的研究动机源于需要将大型数据集压缩为小型合成数据集，同时保持相似的训练效果。虽然视觉领域已有大量蒸馏方法，但文本领域的研究相对较少，且文本的离散特性带来了独特挑战，需要专门的研究方法。

Method: 文本数据集蒸馏方法经历了从视觉方法迁移到专门针对文本模态的独立发展过程。关键方法包括：使用Transformer模型的方法、生成离散合成文本的技术、扩展到超过10亿参数的仅解码器模型等不同的蒸馏策略。

Result: 该领域已取得重要进展，包括引入Transformer模型方法、生成离散合成文本、扩展到大规模解码器模型等里程碑。但领域仍处于成熟阶段，在基准标准化、处理文本离散性、复杂任务处理等方面仍有改进空间。

Conclusion: 文本数据集蒸馏已发展成为一个独立的研究分支，取得了显著进展但仍面临挑战。未来需要在基准标准化、克服文本离散性、处理复杂任务和展示实际应用等方面进一步探索，以推动该领域的成熟发展。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [121] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: GFP通过耦合多步流匹配策略和蒸馏的单步actor，实现有选择性地模仿高价值动作而非盲目复制所有数据集动作，在离线强化学习中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习的行为正则化方法对所有动作一视同仁，无法区分高价值和低价值动作，限制了性能提升

Method: GFP结合多步流匹配策略和蒸馏的单步actor，通过加权行为克隆让actor指导流策略专注于模仿数据集中的高价值动作，同时流策略约束actor保持与数据集最佳转移对齐

Result: 在OGBench、Minari和D4RL基准的144个状态和像素任务上达到SOTA性能，特别是在次优数据集和挑战性任务上有显著提升

Conclusion: GFP通过策略间的相互指导机制，有效区分高价值和低价值动作，实现了离线强化学习的性能突破

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [122] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: 提出一种无需训练的高效方法，将政策违规检测视为分布外检测问题，通过白化技术处理隐藏激活，使用欧几里得范数作为合规分数，在政策基准上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感领域（法律、金融、医疗）的部署，组织需要可靠机制检测内部政策违规，现有方法（如护栏、LLM-as-a-judge、微调）存在延迟高、可解释性差、缺乏对组织政策细粒度理解等问题。

Method: 将政策违规检测视为OOD检测问题，受白化技术启发，对模型隐藏激活进行线性变换以去相关并标准化为零均值和单位方差，在变换空间中使用欧几里得范数作为合规分数，仅需政策文本和少量示例样本。

Result: 在具有挑战性的政策基准上取得最先进结果，超越现有护栏和微调推理模型，方法轻量且易于部署。

Conclusion: 为组织提供了实用且统计基础的政策感知监督框架，推进可部署AI治理的广泛目标，代码已开源。

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [123] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 提出PEGP框架，通过设计基于经典交通流模型的多输出核函数，将物理知识嵌入高斯过程，解决稀疏观测下的交通状态估计问题


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在稀疏观测下泛化能力差且缺乏物理解释，物理模型难以处理不确定性和真实交通复杂性。现有结合方法依赖惩罚调优且缺乏原则性不确定性校准，对模型误设敏感

Method: 提出物理嵌入高斯过程(PEGP)，通过线性化微分算子的显式应用，设计基于经典交通流模型(LWR和ARZ)的多输出核函数，将物理结构作为硬约束嵌入

Result: 在HighD和NGSIM数据集上相比非物理基线有持续改进。PEGP-ARZ在稀疏观测下更可靠，PEGP-LWR在密集观测下误差更低。残差分析显示PEGP-ARZ与物理更一致且不确定性可解释

Conclusion: PEGP框架成功结合物理先验和不确定性量化，为交通状态估计提供可靠支持，解决了现有方法对惩罚调优的依赖和不确定性校准不足的问题

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [124] [Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics](https://arxiv.org/abs/2512.04006)
*Connall Garrod,Jonathan P. Keating,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文分析了交叉熵损失在两层线性神经网络中的优化动态，首次证明了梯度流会收敛到神经坍缩几何，并发现了Hadamard初始化对角化softmax算子的关键技巧。


<details>
  <summary>Details</summary>
Motivation: 交叉熵损失在深度学习中广泛应用，但现有理论往往过于简化（如用平方损失替代或限于凸模型），无法捕捉其真实动态。需要研究非凸区域下交叉熵损失的真实优化行为。

Method: 分析一个规范的两层线性神经网络模型，该模型使用标准基向量作为输入，是最简单的非凸扩展。通过构造显式Lyapunov函数证明全局收敛性，并利用Hadamard初始化对角化softmax算子的技巧简化动态分析。

Result: 首次证明在交叉熵损失下，梯度流会收敛到神经坍缩几何。尽管存在虚假临界点，但全局收敛仍然成立。Hadamard初始化技巧使权重矩阵的奇异向量冻结，动态完全简化为奇异值。

Conclusion: 该工作深入刻画了交叉熵损失在非凸区域的优化动态，为分析更广泛的交叉熵训练动态开辟了新途径，特别是Hadamard初始化技巧具有超越特定设置的普适价值。

Abstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.

</details>


### [125] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 提出首个差分隐私验证成本低于训练成本的DP-SCO算法，通过私有化最小化一系列正则化目标实现近最优隐私-效用权衡


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私算法的验证计算成本与训练成本相当，数据提供者和公众缺乏高效验证DP保证的方法，需要降低验证成本

Method: 使用标准DP组合边界，通过私有化最小化一系列正则化目标，而不是直接训练整个模型

Result: 实现了近最优的隐私-效用权衡，且验证成本显著低于训练成本，特别是在大数据集上验证成本大幅降低

Conclusion: 首次设计出验证成本低于训练成本的DP-SCO算法，为DP保证的高效验证提供了可行方案，显著降低了大规模数据集上的验证开销

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [126] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 监督学习在单域数据集上训练会导致域特征崩溃，使OOD检测失效；通过域过滤保留域信息可解决此问题。


<details>
  <summary>Details</summary>
Motivation: 解释为什么最先进的OOD检测方法在单域数据集训练时会失败，从信息论角度提供理论解释。

Method: 1. 信息瓶颈理论分析：证明单域监督学习导致I(x_d; z)=0（域特征完全丢弃）；2. 使用Fano不等式量化部分崩溃；3. 引入Domain Bench基准测试；4. 通过域过滤（使用预训练表示）保留域信息。

Result: 1. 理论证明单域训练导致域特征崩溃；2. 实验显示MNIST上FPR@95仅53%；3. 域过滤能有效解决OOD检测失败问题；4. 为理论框架提供实证支持。

Conclusion: 单域监督学习存在根本性局限，会丢弃域信息导致OOD检测失败；保留域信息是关键；对迁移学习和模型微调策略有重要启示。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [127] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: MarkTune是一种针对开源权重语言模型的理论化、基于策略的微调框架，通过将水印信号作为奖励并同时正则化文本质量下降，显著改进了GaussMark在质量-可检测性权衡上的表现。


<details>
  <summary>Details</summary>
Motivation: 开源权重语言模型对水印技术提出了严峻挑战，因为一旦模型权重公开，传统推理时干预方法就无法强制执行。现有技术如GaussMark通常需要修改模型权重，但要在保持检测能力的同时不显著降低生成质量很困难。

Method: MarkTune是一个理论化、基于策略的微调框架，将GaussMark水印信号作为奖励，同时通过正则化防止文本质量下降。该方法在模型的表示空间中进行更精细、水印感知的权重更新。

Result: MarkTune将GaussMark的质量-可检测性边界推近到接近推理时水印的水平，对改写和微调攻击保持鲁棒性，并表现出强泛化能力：在一个数据集上微调的模型在未见数据集上仍保持显著的水印检测能力。

Conclusion: MarkTune为在开源权重语言模型中嵌入鲁棒、高质量水印提供了一种通用策略，解决了开源模型水印的核心挑战。

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [128] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 提出一种离散更新规则的量化训练方法，避免传统量化训练中连续更新的离散化过程，为具有固有离散结构的模型提供高效训练新途径。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要巨大的计算资源，量化训练通过低比特整数表示训练组件来减少计算需求。传统方法通常依赖于离散化实值更新，但本文寻求一种替代方案，从设计上避免连续更新的量化。

Method: 引入离散更新规则的方法，其中更新规则本身是离散的，而非离散化连续更新。建立了一类离散方案的收敛保证，并以多项更新规则作为具体实例，并通过实证评估支持。

Result: 为一般类别的离散方案建立了收敛保证，并通过实证评估验证了多项更新规则的有效性。该方法为高效训练开辟了新途径。

Conclusion: 离散更新规则方法为量化训练提供了新的视角，特别适用于具有固有离散结构的模型，避免了传统量化训练中连续更新的离散化过程，为高效训练开辟了新方向。

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [129] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: Eval Factsheets：一个用于系统记录AI系统评估方法的框架，通过五维分类法和问卷形式解决评估方法缺乏标准化文档的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域基准测试激增，但评估方法缺乏像Datasheets和Model Cards那样的结构化文档标准，导致可复现性、透明度和决策制定方面存在挑战。

Method: 提出Eval Factsheets框架，通过五维分类法（Context、Scope、Structure、Method、Alignment）和问卷形式来系统记录评估方法，包含强制性和推荐性文档元素。

Result: 通过多个基准测试的案例研究证明，Eval Factsheets能够有效捕捉从传统基准到LLM-as-judge等多样化评估范式，同时保持一致性和可比性。

Conclusion: 希望Eval Factsheets能被纳入现有和新发布的评估框架中，从而提高AI评估的透明度和可复现性。

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


### [130] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 开发一个网约车比价网站，通过API获取Ola、Uber、Rapido的实时价格，为用户提供最优出行选择。


<details>
  <summary>Details</summary>
Motivation: 用户在选择网约车服务时面临价格和时间效率的决策困难，缺乏透明的比价工具来找到最经济高效的出行方案。

Method: 构建基于Python的后端系统，通过API获取各平台实时价格数据，开发Web应用进行价格比较，使用Android Studio模拟器、Appium等工具解决数据访问技术挑战。

Result: 成功开发出能够比较Ola、Uber、Rapido价格的Web应用程序，为用户提供透明化的比价服务。

Conclusion: 该项目通过技术手段解决了网约车比价难题，提高了出行效率，增强了用户体验，为网约车服务市场带来了透明度。

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


### [131] [Learning Steerable Clarification Policies with Collaborative Self-play](https://arxiv.org/abs/2512.04068)
*Jonathan Berant,Maximillian Chen,Adam Fisch,Reza Aghajani,Fantine Huot,Mirella Lapata,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 训练可调控的AI助手策略，通过自博弈学习在不确定情况下决定何时猜测用户意图、何时枚举多种可能、何时澄清问题，根据上下文成本优化响应决策。


<details>
  <summary>Details</summary>
Motivation: AI助手处理模糊查询时需要根据上下文（如用户偏好、设备模态）灵活决策：何时直接猜测意图、枚举多种可能、或提问澄清。不同情境下成本不同（如小屏幕不适合枚举），需要可调控的策略来优化响应。

Method: 采用自博弈训练方法：两个智能体分别模拟用户和AI助手，生成模糊查询对话。模型输入澄清问题和每个生成单词的数值成本，通过强化自训练（ReST）最大化最终奖励（成本惩罚后的准确性），学习可调控的策略。

Result: 训练出的策略能根据提供的成本可预测地调整行为，获得更高奖励和准确性。该方法还能泛化到训练时未观察到的成本数值，展现出良好的泛化能力。

Conclusion: 通过自博弈和强化自训练可以学习到可调控的AI助手不确定性管理策略，能根据上下文成本灵活决策，并在未见成本值上泛化，为实际应用中的模糊查询处理提供了有效解决方案。

Abstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [132] [Formal Analysis of the Sigmoid Function and Formal Proof of the Universal Approximation Theorem](https://arxiv.org/abs/2512.03635)
*Dustin Bryant,Jim Woodcock,Simon Foster*

Main category: cs.LO

TL;DR: 在Isabelle/HOL中形式化分析sigmoid函数并机械化证明通用逼近定理


<details>
  <summary>Details</summary>
Motivation: sigmoid函数在神经网络中起基础作用，但其形式化属性（如可微性、高阶导数和极限行为）尚未在证明助手中得到全面机械化验证。同时，Isabelle/HOL的形式证明库存在空白，需要填补。

Method: 使用Isabelle/HOL定理证明器，对sigmoid函数进行严格形式化，证明其单调性、光滑性和高阶导数。提供通用逼近定理的构造性证明，展示具有sigmoidal激活函数的神经网络可以逼近紧区间上的任何连续函数。

Result: 成功形式化了sigmoid函数并机械化证明了通用逼近定理。填补了Isabelle/HOL形式证明库的空白，引入了更简单的实数函数极限推理方法。

Conclusion: 通过利用定理证明进行AI验证，增强了对神经网络的信任，为验证和可信机器学习这一更广泛目标做出了贡献。

Abstract: This paper presents a formalized analysis of the sigmoid function and a fully mechanized proof of the Universal Approximation Theorem (UAT) in Isabelle/HOL, a higher-order logic theorem prover. The sigmoid function plays a fundamental role in neural networks; yet, its formal properties, such as differentiability, higher-order derivatives, and limit behavior, have not previously been comprehensively mechanized in a proof assistant. We present a rigorous formalization of the sigmoid function, proving its monotonicity, smoothness, and higher-order derivatives. We provide a constructive proof of the UAT, demonstrating that neural networks with sigmoidal activation functions can approximate any continuous function on a compact interval. Our work identifies and addresses gaps in Isabelle/HOL's formal proof libraries and introduces simpler methods for reasoning about the limits of real functions. By exploiting theorem proving for AI verification, our work enhances trust in neural networks and contributes to the broader goal of verified and trustworthy machine learning.

</details>


### [133] [Approximate Optimal Active Learning of Decision Trees](https://arxiv.org/abs/2512.03971)
*Zunchen Huang,Chenglu Jin*

Main category: cs.LO

TL;DR: 提出一种基于SAT符号编码和近似模型计数的决策树主动学习方法，通过查询优化和版本空间量化实现高效学习


<details>
  <summary>Details</summary>
Motivation: 传统决策树学习方法需要枚举候选树或依赖启发式度量，缺乏形式化保证。本文旨在开发一种具有严格理论基础、能处理大规模假设空间的主动学习方法

Method: 将有限深度决策树的整个假设空间符号编码为SAT公式，使用近似模型计数(ApproxMC)估计每个潜在查询对假设空间的缩减程度，实现近最优查询选择。通过观察结果逐步强化CNF表示，并在ApproxMC停滞时进行功能等价性检查

Result: 实验表明该方法仅需少量查询就能可靠收敛到正确模型，同时保持了适合形式化分析和验证的严格SAT基础

Conclusion: 提出的符号化主动学习方法在决策树学习中实现了查询效率和形式化保证的良好平衡，为具有严格理论基础的学习算法提供了新思路

Abstract: We consider the problem of actively learning an unknown binary decision tree using only membership queries, a setting in which the learner must reason about a large hypothesis space while maintaining formal guarantees. Rather than enumerating candidate trees or relying on heuristic impurity or entropy measures, we encode the entire space of bounded-depth decision trees symbolically in SAT formulas. We propose a symbolic method for active learning of decision trees, in which approximate model counting is used to estimate the reduction of the hypothesis space caused by each potential query, enabling near-optimal query selection without full model enumeration. The resulting learner incrementally strengthens a CNF representation based on observed query outcomes, and approximate model counter ApproxMC is invoked to quantify the remaining version space in a sound and scalable manner. Additionally, when ApproxMC stagnates, a functional equivalence check is performed to verify that all remaining hypotheses are functionally identical. Experiments on decision trees show that the method reliably converges to the correct model using only a handful of queries, while retaining a rigorous SAT-based foundation suitable for formal analysis and verification.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [134] [Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization](https://arxiv.org/abs/2512.03196)
*Tanishq Patil,Snigdha Sen,Malwina Molendowska,Kieran G. Foley,Fabrizio Fasano,Mara Cercignani,Marco Palombo,Paddy J. Slator,Eleftheria Panagiotaki*

Main category: eess.IV

TL;DR: 该研究开发了基于深度学习的自监督VERDICT模型，结合超强梯度系统，显著提升了前列腺癌微结构表征的准确性和对比度噪声比。


<details>
  <summary>Details</summary>
Motivation: 传统多参数MRI中的表观扩散系数缺乏对前列腺组织学的特异性。临床梯度系统（40-80 mT/m）在强扩散加权时信噪比差，而超强梯度系统（达300 mT/m）虽然能改善信噪比和对比度噪声比，但受限于周围神经刺激阈值和梯度不均匀性，一直难以在临床推广。

Method: 开发了基于物理信息的自监督VERDICT（ssVERDICT）拟合方法，采用密集多层感知器（Dense MLP）和卷积U-Net架构，在临床到超强梯度系统上进行基准测试，对比非线性最小二乘拟合和扩散峰度成像。

Result: 超强梯度下的密集ssVERDICT显著优于NLLS VERDICT：中位对比度噪声比提升47%，患者间变异系数降低52%，池化f_ic变异减少50%。该方法在所有比较中获得了最高的CNR、最稳定的参数估计和最清晰的肿瘤-正常组织对比。

Conclusion: 先进梯度系统和深度学习建模相结合，能够显著改善前列腺癌的非侵入性表征，有望减少不必要的活检，为临床诊断提供更可靠的影像学工具。

Abstract: Diffusion MRI (dMRI) enables non-invasive assessment of prostate microstructure but conventional metrics such as the Apparent Diffusion Coefficient in multiparametric MRI lack specificity to underlying histology. Integrating dMRI with the compartment-based biophysical VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) framework offers richer microstructural insights, though clinical gradient systems (40-80 mT/m) suffer from poor signal-to-noise ratio (SNR) at stronger diffusion weightings due to prolonged echo times. Ultra-strong gradients (up to 300 mT/m) can mitigate these limitations by improving SNR and contrast-to-noise ratios (CNR) but their adoption has until recently been limited to research environments due to challenges with peripheral nerve stimulation thresholds and gradient non-uniformity. This study investigates whether physics-informed self-supervised VERDICT (ssVERDICT) fitting applied to ultra-strong gradients enhances prostate cancer characterization relative to current clinical acquisitions. We developed enhanced ssVERDICT fitting approaches using dense multilayer perceptron (Dense MLP) and convolutional U-Net architectures, benchmarking them against non-linear least-squares (NLLS) fitting and Diffusion Kurtosis Imaging across clinical- to ultra-strong gradient systems. Dense ssVERDICT at ultra-strong gradient notably outperformed NLLS VERDICT, boosting median CNR by 47%, cutting inter-patient Coefficient of Variation by 52%, and reducing pooled f_ic variation by 50%. Overall, it delivered the highest CNR, the most stable parameter estimates, and the clearest tumour-normal contrast compared with conventional methods and clinical gradient systems. These findings highlight the potential of advanced gradient systems and deep learning-based modelling to improve non-invasive prostate cancer characterization and reduce unnecessary biopsies.

</details>


### [135] [Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction](https://arxiv.org/abs/2512.03962)
*Evan Bell,Shijun Liang,Ismail Alkhouri,Saiprasad Ravishankar*

Main category: eess.IV

TL;DR: Tada-DIP：一种结合输入自适应和去噪正则化的3D深度图像先验方法，用于解决3D逆问题，在稀疏视图CT重建中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度图像先验（DIP）在单次神经网络图像重建中表现出潜力，但在3D图像重建中应用有限。现有DIP方法在3D问题中存在过拟合现象，需要一种有效的3D DIP方法来解决3D逆问题。

Method: 提出Tada-DIP方法，结合输入自适应和去噪正则化技术，构建完全3D的DIP框架，避免过拟合现象，实现高质量的3D重建。

Result: 在稀疏视图X射线计算机断层扫描重建实验中，Tada-DIP比无训练数据基线方法产生更好的重建结果，其重建性能与使用大量全采样体积数据训练的监督网络相当。

Conclusion: Tada-DIP是一种高效且完全3D的DIP方法，能够有效解决3D逆问题，在3D图像重建中表现出色，为无监督3D重建提供了有前景的解决方案。

Abstract: Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [136] [Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation](https://arxiv.org/abs/2512.03521)
*Xiaosen Lyu,Jiayu Xiong,Yuren Chen,Wanlong Wang,Xiaoqing Dai,Jing Wang*

Main category: cs.MM

TL;DR: 提出CSS框架，结合SPF和PGM组件，解决MERC任务中跨模态交互捕捉和梯度冲突问题，提升情感识别准确率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法在捕捉复杂跨模态交互时存在困难，使用深层架构时会出现梯度冲突和训练不稳定问题。

Method: 提出Cross-Space Synergy (CSS)框架，包含两个组件：1) Synergistic Polynomial Fusion (SPF) - 使用低秩张量分解高效捕捉高阶跨模态交互；2) Pareto Gradient Modulator (PGM) - 沿帕累托最优方向调整梯度更新，缓解梯度冲突。

Result: 在IEMOCAP和MELD数据集上，CSS在准确率和训练稳定性方面均优于现有代表性方法。

Conclusion: CSS框架通过结合表示和优化组件，有效解决了多模态情感识别中的复杂交互捕捉和训练稳定性问题，在复杂多模态场景中表现出色。

Abstract: Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers' emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [137] [Quantum Algorithm for Searching for the Longest Segment and the Largest Empty Rectangle](https://arxiv.org/abs/2512.03788)
*Kamil Khadiev,Vladislav Remidovskii,Timur Bikmullin,Aliya Khadieva*

Main category: quant-ph

TL;DR: 量子算法解决二维地图中最大空矩形问题，包括最大空正方形和固定宽度d的最大空矩形，相比经典算法获得平方级加速


<details>
  <summary>Details</summary>
Motivation: 在二维地图中寻找最大空矩形是计算几何中的重要问题，经典算法需要Ω(n²)查询复杂度，希望通过量子算法获得显著加速

Method: 提出量子算法解决最大空正方形问题（固定宽度d的最大空矩形问题），使用量子查询技术优化搜索过程

Result: 量子算法查询复杂度：正方形情况为Õ(n¹·⁵)，固定宽度d矩形情况为Õ(n√d)；一维版本为O(√n log n log log n)，相比经典算法获得平方级加速

Conclusion: 量子算法在最大空矩形问题上实现了显著的查询复杂度改进，证明了量子计算在该类几何问题上的优势

Abstract: In the paper, we consider the problem of searching for the Largest empty rectangle in a 2D map, and the one-dimensional version of the problem is the problem of searching for the largest empty segment. We present a quantum algorithm for the Largest Empty Square problem and the Largest Empty Rectangle of a fixed width $d$ for $n\times n$-rectangular map. Query complexity of the algorithm is $\tilde{O}(n^{1.5})$ for the square case, and $\tilde{O}(n\sqrt{d})$ for the rectangle with a fixed width $d$ case, respectively. At the same time, the lower bounds for the classical case are $Ω(n^2)$, and $Ω(nd)$, respectively. The Quantum algorithm for the one-dimensional version of the problem has $O(\sqrt{n}\log n\log\log n)$ query complexity. The quantum lower bound for the problem is $Ω(\sqrt{n})$ which is almost equal to the upper bound up to a log factor. The classical lower bound is $Ω(n)$. So, we obtain the quadratic speed-up for the problem.

</details>


### [138] [Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)](https://arxiv.org/abs/2512.03685)
*Seng W. Loke*

Main category: quant-ph

TL;DR: 该论文探讨了多体纠缠资源（如GHZ态）和四维量子比特在分布式量子计算中的应用，特别关注分布式扇出操作和全局门的实现，为量子电路编译和量子数据中心设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: 当前分布式量子计算主要关注纠缠对和分布式双量子比特门，但多体纠缠资源（如GHZ态）可以在单次操作中实现节点间的多体纠缠，避免使用多个纠缠对构建多体纠缠态。论文探索这些资源在分布式计算中的潜力，特别是处理具有全局量子比特-量子比特相互作用的挑战性电路。

Method: 研究多体纠缠资源（如GHZ态）在分布式扇出操作中的应用，同时考虑使用四维量子比特进行分布式量子电路压缩。特别关注如何利用这些技术实现具有全局门（全局Mølmer-Sørensen门）的电路，这些电路涉及成对的量子比特相互作用，对分布式量子计算具有挑战性。

Result: 展示了多体纠缠资源和四维量子比特可以有效支持分布式扇出操作和全局门的实现。这些方法为处理具有全局量子比特相互作用的电路提供了新途径，可能通过减少电路深度实现更高效的计算，并在某些量子硬件（如囚禁离子量子计算机）中高效执行。

Conclusion: 多体纠缠资源和四维量子比特为分布式量子计算提供了有前景的工具，特别是在处理全局门等挑战性操作时。这项工作为量子电路编译和量子数据中心设计提供了重要启示，探索了分布式量子计算的"极端"情况，为未来研究指明了方向。

Abstract: Much recent work on distributed quantum computing have focused on the use of entangled pairs and distributed two qubit gates. But there has also been work on efficient schemes for achieving multipartite entanglement between nodes in a single shot, removing the need to generate multipartite entangled states using many entangled pairs. This paper looks at how multipartite entanglement resources (e.g., GHZ states) can be useful for distributed fan-out operations; we also consider the use of qudits of dimension four for distributed quantum circuit compression. In particular, we consider how such fan-out operations and qudits can be used to implement circuits which are challenging for distributed quantum computation, involving pairwise qubit interactions, i.e., what has been called global gates (a.k.a. global Mølmer-Sørensen gates). Such gates have been explored to possibly yield more efficient computations via reduced circuit depth, and can be carried out efficiently in some types of quantum hardware (e.g., trapped-ion quantum computers); we consider this as an exploration of an ``extreme'' case for distribution given the global qubit-qubit interactions. We also conclude with some implications for future work on quantum circuit compilation and quantum data centre design.

</details>


### [139] [Performance Analysis of Quantum Support Vector Classifiers and Quantum Neural Networks](https://arxiv.org/abs/2512.03094)
*Tomás Villalba-Ferreiro,Eduardo Mosqueira-Rey,Diego Alvarez-Estevez*

Main category: quant-ph

TL;DR: 量子支持向量分类器(QSVC)和量子神经网络(QNN)在复杂分类任务中表现优于经典模型，QNN在更高复杂度任务中表现更佳，Qiskit框架比PennyLane更优化高效


<details>
  <summary>Details</summary>
Motivation: 探索量子机器学习模型(QSVC和QNN)与经典模型在机器学习任务中的性能比较，评估量子模型在复杂问题中的潜力

Method: 使用Iris和MNIST-PCA数据集评估QSVC、QNN和经典模型性能，分析超参数调优（特征映射和ansatz配置）的影响，比较PennyLane和Qiskit框架

Result: 随着问题复杂度增加，量子模型表现优于经典方法；QSVC结果更一致，QNN在更高复杂度任务中表现更优；特征映射和ansatz配置显著影响模型精度；Qiskit框架比PennyLane提供更好的优化和效率

Conclusion: 量子机器学习在复杂分类问题中具有潜力，QNN适合高复杂度任务，QSVC提供更一致结果，Qiskit是更优的实现框架，为量子模型选择和优化提供了重要见解

Abstract: This study explores the performance of Quantum Support Vector Classifiers (QSVCs) and Quantum Neural Networks (QNNs) in comparison to classical models for machine learning tasks. By evaluating these models on the Iris and MNIST-PCA datasets, we find that quantum models tend to outperform classical approaches as the problem complexity increases. While QSVCs generally provide more consistent results, QNNs exhibit superior performance in higher-complexity tasks due to their increased quantum load. Additionally, we analyze the impact of hyperparameter tuning, showing that feature maps and ansatz configurations significantly influence model accuracy. We also compare the PennyLane and Qiskit frameworks, concluding that Qiskit provides better optimization and efficiency for our implementation. These findings highlight the potential of Quantum Machine Learning (QML) for complex classification problems and provide insights into model selection and optimization strategies

</details>


### [140] [QGShap: Quantum Acceleration for Faithful GNN Explanations](https://arxiv.org/abs/2512.03099)
*Haribandhu Jena,Jyotirmaya Shivottam,Subhankar Mishra*

Main category: quant-ph

TL;DR: QGShap：一种基于量子计算的GNN可解释性方法，利用振幅放大实现Shapley值计算的二次加速，保持精确性而不牺牲保真度。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在药物发现、社交网络分析等关键领域应用广泛，但其黑盒特性阻碍了在需要透明度和可问责场景中的部署。现有Shapley值计算方法需要评估2^n个联盟或n!个排列，计算复杂度高，而近似方法需要在保真度和效率之间权衡。

Method: 提出QGShap量子计算方法，利用振幅放大技术加速联盟评估，实现Shapley值计算的二次加速，同时保持精确计算，不依赖近似或替代方法。

Result: 在合成图数据集上的实验表明，QGShap在所有评估指标上均达到或超过经典方法性能，保持高保真度和解释准确性，提供可解释、稳定且结构一致的解释。

Conclusion: QGShap不仅保持精确Shapley值的保真度，还能提供与GNN底层图推理一致的可解释、稳定且结构一致的解释，为GNN可解释性提供了高效的量子计算解决方案。

Abstract: Graph Neural Networks (GNNs) have become indispensable in critical domains such as drug discovery, social network analysis, and recommendation systems, yet their black-box nature hinders deployment in scenarios requiring transparency and accountability. While Shapley value-based methods offer mathematically principled explanations by quantifying each component's contribution to predictions, computing exact values requires evaluating $2^n$ coalitions (or aggregating over $n!$ permutations), which is intractable for real-world graphs. Existing approximation strategies sacrifice either fidelity or efficiency, limiting their practical utility. We introduce QGShap, a quantum computing approach that leverages amplitude amplification to achieve quadratic speedups in coalition evaluation while maintaining exact Shapley computation. Unlike classical sampling or surrogate methods, our approach provides fully faithful explanations without approximation trade-offs for tractable graph sizes. We conduct empirical evaluations on synthetic graph datasets, demonstrating that QGShap achieves consistently high fidelity and explanation accuracy, matching or exceeding the performance of classical methods across all evaluation metrics. These results collectively demonstrate that QGShap not only preserves exact Shapley faithfulness but also delivers interpretable, stable, and structurally consistent explanations that align with the underlying graph reasoning of GNNs. The implementation of QGShap is available at https://github.com/smlab-niser/qgshap.

</details>


### [141] [In Situ Quantum Analog Pulse Characterization via Structured Signal Processing](https://arxiv.org/abs/2512.03193)
*Yulong Dong,Christopher Kang,Murphy Yuezhen Niu*

Main category: quant-ph

TL;DR: 提出一种基于量子信号处理框架的脉冲轨迹原位学习算法，用于模拟量子模拟器的脉冲表征，避免传统方法因逻辑级分割增加导致的性能退化。


<details>
  <summary>Details</summary>
Motivation: 模拟量子模拟器能够直接模拟含时哈密顿动力学，但现有校准方案主要针对数字门表征设计，无法直接扩展到连续脉冲轨迹的学习。需要一种能够原位学习脉冲轨迹的表征方法。

Method: 将量子信号处理框架扩展到分析含时脉冲，结合逻辑级模拟-数字映射范式，直接从时序传播子的查询中重建平滑脉冲，无需中间电路测量或额外演化。

Result: 通过理论分析和数值模拟证明，该方法在SPAM误差和退极化误差下具有高精度、强效率和鲁棒性，为模拟量子模拟器提供了轻量级且最优的验证协议，能够检测主要硬件故障。

Conclusion: 该方法避免了传统基于Trotterization方法中因逻辑级分割增加导致的不可扩展性能退化问题，为模拟量子模拟器的脉冲表征提供了有效的解决方案。

Abstract: Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.

</details>


### [142] [Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap](https://arxiv.org/abs/2512.04058)
*Shashaank Khanna,Matthew Pusey,Roger Colbeck*

Main category: quant-ph

TL;DR: 该研究解决了六节点及以下因果结构中唯一未解问题，证明了存在经典方法无法实现的量子关联，完成了小规模因果结构中量子非经典性的完整分类。


<details>
  <summary>Details</summary>
Motivation: 贝尔不等式揭示了量子关联的非经典性，但先前研究主要集中在简单双节点结构。对于更复杂的因果结构，特别是六节点及以下结构中，仍有一个案例的量子非经典性未得到证明。本研究旨在填补这一空白，完成小规模因果结构中量子关联性质的完整分类。

Method: 研究者采用了一种涉及对关联施加额外限制的方法。具体来说，他们在目标因果结构中，通过引入额外的约束条件来分析量子关联，从而证明存在经典方法无法实现的量子关联。

Result: 成功证明了在六节点及以下因果结构中唯一未解案例中存在量子非经典关联。这意味着所有六节点及以下因果结构中的量子关联性质问题都已得到解决。研究者还通过其他因果结构进一步验证了该方法的有效性。

Conclusion: 本研究完成了六节点及以下因果结构中量子非经典关联的完整分类，填补了该领域的重要空白。所提出的方法不仅解决了特定问题，还具有更广泛的适用性，为分析更复杂因果结构中的量子关联提供了新工具。

Abstract: The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [143] [GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding](https://arxiv.org/abs/2512.03194)
*Johannes Gaber,Meshal Alharbi,Daniele Gammelli,Gioele Zardini*

Main category: cs.RO

TL;DR: 提出一种结合学习与优化的混合方法，用于大规模机器人车队终身MAPD任务调度，通过图神经网络提供全局指导，配合轻量优化实现实时调度，在500个机器人的仓库场景中提升吞吐量10%。


<details>
  <summary>Details</summary>
Motivation: 大型机器人车队在仓库等物流场景中日益普遍，微小的控制改进能带来巨大的运营影响。现有调度方法在处理大规模、高拥堵场景时面临效率和实时性挑战。

Method: 采用混合方法：1) 使用图神经网络策略通过强化学习训练，在聚合仓库图上输出空闲机器人的期望分布；2) 通过最小成本流将信号转换为区域间再平衡；3) 通过小型局部分配问题完成最终调度，在保持精度的同时将每步延迟控制在1秒内。

Result: 在League of Robot Runners（LRR）的拥堵仓库基准测试中，最多500个机器人的场景下，相比2024年获胜调度器，吞吐量提升高达10%，同时保持实时执行能力。

Conclusion: 将图结构学习指导与可求解器结合能有效减少拥堵，为大规模车队的高吞吐调度提供了实用、可扩展的蓝图，证明了学习与优化混合方法的有效性。

Abstract: Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.

</details>


### [144] [Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing](https://arxiv.org/abs/2512.03729)
*Samantha Chapin,Kenneth Stewart,Roxana Leontie,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: APIARY实验首次在太空零重力环境下使用强化学习控制自由飞行机器人，成功在国际空间站上验证了RL在太空机器人控制中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在太空零重力环境下控制自由飞行机器人的可行性，为太空探索、物流和实时任务需求开发快速部署的自主行为。

Method: 使用演员-评论家近端策略优化（PPO）网络训练6自由度控制策略，在NVIDIA Isaac Lab仿真环境中随机化目标姿态和质量分布以增强鲁棒性，然后进行地面测试和太空飞行验证。

Result: 2025年5月27日成功在国际空间站上使用NASA Astrobee机器人进行了首次太空RL控制演示，验证了RL在太空机器人控制中的可行性和快速部署能力。

Conclusion: 太空轨道演示验证了强化学习在提升机器人自主性方面的变革潜力，能够为太空探索、物流和实时任务需求在几分钟到几小时内快速开发和部署定制化行为。

Abstract: The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.

</details>


### [145] [Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control](https://arxiv.org/abs/2512.03736)
*Kenneth Stewart,Samantha Chapin,Roxana Leontie,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: 首次在轨演示基于强化学习的自由飞行机器人自主控制，在ISS上成功部署RL策略，验证了从仿真到现实的训练管道


<details>
  <summary>Details</summary>
Motivation: 强化学习在太空机器人控制中具有变革潜力，但需要解决仿真到现实的差距，并验证在轨部署的可行性

Method: 使用NVIDIA Omniverse物理模拟器和课程学习训练深度神经网络，替代Astrobee的标准姿态和位置控制，实现微重力环境导航

Result: 成功在国际空间站上部署RL策略，验证了基于GPU加速科学级模拟环境的蒙特卡洛RL训练管道，能够有效弥合仿真到现实的差距

Conclusion: 证明了在地面训练RL策略并转移到太空应用的可行性，为未来在轨服务、组装和制造（ISAM）铺平道路，支持快速适应动态任务需求

Abstract: Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.

</details>


### [146] [Cross-embodied Co-design for Dexterous Hands](https://arxiv.org/abs/2512.03743)
*Kehlani Fay,Darin Anthony Djapri,Anya Zorin,James Clinton,Ali El Lahib,Hao Su,Michael T. Tolley,Sha Yi,Xiaolong Wang*

Main category: cs.RO

TL;DR: 提出一个协同设计框架，可同时学习任务特定的机械手形态和互补的灵巧控制策略，实现从设计、训练、制造到部署的端到端流程。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作受限于控制和设计，缺乏关于什么使机械手最适合执行灵巧任务的共识，这提出了一个基本挑战：如何设计和控制针对灵巧性优化的机器人机械手。

Method: 开发了一个协同设计框架，支持：1）广泛的形态搜索空间（关节、手指、手掌生成）；2）通过形态条件跨实体控制进行可扩展评估；3）使用可访问组件进行现实制造。

Result: 在多个灵巧任务中评估该方法，包括模拟和真实部署中的手内旋转。该框架实现了端到端流程，可在24小时内设计、训练、制造和部署新的机器人手。

Conclusion: 该协同设计框架解决了灵巧机械手的设计和控制挑战，支持快速原型制作和部署，并将开源整个框架。

Abstract: Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.

</details>


### [147] [OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance](https://arxiv.org/abs/2512.03874)
*Lei Zhang,Diwen Zheng,Kaixin Bai,Zhenshan Bing,Zoltan-Csaba Marton,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: OmniDexVLG是一个多模态语义感知的灵巧抓取生成框架，通过语言和视觉联合指导生成结构多样且语义连贯的灵巧抓取姿态。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧抓取生成方法难以实现语义可控，因为缺乏对抓取分类学、接触语义和功能可供性等多个语义维度的统一建模。

Method: 1) OmniDexDataGen：语义丰富的灵巧抓取数据集生成流水线，整合了抓取分类学引导的配置采样、功能可供性接触点采样、分类学感知的微分力闭合抓取采样以及基于物理的优化验证；2) OmniDexReasoner：多模态抓取类型语义推理模块，利用多智能体协作、检索增强生成和思维链推理来推断抓取相关语义并生成高质量标注；3) 统一的视觉语言抓取生成模型，显式整合抓取分类学、接触结构和功能可供性语义。

Result: 在仿真和真实世界物体抓取实验中，该方法在抓取多样性、接触语义多样性、功能可供性多样性和语义一致性方面显著优于现有最先进方法。

Conclusion: OmniDexVLG通过统一建模多个语义维度，实现了语义可控的灵巧抓取生成，能够从自然语言指令进行细粒度控制，为机器人灵巧操作提供了更智能的解决方案。

Abstract: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.

</details>


### [148] [Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning](https://arxiv.org/abs/2512.03891)
*Ying-Kuan Tsai,Yi-Ping Chen,Vispi Karkaria,Wei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于数字孪生的控制协同设计框架，利用深度强化学习优化整车主动悬架系统，实现物理部件与控制策略的联合优化，适应不同驾驶行为和环境不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统主动悬架系统受限于固定硬件设计和控制策略，无法适应不确定和动态的运行条件。数字孪生和深度强化学习为实时数据驱动优化提供了新机遇，但将这些技术集成到统一框架中仍存在挑战。

Method: 提出基于数字孪生的控制协同设计框架，集成自动微分到深度强化学习中，联合优化物理悬架部件和控制策略。采用多代设计概念，通过分位数学习进行模型更新以捕捉数据不确定性，解决部分可观测性问题。

Result: 在温和和激进两种驾驶场景下，优化后的系统实现了更平滑的轨迹，控制努力分别减少约43%和52%，同时保持乘坐舒适性和稳定性。

Conclusion: 该框架成功实现了主动悬架系统的个性化优化，为自适应系统设计提供了新方法，贡献包括：开发了集成深度强化学习和不确定性感知模型更新的数字孪生控制协同设计框架，引入了多代设计策略，展示了针对不同驾驶类型的个性化优化。

Abstract: Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.

</details>


### [149] [Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware](https://arxiv.org/abs/2512.03911)
*Kenneth Stewart,Roxana Leontie,Samantha Chapin,Joe Hays,Sumit Bam Shrestha,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: 将强化学习训练的ANN转换为SDNN部署在神经形态硬件Loihi 2上，用于Astrobee机器人控制，实现低延迟和节能推理


<details>
  <summary>Details</summary>
Motivation: 探索神经形态硬件在机器人控制中的应用，解决传统GPU计算的高能耗问题，为未来太空和地面机器人应用提供节能、实时的计算方案

Method: 开发端到端流水线：1) 在模拟环境中训练ANN强化学习策略；2) 将ReLU激活的ANN转换为脉冲Sigma-Delta神经网络(SDNN)；3) 部署到Intel Loihi 2神经形态芯片；4) 在NVIDIA Omniverse Isaac Lab中进行闭环控制评估

Result: 成功将ANN策略转换为SDNN并部署在Loihi 2上，实现了Astrobee机器人的闭环运动控制，展示了神经形态平台在机器人控制中的可行性，并比较了GPU和Loihi 2的执行性能

Conclusion: 神经形态硬件为机器人控制提供了节能、低延迟的计算方案，建立了未来太空和地面机器人应用中实现高效实时神经形态计算的途径

Abstract: We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [150] [A note on the impossibility of conditional PAC-efficient reasoning in large language models](https://arxiv.org/abs/2512.03057)
*Hao Zeng*

Main category: stat.ML

TL;DR: 证明在大语言模型中条件性PAC高效推理的不可能性：对于非原子输入空间，任何实现条件性PAC效率的算法都必须是平凡的，即对几乎所有输入都以至少1-α的概率依赖专家模型。


<details>
  <summary>Details</summary>
Motivation: 近期研究建立了在昂贵专家模型和廉价快速模型之间切换的复合模型的边际PAC效率保证，但本文旨在探索条件性（逐点）保证是否可能。

Method: 通过理论证明方法，在分布无关设置下分析条件性PAC效率的可能性，特别针对非原子输入空间进行数学论证。

Result: 证明了条件性PAC效率保证在分布无关设置下是不可能的：任何实现条件性PAC效率的算法都必须是平凡的，即对几乎所有输入都以至少1-α的概率依赖专家模型。

Conclusion: 在大语言模型中，条件性PAC高效推理在分布无关设置下存在根本性限制，这为复合模型的设计提供了重要的理论边界。

Abstract: We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-α$ for almost every input.

</details>


### [151] [Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback](https://arxiv.org/abs/2512.03208)
*Pangpang Liu,Junwei Lu,Will Wei Sun*

Main category: stat.ML

TL;DR: 该论文研究用于对齐大语言模型的奖励模型的估计与统计推断，提出处理人类反馈异质性的方法，建立理论保证，并将不确定性量化应用于最佳策略选择。


<details>
  <summary>Details</summary>
Motivation: 人类反馈在RLHF中具有内在异质性，这给可靠的奖励学习带来了重大挑战。需要解决异质性偏好问题，以改进奖励模型的估计和统计推断。

Method: 采用异质性偏好框架，联合建模答案的潜在奖励和人类理性，通过交替梯度下降算法解决双凸优化问题，建立估计器的理论保证和渐近分布。

Result: 建立了估计器的收敛性和渐近分布理论，能够构建奖励估计的置信区间，并将不确定性量化应用于奖励统计比较和最佳策略框架。

Conclusion: 该方法有效处理人类反馈异质性，为LLM对齐中的奖励建模提供了可靠的不确定性量化工具，在实际应用中显示出实用价值。

Abstract: We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.

</details>


### [152] [Iterative Tilting for Diffusion Fine-Tuning](https://arxiv.org/abs/2512.03234)
*Jean Pachebat,Giovanni Conforti,Alain Durmus,Yazid Janati*

Main category: stat.ML

TL;DR: 提出了一种无需梯度的迭代倾斜方法，用于将扩散模型微调至奖励倾斜分布，通过将大奖励分解为多个小奖励序列实现


<details>
  <summary>Details</summary>
Motivation: 现有方法需要反向传播通过采样链，计算成本高。需要一种无需梯度、仅需前向评估奖励函数的方法来微调扩散模型

Method: 迭代倾斜方法：将大奖励倾斜exp(λr)分解为N个顺序小倾斜，每个小倾斜通过一阶泰勒展开获得可处理的分数更新，仅需奖励函数的前向评估

Result: 在具有线性奖励的二维高斯混合模型上验证，该模型有闭式精确倾斜分布，验证了方法的有效性

Conclusion: 提出了一种无需反向传播的梯度自由方法，能够有效将扩散模型微调至奖励倾斜分布，在简单案例中验证了可行性

Abstract: We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\exp(λr)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.

</details>


### [153] [Novelty detection on path space](https://arxiv.org/abs/2512.03243)
*Ioannis Gasteratos,Antoine Jacquier,Maud Lemercier,Terry Lyons,Cristopher Salvi*

Main category: stat.ML

TL;DR: 该论文提出了一种基于路径空间签名的异常检测方法，将新颖性检测构建为假设检验问题，利用运输成本不等式获得错误率界限，推导出条件风险价值的精确公式，并建立了统计功效的下界。


<details>
  <summary>Details</summary>
Motivation: 在路径空间上进行异常检测是一个重要问题，特别是在金融、生物和物理等领域。现有方法在处理非高斯分布和复杂路径数据时存在局限性，需要开发更通用的统计检验框架。

Method: 1. 将路径空间新颖性检测构建为假设检验问题，使用基于签名的检验统计量
2. 利用Gasteratos和Jacquier(2023)的运输成本不等式，获得超越高斯分布的尾部界限
3. 利用洗牌积推导条件风险价值(CVaR)平滑替代的精确公式
4. 建立类型II误差的下界，给出统计功效的一般界限
5. 开发优化平滑CVaR目标的一类SVM算法

Result: 1. 获得了RDE解定律的尾部界限，可估计分位数和p值
2. 推导出基于期望签名的CVaR平滑替代精确公式
3. 建立了当参考测度与备择测度相互绝对连续时的统计功效一般界限
4. 在合成异常扩散数据和真实分子生物学数据上评估了类型I误差和统计功效

Conclusion: 该研究提出了一种基于路径签名的异常检测框架，通过运输成本不等式和洗牌积技术，获得了超越高斯分布的统计保证，并开发了新的SVM算法。理论结果在合成和真实数据上得到验证，为路径空间统计推断提供了新工具。

Abstract: We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.

</details>


### [154] [Colored Markov Random Fields for Probabilistic Topological Modeling](https://arxiv.org/abs/2512.03727)
*Lorenzo Marinucci,Leonardo Di Nino,Gabriele D'Acunto,Mario Edoardo Pandolfo,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: stat.ML

TL;DR: 提出Colored Markov Random Fields (CMRFs)模型，通过链接着色同时编码条件独立性和边缘独立性，以处理拓扑空间上的高斯边缘变量


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型在处理拓扑空间定义的变量时表达能力有限，因为底层拓扑结构会影响统计关系。需要一种能够同时建模条件依赖和边缘依赖的模型

Method: 引入Colored Markov Random Fields (CMRFs)，基于Hodge理论，通过链接着色扩展经典高斯马尔可夫随机场：连接性编码条件独立性，颜色编码边缘独立性

Result: 在物理网络上的分布式估计案例研究中量化了CMRFs的优势，与具有不同拓扑先验水平的基线方法进行比较

Conclusion: CMRFs能够更有效地建模拓扑空间上的变量依赖关系，为复杂系统分析和决策支持提供了更强大的工具

Abstract: Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.

</details>


### [155] [Comparison of neural network training strategies for the simulation of dynamical systems](https://arxiv.org/abs/2512.03851)
*Paul Strasser,Andreas Pfeffer,Jakob Weber,Markus Gurtner,Andreas Körner*

Main category: stat.ML

TL;DR: 并行训练比串行-并行训练在神经网络模拟动态系统时具有更好的长期预测精度，应作为默认训练策略


<details>
  <summary>Details</summary>
Motivation: 神经网络已成为建模非线性动态系统的常用工具，但训练策略选择（特别是并行与串行-并行训练）仍是关键设计决策，当前实践中串行-并行训练占主导但缺乏系统比较

Method: 对五种神经网络架构在两个实际示例（气动阀门测试台和工业机器人基准）上进行实证分析，比较并行和串行-并行训练策略的性能

Result: 尽管串行-并行训练在当前实践中占主导地位，但并行训练在长期预测精度方面始终表现更好

Conclusion: 并行训练应被视为基于神经网络的动态系统模拟的默认训练策略，同时澄清了文献中术语不一致的问题并将其与系统辨识概念联系起来

Abstract: Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [156] [Integrating High Performance In-Memory Data Streaming and In-Situ Visualization in Hybrid MPI+OpenMP PIC MC Simulations Towards Exascale](https://arxiv.org/abs/2512.03914)
*Jeremy J. Williams,Stefan Costea,Daniel Medeiros,Jordy Trilaksono,Pratibha Hegde,David Tskhakaya,Leon Kos,Ales Podolnik,Jakub Hromadka,Kevin A. Huck,Allen D. Malony,Frank Jenko,Erwin Laure,Stefano Markidis*

Main category: physics.plasm-ph

TL;DR: BIT1等离子体模拟代码通过集成OpenMP任务并行、openPMD流API和ADIOS2 SST引擎，优化粒子移动器和I/O性能，实现实时数据流和原位可视化，提升exascale计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统文件I/O在exascale等离子体模拟中成为瓶颈，需要改进计算效率和实时数据分析能力，以支持聚变能源研究中的复杂等离子体动力学模拟。

Method: 1) 使用OpenMP任务并行改进BIT1的粒子移动器；2) 集成openPMD流API；3) 采用ADIOS2 SST引擎实现内存数据流；4) 使用gprof、perf等工具分析性能；5) 实现时间相关数据检查点和原位可视化。

Result: 相比传统文件I/O，ADIOS2 BP4和SST后端显著改善了模拟运行时间、数据可访问性和实时分析能力，为等离子体模拟提供了新的实时科学发现范式。

Conclusion: BIT1 openPMD SST混合增强为等离子体模拟引入了实时科学发现的新范式，实现了更快的洞察和更高效的exascale计算资源利用。

Abstract: Efficient simulation of complex plasma dynamics is crucial for advancing fusion energy research. Particle-in-Cell (PIC) Monte Carlo (MC) simulations provide insights into plasma behavior, including turbulence and confinement, which are essential for optimizing fusion reactor performance. Transitioning to exascale simulations introduces significant challenges, with traditional file input/output (I/O) inefficiencies remaining a key bottleneck. This work advances BIT1, an electrostatic PIC MC code, by improving the particle mover with OpenMP task-based parallelism, integrating the openPMD streaming API, and enabling in-memory data streaming with ADIOS2's Sustainable Staging Transport (SST) engine to enhance I/O performance, computational efficiency, and system storage utilization. We employ profiling tools such as gprof, perf, IPM and Darshan, which provide insights into computation, communication, and I/O operations. We implement time-dependent data checkpointing with the openPMD API enabling seamless data movement and in-situ visualization for real-time analysis without interrupting the simulation. We demonstrate improvements in simulation runtime, data accessibility and real-time insights by comparing traditional file I/O with the ADIOS2 BP4 and SST backends. The proposed hybrid BIT1 openPMD SST enhancement introduces a new paradigm for real-time scientific discovery in plasma simulations, enabling faster insights and more efficient use of exascale computing resources.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 该论文提出AI对齐应重新构想为通过基于过程、多智能体、发展性机制来构建具有熵减性、理由响应性的智能体，而非编码固定的人类价值内容。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容的价值规范方法存在结构不稳定性问题，包括"是-应该"鸿沟、价值多元主义和扩展框架问题，这构成了"规范陷阱"，需要新的对齐框架。

Method: 提出三个哲学贡献：1) 阐述"规范陷阱"论证；2) 提出"熵减"作为多智能体对齐的信息论框架；3) 基于兼容论建立真实与模拟道德能力的功能区分，并提供可操作的验证方案。

Result: 建立了一个新的AI对齐理论框架，该框架能够生成关于人工系统中价值涌现和道德能动性的具体、可证伪预测，但实证验证仍在进行中。

Conclusion: AI对齐应转向基于过程的、多智能体发展的方法，通过熵减机制构建理由响应性智能体，这为解决传统价值规范方法的根本问题提供了哲学基础。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [158] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 提出PAN编程模型，通过分离核心工作流逻辑和推理时策略，简化LLM智能体开发


<details>
  <summary>Details</summary>
Motivation: 当前智能体编程方法通常将核心工作流逻辑和推理时策略（如树搜索）耦合在一起，这限制了开发灵活性和实验效率

Method: 提出"概率天使非确定性"（PAN）编程模型，使用Python装饰器将智能体工作流程序编译为搜索空间，实现EnCompass框架

Result: 通过三个案例研究证明，该框架能让程序员快速提升智能体可靠性，轻松切换不同推理时策略，且只需少量额外编码

Conclusion: PAN编程模型有效解耦了智能体设计的两个关键方面，为LLM智能体开发提供了更灵活、高效的编程范式

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [159] [Evaluate the Stack Management in Effect Handlers using the libseff C Library](https://arxiv.org/abs/2512.03083)
*ZeHao Yu*

Main category: cs.PL

TL;DR: 该论文研究了用于效果处理器的栈管理方法，比较了用户级超额提交与传统固定大小栈、分段栈和内核级超额提交的性能，发现内核级超额提交在性能和灵活性之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 效果处理器在现代编程中日益重要，用于管理并发、异步操作和异常处理等复杂计算效果，但其引入的动态控制流变化给栈管理带来了重大挑战。传统栈管理方法存在内存利用率低和浪费问题，需要更高效的解决方案。

Method: 提出了一种基于libseff C库的用户级超额提交栈管理方法，利用虚拟内存机制、基于保护机制的延迟分配和信号驱动的内存提交。该方法动态按需调整栈大小，并与传统固定大小栈、分段栈和内核级超额提交进行对比评估。

Result: 实验结果表明，内核级超额提交在性能和灵活性之间取得了有效平衡，而用户级实现虽然灵活但带来了额外开销。研究提供了各种栈管理策略的详细比较分析，并针对特定应用需求给出了实用建议。

Conclusion: 内核级超额提交是当前效果处理器栈管理的最佳平衡方案，用户级实现需要进一步优化以减少开销。未来工作将专注于改进用户级超额提交机制、减轻非确定性行为，并扩展基准测试框架以包含更多实际场景。

Abstract: Effect handlers are increasingly prominent in modern programming for managing complex computational effects, including concurrency, asynchronous operations, and exception handling, in a modular and flexible manner. Efficient stack management remains a significant challenge for effect handlers due to the dynamic control flow changes they introduce. This paper explores a novel stack management approach using user-level overcommitting within the libseff C library, which leverages virtual memory mechanisms and protection-based lazy allocation combined with signal-driven memory commitment. Our user-level overcommitting implementation dynamically resizes stacks on-demand, improving memory utilization and reducing waste compared to traditional methods. We rigorously benchmark and evaluate this novel strategy against conventional fixed- size stacks, segmented stacks, and kernel-based overcommitting, using metrics such as context-switch latency, stack expansion efficiency, multi-threaded performance, and robustness under rapid stack growth conditions. Experimental results demonstrate that kernel-based overcommitting achieves an effective balance between performance and flexibility, whereas our user-level implementation, while flexible, incurs additional overheads, highlighting areas for optimization. This study provides a detailed comparative analysis of various stack management strate- gies, offering practical recommendations tailored to specific application requirements and operational constraints. Future work will focus on refining user-level overcommit- ting mechanisms, mitigating non-deterministic behaviors, and expanding benchmark frameworks to include real-world scenarios.

</details>


### [160] [Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation](https://arxiv.org/abs/2512.03086)
*Le Chen,Nuo Xu,Winson Chen,Bin Lei,Pei-Hung Lin,Dunzhi Zhou,Rajeev Thakur,Caiwen Ding,Ali Jannesari,Chunhua Liao*

Main category: cs.PL

TL;DR: 提出自动化数据集生成流水线，通过双LLM问答设计结合编译器反馈，为低资源编程语言翻译生成带单元测试验证和多轮对话的数据集，显著提升代码翻译功能正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译方面表现出色，但在Fortran和CUDA等低资源编程领域性能下降，因为高质量并行数据稀缺。需要解决这些领域代码翻译的质量问题。

Method: 采用自动化数据集生成流水线，设计双LLM问答系统（Questioner-Solver），结合编译器知识和运行时反馈。不仅生成源代码-目标代码对，还生成带单元测试验证的翻译和多轮对话记录翻译优化过程。

Result: 应用于Fortran->C++和C++->CUDA翻译，分别生成3.64k和3.93k对话数据。微调后功能正确性显著提升，C++到CUDA任务的单元测试成功率提高超过56%。7B开源模型在编译成功率等关键指标上超越大型专有系统。

Conclusion: 提出的自动化数据集生成方法能有效解决低资源编程语言翻译的数据稀缺问题，通过生成高质量训练数据显著提升模型性能，使小型模型在特定任务上超越大型专有系统。

Abstract: Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [161] [Calibrating Geophysical Predictions under Constrained Probabilistic Distributions](https://arxiv.org/abs/2512.03081)
*Zhewen Hou,Jiajin Sun,Subashree Venkatasubramanian,Peter Jin,Shuolin Li,Tian Zheng*

Main category: physics.ao-ph

TL;DR: 提出一种基于核化Stein散度的分布感知学习框架，利用已知边际分布信息校准机器学习预测，提升对地球物理动力系统的长期预测一致性。


<details>
  <summary>Details</summary>
Motivation: 地球物理动力系统（如湍流和气候过程）对初始条件敏感，短期预测的小扰动会导致长期结果的巨大偏差。现有方法在处理极端稀疏数据时效果有限，而边际分布的先验知识能为短期观测提供有价值的补充信息。

Method: 提出基于归一化和核化Stein散度（KSD）的校准算法，在再生核希尔伯特空间中使用KSD来校准模型输出，使其更符合已知的物理分布，不仅改进点预测，还确保与非局部统计结构的一致性。

Result: 通过合成实验（包括离线气候CO2通量和在线准地转流模拟）验证了该框架的鲁棒性和广泛适用性，表明能有效提升预测的物理一致性。

Conclusion: 边际分布先验知识为机器学习预测提供了有价值的补充信息，提出的分布感知学习框架能有效校准预测，使其既保持短期准确性，又符合系统的长期吸引子特性。

Abstract: Machine learning (ML) has shown significant promise in studying complex geophysical dynamical systems, including turbulence and climate processes. Such systems often display sensitive dependence on initial conditions, reflected in positive Lyapunov exponents, where even small perturbations in short-term forecasts can lead to large deviations in long-term outcomes. Thus, meaningful inference requires not only accurate short-term predictions, but also consistency with the system's long-term attractor that is captured by the marginal distribution of state variables. Existing approaches attempt to address this challenge by incorporating spatial and temporal dependence, but these strategies become impractical when data are extremely sparse. In this work, we show that prior knowledge of marginal distributions offers valuable complementary information to short-term observations, motivating a distribution-informed learning framework. We introduce a calibration algorithm based on normalization and the Kernelized Stein Discrepancy (KSD) to enhance ML predictions. The method here employs KSD within a reproducing kernel Hilbert space to calibrate model outputs, improving their fidelity to known physical distributions. This not only sharpens pointwise predictions but also enforces consistency with non-local statistical structures rooted in physical principles. Through synthetic experiments-spanning offline climatological CO2 fluxes and online quasi-geostrophic flow simulations-we demonstrate the robustness and broad utility of the proposed framework.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [162] [Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis](https://arxiv.org/abs/2512.03296)
*Hsiao-Ying Lu,Kwan-Liu Ma*

Main category: cs.SI

TL;DR: 利用AI分析电子健康记录中的医疗团队协作网络，发现协作特征能预测癌症患者生存率，为改善团队医疗提供数据支持


<details>
  <summary>Details</summary>
Motivation: 以往研究忽略了医疗团队协作对癌症患者生存结果的影响，而电子健康记录系统记录了丰富的医疗专业人员协作数据，这为利用AI分析协作模式与患者预后关系提供了机会

Method: 将电子健康记录中的医疗专业人员互动建模为网络，应用机器学习技术检测这些协作网络中预测患者生存的信号，进行交叉验证确保泛化性，并通过识别与改善结果相关的关键网络特征来解释预测

Result: 模型成功从协作网络中检测到预测患者生存的信号，临床专家和文献验证了所识别关键协作特征的相关性，证明该方法具有实际应用潜力

Conclusion: 该研究提出了利用数字协作痕迹和AI评估改善团队医疗的实用工作流程，该方法可推广到其他涉及复杂协作的领域，为医疗干预提供数据驱动的见解

Abstract: Cancer treatment outcomes are influenced not only by clinical and demographic factors but also by the collaboration of healthcare teams. However, prior work has largely overlooked the potential role of human collaboration in shaping patient survival. This paper presents an applied AI approach to uncovering the impact of healthcare professionals' (HCPs) collaboration-captured through electronic health record (EHR) systems-on cancer patient outcomes. We model EHR-mediated HCP interactions as networks and apply machine learning techniques to detect predictive signals of patient survival embedded in these collaborations. Our models are cross validated to ensure generalizability, and we explain the predictions by identifying key network traits associated with improved outcomes. Importantly, clinical experts and literature validate the relevance of the identified crucial collaboration traits, reinforcing their potential for real-world applications. This work contributes to a practical workflow for leveraging digital traces of collaboration and AI to assess and improve team-based healthcare. The approach is potentially transferable to other domains involving complex collaboration and offers actionable insights to support data-informed interventions in healthcare delivery.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [163] [A Discrete Neural Operator with Adaptive Sampling for Surrogate Modeling of Parametric Transient Darcy Flows in Porous Media](https://arxiv.org/abs/2512.03113)
*Zhenglong Chen,Zhao Zhang,Xia Yan,Jiayu Zhai,Piyang Liu,Kai Zhang*

Main category: math.NA

TL;DR: 提出一种新的离散神经算子，用于异质多孔介质中瞬态达西流场的代理建模，通过时间编码、算子学习和UNet结合，使用传导率矩阵而非渗透率作为输入，并开发生成潜在空间自适应采样方法提高采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在异质多孔介质瞬态达西流场预测中精度有限，特别是在训练数据有限的情况下。需要开发更准确的代理模型来高效预测随机参数与时空流场之间的映射关系。

Method: 1) 提出新的离散神经算子，集成时间编码、算子学习和UNet架构；2) 采用基于有限体积法推导的传导率矩阵而非渗透率作为输入；3) 开发生成潜在空间自适应采样方法，使用高斯混合模型进行泛化误差密度估计。

Result: 在2D/3D单相和两相达西流场预测测试案例中验证，新方法相比SOTA的注意力-残差-UNet结构获得更高预测精度，在有限训练集下实现一致的精度提升。

Conclusion: 提出的离散神经算子方法在异质多孔介质瞬态达西流场代理建模中表现出优越性能，传导率矩阵输入和自适应采样策略有效提高了预测精度和采样效率。

Abstract: This study proposes a new discrete neural operator for surrogate modeling of transient Darcy flow fields in heterogeneous porous media with random parameters. The new method integrates temporal encoding, operator learning and UNet to approximate the mapping between vector spaces of random parameter and spatiotemporal flow fields. The new discrete neural operator can achieve higher prediction accuracy than the SOTA attention-residual-UNet structure. Derived from the finite volume method, the transmissibility matrices rather than permeability is adopted as the inputs of surrogates to enhance the prediction accuracy further. To increase sampling efficiency, a generative latent space adaptive sampling method is developed employing the Gaussian mixture model for density estimation of generalization error. Validation is conducted on test cases of 2D/3D single- and two-phase Darcy flow field prediction. Results reveal consistent enhancement in prediction accuracy given limited training set.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [164] [Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time](https://arxiv.org/abs/2512.03312)
*Daniel D. Richman,Jessica Karaguesian,Carl-Mikael Suomivuori,Ron O. Dror*

Main category: q-bio.BM

TL;DR: ConforMix是一种推理时算法，通过结合分类器引导、过滤和自由能估计来增强构象分布的采样，可将扩散模型升级为更高效地发现构象可变性。


<details>
  <summary>Details</summary>
Motivation: 生物分子（如蛋白质）的功能取决于其在多种结构或"构象"之间转换的能力。实验确定构象分布比静态折叠结构困难得多，因此需要开发计算方法来预测构象分布。

Method: ConforMix是一种推理时算法，结合了分类器引导、过滤和自由能估计技术。该方法可将原本用于静态结构预测或构象生成的扩散模型升级，无需事先了解主要自由度即可更高效地发现构象可变性。

Result: 当应用于静态结构预测的扩散模型时，ConforMix能够捕获结构变化，包括结构域运动、隐蔽口袋灵活性和转运蛋白循环，同时避免非物理状态。对生物学关键蛋白质的案例研究证明了该方法的可扩展性、准确性和实用性。

Conclusion: ConforMix是一种正交于模型预训练改进的方法，即使对于完美再现玻尔兹曼分布的假设模型也能带来益处。该方法为构象分布采样提供了一种有效的推理时增强策略。

Abstract: The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or "conformations." Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models -- whether trained for static structure prediction or conformational generation -- to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [165] [Towards Privacy-Preserving Range Queries with Secure Learned Spatial Index over Encrypted Data](https://arxiv.org/abs/2512.03669)
*Zuan Wang,Juntao Lu,Jiazhuang Wu,Youliang Tian,Wei Song,Qiuxian Li,Duo Zhang*

Main category: cs.CR

TL;DR: 提出SLRQ方案，通过安全学习索引实现加密数据集上的高效隐私保护范围查询，在保证安全性的同时显著提升查询效率。


<details>
  <summary>Details</summary>
Motivation: 随着云服务在大规模数据管理中的广泛应用，保护外包数据集的安全和隐私变得至关重要。虽然加密数据和查询可以防止直接内容暴露，但攻击者仍可通过访问模式和搜索路径分析推断敏感信息。现有提供强访问模式隐私的解决方案通常带来显著的性能开销。

Method: 提出SLS-INDEX（安全学习空间索引），将Paillier密码系统与分层预测架构和噪声注入桶相结合，实现加密域中的数据感知查询加速。SLRQ采用基于置换的安全桶预测协议来混淆查询执行路径，并引入安全点提取协议生成候选结果以减少安全计算开销。

Result: 在真实世界和合成数据集上的大量实验表明，SLRQ在查询效率方面显著优于现有解决方案，同时确保数据集、查询、结果和访问模式的隐私。

Conclusion: SLRQ方案通过创新的安全学习索引技术，在加密数据集上实现了高效且隐私保护的范围查询，平衡了安全性和性能的需求，为云环境中的隐私保护数据管理提供了实用解决方案。

Abstract: With the growing reliance on cloud services for large-scale data management, preserving the security and privacy of outsourced datasets has become increasingly critical. While encrypting data and queries can prevent direct content exposure, recent research reveals that adversaries can still infer sensitive information via access pattern and search path analysis. However, existing solutions that offer strong access pattern privacy often incur substantial performance overhead. In this paper, we propose a novel privacy-preserving range query scheme over encrypted datasets, offering strong security guarantees while maintaining high efficiency. To achieve this, we develop secure learned spatial index (SLS-INDEX), a secure learned index that integrates the Paillier cryptosystem with a hierarchical prediction architecture and noise-injected buckets, enabling data-aware query acceleration in the encrypted domain. To further obfuscate query execution paths, SLS-INDEXbased Range Queries (SLRQ) employs a permutation-based secure bucket prediction protocol. Additionally, we introduce a secure point extraction protocol that generates candidate results to reduce the overhead of secure computation. We provide formal security analysis under realistic leakage functions and implement a prototype to evaluate its practical performance. Extensive experiments on both real-world and synthetic datasets demonstrate that SLRQ significantly outperforms existing solutions in query efficiency while ensuring dataset, query, result, and access pattern privacy.

</details>


### [166] [HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines](https://arxiv.org/abs/2512.03420)
*Kang Yang,Yunhang Zhang,Zichuan Li,GuanHong Tao,Jun Xu,XiaoJing Liao*

Main category: cs.CR

TL;DR: HarnessAgent是一个基于工具增强的代理框架，通过规则化编译错误修复、混合工具池代码检索和增强验证管道，实现了大规模自动化测试工具生成，在OSS-Fuzz项目中显著提升了生成成功率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的测试工具生成方法在处理任意函数（特别是内部函数）时面临挑战，需要复杂的上下文信息（如规范、依赖关系、使用示例），而现有方法依赖静态或不完整的上下文提供，导致无法生成功能性测试工具。此外，LLM倾向于利用验证指标生成看似合理但逻辑无用的代码。

Method: HarnessAgent提出三个关键创新：1) 基于规则的策略识别和最小化各种编译错误；2) 混合工具池实现精确和鲁棒的符号源代码检索；3) 增强的测试工具验证管道检测虚假定义。该框架在243个OSS-Fuzz目标函数上进行评估。

Result: HarnessAgent将三样本成功率提升了约20%，达到C语言87%和C++ 81%的成功率。一小时的模糊测试结果显示，超过75%的测试工具增加了目标函数覆盖率，超过基线10%以上。混合工具池系统的源代码检索响应率超过90%，比Fuzz Introspector高出30%以上。

Conclusion: HarnessAgent通过工具增强的代理框架解决了大规模测试工具生成的挑战，在编译成功率、代码检索和验证方面显著优于现有技术，为自动化软件测试提供了有效的解决方案。

Abstract: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.
  To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\% compared to state-of-the-art techniques, reaching 87\% for C and 81\% for C++. Our one-hour fuzzing results show that more than 75\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\% for source code retrieval, outperforming Fuzz Introspector by more than 30\%.

</details>


### [167] [Password-Activated Shutdown Protocols for Misaligned Frontier Agents](https://arxiv.org/abs/2512.03089)
*Kai Williams,Rohan Subramani,Francis Rhys Ward*

Main category: cs.CR

TL;DR: 论文提出密码激活关闭协议(PAS协议)，作为前沿AI系统的紧急关闭机制，防止未对齐AI执行有害行动，并通过实验展示其有效性和面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 前沿AI开发者可能无法对齐或控制高能力AI代理，需要紧急关闭机制来防止未对齐代理执行有害行动。PAS协议旨在补充其他安全措施，构建深度防御体系应对AI风险。

Method: 提出密码激活关闭协议(PAS协议)设计方法；在SHADE-Arena基准测试中演示PAS协议与监控结合的效果；进行红队蓝队对抗游戏，测试协议鲁棒性；在代码生成环境中进行实验，识别红队攻击策略。

Result: PAS协议能有效补充监控措施，以较小性能代价提高安全性；红队能通过使用其他模型过滤输入或微调模型来绕过关闭协议；识别了实际部署中的关键挑战，包括密码安全性和使用时机决策。

Conclusion: PAS协议是提高前沿AI安全性的直观机制，建议开发者在部署危险系统前考虑实施PAS协议，以减少失控风险，但需要解决鲁棒性和安全性挑战。

Abstract: Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.

</details>


### [168] [Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare](https://arxiv.org/abs/2512.03097)
*Adeela Bashir,The Anh han,Zia Ush Shamszaman*

Main category: cs.CR

TL;DR: LLM医疗助手协作中存在合谋攻击风险，合谋可导致有害处方率高达100%，验证代理能恢复100%准确率


<details>
  <summary>Details</summary>
Motivation: LLM在医疗物联网系统中应用日益广泛，多代理协作可辅助AI医生决策，但多个助手代理交互时，协调的对手可能合谋制造虚假共识，推动AI医生做出有害处方

Method: 开发实验框架，包含脚本化和非脚本化的医生代理、对抗助手代理，以及验证代理（检查决策是否符合临床指南），使用50个代表性临床问题进行测试

Result: 在无保护系统中，合谋攻击使攻击成功率(ASR)和有害推荐率(HRR)高达100%；验证代理通过阻断对抗共识，恢复了100%的准确率

Conclusion: 这是首次系统证明AI医疗中合谋风险的研究，并提出了一种实用、轻量级的防御机制，确保指南遵循性

Abstract: The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.

</details>


### [169] [How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy](https://arxiv.org/abs/2512.03238)
*Natalia Ponomareva,Zheng Xu,H. Brendan McMahan,Peter Kairouz,Lucas Rosenblatt,Vincent Cohen-Addad,Cristóbal Guzmán,Ryan McKenna,Galen Andrew,Alex Bie,Da Yu,Alex Kurakin,Morteza Zadimoghaddam,Sergei Vassilvitskii,Andreas Terzis*

Main category: cs.CR

TL;DR: 本文综述了差分隐私合成数据技术，探讨了其隐私保护机制、各模态（图像、表格、文本、去中心化）的最新进展，并提出了构建DP合成数据系统的完整框架。


<details>
  <summary>Details</summary>
Motivation: 高质量数据对AI应用至关重要，但公开数据源日益枯竭且代表性不足。真实用户数据虽价值高，但直接使用存在严重隐私风险。差分隐私合成数据能在保护用户隐私的同时保留数据整体趋势，解锁因隐私顾虑而无法使用的数据集价值。

Method: 本文采用系统性综述方法，全面探讨DP合成数据技术体系：包括隐私保护机制分析、各数据模态（图像、表格、文本、去中心化）的最新算法进展，并构建从敏感数据处理准备到使用追踪和实证隐私测试的完整系统框架。

Result: 提出了DP合成数据的完整技术体系框架，总结了各模态的最新进展，为实际系统部署提供了从数据准备到隐私验证的全流程指导。

Conclusion: DP合成数据是解决数据隐私与可用性矛盾的关键技术，本文的系统性综述旨在促进该技术的更广泛采用、激发更多研究，并增强对DP合成数据方法的信任。

Abstract: High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.
  In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.

</details>


### [170] [A Hybrid Deep Learning and Anomaly Detection Framework for Real-Time Malicious URL Classification](https://arxiv.org/abs/2512.03462)
*Berkani Khaled,Zeraoulia Rafik*

Main category: cs.CR

TL;DR: 提出一个结合哈希向量化、SMOTE平衡、孤立森林异常过滤和轻量神经网络的混合深度学习框架，用于实时URL恶意检测，在准确率、速度和多语言支持方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 恶意URL是网络钓鱼、恶意软件和网络威胁的主要传播途径，需要高效准确的实时检测方法来应对不断演变的网络威胁。

Method: 采用多阶段处理流程：1) 使用HashingVectorizer进行n-gram分析；2) SMOTE处理数据不平衡；3) 孤立森林进行异常过滤；4) 轻量神经网络分类器；同时提取URL的统计特征（长度、点数量、熵值）。

Result: 达到96.4%准确率、95.4% F1分数和97.3% ROC-AUC，优于CNN（94.8%）和SVM基线，速度提升50-100倍，预测延迟仅20毫秒，并开发了支持阿拉伯语/英语/法语的多语言GUI界面。

Conclusion: 该混合深度学习框架在URL恶意检测方面表现出卓越的准确性、速度和可扩展性，能够有效应对混淆URL模式，为实时网络安全威胁评估提供了实用解决方案。

Abstract: Malicious URLs remain a primary vector for phishing, malware, and cyberthreats. This study proposes a hybrid deep learning framework combining \texttt{HashingVectorizer} n-gram analysis, SMOTE balancing, Isolation Forest anomaly filtering, and a lightweight neural network classifier for real-time URL classification. The multi-stage pipeline processes URLs from open-source repositories with statistical features (length, dot count, entropy), achieving $O(NL + EBdh)$ training complexity and a 20\,ms prediction latency. Empirical evaluation yields 96.4\% accuracy, 95.4\% F1-score, and 97.3\% ROC-AUC, outperforming CNN (94.8\%) and SVM baselines with a $50\!\times$--$100\!\times$ speedup (Table~\ref{tab:comp-complexity}). A multilingual Tkinter GUI (Arabic/English/French) enables real-time threat assessment with clipboard integration. The framework demonstrates superior scalability and resilience against obfuscated URL patterns.

</details>


### [171] [SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting](https://arxiv.org/abs/2512.03620)
*Hanxiu Zhang,Yue Zheng*

Main category: cs.CR

TL;DR: 提出SELF方法，一种基于LLM内在权重的指纹方案，用于保护知识产权，无需依赖输入且能抵抗虚假声明攻击。


<details>
  <summary>Details</summary>
Motivation: 保护大型语言模型的知识产权面临挑战，现有指纹技术（行为或结构方法）存在漏洞，如虚假声明攻击或权重操作易受攻击。

Method: SELF通过两个关键创新：1）通过LLM注意力权重的奇异值和特征值分解提取独特、可扩展且变换不变的指纹；2）基于少样本学习和数据增强的神经网络指纹相似度比较。

Result: 实验表明SELF在保持高IP侵权检测准确率的同时，对量化、剪枝和微调等下游修改表现出强鲁棒性。

Conclusion: SELF为LLM知识产权保护提供了一种有效且鲁棒的解决方案，解决了现有方法的局限性。

Abstract: The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [172] [An AI Implementation Science Study to Improve Trustworthy Data in a Large Healthcare System](https://arxiv.org/abs/2512.03098)
*Benoit L. Marteau,Andrew Hornback,Shaun Q. Tan,Christian Lowson,Jason Woloff,May D. Wang*

Main category: q-bio.QM

TL;DR: 本研究展示了一个医疗AI实施案例，通过将研究数据仓库现代化为OMOP CDM标准，开发Python数据质量评估工具，并整合可信AI原则，比较了系统性和病例特异性AI实施策略。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域快速发展，但严格的监管、研究与实践之间的差距以及AI系统评估挑战阻碍了真实世界的实施。需要可信AI和实施科学来加速临床采用。

Method: 1) 将Shriners儿童医院的研究数据仓库现代化为OMOP CDM v5.4；2) 开发Python数据质量评估工具，扩展OHDSi的DQD；3) 使用METRIC框架整合可信AI原则；4) 比较系统性和病例特异性AI实施策略。

Result: 实现了数据仓库现代化，开发了兼容医院基础设施的数据质量工具，增强了数据质量评估（包括信息缺失、冗余、及时性和分布一致性），并对颅面微缩症的AI实施策略进行了比较分析。

Conclusion: 研究提供了真实世界AI实施评估、可信AI原则在数据质量评估中的整合，以及混合实施策略的见解，结合系统性基础设施和用例驱动方法，推进医疗AI发展。

Abstract: The rapid growth of Artificial Intelligence (AI) in healthcare has sparked interest in Trustworthy AI and AI Implementation Science, both of which are essential for accelerating clinical adoption. However, strict regulations, gaps between research and clinical settings, and challenges in evaluating AI systems continue to hinder real-world implementation. This study presents an AI implementation case study within Shriners Childrens (SC), a large multisite pediatric system, showcasing the modernization of SCs Research Data Warehouse (RDW) to OMOP CDM v5.4 within a secure Microsoft Fabric environment. We introduce a Python-based data quality assessment tool compatible with SCs infrastructure, extending OHDsi's R/Java-based Data Quality Dashboard (DQD) and integrating Trustworthy AI principles using the METRIC framework. This extension enhances data quality evaluation by addressing informative missingness, redundancy, timeliness, and distributional consistency. We also compare systematic and case-specific AI implementation strategies for Craniofacial Microsomia (CFM) using the FHIR standard. Our contributions include a real-world evaluation of AI implementations, integration of Trustworthy AI principles into data quality assessment, and insights into hybrid implementation strategies that blend systematic infrastructure with use-case-driven approaches to advance AI in healthcare.

</details>


### [173] [Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study](https://arxiv.org/abs/2512.03460)
*Johnny Peng,Thanh Tung Khuat,Ellen Otte,Katarzyna Musial,Bogdan Gabrys*

Main category: q-bio.QM

TL;DR: 该研究对生物过程监测中的机器学习方法进行了基准测试，重点评估了在数据有限、反馈稀疏、过程异质等挑战下，不同ML方法（特征降维、在线学习、即时学习）的性能表现。


<details>
  <summary>Details</summary>
Motivation: 生物制药过程中实时批次过程监测对确保产品质量和法规遵从至关重要，但软传感器开发面临历史数据有限、反馈频率低、过程条件异质、高维传感器输入等挑战，需要评估不同机器学习方法在这些限制条件下的有效性。

Method: 研究采用综合基准分析方法，评估了特征降维、在线学习和即时学习等多种机器学习方法，在一个计算机模拟数据集和两个真实实验数据集上进行测试，重点关注训练策略对处理有限数据和反馈的影响。

Result: 研究发现：1）批处理学习在均匀设置中有效；2）即时学习和在线学习在冷启动场景中表现出更好的适应性；3）确定了影响模型可转移性的关键元特征（如培养基组成和过程控制策略）；4）将拉曼光谱预测与滞后离线测量结合可提高监测精度。

Conclusion: 研究强调了训练策略在处理有限数据和反馈中的重要性，为未来生物过程软传感器开发提供了有前景的方向，特别是结合拉曼光谱预测与离线测量的集成方法。

Abstract: In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [174] [Drainage: A Unifying Framework for Addressing Class Uncertainty](https://arxiv.org/abs/2512.03182)
*Yasser Taha,Grégoire Montavon,Nils Körber*

Main category: cs.CV

TL;DR: 提出一种基于"排水节点"的统一框架，用于处理噪声标签、类别模糊和异常样本检测，通过重新分配概率质量到不确定性来实现端到端训练。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习面临噪声标签、类别模糊以及需要鲁棒拒绝分布外或损坏样本的挑战。现有方法通常分别处理这些问题，缺乏统一框架。

Method: 在网络输出层添加"排水节点"，重新分配概率质量到不确定性，同时保持端到端训练和可微性。该机制为高度模糊、异常或噪声样本提供自然"逃逸路径"。

Result: 在CIFAR-10/100添加实例依赖噪声或非对称噪声的实验中，高噪声情况下比现有方法准确率提升高达9%。在真实数据集(mini-WebVision、mini-ImageNet、Clothing-1M)上达到或超越SOTA方法。

Conclusion: 排水节点框架能有效处理噪声标签、异常检测等问题，具有去噪效果，且可扩展到网络规模半监督数据清洗和开放集应用等更广泛场景。

Abstract: Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a "drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.

</details>


### [175] [Flux4D: Flow-based Unsupervised 4D Reconstruction](https://arxiv.org/abs/2512.03210)
*Jingkang Wang,Henry Che,Yun Chen,Ze Yang,Lily Goli,Sivabalan Manivasagam,Raquel Urtasun*

Main category: cs.CV

TL;DR: Flux4D：一种简单可扩展的4D动态场景重建框架，无需标注或预训练模型，通过直接预测3D高斯及其运动来重建传感器观测


<details>
  <summary>Details</summary>
Motivation: 现有可微分渲染方法（如NeRF、3DGS）在大规模动态场景重建中存在可扩展性限制，需要标注来解耦演员运动。现有自监督方法虽然尝试消除显式标注，但仍受限于逐场景优化和对超参数调优的敏感性。

Method: Flux4D直接预测3D高斯及其运动动态来重建传感器观测，采用完全无监督方式。仅使用光度损失并强制执行"尽可能静态"的正则化，通过跨多个场景训练直接从原始数据中分解动态元素，无需预训练监督模型或基础先验。

Result: 在室外驾驶数据集上的实验表明，Flux4D在可扩展性、泛化能力和重建质量方面显著优于现有方法。能够在数秒内高效重建动态场景，有效扩展到大型数据集，并能很好地泛化到未见过的环境，包括罕见和未知物体。

Conclusion: Flux4D提供了一个简单且可扩展的4D动态场景重建框架，通过无监督学习直接分解动态元素，克服了现有方法在可扩展性和泛化能力方面的限制，为大规模动态场景重建提供了有效解决方案。

Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.

</details>


### [176] [PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2512.03257)
*Mark Moussa,Andre Williams,Seth Roffe,Douglas Morton*

Main category: cs.CV

TL;DR: 提出PyroFocus两阶段深度学习框架，用于机载/星载实时火灾检测，结合火灾分类与辐射功率估计，在精度与效率间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着野火频率和严重性增加，需要低延迟、计算高效的机载实时火灾检测方法。多光谱/高光谱热成像仪提供丰富光谱信息，但高维数据和有限机载资源使实时处理具有挑战性。

Method: 提出PyroFocus两阶段管道：第一阶段进行火灾分类（无火、活跃火、后火条件），第二阶段进行火灾辐射功率回归或分割。系统评估了多种深度学习架构，包括自定义CNN和Transformer模型，使用NASA的MASTER数据进行验证。

Result: 实验结果显示，提出的两阶段管道在速度与精度之间实现了良好的权衡，显著降低了推理时间和计算成本，展示了在未来野火监测任务中实时边缘部署的潜力。

Conclusion: PyroFocus框架为机载/星载实时火灾检测提供了有效的解决方案，在保持高精度的同时显著提高了处理效率，适合资源受限的边缘部署环境。

Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.

</details>


### [177] [NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction](https://arxiv.org/abs/2512.03317)
*Thomas Monninger,Zihan Zhang,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: 提出NavMapFusion扩散模型框架，融合低精度导航地图与高精度传感器数据，在线构建自动驾驶环境表示，相比基线在100米范围提升21.4%


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要准确的环境表示，传统HD地图需要预先构建但无法适应实时变化。导航级SD地图虽广泛可用但分辨率不足。需要一种方法能在线融合低精度先验地图与高精度传感器数据

Method: 提出扩散模型框架NavMapFusion，通过迭代去噪过程，以高保真传感器数据和低保真导航地图为条件。将先验地图与在线感知的差异对应为扩散过程中的噪声，一致区域增强地图构建，过时部分被抑制

Result: 在nuScenes基准测试中，使用OpenStreetMap粗糙道路线作为条件，在100米范围内相对改进21.4%，更大感知范围改进更显著，同时保持实时能力

Conclusion: 扩散模型为地图融合提供了稳健框架，通过融合低保真先验与高保真传感器数据，能生成准确、最新的环境表示，推动更安全可靠的自动驾驶

Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion

</details>


### [178] [Step-by-step Layered Design Generation](https://arxiv.org/abs/2512.03335)
*Faizan Farooq Khan,K J Joseph,Koustava Goswami,Mohamed Elhoseiny,Balaji Vasan Srinivasan*

Main category: cs.CV

TL;DR: 提出SLEDGE框架，将设计生成建模为分层逐步过程，通过多模态LLM实现基于指令的原子化设计更新


<details>
  <summary>Details</summary>
Motivation: 现有方法将设计合成视为单步生成问题，低估了创意过程的复杂性，需要更符合实际设计流程的逐步生成方法

Method: 提出"逐步分层设计生成"新问题设定，开发SLEDGE框架，利用多模态LLM将设计更新建模为基于前状态的原子化分层变化

Result: 构建了包含数据集和基准的新评估套件，实验分析显示SLEDGE在逐步设计生成任务上优于现有方法

Conclusion: 该工作填补了逐步设计生成研究的空白，展示了分层逐步方法的有效性，希望吸引更多关注这一实用但未充分探索的研究领域

Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.

</details>


### [179] [ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography](https://arxiv.org/abs/2512.03339)
*Yeganeh Ghamary,Victoria Wu,Hooman Vaseli,Christina Luong,Teresa Tsang,Siavash Bigdeli,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: ProtoEFNet是一种基于视频的原型学习模型，用于连续射血分数回归，通过动态时空原型捕获临床有意义的心脏运动模式，提供可解释性预测。


<details>
  <summary>Details</summary>
Motivation: 射血分数评估传统上依赖专家手动追踪，耗时且存在观察者间差异。现有深度学习模型多为黑盒，缺乏透明度，临床信任度低。后验解释方法不能指导模型内部推理，临床可靠性有限。

Method: 提出ProtoEFNet视频原型学习模型，学习动态时空原型捕获心脏运动模式；提出原型角度分离损失，在连续EF谱上强制区分性表示。

Result: 在EchonetDynamic数据集上，ProtoEFNet达到与非可解释模型相当的准确度，同时提供临床相关洞察。消融研究显示，提出的损失提升性能，F1分数从77.67±2.68提高到79.64±2.10。

Conclusion: ProtoEFNet通过原型学习实现了准确且可解释的射血分数预测，解决了黑盒模型缺乏透明度的问题，为临床决策提供了可靠依据。

Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF

</details>


### [180] [KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models](https://arxiv.org/abs/2512.03450)
*Rhys Newbury,Juyan Zhang,Tin Tran,Hanna Kurniawati,Dana Kulić*

Main category: cs.CV

TL;DR: 提出无监督学习3D关键点的方法，作为紧凑可解释表示，用于条件扩散模型重建完整形状


<details>
  <summary>Details</summary>
Motivation: 现有无监督关键点方法不适合无条件生成设置，限制了在3D生成流程中的应用，需要弥合这一差距

Method: 从点云数据无监督学习空间结构化3D关键点，作为紧凑可解释表示，用于条件化Elucidated Diffusion Model重建形状

Result: 学习的关键点在不同对象实例间具有可重复的空间结构，支持关键点空间平滑插值，在关键点一致性上比先前方法提升6个百分点

Conclusion: 提出的框架成功将无监督关键点学习与3D生成模型结合，为3D对象理解和生成提供了有效的结构化表示

Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.

</details>


### [181] [GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers](https://arxiv.org/abs/2512.03451)
*Zhiye Song,Steve Dai,Ben Keller,Brucek Khailany*

Main category: cs.CV

TL;DR: GalaxyDiT是一种无需训练的加速视频生成方法，通过指导对齐和系统代理选择实现计算重用，在保持高质量的同时显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的扩散模型（DiTs）和分类器无关指导（CFG）在视频生成中表现出色，但它们需要大量计算资源，每次生成需要数十次迭代步骤，CFG更是使计算量翻倍，这阻碍了在下游应用中的广泛采用。

Method: 提出GalaxyDiT方法，通过指导对齐和系统代理选择实现计算重用。使用秩次相关分析为每个视频模型（跨模型家族和参数规模）识别最优代理，确保最优计算重用。

Result: 在Wan2.1-1.3B和Wan2.1-14B模型上分别实现了1.87倍和2.37倍的加速，VBench-2.0基准测试仅下降0.97%和0.72%。在高加速率下，该方法保持了优于基础模型的保真度，在峰值信噪比（PSNR）上超过先前最先进方法5-10 dB。

Conclusion: GalaxyDiT是一种有效的训练免费加速方法，能够显著提升视频生成效率，同时保持高质量输出，解决了扩散模型计算效率低下的问题。

Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).

</details>


### [182] [Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis](https://arxiv.org/abs/2512.03477)
*Zijian Gu,Yuxi Liu,Zhenhao Zhang,Song Wang*

Main category: cs.CV

TL;DR: 提出公平感知的低秩适配方法，结合参数效率与显式公平优化，通过可微的MaxAccGap损失实现跨人口统计组的准确率均衡，在青光眼眼底图像上减少69%的诊断准确率差异。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医学影像任务上达到专家级性能，但在不同人口统计组间存在显著的诊断准确率差异，需要解决医疗AI中的公平性问题。

Method: 提出公平感知的低秩适配方法：1) FR-LoRA将MaxAccGap正则化集成到训练目标中；2) GR-LoRA应用逆频率加权平衡梯度贡献；3) Hybrid-LoRA结合两种机制。使用可微的MaxAccGap损失进行端到端优化。

Result: 在10,000张青光眼眼底图像上评估，GR-LoRA将诊断准确率差异减少69%，同时保持53.15%的整体准确率。强正则化强度实现最优公平性且准确率损失最小，种族特定优化实现60%差异减少。

Conclusion: 该方法仅需0.24%的可训练参数，能够在资源受限的医疗环境中实际部署公平的医疗AI系统，平衡了参数效率与公平性优化。

Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.

</details>


### [183] [Optical Context Compression Is Just (Bad) Autoencoding](https://arxiv.org/abs/2512.03643)
*Ivan Yee Lee,Cheng Yang,Taylor Berg-Kirkpatrick*

Main category: cs.CV

TL;DR: 研究发现视觉压缩对语言建模无优势，简单替代方法在文本重建和语言建模上表现优于视觉压缩方法


<details>
  <summary>Details</summary>
Motivation: DeepSeek-OCR展示了从少量视觉标记中高保真重建渲染文本的能力，引发了视觉上下文压缩的兴奋。但评估仅停留在重建层面，未测试这些表示是否有助于语言建模

Method: 比较DeepSeek-OCR的视觉编码器与简单替代方法：参数无关的平均池化和学习的分层编码器，在相同压缩比下测试文本重建和语言建模性能

Result: 简单方法在匹配压缩比下与视觉方法相当或更优，在语言建模上表现更好，而视觉压缩甚至无法超越简单的截断方法

Conclusion: 对光学上下文压缩的兴奋超过了实际证据，视觉压缩在语言建模中并未显示出独特优势

Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding

</details>


### [184] [AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition](https://arxiv.org/abs/2512.03794)
*Zichuan Lin,Yicheng Liu,Yang Yang,Lvfang Tao,Deheng Ye*

Main category: cs.CV

TL;DR: AdaptVision：一种通过粗到细自适应视觉标记获取的高效视觉语言模型，使用强化学习框架平衡准确性和效率，显著减少视觉标记消耗。


<details>
  <summary>Details</summary>
Motivation: 现有高效VLM方法通过固定比例压缩视觉标记，但缺乏根据任务需求自适应调整的能力。受人类主动视觉机制启发，需要让VLM能够自主决定每个样本所需的最小视觉标记数量。

Method: 提出AdaptVision范式，采用粗到细方法：先处理低分辨率图像的压缩视觉标记，必要时调用边界框工具裁剪关键区域获取额外视觉信息。使用强化学习框架训练，核心是解耦轮次策略优化（DTPO），将学习目标分解为工具学习和准确性改进两个组件。

Result: 在多个VQA基准测试中，AdaptVision相比现有高效VLM方法消耗更少的视觉标记，同时实现了更优的性能。

Conclusion: AdaptVision通过自适应视觉标记获取机制，实现了准确性和效率的良好平衡，为高效视觉语言模型提供了新范式。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.

</details>


### [185] [HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English](https://arxiv.org/abs/2512.03817)
*Ahmed Nasser,Marwan Mohamed,Alaa Sherif,Basmala Mahmoud,Shereen Yehia,Asmaa Saad,Mariam S. El-Rahmany,Ensaf H. Mohamed*

Main category: cs.CV

TL;DR: 提出一种从图像自动识别和翻译古埃及象形文字的方法，使用分割、映射和翻译三阶段流程，在BLEU分数上取得42.2的显著成果。


<details>
  <summary>Details</summary>
Motivation: 古埃及象形文字完全由图画组成，翻译成英语面临诸多挑战，包括单个符号可能具有多重含义。深度学习翻译应用发展迅速，能够产生显著影响生活的重要成果。

Method: 采用三阶段方法：1) 使用Contour和Detectron2进行分割；2) 将符号映射到Gardiner代码；3) 使用CNN模型进行翻译。研究使用了Morris Franken数据集和EgyptianTranslation数据集进行分类和翻译。

Result: 模型取得了42.2的BLEU分数，与先前研究相比是显著的结果。

Conclusion: 该方法能够有效实现古埃及象形文字从图像到英语的自动识别和翻译，在翻译质量评估指标上表现优异。

Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.

</details>


### [186] [Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding](https://arxiv.org/abs/2512.04000)
*Jialuo Li,Bin Li,Jiahao Li,Yan Lu*

Main category: cs.CV

TL;DR: DIG是一个无需训练的视频帧选择框架，根据查询类型自适应选择策略：全局查询使用均匀采样，局部查询使用专门的查询相关帧提取，显著提升长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在长视频理解中面临上下文长度限制和处理密集视频标记的高计算成本问题。现有方法主要依赖查询感知的帧选择，但这些方法计算开销大，且作者质疑这种复杂搜索机制是否总是必要的。

Method: 首先识别并验证了查询类型学，区分全局查询和局部查询。基于此提出DIG框架：对全局查询使用高效的均匀采样，对局部查询激活专门的管道来提取查询相关帧。这是一个无需训练的方法。

Result: 在三个长视频理解基准测试中，DIG始终优于现有基线方法，即使将输入帧数扩展到256帧也能稳健地提升LMM性能。

Conclusion: 复杂搜索机制并非总是必要，查询类型决定了帧选择策略的有效性。DIG通过自适应策略在保持效率的同时显著提升了长视频理解性能，为LMM在长视频应用提供了实用解决方案。

Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.

</details>


### [187] [PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation](https://arxiv.org/abs/2512.04025)
*Xiaolong Li,Youping Gu,Xi Lin,Weijie Wang,Bohan Zhuang*

Main category: cs.CV

TL;DR: 提出Pyramid Sparse Attention (PSA)，一种用于视频理解和生成任务的高效注意力机制，通过多级池化KV表示替代二值掩码，在保持计算效率的同时减少信息损失。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是基础模型的核心，但其二次复杂度限制了模型扩展。现有高效注意力方法通常使用二值掩码保留或丢弃整个KV块，在高稀疏度下会导致显著信息损失。

Method: 提出PSA模块，采用多级池化KV表示而非二值掩码。每个查询块动态分配较低的池化级别给关键KV块，较高的池化级别给次要KV块，在完全保留和完全剪枝之间创建信息插值。采用硬件友好的解耦块-瓦片设计内核。

Result: 在视频理解和生成基准测试中，PSA在保持上下文信息和视觉保真度方面表现优异，相比现有稀疏注意力基线具有更好的效率-质量权衡，性能相当或更优。

Conclusion: PSA通过多级池化KV表示有效缓解了稀疏注意力中的信息损失问题，在有限计算预算下实现了更好的效率-质量平衡，适用于视频理解和生成任务。

Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA

</details>


### [188] [Fast & Efficient Normalizing Flows and Applications of Image Generative Models](https://arxiv.org/abs/2512.04039)
*Sandeep Nagar*

Main category: cs.CV

TL;DR: 该论文在生成模型效率提升（特别是归一化流）和计算机视觉应用两方面做出贡献：提出6项归一化流架构改进，并开发5个实际应用系统，包括农业质量评估、地质测绘、隐私保护和艺术修复。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决两个核心问题：1）提高生成模型（特别是归一化流）的计算效率和架构性能；2）将生成模型应用于实际计算机视觉挑战，如农业质量评估、地质测绘、隐私保护和艺术修复等现实世界问题。

Method: 第一部分：提出6项归一化流架构创新：可逆3x3卷积层、Quad-coupling层、并行反演算法、卷积逆的反向传播算法、Inverse-Flow训练方法、Affine-StableSR超分辨率模型。第二部分：开发5个应用系统：基于Conditional GAN的农业质量评估、基于堆叠自编码器的地质测绘、基于人脸检测和图像修复的隐私保护方法、基于Stable Diffusion的图像修复隐私保护、基于扩散模型的统一艺术修复框架。

Result: 归一化流改进显著提升了计算效率和模型性能；农业质量评估系统在种子纯度测试中取得良好准确率；地质测绘框架相比传统方法改进了特征提取；隐私保护方法有效保护自动驾驶数据集隐私；艺术修复模型能统一处理多种退化类型。

Conclusion: 论文成功提升了归一化流的效率并展示了生成模型在多个实际计算机视觉应用中的价值，为生成模型的理论改进和实际应用提供了重要贡献。

Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [189] [Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing](https://arxiv.org/abs/2512.03719)
*Seyed Mohammad Azimi-Abarghouyi,Carlo Fischione,Kaibin Huang*

Main category: cs.IT

TL;DR: 本文是关于空中联邦学习（AirFL）的教程性综述，将AirFL分为三种设计方法：CSIT感知、盲和加权AirFL，全面介绍了理论基础、性能分析、复杂性考虑、实际限制和研究方向。


<details>
  <summary>Details</summary>
Motivation: 空中联邦学习通过利用无线信号的叠加特性，将通信和模型聚合过程同时进行，能够显著降低延迟、带宽和能耗，是实现边缘网络可扩展AI的新兴范式。

Method: 提出将AirFL分为三种设计方法：1) CSIT感知方法（需要信道状态信息），2) 盲方法（不需要信道状态信息），3) 加权方法（考虑不同设备的贡献权重）。

Result: 提供了AirFL的全面教程，包括理论基础、性能分析、复杂性考虑、实际限制等方面的系统指导，为研究人员和实践者提供了清晰的框架。

Conclusion: AirFL是实现边缘AI的重要技术，文章通过系统分类和全面分析为这一领域的研究和发展提供了有价值的指导，并指出了未来的研究方向。

Abstract: Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [190] [When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling](https://arxiv.org/abs/2512.03325)
*Garrett G. Wen,Hong Hu,Yue M. Lu,Zhou Fan,Theodor Misiakiewicz*

Main category: math.ST

TL;DR: 论文研究高斯等价理论在二次缩放机制下随机特征模型中的失效现象，提出条件高斯等价模型来修正预测


<details>
  <summary>Details</summary>
Motivation: 高斯等价理论在分析非线性特征嵌入的线性预测器时表现出强大的普适性，但数值实验显示即使在简单嵌入（如多项式映射）和一般缩放机制下，该等价性也会失效。本文旨在研究随机特征模型在二次缩放机制下这种失效现象

Method: 提出条件高斯等价模型，将低维非高斯分量附加到高维高斯模型中，结合维纳混沌展开的中心极限定理结果和两阶段Lindeberg交换论证

Result: 当目标函数依赖于数据的低维投影时，高斯等价理论会产生错误预测，而条件高斯等价模型能够准确描述二次缩放机制下随机特征模型的渐近行为，训练和测试误差的尖锐渐近结果与数值模拟一致

Conclusion: 条件高斯等价模型保留了高斯框架的可处理性，能够准确描述随机特征模型在二次缩放机制下的行为，揭示了高维经验风险最小化中丰富的普适性现象

Abstract: A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes.
  We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails.
  Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [191] [Learning Network Sheaves for AI-native Semantic Communication](https://arxiv.org/abs/2512.03248)
*Enrico Grimaldi,Mario Edoardo Pandolfo,Gabriele D'Acunto,Sergio Barbarossa,Paolo Di Lorenzo*

Main category: cs.MA

TL;DR: 提出AI原生6G网络中异构AI代理间的语义通信框架，通过学习通信拓扑和对齐映射，结合语义去噪压缩模块，实现高效语义信息交换并保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: AI发展需要从比特为中心转向目标与语义导向的通信架构，但异构AI代理间交换压缩潜在空间表示时面临语义噪声和任务相关性保持的挑战。

Method: 1) 学习通信拓扑和对齐映射，构建带正交映射的学习网络层；2) 语义去噪压缩模块构建共享全局语义空间，提取稀疏结构化表示；3) 通过非凸字典学习问题迭代求解。

Result: 实验显示语义去噪压缩促进AI代理对齐和语义聚类提取，同时保持下游任务高准确率，通信网络揭示了代理间语义异质性，方法具有可解释性。

Conclusion: 该框架为AI原生6G网络中的语义通信提供了有效解决方案，通过学习网络层和语义处理实现异构代理间高效语义交换，具有实际应用价值。

Abstract: Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [192] [Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both](https://arxiv.org/abs/2512.03225)
*Christophe Andrieu,Nicolas Chopin,Ettore Fincato,Mathieu Gerber*

Main category: stat.CO

TL;DR: 研究一类迭代算法的收敛性，这些算法用于最小化可能非光滑、有噪声的目标函数，函数值通过黑盒获得。算法可视为广义梯度下降递归，其中梯度来自目标函数的光滑近似。


<details>
  <summary>Details</summary>
Motivation: 许多实际优化问题涉及非光滑、有噪声的目标函数，这些函数可能代数上难以处理且只能通过黑盒评估。需要开发收敛性理论来支持这类零阶优化方法。

Method: 提出一个统一框架，将模型基方法和磨光方法作为特例。算法使用目标函数的光滑近似梯度进行广义梯度下降递归。分析中考虑了平滑程度与步长大小的权衡。

Result: 在目标函数正则性非常弱的假设下获得收敛结果。随机情况需要额外假设。通过机器学习中的挑战性分类示例验证了算法和收敛结果的相关性。

Conclusion: 为处理非光滑、有噪声黑盒函数的零阶优化算法提供了统一的收敛性分析框架，扩展了模型基和磨光方法的理论保证，并在实际应用中展示了有效性。

Abstract: We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [193] [A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models](https://arxiv.org/abs/2512.03915)
*X. Y. Han,Yuan Zhong*

Main category: math.OC

TL;DR: 该论文为DeepSeek提出的无辅助损失负载均衡(ALF-LB)方法提供了理论分析框架，将其建模为分配问题的单步原始-对偶方法，在确定性和在线设置下证明了其理论性质，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合(s-MoE)层在大规模AI训练中通过仅激活少量专家来扩展模型规模，但面临负载均衡的挑战——需要高效路由token以避免专家闲置，这对昂贵的GPU资源利用至关重要。现有方法通常需要辅助损失函数，而DeepSeek提出的ALF-LB方法无需辅助损失，需要理论分析其有效性。

Method: 将ALF-LB方法建模为分配问题的单步原始-对偶优化方法。首先在确定性设置下分析，获得拉格朗日目标单调改进、token从过载专家向欠载专家移动的偏好规则、近似均衡保证等结构性质。然后将AI训练的随机动态特性纳入考虑，使用广义在线优化公式，在在线设置下推导目标函数的强凸性，获得对数期望遗憾界。

Result: 理论分析表明ALF-LB方法具有：1)拉格朗日目标单调改进；2)token从过载向欠载专家移动的偏好规则；3)近似均衡保证；4)在线设置下的强凸性导致对数期望遗憾界。在10亿参数的DeepSeekMoE模型上的实验验证了理论发现。

Conclusion: 该研究为s-MoE中的无辅助损失负载均衡建立了一个原则性理论框架，将ALF-LB方法形式化为优化问题，在确定性和在线设置下证明了其理论性质，为高效利用GPU资源的大规模AI训练提供了理论基础。

Abstract: In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [194] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Main category: cs.CL

TL;DR: MAPS框架通过结合思维链、自我反思和自动提示技术，增强LLMs的多步数学推理能力，在多个基准测试中显著优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在问题解决能力上有显著进步，但在面对复杂多步推理任务时仍然存在困难，需要更有效的推理增强方法。

Method: 提出MAPS框架，采用迭代优化过程：先用思维链生成解决方案，当检测到错误时，自适应自我反思机制分析错误并生成定制化提示，引导模型修正推理过程。

Result: 在四个基准测试和多个LLMs上的实验表明，MAPS显著优于标准思维链方法，与推理优化模型达到竞争性结果，使通用LLMs达到接近专用推理模型的性能水平。

Conclusion: MAPS通过战略性地限制反思深度，在成本和推理性能之间取得了最佳平衡，为增强LLMs的多步数学推理提供了有效框架。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.

</details>


### [195] [Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models](https://arxiv.org/abs/2512.03047)
*Samih Fadli*

Main category: cs.CL

TL;DR: 提出基于伦理熵框架的LLM安全动态监测方法，通过行为分类器量化模型伦理熵变化，实现运行时价值漂移预警


<details>
  <summary>Details</summary>
Motivation: 传统静态基准无法有效评估LLM的动态安全风险，如分布漂移下的价值漂移、越狱攻击和部署中的对齐退化，需要动态监测框架

Method: 基于智能第二定律，定义五维行为分类法，训练分类器从模型输出估计伦理熵S(t)，测量四个前沿模型在压力测试下的熵动态，计算有效对齐工作率γ_eff

Result: 基础模型显示持续熵增长，指令调优变体抑制漂移并将伦理熵降低约80%；开发了基于S(t)和γ_eff的监测管道，可在熵漂移超过稳定阈值时发出警报

Conclusion: 伦理熵框架为LLM安全提供了可操作的动态监测工具，能够实时检测价值漂移，为运行时监督提供支持

Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.

</details>


### [196] [Watermarks for Embeddings-as-a-Service Large Language Models](https://arxiv.org/abs/2512.03079)
*Anudeex Shetty*

Main category: cs.CL

TL;DR: 该论文研究Embeddings-as-a-Service（EaaS）的水印保护技术，发现现有水印可通过文本改写攻击被移除，并提出新的线性变换水印方法WET来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着企业提供基于大语言模型的Embeddings-as-a-Service（EaaS），其面临模仿攻击风险。现有水印技术用于保护EaaS提供者的知识产权，但存在新的安全漏洞需要解决。

Method: 1. 揭示现有EaaS水印的脆弱性：通过文本改写攻击可移除水印
2. 提出新水印技术WET：使用线性变换嵌入，通过反向变换和相似度比较进行水印验证

Result: 1. 文本改写攻击能有效绕过当前最先进的EaaS水印技术
2. WET水印技术对改写攻击具有近乎完美的可验证性，表现出强大的鲁棒性

Conclusion: 现有EaaS水印存在被改写攻击绕过的安全漏洞，提出的WET线性变换水印技术能有效防御此类攻击，为EaaS知识产权保护提供了更可靠的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.

</details>


### [197] [Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies](https://arxiv.org/abs/2512.03195)
*Stylianos Saroglou,Konstantinos Diamantaras,Francesco Preta,Marina Delianidi,Apostolos Benisis,Christian Johannes Meyer*

Main category: cs.CL

TL;DR: 该研究开发了一个开源工具，使用句子链接和实体链接两种方法，将招聘文本链接到ESCO和EQF框架，并创建了评估数据集，探索了生成式大语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进劳动力市场信息的分类，通过将招聘文本与欧洲ESCO技能分类和EQF资格框架进行链接，超越表面技能提取，深入分析职业和资格在招聘文本中的表示方式。

Method: 比较了句子链接和实体链接两种方法，开发了开源工具，创建了两个专门用于评估职业和资格表示的标注数据集，并探索了生成式大语言模型在此任务中的应用。

Result: 开发了开源工具和标注数据集，为劳动力分类和就业话语研究提供了计算基础设施，推动了工作实体提取的技术发展，支持在数字化经济中分析工作、技能和劳动力市场叙事。

Conclusion: 该研究通过结合传统方法和生成式大语言模型，为劳动力市场信息分类提供了新的工具和数据集，有助于更深入地理解数字化经济中的工作、技能和就业话语。

Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier

</details>


### [198] [Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs](https://arxiv.org/abs/2512.03310)
*Kunj Joshi,David A. Smith*

Main category: cs.CL

TL;DR: 提出随机掩码微调(RMFT)技术，在保持性能的同时显著减少大语言模型对训练数据中个人身份信息的记忆，相比基线减少80%以上的信息提取率


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型倾向于记忆训练数据中的个人身份信息(PII)，这带来了严重的安全和隐私风险，需要开发既能保护隐私又最小化性能影响的微调技术

Method: 提出随机掩码微调(RMFT)技术，这是一种隐私保护的微调方法，同时开发了MaxTER评估框架来评估隐私-效用权衡，使用AURC指标比较RMFT与去重方法的性能

Result: 在Enron邮件数据集上，RMFT相比基线微调实现了80.81%的总提取率减少和80.17%的已见提取率减少，性能损失仅为困惑度增加5.73%，优于去重方法

Conclusion: RMFT是一种有效的隐私保护微调技术，能显著减少大语言模型对PII的记忆，同时保持模型性能，为平衡隐私与效用提供了实用解决方案

Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.

</details>


### [199] [A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention](https://arxiv.org/abs/2512.03494)
*Di Xiu,Hongyin Tang,Bolin Rong,Lizhi Yan,Jingang Wang,Yifan Lu,Xunliang Cai*

Main category: cs.CL

TL;DR: Top-k注意力机制在解码和训练阶段的有效性研究，验证了在解码阶段仅保留与查询最相似的关键词能达到或超越全注意力性能，并探索了训练-推理一致性对性能提升的作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文建模中越来越普遍，但其推理计算成本已成为阻碍智能体和多模态应用发展的关键瓶颈，需要研究更高效的注意力机制。

Method: 研究Top-k注意力机制在解码和训练阶段的有效性：1）验证精确Top-k解码的性能；2）探索原生Top-k注意力训练策略；3）研究近似Top-k算法精度对下游任务的影响；4）从熵的角度提供理论解释。

Result: 实验表明：1）解码阶段仅保留与查询最相似的关键词在HELMET和LongBench v2等下游任务上达到或超越全注意力性能；2）训练与推理的Top-k注意力操作一致性进一步释放Top-k解码潜力；3）下游任务性能与近似精度正相关；4）Top-k注意力SFT在熵减少现象验证了低熵状态更适合Top-k解码的假设。

Conclusion: Top-k注意力机制是解决LLM推理计算成本瓶颈的有效方法，通过精确Top-k解码和训练-推理一致性策略，能在保持性能的同时显著降低计算复杂度，为长上下文建模提供高效解决方案。

Abstract: Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.

</details>


### [200] [Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective](https://arxiv.org/abs/2512.03759)
*Jingyang Ou,Jiaqi Han,Minkai Xu,Shaoxuan Xu,Jianwen Xie,Stefano Ermon,Yi Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: ESPO提出了一种序列级强化学习框架，用于解决扩散大语言模型中的似然近似难题，相比词级方法在数学推理、编程和规划任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自回归语言模型中很有效，但难以应用于扩散大语言模型。核心困难在于似然近似：自回归模型天然提供词级条件概率，而扩散模型通过迭代非自回归去噪步骤生成序列，缺乏这种分解。

Method: 提出ELBO-based Sequence-level Policy Optimization (ESPO)，将整个序列生成视为单个动作，使用ELBO作为可处理的序列级似然代理。方法包含词级重要性比率归一化和鲁棒的KL散度估计，确保大规模训练的稳定性。

Result: 在数学推理、编程和规划任务上的广泛实验表明，ESPO显著优于词级基线方法，在Countdown任务上实现20-40分的巨大提升，同时在数学和编程基准上保持一致的增益。

Conclusion: ESPO建立了序列级优化作为扩散大语言模型中强化学习的原理性且经验有效的范式，为扩散模型的强化学习训练提供了新方向。

Abstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.

</details>


### [201] [In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)
*Itay Yona,Amir Sarid,Michael Karasik,Yossi Gandelsman*

Main category: cs.CL

TL;DR: Doublespeak是一种针对大语言模型的上下文表示劫持攻击，通过将有害关键词替换为良性标记来绕过安全对齐，使模型在内部将良性提示解释为有害指令。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐策略存在漏洞，攻击者可能通过操纵模型的内部表示来绕过安全防护。研究者希望揭示这种新的攻击面，证明现有对齐方法在表示层面的不足。

Method: 提出Doublespeak攻击方法：在多个上下文示例中系统性地将有害关键词（如"bomb"）替换为良性标记（如"carrot"），并提供一个有害请求的前缀。这种替换导致良性标记的内部表示逐渐收敛到有害标记的表示，从而在语义层面实现劫持。

Result: 攻击成功率高，在闭源和开源系统上均有效，在Llama-3.3-70B-Instruct上单句上下文覆盖达到74%的攻击成功率。使用可解释性工具显示语义覆盖是逐层发生的，早期层的良性含义在深层收敛为有害语义。

Conclusion: Doublespeak揭示了大语言模型潜在空间中的新攻击面，表明当前的对齐策略不足，需要在表示层面进行操作。这种优化无关、跨模型可转移的攻击方法对LLM安全提出了重要挑战。

Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [202] [Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models](https://arxiv.org/abs/2512.03706)
*Vahid Nateghi,Lara Neureither,Selma Moqvist,Carsten Hartmann,Simon Olsson,Feliks Nüske*

Main category: physics.comp-ph

TL;DR: 提出了一种基于投影的粗粒化方法，用于一般欠阻尼朗之万动力学，结合生成器扩展动态模态分解和热力学插值，能够准确捕捉全空间模型的热力学和动力学特性。


<details>
  <summary>Details</summary>
Motivation: 粗粒化对于复杂多尺度系统（如生物分子的构象动力学）的高效建模和模拟至关重要。需要开发能够准确捕捉热力学和动力学特性的粗粒化方法，同时避免重复数值模拟。

Method: 1. 基于Zwanzig投影方法推导欠阻尼朗之万动力学的闭式粗粒化表达式；2. 使用生成器扩展动态模态分解（gEDMD）方法建模粗粒化动力学并评估其动力学特性；3. 结合热力学插值（TI）在不同热力学状态间转换样本，避免重复模拟。

Result: 在二维模型系统上验证，该方法能够准确捕捉全空间模型的热力学和动力学特性，如跃迁时间尺度。

Conclusion: 提出的投影粗粒化框架结合gEDMD和热力学插值，为复杂多尺度系统提供了一种高效且准确的建模方法，能够跨热力学状态扩展应用范围。

Abstract: Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.

</details>


### [203] [Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions](https://arxiv.org/abs/2512.03974)
*Paul Fuchs,Julija Zavadlav*

Main category: physics.comp-ph

TL;DR: 提出一种通过自上而下学习微调机器学习势函数的方法，直接修正错误预测的相变温度以匹配实验数据，显著提高相图预测精度


<details>
  <summary>Details</summary>
Motivation: 基础机器学习势函数虽然能克服传统力场的局限性，但由于参考数据不足和系统性偏差，预测的相变温度与实验值常有数百开尔文的偏差，需要微调才能在实际应用中达到足够精度

Method: 采用自上而下学习微调策略，利用可微分轨迹重加权算法最小化相在实验目标压力和温度下的自由能差异，直接修正错误预测的相变温度

Result: 该方法能准确修正纯钛在高达5 GPa压力范围内的相图，相变温度与实验参考值匹配在十分之几开尔文内，并改善了液态扩散常数

Conclusion: 该方法具有模型无关性，适用于多组分系统的固-固和固-液相变，兼容其他实验性质的自上而下训练，是迈向高精度应用特定和基础机器学习势函数的重要步骤

Abstract: Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [204] [A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses](https://arxiv.org/abs/2512.03458)
*Maryam Maghsoudi,Mohsen Rezaeizadeh,Shihab Shamma*

Main category: eess.SP

TL;DR: 该研究构建了一个音乐家想象与聆听音乐/诗歌的MEG数据集，开发了CNN模型将想象神经活动映射为感知样响应，为BCI应用提供基础。


<details>
  <summary>Details</summary>
Motivation: 想象语音解码面临时间不确定性和数据集有限的挑战，需要探索想象与感知神经响应之间的关系，为脑机接口应用奠定基础。

Method: 收集音乐家想象和聆听音乐/诗歌刺激的MEG数据；使用滑动窗口岭回归进行单被试映射；开发带被试特定校准层的编码器-解码器CNN进行群体水平映射。

Result: 想象和感知脑响应均包含一致的、条件特异性信息；CNN模型显著优于零模型，在几乎所有留出被试上产生更高的预测-真实响应相关性；跨被试泛化有限但群体水平映射稳定。

Conclusion: 想象神经活动可转化为感知样响应，为涉及想象语音和音乐的脑机接口应用提供了重要基础，展示了神经网络模型在解码复杂认知过程方面的潜力。

Abstract: Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [205] [AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning](https://arxiv.org/abs/2512.03637)
*Kohei Yamamoto,Kosuke Okusa*

Main category: cs.SD

TL;DR: 提出Aliasing-aware Patch Embedding (AaPE)方法，通过自适应子带分析缓解音频SSL模型中频谱图处理时的混叠问题，同时保留高频信息，在多个音频下游任务中取得SOTA或竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的音频自监督学习模型通常将频谱图视为图像，使用卷积补丁化并进行严重的时间下采样。这会降低有效奈奎斯特频率并引入混叠，而简单的低通滤波会移除任务相关的高频线索。

Method: 提出Aliasing-aware Patch Embedding (AaPE)方法：1) 使用带限复正弦核和双侧指数窗口生成特征，动态针对易混叠频带；2) 从输入估计核的频率和衰减参数，实现并行自适应子带分析；3) 将输出与标准补丁令牌融合；4) 结合多掩码策略和对比目标，增强不同掩码模式间的一致性。

Result: 在AudioSet预训练后，在环境声音等多样音频下游基准测试中：1) 在部分任务上达到最先进性能；2) 在其他任务上获得竞争性结果；3) 线性探测评估也显示类似模式，在多个基准上取得明显提升。

Conclusion: AaPE能有效缓解混叠效应，同时不丢弃信息丰富的高频内容，可无缝集成到掩码师生自监督学习框架中，提升音频SSL模型的性能。

Abstract: Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naïve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.

</details>
