<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DS](#cs.DS) [Total: 14]
- [cs.SE](#cs.SE) [Total: 22]
- [cs.NI](#cs.NI) [Total: 10]
- [cs.LG](#cs.LG) [Total: 138]
- [math.OC](#math.OC) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 11]
- [quant-ph](#quant-ph) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.CV](#cs.CV) [Total: 17]
- [cs.AI](#cs.AI) [Total: 5]
- [physics.hist-ph](#physics.hist-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CL](#cs.CL) [Total: 18]
- [cs.PF](#cs.PF) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [A Conceptual Model for Context Awareness in Ethical Data Management](https://arxiv.org/abs/2511.21942)
*Elisa Quintarelli,Fabio Alberto Schreiber,Kostas Stefanidis,Letizia Tanca,Barbara Oliboni*

Main category: cs.DB

TL;DR: 论文提出一个双部分概念模型（CDT和ERT）来管理不同情境下的数据伦理要求，确保算法和数据符合伦理规则


<details>
  <summary>Details</summary>
Motivation: 信息管理领域日益关注伦理问题，算法和数据需要满足伦理规则以避免不道德行为。然而，伦理规则会因应用场景（上下文）的不同而变化，因此需要一种系统方法来管理这些变化

Method: 提出一个双部分概念模型：1) 上下文维度树（CDT）描述可能的上下文情境；2) 伦理需求树（ERT）表示在不同上下文中调整和预处理数据集所需的伦理规则。通过这两个工具来确保输入数据分析和学习系统的数据符合特定情境的伦理要求

Result: 开发了一个概念框架，能够系统地识别和管理不同上下文中的伦理要求，为数据预处理和算法设计提供伦理指导

Conclusion: 该概念模型为信息管理社区提供了一种实用的工具，能够在不同应用场景中确保算法和数据符合伦理标准，通过上下文感知的伦理要求管理来促进负责任的AI和数据管理实践

Abstract: Ethics has become a major concern to the information management community, as both algorithms and data should satisfy ethical rules that guarantee not to generate dishonourable behaviours when they are used. However, these ethical rules may vary according to the situation-the context-in which the application programs must work. In this paper, after reviewing the basic ethical concepts and their possible influence on data management, we propose a bipartite conceptual model, composed of the Context Dimensions Tree (CDT), which describes the possible contexts, and the Ethical Requirements Tree (ERT), representing the ethical rules necessary to tailor and preprocess the datasets that should be fed to Data Analysis and Learning Systems in each possible context. We provide some examples and suggestions on how these conceptual tools can be used.

</details>


### [2] [Relation-Stratified Sampling for Shapley Values Estimation in Relational Databases](https://arxiv.org/abs/2511.22035)
*Amirhossein Alizad,Mostafa Milani*

Main category: cs.DB

TL;DR: 提出RSS和ARSS采样方法，用于高效计算关系查询中元组的Shapley-like贡献值，相比传统方法显著降低误差和方差。


<details>
  <summary>Details</summary>
Motivation: Shapley-like值（如Shapley和Banzhaf值）能量化元组对查询结果的贡献，但精确计算需要指数级排列组合，计算不可行。现有采样方法未充分利用关系模式和连接结构。

Method: 提出关系分层采样(RSS)：按关系计数向量而非仅按联盟大小分层，聚焦结构有效联盟。进一步开发自适应变体(ARSS)：根据采样方差动态调整预算分配。实现重用编译视图降低单样本查询成本。

Result: 在TPCH多关系连接和聚合查询中，RSS和ARSS一致优于传统蒙特卡洛采样(MCS)和基于大小的分层采样(SS)，用更少样本获得更低误差和方差。消融实验显示关系感知分层和自适应分配有互补增益。

Conclusion: ARSS成为数据库中心Shapley归因的简单、有效、随时可用的估计器，关系感知分层和自适应预算分配共同提升估计效率。

Abstract: Shapley-like values, including the Shapley and Banzhaf values, provide a principled way to quantify how individual tuples contribute to a query result. Their exact computation, however, is intractable because it requires aggregating marginal contributions over exponentially many permutations or subsets. While sampling-based estimators have been studied in cooperative game theory, their direct use for relational query answering remains underexplored and often ignores the structure of schemas and joins.
  We study tuple-level attribution for relational queries through sampling and introduce Relation-Stratified Sampling (RSS). Instead of stratifying coalitions only by size, RSS partitions the sample space by a relation-wise count vector that records how many tuples are drawn from each relation. This join-aware stratification concentrates samples on structurally valid and informative coalitions and avoids strata that cannot satisfy query conditions. We further develop an adaptive variant, ARSS, that reallocates budget across strata using variance estimates obtained during sampling, improving estimator efficiency without increasing the total number of samples. We analyze these estimators, describe a practical implementation that reuses compiled views to reduce per-sample query cost, and evaluate them on TPCH workloads.
  Across diverse queries with multi-relation joins and aggregates, RSS and ARSS consistently outperform classical Monte Carlo (MCS) and size-based Stratified Sampling (SS), yielding lower error and variance with fewer samples. An ablation shows that relation-aware stratification and adaptive allocation contribute complementary gains, making ARSS a simple, effective, and anytime estimator for database-centric Shapley attribution.

</details>


### [3] [Performant Synchronization in Geo-Distributed Databases](https://arxiv.org/abs/2511.22444)
*Duling Xu,Tong Li,Zegang Sun,Zheng Chen,Weixing Zhou,Yanfeng Zhang,Wei Lu,Xiaoyong Du*

Main category: cs.DB

TL;DR: GeoCoCo是一个用于跨区域分布式数据库的同步加速框架，通过分组重调度、数据过滤和一致性保证传输来降低WAN带宽使用，减少同步成本达40.3%，提升吞吐量14.1%


<details>
  <summary>Details</summary>
Motivation: 分布式数据库在广域网中面临高延迟问题，主要瓶颈在于跨节点数据一致性的同步成本。研究发现网络集群现象、传输三角不等式违规和冗余数据传输等优化机会

Method: 提出GeoCoCo框架：1) 适应实时网络条件的组重调度策略最大化WAN传输效率；2) 任务保持数据过滤方法减少WAN传输数据量；3) 集成分组和剪枝的一致性保证传输框架

Result: 在跟踪驱动模拟和实际部署中，GeoCoCo将同步成本（主要通过降低WAN带宽使用）降低达40.3%，在GeoGauss中将系统吞吐量提升达14.1%

Conclusion: GeoCoCo通过优化WAN传输效率和减少数据传输量，有效解决了分布式数据库在广域网中的同步瓶颈问题，显著提升了系统性能

Abstract: The deployment of databases across geographically distributed regions has become increasingly critical for ensuring data reliability and scalability. Recent studies indicate that distributed databases exhibit significantly higher latency than single-node databases, primarily due to consensus protocols maintaining data consistency across multiple nodes. We argue that synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN). Fortunately, we identify opportunities to optimize synchronization costs in real production environments: (1) network clustering phenomena, (2) triangle inequality violations in transmission, and (3) redundant data transfers. Based on these observations, we propose GeoCoCo, a synchronization acceleration framework for cross-region distributed databases. First, GeoCoCo presents a group rescheduling strategy that adapts to real-time network conditions to maximize WAN transmission efficiency. Second, GeoCoCo introduces a task-preserving data filtering method that reduces data volume transmitted over the WAN. Finally, GeoCoCo develops a consistency-guaranteed transmission framework integrating grouping and pruning. Extensive evaluations in both trace-driven simulations and real-world deployments demonstrate that GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss.

</details>


### [4] [Structured Multi-Step Reasoning for Entity Matching Using Large Language Model](https://arxiv.org/abs/2511.22832)
*Rohan Bopardikar,Jin Wang,Jia Zou*

Main category: cs.DB

TL;DR: 该论文提出了一种基于大语言模型的三步推理框架来改进实体匹配，通过分解匹配过程为多个显式推理阶段，并探索了基于辩论的策略来提高决策鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的实体匹配方法大多依赖单步提示，对结构化推理策略的研究有限。作者旨在通过将匹配过程分解为多个明确的推理阶段来增强LLM在实体匹配中的表现。

Method: 提出了一个三步推理框架：1)识别两个记录之间匹配和不匹配的token；2)确定对匹配决策影响最大的属性；3)预测记录是否指向同一真实世界实体。此外，还探索了基于辩论的策略，通过对比支持和反对论据来提高决策鲁棒性。

Result: 在多个真实世界实体匹配基准数据集上的实验结果表明，结构化多步推理在多个案例中能够提高匹配性能，同时也突显了推理引导的LLM方法面临的挑战和进一步改进的机会。

Conclusion: 结构化多步推理可以改善实体匹配性能，但仍有挑战需要解决。这为推理引导的LLM方法提供了进一步精炼的机会。

Abstract: Entity matching is a fundamental task in data cleaning and data integration. With the rapid adoption of large language models (LLMs), recent studies have explored zero-shot and few-shot prompting to improve entity matching accuracy. However, most existing approaches rely on single-step prompting and offer limited investigation into structured reasoning strategies. In this work, we investigate how to enhance LLM-based entity matching by decomposing the matching process into multiple explicit reasoning stages. We propose a three-step framework that first identifies matched and unmatched tokens between two records, then determines the attributes most influential to the matching decision, and finally predicts whether the records refer to the same real-world entity. In addition, we explore a debate-based strategy that contrasts supporting and opposing arguments to improve decision robustness. We evaluate our approaches against multiple existing baselines on several real-world entity matching benchmark datasets. Experimental results demonstrate that structured multi-step reasoning can improve matching performance in several cases, while also highlighting remaining challenges and opportunities for further refinement of reasoning-guided LLM approaches.

</details>


### [5] [Extended Serial Safety Net: A Refined Serializability Criterion for Multiversion Concurrency Control](https://arxiv.org/abs/2511.22956)
*Atsushi Kitazawa,Chihaya Ito,Yuta Yoshida,Takamitsu Shioi*

Main category: cs.DB

TL;DR: ESSN是SSN的改进版，通过放宽排除条件允许更多事务安全提交，保持多版本可串行化，严格包含SSN，使用KTO进行图可串行化推理，提交时检查工作线性于读写数量。


<details>
  <summary>Details</summary>
Motivation: 传统并发控制协议基于单一串行化点（开始或提交）的假设与快照隔离不兼容，SSN虽然提供轻量级提交时测试但较为保守且以提交时间为唯一锚点，需要更灵活高效的协议。

Method: 提出ESSN作为SSN的原则性泛化，放宽排除条件，引入已知全序（KTO）用于多版本串行化图推理，采用基于不变量的语义进行单次提交时检查，保持版本链单调性，消除链遍历。

Result: ESSN严格包含SSN，保持多版本可串行化，在提交有序KTO下，使用开始快照读取可将长事务中止率降低约0.25绝对值（约50%相对值）。

Conclusion: ESSN是SSN的有效改进，通过放宽排除条件和引入KTO推理，在保持线性工作复杂度的同时显著降低长事务中止率，为混合工作负载提供更好的并发控制。

Abstract: A long line of concurrency-control (CC) protocols argues correctness via a single serialization point (begin or commit), an assumption that is incompatible with snapshot isolation (SI), where read-write anti-dependencies arise. Serial Safety Net (SSN) offers a lightweight commit-time test but is conservative and effectively anchored on commit time as the sole point. We present ESSN, a principled generalization of SSN that relaxes the exclusion condition to allow more transactions to commit safely, and we prove that this preserves multiversion serializability (MVSR) and that it strictly subsumes SSN. ESSN states an MVSG (Multiversion Serialization Graph)-based criterion and introduces a known total order over transactions (KTO; e.g., begin-ordered or commit-ordered) for reasoning about the graph's serializability. With a single commit-time check under invariant-based semantics, ESSN's exclusion condition preserves monotonicity along per-item version chains, and eliminates chain traversal. The protocol is Direct Serialization Graph (DSG)-based with commit-time work linear in the number of reads and writes, matching SSN's per-version footprint. We also make mixed workloads explicit by defining a Long transaction via strict interval containment of Short transactions, and we evaluate ESSN on reproducible workloads. Under a commit-ordered KTO, using begin-snapshot reads reduces the long-transaction abort rate by up to approximately 0.25 absolute (about 50% relative) compared with SSN.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 提出一个结合高性能集群计算与智能算法的区块链框架，通过改进的PoW共识、动态信任评级和统计抽签系统，实现高效、包容且环保的分布式计算。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法需要大量计算资源，导致高能耗并排斥计算能力较弱的系统。需要一种更包容、可扩展且环保的智能算法开发和实施方法。

Method: 提出新框架，将高性能集群计算与智能算法集成到区块链基础设施中。核心包括：改进的PoW共识机制（将计算工作与区块奖励直接挂钩）、动态信任评级系统（基于验证准确性记录）、统计抽签系统（让弱节点也有机会参与区块创建）。

Result: 该框架实现了高效资源利用和广泛参与，通过信任评级创建了基于贡献质量的奖励系统，同时通过抽签机制确保了计算能力较弱节点的参与机会。

Conclusion: 该策略为解决高性能计算和智能算法的能耗与包容性问题提供了有吸引力的解决方案，实现了效率、广泛参与和环保的平衡。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [7] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 本文研究了异步消息传递模型(AMP_f)与Heard-Of模型(HO_f)的等价性，发现对于无色任务在n>2f时等价，对于有色任务仅在f=1时等价，揭示了轮次抽象在异步计算中的表达能力界限。


<details>
  <summary>Details</summary>
Motivation: 重新审视两个分布式计算基础模型之间的关系：最多f个崩溃故障的异步消息传递模型(AMP_f)和最多f个消息遗漏的Heard-Of模型(HO_f)。理解这些模型在任务可解性方面的等价性，以及轮次抽象在异步计算中的表达能力。

Method: 通过双向模拟方法，在AMP_f和HO_f之间建立等价关系证明，引入中间模型来捕捉"静默进程"概念。使用双向模拟证明技术，并扩展到针对非自适应对手的随机化协议。

Result: 对于n>2f，两个模型在无色任务可解性方面等价；对于有色任务，仅当f=1(且n>2)时等价。对于更大的f，由于HO_f中存在静默进程可能导致决策冲突，两个模型分离。结果扩展到随机化协议，表明轮次抽象的表达限制是结构性的而非概率性的。

Conclusion: 研究精确界定了轮次抽象在何处能够捕捉异步计算，在何处不能。结果表明轮次抽象的表达能力存在结构性限制，特别是在有色任务和较大故障数情况下，静默进程的存在导致模型不等价。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [8] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出一种基于延迟约束的解耦架构，将集群资源分为延迟严格和延迟宽松池，通过瓶颈调度器和快速抢占机制，在保证在线服务SLO的同时提升离线任务吞吐量3倍。


<details>
  <summary>Details</summary>
Motivation: LLM部署中在线服务（延迟敏感）和离线任务（成本敏感）共置可提高资源利用率，但在Prefill/Decode解耦系统中，请求混合波动导致P/D比例失衡，现有动态调整技术无法应对突发流量模式。

Method: 1) 延迟约束解耦架构：按任务延迟需求分离集群资源为延迟严格池和延迟宽松池；2) 基于瓶颈的调度器：使用Roofline性能模型指导性能瓶颈调度；3) 快速抢占机制：严格强制执行在线请求的SLO。

Result: 在真实世界trace上的实验表明，相比现有离线系统方法，该方法在保持在线请求SLO的同时，将离线吞吐量提升高达3倍。

Conclusion: 提出的延迟约束解耦架构和配套调度机制有效解决了P/D比例失衡问题，在保证在线服务质量的同时显著提升了离线任务吞吐量，为LLM混合工作负载部署提供了高效解决方案。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [9] [DisCEdge: Distributed Context Management for Large Language Models at the Edge](https://arxiv.org/abs/2511.22599)
*Mohammadreza Malekabbasi,Minghe Wang,David Bermbach*

Main category: cs.DC

TL;DR: DisCEdge：一种在边缘节点间以token化形式存储和复制用户上下文的分布式管理系统，通过避免冗余计算提升LLM边缘服务性能


<details>
  <summary>Details</summary>
Motivation: 在边缘部署LLM服务有利于延迟敏感和隐私保护应用，但LLM的无状态特性使得跨地理分布式边缘节点管理用户上下文（如会话、偏好）具有挑战性。现有解决方案（如客户端上下文存储）常引入网络延迟和带宽开销，削弱了边缘部署的优势。

Method: 提出DisCEdge分布式上下文管理系统，以token序列而非原始文本形式存储和复制用户上下文，避免冗余计算并实现高效数据复制。在真实边缘环境中使用商用硬件实现并评估开源原型。

Result: 相比基于原始文本的系统，DisCEdge将中位响应时间提升达14.46%，中位节点间同步开销降低达15%。相比客户端上下文管理，中位客户端请求大小减少90%，同时保证数据一致性。

Conclusion: DisCEdge通过token化上下文管理有效解决了LLM边缘部署中的上下文管理挑战，显著提升了性能并降低了开销，为边缘LLM服务提供了实用的解决方案。

Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.

</details>


### [10] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+是针对元数据缓存设计的缓存替换算法，通过在小FIFO队列中引入关联窗口来避免误判热块，在元数据跟踪上比S3-FIFO降低28.5%的缺失率，并具有低CPU开销、低内存开销等优点。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存存在固有的关联引用特性，即使对应的数据访问不包含关联引用。这种关联引用会降低缓存替换算法的效果，因为它们经常被错误地归类为热块。

Method: Clock2Q+采用类似S3-FIFO的三队列结构，但在小FIFO队列中引入了关联窗口，处于该窗口中的块不设置引用位，从而避免将关联引用误判为热块。

Result: 在元数据跟踪上，Clock2Q+比表现第二好的S3-FIFO算法降低了高达28.5%的缺失率。该算法在VMware的vSAN和VDFS存储产品中已实现，并在数据跟踪上也优于最先进的缓存替换算法。

Conclusion: Clock2Q+是针对元数据缓存特性设计的有效缓存替换算法，通过简单的增强显著提升了性能，同时具备大规模存储系统所需的关键特性：低CPU开销、低内存开销、多CPU扩展性好、易于调优和实现。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [11] [ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast](https://arxiv.org/abs/2511.21969)
*Matteo Bjornsson,Taylor Hardin,Taylor Heinecke,Marcin Furtak,David L. Millman,Mike P. Wittie*

Main category: cs.DC

TL;DR: ZipperChain是一种无需分布式共识的区块链技术，通过专用服务管道构建区块，实现接近网络线速的交易吞吐量和500毫秒的最终性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式账本技术依赖分布式共识机制，受限于节点间网络通信性能。作者希望设计一种既能保证交易数据的不可变性、一致性和可用性，又能避免分布式共识性能瓶颈的解决方案。

Method: 采用中心化区块构建方法，通过部署在少量节点上的专用服务管道构建区块，这些节点通过快速数据中心网络连接。将信任从广泛使用的第三方服务转移到ZipperChain的正确性保证上。

Result: ZipperChain实现了接近网络线速的交易吞吐量，区块最终性约为500毫秒。由于采用中心化基础设施构建区块，无需原生代币来激励验证者社区。

Conclusion: ZipperChain提供了一种无需分布式共识的区块链架构，通过中心化区块构建管道实现了高性能的交易处理，同时保持了传统区块链的不可变性、一致性和可用性保证。

Abstract: Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.

</details>


### [12] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 在跨语言复制数据系统中，采用通用数据格式（CDF）比外部函数接口（FFI）在软件质量、延迟、内存消耗和吞吐量方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要在多语言环境中复制数据，但现有复制数据库（RDL）通常只支持单一语言，导致集成时需要特殊代码，其软件质量和性能特性缺乏研究。

Method: 通过实证研究比较两种RDL集成策略：外部函数接口（FFI）和通用数据格式（CDF），测量并比较它们的软件指标和性能。

Result: 结果显示CDF在软件质量、延迟、内存消耗和吞吐量方面具有优势；研究还创建了支持编译、解释和托管语言的CDF-based RDL，并通过插件扩展性增强了其功能。

Conclusion: 在多语言分布式系统中，CDF为设计复制数据库提供了新的设计思路，能够更好地支持跨语言交互。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [13] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT提出了一种前缀感知的注意力核实现，通过打包-前向-合并范式优化LLM解码中的注意力计算，显著减少内存访问和资源气泡，在真实和合成工作负载上平均降低注意力延迟67.4%。


<details>
  <summary>Details</summary>
Motivation: LLM服务中解码注意力成为主要瓶颈，这是内存受限的操作，需要从全局内存加载大量KV缓存。现实工作负载中存在大量跨请求的层次化共享前缀（如系统提示、工具/模板、RAG），现有注意力实现未能充分利用前缀共享，导致重复加载共享前缀KV缓存和资源利用不足。

Method: PAT采用打包-前向-合并范式：1）按共享前缀打包查询以减少重复内存访问；2）运行定制的多瓦片核实现高资源效率；3）应用多流前向和KV分割减少资源气泡；4）最终合并执行在线softmax，开销可忽略。实现为vLLM的即插即用插件。

Result: 在真实和合成工作负载评估中，PAT平均降低注意力延迟67.4%，在相同配置下比最先进注意力核降低TPOT 13.6-83.4%。

Conclusion: PAT通过前缀感知的注意力核实现，有效利用请求间的共享前缀，显著优化LLM解码性能，减少内存带宽压力，为现实工作负载提供高效解决方案。

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [14] [Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)](https://arxiv.org/abs/2511.22380)
*Kaya Alpturer,Ron van der Meyden,Sushmita Ruj,Godfrey Wong*

Main category: cs.DC

TL;DR: 该论文研究在崩溃故障模型中，针对不同有限信息交换方式的同步共识问题，推导出各信息交换方式下的最优协议。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识逻辑的最优容错共识协议研究主要关注"完全信息"交换方式，但这种方式消息开销大。需要研究在有限信息交换条件下的最优协议设计。

Method: 考虑崩溃故障模型，分析文献中的多种有限信息交换方式（FloodSet协议变体、故障计数变体、代理关联值变体），并引入新的信息交换方式。通过实现基于知识的程序，推导出每种信息交换方式下的最优协议。

Result: 为每种有限信息交换方式推导出最优协议，新引入的信息交换方式使决策最多比Dwork和Moses的最优协议晚一轮，但计算成本和空间需求更低。

Conclusion: 在崩溃故障模型中，针对不同有限信息交换方式可以设计出相应的最优同步共识协议，新提出的信息交换方式在性能和效率之间取得了良好平衡。

Abstract: Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the "full information" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.

</details>


### [15] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer是一个统一的系统级加速框架，通过专家放置、缓存压缩和调度优化来最大化LLM服务效率，在DeepSeek-R1上实现616 QPM，降低TPOT 36%和TTFT 38%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在服务时面临计算密集、延迟严格和吞吐瓶颈等挑战，需要系统级优化来提高端到端服务效率。

Method: OmniInfer包含三个组件：OmniPlacement用于负载感知的Mixture-of-Experts调度，OmniAttn用于稀疏注意力加速，OmniProxy用于解耦感知的请求调度，基于vLLM构建，通过自适应资源解耦、高效稀疏利用和全局协调实现优化。

Result: 在10节点Ascend 910C集群上评估DeepSeek-R1，OmniInfer达到616 QPM，统一框架降低TPOT 36%，OmniProxy进一步降低TTFT 38%。

Conclusion: OmniInfer通过系统级优化显著提升LLM服务效率，已开源供社区使用。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [16] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC：利用GPU光线追踪核心硬件加速的蒙特卡洛算法，显著提升网格蒙特卡洛模拟速度，简化工作流程


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛（MMC）方法虽然精度高，但频繁的光线-边界相交测试计算成本高，限制了性能提升。现代GPU的光线追踪核心（RT-cores）提供了硬件加速能力，但尚未被充分利用于生物光子学模拟。

Method: 基于NVIDIA OptiX平台实现RT-MMC算法，将图形光线追踪管线扩展到浑浊介质中的体积光线追踪。该方法无需复杂的四面体网格生成，利用硬件RT-cores进行光线遍历和相交测试，并天然支持宽场光源而无需网格重划分。

Result: RT-MMC与传统软件光线追踪MMC算法结果高度一致，同时在多种GPU架构上实现1.5倍到4.5倍的加速。性能提升显著增强了MMC在实际模拟中的实用性。

Conclusion: 从软件转向硬件光线追踪不仅大大简化了MMC模拟工作流程，还带来了显著的加速效果。随着光线追踪硬件的快速普及，性能提升预计会进一步增加。采用图形光线追踪管线使生物光子学应用能够利用新兴硬件资源。

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [17] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe是一个动态适配器放置和路由框架，专门解决多租户LoRA服务中的rank多样性问题，通过动态重新平衡适配器和远程访问优化，显著提升吞吐量并减少GPU需求。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA服务系统在处理多租户环境中不同rank（大小）的适配器时存在性能偏差问题，导致GPU资源利用率低下，需要更多GPU来满足服务级别目标。

Method: 提出LoRAServe框架，采用工作负载感知的动态适配器放置和路由机制，通过动态重新平衡跨GPU的适配器分布，并利用GPU Direct RDMA进行远程访问。

Result: 在真实生产环境中，LoRAServe相比现有系统实现了2倍吞吐量提升、9倍TTFT降低，并在满足SLO约束下减少50%GPU使用。

Conclusion: LoRAServe有效解决了LoRA服务中的rank多样性问题，显著提升了多租户环境下的系统效率和资源利用率。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


### [18] [Areon: Latency-Friendly and Resilient Multi-Proposer Consensus](https://arxiv.org/abs/2511.23025)
*Álvaro Castro-Castilla,Marcin Pawlowski,Hong-Sheng Zhou*

Main category: cs.DC

TL;DR: Areon是一种基于DAG的多提议者PoS共识协议，通过滑动窗口内的块引用形成并行投票，实现部分同步下的鲁棒性和有界延迟最终性。


<details>
  <summary>Details</summary>
Motivation: 传统链式PoS协议在部分同步网络条件下存在重组频率高、延迟不确定的问题。需要设计一种能够实现有界延迟最终性、低重组频率的共识协议。

Method: 1. 多提议者每时隙并行提议块，组织成DAG结构；2. 滑动窗口内块相互引用形成最大反链作为并行投票；3. 基于最近短引用数量的权重比较解决冲突子DAG；4. 引入尖端有界性(TB)保证有界宽度前沿；5. 设计理想协议(Areon-Ideal)和实用协议(Areon-Base)。

Result: 1. 证明了(k,ε)-最终性定理，将确认深度校准为窗口长度和目标尾概率的函数；2. 模拟显示在匹配块到达率下，相比Ouroboros Praos，Areon-Base在广泛对抗股权和网络延迟范围内实现有界延迟最终性，重组频率和深度更低。

Conclusion: Areon协议通过DAG结构和多提议者设计，在部分同步条件下实现了鲁棒的有界延迟最终性，显著降低了重组频率和深度，为PoS共识提供了新的解决方案。

Abstract: We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.
  We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.
  Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.

</details>


### [19] [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)
*Chenyu Liu,Zhaoyang Zhang,Zirui Chen,Zhaohui Yang*

Main category: cs.DC

TL;DR: 提出C²P²SL方法，通过流水线并行化通信和计算过程，显著减少分割学习在无线网络中的训练时间


<details>
  <summary>Details</summary>
Motivation: 传统分割学习虽然保护了本地数据隐私，但其通信和计算过程是顺序执行的，导致系统效率有限。需要克服这一限制，提高无线网络中分割学习的训练效率。

Method: 将分布式训练中的流水线并行技术应用于分割学习，提出C²P²SL方法。将用户设备和基站的通信与计算过程视为整体流水线，在不同微批次之间实现流水线并行化，从而重叠通信和计算时间。

Result: 实验结果表明，C²P²SL在不同通信条件下能够将系统训练时间减少超过38%，同时保持收敛精度。

Conclusion: 通过引入流水线并行技术，C²P²SL有效解决了传统分割学习中通信和计算顺序执行导致的效率瓶颈问题，显著提升了无线网络中分割学习的训练效率。

Abstract: Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\% while maintaining convergence accuracy under different communication conditions.

</details>


### [20] [Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election](https://arxiv.org/abs/2511.23297)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 本文研究了在内容无关通信模型下，利用网络拓扑知识进行领导者选举的可能性，证明了拓扑知识对于某些图族中终止性领导者选举的必要性。


<details>
  <summary>Details</summary>
Motivation: 内容无关通信模型是一种极弱的通信形式，节点只能发送异步、无内容的脉冲。先前研究表明，在单边上两个节点无法计算任何非常数函数，这似乎排除了许多非2边连通图上的自然图问题。本文旨在探索在已知网络拓扑的情况下，领导者选举是否可能。

Method: 通过理论分析，研究不同图类中领导者选举的可能性与不可能性。首先证明关于边对称的图不允许随机终止性领导者选举算法。然后为不对称树设计领导者选举算法，包括基于完整拓扑知识的O(n²)消息算法和仅知道直径的O(nr)算法。最后通过图族{P₃, P₅}证明拓扑知识的必要性。

Result: 1. 关于边对称的图不可能实现随机终止性领导者选举；2. 不对称树可实现静默终止领导者选举，消息复杂度为O(n²)；3. 偶数直径树仅需知道直径D=2r即可实现终止性领导者选举，消息复杂度为O(nr)；4. 在图族{P₃, P₅}中，精确拓扑知识可实现静默终止领导者选举，但仅知道拓扑属于该图族则不可能。

Conclusion: 网络拓扑知识在内容无关通信模型中对于领导者选举至关重要。即使在这种极弱的通信模型下，利用拓扑知识可以在许多图类中实现领导者选举，但拓扑知识的精确程度直接影响算法的可行性。

Abstract: The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.
  In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.
  Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.
  Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.
  Necessity of topology knowledge: In the family of graphs $\mathcal{G} = \{P_3, P_5\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\mathcal{G}$, then terminating leader election is impossible.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [21] [A Combinatorial Characterization of Constant Mixing Time](https://arxiv.org/abs/2511.21868)
*Lap Chi Lau,Raymond Liu*

Main category: cs.DS

TL;DR: 提出了一种基于小集合二分密度的组合特征，用于刻画具有常数混合时间的图，该特征比具有近最优谱半径更弱，但比具有近最优小集合顶点扩展更强。


<details>
  <summary>Details</summary>
Motivation: 经典谱图理论刻画了对数混合时间的图，但缺乏对常数混合时间图的组合特征。本文旨在填补这一空白，提供更精细的图结构分析工具。

Method: 提出小集合二分密度条件作为组合特征，该条件介于谱半径和小集合顶点扩展之间，用于刻画具有常数混合时间的图。

Result: 建立了常数混合时间图的小集合二分密度特征，该特征比谱半径条件更弱但比顶点扩展条件更强，为图结构分析提供了新的组合工具。

Conclusion: 小集合二分密度条件为常数混合时间图提供了有效的组合特征，填补了谱图理论在这一领域的空白，具有理论和应用价值。

Abstract: Classical spectral graph theory characterizes graphs with logarithmic mixing time. In this work, we present a combinatorial characterization of graphs with constant mixing time. The combinatorial characterization is based on the small-set bipartite density condition, which is weaker than having near-optimal spectral radius and is stronger than having near-optimal small-set vertex expansion.

</details>


### [22] [Differential privacy from axioms](https://arxiv.org/abs/2511.21876)
*Guy Blanc,William Pires,Toniann Pitassi*

Main category: cs.DS

TL;DR: 该论文证明任何满足四个核心公理的隐私度量都等价于差分隐私，表明在统计设置中无法找到比差分隐私更弱但仍具有良好性质的隐私概念。


<details>
  <summary>Details</summary>
Motivation: 探索是否存在比差分隐私更弱的隐私概念，这些概念能否在保证基本隐私的同时，更高效地实现隐私保护或适用于更广泛的任务。

Method: 提出四个核心公理：预处理不变性、禁止公然非隐私、强组合性和线性可扩展性，证明任何满足这些公理的隐私度量都等价于差分隐私。

Result: 主要定理表明，在统计设置中，任何合理的隐私度量如果满足非平凡组合性，都等价于差分隐私（在样本复杂度上最多相差多项式因子）。

Conclusion: 差分隐私在统计设置中是"不可避免的"——任何满足基本公理的隐私概念都会归结为差分隐私，表明无法找到既弱于差分隐私又具有良好性质的替代方案。

Abstract: Differential privacy (DP) is the de facto notion of privacy both in theory and in practice. However, despite its popularity, DP imposes strict requirements which guard against strong worst-case scenarios. For example, it guards against seemingly unrealistic scenarios where an attacker has full information about all but one point in the data set, and still nothing can be learned about the remaining point. While preventing such a strong attack is desirable, many works have explored whether average-case relaxations of DP are easier to satisfy [HWR13,WLF16,BF16,LWX23].
  In this work, we are motivated by the question of whether alternate, weaker notions of privacy are possible: can a weakened privacy notion still guarantee some basic level of privacy, and on the other hand, achieve privacy more efficiently and/or for a substantially broader set of tasks? Our main result shows the answer is no: even in the statistical setting, any reasonable measure of privacy satisfying nontrivial composition is equivalent to DP. To prove this, we identify a core set of four axioms or desiderata: pre-processing invariance, prohibition of blatant non-privacy, strong composition, and linear scalability. Our main theorem shows that any privacy measure satisfying our axioms is equivalent to DP, up to polynomial factors in sample complexity. We complement this result by showing our axioms are minimal: removing any one of our axioms enables ill-behaved measures of privacy.

</details>


### [23] [Identifying all snarls and superbubbles in linear-time, via a unified SPQR-tree framework](https://arxiv.org/abs/2511.21919)
*Francisco Sena,Aleksandr Politov,Corentin Moumard,Manuel Cáceres,Sebastian Schmidt,Juha Harviainen,Alexandru I. Tomescu*

Main category: cs.DS

TL;DR: 提出了首个线性时间算法来识别所有snarls，并基于统一框架提供了新的线性时间算法来寻找superbubbles，利用SPQR树分解高效处理pangenome中的气泡状结构。


<details>
  <summary>Details</summary>
Motivation: Snarls和superbubbles是pangenome分解中的基本结构，支持结构变异基因分型、距离索引、单倍型采样和变异注释等关键任务。现有方法要么是二次时间复杂度（snarls），要么是高度专业化的线性时间解决方案（superbubbles），需要更高效统一的算法。

Method: 基于所有气泡状结构都由两个顶点与图其余部分分离的观察，利用SPQR树分解（编码所有2-分离器）指导遍历，提出了新的snarls线性大小表示和线性时间计算算法，以及统一的superbubbles线性时间算法。

Result: 算法在C++中实现，在多个pangenomic数据集上评估：对于snarls，比vg快达2倍且能识别所有snarls；对于superbubbles，比BubbleGun快达50倍。

Conclusion: SPQR树框架为pangenomics中的气泡状结构提供了统一视角，并为高效寻找其他气泡状结构提供了模板，首次实现了snarls的线性时间识别。

Abstract: Snarls and superbubbles are fundamental pangenome decompositions capturing variant sites. These bubble-like structures underpin key tasks in computational pangenomics, including structural-variant genotyping, distance indexing, haplotype sampling, and variant annotation. Snarls can be quadratically-many in the size of the graph, and since their introduction in 2018 with the vg toolkit, there has been no work on identifying all snarls in linear time. Moreover, while it is known how to find superbubbles in linear time, this result is a highly specialized solution only achieved after a long series of papers.
  We present the first algorithm identifying all snarls in linear time. This is based on a new representation of all snarls, of size linear in the input graph size, and which can be computed in linear time. Our algorithm is based on a unified framework that also provides a new linear-time algorithm for finding superbubbles. An observation behind our results is that all such structures are separated from the rest of the graph by two vertices (except for cases which are trivially computable), i.e. their endpoints are a 2-separator of the underlying undirected graph. Based on this, we employ the well-known SPQR tree decomposition, which encodes all 2-separators, to guide a traversal that finds the bubble-like structures efficiently.
  We implemented our algorithms in C++ (available at https://github.com/algbio/BubbleFinder) and evaluated them on various pangenomic datasets. Our algorithms outcompete or they are on the same level of existing methods. For snarls, we are up to two times faster than vg, while identifying all snarls. When computing superbubbles, we are up to 50 times faster than BubbleGun. Our SPQR tree framework provides a unifying perspective on bubble-like structures in pangenomics, together with a template for finding other bubble-like structures efficiently.

</details>


### [24] [MagnifierSketch: Quantile Estimation Centered at One Point](https://arxiv.org/abs/2511.22070)
*Jiarui Guo,Qiushi Lyu,Yuhan Wu,Haoyu Li,Zhaoqian Yao,Yuqi Dong,Xiaolin Wang,Bin Cui,Tong Yang*

Main category: cs.DS

TL;DR: MagnifierSketch：一种针对数据流中单点分位数估计的算法，支持单键和每键分位数估计，在精度和吞吐量上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有数据流分位数估计算法不适合针对特定分位点（如0.95或0.99分位数）的每键估计，在精度和吞吐量上无法满足实际需求，特别是在尾部延迟测量等场景中

Method: 提出MagnifierSketch算法，采用Value Focus（值聚焦）、Distribution Calibration（分布校准）和Double Filtration（双重过滤）等关键技术，支持单键和每键分位数估计

Result: 实验结果显示MagnifierSketch的平均误差显著低于现有最优方法，在单键和每键情况下都表现更好；在RocksDB数据库中的实现有效降低了分位数查询延迟

Conclusion: MagnifierSketch解决了数据流中特定分位点的每键分位数估计问题，在精度和性能上优于现有方法，已开源并在实际数据库中得到应用验证

Abstract: In this paper, we take into consideration quantile estimation in data stream models, where every item in the data stream is a key-value pair. Researchers sometimes aim to estimate per-key quantiles (i.e. quantile estimation for every distinct key), and some popular use cases, such as tail latency measurement, recline on a predefined single quantile (e.g. 0.95- or 0.99- quantile) rather than demanding arbitrary quantile estimation. However, existing algorithms are not specially designed for per-key estimation centered at one point. They cannot achieve high accuracy in our problem setting, and their throughput are not satisfactory to handle high-speed items in data streams. To solve this problem, we propose MagnifierSketch for point-quantile estimation. MagnifierSketch supports both single-key and per-key quantile estimation, and its key techniques are named Value Focus, Distribution Calibration and Double Filtration. We provide strict mathematical derivations to prove the unbiasedness of MagnifierSketch and show its space and time complexity. Our experimental results show that the Average Error (AE) of MagnifierSketch is significantly lower than the state-of-the-art in both single-key and per-key situations. We also implement MagnifierSketch on RocksDB database to reduce quantile query latency in real databases. All related codes of MagnifierSketch are open-sourced and available at GitHub.

</details>


### [25] [How fast are algorithms reducing the demands on memory? A survey of progress in space complexity](https://arxiv.org/abs/2511.22084)
*Hayden Rome,Jayson Lynch,Jeffery Li,Chirag Falor,Neil Thompson*

Main category: cs.DS

TL;DR: 该论文首次对算法内存使用（空间复杂度）的改进进行了广泛调查，分析了118个核心算法问题的800多个算法，发现空间复杂度在应对"内存墙"瓶颈中变得日益重要，20%的大规模问题中算法进步比硬件进步更有效地减少了内存访问延迟。


<details>
  <summary>Details</summary>
Motivation: 传统算法研究主要关注时间复杂性（操作次数），但许多问题的运行时间和能耗主要由内存访问决定。随着"内存墙"问题的出现，内存访问成为性能瓶颈，需要系统研究算法在空间复杂度方面的进展。

Method: 对计算机科学中118个最重要的算法问题进行广泛调查，分析了解决这些问题的800多个算法，特别关注空间复杂性的改进趋势和与时间复杂性的权衡关系。

Result: 1. 空间复杂度在近年变得日益重要；2. 20%的大规模问题（n=10亿）中，空间复杂性的改进速度超过了DRAM访问速度的改进，表明算法进步比硬件进步更有效地减少了内存访问延迟；3. 出现了算法帕累托前沿，即改善渐近时间复杂性通常需要牺牲空间复杂性，反之亦然。

Conclusion: 程序员需要综合考虑时间和空间复杂性的权衡来选择最适合特定问题的算法。为帮助理论家和实践者考虑这些权衡，作者创建了https://algorithm-wiki.csail.mit.edu作为参考资源。

Abstract: Algorithm research focuses primarily on how many operations processors need to do (time complexity). But for many problems, both the runtime and energy used are dominated by memory accesses. In this paper, we present the first broad survey of how algorithmic progress has improved memory usage (space complexity). We analyze 118 of the most important algorithm problems in computer science, reviewing the 800+ algorithms used to solve them.
  Our results show that space complexity has become much more important in recent years as worries have arisen about memory access bottle-necking performance (the ``memory wall''). In 20% of cases we find that space complexity improvements for large problems (n=1 billion) outpaced improvements in DRAM access speed, suggesting that for these problems algorithmic progress played a larger role than hardware progress in minimizing memory access delays. Increasingly, we also see the emergence of algorithmic Pareto frontiers, where getting better asymptotic time complexity for a problem requires getting worse asymptotic space complexity, and vice-versa. This tension implies that programmers will increasingly need to consider multiple algorithmic options to understand which is best for their particular problem. To help theorists and practitioners alike consider these trade-offs, we have created a reference for them at https://algorithm-wiki.csail.mit.edu.

</details>


### [26] [Balancing Two-Dimensional Straight-Line Programs](https://arxiv.org/abs/2511.22212)
*Itai Boneh,Estéban Gabory,Paweł Gawrychowski,Adam Górkiewicz*

Main category: cs.DS

TL;DR: 对于二维字符串，无法在保持SLP大小O(g)的同时获得O(log N)深度，但可以通过带洞的2D SLP实现O(g)大小和O(log N)深度的平衡，从而支持O(log N)时间的随机访问。


<details>
  <summary>Details</summary>
Motivation: 研究二维字符串的随机访问问题，探索是否能在保持SLP大小近似不变的情况下获得较小的深度，类似于一维字符串中已知的O(g)大小和O(log N)深度的平衡结果。

Method: 1. 证明下界：构造无限族二维字符串，其2D SLP大小为g，但任何深度为O(log N)的等价2D SLP大小必须为Ω(g·N/log³N)。2. 提出上界：构造深度为O(log N)的2D SLP，大小为O(g·N)。3. 引入带洞的2D SLP概念，利用已知平衡定理构造大小为O(g)、深度为O(log N)的带洞2D SLP。

Result: 1. 下界证明：无法在保持SLP大小O(g)的同时获得O(log N)深度。2. 上界构造：可实现O(g·N)大小和O(log N)深度。3. 带洞2D SLP：可实现O(g)大小和O(log N)深度，从而支持O(log N)时间的随机访问。4. 进一步优化：可获得O(g log^ε N)大小和O(log N/log log N)时间的随机访问结构。

Conclusion: 对于二维字符串，无法像一维情况那样在保持SLP大小不变的同时获得对数深度，但通过引入带洞的2D SLP概念，可以实现大小和深度的平衡，从而构建高效的随机访问数据结构。

Abstract: We consider building, given a straight-line program (SLP) consisting of $g$ productions deriving a two-dimensional string $T$ of size $N\times N$, a structure capable of providing random access to any character of $T$. For one-dimensional strings, it is now known how to build a structure of size $\mathcal{O}(g)$ that provides random access in $\mathcal{O}(\log N)$ time. In fact, it is known that this can be obtained by building an equivalent SLP of size $\mathcal{O}(g)$ and depth $\mathcal{O}(\log N)$ [Ganardi, Jeż, Lohrey, JACM 2021]. We consider the analogous question for two-dimensional strings: can we build an equivalent SLP of roughly the same size and small depth?
  We show that the answer is negative: there exists an infinite family of two-dimensional strings of size $N\times N$ described by a 2D SLP of size $g$ such that any 2D SLP describing the same string of depth $\mathcal{O}(\log N)$ must be of size $Ω(g\cdot N/\log^{3}N)$. We complement this with an upper bound showing how to construct such a 2D SLP of size $\mathcal{O}(g\cdot N)$. Next, we observe that one can naturally define a generalization of 2D SLP, which we call 2D SLP with holes. We show that a known general balancing theorem by [Ganardi, Jeż, Lohrey, JACM 2021] immediately implies that, given a 2D SLP of size $g$ deriving a string of size $N\times N$, we can construct a 2D SLP with holes of depth $\mathcal{O}(\log N)$ and size $\mathcal{O}(g)$. This allows us to conclude that there is a structure of size $\mathcal{O}(g)$ providing random access in $\mathcal{O}(\log N)$ time for such a 2D SLP. Further, this can be extended (analogously as for a 1D SLP) to obtain a structure of size $\mathcal{O}(g \log^εN)$ providing random access in $\mathcal{O}(\log N/\log \log N)$ time, for any $ε>0$.

</details>


### [27] [Efficient Trace Frequency Queries in Sparse Graphs](https://arxiv.org/abs/2511.22289)
*Christine Awofeso,Pål Grønås Drange,Patrick Greaves,Oded Lachish,Felix Reidl*

Main category: cs.DS

TL;DR: 该论文提出了一种在稀疏图上高效计算顶点集X的trace频率的数据结构，使用强2着色数替代退化度，显著提升了查询效率。


<details>
  <summary>Details</summary>
Motivation: 在图中理解顶点与顶点集的关系是基本任务，现有方法在稀疏图上查询时间随|X|指数增长，不实用。需要更高效的算法来处理实际网络中的trace查询问题。

Method: 使用强2着色数s2作为参数，构建数据结构，在O(d·||G||)时间内初始化，查询时间为O((d² + s2^{d+2})|X|)，其中d是退化度。通过启发式算法计算具有小s2值的排序。

Result: 在217个真实网络（最多110万边）上测试，新方法在几乎所有设置中都优于简单方法。证明计算小s2排序的启发式算法是可行的。

Conclusion: 基于强2着色数的数据结构比基于退化度的方法更实用，在稀疏图上能高效处理trace频率查询，为图分析提供了有效的工具。

Abstract: Understanding how a vertex relates to a set of vertices is a fundamental task in graph analysis. Given a graph $G$ and a vertex set $X \subseteq V(G)$, consider the collection of subsets of the form $N(u) \cap X$ where $u$ ranges over all vertices outside $X$. These intersections, which we call the traces of $X$, capture all ways vertices in $G$ connect to $X$, and in this paper we consider the problem of listing these traces efficiently, and the related problem of recording the multiplicity (frequency) of each trace. For a given query set $X$, both problems have obvious algorithms with running time $O(|N(X)| \cdot |X|)$ and conditional lower bounds suggest that, on general graphs, one cannot expect better. However, in certain sparse graph classes, more efficient algorithms are possible: Drange \etal (IPEC 2023) used a data structure that answers trace queries in $d$-degenerate graphs with linear initialisation time and query time that only depends on the query set $X$ and $d$. However, the query time is exponential in $|X|$, which makes this approach impractical. By using a stronger parameter than degeneracy, namely the strong $2$-colouring number $s_2$, we construct a data structure in $O(d \cdot \|G\|)$ time, which answers subsequent trace frequency queries in time $O\big((d^2 + s_2^{d+2})|X|\big)$, where $\|G\|$ is the number of edges of $G$, $s_2$ is the strong $2$-colouring number and $d$ the degeneracy of a suitable ordering of $G$. We demonstrate that this data structure is indeed practical and that it beats the simple, obvious alternative in almost all tested settings, using a collection of 217 real-world networks with up to 1.1M edges. As part of this effort, we demonstrate that computing an ordering with a small strong $2$-colouring number is feasible with a simple heuristic.

</details>


### [28] [Improved exploration of temporal graphs](https://arxiv.org/abs/2511.22604)
*Paul Bastide,Carla Groenland,Lukas Michel,Clément Rambaud*

Main category: cs.DS

TL;DR: 本文改进了时序图探索问题的上界，从O(n^{7/4})提升到O(n^{3/2}√(log n))，并提出了更一般的O(n^{3/2}√(D log n))上界，其中D是平均时序最大度。


<details>
  <summary>Details</summary>
Motivation: 时序图探索是时序图理论中的基本问题，需要找到访问所有顶点的最短时间序列。先前研究在连通且最大度有界的情况下给出了O(n^{7/4})的上界，但这一界限仍有改进空间。本文旨在显著改进这一上界，并为更一般的情况提供统一的分析框架。

Method: 引入平均时序最大度D的概念，定义为所有顶点在时序图中最大度的平均值。通过分析时序图的连通性和度分布特性，设计新的探索算法，证明在更一般的条件下存在更优的上界。

Result: 证明了时序图探索问题存在O(n^{3/2}√(log n))时间步的探索序列，显著改进了之前的O(n^{7/4})上界。更一般地，证明了在平均时序最大度D的条件下，存在O(n^{3/2}√(D log n))时间步的探索。

Conclusion: 本文显著改进了时序图探索问题的上界，为有界平均度、平面图、有界树宽等特殊情形提供了统一的次二次上界，当D=o(n/log n)时即可获得次二次复杂度，扩展了该问题的理论理解。

Abstract: A temporal graph $G$ is a sequence $(G_t)_{t \in I}$ of graphs on the same vertex set of size $n$. The \emph{temporal exploration problem} asks for the length of the shortest sequence of vertices that starts at a given vertex, visits every vertex, and at each time step $t$ either stays at the current vertex or moves to an adjacent vertex in $G_t$. Bounds on the length of a shortest temporal exploration have been investigated extensively. Perhaps the most fundamental case is when each graph $G_t$ is connected and has bounded maximum degree. In this setting, Erlebach, Kammer, Luo, Sajenko, and Spooner [ICALP 2019] showed that there exists an exploration of $G$ in $\mathcal{O}(n^{7/4})$ time steps. We significantly improve this bound by showing that $\mathcal{O}(n^{3/2} \sqrt{\log n})$ time steps suffice.
  In fact, we deduce this result from a much more general statement. Let the \emph{average temporal maximum degree} $D$ of $G$ be the average of $\max_{t \in I} d_{G_t}(v)$ over all vertices $v \in V(G)$, where $d_{G_t}(v)$ denotes the degree of $v$ in $G_t$. If each graph $G_t$ is connected, we show that there exists an exploration of $G$ in $\mathcal{O}(n^{3/2} \sqrt{D \log n})$ time steps. In particular, this gives the first subquadratic upper bound when the underlying graph has bounded average degree. As a special case, this also improves the previous best bounds when the underlying graph is planar or has bounded treewidth and provides a unified approach for all of these settings. Our bound is subquadratic already when $D=o(n/\log n)$.

</details>


### [29] [Sublinear Edge Fault Tolerant Spanners for Hypergraphs](https://arxiv.org/abs/2511.22803)
*Jialin He,Nicholas Popescu,Chunjiang Zhu*

Main category: cs.DS

TL;DR: 该论文首次研究超图中的容错生成器，提出基于聚类的算法构建边容错超生成器，获得次线性规模，并建立了下界证明。


<details>
  <summary>Details</summary>
Motivation: 传统容错生成器在超图中难以直接扩展，简单方法只能得到线性规模，而最优规模应是次线性的，需要开发专门算法。

Method: 采用基于聚类的算法，结合容错聚类技术，构建边容错超生成器，算法复杂度为 $\widetilde{O}(mr^3+fn)$。

Result: 构造出拉伸度为 $2k-1$、规模为 $O(k^2f^{1-1/(rk)}n^{1+1/k}\log n)$ 的边容错超生成器，并建立了 $\Omega(f^{1-1/r-1/rk}n^{1+1/k-o(1)})$ 的下界。

Conclusion: 这是超图容错生成器的首次研究，揭示了传统方法在容错设置下的局限性，提出了改进算法并建立了理论界限，为后续研究奠定了基础。

Abstract: We initiate the study on fault-tolerant spanners in hypergraphs and develop fast algorithms for their constructions. A fault-tolerant (FT) spanner preserves approximate distances under network failures, often used in applications like network design and distributed systems. While classic (fault-free) spanners are believed to be easily extended to hypergraphs such as by the method of associated graphs, we reveal that this is not the case in the fault-tolerant setting: simple methods can only get a linear size in the maximum number of faults $f$. In contrast, all known optimal size of FT spanners are sublinear in $f$. Inspired by the FT clustering technique, we propose a clustering based algorithm that achieves an improved sublinear size bound. For an $n$-node $m$-edge hypergraph with rank $r$ and a sketch parameter $k$, our algorithm constructs edge FT (EFT) hyperspanners of stretch $2k-1$ and size $O(k^2f^{1-1/(rk)}n^{1+1/k}\log n)$ with high probability in time $\widetilde{O}(mr^3+fn)$. We also establish a lower bound of $Ω(f^{1-1/r-1/rk}n^{1+1/k-o(1)})$ edges for EFT hyperspanners, which leaves a gap of poly$(k)f^{1/r}$. Finally, we provide an algorithm for constructing additive EFT hyperspanners by combining multiplicative EFT hyperspanners with additive hyperspanners. We believe that our work will spark interest in developing optimal FT spanners for hypergraphs.

</details>


### [30] [Spanning Trees with a Small Vertex Cover: the Complexity on Specific Graph Classes](https://arxiv.org/abs/2511.22912)
*Toranosuke Kokai,Akira Suzuki,Takahiro Suzuki,Yuma Tamura,Xiao Zhou*

Main category: cs.DS

TL;DR: MCST问题研究具有小顶点覆盖的生成树存在性，揭示了与支配集问题的等价关系，证明了多个图类的NP完全性，并给出了参数化算法和线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究具有理想性质的生成树是算法理论的重要课题。MCST问题关注是否存在顶点覆盖大小不超过k的生成树，该问题在先前研究中存在未解决的开放性问题。

Method: 通过揭示MCST与支配集问题在直径≤2或P5-free图上的等价关系，分析问题的可解性。使用归约方法证明多个图类的NP完全性，并设计基于团宽度的FPT算法和针对区间图的线性时间算法。

Result: 证明了MCST在直径≤2图、P5-free图、二分平面图（最大度4）和单位圆盘图上都是NP完全的，解决了先前研究的开放问题。同时给出了基于团宽度的FPT算法和区间图的线性时间算法。

Conclusion: MCST问题在多个图类中都是难解的，但与支配集问题的等价关系为理解其计算复杂性提供了新视角。参数化算法和特殊图类的高效算法为实际问题提供了解决方案。

Abstract: In the context of algorithm theory, various studies have been conducted on spanning trees with desirable properties. In this paper, we consider the \textsc{Minimum Cover Spanning Tree} problem (MCST for short). Given a graph $G$ and a positive integer $k$, the problem determines whether $G$ has a spanning tree with a vertex cover of size at most $k$. We reveal the equivalence between \mcst\ and the \textsc{Dominating Set} problem when $G$ is of diameter at most~$2$ or $P_5$-free. This provides the intractability for these graphs and the tractability for several subclasses of $P_5$-free graphs. We also show that \mcst\ is NP-complete for bipartite planar graphs of maximum degree~$4$ and unit disk graphs. These hardness results resolve open questions posed in prior research. Finally, we present an FPT algorithm for {\mcst} parameterized by clique-width and a linear-time algorithm for interval graphs.

</details>


### [31] [Towards an algebraic approach to the reconfiguration CSP](https://arxiv.org/abs/2511.22914)
*Kei Kimura*

Main category: cs.DS

TL;DR: 该论文提出了一种基于部分运算的代数方法，用于研究约束满足问题（CSP）的重配置变体（RCSP），将布尔域的复杂度结果扩展到更一般的领域。


<details>
  <summary>Details</summary>
Motivation: RCSP作为CSP的重配置变体，在理论计算机科学中受到越来越多的关注。传统方法主要基于完全运算，难以处理包含等式约束的约简。需要一种新的代数框架来扩展布尔域的复杂度结果到更一般的领域。

Method: 提出了一种新颖的代数方法，使用部分运算而非传统的完全运算。该框架能够捕捉涉及等式约束的约简，为分析RCSP的复杂度提供了更灵活的工具。

Result: 展示了部分运算在识别可处理的RCSP实例方面的多功能性。该方法有助于将布尔域的复杂度结果扩展到更一般的设置。

Conclusion: 部分运算为RCSP的复杂度分析提供了有效的代数框架，能够处理传统完全运算方法难以处理的约束类型，为更广泛领域的重配置问题研究开辟了新途径。

Abstract: This paper investigates the reconfiguration variant of the Constraint Satisfaction Problem (CSP), referred to as the Reconfiguration CSP (RCSP). Given a CSP instance and two of its solutions, RCSP asks whether one solution can be transformed into the other via a sequence of intermediate solutions, each differing by the assignment of a single variable. RCSP has attracted growing interest in theoretical computer science, and when the variable domain is Boolean, the computational complexity of RCSP exhibits a dichotomy depending on the allowed constraint types. A notable special case is the reconfiguration of graph homomorphisms -- also known as graph recoloring -- which has been studied using topological methods. We propose a novel algebraic approach to RCSP, inspired by techniques used in classical CSP complexity analysis. Unlike traditional methods based on total operations, our framework employs partial operations to capture a reduction involving equality constraints. This perspective facilitates the extension of complexity results from Boolean domains to more general settings, demonstrating the versatility of partial operations in identifying tractable RCSP instances.

</details>


### [32] [Solution Discovery for Vertex Cover, Independent Set, Dominating Set, and Feedback Vertex Set](https://arxiv.org/abs/2511.23012)
*Rin Saito,Anouk Sommer,Tatsuhiro Suga,Takahiro Suzuki,Yuma Tamura*

Main category: cs.DS

TL;DR: 本文研究了图论中几个基本顶点子集问题（顶点覆盖、独立集、支配集、反馈顶点集）的解决方案发现问题，分析了不同图类上的计算复杂性，并设计了参数化算法。


<details>
  <summary>Details</summary>
Motivation: 研究图论中基本顶点子集问题的解决方案发现问题，即给定初始令牌放置，能否通过少量修改转化为可行解。这在实际应用中具有重要意义，如网络配置、资源分配等场景。

Method: 1. 针对所有四个问题设计了基于团宽度的XP算法；2. 证明了顶点覆盖、独立集和反馈顶点集发现问题在弦图和直径2的图上是NP完全的；3. 展示了这些问题在分裂图上有多项式时间算法；4. 为反馈顶点集发现问题设计了基于令牌数量的FPT算法。

Result: 1. 成功设计了基于团宽度的XP算法；2. 证明了三个问题在弦图和直径2图上的NP完全性；3. 发现这些问题在分裂图上有多项式时间可解性；4. 为反馈顶点集发现问题设计了基于令牌数的FPT算法。

Conclusion: 解决方案发现问题在图论中具有丰富的复杂性结构：虽然某些图类（弦图、直径2图）上问题困难，但在分裂图上可高效求解。参数化方法（团宽度、令牌数）提供了有效的算法设计途径，为实际问题提供了理论指导。

Abstract: In the solution discovery problem for a search problem on graphs, we are given an initial placement of $k$ tokens on the vertices of a graph and asked whether this placement can be transformed into a feasible solution by applying a small number of modifications. In this paper, we study the computational complexity of solution discovery for several fundamental vertex-subset problems on graphs, namely Vertex Cover Discovery, Independent Set Discovery, Dominating Set Discovery, and Feedback Vertex Set Discovery. We first present XP algorithms for all four problems parameterized by clique-width. We then prove that Vertex Cover Discovery, Independent Set Discovery, and Feedback Vertex Set Discovery are NP-complete for chordal graphs and graphs of diameter 2, which have unbounded clique-width. In contrast to these hardness results, we show that all three problems can be solved in polynomial time on split graphs. Furthermore, we design an FPT algorithm for Feedback Vertex Set Discovery parameterized by the number of tokens.

</details>


### [33] [Improved and Parameterized Algorithms for Online Multi-level Aggregation: A Memory-based Approach](https://arxiv.org/abs/2511.23211)
*Alexander Turoczy,Young-San Lin*

Main category: cs.DS

TL;DR: 提出了针对带截止期的在线多级聚合问题(MLAP-D)的改进参数化算法，包括基于树深度的e(D+1)-竞争算法和基于毛毛虫维度的e(4H+2)-竞争算法，后者在H较小时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的在线多级聚合问题算法要么基于树深度(D)，要么基于对数复杂度(O(log|V|))。本文旨在开发基于更精细结构参数(毛毛虫维度H)的算法，该参数对许多简单但丰富的树类保持常数，从而在这些情况下获得更好的竞争比。

Method: 提出了一个简单框架，直接适用于任何结构的树，而不像先前方法需要将问题规约到特定结构的树。该框架包含两个主要算法：一个基于树深度的e(D+1)-竞争算法，另一个基于毛毛虫维度H的e(4H+2)-竞争算法。

Result: 1) 提出了e(D+1)-竞争算法；2) 提出了e(4H+2)-竞争算法，其中H是树的毛毛虫维度，满足H≤D且H≤log₂|V|；3) 当H=o(min{D,log₂|V|})时，新框架优于现有最佳算法(6(D+1)-竞争和O(log|V|)-竞争)。

Conclusion: 本文首次提出了基于毛毛虫维度这一优于深度的度量的在线算法，为简单但丰富的树类(如线图、毛毛虫图、龙虾图)提供了常数竞争比，扩展了MLAP-D问题的算法设计框架。

Abstract: We study the online multi-level aggregation problem with deadlines (MLAP-D) introduced by Bienkowski et al. (ESA 2016, OR 2020). In this problem, requests arrive over time at the vertices of a given vertex-weighted tree, and each request has a deadline that it must be served by. The cost of serving a request equals the cost of a path from the root to the vertex where the request resides. Instead of serving each request individually, requests can be aggregated and served by transmitting a subtree from the root that spans the vertices on which the requests reside, to potentially be more cost-effective. The aggregated cost is the weight of the transmission subtree. The goal of MLAP-D is to find an aggregation solution that minimizes the total cost while serving all requests.
  We present improved and parameterized algorithms for MLAP-D. Our result is twofold. First, we present an $e(D+1)$-competitive algorithm where $D$ is the depth of the tree. Second, we present an $e(4H+2)$-competitive algorithm where $H$ is the caterpillar dimension of the tree. Here, $H \le D$ and $H \le \log_2 |V|$ where $|V|$ is the number of vertices in the given tree. The caterpillar dimension remains constant for rich but simple classes of trees, such as line graphs ($H=1$), caterpillar graphs ($H=2$), and lobster graphs ($H=3$). To the best of our knowledge, this is the first online algorithm parameterized on a measure better than depth. The state-of-the-art online algorithms are $6(D+1)$-competitive by Buchbinder, Feldman, Naor, and Talmon (SODA 2017) and $O(\log |V|)$-competitive by Azar and Touitou (FOCS 2020). Our framework outperforms the state-of-the-art ratios when $H = o(\min\{D,\log_2 |V|\})$. Our simple framework directly applies to trees with any structure and differs from the previous frameworks that reduce the problem to trees with specific structures.

</details>


### [34] [Homomorphism Testing with Resilience to Online Manipulations](https://arxiv.org/abs/2511.23363)
*Esty Kelman,Uri Meir,Debanuj Nayak,Sofya Raskhodnikova*

Main category: cs.DS

TL;DR: 提出了一种在在线操纵模型下抵抗操纵的群同态测试器，查询复杂度为O(1/ε+log t)，其中ε是距离参数，t是每次查询中对手可擦除或破坏的函数值数量。


<details>
  <summary>Details</summary>
Motivation: 传统的代数性质测试假设无限制、无噪声的函数访问，这在对抗性或动态设置中不成立。需要研究在在线操纵模型下能够抵抗对手擦除或破坏查询响应的测试方法。

Method: 提出"随机符号测试"方法，将已知的F2^n→F2的操纵抵抗线性测试器推广到一般群域和值域。通过为每个随机元素随机选择加号或减号，验证同态条件，而不是仅使用随机元素的和。

Result: 获得了最优的查询复杂度O(1/ε+log t)，恢复了Ben-Or等人在标准属性测试模型中的O(1/ε)界限。对于关键群族获得了改进的群特定查询界限。

Conclusion: 成功将操纵抵抗测试从线性函数扩展到群同态，为在线操纵模型下的代数性质测试提供了新方法，实现了最优查询复杂度。

Abstract: A central challenge in property testing is verifying algebraic structure with minimal access to data. A landmark result addressing this challenge, the linearity test of Blum, Luby, and Rubinfeld (JCSS `93), spurred a rich body of work on testing algebraic properties such as linearity and its generalizations to low-degree polynomials and group homomorphisms. However, classical tests for these properties assume unrestricted, noise-free access to the input function--an assumption that breaks down in adversarial or dynamic settings. To address this, Kalemaj, Raskhodnikova, and Varma (Theory of Computing `23) introduced the online manipulation model, where an adversary may erase or corrupt query responses over time, based on the tester's past queries.
  We initiate the study of {manipulation-resilient} testing for {group homomorphism} in this online model. Our main result is an {optimal} tester that makes $O(1/\varepsilon+\log t)$ queries, where $\varepsilon$ is the distance parameter and $t$ is the number of function values the adversary can erase or corrupt per query. Our result recovers the celebrated $O(1/\varepsilon)$ bound by Ben-Or, Coppersmith, Luby, and Rubinfeld (Random Struct.\ Algorithms `08) for homomorphism testing in the standard property testing model, albeit with a different tester. Our tester, $\mathsf{Random\ Signs\ Test}$, {lifts} known manipulation-resilient linearity testers for $\mathbb{F}_2^n\to \mathbb{F}_2$ to general group domains and codomains by introducing more randomness: instead of verifying the homomorphism condition for a sum of random elements, it uses additions and subtractions of random elements, randomly selecting a sign for each element. We also obtain improved group-specific query bounds for key families of groups.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [35] [Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem](https://arxiv.org/abs/2511.21769)
*Royer David Estrada-Esponda,Gerardo Matturro,Jose Reinaldo Sabogal-Pinilla*

Main category: cs.SE

TL;DR: 研究调查了哥伦比亚软件初创企业创始团队最看重的技术知识和软技能，以及这些需求如何随公司成长而变化


<details>
  <summary>Details</summary>
Motivation: 创业团队成员的技术知识和软技能对软件初创企业的早期阶段有重要影响，创业成功与否很大程度上取决于创始团队的质量，因此需要了解哪些知识和技能最受重视

Method: 在哥伦比亚创业生态系统中进行调查研究，通过对软件初创企业代表进行问卷调查，分析他们最看重的技术知识和软技能

Result: 最受重视的技术知识包括：需求工程、软件测试、项目规划与管理、敏捷方法、市场营销、商业模式定义和预算编制；最受重视的软技能是沟通、领导力和团队合作

Conclusion: 这项研究的结果对软件创业者、孵化器和研究人员具有重要意义，帮助他们理解初创企业成长过程中对知识和技能需求的变化

Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.

</details>


### [36] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 提出基于LLM的多语言代码重构框架，通过提示工程和指令微调提升重构效果，在Java上达到最高正确率99.99%


<details>
  <summary>Details</summary>
Motivation: 现有重构方法依赖手工规则，难以跨语言泛化，需要开发能处理多种编程语言的自动化重构方案

Method: 结合微调提示工程和少样本学习的多语言代码重构模型，研究温度参数和不同样本策略对重构效果的影响

Result: Java在10-shot设置下正确率最高达99.99%，平均可编译性94.78%；Python在所有样本设置下结构距离最小，保持中等相似度

Conclusion: LLM结合提示工程能有效实现多语言代码重构，不同语言在重构中展现不同特性，为自动化代码优化提供新途径

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [37] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 提出基于事件链驱动、LLM赋能的汽车代码生成工作流，从自然语言需求生成经过验证的汽车代码


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言需求生成汽车代码时面临的挑战：减少LLM幻觉、确保架构正确性、保证行为一致性和实时可行性

Method: 1) RAG层从车辆信号规范(VSS)目录检索相关信号作为上下文；2) 信号映射验证后转换为编码因果和时序约束的事件链；3) 事件链指导约束LLM代码合成

Result: 在紧急制动案例研究中，实现了有效的信号使用和一致的代码生成，无需LLM重新训练

Conclusion: 提出的工作流能够生成经过验证的汽车代码，确保架构正确性和行为一致性，为汽车软件开发提供有效解决方案

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [38] [Advancing Automated In-Isolation Validation in Repository-Level Code Translation](https://arxiv.org/abs/2511.21878)
*Kaiyao Ke,Ali Reza Ibrahimzada,Rangeet Pan,Saurabh Sinha,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: TRAM是一个结合上下文感知类型解析和基于模拟的隔离验证的仓库级代码翻译框架，在Java到Python翻译中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 尽管仓库级代码翻译有所进展，但验证翻译结果仍然具有挑战性。现有方法要么依赖语言互操作性需要大量人工工作，要么使用代理进行验证成本高昂

Method: 1) 检索API文档和上下文代码信息进行类型解析；2) 使用LLM结合检索信息进行跨语言类型映射；3) 通过自定义序列化/反序列化工作流在目标语言中自动构建等效模拟对象；4) 对每个方法片段进行隔离验证

Result: TRAM在Java到Python翻译中展示了最先进的性能，证明了其RAG-based类型解析与可靠隔离验证集成的有效性

Conclusion: TRAM通过结合上下文感知类型解析和基于模拟的隔离验证，实现了高质量的跨语言代码翻译，解决了现有方法在验证方面的挑战

Abstract: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.

</details>


### [39] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 评估开源大语言模型在科学数据分析和可视化代码生成中的可信度，发现无人工干预时可靠性有限，提出了三种改进策略并创建了可复用的基准测试


<details>
  <summary>Details</summary>
Motivation: 随着科学日益数据密集化，许多领域科学家缺乏编程专业知识来开发自定义数据分析工作流，这阻碍了及时有效的洞察。大语言模型通过从自然语言描述生成可执行代码提供了有前景的解决方案，但其在科学数据分析和可视化中的可信度需要评估。

Method: 构建反映真实世界研究任务的领域启发式提示基准套件，系统评估生成代码的可执行性和正确性。设计了三种互补策略：数据感知提示消歧、检索增强提示改进和迭代错误修复，并评估这些方法的有效性。

Result: 研究发现，在没有人工干预的情况下，LLM生成代码的可靠性有限，失败通常由模糊提示和模型对领域特定上下文理解不足引起。提出的三种改进策略显著提高了执行成功率和输出质量，但仍需进一步改进。

Conclusion: 这项工作突出了LLM驱动自动化在科学工作流程中的前景和当前局限性，引入了可操作的技术和可复用的基准，用于构建更具包容性、可访问性和可信度的AI辅助研究工具。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [40] [Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications](https://arxiv.org/abs/2511.21956)
*M. Polzin,M. Guzman*

Main category: cs.SE

TL;DR: 现代化遗留应用时，不应简单复制原有设计，而应通过用户参与创建更直观、支持任务效率的新应用，利用现有应用作为改进参考而非限制。


<details>
  <summary>Details</summary>
Motivation: 现代化遗留应用时，人们倾向于创建工具和设计更新的简单复制品，但这会延续原有痛点、低效流程，最终无法解决现有问题。现有应用虽然带来技术债务，但也提供了专家用户的熟悉度和操作完整性，需要平衡保留优点与改进不足。

Method: 通过让用户参与现代化过程，在简单复制原有设计和引入全新GUI设计之间找到平衡点。将现有应用作为开发新应用的参考工具，利用其提供的洞察来指导改进，而不是从零开始。

Result: 用户参与的方法能够弥合简单复制与全新设计之间的差距，创建出既满足专家用户熟悉度需求，又能解决原有痛点、提高效率的现代化应用。现有应用不再是开发障碍，而是宝贵的参考资源。

Conclusion: 现代化遗留应用时，不应简单复制，而应通过用户参与利用现有应用作为指导，创建更直观、高效的新应用，平衡专家用户熟悉度与解决原有问题的需求。

Abstract: When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.

</details>


### [41] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: DRS-OSS是一个开源的风险评分系统，使用微调的Llama 3.1 8B模型分析代码变更风险，帮助大型开源项目优先审查高风险提交，防止缺陷引入。


<details>
  <summary>Details</summary>
Motivation: 大型开源项目每天有数百个拉取请求，每个都可能引入回归缺陷。需要一种方法来评估代码变更的风险，以便更好地进行审查优先级排序、测试规划和CI/CD门控。

Method: 使用微调的Llama 3.1 8B序列分类器，结合提交信息、结构化差异和变更指标的长上下文表示。通过参数高效适配、4位QLoRA和DeepSpeed ZeRO-3 CPU卸载技术，在单张20GB GPU上训练22k令牌的上下文。

Result: 在ApacheJIT基准测试中达到最先进性能（F1=0.64，ROC-AUC=0.89）。模拟显示，仅门控风险最高的30%提交就能防止高达86.4%的缺陷引入变更。

Conclusion: DRS-OSS提供了一个实用的开源解决方案，通过API、Web UI和GitHub插件集成到开发者工作流中，帮助项目有效管理代码变更风险，提高软件质量。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [42] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: Mnimi是一种LLM缓存设计模式，通过类型封装统计约束，确保缓存响应在概率工作流中的统计独立性


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高且速度慢，本地缓存可降低成本延迟，但简单重用缓存会破坏统计独立性，而现有缓存系统缺乏统计约束机制

Method: 引入Mnimi设计模式，在LLM引用类型中封装统计约束，使用装饰器和无限序列迭代器在Python中实现

Result: 在SpecFix程序规范修复系统的案例研究中，Mnimi提高了可重现性、调试便利性、时间和成本效率，同时保持统计正确性

Conclusion: Mnimi为模块化LLM工作流提供了统计完整性保障的设计模式，解决了缓存与统计独立性的矛盾

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [43] [Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem](https://arxiv.org/abs/2511.22186)
*Chayanid Termphaiboon,Raula Gaikovina Kula,Youmei Fan,Morakot Choetkiertikul,Chaiyong Ragkhitwetsagul,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究PyPI项目中安全政策（SECURITY.md）与依赖管理的关系，发现拥有安全政策的项目依赖更广泛但深度相似，且后期采用者依赖更新更频繁。


<details>
  <summary>Details</summary>
Motivation: 尽管SECURITY.md等安全政策在开源项目中日益普及，但它们如何影响软件依赖的结构和演化尚不清楚。软件依赖的互联性影响功能和安全性，需要研究安全政策与依赖管理的关系。

Method: 分析PyPI项目中拥有和没有SECURITY.md文件的项目，检查它们的依赖树并追踪依赖随时间的变化情况。

Result: 拥有安全政策的项目倾向于依赖更广泛的直接依赖，但整体深度和传递依赖相似。历史上，SECURITY.md引入后创建的项目（特别是后期采用者）显示更频繁的依赖更新。

Conclusion: 安全政策与更模块化和功能丰富的项目相关，SECURITY.md在促进主动依赖管理和降低软件供应链风险方面发挥重要作用。

Abstract: Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.

</details>


### [44] [UniBOM -- A Unified SBOM Analysis and Visualisation Tool for IoT Systems and Beyond](https://arxiv.org/abs/2511.22359)
*Vadim Safronov,Ionut Bostan,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: UniBOM是一个先进的SBOM生成、分析和可视化工具，专门用于增强网络系统的安全可追溯性，特别针对二进制分析和非包管理语言如C/C++，通过集成多种分析技术实现细粒度漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 现代网络系统依赖复杂的软件栈，这些软件栈往往隐藏着由复杂依赖关系产生的漏洞。现有的SBOM解决方案在二进制分析和非包管理语言（如C/C++）方面缺乏精确性，无法有效识别依赖关系和缓解安全风险。

Method: UniBOM集成了二进制分析、文件系统分析和源代码分析三种技术。关键特性包括历史CPE追踪、基于AI的漏洞严重性和内存安全分类，以及对非包管理的C/C++依赖的支持。该工具采用端到端统一分析和可视化解决方案。

Result: 通过对258个无线路由器固件二进制文件和四个流行物联网操作系统源代码的比较漏洞分析，证明了UniBOM相比其他广泛使用的SBOM生成和分析工具具有更优越的检测能力。该工具已打包为开源软件分发。

Conclusion: UniBOM为网络系统和更广泛的软件提供了先进的SBOM驱动的安全管理解决方案，通过统一的端到端分析和可视化能力，提升了软件物料清单在安全可追溯性方面的有效性。

Abstract: Modern networked systems rely on complex software stacks, which often conceal vulnerabilities arising from intricate interdependencies. A Software Bill of Materials (SBOM) is effective for identifying dependencies and mitigating security risks. However, existing SBOM solutions lack precision, particularly in binary analysis and non-package-managed languages like C/C++.
  This paper introduces UniBOM, an advanced tool for SBOM generation, analysis, and visualisation, designed to enhance the security accountability of networked systems. UniBOM integrates binary, filesystem, and source code analysis, enabling fine-grained vulnerability detection and risk management. Key features include historical CPE tracking, AI-based vulnerability classification by severity and memory safety, and support for non-package-managed C/C++ dependencies.
  UniBOM's effectiveness is demonstrated through a comparative vulnerability analysis of 258 wireless router firmware binaries and the source code of four popular IoT operating systems, highlighting its superior detection capabilities compared to other widely used SBOM generation and analysis tools. Packaged for open-source distribution, UniBOM offers an end-to-end unified analysis and visualisation solution, advancing SBOM-driven security management for dependable networked systems and broader software.

</details>


### [45] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: NOMAD是一个认知启发的模块化多智能体框架，通过任务分解专门生成UML类图，在性能上超越基线方法，并首次系统分类了LLM生成UML图的错误类型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中应用日益广泛，但其生成UML图等结构化产物的能力尚未充分探索。现有方法在生成结构化模型方面存在局限，需要更可靠的语言到模型的工作流程。

Method: NOMAD采用认知启发的模块化多智能体框架，将UML生成分解为实体提取、关系分类、图合成等专门子任务，每个智能体处理不同的建模活动，模仿工程师的目标导向推理过程。

Result: NOMAD在所有选定的基线方法中表现最佳，同时揭示了在细粒度属性提取方面的持续挑战。研究首次系统分类了LLM生成UML图的错误类型（结构、关系、语义/逻辑），并探讨了验证策略的混合效果。

Conclusion: NOMAD既是一个有效的UML类图生成框架，也为可靠的语言到模型工作流程的广泛研究挑战提供了视角，展示了任务分解在提高可解释性和针对性验证方面的价值。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [46] [Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X](https://arxiv.org/abs/2511.22513)
*Jérôme Pfeiffer,Nicolai Maisch,Sebastian Friedl,Matthias Milan Strljic,Armin Lechler,Oliver Riedel,Andreas Wortmann*

Main category: cs.SE

TL;DR: 提出一种基于领域特定语言（DSL）的方法，使领域专家能够以声明式、人类可读且机器可执行的方式定义数据使用策略，用于制造业数据空间中的主权数据共享。


<details>
  <summary>Details</summary>
Motivation: 随着GAIA-X和国际数据空间（IDS）等联邦数据空间的采用，制造业生态系统需要跨组织边界的安全主权数据共享。虽然AAS、EDC、ID-Link和OPC UA等技术提供了基础，但如何让非软件工程背景的领域专家实际描述和执行上下文相关的数据使用策略仍然是一个重大挑战。

Method: 提出利用领域特定语言（DSL）的方法，使领域专家能够以声明式、人类可读且机器可执行的方式定义数据治理策略，而无需编写命令式代码。DSL支持细粒度的数据治理要求，如限制特定生产批次数据的访问或定义保留期后自动删除等。

Result: 该方法为领域专家提供了直观的策略定义工具，使他们能够直接指定复杂的数据治理规则，解决了传统方法中需要软件工程专业知识的问题，促进了数据空间连接器中的主权数据共享。

Conclusion: 通过DSL方法，制造业生态系统中的领域专家能够有效定义和执行数据使用策略，解决了数据空间实施中的关键障碍，推动了跨工厂流程优化、预测性维护和供应商集成等用例的实现。

Abstract: The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.

</details>


### [47] [The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods](https://arxiv.org/abs/2511.22726)
*Ethan Friesen,Sasha Morton-Salmon,Md Nahidul Islam Opu,Shahidul Islam,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该研究首次大规模分析了"极度易错方法"（涉及多次bug修复的方法），发现它们虽然只占所有方法的极小部分，却导致了不成比例的大量bug。这些方法在创建时就比单次bug方法和无bug方法更大、更复杂、可读性更差。尽管存在这些可测量的差异，但早期预测仍然不可靠，需要更丰富的、考虑代码演化的表示方法。


<details>
  <summary>Details</summary>
Motivation: 识别那些反复吸引bug的源代码子集对于减少长期维护成本至关重要。研究旨在了解极度易错方法的普遍性、特征和可预测性，为开发人员提供早期识别高风险方法的工具。

Method: 使用98个开源Java项目的125万多个方法的数据集，分析极度易错方法的特征。进行了五个机器学习模型的全面评估，并对265个极度易错方法进行了主题分析，揭示视觉问题、上下文角色和常见缺陷模式。

Result: 极度易错方法只占所有方法的极小部分，却导致了不成比例的大量bug。这些方法在创建时就显著更大、更复杂、可读性更差、可维护性更低。然而，早期预测仍然高度不可靠，主要由于数据不平衡、项目异质性以及许多bug是通过后续演化而非初始实现产生的。

Conclusion: 研究强调了需要更丰富的、考虑代码演化的表示方法来改进极度易错方法的早期预测。主题分析为实践者提供了可操作的见解，帮助他们优先处理高风险方法，包括关注核心逻辑、数据转换、外部资源处理等关键角色，以及识别常见的缺陷模式。

Abstract: Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.

</details>


### [48] [MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement](https://arxiv.org/abs/2511.22921)
*Hengyuan Liu,Xia Song,Yong Liu,Zheng Li*

Main category: cs.SE

TL;DR: 提出DKMR方法，通过信号处理技术对MBFL中的kill矩阵进行去噪，提升故障定位效果


<details>
  <summary>Details</summary>
Motivation: 基于突变的故障定位(MBFL)存在噪声问题，特别是突变体与测试之间的虚假kill关系，这会显著降低定位效果。现有方法主要修正最终定位结果，而非直接处理底层噪声。

Method: 提出DKMR方法，将kill矩阵视为包含故障相关模式和噪声的信号，采用两阶段处理：1) 通过混合矩阵构造进行信号增强以提高信噪比；2) 通过频域滤波进行信号去噪，抑制噪声同时保留故障模式。基于此开发MBFL-DKMR框架，使用精炼后的模糊值矩阵计算可疑度。

Result: 在Defects4J v2.0.0上的评估显示，MBFL-DKMR有效缓解了噪声问题，优于最先进的MBFL技术：Top-1定位了129个故障（BLMu为85个，Delta4Ms为103个），计算开销极小（0.11秒，占总时间的0.001%）。

Conclusion: 该工作展示了信号处理技术在通过精炼kill矩阵来增强MBFL效果方面的潜力，为解决MBFL中的噪声问题提供了新思路。

Abstract: Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.

</details>


### [49] [A transfer learning approach for automatic conflicts detection in software requirement sentence pairs based on dual encoders](https://arxiv.org/abs/2511.23007)
*Yizheng Wang,Tao Jiang,Jinyan Bai,Zhengbin Zou,Tiancheng Xue,Nan Zhang,Jie Luan*

Main category: cs.SE

TL;DR: 提出TSRCDF-SS框架，结合SBERT和SimCSE双编码器，通过六元素拼接策略和混合损失优化，提升软件需求冲突检测的准确性和跨域迁移能力。


<details>
  <summary>Details</summary>
Motivation: 软件需求文档通常包含数万个需求，确保需求间一致性对项目成功至关重要。现有自动化检测方法面临数据不平衡导致检测精度低、单编码器语义提取有限、跨域迁移学习性能不佳等挑战。

Method: 1) 使用SBERT和SimCSE双独立编码器为需求对生成句子嵌入，采用六元素拼接策略；2) 通过两层全连接前馈神经网络分类器，结合改进的Focal Loss变体、领域特定约束和置信度惩罚项的混合损失优化；3) 协同整合序列和跨域迁移学习。

Result: 实验结果显示，该框架在域内设置中macro-F1和weighted-F1分数均提升10.4%，在跨域场景中macro-F1提升11.4%。

Conclusion: TSRCDF-SS框架通过双编码器设计、混合损失优化和迁移学习策略，有效解决了软件需求冲突检测中的关键挑战，显著提升了检测性能。

Abstract: Software Requirement Document (RD) typically contain tens of thousands of individual requirements, and ensuring consistency among these requirements is critical for the success of software engineering projects. Automated detection methods can significantly enhance efficiency and reduce costs; however, existing approaches still face several challenges, including low detection accuracy on imbalanced data, limited semantic extraction due to the use of a single encoder, and suboptimal performance in cross-domain transfer learning. To address these issues, this paper proposes a Transferable Software Requirement Conflict Detection Framework based on SBERT and SimCSE, termed TSRCDF-SS. First, the framework employs two independent encoders, Sentence-BERT (SBERT) and Simple Contrastive Sentence Embedding (SimCSE), to generate sentence embeddings for requirement pairs, followed by a six-element concatenation strategy. Furthermore, the classifier is enhanced by a two-layer fully connected feedforward neural network (FFNN) with a hybrid loss optimization strategy that integrates a variant of Focal Loss, domain-specific constraints, and a confidence-based penalty term. Finally, the framework synergistically integrates sequential and cross-domain transfer learning. Experimental results demonstrate that the proposed framework achieves a 10.4% improvement in both macro-F1 and weighted-F1 scores in in-domain settings, and an 11.4% increase in macro-F1 in cross-domain scenarios.

</details>


### [50] [APDT: A Digital Twin for Assessing Access Point Characteristics in a Network](https://arxiv.org/abs/2511.23009)
*D. Sree Yashaswinee,Gargie Tambe,Y. Raghu Reddy,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 提出名为APDT的接入点数字孪生系统，用于网络性能监控、预测和优化，通过实时数据采集和NS-3仿真来预测流量拥塞并改善服务质量。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在网络领域的应用尚未充分探索，当前网络面临客户端密度增加和流量拥塞等问题，需要更好的监控和预测工具来改善网络性能和服务质量。

Method: 开发APDT系统，通过Ruckus SmartZone API从三个接入点收集实时数据，结合NS-3仿真平台创建网络数字孪生模型，并构建预测模型来预测流量拥塞和进行客户端主动卸载。

Result: 初步结果表明APDT能够成功预测短期流量激增，从而改善服务质量并减少流量拥塞，在校园网络测试中验证了系统的有效性。

Conclusion: APDT数字孪生系统为网络管理提供了有效的监控、仿真和预测工具，能够通过预测性分析和主动管理优化网络性能，展示了数字孪生技术在网络领域的应用潜力。

Abstract: Digital twins (DT) have emerged as a transformative technology, enabling real-time monitoring, simulations, and predictive maintenance across various domains, though their Application in the networking domain remains underexplored. This paper focuses on issues such as increasing client density and traffic congestion by proposing a digital twin for computer networks. Our Digital Twin, named Access Point Digital Twin (APDT) is used for tracking user behavior and changing bandwidth demands, directly impacting network performance and Quality of Service (QoS) parameters like latency, jitter, etc. APDT captures the real-time state of networks with data from access points (APs), enabling simulation-based analyses and predictive modelling. APDT facilitates the simulation of various what-if scenarios thereby providing a better understanding of various aspects of the network characteristics. We tested APDT on our University network. APDT uses data collected from three access points via the Ruckus SmartZone API and incorporates NS-3 based simulations. The simulation replicates a real-time snapshot from a Ruckus access point and models metrics such as latency and inter-packet transfer time. Additionally, a forecasting model predicts traffic congestion and suggests proactive client offloading, enhancing network management and performance optimization. Preliminary results indicate that APDT can successfully predict short-term traffic surges, leading to improved QoS and reduced traffic congestion.

</details>


### [51] [Software for Studying CASCADE Error Correction Protocols in Quantum Communications](https://arxiv.org/abs/2511.23050)
*Nikita Repnkiov,Vladimir Faerman*

Main category: cs.SE

TL;DR: 开发了基于CASCADE协议的量子密钥协调软件原型，采用并行纠错算法提升效率，但存在消息传递计算成本高、错误处理复杂等问题，提出了架构重构、数据导出接口等改进方向。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁下量子通信方法的发展需求，强调密钥协调在量子通信系统中的重要性，需要研究高效的密钥协调协议实现。

Method: 基于CASCADE协议设计软件原型，实现基于actor模型的并行纠错算法，减少交换数据量，提升密钥协调效率。

Result: 原型验证了CASCADE核心算法的正确实现，但存在消息传递计算成本高、错误处理复杂、代码冗余等限制，并行算法提升了效率。

Conclusion: 需要重构系统架构、开发数据导出接口、分离通信通道组件、扩展验证工具，为未来盲密钥协调方法的系统比较研究奠定基础。

Abstract: This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.

</details>


### [52] [Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning](https://arxiv.org/abs/2511.23157)
*Hana Kataoka,Jialong Li,Yutaka Matsuno*

Main category: cs.SE

TL;DR: 最新LLM在软件工程PBL教学中具有双重作用：既是"均衡器"提升平均表现，又是"放大器"扩大成绩差距


<details>
  <summary>Details</summary>
Motivation: 随着LLM重塑软件开发，将其整合到软件工程教育中变得迫切。现有研究主要关注LLM在入门编程或孤立SE任务中的教育应用，但在更开放的项目式学习(PBL)中的影响尚未探索。

Method: 进行为期两年的纵向研究，比较2024年（使用早期免费LLM，n=48）和2025年（使用最新付费LLM，n=46）两个学生群体。

Result: 最新强大的LLM具有双重作用：作为"均衡器"提升平均表现，特别是帮助编程能力较弱的学生，为更真实的SE实践提供机会；同时作为"放大器"显著扩大绝对成绩差距，为教育公平带来新的教学挑战。

Conclusion: LLM在软件工程PBL教学中具有复杂的双重影响，既提供了提升教学效果的机会，也带来了新的教育公平挑战，需要相应的教学策略来应对。

Abstract: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

</details>


### [53] [AI for software engineering: from probable to provable](https://arxiv.org/abs/2511.23159)
*Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文提出结合AI编程的创造力与形式化规范方法及程序验证的严谨性，以解决AI编程中的目标指定困难和幻觉问题


<details>
  <summary>Details</summary>
Motivation: AI编程面临两大障碍：目标指定困难（提示工程本质上是需求工程，是软件工程中最困难的领域之一）和幻觉现象。程序只有在正确或接近正确时才有用

Method: 将人工智能的创造力与形式化规范方法和形式化程序验证相结合，并借助现代证明工具的支持

Result: 未在摘要中明确说明具体结果，但提出了解决方案框架

Conclusion: 通过结合AI的创造力与形式化方法的严谨性，可以解决AI编程中的核心问题，使AI生成的程序更加可靠和有用

Abstract: Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals ("prompt engineering" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.
  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.

</details>


### [54] [GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis](https://arxiv.org/abs/2511.23213)
*Samuele Doria,Eleonora Losiouk*

Main category: cs.SE

TL;DR: GAPS是一个结合静态分析和动态执行的Android方法可达性分析系统，能够自动生成路径到达目标方法，在基准测试中显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: Android应用中动态解析方法可达性是一个关键但未解决的问题。现有工具无法可靠地驱动执行到达特定目标方法（特别是非图形组件中的方法），而这对漏洞验证、调试和行为分析等任务至关重要。

Method: GAPS集成了静态、方法引导的调用图分析和动态、交互驱动的执行。系统通过数据流分析引导的轻量级后向调用图遍历来重建到达目标方法的路径，然后将这些路径转换为指导运行时应用探索的指令。

Result: 在AndroTest基准测试中，GAPS静态识别出88.24%目标方法的路径（平均4.27秒/应用），动态到达57.44%的目标方法。相比之下，最佳动态工具APEs达到12.82%，FlowDroid静态分析达到58.81%（35.06秒/应用）。在50个真实应用中，GAPS静态重建62.03%目标方法的路径（平均278.9秒），动态到达59.86%。

Conclusion: GAPS是第一个集成静态和动态分析来解决Android方法可达性问题的系统，在基准测试和真实应用中均显著优于现有工具，展示了在实际安全关键代码分析中的实用价值。

Abstract: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.
  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.
  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

</details>


### [55] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 提出MBFL-FLIM框架，通过LLM语义分析识别和缓解干扰突变体，提升基于突变的故障定位效果


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的故障定位(MBFL)面临干扰突变体问题，这些从非故障代码生成的突变体会被失败测试杀死，模仿真实故障行为，削弱故障定位效果

Method: 1) 提出故障定位干扰突变体(FLIMs)概念，基于RIPR模型进行理论分析识别四种干扰原因；2) 使用LLM语义分析识别FLIMs，通过微调和置信度估计解决LLM输出不稳定问题；3) 通过精炼可疑度分数缓解FLIMs影响，集成到MBFL工作流中形成MBFL-FLIM框架

Result: 在Defects4J基准的395个程序版本上，使用8个LLM进行实验，MBFL-FLIM在Top-1指标上平均改进44个故障，优于传统SBFL/MBFL方法、动态特征方法和近期LLM故障定位技术，在多故障场景下表现稳健

Conclusion: MBFL-FLIM通过LLM语义分析有效识别和缓解干扰突变体，显著提升基于突变的故障定位效果，微调和置信度估计组件对性能提升有重要贡献

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [56] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: C2C-MoLA：结合MoE和LoRA的多模态框架，用于图表到代码生成，提升准确率17%、减少GPU内存18%、加速收敛20%


<details>
  <summary>Details</summary>
Motivation: 现有图表到代码生成方法在跨类型泛化、内存效率和模块化设计方面存在不足，需要更高效的解决方案

Method: 提出C2C-MoLA框架，结合混合专家(MoE)和低秩适应(LoRA)。MoE使用复杂度感知路由机制，LoRA实现参数高效更新，配合定制训练策略

Result: 在Chart2Code-160k数据集上，相比标准微调和纯LoRA基线，准确率提升17%，GPU峰值内存减少18%，收敛速度加快20%

Conclusion: C2C-MoLA有效解决了图表到代码生成中的泛化、效率和模块化问题，通过MoE-LoRA协同实现了性能、内存和收敛速度的平衡

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [57] [Sensing and Understanding the World over Air: A Large Multimodal Model for Mobile Networks](https://arxiv.org/abs/2511.21707)
*Zhuoran Duan,Yuhao Wei,Guoshun Nan,Zijun Wang,Yan Yan,Lihua Xiong,Yuhan Ran,Ji Zhang,Jian Li,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 该论文提出了一种无线原生多模态大模型（WMLM），利用无线信号作为锚定模态进行对比学习，在真实大规模数据集上训练GPT风格的模型，验证了无线信号作为通用模态的可行性。


<details>
  <summary>Details</summary>
Motivation: 大型模型（如ChatGPT）在多个领域产生重大影响，有望推动网络智能发展。无线原生多模态大模型（WMLMs）能够通过多模态数据感知和理解物理世界，是通信、感知和智能融合的关键使能技术，但目前WMLMs研究仍处于起步阶段，针对无线网络的领域特定多模态大模型构建尚未充分探索。

Method: 提出无线原生多模态训练范式，构建GPT风格的WMLM模型，在真实世界大规模数据集上进行训练，利用无线信号作为锚定模态进行对比学习。

Result: 该方法相比现有小规模模型和大多模态模型表现出优异性能，验证了使用无线信号作为通用模态的可行性，凸显了WMLM作为未来无线网络新范式的潜力。

Conclusion: 无线原生多模态大模型（WMLMs）有望成为未来无线网络的新范式，通过无线信号作为锚定模态的对比学习方法有效整合通信、感知和智能，为智能服务提供强大支持。

Abstract: Large models (LMs), such as ChatGPT, have made a significant impact across diverse domains and hold great potential to facilitate the evolution of network intelligence. Wireless-native multi-modal large models (WMLMs) can sense and understand the physical world through multi-modal data, serving as a key enabler that integrates communication, sensing, and intelligence, and thus they can boost various smart services to billions of users. However, research on WMLMs remains in its infancy, and the construction of domain-specific multi-modal large models for wireless networks is still underexplored. In this paper, we outlines the key characteristics of WMLMs and summarizes existing methods, on the basis of which a wireless-native multimodal training paradigm is proposed. Specifically, we constructed a GPT-style WMLM model and trained it on a real-world large-scale dataset, leveraging wireless signals as an anchor modality for contrastive learning. Our approach demonstrates outstanding performance compared with existing small-scale models and large multi-modal models, validating the feasibility of using wireless signals as a universal modality and highlighting WMLM's potential to emerge as a new paradigm for future wireless networks.

</details>


### [58] [Secure Command, Control and Communications Systems (C3) for Army UxVs](https://arxiv.org/abs/2511.21936)
*T. Rebolo,A. Grilo,C. Ribeiro*

Main category: cs.NI

TL;DR: 为军用无人机设计的安全指挥控制系统NC2S，采用零信任架构和mTLS/ECDSA加密，在战术无线电上验证可行


<details>
  <summary>Details</summary>
Motivation: 军用无人机广泛使用但依赖不安全的MAVLink协议，缺乏认证和加密机制，需要安全可靠的指挥控制系统来保护通信安全

Method: 设计NC2S系统，采用零信任模型和分层凭证权限管理，使用mTLS/ECDSA证书和ECDH密钥交换，HMAC确保消息完整性，开发轻量级协议管理凭证、密钥更新和控制移交

Result: 在Wi-Fi和Rohde&Schwarz HR-5000H战术无线电上验证原型，战术无线电延迟比宽带技术高约两个数量级，但仍能保持稳定通信和最小消息丢失

Conclusion: NC2S系统能确保无人机指挥控制的机密性、完整性和认证，战术无线电虽然延迟较高但仍适用于TC终端和GCS之间的通信

Abstract: Unmanned Vehicles (UxVs) are increasingly used in modern military operations for reconnaissance, surveillance, and strike missions, enhancing situational awareness while reducing risk to personnel. Their affordability and rapid deployment have encouraged the adoption of commercial solutions. However, many rely on insecure protocols such as MAVLink, which lack authentication and encryption mechanisms. This paper designed, implemented, and evaluated a new secure command-and-control architecture that ensures confidentiality, integrity, and authentication (CIA) while supporting real-time control delegation between Ground Control Stations (GCSs). The proposed solution, named New Command and Control System (NC2S), enforces a zero-trust model integrating hierarchical credential-based privileges to regulate access and control among Tactical Commanders (TC), GCSs, and UxVs. It employs mutual Transport Layer Security (mTLS) with Elliptic Curve Digital Signature Algorithm (ECDSA) certificates and Elliptic Curve Diffie-Hellman (ECDH) key exchange, while message integrity is ensured through Hash-based Message Authentication Codes (HMAC). Multiple lightweight protocols were developed for credential management, key renewal, and control handover. The NC2S prototype was experimentally validated over Wi-Fi and Rohde&Schwarz HR-5000H tactical radios. Results showed that HR-5000H links introduce latencies roughly two orders of magnitude higher than broadband technologies (e.g., Wi-Fi or 5G&Beyond technologies) but are still able to maintain stable communication with minimal message loss, making them suitable for the NC2S links among TC terminals and GCSs.

</details>


### [59] [Resilient and Reliable Cloud Network Control for Mission-Critical Latency-Sensitive Service Chains](https://arxiv.org/abs/2511.21960)
*Chin-Wei Huang,Jaime Llorca,Antonia M. Tulino,Andreas F. Molisch*

Main category: cs.NI

TL;DR: 提出MC-LC-ResRNC问题，设计MC-ResRCNC算法，在云网络中同时保证可靠性和弹性，满足时延敏感服务的需求。


<details>
  <summary>Details</summary>
Motivation: 随着任务关键型时延敏感服务的普及，下一代云集成网络需要同时保证可靠和弹性的服务交付。现有研究主要关注可靠网络控制策略，但缺乏结合可靠性和弹性的综合解决方案。

Method: 将多商品最小成本弹性可靠网络控制问题（MC-LC-ResRNC）建模为具有长期和短期及时吞吐量约束的随机控制问题，提出多商品弹性和可靠云网络控制算法（MC-ResRCNC）。

Result: 通过数值实验证明，MC-ResRCNC算法能够在正常情况下保证可靠性，在网络故障时提供弹性恢复能力。

Conclusion: 该研究填补了同时保证网络可靠性和弹性的控制策略空白，为下一代云集成网络提供了综合解决方案。

Abstract: The proliferation of mission-critical latency-sensitive services has intensified the demand for next-generation cloud-integrated networks to guarantee both reliable and resilient service delivery. While reliability imposes timely-throughput requirements, i.e., percentage of packets to be delivered within a prescribed per-packet deadline, resilience relates to the network's ability to swiftly recover timely-throughput performance following an outage event, such as node or link failures. While recent studies have increasingly focused on designing reliable network control policies, a comprehensive solution that combines reliable and resilient network control has yet to be fully explored. This paper formulates the multi-commodity least-cost resilient and reliable network control (MC-LC-ResRNC) problem as a stochastic control problem with long and short-term timely throughput constraints. We then present a solution through the Multi-Commodity Resilient and Reliable Cloud Network Control (MC-ResRCNC) algorithm and show through numerical experiments that it jointly ensures reliability under normal conditions and resilience upon network failure.

</details>


### [60] [AutoRec: Accelerating Loss Recovery for Live Streaming in a Multi-Supplier Market](https://arxiv.org/abs/2511.22046)
*Tong Li,Xu Yan,Bo Wu,Cheng Luo,Fuyu Wang,Jiuxiang Zhu,Haoyi Fang,Xinle Du,Ke Xu*

Main category: cs.NI

TL;DR: 论文提出AutoRec机制，利用直播流中的频繁启停模式切换来降低丢包恢复延迟，无需客户端修改，在QUIC上实现并验证了实用性和效益。


<details>
  <summary>Details</summary>
Motivation: 由于CDN厂商在多供应商市场中升级双边（服务端和客户端）容错方案的权限有限，现代大规模直播服务仍使用基于ARQ的丢包恢复机制。研究发现丢包恢复延迟（因普遍存在的重传丢失而放大）是影响直播客户端QoE（如视频卡顿）的关键因素。

Method: 首先对5000万直播流进行大规模测量研究，发现丢包动态性和频繁的启停模式切换。提出AutoRec增强恢复机制，将启停模式切换的劣势转化为降低恢复延迟的优势，允许用户自定义开销容忍度和恢复延迟容忍度，自适应调整策略以适应网络环境变化。

Result: 在QUIC上实现AutoRec，通过测试床和真实商业服务部署进行评估。实验结果表明AutoRec具有实用性和盈利性。

Conclusion: AutoRec能够在不修改客户端的情况下，有效利用直播流的启停模式切换特性来降低丢包恢复延迟，在控制开销的同时满足用户对恢复延迟的需求，为大规模直播服务提供了实用的增强恢复方案。

Abstract: Due to the limited permissions for upgrading dualside (i.e., server-side and client-side) loss tolerance schemes from the perspective of CDN vendors in a multi-supplier market, modern large-scale live streaming services are still using the automatic-repeat-request (ARQ) based paradigm for loss recovery, which only requires server-side modifications. In this paper, we first conduct a large-scale measurement study with up to 50 million live streams. We find that loss shows dynamics and live streaming contains frequent on-off mode switching in the wild. We further find that the recovery latency, enlarged by the ubiquitous retransmission loss, is a critical factor affecting live streaming's client-side QoE (e.g., video freezing). We then propose an enhanced recovery mechanism called AutoRec, which can transform the disadvantages of on-off mode switching into an advantage for reducing loss recovery latency without any modifications on the client side. AutoRec allows users to customize overhead tolerance and recovery latency tolerance and adaptively adjusts strategies as the network environment changes to ensure that recovery latency meets user demands whenever possible while keeping overhead under control. We implement AutoRec upon QUIC and evaluate it via testbed and real-world commercial services deployments. The experimental results demonstrate the practicability and profitability of AutoRec.

</details>


### [61] [Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning](https://arxiv.org/abs/2511.22217)
*Yuxuan Chen,Rongpeng Li,Xianfu Chen,Celimuge Wu,Chenghui Peng,Zhifeng Zhao,Honggang Zhang*

Main category: cs.NI

TL;DR: NetGPT框架通过云边协同实现智能路由：网络感知的路由策略将复杂请求导向云端，简单请求留在边缘；同时利用云端数据通过强化学习提升边缘代理能力，实现质量与成本的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在边缘计算中的质量-成本权衡问题：边缘代理延迟低但能力有限，云端代理能力强但延迟和成本高。需要在动态网络条件下实现智能路由决策。

Method: 提出云边协同框架：1) 网络感知路由策略，基于带宽和RTT计算分数，采用回退阈值决策；2) 边缘代理自改进，利用云端收集的数据进行模式保持的强化学习，结合反向KL信任域和正向KL对齐。

Result: 实验显示：在控制网络状态和定价方案下，实现了平滑的质量-成本边界；动态回退阈值优于固定策略；在保持任务成功率和模式正确输出的同时，持续减少卸载到云端的请求。

Conclusion: NetGPT框架通过云边协同有效平衡了LLM代理的质量与成本，网络感知路由和边缘自改进的联合更新策略在实际动态环境中表现优越，为边缘智能系统提供了可行的解决方案。

Abstract: Large language model (LLM) agents at the network edge offer low-latency execution for routine queries. In contrast, complex requests often require the superior capability of cloud models, incurring higher latency and cost. To navigate this quality-cost trade-off under dynamic network conditions, we propose a cloud-edge synergy for NetGPT that integrates network-aware routing with on-edge self-improvement. Specifically, our framework routes structured tool-calling requests to cloud or edge agents via a novel scoring policy. We prove that, under mild regularity assumptions, the optimal routing rule admits a unique fallback threshold with monotone dependence on bandwidth and round-trip time (RTT). Concurrently, based on the dataset collected from requests routed to the cloud and corresponding responses, we instantiate a schema-preserving reinforcement learning (RL) to improve the capability of the edge agent. We analyze a supervised finetuning (SFT)-anchored composite objective that combines a reverse-KL trust-region step with a forward-KL realignment toward the SFT prior, explaining stability and constraining policy drift. Both the network-aware routing policy and the edge agent are updated coherently. Experiments across controlled network states and pricing schedules demonstrate smooth quality-cost frontiers, consistent gains of dynamic fallback thresholds over fixed policies, and sustained reductions in offloading while maintaining task success and schema-correct outputs.

</details>


### [62] [Semantic-Aware Caching for Efficient Image Generation in Edge Computing](https://arxiv.org/abs/2511.22421)
*Hanshuai Cui,Zhiqing Tang,Zhi Yao,Weijia Ji,Wei Zhao*

Main category: cs.NI

TL;DR: CacheGenius：边缘计算中的混合图像生成系统，通过结合文生图和图生图流程，利用缓存参考图像加速生成，减少41%延迟和48%计算成本


<details>
  <summary>Details</summary>
Motivation: 扩散模型在资源受限的移动和边缘环境中面临挑战，因为需要从随机噪声进行多次去噪步骤。通过使用与目标相似的带噪参考图像初始化可以加速去噪过程。

Method: 提出CacheGenius混合图像生成系统：1）语义感知分类存储方案；2）确保参考图像与目标语义对齐的请求调度算法；3）通过相关性分析主动淘汰过时条目的缓存维护策略

Result: 在分布式边缘计算系统中评估，相比基线方法，CacheGenius将生成延迟降低41%，计算成本降低48%，同时保持有竞争力的评估指标

Conclusion: CacheGenius通过有效利用缓存参考图像加速扩散模型生成，为资源受限的边缘环境提供了一种实用的图像生成加速解决方案

Abstract: Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.

</details>


### [63] [Day in the Life of RIPE Atlas: Operational Insights and Applications in Network Measurements](https://arxiv.org/abs/2511.22474)
*Yevheniya Nosyk,Malte Tashiro,Qasim Lone,Robert Kisteleki,Andrzej Duda,Maciej Korczyński*

Main category: cs.NI

TL;DR: 该论文深入分析了RIPE Atlas测量平台一天内的运行情况，涵盖5.09万个独特测量和13亿个结果，揭示了平台运行特征、潜在偏差，并提出了使用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管RIPE Atlas每天产生超过1TB的测量数据，但对其底层运行机制了解有限。研究人员需要理解平台如何运作、测量分布特征以及可能引入的偏差，以确保研究的透明度和可靠性。

Method: 通过分析RIPE Atlas一天内的完整测量数据（50.9K个独特测量，超过13亿个结果），系统性地研究了不同探测器和测量类型对平台日常运行的贡献，并评估了它们可能引入的偏差。

Result: 研究发现：虽然大多数日常测量是用户定义的，但内置测量和锚点网格测量产生了89%的结果；平台运行存在特定模式；现有测量数据可用于研究审查、traceroute对称性和保留地址块使用等多种网络现象。

Conclusion: 论文为RIPE Atlas平台用户提供了一套建议，以促进研究的透明度、可重复性和伦理性，帮助研究人员更好地理解和利用这一重要的网络测量资源。

Abstract: Network measurement platforms are increasingly popular among researchers and operators alike due to their distributed nature, simplifying measuring the remote parts of the Internet. RIPE Atlas boasts over 12.9K vantage points in 178 countries worldwide and serves as a vital tool for analyzing anycast deployment, network latency, and topology, to name a few. Despite generating over a terabyte of measurement results per day, there is limited understanding of the underlying processes. This paper delves into one day in the life of RIPE Atlas, encompassing 50.9K unique measurements and over 1.3 billion results. While most daily measurements are user-defined, it is built-ins and anchor meshes that account for 89% of produced results. We extensively examine how different probes and measurements contribute to the daily operations of RIPE Atlas and consider any bias they may introduce. Furthermore, we demonstrate how existing measurements can be leveraged to investigate censorship, traceroute symmetry, and the usage of reserved address blocks, among others. Finally, we curate a set of recommendations for researchers using the RIPE Atlas platform to foster transparency, reproducibility, and ethics.

</details>


### [64] [RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications](https://arxiv.org/abs/2511.23278)
*Jhonatan Tavori,Anat Bremler-Barr,Hanoch Levy,Ofek Lavi*

Main category: cs.NI

TL;DR: RetryGuard：分布式框架，用于控制微服务间的重试模式，防止重试风暴，降低资源使用和成本


<details>
  <summary>Details</summary>
Motivation: 现代云应用采用独立多样的微服务架构，但不同服务间的默认重试模式可能导致重试风暴，引发资源使用激增和成本上升，造成自发的"钱包拒绝"（DoW）场景

Method: 提出RetryGuard分布式框架，通过基于服务的重试策略管理和并行决策，防止重试风暴；基于分析模型，捕捉重试、吞吐量（拒绝）、延迟和成本之间的关系

Result: 实验结果显示，RetryGuard相比AWS标准和高级重试策略显著减少资源使用和成本；在更复杂的Kubernetes部署和Istio服务网格中，展现出可扩展性和优越性能

Conclusion: RetryGuard能有效解决微服务间协调挑战，防止重试风暴，控制资源使用和运营成本，在复杂云环境中具有实际应用价值

Abstract: Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.

</details>


### [65] [Performance Evaluation of Multi-Armed Bandit Algorithms for Wi-Fi Channel Access](https://arxiv.org/abs/2511.23352)
*Miguel Casasnovas,Francesc Wilhelmi,Richard Combes,Maksymilian Wojnar,Katarzyna Kosek-Szott,Szymon Szott,Anders Jonsson,Luis Esteve,Boris Bellalta*

Main category: cs.NI

TL;DR: 该论文研究多臂老虎机策略在Wi-Fi去中心化在线信道接入优化中的应用，评估了不同设计方法（联合vs因子化动作空间、上下文信息、动作选择策略），提出了轻量级上下文方法E-RLB，并分析了各种策略在动态环境下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有无线网络协议的自适应性有限，需要动态自学习的实时优化方案。多臂老虎机策略作为一种数据驱动方法，可用于Wi-Fi去中心化在线信道接入优化，以应对动态信道接入环境（主信道、信道宽度、竞争窗口调整）的挑战。

Method: 研究多臂老虎机策略在Wi-Fi信道接入优化中的应用，探讨关键设计方面：联合vs因子化动作空间、上下文信息包含、动作选择策略类型（乐观驱动、单峰或随机化）。提出轻量级上下文方法E-RLB，通过仿真评估最先进算法和E-RLB的性能。

Result: 上下文和乐观驱动策略在重复条件下始终实现最高性能和最快适应速度。单峰结构需要仔细构建图以确保单峰假设成立。随机化探索（E-RLB采用）在多用户设置中可能引发破坏性参数重新分配。将动作空间分解到多个专门代理可加速收敛，但增加对随机探索的敏感性，并需要在共享奖励下协调以避免相关学习。尽管epsilon-greedy探索存在固有低效性，E-RLB仍表现出有效的适应和学习能力。

Conclusion: 多臂老虎机策略为Wi-Fi动态信道接入优化提供了有前景的数据驱动方法。上下文和乐观驱动策略性能最佳，而轻量级E-RLB方法尽管存在探索低效性，仍显示出作为现实动态部署中可行低复杂度解决方案的潜力。设计选择需要在性能、复杂度和鲁棒性之间权衡。

Abstract: The adoption of dynamic, self-learning solutions for real-time wireless network optimization has recently gained significant attention due to the limited adaptability of existing protocols. This paper investigates multi-armed bandit (MAB) strategies as a data-driven approach for decentralized, online channel access optimization in Wi-Fi, targeting dynamic channel access settings: primary channel, channel width, and contention window (CW) adjustment. Key design aspects are examined, including the adoption of joint versus factorial action spaces, the inclusion of contextual information, and the nature of the action-selection strategy (optimism-driven, unimodal, or randomized). State-of-the-art algorithms and a proposed lightweight contextual approach, E-RLB, are evaluated through simulations. Results show that contextual and optimism-driven strategies consistently achieve the highest performance and fastest adaptation under recurrent conditions. Unimodal structures require careful graph construction to ensure that the unimodality assumption holds. Randomized exploration, adopted in the proposed E-RLB, can induce disruptive parameter reallocations, especially in multi-player settings. Decomposing the action space across several specialized agents accelerates convergence but increases sensitivity to randomized exploration and demands coordination under shared rewards to avoid correlated learning. Finally, despite its inherent inefficiencies from epsilon-greedy exploration, E-RLB demonstrates effective adaptation and learning, highlighting its potential as a viable low-complexity solution for realistic dynamic deployments.

</details>


### [66] [Joint Resource Allocation to Transparently Integrate 5G TDD Uplink with Time-Aware TSN](https://arxiv.org/abs/2511.23373)
*Laura Becker,Yash Deshpande,Wolfgang Kellerer*

Main category: cs.NI

TL;DR: 提出异构无线资源调度器，整合静态与动态调度，支持5G-TSN系统中时间敏感流量的确定性传输，相比基准方案提升28%资源效率


<details>
  <summary>Details</summary>
Motivation: 5G与TSN融合实现工业移动通信需要端到端确定性传输，但传统上行调度器优化吞吐量而无法满足截止时间要求，需要解决5G桥接延迟确定性和时间敏感流量调度问题

Method: 提出异构无线资源调度算法：静态预分配资源给时间敏感周期性流（基于报告桥接延迟，对齐TSN机制），剩余资源动态分配给非确定性流（使用PF、Max C/I或QoS优先级调度）

Result: OMNeT++仿真显示：支持多样化TSN流，确保移动场景中时间敏感上行流量的截止时间感知调度，相比Configured Grant基准提升28%资源效率，非确定性速率敏感流也受益于资源利用率提升获得更高吞吐量

Conclusion: 所提调度器成功实现5G作为透明TSN桥接，通过整合静态与动态调度，在保证可靠性的同时满足时间敏感流的确定性传输需求，提升整体资源效率

Abstract: To enable mobility in industrial communication systems, the seamless integration of 5G with Time-Sensitive Networking (TSN) is a promising approach. Deterministic communication across heterogeneous 5G-TSN systems requires joint scheduling between both domains. A key prerequisite for time-aware end-to-end scheduling is determining the forwarding delay for each TSN Traffic Class at every bridge, referred to as Bridge Delay (BD). Hence, to integrate 5G as a transparent TSN bridge, the 5G BD must be determined and guaranteed. Unlike wired bridges, the 5G BD relies on wireless resource management characteristics, such as the Time Division Duplex pattern and radio resource allocation procedure. In particular, traditional Uplink (UL) schedulers are optimized for throughput but often fail to meet the deadline requirements. To address this challenge, we propose a heterogeneous radio resource scheduler that integrates static and dynamic scheduling. The algorithm pre-allocates resources for time-sensitive periodic streams based on the reported BDs, ensuring alignment with the TSN mechanisms Time-Aware Shaper and Per-Stream Filtering and Policing. Meanwhile, remaining resources are dynamically allocated to non-deterministic flows using established strategies such as Proportional Fair, Max C/I, or a Quality of Service-aware priority-based scheduler. The scheduler's performance is evaluated through OMNeT++ simulations. The results demonstrate support for diverse TSN flows while ensuring deadline-aware scheduling of time-sensitive UL traffic in mobility scenarios. Periodic time-sensitive flows are end-to-end scheduled across domains, improving the resource efficiency by 28% compared to the Configured Grant baseline. While reliability is preserved, non-deterministic rate-sensitive flows benefit from the improved resource utilization, resulting in higher throughput

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [Artificial intelligence for methane detection: from continuous monitoring to verified mitigation](https://arxiv.org/abs/2511.21777)
*Anna Allen,Gonzalo Mateo-Garcia,Itziar Irakulis-Loitxate,Manuel Montesino-San Martin,Marc Watine,James Requeima,Javier Gorroño,Cynthia Randles,Tharwat Mokalled,Luis Guanter,Richard E. Turner,Claudio Cifarelli,Manfredi Caltagirone*

Main category: cs.LG

TL;DR: MARS-S2L是一个机器学习模型，利用公开的多光谱卫星图像检测甲烷排放，每两天提供高分辨率检测，能够识别78%的排放羽流，已向20个国家发出1015次通知，实现了可验证的甲烷减排。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，少数大型点源排放占不成比例份额，但大规模检测和归因仍具挑战性。需要一种可扩展的方法来检测和通知资产所有者。

Method: 开发MARS-S2L机器学习模型，在超过80,000张人工标注的多光谱卫星图像数据集上训练，每两天提供高分辨率检测，实现设施级归因。

Result: 模型在697个未见站点上识别78%的排放羽流，假阳性率8%。已向20个国家发出1,015次通知，实现了6个持续排放源的验证性永久减排，包括利比亚一个先前未知的站点。

Conclusion: MARS-S2L展示了从卫星检测到可量化甲烷减排的可扩展路径，为大规模温室气体减排提供了有效工具。

Abstract: Methane is a potent greenhouse gas, responsible for roughly 30\% of warming since pre-industrial times. A small number of large point sources account for a disproportionate share of emissions, creating an opportunity for substantial reductions by targeting relatively few sites. Detection and attribution of large emissions at scale for notification to asset owners remains challenging. Here, we introduce MARS-S2L, a machine learning model that detects methane emissions in publicly available multispectral satellite imagery. Trained on a manually curated dataset of over 80,000 images, the model provides high-resolution detections every two days, enabling facility-level attribution and identifying 78\% of plumes with an 8\% false positive rate at 697 previously unseen sites. Deployed operationally, MARS-S2L has issued 1,015 notifications to stakeholders in 20 countries, enabling verified, permanent mitigation of six persistent emitters, including a previously unknown site in Libya. These results demonstrate a scalable pathway from satellite detection to quantifiable methane mitigation.

</details>


### [68] [Physics-Informed Spiking Neural Networks via Conservative Flux Quantization](https://arxiv.org/abs/2511.21784)
*Chi Zhang,Lin Wang*

Main category: cs.LG

TL;DR: 提出PISNN框架，结合物理约束与脉冲神经网络，通过C-LIF神经元和CFQ策略实现严格物理守恒和长期泛化，适用于边缘设备实时物理预测。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要实时、物理一致的预测，但传统PINNs能耗高且难以严格保证物理守恒定律。脉冲神经网络虽适合边缘计算，但简单转换会降低物理保真度。

Method: 提出PISNN框架：1) 设计C-LIF神经元，其动力学结构保证局部质量守恒；2) 提出CFQ策略，将神经脉冲重新定义为物理通量的离散包，学习时间不变的物理演化算子。

Result: 在1D热方程和2D拉普拉斯方程等基准测试中表现出色，能准确模拟系统动力学，同时保持完美的质量守恒，优于传统PINNs。

Conclusion: PISNN将科学计算的严谨性与神经形态工程的效率相结合，为智能系统实现复杂、长期、高能效的物理预测提供了稳健框架。

Abstract: Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.

</details>


### [69] [Dynamical Implicit Neural Representations](https://arxiv.org/abs/2511.21787)
*Yesom Park,Kelvin Kan,Thomas Flynn,Yi Huang,Shinjae Yoo,Stanley Osher,Xihaier Luo*

Main category: cs.LG

TL;DR: DINR将隐式神经表示建模为连续时间动力系统而非离散层堆叠，通过连续特征演化缓解频谱偏差，提升高频细节表达能力。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示在建模复杂视觉和几何信号方面具有强大能力，但频谱偏差限制了其捕捉高频细节的能力。现有解决方案效果有限，需要新的建模框架来克服这一根本挑战。

Method: 提出动态隐式神经表示（DINR）框架，将特征演化视为连续时间动力系统而非离散层堆叠。通过连续特征演化实现更丰富、更自适应的频率表示，并基于Rademacher复杂度和神经正切核进行理论分析，通过正则化底层动力学的复杂度来平衡表达能力和泛化能力。

Result: 在图像表示、场重建和数据压缩等任务上的广泛实验表明，DINR相比传统静态INR具有更稳定的收敛性、更高的信号保真度和更强的泛化能力。

Conclusion: DINR通过将INR建模为连续时间动力系统，有效缓解了频谱偏差问题，在保持理论保证的同时提升了实际性能，为隐式神经表示提供了新的建模框架。

Abstract: Implicit Neural Representations (INRs) provide a powerful continuous framework for modeling complex visual and geometric signals, but spectral bias remains a fundamental challenge, limiting their ability to capture high-frequency details. Orthogonal to existing remedy strategies, we introduce Dynamical Implicit Neural Representations (DINR), a new INR modeling framework that treats feature evolution as a continuous-time dynamical system rather than a discrete stack of layers. This dynamical formulation mitigates spectral bias by enabling richer, more adaptive frequency representations through continuous feature evolution. Theoretical analysis based on Rademacher complexity and the Neural Tangent Kernel demonstrates that DINR enhances expressivity and improves training dynamics. Moreover, regularizing the complexity of the underlying dynamics provides a principled way to balance expressivity and generalization. Extensive experiments on image representation, field reconstruction, and data compression confirm that DINR delivers more stable convergence, higher signal fidelity, and stronger generalization than conventional static INRs.

</details>


### [70] [Multiclass threshold-based classification and model evaluation](https://arxiv.org/abs/2511.21794)
*Edoardo Legnaro,Sabrina Guastavino,Francesco Marchetti*

Main category: cs.LG

TL;DR: 提出基于多维阈值的多类分类框架，替代标准argmax规则，通过阈值调优提升分类性能，并引入基于ROC云的多类ROC分析


<details>
  <summary>Details</summary>
Motivation: 标准多类分类使用softmax输出和argmax规则，缺乏像二分类那样的阈值调优能力。本文旨在将二分类中阈值优化的思想推广到多类场景，提升任何已训练网络的预测能力

Method: 将softmax输出的概率解释转换为多维单纯形上的几何解释，引入多维阈值进行分类决策。提出后验阈值调优方法，并基于ROC云进行多类ROC分析，使用DFP分数总结性能

Result: 实验表明多维阈值调优能在不同网络和数据集上带来性能提升。提出的ROC云分析和DFP分数为多类分类提供了比传统OvR曲线更一致的评估方法

Conclusion: 提出的阈值框架为多类分类提供了类似二分类的阈值调优能力，能够进一步提升任何已训练网络的预测性能，同时提供了更一致的多类ROC分析方法

Abstract: In this paper, we introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule. This is done by replacing the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex, where the classification depends on a multidimensional threshold. This change of perspective enables for any trained classification network an \textit{a posteriori} optimization of the classification score by means of threshold tuning, as usually carried out in the binary setting, thus allowing for a further refinement of the prediction capability of any network. Our experiments show indeed that multidimensional threshold tuning yields performance improvements across various networks and datasets. Moreover, we derive a multiclass ROC analysis based on \emph{ROC clouds} -- the attainable (FPR,TPR) operating points induced by a single multiclass threshold -- and summarize them via a \emph{Distance From Point} (DFP) score to $(0,1)$. This yields a coherent alternative to standard One-vs-Rest (OvR) curves and aligns with the observed tuning gains.

</details>


### [71] [The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning](https://arxiv.org/abs/2511.21799)
*Ethan Hsu,Harry Chen,Chudi Zhong,Lesia Semenova*

Main category: cs.LG

TL;DR: Rashomon集合（多个近似最优模型）在可信机器学习中展现出双重作用：一方面通过多样性增强鲁棒性，另一方面增加隐私泄露风险


<details>
  <summary>Details</summary>
Motivation: 现实机器学习流程通常产生多个近似最优模型（Rashomon集合），而非单一模型。需要研究这种多重性如何影响机器学习的可信度，特别是在鲁棒性和隐私保护之间的权衡关系。

Method: 通过理论分析和稀疏决策树、线性模型的实证研究，分析Rashomon集合对鲁棒性和隐私的影响。在个体模型层面分析稀疏可解释模型特性，在集合层面研究多样性带来的影响。

Result: 稀疏可解释模型保护隐私但对对抗攻击脆弱；Rashomon集合的多样性支持反应式鲁棒性（攻击破坏一个模型时其他模型仍准确）和分布偏移稳定性；但多样性增加信息泄露风险，披露更多近似最优模型使攻击者获得更丰富的训练数据视图。

Conclusion: Rashomon集合在可信机器学习中具有双重角色：既是增强鲁棒性的资源，也是增加隐私泄露风险的因素。需要理解这种鲁棒性-隐私权衡，以更好地管理机器学习系统的可信度。

Abstract: Real-world machine learning (ML) pipelines rarely produce a single model; instead, they produce a Rashomon set of many near-optimal ones. We show that this multiplicity reshapes key aspects of trustworthiness. At the individual-model level, sparse interpretable models tend to preserve privacy but are fragile to adversarial attacks. In contrast, the diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate. Rashomon sets are also stable under small distribution shifts. However, this same diversity increases information leakage, as disclosing more near-optimal models provides an attacker with progressively richer views of the training data. Through theoretical analysis and empirical studies of sparse decision trees and linear models, we characterize this robustness-privacy trade-off and highlight the dual role of Rashomon sets as both a resource and a risk for trustworthy ML.

</details>


### [72] [Unsupervised Anomaly Detection for Smart IoT Devices: Performance and Resource Comparison](https://arxiv.org/abs/2511.21842)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 本文比较了两种无监督异常检测方法（孤立森林和一类支持向量机）在IoT环境中的性能，发现孤立森林在检测精度和计算效率方面均优于OC-SVM，更适合资源受限的IoT边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备在各行业的快速部署，虽然提升了运营效率，但也增加了网络安全漏洞。传统基于签名的异常检测系统难以识别新兴和零日威胁，因此需要研究更有效的无监督异常检测方法。

Method: 使用TON_IoT恒温器数据集，对两种无监督异常检测技术（孤立森林和一类支持向量机）进行综合评估。评估指标包括标准性能指标（准确率、精确率、召回率、F1分数）和关键资源利用指标（推理时间、模型大小、峰值RAM使用量）。

Result: 实验结果显示，孤立森林在各方面均优于一类支持向量机：获得更高的检测准确率、更优的精确率和召回率，以及显著更好的F1分数。此外，孤立森林的计算足迹明显更优，更适合在资源受限的IoT边缘设备上部署。

Conclusion: 孤立森林在高维和不平衡的IoT环境中表现出更强的鲁棒性，并显示出在实时异常检测中的实际可行性，是IoT安全监测的更优选择。

Abstract: The rapid expansion of Internet of Things (IoT) deployments across diverse sectors has significantly enhanced operational efficiency, yet concurrently elevated cybersecurity vulnerabilities due to increased exposure to cyber threats. Given the limitations of traditional signature-based Anomaly Detection Systems (ADS) in identifying emerging and zero-day threats, this study investigates the effectiveness of two unsupervised anomaly detection techniques, Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM), using the TON_IoT thermostat dataset. A comprehensive evaluation was performed based on standard metrics (accuracy, precision, recall, and F1-score) alongside critical resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results revealed that IF consistently outperformed OC-SVM, achieving higher detection accuracy, superior precision, and recall, along with a significantly better F1-score. Furthermore, Isolation Forest demonstrated a markedly superior computational footprint, making it more suitable for deployment on resource-constrained IoT edge devices. These findings underscore Isolation Forest's robustness in high-dimensional and imbalanced IoT environments and highlight its practical viability for real-time anomaly detection.

</details>


### [73] [Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics](https://arxiv.org/abs/2511.21848)
*Eric Leonardis,Akira Nagamori,Ayesha Thanawalla,Yuanjia Yang,Joshua Park,Hutton Saunders,Eiman Azim,Talmo Pereira*

Main category: cs.LG

TL;DR: 开发了一个用于行为驱动仿真的通用平台，通过模仿学习框架在物理模拟环境中实现小鼠前肢抓取任务，发现能量和速度的自然约束能更好地预测真实EMG信号。


<details>
  <summary>Details</summary>
Motivation: 大脑进化出有效控制身体的能力，为了理解这种关系，需要建模体现控制中的感觉运动转换。需要开发一个高保真行为动力学、生物力学和神经回路架构的通用仿真平台。

Method: 开发了一个从神经科学实验室获取运动学数据并创建生物力学模型重现自然运动的流程。实现了模仿学习框架，在模拟物理环境中使用肌肉骨骼模型执行灵巧的前肢抓取任务。利用JAX和Mujoco-MJX进行GPU加速，实现每秒超过100万训练步骤。

Result: 结果表明，添加能量和速度的自然约束能产生更好地预测真实EMG信号的模拟肌肉骨骼活动。能量和控制约束对于建模肌肉骨骼运动控制至关重要。

Conclusion: 这项工作提供了证据表明能量和控制约束对于建模肌肉骨骼运动控制是关键的。开发了一个有效的仿真平台，能够通过模仿学习重现自然运动，并更好地预测真实神经肌肉活动。

Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.

</details>


### [74] [Lightweight ML-Based Air Quality Prediction for IoT and Embedded Applications](https://arxiv.org/abs/2511.21857)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 比较完整版和轻量版XGBoost模型在预测CO和NO2浓度方面的性能，完整版精度更高，轻量版计算效率更优，适合资源受限的物联网应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同版本的XGBoost回归模型在空气污染物浓度预测中的表现，特别关注在资源受限环境（如物联网设备）中部署简化模型的可行性。

Method: 使用AirQualityUCI数据集（城市环境一年数据），对比完整版和轻量版XGBoost模型。评估指标包括MAE、RMSE、MBE、R2等预测精度指标，以及推理时间、模型大小、峰值RAM使用等资源指标。

Result: 完整版XGBoost模型对CO和NO2的预测精度更高，而轻量版模型虽然精度稍低，但推理时间显著减少，模型存储需求大幅降低，计算效率优势明显。

Conclusion: 轻量版XGBoost模型在保持可接受预测质量的同时，提供了显著的计算效率优势，适合在资源受限的物联网和嵌入式系统中进行实时空气质量监测。

Abstract: This study investigates the effectiveness and efficiency of two variants of the XGBoost regression model, the full-capacity and lightweight (tiny) versions, for predicting the concentrations of carbon monoxide (CO) and nitrogen dioxide (NO2). Using the AirQualityUCI dataset collected over one year in an urban environment, we conducted a comprehensive evaluation based on widely accepted metrics, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). In addition, we assessed resource-oriented metrics such as inference time, model size, and peak RAM usage. The full XGBoost model achieved superior predictive accuracy for both pollutants, while the tiny model, though slightly less precise, offered substantial computational benefits with significantly reduced inference time and model storage requirements. These results demonstrate the feasibility of deploying simplified models in resource-constrained environments without compromising predictive quality. This makes the tiny XGBoost model suitable for real-time air-quality monitoring in IoT and embedded applications.

</details>


### [75] [Towards a Foundation Model for Partial Differential Equations Across Physics Domains](https://arxiv.org/abs/2511.21861)
*Eduardo Soares,Emilio Vital Brazil,Victor Shirasuna,Breno W. S. R. de Carvalho,Cristiano Malossi*

Main category: cs.LG

TL;DR: PDE-FM是一个用于物理信息机器学习的模块化基础模型，通过统一空间、频谱和时间推理来处理异质偏微分方程系统，在多个物理领域达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 现有任务特定的神经算子方法缺乏跨物理系统的泛化能力，需要为不同物理机制设计专门架构。研究者希望开发一个统一的基础模型，能够通过大规模预训练获得可迁移的物理动力学表示。

Method: 结合空间-频谱标记化、物理感知条件化和基于Mamba的状态空间主干网络，配合算子理论解码器。模型在多样PDE数据集上进行一次性预训练，无需架构或数据特定修改即可迁移到新物理机制。

Result: 在The Well基准测试的12个2D和3D数据集（涵盖流体动力学、辐射、弹性和天体物理现象）上，PDE-FM在6个领域达到最先进精度，相对于先前算子学习基线平均VRMSE降低46%。模型在湍流和辐射系统中表现优异，同时在线性和稳态机制中保持强劲性能。

Conclusion: 跨多样物理过程的大规模预训练能够产生可迁移的动力学表示，标志着向统一的多物理模拟和科学发现基础级代理模型迈出了一步。

Abstract: We present PDE-FM, a modular foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation (PDE) systems. PDE-FM combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. In contrast to task-specific neural operators, PDE-FM is pretrained once on diverse PDE datasets and can be transferred to new physical regimes without architectural or data-specific modifications. Evaluated on twelve 2D and 3D datasets from The Well benchmark - spanning hydrodynamic, radiative, elastic, and astrophysical phenomena - PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines. The model demonstrates robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes. These results suggest that large-scale pretraining across diverse physical processes can yield transferable representations of dynamics, marking a step toward unified, foundation-level surrogates for multi-physics simulation and scientific discovery.

</details>


### [76] [Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium](https://arxiv.org/abs/2511.21882)
*Akbar Anbar Jafari,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 本文提出均衡变换器(EqT)，通过闭环预测原则解决自回归变换器的开环瓶颈问题，使模型在生成每个token前迭代优化潜在表示直至达到自洽平衡。


<details>
  <summary>Details</summary>
Motivation: 当前自回归变换器采用开环操作：每个隐藏状态通过单次前向传播计算且永不修正，导致错误在序列中传播。这造成了长程推理、事实一致性和多步规划方面的失败。作者认为这是自回归架构的根本限制。

Method: 引入闭环预测原则，要求模型在生成每个token前迭代优化潜在表示直至达到自洽平衡。具体实现为均衡变换器(EqT)，在标准变换器层基础上增加均衡优化模块，通过梯度下降最小化学得的能量函数。能量函数强制执行双向预测一致性、情景记忆连贯性和输出置信度。

Result: 理论证明EqT在潜在能量基模型中执行近似MAP推理，建立了线性收敛保证，并显示优化在单次推理次优的困难实例上改善预测。在二进制奇偶校验任务上的初步实验显示平均提升3.28%，在标准变换器接近随机性能的情况下提升达8.07%，验证了深思熟虑的益处随任务难度而增加。

Conclusion: 正如注意力机制解决了循环网络的序列瓶颈，闭环均衡可能解决开环自回归的承诺瓶颈，代表了向语言模型发展的基础性步骤。该框架统一了深度均衡模型、扩散语言模型和测试时训练作为特例。

Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.

</details>


### [77] [Physically Interpretable Representation Learning with Gaussian Mixture Variational AutoEncoder (GM-VAE)](https://arxiv.org/abs/2511.21883)
*Tiffany Fan,Murray Cutforth,Marta D'Elia,Alexandre Cortiella,Alireza Doostan,Eric Darve*

Main category: cs.LG

TL;DR: 提出GM-VAE框架，结合EM训练方案和谱可解释性度量，从高维科学数据中提取物理可解释的紧凑表示，在复杂物理系统数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 从高维科学数据中提取紧凑且物理可解释的表示是一个持续挑战，因为物理系统具有复杂的非线性结构。传统VAE在联合优化重建和聚类时存在训练不稳定性问题。

Method: 提出高斯混合变分自编码器(GM-VAE)框架，采用块坐标下降策略，交替执行期望步和最大化步，稳定训练并使潜在簇与不同物理机制对齐。引入基于图拉普拉斯平滑度的定量度量来评估学习表示。

Result: 在表面反应ODE、Navier-Stokes尾流和实验激光诱导燃烧纹影图像等数据集上验证，GM-VAE产生平滑、物理一致的流形和准确的机制聚类，为解释湍流和反应流系统提供了鲁棒的数据驱动工具。

Conclusion: GM-VAE框架通过EM训练方案和谱可解释性度量，能够从复杂科学数据中提取物理可解释的表示，为湍流和反应流系统的数据驱动解释提供了有效工具。

Abstract: Extracting compact, physically interpretable representations from high-dimensional scientific data is a persistent challenge due to the complex, nonlinear structures inherent in physical systems. We propose a Gaussian Mixture Variational Autoencoder (GM-VAE) framework designed to address this by integrating an Expectation-Maximization (EM)-inspired training scheme with a novel spectral interpretability metric. Unlike conventional VAEs that jointly optimize reconstruction and clustering (often leading to training instability), our method utilizes a block-coordinate descent strategy, alternating between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes. To objectively evaluate the learned representations, we introduce a quantitative metric based on graph-Laplacian smoothness, which measures the coherence of physical quantities across the latent manifold. We demonstrate the efficacy of this framework on datasets of increasing complexity: surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. The results show that our GM-VAE yields smooth, physically consistent manifolds and accurate regime clustering, offering a robust data-driven tool for interpreting turbulent and reactive flow systems.

</details>


### [78] [Exploring Fusion Strategies for Multimodal Vision-Language Systems](https://arxiv.org/abs/2511.21889)
*Regan Willis,Jason Bakos*

Main category: cs.LG

TL;DR: 该论文研究了多模态机器学习中不同融合策略对准确性和延迟的权衡，通过BERT与视觉网络（MobileNetV2和ViT）的混合框架，在CMU MOSI数据集上评估了早、中、晚期融合的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态机器学习需要平衡准确性和延迟需求，不同融合策略（早期、中期、晚期）会影响模型性能。需要系统研究这种权衡关系，为实际应用提供指导。

Method: 采用BERT与视觉网络（MobileNetV2和ViT）的混合框架，为每种视觉网络设计三种模型：晚期融合、中期融合和早期融合。在CMU MOSI数据集上评估准确性，在NVIDIA Jetson Orin AGX上基准测试延迟。

Result: 晚期融合获得最高准确性，早期融合提供最低推理延迟。准确性随融合阶段提前而降低，延迟随融合阶段提前而减少。

Conclusion: 模型架构中较早的数据融合会导致更快的推理时间，但以准确性为代价。应用需要根据准确性和延迟需求选择适当的融合策略。

Abstract: Modern machine learning models often combine multiple input streams of data to more accurately capture the information that informs their decisions. In multimodal machine learning, choosing the strategy for fusing data together requires careful consideration of the application's accuracy and latency requirements, as fusing the data at earlier or later stages in the model architecture can lead to performance changes in accuracy and latency. To demonstrate this tradeoff, we investigate different fusion strategies using a hybrid BERT and vision network framework that integrates image and text data. We explore two different vision networks: MobileNetV2 and ViT. We propose three models for each vision network, which fuse data at late, intermediate, and early stages in the architecture. We evaluate the proposed models on the CMU MOSI dataset and benchmark their latency on an NVIDIA Jetson Orin AGX. Our experimental results demonstrate that while late fusion yields the highest accuracy, early fusion offers the lowest inference latency. We describe the three proposed model architectures and discuss the accuracy and latency tradeoffs, concluding that data fusion earlier in the model architecture results in faster inference times at the cost of accuracy.

</details>


### [79] [Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2511.21893)
*Fatemeh Akbarian,Anahita Baninajjar,Yingyi Zhang,Ananth Balashankar,Amir Aminifar*

Main category: cs.LG

TL;DR: 提出一种基于生成模型的任务无关防御机制，通过重构对抗性扰动输入来维持多模态对齐，显著降低对抗幻觉攻击成功率


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型虽然能在共享嵌入空间中对齐图像、文本等模态，但容易受到对抗幻觉攻击的影响，这些攻击通过微小扰动破坏跨模态对齐并误导下游任务

Method: 使用生成模型（如变分自编码器）从攻击者扰动的输入中重构原始输入，以维持自然对齐；进一步采用生成采样策略结合基于共识的聚合方案来增强防御机制

Result: 在先进的多模态编码器上，该方法将幻觉攻击成功率大幅降低至接近零，在未扰动和扰动输入设置下分别将跨模态对齐提高了4%（42到46）和11%（32到43）

Conclusion: 该方法提供了一种有效且模型无关的防御机制，能够有效对抗多模态基础模型中的对抗幻觉攻击

Abstract: Multi-modal foundation models align images, text, and other modalities in a shared embedding space but remain vulnerable to adversarial illusions (Zhang et al., 2025), where imperceptible perturbations disrupt cross-modal alignment and mislead downstream tasks. To counteract the effects of adversarial illusions, we propose a task-agnostic mitigation mechanism that reconstructs the input from the attacker's perturbed input through generative models, e.g., Variational Autoencoders (VAEs), to maintain natural alignment. To further enhance our proposed defense mechanism, we adopt a generative sampling strategy combined with a consensus-based aggregation scheme over the outcomes of the generated samples. Our experiments on the state-of-the-art multi-modal encoders show that our approach substantially reduces the illusion attack success rates to near-zero and improves cross-modal alignment by 4% (42 to 46) and 11% (32 to 43) in unperturbed and perturbed input settings respectively, providing an effective and model-agnostic defense against adversarial illusions.

</details>


### [80] [Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning](https://arxiv.org/abs/2511.21900)
*Patricia Suriana,Joshua A. Rackers,Ewa M. Nowara,Pedro O. Pinheiro,John M. Nicoloudis,Vishnu Sresht*

Main category: cs.LG

TL;DR: 比较三种体素输入（原子类型、原始电子密度、密度梯度）在3D CNN中的表现，发现电子密度在低数据量下对蛋白质-配体结合亲和力预测更优，在量子性质预测中始终优于原子类型


<details>
  <summary>Details</summary>
Motivation: 传统基于原子的3D分子性质预测模型可能忽略细微物理信息，而电子密度图作为X射线晶体学和冷冻电镜的直接输出，提供了连续、物理基础更强的替代方案

Method: 使用三种体素输入类型（原子类型、原始电子密度、密度梯度大小）训练3D卷积神经网络，在两个分子任务上进行测试：蛋白质-配体结合亲和力预测（PDBbind）和量子性质预测（QM9）

Result: 在PDBbind上，全数据时所有表示表现相似，但在低数据量下，基于密度的输入优于原子类型；在QM9上，即使输入密度来自较低级别方法，基于密度的输入在规模上仍优于基于原子的输入

Conclusion: 电子密度输入具有任务和机制依赖性优势，在亲和力预测中提高数据效率，在量子性质建模中提高准确性，突显了密度编码的丰富结构和电子信息

Abstract: Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.

</details>


### [81] [Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals](https://arxiv.org/abs/2511.21908)
*Hamid Shamszare,Avishek Choudhury*

Main category: cs.LG

TL;DR: 多模态机器学习框架结合面部表情和皮肤电反应数据预测用户在AI辅助医疗决策中的早期信任度


<details>
  <summary>Details</summary>
Motivation: 准确预测人类对AI系统的信任对于AI决策支持工具的安全集成至关重要，特别是在医疗健康领域，误校准的信任会影响诊断和治疗结果

Method: 提出多模态机器学习框架，结合图像（面部视频）和生理信号（皮肤电反应）数据。使用OpenCV提取视频帧，预训练transformer模型提取情感特征；GSR信号分解为紧张性和阶段性成分。定义两个时间窗口：早期检测窗口（决策前6-3秒）和近端检测窗口（决策前3-0秒）。分别使用单模态和多模态特征进行预测，通过堆叠集成方法融合最佳单模态模型

Result: 多模态融合显著提升预测性能：早期检测窗口准确率0.83，F1分数0.88，ROC-AUC 0.87；近端检测窗口准确率0.75，F1分数0.82，ROC-AUC 0.66

Conclusion: 生物信号可作为实时、客观的用户信任标记，使AI系统能够动态调整响应以维持校准的信任，这在心理健康应用中尤为重要

Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.

</details>


### [82] [Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck](https://arxiv.org/abs/2511.21923)
*Xinyu Liu,Xu Zhang,Can Chen,Ren Wang*

Main category: cs.LG

TL;DR: 论文通过信息瓶颈原理分析后门数据对神经网络训练动态的影响，发现后门攻击会产生独特的互信息特征，并提出基于动态的隐蔽性度量方法。


<details>
  <summary>Details</summary>
Motivation: 理解后门数据如何影响神经网络训练动态是一个复杂且未被充分探索的挑战，需要深入分析后门数据对学习过程的影响，特别是目标类与其他干净类之间的差异行为。

Method: 利用信息瓶颈原理结合内部表示的聚类分析，研究后门攻击产生的互信息特征在不同训练阶段的变化，并基于这些洞察提出基于动态的隐蔽性度量方法。

Result: 发现后门攻击会产生独特的互信息特征，这些特征随训练阶段演变且因攻击机制而异；视觉明显的攻击（如BadNets）从信息论角度看可能具有更高的隐蔽性；提出的动态隐蔽性度量方法在多数据集和多种攻击类型上得到验证。

Conclusion: 该研究为理解和评估后门威胁提供了新的维度，揭示了后门攻击在信息层面的隐蔽性特征，并提出了有效的动态评估方法。

Abstract: Understanding how backdoor data influences neural network training dynamics remains a complex and underexplored challenge. In this paper, we present a rigorous analysis of the impact of backdoor data on the learning process, with a particular focus on the distinct behaviors between the target class and other clean classes. Leveraging the Information Bottleneck (IB) principle connected with clustering of internal representation, We find that backdoor attacks create unique mutual information (MI) signatures, which evolve across training phases and differ based on the attack mechanism. Our analysis uncovers a surprising trade-off: visually conspicuous attacks like BadNets can achieve high stealthiness from an information-theoretic perspective, integrating more seamlessly into the model than many visually imperceptible attacks. Building on these insights, we propose a novel, dynamics-based stealthiness metric that quantifies an attack's integration at the model level. We validate our findings and the proposed metric across multiple datasets and diverse attack types, offering a new dimension for understanding and evaluating backdoor threats. Our code is available in: https://github.com/XinyuLiu71/Information_Bottleneck_Backdoor.git.

</details>


### [83] [A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization](https://arxiv.org/abs/2511.22080)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Chang Liu,Qipeng Xie,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWMSAM：一种解决联邦学习中动量与SAM结合时出现的局部-全局曲率错位和动量回波振荡问题的新方法


<details>
  <summary>Details</summary>
Motivation: 联邦学习中需要模型在有限通信预算下快速收敛，同时在非独立同分布客户端分布上保持良好泛化能力。虽然动量和SAM分别能解决这两个问题，但简单结合会在非IID FL中产生两个结构性问题：局部-全局曲率错位和动量回波振荡。

Method: 提出FedWMSAM方法：1）构建动量引导的全局扰动，使用服务器聚合的动量来对齐客户端的SAM方向与全局下降几何，实现单反向传播SAM近似；2）通过余弦相似度自适应规则耦合动量和SAM，形成早期动量、晚期SAM的两阶段训练计划。

Result: 提供了非IID收敛界限，明确建模了扰动引起的方差及其对参数(S, K, R, N)的依赖。在多个数据集和模型架构上的实验验证了方法的有效性、适应性和鲁棒性。

Conclusion: FedWMSAM成功解决了联邦学习中动量与SAM结合时的两个关键失败模式，在优化挑战方面表现出优越性，为联邦学习优化提供了有效的解决方案。

Abstract: In federated learning (FL), models must \emph{converge quickly} under tight communication budgets while \emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \emph{momentum} to accelerate progress, and \emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.

</details>


### [84] [Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs](https://arxiv.org/abs/2511.21928)
*Yifan Zhou,Sachin Grover,Mohamed El Mistiri,Kamalesh Kalirathnam,Pratyush Kerhalkar,Swaroop Mishra,Neelesh Kumar,Sanket Gaurav,Oya Aran,Heni Ben Amor*

Main category: cs.LG

TL;DR: ProPS是一种新颖的强化学习方法，将大型语言模型置于策略优化循环的核心，直接基于奖励反馈和自然语言输入提出策略更新，统一了数值和语义推理。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励信号，无法充分利用现实任务中丰富的语义知识。相比之下，人类通过结合数值反馈、语言、先验知识和常识来高效学习，因此需要一种能统一数值和语义推理的方法。

Method: 提出Prompted Policy Search (ProPS)方法，将大型语言模型置于策略优化循环的核心，让LLM直接基于奖励反馈和自然语言输入（如目标、领域知识、策略提示）提出策略更新。LLM在上下文中执行数值优化，结合语义信号实现更明智的探索。

Result: 在15个Gymnasium任务（经典控制、Atari游戏、MuJoCo环境）上评估，与7种广泛采用的RL算法（PPO、SAC、TRPO等）比较。ProPS在15个任务中的8个上优于所有基线方法，在提供领域知识时表现出显著优势。

Conclusion: 研究结果表明统一语义和数值推理对于透明、可泛化且符合人类认知的强化学习具有重要潜力，为更高效、更符合人类学习方式的RL方法开辟了新方向。

Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.

</details>


### [85] [Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment](https://arxiv.org/abs/2511.21931)
*Henry Salgado,Meagan Kendall,Martine Ceberio*

Main category: cs.LG

TL;DR: 提出一个简单高效框架，通过比较数据本身特征效应与模型解释，评估机器学习模型是否与数据结构对齐


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法主要关注解释模型行为，但缺乏评估模型是否真正反映数据内在结构的方法。需要建立直接从数据本身推导的基线来评估模型-数据对齐

Method: 受Rubin潜在结果框架启发，量化每个特征在二分类任务中分离两个结果组的强度，超越传统描述统计，估计每个特征对结果的影响效应。将数据驱动的特征排序与基于模型的解释进行比较

Result: 开发了一个可解释且模型无关的方法来评估模型-数据对齐，为实践者提供工具判断模型是否"说出了数据所说的内容"

Conclusion: 该方法建立数据本身基准，通过比较数据特征效应与模型解释，提供评估模型与数据结构对齐的实用框架，弥补现有可解释性方法的不足

Abstract: In this work, we propose a simple and computationally efficient framework to evaluate whether machine learning models align with the structure of the data they learn from; that is, whether \textit{the model says what the data says}. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings against model-based explanations, we provide practitioners with an interpretable and model-agnostic method to assess model--data alignment.

</details>


### [86] [Learning-Augmented Online Bipartite Matching in the Random Arrival Order Model](https://arxiv.org/abs/2511.23388)
*Kunanon Burathep,Thomas Erlebach,William K. Moses*

Main category: cs.LG

TL;DR: 本文研究了学习增强设置下的在线二分图匹配问题，通过不可信预测改进随机到达顺序模型中的算法性能，移除了先前工作对最优匹配大小的限制。


<details>
  <summary>Details</summary>
Motivation: 先前工作（Choo等人）在随机到达顺序模型中提出了学习增强算法，但假设最优匹配大小为n（即每个在线顶点都能匹配）。本文旨在移除这一限制，使算法适用于更一般的情况，只需预测匹配大小至少为αn（0<α≤1）。

Method: 扩展Choo等人的方法：使用到达序列的前缀作为样本来评估预测质量，然后根据评估结果决定是遵循预测还是使用忽略预测的基线算法。主要创新是移除了对最优匹配大小的假设，只要求预测匹配大小至少为αn。

Result: 提出的学习增强算法实现了(1-o(1))-一致性和(β-o(1))-鲁棒性，其中β是基线算法的竞争比。同时证明了随着预测误差增加，竞争比在一致性和鲁棒性之间平滑退化。

Conclusion: 本文成功将学习增强方法推广到更一般的在线二分图匹配场景，移除了对最优匹配大小的限制，同时保持了良好的一致性和鲁棒性保证，为学习增强在线算法提供了更广泛的应用框架。

Abstract: We study the online unweighted bipartite matching problem in the random arrival order model, with $n$ offline and $n$ online vertices, in the learning-augmented setting: The algorithm is provided with untrusted predictions of the types (neighborhoods) of the online vertices. We build upon the work of Choo et al. (ICML 2024, pp. 8762-8781) who proposed an approach that uses a prefix of the arrival sequence as a sample to determine whether the predictions are close to the true arrival sequence and then either follows the predictions or uses a known baseline algorithm that ignores the predictions and is $β$-competitive. Their analysis is limited to the case that the optimal matching has size $n$, i.e., every online vertex can be matched. We generalize their approach and analysis by removing any assumptions on the size of the optimal matching while only requiring that the size of the predicted matching is at least $αn$ for any constant $0 < α\le 1$. Our learning-augmented algorithm achieves $(1-o(1))$-consistency and $(β-o(1))$-robustness. Additionally, we show that the competitive ratio degrades smoothly between consistency and robustness with increasing prediction error.

</details>


### [87] [Modeling Quantum Autoencoder Trainable Kernel for IoT Anomaly Detection](https://arxiv.org/abs/2511.21932)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Swati Kumari,Navneet Singh*

Main category: cs.LG

TL;DR: 量子自编码器框架用于物联网入侵检测，在NISQ设备上实现实用量子优势


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法难以应对日益增长的网络安全威胁和高维物联网流量复杂性，深度学习存在计算瓶颈，限制了实时大规模部署

Method: 提出量子自编码器(QAE)框架，将网络流量压缩为判别性潜在表示，并使用量子支持向量分类(QSVC)进行入侵检测

Result: 在三个数据集上评估，在理想模拟器和IBM量子硬件上都实现了更高的准确率，展示了在当前NISQ设备上的实用量子优势。适度的去极化噪声作为隐式正则化，稳定训练并增强泛化能力

Conclusion: 这项工作确立了量子机器学习作为解决现实世界网络安全挑战的可行、硬件就绪的解决方案

Abstract: Escalating cyber threats and the high-dimensional complexity of IoT traffic have outpaced classical anomaly detection methods. While deep learning offers improvements, computational bottlenecks limit real-time deployment at scale. We present a quantum autoencoder (QAE) framework that compresses network traffic into discriminative latent representations and employs quantum support vector classification (QSVC) for intrusion detection. Evaluated on three datasets, our approach achieves improved accuracy on ideal simulators and on the IBM Quantum hardware demonstrating practical quantum advantage on current NISQ devices. Crucially, moderate depolarizing noise acts as implicit regularization, stabilizing training and enhancing generalization. This work establishes quantum machine learning as a viable, hardware-ready solution for real-world cybersecurity challenges.

</details>


### [88] [Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers](https://arxiv.org/abs/2511.22616)
*Meriem Arbaoui,Mohamed-el-Amine Brahmia,Abdellatif Rahmoun,Mourad Zghal*

Main category: cs.LG

TL;DR: 这篇综述论文系统性地回顾了联邦学习（FL）的研究现状，重点关注个性化、优化和鲁棒性三个方向，通过混合方法（文献计量分析+系统综述）对聚合策略、评估方法进行梳理，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网和人工智能的融合带来了创新，但隐私问题和数据孤岛阻碍了发展。传统的集中式机器学习难以解决这些挑战，因此需要联邦学习这种去中心化的协作训练范式，能够在保护数据隐私的同时实现模型训练。

Method: 采用混合方法学，结合文献计量分析和系统综述，对联邦学习研究进行结构化分类。重点关注三个研究方向：个性化、优化和鲁棒性，分析聚合策略（包括架构、同步方法和联邦目标），并进行实验比较不同聚合方法在IID和非IID数据分布下的表现。

Result: 提供了联邦学习的全面综述，包括对异质性、效率、安全和隐私等挑战的分析，聚合策略的分类，以及实验评估结果。识别了该领域最有影响力的工作，并为未来研究提供了指导框架。

Conclusion: 联邦学习是解决数据隐私和孤岛问题的有前景范式，但仍面临异质性带来的复杂性。论文通过系统性的综述和分类，为研究人员提供了清晰的路线图，并指出了未来需要重点关注的创新方向，以推动这一快速发展的领域向前发展。

Abstract: The integration of IoT and AI has unlocked innovation across industries, but growing privacy concerns and data isolation hinder progress. Traditional centralized ML struggles to overcome these challenges, which has led to the rise of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing local raw data. FL ensures data privacy, reduces communication overhead, and supports scalability, yet its heterogeneity adds complexity compared to centralized approaches. This survey focuses on three main FL research directions: personalization, optimization, and robustness, offering a structured classification through a hybrid methodology that combines bibliometric analysis with systematic review to identify the most influential works. We examine challenges and techniques related to heterogeneity, efficiency, security, and privacy, and provide a comprehensive overview of aggregation strategies, including architectures, synchronization methods, and diverse federation objectives. To complement this, we discuss practical evaluation approaches and present experiments comparing aggregation methods under IID and non-IID data distributions. Finally, we outline promising research directions to advance FL, aiming to guide future innovation in this rapidly evolving field.

</details>


### [89] [Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation](https://arxiv.org/abs/2511.21934)
*Tao Zhe,Huazhen Fang,Kunpeng Liu,Qian Lou,Tamzidul Hoque,Dongjie Wang*

Main category: cs.LG

TL;DR: 提出一个异构多智能体强化学习框架，用于自动化特征转换，通过共享评论家机制和注意力特征选择解决动态特征空间和智能体协作问题。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习有进展，但特征转换对结构化数据仍然重要。现有自动化方法依赖启发式或穷举搜索效率低，强化学习方法存在动态特征扩展不稳定和智能体协作不足的问题。

Method: 提出异构多智能体强化学习框架，包含三类智能体：两种类型分别选择特征和操作。采用共享评论家机制促进智能体间通信，使用多头注意力特征智能体处理动态扩展特征空间，引入状态编码技术稳定学习过程。

Result: 通过大量实验验证了模型在有效性、效率、鲁棒性和可解释性方面的优势。

Conclusion: 该框架解决了现有强化学习方法在特征转换中的动态特征扩展和智能体协作问题，实现了更稳定、高效和可解释的自动化特征转换。

Abstract: Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.

</details>


### [90] [Breaking Algorithmic Collusion in Human-AI Ecosystems](https://arxiv.org/abs/2511.21935)
*Natalie Collina,Eshwar Ram Arunachaleswaran,Meena Jagadeesan*

Main category: cs.LG

TL;DR: 研究人类-AI混合生态系统中算法合谋的稳定性，发现人类不采用AI而使用无遗憾策略会破坏AI间的价格合谋，即使单个人类参与者也能显著降低价格。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在生态系统中与人类和其他AI频繁互动，需要理解这种混合生态系统中算法合谋的稳定性。特别是当人类不采用AI而使用手动定价策略时，是否会破坏AI代理之间维持的超竞争价格。

Method: 使用重复定价博弈的经典理论框架，建立风格化模型：AI代理采用均衡策略，而一个或多个人类参与者采用无遗憾策略进行手动定价。研究人类"叛逃"对AI间价格合谋的影响。

Result: 即使单个人类参与者也能破坏合谋并显著降低价格，多个人类参与者会使价格更接近竞争水平。研究还展示了在叛逃感知AI代理下合谋性质的变化。

Conclusion: 算法合谋在混合生态系统中可能是脆弱的，人类参与者的存在会破坏AI代理之间的价格合谋，但具体效果取决于AI是否意识到人类叛逃的存在。

Abstract: AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.

</details>


### [91] [Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection](https://arxiv.org/abs/2511.21940)
*Kiran Nair,Hubert Cecotti*

Main category: cs.LG

TL;DR: 该研究提出多种深度学习架构用于C-VEP脑机接口解码，包括CNN和孪生网络，显著优于传统方法，其中多类孪生网络达到96.89%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 基于C-VEP的非侵入式脑机接口需要高度鲁棒的解码方法来应对EEG信号的时间变异性和会话依赖性噪声，传统方法在这方面存在局限。

Method: 提出并评估了多种深度学习架构：用于63位m序列重建和分类的卷积神经网络、基于相似性解码的孪生网络，以及CCA基线方法。使用地球移动距离和约束EMD进行距离解码，并通过时间数据增强（小位移）提高跨会话泛化能力。

Result: 深度学习模型显著优于传统方法，基于距离的解码（特别是EMD和约束EMD）对延迟变化比欧几里得和马氏距离更鲁棒。多类孪生网络表现最佳，平均准确率达到96.89%。

Conclusion: 数据驱动的深度学习架构在单次试验C-VEP解码中展现出巨大潜力，能够为自适应非侵入式脑机接口系统提供可靠解决方案，特别是在处理时间变异性和会话间差异方面。

Abstract: Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.

</details>


### [92] [Closing the Generalization Gap in Parameter-efficient Federated Edge Learning](https://arxiv.org/abs/2511.23282)
*Xinnong Du,Zhonghao Lyu,Xiaowen Cao,Chunyang Wen,Shuguang Cui,Jie Xu*

Main category: cs.LG

TL;DR: 提出一个参数高效的联邦边缘学习框架，通过联合模型剪枝和客户端选择来应对数据异构和资源受限的挑战，结合泛化感知分析和系统级优化提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习虽然能保护数据隐私，但面临本地数据集有限且异构、资源受限部署的问题，这会严重降低模型泛化能力和资源利用率，从而影响学习性能。

Method: 1) 推导信息论泛化界，刻画训练和测试函数损失的差异，并将其嵌入收敛分析；2) 构建泛化感知的平均平方梯度范数界最小化问题，联合优化剪枝率、客户端选择和通信计算资源；3) 提出交替优化算法高效求解这个非凸混合整数问题。

Result: 广泛的实验表明，所提出的设计在多个基准测试中优于现有基线方法，验证了将泛化感知分析与系统级优化相结合对高效联邦边缘学习的有效性。

Conclusion: 通过联合模型剪枝和客户端选择，并结合泛化感知分析与系统级优化，能够有效应对联邦边缘学习中的数据异构和资源约束挑战，显著提升学习性能。

Abstract: Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.

</details>


### [93] [ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions](https://arxiv.org/abs/2511.21952)
*Krishna Khadka,Sunny Shree,Pujan Budhathoki,Yu Lei,Raghu Kacker,D. Richard Kuhn*

Main category: cs.LG

TL;DR: 提出ABEL方法，通过生成对抗点对来逼近局部决策边界，提高解释的稳定性和保真度


<details>
  <summary>Details</summary>
Motivation: 现有局部解释方法（如LIME）存在不稳定和局部保真度差的问题，需要更可靠的可解释AI方法

Method: 1) 在测试实例附近生成邻域点；2) 对每个邻域点进行两次对抗攻击，生成对抗点对(A,A')来界定决策边界；3) 在这些对抗点对上训练线性模型来近似局部决策边界

Result: 在六个UCI基准数据集和三种深度神经网络架构上的实验表明，ABEL方法比现有最先进方法具有更高的稳定性和保真度

Conclusion: ABEL方法通过对抗点对界定决策边界，有效提高了局部解释的稳定性和保真度，为黑盒模型提供了更可靠的可解释性

Abstract: Machine learning models are increasingly used in critical applications but are mostly "black boxes" due to their lack of transparency. Local explanation approaches, such as LIME, address this issue by approximating the behavior of complex models near a test instance using simple, interpretable models. However, these approaches often suffer from instability and poor local fidelity. In this paper, we propose a novel approach called Adversarially Bracketed Local Explanation (ABLE) to address these limitations. Our approach first generates a set of neighborhood points near the test instance, x_test, by adding bounded Gaussian noise. For each neighborhood point D, we apply an adversarial attack to generate an adversarial point A with minimal perturbation that results in a different label than D. A second adversarial attack is then performed on A to generate a point A' that has the same label as D (and thus different than A). The points A and A' form an adversarial pair that brackets the local decision boundary for x_test. We then train a linear model on these adversarial pairs to approximate the local decision boundary. Experimental results on six UCI benchmark datasets across three deep neural network architectures demonstrate that our approach achieves higher stability and fidelity than the state-of-the-art.

</details>


### [94] [Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation](https://arxiv.org/abs/2511.23440)
*Bernhard Klein,Falk Selker,Hendrik Borras,Sophie Steger,Franz Pernkopf,Holger Fröning*

Main category: cs.LG

TL;DR: PFP-BNNs通过高斯分布假设实现高效贝叶斯不确定性估计，在ARM嵌入式CPU上实现4200倍加速，保持与SVI相当的精度和OOD检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在安全关键应用中因不确定性处理不足而受限，贝叶斯神经网络(BNNs)能提供概率估计但计算成本高，需要高效部署方案。

Method: 提出概率前向传播(PFP)方法，假设权重和激活值服从高斯分布，实现完全解析的不确定性传播，用单次确定性前向传播替代采样。结合TVM深度学习编译器，为MLP和CNN实现高斯传播算子库，采用手动和自动调优策略。

Result: PFP在计算效率上显著优于SVI，小批量处理时加速达4200倍。在Dirty-MNIST数据集上，PFP-BNNs在精度、不确定性估计和OOD检测方面与SVI-BNNs相当，同时大幅降低计算成本。

Conclusion: 贝叶斯近似与代码生成结合可在资源受限系统上实现高效的BNN部署，为安全关键应用提供可行的不确定性感知解决方案。

Abstract: Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.

</details>


### [95] [CTR Prediction on Alibaba's Taobao Advertising Dataset Using Traditional and Deep Learning Models](https://arxiv.org/abs/2511.21963)
*Hongyu Yang,Chunxi Wen,Jiyin Zhang,Nanfei Shen,Shijiao Zhang,Xiyan Han*

Main category: cs.LG

TL;DR: 该研究探索使用淘宝大规模数据集进行CTR预测，从传统机器学习模型（逻辑回归、Light-GBM）到深度学习模型（MLP、Transformer），通过结合用户行为序列建模显著提升预测性能，并探讨了技术在公共卫生等领域的扩展应用。


<details>
  <summary>Details</summary>
Motivation: 点击率预测在现代广告系统中至关重要，直接影响平台效率和商业价值。传统基于静态特征的模型难以捕捉用户行为模式，需要更有效地建模用户意图和行为序列。

Method: 1. 使用阿里巴巴发布的淘宝大规模数据集；2. 从监督学习模型（逻辑回归、Light-GBM）开始，基于静态特征；3. 结合22天内数亿次交互的行为数据，提取用户行为序列；4. 使用深度学习模型融合行为嵌入和静态特征，包括MLP和基于Transformer的架构；5. Transformer使用自注意力机制学习行为序列的上下文依赖关系；6. 提出A/B测试策略进行实际评估。

Result: Transformer模型相比基线逻辑回归模型将AUC提升了2.81%，对于兴趣多样化或随时间变化的用户提升效果最显著。深度学习模型特别是Transformer在CTR预测上取得了显著性能改进。

Conclusion: 研究展示了结合用户行为序列建模对CTR预测的重要性，Transformer架构能有效捕捉用户兴趣的时序动态。该技术不仅可用于电商广告，还可扩展到公共卫生等领域的精准信息推送，为CTR预测的进步和跨领域应用提供了路线图。

Abstract: Click-through rates prediction is critical in modern advertising systems, where ranking relevance and user engagement directly impact platform efficiency and business value. In this project, we explore how to model CTR more effectively using a large-scale Taobao dataset released by Alibaba. We start with supervised learning models, including logistic regression and Light-GBM, that are trained on static features such as user demographics, ad attributes, and contextual metadata. These models provide fast, interpretable benchmarks, but have limited capabilities to capture patterns of behavior that drive clicks. To better model user intent, we combined behavioral data from hundreds of millions of interactions over a 22-day period. By extracting and encoding user action sequences, we construct representations of user interests over time. We use deep learning models to fuse behavioral embeddings with static features. Among them, multilayer perceptrons (MLPs) have achieved significant performance improvements. To capture temporal dynamics, we designed a Transformer-based architecture that uses a self-attention mechanism to learn contextual dependencies across behavioral sequences, modeling not only what the user interacts with, but also the timing and frequency of interactions. Transformer improves AUC by 2.81 % over the baseline (LR model), with the largest gains observed for users whose interests are diverse or change over time. In addition to modeling, we propose an A/B testing strategy for real-world evaluation. We also think about the broader implications: personalized ad targeting technology can be applied to public health scenarios to achieve precise delivery of health information or behavior guidance. Our research provides a roadmap for advancing click-through rate predictions and extending their value beyond e-commerce.

</details>


### [96] [MOTIF-RF: Multi-template On-chip Transformer Synthesis Incorporating Frequency-domain Self-transfer Learning for RFIC Design Automation](https://arxiv.org/abs/2511.21970)
*Houbo He,Yizhou Xu,Lei Xia,Yaolong Hu,Fan Cai,Taiyun Chi*

Main category: cs.LG

TL;DR: 该论文提出了一种用于射频集成电路变压器逆向设计的机器学习框架，包括多模板ML代理模型、频域自迁移学习技术和基于CMA-ES的逆向设计方法。


<details>
  <summary>Details</summary>
Motivation: 射频集成电路中变压器的设计过程复杂且耗时，需要自动化AI辅助设计工具来提高设计效率和质量，实现从规格到版图的自动化流程。

Method: 1. 对四种ML架构（MLP、CNN、UNet、GT）进行基准测试；2. 提出频域自迁移学习技术，利用相邻频段相关性提高S参数预测精度；3. 开发基于CMA-ES算法的逆向设计框架。

Result: 频域自迁移学习使S参数预测精度提升30%-50%；基于CMA-ES的逆向设计框架在多个阻抗匹配任务中表现出快速收敛和可靠性能。

Conclusion: 该研究推进了射频集成电路AI辅助设计自动化目标，为设计师提供了可操作的AI集成工具，实现了从规格到版图的自动化设计流程。

Abstract: This paper presents a systematic study on developing multi-template machine learning (ML) surrogate models and applying them to the inverse design of transformers (XFMRs) in radio-frequency integrated circuits (RFICs). Our study starts with benchmarking four widely used ML architectures, including MLP-, CNN-, UNet-, and GT-based models, using the same datasets across different XFMR topologies. To improve modeling accuracy beyond these baselines, we then propose a new frequency-domain self-transfer learning technique that exploits correlations between adjacent frequency bands, leading to around 30%-50% accuracy improvement in the S-parameters prediction. Building on these models, we further develop an inverse design framework based on the covariance matrix adaptation evolutionary strategy (CMA-ES) algorithm. This framework is validated using multiple impedance-matching tasks, all demonstrating fast convergence and trustworthy performance. These results advance the goal of AI-assisted specs-to-GDS automation for RFICs and provide RFIC designers with actionable tools for integrating AI into their workflows.

</details>


### [97] [A Safety and Security Framework for Real-World Agentic Systems](https://arxiv.org/abs/2511.21990)
*Shaona Ghosh,Barnaby Simkin,Kyriacos Shiarlis,Soumili Nandi,Dan Zhao,Matthew Fiedler,Julia Bazinska,Nikki Pope,Roopa Prabhu,Daniel Rohrer,Michael Demoret,Bartley Richardson*

Main category: cs.LG

TL;DR: 提出一个动态可操作的框架，用于保护企业部署中的智能体AI系统，将安全和安全视为智能体交互中涌现的属性，通过AI驱动的红队测试发现新型智能体风险。


<details>
  <summary>Details</summary>
Motivation: 传统AI安全方法将安全和安全视为固定属性，但智能体AI系统中，安全和安全是模型、编排器、工具和数据在操作环境中动态交互涌现的属性。需要新的框架来识别和管理智能体特有的风险。

Method: 提出动态智能体安全与安全框架，包括：1) 定义统一的智能体风险分类法；2) 使用辅助AI模型和智能体进行上下文风险管理；3) 通过沙盒化AI驱动红队测试进行风险发现；4) 在NVIDIA AI-Q研究助手案例中验证框架。

Result: 框架在NVIDIA旗舰智能体研究助手AI-Q中成功应用，发现了新型智能体风险并进行上下文缓解。发布了包含10,000多次现实攻击和防御执行轨迹的数据集，推动智能体安全研究。

Conclusion: 智能体AI系统的安全和安全需要动态、上下文感知的方法。提出的框架通过统一风险分类、AI辅助风险管理和红队测试，有效识别和缓解智能体特有风险，为企业部署提供实用解决方案。

Abstract: This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.

</details>


### [98] [Distance-based Learning of Hypertrees](https://arxiv.org/abs/2511.22014)
*Shaun Fallat,Kamyar Khodamoradi,David Kirkpatrick,Valerii Maliuk,S. Ahmad Mojallal,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文提出了首个可证明最优的在线算法，用于学习一类称为有序超树的超图，该算法也可转化为最优离线算法。有序超树属于Fagin层次结构中的可学习子类，且严格包含该层次中所有可用亚二次SP查询复杂度学习的超图。此外，针对距离测量随距离增加而衰减的场景，本文还研究了有界距离查询模型，并给出了学习一般超树的渐近紧复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过最短路径查询（SP-queries）学习超图结构。超图在数据库理论、进化树重建等领域有重要应用，但现有方法在查询复杂度上不够高效。特别是在进化树重建等场景中，距离测量会随距离增加而衰减，需要更符合实际的学习模型。

Method: 1. 针对有序超树（orderly hypertrees）提出首个可证明最优的在线算法，该算法使用最短路径查询学习超图结构。2. 将在线算法转化为可证明最优的离线算法。3. 将有序超树定位在Fagin无环超图层次结构中。4. 针对距离测量衰减的场景，研究有界距离查询模型，并分析学习一般超树的复杂度界限。

Result: 1. 提出了首个可证明最优的在线算法，用于学习有序超树。2. 证明了有序超树严格包含Fagin层次结构中所有可用亚二次SP查询复杂度学习的超图。3. 在有界距离查询模型中，给出了学习一般超树的渐近紧复杂度界限。

Conclusion: 本文在超图学习领域取得了重要进展：提出了首个可证明最优的在线算法用于学习有序超树，并建立了有序超树在Fagin层次结构中的理论位置。同时，针对实际应用中距离测量衰减的问题，提出了有界距离查询模型并获得了紧复杂度界限，为进化树重建等应用提供了理论基础。

Abstract: We study the problem of learning hypergraphs with shortest-path queries (SP-queries), and present the first provably optimal online algorithm for a broad and natural class of hypertrees that we call orderly hypertrees. Our online algorithm can be transformed into a provably optimal offline algorithm. Orderly hypertrees can be positioned within the Fagin hierarchy of acyclic hypergraph (well-studied in database theory), and strictly encompass the broadest class in this hierarchy that is learnable with subquadratic SP-query complexity.
  Recognizing that in some contexts, such as evolutionary tree reconstruction, distance measurements can degrade with increased distance, we also consider a learning model that uses bounded distance queries. In this model, we demonstrate asymptotically tight complexity bounds for learning general hypertrees.

</details>


### [99] [Equilibrium Propagation Without Limits](https://arxiv.org/abs/2511.22024)
*Elon Litman*

Main category: cs.LG

TL;DR: 该论文为平衡传播建立了有限扰动理论基础，证明对比赫布学习更新是任意有限扰动的精确梯度估计器，无需无穷小近似或凸性假设。


<details>
  <summary>Details</summary>
Motivation: 传统平衡传播方法依赖于无穷小扰动的近似，限制了其在实际应用中的有效性。作者希望将平衡传播从无穷小扰动的限制中解放出来，建立适用于有限扰动的理论基础。

Method: 将网络状态建模为吉布斯-玻尔兹曼分布而非确定性点，证明自由能差梯度等于局部能量导数的期望差。推导基于损失-能量协方差路径积分的广义平衡传播算法。

Result: 证明了对比赫布学习更新是任意有限扰动的精确梯度估计器，无需无穷小近似或凸性假设。提出了支持强误差信号的广义平衡传播算法。

Conclusion: 该研究为平衡传播建立了坚实的有限扰动理论基础，使学习算法能够处理标准无穷小近似无法支持的强误差信号，扩展了平衡传播的应用范围。

Abstract: We liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.

</details>


### [100] [Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation](https://arxiv.org/abs/2511.22030)
*Geun-Deok Jang,Dong-Kyun Han,Seo-Hyeon Park,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出基于在线测试时适应的驾驶员疲劳检测框架，通过动态调整批归一化层参数和记忆库管理，解决EEG信号个体差异导致的域偏移问题，在模拟驾驶数据上取得81.73%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 基于脑电图的疲劳检测系统面临信号个体差异大、需要繁琐校准的问题，特别是跨被试的域偏移问题使得模型难以泛化到未见过的目标被试。

Method: 提出在线测试时适应框架：1）更新批归一化层可学习参数，同时保留预训练归一化统计量；2）使用基于负能量分数和持续时间的记忆库动态管理流式EEG片段；3）引入原型学习以增强对时间分布偏移的鲁棒性。

Result: 在模拟持续注意驾驶数据集上验证，平均F1分数达到81.73%，比最佳测试时适应基线提升11.73%，显著增强了非独立同分布场景下的适应性。

Conclusion: 提出的在线测试时适应方法能有效解决EEG信号个体差异导致的域偏移问题，显著提升疲劳检测系统在非独立同分布场景下的适应性和泛化能力。

Abstract: Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\%, an improvement of 11.73\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.

</details>


### [101] [Predicting Public Health Impacts of Electricity Usage](https://arxiv.org/abs/2511.22031)
*Yejia Liu,Zhifeng Wu,Pengfei Li,Shaolei Ren*

Main category: cs.LG

TL;DR: 开发HealthPredictor AI模型，通过端到端管道将电力使用与公共健康结果连接，实现健康驱动的需求侧能源管理


<details>
  <summary>Details</summary>
Motivation: 电力行业是空气污染的主要来源，尽管已有监管措施，但化石燃料仍是能源供应的重要组成部分，需要更先进的需求侧方法来减少公共健康影响

Method: 开发包含三个组件的HealthPredictor模型：燃料组合预测器（估计不同发电来源贡献）、空气质量转换器（建模污染物排放和大气扩散）、健康影响评估器（将污染物变化转化为货币化健康损害）

Result: 在美国多个地区，健康驱动的优化框架比燃料组合驱动基线在公共健康影响预测方面误差显著降低；电动汽车充电调度案例研究展示了该方法带来的公共健康收益和可操作指导

Conclusion: 这项工作展示了如何明确设计AI模型以实现健康驱动的能源管理，促进公共健康和更广泛的社会福祉；数据集和代码已开源

Abstract: The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.

</details>


### [102] [Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian](https://arxiv.org/abs/2511.22069)
*Yiran Zhang,Weihang Xu,Mo Zhou,Maryam Fazel,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 该论文研究了过参数化模型在分数匹配目标下学习单个高斯分布的优化动态，证明了在不同噪声机制下的收敛性结果，包括高噪声下的全局收敛、低噪声下的特定初始化条件收敛，以及随机初始化下的参数发散但损失收敛现象。


<details>
  <summary>Details</summary>
Motivation: 分数匹配在现代生成建模中已成为核心训练目标，特别是在扩散模型中用于学习高维数据分布。尽管经验上成功，但在过参数化机制下分数匹配优化行为的理论理解仍然有限。本文旨在填补这一理论空白。

Method: 使用具有n个可学习参数的学生模型，在从单个真实高斯分布生成的数据上训练，使用总体分数匹配目标。分析梯度下降在不同机制下的优化动态：高噪声机制、低噪声机制、指数小初始化条件和随机高斯初始化条件。

Result: 1) 噪声足够大时，证明了梯度下降的全局收敛性；2) 低噪声机制下存在驻点，但指数小初始化可确保所有参数收敛到真实值；3) 无指数小初始化时参数可能不收敛；4) 随机高斯初始化时，高概率下只有一个参数收敛而其他参数发散，但损失仍以1/τ速率收敛到零；5) 建立了该机制下收敛率的近乎匹配下界。

Conclusion: 这是首个在分数匹配框架下为至少三个分量的高斯混合建立全局收敛保证的工作，揭示了过参数化分数匹配优化的复杂动态，包括参数发散但损失收敛的"隐式正则化"现象，为理解现代生成模型的训练行为提供了理论洞见。

Abstract: Score matching has become a central training objective in modern generative modeling, particularly in diffusion models, where it is used to learn high-dimensional data distributions through the estimation of score functions. Despite its empirical success, the theoretical understanding of the optimization behavior of score matching, particularly in over-parameterized regimes, remains limited. In this work, we study gradient descent for training over-parameterized models to learn a single Gaussian distribution. Specifically, we use a student model with $n$ learnable parameters and train it on data generated from a single ground-truth Gaussian using the population score matching objective. We analyze the optimization dynamics under multiple regimes. When the noise scale is sufficiently large, we prove a global convergence result for gradient descent. In the low-noise regime, we identify the existence of a stationary point, highlighting the difficulty of proving global convergence in this case. Nevertheless, we show convergence under certain initialization conditions: when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters to the ground truth. We further prove that without the exponentially small initialization, the parameters may not converge to the ground truth. Finally, we consider the case where parameters are randomly initialized from a Gaussian distribution far from the ground truth. We prove that, with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero with a $1/τ$ rate, where $τ$ is the number of iterations. We also establish a nearly matching lower bound on the convergence rate in this regime. This is the first work to establish global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework.

</details>


### [103] [A Multi-View Multi-Timescale Hypergraph-Empowered Spatiotemporal Framework for EV Charging Forecasting](https://arxiv.org/abs/2511.22072)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: HyperCast：基于超图建模的电动汽车充电需求预测框架，通过捕捉充电站间的高阶时空依赖关系，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的预测方法通常只能建模充电站间的成对关系，无法捕捉城市充电网络中复杂的群体动态。准确预测电动汽车充电需求对电网稳定运行和电动汽车参与电力市场至关重要。

Method: 提出HyperCast框架，利用超图建模EV充电模式中的高阶时空依赖关系。整合多视图超图（捕捉静态地理邻近性和动态需求相似性）和多时间尺度输入（区分近期趋势和周期性模式）。采用专门的超时空块和定制的交叉注意力机制融合不同视图和时间尺度的信息。

Result: 在四个公共数据集上的广泛实验表明，HyperCast显著优于多种最先进的基线方法，证明了显式建模集体充电行为对提高预测准确性的有效性。

Conclusion: 通过超图建模充电站间的高阶时空依赖关系能够更准确地预测电动汽车充电需求，为电网稳定运行和电动汽车参与电力市场提供了更有效的预测工具。

Abstract: Accurate electric vehicle (EV) charging demand forecasting is essential for stable grid operation and proactive EV participation in electricity market. Existing forecasting methods, particularly those based on graph neural networks, are often limited to modeling pairwise relationships between stations, failing to capture the complex, group-wise dynamics inherent in urban charging networks. To address this gap, we develop a novel forecasting framework namely HyperCast, leveraging the expressive power of hypergraphs to model the higher-order spatiotemporal dependencies hidden in EV charging patterns. HyperCast integrates multi-view hypergraphs, which capture both static geographical proximity and dynamic demand-based functional similarities, along with multi-timescale inputs to differentiate between recent trends and weekly periodicities. The framework employs specialized hyper-spatiotemporal blocks and tailored cross-attention mechanisms to effectively fuse information from these diverse sources: views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms a wide array of state-of-the-art baselines, demonstrating the effectiveness of explicitly modeling collective charging behaviors for more accurate forecasting.

</details>


### [104] [ARES: Anomaly Recognition Model For Edge Streams](https://arxiv.org/abs/2511.22078)
*Simone Mungari,Albert Bifet,Giuseppe Manco,Bernhard Pfahringer*

Main category: cs.LG

TL;DR: ARES是一个用于时序图边流异常检测的无监督框架，结合图神经网络进行特征提取和半空间树进行异常评分，通过监督阈值机制优化检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的流式信息可以表示为时序图，其中边随时间动态变化。实时检测边异常对于降低潜在风险至关重要，但由于概念漂移、大数据量和实时响应需求，这一任务比传统异常检测更具挑战性。

Method: ARES结合图神经网络（GNNs）进行特征提取和半空间树（HST）进行异常评分。GNNs通过将节点和边属性嵌入到潜在空间中来捕获流中的尖峰和突发异常行为，而HST则对该空间进行分区以高效隔离异常。此外，还引入了基于统计离散度的监督阈值机制，使用少量标记数据确定最优阈值。

Result: 在多个真实网络攻击场景中进行广泛评估，与现有方法相比表现出色，同时分析了其空间和时间复杂度。

Conclusion: ARES是一个有效的无监督异常检测框架，能够实时检测时序图中的边异常，通过结合GNNs和HST的优势，并辅以监督阈值机制，在不同领域具有良好的适应性和检测能力。

Abstract: Many real-world scenarios involving streaming information can be represented as temporal graphs, where data flows through dynamic changes in edges over time. Anomaly detection in this context has the objective of identifying unusual temporal connections within the graph structure. Detecting edge anomalies in real time is crucial for mitigating potential risks. Unlike traditional anomaly detection, this task is particularly challenging due to concept drifts, large data volumes, and the need for real-time response. To face these challenges, we introduce ARES, an unsupervised anomaly detection framework for edge streams. ARES combines Graph Neural Networks (GNNs) for feature extraction with Half-Space Trees (HST) for anomaly scoring. GNNs capture both spike and burst anomalous behaviors within streams by embedding node and edge properties in a latent space, while HST partitions this space to isolate anomalies efficiently. ARES operates in an unsupervised way without the need for prior data labeling. To further validate its detection capabilities, we additionally incorporate a simple yet effective supervised thresholding mechanism. This approach leverages statistical dispersion among anomaly scores to determine the optimal threshold using a minimal set of labeled data, ensuring adaptability across different domains. We validate ARES through extensive evaluations across several real-world cyber-attack scenarios, comparing its performance against existing methods while analyzing its space and time complexity.

</details>


### [105] [Quantum Bayesian Optimization for Quality Improvement in Fuselage Assembly](https://arxiv.org/abs/2511.22090)
*Jiayu Liu,Chong Liu,Trevor Rhone,Yinan Wang*

Main category: cs.LG

TL;DR: 量子贝叶斯优化框架用于航空航天机身装配中的形状控制，通过量子算法提高样本效率，减少装配间隙


<details>
  <summary>Details</summary>
Motivation: 现有智能制造方法在航空航天机身装配中面临样本效率低的问题，传统蒙特卡洛方法在从分布中获取平均响应时存在局限性。量子算法能以更少样本达到相同估计精度，因此可以利用量子算法优势改进制造实践中的样本效率

Method: 提出量子贝叶斯优化框架，基于有限元分析或代理模型构建量子预言机，以更少查询获得更准确的环境响应估计。采用上置信界作为采集函数，战略性地选择最可能最大化目标函数的输入值

Result: 实验结果表明，相比传统方法，量子贝叶斯优化在相同模拟查询下实现了显著更低的尺寸误差和不确定性

Conclusion: 量子贝叶斯优化框架能够显著提高航空航天机身装配中的样本效率，实现更精确的形状控制，减少装配间隙，为智能制造提供了新的优化方法

Abstract: Recent efforts in smart manufacturing have enhanced aerospace fuselage assembly processes, particularly by innovating shape adjustment techniques to minimize dimensional gaps between assembled sections. Existing approaches have shown promising results but face the issue of low sample efficiency from the manufacturing systems. It arises from the limitation of the classical Monte Carlo method when uncovering the mean response from a distribution. In contrast, recent work has shown that quantum algorithms can achieve the same level of estimation accuracy with significantly fewer samples than the classical Monte Carlo method from distributions. Therefore, we can adopt the estimation of the quantum algorithm to obtain the estimation from real physical systems (distributions). Motivated by this advantage, we propose a Quantum Bayesian Optimization (QBO) framework for precise shape control during assembly to improve the sample efficiency in manufacturing practice. Specifically, this approach utilizes a quantum oracle, based on finite element analysis (FEA)-based models or surrogate models, to acquire a more accurate estimation of the environment response with fewer queries for a certain input. QBO employs an Upper Confidence Bound (UCB) as the acquisition function to strategically select input values that are most likely to maximize the objective function. It has been theoretically proven to require much fewer samples while maintaining comparable optimization results. In the case study, force-controlled actuators are applied to one fuselage section to adjust its shape and reduce the gap to the adjoining section. Experimental results demonstrate that QBO achieves significantly lower dimensional error and uncertainty compared to classical methods, particularly using the same queries from the simulation.

</details>


### [106] [Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs](https://arxiv.org/abs/2511.22099)
*Daniel Agyei Asante,Md Mokarram Chowdhury,Yang Li*

Main category: cs.LG

TL;DR: 低秩压缩对LLM可信度的影响：隐私保护减弱但训练数据隐私改善，对抗鲁棒性保持或增强，伦理推理在零样本下降但少样本可恢复，公平性下降


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限环境部署困难，低秩压缩是有效的模型压缩方法，但其对模型可信度（隐私、对抗鲁棒性、公平性、伦理对齐）的影响尚未被充分研究

Method: 对多种不同规模和变体的LLM使用多种低秩算法进行压缩，全面评估压缩对隐私、对抗鲁棒性、公平性和伦理对齐的影响，并进行梯度归因分析识别对抗鲁棒性的关键层

Result: 1) 低秩压缩保持或改善训练数据隐私但削弱对话中的PII保护；2) 对抗鲁棒性通常保持甚至增强，即使在深度压缩下；3) 伦理推理在零样本设置下退化但通过少样本提示部分恢复；4) 公平性在压缩下下降

Conclusion: 低秩压缩对LLM可信度有复杂影响，需要平衡压缩收益与可信度损失，梯度归因分析为设计可信压缩策略提供指导，需考虑模型规模和微调对可信度的影响

Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.

</details>


### [107] [Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba](https://arxiv.org/abs/2511.22101)
*Zhaofeng Zhang*

Main category: cs.LG

TL;DR: 该报告复制并改进了"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning"一文，提出结合Mamba与DDQN的新结构，在某些测试中表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: 复制并改进Uniswap V3中基于深度强化学习的自适应流动性提供方法，以提升模型的理论基础和实际性能。

Method: 1) 从Uniswap Subgraph获取数据并实现原模型；2) 提出新结构：结合Mamba与DDQN，设计新奖励函数；3) 重新清洗数据并引入两个新基线进行比较。

Result: 新模型虽未应用于所有数据集，但比原模型具有更强的理论支持，并在某些测试中表现更好。

Conclusion: 提出的Mamba+DDQN结构在自适应流动性提供问题上显示出潜力，但需要进一步验证其在更广泛数据集上的性能。

Abstract: The report goes through the main steps of replicating and improving the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.

</details>


### [108] [Representative Action Selection for Large Action Space: From Bandits to MDPs](https://arxiv.org/abs/2511.22104)
*Quan Zhou,Shie Mannor*

Main category: cs.LG

TL;DR: 提出一种从大规模动作空间中选取代表性子集的方法，使该子集在每个环境中都包含近似最优动作，从而在MDP中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 在库存管理和推荐系统等应用中，动作空间极大，直接在整个空间上学习不可行。需要找到一个小型代表性动作子集，使其在每个环境中都包含近似最优动作。

Method: 将元多臂赌博机的方法扩展到马尔可夫决策过程，在放松的非中心化次高斯过程模型下，证明现有算法能达到与使用完整动作空间相当的性能。

Result: 理论证明该方法在更宽松的环境异质性假设下，仍能保证性能与使用完整动作空间相当，为大规模组合决策提供计算和样本高效的解决方案。

Conclusion: 该方法成功将元多臂赌博机扩展到MDP，在放松的环境模型下提供理论保证，为大规模不确定环境下的组合决策问题提供了实用解决方案。

Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.

</details>


### [109] [Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.22105)
*Saad Masrur,Ismail Guvenc,David Lopez Perez*

Main category: cs.LG

TL;DR: 提出基于多智能体深度强化学习（MARL-DDQN）的动态睡眠模式优化框架，用于毫米波网络中最大化能效并满足服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波网络中的睡眠模式优化方法依赖静态基站流量模型，无法捕捉非平稳流量动态，且状态-动作空间过大，限制了实际部署。需要解决动态环境下的能效优化问题。

Method: 提出MARL-DDQN框架，使用双深度Q网络进行多智能体强化学习。在3D城市环境中采用时变和社区化的用户设备移动模型，集成真实的基站功耗模型和波束赋形技术，通过分布式决策最小化信令开销。

Result: 仿真显示MARL-DDQN优于现有策略（All On、IT-QoS-LB、MARL-DDPG、MARL-PPO），达到0.60 Mbit/Joule能效，8.5 Mbps第10百分位吞吐量，在动态场景下95%时间满足QoS约束。

Conclusion: MARL-DDQN框架能有效优化毫米波网络的动态睡眠模式，在保证服务质量的同时显著提升能效，为解决非平稳流量动态和大状态-动作空间问题提供了可行方案。

Abstract: Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.

</details>


### [110] [An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface](https://arxiv.org/abs/2511.22108)
*Zhou Biyan,Arindam Basu*

Main category: cs.LG

TL;DR: 提出基于深度脉冲神经网络的连续学习方法，使用Banditron和AGREL两种强化学习算法，在植入式脑机接口中实现高效、低功耗的神经解码器连续学习。


<details>
  <summary>Details</summary>
Motivation: 植入式脑机接口中同时记录的神经元数量呈指数增长，需要在植入设备中集成神经解码器进行数据压缩。但系统的非平稳性使解码器性能不可靠，频繁重新训练不现实，需要连续学习方法来确保用户安全舒适。

Method: 将强化学习算法（Banditron和AGREL）适配到深度脉冲神经网络中，这两种算法计算资源需求低，能有效处理非平稳问题并符合植入设备的能耗限制。通过开环和闭环实验评估方法有效性。

Result: 开环实验中，DSNN Banditron和DSNN AGREL的准确率在长时间内保持稳定。闭环实验中，DSNN Banditron与DSNN AGREL性能相当，但训练时内存访问使用减少98%，乘累加操作需求减少99%。相比之前的连续学习SNN解码器，计算量减少98%。

Conclusion: DSNN Banditron是未来无线植入式脑机接口系统的理想候选方案，能在保持性能的同时大幅降低计算资源需求，满足植入设备的严格能耗限制。

Abstract: The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.

</details>


### [111] [Toward Data-Driven Surrogates of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.22112)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 使用球面傅里叶神经算子(SFNO)开发太阳风稳态建模的替代模型，相比传统数值替代模型HUX在多个指标上表现相当或更好，可实现高效实时预报。


<details>
  <summary>Details</summary>
Motivation: 太阳风变化对地球空间系统有重大影响，但传统的3D磁流体动力学模型计算成本高，限制了边界条件不确定性研究。需要开发更高效的替代模型用于空间天气预报。

Method: 采用球面傅里叶神经算子(SFNO)构建太阳风稳态建模的替代模型，并与先前开发的数值替代模型HUX进行比较。

Result: SFNO在多个性能指标上达到与HUX相当或更好的表现，虽然HUX在物理平滑性方面仍有优势，但SFNO展示了作为灵活可训练方法的潜力。

Conclusion: SFNO为太阳风建模提供了高效、可实时预报的替代方案，能够随着数据增加而改进性能，但需要改进评估标准来更好地衡量此类模型的物理特性。

Abstract: The solar wind, a continuous stream of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Variations such as high-speed streams and coronal mass ejections can disrupt satellites, power grids, and communications, making accurate modeling essential for space weather forecasting. While 3D magnetohydrodynamic (MHD) models are used to simulate and investigate these variations in the solar wind, they tend to be computationally expensive, limiting their usefulness in investigating the impacts of boundary condition uncertainty. In this work, we develop a surrogate for steady state solar wind modeling, using a Spherical Fourier Neural Operator (SFNO). We compare our model to a previously developed numerical surrogate for this task called HUX, and we show that the SFNO achieves comparable or better performance across several metrics. Though HUX retains advantages in physical smoothness, this underscores the need for improved evaluation criteria rather than a flaw in SFNO. As a flexible and trainable approach, SFNO enables efficient real-time forecasting and can improve with more data. The source code and more visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity.

</details>


### [112] [IVGAE: Handling Incomplete Heterogeneous Data with a Variational Graph Autoencoder](https://arxiv.org/abs/2511.22116)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal%*

Main category: cs.LG

TL;DR: IVGAE是一个基于变分图自编码器的框架，用于处理异构数据（数值和分类特征）的缺失值填补，通过构建样本-特征二分图和使用双解码器架构来建模结构依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的表格数据集经常存在缺失数据，特别是当数据包含数值和分类特征的异构数据时。现有的填补方法往往难以捕捉复杂的结构依赖关系，也无法有效处理异构数据。

Method: IVGAE构建样本-特征二分图来表示样本与特征之间的关系，应用图表示学习来建模结构依赖。核心创新是双解码器架构：一个解码器重建特征嵌入，另一个建模缺失模式，提供对缺失机制有认知的结构先验。为了更好编码分类变量，引入了基于Transformer的异构嵌入模块，避免高维one-hot编码。

Result: 在16个真实世界数据集上的广泛实验表明，IVGAE在MCAR、MAR和MNAR三种缺失机制下，在30%缺失率的情况下，在RMSE和下游F1分数上都取得了持续改进。

Conclusion: IVGAE通过图表示学习和双解码器架构，为异构数据的缺失值填补提供了一个有效的解决方案，能够捕捉复杂的数据结构依赖关系并处理不同类型的缺失机制。

Abstract: Handling missing data remains a fundamental challenge in real-world tabular datasets, especially when data are heterogeneous with both numerical and categorical features. Existing imputation methods often fail to capture complex structural dependencies and handle heterogeneous data effectively. We present \textbf{IVGAE}, a Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data. IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning to model structural dependencies. A key innovation is its \textit{dual-decoder architecture}, where one decoder reconstructs feature embeddings and the other models missingness patterns, providing structural priors aware of missing mechanisms. To better encode categorical variables, we introduce a Transformer-based heterogeneous embedding module that avoids high-dimensional one-hot encoding. Extensive experiments on 16 real-world datasets show that IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30\% missing rates. Code and data are available at: https://github.com/echoid/IVGAE.

</details>


### [113] [A Variational Manifold Embedding Framework for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2511.22128)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 提出一个变分框架，将降维算法视为最优流形嵌入问题的解，该框架允许非线性嵌入，比PCA更灵活，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法各有局限：PCA及其变体简单可解释但无法捕捉非线性流形结构；自编码器灵活但难以解释；基于图嵌入的方法可能产生病态几何扭曲。需要一种既灵活又可解释的降维方法。

Method: 提出变分框架，将降维算法形式化为最优流形嵌入问题的解。该框架通过构造允许非线性嵌入，其解满足一组偏微分方程，并能反映嵌入目标的对称性。

Result: 框架具有有用的可解释性特征：每个解满足偏微分方程，反映嵌入目标的对称性，某些情况下可以解析表征。有趣的是，一个特例精确恢复了PCA。

Conclusion: 该变分框架为降维提供了统一视角，既保持了PCA的可解释性优点，又允许非线性嵌入的灵活性，解决了现有方法的局限性。

Abstract: Dimensionality reduction algorithms like principal component analysis (PCA) are workhorses of machine learning and neuroscience, but each has well-known limitations. Variants of PCA are simple and interpretable, but not flexible enough to capture nonlinear data manifold structure. More flexible approaches have other problems: autoencoders are generally difficult to interpret, and graph-embedding-based methods can produce pathological distortions in manifold geometry. Motivated by these shortcomings, we propose a variational framework that casts dimensionality reduction algorithms as solutions to an optimal manifold embedding problem. By construction, this framework permits nonlinear embeddings, allowing its solutions to be more flexible than PCA. Moreover, the variational nature of the framework has useful consequences for interpretability: each solution satisfies a set of partial differential equations, and can be shown to reflect symmetries of the embedding objective. We discuss these features in detail and show that solutions can be analytically characterized in some cases. Interestingly, one special case exactly recovers PCA.

</details>


### [114] [Benchmarking In-context Experiential Learning Through Repeated Product Recommendations](https://arxiv.org/abs/2511.22130)
*Gilbert Yang,Yaqin Chen,Thomson Yen,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 论文提出了BELA基准，用于评估智能体在动态环境中通过经验学习和主动探索的能力，发现当前前沿模型难以在对话中有效改进推荐策略。


<details>
  <summary>Details</summary>
Motivation: 当前评估主要关注确定性任务，缺乏对智能体在不确定环境中通过累积经验进行自适应学习和推理能力的测量。现实世界如产品推荐场景中，智能体需要应对不断变化的客户偏好和产品环境。

Method: 构建BELA基准，包含：(1)亚马逊真实产品数据，(2)代表异质潜在偏好的多样化用户角色，(3)基于角色的LLM用户模拟器，用于生成丰富的交互轨迹。

Result: 当前前沿模型在跨多轮对话中难以实现有意义的改进，表明需要具备强大上下文学习能力的智能体系统。

Conclusion: 需要开发能够通过经验学习和主动探索来适应动态环境的智能体系统，BELA基准为此类能力评估提供了重要工具。

Abstract: To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.

</details>


### [115] [Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks](https://arxiv.org/abs/2511.22133)
*Sahil Kashyap,Rajdip Nayek*

Main category: cs.LG

TL;DR: 提出概率数字孪生框架，用于处理物理模型错误下的动态系统响应预测，结合高斯过程潜在力模型和贝叶斯神经网络实现端到端不确定性感知推理


<details>
  <summary>Details</summary>
Motivation: 针对物理模型错误（model-form errors）的动态系统，传统方法难以准确预测响应，需要能够系统传播诊断到预测不确定性的可信数字孪生框架

Method: 1) 诊断阶段：将模型形式误差视为线性动态系统的潜在输入力，使用高斯过程潜在力模型从传感器测量中联合估计系统状态和误差；2) 训练贝叶斯神经网络学习从系统状态到模型形式误差的概率非线性映射；3) 预测阶段：利用该映射生成伪测量，通过卡尔曼滤波进行状态预测

Result: 在四个非线性示例中验证了框架的有效性：单自由度振荡器、多自由度系统、Bouc-Wen迟滞系统和Silverbox实验数据集，展示了预测准确性和对模型错误的鲁棒性

Conclusion: 该框架能够系统地从诊断传播不确定性到预测，是实现可信数字孪生的关键能力，特别适用于物理模型错误的动态系统响应预测

Abstract: This work presents a probabilistic digital twin framework for response prediction in dynamical systems governed by misspecified physics. The approach integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to enable end-to-end uncertainty-aware inference and prediction. In the diagnosis phase, model-form errors (MFEs) are treated as latent input forces to a nominal linear dynamical system and jointly estimated with system states using GPLFM from sensor measurements. A BNN is then trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, while capturing diagnostic uncertainty. For prognosis, this mapping is used to generate pseudo-measurements, enabling state prediction via Kalman filtering. The framework allows for systematic propagation of uncertainty from diagnosis to prediction, a key capability for trustworthy digital twins. The framework is demonstrated using four nonlinear examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, and two established benchmarks -- the Bouc-Wen hysteretic system and the Silverbox experimental dataset -- highlighting its predictive accuracy and robustness to model misspecification.

</details>


### [116] [TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices](https://arxiv.org/abs/2511.22138)
*Mohd Ariful Haque,Fahad Rahman,Kishor Datta Gupta,Khalil Shujaee,Roy George*

Main category: cs.LG

TL;DR: 本文研究了小型语言模型在边缘设备上执行代理任务的有效性，通过多种优化策略在BFCL基准上评估，发现中等规模模型（1-3B参数）性能显著优于超紧凑模型，最高达到65.74%整体准确率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索小型语言模型在边缘设备上执行代理任务（函数/工具/API调用）的能力，实现在不依赖云基础设施的情况下运行隐私保护、低延迟的自主代理。

Method: 使用伯克利函数调用排行榜框架评估SLMs，采用参数驱动的优化策略：监督微调、参数高效微调、强化学习优化、直接偏好优化的偏好对齐以及混合方法。构建了基于AgentBank数据的DPO训练流程。

Result: 结果显示模型规模存在明显准确率差异：中等规模模型（1-3B参数）显著优于超紧凑模型（<1B参数），混合优化下整体准确率达65.74%，多轮对话准确率达55.62%。

Conclusion: 研究表明混合优化策略对小型语言模型在边缘设备上实现准确、高效、稳定的代理AI至关重要，使隐私保护、低延迟的自主代理在云端之外变得实用可行。

Abstract: This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.

</details>


### [117] [From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures](https://arxiv.org/abs/2511.22150)
*Florian Rottach,William Rudman,Bastain Rieck,Harrisen Scells,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 本文提出统一拓扑签名(UTS)框架，通过综合分析文本嵌入空间的拓扑几何特征来预测模型特性和检索性能。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入在空间中的组织方式不仅能增强模型可解释性，还能揭示影响下游任务性能的因素。现有度量指标存在冗余且单个指标往往无法充分区分不同的嵌入空间。

Method: 提出统一拓扑签名(UTS)框架，对多种文本嵌入模型和数据集进行拓扑几何度量的综合分析，通过多属性视角来表征嵌入空间。

Result: UTS能够预测模型特定属性、揭示模型架构驱动的相似性，将拓扑结构与排名效果联系起来，并准确预测文档可检索性。

Conclusion: 理解并利用文本嵌入的几何结构需要采用整体的多属性视角，UTS框架为此提供了有效的分析方法。

Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.

</details>


### [118] [Designing Instance-Level Sampling Schedules via REINFORCE with James-Stein Shrinkage](https://arxiv.org/abs/2511.22177)
*Peiyu Yu,Suraj Kothawade,Sirui Xie,Ying Nian Wu,Hongliang Fei*

Main category: cs.LG

TL;DR: 提出一种通过重新调度采样时间线来提升文本到图像生成器性能的方法，而不是修改模型权重。使用单次Dirichlet策略学习实例级（提示和噪声条件）调度，并引入基于James-Stein估计器的新奖励基线来降低梯度估计误差。


<details>
  <summary>Details</summary>
Motivation: 大多数后训练方法专注于模型权重的微调或蒸馏，但本文采取不同路线：通过重新调度冻结采样器的采样时间线来提升性能，这是一种模型无关的后训练杠杆。

Method: 1) 学习实例级（提示和噪声条件）调度而非固定全局调度；2) 使用单次Dirichlet策略；3) 引入基于James-Stein估计器的新奖励基线来确保高维策略学习中的准确梯度估计。

Result: 1) 在Stable Diffusion和Flux模型家族中一致提升文本图像对齐，包括文本渲染和组合控制；2) 5步Flux-Dev采样器使用该调度能达到与精心蒸馏的采样器（如Flux-Schnell）相当的生成质量。

Conclusion: 该调度框架作为一种新兴的模型无关后训练杠杆，能够释放预训练采样器中额外的生成潜力，为文本到图像生成提供了一种新的优化途径。

Abstract: Most post-training methods for text-to-image samplers focus on model weights: either fine-tuning the backbone for alignment or distilling it for few-step efficiency. We take a different route: rescheduling the sampling timeline of a frozen sampler. Instead of a fixed, global schedule, we learn instance-level (prompt- and noise-conditioned) schedules through a single-pass Dirichlet policy. To ensure accurate gradient estimates in high-dimensional policy learning, we introduce a novel reward baseline based on a principled James-Stein estimator; it provably achieves lower estimation errors than commonly used variants and leads to superior performance. Our rescheduled samplers consistently improve text-image alignment including text rendering and compositional control across modern Stable Diffusion and Flux model families. Additionally, a 5-step Flux-Dev sampler with our schedules can attain generation quality comparable to deliberately distilled samplers like Flux-Schnell. We thus position our scheduling framework as an emerging model-agnostic post-training lever that unlocks additional generative potential in pretrained samplers.

</details>


### [119] [PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units](https://arxiv.org/abs/2511.22199)
*Sejeong Jang,Joo Heung Yoon,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: PULSE-ICU是一个自监督基础模型，直接从大规模ICU电子健康记录序列中学习事件级表示，无需重采样或手动特征工程，在18个预测任务中表现优异，并在外部验证中显示出强大的领域适应能力。


<details>
  <summary>Details</summary>
Motivation: ICU数据具有高度不规则、异质性和时间碎片化的特点，这给临床预测的泛化性带来了挑战。现有方法通常需要重采样或手动特征工程，限制了模型的适应性和可扩展性。

Method: 开发了PULSE-ICU自监督基础模型，包含统一的嵌入模块（编码事件身份、连续值、单位和时间属性）和基于Longformer的编码器，能够高效建模长轨迹。模型直接从大规模EHR序列学习，无需重采样或手动特征工程。

Result: 在18个预测任务（包括死亡率、干预预测和表型识别）上微调后表现优异。在eICU、HiRID和P12数据集上的外部验证显示，只需最小微调即可获得显著改进，证明模型对领域偏移和变量约束具有鲁棒性。

Conclusion: 基础模型风格的方法可以提高数据效率和适应性，为不同临床环境中的ICU决策支持提供可扩展框架。PULSE-ICU展示了自监督学习在复杂医疗时序数据处理中的潜力。

Abstract: Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.

</details>


### [120] [BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning](https://arxiv.org/abs/2511.22210)
*Junsung Park*

Main category: cs.LG

TL;DR: BiCQL-ML：一种免策略的离线逆强化学习算法，通过双层框架联合优化奖励函数和保守Q函数，避免显式策略学习，在标准基准测试中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 离线逆强化学习（IRL）仅使用固定的演示数据恢复解释专家行为的奖励函数，无需在线交互。现有方法通常需要显式策略学习，BiCQL-ML旨在通过免策略方法解决这一问题。

Method: 提出BiCQL-ML算法，采用双层框架：1）在当前奖励下通过保守Q学习（CQL）学习保守Q函数；2）更新奖励参数以最大化专家动作的期望Q值，同时抑制对分布外动作的过度泛化。该方法可视为基于软值匹配原则的最大似然估计。

Result: 理论保证：BiCQL-ML收敛到使专家策略达到软最优的奖励函数。实证结果：在标准离线RL基准测试中，BiCQL-ML在奖励恢复和下游策略性能方面均优于现有离线IRL基线方法。

Conclusion: BiCQL-ML是一种有效的免策略离线IRL算法，通过联合优化奖励和Q函数避免了显式策略学习，在理论和实证上都表现出优越性能。

Abstract: Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

</details>


### [121] [FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2511.22265)
*Yuan Yao,Lixu Wang,Jiaqi Wu,Jin Song,Simin Chen,Zehua Wang,Zijian Tian,Wei Chen,Huixia Li,Xiaoxiao Li*

Main category: cs.LG

TL;DR: FedRE提出了一种基于纠缠表示的联邦学习框架，通过随机权重聚合本地表示和标签编码，解决异构模型架构下的联邦学习问题，平衡性能、隐私和通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法大多假设同构模型架构，但实际中客户端在数据和资源方面存在异构性，这使得同构假设不切实际，因此需要模型异构的联邦学习方法。

Method: 提出联邦表示纠缠(FedRE)框架，使用归一化随机权重将本地表示聚合成纠缠表示，并用相同权重将one-hot标签编码聚合成纠缠标签编码，上传到服务器训练全局分类器。每轮重新采样随机权重以增加多样性。

Result: 实验表明FedRE在模型性能、隐私保护和通信开销之间实现了有效权衡。上传单个跨类别纠缠表示和纠缠标签编码减少了通信开销，同时降低了表示反转攻击的风险。

Conclusion: FedRE通过纠缠表示机制有效解决了模型异构联邦学习问题，在保持隐私保护的同时实现了良好的性能，并显著降低了通信成本。

Abstract: Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.

</details>


### [122] [TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation](https://arxiv.org/abs/2511.22277)
*Henrijs Princis,Arindam Sharma,Cristina David*

Main category: cs.LG

TL;DR: TreeCoder是一个用于代码生成的解码框架，通过树搜索和约束函数确保LLM输出符合语法和语义规则，显著提升代码生成准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成代码时，仅通过自然语言提示往往会产生违反语法或语义约束的输出，需要更有效的解码方法来确保代码正确性。

Method: TreeCoder将解码过程表示为候选程序的树搜索，将解码策略和约束函数（如风格、语法、执行）作为可优化的组件，支持系统化探索和自动调优。

Result: 在MBPP（Python）和SQL-Spider基准测试中，TreeCoder显著提升了CodeLlama、Mistral和DeepSeek等开源模型的准确性，通常以较大优势超越无约束基线。

Conclusion: TreeCoder提供了一个通用灵活的框架，通过在解码过程中强制执行正确性和结构约束，而非依赖提示工程，有效提升了LLM代码生成的质量和可靠性。

Abstract: Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.

</details>


### [123] [The Hidden Cost of Approximation in Online Mirror Descent](https://arxiv.org/abs/2511.22283)
*Ofir Schlisselberg,Uri Sherman,Tomer Koren,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文系统研究了在线镜像下降(OMD)的近似版本，揭示了正则化器平滑性与近似误差鲁棒性之间的复杂关系，发现不同正则化器对误差的容忍度存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 在线镜像下降(OMD)是优化、机器学习和序列决策中许多算法的基础范式。实际应用中，OMD子问题通常只能近似求解，导致算法的不精确版本。然而，现有OMD分析通常假设理想的无误差设置，限制了我们对实际性能保证的理解。

Method: 系统研究不精确OMD，分析正则化器平滑性与近似误差鲁棒性之间的关系。特别关注单纯形及其子集上的屏障正则化器，包括负熵、log-barrier和Tsallis正则化器。

Result: 1) 当正则化器均匀平滑时，建立了误差导致的超额遗憾的紧界；2) 对于单纯形上的屏障正则化器，发现负熵需要指数级小误差才能避免线性遗憾，而log-barrier和Tsallis正则化器即使在多项式误差下仍保持鲁棒；3) 当损失是随机且域为单纯形时，负熵重新获得鲁棒性，但这一性质不扩展到所有子集。

Conclusion: 正则化器的选择对不精确OMD的鲁棒性至关重要。不同正则化器对近似误差的容忍度存在显著差异，这在实际算法设计中需要仔细考虑。研究为理解实际OMD实现中的性能保证提供了系统框架。

Abstract: Online mirror descent (OMD) is a fundamental algorithmic paradigm that underlies many algorithms in optimization, machine learning and sequential decision-making. The OMD iterates are defined as solutions to optimization subproblems which, oftentimes, can be solved only approximately, leading to an inexact version of the algorithm. Nonetheless, existing OMD analyses typically assume an idealized error free setting, thereby limiting our understanding of performance guarantees that should be expected in practice. In this work we initiate a systematic study into inexact OMD, and uncover an intricate relation between regularizer smoothness and robustness to approximation errors. When the regularizer is uniformly smooth, we establish a tight bound on the excess regret due to errors. Then, for barrier regularizers over the simplex and its subsets, we identify a sharp separation: negative entropy requires exponentially small errors to avoid linear regret, whereas log-barrier and Tsallis regularizers remain robust even when the errors are only polynomial. Finally, we show that when the losses are stochastic and the domain is the simplex, negative entropy regains robustness-but this property does not extend to all subsets, where exponentially small errors are again necessary to avoid suboptimal regret.

</details>


### [124] [Online Dynamic Pricing of Complementary Products](https://arxiv.org/abs/2511.22291)
*Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文提出一种针对互补产品的动态定价算法，通过考虑产品间的需求交互关系来最大化整体收入。


<details>
  <summary>Details</summary>
Motivation: 传统定价模型多为静态或基于规则，现代机器学习算法虽然使定价更动态化，但大多独立优化单个产品价格，忽略了产品间的需求交互关系。这种忽略可能导致卖家无法充分利用互补产品的协同定价潜力。

Method: 提出在线学习算法，考虑产品需求间的正负交互作用。算法利用交易数据通过整数规划问题识别有利的互补关系，然后基于异方差高斯过程的数据驱动、计算高效的多臂老虎机解决方案来优化定价策略。

Result: 在模拟环境中验证了解决方案，证明该算法相比忽略产品交互关系的可比学习算法能够提高收入。

Conclusion: 针对互补产品的动态定价算法能够有效利用产品间的需求交互关系，通过协调定价策略实现更高的整体收入，为数据驱动的定价优化提供了新方向。

Abstract: Traditional pricing paradigms, once dominated by static models and rule-based heuristics, are increasingly being replaced by dynamic, data-driven approaches powered by machine learning algorithms. Despite their growing sophistication, most dynamic pricing algorithms focus on optimizing the price of each product independently, disregarding potential interactions among items. By neglecting these interdependencies in consumer demand across related goods, sellers may fail to capture the full potential of coordinated pricing strategies. In this paper, we address this problem by exploring dynamic pricing mechanisms designed explicitly for complementary products, aiming to exploit their joint demand structure to maximize overall revenue. We present an online learning algorithm considering both positive and negative interactions between products' demands. The algorithm utilizes transaction data to identify advantageous complementary relationships through an integer programming problem between different items, and then optimizes pricing strategies using data-driven and computationally efficient multi-armed bandit solutions based on heteroscedastic Gaussian processes. We validate our solution in a simulated environment, and we demonstrate that our solution improves the revenue w.r.t. a comparable learning algorithm ignoring such interactions.

</details>


### [125] [Adaptive tumor growth forecasting via neural & universal ODEs](https://arxiv.org/abs/2511.22292)
*Kavya Subramanian,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该研究利用神经ODE和通用微分方程构建自适应肿瘤生长模型，通过替换Gompertz模型中的刚性项为神经网络，在Julia中实现数据驱动预测和符号恢复。


<details>
  <summary>Details</summary>
Motivation: 传统肿瘤生长模型（如Gompertz和Bertalanffy方程）能捕捉一般动态，但难以适应患者特异性变异，特别是在数据有限的情况下。需要更灵活的方法来改进预测准确性，指导个性化治疗策略。

Method: 采用科学机器学习方法，使用神经ODE和通用微分方程，以Gompertz模型为基础，用自适应神经网络替换刚性项，在Julia编程语言中实现。进行数据约束下的预测和符号恢复，将学习到的动态转化为显式数学表达式。

Result: 该方法能够从实验数据中学习隐藏动态，在数据有限条件下进行预测，并通过符号恢复获得可解释的数学模型，有望提高预测准确性。

Conclusion: 基于神经ODE和通用微分方程的自适应肿瘤生长模型有潜力改善预测准确性，为动态有效的治疗策略提供指导，从而改善临床结果。

Abstract: Forecasting tumor growth is critical for optimizing treatment. Classical growth models such as the Gompertz and Bertalanffy equations capture general tumor dynamics but may fail to adapt to patient-specific variability, particularly with limited data available. In this study, we leverage Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs), two pillars of Scientific Machine Learning (SciML), to construct adaptive tumor growth models capable of learning from experimental data. Using the Gompertz model as a baseline, we replace rigid terms with adaptive neural networks to capture hidden dynamics through robust modeling in the Julia programming language. We use our models to perform forecasting under data constraints and symbolic recovery to transform the learned dynamics into explicit mathematical expressions. Our approach has the potential to improve predictive accuracy, guiding dynamic and effective treatment strategies for improved clinical outcomes.

</details>


### [126] [FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts](https://arxiv.org/abs/2511.22305)
*Dario Fenoglio,Mohan Li,Pietro Barbiero,Nicholas D. Lane,Marc Langheinrich,Martin Gjoreski*

Main category: cs.LG

TL;DR: FLUX是一个基于聚类的联邦学习框架，通过隐私保护的客户端描述符提取和无监督聚类，解决训练和测试时的四种常见分布偏移问题，无需先验知识且支持测试时适应。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法假设客户端数据独立同分布(IID)，但在现实场景中这一假设往往不成立，导致全局模型性能显著下降。现有方法需要先验知识且无法处理测试时的分布偏移，限制了联邦学习的实际应用。

Method: 提出FLUX框架：1）使用隐私保护的客户端描述符提取技术；2）采用无监督聚类方法自动识别客户端集群；3）支持测试时适应，让未见过的未标记客户端也能受益于最合适的集群特定模型；4）无需分布偏移类型或集群数量的先验知识。

Result: 在四个标准基准测试、两个真实数据集和十个最先进基线方法上的实验表明，FLUX在不同分布偏移下显著提升性能和稳定性，平均准确率比最佳基线提高高达23个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: FLUX通过创新的聚类方法有效解决了联邦学习中的非IID分布偏移问题，无需先验知识且支持测试时适应，在保持隐私保护的同时显著提升了模型性能，为现实世界联邦学习应用提供了实用解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients while preserving data privacy. Traditional FL methods often use a global model to fit all clients, assuming that clients' data are independent and identically distributed (IID). However, when this assumption does not hold, the global model accuracy may drop significantly, limiting FL applicability in real-world scenarios. To address this gap, we propose FLUX, a novel clustering-based FL (CFL) framework that addresses the four most common types of distribution shifts during both training and test time. To this end, FLUX leverages privacy-preserving client-side descriptor extraction and unsupervised clustering to ensure robust performance and scalability across varying levels and types of distribution shifts. Unlike existing CFL methods addressing non-IID client distribution shifts, FLUX i) does not require any prior knowledge of the types of distribution shifts or the number of client clusters, and ii) supports test-time adaptation, enabling unseen and unlabeled clients to benefit from the most suitable cluster-specific models. Extensive experiments across four standard benchmarks, two real-world datasets and ten state-of-the-art baselines show that FLUX improves performance and stability under diverse distribution shifts, achieving an average accuracy gain of up to 23 percentage points over the best-performing baselines, while maintaining computational and communication overhead comparable to FedAvg.

</details>


### [127] [DeXposure: A Dataset and Benchmarks for Inter-protocol Credit Exposure in Decentralized Financial Networks](https://arxiv.org/abs/2511.22314)
*Wenbin Wu,Kejiang Qian,Alexis Lui,Christopher Jack,Yue Wu,Peter McBurney,Fengxiang He,Bryan Zhang*

Main category: cs.LG

TL;DR: DeXposure：首个大规模去中心化金融网络跨协议信用风险暴露数据集，包含4370万条目，涵盖4300个协议、602条区块链和24300种代币，提供三个机器学习基准任务。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模的去中心化金融（DeFi）跨协议信用风险暴露数据集，难以进行系统性风险分析和机器学习研究。需要构建一个覆盖全球市场、多协议、多区块链的信用风险暴露数据集来支持金融风险监控和政策分析。

Method: 1. 基于DefiLlama元数据构建代币到协议的模型，从代币存量动态推断跨协议信用风险暴露；2. 定义基于总锁定价值（TVL）变化的价值关联信用风险暴露指标；3. 开发三个机器学习基准：图聚类用于网络结构演化跟踪，向量自回归用于冲击传播分析，时序图神经网络用于动态链接预测。

Result: 1. 网络规模快速增长；2. 风险向关键协议集中；3. 网络密度下降（实际连接与可能连接的比例）；4. 不同部门（借贷平台、交易交易所、资产管理协议）的冲击传播模式不同。数据集和代码已公开。

Conclusion: DeXposure数据集填补了DeFi跨协议信用风险暴露数据空白，为机器学习和金融风险研究提供了重要资源。数据集支持图聚类、向量自回归和时序图分析等基准任务，有助于DeFi市场建模、风险监控和政策分析。

Abstract: We curate the DeXposure dataset, the first large-scale dataset for inter-protocol credit exposure in decentralized financial networks, covering global markets of 43.7 million entries across 4.3 thousand protocols, 602 blockchains, and 24.3 thousand tokens, from 2020 to 2025. A new measure, value-linked credit exposure between protocols, is defined as the inferred financial dependency relationships derived from changes in Total Value Locked (TVL). We develop a token-to-protocol model using DefiLlama metadata to infer inter-protocol credit exposure from the token's stock dynamics, as reported by the protocols. Based on the curated dataset, we develop three benchmarks for machine learning research with financial applications: (1) graph clustering for global network measurement, tracking the structural evolution of credit exposure networks, (2) vector autoregression for sector-level credit exposure dynamics during major shocks (Terra and FTX), and (3) temporal graph neural networks for dynamic link prediction on temporal graphs. From the analysis, we observe (1) a rapid growth of network volume, (2) a trend of concentration to key protocols, (3) a decline of network density (the ratio of actual connections to possible connections), and (4) distinct shock propagation across sectors, such as lending platforms, trading exchanges, and asset management protocols. The DeXposure dataset and code have been released publicly. We envision they will help with research and practice in machine learning as well as financial risk monitoring, policy analysis, DeFi market modeling, amongst others. The dataset also contributes to machine learning research by offering benchmarks for graph clustering, vector autoregression, and temporal graph analysis.

</details>


### [128] [SingleQuant: Efficient Quantization of Large Language Models in a Single Pass](https://arxiv.org/abs/2511.22316)
*Jinying Xiao,Bin Ji,Shasha Li,Xiaodong Liu,Ma Jun,Ye Zhong,Wei Li,Xuan Xie,Qingbo Wu,Jie Yu*

Main category: cs.LG

TL;DR: SingleQuant是一种单次量化框架，通过解耦量化截断，消除梯度噪声和非平滑性，实现快速高效的LLM量化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM量化方法结合不兼容的梯度优化和量化截断，导致严重的收敛病理问题，延长量化时间并降低模型性能。STE在Stiefel流形上引入非平滑性和梯度噪声，阻碍优化收敛。

Method: 提出SingleQuant单次量化框架，构造对齐旋转变换(ART)和均匀旋转变换(URT)处理激活异常值。ART通过闭式最优旋转平滑异常值，URT通过几何映射重塑分布。两种矩阵都使用严格公式化的Givens旋转。

Result: 在7B-70B LLM的多样化任务上优于基线方法。量化LLaMA-2-13B时实现1400倍量化加速，平均任务性能提升+0.57%。

Conclusion: SingleQuant通过消除梯度噪声和非平滑性，能够在短时间内实现高性能的LLM量化，显著提升量化效率和模型性能。

Abstract: Large Language Models (LLMs) quantization facilitates deploying LLMs in resource-limited settings, but existing methods that combine incompatible gradient optimization and quantization truncation lead to serious convergence pathology. This prolongs quantization time and degrades LLMs' task performance. Our studies confirm that Straight-Through Estimator (STE) on Stiefel manifolds introduce non-smoothness and gradient noise, obstructing optimization convergence and blocking high-fidelity quantized LLM development despite extensive training. To tackle the above limitations, we propose SingleQuant, a single-pass quantization framework that decouples from quantization truncation, thereby eliminating the above non-smoothness and gradient noise factors. Specifically, SingleQuant constructs Alignment Rotation Transformation (ART) and Uniformity Rotation Transformation (URT) targeting distinct activation outliers, where ART achieves smoothing of outlier values via closed-form optimal rotations, and URT reshapes distributions through geometric mapping. Both matrices comprise strictly formulated Givens rotations with predetermined dimensions and rotation angles, enabling promising LLMs task performance within a short time. Experimental results demonstrate SingleQuant's superiority over the selected baselines across diverse tasks on 7B-70B LLMs. To be more precise, SingleQuant enables quantized LLMs to achieve higher task performance while necessitating less time for quantization. For example, when quantizing LLaMA-2-13B, SingleQuant achieves 1,400$\times$ quantization speedup and increases +0.57\% average task performance compared to the selected best baseline.

</details>


### [129] [Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement](https://arxiv.org/abs/2511.22343)
*Panteleimon Dogoulis,Mohammad Iman Alizadeh,Sylvain Kubler,Maxime Cordy*

Main category: cs.LG

TL;DR: 提出PI-TTT框架，通过推理时的自监督微调，在保持机器学习计算优势的同时，显著提升潮流计算的物理一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的潮流计算方法相比传统数值方法具有计算优势，但往往难以保持完全的物理一致性。需要一种方法在保持计算效率的同时，确保物理约束的满足。

Method: 提出物理信息推理时训练（PI-TTT）框架，在推理时通过少量梯度更新对机器学习代理模型输出进行轻量级自监督微调，直接强制执行交流潮流等式和运行约束。

Result: 在IEEE 14、118、300节点系统和PEGASE 1354节点网络上的实验表明，PI-TTT将潮流残差和运行约束违反降低1-2个数量级，同时保持计算优势。

Conclusion: PI-TTT提供了快速、准确且物理可靠的预测，代表了电力系统分析中可扩展且物理一致学习的有前景方向。

Abstract: Power Flow (PF) calculation based on machine learning (ML) techniques offer significant computational advantages over traditional numerical methods but often struggle to maintain full physical consistency. This paper introduces a physics-informed test-time training (PI-TTT) framework that enhances the accuracy and feasibility of ML-based PF surrogates by enforcing AC power flow equalities and operational constraints directly at inference time. The proposed method performs a lightweight self-supervised refinement of the surrogate outputs through few gradient-based updates, enabling local adaptation to unseen operating conditions without requiring labeled data. Extensive experiments on the IEEE 14-, 118-, and 300-bus systems and the PEGASE 1354-bus network show that PI-TTT reduces power flow residuals and operational constraint violations by one to two orders of magnitude compared with purely ML-based models, while preserving their computational advantage. The results demonstrate that PI-TTT provides fast, accurate, and physically reliable predictions, representing a promising direction for scalable and physics-consistent learning in power system analysis.

</details>


### [130] [Cleaning the Pool: Progressive Filtering of Unlabeled Pools in Deep Active Learning](https://arxiv.org/abs/2511.22344)
*Denis Huseljic,Marek Herde,Lukas Rauch,Paul Hahn,Bernhard Sick*

Main category: cs.LG

TL;DR: REFINE是一个集成主动学习方法，通过两阶段流程（渐进过滤和覆盖选择）结合多种AL策略，无需预先知道哪种策略最优，在各种数据集和模型上表现优于单一策略和现有集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习策略（如不确定性、代表性）在不同数据集、模型和AL周期中效果差异很大。单一策略可能在整个AL过程中表现不佳，因为没有任何一种策略始终占优。

Method: REFINE采用两阶段方法：1）渐进过滤：考虑AL策略集成，迭代细化未标注池，保留捕捉不同价值概念的有希望候选样本；2）覆盖选择：从细化池中选择最终批次，确保考虑所有先前识别的价值概念。

Result: 在6个分类数据集和3个基础模型上的实验表明，REFINE始终优于单一策略和现有集成方法。渐进过滤作为强大的预处理步骤，能提升任何应用于细化池的个体AL策略性能。

Conclusion: REFINE提供了一种有效集成多种主动学习策略的方法，无需预先知道哪种策略最优，且其集成框架可轻松扩展以纳入新的先进AL策略。

Abstract: Existing active learning (AL) strategies capture fundamentally different notions of data value, e.g., uncertainty or representativeness. Consequently, the effectiveness of strategies can vary substantially across datasets, models, and even AL cycles. Committing to a single strategy risks suboptimal performance, as no single strategy dominates throughout the entire AL process. We introduce REFINE, an ensemble AL method that combines multiple strategies without knowing in advance which will perform best. In each AL cycle, REFINE operates in two stages: (1) Progressive filtering iteratively refines the unlabeled pool by considering an ensemble of AL strategies, retaining promising candidates capturing different notions of value. (2) Coverage-based selection then chooses a final batch from this refined pool, ensuring all previously identified notions of value are accounted for. Extensive experiments across 6 classification datasets and 3 foundation models show that REFINE consistently outperforms individual strategies and existing ensemble methods. Notably, progressive filtering serves as a powerful preprocessing step that improves the performance of any individual AL strategy applied to the refined pool, which we demonstrate on an audio spectrogram classification use case. Finally, the ensemble of REFINE can be easily extended with upcoming state-of-the-art AL strategies.

</details>


### [131] [AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices](https://arxiv.org/abs/2511.22355)
*Mengyang Liu,Chenyu Lu,Haodong Tian,Fang Dong,Ruiting Zhou,Wei Wang,Dian Shen,Guangtong Li,Ye Wan,Li Li*

Main category: cs.LG

TL;DR: AutoTailor：首个自动化端到端SuperNet自适应模型部署框架，通过计算图引导编译自动构建SuperNet，使用无学习性能预测器，显著降低开发成本和提升部署效率。


<details>
  <summary>Details</summary>
Motivation: 当前SuperNet方法在实际应用中面临两大挑战：1）需要繁琐的模型感知开发（手动构建SuperNet），2）耗时的硬件感知性能分析，这限制了其在边缘设备上的实际采用。

Method: 1）采用计算图引导的编译方法，自动将用户提供的ML模型转换为SuperNet；2）集成无学习的延迟和精度预测器，实现低成本而准确的性能预测。

Result: 1）SuperNet构建代码量减少11-27倍；2）硬件感知分析成本降低至少11倍；3）相比最先进方法，精度提升最高15.60%，延迟降低最高60.03%（跨多种模型和设备）。

Conclusion: AutoTailor通过自动化SuperNet构建和高效性能预测，解决了现有框架的局限性，为边缘设备自适应模型部署提供了实用解决方案。

Abstract: On-device machine learning (ML) has become a fundamental component of emerging mobile applications. Adaptive model deployment delivers efficient inference for heterogeneous device capabilities and performance requirements through customizing neural architectures. SuperNet-based approaches offer a promising solution by generating a large number of model variants from a pre-trained ML model. However, applying SuperNet in existing frameworks suffers from tedious model-aware development and time-consuming hardware-aware profiling, which limits their practical adoption.
  We present AutoTailor, the first framework to enable automated, end-to-end SuperNet-based adaptive model deployment for edge devices. Unlike manual SuperNet construction, AutoTailor employs a computation graph-guided compilation approach to automatically transform user-provided ML models into SuperNets. To support efficient specialization, AutoTailor incorporates learning-free latency and accuracy predictors, enabling low-cost yet accurate performance prediction. Our extended evaluations demonstrate that AutoTailor reduces the lines of code for SuperNet construction by 11--27$\times$, decreases hardware-aware profiling costs by at least 11$\times$, and achieves up to 15.60\% absolute accuracy improvement and 60.03\% latency reduction compared to state-of-the-art approaches across diverse models and devices.

</details>


### [132] [Efficient-Husformer: Efficient Multimodal Transformer Hyperparameter Optimization for Stress and Cognitive Loads](https://arxiv.org/abs/2511.22362)
*Merey Orazaly,Fariza Temirkhanova,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 提出Efficient-Husformer，一种通过超参数优化设计的Transformer架构，用于多类压力检测，在WESAD和CogLoad数据集上实现性能提升，模型参数仅约30k。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在生理信号分析中表现出色，但计算强度和内存需求高。需要开发更高效的Transformer架构用于多模态生理数据的压力检测。

Method: 提出Efficient-Husformer架构，通过结构化搜索空间进行超参数优化，设计紧凑的Transformer结构，使用单层、3个注意力头、模型维度18/30、FFN维度120/30等配置。

Result: 在WESAD数据集上准确率达到88.41（提升13.83%），在CogLoad数据集上达到92.61（提升6.98%），最佳配置使用(L+dm)或(L+FFN)模态组合，模型参数仅约30k。

Conclusion: Efficient-Husformer通过超参数优化实现了高效的压力检测，在保持高性能的同时显著减少了模型复杂度，为生理信号分析提供了实用的Transformer解决方案。

Abstract: Transformer-based models have gained considerable attention in the field of physiological signal analysis. They leverage long-range dependencies and complex patterns in temporal signals, allowing them to achieve performance superior to traditional RNN and CNN models. However, they require high computational intensity and memory demands. In this work, we present Efficient-Husformer, a novel Transformer-based architecture developed with hyperparameter optimization (HPO) for multi-class stress detection across two multimodal physiological datasets (WESAD and CogLoad). The main contributions of this work are: (1) the design of a structured search space, targeting effective hyperparameter optimization; (2) a comprehensive ablation study evaluating the impact of architectural decisions; (3) consistent performance improvements over the original Husformer, with the best configuration achieving an accuracy of 88.41 and 92.61 (improvements of 13.83% and 6.98%) on WESAD and CogLoad datasets, respectively. The best-performing configuration is achieved with the (L + dm) or (L + FFN) modality combinations, using a single layer, 3 attention heads, a model dimension of 18/30, and FFN dimension of 120/30, resulting in a compact model with only about 30k parameters.

</details>


### [133] [SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning](https://arxiv.org/abs/2511.22367)
*Hugo Hazard,Zafeirios Fountas,Martin A. Benfeghoul,Adnan Oomerjee,Jun Wang,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 论文提出SuRe（基于惊讶度的回放）和双学习者设计，通过惊讶度优先选择和慢权重整合来缓解LLM持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习在视觉领域已有较好方法，但在大语言模型（LLM）中，特别是面对大量任务时，现有方法（如正则化和回放）仍落后于多任务学习。论文旨在解决回放方法中的两个关键失败模式：选择（回放什么）和整合（如何整合新知识）。

Method: 提出Surprise-prioritised Replay (SuRe)，基于负对数似然（NLL）选择最令人惊讶的序列进行回放；同时设计双学习者架构，包含快速和慢速LoRA适配器，通过指数移动平均（EMA）合并，实现快速适应和长期知识稳定。

Result: SuRe在大量任务（LNT）设置中达到最先进性能，在标准CL和LNT基准测试中平均表现最佳。结合双学习者设计后，在LNT上比之前SOTA提升高达+5个准确率点。消融研究证实该方法在减少回放频率和小缓冲区下仍保持鲁棒。

Conclusion: 回放是LLM持续微调的有效基线方法，惊讶度优先选择和慢权重整合是缓解灾难性遗忘的互补组件，为LLM持续学习提供了新的解决方案。

Abstract: Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.

</details>


### [134] [Predicting and Interpolating Spatiotemporal Environmental Data: A Case Study of Groundwater Storage in Bangladesh](https://arxiv.org/abs/2511.22378)
*Anna Pazola,Mohammad Shamsudduha,Richard G. Taylor,Allan Tucker*

Main category: cs.LG

TL;DR: 该研究比较了两种深度学习策略（网格到网格 vs 网格到点）用于时空预测，发现空间插值比时间预测更困难，地质不确定性对点时间行为有重要影响。


<details>
  <summary>Details</summary>
Motivation: 地理空间观测数据通常仅限于点测量，因此需要时间预测和空间插值来构建连续场。研究旨在评估两种深度学习策略的有效性。

Method: 使用孟加拉国地下水储量数据作为案例研究，比较两种方法：(1) 网格到网格方法（建模前聚合），使用网格化预测变量建模栅格化目标；(2) 网格到点方法（建模后聚合），使用网格化预测变量建模点目标，然后通过克里金插值填充域。

Result: 研究发现空间插值比时间预测困难得多。最近邻点不一定最相似，地质不确定性强烈影响点的时间行为。这些发现为基于时间序列动态聚类位置的高级插值方法提供了动机。

Conclusion: 虽然以地下水储量为案例，但结论适用于其他由间接可观测因素控制的环境变量。未来工作应开发基于时间序列动态聚类的高级插值方法。

Abstract: Geospatial observational datasets are often limited to point measurements, making temporal prediction and spatial interpolation essential for constructing continuous fields. This study evaluates two deep learning strategies for addressing this challenge: (1) a grid-to-grid approach, where gridded predictors are used to model rasterised targets (aggregation before modelling), and (2) a grid-to-point approach, where gridded predictors model point targets, followed by kriging interpolation to fill the domain (aggregation after modelling). Using groundwater storage data from Bangladesh as a case study, we compare the effcacy of these approaches. Our findings indicate that spatial interpolation is substantially more difficult than temporal prediction. In particular, nearest neighbours are not always the most similar, and uncertainties in geology strongly influence point temporal behaviour. These insights motivate future work on advanced interpolation methods informed by clustering locations based on time series dynamics. Demonstrated on groundwater storage, the conclusions are applicable to other environmental variables governed by indirectly observable factors. Code is available at https://github.com/pazolka/interpolation-prediction-gwsa.

</details>


### [135] [TS2Vec-Ensemble: An Enhanced Self-Supervised Framework for Time Series Forecasting](https://arxiv.org/abs/2511.22395)
*Ganeshan Niroshan,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: TS2Vec-Ensemble：一种新颖的混合框架，通过融合TS2Vec预训练编码器的隐式学习动态与显式工程化时间特征，结合自适应加权双模型集成架构，显著提升长时域时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督对比学习方法（如TS2Vec）在预测任务中表现不佳，因为其目标函数侧重于实例区分而非捕捉对预测至关重要的确定性模式（如季节性和趋势）。需要一种能结合隐式学习动态和显式时间先验的混合策略。

Method: 提出TS2Vec-Ensemble混合框架：1）使用预训练的TS2Vec编码器提取隐式学习动态；2）融合显式工程化的周期性时间特征；3）采用双模型集成架构，两个回归头分别关注学习动态和季节模式；4）通过自适应加权方案结合两者，权重针对每个预测时域独立优化。

Result: 在ETT基准数据集上进行单变量和多变量预测的广泛实验，结果表明TS2Vec-Ensemble始终显著优于标准TS2Vec基线和其他最先进模型，验证了混合策略在长时域时间序列预测中的优越性。

Conclusion: 结合学习表示和显式时间先验的混合策略是长时域时间序列预测的优越方法。TS2Vec-Ensemble通过自适应集成隐式动态和显式季节特征，有效弥补了纯自监督方法在预测任务中的不足。

Abstract: Self-supervised representation learning, particularly through contrastive methods like TS2Vec, has advanced the analysis of time series data. However, these models often falter in forecasting tasks because their objective functions prioritize instance discrimination over capturing the deterministic patterns, such as seasonality and trend, that are critical for accurate prediction. This paper introduces TS2Vec-Ensemble, a novel hybrid framework designed to bridge this gap. Our approach enhances the powerful, implicitly learned dynamics from a pretrained TS2Vec encoder by fusing them with explicit, engineered time features that encode periodic cycles. This fusion is achieved through a dual-model ensemble architecture, where two distinct regression heads -- one focused on learned dynamics and the other on seasonal patterns -- are combined using an adaptive weighting scheme. The ensemble weights are optimized independently for each forecast horizon, allowing the model to dynamically prioritize short-term dynamics or long-term seasonality as needed. We conduct extensive experiments on the ETT benchmark datasets for both univariate and multivariate forecasting. The results demonstrate that TS2Vec-Ensemble consistently and significantly outperforms the standard TS2Vec baseline and other state-of-the-art models, validating our hypothesis that a hybrid of learned representations and explicit temporal priors is a superior strategy for long-horizon time series forecasting.

</details>


### [136] [Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions](https://arxiv.org/abs/2511.22406)
*Roland Stolz,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文提出针对动作约束强化学习的高效数值近似方法，解决截断正态分布下熵、对数概率及其梯度计算困难的问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有动作约束强化学习方法在处理截断正态分布时，关键特性（如熵、对数概率及其梯度）的计算变得难以处理。先前工作使用非截断分布进行近似，严重降低了性能。作者认为在动作约束RL设置中准确估计这些特性至关重要。

Method: 提出高效数值近似方法来计算截断正态分布的关键特性（熵、对数概率及其梯度），并提供截断策略分布的高效采样策略。

Result: 在三个基准环境中验证了该方法，当使用准确估计时显示出显著的性能改进。

Conclusion: 准确估计截断正态分布的关键特性对于动作约束强化学习至关重要，提出的高效数值近似方法能显著提升性能。

Abstract: In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.

</details>


### [137] [PISA: Prioritized Invariant Subgraph Aggregation](https://arxiv.org/abs/2511.22435)
*Ali Ghasemi,Farooq Ahmad Wani,Maria Sofia Bucarelli,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: PISA提出动态MLP聚合机制，在15个数据集上比现有方法提升最多5%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 现有图OOD泛化方法存在局限：CIGA只提取单一不变子图可能遗漏多个因果模式，SuGAr虽然学习多个不变子图但聚合方式简单（均匀或贪婪），需要更有效的子图表示聚合机制

Method: 提出PISA框架，采用动态MLP聚合机制，优先选择和组合子图表示，更有效地整合多个不变子图

Result: 在15个数据集（包括DrugOOD）上的实验表明，PISA相比现有方法获得最高5%的分类准确率提升

Conclusion: 动态MLP聚合机制能更有效地整合多个不变子图表示，提升图OOD泛化性能

Abstract: Recent work has extended the invariance principle for out-of-distribution (OOD) generalization from Euclidean to graph data, where challenges arise due to complex structures and diverse distribution shifts in node attributes and topology. To handle these, Chen et al. proposed CIGA (Chen et al., 2022b), which uses causal modeling and an information-theoretic objective to extract a single invariant subgraph capturing causal features. However, this single-subgraph focus can miss multiple causal patterns. Liu et al. (2025) addressed this with SuGAr, which learns and aggregates diverse invariant subgraphs via a sampler and diversity regularizer, improving robustness but still relying on simple uniform or greedy aggregation. To overcome this, the proposed PISA framework introduces a dynamic MLP-based aggregation that prioritizes and combines subgraph representations more effectively. Experiments on 15 datasets, including DrugOOD (Ji et al., 2023), show that PISA achieves up to 5% higher classification accuracy than prior methods.

</details>


### [138] [An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction](https://arxiv.org/abs/2511.22460)
*Yifan Lei,Jiahua Luo,Tingyu Jiang,Bo Zhang,Lifeng Wang,Dapeng Liu,Zhaoren Wu,Haijie Gu,Huan Yu,Jie Jiang*

Main category: cs.LG

TL;DR: 提出基于GPU的高效特征交互双塔网络，通过压缩倒排列表实现检索阶段的早期特征交互，显著提升广告推荐系统检索准确率并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有双塔模型在广告推荐检索阶段存在特征交互不足的问题，用户和广告嵌入仅在最后内积计算时交互，而支持早期特征交互的DNN模型计算成本过高，不适用于检索阶段。

Method: 提出基于GPU的高效特征交互双塔网络，设计新颖的压缩倒排列表用于GPU加速，实现大规模特征交互计算，首次在工业界检索系统中成功实现Wide and Deep架构。

Result: 在腾讯广告真实业务场景中，该方法在离线评估中优于现有方法，并已成功部署到腾讯广告推荐系统，带来显著的在线性能提升。

Conclusion: 该方法不仅验证了所提方法的有效性，还为优化大规模广告检索系统提供了新的实践指导，成功解决了双塔模型特征交互不足与计算效率之间的平衡问题。

Abstract: In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.

</details>


### [139] [Adversarial Flow Models](https://arxiv.org/abs/2511.22475)
*Shanchuan Lin,Ceyuan Yang,Zhijie Lin,Hao Chen,Haoqi Fan*

Main category: cs.LG

TL;DR: 提出对抗流模型，统一对抗模型和流模型，支持原生单步或多步生成，通过对抗目标训练，在ImageNet-256px上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 传统GAN生成器学习任意传输方案，训练不稳定；一致性方法需要学习中间时间步，模型容量消耗大、训练迭代多、误差累积。需要一种既能稳定训练又能高效单步生成的方法

Method: 提出对抗流模型，生成器学习确定性噪声到数据的映射（与流匹配模型相同的优化传输），使用对抗目标训练，支持原生单步或多步生成，无需学习中间时间步的概率流

Result: 在ImageNet-256px上，B/2模型接近一致性XL/2模型性能，XL/2模型创下FID 2.38的新记录。通过深度重复端到端训练56层和112层模型，单步前向获得FID 2.08和1.94，超越2NFE和4NFE对应模型

Conclusion: 对抗流模型成功统一对抗模型和流模型，提供稳定训练和高效单步生成，在图像生成任务上取得最先进性能，展示了深度模型端到端训练的潜力

Abstract: We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.

</details>


### [140] [Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges](https://arxiv.org/abs/2511.22483)
*Guanxi Lu,Hao Mark Chen,Zhiqiang Que,Wayne Luk,Hongxiang Fan*

Main category: cs.LG

TL;DR: 论文系统研究量化对LLM可信度指标的影响，并提出混合精度集成投票方法提升可信度性能


<details>
  <summary>Details</summary>
Motivation: 现有量化框架主要关注困惑度或分类准确率，忽略了可信度指标，这在金融、医疗等高风险领域应用时存在风险

Method: 系统研究量化对四个可信度指标的影响，提出基于混合精度变体的精度集成投票方法

Result: 量化在不同压缩比和方法下对可信度指标表现出不稳定性，提出的集成方法将可信度指标性能提升达5.8%

Conclusion: 开发模型压缩技术时必须考虑可信度指标，压缩与可信度的交叉研究对安全关键应用具有重要意义

Abstract: Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.

</details>


### [141] [Space Explanations of Neural Network Classification](https://arxiv.org/abs/2511.22498)
*Faezeh Labbaf,Tomáš Kolárik,Martin Blicha,Grigory Fedyukovich,Michael Wand,Natasha Sharygina*

Main category: cs.LG

TL;DR: 提出一种基于逻辑的"空间解释"概念，为神经网络在输入特征空间的连续区域提供可证明的行为保证，通过Craig插值和不可满足核生成自动生成解释，在真实案例中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络解释方法缺乏对网络在连续输入区域行为的可证明保证，需要一种能够提供形式化保证的解释框架，以增强神经网络的可信度和可解释性。

Method: 提出基于逻辑的"空间解释"概念，利用Craig插值算法和不可满足核生成技术自动生成解释，这些解释能够证明神经网络在输入特征空间连续区域的行为特性。

Result: 在从小型到大型的真实案例研究中，生成的解释比现有最先进方法更有意义，能够提供神经网络行为的可证明保证。

Conclusion: 空间解释为神经网络分类提供了具有可证明保证的解释框架，通过形式化方法增强了神经网络的可解释性和可信度，在真实应用中表现出优越性。

Abstract: We present a novel logic-based concept called Space Explanations for classifying neural networks that gives provable guarantees of the behavior of the network in continuous areas of the input feature space. To automatically generate space explanations, we leverage a range of flexible Craig interpolation algorithms and unsatisfiable core generation. Based on real-life case studies, ranging from small to medium to large size, we demonstrate that the generated explanations are more meaningful than those computed by state-of-the-art.

</details>


### [142] [Privacy-Utility-Bias Trade-offs for Privacy-Preserving Recommender Systems](https://arxiv.org/abs/2511.22515)
*Shiva Parsarad,Isabel Wagner*

Main category: cs.LG

TL;DR: 论文对推荐系统中两种差分隐私机制（DPSGD和LDP）在四种推荐模型上的效果进行了全面评估，发现更强的隐私保护会降低推荐效用，但不同模型受影响程度不同，没有一种隐私机制在所有情况下都最优。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统越来越多地采用差分隐私来保护用户数据，需要了解隐私机制如何影响推荐准确性和公平性。目前缺乏对不同隐私机制和推荐模型的全面比较研究。

Method: 对两种差分隐私机制（DPSGD和LDP）在四种推荐系统（NCF、BPR、SVD、VAE）上进行跨模型评估，使用MovieLens-1M和Yelp数据集，分析隐私保护对推荐准确性和公平性的影响。

Result: 更强的隐私保护会降低推荐效用，但不同模型受影响程度不同：NCF在DPSGD下准确率损失最小（ε≈1时低于10%），SVD和BPR损失较大，VAE对隐私最敏感。DPSGD能减少热门与冷门物品推荐差距，LDP则更保持原有模式。

Conclusion: 没有单一的差分隐私机制在所有情况下都最优，每种机制在不同隐私级别和数据条件下都有各自的权衡。选择隐私机制需要考虑具体推荐模型、数据特征和隐私要求。

Abstract: Recommender systems (RSs) output ranked lists of items, such as movies or restaurants, that users may find interesting, based on the user's past ratings and ratings from other users. RSs increasingly incorporate differential privacy (DP) to protect user data, raising questions about how privacy mechanisms affect both recommendation accuracy and fairness. We conduct a comprehensive, cross-model evaluation of two DP mechanisms, differentially private stochastic gradient descent (DPSGD) and local differential privacy (LDP), applied to four recommender systems (Neural Collaborative Filtering (NCF), Bayesian Personalized Ranking (BPR), Singular Value Decomposition (SVD), and Variational Autoencoder (VAE)) on the MovieLens-1M and Yelp datasets. We find that stronger privacy consistently reduces utility, but not uniformly. NCF under DPSGD shows the smallest accuracy loss (under 10 percent at epsilon approximately 1), whereas SVD and BPR experience larger drops, especially for users with niche preferences. VAE is the most sensitive to privacy, with sharp declines for sparsely represented groups. The impact on bias metrics is similarly heterogeneous. DPSGD generally reduces the gap between recommendations of popular and less popular items, whereas LDP preserves existing patterns more closely. These results highlight that no single DP mechanism is uniformly superior; instead, each provides trade-offs under different privacy regimes and data conditions.

</details>


### [143] [List-Decodable Regression via Expander Sketching](https://arxiv.org/abs/2511.22524)
*Herbod Pourali,Sajjad Hashemian,Ebrahim Ardeshir-Larijani*

Main category: cs.LG

TL;DR: 提出基于扩展图草图技术的列表可解码线性回归方法，在亚高斯假设下实现样本复杂度$\tilde{O}((d+\log(1/δ))/α)$、列表大小$O(1/α)$和近输入稀疏运行时间$\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$。


<details>
  <summary>Details</summary>
Motivation: 解决列表可解码线性回归中的效率和样本复杂度问题，避免使用SoS（平方和）机制和显式批次结构，提供更实用的解决方案。

Method: 使用扩展图草图框架，通过无损扩展器合成轻度污染批次，结合鲁棒聚合和短谱滤波阶段，实现高效计算。

Result: 在标准亚高斯假设下，实现了最优的样本复杂度、列表大小和运行时间保证，同时避免了复杂的SoS机制和显式批次结构。

Conclusion: 扩展图草图框架为列表可解码线性回归提供了高效实用的解决方案，在保持理论保证的同时显著提升了计算效率。

Abstract: We introduce an expander-sketching framework for list-decodable linear regression that achieves sample complexity $\tilde{O}((d+\log(1/δ))/α)$, list size $O(1/α)$, and near input-sparsity running time $\tilde{O}(\mathrm{nnz}(X)+d^{3}/α)$ under standard sub-Gaussian assumptions. Our method uses lossless expanders to synthesize lightly contaminated batches, enabling robust aggregation and a short spectral filtering stage that matches the best known efficient guarantees while avoiding SoS machinery and explicit batch structure.

</details>


### [144] [Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs](https://arxiv.org/abs/2511.22567)
*Feyza Eksen,Stefan Oehmcke,Stefan Lüdtke*

Main category: cs.LG

TL;DR: 提出基于认知不确定性的传感器放置新方法，使用混合密度网络扩展ConvCNPs来估计认知不确定性，相比基于总体不确定性的方法能更有效降低模型误差。


<details>
  <summary>Details</summary>
Motivation: 现有传感器放置方法依赖总体预测不确定性，混淆了认知不确定性和偶然不确定性，可能导致在模糊区域选择次优传感器位置。

Method: 提出认知不确定性期望减少作为新的传感器放置获取函数，通过混合密度网络输出头扩展卷积条件神经过程来估计认知不确定性。

Result: 初步结果表明，基于认知不确定性的传感器放置比基于总体不确定性的方法能更有效地减少模型误差。

Conclusion: 认知不确定性驱动的传感器放置方法优于传统基于总体不确定性的方法，为时空系统建模提供了更有效的传感器配置策略。

Abstract: Accurate sensor placement is critical for modeling spatio-temporal systems such as environmental and climate processes. Neural Processes (NPs), particularly Convolutional Conditional Neural Processes (ConvCNPs), provide scalable probabilistic models with uncertainty estimates, making them well-suited for data-driven sensor placement. However, existing approaches rely on total predictive uncertainty, which conflates epistemic and aleatoric components, that may lead to suboptimal sensor selection in ambiguous regions. To address this, we propose expected reduction in epistemic uncertainty as a new acquisition function for sensor placement. To enable this, we extend ConvCNPs with a Mixture Density Networks (MDNs) output head for epistemic uncertainty estimation. Preliminary results suggest that epistemic uncertainty driven sensor placement more effectively reduces model error than approaches based on overall uncertainty.

</details>


### [145] [Entropy is all you need for Inter-Seed Cross-Play in Hanabi](https://arxiv.org/abs/2511.22581)
*Johannes Forkel,Jakob Foerster*

Main category: cs.LG

TL;DR: 在Hanabi游戏中，标准独立PPO算法通过提高熵系数到0.05（而非通常的0.01），在跨种子交叉游戏中达到新的最优性能，超越了所有专门为此场景设计的算法。


<details>
  <summary>Details</summary>
Motivation: 研究在零样本协调和临时团队协作中最复杂的基准之一Hanabi游戏中，如何通过简单的超参数调整来提升跨种子交叉游戏性能，探索标准算法在此场景下的潜力。

Method: 使用标准独立PPO算法，但将熵系数从通常的0.01提高到0.05，同时发现较高的GAE λ值（约0.9）和使用RNN而非前馈网络能显著提升跨种子交叉游戏性能。

Result: 该方法在Hanabi的跨种子交叉游戏中实现了新的最优性能，显著超越了所有专门为此场景设计的算法。研究还发现高熵正则化能确保不同随机种子产生的策略相互兼容。

Conclusion: 虽然超参数调整对跨种子交叉游戏性能有显著影响，但在某些简单Dec-POMDP中，增加熵正则化的标准策略梯度方法仍无法实现完美的跨种子交叉游戏，表明仍需开发新的零样本协调算法。

Abstract: We find that in Hanabi, one of the most complex and popular benchmarks for zero-shot coordination and ad-hoc teamplay, a standard implementation of independent PPO with a slightly higher entropy coefficient 0.05 instead of the typically used 0.01, achieves a new state-of-the-art in cross-play between different seeds, beating by a significant margin all previous specialized algorithms, which were specifically designed for this setting. We provide an intuition for why sufficiently high entropy regularization ensures that different random seed produce joint policies which are mutually compatible. We also empirically find that a high $λ_{\text{GAE}}$ around 0.9, and using RNNs instead of just feed-forward layers in the actor-critic architecture, strongly increase inter-seed cross-play. While these results demonstrate the dramatic effect that hyperparameters can have not just on self-play scores but also on cross-play scores, we show that there are simple Dec-POMDPs though, in which standard policy gradient methods with increased entropy regularization are not able to achieve perfect inter-seed cross-play, thus demonstrating the continuing necessity for new algorithms for zero-shot coordination.

</details>


### [146] [The Multiclass Score-Oriented Loss (MultiSOL) on the Simplex](https://arxiv.org/abs/2511.22587)
*Francesco Marchetti,Edoardo Legnaro,Sabrina Guastavino*

Main category: cs.LG

TL;DR: 将二元分类中的score-oriented损失函数扩展到多类分类，提出MultiSOL损失函数，可直接优化目标指标，避免后验阈值调整


<details>
  <summary>Details</summary>
Motivation: 在监督二元分类中，score-oriented损失函数可直接在训练阶段优化目标性能指标，避免后验阈值调整。但在多类分类中缺乏类似方法，需要扩展这一框架

Method: 使用最近提出的多维阈值分类框架，将score-oriented损失函数扩展到多类分类，定义Multiclass Score-Oriented Loss (MultiSOL)函数。在构建中将决策阈值视为具有先验分布的随机变量

Result: MultiSOL保持了二元设置中的主要优势：直接优化目标指标、对类别不平衡的鲁棒性，性能与其他最先进的损失函数相当，并提供了单纯形几何与score-oriented学习交互的新见解

Conclusion: 成功将score-oriented损失函数扩展到多类分类，提出的MultiSOL函数在保持二元设置优点的同时，为多类分类提供了有效的直接优化方法

Abstract: In the supervised binary classification setting, score-oriented losses have been introduced with the aim of optimizing a chosen performance metric directly during the training phase, thus avoiding \textit{a posteriori} threshold tuning. To do this, in their construction, the decision threshold is treated as a random variable provided with a certain \textit{a priori} distribution. In this paper, we use a recently introduced multidimensional threshold-based classification framework to extend such score-oriented losses to multiclass classification, defining the Multiclass Score-Oriented Loss (MultiSOL) functions. As also demonstrated by several classification experiments, this proposed family of losses is designed to preserve the main advantages observed in the binary setting, such as the direct optimization of the target metric and the robustness to class imbalance, achieving performance comparable to other state-of-the-art loss functions and providing new insights into the interaction between simplex geometry and score-oriented learning.

</details>


### [147] [LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://arxiv.org/abs/2511.22598)
*Huanyu Li,Zongyuan Li,Wei Huang,Xian Guo*

Main category: cs.LG

TL;DR: LLM-Cave：一个用于评估大语言模型序列推理和决策能力的轻量级基准环境，相比复杂游戏环境更高效


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准局限于单步交互，而现有的序列决策环境（如TextStarCraftII）过于复杂，需要数小时才能完成一局游戏，缺乏高效评估LLM序列推理和决策能力的轻量级环境

Method: 引入LLM-Cave基准环境，这是一个符号主义时代的经典实例，智能体通过部分可观测状态信息推理附近危险来探索环境并避免潜在损失。评估了GPT-4o-mini、o1-mini、DeepSeek-R1等主流LLM的序列推理能力、决策性能和计算效率

Result: DeepSeek-R1在复杂推理任务上获得最高成功率，但较小模型如4o-mini通过使用推测链和规划者-批评者策略显著缩小了性能差距，代价是计算效率降低。结构化多步推理结合基于LLM的反馈机制能显著增强LLM的决策能力

Conclusion: 结构化多步推理与基于LLM的反馈机制相结合可以显著提升LLM的决策能力，为改进较弱模型的推理提供了有前景的方向，并提出了以推理为中心的LLM评估新基准

Abstract: Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.

</details>


### [148] [Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning](https://arxiv.org/abs/2511.22640)
*Riccardo De Santi,Marin Vlastelica,Ya-Ping Hsieh,Zebang Shen,Niao He,Andreas Krause*

Main category: cs.LG

TL;DR: 提出Flow Density Control (FDC)算法，用于优化预训练生成模型以处理超越平均奖励的通用效用函数，同时通过多种距离度量保持先验信息。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法主要关注最大化生成样本的期望奖励，并通过KL散度正则化保持预训练模型知识。但实际应用中需要优化更通用的效用函数（如风险规避、新颖性寻求、多样性度量等），并考虑更通用的先验信息保持方式（如最优传输距离、Renyi散度）。

Method: 提出Flow Density Control (FDC)算法，将复杂优化问题分解为一系列更简单的微调任务序列，每个子任务可通过现有可扩展方法求解。算法基于镜像流的最新理论理解，在现实假设下推导收敛保证。

Result: 在多个任务上验证方法有效性：1）说明性设置；2）文本到图像生成；3）分子设计。结果显示FDC能够引导预训练生成模型优化目标，解决当前微调方案无法处理的实用相关任务。

Conclusion: FDC提供了一种通用框架，能够优化超越平均奖励的广泛效用函数，同时通过多种距离度量保持先验信息，扩展了生成模型在实际应用中的能力范围。

Abstract: Adapting large-scale foundation flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Renyi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes.

</details>


### [149] [Spatially Aware Dictionary-Free Eigenfunction Identification for Modeling and Control of Nonlinear Dynamical Systems](https://arxiv.org/abs/2511.22648)
*David Grasev*

Main category: cs.LG

TL;DR: 提出一种无需预定义基函数的数据驱动Koopman特征函数发现方法，通过参考轨迹识别特征值，利用正则化最小二乘拟合投影，结合全局优化和PDE约束提升鲁棒性，在多个非线性系统上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统Koopman算子方法通常需要预定义基函数，限制了其适用性和准确性。本文旨在开发一种无需预定义基函数的数据驱动方法，直接从数据中发现Koopman特征函数，提高对复杂非线性系统的建模能力。

Method: 基于参考轨迹识别Koopman模态振幅，将模态分解转换到包含特征值和时间的基本函数新基上。通过正则化最小二乘拟合将轨迹投影到该基上获得特征函数初值，使用全局优化器优化特征值。通过映射初始状态值到特征函数值揭示其空间结构，数值计算梯度并惩罚偏离Koopman偏微分方程的解。

Result: 方法在多个基准非线性系统上成功测试，包括带输入的FitzHugh-Nagumo系统、van der Pol和Duffing振荡器以及带控制的2轴涡轮喷气发动机。结果表明，结合主特征值和空间结构完整性促进显著提高了Koopman预测器的准确性，即使在稀疏状态空间采样下也能有效发现Koopman谱分量，并揭示状态空间的几何特征。

Conclusion: 该方法提供了一种实用的数据驱动Koopman特征函数发现框架，无需预定义基函数，通过结合特征值优化和PDE约束提高了鲁棒性。特征函数梯度的数值近似可用于输入动力学建模和控制设计，支持该方法在各种动力系统中的实际应用。

Abstract: A new approach to data-driven discovery of Koopman eigenfunctions without a pre-defined set of basis functions is proposed. The approach is based on a reference trajectory, for which the Koopman mode amplitudes are first identified, and the Koopman mode decomposition is transformed to a new basis, which contains fundamental functions of eigenvalues and time. The initial values of the eigenfunctions are obtained by projecting trajectories onto this basis via a regularized least-squares fit. A global optimizer was employed to optimize the eigenvalues. Mapping initial-state values to eigenfunction values reveals their spatial structure, enabling the numerical computation of their gradients. Thus, deviations from the Koopman partial differential equation are penalized, leading to more robust solutions. The approach was successfully tested on several benchmark nonlinear dynamical systems, including the FitzHugh-Nagumo system with inputs, van der Pol and Duffing oscillators, and a 2-spool turbojet engine with control. The study demonstrates that incorporating principal eigenvalues and spatial structure integrity promotion significantly improves the accuracy of Koopman predictors. The approach effectively discovers Koopman spectral components even with sparse state-space sampling and reveals geometric features of the state space, such as invariant partitions. Finally, the numerical approximation of the eigenfunction gradient can be used for input dynamics modeling and control design. The results support the practicality of the approach for use with various dynamical systems.

</details>


### [150] [Automated Design Optimization via Strategic Search with Large Language Models](https://arxiv.org/abs/2511.22651)
*Anthony Carreon,Vansh Sharma,Venkat Raman*

Main category: cs.LG

TL;DR: AUTO是一个基于大语言模型的智能体框架，将设计优化视为无梯度搜索问题，通过战略推理实现GPU代码优化，性能接近专家实现，搜索效率达到贝叶斯优化的50-70%。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在定义明确的设计空间中表现良好，但在设计参数难以明确定义的设计问题中表现不佳。大语言模型能够动态解释设计空间并利用编码的领域知识，为解决这类问题提供了有前景的替代方案。

Method: AUTO框架将设计优化视为无梯度搜索问题，采用两个协作智能体：战略家（Strategist）负责在探索和利用策略之间选择，实施者（Implementor）负责执行详细设计。该框架应用于GPU代码优化领域。

Result: 在化学动力学积分和稠密矩阵乘法等GPU代码优化任务中，AUTO生成的解决方案与专家实现具有竞争力。框架的搜索效率达到贝叶斯优化方法的50-70%，每次优化约需8小时，成本约159美元，而使用中等薪资软件开发者则需约480美元。

Conclusion: AUTO框架为在定义不明确、先验信息有限的设计空间中实现自动化优化开辟了新途径，展示了LLM智能体在复杂设计优化问题中的潜力。

Abstract: Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \$159 per run, compared to an estimated cost of up to \$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.

</details>


### [151] [Structure-aware Hybrid-order Similarity Learning for Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.22656)
*Lin Xu,Ke Li,Dongjie Wang,Fengmao Lv,Tianrui Li,Yanyong Huang*

Main category: cs.LG

TL;DR: SHINE-FS提出了一种新的多视图无监督特征选择方法，通过联合学习一阶和二阶相似性图来捕获局部和全局结构，从而提高特征选择性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法主要使用一阶相似性图来保留局部结构，但忽略了二阶相似性可以捕获的全局结构。少数使用预定义二阶相似性图的方法容易受到噪声和异常值的影响，导致特征选择性能不佳。

Method: SHINE-FS首先学习共识锚点和相应的锚点图来捕获锚点与样本之间的跨视图关系。基于获得的跨视图共识信息，生成样本的低维表示，通过识别判别性特征来重构多视图数据。然后利用锚点-样本关系学习二阶相似性图。最后通过联合学习一阶和二阶相似性图，构建混合阶相似性图来捕获局部和全局结构。

Result: 在真实多视图数据集上的综合实验结果表明，SHINE-FS优于现有的最先进方法。

Conclusion: SHINE-FS通过联合学习一阶和二阶相似性图，能够同时捕获局部和全局结构，从而更好地揭示数据的内在结构，提高多视图无监督特征选择的性能。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently emerged as an effective dimensionality reduction method for unlabeled multi-view data. However, most existing methods mainly use first-order similarity graphs to preserve local structure, often overlooking the global structure that can be captured by second-order similarity. In addition, a few MUFS methods leverage predefined second-order similarity graphs, making them vulnerable to noise and outliers and resulting in suboptimal feature selection performance. In this paper, we propose a novel MUFS method, termed Structure-aware Hybrid-order sImilarity learNing for multi-viEw unsupervised Feature Selection (SHINE-FS), to address the aforementioned problem. SHINE-FS first learns consensus anchors and the corresponding anchor graph to capture the cross-view relationships between the anchors and the samples. Based on the acquired cross-view consensus information, it generates low-dimensional representations of the samples, which facilitate the reconstruction of multi-view data by identifying discriminative features. Subsequently, it employs the anchor-sample relationships to learn a second-order similarity graph. Furthermore, by jointly learning first-order and second-order similarity graphs, SHINE-FS constructs a hybrid-order similarity graph that captures both local and global structures, thereby revealing the intrinsic data structure to enhance feature selection. Comprehensive experimental results on real multi-view datasets show that SHINE-FS outperforms the state-of-the-art methods.

</details>


### [152] [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/abs/2511.22662)
*Lewis Smith,Bilal Chughtai,Neel Nanda*

Main category: cs.LG

TL;DR: 论文指出当前缺乏可靠的AI欺骗检测方法，因为缺少可明确标记为欺骗或诚实行为的训练数据，并分析了收集这类数据的障碍


<details>
  <summary>Details</summary>
Motivation: 构建可靠的AI欺骗检测器对于缓解先进AI系统的风险至关重要，但评估这些检测器的可靠性需要明确标记为欺骗或诚实行为的示例

Method: 通过概念论证、现有实证研究分析以及新颖案例研究分析，识别收集欺骗检测训练数据的具体障碍

Result: 发现当前缺乏必要的欺骗/诚实行为示例，并识别了多个收集这类数据的具体障碍；现有实证解决方案虽然有价值但单独使用不足

Conclusion: 欺骗检测的进展需要进一步考虑数据收集问题，当前方法不足以构建可靠的AI欺骗检测器

Abstract: Building reliable deception detectors for AI systems -- methods that could predict when an AI system is being strategically deceptive without necessarily requiring behavioural evidence -- would be valuable in mitigating risks from advanced AI systems. But evaluating the reliability and efficacy of a proposed deception detector requires examples that we can confidently label as either deceptive or honest. We argue that we currently lack the necessary examples and further identify several concrete obstacles in collecting them. We provide evidence from conceptual arguments, analysis of existing empirical works, and analysis of novel illustrative case studies. We also discuss the potential of several proposed empirical workarounds to these problems and argue that while they seem valuable, they also seem insufficient alone. Progress on deception detection likely requires further consideration of these problems.

</details>


### [153] [Modèles de Fondation et Ajustement : Vers une Nouvelle Génération de Modèles pour la Prévision des Séries Temporelles](https://arxiv.org/abs/2511.22674)
*Morad Laglil,Emilie Devijver,Eric Gaussier,Bertrand Pracca*

Main category: cs.LG

TL;DR: 该论文综述了零样本时间序列预测的基础模型，分析了主要架构、预训练策略和优化方法，并通过实验证明微调能提升零样本预测能力，特别是对长期预测效果更显著。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型启发，时间序列领域也发展了基础模型用于零样本预测，这些模型在大规模时间序列数据上预训练，学习通用表示，减少了对任务特定架构和手动调参的需求。然而，预训练后的微调对提升特定数据集性能的影响需要系统研究。

Method: 论文系统回顾了零样本时间序列预测基础模型的主要架构、预训练策略和优化方法，并研究了预训练后微调对特定数据集性能的影响，通过实证分析评估微调效果。

Result: 实验结果表明，微调通常能提升零样本预测能力，特别是对长期预测效果改善更为显著。这验证了在基础模型上针对特定数据集进行微调的有效性。

Conclusion: 时间序列基础模型通过大规模预训练学习通用表示，结合针对特定数据集的微调，能够有效提升零样本预测性能，特别是对长期预测任务，为时间序列预测提供了更灵活高效的解决方案。

Abstract: Inspired by recent advances in large language models, foundation models have been developed for zero-shot time series forecasting, enabling prediction on datasets unseen during pretraining. These large-scale models, trained on vast collections of time series, learn generalizable representations for both point and probabilistic forecasting, reducing the need for task-specific architectures and manual tuning.
  In this work, we review the main architectures, pretraining strategies, and optimization methods used in such models, and study the effect of fine-tuning after pretraining to enhance their performance on specific datasets. Our empirical results show that fine-tuning generally improves zero-shot forecasting capabilities, especially for long-term horizons.

</details>


### [154] [Test-time scaling of diffusions with flow maps](https://arxiv.org/abs/2511.22688)
*Amirmojtaba Sabour,Michael S. Albergo,Carles Domingo-Enrich,Nicholas M. Boffi,Sanja Fidler,Karsten Kreis,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出FMTT方法，通过流映射直接处理扩散模型测试时优化问题，比传统基于奖励梯度的方法更有效


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型测试时优化方法存在问题：用户指定的奖励函数通常只在生成结束时定义良好，而现有方法使用去噪器估计样本在生成结束时的状态存在局限性

Method: 利用流映射与速度场之间的关系，提出Flow Map Trajectory Tilting (FMTT)算法，直接处理流映射而不是依赖奖励梯度

Result: FMTT在奖励提升方面理论上优于标准测试时方法，支持精确采样和原则性搜索，能处理复杂奖励函数，实现新的图像编辑形式

Conclusion: FMTT为扩散模型测试时优化提供了更可靠的方法，特别适用于处理复杂奖励函数，如与视觉语言模型接口的图像编辑任务

Abstract: A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.

</details>


### [155] [Generative Anchored Fields: Controlled Data Generation via Emergent Velocity Fields and Transport Algebra](https://arxiv.org/abs/2511.22693)
*Deressa Wodajo Deressa,Hannes Mareen,Peter Lambert,Glenn Van Wallendael*

Main category: cs.LG

TL;DR: GAF是一种生成模型，通过分解学习独立的端点预测器J（噪声）和K（数据），而非轨迹预测器，实现速度场v=K-J，支持传输代数和组合控制。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常学习轨迹预测器，限制了组合控制能力。GAF旨在通过分解学习独立的端点预测器，实现更丰富的组合生成和可控传输。

Method: 提出生成锚定场（GAF），学习时间条件化的独立端点预测器J（噪声）和K（数据），速度场v=K-J从它们的差异中产生。支持传输代数：对学习的{(J_n,K_n)}头进行代数操作以实现组合控制。

Result: 在CelebA-HQ 64×64上达到FID 7.5的强样本质量，支持可控插值、混合生成和语义变形。GAF在初始和最终状态之间实现无损循环传输（LPIPS=0.0）。

Conclusion: GAF通过分解学习独立端点预测器，实现了传输代数和组合控制，在保持高质量生成的同时，提供了独特的组合生成架构原语。

Abstract: We present Generative Anchored Fields (GAF), a generative model that learns independent endpoint predictors $J$ (noise) and $K$ (data) rather than a trajectory predictor. The velocity field $v=K-J$ emerges from their time-conditioned disagreement. This factorization enables \textit{Transport Algebra}: algebraic operation on learned $\{(J_n,K_n)\}_{n=1}^N$ heads for compositional control. With class-specific $K_n$ heads, GAF supports a rich family of directed transport maps between a shared base distribution and multiple modalities, enabling controllable interpolation, hybrid generation, and semantic morphing through vector arithmetic. We achieve strong sample quality (FID 7.5 on CelebA-HQ $64\times 64$) while uniquely providing compositional generation as an architectural primitive. We further demonstrate, GAF has lossless cyclic transport between its initial and final state with LPIPS=$0.0$. Code available at https://github.com/IDLabMedia/GAF

</details>


### [156] [Integrated Transcriptomic-proteomic Biomarker Identification for Radiation Response Prediction in Non-small Cell Lung Cancer Cell Lines](https://arxiv.org/abs/2511.22735)
*Yajun Yu,Guoping Xu,Steve Jiang,Robert Timmerman,John Minna,Yuanyuan Zhang,Hao Peng*

Main category: cs.LG

TL;DR: 开发了首个用于预测非小细胞肺癌辐射反应（SF2）的转录组-蛋白质组整合框架，通过多组学数据识别并发生物标志物。


<details>
  <summary>Details</summary>
Motivation: 开发一个整合转录组和蛋白质组数据的框架，以识别能够预测非小细胞肺癌细胞系辐射反应（SF2）的并发生物标志物，弥补单一组学方法的局限性。

Method: 收集73个NSCLC细胞系的RNA-seq数据和46个细胞系的DIA-MS蛋白质组数据，保留1,605个共享基因。使用Lasso回归进行特征选择，构建转录组、蛋白质组和整合数据的支持向量回归模型，通过交叉验证评估性能。

Result: RNA与蛋白质表达呈显著正相关（中位数r=0.363）。单组学模型跨组学泛化能力有限，而整合模型在两个数据集中均表现出平衡的预测准确性（转录组R²=0.461，蛋白质组R²=0.604）。识别出20个优先基因特征。

Conclusion: 这是首个用于NSCLC SF2预测的蛋白质转录组学框架，证明了整合转录组和蛋白质组数据的互补价值。识别的并发生物标志物同时捕捉了转录调控和功能性蛋白质活性，具有机制洞察和转化潜力。

Abstract: To develop an integrated transcriptome-proteome framework for identifying concurrent biomarkers predictive of radiation response, as measured by survival fraction at 2 Gy (SF2), in non-small cell lung cancer (NSCLC) cell lines. RNA sequencing (RNA-seq) and data-independent acquisition mass spectrometry (DIA-MS) proteomic data were collected from 73 and 46 NSCLC cell lines, respectively. Following preprocessing, 1,605 shared genes were retained for analysis. Feature selection was performed using least absolute shrinkage and selection operator (Lasso) regression with a frequency-based ranking criterion under five-fold cross-validation repeated ten times. Support vector regression (SVR) models were constructed using transcriptome-only, proteome-only, and combined transcriptome-proteome feature sets. Model performance was assessed by the coefficient of determination (R2) and root mean square error (RMSE). Correlation analyses evaluated concordance between RNA and protein expression and the relationships of selected biomarkers with SF2. RNA-protein expression exhibited significant positive correlations (median Pearson's r = 0.363). Independent pipelines identified 20 prioritized gene signatures from transcriptomic, proteomic, and combined datasets. Models trained on single-omic features achieved limited cross-omic generalizability, while the combined model demonstrated balanced predictive accuracy in both datasets (R2=0.461, RMSE=0.120 for transcriptome; R2=0.604, RMSE=0.111 for proteome). This study presents the first proteotranscriptomic framework for SF2 prediction in NSCLC, highlighting the complementary value of integrating transcriptomic and proteomic data. The identified concurrent biomarkers capture both transcriptional regulation and functional protein activity, offering mechanistic insights and translational potential.

</details>


### [157] [VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization](https://arxiv.org/abs/2511.22749)
*Zeng Wang,Weihua Xiao,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Ramesh Karri*

Main category: cs.LG

TL;DR: VeriDispatcher是一个多LLM RTL生成框架，通过预推理难度预测将任务分发给合适的LLM，在保持准确性的同时显著降低商业API调用成本。


<details>
  <summary>Details</summary>
Motivation: 不同LLM在RTL生成任务上各有优势，但现有工作主要关注单个模型的提示或微调。如何协调多个不同LLM来共同提高RTL质量并降低成本，而不是运行所有模型并选择最佳输出，这个问题尚未得到充分研究。

Method: 提出VeriDispatcher框架：1）为每个LLM训练紧凑的分类器，基于任务描述的语义嵌入和难度评分；2）难度评分来自结合语法、结构相似性和功能正确性的基准变体；3）在推理时，使用这些预测器将任务路由到选定的LLM子集。

Result: 在RTLLM和VerilogEval基准上测试10个不同LLM：1）在RTLLM上，仅使用40%的商业调用就实现了18%的准确率提升；2）在VerilogEval上，保持准确性的同时将商业使用减少了25%。

Conclusion: VeriDispatcher能够在硬件设计自动化中实现成本效益高、高质量的LLM部署，通过智能调度多个LLM来优化性能和成本平衡。

Abstract: Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.

</details>


### [158] [Exact Learning of Arithmetic with Differentiable Agents](https://arxiv.org/abs/2511.22751)
*Hristo Papazov,Francesco D'Angelo,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 提出可微分有限状态转换器（DFSTs），通过梯度方法实现精确算法学习，在算术任务上实现强长度泛化


<details>
  <summary>Details</summary>
Motivation: 探索基于梯度的方法实现精确算法学习的可能性，解决现有架构在长度泛化方面的局限性

Method: 使用可微分有限状态转换器（DFSTs），这是一种图灵完备的模型族，支持恒定精度、恒定时间生成和端到端对数并行可微分训练。利用专家智能体的策略轨迹观察来训练DFSTs执行二进制和十进制加减乘运算

Result: 在极小数据集上训练的模型能够无错误地泛化到比训练样本长数千倍的输入，实现了强长度泛化

Conclusion: 在结构化中间监督下训练可微分智能体可能为基于梯度的精确算法技能学习开辟新途径

Abstract: We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.

</details>


### [159] [GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels](https://arxiv.org/abs/2511.22793)
*Bhavya Sai Nukapotula,Rishabh Tripathi,Seth Pregler,Dileep Kalathil,Srinivas Shakkottai,Theodore S. Rappaport*

Main category: cs.LG

TL;DR: GSpaRC使用3D高斯基元表示RF环境，通过定制CUDA流水线实现亚毫秒级延迟的实时信道状态信息重建，显著降低5G网络中的导频开销。


<details>
  <summary>Details</summary>
Motivation: 5G网络中获取信道状态信息(CSI)消耗高达25%的频谱资源，现有方法虽然能减少导频开销，但推理延迟在5-100ms范围，无法满足实时系统需求。

Method: 使用3D高斯基元表示RF环境，每个基元由轻量级神经网络参数化并加入物理感知特征（如距离衰减）。采用等距柱状投影到以接收器为中心的半球面，反映全向天线特性。定制CUDA流水线实现全并行化的方向排序、溅射和渲染。

Result: 在多个RF数据集上评估，GSpaRC达到与最新方法相似的CSI重建精度，同时将训练和推理时间降低一个数量级以上，突破1ms延迟壁垒。

Conclusion: GSpaRC通过适度的GPU计算换取显著的导频开销减少，实现了可扩展、低延迟的信道估计，适用于5G及未来无线系统部署。

Abstract: Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.

</details>


### [160] [Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?](https://arxiv.org/abs/2511.22794)
*Fitria Wulandari Ramlan,Colm O'Riordan,Gabriel Kronberger,James McDermott*

Main category: cs.LG

TL;DR: 该研究提出一种使用合成数据改善符号回归模型外推性能的方法，通过核密度估计识别稀疏区域，利用知识蒸馏生成合成数据，在六个基准数据集上验证了GP模型在外推区域的改进效果。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习模型在训练数据范围内表现良好，但在外推（超出训练范围）时表现不佳。符号回归使用遗传编程可以生成灵活模型，但在外推时容易出现不可靠行为。研究旨在探索添加合成数据是否能改善这种外推性能。

Method: 1. 使用核密度估计识别输入空间中训练数据稀疏的区域；2. 采用知识蒸馏方法生成合成数据：教师模型在新输入点上生成预测，用于训练学生模型；3. 在六个基准数据集上评估该方法，使用神经网络、随机森林和遗传编程作为教师模型（生成合成数据）和学生模型（在增强数据上训练）。

Result: 1. GP模型在合成数据上训练后通常能改善性能，特别是在外推区域；2. 改进程度取决于数据集和使用的教师模型；3. 最显著的改进出现在使用GPe生成的合成数据训练GPp模型时；4. 插值区域仅显示轻微变化；5. 观察到异质性误差，模型性能在输入空间的不同区域存在差异。

Conclusion: 该方法为改善外推性能提供了实用解决方案，特别是对于符号回归模型。合成数据的生成策略和教师模型的选择对最终效果有重要影响，在外推区域能获得显著改进，而在插值区域变化有限。

Abstract: Many machine learning models perform well when making predictions within the training data range, but often struggle when required to extrapolate beyond it. Symbolic regression (SR) using genetic programming (GP) can generate flexible models but is prone to unreliable behaviour in extrapolation. This paper investigates whether adding synthetic data can help improve performance in such cases. We apply Kernel Density Estimation (KDE) to identify regions in the input space where the training data is sparse. Synthetic data is then generated in those regions using a knowledge distillation approach: a teacher model generates predictions on new input points, which are then used to train a student model. We evaluate this method across six benchmark datasets, using neural networks (NN), random forests (RF), and GP both as teacher models (to generate synthetic data) and as student models (trained on the augmented data). Results show that GP models can often improve when trained on synthetic data, especially in extrapolation areas. However, the improvement depends on the dataset and teacher model used. The most important improvements are observed when synthetic data from GPe is used to train GPp in extrapolation regions. Changes in interpolation areas show only slight changes. We also observe heterogeneous errors, where model performance varies across different regions of the input space. Overall, this approach offers a practical solution for better extrapolation. Note: An earlier version of this work appeared in the GECCO 2025 Workshop on Symbolic Regression. This arXiv version corrects several parts of the original submission.

</details>


### [161] [Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence](https://arxiv.org/abs/2511.22813)
*Antoine Salomon*

Main category: cs.LG

TL;DR: INN提出了一种新型神经网络范式，将神经元作为具有内部记忆和学习通信模式的一等实体，通过图结构而非层级组织实现智能计算，在文本建模任务上显著优于Transformer并匹配LSTM性能。


<details>
  <summary>Details</summary>
Motivation: 受生物神经元具有内部状态、选择性通信和自组织成复杂图结构而非刚性层级的特点启发，研究者探索是否能让AI从类似的智能计算单元中涌现。传统神经网络缺乏神经元层面的智能和灵活通信机制。

Method: 提出智能神经网络（INN），每个智能神经元结合选择性状态空间动态（知道何时激活）和基于注意力的路由机制（知道向谁发送信号），在完全图中组织而非顺序层，实现通过图结构交互的涌现计算。

Result: 在Text8字符建模基准测试中，INN达到1.705 BPC，显著优于可比较的Transformer（2.055 BPC），匹配高度优化的LSTM基线。参数匹配的堆叠Mamba块基线在相同训练协议下无法收敛（>3.4 BPC），证明INN的图拓扑提供必要的训练稳定性。消融研究证实去除神经元间通信会降低性能或导致不稳定。

Conclusion: 以神经元为中心的设计与图组织不仅是受生物启发的，而且是计算有效的，为模块化、可解释和可扩展的神经架构开辟了新方向，证明了学习神经路由的价值。

Abstract: Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers.
  Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing.
  This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures.

</details>


### [162] [A Unified and Stable Risk Minimization Framework for Weakly Supervised Learning with Theoretical Guarantees](https://arxiv.org/abs/2511.22823)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出一个统一的弱监督学习框架，通过直接构建稳定的代理风险来避免后处理调整，涵盖多种监督模式，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督学习方法通常针对特定监督模式（如PU、UU、CLL等），需要后处理校正来缓解间接监督带来的不稳定性，缺乏统一的理论框架。

Method: 提出基于弱监督数据结构直接构建稳定代理风险的原则性统一框架，将多种弱监督设置（PU、UU、CLL、PLL等）纳入单一优化目标，并通过Rademacher复杂度建立非渐近泛化界。

Result: 实验表明该方法在不同类别先验、数据集规模和类别数量下均取得一致性能提升，无需启发式稳定化，且对过拟合具有鲁棒性。

Conclusion: 该统一框架为弱监督学习提供了理论基础和实用方法，能够处理多种监督模式，具有理论保证和实际性能优势。

Abstract: Weakly supervised learning has emerged as a practical alternative to fully supervised learning when complete and accurate labels are costly or infeasible to acquire. However, many existing methods are tailored to specific supervision patterns -- such as positive-unlabeled (PU), unlabeled-unlabeled (UU), complementary-label (CLL), partial-label (PLL), or similarity-unlabeled annotations -- and rely on post-hoc corrections to mitigate instability induced by indirect supervision. We propose a principled, unified framework that bypasses such post-hoc adjustments by directly formulating a stable surrogate risk grounded in the structure of weakly supervised data. The formulation naturally subsumes diverse settings -- including PU, UU, CLL, PLL, multi-class unlabeled, and tuple-based learning -- under a single optimization objective. We further establish a non-asymptotic generalization bound via Rademacher complexity that clarifies how supervision structure, model capacity, and sample size jointly govern performance. Beyond this, we analyze the effect of class-prior misspecification on the bound, deriving explicit terms that quantify its impact, and we study identifiability, giving sufficient conditions -- most notably via supervision stratification across groups -- under which the target risk is recoverable. Extensive experiments show consistent gains across class priors, dataset scales, and class counts -- without heuristic stabilization -- while exhibiting robustness to overfitting.

</details>


### [163] [CausalProfiler: Generating Synthetic Benchmarks for Rigorous and Transparent Evaluation of Causal Machine Learning](https://arxiv.org/abs/2511.22842)
*Panayiotis Panayiotou,Audrey Poinsot,Alessandro Leite,Nicolas Chesneau,Marc Schoenauer,Özgür Şimşek*

Main category: cs.LG

TL;DR: CausalProfiler：首个具有覆盖保证和透明假设的随机因果基准生成器，用于在观察、干预和反事实三个层面评估因果机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前因果机器学习（Causal ML）的实证评估实践有限，现有基准通常依赖少量手工制作或半合成数据集，导致结论脆弱且不可泛化，需要更系统、透明的评估框架。

Method: 提出CausalProfiler，一个基于明确设计选择的合成基准生成器，随机采样因果模型、数据、查询和真实值，构建合成因果基准，支持在观察、干预和反事实三个层面进行透明评估。

Result: CausalProfiler能够生成多样化的合成基准，用于评估多种最先进的因果ML方法，包括在识别机制内外的不同条件和假设下，展示了该工具支持的分析类型和洞察。

Conclusion: CausalProfiler填补了因果ML评估的空白，提供了首个具有覆盖保证和透明假设的随机基准生成器，能够支持更严谨、透明的因果方法评估，促进该领域的发展。

Abstract: Causal machine learning (Causal ML) aims to answer "what if" questions using machine learning algorithms, making it a promising tool for high-stakes decision-making. Yet, empirical evaluation practices in Causal ML remain limited. Existing benchmarks often rely on a handful of hand-crafted or semi-synthetic datasets, leading to brittle, non-generalizable conclusions. To bridge this gap, we introduce CausalProfiler, a synthetic benchmark generator for Causal ML methods. Based on a set of explicit design choices about the class of causal models, queries, and data considered, the CausalProfiler randomly samples causal models, data, queries, and ground truths constituting the synthetic causal benchmarks. In this way, Causal ML methods can be rigorously and transparently evaluated under a variety of conditions. This work offers the first random generator of synthetic causal benchmarks with coverage guarantees and transparent assumptions operating on the three levels of causal reasoning: observation, intervention, and counterfactual. We demonstrate its utility by evaluating several state-of-the-art methods under diverse conditions and assumptions, both in and out of the identification regime, illustrating the types of analyses and insights the CausalProfiler enables.

</details>


### [164] [PerfMamba: Performance Analysis and Pruning of Selective State Space Models](https://arxiv.org/abs/2511.22849)
*Abdullah Al Asif,Mobina Kashaniyan,Sixing Yu,Juan Pablo Muñoz,Ali Jannesari*

Main category: cs.LG

TL;DR: 该论文对Mamba-1和Mamba-2选择性状态空间模型进行了全面的性能分析，发现SSM组件消耗大量计算资源，并提出了一种基于低活跃度状态剪枝的优化方法，实现了1.14倍加速和11.50%内存节省。


<details>
  <summary>Details</summary>
Motivation: 选择性SSM作为Transformer的替代架构具有理论计算效率优势，但其运行时行为、资源利用模式和扩展特性尚未得到充分理解，阻碍了最优部署和架构改进。

Method: 对Mamba-1和Mamba-2进行系统性性能分析，研究计算模式、内存访问、I/O特性和扩展特性（序列长度64-16384）。基于SSM组件资源消耗大的发现，提出选择性剪除SSM中低活跃度状态的技术。

Result: SSM组件在Mamba块中消耗大量计算资源。提出的剪枝技术在适度剪枝范围内保持精度的同时，实现了1.14倍加速和11.50%内存使用减少，在不同序列长度下均获得性能提升。

Conclusion: 该研究为设计更高效的SSM架构提供了有价值的指导，可应用于广泛的现实应用场景。通过理解选择性SSM的性能特征并提出优化方法，推动了状态空间模型的实用化发展。

Abstract: Recent advances in sequence modeling have introduced selective SSMs as promising alternatives to Transformer architectures, offering theoretical computational efficiency and sequence processing advantages. A comprehensive understanding of selective SSMs in runtime behavior, resource utilization patterns, and scaling characteristics still remains unexplored, thus obstructing their optimal deployment and further architectural improvements. This paper presents a thorough empirical study of Mamba-1 and Mamba-2, systematically profiled for performance to assess the design principles that contribute to their efficiency in state-space modeling. A detailed analysis of computation patterns, memory access, I/O characteristics, and scaling properties was performed for sequence lengths ranging from 64 to 16384 tokens. Our findings show that the SSM component, a central part of the selective SSM architecture, demands a significant portion of computational resources compared to other components in the Mamba block. Based on these insights, we propose a pruning technique that selectively removes low-activity states within the SSM component, achieving measurable throughput and memory gains while maintaining accuracy within a moderate pruning regime. This approach results in performance improvements across varying sequence lengths, achieving a 1.14x speedup and reducing memory usage by 11.50\%. These results offer valuable guidance for designing more efficient SSM architectures that can be applied to a wide range of real-world applications.

</details>


### [165] [TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE](https://arxiv.org/abs/2511.22853)
*Jiawen Wei,Lan Jiang,Pengbo Wei,Ziwen Ye,Teng Song,Chen Chen,Guangrui Ma*

Main category: cs.LG

TL;DR: TARFVAE：结合Transformer自回归流和变分自编码器的高效一步生成式时间序列预测框架


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法大多采用循环生成操作或重复去噪步骤，预测过程繁琐，尤其在长期预测中效率低下。这些方法通常只在短期预测上进行实验，与确定性方法在长期预测中的比较有限，实际优势不明确。

Method: 提出TARFVAE框架，结合Transformer自回归流(TARFLOW)和变分自编码器(VAE)。TARFLOW模块增强VAE的后验估计，打破高斯假设，实现更丰富的潜在空间表示。仅使用TARFLOW的前向过程，避免自回归逆运算，实现快速生成。从先验潜在空间采样，通过VAE解码器直接生成完整预测序列。

Result: 在基准数据集上，TARFVAE在不同预测范围内均优于最先进的确定性和生成式模型，同时保持高效的预测速度。使用简单的MLP模块即可实现优越性能。

Conclusion: TARFVAE为生成式时间序列预测提供了一个高效且强大的解决方案，通过创新的架构设计解决了现有生成式方法在效率和长期预测方面的局限性。

Abstract: Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting.

</details>


### [166] [CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate](https://arxiv.org/abs/2511.22854)
*Finn G. Vamosi,Nils D. Forkert*

Main category: cs.LG

TL;DR: 提出双智能体辩论框架，通过模型间的批判性对话提升因果推理准确性，在CLadder数据集上显著改进推理性能


<details>
  <summary>Details</summary>
Motivation: 人类因果推理常涉及多个"如果"场景的对比思考，类似地，语言模型可通过多智能体辩论模拟这种内部对话，利用不同视角提升因果推理的准确性和鲁棒性

Method: 采用双智能体辩论框架：一个模型提供结构化因果推理，另一个模型批判性审查逻辑缺陷；当出现分歧时，智能体相互说服、挑战逻辑并修订结论，直至达成共识；使用推理语言模型作为辩论智能体

Result: 在CLadder数据集上，DeepSeek-R1整体准确率从78.03%提升至87.45%，反事实类别从67.94%提升至80.04%；Qwen3整体准确率从84.16%提升至89.41%，反事实问题从71.53%提升至80.35%；强模型仍能从与弱智能体的辩论中显著受益

Conclusion: 推理模型可作为多智能体系统构建块用于因果推理，多样视角对因果问题解决至关重要；辩论框架能有效提升因果推理性能，特别是在反事实推理等复杂任务上

Abstract: When people reason about cause and effect, they often consider many competing "what if" scenarios before deciding which explanation fits best. Analogously, advanced language models capable of causal inference can consider multiple interventions and counterfactuals to judge the validity of causal claims. Crucially, this type of reasoning is less like a single calculation and more like an internal dialogue between alternative hypotheses. In this paper, we make this dialogue explicit through a dual-agent debate framework where one model provides a structured causal inference, and the other critically examines this reasoning for logical flaws. When disagreements arise, agents attempt to persuade each other, challenging each other's logic and revising their conclusions until they converge on a mutually agreed answer. To take advantage of this deliberative process, we specifically use reasoning language models, whose strengths in both causal inference and adversarial debate remain under-explored relative to standard large language models. We evaluate our approach on the CLadder dataset, a benchmark linking natural language questions to formally defined causal graphs across all three rungs of Pearl's ladder of causation. With Qwen3 and DeepSeek-R1 as debater agents, we demonstrate that multi-agent debate improves DeepSeek-R1's overall accuracy in causal inference from 78.03% to 87.45%, with the counterfactual category specifically improving from 67.94% to 80.04% accuracy. Similarly, Qwen3's overall accuracy improves from 84.16% to 89.41%, and counterfactual questions from 71.53% to 80.35%, showing that strong models can still benefit greatly from debate with weaker agents. Our results highlight the potential of reasoning models as building blocks for multi-agent systems in causal inference, and demonstrate the importance of diverse perspectives in causal problem-solving.

</details>


### [167] [Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation](https://arxiv.org/abs/2511.22862)
*Jiacheng Li,Songhe Feng*

Main category: cs.LG

TL;DR: 提出BriMPR框架，通过渐进式重对齐解决多模态测试时适应中的模态耦合问题


<details>
  <summary>Details</summary>
Motivation: 多模态场景中，不同模态的分布偏移程度不同，导致单模态浅层特征偏移和跨模态高层语义错位的复杂耦合效应，阻碍现有TTA方法扩展到多模态领域

Method: 提出BriMPR框架，包含两个渐进增强模块：1) 将MMTTA分解为多个单模态特征对齐子问题，利用提示调优校准单模态全局特征分布；2) 为掩码和完整模态组合分配可信伪标签，引入模态间实例级对比学习增强信息交互

Result: 在基于损坏和真实世界域偏移基准的MMTTA任务上进行了广泛实验，证明了方法的优越性

Conclusion: BriMPR通过渐进式重对齐有效解决了多模态测试时适应中的模态耦合问题，为多模态TTA提供了有效解决方案

Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at [this URL](https://github.com/Luchicken/BriMPR).

</details>


### [168] [ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining](https://arxiv.org/abs/2511.22866)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.LG

TL;DR: ARM-Explainer：一种基于关联规则挖掘的后处理模型级解释器，用于解释图神经网络在组合优化问题中的预测，并在最大团问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前图神经网络在解决图组合优化问题方面已有许多算法，但对其预测的解释方法仍然缺乏。需要开发能够解释GNN预测的方法，以增强模型的可解释性。

Method: 提出ARM-Explainer，一种基于关联规则挖掘的后处理模型级解释器。该方法应用于混合几何散射GNN在最大团问题上的预测，从TWITTER和BHOSLIB-DIMACS基准数据集中发现最具解释性的关联规则。

Result: 发现的八个最具解释性的关联规则在测试实例上获得了中位提升度2.42和中位置信度0.49。解释器识别出影响GNN预测的最重要节点特征及其取值范围。通过增强信息性节点特征，GNN在最大团问题上的性能显著提升，在BHOSLIB-DIMACS数据集的大图上，找到的最大团大小中位数增加了22%（从29.5到36）。

Conclusion: ARM-Explainer能够有效解释GNN在图组合优化问题中的预测，识别关键特征，并且这些解释性特征可以反过来提升GNN的性能，实现了可解释性与性能提升的双重目标。

Abstract: Numerous graph neural network (GNN)-based algorithms have been proposed to solve graph-based combinatorial optimization problems (COPs), but methods to explain their predictions remain largely undeveloped. We introduce ARM-Explainer, a post-hoc, model-level explainer based on association rule mining, and demonstrate it on the predictions of the hybrid geometric scattering (HGS) GNN for the maximum clique problem (MCP), a canonical NP-hard graph-based COP. The eight most explanatory association rules discovered by ARM-Explainer achieve high median lift and confidence values of 2.42 and 0.49, respectively, on test instances from the TWITTER and BHOSLIB-DIMACS benchmark datasets. ARM-Explainer identifies the most important node features, together with their value ranges, that influence the GNN's predictions on these datasets. Furthermore, augmenting the GNN with informative node features substantially improves its performance on the MCP, increasing the median largest-found clique size by 22% (from 29.5 to 36) on large graphs from the BHOSLIB-DIMACS dataset.

</details>


### [169] [Covering-Space Normalizing Flows: Approximating Pushforwards on Lens Spaces](https://arxiv.org/abs/2511.22882)
*William Ghanem*

Main category: cs.LG

TL;DR: 该论文构建了通过通用覆盖映射从S^3到L(p;q)的推前分布，并利用L(p;q)上的流来近似这些分布，特别强调了在对称S^3分布情况下消除冗余性的方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过通用覆盖映射将S^3上的分布推前到透镜空间L(p;q)，并开发有效的方法来近似这些推前分布，特别是在具有对称性的情况下。

Method: 使用通用覆盖映射ρ: S^3 → L(p;q)构建推前分布，利用L(p;q)上的流来近似这些分布，特别设计了消除对称S^3分布中冗余性的方法。

Result: 成功近似了von Mises-Fisher诱导的目标密度推前分布，以及为模拟苯分子构建的Z_12对称玻尔兹曼分布在S^3上的推前分布。

Conclusion: 提出的方法能够有效处理从S^3到透镜空间的推前分布近似问题，特别是在对称分布情况下能够消除冗余性，为复杂分子系统的建模提供了有效工具。

Abstract: We construct pushforward distributions via the universal covering map rho: S^3 -> L(p;q) with the goal of approximating these distributions using flows on L(p;q). We highlight that our method deletes redundancies in the case of a symmetric S^3 distribution. Using our model, we approximate the pushforwards of von Mises-Fisher-induced target densities as well as that of a Z_12-symmetric Boltzmann distribution on S^3 constructed to model benzene.

</details>


### [170] [Modeling Chaotic Pedestrian Behavior Using Chaos Indicators and Supervised Learning](https://arxiv.org/abs/2511.22887)
*Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.LG

TL;DR: 该研究提出一个数据驱动框架，使用监督学习和混沌指标来量化行人运动的不规则性和不可预测性，帮助识别高风险区域和改进基础设施。


<details>
  <summary>Details</summary>
Motivation: 随着城市改善步行性和安全性的需求增加，理解行人行为的不可预测性变得重要。传统方法难以捕捉行人运动的混沌特性，需要数据驱动的方法来量化和预测行为不稳定性。

Method: 通过计算机视觉技术提取白天和夜晚的行人轨迹，使用近似熵和李雅普诺夫指数等四个混沌指标量化行为混沌，通过PCA整合为统一混沌分数。构建个体、群体和交通特征，训练随机森林和CatBoost回归模型。

Result: CatBoost模型表现最佳：白天PCA模型R²=0.8319，夜晚PCA模型R²=0.8574。SHAP分析显示行程距离、运动时长和速度变异性是混沌行为的主要贡献因素。

Conclusion: 该框架能够量化现实环境中的行为不稳定性，帮助规划者识别高风险区域、改进基础设施、校准微观仿真模型，并为自动驾驶系统提供适应性风险评估。

Abstract: As cities around the world aim to improve walkability and safety, understanding the irregular and unpredictable nature of pedestrian behavior has become increasingly important. This study introduces a data-driven framework for modeling chaotic pedestrian movement using empirically observed trajectory data and supervised learning. Videos were recorded during both daytime and nighttime conditions to capture pedestrian dynamics under varying ambient and traffic contexts. Pedestrian trajectories were extracted through computer vision techniques, and behavioral chaos was quantified using four chaos metrics: Approximate Entropy and Lyapunov Exponent, each computed for both velocity and direction change. A Principal Component Analysis (PCA) was then applied to consolidate these indicators into a unified chaos score. A comprehensive set of individual, group-level, and contextual traffic features was engineered and used to train Random Forest and CatBoost regression models. CatBoost models consistently achieved superior performance. The best daytime PCA-based CatBoost model reached an R^2 of 0.8319, while the nighttime PCA-based CatBoost model attained an R^2 of 0.8574. SHAP analysis highlighted that features such as distance travel, movement duration, and speed variability were robust contributors to chaotic behavior. The proposed framework enables practitioners to quantify and anticipate behavioral instability in real-world settings. Planners and engineers can use chaos scores to identify high-risk pedestrian zones, apprise infrastructure improvements, and calibrate realistic microsimulation models. The approach also supports adaptive risk assessment in automated vehicle systems by capturing short-term motion unpredictability grounded in observable, interpretable features.

</details>


### [171] [EnECG: Efficient Ensemble Learning for Electrocardiogram Multi-task Foundation Model](https://arxiv.org/abs/2511.22935)
*Yuhao Xu,Xiaoda Wang,Jiaying Lu,Sirui Ding,Defu Cao,Huaxiu Yao,Yan Liu,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: EnECG是一个基于专家混合的集成学习框架，用于心电图多任务分析，通过轻量级适配策略和LoRA技术降低计算成本，同时保持基础模型的强大表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有心电图分析模型未能充分利用各种心脏异常之间的相互关系，而开发一个能够提取所有相关特征的多任务模型又面临重大挑战。大规模基础模型虽然强大，但通常未在心电图数据上进行预训练，完全重新训练或微调计算成本高昂。

Method: 提出EnECG框架：1）集成多个专门的基础模型，每个模型擅长心电图解释的不同方面；2）采用轻量级适配策略：为每个基础模型附加专用输出层，仅对新添加的参数应用低秩适配（LoRA）；3）使用专家混合（MoE）机制学习集成权重，有效结合各个模型的互补专业知识。

Result: 实验结果表明，通过最小化微调范围，EnECG能够帮助降低计算和内存成本，同时保持基础模型的强大表示能力。该框架不仅增强了特征提取和预测性能，还确保了实际临床应用的效率。

Conclusion: EnECG通过集成多个专门的基础模型和轻量级适配策略，有效解决了心电图多任务分析中的计算成本问题，同时提高了特征提取和预测性能，为实际临床应用提供了实用高效的解决方案。

Abstract: Electrocardiogram (ECG) analysis plays a vital role in the early detection, monitoring, and management of various cardiovascular conditions. While existing models have achieved notable success in ECG interpretation, they fail to leverage the interrelated nature of various cardiac abnormalities. Conversely, developing a specific model capable of extracting all relevant features for multiple ECG tasks remains a significant challenge. Large-scale foundation models, though powerful, are not typically pretrained on ECG data, making full re-training or fine-tuning computationally expensive. To address these challenges, we propose EnECG(Mixture of Experts-based Ensemble Learning for ECG Multi-tasks), an ensemble-based framework that integrates multiple specialized foundation models, each excelling in different aspects of ECG interpretation. Instead of relying on a single model or single task, EnECG leverages the strengths of multiple specialized models to tackle a variety of ECG-based tasks. To mitigate the high computational cost of full re-training or fine-tuning, we introduce a lightweight adaptation strategy: attaching dedicated output layers to each foundation model and applying Low-Rank Adaptation (LoRA) only to these newly added parameters. We then adopt a Mixture of Experts (MoE) mechanism to learn ensemble weights, effectively combining the complementary expertise of individual models. Our experimental results demonstrate that by minimizing the scope of fine-tuning, EnECG can help reduce computational and memory costs while maintaining the strong representational power of foundation models. This framework not only enhances feature extraction and predictive performance but also ensures practical efficiency for real-world clinical applications. The code is available at https://github.com/yuhaoxu99/EnECG.git.

</details>


### [172] [CORGI: GNNs with Convolutional Residual Global Interactions for Lagrangian Simulation](https://arxiv.org/abs/2511.22938)
*Ethan Ji,Yuanzhou Chen,Arush Ramteke,Fang Sun,Tianrun Yu,Jai Parera,Wei Wang,Yizhou Sun*

Main category: cs.LG

TL;DR: CORGI：一种混合架构，通过将粒子特征投影到网格上并使用卷积更新来增强GNN求解器，从而捕捉流体模拟中的全局相互作用，显著提高精度而计算开销很小。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解器在非线性流体动力学中计算成本高，现有的拉格朗日神经代理模型（如GNS和SEGNN）感受野有限，难以捕捉流体流动中固有的全局相互作用。

Method: 提出CORGI混合架构，将任何基于GNN的求解器与轻量级欧拉组件结合：1）将粒子特征投影到网格上；2）应用卷积更新捕捉全局上下文；3）将更新后的特征映射回粒子域。

Result: 在GNS骨干上，CORGI将推演精度提高57%，推理时间仅增加13%，训练时间增加31%；相比SEGNN，精度提高49%，推理时间减少48%，训练时间减少30%；在相同运行时约束下，平均比GNS好47%。

Conclusion: CORGI通过结合拉格朗日和欧拉方法的优势，有效捕捉流体模拟中的长程依赖关系，在不同计算预算下都表现出优越的性能和通用性。

Abstract: Partial differential equations (PDEs) are central to dynamical systems modeling, particularly in hydrodynamics, where traditional solvers often struggle with nonlinearity and computational cost. Lagrangian neural surrogates such as GNS and SEGNN have emerged as strong alternatives by learning from particle-based simulations. However, these models typically operate with limited receptive fields, making them inaccurate for capturing the inherently global interactions in fluid flows. Motivated by this observation, we introduce Convolutional Residual Global Interactions (CORGI), a hybrid architecture that augments any GNN-based solver with a lightweight Eulerian component for global context aggregation. By projecting particle features onto a grid, applying convolutional updates, and mapping them back to the particle domain, CORGI captures long-range dependencies without significant overhead. When applied to a GNS backbone, CORGI achieves a 57% improvement in rollout accuracy with only 13% more inference time and 31% more training time. Compared to SEGNN, CORGI improves accuracy by 49% while reducing inference time by 48% and training time by 30%. Even under identical runtime constraints, CORGI outperforms GNS by 47% on average, highlighting its versatility and performance on varied compute budgets.

</details>


### [173] [Bandit Guided Submodular Curriculum for Adaptive Subset Selection](https://arxiv.org/abs/2511.22944)
*Prateek Chanda,Prayas Agrawal,Saral Sureka,Lokesh Reddy Polu,Atharv Kshirsagar,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出ONLINESUBMOD方法，将自适应子集选择重新表述为多臂老虎机问题，通过在线贪心策略优化效用驱动的奖励，在多种采样机制下实现无遗憾性能。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习从简单到困难样本进行，但定义可靠的难度概念仍然困难。先前工作使用子模函数在课程学习中诱导难度分数，需要更原则性的方法来指导课程安排。

Method: 将自适应子集选择重新表述为多臂老虎机问题，每个臂对应一个指导样本选择的子模函数。提出ONLINESUBMOD在线贪心策略，优化效用驱动的奖励，在多种采样机制下实现无遗憾性能。

Result: ONLINESUBMOD在视觉和语言数据集上优于传统课程学习和双层优化方法，显示出更优的准确率-效率权衡。验证驱动的奖励指标为课程安排提供了原则性指导。

Conclusion: 通过将课程学习重新表述为多臂老虎机问题，ONLINESUBMOD提供了一种原则性的自适应子集选择方法，验证驱动的奖励指标能够有效指导课程安排，在多个领域实现了性能提升。

Abstract: Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce ONLINESUBMOD, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, ONLINESUBMOD outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validationdriven reward metrics offer a principled way to guide the curriculum schedule.

</details>


### [174] [Experts are all you need: A Composable Framework for Large Language Model Inference](https://arxiv.org/abs/2511.22955)
*Shrihari Sridharan,Sourjya Roy,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: Comp-LLM是一个可组合推理框架，通过子查询依赖图实现专家协作，在保持准确性的同时减少模型大小和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：MoE模型需要联合预训练专家且不支持多步推理，而多智能体框架存在显著的顺序处理延迟。需要一种既能实现专家协作又能减少延迟的推理框架。

Method: Comp-LLM包含三个组件：1) 子查询生成器分解输入查询，基于嵌入相似度分配专家，构建依赖图；2) 查询执行器处理图节点，基于依赖和资源约束识别并行机会；3) 响应聚合器合成专家中间响应为最终答案。

Result: 在多个基准测试中，Comp-LLM相比相似大小的单体LLM准确率提升达11.01%，模型大小减少1.67-3.56倍，相比最大模型无显著性能下降，相比顺序子查询处理延迟改善1.1-1.7倍。

Conclusion: Comp-LLM通过可组合推理框架有效解决了MoE和多智能体框架的局限性，在准确性、模型效率和推理延迟方面取得了显著改进，为高效LLM推理提供了新方向。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.

</details>


### [175] [A Trainable Centrality Framework for Modern Data](https://arxiv.org/abs/2511.22959)
*Minh Duc Vu,Mingshuo Liu,Doudou Zhou*

Main category: cs.LG

TL;DR: FUSE是一个神经中心性框架，通过结合全局距离比较和局部密度估计，在任意数据表示上计算中心性得分，适用于高维和非欧几里得数据。


<details>
  <summary>Details</summary>
Motivation: 传统深度概念在高维空间中计算昂贵且不稳定，难以扩展到非欧几里得数据，需要一种更通用、高效的替代方案。

Method: FUSE包含两个模块：全局头通过成对距离比较学习无锚点中心性得分；局部头通过去噪得分匹配近似平滑对数密度势。通过0-1参数插值这两个校准信号。

Result: 在合成分布、真实图像、时间序列、文本数据和标准异常检测基准上，FUSE恢复了有意义的经典排序，揭示了多尺度几何结构，性能与强基线相当且更简单高效。

Conclusion: FUSE提供了一个统一的神经框架，能够在任意数据表示上计算中心性得分，解决了传统方法在高维和非欧几里得数据中的局限性。

Abstract: Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.

</details>


### [176] [A Modular Framework for Rapidly Building Intrusion Predictors](https://arxiv.org/abs/2511.23000)
*Xiaoxuan Wang,Rolf Stadler*

Main category: cs.LG

TL;DR: 提出模块化框架，通过可复用组件快速组装在线攻击预测器，解决传统单体预测器针对特定攻击类型不可扩展的问题


<details>
  <summary>Details</summary>
Motivation: 现有攻击预测器通常针对特定攻击类型构建单体预测器，但MITRE框架包含数百种攻击类型，为每种类型训练单独预测器不可行。需要一种可扩展的解决方案来实时检测攻击并识别攻击阶段。

Method: 提出模块化框架，使用可复用组件快速组装在线攻击预测器。模块化设计便于控制预测的及时性和准确性等关键指标，并调整两者之间的权衡。在训练过程中可以从模块化组件网络中动态组装有效的预测器。

Result: 使用公共数据集进行训练和评估，提供了许多模块化预测器的示例，展示了如何从模块化组件网络中动态组装有效的预测器。

Conclusion: 模块化框架为解决大规模攻击类型预测问题提供了可行方案，能够快速构建可扩展的在线攻击预测系统，同时平衡预测及时性和准确性。

Abstract: We study automated intrusion prediction in an IT system using statistical learning methods. The focus is on developing online attack predictors that detect attacks in real time and identify the current stage of the attack. While such predictors have been proposed in the recent literature, these works typically rely on constructing a monolithic predictor tailored to a specific attack type and scenario. Given that hundreds of attack types are cataloged in the MITRE framework, training a separate monolithic predictor for each of them is infeasible. In this paper, we propose a modular framework for rapidly assembling online attack predictors from reusable components. The modular nature of a predictor facilitates controlling key metrics like timeliness and accuracy of prediction, as well as tuning the trade-off between them. Using public datasets for training and evaluation, we provide many examples of modular predictors and show how an effective predictor can be dynamically assembled during training from a network of modular components.

</details>


### [177] [Masked Diffusion for Generative Recommendation](https://arxiv.org/abs/2511.23021)
*Kulin Shah,Bhuvesh Kumar,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了一种基于掩码扩散的生成式推荐方法，替代传统的自回归模型，通过并行解码提升推理效率，在数据受限和粗粒度召回场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义ID的自回归生成式推荐模型存在推理成本高、训练数据利用效率低、偏向学习短上下文关系等问题。受NLP领域最新突破启发，作者希望开发更高效的序列建模方法。

Method: 提出使用掩码扩散模型来学习用户语义ID序列的概率分布。该方法采用离散掩码噪声促进序列分布学习，将掩码token建模为在未掩码token条件下独立，从而实现并行解码。

Result: 实验表明，该方法在各项指标上持续优于自回归模型，特别是在数据受限设置和粗粒度召回方面表现突出。同时保持了并行预测多个语义ID的灵活性。

Conclusion: 掩码扩散模型为生成式推荐提供了一种更高效的替代方案，解决了自回归模型的局限性，在性能和效率方面都有显著提升。

Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

</details>


### [178] [Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring](https://arxiv.org/abs/2511.23036)
*Changhun Kim,Yechan Mun,Hyeongwon Jang,Eunseo Lee,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: Delta-XAI 提出在线时间序列监控模型解释框架，通过包装函数适配14种现有XAI方法，并引入系统性评估套件。研究发现梯度方法在时序分析中表现优异，进而提出SWING方法，通过整合历史观测来捕捉时序依赖。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列XAI方法大多独立分析每个时间步，忽略了时序依赖关系，导致解释预测变化困难、无法利用在线动态、评估困难等问题。在医疗和金融等敏感领域，需要更好的在线时间序列模型解释方法。

Method: 1) 提出Delta-XAI框架，通过包装函数适配14种现有XAI方法；2) 设计针对在线场景的原则性评估套件，评估忠实性、充分性、一致性等多个方面；3) 基于实验结果提出SWING方法，在积分路径中纳入历史观测来系统捕捉时序依赖并缓解分布外效应。

Result: 实验表明，经典梯度方法（如Integrated Gradients）在适配时序分析后能超越最新方法。SWING方法在多样化设置和多个评估指标下均表现出色，有效捕捉时序依赖关系。

Conclusion: Delta-XAI为在线时间序列监控模型提供了有效的解释框架和评估方法，SWING通过整合历史观测显著提升了时序依赖的捕捉能力，为敏感领域的决策支持提供了更好的可解释性工具。

Abstract: Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.

</details>


### [179] [Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory](https://arxiv.org/abs/2511.23083)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 论文揭示了高容量核Hopfield网络中的"优化脊"现象对应于统计流形上的"稳定性边缘"，即Fisher信息矩阵变得奇异的临界边界。


<details>
  <summary>Details</summary>
Motivation: 高容量核Hopfield网络表现出具有极端稳定性的"优化脊"现象，虽然之前与"谱集中"相关，但其起源仍然不明确。

Method: 在统计流形上分析网络动力学，揭示"优化脊"对应于"稳定性边缘"，即Fisher信息矩阵变得奇异的临界边界。

Result: 表明表观欧几里得力拮抗是黎曼空间中"对偶平衡"的表现，通过最小描述长度原理统一了学习动力学和容量。

Conclusion: 提供了一个自组织临界性的几何理论，将学习动力学和容量通过最小描述长度原理统一起来。

Abstract: High-capacity kernel Hopfield networks exhibit a "Ridge of Optimization" characterized by extreme stability. While previously linked to "Spectral Concentration," its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the "Edge of Stability," a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.

</details>


### [180] [Freeze, Diffuse, Decode: Geometry-Aware Adaptation of Pretrained Transformer Embeddings for Antimicrobial Peptide Design](https://arxiv.org/abs/2511.23120)
*Pankhil Gawade,Adam Izdebski,Myriam Lizotte,Kevin R. Moon,Jake S. Rhodes,Guy Wolf,Ewa Szczurek*

Main category: cs.LG

TL;DR: FDD是一种基于扩散的框架，通过冻结预训练嵌入、扩散监督信号和解码表示，在适应下游任务时保持嵌入的几何结构


<details>
  <summary>Details</summary>
Motivation: 当前迁移策略（微调和探测）要么扭曲预训练嵌入的几何结构，要么缺乏足够的表达能力来捕捉任务相关信号，这在监督数据稀缺时问题更加突出

Method: 提出Freeze, Diffuse, Decode (FDD)框架：冻结预训练嵌入，沿着嵌入的固有流形传播监督信号，实现几何感知的嵌入空间适应

Result: 应用于抗菌肽设计时，FDD产生了低维、可预测且可解释的表示，支持属性预测、检索和潜在空间插值

Conclusion: FDD提供了一种新颖的扩散框架，能够在适应下游任务时保持预训练嵌入的几何结构，特别适用于监督数据稀缺的场景

Abstract: Pretrained transformers provide rich, general-purpose embeddings, which are transferred to downstream tasks. However, current transfer strategies: fine-tuning and probing, either distort the pretrained geometric structure of the embeddings or lack sufficient expressivity to capture task-relevant signals. These issues become even more pronounced when supervised data are scarce. Here, we introduce Freeze, Diffuse, Decode (FDD), a novel diffusion-based framework that adapts pre-trained embeddings to downstream tasks while preserving their underlying geometric structure. FDD propagates supervised signal along the intrinsic manifold of frozen embeddings, enabling a geometry-aware adaptation of the embedding space. Applied to antimicrobial peptide design, FDD yields low-dimensional, predictive, and interpretable representations that support property prediction, retrieval, and latent-space interpolation.

</details>


### [181] [Automated Discovery of Laser Dicing Processes with Bayesian Optimization for Semiconductor Manufacturing](https://arxiv.org/abs/2511.23141)
*David Leeftink,Roman Doll,Heleen Visserman,Marco Post,Faysal Boughorbel,Max Hinne,Marcel van Gerven*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯优化的自动化方法，用于在工业激光切割系统中发现生产就绪的激光切割工艺，显著减少了传统专家调优所需的时间。


<details>
  <summary>Details</summary>
Motivation: 半导体晶圆激光切割是微电子制造的关键步骤，传统上针对新晶圆材料调整复杂的顺序激光切割工艺需要数周的专家努力来平衡加工速度、分离质量和材料完整性。需要一种自动化方法来减少对专家经验的依赖并提高工艺开发效率。

Method: 将问题表述为高维约束多目标贝叶斯优化任务，采用顺序两级保真度策略来最小化昂贵的破坏性芯片强度评估。在工业LASER1205切割系统上实现自动化工艺发现。

Result: 在裸硅和产品晶圆上，该方法自主提供了可行的配置，在生产速度、芯片强度和结构完整性方面匹配或超过了专家基线，仅需技术人员级别的操作。后验分析显示可以从最终代理模型中获得具有不同权衡的多个可行解。

Conclusion: 专家对发现工艺的进一步细化可以在保持芯片强度和结构完整性的同时进一步提高生产速度，超越了纯手动或纯自动方法。该方法为激光切割工艺开发提供了高效、自动化的解决方案。

Abstract: Laser dicing of semiconductor wafers is a critical step in microelectronic manufacturing, where multiple sequential laser passes precisely separate individual dies from the wafer. Adapting this complex sequential process to new wafer materials typically requires weeks of expert effort to balance process speed, separation quality, and material integrity. We present the first automated discovery of production-ready laser dicing processes on an industrial LASER1205 dicing system. We formulate the problem as a high-dimensional, constrained multi-objective Bayesian optimization task, and introduce a sequential two-level fidelity strategy to minimize expensive destructive die-strength evaluations. On bare silicon and product wafers, our method autonomously delivers feasible configurations that match or exceed expert baselines in production speed, die strength, and structural integrity, using only technician-level operation. Post-hoc validation of different weight configurations of the utility functions reveals that multiple feasible solutions with qualitatively different trade-offs can be obtained from the final surrogate model. Expert-refinement of the discovered process can further improve production speed while preserving die strength and structural integrity, surpassing purely manual or automated methods.

</details>


### [182] [Adapting Neural Audio Codecs to EEG](https://arxiv.org/abs/2511.23142)
*Ard Kastrati,Luca Lanzendörfer,Riccardo Rigoni,John Staib Matilla,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 使用预训练的神经音频编解码器（DAC）进行EEG压缩，通过预处理适配EEG数据，无需修改架构即可获得稳定重建，微调后性能更优


<details>
  <summary>Details</summary>
Motivation: EEG和音频是两种不同的模态，但研究发现预训练的神经音频编解码器可以作为EEG压缩的有效起点，这为跨模态知识迁移提供了新思路

Method: 1. 使用DAC作为基础模型，将原始EEG数据映射到编解码器的步进帧结构中；2. 提出DAC-MC多通道扩展，引入基于注意力的跨通道聚合和通道特定解码；3. 系统探索压缩质量权衡，包括残差码本深度、码本大小和输入采样率

Result: 在TUH异常和癫痫数据集上的评估表明，适配后的编解码器保留了临床相关信息，在频谱图重建损失和下游分类准确率方面表现良好

Conclusion: 预训练的神经音频编解码器可以有效地用于EEG压缩，通过适当的预处理和微调，能够实现高质量的EEG重建并保留临床相关信息，为跨模态迁移学习提供了有前景的方向

Abstract: EEG and audio are inherently distinct modalities, differing in sampling rate, channel structure, and scale. Yet, we show that pretrained neural audio codecs can serve as effective starting points for EEG compression, provided that the data are preprocessed to be suitable to the codec's input constraints. Using DAC, a state-of-the-art neural audio codec as our base, we demonstrate that raw EEG can be mapped into the codec's stride-based framing, enabling direct reuse of the audio-pretrained encoder-decoder. Even without modification, this setup yields stable EEG reconstructions, and fine-tuning on EEG data further improves fidelity and generalization compared to training from scratch. We systematically explore compression-quality trade-offs by varying residual codebook depth, codebook (vocabulary) size, and input sampling rate. To capture spatial dependencies across electrodes, we propose DAC-MC, a multi-channel extension with attention-based cross-channel aggregation and channel-specific decoding, while retaining the audio-pretrained initialization. Evaluations on the TUH Abnormal and Epilepsy datasets show that the adapted codecs preserve clinically relevant information, as reflected in spectrogram-based reconstruction loss and downstream classification accuracy.

</details>


### [183] [A Theoretical Framework for Discovering Groups and Unitary Representations via Tensor Factorization](https://arxiv.org/abs/2511.23152)
*Dongsung Huh,Halyun Jeong*

Main category: cs.LG

TL;DR: HyperCube模型通过算子值张量分解发现群结构及其酉表示，其归纳偏倚源于目标函数分解为尺度调节项和方向对齐项，导致优化收敛于共线流形，该流形仅对群同位素可行，并在其中产生向酉性的变分压力。


<details>
  <summary>Details</summary>
Motivation: 为理解HyperCube模型（一种算子值张量分解架构）为何能发现群结构及其酉表示提供严格的理论解释，阐明其内在的归纳偏倚机制。

Method: 将模型目标函数分解为尺度调节项(ℬ)和方向对齐项(ℛ≥0)，分离出共线流形(ℛ=0)，证明该流形仅对群同位素可行，并在其中分析ℬ项产生的变分压力。提出"共线性主导猜想"连接局部与全局景观。

Result: 证明：1) 共线流形仅对群同位素可行；2) 在该流形内，ℬ项产生向酉性的变分压力；3) 在共线性主导条件下，全局最小值由群的酉正则表示实现；4) 非群操作具有严格更高的目标值，量化了模型对群结合结构（直至同位素）的归纳偏倚。

Conclusion: HyperCube模型的归纳偏倚源于目标函数的数学结构，该结构自然地引导优化过程发现群结构及其酉表示，为理解此类张量分解架构的学习机制提供了理论框架。

Abstract: We analyze the HyperCube model, an \textit{operator-valued} tensor factorization architecture that discovers group structures and their unitary representations. We provide a rigorous theoretical explanation for this inductive bias by decomposing its objective into a term regulating factor scales ($\mathcal{B}$) and a term enforcing directional alignment ($\mathcal{R} \geq 0$). This decomposition isolates the \textit{collinear manifold} ($\mathcal{R}=0$), to which numerical optimization consistently converges for group isotopes. We prove that this manifold admits feasible solutions exclusively for group isotopes, and that within it, $\mathcal{B}$ exerts a variational pressure toward unitarity. To bridge the gap to the global landscape, we formulate a \textit{Collinearity Dominance Conjecture}, supported by empirical observations. Conditional on this dominance, we prove two key results: (1) the global minimum is achieved by the unitary regular representation for groups, and (2) non-group operations incur a strictly higher objective value, formally quantifying the model's inductive bias toward the associative structure of groups (up to isotopy).

</details>


### [184] [Estimating the Event-Related Potential from Few EEG Trials](https://arxiv.org/abs/2511.23162)
*Anders Vestergaard Nørskov,Kasper Jørgensen,Alexander Neergaard Zahid,Morten Mørup*

Main category: cs.LG

TL;DR: EEG2ERP：一种不确定性感知的自编码器方法，可将任意数量的EEG试次映射到其相关的ERP，显著减少传统平均方法所需的试次数


<details>
  <summary>Details</summary>
Motivation: 传统ERP估计需要大量EEG试次的平均来降低噪声和信号变异性，这限制了ERP研究的效率和应用。需要开发能够减少所需试次数的新方法。

Method: 提出EEG2ERP，一种不确定性感知的自编码器方法，使用自举训练目标和单独的方差解码器来建模ERP估计的不确定性，支持零样本泛化到新被试。

Result: 在三个公开数据集（ERP CORE、P300 Speller BCI、面部感知神经影像）上验证，在少量试次情况下，EEG2ERP比传统平均和鲁棒平均方法提供显著更好的ERP估计。

Conclusion: EEG2ERP是首个将EEG信号映射到相关ERP的深度学习方法，能够显著减少ERP研究所需的试次数，推动ERP研究向更高效的方向发展。

Abstract: Event-related potentials (ERP) are measurements of brain activity with wide applications in basic and clinical neuroscience, that are typically estimated using the average of many trials of electroencephalography signals (EEG) to sufficiently reduce noise and signal variability. We introduce EEG2ERP, a novel uncertainty-aware autoencoder approach that maps an arbitrary number of EEG trials to their associated ERP. To account for the ERP uncertainty we use bootstrapped training targets and introduce a separate variance decoder to model the uncertainty of the estimated ERP. We evaluate our approach in the challenging zero-shot scenario of generalizing to new subjects considering three different publicly available data sources; i) the comprehensive ERP CORE dataset that includes over 50,000 EEG trials across six ERP paradigms from 40 subjects, ii) the large P300 Speller BCI dataset, and iii) a neuroimaging dataset on face perception consisting of both EEG and magnetoencephalography (MEG) data. We consistently find that our method in the few trial regime provides substantially better ERP estimates than commonly used conventional and robust averaging procedures. EEG2ERP is the first deep learning approach to map EEG signals to their associated ERP, moving toward reducing the number of trials necessary for ERP research. Code is available at https://github.com/andersxa/EEG2ERP

</details>


### [185] [Energy-Efficient Vision Transformer Inference for Edge-AI Deployment](https://arxiv.org/abs/2511.23166)
*Nursultan Amanzhol,Jurn-Gyu Park*

Main category: cs.LG

TL;DR: 提出一个两阶段评估ViT能效的流程，结合设备无关模型筛选和设备相关测量，在边缘设备和移动GPU上测试13个ViT模型，发现混合模型在边缘设备上能效更高，蒸馏模型在移动GPU上表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformers在能耗受限设备上的部署增加，需要超越单一精度评估的综合能效评估方法，以平衡模型性能和能源效率。

Method: 采用两阶段评估流程：第一阶段使用NetScore指标进行设备无关的模型筛选；第二阶段使用可持续准确度指标(SAM)结合设备相关测量进行模型排名。在NVIDIA Jetson TX2和RTX 3050上测试13个ViT模型，数据集包括ImageNet-1K和CIFAR-10。

Result: 混合模型如LeViT_Conv_192在边缘设备上能效最高，相比ViT基线能耗降低53%；蒸馏模型如TinyViT-11M_Distilled在移动GPU上表现最佳，在RTX 3050/CIFAR-10上SAM5=1.72，在RTX 3050/ImageNet-1K上SAM5=0.76。

Conclusion: 不同ViT架构在不同硬件平台上的能效表现存在显著差异，需要针对特定部署场景选择合适的模型架构，混合模型适合边缘设备，蒸馏模型适合移动GPU。

Abstract: The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).

</details>


### [186] [SDE-Attention: Latent Attention in SDE-RNNs for Irregularly Sampled Time Series with Missing Data](https://arxiv.org/abs/2511.23238)
*Yuting Fang,Qouc Le Gia,Flora Salim*

Main category: cs.LG

TL;DR: SDE-Attention：一种用于不规则采样时间序列的SDE-RNN模型，通过通道级注意力机制（包括通道重校准、时变特征注意力和金字塔多尺度自注意力）提升在缺失数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗和传感器网络中常见的不规则采样时间序列通常存在大量缺失观测值，现有方法对此处理不足，需要开发能够有效处理高缺失率时间序列的模型。

Method: 提出SDE-Attention系列模型，在SDE-RNN的潜在前RNN状态上引入三种通道级注意力机制：通道重校准、时变特征注意力（SDE-TVF-L）和金字塔多尺度自注意力，以增强模型对缺失数据的鲁棒性。

Result: 在合成周期数据集和真实世界基准测试中，注意力机制显著提升性能：在UCR单变量数据集上，SDE-TVF-L在30%、60%和90%缺失率下分别比基线平均提升约4%、6%和10%；在UEA多变量基准上，注意力增强模型最高可获得7%的平均准确率提升。

Conclusion: SDE-Attention通过注意力机制有效提升了不规则采样时间序列的处理能力，时变特征注意力在单变量数据集上最稳健，而不同注意力机制在不同多变量任务中各有优势，表明该框架可根据问题结构灵活适配。

Abstract: Irregularly sampled time series with substantial missing observations are common in healthcare and sensor networks. We introduce SDE-Attention, a family of SDE-RNNs equipped with channel-level attention on the latent pre-RNN state, including channel recalibration, time-varying feature attention, and pyramidal multi-scale self-attention. We therefore conduct a comparison on a synthetic periodic dataset and real-world benchmarks, under varying missing rate. Latent-space attention consistently improves over a vanilla SDE-RNN. On the univariate UCR datasets, the LSTM-based time-varying feature model SDE-TVF-L achieves the highest average accuracy, raising mean performance by approximately 4, 6, and 10 percentage points over the baseline at 30%, 60% and 90% missingness, respectively (averaged across datasets). On multivariate UEA benchmarks, attention-augmented models again outperform the backbone, with SDE-TVF-L yielding up to a 7% gain in mean accuracy under high missingness. Among the proposed mechanisms, time-varying feature attention is the most robust on univariate datasets. On multivariate datasets, different attention types excel on different tasks, showing that SDE-Attention can be flexibly adapted to the structure of each problem.

</details>


### [187] [Towards Understanding Transformers in Learning Random Walks](https://arxiv.org/abs/2511.23239)
*Wei Shi,Yuan Cao*

Main category: cs.LG

TL;DR: 本文从理论和实验角度研究了Transformer在学习随机游走模型方面的能力和可解释性，证明了单层Transformer通过梯度下降可以达到最优预测精度，并揭示了其工作机制。


<details>
  <summary>Details</summary>
Motivation: Transformer在各种应用中表现出色，但缺乏清晰的可解释性，其成功在理论上尚未得到充分理解。本文旨在研究Transformer在学习经典统计模型（特别是圆上随机游走）方面的能力和可解释性。

Method: 使用单层Transformer模型，通过梯度下降训练来学习圆上随机游走模型。理论分析证明了模型的最优性和可解释性，并通过实验验证理论发现。

Result: 理论上证明了训练后的单层Transformer可以达到随机游走预测的最优精度。分析显示模型具有可解释性：softmax注意力作为token选择器聚焦于直接父状态，值矩阵基于该父状态执行一步概率转移来预测下一个状态位置。同时识别了理论未覆盖的失败案例。

Conclusion: Transformer能够有效学习随机游走模型并达到最优性能，其工作机制具有可解释性。研究还发现，在某些简单任务中，小初始化的梯度下降可能无法收敛到好的解，这为理解Transformer的局限性提供了理论依据。

Abstract: Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.

</details>


### [188] [Heteroscedastic Neural Networks for Path Loss Prediction with Link-Specific Uncertainty](https://arxiv.org/abs/2511.23243)
*Jonathan Ethier*

Main category: cs.LG

TL;DR: 提出一种神经网络模型，联合预测路径损耗的均值和链路特定方差，实现异方差不确定性估计，在RF驱动测试数据上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统和现代机器学习路径损耗模型通常假设恒定预测方差，无法提供链路特定的不确定性估计，限制了RF规划和干扰分析的准确性。

Method: 设计神经网络通过最小化高斯负对数似然联合预测均值和链路特定方差，比较共享参数、部分共享参数和独立参数三种架构。

Result: 共享参数架构表现最佳：RMSE为7.4 dB，95%预测区间的覆盖率为95.1%，平均区间宽度为29.6 dB，优于传统恒定方差假设。

Conclusion: 该方法提供链路特定不确定性估计，支持链路特定覆盖余量，改善RF规划和干扰分析，并提供有效的模型弱点自诊断能力。

Abstract: Traditional and modern machine learning-based path loss models typically assume a constant prediction variance. We propose a neural network that jointly predicts the mean and link-specific variance by minimizing a Gaussian negative log-likelihood, enabling heteroscedastic uncertainty estimates. We compare shared, partially shared, and independent-parameter architectures using accuracy, calibration, and sharpness metrics on blind test sets from large public RF drive-test datasets. The shared-parameter architecture performs best, achieving an RMSE of 7.4 dB, 95.1 percent coverage for 95 percent prediction intervals, and a mean interval width of 29.6 dB. These uncertainty estimates further support link-specific coverage margins, improve RF planning and interference analyses, and provide effective self-diagnostics of model weaknesses.

</details>


### [189] [Time Series Forecasting via Direct Per-Step Probability Distribution Modeling](https://arxiv.org/abs/2511.23260)
*Linghao Kong,Xiaopeng Hong*

Main category: cs.LG

TL;DR: 提出interPDN模型，通过双分支架构直接构建离散概率分布进行时间序列预测，解决传统神经网络模型难以处理预测不确定性的问题


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络时间序列预测模型直接输出标量值，难以处理预测的不确定性，需要一种能够直接构建概率分布的方法

Method: 提出interleaved dual-branch Probability Distribution Network (interPDN)，使用双分支架构和交错支持集，在每个时间步直接构建离散概率分布而非标量值，通过计算预测分布在预定义支持集上的期望得到回归输出，并引入粗时间尺度分支进行长期趋势预测

Result: 在多个真实世界数据集上的广泛实验表明interPDN具有优越性能

Conclusion: interPDN通过直接构建概率分布和双分支架构有效解决了时间序列预测中的不确定性问题，展示了优越的预测性能

Abstract: Deep neural network-based time series prediction models have recently demonstrated superior capabilities in capturing complex temporal dependencies. However, it is challenging for these models to account for uncertainty associated with their predictions, because they directly output scalar values at each time step. To address such a challenge, we propose a novel model named interleaved dual-branch Probability Distribution Network (interPDN), which directly constructs discrete probability distributions per step instead of a scalar. The regression output at each time step is derived by computing the expectation of the predictive distribution on a predefined support set. To mitigate prediction anomalies, a dual-branch architecture is introduced with interleaved support sets, augmented by coarse temporal-scale branches for long-term trend forecasting. Outputs from another branch are treated as auxiliary signals to impose self-supervised consistency constraints on the current branch's prediction. Extensive experiments on multiple real-world datasets demonstrate the superior performance of interPDN.

</details>


### [190] [An Improved and Generalised Analysis for Spectral Clustering](https://arxiv.org/abs/2511.23261)
*George Tyler,Luca Zanetti*

Main category: cs.LG

TL;DR: 论文重新分析了谱聚类的理论性能，证明只要最小特征值分组良好且与谱的其余部分分离，谱聚类就能有效工作，这适用于存在多尺度层次聚类的情况，并扩展到了有向图的厄米特表示。


<details>
  <summary>Details</summary>
Motivation: 重新审视谱聚类的理论性能，特别是要理解在什么条件下谱聚类能有效工作。传统分析未能捕捉到存在多尺度层次聚类的情况，且需要将谱聚类理论扩展到传统图拉普拉斯矩阵之外，特别是针对有向图的表示。

Method: 采用理论分析方法，研究谱聚类在最小特征值分组良好且与谱的其余部分分离时的性能。将分析扩展到传统图拉普拉斯矩阵之外，研究有向图的厄米特表示，并验证谱聚类在簇间边大多同向时的分区恢复能力。

Result: 证明谱聚类在最小特征值分组良好且与谱的其余部分分离时表现良好，这适用于存在多尺度层次聚类的情况。对于有向图的厄米特表示，谱聚类能恢复簇间边大多同向的分区。理论结果在合成和真实数据集上得到验证。

Conclusion: 谱聚类在最小特征值分组良好且与谱的其余部分分离时表现良好，这一条件比传统分析更通用，适用于多尺度层次聚类和有向图。研究结果为谱聚类在生态网络营养级分析等应用提供了理论基础。

Abstract: We revisit the theoretical performances of Spectral Clustering, a classical algorithm for graph partitioning that relies on the eigenvectors of a matrix representation of the graph. Informally, we show that Spectral Clustering works well as long as the smallest eigenvalues appear in groups well separated from the rest of the matrix representation's spectrum. This arises, for example, whenever there exists a hierarchy of clusters at different scales, a regime not captured by previous analyses. Our results are very general and can be applied beyond the traditional graph Laplacian. In particular, we study Hermitian representations of digraphs and show Spectral Clustering can recover partitions where edges between clusters are oriented mostly in the same direction. This has applications in, for example, the analysis of trophic levels in ecological networks. We demonstrate that our results accurately predict the performances of Spectral Clustering on synthetic and real-world data sets.

</details>


### [191] [BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning](https://arxiv.org/abs/2511.23264)
*Ariful Islam,Md Rifat Hossen,Tanvir Mahmud*

Main category: cs.LG

TL;DR: BanglaSentNet：一个可解释的混合深度学习框架，用于孟加拉语电商评论的多方面情感分析，通过动态加权集成学习结合LSTM、BiLSTM、GRU和BanglaBERT，在8,755条标注数据上达到85%准确率，并提供SHAP特征归因和注意力可视化。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语电商评论的多方面情感分析面临挑战：标注数据集有限、形态复杂性、代码混合现象和领域转移问题，影响3亿孟加拉语用户。现有方法缺乏可解释性和跨领域泛化能力，难以实际部署。

Method: 提出BanglaSentNet框架，集成LSTM、BiLSTM、GRU和BanglaBERT，采用动态加权集成学习进行多方面情感分类。引入8,755条手动标注的孟加拉语产品评论数据集，涵盖质量、服务、价格、装饰四个方面。结合SHAP特征归因和注意力可视化提供可解释性。

Result: 达到85%准确率和0.88 F1分数，优于独立深度学习模型3-7%。可解释性套件获得9.4/10可解释性评分，人类一致性达87.6%。跨领域迁移学习显示：零样本性能在多个领域保持67-76%效果；少样本学习（500-1000样本）达到完整微调性能的90-95%，显著降低标注成本。

Conclusion: 该研究为孟加拉语情感分析建立了新的最先进基准，推进了低资源语言的集成学习方法，并为孟加拉电商平台提供了实用的数据驱动决策解决方案，可用于定价优化、服务改进和客户体验提升。

Abstract: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.

</details>


### [192] [Beyond Curve Fitting: Neuro-Symbolic Agents for Context-Aware Epidemic Forecasting](https://arxiv.org/abs/2511.23276)
*Joongwon Chae,Runming Wang,Chen Xiong,Gong Yunhan,Lian Zhang,Ji Jiansong,Dongmei Yu,Peiwu Qin*

Main category: cs.LG

TL;DR: 提出一个两智能体框架，将上下文解释与概率预测解耦，用于手足口病预测，结合LLM处理学校日程、天气等信号，实现可解释的校准预测。


<details>
  <summary>Details</summary>
Motivation: 传统模型和基础模型虽然考虑协变量，但缺乏语义推理能力来解释冲突驱动因素之间的因果相互作用，需要能结合领域知识进行上下文感知预测的方法。

Method: 两智能体框架：1) LLM"事件解释器"处理学校日程、气象摘要和报告等异构信号，生成标量传播影响信号；2) 神经符号核心结合历史病例数，产生校准的概率预测。

Result: 在香港(2023-2024)和丽水(2024)的真实HFMD数据集上评估，相比传统和基础模型基线，在点预测准确性上具有竞争力，同时提供稳健的90%预测区间(覆盖度0.85-1.00)和人类可解释的推理。

Conclusion: 通过LLM结构化整合领域知识可以匹配最先进性能，同时产生符合公共卫生工作流程的上下文感知预测，为疾病监测提供可解释的预测框架。

Abstract: Effective surveillance of hand, foot and mouth disease (HFMD) requires forecasts accounting for epidemiological patterns and contextual drivers like school calendars and weather. While classical models and recent foundation models (e.g., Chronos, TimesFM) incorporate covariates, they often lack the semantic reasoning to interpret the causal interplay between conflicting drivers. In this work, we propose a two-agent framework decoupling contextual interpretation from probabilistic forecasting. An LLM "event interpreter" processes heterogeneous signals-including school schedules, meteorological summaries, and reports-into a scalar transmission-impact signal. A neuro-symbolic core then combines this with historical case counts to produce calibrated probabilistic forecasts. We evaluate the framework on real-world HFMD datasets from Hong Kong (2023-2024) and Lishui, China (2024). Compared to traditional and foundation-model baselines, our approach achieves competitive point forecasting accuracy while providing robust 90% prediction intervals (coverage 0.85-1.00) and human-interpretable rationales. Our results suggest that structurally integrating domain knowledge through LLMs can match state-of-the-art performance while yielding context-aware forecasts that align with public health workflows. Code is available at https://github.com/jw-chae/forecast_MED .

</details>


### [193] [Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla](https://arxiv.org/abs/2511.23287)
*Ariful Islam,Tanvir Mahmud,Md Rifat Hossen*

Main category: cs.LG

TL;DR: 提出BangACMM框架，通过中间融合策略结合文本和视觉特征，在孟加拉语社交媒体意图分类任务上达到84.11%的宏F1分数，比现有方法提升8.4个百分点。


<details>
  <summary>Details</summary>
Motivation: 社交媒体内容爆炸式增长，作者意图理解对社交媒体内容解读至关重要。现有单模态方法存在局限性，需要利用多模态信息提升孟加拉语社交媒体意图分类性能。

Method: 使用Uddessho数据集（3,048个帖子，6个意图类别），系统评估基于Transformer的语言模型（mBERT、DistilBERT、XLM-RoBERTa）和视觉架构（ViT、Swin、SwiftFormer、ResNet、DenseNet、MobileNet），提出新颖的中间融合策略。

Result: 中间融合策略显著优于早期和晚期融合，特别是mBERT和Swin Transformer组合达到84.11%宏F1分数，比先前孟加拉语多模态方法提升8.4个百分点，创下新SOTA。

Conclusion: 视觉上下文显著增强意图分类，中间层跨模态特征集成在模态特定表示和跨模态学习之间提供最佳平衡，为孟加拉语和其他低资源语言建立了新基准和方法标准。

Abstract: The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).

</details>


### [194] [Machine Learning for Scientific Visualization: Ensemble Data Analysis](https://arxiv.org/abs/2511.23290)
*Hamid Gadirov*

Main category: cs.LG

TL;DR: 该论文提出深度学习方法来分析和可视化时空科学集合数据，包括基于自动编码器的降维、FLINT模型用于流场估计和时间插值，以及HyperFLINT超网络方法实现参数感知适应。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和实验测量产生大量时空数据，但高维度、复杂结构和缺失信息使得提取有意义的洞察具有挑战性。传统分析方法难以处理这些问题，需要更鲁棒的数据驱动方法。

Method: 1) 基于自动编码器的降维方法，评估部分标签下的投影稳定性，使用帕累托最优选择策略；2) FLINT深度学习模型，用于流场估计和时间插值，支持有监督和无监督设置；3) HyperFLINT超网络方法，基于模拟参数进行条件化，实现参数感知适应。

Result: 提出的方法能够生成表达性强且可靠的低维嵌入，重建缺失的流场，生成高保真的时间插值，并在稀疏或不完整数据下实现跨领域的准确重建。

Conclusion: 该论文推进了科学可视化的深度学习技术，为解释复杂时空集合数据提供了可扩展、适应性强且高质量的解决方案。

Abstract: Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.

</details>


### [195] [Hard-Constrained Neural Networks with Physics-Embedded Architecture for Residual Dynamics Learning and Invariant Enforcement in Cyber-Physical Systems](https://arxiv.org/abs/2511.23307)
*Enzo Nicolás Spotorno,Josafat Leal Filho,Antônio Augusto Fröhlich*

Main category: cs.LG

TL;DR: 提出HRPINN和PHRPINN框架，将已知物理知识作为硬约束嵌入循环积分器中学习剩余动力学，并通过预测-投影机制严格强制执行代数不变量，在电池预测和约束基准测试中验证了高精度和数据效率。


<details>
  <summary>Details</summary>
Motivation: 针对具有未知动力学和代数不变量的微分方程控制的复杂信息物理系统，需要一种能够嵌入已知物理知识作为硬约束、同时学习剩余动力学并严格强制执行代数不变量的物理信息学习框架。

Method: 1. 提出HRPINN（混合循环物理信息神经网络），将已知物理作为硬结构约束嵌入循环积分器中学习剩余动力学；2. 提出PHRPINN（投影HRPINN），通过预测-投影机制设计上严格强制执行代数不变量；3. 提供理论分析支持框架的表征能力。

Result: 在真实世界电池预测DAE上验证HRPINN，在标准约束基准测试套件上评估PHRPINN，结果显示框架具有高精度和数据效率潜力，同时揭示了物理一致性、计算成本和数值稳定性之间的关键权衡。

Conclusion: 该框架为复杂信息物理系统提供了一种有效的物理信息学习方法，在保持物理一致性的同时实现了高精度和数据效率，为实际部署提供了实用指导，特别是关于物理一致性、计算成本和数值稳定性之间的权衡。

Abstract: This paper presents a framework for physics-informed learning in complex cyber-physical systems governed by differential equations with both unknown dynamics and algebraic invariants. First, we formalize the Hybrid Recurrent Physics-Informed Neural Network (HRPINN), a general-purpose architecture that embeds known physics as a hard structural constraint within a recurrent integrator to learn only residual dynamics. Second, we introduce the Projected HRPINN (PHRPINN), a novel extension that integrates a predict-project mechanism to strictly enforce algebraic invariants by design. The framework is supported by a theoretical analysis of its representational capacity. We validate HRPINN on a real-world battery prognostics DAE and evaluate PHRPINN on a suite of standard constrained benchmarks. The results demonstrate the framework's potential for achieving high accuracy and data efficiency, while also highlighting critical trade-offs between physical consistency, computational cost, and numerical stability, providing practical guidance for its deployment.

</details>


### [196] [Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.23315)
*Azusa Yamaguchi*

Main category: cs.LG

TL;DR: 论文通过大规模实验发现去中心化多智能体强化学习存在三个相：协调稳定相、脆弱过渡区和堵塞无序相，由核漂移和同步竞争驱动


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解去中心化多智能体强化学习中协调何时出现、波动或崩溃的动态特性，需要更清晰的理论框架

Method: 使用完全独立Q学习作为最小测试平台，在不同环境大小L和智能体密度ρ下进行大规模实验，构建基于合作成功率（CSR）和TD误差方差稳定性指数的相图

Result: 发现三个不同的相：协调稳定相、脆弱过渡区和堵塞无序相，由双不稳定性脊分隔；核漂移（智能体间策略更新引起的有效转移核时变偏移）是关键驱动因素；移除智能体标识符会消除漂移并破坏三相结构

Conclusion: 去中心化MARL表现出由规模、密度和核漂移相互作用支配的相干相结构，表明涌现协调行为是一种分布交互驱动的相现象

Abstract: A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.

</details>


### [197] [ParaGate: Parasitic-Driven Domain Adaptation Transfer Learning for Netlist Performance Prediction](https://arxiv.org/abs/2511.23340)
*Bin Sun,Jingyi Zhou,Jianan Mu,Zhiteng Chao,Tianmeng Yang,Ziyue Xu,Jing Ye,Huawei Li*

Main category: cs.LG

TL;DR: ParaGate是一个三阶段跨阶段预测框架，直接从网表推断布局级时序和功耗，解决传统EDA流程中布局级性能指标只能在布局布线后获取的问题。


<details>
  <summary>Details</summary>
Motivation: 传统EDA流程中，布局级性能指标只能在布局布线后获取，阻碍了早期阶段的全局优化。现有的神经网络解决方案由于商业布局布线工具的黑盒启发式方法导致数据差异大，面临泛化挑战。

Method: 提出ParaGate三阶段框架：1）采用两阶段迁移学习方法预测寄生参数，先在中等规模电路上预训练，再在大型电路上微调以捕捉极端条件；2）依赖EDA工具进行时序分析，卸载长路径数值推理；3）使用子图特征进行全局校准。

Result: ParaGate在openE906上实现了强泛化能力，到达时间R2从0.119提升到0.897，仅需少量微调数据。这表明ParaGate可以为综合和布局阶段的全局优化提供指导。

Conclusion: ParaGate框架能够有效从网表预测布局级性能，解决了传统EDA流程的局限性，为早期设计阶段的全局优化提供了可行方案。

Abstract: In traditional EDA flows, layout-level performance metrics are only obtainable after placement and routing, hindering global optimization at earlier stages. Although some neural-network-based solutions predict layout-level performance directly from netlists, they often face generalization challenges due to the black-box heuristics of commercial placement-and-routing tools, which create disparate data across designs. To this end, we propose ParaGate, a three-step cross-stage prediction framework that infers layout-level timing and power from netlists. First, we propose a two-phase transfer-learning approach to predict parasitic parameters, pre-training on mid-scale circuits and fine-tuning on larger ones to capture extreme conditions. Next, we rely on EDA tools for timing analysis, offloading the long-path numerical reasoning. Finally, ParaGate performs global calibration using subgraph features. Experiments show that ParaGate achieves strong generalization with minimal fine-tuning data: on openE906, its arrival-time R2 from 0.119 to 0.897. These results demonstrate that ParaGate could provide guidance for global optimization in the synthesis and placement stages.

</details>


### [198] [Distributed Dynamic Associative Memory via Online Convex Optimization](https://arxiv.org/abs/2511.23347)
*Bowen Wang,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出分布式动态联想记忆(DDAM)框架，将经典联想记忆扩展到多智能体时变数据流场景，并设计基于树的分布式在线梯度下降算法(DDAM-TOGD)，在静态和动态环境中均提供理论性能保证。


<details>
  <summary>Details</summary>
Motivation: 联想记忆是现代神经网络架构(如Transformer)的关键机制，但现有方法主要针对单智能体静态环境。需要扩展到多智能体动态环境，使每个智能体既能存储本地关联，又能基于兴趣矩阵选择性地记忆其他智能体的信息。

Method: 提出DDAM-TOGD算法：基于树的分布式在线梯度下降方法，智能体通过指定的路由树进行通信，在线更新本地记忆。还设计了组合树优化策略，通过最小化通信延迟来优化路由树结构。

Result: 理论分析证明：在静态环境中获得次线性静态遗憾，在非静态环境中获得路径长度相关的动态遗憾界。数值实验显示DDAM-TOGD在准确性和鲁棒性上优于共识分布式优化等基线方法。

Conclusion: DDAM框架成功将联想记忆扩展到分布式动态环境，DDAM-TOGD算法提供了理论保证和实际性能优势，为多智能体系统中的在线学习和记忆机制提供了新思路。

Abstract: An associative memory (AM) enables cue-response recall, and it has recently been recognized as a key mechanism underlying modern neural architectures such as Transformers. In this work, we introduce the concept of distributed dynamic associative memory (DDAM), which extends classical AM to settings with multiple agents and time-varying data streams. In DDAM, each agent maintains a local AM that must not only store its own associations but also selectively memorize information from other agents based on a specified interest matrix. To address this problem, we propose a novel tree-based distributed online gradient descent algorithm, termed DDAM-TOGD, which enables each agent to update its memory on the fly via inter-agent communication over designated routing trees. We derive rigorous performance guarantees for DDAM-TOGD, proving sublinear static regret in stationary environments and a path-length dependent dynamic regret bound in non-stationary environments. These theoretical results provide insights into how communication delays and network structure impact performance. Building on the regret analysis, we further introduce a combinatorial tree design strategy that optimizes the routing trees to minimize communication delays, thereby improving regret bounds. Numerical experiments demonstrate that the proposed DDAM-TOGD framework achieves superior accuracy and robustness compared to representative online learning baselines such as consensus-based distributed optimization, confirming the benefits of the proposed approach in dynamic, distributed environments.

</details>


### [199] [Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning](https://arxiv.org/abs/2511.23402)
*Jiajun Guo,Xin Luo,Jie Liu*

Main category: cs.LG

TL;DR: 提出一种结合学习型数据压缩的多模态模型结构，将模型嵌入压缩为低比特整数，基于熵编码理论确定最优离散表示级别，显著降低分割学习中的传输成本


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能解决数据隐私问题，但大型基础模型需要传输大量高维数据，导致网络通信成本高昂，这是分割学习的主要障碍

Method: 提出新的多模态模型结构，结合学习型数据压缩方法，将模型嵌入压缩为低比特整数，同时基于熵编码理论确定最优离散表示级别

Result: 在保持模型性能的同时，显著降低了分割学习中分区间的传输成本

Conclusion: 通过结合学习型压缩和熵编码理论，有效解决了分割学习中高网络通信成本的问题，为隐私保护下的分布式模型训练提供了高效解决方案

Abstract: Split learning is well known as a method for resolving data privacy concerns by training a model on distributed devices, thereby avoiding data sharing that raises privacy issues. However, high network communication costs are always an impediment to split learning, especially for large foundation models that require transmitting large amounts of high-dimensional data. To resolve this issue, we present a new multimodal model structure that incorporates a learning-based data compression method, which compresses model embeddings into low-bit integers while preserving the model's performance, greatly reducing the transmission costs between partitions. We then determine the optimal number of discrete representation levels based on a solid theoretical foundation from entropy coding.

</details>


### [200] [ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts](https://arxiv.org/abs/2511.23442)
*Hang Yu,Di Zhang,Qiwei Du,Yanping Zhao,Hai Zhang,Guang Chen,Eduardo E. Veas,Junqiao Zhao*

Main category: cs.LG

TL;DR: ASTRO是一个离线强化学习数据增强框架，通过生成分布新颖且符合动态约束的轨迹来解决次优和碎片化数据集中的奖励传播问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集的数据集中学习最优策略，但次优和碎片化的轨迹导致奖励传播困难，造成价值估计不准确和政策性能下降。现有的轨迹拼接方法要么局限于行为策略的支持范围，要么违反底层动态约束，限制了政策改进效果。

Method: ASTRO首先学习时间距离表示来识别不同且可达的拼接目标，然后采用动态引导的拼接规划器，通过Rollout Deviation Feedback（目标状态序列与实际到达状态序列之间的差距）自适应生成连接动作序列，提高轨迹拼接的可行性和可达性。

Result: ASTRO在各种算法上优于先前的离线RL增强方法，在具有挑战性的OGBench套件上实现了显著的性能提升，并在标准离线RL基准测试（如D4RL）上展示了一致的改进。

Conclusion: ASTRO通过生成分布新颖且符合动态约束的轨迹，有效解决了离线强化学习中的数据增强问题，显著提升了策略学习效果。

Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.

</details>


### [201] [Physics-Informed Neural Networks for Thermophysical Property Retrieval](https://arxiv.org/abs/2511.23449)
*Ali Waseem,Malcolm Mielle*

Main category: cs.LG

TL;DR: 提出基于物理信息神经网络(PINN)的迭代框架，利用热成像图非侵入式估计墙体导热系数k，无需长时间测量且对环境变化鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有测量导热系数的方法要么具有侵入性，要么需要长时间观测，或对环境条件敏感。需要一种能在现场条件下可靠、快速、非侵入式估计材料热物性的方法。

Method: 提出PINN迭代框架：交替进行(1)固定k时用PINN求解正向热问题，(2)通过比较热成像图与PINN预测的表面温度来优化k，重复直至k收敛。

Result: 在不同环境条件和数据采集时间下准确预测k（假设黎明时墙体温度接近稳态）。即使违反稳态假设，最大MAE仅为4.0851，表现稳健。

Conclusion: PINN方法为现场条件下可靠估计材料热物性提供了潜力，无需长时间测量。该工作是机器学习（特别是PINN）解决现场逆问题的起点。

Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.

</details>


### [202] [The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference](https://arxiv.org/abs/2511.23455)
*Hans Gundlach,Jayson Lynch,Matthias Mertens,Neil Thompson*

Main category: cs.LG

TL;DR: 论文分析了AI模型在基准测试上的成本下降趋势，发现前沿模型在知识、推理、数学和软件工程基准上的性能成本每年下降5-10倍，其中算法效率提升贡献约3倍/年。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在高级基准测试上取得巨大进展，但这些进展往往依赖于更昂贵的模型。基准测试可能扭曲了实际能力与成本之间的关系，无法反映每美元的实际进步。需要量化AI基准测试的成本变化趋势。

Method: 使用Artificial Analysis和Epoch AI的数据构建了当前和历史价格的最大数据集，分析不同基准测试（知识、推理、数学、软件工程）的性能成本变化。通过分离开源模型控制竞争效应，并除以硬件价格下降来估计算法效率进步。

Result: 前沿模型在基准测试上的性能成本每年下降5-10倍。算法效率进步约为每年3倍。成本下降主要来自经济力量、硬件效率改进和算法效率改进三方面因素。

Conclusion: 评估者应该公开并考虑基准测试的价格，作为衡量AI实际影响的重要组成部分。AI推理成本的快速下降表明实际能力进步可能比单纯基准分数显示的要快。

Abstract: Language models have seen enormous progress on advanced benchmarks in recent years, but much of this progress has only been possible by using more costly models. Benchmarks may therefore present a warped picture of progress in practical capabilities per dollar. To remedy this, we use data from Artificial Analysis and Epoch AI to form the largest dataset of current and historical prices to run benchmarks to date. We find that the price for a given level of benchmark performance has decreased remarkably fast, around $5\times$ to $10\times$ per year, for frontier models on knowledge, reasoning, math, and software engineering benchmarks. These reductions in the cost of AI inference are due to economic forces, hardware efficiency improvements, and algorithmic efficiency improvements. Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around $3\times$ per year. Finally, we recommend that evaluators both publicize and take into account the price of benchmarking as an essential part of measuring the real-world impact of AI.

</details>


### [203] [SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments](https://arxiv.org/abs/2511.23465)
*Xinyi Li,Zaishuo Xia,Weyl Lu,Chenjie Hao,Yubei Chen*

Main category: cs.LG

TL;DR: SmallWorld Benchmark：用于系统评估世界模型能力的测试平台，在完全可观测状态下对多种架构进行实验，揭示模型捕捉环境结构和长期预测能力


<details>
  <summary>Details</summary>
Motivation: 当前世界模型缺乏统一且受控的系统评估设置，难以判断它们是否真正捕捉了环境动态的底层规则

Method: 引入SmallWorld Benchmark测试平台，在完全可观测状态空间中对循环状态空间模型、Transformer、扩散模型和神经ODE等代表性架构进行综合实验，涵盖六个不同领域

Result: 实验结果显示这些模型捕捉环境结构的有效性，以及它们在长期推演中预测能力的退化情况，突显了当前建模范式的优势和局限

Conclusion: 该基准测试为世界模型评估提供了系统框架，揭示了当前方法的局限性，为表示学习和动态建模的未来改进方向提供了见解

Abstract: Current world models lack a unified and controlled setting for systematic evaluation, making it difficult to assess whether they truly capture the underlying rules that govern environment dynamics. In this work, we address this open challenge by introducing the SmallWorld Benchmark, a testbed designed to assess world model capability under isolated and precisely controlled dynamics without relying on handcrafted reward signals. Using this benchmark, we conduct comprehensive experiments in the fully observable state space on representative architectures including Recurrent State Space Model, Transformer, Diffusion model, and Neural ODE, examining their behavior across six distinct domains. The experimental results reveal how effectively these models capture environment structure and how their predictions deteriorate over extended rollouts, highlighting both the strengths and limitations of current modeling paradigms and offering insights into future improvement directions in representation learning and dynamics modeling.

</details>


### [204] [ThetaEvolve: Test-time Learning on Open Problems](https://arxiv.org/abs/2511.23473)
*Yiping Wang,Shao-Rong Su,Zhiyuan Zeng,Eva Xu,Liliang Ren,Xinyu Yang,Zeyi Huang,Xuehai He,Luyao Ma,Baolin Peng,Hao Cheng,Pengcheng He,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: ThetaEvolve是一个开源的数学发现框架，通过结合上下文学习和强化学习，使小型开源模型能在开放优化问题上取得新的最优边界。


<details>
  <summary>Details</summary>
Motivation: AlphaEvolve等现有系统依赖闭源前沿LLM集成，且是纯推理系统，模型无法内化演化策略。需要开源框架让小型模型也能在数学发现中取得突破。

Method: ThetaEvolve框架采用单一LLM、大型程序数据库增强探索、批量采样提高吞吐、惰性惩罚防止停滞输出、可选奖励塑形稳定训练信号，支持测试时上下文学习和强化学习。

Result: ThetaEvolve首次让小型开源模型（如DeepSeek-R1-0528-Qwen3-8B）在AlphaEvolve提到的开放问题（圆堆积和自相关不等式）上取得新的最优边界。在两个模型四个任务中，测试时RL始终优于纯推理基线。

Conclusion: ThetaEvolve成功简化并扩展了AlphaEvolve，使小型开源模型能够通过持续学习在数学发现中取得突破，且RL训练后的检查点在目标任务和未见任务上都表现更好。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [205] [A lasso-alternative to Dijkstra's algorithm for identifying short paths in networks](https://arxiv.org/abs/2511.22745)
*Anqi Dong,Amirhossein Taghvaei,Tryphon T. Georgiou*

Main category: math.OC

TL;DR: 将最短路径问题重新表述为Lasso回归，建立LARS算法与双向Dijkstra的联系，利用ADMM求解并支持拓扑变化的高效更新


<details>
  <summary>Details</summary>
Motivation: 重新审视图论中最短路径问题，希望通过正则化回归框架提供新的求解视角，并利用优化方法获得更好的计算特性

Method: 将最短路径问题表述为ℓ₁正则化回归（Lasso），使用LARS算法实现数值求解，建立与双向Dijkstra算法的联系，应用ADMM方法求解，并设计拓扑变化的高效更新机制

Result: 建立了最短路径问题与Lasso回归的理论联系，证明了LARS算法与双向Dijkstra的等价性，展示了ADMM在路径查找中的应用潜力，实现了对图拓扑变化的高效适应

Conclusion: 最短路径问题可以通过ℓ₁正则化回归框架重新表述，这一新视角不仅建立了与经典算法的联系，还提供了利用现代优化方法（如ADMM）求解的途径，并支持对动态图的高效处理

Abstract: We revisit the problem of finding the shortest path between two selected vertices of a graph and formulate this as an $\ell_1$-regularized regression -- Least Absolute Shrinkage and Selection Operator (lasso). We draw connections between a numerical implementation of this lasso-formulation, using the so-called LARS algorithm, and a more established algorithm known as the bi-directional Dijkstra. Appealing features of our formulation include the applicability of the Alternating Direction of Multiplier Method (ADMM) to the problem to identify short paths, and a relatively efficient update to topological changes.

</details>


### [206] [On the Condition Number Dependency in Bilevel Optimization](https://arxiv.org/abs/2511.22331)
*Lesi Chen,Jingzhao Zhang*

Main category: math.OC

TL;DR: 本文研究了双层优化问题的oracle复杂度，建立了Ω(κ²ε⁻²)下界和Õ(κ^{7/2}ε⁻²)上界，首次证明了双层问题与极小极大问题在此设置下的复杂度差距。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究在ε依赖上取得了近乎最优的结果，但条件数κ的最优依赖关系仍未知。本文旨在填补这一空白，探究双层优化问题与极小极大问题在复杂度上的本质差异。

Method: 采用理论分析的方法，通过建立新的下界和上界来研究双层优化问题的oracle复杂度。研究涵盖了多种设置：高阶光滑函数、随机oracle和凸超目标问题。

Result: 1. 建立了Ω(κ²ε⁻²)下界和Õ(κ^{7/2}ε⁻²)上界，首次证明双层问题与极小极大问题的复杂度差距
2. 在二阶和任意光滑问题上分别得到Ω(κ_y^{13/4}ε^{-12/7})和Ω(κ^{17/10}ε^{-8/5})下界
3. 对凸-强凸问题将下界从Ω(κ/√ε)改进到Ω(κ^{5/4}/√ε)
4. 对光滑随机问题得到Ω(κ⁴ε⁻⁴)下界

Conclusion: 本文首次建立了双层优化问题与极小极大问题在oracle复杂度上的可证明差距，揭示了条件数依赖关系的最优界限，为理解双层优化的计算复杂性提供了重要理论洞见。

Abstract: Bilevel optimization minimizes an objective function, defined by an upper-level problem whose feasible region is the solution of a lower-level problem. We study the oracle complexity of finding an $ε$-stationary point with first-order methods when the upper-level problem is nonconvex and the lower-level problem is strongly convex. Recent works (Ji et al., ICML 2021; Arbel and Mairal, ICLR 2022; Chen el al., JMLR 2025) achieve a $\tilde{\mathcal{O}}(κ^4 ε^{-2})$ upper bound that is near-optimal in $ε$. However, the optimal dependency on the condition number $κ$ is unknown. In this work, we establish a new $Ω(κ^2 ε^{-2})$ lower bound and $\tilde{\mathcal{O}}(κ^{7/2} ε^{-2})$ upper bound for this problem, establishing the first provable gap between bilevel problems and minimax problems in this setup. Our lower bounds can be extended to various settings, including high-order smooth functions, stochastic oracles, and convex hyper-objectives: (1) For second-order and arbitrarily smooth problems, we show $Ω(κ_y^{13/4} ε^{-12/7})$ and $Ω(κ^{17/10} ε^{-8/5})$ lower bounds, respectively. (2) For convex-strongly-convex problems, we improve the previously best lower bound (Ji and Liang, JMLR 2022) from $Ω(κ/\sqrtε)$ to $Ω(κ^{5/4} / \sqrtε)$. (3) For smooth stochastic problems, we show an $Ω(κ^4 ε^{-4})$ lower bound.

</details>


### [207] [Variational analysis of determinantal varieties](https://arxiv.org/abs/2511.22613)
*Yan Yang,Bin Gao,Ya-xiang Yuan*

Main category: math.OC

TL;DR: 本文开发了统一框架推导低秩集合的一阶和二阶切集公式，涵盖矩阵、张量、对称矩阵和半正定矩阵，并应用于最优性条件分析，证明验证二阶最优性是NP难问题。


<details>
  <summary>Details</summary>
Motivation: 低秩优化中，行列式簇（有界秩矩阵或张量集合）的一阶切锥已有广泛研究，但二阶几何（曲率信息）更为复杂。需要统一框架来推导各类低秩集合的显式切集公式，以支持几何优化方法。

Method: 开发统一框架推导低秩集合的一阶和二阶切集显式公式，包括矩阵、张量、对称矩阵和半正定矩阵。框架还处理低秩集合与其他集合的交集，得到切集交规则。通过切集视角分析非光滑问题与光滑参数化的二阶驻点等价性，并研究法锥图的变分几何。

Result: 获得了各类低秩集合的显式切集公式；建立了非光滑问题与光滑参数化二阶驻点等价性的充要条件；证明了验证二阶最优性是NP难问题；推导了法锥图的Bouligand切锥、Fréchet和Mordukhovich法锥的显式表达式。

Conclusion: 本文的统一框架为低秩优化提供了完整的几何分析工具，涵盖一阶和二阶切集、最优性条件分析，并揭示了二阶最优性验证的计算复杂性。结果可应用于低秩双层规划的最优性条件推导。

Abstract: Determinantal varieties -- the sets of bounded-rank matrices or tensors -- have attracted growing interest in low-rank optimization. The tangent cone to low-rank sets is widely studied and underpins a range of geometric methods. The second-order geometry, which encodes curvature information, is more intricate. In this work, we develop a unified framework to derive explicit formulas for both first- and second-order tangent sets to various low-rank sets, including low-rank matrices, tensors, symmetric matrices, and positive semidefinite matrices. The framework also accommodates the intersection of a low-rank set and another set satisfying mild assumptions, thereby yielding a tangent intersection rule. Through the lens of tangent sets, we establish a necessary and sufficient condition under which a nonsmooth problem and its smooth parameterization share equivalent second-order stationary points. Moreover, we exploit tangent sets to characterize optimality conditions for low-rank optimization and prove that verifying second-order optimality is NP-hard. In a separate line of analysis, we investigate variational geometry of the graph of the normal cone to matrix varieties, deriving the explicit Bouligand tangent cone, Fréchet and Mordukhovich normal cones to the graph. These results are further applied to develop optimality conditions for low-rank bilevel programs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [208] [When Are Reactive Notebooks Not Reactive?](https://arxiv.org/abs/2511.21994)
*Megan Zheng,Will Crichton,Akshay Narayan,Deepti Raghavan,Nikos Vasilakis*

Main category: cs.HC

TL;DR: 提出Rex测试套件来评估和讨论反应式笔记本系统的反应能力，帮助程序员理解反应性何时失效，并帮助系统改进。


<details>
  <summary>Details</summary>
Motivation: 现有的反应式笔记本系统（如Ipyflow、Marimo和Observable）虽然试图通过重新执行最小单元格集来保持状态同步，但各自定义反应性的方式不同，且存在简单修改就会破坏系统的问题，导致用户难以构建对反应式笔记本实现的心理模型。

Method: 提出Rex，一个细粒度的测试套件，用于讨论和评估反应式笔记本系统的反应能力。在三个现有反应式笔记本系统上评估Rex，并对它们的失败进行分类。

Result: 通过Rex测试套件评估了三个反应式笔记本系统，分类了它们的失败情况，揭示了各系统在反应性实现上的不一致性和局限性。

Conclusion: Rex测试套件有助于程序员理解反应性何时失效，并帮助笔记本实现改进，促进反应式笔记本系统的标准化和可靠性提升。

Abstract: Computational notebooks are convenient for programmers, but can easily become confusing and inconsistent due to the ability to incrementally edit a program that is running. Recent reactive notebook systems, such as Ipyflow, Marimo and Observable, strive to keep notebook state in sync with the current cell code by re-executing a minimal set of cells upon modification. However, each system defines reactivity a different way. Additionally, within any definition, we find simple notebook modifications that can break each system. Overall, these inconsistencies make it difficult for users to construct a mental model of their reactive notebook's implementation. This paper proposes Rex, a fine-grained test suite to discuss and assess reactivity capabilities within reactive notebook systems. We evaluate Rex on three existing reactive notebook systems and classify their failures with the aims of (i) helping programmers understand when reactivity fails and (ii) helping notebook implementations improve.

</details>


### [209] [Learning Programming in Informal Spaces: Using Emotion as a Lens to Understand Novice Struggles on r/learnprogramming](https://arxiv.org/abs/2511.22789)
*Alif Al Hasan,Subarna Saha,Mia Mohammad Imran*

Main category: cs.HC

TL;DR: 该研究分析了新手程序员在非正式在线学习环境中的情感体验，发现困惑、好奇和挫败是最常见的情感，主要触发因素包括模糊错误、不明确学习路径和资源不匹配，并提出了五个关键支持领域。


<details>
  <summary>Details</summary>
Motivation: 新手程序员在非正式在线学习环境中面临情感困难，困惑和挫败感会阻碍学习动机和成果。需要了解他们的情感体验、识别情感挣扎的原因，并探索情感感知支持系统的设计机会。

Method: 使用学习中心情感框架手动标注了r/learnprogramming子论坛的1500个帖子，进行了聚类分析和轴向编码，以识别情感模式、触发因素和支持需求。

Result: 困惑、好奇和挫败是最常见的情感，常同时出现并与早期学习阶段相关；积极情感相对较少；主要情感触发因素包括模糊错误、不明确学习路径和资源不匹配；识别出五个关键支持领域。

Conclusion: 研究强调了在非正式学习空间中需要智能、情感敏感的机制，能够根据学习者情感状态提供及时支持，以改善新手程序员的学习体验和成果。

Abstract: Novice programmers experience emotional difficulties in informal online learning environments, where confusion and frustration can hinder motivation and learning outcomes. This study investigates novice programmers' emotional experiences in informal settings, identifies the causes of emotional struggle, and explores design opportunities for affect-aware support systems. We manually annotated 1,500 posts from r/learnprogramming using the Learning-Centered Emotions framework and conducted clustering and axial coding. Confusion, curiosity, and frustration were the most common emotions, often co-occurring and associated with early learning stages. Positive emotions were relatively rare. The primary emotional triggers included ambiguous errors, unclear learning pathways, and misaligned learning resources. We identify five key areas where novice programmers need support in informal learning spaces: stress relief and resilient motivation, topic explanation and resource recommendation, strategic decision-making and learning guidance, technical support, and acknowledgment of their challenges. Our findings highlight the need for intelligent, affect-sensitive mechanisms that provide timely support aligned with learners' emotional states.

</details>


### [210] [MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks](https://arxiv.org/abs/2511.22420)
*Sebe Vanbrabant,Gustavo Rovelo Ruiz,Davy Vanacken*

Main category: cs.HC

TL;DR: 提出MATCH框架，通过结构化构建块和解释性构建块来增强交互式系统中AI模型的可解释性，解决黑盒问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术越来越多地集成到交互式系统中，AI模型的黑盒问题扩展到整个系统层面。现有的可解释AI技术主要关注单个模型的可解释性，但整个系统架构仍然不透明，这限制了人类和对话式XAI方法对系统的理解。

Method: 提出MATCH框架，将交互式系统概念化为结构化构建块的序列，包括AI模型和控制机制。通过补充的解释性构建块（如LIME、SHAP等XAI技术）来解释这些结构块。构建块的流程和API形成明确的系统概览，作为人类和自动化代理的沟通基础。

Result: 提出了一个用于工程化多智能体透明可控人本系统的框架，能够促进可解释性集成到现有交互式系统中，对齐人类和机器对嵌入AI模型的可解释性。

Conclusion: MATCH框架通过结构化方法解决了交互式系统中AI可解释性的系统级挑战，为（对话式）XAI领域做出了贡献，使现有系统更容易集成可解释性。

Abstract: While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [211] [Sparse Multiple Kernel Learning: Alternating Best Response and Semidefinite Relaxations](https://arxiv.org/abs/2511.21890)
*Dimitris Bertsimas,Caio de Prospero Iglesias,Nicholas A. G. Johnson*

Main category: stat.ML

TL;DR: 提出一种稀疏多核学习方法，通过显式基数约束和l2惩罚解决核选择问题，使用交替最优响应算法求解，并通过混合整数半定优化提供最优性证明。


<details>
  <summary>Details</summary>
Motivation: 现有l1正则化方法只能近似稀疏性，需要更直接有效的稀疏多核学习方法，同时保证鲁棒性和可验证的最优性。

Method: 采用显式基数约束和l2惩罚的优化框架，使用交替最优响应算法（alpha子问题用LIBSVM，beta子问题用贪心选择器和单纯形投影器），并建立混合整数半定优化的凸松弛层次结构。

Result: 在10个UCI基准数据集上，随机初始化方法比现有最佳MKL方法平均提升3.34%预测准确率，热启动提升4.05%，且能选择少量核，凸松弛可证明某些情况下获得全局最优解。

Conclusion: 提出的稀疏多核学习方法在预测准确率和核选择稀疏性方面优于现有方法，且通过凸松弛提供了最优性验证机制，具有理论和实践优势。

Abstract: We study Sparse Multiple Kernel Learning (SMKL), which is the problem of selecting a sparse convex combination of prespecified kernels for support vector binary classification. Unlike prevailing l1 regularized approaches that approximate a sparsifying penalty, we formulate the problem by imposing an explicit cardinality constraint on the kernel weights and add an l2 penalty for robustness. We solve the resulting non-convex minimax problem via an alternating best response algorithm with two subproblems: the alpha subproblem is a standard kernel SVM dual solved via LIBSVM, while the beta subproblem admits an efficient solution via the Greedy Selector and Simplex Projector algorithm. We reformulate SMKL as a mixed integer semidefinite optimization problem and derive a hierarchy of semidefinite convex relaxations which can be used to certify near-optimality of the solutions returned by our best response algorithm and also to warm start it. On ten UCI benchmarks, our method with random initialization outperforms state-of-the-art MKL approaches in out-of-sample prediction accuracy on average by 3.34 percentage points (relative to the best performing benchmark) while selecting a small number of candidate kernels in comparable runtime. With warm starting, our method outperforms the best performing benchmark's out-of-sample prediction accuracy on average by 4.05 percentage points. Our convex relaxations provide a certificate that in several cases, the solution returned by our best response algorithm is the globally optimal solution.

</details>


### [212] [Algorithms and Scientific Software for Quasi-Monte Carlo, Fast Gaussian Process Regression, and Scientific Machine Learning](https://arxiv.org/abs/2511.21915)
*Aleksei G. Sorokin*

Main category: stat.ML

TL;DR: 该论文提出了在准蒙特卡洛方法、高斯过程回归和科学机器学习三个领域的算法与软件创新，包括QMCPy、FastGPs等开源工具，以及高精度PDE求解新方法。


<details>
  <summary>Details</summary>
Motivation: 科学计算领域需要高效算法和易用软件，特别是在高维积分、不确定性量化插值和偏微分方程求解方面存在计算效率和精度挑战。

Method: 1) 开发准蒙特卡洛方法：向量化误差估计算法和QMCPy开源Python接口；2) 高斯过程回归：推导数字平移不变核函数，开发快速多任务GP算法和FastGPs软件；3) 科学机器学习：开发能实现机器精度恢复随机系数PDE的新算法。

Result: 成功开发了QMCPy和FastGPs两个开源软件包，提出了多个新算法，并在失效概率估计、达西流方程、辐射传输建模和贝叶斯多级QMC等应用中进行了验证。

Conclusion: 该研究在三个关键科学计算领域提供了算法创新和软件工具，显著提升了高维积分、不确定性量化和PDE求解的计算效率和精度，为科学计算社区贡献了实用资源。

Abstract: Most scientific domains elicit the development of efficient algorithms and accessible scientific software. This thesis unifies our developments in three broad domains: Quasi-Monte Carlo (QMC) methods for efficient high-dimensional integration, Gaussian process (GP) regression for high-dimensional interpolation with built-in uncertainty quantification, and scientific machine learning (sciML) for modeling partial differential equations (PDEs) with mesh-free solvers. For QMC, we built new algorithms for vectorized error estimation and developed QMCPy (https://qmcsoftware.github.io/QMCSoftware/): an open-source Python interface to randomized low-discrepancy sequence generators, automatic variable transforms, adaptive error estimation procedures, and diverse use cases. For GPs, we derived new digitally-shift-invariant kernels of higher-order smoothness, developed novel fast multitask GP algorithms, and produced the scalable Python software FastGPs (https://alegresor.github.io/fastgps/). For sciML, we developed a new algorithm capable of machine precision recovery of PDEs with random coefficients. We have also studied a number of applications including GPs for probability of failure estimation, multilevel GPs for the Darcy flow equation, neural surrogates for modeling radiative transfer, and fast GPs for Bayesian multilevel QMC.

</details>


### [213] [A Sensitivity Approach to Causal Inference Under Limited Overlap](https://arxiv.org/abs/2511.22003)
*Yuanzhe Ma,Hongseok Namkoong*

Main category: stat.ML

TL;DR: 提出一个敏感性分析框架，用于评估在有限重叠情况下标准修剪方法引入的偏差，通过量化最坏情况置信边界来保护研究免受虚假发现的影响。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，处理组和对照组之间有限的重叠是一个关键挑战。标准的修剪权重方法虽然能减少方差，但会引入根本性偏差。需要一种方法来评估在有限重叠情况下研究结果的稳健性。

Method: 提出一个敏感性分析框架，基于最坏情况置信边界来评估标准修剪方法引入的偏差。该方法在明确的假设下，评估结果函数需要多不规则才能使主要发现无效，从而从重叠区域外推反事实估计。

Result: 实证研究表明，该敏感性框架通过量化有限重叠区域的不确定性，能够有效保护研究免受虚假发现的影响。

Conclusion: 该敏感性分析框架为有限重叠情况下的观察性研究提供了实用的工具，能够帮助研究者评估研究结果的稳健性并避免虚假发现。

Abstract: Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.

</details>


### [214] [On the Effect of Regularization on Nonparametric Mean-Variance Regression](https://arxiv.org/abs/2511.22004)
*Eliot Wong-Toi,Alex Boyd,Vincent Fortuin,Stephan Mandt*

Main category: stat.ML

TL;DR: 该论文研究了过参数化均值-方差回归模型中的信号-噪声模糊性问题，发现正则化水平会驱动模型在完美拟合与无信息预测之间的尖锐相变，并提出了统计场论框架来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 过参数化的均值-方差回归模型存在信号-噪声模糊性问题：模型难以决定预测目标应该归因于信号（均值）还是噪声（方差）。这导致模型要么完美拟合所有训练目标（零残差噪声），要么提供恒定无信息的预测并将目标解释为噪声。

Method: 通过实证研究观察不同正则化水平下的相变现象，并开发统计场论框架来解释这种行为。该分析将正则化超参数搜索空间从二维减少到一维，显著降低了计算成本。

Result: 实验观察到正则化驱动的尖锐相变现象，重复运行中表现出显著的变异性。统计场论框架成功捕捉到观察到的相变，与实验结果一致。在UCI数据集和大型ClimSim数据集上的实验展示了稳健的校准性能，有效量化了预测不确定性。

Conclusion: 该研究揭示了过参数化均值-方差模型中信号-噪声模糊性的相变行为，提出的统计场论框架不仅解释了这一现象，还通过减少超参数搜索维度显著提高了计算效率，为不确定性量化提供了有效的解决方案。

Abstract: Uncertainty quantification is vital for decision-making and risk assessment in machine learning. Mean-variance regression models, which predict both a mean and residual noise for each data point, provide a simple approach to uncertainty quantification. However, overparameterized mean-variance models struggle with signal-to-noise ambiguity, deciding whether prediction targets should be attributed to signal (mean) or noise (variance). At one extreme, models fit all training targets perfectly with zero residual noise, while at the other, they provide constant, uninformative predictions and explain the targets as noise. We observe a sharp phase transition between these extremes, driven by model regularization. Empirical studies with varying regularization levels illustrate this transition, revealing substantial variability across repeated runs. To explain this behavior, we develop a statistical field theory framework, which captures the observed phase transition in alignment with experimental results. This analysis reduces the regularization hyperparameter search space from two dimensions to one, significantly lowering computational costs. Experiments on UCI datasets and the large-scale ClimSim dataset demonstrate robust calibration performance, effectively quantifying predictive uncertainty.

</details>


### [215] [Support Vector Machine Classifier with Rescaled Huberized Pinball Loss](https://arxiv.org/abs/2511.22065)
*Shibo Diao*

Main category: stat.ML

TL;DR: 本文提出了一种基于新型重缩放Huber化弹球损失函数的SVM模型（RHPSVM），该模型具有抗异常值、重采样稳定性和高分类精度等优点。


<details>
  <summary>Details</summary>
Motivation: 传统SVM模型对异常值敏感且在重采样时不稳定，这限制了其在实际应用中的性能。需要开发一种更鲁棒、更稳定的SVM变体来解决这些问题。

Method: 提出了一种具有非对称、非凸、平滑特性的重缩放Huber化弹球损失函数，并基于此构建了RHPSVM模型。使用凹-凸过程（CCCP）将非凸优化问题转化为一系列凸子问题，并用ClipDCD算法求解。

Result: 理论分析表明RHPSVM符合贝叶斯规则，具有严格的泛化误差界、有界影响函数和可控的最优性条件。在模拟数据、UCI数据集和小样本作物叶片图像分类任务上的实验显示，RHPSVM在噪声和无噪声场景下均优于现有SVM模型，特别擅长处理高维小样本数据。

Conclusion: RHPSVM通过新型损失函数设计解决了传统SVM的局限性，提供了优异的分类精度、异常值不敏感性和重采样稳定性，且具有可扩展性，能够适应各种高级SVM变体需求。

Abstract: Support vector machines are widely used in machine learning classification tasks, but traditional SVM models suffer from sensitivity to outliers and instability in resampling, which limits their performance in practical applications. To address these issues, this paper proposes a novel rescaled Huberized pinball loss function with asymmetric, non-convex, and smooth properties. Based on this loss function, we develop a corresponding SVM model called RHPSVM (Rescaled Huberized Pinball Loss Support Vector Machine). Theoretical analyses demonstrate that RHPSVM conforms to Bayesian rules, has a strict generalization error bound, a bounded influence function, and controllable optimality conditions, ensuring excellent classification accuracy, outlier insensitivity, and resampling stability. Additionally, RHPSVM can be extended to various advanced SVM variants by adjusting parameters, enhancing its flexibility. We transform the non-convex optimization problem of RHPSVM into a series of convex subproblems using the concave-convex procedure (CCCP) and solve it with the ClipDCD algorithm, which is proven to be convergent. Experimental results on simulated data, UCI datasets, and small-sample crop leaf image classification tasks show that RHPSVM outperforms existing SVM models in both noisy and noise-free scenarios, especially in handling high-dimensional small-sample data.

</details>


### [216] [Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs](https://arxiv.org/abs/2511.22270)
*Zhongjie Shi,Puyu Wang,Chenyang Zhang,Yuan Cao*

Main category: stat.ML

TL;DR: DP-GD（差分隐私梯度下降）在训练两层Huberized ReLU CNN时，在某些条件下比普通GD具有更好的泛化性能，同时提供隐私保护。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练数据常包含敏感信息（如个人联系信息、财务数据、医疗记录），需要开发既能保持良好性能又能保护隐私的训练算法。

Method: 研究差分隐私梯度下降（DP-GD）算法，这是梯度下降（GD）的隐私保护变体，通过在每次迭代中向梯度添加额外噪声来实现差分隐私。

Result: 在训练两层Huberized ReLU卷积神经网络时，DP-GD在某些条件下比GD具有更好的泛化性能。当信噪比较小时，GD可能产生测试精度较差的模型，而DP-GD在信噪比不太小时能产生具有良好测试精度和隐私保证的模型。

Conclusion: DP-GD在某些学习任务中具有在确保隐私保护的同时提升模型性能的潜力，数值模拟进一步支持了理论结果。

Abstract: Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.

</details>


### [217] [UCB for Large-Scale Pure Exploration: Beyond Sub-Gaussianity](https://arxiv.org/abs/2511.22273)
*Zaile Li,Weiwei Fan,L. Jeff Hong*

Main category: stat.ML

TL;DR: 本文研究UCB算法在大规模非次高斯纯探索问题中的性能，证明了在两种非次高斯分布场景下，元UCB算法能达到样本最优性。


<details>
  <summary>Details</summary>
Motivation: 传统纯探索方法主要依赖高斯或次高斯假设，限制了在非次高斯（特别是重尾）问题中的应用。大规模问题对分布假设尤为敏感，需要超越次高斯性的方法。UCB算法在纯探索中广泛应用，但其在非次高斯大规模设置下的性能尚不清楚。

Method: 提出元UCB算法框架，将UCB值定义为样本均值加仅依赖自身样本量的探索奖励。算法停止时选择样本量最大的备选方案作为最优。分析两种非次高斯场景：1) 所有备选方案具有共同位置尺度结构和有界方差；2) 无此结构时，每个备选方案具有q>3阶有界绝对矩。

Result: 推导了元UCB算法正确选择的分布无关下界。在两种非次高斯设置下，证明了元UCB算法及广泛的UCB算法类能达到样本最优性。数值实验支持理论结果，并提供了元UCB框架内外UCB算法比较行为的额外见解。

Conclusion: UCB算法可用于解决大规模非次高斯分布的纯探索问题，在两种非次高斯场景下都能达到样本最优性，扩展了UCB算法在非次高斯设置下的适用性。

Abstract: Selecting the best alternative from a finite set represents a broad class of pure exploration problems. Traditional approaches to pure exploration have predominantly relied on Gaussian or sub-Gaussian assumptions on the performance distributions of all alternatives, which limit their applicability to non-sub-Gaussian especially heavy-tailed problems. The need to move beyond sub-Gaussianity may become even more critical in large-scale problems, which tend to be especially sensitive to distributional specifications. In this paper, motivated by the widespread use of upper confidence bound (UCB) algorithms in pure exploration and beyond, we investigate their performance in the large-scale, non-sub-Gaussian settings. We consider the simplest category of UCB algorithms, where the UCB value for each alternative is defined as the sample mean plus an exploration bonus that depends only on its own sample size. We abstract this into a meta-UCB algorithm and propose letting it select the alternative with the largest sample size as the best upon stopping. For this meta-UCB algorithm, we first derive a distribution-free lower bound on the probability of correct selection. Building on this bound, we analyze two general non-sub-Gaussian scenarios: (1) all alternatives follow a common location-scale structure and have bounded variance; and (2) when such a structure does not hold, each alternative has a bounded absolute moment of order $q > 3$. In both settings, we show that the meta-UCB algorithm and therefore a broad class of UCB algorithms can achieve the sample optimality. These results demonstrate the applicability of UCB algorithms for solving large-scale pure exploration problems with non-sub-Gaussian distributions. Numerical experiments support our results and provide additional insights into the comparative behaviors of UCB algorithms within and beyond our meta-UCB framework.

</details>


### [218] [Data-driven informative priors for Bayesian inference with quasi-periodic data](https://arxiv.org/abs/2511.22296)
*Javier Lopez-Santiago,Luca Martino,Joaquin Miguez,Gonzalo Vazquez-Vilar*

Main category: stat.ML

TL;DR: 提出一种基于高斯过程周期核的经验贝叶斯方法，通过自适应重要性采样近似周期超参数的后验分布，并将其作为先验用于参数模型的贝叶斯推断，以解决周期参数后验高度集中导致的推断效率问题。


<details>
  <summary>Details</summary>
Motivation: 在具有周期性的模型中，周期参数的边际后验分布通常高度集中在参数空间的极小区域内，导致传统贝叶斯计算策略效率低下。需要为推断方法提供更多先验信息来提高效率。

Method: 使用具有周期核的高斯过程拟合数据构建先验分布，通过自适应重要性采样近似高斯过程超参数的后验分布，然后将周期超参数的边际后验分布作为参数模型中周期参数的先验分布，形成模块化的经验贝叶斯工作流。

Result: 方法在合成数据和真实数据上均得到验证，成功近似了高斯过程核的周期后验分布，并将其作为后验-先验传递到参数模型中，显著改善了周期参数的边际后验分布。

Conclusion: 提出的经验贝叶斯方法能够有效构建数据驱动的周期参数先验分布，解决了周期模型贝叶斯推断中的计算效率问题，为周期性数据分析提供了实用的模块化工作流。

Abstract: Bayesian computational strategies for inference can be inefficient in approximating the posterior distribution in models that exhibit some form of periodicity. This is because the probability mass of the marginal posterior distribution of the parameter representing the period is usually highly concentrated in a very small region of the parameter space. Therefore, it is necessary to provide as much information as possible to the inference method through the parameter prior distribution. We intend to show that it is possible to construct a prior distribution from the data by fitting a Gaussian process (GP) with a periodic kernel. More specifically, we want to show that it is possible to approximate the marginal posterior distribution of the hyperparameter corresponding to the period in the kernel. Subsequently, this distribution can be used as a prior distribution for the inference method. We use an adaptive importance sampling method to approximate the posterior distribution of the hyperparameters of the GP. Then, we use the marginal posterior distribution of the hyperparameter related to the periodicity in order to construct a prior distribution for the period of the parametric model. This workflow is empirical Bayes, implemented as a modular (cut) transfer of a GP posterior for the period to the parametric model. We applied the proposed methodology to both synthetic and real data. We approximated the posterior distribution of the period of the GP kernel and then passed it forward as a posterior-as-prior with no feedback. Finally, we analyzed its impact on the marginal posterior distribution.

</details>


### [219] [A PLS-Integrated LASSO Method with Application in Index Tracking](https://arxiv.org/abs/2511.23205)
*Shiqin Tang,Yining Dong,S. Joe Qin*

Main category: stat.ML

TL;DR: 提出PLS-Lasso方法，将降维直接整合到回归过程中，包含两个版本PLS-Lasso-v1和PLS-Lasso-v2，在金融指数追踪任务中表现优于Lasso。


<details>
  <summary>Details</summary>
Motivation: 传统多元数据分析中，降维和回归被视为独立任务。虽然PCR和PLS等方法会计算潜在成分作为中间步骤，但降维和回归过程仍然是分离的。作者希望开发一种将降维直接整合到回归过程中的新方法。

Method: 提出PLS-Lasso方法，包含两个具体实现：PLS-Lasso-v1和PLS-Lasso-v2。该方法将偏最小二乘（PLS）的降维概念与Lasso回归直接整合，并提供了确保收敛到全局最优的清晰有效算法。

Result: 在金融指数追踪任务中，PLS-Lasso-v1和PLS-Lasso-v2与Lasso进行比较，显示出有前景的结果，表明整合降维的回归方法具有优势。

Conclusion: PLS-Lasso是一种创新的回归方法，成功地将降维直接整合到回归过程中，为多元数据分析提供了新的思路，在金融指数追踪等应用中显示出实用价值。

Abstract: In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results.

</details>


### [220] [Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests](https://arxiv.org/abs/2511.23212)
*Tomoshige Nakamura,Hiroshi Shiraishi*

Main category: stat.ML

TL;DR: QRF变量重要性推断存在挑战，本文建立了基于pinball损失风险的渐近理论，揭示了子采样率β控制的"相变"现象：当β≥1/2时（实践中常用的大子样本量），标准推断失效，估计量收敛到确定性偏差常数而非零均值正态分布。


<details>
  <summary>Details</summary>
Motivation: 量化回归森林广泛用于非参数条件分位数估计，但由于损失函数的非光滑性和复杂的偏差-方差权衡，变量重要性度量的统计推断仍然具有挑战性。

Method: 1. 通过Knight恒等式处理不可微的pinball损失，建立QRF估计量的渐近正态性；2. 分析子采样率β控制的"相变"现象；3. 推导渐近偏差的显式解析形式，讨论通过解析偏差校正恢复有效推断的理论可行性。

Result: 1. 建立了QRF估计量的渐近正态性；2. 发现当β≥1/2时（偏差主导机制），标准推断失效，估计量收敛到确定性偏差常数；3. 推导了渐近偏差的显式解析形式，为通过偏差校正恢复有效推断提供了理论基础。

Conclusion: 研究结果揭示了预测性能与推断有效性之间的基本权衡，为理解高维设置中随机森林推断的内在局限性提供了理论基础。实践中常用的大子样本量（β≥1/2）会导致偏差主导机制，使标准推断失效。

Abstract: Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a "phase transition" phenomenon governed by the subsampling rate $β$ (where $s \asymp n^β$). We prove that in the bias-dominated regime ($β\ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.

</details>


### [221] [OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning](https://arxiv.org/abs/2511.23310)
*Zixun Huang,Jiayi Sheng,Zeyu Zheng*

Main category: stat.ML

TL;DR: 提出统一理论框架分析RL后训练中的策略梯度估计器统计特性，基于理论推导开发OBLR-PO算法，在Qwen模型上取得稳定性能提升


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的大语言模型后训练方法主要依赖启发式设计，缺乏系统理论指导，限制了我们对梯度估计器特性和优化算法的理解，从而制约了训练稳定性和性能提升

Method: 建立统一理论框架分析常用策略梯度估计器的统计特性，证明无偏性、推导精确方差表达式、建立优化损失上界；基于理论结果证明收敛保证，推导由梯度信噪比控制的自适应学习率调度；提出方差最优基线是梯度加权估计器；开发OBLR-PO算法联合自适应调整学习率和基线

Result: 在Qwen3-4B-Base和Qwen3-8B-Base模型上的实验表明，OBLR-PO算法相比现有策略优化方法取得一致性能提升，验证了理论贡献能转化为大规模后训练的实际改进

Conclusion: 通过建立统一理论框架，为RL后训练提供了系统性理论指导，提出的OBLR-PO算法基于理论推导实现了学习率和基线的联合自适应调整，在实验中验证了理论的有效性，为提升训练稳定性和性能提供了新途径

Abstract: Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [222] [RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks](https://arxiv.org/abs/2511.22321)
*Tobias Meuser,Jannis Weil,Aninda Lahiri,Marius Paraschiv*

Main category: quant-ph

TL;DR: RELiQ：基于强化学习的量子网络纠缠路由方法，仅依赖本地信息和迭代消息交换，使用图神经网络学习图表示，在随机和真实拓扑中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 量子网络在量子计算和量子传感方面日益重要，但纠缠路由面临量子链路高动态性和量子操作概率性等挑战。手工设计的启发式方法难以优化，特别是在缺乏全局网络拓扑信息时表现不佳。

Method: 提出RELiQ方法：基于强化学习的纠缠路由方法，仅依赖本地信息和迭代消息交换。使用图神经网络学习图表示，避免对特定网络拓扑的过拟合。在随机图上训练，适用于各种网络拓扑。

Result: RELiQ在随机和真实拓扑中一致优于现有的本地信息启发式方法和基于学习的方法。与全局信息启发式方法相比，由于对拓扑变化响应迅速，RELiQ实现了相似或更优的性能。

Conclusion: RELiQ通过强化学习和图神经网络，仅使用本地信息就能实现高效的量子网络纠缠路由，对拓扑变化响应迅速，在多种网络环境中表现优异。

Abstract: Quantum networks are becoming increasingly important because of advancements in quantum computing and quantum sensing, such as recent developments in distributed quantum computing and federated quantum machine learning. Routing entanglement in quantum networks poses several fundamental as well as technical challenges, including the high dynamicity of quantum network links and the probabilistic nature of quantum operations. Consequently, designing hand-crafted heuristics is difficult and often leads to suboptimal performance, especially if global network topology information is unavailable.
  In this paper, we propose RELiQ, a reinforcement learning-based approach to entanglement routing that only relies on local information and iterative message exchange. Utilizing a graph neural network, RELiQ learns graph representations and avoids overfitting to specific network topologies - a prevalent issue for learning-based approaches. Our approach, trained on random graphs, consistently outperforms existing local information heuristics and learning-based approaches when applied to random and real-world topologies. When compared to global information heuristics, our method achieves similar or superior performance because of its rapid response to topology changes.

</details>


### [223] [Accuracy and resource advantages of quantum eigenvalue estimation with non-Hermitian transcorrelated electronic Hamiltonians](https://arxiv.org/abs/2511.21867)
*Alexey Uvarov,Artur F. Izmaylov*

Main category: quant-ph

TL;DR: 量子算法应用于非厄米特transcorrelated哈密顿量，在STO-6G基组下比传统cc-pVQZ基组更准确，T门数相当但量子比特数减少2.5倍


<details>
  <summary>Details</summary>
Motivation: transcorrelated方法可以减少基组尺寸，但其哈密顿量是非厄米特的，使得许多标准量子算法无法直接应用。需要研究如何将量子算法应用于这种非厄米特哈密顿量。

Method: 使用最近提出的针对非厄米特哈密顿量（具有实谱）的量子本征值估计算法，应用于第二行原子的transcorrelated电子哈密顿量，并与标准qubitization方法在非transcorrelated哈密顿量上的成本进行比较。

Result: 在STO-6G基组下，transcorrelated哈密顿量的基态能量比标准哈密顿量在cc-pVQZ基组下更准确。两种方法的T门计数相当，但transcorrelated方法的量子比特数减少了2.5倍。

Conclusion: transcorrelated方法结合非厄米特量子算法可以在保持计算精度的同时显著减少量子资源需求，为电子结构计算提供了有前景的途径。

Abstract: In electronic structure calculations, the transcorrelated method enables a reduction of the basis set size by incorporating the electron-electron correlations directly into the Hamiltonian. However, the transcorrelated Hamiltonian is non-Hermitian, which makes many common quantum algorithms inapplicable. Recently, a quantum eigenvalue estimation algorithm was proposed for non-Hermitian Hamiltonians with real spectra [FOCS 65, 1051 (2024)]. Here we investigate the cost of this algorithm applied to transcorrelated electronic Hamiltonians of second-row atoms and compare it to the cost of applying standard qubitization to non-transcorrelated Hamiltonians. We find that the ground state energy of the transcorrelated Hamiltonian in the STO-6G basis is more accurate than that of a standard Hamiltonian in the cc-pVQZ basis. The T gate counts of the two methods are comparable, while the qubit count of the transcorrelated method is 2.5 times smaller.

</details>


### [224] [Efficient Identification of Permutation Symmetries in Many-Body Hamiltonians via Graph Theory](https://arxiv.org/abs/2511.23160)
*Saumya Shah,Patrick Rebentrost*

Main category: quant-ph

TL;DR: 提出一种通用算法，通过构建彩色二分图来识别任意泡利哈密顿量的完整置换对称群，将对称性发现问题转化为图自同构问题，在局域性和相互作用度有界的情况下具有多项式时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 量子多体系统模拟的计算成本可以通过利用物理对称性来降低。虽然存在针对特定对称类的方法，但缺乏一个通用算法来寻找任意泡利哈密顿量的完整置换对称群。本文旨在填补这一空白。

Method: 建立哈密顿量置换对称群与从哈密顿量构建的彩色二分图的自同构群之间的同构关系。通过形式化证明这一同构，对于具有有界局域性和相互作用度的物理哈密顿量，所得图具有有界度，从而将寻找自同构群的计算问题简化为多项式时间。

Result: 算法在各种已知对称性的物理模型上得到经验验证。进一步证明，判断两个哈密顿量是否置换等价的问题可以通过图表示多项式时间归约到图同构问题。

Conclusion: 这项工作提供了一个通用、结构精确的算法对称性发现工具，使得这些对称性能够可扩展地应用于哈密顿量模拟问题。

Abstract: The computational cost of simulating quantum many-body systems can often be reduced by taking advantage of physical symmetries. While methods exist for specific symmetry classes, a general algorithm to find the full permutation symmetry group of an arbitrary Pauli Hamiltonian is notably lacking. This paper introduces a new method that identifies this symmetry group by establishing an isomorphism between the Hamiltonian's permutation symmetry group and the automorphism group of a coloured bipartite graph constructed from the Hamiltonian. We formally prove this isomorphism and show that for physical Hamiltonians with bounded locality and interaction degree, the resulting graph has a bounded degree, reducing the computational problem of finding the automorphism group to polynomial time. The algorithm's validity is empirically confirmed on various physical models with known symmetries. We further show that the problem of deciding whether two Hamiltonians are permutation-equivalent is polynomial-time reducible to the graph isomorphism problem using our graph representation. This work provides a general, structurally exact tool for algorithmic symmetry finding, enabling the scalable application of these symmetries to Hamiltonian simulation problems.

</details>


### [225] [Nonstabilizerness Estimation using Graph Neural Networks](https://arxiv.org/abs/2511.23224)
*Vincenzo Lipardi,Domenica Dibenedetto,Georgios Stamoulis,Evert van Nieuwenburg,Mark H. M. Winands*

Main category: quant-ph

TL;DR: 提出基于图神经网络的方法来估计量子电路中的非稳定子性，通过分类和回归任务实现稳定子Rényi熵的高效估计，并在不同场景下展示良好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 非稳定子性是实现量子优势的关键资源，而稳定子Rényi熵的高效估计在实际应用中具有重要价值。传统方法在计算上具有挑战性，需要开发更高效的估计技术。

Method: 采用图神经网络方法，将量子电路表示为图结构，通过三种监督学习任务（从简单分类到复杂回归）来估计非稳定子性。图表示自然地整合了硬件特定信息。

Result: GNN能够从图表示中捕获有意义的特征，在分类任务中从积态训练并泛化到Clifford操作、纠缠态和更多量子比特的电路；在回归任务中显著改善了在分布外电路上的SRE估计，包括随机量子电路和横向场伊辛模型的结构化电路。

Conclusion: 提出的GNN方法能够有效估计量子电路的非稳定子性，具有良好的泛化能力，特别是在有噪声量子硬件上的模拟显示了该方法在实际量子设备上预测SRE的潜力。

Abstract: This article proposes a Graph Neural Network (GNN) approach to estimate nonstabilizerness in quantum circuits, measured by the stabilizer Rényi entropy (SRE). Nonstabilizerness is a fundamental resource for quantum advantage, and efficient SRE estimations are highly beneficial in practical applications. We address the nonstabilizerness estimation problem through three supervised learning formulations starting from easier classification tasks to the more challenging regression task. Experimental results show that the proposed GNN manages to capture meaningful features from the graph-based circuit representation, resulting in robust generalization performances achieved across diverse scenarios. In classification tasks, the GNN is trained on product states and generalizes on circuits evolved under Clifford operations, entangled states, and circuits with higher number of qubits. In the regression task, the GNN significantly improves the SRE estimation on out-of-distribution circuits with higher number of qubits and gate counts compared to previous work, for both random quantum circuits and structured circuits derived from the transverse-field Ising model. Moreover, the graph representation of quantum circuits naturally integrates hardware-specific information. Simulations on noisy quantum hardware highlight the potential of the proposed GNN to predict the SRE measured on quantum devices.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [226] [Optical diffraction neural networks assisted computational ghost imaging through dynamic scattering media](https://arxiv.org/abs/2511.22913)
*Yue-Gang Li,Ze Zheng,Jun-jie Wang,Ming He,Jianping Fan,Tailong Xiao,Guihua Zeng*

Main category: physics.optics

TL;DR: 提出一种基于光学衍射神经网络(ODNNs)辅助的鬼成像方法，用于通过动态散射介质成像，通过训练好的固定ODNNs主动校正散射引起的随机畸变。


<details>
  <summary>Details</summary>
Motivation: 传统鬼成像虽然能减少物体与探测器之间的散射干扰，但对光源与物体之间的散射敏感。动态散射介质会引入随机畸变，影响成像质量，需要解决这一问题。

Method: 在实验光路中引入一组固定的光学衍射神经网络(ODNNs)，这些网络在模拟数据集上训练，用于主动校正动态散射介质引起的随机畸变。该方法还可与基于物理先验的重建算法结合。

Result: 通过旋转单层和双层毛玻璃的实验验证了该方法的可行性和有效性。在欠采样条件下也能实现高质量成像。

Conclusion: 提出了一种通过动态散射介质成像的新策略，该方法可扩展到其他成像系统，为解决散射介质成像问题提供了创新方案。

Abstract: Ghost imaging leverages a single-pixel detector with no spatial resolution to acquire object echo intensity signals, which are correlated with illumination patterns to reconstruct an image. This architecture inherently mitigates scattering interference between the object and the detector but sensitive to scattering between the light source and the object. To address this challenge, we propose an optical diffraction neural networks (ODNNs) assisted ghost imaging method for imaging through dynamic scattering media. In our scheme, a set of fixed ODNNs, trained on simulated datasets, is incorporated into the experimental optical path to actively correct random distortions induced by dynamic scattering media. Experimental validation using rotating single-layer and double-layer ground glass confirms the feasibility and effectiveness of our approach. Furthermore, our scheme can also be combined with physics-prior-based reconstruction algorithms, enabling high-quality imaging under undersampled conditions. This work demonstrates a novel strategy for imaging through dynamic scattering media, which can be extended to other imaging systems.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [227] [Constraining dark matter halo profiles with symbolic regression](https://arxiv.org/abs/2511.23073)
*Alicia Martín,Tariq Yasin,Deaglan J. Bartlett,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: 使用符号回归直接从观测数据约束暗物质晕密度分布，无需依赖模拟假设


<details>
  <summary>Details</summary>
Motivation: 传统暗物质晕密度分布模型（如NFW）依赖于模拟假设，但模拟结果受暗物质物理和重子模型不确定性影响。需要一种直接从观测数据推断密度分布的方法，减少对模拟的依赖。

Method: 采用穷举符号回归（ESR）技术，在解析表达式空间中搜索最佳平衡精度和简洁性的函数。使用模拟弱引力透镜数据（ESD）测试方法，考虑不同数据精度（分数误差）和样本量（星团数量）对模型选择的影响。

Result: 当分数误差约为5%时，ESR能从仅20个星团的样本中恢复NFW分布。在当前调查代表性的较高不确定性下，更简单的函数比NFW更受青睐，但NFW仍具竞争力。弱透镜误差在外围最小，导致拟合主要受外部分布主导。

Conclusion: ESR提供了一个稳健、独立于模拟的框架，既能测试质量模型，又能确定晕密度分布的哪些特征真正受到数据约束。

Abstract: Dark matter haloes are typically characterised by radial density profiles with fixed forms motivated by simulations (e.g. NFW). However, simulation predictions depend on uncertain dark matter physics and baryonic modelling. Here, we present a method to constrain halo density profiles directly from observations using Exhaustive Symbolic Regression (ESR), a technique that searches the space of analytic expressions for the function that best balances accuracy and simplicity for a given dataset. We test the approach on mock weak lensing excess surface density (ESD) data of synthetic clusters with NFW profiles. Motivated by real data, we assign each ESD data point a constant fractional uncertainty and vary this uncertainty and the number of clusters to probe how data precision and sample size affect model selection. For fractional errors around 5%, ESR recovers the NFW profile even from samples as small as 20 clusters. At higher uncertainties representative of current surveys, simpler functions are favoured over NFW, though it remains competitive. This preference arises because weak lensing errors are smallest in the outskirts, causing the fits to be dominated by the outer profile. ESR therefore provides a robust, simulation-independent framework both for testing mass models and determining which features of a halo's density profile are genuinely constrained by the data.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [228] [Making an oriented graph acyclic using inversions of bounded or prescribed size](https://arxiv.org/abs/2511.22562)
*Jørgen Bang-Jensen,Frédéric Havet,Florian Hörsch,Clément Rambaud,Amadeus Reinald,Caroline Silva*

Main category: math.CO

TL;DR: 本文研究有向图中的顶点子集反转操作及其复杂性，建立了(=p)-可反转性的多项式时间算法，分析了(=p)-反转数的界，并证明了相关决策问题的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究有向图中通过反转顶点子集来消除环的操作复杂性，探索不同参数p下的可计算性和界函数存在性。

Method: 通过构造多项式时间算法判断(=p)-可反转性（当p≠n-1时），分析(=p)-反转数的上界，研究相关决策问题的计算复杂性（NP-hard、W[1]-hard等）。

Result: 1. 当p≠n-1时，(=p)-可反转性可在多项式时间内判定；2. (=p)-反转数有上界|A(D)|（p≥2）；3. 偶数p时反转数有线性界，奇数p时无界函数；4. 相关决策问题在锦标赛中NP-hard，一般有向图中W[1]-hard。

Conclusion: 有向图的反转操作在不同参数设置下表现出不同的计算复杂性：可判定性问题在大多数情况下可多项式解决，但优化问题通常困难，特别是在锦标赛中。偶数p和奇数p在界函数存在性上存在本质差异。

Abstract: Given an oriented graph $D$, the inversion of a subset $X$ of vertices consists in reversing the orientation of all arcs with both endpoints in $X$. When the subset $X$ is of size $p$ (resp. at most $p$), this operation is called an $(=p)$-inversion (resp. $(\leq p)$-inversion). Then, an oriented graph is $(=p)$-invertible if it can be made acyclic by a sequence of $p$-inversions. We observe that, for $n=|V(D)|$, deciding whether $D$ is $(=n-1)$-invertible is equivalent to deciding whether $D$ is acyclically pushable, and thus NP-complete. In all other cases, when $p \neq n-1$, we construct a polynomial-time algorithm to decide $(=p)$-invertibility.
  We then consider the $(= p)$-inversion number, $\text{inv}^{= p}(D)$ (resp. $(\leq p)$-inversion number, $\text{inv}^{\leq p}(D)$), defined as the minimum number of $(=p)$-inversions (resp. $(\leq p)$-inversions) rendering $D$ acyclic. We show that every $(=p)$-invertible digraph $D$ satisfies $\text{inv}^{= p}(D) \leq |A(D)|$ for every integer $p\geq 2$. When $p$ is even, we bound $\text{inv}^{= p}$ by a (linear) function of the feedback arc set number, and rule out the existence of any bounding function for odd $p$.
  Finally, we study the complexity of deciding whether the $(= p)$-inversion number, or the $(\leq p)$-inversion number, of a given oriented graph is at most a given integer $k$. For any fixed positive integer $p \geq 2$, when $k$ is part of the input, we show that both problems are NP-hard even in tournaments. In general oriented graphs, we prove $W[1]$-hardness for both problems when parameterized by $p$, even for $k=1$. In contrast, we exhibit polynomial kernels in $p + k$ for both problems in tournaments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [229] [LILAD: Learning In-context Lyapunov-stable Adaptive Dynamics Models](https://arxiv.org/abs/2511.21846)
*Amit Jena,Na Li,Le Xie*

Main category: eess.SY

TL;DR: LILAD是一个通过上下文学习联合学习动力学模型和李雅普诺夫函数的系统辨识框架，同时保证适应性和稳定性，即使在分布外场景下也能保持稳定性保证。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在系统辨识中虽然预测准确，但往往无法保持稳定性等关键物理属性，且通常假设静态动力学，限制了在分布偏移下的适用性。现有方法通常单独处理稳定性或适应性，缺乏同时保证两者的统一框架。

Method: LILAD通过上下文学习同时学习动力学模型和李雅普诺夫函数，明确考虑参数不确定性。训练时使用多样化任务集，产生稳定性感知的自适应动力学模型和自适应李雅普诺夫证书。测试时，两个组件通过短轨迹提示适应新系统实例。为确保稳定性，还计算状态依赖衰减器，强制李雅普诺夫函数在新系统实例的任何状态下满足充分递减条件。

Result: 在基准自主系统上的评估表明，LILAD在预测准确性方面优于自适应、鲁棒和非自适应基线方法。

Conclusion: LILAD提供了一个统一框架，能够同时保证系统辨识中的适应性和稳定性，即使在分布外和任务外场景下也能扩展稳定性保证，展示了在预测准确性和物理一致性方面的优越性能。

Abstract: System identification in control theory aims to approximate dynamical systems from trajectory data. While neural networks have demonstrated strong predictive accuracy, they often fail to preserve critical physical properties such as stability and typically assume stationary dynamics, limiting their applicability under distribution shifts. Existing approaches generally address either stability or adaptability in isolation, lacking a unified framework that ensures both. We propose LILAD (Learning In-Context Lyapunov-stable Adaptive Dynamics), a novel framework for system identification that jointly guarantees adaptability and stability. LILAD simultaneously learns a dynamics model and a Lyapunov function through in-context learning (ICL), explicitly accounting for parametric uncertainty. Trained across a diverse set of tasks, LILAD produces a stability-aware, adaptive dynamics model alongside an adaptive Lyapunov certificate. At test time, both components adapt to a new system instance using a short trajectory prompt, which enables fast generalization. To rigorously ensure stability, LILAD also computes a state-dependent attenuator that enforces a sufficient decrease condition on the Lyapunov function for any state in the new system instance. This mechanism extends stability guarantees even under out-of-distribution and out-of-task scenarios. We evaluate LILAD on benchmark autonomous systems and demonstrate that it outperforms adaptive, robust, and non-adaptive baselines in predictive accuracy.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [230] [Maritime Activities Observed Through Open-Access Positioning Data: Moving and Stationary Vessels in the Baltic Sea](https://arxiv.org/abs/2511.23016)
*Moritz Hütten*

Main category: cs.CE

TL;DR: 该研究利用公开AIS数据重建波罗的海船舶活动，开发数据清洗与航程模型，提供高精度船舶计数、交通估计和空间密度图，结果与专有数据研究一致。


<details>
  <summary>Details</summary>
Motivation: 理解海上活动模式对航行安全、环境评估和商业运营至关重要。随着公开AIS数据服务的增加，需要验证利用有限质量和不完整覆盖的公开数据能否准确重建沿海船舶活动。

Method: 使用2024年8-10月波罗的海公开AIS数据，开发数据清洗和重建方法提高数据质量，构建航程模型将AIS消息转换为船舶计数、交通估计和约400米分辨率的空间船舶密度图。

Result: 成功重建波罗的海船舶活动：平均每天有≥4000艘船舶同时运营，每天超过300艘船舶进出该区域。船舶密度图能识别港口位置和最繁忙的沿海区域。结果与基于专有数据的研究在20%误差范围内一致。

Conclusion: 即使数据质量有限且接收器覆盖不完整，也能利用公开AIS数据高精度重建沿海船舶活动。该方法为导航安全、环境评估和商业运营提供了可靠的数据支持。

Abstract: Understanding past and present maritime activity patterns is critical for navigation safety, environmental assessment, and commercial operations. An increasing number of services now openly provide positioning data from the Automatic Identification System (AIS) via ground-based receivers. We show that coastal vessel activity can be reconstructed from open access data with high accuracy, even with limited data quality and incomplete receiver coverage. For three months of open AIS data in the Baltic Sea from August to October 2024, we present (i) cleansing and reconstruction methods to improve the data quality, and (ii) a journey model that converts AIS message data into vessel counts, traffic estimates, and spatially resolved vessel density at a resolution of $\sim$400 m. Vessel counts are provided, along with their uncertainties, for both moving and stationary activity. Vessel density maps also enable the identification of port locations, and we infer the most crowded and busiest coastal areas in the Baltic Sea. We find that on average, $\gtrsim$4000 vessels simultaneously operate in the Baltic Sea, and more than 300 vessels enter or leave the area each day. Our results agree within 20\% with previous studies relying on proprietary data.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [231] [Machine learning for violence prediction: a systematic review and critical appraisal](https://arxiv.org/abs/2511.23118)
*Stefaniya Kozhevnikova,Denis Yukhnenko,Giulio Scola,Seena Fazel*

Main category: stat.ME

TL;DR: 对机器学习暴力行为预测模型的系统综述显示，现有模型在临床应用中实用性有限，存在方法学质量、可解释性和泛化性等问题。


<details>
  <summary>Details</summary>
Motivation: 系统评估机器学习模型在预测暴力行为方面的有效性、实用性和性能，为临床实践和研究提供指导。

Method: 系统检索9个文献数据库和Google Scholar至2025年9月，纳入暴力行为预测机器学习模型的开发和验证研究，通过总结区分度和校准性能统计量，评估研究质量和临床实用性。

Result: 纳入38项研究（40个模型），AUC范围0.68-0.99，仅8项研究报告校准性能，3项进行外部验证。31项研究存在高风险偏倚，主要来自分析领域。整体临床实用性较差，存在样本小、报告不透明、泛化性低等问题。

Conclusion: 黑盒机器学习模型在临床应用中有限，但对识别高风险个体有潜力。建议：1）提高方法学质量和跨学科合作；2）仅对复杂数据使用黑盒算法；3）纳入动态预测；4）开发可解释算法；5）适当应用因果机器学习方法。

Abstract: Purpose To conduct a systematic review of machine learning models for predicting violent behaviour by synthesising and appraising their validity, usefulness, and performance.
  Methods We systematically searched nine bibliographic databases and Google Scholar up to September 2025 for development and/or validation studies on machine learning methods for predicting all forms of violent behaviour. We synthesised the results by summarising discrimination and calibration performance statistics and evaluated study quality by examining risk of bias and clinical utility.
  Results We identified 38 studies reporting the development and validation of 40 models. Most studies reported Area Under the Curve (AUC) as the discrimination statistic with a range of 0.68-0.99. Only eight studies reported calibration performance, and three studies reported external validation. 31 studies had a high risk of bias, mainly in the analysis domain, and three studies had low risk of bias. The overall clinical utility of violence prediction models is poor, as indicated by risks of overfitting due to small samples, lack of transparent reporting, and low generalisability.
  Conclusion Although black box machine learning models currently have limited applicability in clinical settings, they may show promise for identifying high-risk individuals. We recommend five key considerations for violence prediction modelling: (i) ensuring methodological quality (e.g. following guidelines) and interdisciplinary collaborations; (ii) using black box algorithms only for highly complex data; (iii) incorporating dynamic predictions to allow for risk monitoring; (iv) developing more trustworthy algorithms using explainable methods; and (v) applying causal machine learning approaches where appropriate.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [232] [Generative models for crystalline materials](https://arxiv.org/abs/2511.22652)
*Houssam Metni,Laura Ruple,Lauren N. Walters,Luca Torresi,Jonas Teufel,Henrik Schopmans,Jona Östreicher,Yumeng Zhang,Marlen Neubert,Yuri Koide,Kevin Steiner,Paul Link,Lukas Bär,Mariana Petrova,Gerbrand Ceder,Pascal Friederich*

Main category: cond-mat.mtrl-sci

TL;DR: 这篇综述分析了晶体结构预测和从头生成的生成建模现状，涵盖晶体表示、生成模型、评估方法及软件工具，并探讨了新兴研究方向。


<details>
  <summary>Details</summary>
Motivation: 理解材料的结构-性能关系是凝聚态物理和材料科学的基础。机器学习已成为推进这一理解和加速材料发现的有力工具，特别是从早期的大规模筛选转向使用端到端生成模型来生成晶体结构。

Method: 这篇综述采用系统性分析方法：1) 检查晶体表示方法；2) 概述用于设计晶体结构的生成模型；3) 评估各种模型的优缺点；4) 分析生成结构的实验评估考虑；5) 推荐现有软件工具；6) 探索新兴研究方向。

Result: 综述系统性地总结了晶体结构生成建模的当前状态，包括不同表示方法和生成模型的比较分析，提供了实验评估指南和软件工具推荐，并指出了建模无序和缺陷、集成先进表征、纳入合成可行性约束等新兴研究方向。

Conclusion: 这篇综述旨在为实验科学家提供适合其特定情况的ML模型选择指导，同时帮助ML专家理解逆向材料设计和发现中的独特挑战，推动生成建模在晶体结构预测和设计领域的进一步发展。

Abstract: Understanding structure-property relationships in materials is fundamental in condensed matter physics and materials science. Over the past few years, machine learning (ML) has emerged as a powerful tool for advancing this understanding and accelerating materials discovery. Early ML approaches primarily focused on constructing and screening large material spaces to identify promising candidates for various applications. More recently, research efforts have increasingly shifted toward generating crystal structures using end-to-end generative models. This review analyzes the current state of generative modeling for crystal structure prediction and \textit{de novo} generation. It examines crystal representations, outlines the generative models used to design crystal structures, and evaluates their respective strengths and limitations. Furthermore, the review highlights experimental considerations for evaluating generated structures and provides recommendations for suitable existing software tools. Emerging topics, such as modeling disorder and defects, integration in advanced characterization, and incorporating synthetic feasibility constraints, are explored. Ultimately, this work aims to inform both experimental scientists looking to adapt suitable ML models to their specific circumstances and ML specialists seeking to understand the unique challenges related to inverse materials design and discovery.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [233] [$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion](https://arxiv.org/abs/2511.21542)
*Zhihao Zhan,Jiaying Zhou,Likui Zhang,Qinhan Lv,Hao Liu,Jusheng Zhang,Weizheng Li,Ziliang Chen,Tianshui Chen,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: E0是一个基于离散扩散的VLA模型，通过迭代去噪量化动作token来生成精确动作，在14个环境中实现SOTA性能，比基线平均提升10.7%


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在跨任务、场景和视角的泛化能力不足，且生成的动作粗糙不稳定。真实机器人控制由于硬件限制本质上是离散的，而连续扩散策略无法准确建模这种离散分布

Method: 提出E0框架：1) 将动作生成建模为量化动作token上的迭代去噪过程；2) 引入球形视角扰动增强方法提升相机偏移鲁棒性；3) 离散token与预训练VLM/VLA主干对齐，支持更大更细粒度的动作词汇

Result: 在LIBERO、VLABench和ManiSkill的14个多样化环境中实现SOTA，平均比基线提升10.7%。真实世界Franka机械臂评估显示E0能提供精确、鲁棒且可迁移的操控

Conclusion: 离散扩散是通用VLA策略学习的有前景方向，E0通过离散化建模真实机器人控制的量化本质，实现了更强的语义条件化和泛化能力

Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.

</details>


### [234] [Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin](https://arxiv.org/abs/2511.23017)
*Elham Ahmadi,Alireza Olama,Petri Välisuo,Heidi Kuusniemi*

Main category: cs.RO

TL;DR: 提出一种基于因子图的鲁棒GNSS/IMU融合框架，采用Barron损失函数自适应抑制不可靠GNSS测量，在GNSS受限环境中显著提升定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS受限环境（如城市峡谷）中的可靠定位是导航系统的关键挑战。传统的紧耦合GNSS/IMU融合虽然提高了鲁棒性，但仍容易受到非高斯噪声和异常值的影响。

Method: 提出基于因子图的鲁棒自适应融合框架：1）直接集成GNSS伪距测量与IMU预积分因子；2）引入Barron损失函数（统一多种M估计器的通用鲁棒损失函数），通过可调参数自适应降低不可靠GNSS测量的权重；3）在扩展的GTSAM框架中实现。

Result: 在UrbanNav数据集上评估：相比标准因子图优化（FGO）定位误差减少高达41%；相比扩展卡尔曼滤波（EKF）基线，在城市峡谷环境中改进更为显著。

Conclusion: Barron损失函数能有效增强GNSS/IMU导航在城区和信号受限环境中的鲁棒性，提出的框架为GNSS挑战环境提供了更可靠的定位解决方案。

Abstract: Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.

</details>


### [235] [Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging](https://arxiv.org/abs/2511.23193)
*Yuchen Shi,Huaxin Pei,Yi Zhang,Danya Yao*

Main category: cs.RO

TL;DR: 提出一种针对自动驾驶车辆协同驾驶的容错多智能体强化学习方法，通过对抗故障注入和自诊断机制提升系统对观测故障的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在协同驾驶中应用潜力大，但实际部署受限于观测故障的容错能力不足。观测故障表现为车辆感知数据的扰动，会严重影响MARL驾驶系统的性能。

Method: 提出包含两个关键智能体的容错MARL方法：1) 对抗故障注入智能体，通过协同训练生成扰动来挑战和强化车辆策略；2) 容错车辆智能体，具备自诊断能力，利用车辆状态序列的时空相关性检测故障并重建可信观测。

Result: 在模拟高速公路汇入场景中，该方法显著优于基线MARL方法，在各种观测故障模式下实现了接近无故障水平的安全性和效率。

Conclusion: 提出的容错MARL方法有效解决了协同驾驶中观测故障的鲁棒性问题，通过对抗训练和自诊断机制提升了自动驾驶车辆在现实环境中的可靠性。

Abstract: Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [236] [Invited to Develop: Institutional Belonging and the Counterfactual Architecture of Development](https://arxiv.org/abs/2511.21865)
*Diego Vallarino*

Main category: econ.GN

TL;DR: 本文通过西班牙和乌拉圭的对比研究，探讨制度归属如何影响长期发展，发现国家在跨国制度网络中的结构性位置对发展路径有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解为什么具有相似历史禀赋的两个小型民主国家（西班牙和乌拉圭）在1960年代后发展轨迹出现显著分歧。作者希望探究制度嵌入如何塑造长期发展路径，特别是跨国制度网络的作用。

Method: 开发了一个基于经济复杂性、制度路径依赖和Wasserstein GAN的生成性反事实框架，使用1960-2020年的数据训练，通过"预期发展偏移"（EDS）量化不同制度生态系统嵌入带来的结构性收益或损失。

Result: 反事实模拟显示：西班牙若处于拉美制度配置下会经历显著发展衰退，而乌拉圭若嵌入欧洲制度体系则会获得更高的经济复杂性和韧性。这表明发展不仅取决于国内改革，更取决于国家在跨国制度网络中的结构性位置。

Conclusion: 长期发展不仅由国内制度决定，更受国家在跨国制度网络中的结构性位置影响。制度嵌入在塑造发展路径中起关键作用，这为理解发展差异提供了新的制度网络视角。

Abstract: This paper examines how institutional belonging shapes long-term development by comparing Spain and Uruguay, two small democracies with similar historical endowments whose trajectories diverged sharply after the 1960s. While Spain integrated into dense European institutional architectures, Uruguay remained embedded within the Latin American governance regime, characterized by weaker coordination and lower institutional coherence. To assess how alternative institutional embeddings could have altered these paths, the study develops a generative counterfactual framework grounded in economic complexity, institutional path dependence, and a Wasserstein GAN trained on data from 1960-2020. The resulting Expected Developmental Shift (EDS) quantifies structural gains or losses from hypothetical re-embedding in different institutional ecosystems. Counterfactual simulations indicate that Spain would have experienced significant developmental decline under a Latin American configuration, while Uruguay would have achieved higher complexity and resilience within a European regime. These findings suggest that development is not solely determined by domestic reforms but emerges from a country's structural position within transnational institutional networks.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [237] [Resolving Sharp Gradients of Unstable Singularities to Machine Precision via Neural Networks](https://arxiv.org/abs/2511.22819)
*Yongji Wang,Tristan Léger,Ching-Yao Lai,Tristan Buckmaster*

Main category: math.AP

TL;DR: 提出梯度归一化PDE残差重加权方案，结合多阶段神经网络架构，解决高梯度不稳定自相似奇点的高精度计算难题，发现新的高度不稳定奇点。


<details>
  <summary>Details</summary>
Motivation: 现有计算框架在处理具有极端梯度的高度不稳定自相似解时精度不足，主要障碍是尖锐梯度导致训练时产生大的局部PDE残差，阻碍收敛并掩盖原点附近识别自相似缩放参数λ所需的微妙信号。

Method: 引入梯度归一化PDE残差重加权方案，解决高梯度挑战同时放大原点处关键残差信号用于λ识别；结合多阶段神经网络架构，将PDE残差降低到舍入误差水平。

Result: 方法成功将PDE残差降低到舍入误差水平，覆盖之前发现的各种不稳定自相似奇点；发现了新的高度不稳定奇点：IPM方程的第4不稳定解和NLS方程的新高度不稳定孤子族。

Conclusion: 实现了高梯度解的高精度计算，为连接数值发现与计算机辅助证明非线性PDE中不稳定现象提供了重要工具。

Abstract: Recent work introduced a robust computational framework combining embedded mathematical structures, advanced optimization, and neural network architecture, leading to the discovery of multiple unstable self-similar solutions for key fluid dynamics equations, including the Incompressible Porous Media (IPM) and 2D Boussinesq systems. While this framework confirmed the existence of these singularities, an accuracy level approaching double-float machine precision was only achieved for stable and 1st unstable solutions of the 1D Córdoba-Córdoba-Fontelos model. For highly unstable solutions characterized by extreme gradients, the accuracy remained insufficient for validation. The primary obstacle is the presence of sharp solution gradients. Those gradients tend to induce large, localized PDE residuals during training, which not only hinder convergence, but also obscure the subtle signals near the origin required to identify the correct self-similar scaling parameter lambda of the solutions. In this work, we introduce a gradient-normalized PDE residual re-weighting scheme to resolve the high-gradient challenge while amplifying the critical residual signals at the origin for lambda identification. Coupled with the multi-stage neural network architecture, the PDE residuals are reduced to the level of round-off error across a wide spectrum of unstable self-similar singularities previously discovered. Furthermore, our method enables the discovery of new highly unstable singularities, i.e. the 4th unstable solution for IPM equations and a novel family of highly unstable solitons for the Nonlinear Schrödinger equations. This results in achieving high-gradient solutions with high precision, providing an important ingredient for bridging the gap between numerical discovery and computer-assisted proofs for unstable phenomena in nonlinear PDEs.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [238] [QuantumChem-200K: A Large-Scale Open Organic Molecular Dataset for Quantum-Chemistry Property Screening and Language Model Benchmarking](https://arxiv.org/abs/2511.21747)
*Yinqi Zeng,Renjie Li*

Main category: physics.chem-ph

TL;DR: 研究人员创建了包含20万有机分子量子化学性质的大规模数据集QuantumChem-200K，并基于此微调Qwen2.5-32B模型，开发了用于光引发剂设计的化学AI助手。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏包含光解离和激发态行为所需量子化学性质的大型开放数据集，阻碍了双光子聚合用新一代光引发剂的发现。现有分子数据集通常只提供基本的物理化学描述符，无法支持数据驱动的光引发剂筛选或AI辅助设计。

Method: 构建包含20多万有机分子的QuantumChem-200K数据集，标注11种量子化学性质。采用混合计算工作流，整合密度泛函理论、半经验激发态方法、原子量子求解器和神经网络预测器。使用该数据集微调开源的Qwen2.5-32B大语言模型，创建能从SMILES进行正向性质预测的化学AI助手。

Result: 在VQM24和ZINC20的3000个未见分子上进行基准测试，显示领域特定微调显著提高了预测准确性，特别是在光引发剂设计核心的双光子吸收和单重态-三重态系间窜越预测方面，表现优于GPT-4o、Llama-3.1-70B和基础Qwen2.5-32B模型。

Conclusion: QuantumChem-200K数据集和相应的AI助手共同提供了首个可扩展的高通量、LLM驱动的光引发剂筛选平台，加速了光敏材料的发现。

Abstract: The discovery of next-generation photoinitiators for two-photon polymerization (TPP) is hindered by the absence of large, open datasets containing the quantum-chemical and photophysical properties required to model photodissociation and excited-state behavior. Existing molecular datasets typically provide only basic physicochemical descriptors and therefore cannot support data-driven screening or AI-assisted design of photoinitiators. To address this gap, we introduce QuantumChem-200K, a large-scale dataset of over 200,000 organic molecules annotated with eleven quantum-chemical properties, including two-photon absorption (TPA) cross sections, TPA spectral ranges, singlet-triplet intersystem crossing (ISC) energies, toxicity and synthetic accessibility scores, hydrophilicity, solubility, boiling point, molecular weight, and aromaticity. These values are computed using a hybrid workflow that integrates density function theory (DFT), semi-empirical excited-state methods, atomistic quantum solvers, and neural-network predictors. Using QuantumChem-200K, we fine tune the open-source Qwen2.5-32B large language model to create a chemistry AI assistant capable of forward property prediction from SMILES. Benchmarking on 3000 unseen molecules from VQM24 and ZINC20 demonstrates that domain-specific fine-tuning significantly improves accuracy over GPT-4o, Llama-3.1-70B, and the base Qwen2.5-32B model, particularly for TPA and ISC predictions central to photoinitiator design. QuantumChem-200K and the corresponding AI assistant together provide the first scalable platform for high-throughput, LLM-driven photoinitiator screening and accelerated discovery of photosensitive materials.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [239] [Time Extrapolation with Graph Convolutional Autoencoder and Tensor Train Decomposition](https://arxiv.org/abs/2511.23037)
*Yuanhong Chen,Federico Pichi,Zhen Gao,Gianluigi Rozza*

Main category: math.NA

TL;DR: 该论文提出了一种结合图卷积自编码器、张量分解和算子推理的方法，用于参数化偏微分方程在复杂几何上的时间外推预测。


<details>
  <summary>Details</summary>
Motivation: 图自编码器在处理非结构化网格上的参数化偏微分方程时具有几何一致性优势，但在参数空间中进行时间外推预测时面临挑战，需要同时满足时间因果性和参数泛化性。

Method: 结合图卷积自编码器、张量分解和算子推理：1) 通过张量分解将高保真快照表示为参数、空间和时间核的组合；2) 使用算子推理学习时间核的演化；3) 基于DeepONet框架开发多保真度两阶段方法，将空间和时间核作为主干网络，参数核作为分支网络。

Result: 数值实验（热传导、对流扩散和涡旋脱落现象）表明，该方法在复杂几何的动态学习方面表现出色，特别是在外推预测方面优于MeshGraphNets等先进方法。

Conclusion: 该方法成功解决了图自编码器在参数化动力系统时间外推预测中的挑战，为复杂几何上的非线性降阶建模提供了有效解决方案。

Abstract: Graph autoencoders have gained attention in nonlinear reduced-order modeling of parameterized partial differential equations defined on unstructured grids. Despite they provide a geometrically consistent way of treating complex domains, applying such architectures to parameterized dynamical systems for temporal prediction beyond the training data, i.e. the extrapolation regime, is still a challenging task due to the simultaneous need of temporal causality and generalizability in the parametric space. In this work, we explore the integration of graph convolutional autoencoders (GCAs) with tensor train (TT) decomposition and Operator Inference (OpInf) to develop a time-consistent reduced-order model. In particular, high-fidelity snapshots are represented as a combination of parametric, spatial, and temporal cores via TT decomposition, while OpInf is used to learn the evolution of the latter. Moreover, we enhance the generalization performance by developing a multi-fidelity two-stages approach in the framework of Deep Operator Networks (DeepONet), treating the spatial and temporal cores as the trunk networks, and the parametric core as the branch network. Numerical results, including heat-conduction, advection-diffusion and vortex-shedding phenomena, demonstrate great performance in effectively learning the dynamic in the extrapolation regime for complex geometries, also in comparison with state-of-the-art approaches e.g. MeshGraphNets.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [240] [AdS/Deep-Learning made easy II: neural network-based approaches to holography and inverse problems](https://arxiv.org/abs/2511.22522)
*Hyun-Sik Jeong,Hanse Kim,Keun-Young Kim,Gaya Yun,Hyeonwoo Yu,Kwan Yun*

Main category: hep-th

TL;DR: 该论文应用物理信息机器学习（PIML）解决全息学和经典力学中的逆问题，使用神经ODE和PINN求解非线性运动微分方程，并探索KAN作为传统神经网络的替代方案。


<details>
  <summary>Details</summary>
Motivation: 为高能物理领域的研究者提供一个系统性的框架，使用神经网络解决逆问题，特别是在全息学中从边界量子数据重建体时空和有效势能。

Method: 采用物理信息机器学习（PIML），包括神经常微分方程（Neural ODEs）和物理信息神经网络（PINNs）来求解非线性运动微分方程。同时探索Kolmogorov-Arnold网络（KANs）作为替代方案。

Result: 成功应用PIML解决全息学逆问题，包括QCD状态方程和T-线性电阻率案例。展示了全息问题与经典力学中摩擦力的逆问题之间的类比关系。

Conclusion: 该研究为高能物理中的机器学习应用提供了系统性框架，所提出的方法在数学、工程和自然科学领域具有广泛的应用潜力。

Abstract: We apply physics-informed machine learning (PIML) to solve inverse problems in holography and classical mechanics, focusing on neural ordinary differential equations (Neural ODEs) and physics-informed neural networks (PINNs) for solving non-linear differential equations of motion. First, we introduce holographic inverse problems and demonstrate how PIML can reconstruct bulk spacetime and effective potentials from boundary quantum data. To illustrate this, two case studies are explored: the QCD equation of state in holographic QCD and $T$-linear resistivity in holographic strange metals. Additionally, we explicitly show how such holographic problems can be analogized to inverse problems in classical mechanics, modeling frictional forces with neural networks. We also explore Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks, offering more efficient solutions in certain cases. This manuscript aim to provide a systematic framework for using neural networks in inverse problems, serving as a comprehensive reference for researchers in machine learning for high-energy physics, with methodologies that also have broader applications in mathematics, engineering, and the natural sciences.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [241] [Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns](https://arxiv.org/abs/2511.22259)
*Christoph Weissenborn,Steffen Wendzel*

Main category: cs.CR

TL;DR: 本文提出了一种基于网络时序模式相对指针的新型历史隐蔽信道方法，减少了对集中式时间同步的依赖，提高了抗检测性，实验显示比特率优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 传统隐蔽信道容易被检测，历史隐蔽信道通过引用历史数据来嵌入隐蔽信息，但现有方法依赖集中式时间同步且易被网络监控工具发现。需要开发更隐蔽、更稳健的隐蔽通信方法。

Method: 提出使用网络时序模式的相对指针来建立和维护隐蔽通信链路，减少对集中式时间同步的依赖。同时探索如何定制HCC以优化其鲁棒性和不可检测性特征。

Result: 实验结果表明，该方法相比先前工作具有更好的比特率，同时降低了被标准网络监控工具检测的可能性。

Conclusion: 基于相对指针的历史隐蔽信道方法能够有效减少对集中式时间同步的依赖，提高隐蔽性和鲁棒性，为隐蔽通信提供了更有效的解决方案。

Abstract: A Covert Channel (CC) exploits legitimate communication mechanisms to stealthily transmit information, often bypassing traditional security controls. Among these, a novel paradigm called History Covert Channels (HCC) leverages past network events as reference points to embed covert messages. Unlike traditional timing- or storage-based CCs, which directly manipulate traffic patterns or packet contents, HCCs minimize detectability by encoding information through small pointers to historical data. This approach enables them to amplify the size of transmitted covert data by referring to more bits than are actually embedded. Recent research has explored the feasibility of such methods, demonstrating their potential to evade detection by repurposing naturally occurring network behaviors as a covert transmission medium.
  This paper introduces a novel method for establishing and maintaining covert communication links using relative pointers to network timing patterns, which minimizes the reliance of the HCC on centralized timekeeping and reduces the likelihood of being detected by standard network monitoring tools. We also explore the tailoring of HCCs to optimize their robustness and undetectability characteristics. Our experiments reveal a better bitrate compared to previous work.

</details>


### [242] [Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities](https://arxiv.org/abs/2511.23408)
*Aayush Garg,Zanis Ali Khan,Renzo Degiovanni,Qiang Tang*

Main category: cs.CR

TL;DR: LLMs在真实漏洞修复上表现优于人工构造漏洞，不同模型在修复重叠性和互补性上存在显著差异，选择合适的LLM对有效修复漏洞至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估LLMs在公开披露漏洞上的修复能力，但对其在相关人工构造漏洞上的有效性缺乏探索。需要实证评估LLMs在真实和人工漏洞上的修复效果和互补性。

Method: 使用OpenAI GPT变体、LLaMA、DeepSeek和Mistral等主流LLMs，通过PoV测试执行来具体评估LLM生成的源代码是否能成功修复漏洞，同时分析模型间的重叠性和互补性。

Result: LLMs修复真实漏洞的效果优于人工构造漏洞；不同LLMs在修复重叠性（多个模型修复相同漏洞）和互补性（仅单个模型能修复的漏洞）方面存在显著差异。

Conclusion: 选择合适的LLM对有效修复漏洞至关重要，不同模型在修复能力上具有互补性，未来研究应考虑模型组合策略以提高漏洞修复的整体效果。

Abstract: Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.

</details>


### [243] [Beyond Membership: Limitations of Add/Remove Adjacency in Differential Privacy](https://arxiv.org/abs/2511.21804)
*Gauri Pradhan,Joonas Jälkö,Santiago Zanella-Bèguelin,Antti Honkela*

Main category: cs.CR

TL;DR: 论文指出在差分隐私中，add/remove邻接关系会高估属性隐私保护，而substitute邻接关系更适合保护记录属性而非成员身份。


<details>
  <summary>Details</summary>
Motivation: 大多数DP实现使用add/remove邻接关系来保护成员身份，但在许多ML应用中，目标是保护单个记录的属性（如监督微调中的标签）。add/remove邻接关系会高估属性隐私保护。

Method: 开发了新的攻击方法来审计substitute邻接关系下的DP，并实证比较add/remove和substitute邻接关系下的隐私保证。

Result: 审计结果显示与add/remove报告的DP保证不一致，但与substitute邻接关系计算的隐私预算一致，表明add/remove会高估属性隐私。

Conclusion: 当保护目标是记录属性而非成员身份时，报告DP保证时选择正确的邻接关系至关重要，substitute邻接关系更适合属性隐私保护。

Abstract: Training machine learning models with differential privacy (DP) limits an adversary's ability to infer sensitive information about the training data. It can be interpreted as a bound on adversary's capability to distinguish two adjacent datasets according to chosen adjacency relation. In practice, most DP implementations use the add/remove adjacency relation, where two datasets are adjacent if one can be obtained from the other by adding or removing a single record, thereby protecting membership. In many ML applications, however, the goal is to protect attributes of individual records (e.g., labels used in supervised fine-tuning). We show that privacy accounting under add/remove overstates attribute privacy compared to accounting under the substitute adjacency relation, which permits substituting one record. To demonstrate this gap, we develop novel attacks to audit DP under substitute adjacency, and show empirically that audit results are inconsistent with DP guarantees reported under add/remove, yet remain consistent with the budget accounted under the substitute adjacency relation. Our results highlight that the choice of adjacency when reporting DP guarantees is critical when the protection target is per-record attributes rather than membership.

</details>


### [244] [Real-PGDN: A Two-level Classification Method for Full-Process Recognition of Newly Registered Pornographic and Gambling Domain Names](https://arxiv.org/abs/2511.22215)
*Hao Wang,Yingshuo Wang,Junang Gan,Yanan Cheng,Jinshuai Zhang*

Main category: cs.CR

TL;DR: 本文提出Real-PGDN方法，用于色情和赌博域名(PGDN)分类，通过两级分类器实现97.88%的精确率，并在真实场景中保持超过70%的预测精度。


<details>
  <summary>Details</summary>
Motivation: 在线色情和赌博对个人资产和隐私构成威胁，需要研究新注册PGDN的分类方法。现有研究要么使用理想样本追求高精度，要么使用真实数据但精度较低，缺乏兼顾时效性和准确性的解决方案。

Method: 提出Real-PGDN方法，包含完整流程：及时全面的真实数据爬取、具有特征缺失容忍度的特征提取、精确的PGDN分类、实际场景应用效果评估。使用两级分类器，集成CoSENT（基于BERT）、多层感知机(MLP)和传统分类算法。

Result: 分类器达到97.88%的精确率。构建了NRD2024数据集，包含150万个新注册域名在6个方向上的20天连续检测信息。案例研究表明，对于注册后延迟使用的PGDN，该方法仍保持超过70%的预测精度。

Conclusion: Real-PGDN方法在PGDN分类中实现了高精度和实际应用价值的平衡，为在线色情和赌博域名的监管提供了有效的技术解决方案。

Abstract: Online pornography and gambling have consistently posed regulatory challenges for governments, threatening both personal assets and privacy. Therefore, it is imperative to research the classification of the newly registered Pornographic and Gambling Domain Names (PGDN). However, scholarly investigation into this topic is limited. Previous efforts in PGDN classification pursue high accuracy using ideal sample data, while others employ up-to-date data from real-world scenarios but achieve lower classification accuracy. This paper introduces the Real-PGDN method, which accomplishes a complete process of timely and comprehensive real-data crawling, feature extraction with feature-missing tolerance, precise PGDN classification, and assessment of application effects in actual scenarios. Our two-level classifier, which integrates CoSENT (BERT-based), Multilayer Perceptron (MLP), and traditional classification algorithms, achieves a 97.88% precision. The research process amasses the NRD2024 dataset, which contains continuous detection information over 20 days for 1,500,000 newly registered domain names across 6 directions. Results from our case study demonstrate that this method also maintains a forecast precision of over 70% for PGDN that are delayed in usage after registration.

</details>


### [245] [GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents](https://arxiv.org/abs/2511.22441)
*Xinyu Zhang,Yixin Wu,Boyang Zhang,Chenhao Lin,Chao Shen,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: Geo-Detective是一个模仿人类推理和工具使用的智能体，通过四步流程和专用工具（如视觉反向搜索）进行图像地理位置推断，在缺乏明显地理特征的图像上表现优异，但也带来了隐私风险。


<details>
  <summary>Details</summary>
Motivation: 社交媒体图像常包含地理线索，现有的大型视觉语言模型（LVLMs）虽然能进行地理位置推断，但并未针对此任务优化。作者旨在探索图像地理位置推断的潜力及相关隐私风险。

Method: 提出Geo-Detective智能体，模仿人类推理和工具使用，采用四步流程（根据图像难度自适应选择策略），配备专用工具如视觉反向搜索来收集外部地理线索。

Result: Geo-Detective在整体上优于基线LVLMs，特别是在缺乏可见地理特征的图像上。在国家级任务中提升11.1%，在更细粒度级别仍有5.2%的性能增益。配备外部线索时，准确预测率提高，"未知"预测率降低50.6%以上。

Conclusion: Geo-Detective展示了强大的地理位置推断能力，但也凸显了隐私风险。研究发现Geo-Detective具有较强的鲁棒性，表明需要更有效的隐私保护措施。

Abstract: Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the "unknown" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.

</details>


### [246] [An Efficient Privacy-preserving Intrusion Detection Scheme for UAV Swarm Networks](https://arxiv.org/abs/2511.22791)
*Kanchon Gharami,Shafika Showkat Moni*

Main category: cs.CR

TL;DR: 提出基于轻量级联邦持续学习的无人机群入侵检测系统，解决传统IDS的延迟、隐私、性能开销和模型漂移问题


<details>
  <summary>Details</summary>
Motivation: 无人机群网络在监控、灾害管理、农业和国防等领域应用广泛，但面临各种安全攻击威胁任务成功。传统入侵检测系统存在二进制分类局限、资源密集、延迟、隐私泄露、性能开销大和模型漂移等问题

Method: 开发轻量级联邦持续学习IDS方案，支持跨异构无人机群的去中心化训练，确保数据异质性和隐私保护

Result: 在多个数据集上表现优异：UKM-IDS准确率99.45%，UAV-IDS准确率99.99%，TLM-UAV数据集96.85%，Cyber-Physical数据集98.05%

Conclusion: 提出的轻量级联邦持续学习IDS方案能有效解决传统IDS的挑战，在保护隐私的同时实现高精度入侵检测，适用于无人机群网络的安全防护

Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) and their applications in diverse domains, such as surveillance, disaster management, agriculture, and defense, have revolutionized modern technology. While the potential benefits of swarm-based UAV networks are growing significantly, they are vulnerable to various security attacks that can jeopardize the overall mission success by degrading their performance, disrupting decision-making, and compromising the trajectory planning process. The Intrusion Detection System (IDS) plays a vital role in identifying potential security attacks to ensure the secure operation of UAV swarm networks. However, conventional IDS primarily focuses on binary classification with resource-intensive neural networks and faces challenges, including latency, privacy breaches, increased performance overhead, and model drift. This research aims to address these challenges by developing a novel lightweight and federated continuous learning-based IDS scheme. Our proposed model facilitates decentralized training across diverse UAV swarms to ensure data heterogeneity and privacy. The performance evaluation of our model demonstrates significant improvements, with classification accuracies of 99.45% on UKM-IDS, 99.99% on UAV-IDS, 96.85% on TLM-UAV dataset, and 98.05% on Cyber-Physical datasets.

</details>


### [247] [Clustering Malware at Scale: A First Full-Benchmark Study](https://arxiv.org/abs/2511.23198)
*Martin Mocko,Jakub Ševcech,Daniela Chudá*

Main category: cs.CR

TL;DR: 该研究评估了恶意软件聚类在Bodmas和Ember两大公开基准数据集上的性能，首次在整个基准数据集上进行聚类分析，并探索了包含良性样本的聚类效果，发现K-Means和BIRCH表现最佳。


<details>
  <summary>Details</summary>
Motivation: 恶意软件攻击仍然频繁发生，但现有研究在恶意软件聚类方面存在不足：1) 很少包含良性样本；2) 未能充分利用大型公开基准数据集；3) 当前最优聚类方法不明确。

Method: 在Bodmas和Ember两个大型公开恶意软件基准数据集上进行聚类实验，首次在整个数据集上进行聚类分析，并扩展任务包含良性样本，评估多种聚类算法性能。

Result: 1) 包含良性样本不会显著降低聚类质量；2) 不同数据集（Ember、Bodmas和私有数据集）的聚类质量存在显著差异；3) 与普遍认知相反，K-Means和BIRCH表现最佳，而DBSCAN和HAC表现较差。

Conclusion: 该研究首次在整个恶意软件基准数据集上建立聚类基准，证明包含良性样本的可行性，并发现传统聚类算法K-Means和BIRCH在实际大规模数据集上表现优于更复杂的算法。

Abstract: Recent years have shown that malware attacks still happen with high frequency. Malware experts seek to categorize and classify incoming samples to confirm their trustworthiness or prove their maliciousness. One of the ways in which groups of malware samples can be identified is through malware clustering. Despite the efforts of the community, malware clustering which incorporates benign samples has been under-explored. Moreover, despite the availability of larger public benchmark malware datasets, malware clustering studies have avoided fully utilizing these datasets in their experiments, often resorting to small datasets with only a few families. Additionally, the current state-of-the-art solutions for malware clustering remain unclear. In our study, we evaluate malware clustering quality and establish the state-of-the-art on Bodmas and Ember - two large public benchmark malware datasets. Ours is the first study of malware clustering performed on whole malware benchmark datasets. Additionally, we extend the malware clustering task by incorporating benign samples. Our results indicate that incorporating benign samples does not significantly degrade clustering quality. We find that there are significant differences in the quality of the created clusters between Ember and Bodmas, as well as a private industry dataset. Contrary to popular opinion, our top clustering performers are K-Means and BIRCH, with DBSCAN and HAC falling behind.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [248] [Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training](https://arxiv.org/abs/2511.21863)
*Eric Yeats,Darryl Hannan,Wilson Fearn,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.CV

TL;DR: 提出了一种新的无监督引导方法SFG，利用对数密度估计在鞍点区域的正曲率来引导基于分数的生成模型，无需标签数据或额外模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有引导方法如Classifier-Free Guidance需要标签数据并训练额外模型，Auto-Guidance需要较小模型版本，这些方法在缺乏标签数据或无法训练新模型时难以应用。作者发现对数密度估计在鞍点区域的正曲率可以提供强引导信号。

Method: 提出鞍点自由引导(SFG)，通过维护对数密度的最大正曲率估计来引导单个基于分数的模型。该方法与CFG计算成本相同，无需额外训练，可直接用于现成的扩散和流匹配模型。

Result: SFG在单模型无条件ImageNet-512生成中实现了最先进的FID和FD-DINOv2指标。与Auto-Guidance结合时，在FD-DINOv2分数上达到通用最先进水平。在FLUX.1-dev和Stable Diffusion v3.5实验中，SFG相比CFG提升了输出图像的多样性，同时保持了优秀的提示遵循和图像保真度。

Conclusion: SFG是一种有效且实用的引导方法，无需标签数据或额外模型训练，为基于分数的生成模型提供了新的无监督引导途径，在保持生成质量的同时提升了多样性。

Abstract: Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.

</details>


### [249] [Adaptive Parameter Optimization for Robust Remote Photoplethysmography](https://arxiv.org/abs/2511.21903)
*Cecilia G. Morales,Fanurs Chi En Teh,Kai Li,Pushpak Agrawal,Artur Dubrawski*

Main category: cs.CV

TL;DR: PRISM算法是一种无需训练的自适应远程光电容积描记方法，通过在线信号质量评估优化参数，在多种环境下实现实时心率监测。


<details>
  <summary>Details</summary>
Motivation: 现有rPPG方法使用固定参数，仅针对特定光照条件和相机设置优化，限制了在不同部署环境中的适应性。

Method: 提出基于投影的鲁棒信号混合（PRISM）算法，通过在线信号质量评估联合优化光度去趋势和颜色混合，无需训练即可自适应调整参数。

Result: 在PURE数据集上MAE为0.77 bpm，UBFC-rPPG上为0.66 bpm；在5 bpm阈值下准确率分别为97.3%和97.5%，性能与领先的监督方法相当（p > 0.2）。

Conclusion: 自适应时间序列优化显著提高了rPPG在不同条件下的性能，PRISM在保持实时CPU性能的同时无需训练，验证了该方法在多样化环境中的有效性。

Abstract: Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.

</details>


### [250] [DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification](https://arxiv.org/abs/2511.21959)
*Walid Houmaidi,Mohamed Hadadi,Youssef Sabiri,Yousra Chtouki*

Main category: cs.CV

TL;DR: 该研究在包含4000张内窥镜图像的新胃肠道医学影像数据集上进行了全面的模型比较分析，VGG16和MobileNetV2取得了96.5%的最佳测试准确率，并通过Grad-CAM可视化增强临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜图像分析中的常见挑战，如光照变化、相机角度波动和成像伪影，为胃肠道疾病自动分类建立稳健基准，并强调医学AI研究中数据集多样性和模型可解释性的重要性。

Method: 使用最先进的深度学习技术，在包含四种关键疾病类别（憩室病、肿瘤、腹膜炎、输尿管）的4000张内窥镜图像数据集上进行模型比较分析，并采用Grad-CAM可视化实现可解释AI。

Result: VGG16和MobileNetV2模型均达到96.5%的测试准确率，Xception模型达到94.24%，建立了稳健的疾病分类基准，并通过可视化识别对模型预测最具影响力的图像区域。

Conclusion: 该研究展示了即使在复杂真实世界条件下也能实现稳健、准确且可解释的医学图像分析，为胃肠道计算机辅助诊断提供了原始基准、比较见解和视觉解释，推动了该领域的发展。

Abstract: This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.

</details>


### [251] [MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation](https://arxiv.org/abs/2511.22102)
*Simon Joseph Clément Crête,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: 使用监督对比学习与Rank-N-Contrast损失进行MRI脑龄估计，显著优于传统深度回归方法，并探索脑龄差与神经退行性疾病严重程度的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的脑龄估计方法往往无法捕捉神经形态变化的连续性，可能导致次优的特征表示和结果。神经退行性疾病会加速大脑衰老，测量这一现象可作为临床应用的潜在生物标志物。

Method: 首次将监督对比学习与Rank-N-Contrast损失应用于基于T1w结构MRI的脑龄估计，使用ResNet骨干网络，并利用Grad-RAM对回归结果进行可视化解释。

Result: 在有限训练样本下，MAE为4.27年，R²为0.93，显著优于相同ResNet骨干的传统深度回归方法，与使用更大训练数据的SOTA方法相当或更好。Grad-RAM显示RNC损失能捕捉更细微的年龄相关特征。

Conclusion: 该方法在脑龄估计上表现优异，脑龄差与阿尔茨海默病和帕金森病严重程度相关，展示了其作为神经退行性疾病生物标志物的潜力。

Abstract: MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.

</details>


### [252] [Autonomous labeling of surgical resection margins using a foundation model](https://arxiv.org/abs/2511.22131)
*Xilin Yang,Musa Aydin,Yuhong Lu,Sahan Yoruc Selcuk,Bijie Bai,Yijie Zhang,Andrew Birkeland,Katjana Ehrlich,Julien Bec,Laura Marcu,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出虚拟墨水网络(VIN)，利用冻结的基础模型提取特征，通过两层MLP进行补丁级分类，自动定位H&E染色切片中的手术切面，减少对物理墨水的依赖，实现标准化边缘评估。


<details>
  <summary>Details</summary>
Motivation: 当前病理标本评估中，物理墨水应用不一致，且烧灼伪影会掩盖组织切片的真实边缘，影响患者预后评估。需要一种标准化、不依赖墨水的边缘定位方法。

Method: 使用冻结的基础模型作为特征提取器，配合紧凑的两层多层感知器(MLP)，训练用于识别烧灼一致性特征的补丁级分类器。数据集包含120张H&E染色切片，由认证病理学家提供边界标注。

Result: 在20张未见过的切片盲测中，VIN生成连贯的边缘覆盖层，与专家标注定性一致。区域级准确率约73.3%，错误主要局限于不影响整体边缘图连续性的有限区域。

Conclusion: VIN能够捕捉烧灼相关的组织形态学特征，提供可重复、无需墨水的边缘描绘，适合整合到常规数字病理工作流程中，用于下游边缘距离测量。

Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.

</details>


### [253] [3D-Consistent Multi-View Editing by Diffusion Guidance](https://arxiv.org/abs/2511.22228)
*Josef Bengtson,David Nilsson,Dong In Lee,Fredrik Kahl*

Main category: cs.CV

TL;DR: 提出无需训练的多视角一致性扩散框架，用于3D表示（如NeRF、高斯溅射）的图像编辑，通过一致性损失确保不同视角编辑的几何和光度一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像编辑方法在多视角编辑中会产生几何和光度不一致的结果，这对3D表示（如NeRF、高斯溅射）的编辑尤其成问题。

Method: 提出训练免费的扩散框架，核心假设是未编辑图像中的对应点在编辑后应经历相似的变换。引入一致性损失引导扩散采样实现连贯编辑，支持密集和稀疏多视角编辑设置。

Result: 实验表明该方法显著提升了3D一致性，优于现有多视角编辑方法，并实现了高质量的高斯溅射编辑，具有锐利细节和与文本提示的强保真度。

Conclusion: 该训练免费的多视角一致性扩散框架有效解决了3D表示编辑中的一致性问题，支持多种编辑方法，为3D场景编辑提供了高质量解决方案。

Abstract: Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/

</details>


### [254] [Structure is Supervision: Multiview Masked Autoencoders for Radiology](https://arxiv.org/abs/2511.22294)
*Sonia Laguna,Andrea Agostini,Alain Ryser,Samuel Ruiperez-Campillo,Irene Cannistraci,Moritz Vandenhirtz,Stephan Mandt,Nicolas Deperrois,Farhad Nooralahzadeh,Michael Krauthammer,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.CV

TL;DR: MVMAE是一种自监督框架，利用放射学研究的自然多视图组织学习视图不变和疾病相关表示，通过掩码图像重建和跨视图对齐，结合放射学报告作为文本监督信号，在疾病分类任务上优于监督和视觉语言基线。


<details>
  <summary>Details</summary>
Motivation: 构建稳健的医疗机器学习系统需要利用临床数据内在结构的预训练策略。放射学研究具有自然的多视图组织，这为学习视图不变和疾病相关表示提供了机会。

Method: 提出MVMAE框架，结合掩码图像重建与跨视图对齐，将临床投影中的冗余转化为自监督信号。进一步扩展为MVMAE-V2T，融入放射学报告作为文本学习信号以增强语义基础，同时保持完全基于视觉的推理。

Result: 在MIMIC-CXR、CheXpert和PadChest三个大规模公共数据集的下游疾病分类任务评估中，MVMAE始终优于监督和视觉语言基线。MVMAE-V2T提供额外增益，特别是在低标签情况下，结构化文本监督最为有益。

Conclusion: 这些结果确立了结构和文本监督作为构建可扩展、临床基础的医疗基础模型的互补路径的重要性。

Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.

</details>


### [255] [Unexplored flaws in multiple-choice VQA evaluations](https://arxiv.org/abs/2511.22341)
*Fabio Rosenthal,Sebastian Schmidt,Thorsten Graf,Thorsten Bagodonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.CV

TL;DR: 多模态大语言模型在视觉问答评估中存在对提示格式的敏感性，即使语义中性的格式变化也会显著影响结果，现有偏差缓解策略无法解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究已经发现视觉问答基准测试对答案选项顺序敏感，但本文指出还存在未被探索的提示格式偏差，这些偏差质疑了当前多模态大语言模型评估的可靠性。

Method: 通过大规模研究分析提示格式的三个关键变化因素，涉及7个多模态大语言模型和5个视觉问答数据集，涵盖48种不同的提示格式变体。

Result: 研究发现多项选择视觉问答对微小的提示格式变化高度敏感，即使这些变化是语义中性的。这些偏差独立于已知的顺序偏差或模型对正确答案的置信度而持续存在。

Conclusion: 现有的偏差缓解策略无法解决这些新发现的偏差，提示格式的敏感性对多模态大语言模型的可靠评估构成了挑战。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate strong capabilities in handling image-text inputs. A common way to assess this ability is through multiple-choice Visual Question Answering (VQA). Earlier works have already revealed that these benchmarks are sensitive to answer choice order, a limitation that can be mitigated through careful design. Yet, we highlight additional, unexplored biases in prompt formatting that question the reliability of current MLLM evaluations. Specifically, we identify three key variation factors in prompt formatting and analyze their impact through a large-scale study involving $\mathbf{\text{seven}}$ MLLMs and $\mathbf{\text{five}}$ VQA datasets, spanning $\mathbf{48}$ distinct $\mathbf{\text{prompt format variations}}$. Our findings reveal that multiple-choice VQA is highly sensitive to minor prompt format changes, even when these changes are semantically neutral. We further demonstrate that these biases persist independently of known order biases or the MLLM's confidence in the correct answer. Finally, we demonstrate that existing bias mitigation strategies fail to address these newly identified biases.

</details>


### [256] [Benchmarking machine learning models for multi-class state recognition in double duantum dot data](https://arxiv.org/abs/2511.22451)
*Valeria Díaz Moreno,Ryan P Khalili,Daniel Schug,Patrick J. Walsh,Justyna P. Zwolak*

Main category: cs.CV

TL;DR: 本研究对四种现代机器学习架构在双量子点电荷稳定性图中的多类状态识别进行了全面基准测试，发现CNN在实验数据上表现最佳，而U-Net和ViT虽在合成数据上表现优异但泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 半导体量子点是可扩展量子处理器的重要平台，但扩展到大型阵列需要可靠、自动化的调谐策略，其中准确识别量子点设备状态是关键挑战。

Method: 对四种现代机器学习架构（U-Net、视觉变换器、混合密度网络、卷积神经网络）在双量子点电荷稳定性图上的多类状态识别进行基准测试，评估不同数据预算和归一化方案下的性能，使用合成和实验数据。

Result: U-Net和ViT在合成数据上达到最高MSE分数（超过0.98），但无法泛化到实验数据；MDN计算效率最高且训练稳定，但性能较低；CNN在实验数据上表现最佳，参数数量比U-Net和ViT少两个数量级；min-max归一化通常产生更高MSE分数但收敛不稳定，z-score归一化训练动态更可预测但准确率较低。

Conclusion: CNN配合min-max归一化是处理量子点电荷稳定性图的实用方法，在实验数据上实现了良好的准确性与计算效率的平衡。

Abstract: Semiconductor quantum dots (QDs) are a leading platform for scalable quantum processors. However, scaling to large arrays requires reliable, automated tuning strategies for devices' bootstrapping, calibration, and operation, with many tuning aspects depending on accurately identifying QD device states from charge-stability diagrams (CSDs). In this work, we present a comprehensive benchmarking study of four modern machine learning (ML) architectures for multi-class state recognition in double-QD CSDs. We evaluate their performance across different data budgets and normalization schemes using both synthetic and experimental data. We find that the more resource-intensive models -- U-Nets and visual transformers (ViTs) -- achieve the highest MSE score (defined as $1-\mathrm{MSE}$) on synthetic data (over $0.98$) but fail to generalize to experimental data. MDNs are the most computationally efficient and exhibit highly stable training, but with substantially lower peak performance. CNNs offer the most favorable trade-off on experimental CSDs, achieving strong accuracy with two orders of magnitude fewer parameters than the U-Nets and ViTs. Normalization plays a nontrivial role: min-max scaling generally yields higher MSE scores but less stable convergence, whereas z-score normalization produces more predictable training dynamics but at reduced accuracy for most models. Overall, our study shows that CNNs with min-max normalization are a practical approach for QD CSDs.

</details>


### [257] [What Shape Is Optimal for Masks in Text Removal?](https://arxiv.org/abs/2511.22499)
*Hyakka Nakada,Marika Kubota*

Main category: cs.CV

TL;DR: 该论文针对密集文本图像去除任务，提出通过贝叶斯优化学习灵活掩码轮廓的方法，发现字符级掩码优于最小覆盖区域，为手动掩码提供用户友好指南。


<details>
  <summary>Details</summary>
Motivation: 现有文本去除方法主要针对简单场景文本，缺乏对密集文本图像的处理研究。研究发现文本去除性能对掩码轮廓扰动敏感，精确调整掩码形状对实际应用至关重要。

Method: 创建了包含大量文本的基准数据集，开发了建模高度灵活掩码轮廓的方法，并使用贝叶斯优化学习其参数。

Result: 发现最优掩码轮廓是字符级掩码，且最小文本区域覆盖并非最优。为手动掩码提供了用户友好的指导原则。

Conclusion: 该研究为密集文本图像去除任务提供了有效的掩码优化方法，通过贝叶斯优化学习到的字符级掩码显著提升了去除性能，为实际应用提供了实用指南。

Abstract: The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.

</details>


### [258] [GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing](https://arxiv.org/abs/2511.22607)
*Xiaoyin Yang*

Main category: cs.CV

TL;DR: 提出GazeTrack高精度眼动追踪数据集和新的形状误差正则化方法，结合类似纸张展开的坐标变换技术，显著降低注视角度误差并减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前眼动追踪技术在虚拟和增强现实应用中的注视精度不足，无法满足空间计算需求，需要更高精度的数据集和方法来提升性能。

Method: 1) 设计眼动数据收集框架，使用高精度设备构建GazeTrack多民族、年龄、视力条件数据集；2) 提出形状误差正则化方法约束瞳孔椭圆拟合；3) 发明类似纸张展开的坐标变换方法预测注视向量；4) 构建低计算复杂度的注视向量生成模型。

Result: 在GazeTrack数据集上实现了注视角度误差的显著降低，同时相比其他方法具有更低计算复杂度，提升了眼动追踪的精度和效率。

Conclusion: 通过高质量数据集、形状误差正则化和创新的坐标变换方法，成功提升了眼动追踪精度，为虚拟/增强现实应用提供了更可靠的注视追踪解决方案。

Abstract: Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.

</details>


### [259] [Stable-Drift: A Patient-Aware Latent Drift Replay Method for Stabilizing Representations in Continual Learning](https://arxiv.org/abs/2511.22615)
*Paraskevi-Antonia Theofilou,Anuhya Thota,Stefanos Kollias,Mamatha Thota*

Main category: cs.CV

TL;DR: 提出一种基于潜在漂移引导的重放方法，通过识别和重放具有高表示不稳定性的样本来缓解医学影像中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在顺序训练新数据时会出现灾难性遗忘，这在医学影像中尤为关键，因为模型需要持续适应新医院数据而不损害已有诊断知识。

Method: 使用潜在漂移量化表示不稳定性，即在朴素域适应后样本内部特征表示的变化。在患者层面聚合漂移，存储显示最大多层表示偏移的每个患者的切片。

Result: 在跨医院COVID-19 CT分类任务中，使用CNN和Vision Transformer骨干网络，该方法相比朴素微调和随机重放显著减少了遗忘。

Conclusion: 潜在漂移作为实际且可解释的重放信号，有助于推进现实世界医学场景中的鲁棒持续学习。

Abstract: When deep learning models are sequentially trained on new data, they tend to abruptly lose performance on previously learned tasks, a critical failure known as catastrophic forgetting. This challenge severely limits the deployment of AI in medical imaging, where models must continually adapt to data from new hospitals without compromising established diagnostic knowledge. To address this, we introduce a latent drift-guided replay method that identifies and replays samples with high representational instability. Specifically, our method quantifies this instability via latent drift, the change in a sample internal feature representation after naive domain adaptation. To ensure diversity and clinical relevance, we aggregate drift at the patient level, our memory buffer stores the per patient slices exhibiting the greatest multi-layer representation shift. Evaluated on a cross-hospital COVID-19 CT classification task using state-of-the-art CNN and Vision Transformer backbones, our method substantially reduces forgetting compared to naive fine-tuning and random replay. This work highlights latent drift as a practical and interpretable replay signal for advancing robust continual learning in real world medical settings.

</details>


### [260] [From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images](https://arxiv.org/abs/2511.22805)
*Yiming Chen,Junlin Han,Tianyi Bai,Shengbang Tong,Filippos Kokkinos,Philip Torr*

Main category: cs.CV

TL;DR: 论文提出了CogIP-Bench基准测试，用于评估多模态大语言模型对图像主观认知属性（如记忆性、趣味性、美观性）的理解能力，发现现有模型与人类感知存在显著差距，并通过后训练有效提升了模型的对齐度，且这种认知对齐可迁移到下游创意任务中。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型擅长识别图像中的物体和描述场景，但缺乏对人类主观认知属性的理解能力，如判断图像是否令人难忘、有趣、美观或具有情感感染力。这种差距限制了模型在人类中心化AI应用中的潜力。

Method: 1. 引入CogIP-Bench基准测试，系统评估MLLMs对图像认知属性的理解；2. 通过后训练阶段提升模型与人类感知的对齐度；3. 将认知对齐的MLLM集成到图像生成流程中，指导合成过程以产生具有特定认知属性的图像。

Result: 评估显示当前模型与人类对这些细微属性的感知存在显著差距；后训练能有效弥合这一差距，显著提升模型与人类判断的对齐度；学习到的认知对齐不仅具有预测性，还能迁移到下游创意任务中，指导生成更具记忆性或视觉吸引力的图像。

Conclusion: 该工作提供了衡量人类感知的基准测试、增强对齐度的后训练流程，并证明了这种对齐能够解锁更人类中心化的AI，使模型不仅能理解图像内容，还能理解图像给人的主观感受。

Abstract: While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.

</details>


### [261] [ClearGCD: Mitigating Shortcut Learning For Robust Generalized Category Discovery](https://arxiv.org/abs/2511.22892)
*Kailin Lyu,Jianwei He,Long Xiao,Jianing Zeng,Liang Fan,Lin Shu,Jie Hao*

Main category: cs.CV

TL;DR: ClearGCD是一个解决广义类别发现中原型混淆问题的框架，通过语义视图对齐和捷径抑制正则化来减少对非语义线索的依赖，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在开放世界场景中，广义类别发现需要在未标记数据中识别已知和未知类别。现有方法常因捷径学习导致原型混淆，这会削弱泛化能力并导致已知类别的遗忘。

Method: ClearGCD框架包含两个互补机制：1) 语义视图对齐(SVA)：通过跨类别补丁替换生成强增强，并使用弱增强强制语义一致性；2) 捷径抑制正则化(SSR)：维护自适应原型库，对齐已知类别同时鼓励潜在未知类别的分离。

Result: ClearGCD可以无缝集成到参数化GCD方法中，在多个基准测试中持续优于最先进的方法。

Conclusion: ClearGCD通过减少对非语义线索的依赖，有效解决了广义类别发现中的原型混淆问题，提高了模型的泛化能力和已知类别的保留。

Abstract: In open-world scenarios, Generalized Category Discovery (GCD) requires identifying both known and novel categories within unlabeled data. However, existing methods often suffer from prototype confusion caused by shortcut learning, which undermines generalization and leads to forgetting of known classes. We propose ClearGCD, a framework designed to mitigate reliance on non-semantic cues through two complementary mechanisms. First, Semantic View Alignment (SVA) generates strong augmentations via cross-class patch replacement and enforces semantic consistency using weak augmentations. Second, Shortcut Suppression Regularization (SSR) maintains an adaptive prototype bank that aligns known classes while encouraging separation of potential novel ones. ClearGCD can be seamlessly integrated into parametric GCD approaches and consistently outperforms state-of-the-art methods across multiple benchmarks.

</details>


### [262] [Buffer replay enhances the robustness of multimodal learning under missing-modality](https://arxiv.org/abs/2511.23070)
*Hongye Zhu,Xuan Liu,Yanwen Ba,Jingye Xue,Shigeng Zhang*

Main category: cs.CV

TL;DR: REP是一种轻量级多模态学习方法，通过特征重放和私有-共享特征解耦策略，有效应对模态缺失问题，在多种多模态基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么计算成本高（合成缺失模态），要么仅依赖相邻层特征而忽略长距离上下文信息，导致模态缺失时性能显著下降。需要一种轻量级且能有效利用上下文信息的方法来提升对模态缺失的鲁棒性。

Method: 1) 通过残差旁路构建模态特征缓冲区，缓存早期层表示并在深层重放，缓解信息损失；2) 采用私有-共享特征解耦策略，私有缓冲区保留模态特定信号，共享缓冲区编码跨模态语义；3) 设计任务感知动态初始化机制，根据不同缺失模态条件配置缓冲区。

Result: 在视觉-语言、视觉-语言-音频和时间多模态基准测试中，REP在单模态和多模态缺失场景下均优于现有方法，同时仅引入可忽略的参数开销。

Conclusion: REP为挑战性模态缺失环境下的鲁棒多模态学习提供了一个轻量级且有效的范式，通过特征重放和智能缓冲区管理显著提升了模型对模态缺失的容忍度。

Abstract: Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.

</details>


### [263] [MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?](https://arxiv.org/abs/2511.23112)
*Yuandong Wang,Yao Cui,Yuxin Zhao,Zhen Yang,Yangfu Zhu,Zhenzhou Shao*

Main category: cs.CV

TL;DR: MathSight是一个大学级多模态数学推理基准，旨在分离和量化视觉输入的作用，发现视觉信息贡献随问题难度增加而减少，纯文本模型甚至优于多模态变体。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在数学推理方面取得进展，但视觉信息真正贡献程度不明确。现有基准很少隔离图像模态作用，不清楚模型是否真正利用视觉理解还是仅依赖语言先验。

Method: 提出MathSight基准，每个问题包含多个视觉变体（原始、手绘、照片拍摄）和纯文本条件进行控制比较，通过实验量化视觉输入效果。

Result: 实验显示视觉信息贡献随问题难度增加而减少，无图像输入的Qwen3-VL甚至优于其多模态变体和GPT-5，表明当前模型视觉理解能力有限。

Conclusion: 需要像MathSight这样的基准来推动未来模型实现真正的视觉基础推理，当前多模态模型在复杂数学推理中过度依赖语言先验而非视觉理解。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.

</details>


### [264] [db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism](https://arxiv.org/abs/2511.23113)
*Siqi Chen,Ke Hong,Tianchen Zhao,Ruiqi Xie,Zhenhua Zhu,Xudong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: 提出db-SP方法解决扩散变换器序列并行中的负载不平衡问题，通过双级分区和动态并行度调整，实现1.25倍端到端加速


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)推理时，使用序列并行技术减少延迟很重要，但当应用于采用分块稀疏注意力的模型时，由于注意力头间稀疏度差异和稀疏掩码中密集块的不规则分布，会导致严重的负载不平衡问题

Method: 提出db-SP稀疏感知序列并行技术：1) 形式化稀疏不平衡比来量化不平衡程度；2) 采用双级分区方法，在头和块级别实现近乎完美的负载平衡；3) 动态确定头和块维度的并行度以适应去噪步骤和层间变化的稀疏模式

Result: 实验结果显示，db-SP相比最先进的序列并行方法，平均实现1.25倍端到端加速和1.40倍注意力特定加速

Conclusion: db-SP有效解决了扩散变换器序列并行中的负载不平衡问题，通过稀疏感知的双级分区和动态并行度调整，显著提升了推理性能

Abstract: Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [265] [Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation](https://arxiv.org/abs/2511.22565)
*Yannick Brunink,Daniel Daza,Yunjie He,Michael Cochez*

Main category: cs.AI

TL;DR: 神经CQA模型并不总是优于简单的无训练查询松弛方法，两者答案重叠度低，结合使用能提升性能，需要重新评估神经方法进展


<details>
  <summary>Details</summary>
Motivation: 神经知识图谱复杂查询回答方法被认为能学习超越显式图结构的模式，但这一假设缺乏系统验证，需要批判性检验神经方法是否真的比简单查询松弛方法更优越

Method: 系统比较神经CQA模型与无训练查询松弛策略，后者通过放松查询约束并计数路径来检索可能答案；分析两者性能相似性、答案重叠度，以及结合使用的效果

Result: 多个数据集和查询结构下，神经方法与松弛方法表现相似，没有神经模型能持续优于后者；两者检索的答案重叠度低，结合使用能一致提升性能

Conclusion: 当前神经模型未能涵盖查询松弛所捕获的推理模式，需要重新评估神经查询回答的进展，强调更强非神经基准的重要性，未来神经方法可受益于融入查询松弛原理

Abstract: Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.

</details>


### [266] [When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming](https://arxiv.org/abs/2511.22302)
*Ahmad Tarraf,Koutaiba Kassem-Manthey,Seyed Ali Mohammadi,Philipp Martin,Lukas Moj,Semih Burak,Enju Park,Christian Terboven,Felix Wolf*

Main category: cs.AI

TL;DR: 提出基于贝叶斯优化的AI辅助工作流，减少参数优化中的专家参与，加速设计空间探索


<details>
  <summary>Details</summary>
Motivation: 数值模拟虽能降低工业设计成本，但大规模模拟需要大量专家知识、计算资源和时间，参数优化迭代成本高且环境影响大

Method: 采用贝叶斯优化和主动学习变体，深度学习模型提供初始参数估计，优化循环迭代改进设计直至满足终止条件

Result: 基于钣金成形过程验证方法，能够加速设计空间探索并减少专家参与需求

Conclusion: AI辅助工作流通过贝叶斯优化有效减少参数优化中的专家参与，提高设计效率并降低计算成本

Abstract: Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.

</details>


### [267] [Enhanced Conditional Generation of Double Perovskite by Knowledge-Guided Language Model Feedback](https://arxiv.org/abs/2511.22307)
*Inhyo Lee,Junhyeong Lee,Jongwon Park,KyungTae Lim,Seunghwa Ryu*

Main category: cs.AI

TL;DR: 提出多智能体文本梯度驱动框架，通过LLM自评估、领域知识反馈和ML代理反馈三种互补机制，在自然语言条件下生成双钙钛矿材料组成，显著提高稳定性和有效性。


<details>
  <summary>Details</summary>
Motivation: 双钙钛矿材料在可持续能源技术中具有广阔前景，但其巨大的设计空间给条件性材料发现带来挑战。现有方法难以在自然语言条件下有效探索这一复杂空间。

Method: 开发多智能体文本梯度驱动框架，整合三种反馈源：1) LLM自评估反馈；2) 双钙钛矿领域知识指导的反馈；3) ML代理模型反馈。通过知识引导的文本梯度指导生成过程，无需额外训练数据。

Result: 框架实现了超过98%的组成有效性，稳定或亚稳定候选材料比例达54%，显著优于纯LLM基线（43%）和先前GAN方法（27%）。ML梯度在分布内区域表现良好，但在分布外区域不可靠。

Conclusion: 该工作首次系统分析了多智能体知识引导文本梯度在双钙钛矿发现中的应用，为MAS驱动的生成材料设计提供了可推广的蓝图，有助于推进可持续能源技术发展。

Abstract: Double perovskites (DPs) are promising candidates for sustainable energy technologies due to their compositional tunability and compatibility with low-energy fabrication, yet their vast design space poses a major challenge for conditional materials discovery. This work introduces a multi-agent, text gradient-driven framework that performs DP composition generation under natural-language conditions by integrating three complementary feedback sources: LLM-based self-evaluation, DP-specific domain knowledge-informed feedback, and ML surrogate-based feedback. Analogous to how knowledge-informed machine learning improves the reliability of conventional data-driven models, our framework incorporates domain-informed text gradients to guide the generative process toward physically meaningful regions of the DP composition space. Systematic comparison of three incremental configurations, (i) pure LLM generation, (ii) LLM generation with LLM reasoning-based feedback, and (iii) LLM generation with domain knowledge-guided feedback, shows that iterative guidance from knowledge-informed gradients improves stability-condition satisfaction without additional training data, achieving over 98% compositional validity and up to 54% stable or metastable candidates, surpassing both the LLM-only baseline (43%) and prior GAN-based results (27%). Analyses of ML-based gradients further reveal that they enhance performance in in-distribution (ID) regions but become unreliable in out-of-distribution (OOD) regimes. Overall, this work provides the first systematic analysis of multi-agent, knowledge-guided text gradients for DP discovery and establishes a generalizable blueprint for MAS-driven generative materials design aimed at advancing sustainable technologies.

</details>


### [268] [Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation](https://arxiv.org/abs/2511.22311)
*Fiona Y. Wang,Di Sheng Lee,David L. Kaplan,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出一种基于群体智能的去中心化代理框架，用于从头蛋白质设计，无需微调或特定任务数据


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质生成方法（如蛋白质语言模型和扩散模型）需要大量微调、特定任务数据或模型重构，限制了灵活性和可扩展性，需要更通用的解决方案

Method: 采用去中心化、基于代理的群体智能框架，多个LLM代理并行操作，每个代理负责特定残基位置，通过迭代提出上下文感知突变，整合设计目标、局部邻域交互以及记忆和反馈

Result: 框架实现了高效、目标导向的设计（仅需几GPU小时），无需微调或专门训练，在α螺旋和卷曲结构蛋白质上实验验证，展示了涌现行为和有效导航蛋白质适应度景观的能力

Conclusion: 该方法为蛋白质设计提供了通用且可适应的解决方案，并为跨生物分子系统和其他科学发现任务的集体LLM驱动设计奠定了基础

Abstract: Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.

</details>


### [269] [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)
*Kumar Tanmay,Kriti Aggarwal,Paul Pu Liang,Subhabrata Mukherjee*

Main category: cs.AI

TL;DR: 提出Mentalese框架，通过超压缩结构化令牌实现高效推理，结合SLPO优化方法，在保持准确率的同时大幅减少推理令牌数和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型依赖冗长的"思考"令牌链，导致高延迟、冗余和不连贯的推理路径。受人类思维语言假说启发，希望开发类似人类紧凑思维风格的推理框架。

Method: 1. 引入Mentalese框架，将抽象推理编码为超压缩、结构化令牌；2. 提出SLPO强化学习方法，奖励简洁且正确的解决方案，同时允许必要时进行更长推理；3. 应用于Mentalese对齐模型。

Result: ORION模型在多个基准测试中：推理令牌减少4-16倍，推理延迟降低5倍，训练成本减少7-9倍，保持DeepSeek R1 Distilled模型90-98%的准确率，同时比Claude和ChatGPT-4o准确率高5%，保持2倍压缩。

Conclusion: Mentalese风格的压缩推理实现了类人认知效率，在不牺牲准确性的前提下实现实时、经济高效的推理，是向高效推理迈出的重要一步。

Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.

</details>


<div id='physics.hist-ph'></div>

# physics.hist-ph [[Back]](#toc)

### [270] [DNNs, Dataset Statistics, and Correlation Functions](https://arxiv.org/abs/2511.21715)
*Robert W. Batterman,James F. Woodward*

Main category: physics.hist-ph

TL;DR: 论文认为数据集结构对图像识别任务很重要，深度神经网络通过发现高阶相关函数来成功分类图像，这类似于凝聚态物理中的介观尺度相关结构方法。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络在图像分类中成功泛化的原因，挑战传统统计学习理论的解释，探索数据集中的相关结构对神经网络性能的重要性。

Method: 借鉴凝聚态物理和材料科学的方法论，分析实际数据集中的相关结构本质和生成机制，特别是介观尺度相关结构，认为DNN必须发现高阶相关函数。

Result: 深度神经网络成功实现图像分类的关键在于发现数据集中的高阶相关函数，这类似于物理科学中研究介观尺度相关结构的方法，为DNN的泛化能力提供了新解释。

Conclusion: 数据集结构特别是相关结构对DNN性能至关重要，DNN通过发现高阶相关函数实现成功分类，这一发现为理解DNN在统计学习理论框架外的泛化能力提供了新视角。

Abstract: This paper argues that dataset structure is important in image recognition tasks (among other tasks). Specifically, we focus on the nature and genesis of correlational structure in the actual datasets upon which DNNs are trained. We argue that DNNs are implementing a widespread methodology in condensed matter physics and materials science that focuses on mesoscale correlation structures that live between fundamental atomic/molecular scales and continuum scales. Specifically, we argue that DNNs that are successful in image classification must be discovering high order correlation functions. It is well-known that DNNs successfully generalize in apparent contravention of standard statistical learning theory. We consider the implications of our discussion for this puzzle.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [271] [Digital Elevation Model Estimation from RGB Satellite Imagery using Generative Deep Learning](https://arxiv.org/abs/2511.21985)
*Alif Ilham Madani,Riska A. Kuswati,Alex M. Lechner,Muhamad Risqi U. Saputra*

Main category: eess.IV

TL;DR: 使用条件GAN从免费RGB卫星影像生成数字高程模型，在山区表现良好但低地和居民区有局限


<details>
  <summary>Details</summary>
Motivation: 传统DEM生成方法（如LiDAR和摄影测量）需要特定数据，在资源受限地区难以获取。本研究旨在利用免费RGB卫星影像生成DEM，为资源受限地区提供成本效益高的替代方案。

Method: 1) 构建全球数据集：使用2000年的Landsat卫星影像和NASA SRTM高程数据，创建12K RGB-DEM配对；2) 预处理：选择高质量无云区域，聚合归一化RGB合成；3) 两阶段训练：先在全数据集上训练，然后用SSIM筛选的高质量样本进行微调，使用条件GAN模型。

Result: 在山区表现良好，整体RMSE为0.4671，SSIM为0.2065（范围-1到1）。但在低地和居民区存在局限性，表明模型在不同地形上的泛化能力仍有挑战。

Conclusion: 该方法为传统DEM生成提供了成本效益高且适应性强的替代方案，强调了在生成建模中细致预处理和迭代优化的重要性，同时指出了在全球多样化地形上泛化的挑战。

Abstract: Digital Elevation Models (DEMs) are vital datasets for geospatial applications such as hydrological modeling and environmental monitoring. However, conventional methods to generate DEM, such as using LiDAR and photogrammetry, require specific types of data that are often inaccessible in resource-constrained settings. To alleviate this problem, this study proposes an approach to generate DEM from freely available RGB satellite imagery using generative deep learning, particularly based on a conditional Generative Adversarial Network (GAN). We first developed a global dataset consisting of 12K RGB-DEM pairs using Landsat satellite imagery and NASA's SRTM digital elevation data, both from the year 2000. A unique preprocessing pipeline was implemented to select high-quality, cloud-free regions and aggregate normalized RGB composites from Landsat imagery. Additionally, the model was trained in a two-stage process, where it was first trained on the complete dataset and then fine-tuned on high-quality samples filtered by Structural Similarity Index Measure (SSIM) values to improve performance on challenging terrains. The results demonstrate promising performance in mountainous regions, achieving an overall mean root-mean-square error (RMSE) of 0.4671 and a mean SSIM score of 0.2065 (scale -1 to 1), while highlighting limitations in lowland and residential areas. This study underscores the importance of meticulous preprocessing and iterative refinement in generative modeling for DEM generation, offering a cost-effective and adaptive alternative to conventional methods while emphasizing the challenge of generalization across diverse terrains worldwide.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [272] [Advancing Marine Bioacoustics with Deep Generative Models: A Hybrid Augmentation Strategy for Southern Resident Killer Whale Detection](https://arxiv.org/abs/2511.21872)
*Bruno Padovese,Fabio Frazao,Michael Dowd,Ruth Joy*

Main category: cs.SD

TL;DR: 研究评估深度生成模型（VAE、GAN、扩散模型）在海洋哺乳动物叫声检测中的数据增强效果，发现扩散模型增强效果最佳，结合传统方法可获得最高性能。


<details>
  <summary>Details</summary>
Motivation: 海洋哺乳动物叫声的自动检测和分类对保护管理至关重要，但面临标注数据有限和真实海洋环境声学复杂性的挑战。虽然数据增强是有效策略，但现有方法多使用简单变换，需要探索深度生成模型是否能提供额外优势。

Method: 评估三种深度生成模型（变分自编码器、生成对抗网络、去噪扩散概率模型）在南方居留型虎鲸叫声检测中的数据增强效果，与传统方法（时间偏移、叫声掩蔽）进行比较，并探索混合策略。

Result: 所有生成方法都提升了分类性能，其中基于扩散的增强获得最高召回率（0.87）和F1分数（0.75）。混合策略（生成合成+传统方法）获得最佳整体性能，F1分数达0.81。

Conclusion: 深度生成模型作为补充增强策略具有潜力，特别是扩散模型和混合方法能显著提升海洋哺乳动物声学监测性能，值得进一步探索以推进濒危物种保护。

Abstract: Automated detection and classification of marine mammals vocalizations is critical for conservation and management efforts but is hindered by limited annotated datasets and the acoustic complexity of real-world marine environments. Data augmentation has proven to be an effective strategy to address this limitation by increasing dataset diversity and improving model generalization without requiring additional field data. However, most augmentation techniques used to date rely on effective but relatively simple transformations, leaving open the question of whether deep generative models can provide additional benefits. In this study, we evaluate the potential of deep generative for data augmentation in marine mammal call detection including: Variational Autoencoders, Generative Adversarial Networks, and Denoising Diffusion Probabilistic Models. Using Southern Resident Killer Whale (Orcinus orca) vocalizations from two long-term hydrophone deployments in the Salish Sea, we compare these approaches against traditional augmentation methods such as time-shifting and vocalization masking. While all generative approaches improved classification performance relative to the baseline, diffusion-based augmentation yielded the highest recall (0.87) and overall F1-score (0.75). A hybrid strategy combining generative-based synthesis with traditional methods achieved the best overall performance with an F1-score of 0.81. We hope this study encourages further exploration of deep generative models as complementary augmentation strategies to advance acoustic monitoring of threatened marine mammal populations.

</details>


### [273] [GLA-Grad++: An Improved Griffin-Lim Guided Diffusion Model for Speech Synthesis](https://arxiv.org/abs/2511.22293)
*Teysir Baoueb,Xiaoyu Bie,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: 改进GLA-Grad声码器，通过单次Griffin-Lim算法校正加速生成过程，在域外场景表现更优


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语音合成中表现出色，但基于梅尔频谱的声码器在条件分布偏离训练数据时效果受限。GLA-Grad模型通过Griffin-Lim算法改进WaveGrad，但需要进一步优化校正过程以加速生成

Method: 提出改进的GLA-Grad方法，仅在反向过程中应用一次Griffin-Lim算法校正，计算校正项后单次应用，显著加速生成过程

Result: 实验结果显示该方法在多个基准模型上表现一致更优，特别是在域外场景（out-of-domain scenarios）中效果显著提升

Conclusion: 通过单次Griffin-Lim算法校正的改进方法有效提升了GLA-Grad声码器的生成效率和性能，尤其在条件分布偏离训练数据时表现优异

Abstract: Recent advances in diffusion models have positioned them as powerful generative frameworks for speech synthesis, demonstrating substantial improvements in audio quality and stability. Nevertheless, their effectiveness in vocoders conditioned on mel spectrograms remains constrained, particularly when the conditioning diverges from the training distribution. The recently proposed GLA-Grad model introduced a phase-aware extension to the WaveGrad vocoder that integrated the Griffin-Lim algorithm (GLA) into the reverse process to reduce inconsistencies between generated signals and conditioning mel spectrogram. In this paper, we further improve GLA-Grad through an innovative choice in how to apply the correction. Particularly, we compute the correction term only once, with a single application of GLA, to accelerate the generation process. Experimental results demonstrate that our method consistently outperforms the baseline models, particularly in out-of-domain scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [274] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文研究了偏好方差（PVar）对DPO训练效果的影响，发现高PVar的提示能产生更大的梯度更新，从而更有效地训练模型。实验表明基于PVar选择训练数据优于随机选择，且仅用10%高PVar数据即可达到优于全数据集的效果。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据收集成本高且效率低，需要减少所需标注的方法。本文旨在研究偏好方差（PVar）对DPO训练效果的影响，探索如何识别信息量更大的训练示例以提高LLM对齐效率。

Method: 1. 理论分析：建立DPO梯度范数上界，证明其受PVar控制；2. 实验验证：使用奖励模型生成偏好数据，在AlpacaEval 2.0和Arena-Hard基准上微调LLMs；3. 数据选择：比较高PVar提示与随机选择或低PVar提示的效果；4. 鲁棒性测试：使用较小奖励模型（1B、3B）进行选择；5. 真实数据验证：在UltraFeedback人类标注数据上测试。

Result: 1. 理论证明PVar控制DPO梯度更新大小；2. 实验显示高PVar提示优于随机选择或低PVar提示；3. 小奖励模型选择方法依然有效；4. 仅使用10%最高PVar提示的训练效果优于使用全数据集；5. PVar能有效识别信息量大的训练示例。

Conclusion: 偏好方差是识别DPO训练中信息量示例的关键指标，高PVar提示能产生更大的梯度更新，从而更有效地训练模型。基于PVar的数据选择方法能显著提高LLM对齐效率，减少所需标注数据量，且方法鲁棒性强。

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for learning from human preferences in aligning large language models (LLMs). However, collecting human preference data is costly and inefficient, motivating methods to reduce the required annotations. In this work, we investigate the impact of \emph{preference variance} (PVar), which measures the variance in model preferences when comparing pairs of responses, on the effectiveness of DPO training. We provide a theoretical insight by establishing an upper bound on the DPO gradient norm for any given prompt, showing it is controlled by the PVar of that prompt. This implies that prompts with low PVar can only produce small gradient updates, making them less valuable for learning. We validate this finding by fine-tuning LLMs with preferences generated by a reward model, evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental results demonstrate that prompts with higher PVar outperform randomly selected prompts or those with lower PVar. We also show that our PVar-based selection method is robust, when using smaller reward models (1B, 3B) for selection. Notably, in a separate experiment using the original human annotations from the UltraFeedback dataset, we found that training on only the top 10\% of prompts with the highest PVar yields better evaluation performance than training on the full dataset, highlighting the importance of preference variance in identifying informative examples for efficient LLM alignment.

</details>


### [275] [47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations](https://arxiv.org/abs/2511.21701)
*Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song,Ziqian Bi*

Main category: cs.CL

TL;DR: 该研究对27个先进大语言模型在中国医学考试题目上进行全面评估，发现模型性能差异显著，Mixtral-8x7B表现最佳，模型大小与性能无直接关联，不同医学专科间存在性能差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型快速发展，其在医学领域的应用潜力备受关注。然而，缺乏针对中文医学考试的系统性评估，特别是不同医学专科和难度级别的性能表现尚不清楚。

Method: 构建包含7个医学专科（心血管、消化、血液、感染、肾脏、神经、呼吸）的2800道题目数据集，分为主治医师和主任医师两个难度级别。采用稳健评估框架对27个先进大语言模型进行系统性测试。

Result: Mixtral-8x7B以74.25%准确率表现最佳，DeepSeek-R1-671B以64.07%次之。模型大小与性能无一致相关性，小型专家混合架构表现优异。心血管和神经科题目表现较好，消化和肾脏科较差。顶尖模型在两个难度级别间性能下降很小。

Conclusion: 该基准为医学教育和临床决策支持系统中的LLM部署提供关键见解，展示了这些技术在专业医学环境中的潜力和当前局限性，强调了需要针对特定医学专科进行优化。

Abstract: The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.

</details>


### [276] [Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry](https://arxiv.org/abs/2511.21703)
*Siyaxolisa Kabane*

Main category: cs.CL

TL;DR: 研究比较LLM与非LLM编码器在文本嵌入中的泛化能力，探讨SLERP模型融合如何缓解任务适配导致的过专业化问题


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLM)与非LLM编码器在文本嵌入任务中的泛化特性差异，以及球形线性插值(SLERP)模型融合方法是否能有效缓解任务特定适配(如LoRA)导致的过专业化问题

Method: 设计受控实验套件，让模型嵌入短数字序列，评估其根据数论特性聚类和分类的能力。比较四类模型：1)从头训练或微调的非LLM编码器；2)参数高效适配(如LoRA)的LLM编码器；3)LoRA适配后通过模型融合合并到基础权重的LLM；4)使用SLERP跨检查点或阶段融合的LoRA适配LLM。使用聚类指标(轮廓系数和Davies Bouldin)评估表示质量

Result: LLM骨干网络能更好地捕捉高阶组合数字模式，但容易产生适配器主导效应，损害平衡泛化能力；SLERP融合能一致地恢复基础模型结构，同时保留大部分任务增益，在聚类可分性、鲁棒性方面优于模型融合或未融合模型

Conclusion: SLERP模型融合方法能有效缓解LLM适配中的过专业化问题，在保持任务性能的同时恢复基础模型的泛化能力，提供更好的权衡

Abstract: We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.

</details>


### [277] [Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation](https://arxiv.org/abs/2511.21711)
*Fatima Kazi*

Main category: cs.CL

TL;DR: 该研究使用StereoSet和CrowSPairs基准测试评估BERT、GPT-3.5等生成模型中的社会、伦理、文化偏见，发现微调模型在性别偏见方面表现不佳但在种族偏见识别上较好，并提出了通过微调、提示工程和数据增强提升模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等大语言模型的普及，这些模型从训练数据中继承的显性和隐性偏见（包括社会、伦理、文化、宗教等）可能产生有害的刻板印象和错误信息，需要系统评估和缓解这些偏见以确保公平输出。

Method: 采用三管齐下的方法：1) 使用StereoSet和CrowSPairs等偏见特定基准测试；2) 评估BERT、GPT-3.5、ADA等多种生成模型；3) 应用增强学习策略，包括微调、不同提示技术和偏见基准数据增强。

Result: 微调模型在性别偏见方面表现不佳，但在识别和避免种族偏见方面表现良好；LLMs过度依赖提示中的关键词，缺乏对输出准确性和真实性的真正理解；通过增强策略，微调模型在跨数据集测试中展现出良好适应性，在隐性偏见基准上性能提升高达20%。

Conclusion: 研究表明需要持续关注和解决LLMs中的偏见问题，微调和数据增强等策略能有效提升模型性能，但LLMs仍存在过度依赖关键词而缺乏深度理解的局限性，需要进一步研究来确保生成AI的公平性和可靠性。

Abstract: Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [278] [PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations](https://arxiv.org/abs/2511.21721)
*Gao Mo,Naveen Raman,Megan Chai,Cindy Peng,Shannon Pagdon,Nev Jones,Hong Shen,Peggy Swarbrick,Fei Fang*

Main category: cs.CL

TL;DR: PeerCoPilot是一个基于大语言模型的助手，帮助同伴提供者创建健康计划、制定分步目标并定位组织资源，通过检索增强生成确保信息可靠性，已在真实环境中部署使用。


<details>
  <summary>Details</summary>
Motivation: 行为健康问题是美国的主要疾病负担，同伴运营的行为健康组织（PROs）在帮助患者方面发挥关键作用，但资金和人员有限，难以满足所有服务用户需求。

Method: 开发PeerCoPilot，一个基于大语言模型的助手，采用检索增强生成（RAG）管道，后端有超过1300个经过审查的资源数据库，确保信息可靠性。

Result: 人类评估显示超过90%的用户支持使用PeerCoPilot，PeerCoPilot比基线LLM提供更可靠和具体的信息，目前已在CSPNJ组织中由5-10名同伴提供者使用，服务超过10,000名用户。

Conclusion: PeerCoPilot是一个有效的LLM驱动助手，能够帮助同伴提供者更好地支持行为健康服务用户，已在真实环境中成功部署并计划扩大使用范围。

Abstract: Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.

</details>


### [279] [Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks](https://arxiv.org/abs/2511.21726)
*Yicong Zheng,Kevin L. McKee,Thomas Miconi,Zacharie Bugaud,Mick van Gelderen,Jed McCaleb*

Main category: cs.CL

TL;DR: SUMER提出了一种基于强化学习的端到端智能体，通过在不压缩的原始记忆中搜索来回答长上下文问题，在LoCoMo数据集上超越了现有记忆压缩方法，性能提升43%


<details>
  <summary>Details</summary>
Motivation: 现有LLM长时记忆框架过度依赖人工设计的压缩算法，这些算法带有特定基准测试的偏见，缺乏通用性。压缩会丢失信息，且预定义的压缩算法无法适应所有原始数据分布

Method: 提出SUMER（Search in Uncompressed Memory via Experience Replay），一个端到端的强化学习智能体，学习使用搜索工具从原始不压缩的记忆中收集信息并回答问题

Result: 在LoCoMo长上下文对话理解数据集上，SUMER使用Qwen2.5-7B-Instruct模型学习使用搜索工具，超越了所有基于记忆压缩的方法和完整上下文基线，达到SOTA性能（相比之前最佳提升43%）

Conclusion: 简单的原始数据搜索方法优于当前长上下文记忆任务中的目标无关和有偏压缩算法，需要更动态和自主可扩展的新范式与基准测试

Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.

</details>


### [280] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu,Xiaofei Han,Yifei Kang,Chiung-Yi Tseng,Danyang Zhang,Ziqian Bi,Zhimo Han*

Main category: cs.CL

TL;DR: 提出AffectMind多模态情感对话代理，通过主动推理和动态知识基础，在营销对话中实现情感对齐和说服性交互，显著提升情感一致性、说服成功率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对话系统大多是被动的，在情感丰富、目标导向的场景（如营销对话）中表现不佳，需要能够主动推理并动态适应情感状态的能力。

Method: AffectMind包含三个核心组件：主动知识基础网络（PKGN）从文本、视觉和韵律中持续更新事实和情感上下文；情感-意图对齐模型（EIAM）联合建模用户情感和购买意图以调整说服策略；强化话语循环（RDL）通过用户反馈的强化信号优化情感连贯性和参与度。

Result: 在两个新构建的营销对话数据集（MM-ConvMarket和AffectPromo）上，AffectMind在情感一致性（+26%）、说服成功率（+19%）和长期用户参与度（+23%）方面显著优于强LLM基线模型。

Conclusion: 情感基础的主动性是多模态商业代理的关键能力，AffectMind通过主动推理和动态知识基础在情感丰富的目标导向对话中表现出色，为商业应用提供了有效的解决方案。

Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.

</details>


### [281] [A Benchmark for Procedural Memory Retrieval in Language Agents](https://arxiv.org/abs/2511.21730)
*Ishant Kohar,Aswanth Krishnan*

Main category: cs.CL

TL;DR: 论文提出了首个评估程序记忆检索的基准测试，分离了程序记忆检索与任务执行，揭示了当前AI代理在遇到新词汇任务时的泛化能力缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在熟悉环境中表现出色，但在面对具有未见词汇的新任务时表现急剧下降，这是程序记忆系统的核心限制。需要评估代理是否能识别跨越不同对象实例的功能等效程序。

Method: 使用ALFWorld构建专家和LLM生成轨迹的双重语料库，评估六种检索方法，使用系统分层查询。通过控制消融实验分析不同方法的性能差异。

Result: 结果揭示了明显的泛化悬崖：基于嵌入的方法在熟悉语境中表现强劲，但在新语境中显著退化；而LLM生成的程序抽象展示了可靠的跨语境迁移能力。语料库规模比表示丰富性带来更大增益。

Conclusion: 该基准测试提供了首个诊断框架，能够区分真正的程序理解与表面记忆，并为开发具有可靠泛化能力的检索系统提供了工具。嵌入方法本质上将程序视为无序词袋，丢弃了跨语境迁移所需的时间结构。

Abstract: Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).

</details>


### [282] [A Multiscale Geometric Method for Capturing Relational Topic Alignment](https://arxiv.org/abs/2511.21741)
*Conrad D. Hougen,Karl T. Pazdernik,Alfred O. Hero*

Main category: cs.CL

TL;DR: 提出一种几何方法，整合多模态文本和合著者网络数据，使用Hellinger距离和Ward连接构建层次主题树状图，有效识别稀有主题并可视化平滑的主题漂移。


<details>
  <summary>Details</summary>
Motivation: 在科学文献中，识别研究兴趣如何随合著者社区演变很重要，特别是发现被忽视的利基主题。当前基于密集Transformer嵌入的模型往往遗漏稀有主题，且无法捕捉平滑的时间对齐。

Method: 提出几何方法整合多模态文本和合著者网络数据，使用Hellinger距离和Ward连接构建层次主题树状图，支持跨语义和时间维度的多尺度学习。

Result: 该方法能有效识别稀有主题结构，可视化随时间平滑的主题漂移，实验表明可解释的词袋模型与原则性几何对齐结合具有优势。

Conclusion: 通过整合多模态数据和几何方法，能够更好地捕捉科学文献中的稀有主题和平滑时间演变，为可解释的主题建模提供新思路。

Abstract: Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.

</details>


### [283] [Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models](https://arxiv.org/abs/2511.21759)
*Linye Wei,Wenjue Chen,Pingzhi Tang,Xiaotian Guo,Le Ye,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: ODB-dLLM通过自适应长度预测和跳转共享推测解码技术，显著加速扩散大语言模型推理，实现46-162倍加速


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型框架虽然支持KV缓存，但其双向注意力机制需要周期性缓存刷新，导致预填充和解码阶段交错，带来大量计算开销并限制加速效果。预填充阶段固定响应长度引入冗余计算，解码阶段迭代次数多影响效率。

Method: 提出ODB-dLLM框架，采用双边界协调策略：1) 预填充阶段引入自适应长度预测机制，渐进减少预填充开销和不必要计算；2) 解码阶段提出dLLM特定的跳转共享推测解码方法，减少解码迭代次数。

Result: 实验结果显示，ODB-dLLM相比基线dLLM实现46-162倍加速，相比Fast-dLLM实现2.63-6.30倍加速，同时减轻了现有加速框架的准确度下降问题。

Conclusion: ODB-dLLM通过协调预填充和解码阶段的双边界，有效解决了扩散大语言模型推理效率问题，在保持准确性的同时实现了显著的加速效果。

Abstract: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.

</details>


### [284] [FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers](https://arxiv.org/abs/2511.21843)
*Sarina Xi,Vishisht Rao,Justin Payan,Nihar B. Shah*

Main category: cs.CL

TL;DR: FLAWS是一个用于评估LLMs在科研论文中检测和定位错误能力的自动化基准，包含713个论文-错误对，GPT-5在k=10时达到39.1%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 随着科学产出的指数级增长，人类审稿人难以可靠地检测错误，而LLMs在错误检测方面的能力尚未得到充分探索，需要系统性的评估基准。

Method: 通过LLMs系统性地在同行评审论文中插入破坏性错误，构建包含713个论文-错误对的FLAWS基准，并设计自动化评估指标来衡量模型识别和定位错误的能力。

Result: 在评估的五个前沿LLMs中，GPT-5表现最佳，在k=10（LLM生成的top-k错误文本候选）时达到39.1%的识别准确率。

Conclusion: FLAWS基准为评估LLMs在科学错误检测方面的能力提供了系统框架，GPT-5表现最佳但仍有改进空间，该基准有助于推动LLMs在学术评审中的应用。

Abstract: The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.

</details>


### [285] [AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models](https://arxiv.org/abs/2511.22016)
*Yann Le Beux,Oluchi Audu,Oche D. Ankeli,Dhananjay Balakrishnan,Melissah Weya,Marie D. Ralaiarinosy,Ignatius Ezeani*

Main category: cs.CL

TL;DR: AfriStereo：首个基于非洲社会文化背景的开源非洲刻板印象数据集和评估框架，填补了AI偏见评估中非洲视角的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI偏见评估基准主要反映西方视角，导致非洲背景代表性不足，使得各种应用中的有害刻板印象得以延续。

Method: 通过塞内加尔、肯尼亚和尼日利亚的社区参与收集了1,163个刻板印象；使用少样本提示和人工循环验证将数据集扩充至5,000多个刻板印象-反刻板印象对；通过语义聚类和文化知情评审员手动标注进行验证。

Result: 11个语言模型中有9个表现出统计显著的偏见，偏见偏好比(BPR)在0.63到0.78之间(p≤0.05)，表明对刻板印象的系统性偏好；领域特定模型在设置中表现出较弱的偏见。

Conclusion: AfriStereo为基于文化的偏见评估和缓解研究开辟了途径，为AI社区提供了构建更公平、情境感知和全球包容性NLP技术的关键方法。

Abstract: Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.

</details>


### [286] [ResearchArcade: Graph Interface for Academic Tasks](https://arxiv.org/abs/2511.22036)
*Jingjun Xu,Chongshan Lin,Haofei Yu,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: ResearchArcade是一个基于图结构的统一数据接口，连接多个学术数据源，支持多模态信息，统一任务定义，用于开发支持各种学术任务的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 学术研究产生多样化的数据源，随着研究人员越来越多地使用机器学习辅助研究任务，需要构建统一的数据接口来支持开发适用于各种学术任务的机器学习模型，从而加速知识发现。

Method: 提出ResearchArcade，采用基于图结构的接口，使用一致的多表格式组织来自不同来源的数据（如ArXiv学术语料和OpenReview同行评审），支持文本、图表等多模态信息，并保留手稿和社区层面的时间演化信息。

Result: 在六个学术任务上的实验表明，结合跨源和多模态信息能够支持更广泛的任务范围，而图结构的加入相比基线方法持续提升性能，证明了ResearchArcade的有效性。

Conclusion: ResearchArcade展示了构建统一学术数据接口的潜力，能够有效支持机器学习模型开发，促进研究进展。

Abstract: Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.

</details>


### [287] [Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization](https://arxiv.org/abs/2511.22818)
*Vivek Kumar,Pushpraj Singh Rajawat,Eirini Ntoutsi*

Main category: cs.CL

TL;DR: 该研究评估大语言模型在心理治疗中的应用，通过动机性访谈对话摘要生成和MITI框架标注，分析LLMs对复杂心理学概念的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在敏感领域如心理学中存在敏感性不足、事实错误、共情表达不一致、偏见、幻觉等问题，难以捕捉人类理解的深度和复杂性，特别是在低资源领域。需要评估LLMs在心理治疗中的有效性。

Method: 采用混合方法评估LLMs在心理治疗中的效能：1) 使用LLMs生成动机性访谈对话的精确摘要；2) 基于MITI框架（唤起、合作、自主、方向、共情、非评判态度）设计两阶段标注方案；3) 使用专家标注的MI对话作为基准，构建多分类任务；4) 采用渐进式提示技术，包括单样本和少样本提示。

Result: 研究结果提供了LLMs理解复杂心理学概念能力的见解，并提出了在治疗环境中减轻"语义漂移"的最佳实践。为MI社区提供了高质量标注数据集，解决了低资源领域的数据稀缺问题。

Conclusion: 该研究不仅为动机性访谈社区贡献了高质量标注数据集，还为在复杂行为治疗中使用LLMs进行精确上下文解释提供了关键见解，有助于改善LLMs在敏感心理学领域的应用。

Abstract: Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.

</details>


### [288] [Language-conditioned world model improves policy generalization by reading environmental descriptions](https://arxiv.org/abs/2511.22904)
*Anh Nguyen,Stefan Lee*

Main category: cs.CL

TL;DR: 提出LED-WM方法，在DreamerV3基础上构建语言感知世界模型，通过注意力机制将语言描述显式地关联到观察中的实体，无需规划或专家演示就能实现更好的策略泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法证明策略在未见游戏中的泛化能力，要么依赖限制性假设（如容忍推理时规划延迟或需要专家演示）。需要改进从语言条件世界模型的策略泛化，同时去除这些假设。

Method: 提出LED-WM（Language-aware Encoder for Dreamer World Model），基于DreamerV3构建。使用注意力机制让观察编码器显式地将语言描述关联到观察中的实体，通过与环境交互训练语言条件世界模型，并从中学习策略，无需规划或专家演示。

Result: 在MESSENGER和MESSENGER-WM两个环境中，LED-WM训练的策略相比其他基线方法，在由新动态和语言描述的未见游戏中表现出更有效的泛化能力。还展示了策略可以通过世界模型生成的合成测试轨迹进行微调改进。

Conclusion: LED-WM方法能够有效提升从语言条件世界模型学习的策略泛化能力，无需依赖规划或专家演示，为现实世界部署前的策略改进提供了新途径。

Abstract: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.

</details>


### [289] [Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification](https://arxiv.org/abs/2511.22977)
*Sumit Mamtani,Abhijeet Bhure*

Main category: cs.CL

TL;DR: 本文研究将假新闻检测作为Transformer表示的下游评估任务，比较了编码器与解码器预训练模型作为冻结嵌入器与轻量级分类器配对的性能，发现BERT嵌入结合逻辑回归在LIAR数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer表示在假新闻检测任务中的有效性，隔离Transformer架构贡献与分类器复杂性，为真实性验证任务提供架构中心的基础。

Method: 使用冻结的预训练模型（BERT、GPT-2、Transformer-XL）作为嵌入器，配合轻量级分类器（神经网络与线性头）。通过控制预处理比较池化与填充策略，分析序列长度和聚合方法的影响。

Result: 上下文自注意力编码能有效迁移，BERT嵌入结合逻辑回归在LIAR数据集上优于神经网络基线。分析显示模型对截断具有鲁棒性，简单最大或平均池化具有优势。

Conclusion: 基于注意力的标记编码器可作为真实性验证任务的稳健、架构中心的基础，成功隔离了Transformer贡献与分类器复杂性。

Abstract: This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.

</details>


### [290] [Standard Occupation Classifier -- A Natural Language Processing Approach](https://arxiv.org/abs/2511.23057)
*Sidharth Rony,Jack Patman*

Main category: cs.CL

TL;DR: 使用自然语言处理技术构建职业分类器，通过集成Google BERT和神经网络模型，结合职位标题、描述和技能信息，实现对工作广告的SOC代码自动分类，最高准确率达61%（第四层）和72%（第三层）。


<details>
  <summary>Details</summary>
Motivation: 将标准职业分类系统（SOC）与大数据工作广告相结合，可以研究特定职业的劳动力需求。传统方法可能无法及时捕捉劳动力市场变化，需要自动化分类工具来提供实时准确的劳动力市场信息。

Method: 开发了多种分类器，针对英国ONS SOC和美国O*NET SOC系统，使用不同的语言模型。最终采用集成模型，结合Google BERT和神经网络分类器，同时考虑职位标题、工作描述和技能信息。

Result: 集成模型在SOC第四层分类准确率达到61%，第三层达到72%。该模型能够使用工作广告提供及时准确的劳动力市场演变信息。

Conclusion: 基于自然语言处理的集成模型能够有效自动分类工作广告的SOC代码，为劳动力市场分析提供实时数据支持，有助于研究特定职业的劳动力需求变化。

Abstract: Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.

</details>


### [291] [TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies](https://arxiv.org/abs/2511.23225)
*Guang Liang,Jie Shao,Ningyuan Tang,Xinyao Liu,Jianxin Wu*

Main category: cs.CL

TL;DR: TWEO是一种新颖的非侵入式损失函数，通过简单的损失项有效防止Transformer训练中的极端异常值，实现全模型FP8预训练，无需工程技巧或架构修改，显著提升训练吞吐量并实现硬件友好的W8A8量化。


<details>
  <summary>Details</summary>
Motivation: 现代硬件对FP8的原生支持对于训练大型Transformer至关重要，但极端激活异常值严重阻碍了其应用。现有解决方案要么依赖复杂的混合精度工程，要么需要进行侵入式架构修改。本文从根本上挑战了异常值是数据驱动的传统观点。

Method: 论文证明极端异常值是训练过程中产生的机械性伪影，源于权重矩阵的特定结构特性（即共线性）。基于这一洞察，提出了TWEO（Transformers Without Extreme Outliers），这是一种新颖的非侵入式损失函数，通过简单的损失项有效防止极端异常值。

Result: TWEO将异常值从10000+减少到小于20，实现全模型FP8预训练（包括LLM和ViT），无需工程技巧或架构修改。当标准FP8训练灾难性崩溃时，TWEO实现与BF16基线相当的性能，同时训练吞吐量提升36%。此外，TWEO实现了新的量化范式，使硬件友好的W8A8每张量静态量化首次在LLM上达到SOTA性能。

Conclusion: TWEO通过简单的损失函数有效解决Transformer训练中的极端异常值问题，为FP8训练和硬件友好量化开辟了新途径，显著提升了训练效率和量化性能。

Abstract: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [292] [What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$](https://arxiv.org/abs/2511.22442)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 该论文分析了Fβ分数作为精确率和召回率权衡指标的合理性，发现F1分数并非最优权衡，并提供了寻找最优β值的理论工具。


<details>
  <summary>Details</summary>
Motivation: 在分类任务中，精确率和召回率都是重要的多维性能指标，但两者常常存在矛盾。Fβ分数作为加权调和平均数被广泛使用，但其是否能产生有意义的排名和良好的权衡效果缺乏理论保证。

Method: 1) 证明Fβ诱导的排名是有意义的，并定义了精确率和召回率排名之间的最短路径；2) 将两个分数之间的权衡问题表述为基于Kendall秩相关的优化问题；3) 提供理论工具和闭式表达式来寻找任何分布或性能集的最优β值。

Result: 研究发现Fβ诱导的排名确实有意义，但F1分数及其偏斜不敏感版本在权衡精确率和召回率方面远非最优。通过六个案例研究展示了如何找到最优β值。

Conclusion: Fβ分数提供了精确率和召回率之间的合理权衡，但常用的F1分数通常不是最优选择。论文提供了理论框架和工具来根据具体分布或数据集确定最优的β值，从而获得更好的性能权衡。

Abstract: Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_β$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_β$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_β$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $β$ for any distribution or set of performances, and we illustrate their use on six case studies.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [293] [The Machine Learning Approach to Moment Closure Relations for Plasma: A Review](https://arxiv.org/abs/2511.22486)
*Samuel Burles,Enrico Camporeale*

Main category: physics.plasm-ph

TL;DR: 本文综述了机器学习方法在等离子体闭合模型中的应用，旨在开发能够捕捉动力学现象的改进闭合关系，以支持大规模全局模拟。


<details>
  <summary>Details</summary>
Motivation: 等离子体的大规模全局模拟在空间和实验室等离子体物理中是一个持续挑战。基于流体模型的模拟需要高阶等离子体矩的闭合关系，而传统方法难以准确捕捉动力学现象。

Method: 综述分析了近期涌现的机器学习方法，包括方程发现方法和神经网络代理方法，用于开发改进的等离子体闭合模型。这些方法旨在将动力学现象纳入等离子体流体模型中。

Result: 本文收集并分析了各种应用于等离子体闭合问题的方法，提供了该领域现状的全面概述，特别强调了开发数据驱动闭合模型面临的挑战。

Conclusion: 未来工作需要解决数据驱动闭合模型开发中的挑战，朝着计算可行的大规模全局模拟方向发展，以实现更准确高效的等离子体模拟。

Abstract: The requirement for large-scale global simulations of plasma is an ongoing challenge in both space and laboratory plasma physics. Any simulation based on a fluid model inherently requires a closure relation for the high order plasma moments. This review compiles and analyses the recent surge of machine learning approaches developing improved plasma closure models capable of capturing kinetic phenomena within plasma fluid models. The purpose of this review is both to collect and analyse the various methods employed on the plasma closure problem, including both equation discovery methods and neural network surrogate approaches, as well as to provide a general overview of the state of the problem. In particular, we highlight the challenges of developing a data-driven closure as well as the direction future work should take toward addressing these challenges, in the pursuit of a computationally viable large-scale global simulation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [294] [Automated Statistical and Machine Learning Platform for Biological Research](https://arxiv.org/abs/2511.21770)
*Luke Rimmo Lego,Samantha Gauthier,Denver Jn. Baptiste*

Main category: q-bio.QM

TL;DR: 研究人员开发了一个集成平台，将经典统计方法与随机森林分类相结合，为生物科学研究提供统一的数据分析工具，无需大量编程经验。


<details>
  <summary>Details</summary>
Motivation: 当前研究方法需要研究人员使用多种工具进行统计分析和机器学习，导致工作流程效率低下。传统统计软件、现代机器学习框架与生物学研究之间存在鸿沟，需要为没有丰富编程经验的研究人员提供统一接口。

Method: 平台整合了经典统计方法（t检验、ANOVA、Pearson相关分析）和随机森林分类，实现了自动超参数优化、特征重要性分析、自动数据预处理、分类编码以及基于数据集特征的自适应模型配置。

Result: 平台通过集成统计严谨性和机器学习可解释性，加速了生物发现工作流程，同时保持了方法的科学性。模块化架构支持未来扩展到更多与生物信息学相关的机器学习算法和统计程序。

Conclusion: 该集成平台成功弥合了传统统计软件、现代机器学习框架与生物学研究之间的差距，为研究人员提供了统一、易用的数据分析解决方案，能够加速生物科学发现。

Abstract: Research increasingly relies on computational methods to analyze experimental data and predict molecular properties. Current approaches often require researchers to use a variety of tools for statistical analysis and machine learning, creating workflow inefficiencies. We present an integrated platform that combines classical statistical methods with Random Forest classification for comprehensive data analysis that can be used in the biological sciences. The platform implements automated hyperparameter optimization, feature importance analysis, and a suite of statistical tests including t tests, ANOVA, and Pearson correlation analysis. Our methodology addresses the gap between traditional statistical software, modern machine learning frameworks and biology, by providing a unified interface accessible to researchers without extensive programming experience. The system achieves this through automatic data preprocessing, categorical encoding, and adaptive model configuration based on dataset characteristics. Initial testing protocols are designed to evaluate classification accuracy across diverse chemical datasets with varying feature distributions. This work demonstrates that integrating statistical rigor with machine learning interpretability can accelerate biological discovery workflows while maintaining methodological soundness. The platform's modular architecture enables future extensions to additional machine learning algorithms and statistical procedures relevant to bioinformatics.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [295] [Quantum Private Distributed Matrix Multiplication With Degree Tables](https://arxiv.org/abs/2511.23406)
*Mohamed Nomeir,Alptug Aytekin,Lei Hu,Sennur Ulukus*

Main category: cs.IT

TL;DR: 量子资源用于提升私有分布式矩阵乘法速率，通过量子纠缠和量子信道减少所需服务器数量


<details>
  <summary>Details</summary>
Motivation: 传统私有分布式矩阵乘法需要大量服务器，量子资源（纠缠态和量子信道）可能减少所需服务器数量，提高效率

Method: 1. 高隐私场景：分析GASP码的可行性条件，开发新量子码族；2. 低隐私场景：将GASP可行性条件扩展到CAT和DOG码，提出新量子码

Result: 建立了量子设置中GASP码的可行性条件，开发了适用于高隐私和低隐私场景的新量子码族

Conclusion: 量子资源能有效提升私有分布式矩阵乘法效率，减少所需服务器数量，为量子分布式计算提供新方案

Abstract: In this paper, we explore how quantum resources can be used to increase the rate of private distributed matrix multiplication (PDMM). In PDMM, a user who has two high-dimensional matrices, $A$ and $B$, and lacks the computational capabilities to apply matrix multiplication locally, divides the matrices $A$ and $B$ into $K$ and $L$ sub-blocks, respectively. Then, the user sends them to $N$ servers to apply the required multiplication privately from any $T$ servers. The goal is to reduce the number of servers needed to perform the required matrix multiplication. In the quantum setting, we allow the servers to share an entangled state and respond over quantum channels. Upon receiving the qudits, the user applies measurements to obtain the required multiplication. There are two main regimes in the PDMM literature: The high-privacy regime and the low-privacy regime where $T$ is less than $K$ and $L$.
  First, in the high-privacy regime, the state-of-the-art classical code is called the gap additive secure polynomial (GASP) code. We define a feasibility requirement in the quantum setting for the GASP code such that the highest performance is achieved when it is satisfied. When it is not satisfied, we address two main concerns. The first is to find a relation between the minimum privacy requirement and the dimensions of the two matrices needed for the feasibility condition to be satisfied. Second, we develop a new family of codes that can work in the quantum setting.
  Second, since GASP does not work efficiently in the low-privacy regimes compared to cyclic-addition degree tables (CAT) and discretely optimized GASP (DOG), we show that the feasibility condition developed for GASP can be adopted for both CAT and DOG codes as well. In addition, we propose another set of codes that can be used in the low privacy regime in the quantum setting when the feasibility requirement is not satisfied.

</details>
