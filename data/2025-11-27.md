<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 87]
- [math.NA](#math.NA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 15]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [eess.SY](#eess.SY) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [math.CO](#math.CO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference](https://arxiv.org/abs/2511.21160)
*Wu Sai,Xia Ruichen,Yang Dingyu,Wang Rui,Lai Huihang,Guan Jiarui,Bai Jiameng,Zhang Dongxiang,Tang Xiu,Xie Zhongle,Lu Peng,Chen Gang*

Main category: cs.DB

TL;DR: MorphingDB是一个任务中心的AI原生数据库系统，在PostgreSQL中自动化模型存储、选择和推理，通过专门的模式和多维张量数据类型实现灵活存储，采用两阶段迁移学习框架进行模型选择，并通过预嵌入和DAG批处理优化推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI原生DBMS解决方案存在模型中心设计需要手动管理模型的高开发开销，或任务中心AutoML方法计算成本高且与DBMS集成差的问题，需要一种平衡准确率、资源消耗和时间成本的自动化解决方案。

Method: 1) 引入专门模式和多维张量数据类型支持BLOB存储；2) 两阶段迁移学习框架：离线构建历史任务嵌入的迁移性子空间，在线通过特征感知映射进行投影；3) 预嵌入与向量共享消除冗余计算，DAG批处理流水线配合成本感知调度最小化推理时间。

Result: 在9个公共数据集（时序、NLP、图像任务）上评估，MorphingDB优于AI原生DBMS（EvaDB、Madlib、GaussML）和AutoML平台（AutoGluon、AutoKeras、AutoSklearn），在模型选择中实现了准确率、资源消耗和时间成本的稳健平衡，在吞吐量和资源效率方面获得显著提升。

Conclusion: MorphingDB作为PostgreSQL扩展，成功实现了AI原生数据库的自动化模型管理，在保持高性能的同时显著降低了开发开销和计算成本，为深度学习推理在数据库环境中的集成提供了有效解决方案。

Abstract: The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.

</details>


### [2] [HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads](https://arxiv.org/abs/2511.21307)
*Xinyi Zhang,Liang Liang,Anastasia Ailamaki,Jianliang Xu*

Main category: cs.DB

TL;DR: HIRE是一种混合内存索引结构，结合传统索引的稳健性和学习索引的预测能力，在范围查询吞吐量、尾延迟和整体稳定性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 学习索引在点查询中表现优异，但在尾延迟、范围查询性能和不同工作负载下的稳定性方面存在不足，需要一种能保持最坏情况稳定性的混合解决方案。

Method: 采用混合叶子节点适应不同数据分布和工作负载；模型加速的内部节点结合基于日志的更新；非阻塞成本驱动的重新校准机制；考虑叶子节点和内部节点错误的批量加载算法。

Result: 在多个真实数据集上的实验表明，HIRE在混合工作负载下吞吐量提升高达41.7倍，尾延迟降低高达98%，在范围查询吞吐量、尾延迟和整体稳定性方面优于现有学习索引和传统索引。

Conclusion: HIRE成功结合了传统索引的稳健性和学习索引的效率，为现代数据库提供了高效且稳定的索引解决方案。

Abstract: Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.

</details>


### [3] [Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation](https://arxiv.org/abs/2511.21607)
*Zarin Tahia Hossain,Mostafa Milani*

Main category: cs.DB

TL;DR: 系统评估了不同插补方法的不确定性校准性能，发现准确性和校准性往往不一致，为数据清洗和下游机器学习提供了不确定性感知的插补方法选择指南。


<details>
  <summary>Details</summary>
Motivation: 现代插补方法不仅追求准确重建，还通过不同方式表示和量化不确定性，但这些不确定性估计的可靠性和校准性仍缺乏系统研究。

Method: 系统比较了三大类代表性插补方法：统计方法（MICE、SoftImpute）、分布对齐方法（OT-Impute）和深度生成方法（GAIN、MIWAE、TabCSDI），通过多轮变异性、条件采样和预测分布建模三种途径估计不确定性，并使用校准曲线和期望校准误差进行评估。

Result: 准确性和校准性经常不一致：重建精度高的模型不一定能提供可靠的不确定性估计。分析了方法在准确性、校准性和运行时间之间的权衡，确定了稳定配置。

Conclusion: 为数据清洗和下游机器学习流水线提供了选择不确定性感知插补方法的实用指南，强调了在评估插补方法时考虑不确定性校准的重要性。

Abstract: Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 提出了一种基于随机Petri网的方法来分析私有云环境中Nextcloud文件服务器的可用性，评估了四种冗余策略对系统可用性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中的普及，可靠性成为关键需求，特别是对于寻求替代公共云服务的组织而言，评估这些系统的可靠性至关重要。

Method: 使用随机Petri网(SPNs)建模方法，评估了四种架构配置：基准配置、主机级冗余、虚拟机冗余以及两者组合的冗余策略。

Result: 结果显示，在主机和虚拟机级别同时实施冗余能显著提高可用性并减少预期停机时间。

Conclusion: 所提出的方法为评估私有云可用性和支持基础设施设计决策提供了一种有效手段。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [5] [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)
*Dionysios Adamopoulos,Anastasia Poulopoulou,Georgios Goumas,Christina Giannoula*

Main category: cs.DC

TL;DR: Spira是一个基于GPU的稀疏卷积引擎，通过利用体素坐标的整数性、空间有界性和几何连续性，显著提升了3D点云网络的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏卷积引擎未能充分利用体素坐标的三个关键特性（整数性、空间有界性、几何连续性），导致核映射构建时的预处理和后处理开销过高。

Method: 提出Spira引擎，包含：一次性搜索算法构建核映射、压缩原生处理方案访问体素坐标、双数据流执行机制适应层特性、网络级并行化策略并发构建所有层核映射。

Result: Spira在端到端推理上平均提升1.71倍（最高2.31倍），在逐层执行上平均提升2.13倍（最高3.32倍）。

Conclusion: Spira通过充分利用体素坐标特性，显著提升了稀疏卷积的计算效率，为3D点云处理提供了高性能解决方案。

Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.

</details>


### [6] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 提出了一种在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务LLM的解决方案，能够高效处理100-1000个并发请求，端到端延迟仅增加约500ms。


<details>
  <summary>Details</summary>
Motivation: 随着AI推理需求增长，特别是在高等教育领域，需要利用现有基础设施的新解决方案。传统HPC操作模型不适用于同步、面向用户的动态AI应用工作负载。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务大型语言模型。

Result: 初步基准测试表明，该架构能够高效扩展处理100、500和1000个并发请求，端到端延迟仅增加约500毫秒。

Conclusion: 提出的集成架构成功解决了传统HPC在AI推理服务中的局限性，实现了高效的并发处理能力。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [7] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog系统通过动态调整工作流配置来优化多阶段LLM推理任务的性能和成本，在保持精度的同时显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的工作流配置方法在请求执行前就固定配置，无法适应系统负载的动态变化，导致在异构和长时间执行的工作流中效率低下。

Method: Aragog将问题解耦为一次性路由步骤和每阶段调度器，前者识别所有保持精度的配置，后者基于实时系统观察选择最优配置。

Result: 在多样化工作流和模型家族中，Aragog将最大服务吞吐量提升50.0-217.0%，在峰值请求率下将中位延迟降低32.5-78.9%。

Conclusion: Aragog证明了在LLM工作流服务中动态配置调整的有效性，能够在保持精度的同时显著改善系统性能。

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [8] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD是一个动态LLM推理系统，通过实时监控负载动态调整prefill和decoding实例分配比例，解决异构工作负载下的生产者-消费者不平衡问题，显著提升系统吞吐量和响应性能。


<details>
  <summary>Details</summary>
Motivation: 现代LLM将prefill和decoding阶段解耦到不同GPU上以应对各自的瓶颈，但异构工作负载导致这种解耦架构中两种实例类型之间的生产者-消费者不平衡。

Method: 提出DOPD系统，基于实时负载监控动态调整实例分配以达到最优的prefill-to-decoding比例，结合适当的请求调度策略解决实例不平衡和混合长度请求的资源分配不匹配问题。

Result: 相比vLLM和DistServe，DOPD将系统吞吐量提升1.5倍，P90首token时间降低67.5%，P90每输出token时间降低22.8%。动态P/D调整技术基于历史负载进行主动重配置，使用更少额外资源实现超过99%的SLO达标率。

Conclusion: DOPD通过动态调整prefill和decoding实例比例，有效解决了LLM推理中的资源分配不平衡问题，显著提升了系统性能和资源利用率。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [9] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 评估在Patra模型卡系统中采用MCP协议作为接口的收益与权衡，包括性能开销和动态模型卡会话支持能力


<details>
  <summary>Details</summary>
Motivation: 传统模型卡在训练时的一次性评估无法反映模型在实际使用过程中的动态表现，需要研究动态模型卡系统

Method: 在ICICLE AI Institute软件生态中嵌入Patra模型卡，研究MCP协议作为模型卡服务器接口的性能和适用性

Result: 定量评估显示MCP相比REST接口有额外开销，但核心价值在于支持动态模型卡的活跃会话能力

Conclusion: MCP协议虽然存在性能开销，但在支持动态模型卡的活跃会话方面具有重要价值

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>


### [10] [Handling of Memory Page Faults during Virtual-Address RDMA](https://arxiv.org/abs/2511.21018)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文实现了一种与DMA引擎集成的页面错误处理机制，通过ARM SMMU检测错误并通过软硬件协同解决方案解决，避免了传统RDMA技术中内存固定的缺点。


<details>
  <summary>Details</summary>
Motivation: 传统RDMA技术无法容忍页面错误，需要固定内存地址空间，这带来了编程复杂性、内存使用限制和效率低下等问题。现代操作系统优化机制（如透明大页）使得固定方法无法完全避免页面错误。

Method: 在ExaNeSt项目中集成页面错误处理机制到DMA引擎，通过ARM SMMU检测页面错误，采用软硬件协同解决方案请求重传。需要修改Linux SMMU驱动、开发新软件库、调整DMA引擎硬件和调度逻辑。

Result: 在ExaNeSt的Quad-FPGA Daughter Board上进行了实验评估，该平台使用Xilinx Zynq UltraScale+ MPSoCs。

Conclusion: 提出的机制相比固定内存和预错误处理等替代方案具有优势，能够有效处理页面错误而不需要固定内存空间。

Abstract: Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.
  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.
  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.
  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.

</details>


### [11] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine是一个内存感知的细粒度调度框架，通过分块重计算策略解决MoE训练中的内存瓶颈问题，在内存受限的GPU上实现稳定的大规模MoE训练。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型训练面临严重的内存瓶颈，动态令牌路由导致的负载不平衡会造成GPU内存溢出，限制了模型的可扩展性。现有的负载平衡方法会牺牲模型精度，且在内存受限的硬件上失效。

Method: 将令牌分布和专家计算分解为可管理的块，采用分块重计算策略，通过理论内存模型动态优化以平衡内存效率和吞吐量。

Result: 相比完全重计算基线，MemFine减少了48.03%的激活内存，提高了4.42%的吞吐量。

Conclusion: MemFine能够在内存受限的GPU上实现稳定的大规模MoE训练，有效解决了内存瓶颈问题。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [12] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 通过引入数据冗余改善MLFMA中近场算子的GPU内存局部性，提出基于局部性度量的分析模型预测性能趋势，在电磁求解器和恒星动力学代码中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场算子在GPU上因内存局部性差成为性能瓶颈，需要改进内存访问模式。

Method: 引入数据冗余减少内存访问分散，提出结合数据量和访问分散度的局部性度量分析模型，在两种MLFMA应用中验证。

Result: 内核速度提升最高达7倍，但由于数据重构开销增加，端到端应用加速比限制在1.04倍。模型能可靠捕捉性能趋势但无法精确预测绝对加速比。

Conclusion: 数据冗余可提升GPU上P2P算子性能，前提是局部性收益超过数据移动成本，该方法可最小代码修改注入现有实现。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


### [13] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 提出了二维缩放平面模型，通过联合水平（节点数）和垂直（单节点资源）调整的斜向缩放策略，相比传统单向缩放能显著降低延迟、成本和重平衡开销。


<details>
  <summary>Details</summary>
Motivation: 现代云数据库将缩放简化为水平或垂直的单向决策，忽略了性能、成本和协调开销来自水平弹性与单节点资源的联合交互，导致系统对负载波动反应不当或陷入次优状态。

Method: 引入缩放平面二维模型，将数据库配置表示为(H,V)点；定义延迟、吞吐量、协调开销和成本的平滑近似；提出DIAGONALSCALE算法，在缩放平面中评估水平、垂直和斜向移动，选择满足SLA约束的最优配置。

Result: 斜向缩放相比单向自动缩放，p95延迟降低达40%，查询成本降低达37%，重平衡减少2-5倍。

Conclusion: 需要多维缩放模型，为下一代云数据库自动缩放提供基础，斜向缩放路径能同时利用集群并行性和单节点改进。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [14] [Exploiting Low Scanwidth to Resolve Soft Polytomies](https://arxiv.org/abs/2511.20771)
*Sebastian Bruchhold,Mathias Weller*

Main category: cs.DS

TL;DR: 本文提出了一个解决Soft Tree Containment问题的算法，该算法在参数化复杂性框架下具有更好的时间复杂度，特别适用于实践中常见的低scanwidth系统发育网络。


<details>
  <summary>Details</summary>
Motivation: 传统的Tree Containment问题对生物数据中支持度不高的分支不够鲁棒，可能导致假阴性结果。Soft Tree Containment作为其松弛版本，能够更好地处理数据不确定性。

Method: 利用系统发育网络中常见的低scanwidth特性，开发了一个参数化算法，时间复杂度为2^{O(Δ_T·k·log(k))}·n^{O(1)}，其中k=sw(Γ)+Δ_N，sw(Γ)表示网络树扩展的scanwidth。

Result: 算法在参数化复杂性框架下是高效的，特别是当系统发育网络具有低scanwidth时，问题变得更加可处理。

Conclusion: 该工作为处理系统发育网络中的不确定性提供了一种实用的计算方法，扩展了Tree Containment问题的应用范围。

Abstract: Phylogenetic networks allow modeling reticulate evolution, capturing events such as hybridization and horizontal gene transfer. A fundamental computational problem in this context is the Tree Containment problem, which asks whether a given phylogenetic network is compatible with a given phylogenetic tree. However, the classical statement of the problem is not robust to poorly supported branches in biological data, possibly leading to false negatives. In an effort to address this, a relaxed version that accounts for uncertainty, called Soft Tree Containment, has been introduced by Bentert, Malík, and Weller [SWAT'18]. We present an algorithm that solves Soft Tree Containment in $2^{O(Δ_T \cdot k \cdot \log(k))} \cdot n^{O(1)}$ time, where $k = \operatorname{sw}(Γ) + Δ_N$, with $Δ_T$ and $Δ_N$ denoting the maximum out-degrees in the tree and the network, respectively, and $\operatorname{sw}(Γ)$ denoting the ``scanwidth'' [Berry, Scornavacca, and Weller, SOFSEM'20] of a given tree extension of the network, while $n$ is the input size. Our approach leverages the fact that phylogenetic networks encountered in practice often exhibit low scanwidth, making the problem more tractable.

</details>


### [15] [Quadratic-Time Algorithm for the Maximum-Weight $(k, \ell)$-Sparse Subgraph Problem](https://arxiv.org/abs/2511.20882)
*Bence Deák,Péter Madarasi*

Main category: cs.DS

TL;DR: 提出了第一个O(n² + m)时间算法来计算最大权重的(k, ℓ)-稀疏子图，解决了之前方法分析错误的问题，并在刚性理论中有重要应用。


<details>
  <summary>Details</summary>
Motivation: 解决计算最大权重(k, ℓ)-稀疏子图的算法效率问题，之前提出的O(n² + m)方法分析有误，需要确认该时间复杂度是否可达。

Method: 结合高效数据结构和精化分析，开发了第一个O(n² + m)时间算法来计算最大权重的(k, ℓ)-稀疏子图。

Result: 成功实现了O(n² + m)时间复杂度的算法，比之前的O(nm)方法更高效，并提供了公开可用的实现。

Conclusion: 该算法不仅解决了理论问题，还为刚性理论中的关键问题提供了更快的解决方案，包括计算最小权重冗余刚性和全局刚性子图等应用。

Abstract: The family of $(k, \ell)$-sparse graphs, introduced by Lorea, plays a central role in combinatorial optimization and has a wide range of applications, particularly in rigidity theory. A key algorithmic challenge is to compute a maximum-weight $(k, \ell)$-sparse subgraph of a given edge-weighted graph. Although prior approaches have long provided an $O(nm)$-time solution, a previously proposed $O(n^2 + m)$ method was based on an incorrect analysis, leaving open whether this bound is achievable.
  We answer this question affirmatively by presenting the first $O(n^2 + m)$-time algorithm for computing a maximum-weight $(k, \ell)$-sparse subgraph, which combines an efficient data structure with a refined analysis. This quadratic-time algorithm enables faster solutions to key problems in rigidity theory, including computing minimum-weight redundantly rigid and globally rigid subgraphs. Further applications include enumerating non-crossing minimally rigid frameworks and recognizing kinematic joints. Our implementation of the proposed algorithm is publicly available online.

</details>


### [16] [Sublinear Time Low-Rank Approximation of Hankel Matrices](https://arxiv.org/abs/2511.21418)
*Michael Kapralov,Cameron Musco,Kshiteej Sheth*

Main category: cs.DS

TL;DR: 提出了第一个在亚线性时间内计算正半定Hankel矩阵低秩逼近的算法，该算法在O(polylog(n,1/ε))时间内找到秩为O(log n log(1/ε))的Hankel矩阵逼近，误差界与Beckermann-Townsend定理匹配。


<details>
  <summary>Details</summary>
Motivation: Hankel矩阵在计算数学、工程和理论计算机科学中广泛出现，已知正半定Hankel矩阵总是近似低秩的。Beckermann和Townsend的结果表明这类矩阵存在良好的低秩逼近，但缺乏高效的亚线性时间算法。

Method: 首先证明存在保持Hankel结构的低秩逼近，然后利用Hankel矩阵的Vandermonde结构，采用基于采样的方法，利用Vandermonde矩阵的通用岭杠杆得分界来实现亚线性时间算法。

Result: 开发了第一个在O(polylog(n,1/ε))时间内计算秩为O(log n log(1/ε))的Hankel矩阵逼近的算法，误差界与理论最优匹配，且对非Hankel噪声具有鲁棒性。

Conclusion: 该工作首次实现了正半定Hankel矩阵的亚线性时间低秩逼近，结合了结构保持的存在性证明和高效的采样算法，为Hankel矩阵的高效处理提供了新工具。

Abstract: Hankel matrices are an important class of highly-structured matrices, arising across computational mathematics, engineering, and theoretical computer science. It is well-known that positive semidefinite (PSD) Hankel matrices are always approximately low-rank. In particular, a celebrated result of Beckermann and Townsend shows that, for any PSD Hankel matrix $H \in \mathbb{R}^{n \times n}$ and any $ε> 0$, letting $H_k$ be the best rank-$k$ approximation of $H$, $\|H-H_k\|_F \leq ε\|H\|_F$ for $k = O(\log n \log(1/ε))$. As such, PSD Hankel matrices are natural targets for low-rank approximation algorithms. We give the first such algorithm that runs in \emph{sublinear time}. In particular, we show how to compute, in $\polylog(n, 1/ε)$ time, a factored representation of a rank-$O(\log n \log(1/ε))$ Hankel matrix $\widehat{H}$ matching the error guarantee of Beckermann and Townsend up to constant factors. We further show that our algorithm is \emph{robust} -- given input $H+E$ where $E \in \mathbb{R}^{n \times n}$ is an arbitrary non-Hankel noise matrix, we obtain error $\|H - \widehat{H}\|_F \leq O(\|E\|_F) + ε\|H\|_F$. Towards this algorithmic result, our first contribution is a \emph{structure-preserving} existence result - we show that there exists a rank-$k$ \emph{Hankel} approximation to $H$ matching the error bound of Beckermann and Townsend. Our result can be interpreted as a finite-dimensional analog of the widely applicable AAK theorem, which shows that the optimal low-rank approximation of an infinite Hankel operator is itself Hankel. Armed with our existence result, and leveraging the well-known Vandermonde structure of Hankel matrices, we achieve our sublinear time algorithm using a sampling-based approach that relies on universal ridge leverage score bounds for Vandermonde matrices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation](https://arxiv.org/abs/2511.20709)
*Abhijeet Pathak,Suvadra Barua,Dinesh Gudimetla,Rupam Patir,Jiawei Guo,Hongxin Hu,Haipeng Cai*

Main category: cs.SE

TL;DR: 提出DUALGAUGE框架，首个自动化评估LLM生成代码安全性和正确性的基准测试系统，包含DUALGAUGE-BENCH数据集和代理程序执行器。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注漏洞减少或功能正确性，缺乏对安全性和正确性的联合评估，无法满足生成安全代码的核心需求。

Method: 开发包含手动验证测试套件的基准数据集，使用沙盒环境运行程序，结合LLM评估器同时评估正确性和漏洞行为。

Result: 对10个主流LLM的评估揭示了在正确和安全代码生成方面的关键差距，系统支持可重复、可扩展的严格评估。

Conclusion: DUALGAUGE为加速安全代码生成研究提供了开源系统和数据集，解决了现有评估方法的局限性。

Abstract: Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.

</details>


### [18] [Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities](https://arxiv.org/abs/2511.20730)
*Nehal Afifi,Christoph Wittig,Lukas Paehler,Andreas Lindenmann,Kai Wolter,Felix Leitenberger,Melih Dogru,Patric Grauberger,Tobias Düser,Albert Albers,Sven Matthiesen*

Main category: cs.SE

TL;DR: 本文通过PRISMA系统文献综述分析了数据驱动方法在产品开发中的应用现状，发现机器学习、统计方法占主导，深度学习呈上升趋势，但在验证阶段应用有限，存在模型可解释性差、跨阶段可追溯性不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在产品开发中的应用日益增多但呈现碎片化，缺乏关于在何时使用何种方法的明确指导，需要调查DDM在工程设计中的实际使用情况。

Method: 采用PRISMA系统文献综述方法，基于V模型将产品开发简化为四个阶段，在Scopus、Web of Science和IEEE Xplore数据库中检索2014-2024年的1689条记录，最终分析114篇文献。

Result: 发现机器学习和统计方法占主导地位，深度学习呈上升趋势；监督学习、聚类、回归分析和代理建模在设计、实施和集成阶段普遍，但在验证阶段贡献有限；存在模型可解释性差、跨阶段可追溯性不足等挑战。

Conclusion: 这是制定设计阶段指南的第一步，后续需要将计算机科学算法映射到工程设计问题和活动中，并开发可解释的混合模型。

Abstract: The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.

</details>


### [19] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: 本文探讨了支持"边战边训"(TWYF)模式的先进分布式学习平台所需的技术要求，以及现有软件工程模式如何满足这些要求。


<details>
  <summary>Details</summary>
Motivation: 传统训练模式在作战前或作战后进行，无法满足现代作战环境中持续学习的需求。TWYF模式要求在作战行动中实现持续学习，这对学习平台提出了新的技术要求。

Method: 采用设计科学研究方法：(1)从PfPC/北约文档和近期实践中推导挑战；(2)定义解决方案目标；(3)系统性地将挑战映射到已验证的模式。

Result: 识别出七大技术挑战：互操作性、弹性、多语言支持、数据安全与隐私、可扩展性、平台独立性、模块化。通过德国武装部队的国家用例说明了这些模式的应用。

Conclusion: 现有软件工程模式能够有效解决TWYF模式下的技术挑战，为先进分布式学习平台的设计提供了可行方案。

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [20] [Application of machine learning for infrastructure reconstruction programs management](https://arxiv.org/abs/2511.20916)
*Illia Khudiakov,Vladyslav Pliuhin,Sergiy Plankovskyy,Yevgen Tsegelnyk*

Main category: cs.SE

TL;DR: 开发了一种自适应决策支持模型，用于提高工程基础设施重建项目的管理效率，通过系统建模和机器学习预测目标函数值来支持决策。


<details>
  <summary>Details</summary>
Motivation: 现有工程基础设施重建项目管理工具效率不足，需要开发自适应模型来优化项目架构和工作分解结构的创建过程。

Method: 结合系统建模工具、机器学习和人工神经网络，构建包含决策者偏好、决策任务、输入数据和软件组件的自适应模型，使用历史数据进行预测。

Result: 在Microsoft Azure Machine Learning Studio中实现了功能组合，给出了神经网络参数和评估结果，模型适用于热力、燃气、电力、供排水等工程系统的重建项目管理。

Conclusion: 该自适应模型能够有效支持工程基础设施重建项目的决策过程，通过机器学习预测和系统建模提高了管理效率。

Abstract: The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.

</details>


### [21] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 评估DeepSeek-R1模型家族对软件设计概念（内聚性和耦合性）的理解，发现模型在理想条件下有基础理解，但在实际应用中表现脆弱且不对称。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程领域的应用增加，需要系统评估其对核心软件设计概念的掌握程度，特别是内聚性和耦合性的理解。

Method: 通过程序化生成设计不良的代码片段，测试DeepSeek-R1模型家族（14B、32B、70B）在不同指导级别（验证、引导、开放式生成）和不同上下文噪声下的表现。

Result: 模型对耦合性的推理很脆弱，在嘈杂的开放式场景中性能崩溃（F1分数下降超过50%）；而对内聚性的分析在引导任务中表现出显著鲁棒性，但移除所有指导后也会失效。

Conclusion: LLMs在识别设计缺陷方面能提供可靠帮助，但在嘈杂现实环境中自主推理能力有限，需要更可扩展和鲁棒的程序理解能力。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [22] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: 本研究通过SPACE框架和统计方法开发了综合生产力评分(CPS)，发现负面情绪与提交频率正相关，且贡献者互动拓扑比传统指标更能反映协作动态。


<details>
  <summary>Details</summary>
Motivation: 解决传统确定性、单维度生产力启发式方法的局限性，应对开发者效能异质性问题。

Method: 通过开源仓库数据挖掘，使用广义线性混合模型(GLM)和RoBERTa情感分类，构建多维生产力指标。

Result: 发现负面情感状态与提交频率存在显著正相关，表明存在由挫败感驱动的迭代修复循环；贡献者互动拓扑分析比传统基于数量的指标更能准确映射协作动态。

Conclusion: 提出综合生产力评分(CPS)来更全面地评估开发者生产力，强调需要多维度方法来理解软件开发中的生产力表现。

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [23] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 该研究首次系统性地应用10种最先进的模型编辑技术来更新LLMs中过时的API知识，并提出AdaLoRA-L方法在保持性能的同时显著提高编辑特异性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码补全任务中表现出色，但其训练数据的时效性限制导致经常生成已弃用的API。重新训练模型成本高昂，而轻量级模型编辑方法是否能有效更新API知识尚不明确。

Method: 构建EDAPIBench基准测试，包含70多个过时API和3000多个编辑实例。应用10种模型编辑技术，并提出AdaLoRA-L方法，通过定义'通用API层'和'特定API层'来限制编辑范围。

Result: AdaLoRA在生成正确的最新API方面表现最佳，但在特异性（不影响非目标知识）方面不足。AdaLoRA-L显著提高了特异性，同时在其他评估指标上保持可比性能。

Conclusion: AdaLoRA-L方法能有效更新LLMs中的过时API知识，在保持性能的同时提高编辑特异性，为模型知识更新提供了实用解决方案。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [24] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 该论文研究了Android应用在不同地理位置的行为差异，发现了两种重要现象：GeoTwins（功能相似但在不同国家发布的应用）和base.apk文件的区域差异，这些差异对安全性和公平性有重要影响。


<details>
  <summary>Details</summary>
Motivation: 移动应用演化已被广泛研究，但应用行为的地理差异仍未被充分探索。本研究旨在揭示地理位置对Android应用行为的影响，特别是安全性和公平性方面的问题。

Method: 构建了跨多个区域的分布式应用收集管道，分析了数千个应用，并发布了包含81,963个GeoTwins的数据集。

Result: 发现GeoTwins在请求权限、第三方库和隐私披露方面存在差异；即使base.apk文件也存在区域差异，暴露了隐藏的自定义设置。这些地理差异可能导致同一应用在不同地区的安全评估结果不一致。

Conclusion: 移动软件存在系统性区域差异，这对研究人员、开发者、平台架构师和政策制定者都有重要影响，需要关注透明度、同意和地理偏见问题。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [25] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 本文通过6次共同设计工作坊与58名开发者合作，探讨了他们对AI辅助bug检测和代码可读性评估工具的心理模型，提出了"bug侦探"和"质量教练"两种设计范式，并提炼出以人为本的IDE AI设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助开发工具在技术特性上不断进步，但开发者如何心理建模这些工具以及不匹配如何影响信任、控制和采用尚不清楚。

Method: 通过6次共同设计工作坊，与58名开发者合作，引出他们对AI辅助bug检测和可读性功能的心智模型。

Result: 开发者将bug检测工具视为"bug侦探"，仅在关键问题时警告用户，保证透明度、可操作反馈和信心提示；可读性评估工具则被视为"质量教练"，提供情境化、个性化和渐进式指导。信任取决于解释清晰度、时机和用户控制。

Conclusion: 提炼了一套以人为本的IDE AI设计原则，旨在平衡干扰与支持、简洁与深度、自动化与人类能动性。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [26] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 首次实证研究多智能体系统在数据集适应任务中的表现，评估了基于GPT-4.1和Claude Sonnet 4的Copilot系统，发现当前系统能识别关键文件并生成部分适应，但很少产生功能正确的实现。


<details>
  <summary>Details</summary>
Motivation: 自动化软件工程研究工件的跨数据集适应对于可扩展性和可复现性至关重要，但目前研究较少。多智能体系统有望通过协调推理、代码生成和工具交互来自动化复杂开发工作流。

Method: 使用五阶段评估管道（文件理解、代码编辑、命令生成、验证和最终执行），评估Copilot在ROCODE和LogHub2.0等基准仓库上的表现，分析成功率、失败模式，并评估提示干预策略。

Result: 当前系统能识别关键文件并生成部分适应，但很少产生功能正确的实现。提示干预（特别是提供执行错误消息和参考代码）显著提高了与真实情况的结构相似性（从7.25%提高到67.14%）。

Conclusion: 研究揭示了当今多智能体LLM系统在数据集适应方面的潜力和局限性，为构建更可靠、自校正的智能体提供了具体方向。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [27] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 本文对2021-2025年间115篇关于使用LLM进行单元测试生成的文献进行了系统综述，提出了基于测试生成生命周期的统一分类法，分析了核心生成策略和增强技术，发现提示工程是主导方法，迭代验证修复成为标准机制，但故障检测能力和标准化评估仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试方法缺乏生成真实输入和断言的语义信息，而LLM能够利用其代码语义和编程模式的数据驱动知识来弥补这一局限，因此需要系统分析LLM在单元测试生成领域的最新进展。

Method: 对115篇文献进行系统综述，提出基于单元测试生成生命周期的统一分类法，将LLM视为需要系统工程约束的随机生成器，分析核心生成策略和从预生成上下文丰富到后生成质量保证的增强技术。

Result: 提示工程因其灵活性成为主导策略（占研究的89%），迭代验证和修复循环成为确保鲁棒可用性的标准机制，显著提高了编译和执行通过率，但生成的测试在故障检测能力方面仍然较弱，且缺乏标准化评估基准。

Conclusion: 未来研究应朝着自主测试代理和结合LLM与传统软件工程工具的混合系统方向发展，为研究者和从业者提供将LLM潜力转化为工业级测试解决方案的全面视角。

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [28] [Real-World Performance Evaluations of Low-Band 5G NR/4G LTE 4x4 MIMO on Commercial Smartphones](https://arxiv.org/abs/2511.20959)
*Pasapong Wongprasert,Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.NI

TL;DR: 该研究评估了索尼Xperia 1 III和IV在泰国b28/n28频段的4x4 MIMO实际性能，包括不同信号条件下的可靠性测试和最优条件下的吞吐量测试，并与仅支持2Rx天线的设备进行对比。


<details>
  <summary>Details</summary>
Motivation: 尽管大多数商用5G设备在低频段(<1GHz)不支持4x4 MIMO，但泰国运营商在b28/n28频段部署了4T4R网络，需要研究支持设备的实际性能表现。

Method: 使用索尼Xperia 1 III和IV智能手机，通过固件修改可配置为仅使用2个Rx端口，在不同信号条件下进行可靠性测试和最优条件下的吞吐量测试，并与仅支持2Rx天线的其他设备进行对比。

Result: 研究结果显示在泰国b28/n28频段，支持4x4 MIMO的设备在吞吐量和接收性能方面相比仅支持2Rx的设备有显著提升。

Conclusion: 在部署了4T4R网络的低频段，支持4x4 MIMO的设备能够实现理论上的性能优势，为未来更多设备支持低频段4x4 MIMO提供了实证依据。

Abstract: All 3GPP-compliant commercial 5G New Radio (NR)-capable UEs on the market are equipped with 4x4 MIMO support for Mid-Band frequencies (>1.7 GHz) and above, enabling up to rank 4 MIMO transmission. This doubles the theoretical throughput compared to rank 2 MIMO and also improves reception performance. However, 4x4 MIMO support on low-band frequencies (<1 GHz) is absent in every commercial UEs, with the exception of the Xperia 1 flagship smartphones manufactured by Sony Mobile and the Xiaomi 14 Pro as of January 2024. The reason most manufacturers omit 4x4 MIMO support for low-band frequencies is likely due to design challenges or relatively small performance gains in real-world usage due to the lack of 4T4R deployment on low-band by mobile network operators around the world.
  In Thailand, 4T4R deployment on the b28/n28 (APT) band is common on True-H and dtac networks, enabling 4x4 MIMO transmission on supported UEs. In this paper, the real-world 4x4 MIMO performance on the b28/n28 (APT) band will be investigated by evaluating the reliability test under different signal conditions and the maximum throughput test by evaluating the performance under optimal conditions, using the Sony Xperia 1 III and the Sony Xperia 1 IV smartphone. Devices from other manufacturers are also used in the experiment to investigate the performance with 2Rx antennas for comparison. Through firmware modifications, the Sony Xperia 1 III and IV can be configured to use only 2 Rx ports on low-band, enabling the collection of comparative 2 Rx performance data as a reference.

</details>


### [29] [Performance Evaluation of Low-Latency Live Streaming of MPEG-DASH UHD video over Commercial 5G NSA/SA Network](https://arxiv.org/abs/2511.20961)
*Kasidis Arunruangsirilert,Bo Wei,Hang Song,Jiro Katto*

Main category: cs.NI

TL;DR: 5G SA在泰国已覆盖76%人口，相比5G NSA和LTE网络，在UHD视频实时直播中能提供更低的延迟和更高的可靠性，95%以上视频段能在要求时间窗口内成功传输。


<details>
  <summary>Details</summary>
Motivation: 评估5G SA、5G NSA和LTE网络在UHD视频实时直播中的性能差异，验证5G SA对低延迟UHD视频传输的必要性。

Method: 通过MPEG-DASH在不同移动网络技术下进行UHD视频实时直播，使用最小缓冲区以提供最低延迟，评估丢段数量、MAC吞吐量和延迟等性能指标，涵盖静止、城市移动、高速移动和理想SINR条件等多种场景。

Result: 5G SA在所有场景下都能在要求时间窗口内成功传输95%以上的UHD视频段，而5G NSA性能因LTE网络状况而异，LTE网络则有超过20%的视频段无法按时传输。

Conclusion: 5G SA对于低延迟UHD视频流传输是绝对必要的，5G NSA由于依赖传统控制信号可能无法满足此类任务需求。

Abstract: 5G Standalone (SA) is the goal of the 5G evolution, which aims to provide higher throughput and lower latency than the existing LTE network. One of the main applications of 5G is the real-time distribution of Ultra High-Definition (UHD) content with a resolution of 4K or 8K. In Q2/2021, Advanced Info Service (AIS), the biggest operator in Thailand, launched 5G SA, providing both 5G SA/NSA service nationwide in addition to the existing LTE network. While many parts of the world are still in process of rolling out the first phase of 5G in Non-Standalone (NSA) mode, 5G SA in Thailand already covers more than 76% of the population.
  In this paper, UHD video will be a real-time live streaming via MPEG-DASH over different mobile network technologies with minimal buffer size to provide the lowest latency. Then, performance such as the number of dropped segments, MAC throughput, and latency are evaluated in various situations such as stationary, moving in the urban area, moving at high speed, and also an ideal condition with maximum SINR. It has been found that 5G SA can deliver more than 95% of the UHD video segment successfully within the required time window in all situations, while 5G NSA produced mixed results depending on the condition of the LTE network. The result also reveals that the LTE network failed to deliver more than 20% of the video segment within the deadline, which shows that 5G SA is absolutely necessary for low-latency UHD video streaming and 5G NSA may not be good enough for such task as it relies on the legacy control signal.

</details>


### [30] [5G Network Automation Using Local Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2511.21084)
*Ahmadreza Majlesara,Ali Majlesi,Ali Mamaghani,Alireza Shokrani,Babak Hossein Khalaj*

Main category: cs.NI

TL;DR: 展示了本地部署的轻量级LLaMA-3模型结合RAG技术来自动化5G网络管理，强调隐私保护和用户友好性。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络管理中隐私安全问题，避免敏感数据通过外部API传输，同时让非专业用户也能轻松配置网络。

Method: 使用本地部署的LLaMA-3 8b Q-4b轻量级模型，结合检索增强生成(RAG)技术从数据库中检索相关信息来提升模型性能。

Result: 通过RAG增强了轻量级模型的效率和准确性，能够基于自然语言输入生成精确的网络配置，简化了私有网络的创建过程。

Conclusion: 本地LLM与RAG的结合为5G网络提供了安全、高效且适应性强的解决方案，为实现隐私保护和多样用户需求的5G网络铺平了道路。

Abstract: This demonstration showcases the integration of a lightweight, locally deployed Large Language Model (LLaMA-3 8b Q-4b) empowered by retrieval augmented generation (RAG) to automate 5G network management, with a strong emphasis on privacy. By running the LLM on local or edge devices ,we eliminate the need for external APIs, ensuring that sensitive data remains secure and is not transmitted over the internet. Although lightweight models may not match the performance of more complex models like GPT-4, we enhance their efficiency and accuracy through RAG. RAG retrieves relevant information from a comprehensive database, enabling the LLM to generate more precise and effective network configurations based on natural language user input. This approach not only improves the accuracy of the generated configurations but also simplifies the process of creating and configuring private networks, making it accessible to users without extensive networking or programming experience. The objective of this demonstration is to highlight the potential of combining local LLMs and RAG to deliver secure, efficient, and adaptable 5G network solutions, paving the way for a future where 5G networks are both privacy-conscious and versatile across diverse user profiles.

</details>


### [31] [Digital Twin-Driven Secure Access Strategy for SAGIN-Enabled IoT Networks](https://arxiv.org/abs/2511.21156)
*Hui Liang,Zhihui Wu,Runqi Yuan,Guobin Zhang,Yanfeng Zhang,Jinkai Zheng,Tom H. Luan*

Main category: cs.NI

TL;DR: 提出了一种基于数字孪生的安全接入策略，用于空间-空中-地面一体化网络中的物联网设备安全接入，通过量化保密容量和演化博弈来平衡安全性和排队延迟。


<details>
  <summary>Details</summary>
Motivation: 空间-空中-地面一体化网络中的物联网网络面临日益严重的窃听攻击威胁，数据机密性受到挑战，需要有效的安全接入解决方案。

Method: 利用数字孪生框架创建物理环境的虚拟副本，持续评估动态窃听风险；采用演化博弈模型平衡保密容量和排队延迟；开发分布式算法获取均衡接入策略。

Result: 仿真结果表明，该方法显著提高了网络安全性，有效平衡系统负载，防止过载发生，并降低排队延迟，优于基准方案。

Conclusion: 基于数字孪生的安全接入策略能够全面改善空间-空中-地面一体化物联网网络的整体性能，在安全性和效率之间实现良好平衡。

Abstract: In space-air-ground integrated networks (SAGIN)-enabled IoT networks, secure access has become a significant challenge due to the increasing risks of eavesdropping attacks. To address these threats to data confidentiality, this paper proposes a Digital Twin (DT)-driven secure access strategy. The strategy leverages a virtual replica of the physical SAGIN environment within the DT framework to continuously assess dynamic eavesdropping risks by quantifying secrecy capacity. Operating within this DT framework, an evolutionary game model dynamically balances the DT-updated secrecy capacity against queuing delay, steering IoT devices toward more secure and efficient access decisions. Furthermore, a novel distributed algorithm, integral to the DT operation, is developed to obtain the equilibrium access strategy for each device in a scalable manner. Simulation results demonstrate that the proposed DT-based approach substantially improves the security of SAGIN-enabled IoT networks. Additionally, it effectively balances system load, prevents overload occurrences, and decreases queuing delay compared to benchmark schemes, thereby comprehensively improving overall network performance.

</details>


### [32] [LatencyScope: A System-Level Mathematical Framework for 5G RAN Latency](https://arxiv.org/abs/2511.21277)
*Arman Maghsoudnia,Aoyu Gong,Raphael Cannatà,Dan Mihai Dumitriu,Haitham Hassanieh*

Main category: cs.NI

TL;DR: LatencyScope是一个用于准确计算5G RAN中单向延迟的数学框架，能够识别系统级瓶颈并提供配置优化方案。


<details>
  <summary>Details</summary>
Motivation: 5G网络中需要满足超可靠低延迟通信(URLLC)要求，但现有模型无法准确捕捉RAN各层延迟源的复杂依赖关系和随机特性。

Method: 建立RAN各层延迟源的数学模型，考虑无线接口、调度策略和软硬件约束等因素的复杂依赖关系，并开发配置优化器搜索最优配置。

Result: 在两个开源5G RAN测试平台上验证，LatencyScope能够紧密匹配经验延迟分布，显著优于现有分析模型和仿真器，并能找到满足URLLC目标的系统配置。

Conclusion: LatencyScope为网络运营商提供了一种高效识别最佳系统配置的方法，能够满足5G网络的低延迟可靠性要求。

Abstract: This paper presents LatencyScope, a mathematical framework for accurately computing one-way latency (for uplink and downlink) in the 5G RAN across diverse system configurations. LatencyScope models latency sources at every layer of the Radio Access Network (RAN), pinpointing system-level bottlenecks--such as radio interfaces, scheduling policies, and hardware/software constraints--while capturing their intricate dependencies and their stochastic nature. LatencyScope also includes a configuration optimizer that uses its mathematical models to search through hundreds of billions of configurations and find settings that meet latency-reliability targets under user constraints. We validate LatencyScope on two open-sourced 5G RAN testbeds (srsRAN and OAI), demonstrating that it can closely match empirical latency distributions and significantly outperform prior analytical models and widely used simulators (MATLAB 5G Toolbox, 5G-LENA). It can also find system configurations that meet Ultra-Reliable Low-Latency Communications (URLLC) targets and enable network operators to efficiently identify the best setup for their systems.

</details>


### [33] [Toward Secure Content-Centric Approaches for 5G-Based IoT: Advances and Emerging Trends](https://arxiv.org/abs/2511.21336)
*Ghada Jaber,Mohamed Ali Zormati,Walid Cavelius,Louka Chapiro,Mohamed El Ahmadi*

Main category: cs.NI

TL;DR: 本文调查了内容中心网络在5G物联网环境中的安全挑战和解决方案，重点分析了内容认证、数据完整性、隐私保护等关键问题，并提出了轻量级自适应安全机制的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和5G技术的融合，内容中心网络因其在网缓存、可扩展性和高效内容分发等优势成为传统IP架构的有前景替代方案，但在5G物联网环境中部署面临严重的安全挑战。

Method: 采用文献综述方法，对现有内容中心架构在物联网-5G场景中的安全解决方案进行分类和评估。

Result: 识别了当前安全解决方案的趋势和局限性，发现分布式、移动和异构的物联网5G系统加剧了内容认证、数据完整性、隐私保护和抗攻击等安全挑战。

Conclusion: 需要开发轻量级和自适应的安全机制来应对内容中心网络在5G物联网环境中的安全挑战，这是未来重要的研究方向。

Abstract: The convergence of the Internet of Things (IoT) and 5G technologies is transforming modern communication systems by enabling massive connectivity, low latency, and high-speed data transmission. In this evolving landscape, Content-Centric Networking (CCN) is emerging as a promising alternative to traditional Internet Protocol (IP)-based architectures. CCN offers advantages such as in-network caching, scalability, and efficient content dissemination, all of which are particularly well-suited to the constraints of the IoT. However, deploying content-centric approaches in 5G-based IoT environments introduces significant security challenges. Key concerns include content authentication, data integrity, privacy protection, and resilience against attacks such as spoofing and cache poisoning. Such issues are exacerbated by the distributed, mobile, and heterogeneous nature of IoT and 5G systems. In this survey, we review and classify existing security solutions for content-centric architectures in IoT-5G scenarios. We highlight current trends, identify limitations in existing approaches, and outline future research directions with a focus on lightweight and adaptive security mechanisms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [On the Role of Hidden States of Modern Hopfield Network in Transformer](https://arxiv.org/abs/2511.20698)
*Tsubasa Masumura,Masato Taki*

Main category: cs.LG

TL;DR: 本文提出了现代Hopfield注意力(MHA)机制，通过将Hopfield网络的隐藏状态引入自注意力，建立了Hopfield网络与Transformer的更广义对应关系，有效解决了深度Transformer的秩塌陷和token均匀性问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明现代Hopfield网络(MHN)在绝热近似下的状态更新规则与Transformer的自注意力层一致，但作者希望超越这一近似，更深入地探索MHN与自注意力之间的关系。

Method: 通过在现代Hopfield网络中引入隐藏状态变量，并将其整合到自注意力机制中，提出了现代Hopfield注意力(MHA)机制，使注意力分数可以从Transformer的输入层继承到输出层。

Result: 理论和实验证明MHA隐藏状态显著改善了深度Transformer的秩塌陷和token均匀性问题，在Vision Transformer和GPT上无需增加训练参数就能系统性地提升准确率。

Conclusion: 研究结果表明Hopfield网络可以为改进Transformer架构提供有用的视角，MHA机制为两者建立了更广义的对应关系。

Abstract: Associative memory models based on Hopfield networks and self-attention based on key-value mechanisms have been popular approaches in the study of memory mechanisms in deep learning. It has been pointed out that the state update rule of the modern Hopfield network (MHN) in the adiabatic approximation is in agreement with the self-attention layer of Transformer. In this paper, we go beyond this approximation and investigate the relationship between MHN and self-attention. Our results show that the correspondence between Hopfield networks and Transformers can be established in a more generalized form by adding a new variable, the hidden state derived from the MHN, to self-attention. This new attention mechanism, modern Hopfield attention (MHA), allows the inheritance of attention scores from the input layer of the Transformer to the output layer, which greatly improves the nature of attention weights. In particular, we show both theoretically and empirically that MHA hidden states significantly improve serious problem of deep Transformers known as rank collapse and token uniformity. We also confirm that MHA can systematically improve accuracy without adding training parameters to the Vision Transformer or GPT. Our results provide a new case in which Hopfield networks can be a useful perspective for improving the Transformer architecture.

</details>


### [35] [Post-Pruning Accuracy Recovery via Data-Free Knowledge Distillation](https://arxiv.org/abs/2511.20702)
*Chinmay Tripurwar,Utkarsh Maurya,Dishant*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model pruning is a widely adopted technique to reduce the computational complexity and memory footprint of Deep Neural Networks (DNNs). However, global unstructured pruning often leads to significant degradation in accuracy, typically necessitating fine-tuning on the original training dataset to recover performance. In privacy-sensitive domains such as healthcare or finance, access to the original training data is often restricted post-deployment due to regulations (e.g., GDPR, HIPAA). This paper proposes a Data-Free Knowledge Distillation framework to bridge the gap between model compression and data privacy. We utilize DeepInversion to synthesize privacy-preserving ``dream'' images from the pre-trained teacher model by inverting Batch Normalization (BN) statistics. These synthetic images serve as a transfer set to distill knowledge from the original teacher to the pruned student network. Experimental results on CIFAR-10 across various architectures (ResNet, MobileNet, VGG) demonstrate that our method significantly recovers accuracy lost during pruning without accessing a single real data point.

</details>


### [36] [Pretraining Transformer-Based Models on Diffusion-Generated Synthetic Graphs for Alzheimer's Disease Prediction](https://arxiv.org/abs/2511.20704)
*Abolfazl Moslemi,Hossein Peyvandi*

Main category: cs.LG

TL;DR: 提出基于Transformer的阿尔茨海默病诊断框架，结合扩散模型生成合成数据、图表示学习和迁移学习，在数据有限、类别不平衡的临床预测中提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期准确检测至关重要，但机器学习模型开发面临标记数据有限、多站点异质性和类别不平衡等挑战

Method: 使用类别条件DDPM在真实NACC数据集上生成平衡诊断类别的合成队列，通过模态特定图Transformer编码器在合成数据上预学习鲁棒表示，然后在原始数据上训练分类器

Result: 在NACC数据集上的主题交叉验证中，该框架在AUC、准确率、敏感性和特异性方面优于标准基线方法

Conclusion: 基于扩散的合成预训练与图Transformer结合能够改善低样本、不平衡临床预测环境中的泛化性能

Abstract: Early and accurate detection of Alzheimer's disease (AD) is crucial for enabling timely intervention and improving outcomes. However, developing reliable machine learning (ML) models for AD diagnosis is challenging due to limited labeled data, multi-site heterogeneity, and class imbalance. We propose a Transformer-based diagnostic framework that combines diffusion-based synthetic data generation with graph representation learning and transfer learning. A class-conditional denoising diffusion probabilistic model (DDPM) is trained on the real-world NACC dataset to generate a large synthetic cohort that mirrors multimodal clinical and neuroimaging feature distributions while balancing diagnostic classes. Modality-specific Graph Transformer encoders are first pretrained on this synthetic data to learn robust, class-discriminative representations and are then frozen while a neural classifier is trained on embeddings from the original NACC data. We quantify distributional alignment between real and synthetic cohorts using metrics such as Maximum Mean Discrepancy (MMD), Frechet distance, and energy distance, and complement discrimination metrics with calibration and fixed-specificity sensitivity analyses. Empirically, our framework outperforms standard baselines, including early and late fusion deep neural networks and the multimodal graph-based model MaGNet, yielding higher AUC, accuracy, sensitivity, and specificity under subject-wise cross-validation on NACC. These results show that diffusion-based synthetic pretraining with Graph Transformers can improve generalization in low-sample, imbalanced clinical prediction settings.

</details>


### [37] [Solving Diffusion Inverse Problems with Restart Posterior Sampling](https://arxiv.org/abs/2511.20705)
*Bilal Ahmed,Joseph G. Makin*

Main category: cs.LG

TL;DR: RePS是一种基于重启采样的高效框架，使用预训练扩散模型解决线性和非线性逆问题，避免通过分数网络的反向传播，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的逆问题方法依赖后验分布的强近似、需要昂贵的梯度反向传播，或仅限于线性测量模型。

Method: 基于重启采样的思想，扩展至后验推断，使用适用于任何可微分测量模型的条件ODE，并引入简化的重启策略来收缩采样过程中的累积近似误差。

Result: RePS在多种逆问题（包括线性和非线性设置）中比现有基于扩散的基线方法收敛更快，重建质量更优。

Conclusion: RePS提供了一个通用且高效的框架，用于使用预训练扩散模型解决逆问题，在计算效率和重建质量方面均优于现有方法。

Abstract: Inverse problems are fundamental to science and engineering, where the goal is to infer an underlying signal or state from incomplete or noisy measurements. Recent approaches employ diffusion models as powerful implicit priors for such problems, owing to their ability to capture complex data distributions. However, existing diffusion-based methods for inverse problems often rely on strong approximations of the posterior distribution, require computationally expensive gradient backpropagation through the score network, or are restricted to linear measurement models.
  In this work, we propose Restart for Posterior Sampling (RePS), a general and efficient framework for solving both linear and non-linear inverse problems using pre-trained diffusion models. RePS builds on the idea of restart-based sampling, previously shown to improve sample quality in unconditional diffusion, and extends it to posterior inference. Our method employs a conditioned ODE applicable to any differentiable measurement model and introduces a simplified restart strategy that contracts accumulated approximation errors during sampling. Unlike some of the prior approaches, RePS avoids backpropagation through the score network, substantially reducing computational cost.
  We demonstrate that RePS achieves faster convergence and superior reconstruction quality compared to existing diffusion-based baselines across a range of inverse problems, including both linear and non-linear settings.

</details>


### [38] [Active Slice Discovery in Large Language Models](https://arxiv.org/abs/2511.20713)
*Minhui Zhang,Prahar Ijner,Yoav Wald,Elliot Creager*

Main category: cs.LG

TL;DR: 本文提出主动切片发现方法，通过主动学习算法识别LLMs中的系统性错误切片，显著减少人工标注需求


<details>
  <summary>Details</summary>
Motivation: LLMs在特定数据子集上存在系统性错误，手动识别这些错误切片成本高昂，需要开发更高效的发现方法

Method: 使用主动学习算法结合不同特征表示，在毒性分类任务中发现人工定义的错误切片

Result: 不确定性主动学习算法效果最佳，仅需2-10%的切片成员信息即可达到竞争性准确率，显著优于基线方法

Conclusion: 主动切片发现是识别LLMs错误模式的有效方法，能够大幅降低人工标注成本

Abstract: Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.

</details>


### [39] [ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training](https://arxiv.org/abs/2511.20718)
*Chenliang Li,Adel Elmahdy,Alex Boyd,Zhongruo Wang,Alfredo Garcia,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了两种稳定多轮对话中PPO训练的技术：轮级重要性采样和裁剪偏差校正，解决了PPO在LLM训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: PPO在多轮对话和推理任务中广泛用于训练大型语言模型，但性能不稳定且容易崩溃。研究发现不稳定性主要来自两个来源：标记级重要性采样与多轮环境的自然粒度不匹配，以及离策略样本的优势估计不准确。

Method: 引入两种互补的稳定技术：1) 轮级重要性采样，使优化与多轮推理的自然结构对齐；2) 裁剪偏差校正，通过对不可靠的离策略样本降权来归一化梯度。组合这些组件得到三个变体：Turn-PPO、S-PPO和ST-PPO。

Result: 在多轮搜索任务上的实验表明，ST-PPO和S-PPO能防止大型模型训练中的性能崩溃，在整个优化过程中保持较低的裁剪比率，并获得比标准标记级PPO更高的任务性能。

Conclusion: 轮级重要性采样与裁剪偏差校正的结合为稳定多轮LLM智能体训练提供了实用且可扩展的解决方案。

Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

</details>


### [40] [Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge](https://arxiv.org/abs/2511.20726)
*Yuhang Wang,Heye Huang,Zhenhua Xu,Kailai Sun,Baoshen Guo,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出了一种结合条件变分自编码器(CVAE)和大语言模型(LLM)的高保真场景生成框架，用于生成罕见长尾事件和复杂多智能体交互场景，以增强自动驾驶系统的安全验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶面临罕见长尾事件和复杂多智能体交互的挑战，这些场景在真实世界数据中稀缺但对安全验证至关重要。

Method: 使用CVAE编码历史轨迹和地图信息学习潜在交通结构，生成物理一致的基础场景；利用LLM作为对抗推理引擎，将非结构化场景描述解析为领域特定损失函数，动态指导不同风险级别的场景生成。

Result: 在CARLA和SMARTS中的实验表明，该框架显著增加了高风险和长尾事件的覆盖率，改善了模拟与真实交通分布的一致性，生成的交互场景比现有规则或数据驱动方法更具挑战性。

Conclusion: 为自动驾驶安全验证开辟了新途径，能够在罕见但重要的事件下对自主系统进行原则性的压力测试。

Abstract: Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.

</details>


### [41] [CHiQPM: Calibrated Hierarchical Interpretable Image Classification](https://arxiv.org/abs/2511.20779)
*Thomas Norrenbrock,Timo Kaiser,Sovan Biswas,Neslihan Kose,Ramesh Manuvinakurike,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: CHiQPM是一个全局可解释模型，提供全面的全局和局部可解释性，在保持99%非可解释模型准确率的同时，提供分层解释和可解释的Conformal预测方法。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域需要可信AI，全局可解释模型结合详细的局部解释对于支持人类专家在推理过程中至关重要。

Method: 提出校准分层QPM（CHiQPM），通过对比解释多数类别实现全局可解释性，提供分层解释（类似人类推理方式），并包含可解释的Conformal预测方法。

Result: CHiQPM作为点预测器达到最先进准确率，保持非可解释模型99%的准确率，其校准集预测在效率上与其他CP方法竞争，同时提供分层解释的连贯集预测。

Conclusion: CHiQPM在可解释性方面取得实质性改进，在不牺牲整体准确性的前提下实现了可解释性，为人类-AI互补性铺平了道路。

Abstract: Globally interpretable models are a promising approach for trustworthy AI in safety-critical domains. Alongside global explanations, detailed local explanations are a crucial complement to effectively support human experts during inference. This work proposes the Calibrated Hierarchical QPM (CHiQPM) which offers uniquely comprehensive global and local interpretability, paving the way for human-AI complementarity. CHiQPM achieves superior global interpretability by contrastively explaining the majority of classes and offers novel hierarchical explanations that are more similar to how humans reason and can be traversed to offer a built-in interpretable Conformal prediction (CP) method. Our comprehensive evaluation shows that CHiQPM achieves state-of-the-art accuracy as a point predictor, maintaining 99% accuracy of non-interpretable models. This demonstrates a substantial improvement, where interpretability is incorporated without sacrificing overall accuracy. Furthermore, its calibrated set prediction is competitively efficient to other CP methods, while providing interpretable predictions of coherent sets along its hierarchical explanation.

</details>


### [42] [Privacy in Federated Learning with Spiking Neural Networks](https://arxiv.org/abs/2511.21181)
*Dogukan Aksu,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.LG

TL;DR: 首次系统研究脉冲神经网络(SNNs)中的梯度泄漏问题，发现与传统人工神经网络(ANNs)相比，SNNs由于事件驱动动态和替代梯度训练，显著降低了梯度信息量，具有固有的隐私保护潜力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)在边缘AI场景中广泛应用，但梯度反转攻击会从共享梯度中重建敏感训练数据。虽然传统ANNs中的这一漏洞已被广泛研究，但SNNs中的影响尚未探索。SNNs的非可微性和替代梯度训练可能使其梯度与原始输入相关性更低，从而提供更好的隐私保护。

Method: 将不同的梯度泄漏攻击方法适配到脉冲域，在多种数据领域上对SNNs进行梯度泄漏的实证研究，比较SNNs与传统ANNs在梯度信息量方面的差异。

Result: 实验结果显示与传统ANNs形成鲜明对比：ANN梯度可靠地暴露显著输入内容，而SNN梯度产生噪声大、时间不一致的重建结果，无法恢复有意义的空间或时间结构。

Conclusion: SNNs的事件驱动动态和替代梯度训练显著降低了梯度的信息量，突显了神经形态计算固有的隐私保护潜力，为脉冲架构提供了首个梯度反转攻击的系统基准。

Abstract: Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.

</details>


### [43] [Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model](https://arxiv.org/abs/2511.20798)
*Rio Alexa Fear,Payel Mukhopadhyay,Michael McCabe,Alberto Bietti,Miles Cranmer*

Main category: cs.LG

TL;DR: 该研究发现科学基础模型学习物理原理的通用表示，通过提取激活向量作为概念方向，能够因果控制模型对物理行为的预测。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型内部表示是否普遍存在可解释的抽象概念，以及这种现象是否仅限于语言和图像等结构化数据训练模型。

Method: 从物理基础模型中提取不同物理机制下的激活向量，计算delta表示作为概念方向，并在推理时注入这些方向来操控模型预测。

Result: 通过注入概念方向能够因果控制物理行为，如诱导或移除特定物理特征，表明模型学习的是物理原理而非表面相关性。

Conclusion: 科学基础模型学习物理原理的通用表示，为理解和控制科学基础模型开辟了新途径，对AI驱动的科学发现有重要意义。

Abstract: Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute "delta" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.

</details>


### [44] [Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning](https://arxiv.org/abs/2511.20811)
*Aaron O. Feldman,D. Isaiah Harp,Joseph Duncan,Mac Schwager*

Main category: cs.LG

TL;DR: 开发了一种数据驱动的飞行测试运行时安全监控方法，通过离线随机轨迹模拟学习短期安全风险的校准统计模型，为飞行员提供预判标准来提前中止危险机动。


<details>
  <summary>Details</summary>
Motivation: 飞行测试中飞行员在参数不确定的飞机上执行机动时，安全违规可能因不确定性而意外发生，需要为飞行员提供清晰、预判性的中止标准。

Method: 包含三个通用组件：基于近期观测预测未来状态的模型、基于最近邻模型对预测状态进行安全分类、通过保形预测进行分类器校准。

Result: 在具有不确定参数的飞行动力学模型上评估，证明该方法能可靠识别不安全场景、匹配理论保证，并在风险预判分类方面优于基线方法。

Conclusion: 该方法为飞行测试等高风险、不确定性和人机交互场景提供了有效的运行时安全监控解决方案。

Abstract: We develop a data-driven approach for runtime safety monitoring in flight testing, where pilots perform maneuvers on aircraft with uncertain parameters. Because safety violations can arise unexpectedly as a result of these uncertainties, pilots need clear, preemptive criteria to abort the maneuver in advance of safety violation. To solve this problem, we use offline stochastic trajectory simulation to learn a calibrated statistical model of the short-term safety risk facing pilots. We use flight testing as a motivating example for data-driven learning/monitoring of safety due to its inherent safety risk, uncertainty, and human-interaction. However, our approach consists of three broadly-applicable components: a model to predict future state from recent observations, a nearest neighbor model to classify the safety of the predicted state, and classifier calibration via conformal prediction. We evaluate our method on a flight dynamics model with uncertain parameters, demonstrating its ability to reliably identify unsafe scenarios, match theoretical guarantees, and outperform baseline approaches in preemptive classification of risk.

</details>


### [45] [DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving](https://arxiv.org/abs/2511.21669)
*Fengze Yu,Leshu Li,Brad McDanel,Saiqian Zhang*

Main category: cs.LG

TL;DR: DSD是一个分布式推测解码框架，通过协调草稿-目标执行在多设备部署中扩展推测解码，解决了LLM推理的高延迟和跨异构边缘-云环境可扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理存在高解码延迟和在异构边缘-云环境中可扩展性有限的问题，现有推测解码技术仅限于单节点执行。

Method: 提出DSD分布式推测解码框架，通过协调草稿-目标执行实现多设备部署；引入DSD-Sim离散事件模拟器捕捉网络、批处理和调度动态；设计自适应窗口控制策略动态调整推测窗口大小。

Result: 实验显示DSD相比现有推测解码基线实现了最高1.1倍加速和9.7%吞吐量提升，能够在边缘和云环境中实现敏捷可扩展的LLM服务。

Conclusion: DSD框架成功将推测解码扩展到分布式环境，显著提升了LLM推理性能和在异构边缘-云环境中的可扩展性。

Abstract: Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.

</details>


### [46] [Effects of Initialization Biases on Deep Neural Network Training Dynamics](https://arxiv.org/abs/2511.20826)
*Nicholas Pellegrino,David Szczecina,Paul W. Fieguth*

Main category: cs.LG

TL;DR: 未经训练的大型神经网络在随机初始化后倾向于偏好少数类别，这种初始猜测偏差会影响早期训练动态，特别是损失函数的选择对此有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究初始猜测偏差对神经网络早期训练动态的影响，特别是不同损失函数如何与这种偏差相互作用。

Method: 分析未经训练神经网络在随机初始化后的类别偏好行为，研究不同损失函数（包括Blurry和Piecewise-zero损失）在存在初始偏差时的表现。

Result: 发现初始猜测偏差显著影响早期训练动态，某些为标签错误设计的损失函数在存在初始偏差时可能无法有效指导训练方向。

Conclusion: 损失函数选择对网络早期训练阶段有重大影响，需要仔细考虑初始猜测偏差与训练方案各组成部分的相互作用。

Abstract: Untrained large neural networks, just after random initialization, tend to favour a small subset of classes, assigning high predicted probabilities to these few classes and approximately zero probability to all others. This bias, termed Initial Guessing Bias, affects the early training dynamics, when the model is fitting to the coarse structure of the data. The choice of loss function against which to train the model has a large impact on how these early dynamics play out. Two recent loss functions, Blurry and Piecewise-zero loss, were designed for robustness to label errors but can become unable to steer the direction of training when exposed to this initial bias. Results indicate that the choice of loss function has a dramatic effect on the early phase training of networks, and highlights the need for careful consideration of how Initial Guessing Bias may interact with various components of the training scheme.

</details>


### [47] [Autoregressive Surrogate Modeling of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.20830)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 开发了首个基于球形傅里叶神经算子的自回归机器学习代理模型，用于预测太阳风径向速度，相比传统数值方法在精度和计算效率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统三维磁流体动力学模型计算成本高昂，限制了边界条件不确定性的快速探索，需要开发更高效的太阳风预测方法。

Method: 使用球形傅里叶神经算子，通过预测有限径向范围并迭代向外传播解，采用自回归方法改进远距离区域的精度。

Result: 与数值HUX代理模型相比，SFNO表现出相当或更优的性能，同时提供灵活、可训练的数据驱动替代方案。

Conclusion: 建立了一种用于高保真太阳风建模的新方法，为空间天气预报提供了高效的计算工具。

Abstract: The solar wind, a continuous outflow of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Accurate prediction of features such as high-speed streams and coronal mass ejections is critical for space weather forecasting, but traditional three-dimensional magnetohydrodynamic (MHD) models are computationally expensive, limiting rapid exploration of boundary condition uncertainties. We introduce the first autoregressive machine learning surrogate for steady-state solar wind radial velocity using the Spherical Fourier Neural Operator (SFNO). By predicting a limited radial range and iteratively propagating the solution outward, the model improves accuracy in distant regions compared to a single-step approach. Compared with the numerical HUX surrogate, SFNO demonstrates superior or comparable performance while providing a flexible, trainable, and data-driven alternative, establishing a novel methodology for high-fidelity solar wind modeling. The source code and additional visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity-autoregressive.

</details>


### [48] [Primal: A Unified Deterministic Framework for Quasi-Orthogonal Hashing and Manifold Learning](https://arxiv.org/abs/2511.20839)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: Primal是一个基于素数平方根数论独立性的确定性特征映射框架，提供静态和动态两种变体，能够在等距核映射和最大熵单向哈希之间无缝切换。


<details>
  <summary>Details</summary>
Motivation: 传统随机投影方法（如随机傅里叶特征）存在随机性，而Primal旨在利用素数平方根的贝西科维奇性质构建确定性的、可调谐的向量表示，提供数学上更严谨的替代方案。

Method: 提出两种算法变体：StaticPrime生成接近韦尔奇界准正交性的时序位置编码；DynamicPrime通过单一缩放参数σ统一低频等距核映射和高频混沌相位包装两种数学效用。

Result: 实证评估显示该框架在正交性保持和分布紧致性方面优于归一化高斯基线，成为随机矩阵投影的计算高效、数学严谨的替代方案。

Conclusion: Primal框架通过素数平方根的数论特性，成功构建了确定性的特征映射方法，在信号重建、压缩感知、超维计算和隐私保护分割学习等多个领域展现出优越性能。

Abstract: We present Primal, a deterministic feature mapping framework that harnesses the number-theoretic independence of prime square roots to construct robust, tunable vector representations. Diverging from standard stochastic projections (e.g., Random Fourier Features), our method exploits the Besicovitch property to create irrational frequency modulations that guarantee infinite non-repeating phase trajectories. We formalize two distinct algorithmic variants: (1) StaticPrime, a sequence generation method that produces temporal position encodings empirically approaching the theoretical Welch bound for quasi-orthogonality; and (2) DynamicPrime, a tunable projection layer for input-dependent feature mapping. A central novelty of the dynamic framework is its ability to unify two disparate mathematical utility classes through a single scaling parameter σ. In the low-frequency regime, the method acts as an isometric kernel map, effectively linearizing non-convex geometries (e.g., spirals) to enable high-fidelity signal reconstruction and compressive sensing. Conversely, the high-frequency regime induces chaotic phase wrapping, transforming the projection into a maximum-entropy one-way hash suitable for Hyperdimensional Computing and privacy-preserving Split Learning. Empirical evaluations demonstrate that our framework yields superior orthogonality retention and distribution tightness compared to normalized Gaussian baselines, establishing it as a computationally efficient, mathematically rigorous alternative to random matrix projections. The code is available at https://github.com/VladimerKhasia/primal

</details>


### [49] [Pre-train to Gain: Robust Learning Without Clean Labels](https://arxiv.org/abs/2511.20844)
*David Szczecina,Nicholas Pellegrino,Paul Fieguth*

Main category: cs.LG

TL;DR: 使用自监督学习预训练特征提取器，然后在带噪声标签的数据集上进行标准监督训练，可以在无需干净标签子集的情况下训练出更抗噪声的模型。


<details>
  <summary>Details</summary>
Motivation: 深度网络在有噪声标签的情况下训练会导致泛化能力差和准确率下降，因为模型会过度拟合标签噪声。现有方法通常依赖干净数据子集，而本文旨在无需干净标签子集的情况下提高模型对标签噪声的鲁棒性。

Method: 采用自监督学习方法（SimCLR和Barlow Twins）预训练特征提取器，然后在带噪声标签的数据集上进行标准监督训练。在CIFAR-10和CIFAR-100数据集上评估了合成和真实世界噪声下的表现。

Result: 在所有噪声率下，自监督预训练都能持续提高分类准确率并增强下游标签错误检测能力（F1和平衡准确率）。随着噪声率增加，性能差距扩大，显示出更好的鲁棒性。在低噪声水平下与ImageNet预训练模型结果相当，在高噪声条件下显著优于它们。

Conclusion: 自监督预训练是一种有效的策略，可以在无需干净标签子集的情况下提高模型对标签噪声的鲁棒性，特别是在高噪声条件下表现优异。

Abstract: Training deep networks with noisy labels leads to poor generalization and degraded accuracy due to overfitting to label noise. Existing approaches for learning with noisy labels often rely on the availability of a clean subset of data. By pre-training a feature extractor backbone without labels using self-supervised learning (SSL), followed by standard supervised training on the noisy dataset, we can train a more noise robust model without requiring a subset with clean labels. We evaluate the use of SimCLR and Barlow~Twins as SSL methods on CIFAR-10 and CIFAR-100 under synthetic and real world noise. Across all noise rates, self-supervised pre-training consistently improves classification accuracy and enhances downstream label-error detection (F1 and Balanced Accuracy). The performance gap widens as the noise rate increases, demonstrating improved robustness. Notably, our approach achieves comparable results to ImageNet pre-trained models at low noise levels, while substantially outperforming them under high noise conditions.

</details>


### [50] [Selecting Belief-State Approximations in Simulators with Latent States](https://arxiv.org/abs/2511.20870)
*Nan Jiang*

Main category: cs.LG

TL;DR: 本文研究了模拟器中状态重置的问题，特别是当模拟器包含潜在变量时，如何从给定观测历史的后验分布中采样以进行状态重置。


<details>
  <summary>Details</summary>
Motivation: 状态重置是模拟器的基本能力，但在复杂模拟器中实现非平凡，需要从给定观测历史的潜在状态后验分布中采样。现有多种近似信念状态采样器，但如何仅通过采样访问来选择最佳采样器是一个挑战。

Method: 将信念状态选择问题简化为一般条件分布选择任务，开发了新算法和分析方法。提出了两种不同表述：基于潜在状态的选择（直接针对潜在状态的条件分布）和基于观测的选择（针对观测的诱导分布）。

Result: 发现两种表述与下游roll-out方法的交互方式不同：基于观测的选择在单次重置方法下可能失败，但在重复重置方法下具有保证。

Conclusion: 该问题揭示了算法选择、理论细微差别和开放问题的丰富景观，即使在看似简单的问题中也存在复杂的权衡。

Abstract: State resetting is a fundamental but often overlooked capability of simulators. It supports sample-based planning by allowing resets to previously encountered simulation states, and enables calibration of simulators using real data by resetting to states observed in real-system traces. While often taken for granted, state resetting in complex simulators can be nontrivial: when the simulator comes with latent variables (states), state resetting requires sampling from the posterior over the latent state given the observable history, a.k.a. the belief state (Silver and Veness, 2010). While exact sampling is often infeasible, many approximate belief-state samplers can be constructed, raising the question of how to select among them using only sampling access to the simulator.
  In this paper, we show that this problem reduces to a general conditional distribution-selection task and develop a new algorithm and analysis under sampling-only access. Building on this reduction, the belief-state selection problem admits two different formulations: latent state-based selection, which directly targets the conditional distribution of the latent state, and observation-based selection, which targets the induced distribution over the observation. Interestingly, these formulations differ in how their guarantees interact with the downstream roll-out methods: perhaps surprisingly, observation-based selection may fail under the most natural roll-out method (which we call Single-Reset) but enjoys guarantees under the less conventional alternative (which we call Repeated-Reset). Together with discussion on issues such as distribution shift and the choice of sampling policies, our paper reveals a rich landscape of algorithmic choices, theoretical nuances, and open questions, in this seemingly simple problem.

</details>


### [51] [Representation Integrity in Temporal Graph Learning Methods](https://arxiv.org/abs/2511.20873)
*Elahe Kooshafar*

Main category: cs.LG

TL;DR: 本文提出了动态图表示完整性的概念，并开发了一种评估指标来衡量嵌入变化如何忠实反映图结构变化，为动态图学习模型提供了任务无关的评估工具。


<details>
  <summary>Details</summary>
Motivation: 传统动态图学习基准主要关注任务特定分数，但很少评估嵌入本身是否真实反映网络的演化过程。需要一种能够衡量表示完整性的评估框架。

Method: 形式化表示完整性概念，设计42个候选指标，通过三个合成场景（渐进合并、突然移动、周期性重连）筛选，推荐通过理论和实证验证的最佳指标。

Result: 验证的指标始终将可证明稳定的UASE和IPP模型排名最高，神经方法在特定场景下表现优势，且与一步链接预测AUC呈强正相关。

Conclusion: 提出的完整性框架为动态图表示质量提供了任务无关且可解释的评估工具，为模型选择和未来架构设计提供明确指导。

Abstract: Real-world systems ranging from airline routes to cryptocurrency transfers are naturally modelled as dynamic graphs whose topology changes over time. Conventional benchmarks judge dynamic-graph learners by a handful of task-specific scores, yet seldom ask whether the embeddings themselves remain a truthful, interpretable reflection of the evolving network. We formalize this requirement as representation integrity and derive a family of indexes that measure how closely embedding changes follow graph changes. Three synthetic scenarios, Gradual Merge, Abrupt Move, and Periodic Re-wiring, are used to screen forty-two candidate indexes. Based on which we recommend one index that passes all of our theoretical and empirical tests. In particular, this validated metric consistently ranks the provably stable UASE and IPP models highest. We then use this index to do a comparative study on representation integrity of common dynamic graph learning models. This study exposes the scenario-specific strengths of neural methods, and shows a strong positive rank correlation with one-step link-prediction AUC. The proposed integrity framework, therefore, offers a task-agnostic and interpretable evaluation tool for dynamic-graph representation quality, providing more explicit guidance for model selection and future architecture design.

</details>


### [52] [Probabilistic Hash Embeddings for Online Learning of Categorical Features](https://arxiv.org/abs/2511.20893)
*Aodong Li,Abishek Sankararaman,Balakrishnan Narayanaswamy*

Main category: cs.LG

TL;DR: 提出概率哈希嵌入(PHE)模型，通过贝叶斯在线学习处理流式数据中词汇表动态变化的分类特征，解决确定性嵌入方法在在线学习中的遗忘问题和顺序敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统特征哈希方法在离线批处理中表现良好，但在在线学习场景中，确定性嵌入对类别到达顺序敏感且容易遗忘旧类别，导致性能下降。

Method: 将哈希嵌入视为随机变量，应用贝叶斯在线学习从数据中增量学习，推导出可扩展的推理算法来学习模型参数并推断/更新哈希嵌入和其他潜在变量的后验分布。

Result: 在分类、序列建模和推荐系统的在线学习实验中，PHE表现出优越性能，同时保持高内存效率（仅消耗one-hot嵌入表2~4倍的内存）。

Conclusion: PHE模型能够处理不断演变的分类词汇表，适应新项目而不遗忘旧项目，参数数量有界不随流中不同值数量增长，且对项目到达顺序不敏感。

Abstract: We study streaming data with categorical features where the vocabulary of categorical feature values is changing and can even grow unboundedly over time. Feature hashing is commonly used as a pre-processing step to map these categorical values into a feature space of fixed size before learning their embeddings. While these methods have been developed and evaluated for offline or batch settings, in this paper we consider online settings. We show that deterministic embeddings are sensitive to the arrival order of categories and suffer from forgetting in online learning, leading to performance deterioration. To mitigate this issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally from data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle an evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed values on the stream, and (iv) is invariant to the item arrival order. Experiments in classification, sequence modeling, and recommendation systems in online learning setups demonstrate the superior performance of PHE while maintaining high memory efficiency (consumes as low as 2~4 memory of a one-hot embedding table). Supplementary materials are at https://github.com/aodongli/probabilistic-hash-embeddings

</details>


### [53] [Evolved SampleWeights for Bias Mitigation: Effectiveness Depends on Optimization Objectives](https://arxiv.org/abs/2511.20909)
*Anil K. Saini,Jose Guadalupe Hernandez,Emily F. Wong,Debanshi Misra,Jason H. Moore*

Main category: cs.LG

TL;DR: 比较三种样本权重生成方法（遗传算法演化、基于数据集特征计算、等权重）在机器学习模型公平性和预测性能之间的权衡效果。实验表明遗传算法演化的权重能在多个数据集上实现更好的公平性-性能平衡，但效果取决于优化目标的选择。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在真实数据训练时可能产生有偏预测，对边缘化群体造成负面影响。需要方法在保持预测性能的同时提升模型公平性。

Method: 使用遗传算法演化样本权重，并与基于数据集特征的权重计算方法和等权重方法进行比较。评估指标包括两个预测指标（准确率、AUC）和两个公平性指标（人口统计均等差异、子组假负公平性）。在11个公开数据集上进行实验。

Result: 演化样本权重能产生在公平性和预测性能之间实现更好权衡的模型。使用准确率和人口统计均等差异作为优化目标时，演化权重在最多数据集上显著优于其他权重策略。

Conclusion: 遗传算法演化的样本权重是提升模型公平性的有效方法，但优化目标的选择对最终效果有重要影响。

Abstract: Machine learning models trained on real-world data may inadvertently make biased predictions that negatively impact marginalized communities. Reweighting is a method that can mitigate such bias in model predictions by assigning a weight to each data point used during model training. In this paper, we compare three methods for generating these weights: (1) evolving them using a Genetic Algorithm (GA), (2) computing them using only dataset characteristics, and (3) assigning equal weights to all data points. Model performance under each strategy was evaluated using paired predictive and fairness metrics, which also served as optimization objectives for the GA during evolution. Specifically, we used two predictive metrics (accuracy and area under the Receiver Operating Characteristic curve) and two fairness metrics (demographic parity difference and subgroup false negative fairness). Using experiments on eleven publicly available datasets (including two medical datasets), we show that evolved sample weights can produce models that achieve better trade-offs between fairness and predictive performance than alternative weighting methods. However, the magnitude of these benefits depends strongly on the choice of optimization objectives. Our experiments reveal that optimizing with accuracy and demographic parity difference metrics yields the largest number of datasets for which evolved weights are significantly better than other weighting strategies in optimizing both objectives.

</details>


### [54] [Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2511.20913)
*Yingchuan Sun,Shengpu Tang*

Main category: cs.LG

TL;DR: 本文通过实证研究比较了脓毒症管理中四种时间步长（1、2、4、8小时）对离线强化学习的影响，发现更细的时间步长（1-2小时）能获得更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有脓毒症管理的强化学习研究大多使用4小时时间步长，但该时间粒度可能扭曲患者动态并导致次优治疗策略，实际影响程度尚未被充分探索。

Method: 设计了动作重映射方法，在相同离线强化学习流程下对比四种时间步长，进行跨时间步长的模型选择，评估状态表示学习、行为克隆、策略训练和离策略评估。

Result: 性能趋势随学习设置变化，但使用静态行为策略在更细时间步长（1-2小时）学习的策略获得了整体最佳性能和稳定性。

Conclusion: 时间步长是医疗领域离线强化学习的核心设计选择，研究结果为超越传统4小时设置的替代方案提供了证据支持。

Abstract: Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($Δt\!=\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$Δt$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $Δt$ vary as learning setups change, while policies learned at finer time-step sizes ($Δt = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.

</details>


### [55] [Operationalizing Quantized Disentanglement](https://arxiv.org/abs/2511.20927)
*Vitoria Barin-Pacela,Kartik Ahuja,Simon Lacoste-Julien,Pascal Vincent*

Main category: cs.LG

TL;DR: 提出了一种基于轴对齐不连续性的无监督解缠方法Cliff，通过鼓励因子密度中的轴对齐悬崖来实现解缠，在多个基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有理论表明量化因子在任意微分同胚下具有无监督可识别性，但将这一理论原理转化为有效的实践标准仍然具有挑战性，特别是在非线性映射下。

Method: 开发了一种鼓励轴对齐不连续性的无监督解缠标准，通过鼓励因子密度中的悬崖（sharp changes）来实现解缠，确保沿某个因子的悬崖位置与其他因子的值独立。

Result: Cliff方法在所有解缠基准测试中均优于基线方法，证明了其在无监督解缠中的有效性。

Conclusion: 通过鼓励轴对齐不连续性的方法能够有效实现无监督解缠，为理论原理向实践转化提供了可行方案。

Abstract: Recent theoretical work established the unsupervised identifiability of quantized factors under any diffeomorphism. The theory assumes that quantization thresholds correspond to axis-aligned discontinuities in the probability density of the latent factors. By constraining a learned map to have a density with axis-aligned discontinuities, we can recover the quantization of the factors. However, translating this high-level principle into an effective practical criterion remains challenging, especially under nonlinear maps. Here, we develop a criterion for unsupervised disentanglement by encouraging axis-aligned discontinuities. Discontinuities manifest as sharp changes in the estimated density of factors and form what we call cliffs. Following the definition of independent discontinuities from the theory, we encourage the location of the cliffs along a factor to be independent of the values of the other factors. We show that our method, Cliff, outperforms the baselines on all disentanglement benchmarks, demonstrating its effectiveness in unsupervised disentanglement.

</details>


### [56] [Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection](https://arxiv.org/abs/2511.20944)
*Yaw Osei Adjei*

Main category: cs.LG

TL;DR: 本文比较了两种BEC检测方法：基于心理语言学的CatBoost和基于深度学习的DistilBERT。DistilBERT在精度上表现更优(AUC=1.0000)，而CatBoost在延迟和成本方面更具优势。两种方法通过成本敏感学习都能实现超过99.96%的投资回报率。


<details>
  <summary>Details</summary>
Motivation: BEC攻击每年造成超过29亿美元损失，存在显著的经济不对称性：误报成本远低于漏报成本（比例约为1:5,480），需要开发高效的检测方案。

Method: 提出两种检测范式：1) 心理语言学流使用CatBoost分析心理语言线索；2) 语义流使用DistilBERT进行深度学习上下文理解。在对抗性污染数据集(N=7,990)上进行评估。

Result: DistilBERT实现卓越检测性能(AUC=1.0000, F1=0.9981)，延迟7.403毫秒；CatBoost性能竞争性(AUC=0.9905, F1=0.9486)，延迟低8.4倍(0.885毫秒)，计算资源消耗可忽略。

Conclusion: 对于有GPU基础设施的组织，DistilBERT提供更优精度；对于边缘部署或成本敏感环境，CatBoost因可比较的安全性和更低运营成本而更可取。两种方法通过成本敏感学习都能实现超过99.96%的投资回报率。

Abstract: Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies and exploits psychological vulnerabilities, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual adjusted losses, presenting significant economic asymmetry: the cost of a False Negative (fraud loss) exceeds the cost of a False Positive (manual review) by orders of magnitude (approximately 1 to 5,480).
  This paper examines two detection paradigms for BEC: the Forensic Psycholinguistic Stream, which utilizes CatBoost to analyze psycholinguistic cues with high interpretability and low latency, and the Semantic Stream, which employs DistilBERT for deep learning-based contextual language understanding, offering superior accuracy at higher computational cost. We evaluated DistilBERT on an adversarially poisoned dataset (N = 7,990) generated via our Black Hole protocol, benchmarked on Tesla T4 GPU infrastructure, achieving superior detection (AUC = 1.0000, F1 = 0.9981) with acceptable real-time latency (7.403 milliseconds). CatBoost achieves competitive detection (AUC = 0.9905, F1 = 0.9486) at 8.4x lower latency (0.885 milliseconds), consuming negligible computational resources. For organizations with GPU infrastructure, DistilBERT offers superior accuracy. CatBoost is preferable for edge deployments or cost-sensitive environments due to comparable security and lower operational costs. Both approaches demonstrate return on investment exceeding 99.96% when optimized through cost-sensitive learning, by significantly reducing false negatives and associated financial losses.

</details>


### [57] [Dataset Poisoning Attacks on Behavioral Cloning Policies](https://arxiv.org/abs/2511.20992)
*Akansha Kalra,Soumil Datta,Ethan Gilmore,Duc La,Guanhong Tao,Daniel S. Brown*

Main category: cs.LG

TL;DR: 本文首次分析了行为克隆策略对干净标签后门攻击的脆弱性，发现即使使用少量中毒数据训练的策略在部署时也极易受到后门触发攻击。


<details>
  <summary>Details</summary>
Motivation: 随着行为克隆策略在现实世界中部署，其鲁棒性和潜在漏洞成为重要关注点。需要评估这些策略对后门攻击的脆弱性。

Method: 通过在专家演示数据集中注入视觉触发器来创建虚假相关性，并引入基于熵的测试时触发攻击，识别关键状态进行攻击。

Result: 实验表明，即使使用最小量中毒数据训练的行为克隆策略，在部署时也表现出高度脆弱性，尽管任务性能接近基线水平。

Conclusion: 研究结果强调了迫切需要加强对行为克隆策略鲁棒性的研究，特别是当大规模数据集被用于训练现实世界网络物理系统的策略时。

Abstract: Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious correlation that can be exploited at test time. We evaluate how policy vulnerability scales with the fraction of poisoned data, the strength of the trigger, and the trigger type. We also introduce a novel entropy-based test-time trigger attack that substantially degrades policy performance by identifying critical states where test-time triggering of the backdoor is expected to be most effective at degrading performance. We empirically demonstrate that BC policies trained on even minimally poisoned datasets exhibit deceptively high, near-baseline task performance despite being highly vulnerable to backdoor trigger attacks during deployment. Our results underscore the urgent need for more research into the robustness of BC policies, particularly as large-scale datasets are increasingly used to train policies for real-world cyber-physical systems. Videos and code are available at https://sites.google.com/view/dataset-poisoning-in-bc.

</details>


### [58] [Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning](https://arxiv.org/abs/2511.20993)
*Shanwei Fan*

Main category: cs.LG

TL;DR: 提出SGA-ACR框架，通过环境特定子目标图和结构化实体知识，结合多LLM规划管道，解决LLM在强化学习中规划-执行不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在强化学习中提供高层次规划能力，但由于缺乏环境特定知识，产生的子目标往往语义合理但不可行，且单LLM规划将生成与自验证混为一谈，导致不可靠的子目标。

Method: 集成环境特定子目标图和结构化实体知识，采用多LLM规划管道明确分离生成、批判和精炼过程，使用子目标跟踪器监控执行进度并提供辅助奖励。

Result: 在开放世界游戏"Crafter"的22个多样化任务上验证了方法的有效性。

Conclusion: SGA-ACR框架通过结构化规划和执行监控，显著提高了LLM在强化学习中的规划-执行对齐效果。

Abstract: Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.

</details>


### [59] [FANoise: Singular Value-Adaptive Noise Modulation for Robust Multimodal Representation Learning](https://arxiv.org/abs/2511.20997)
*Jiaoyang Li,Jun Fang,Tianhao Gao,Xiaohui Zhang,Zhiyuan Liu,Chao Liu,Pengzhang Liu,Qixia Jiang*

Main category: cs.LG

TL;DR: 提出FANoise方法，通过特征自适应噪声注入策略改进多模态表示学习，在多种基础VLM模型上持续提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖启发式或静态噪声，忽略了训练过程中特征分布的动态特性，需要更智能的噪声注入策略

Method: 提出FANoise特征自适应噪声注入策略，利用对比学习的动态特性，在梯度基和特征分布视角下系统研究噪声作用

Result: 综合实验表明FANoise在多种多模态任务上持续提升整体性能

Conclusion: 特征自适应噪声注入是改进表示学习性能的有效方法

Abstract: Representation learning is fundamental to modern machine learning, powering applications such as text retrieval and multimodal understanding. However, learning robust and generalizable representations remains challenging. While prior work has demonstrated that active noise injection, a form of data augmentation, can enhance encoding performance, most existing methods rely on heuristic or static noise, overlooking the dynamic nature of feature distributions during training. In this work, we systematically study the role of noise in representation learning from both gradient-based and feature distribution perspectives, using InfoNCE loss as a representative example. Focusing on multimodal representation learning, we propose FANoise, a novel feature-adaptive noise injection strategy. By leveraging the dynamics of contrastive learning, FANoise effectively mitigates the negative impacts of noise while preserving its benefits. Under this theoretically grounded framework, comprehensive experiments demonstrate that FANoise consistently improves overall performance on multimodal tasks across various base VLM models.

</details>


### [60] [Estimating Ising Models in Total Variation Distance](https://arxiv.org/abs/2511.21008)
*Constantinos Daskalakis,Vardis Kandiros,Rui Yao*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架来分析Ising模型的极大伪似然估计器(MPLE)，针对两类广义Ising模型：有界算子范数且满足修正对数Sobolev不等式(MLSI)的模型，以及有界无穷范数(有界宽度)的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Ising模型在总变差距离下的统计复杂性已被充分理解，但识别计算和统计高效的算法仍然具有挑战性。现有研究主要针对特定设置(如树结构、高斯交互矩阵等)，缺乏统一的框架。

Method: 使用极大伪似然估计器(MPLE)，结合张量化不等式、测度分解和集中界等工具，对两类广义Ising模型进行统一分析。

Result: 为这两类模型提供了多项式时间算法，并在各种设置下获得了最优或接近最优的样本复杂度保证。

Conclusion: 该研究为Ising模型的估计提供了一个统一的分析框架，扩展了先前仅限于特定设置的结果，并在更一般的条件下实现了高效估计。

Abstract: We consider the problem of estimating Ising models over $n$ variables in Total Variation (TV) distance, given $l$ independent samples from the model. While the statistical complexity of the problem is well-understood [DMR20], identifying computationally and statistically efficient algorithms has been challenging. In particular, remarkable progress has occurred in several settings, such as when the underlying graph is a tree [DP21, BGPV21], when the entries of the interaction matrix follow a Gaussian distribution [GM24, CK24], or when the bulk of its eigenvalues lie in a small interval [AJK+24, KLV24], but no unified framework for polynomial-time estimation in TV exists so far. Our main contribution is a unified analysis of the Maximum Pseudo-Likelihood Estimator (MPLE) for two general classes of Ising models. The first class includes models that have bounded operator norm and satisfy the Modified Log-Sobolev Inequality (MLSI), a functional inequality that was introduced to study the convergence of the associated Glauber dynamics to stationarity. In the second class of models, the interaction matrix has bounded infinity norm (or bounded width), which is the most common assumption in the literature for structure learning of Ising models. We show how our general results for these classes yield polynomial-time algorithms and optimal or near-optimal sample complexity guarantees in a variety of settings. Our proofs employ a variety of tools from tensorization inequalities to measure decompositions and concentration bounds.

</details>


### [61] [ChatGpt Content detection: A new approach using xlm-roberta alignment](https://arxiv.org/abs/2511.21009)
*Md Tasnin Tanvir,Dr Santanu Kumar Dash,Ishan Shahnan,Nafis Fuad,Tanvir Rahman,Abdullah Al Faisal,Asadullah Al Mamun*

Main category: cs.LG

TL;DR: 使用XLM-RoBERTa模型检测AI生成文本，包括完全AI生成和AI改写的人类文本，通过困惑度、语义和可读性特征实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等生成式AI技术普及，区分AI生成文本与人类撰写内容的需求日益迫切，旨在维护学术诚信和促进AI伦理透明度。

Method: 采用XLM-RoBERTa多语言transformer模型，结合严格的预处理和特征提取（困惑度、语义、可读性特征），在平衡的人类与AI生成文本数据集上进行微调。

Result: 模型在不同文本类型中表现出高准确性和稳健性能，特征分析显示困惑度和注意力特征在区分人类与AI生成文本中起关键作用。

Conclusion: 该方法为维护学术诚信提供了有效工具，未来研究方向包括探索更先进模型和扩展数据集以提升模型泛化能力。

Abstract: The challenge of separating AI-generated text from human-authored content is becoming more urgent as generative AI technologies like ChatGPT become more widely available. In this work, we address this issue by looking at both the detection of content that has been entirely generated by AI and the identification of human text that has been reworded by AI. In our work, a comprehensive methodology to detect AI- generated text using XLM-RoBERTa, a state-of-the-art multilingual transformer model. Our approach includes rigorous preprocessing, and feature extraction involving perplexity, semantic, and readability features. We fine-tuned the XLM-RoBERTa model on a balanced dataset of human and AI-generated texts and evaluated its performance. The model demonstrated high accuracy and robust performance across various text genres. Additionally, we conducted feature analysis to understand the model's decision-making process, revealing that perplexity and attention-based features are critical in differentiating between human and AI-generated texts. Our findings offer a valuable tool for maintaining academic integrity and contribute to the broader field of AI ethics by promoting transparency and accountability in AI systems. Future research directions include exploring other advanced models and expanding the dataset to enhance the model's generalizability.

</details>


### [62] [Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.21011)
*Sid Bharthulwar,Stone Tao,Hao Su*

Main category: cs.LG

TL;DR: 提出了交错重置技术，通过在任务时间轴的不同点初始化和重置环境，增加训练批次的时间多样性，减少同步回放引入的非平稳性，显著提高样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 在GPU大规模并行环境中，为了最大化吞吐量通常使用短回放，但这会导致同步重置引入有害的非平稳性，扭曲学习信号并破坏训练稳定性。

Method: 提出交错重置技术，让环境在任务时间轴的不同点进行初始化和重置，从而产生具有更大时间多样性的训练批次。

Result: 在挑战性的高维机器人环境中，该方法实现了显著更高的样本效率、更快的实际收敛速度和更强的最终性能，且随着并行环境数量的增加扩展性更好。

Conclusion: 交错重置是一种简单而有效的技术，能够显著改善大规模并行RL训练中的非平稳性问题，提升训练效率和性能。

Abstract: Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.

</details>


### [63] [Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression](https://arxiv.org/abs/2511.21016)
*Liangzu Peng,Aditya Chattopadhyay,Luca Zancato,Elvis Nunez,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: Gated KalmaNet (GKA) 是一种高效的线性状态空间模型层，通过在线岭回归解决传统SSM在召回任务中的性能差距，同时保持恒定内存和线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统线性状态空间模型(SSMs)虽然计算高效，但只能维护有损的、衰减的过去信息摘要，在召回导向任务中表现较差。需要一种既能考虑完整历史信息又保持SSM效率的方法。

Method: 1. 基于卡尔曼滤波器思想迭代求解在线岭回归问题；2. 使用自适应正则化策略和输入相关门控控制岭回归条件数；3. 采用切比雪夫迭代而非传统迭代求解器以提高低精度环境下的稳定性；4. 开发硬件感知的分块实现和自定义反向传播核。

Result: 在短上下文任务中超越现有SSM层(Mamba2、GLA、Gated DeltaNet)；在长上下文任务(128k tokens)中，RAG和LongQA任务相对其他衰减记忆基线获得超过10%的相对改进。

Conclusion: GKA成功解决了SSM在召回任务中的性能限制，同时保持了计算效率，在长短上下文任务中都表现出色。

Abstract: As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.

</details>


### [64] [Probabilistic Wildfire Spread Prediction Using an Autoregressive Conditional Generative Adversarial Network](https://arxiv.org/abs/2511.21019)
*Taehoon Kang,Taeyong Kim*

Main category: cs.LG

TL;DR: 提出基于自回归条件生成对抗网络(CGAN)的概率性野火蔓延预测模型，相比传统物理模拟器和深度学习模型，在预测精度和边界描绘方面表现更优


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了野火的频率和严重性，需要快速准确的火灾蔓延预测。物理模拟器计算密集，深度学习模型预测过于平滑，无法捕捉复杂的非线性动态

Method: 使用自回归条件生成对抗网络(CGAN)，将预测任务制定为自回归问题，学习序列状态转换，确保长期预测稳定性

Result: 实验结果表明，提出的CGAN模型在整体预测精度和火灾边界描绘方面优于传统深度学习模型，能够捕捉野火蔓延的强非线性和不确定性

Conclusion: 基于CGAN的自回归框架提高了野火蔓延预测的准确性和物理可解释性，为时间敏感的响应和疏散规划提供了有前景的基础

Abstract: Climate change has intensified the frequency and severity of wildfires, making rapid and accurate prediction of fire spread essential for effective mitigation and response. Physics-based simulators such as FARSITE offer high-fidelity predictions but are computationally intensive, limiting their applicability in real-time decision-making, while existing deep learning models often yield overly smooth predictions that fail to capture the complex, nonlinear dynamics of wildfire propagation. This study proposes an autoregressive conditional generative adversarial network (CGAN) for probabilistic wildfire spread prediction. By formulating the prediction task as an autoregressive problem, the model learns sequential state transitions, ensuring long-term prediction stability. Experimental results demonstrate that the proposed CGAN-based model outperforms conventional deep learning models in both overall predictive accuracy and boundary delineation of fire perimeters. These results demonstrate that adversarial learning allows the model to capture the strong nonlinearity and uncertainty of wildfire spread, instead of simply fitting the pixel average. Furthermore, the autoregressive framework facilitates systematic temporal forecasting of wildfire evolution. The proposed CGAN-based autoregressive framework enhances both the accuracy and physical interpretability of wildfire spread prediction, offering a promising foundation for time-sensitive response and evacuation planning.

</details>


### [65] [A Probabilistic Framework for Temporal Distribution Generalization in Industry-Scale Recommender Systems](https://arxiv.org/abs/2511.21032)
*Yuxuan Zhu,Cong Fu,Yabo Ni,Anxiang Zeng,Yuan Fang*

Main category: cs.LG

TL;DR: 提出ELBO$_	ext{TDS}$概率框架解决推荐系统中的时间分布偏移问题，通过数据增强和因果图建模提升时间泛化能力，已在Shopee产品搜索中成功部署。


<details>
  <summary>Details</summary>
Motivation: 时间分布偏移(TDS)会侵蚀推荐系统的长期准确性，现有方法如增量训练、不变性学习和自监督学习存在时间泛化不稳定、表示崩溃或数据利用效率低等问题。

Method: 1. 通过真实生产数据的统计分析识别关键偏移因素，设计数据增强策略重新采样时变因素来扩展训练支持；2. 使用因果图建模时序推荐场景，推导基于因果结构的自监督变分目标ELBO$_	ext{TDS}$。

Result: 实验证明该方法实现了优越的时间泛化能力，用户GMV提升2.33%，并已在Shopee产品搜索中成功部署。

Conclusion: ELBO$_	ext{TDS}$框架有效解决了推荐系统中的时间分布偏移问题，通过结合数据增强和因果建模实现了稳定且高效的时间泛化。

Abstract: Temporal distribution shift (TDS) erodes the long-term accuracy of recommender systems, yet industrial practice still relies on periodic incremental training, which struggles to capture both stable and transient patterns. Existing approaches such as invariant learning and self-supervised learning offer partial solutions but often suffer from unstable temporal generalization, representation collapse, or inefficient data utilization. To address these limitations, we propose ELBO$_\text{TDS}$, a probabilistic framework that integrates seamlessly into industry-scale incremental learning pipelines. First, we identify key shifting factors through statistical analysis of real-world production data and design a simple yet effective data augmentation strategy that resamples these time-varying factors to extend the training support. Second, to harness the benefits of this extended distribution while preventing representation collapse, we model the temporal recommendation scenario using a causal graph and derive a self-supervised variational objective, ELBO$_\text{TDS}$, grounded in the causal structure. Extensive experiments supported by both theoretical and empirical analysis demonstrate that our method achieves superior temporal generalization, yielding a 2.33\% uplift in GMV per user and has been successfully deployed in Shopee Product Search. Code is available at https://github.com/FuCongResearchSquad/ELBO4TDS.

</details>


### [66] [Prediction of Herd Life in Dairy Cows Using Multi-Head Attention Transformers](https://arxiv.org/abs/2511.21034)
*Mahdi Saki,Justin Lipman*

Main category: cs.LG

TL;DR: 开发基于多头注意力Transformer的AI模型，利用历史多变量时间序列数据预测奶牛寿命，在澳大利亚7个农场的19,000头奶牛数据上达到83%的决定系数。


<details>
  <summary>Details</summary>
Motivation: 奶农需要基于客观评估决定保留或淘汰奶牛，识别更具韧性的奶牛以应对农场条件并完成更多泌乳期，这一决策过程复杂且具有显著的环境和经济影响。

Method: 使用多头注意力Transformer技术分析从出生开始记录的历史多变量时间序列数据，处理了来自7个澳大利亚农场的19,000头奶牛的约780,000条记录。

Result: 模型在预测研究农场中奶牛寿命方面达到83%的整体决定系数，显示出在奶牛群管理中的实际应用潜力。

Conclusion: AI驱动的模型能够有效预测奶牛寿命，为奶农提供客观决策支持，有助于提高奶牛群管理效率。

Abstract: Dairy farmers should decide to keep or cull a cow based on an objective assessment of her likely performance in the herd. For this purpose, farmers need to identify more resilient cows, which can cope better with farm conditions and complete more lactations. This decision-making process is inherently complex, with significant environmental and economic implications. In this study, we develop an AI-driven model to predict cow longevity using historical multivariate time-series data recorded from birth. Leveraging advanced AI techniques, specifically Multi-Head Attention Transformers, we analysed approximately 780,000 records from 19,000 unique cows across 7 farms in Australia. The results demonstrate that our model achieves an overall determination coefficient of 83% in predicting herd life across the studied farms, highlighting its potential for practical application in dairy herd management.

</details>


### [67] [RAVQ-HoloNet: Rate-Adaptive Vector-Quantized Hologram Compression](https://arxiv.org/abs/2511.21035)
*Shima Rafiei,Zahra Nabizadeh Shahr Babak,Shadrokh Samavi,Shahram Shirani*

Main category: cs.LG

TL;DR: 提出了RAVQ-HoloNet，一种速率自适应矢量量化框架，在低比特率和超低比特率下实现高保真重建，性能优于现有方法


<details>
  <summary>Details</summary>
Motivation: 全息技术在AR/VR应用中潜力巨大，但受限于高数据压缩需求。现有深度学习方法通常缺乏单一网络内的速率自适应性

Method: 采用速率自适应矢量量化框架，能够在不同比特率下进行高效压缩

Result: 在低比特率下，BD-Rate降低33.91%，BD-PSNR达到1.02 dB，优于现有最佳方法

Conclusion: RAVQ-HoloNet框架在全息数据压缩方面表现出色，特别是在低比特率场景下具有显著优势

Abstract: Holography offers significant potential for AR/VR applications, yet its adoption is limited by the high demands of data compression. Existing deep learning approaches generally lack rate adaptivity within a single network. We present RAVQ-HoloNet, a rate-adaptive vector quantization framework that achieves high-fidelity reconstructions at low and ultra-low bit rates, outperforming current state-of-the-art methods. In low bit, our method exceeds by -33.91% in BD-Rate and achieves a BD-PSNR of 1.02 dB from the best existing method demonstrated by the rate-distortion curve.

</details>


### [68] [CNN-LSTM Hybrid Architecture for Over-the-Air Automatic Modulation Classification Using SDR](https://arxiv.org/abs/2511.21040)
*Dinanath Padhya,Krishna Acharya,Bipul Kumar Dahal,Dinesh Baniya Kshatri*

Main category: cs.LG

TL;DR: 提出基于CNN-LSTM混合架构的自动调制分类系统，结合软件定义无线电平台，在混合数据集上训练，在0-30dB信噪比范围内达到93.48%的准确率。


<details>
  <summary>Details</summary>
Motivation: 自动调制分类是未来无线通信系统的核心技术，对认知无线电、频谱监测和智能通信网络应用至关重要。

Method: 使用CNN进行空间特征提取和LSTM捕捉时间依赖性的混合架构，结合软件定义无线电平台，在RadioML2018数据集和自定义数据集的混合数据集上训练。

Result: 优化模型达到93.48%准确率、93.53%精确率、93.48%召回率和93.45% F1分数，AUC-ROC分析证实了模型在噪声条件下的判别能力。

Conclusion: 实验结果表明混合CNN-LSTM架构对自动调制分类有效，在自适应频谱管理和先进认知无线电系统中具有应用潜力。

Abstract: Automatic Modulation Classification (AMC) is a core technology for future wireless communication systems, enabling the identification of modulation schemes without prior knowledge. This capability is essential for applications in cognitive radio, spectrum monitoring, and intelligent communication networks. We propose an AMC system based on a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architecture, integrated with a Software Defined Radio (SDR) platform. The proposed architecture leverages CNNs for spatial feature extraction and LSTMs for capturing temporal dependencies, enabling efficient handling of complex, time-varying communication signals. The system's practical ability was demonstrated by identifying over-the-air (OTA) signals from a custom-built FM transmitter alongside other modulation schemes. The system was trained on a hybrid dataset combining the RadioML2018 dataset with a custom-generated dataset, featuring samples at Signal-to-Noise Ratios (SNRs) from 0 to 30dB. System performance was evaluated using accuracy, precision, recall, F1 score, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The optimized model achieved 93.48% accuracy, 93.53% precision, 93.48% recall, and an F1 score of 93.45%. The AUC-ROC analysis confirmed the model's discriminative power, even in noisy conditions. This paper's experimental results validate the effectiveness of the hybrid CNN-LSTM architecture for AMC, suggesting its potential application in adaptive spectrum management and advanced cognitive radio systems.

</details>


### [69] [FedAPA: Federated Learning with Adaptive Prototype Aggregation Toward Heterogeneous Wi-Fi CSI-based Crowd Counting](https://arxiv.org/abs/2511.21048)
*Jingtao Guo,Yuyi Mao,Ivan Wang-Hei Ho*

Main category: cs.LG

TL;DR: FedAPA是一种基于Wi-Fi CSI的联邦学习感知算法，通过自适应原型聚合策略解决数据异构问题，在分布式人群计数场景中显著提升性能并大幅降低通信开销。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI感知需要大量站点特定训练数据，联邦学习可避免原始数据共享但面临数据异构和设备资源差异的挑战。

Method: 采用自适应原型聚合策略为对等原型分配相似性权重，结合分类学习和表示对比学习的混合目标函数，为每个客户端生成个性化全局原型。

Result: 在6个环境、最多20人的真实场景中，FedAPA相比基线方法准确率提升至少9.65%，F1分数提升9%，MAE降低0.29，通信开销减少95.94%。

Conclusion: FedAPA有效解决了联邦Wi-Fi感知中的数据异构问题，实现了高性能和低通信开销的分布式感知。

Abstract: Wi-Fi channel state information (CSI)-based sensing provides a non-invasive, device-free approach for tasks such as human activity recognition and crowd counting, but large-scale deployment is hindered by the need for extensive site-specific training data. Federated learning (FL) offers a way to avoid raw data sharing but is challenged by heterogeneous sensing data and device resources. This paper proposes FedAPA, a collaborative Wi-Fi CSI-based sensing algorithm that uses adaptive prototype aggregation (APA) strategy to assign similarity-based weights to peer prototypes, enabling adaptive client contributions and yielding a personalized global prototype for each client instead of a fixed-weight aggregation. During local training, we adopt a hybrid objective that combines classification learning with representation contrastive learning to align local and global knowledge. We provide a convergence analysis of FedAPA and evaluate it in a real-world distributed Wi-Fi crowd counting scenario with six environments and up to 20 people. The results show that our method outperform multiple baselines in terms of accuracy, F1 score, mean absolute error (MAE), and communication overhead, with FedAPA achieving at least a 9.65% increase in accuracy, a 9% gain in F1 score, a 0.29 reduction in MAE, and a 95.94% reduction in communication overhead.

</details>


### [70] [Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs](https://arxiv.org/abs/2511.21050)
*Dongkyu Derek Cho,Huan Song,Arijit Ghosh Chowdhury,Haotian An,Yawei Wang,Rohit Thekkanal,Negin Sokhandan,Sharlina Keshava,Hannah Marlowe*

Main category: cs.LG

TL;DR: RLVR（带可验证奖励的强化学习）可以在提升推理能力的同时保持或改进安全防护，挑战了传统微调中安全与能力必然权衡的假设。


<details>
  <summary>Details</summary>
Motivation: 传统LLM微调方法（SFT和RLHF）存在安全与能力的权衡问题，即在提升任务性能时会降低安全对齐性。RLVR作为有前景的替代方法，其安全性影响尚未被探索。

Method: 通过理论分析推导KL约束优化下的安全漂移上界，并在五个对抗性安全基准上进行广泛实验，研究优化算法、模型规模和任务领域的影响。

Result: 理论证明在特定条件下可以消除安全退化，实证表明RLVR能同时增强推理能力并保持或改进安全防护。

Conclusion: RLVR训练方法可以同时实现安全和能力目标，为安全部署具备推理能力的LLMs提供了见解。

Abstract: Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.

</details>


### [71] [Efficient Diffusion Planning with Temporal Diffusion](https://arxiv.org/abs/2511.21054)
*Jiaming Guo,Rui Zhang,Zerun Li,Yunkai Gao,Shaohui Peng,Siming Lan,Xing Hu,Zidong Du,Xishan Zhang,Ling Li*

Main category: cs.LG

TL;DR: 提出Temporal Diffusion Planner (TDP)，通过将去噪步骤分布在时间维度上，提高扩散规划的决策效率，相比每步重新生成计划的方法提升决策频率11-24.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划方法每步重新生成计划导致计算开销大、决策频率低，且频繁切换计划影响性能。受人类制定短期详细、长期模糊计划的启发，提出更高效的规划方法。

Method: TDP首先生成随时间逐渐模糊的初始计划，后续时间步仅用少量去噪步骤更新前一步计划，而非完全重新生成，并引入自动重规划机制防止计划与现实偏差过大。

Result: 在D4RL基准测试中，相比每步重新生成计划的方法，TDP将决策频率提升11-24.8倍，同时达到更高或相当的性能。

Conclusion: TDP通过时间维度分布去噪步骤和自动重规划机制，显著提高了扩散规划的决策效率，为高效离线强化学习提供了新思路。

Abstract: Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.

</details>


### [72] [A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs](https://arxiv.org/abs/2511.21056)
*Quan Xiao,Tianyi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种通过双层数据选择和在线自优化生成的统一框架来提升LLM在下游任务中的性能，通过为每个问题和响应分配学习权重来优化数据质量。


<details>
  <summary>Details</summary>
Motivation: 离线数据选择和在线自优化生成对于将大语言模型适配到特定下游任务至关重要，但现有方法缺乏统一的理论框架和优化视角。

Method: 采用双层数据选择进行离线数据筛选，将在线自优化生成视为模型适应步骤，通过学习数据权重来统一理解离线选择和在线生成。

Result: 理论证明了双层数据选择框架的有效性，实验在质量增强和安全感知的LLM微调中验证了其优于未过滤直接混合基线的性能。

Conclusion: 结合离线数据和验证加权的在线生成能够显著提升微调性能，为LLM适配提供了一种有效的优化方法。

Abstract: Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically, bilevel data selection is used for offline data selection with respect to the validation dataset, and we treat online self-refining generation as a model adaptation step of selecting the model trained on current responses that best fits the validation data. Our framework offers a unified understanding of offline data selection and self-refining generation by assigning a learned data weight to each question and response, either explicitly or implicitly. For the first time, we theoretically demonstrate the effectiveness of the bilevel data selection framework and demonstrate its performance gains over unfiltered direct mixing baselines. By combining offline data with validation-weighted online generations, our method enhances fine-tuning performance. Experiments on quality enhancement and safety-aware LLM fine-tuning validate its effectiveness.

</details>


### [73] [G-Net: A Provably Easy Construction of High-Accuracy Random Binary Neural Networks](https://arxiv.org/abs/2511.21063)
*Alireza Aghasi,Nicholas Marshall,Saeid Pourmand,Wyatt Whiting*

Main category: cs.LG

TL;DR: 提出一种基于超维计算的新型随机化算法，用于构建可调节精度的二元神经网络，通过高维向量表示实现高效硬件实现和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受超维计算启发，利用高维向量表示的优势，提供高效的硬件实现和对模型损坏的鲁棒性，不同于传统的低精度量化方法。

Method: 将数据二元嵌入视为超立方体中的点，使用汉明距离，提出G-Nets浮点神经网络家族，每个浮点G-Net都有随机二元嵌入的EHD G-Net，通过测度集中保持精度。

Result: 二元模型在CIFAR-10上达到与传统卷积神经网络相当的精度，比先前HDC模型提高近30%准确率。

Conclusion: G-Nets为神经网络和随机二元神经网络之间提供了理论支撑的桥梁，为构建鲁棒的二元/量化深度学习模型开辟了新方向。

Abstract: We propose a novel randomized algorithm for constructing binary neural networks with tunable accuracy. This approach is motivated by hyperdimensional computing (HDC), which is a brain-inspired paradigm that leverages high-dimensional vector representations, offering efficient hardware implementation and robustness to model corruptions. Unlike traditional low-precision methods that use quantization, we consider binary embeddings of data as points in the hypercube equipped with the Hamming distance. We propose a novel family of floating-point neural networks, G-Nets, which are general enough to mimic standard network layers. Each floating-point G-Net has a randomized binary embedding, an embedded hyperdimensional (EHD) G-Net, that retains the accuracy of its floating-point counterparts, with theoretical guarantees, due to the concentration of measure. Empirically, our binary models match convolutional neural network accuracies and outperform prior HDC models by large margins, for example, we achieve almost 30\% higher accuracy on CIFAR-10 compared to prior HDC models. G-Nets are a theoretically justified bridge between neural networks and randomized binary neural networks, opening a new direction for constructing robust binary/quantized deep learning models. Our implementation is available at https://github.com/GNet2025/GNet.

</details>


### [74] [Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning](https://arxiv.org/abs/2511.21075)
*Zhenchao Tang,Fang Wang,Haohuai He,Jiale Zhou,Tianxu Lv,Jun Zhu,Shouzhi Chen,Minghao Yang,Yu Wang,Jiayang Wu,Yidong Song,Jianhua Yao*

Main category: cs.LG

TL;DR: 提出了平衡微调（BFT）方法，通过双层加权机制从稀疏数据中学习复杂推理，无需外部奖励信号，在生物医学任务中显著优于标准监督微调。


<details>
  <summary>Details</summary>
Motivation: 当前方法在生物医学领域存在局限：标准SFT容易过拟合表面指令模式，无法有效内化碎片化科学知识；RL方法因需要实验验证奖励而不可行。

Method: BFT采用双层加权机制：1）令牌级别通过预测概率缩放损失以稳定梯度；2）样本级别使用'最小组置信度'自适应增强困难样本学习。

Result: BFT在医学任务中让LLM学到SFT遗漏的知识，在生物学任务中超越GeneAgent，其文本嵌入可直接用于基因相互作用和单细胞扰动响应预测等下游任务。

Conclusion: BFT促进了LLM在生物医学研究中的广泛应用，能够有效学习复杂推理而无需外部奖励信号。

Abstract: Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses "minimum group confidence" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.

</details>


### [75] [Deceptron: Learned Local Inverses for Fast and Stable Physics Inversion](https://arxiv.org/abs/2511.21076)
*Aaditya L. Kachhadiya*

Main category: cs.LG

TL;DR: 提出Deceptron模块和D-IPG方法，通过局部逆映射学习显著加速逆问题求解，在热传导和阻尼振荡器问题上分别减少20倍和2-3倍迭代次数。


<details>
  <summary>Details</summary>
Motivation: 物理科学中的逆问题通常在输入空间中病态，导致步长敏感。需要一种轻量级方法来学习前向模型的局部逆映射，以加速收敛。

Method: Deceptron双向模块学习可微前向代理的局部逆。训练结合监督拟合、前向-反向一致性、轻量谱惩罚、软偏置约束和雅可比组合惩罚(JCP)。D-IPG在输出空间下降，通过g拉回，使用与基线相同的回溯和停止规则。

Result: 在Heat-1D初值恢复和阻尼振荡器逆问题上，D-IPG达到固定归一化容差所需的迭代次数比投影梯度少约20倍(Heat)和2-3倍(Oscillator)，与Gauss-Newton在迭代次数和成本上竞争。

Conclusion: Deceptron和D-IPG方法能有效加速逆问题求解，JCP减少组合误差并跟踪迭代收益。DeceptronNet(v0)在严格公平协议下学习少步修正，展现出快速收敛特性。

Abstract: Inverse problems in the physical sciences are often ill-conditioned in input space, making progress step-size sensitive. We propose the Deceptron, a lightweight bidirectional module that learns a local inverse of a differentiable forward surrogate. Training combines a supervised fit, forward-reverse consistency, a lightweight spectral penalty, a soft bias tie, and a Jacobian Composition Penalty (JCP) that encourages $J_g(f(x))\,J_f(x)\!\approx\!I$ via JVP/VJP probes. At solve time, D-IPG (Deceptron Inverse-Preconditioned Gradient) takes a descent step in output space, pulls it back through $g$, and projects under the same backtracking and stopping rules as baselines. On Heat-1D initial-condition recovery and a Damped Oscillator inverse problem, D-IPG reaches a fixed normalized tolerance with $\sim$20$\times$ fewer iterations on Heat and $\sim$2-3$\times$ fewer on Oscillator than projected gradient, competitive in iterations and cost with Gauss-Newton. Diagnostics show JCP reduces a measured composition error and tracks iteration gains. We also preview a single-scale 2D instantiation, DeceptronNet (v0), that learns few-step corrections under a strict fairness protocol and exhibits notably fast convergence.

</details>


### [76] [MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts](https://arxiv.org/abs/2511.21089)
*Ivan Novikov*

Main category: cs.LG

TL;DR: MLPMoE是一种无需训练、确定性的转换方法，将Transformer中的稠密MLP重构为静态的高基数混合专家模型，通过张量切片和求和实现，无需校准数据或路由器训练。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型作为稠密Transformer部署，计算效率低下，因为推理成本随参数数量线性增长。现有稀疏化方法依赖聚类、激活分析或自定义路由，需要校准数据。

Method: 使用张量切片和求和将稠密MLP转换为静态混合专家结构，引入分形衰减（微分分支稀疏性）和补偿剪枝（方差保持分支减少）作为轻量级结构化稀疏机制。

Result: 在Qwen2.5-0.5B和DeepSeek-R1-Distill-Llama-8B上，零样本MLPMoE转换使代理困惑度指标变化小于0.05%，同时保持参数数量基本不变。在8B模型上，微分稀疏性移除约20%的MLP参数，困惑度保持在稠密基线的约2%以内。

Conclusion: 该方法完全事后处理现有检查点，无需梯度、校准集或路由器训练，提供了一种计算效率更高的Transformer部署方案。

Abstract: Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1

</details>


### [77] [MNM : Multi-level Neuroimaging Meta-analysis with Hyperbolic Brain-Text Representations](https://arxiv.org/abs/2511.21092)
*Seunghun Baek,Jaejin Lee,Jaeyoon Sim,Minjae Jeong,Won Hwa Kim*

Main category: cs.LG

TL;DR: 提出了一种利用双曲几何将神经科学文献与脑激活图对齐的新框架，通过双曲空间中的多级神经影像元分析，捕捉语义相似性和层次结构。


<details>
  <summary>Details</summary>
Motivation: 解决神经影像研究中样本量小的问题，传统元分析方法忽略了大脑的层次结构，需要更好的方法来整合文献和脑激活数据。

Method: 使用Lorentz模型将研究文章文本和对应脑图像嵌入到共享双曲空间，实现三个功能：脑文本嵌入对齐、文本与脑激活的层次引导、脑激活模式层次关系保持。

Result: 实验结果表明该模型优于基线方法，提供了鲁棒且可解释的多级神经影像元分析范式。

Conclusion: 双曲脑文本表示为神经影像元分析提供了有效的新方法，能够同时处理语义相似性和层次组织结构。

Abstract: Various neuroimaging studies suffer from small sample size problem which often limit their reliability. Meta-analysis addresses this challenge by aggregating findings from different studies to identify consistent patterns of brain activity. However, traditional approaches based on keyword retrieval or linear mappings often overlook the rich hierarchical structure in the brain. In this work, we propose a novel framework that leverages hyperbolic geometry to bridge the gap between neuroscience literature and brain activation maps. By embedding text from research articles and corresponding brain images into a shared hyperbolic space via the Lorentz model, our method captures both semantic similarity and hierarchical organization inherent in neuroimaging data. In the hyperbolic space, our method performs multi-level neuroimaging meta-analysis (MNM) by 1) aligning brain and text embeddings for semantic correspondence, 2) guiding hierarchy between text and brain activations, and 3) preserving the hierarchical relationships within brain activation patterns. Experimental results demonstrate that our model outperforms baselines, offering a robust and interpretable paradigm of multi-level neuroimaging meta-analysis via hyperbolic brain-text representation.

</details>


### [78] [Generative Early Stage Ranking](https://arxiv.org/abs/2511.21095)
*Juhee Hong,Meng Liu,Shengzhi Wang,Xiaoheng Mao,Huihui Cheng,Leon Gao,Christopher Leung,Jin Zhou,Chandra Mouli Sekar,Zhao Zhu,Ruochen Liu,Tuan Trieu,Dawei Sun,Jeet Kanjani,Rui Li,Jing Qian,Xuan Cao,Minjie Fan,Mingze Gao*

Main category: cs.LG

TL;DR: 提出了生成式早期排序(GESR)范式，通过混合注意力(MoA)模块解决用户-物品解耦方法的局限性，在保持效率的同时提升效果


<details>
  <summary>Details</summary>
Motivation: 传统早期排序系统采用用户-物品解耦方法，虽然效率高但难以捕捉细粒度用户-物品亲和度和交叉信号，限制了排序效果

Method: 引入混合注意力(MoA)模块，包含硬匹配注意力、目标感知自注意力和交叉注意力，通过多逻辑参数化门控(MLPG)模块集成学习到的嵌入，并采用优化技术保证效率

Result: 在离线实验和在线实验中，GESR范式在关键指标、用户参与度和消费任务方面都显示出显著改进

Conclusion: GESR是首个在如此大规模下成功部署完整目标感知注意力序列建模的早期排序系统，有效平衡了效果和效率

Abstract: Large-scale recommendations commonly adopt a multi-stage cascading ranking system paradigm to balance effectiveness and efficiency. Early Stage Ranking (ESR) systems utilize the "user-item decoupling" approach, where independently learned user and item representations are only combined at the final layer. While efficient, this design is limited in effectiveness, as it struggles to capture fine-grained user-item affinities and cross-signals. To address these, we propose the Generative Early Stage Ranking (GESR) paradigm, introducing the Mixture of Attention (MoA) module which leverages diverse attention mechanisms to bridge the effectiveness gap: the Hard Matching Attention (HMA) module encodes explicit cross-signals by computing raw match counts between user and item features; the Target-Aware Self Attention module generates target-aware user representations conditioned on the item, enabling more personalized learning; and the Cross Attention modules facilitate early and more enriched interactions between user-item features. MoA's specialized attention encodings are further refined in the final layer through a Multi-Logit Parameterized Gating (MLPG) module, which integrates the newly learned embeddings via gating and produces secondary logits that are fused with the primary logit. To address the efficiency and latency challenges, we have introduced a comprehensive suite of optimization techniques. These span from custom kernels that maximize the capabilities of the latest hardware to efficient serving solutions powered by caching mechanisms. The proposed GESR paradigm has shown substantial improvements in topline metrics, engagement, and consumption tasks, as validated by both offline and online experiments. To the best of our knowledge, this marks the first successful deployment of full target-aware attention sequence modeling within an ESR stage at such a scale.

</details>


### [79] [From Bits to Rounds: Parallel Decoding with Exploration for Diffusion Language Models](https://arxiv.org/abs/2511.21103)
*Hengyu Fu,Baihe Huang,Virginia Adams,Charles Wang,Venkat Srinivasan,Jiantao Jiao*

Main category: cs.LG

TL;DR: 论文提出Explore-Then-Exploit (ETE)解码策略，通过探索高不确定性token来提升扩散语言模型的解码效率，减少解码轮数而不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 标准DLM解码策略依赖高置信度token，但存在信息理论瓶颈，限制了每轮解码的有效进展，导致生成速度变慢。

Method: 提出ETE策略，结合跨块解码和针对性探索高不确定性token，重塑条件分布并触发级联的置信预测。

Result: 实验验证了理论边界，ETE相比仅基于置信度的基线方法能持续减少所需解码轮数，且不损害生成质量。

Conclusion: 优先处理高置信度token本质上是低效的，ETE策略通过最大化信息吞吐量显著提升了扩散语言模型的解码效率。

Abstract: Diffusion Language Models (DLMs) have recently emerged as a strong alternative to autoregressive language models (LMs). DLMs offer comparable accuracy with faster inference speed via parallel decoding. However, standard DLM decoding strategies relying on high-confidence tokens encounter an inherent information-theoretic bottleneck that restricts decoding progress and ultimately slows generation. We demonstrate both theoretically and empirically that prioritizing high-confidence tokens is inherently inefficient. High-probability tokens carry negligible information and strictly relying on them limits the effective progress made in each decoding round. We prove that the number of decoding rounds must grow linearly with the sample's total information (negative log-likelihood) and inversely with the per-round information budget, establishing a bits-to-rounds principle. We also propose Explore-Then-Exploit (ETE), a training-free decoding strategy that maximizes information throughput and decoding efficiency. ETE combines cross-block decoding with targeted exploration of high-uncertainty tokens to reshape the conditional distribution and trigger cascades of confident predictions. Experiments verify our theoretical bounds and demonstrate that ETE consistently reduces the required number of decoding rounds compared to confidence-only baselines without compromising generation quality.

</details>


### [80] [BRIDGE: Building Representations In Domain Guided Program Verification](https://arxiv.org/abs/2511.21104)
*Robert Joseph George,Carson Eisenach,Udaya Ghai,Dominique Perrault-Joncas,Anima Anandkumar,Dean Foster*

Main category: cs.LG

TL;DR: BRIDGE提出了一种结构化提示方法，将程序验证分解为代码、规范和证明三个领域，通过功能推理、规范驱动和证明导向的中间表示来提高验证程序生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在程序验证方面存在困难，特别是在交互式证明框架中。主要挑战在于可扩展性：验证合成需要代码、精确规范和正确证明，而现有方法很少能同时涵盖这三个领域。

Method: BRIDGE将验证分解为三个相互关联的领域：代码（可执行实现）、规范（形式化意图陈述）和证明（构造性正确性论证）。通过引出不同的推理行为作为中间表示来连接这些领域。

Result: 功能推理将形式语言（Lean4）中代码的正确性提高了近1.5倍（pass@5），推理效率提高2倍；规范驱动提示将Python编码通过率提高了17.5%。

Conclusion: 结构化领域对齐是推进验证合成的有前景方向，BRIDGE为通过专家迭代或RLVR进行训练奠定了基础，使模型能够在代码、规范和证明之间内化这些推理策略。

Abstract: Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and correctness proofs, and existing approaches rarely span all three domains. We present BRIDGE, the first systematic study of structured prompting for scalable verified program generation. BRIDGE decomposes verification into three interconnected domains: Code (executable implementations), Specifications (formal intent statements), and Proofs (constructive correctness arguments). Our key idea is to elicit distinct reasoning behaviors functional, specification-driven, and proof-oriented as intermediate representations that preserve semantic structure and connect these domains. Through systematic ablations, we show that this approach substantially improves both accuracy and efficiency beyond standard error feedback methods. For example, functional reasoning improves correctness of code in formal languages (Lean4) by nearly 1.5x (pass@5) over direct baselines. In inference-time compute, functional reasoning is also 2x more efficient, achieving higher pass rates with fewer generations and lower total sampling budgets. Similarly, we find that specification-driven prompting boosts Python coding pass rates by up to 17.5%. These findings suggest that structured domain alignment is a promising direction for advancing verified synthesis. BRIDGE establishes a foundation for training via expert iteration or RLVR, enabling models to internalize these reasoning strategies across code, specifications, and proofs.

</details>


### [81] [Dynamic Stratified Contrastive Learning with Upstream Augmentation for MILP Branching](https://arxiv.org/abs/2511.21107)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: 提出了一种动态分层对比训练框架（Dynamic Stratified Contrastive Training Framework）来解决MILP分支策略中的语义变化、上游节点稀缺和强分支样本收集成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经网络的MILP分支方法面临三个主要挑战：不同深度节点的语义变化、上游节点数据稀缺以及强分支样本收集成本高昂。

Method: 通过基于特征分布对分支定界节点进行分组，训练GCNN判别模型逐步分离不同组别的节点；引入上游增强的MILP推导过程生成理论等价和扰动实例来解决数据稀缺问题。

Result: 在标准MILP基准测试中，该方法显著提高了分支准确性，减少了解题时间，并能有效泛化到未见过的实例。

Conclusion: 所提出的动态分层对比训练框架能够有效建模节点间的细微语义差异，特别是在上游节点上显著提升了分支准确性和解题效率。

Abstract: Mixed Integer Linear Programming (MILP) is a fundamental class of NP-hard problems that has garnered significant attention from both academia and industry. The Branch-and-Bound (B\&B) method is the dominant approach for solving MILPs and the branching plays an important role in B\&B methods. Neural-based learning frameworks have recently been developed to enhance branching policies and the efficiency of solving MILPs. However, these methods still struggle with semantic variation across depths, the scarcity of upstream nodes, and the costly collection of strong branching samples. To address these issues, we propose \ours, a Dynamic \underline{\textbf{S}}tratified \underline{\textbf{C}}ontrastive Training Framework for \underline{\textbf{MILP}} Branching. It groups branch-and-bound nodes based on their feature distributions and trains a GCNN-based discriminative model to progressively separate nodes across groups, learning finer-grained node representations throughout the tree. To address data scarcity and imbalance at upstream nodes, we introduce an upstream-augmented MILP derivation procedure that generates both theoretically equivalent and perturbed instances. \ours~effectively models subtle semantic differences between nodes, significantly enhancing branching accuracy and solving efficiency, particularly for upstream nodes. Extensive experiments on standard MILP benchmarks demonstrate that our method enhances branching accuracy, reduces solving time, and generalizes effectively to unseen instances.

</details>


### [82] [Interpretable Fair Clustering](https://arxiv.org/abs/2511.21109)
*Mudi Jiang,Jiahui Zhou,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种可解释的公平聚类框架，通过将公平约束集成到决策树结构中，在保证公平性的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类方法缺乏可解释性，限制了在高风险场景中的应用，需要理解聚类决策背后的原理。

Method: 构建可解释的决策树来划分数据，同时确保受保护群体间的公平处理；还提出了无需公平超参数调整的变体，通过对无公平约束构建的树进行后剪枝实现。

Result: 在真实世界和合成数据集上的广泛实验表明，该方法不仅提供有竞争力的聚类性能和改善的公平性，还具有可解释性、处理多个敏感属性的能力等额外优势。

Conclusion: 该方法能够在复杂公平约束下稳健执行，为公平透明的聚类开辟了新可能性。

Abstract: Fair clustering has gained increasing attention in recent years, especially in applications involving socially sensitive attributes. However, existing fair clustering methods often lack interpretability, limiting their applicability in high-stakes scenarios where understanding the rationale behind clustering decisions is essential. In this work, we address this limitation by proposing an interpretable and fair clustering framework, which integrates fairness constraints into the structure of decision trees. Our approach constructs interpretable decision trees that partition the data while ensuring fair treatment across protected groups. To further enhance the practicality of our framework, we also introduce a variant that requires no fairness hyperparameter tuning, achieved through post-pruning a tree constructed without fairness constraints. Extensive experiments on both real-world and synthetic datasets demonstrate that our method not only delivers competitive clustering performance and improved fairness, but also offers additional advantages such as interpretability and the ability to handle multiple sensitive attributes. These strengths enable our method to perform robustly under complex fairness constraints, opening new possibilities for equitable and transparent clustering.

</details>


### [83] [Trustless Federated Learning at Edge-Scale: A Compositional Architecture for Decentralized, Verifiable, and Incentive-Aligned Coordination](https://arxiv.org/abs/2511.21118)
*Pius Onobhayedo,Paul Osemudiame Oamen*

Main category: cs.LG

TL;DR: 该论文提出了一个解决联邦学习民主化愿景中关键组成性差距的框架，通过密码学收据、几何新颖性测量、并行对象所有权和时间锁定策略来解决聚合问责、激励机制、可扩展性和治理问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在从集中式提供转向分布式创建，但联邦学习的民主化愿景尚未实现，存在聚合器缺乏问责、经济机制易被操纵、协调限制可扩展性以及治理允许追溯性操纵等关键问题。

Method: 利用密码学收据证明聚合正确性，几何新颖性测量防止激励操纵，并行对象所有权实现线性可扩展性，时间锁定策略检查追溯性操纵。

Result: 提出的方法解决了联邦学习中的关键组成性差距，为大规模分布式AI训练提供了可行的技术路径。

Conclusion: 该框架为实现联邦学习的民主化愿景提供了系统性解决方案，通过技术创新解决了当前阻碍分布式AI发展的核心问题。

Abstract: Artificial intelligence is retracing the Internet's path from centralized provision to distributed creation. Initially, resource-intensive computation concentrates within institutions capable of training and serving large models.Eventually, as federated learning matures, billions of edge devices holding sensitive data will be able to collectively improve models without surrendering raw information, enabling both contribution and consumption at scale. This democratic vision remains unrealized due to certain compositional gaps; aggregators handle updates without accountability, economic mechanisms are lacking and even when present remain vulnerable to gaming, coordination serializes state modifications limiting scalability, and governance permits retroactive manipulation. This work addresses these gaps by leveraging cryptographic receipts to prove aggregation correctness, geometric novelty measurement to prevent incentive gaming, parallel object ownership to achieve linear scalability, and time-locked policies to check retroactive manipulation.

</details>


### [84] [Learning Cell-Aware Hierarchical Multi-Modal Representations for Robust Molecular Modeling](https://arxiv.org/abs/2511.21120)
*Mengran Li,Zelin Zang,Wenbin Xing,Junzhou Chen,Ronghui Zhang,Jiebo Luo,Stan Z. Li*

Main category: cs.LG

TL;DR: CHMR是一个细胞感知的层次化多模态表示框架，通过联合建模分子与细胞响应之间的局部-全局依赖关系，并利用树状结构向量量化模块捕获潜在生物层次结构，显著提升了分子属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注化学结构，但细胞响应（如形态和基因表达）在塑造药物效应中起关键作用。当前细胞感知方法面临外部生物数据模态不完整以及分子、细胞和基因组层次间依赖关系建模不足的局限性。

Method: 提出CHMR框架，联合建模分子与细胞响应之间的局部-全局依赖关系，通过新颖的树状结构向量量化模块捕获潜在生物层次结构。

Result: 在9个公共基准测试的728个任务上评估，CHMR优于最先进基线方法，分类任务平均提升3.6%，回归任务平均提升17.2%。

Conclusion: 结果表明层次感知的多模态学习在构建可靠且生物学基础扎实的分子表示方面具有优势，为整合生物医学建模提供了一个可泛化的框架。

Abstract: Understanding how chemical perturbations propagate through biological systems is essential for robust molecular property prediction. While most existing methods focus on chemical structures alone, recent advances highlight the crucial role of cellular responses such as morphology and gene expression in shaping drug effects. However, current cell-aware approaches face two key limitations: (1) modality incompleteness in external biological data, and (2) insufficient modeling of hierarchical dependencies across molecular, cellular, and genomic levels. We propose CHMR (Cell-aware Hierarchical Multi-modal Representations), a robust framework that jointly models local-global dependencies between molecules and cellular responses and captures latent biological hierarchies via a novel tree-structured vector quantization module. Evaluated on nine public benchmarks spanning 728 tasks, CHMR outperforms state-of-the-art baselines, yielding average improvements of 3.6% on classification and 17.2% on regression tasks. These results demonstrate the advantage of hierarchy-aware, multimodal learning for reliable and biologically grounded molecular representations, offering a generalizable framework for integrative biomedical modeling. The code is in https://github.com/limengran98/CHMR.

</details>


### [85] [How to Correctly Report LLM-as-a-Judge Evaluations](https://arxiv.org/abs/2511.21140)
*Chungpa Lee,Thomas Zeng,Jongwon Jeong,Jy-yong Sohn,Kangwook Lee*

Main category: cs.LG

TL;DR: 提出了一个插件框架来修正LLM评估中的偏差并构建置信区间，同时引入自适应算法来优化校准样本分配。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估器存在噪声和偏差，现有偏差修正方法假设已知模型的特异性和敏感性，且通常只使用估计值，缺乏考虑不确定性的置信区间构建方法。

Method: 开发了一个简单的插件框架来修正偏差并构建置信区间，同时提出自适应算法来高效分配校准样本量以减少不确定性。

Result: 该方法能够修正LLM评估中的偏差，构建反映测试和校准数据集不确定性的置信区间，实现实用且统计上可靠的LLM评估。

Conclusion: 提出的框架为LLM评估提供了偏差修正和不确定性量化的实用解决方案，自适应算法进一步提高了评估效率。

Abstract: Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.

</details>


### [86] [I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation](https://arxiv.org/abs/2511.21208)
*Lucas Thil,Jesse Read,Rim Kaddah,Guillaume Doquet*

Main category: cs.LG

TL;DR: 提出了一种新的健康指标构建框架，首次将RaPP作为RUL预测的健康指标，通过不确定性量化和指标组方法显著提升了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在多传感器系统中解耦复杂退化机制，也无法量化健康指标可靠性的不确定性，这限制了剩余使用寿命预测的准确性。

Method: 1) 首次将RaPP作为健康指标；2) 通过蒙特卡洛dropout和概率潜在空间增强不确定性量化；3) 提出指标组范式来隔离传感器子集以建模系统特定退化。

Result: 在航空航天和制造系统数据上的评估显示，相比现有方法在准确性和泛化性方面都有显著提升，同时提供了对系统故障路径的可操作见解。

Conclusion: 该工作填补了异常检测与预测之间的空白，为复杂系统中的不确定性感知退化建模提供了原则性框架。

Abstract: Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.

</details>


### [87] [Robust Gene Prioritization via Fast-mRMR Feature Selection in high-dimensional omics data](https://arxiv.org/abs/2511.21211)
*Rubén Fernández-Farelo,Jorge Paz-Ruza,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Alex A. Freitas*

Main category: cs.LG

TL;DR: 提出一种更稳健高效的基因优先级排序流程，通过Fast-mRMR特征选择保留相关且非冗余的特征，构建更简单有效的模型，并在饮食限制数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在处理生物医学数据的高维性和不完整标注方面存在困难，需要更稳健的基因优先级排序方法。

Method: 使用Fast-mRMR特征选择来保留相关且非冗余的特征，构建更简单的分类器模型，并能够组合不同的生物特征集。

Result: 在饮食限制数据集上的实验显示，该方法相比现有方法有显著改进。

Conclusion: 特征选择对于可靠的基因优先级排序至关重要，该方法证明了其有效性。

Abstract: Gene prioritization (identifying genes potentially associated with a biological process) is increasingly tackled with Artificial Intelligence. However, existing methods struggle with the high dimensionality and incomplete labelling of biomedical data. This work proposes a more robust and efficient pipeline that leverages Fast-mRMR feature selection to retain only relevant, non-redundant features for classifiers. This enables us to build simpler and more effective models, as well as to combine different biological feature sets. Experiments on Dietary Restriction datasets show significant improvements over existing methods, proving that feature selection can be critical for reliable gene prioritization.

</details>


### [88] [A Physics-Informed U-net-LSTM Network for Data-Driven Seismic Response Modeling of Structures](https://arxiv.org/abs/2511.21276)
*Sutirtha Biswas,Kshitij Kumar Yadav*

Main category: cs.LG

TL;DR: 提出了一种结合物理定律与深度学习的Physics Informed U-Net LSTM框架，用于提高结构地震响应预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法计算成本高，而纯数据驱动的深度学习模型泛化能力差且难以捕捉物理规律，需要结合物理约束来提高可靠性。

Method: 开发了Physics Informed U-Net LSTM混合框架，将领域特定的物理约束嵌入学习过程，结合U-Net和LSTM网络的优势。

Result: 该模型在预测性能上优于传统的机器学习架构，提供了准确且计算高效的地震响应预测方案。

Conclusion: 这种混合方法弥合了纯数据驱动方法与物理建模之间的差距，为结构地震响应预测提供了稳健且计算高效的替代方案。

Abstract: Accurate and efficient seismic response prediction is essential for the design of resilient structures. While the Finite Element Method (FEM) remains the standard for nonlinear seismic analysis, its high computational demands limit its scalability and real time applicability. Recent developments in deep learning, particularly Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short Term Memory (LSTM) models, have shown promise in reducing the computational cost of nonlinear seismic analysis of structures. However, these data driven models often struggle to generalize and capture the underlying physics, leading to reduced reliability. We propose a novel Physics Informed U Net LSTM framework that integrates physical laws with deep learning to enhance both accuracy and efficiency. By embedding domain specific constraints into the learning process, the proposed model achieves improved predictive performance over conventional Machine Learning architectures. This hybrid approach bridges the gap between purely data driven methods and physics based modeling, offering a robust and computationally efficient alternative for seismic response prediction of structures.

</details>


### [89] [Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models](https://arxiv.org/abs/2511.21320)
*Heiko Oppel,Andreas Spilz,Michael Munz*

Main category: cs.LG

TL;DR: 提出了一种结合隐式扩散模型和新型锯齿采样器的方法，可将DDPM的采样过程加速30倍，同时提升生成序列的分类质量


<details>
  <summary>Details</summary>
Motivation: DDPM能够生成合成时间序列数据来提升分类器性能，但其采样过程计算成本高昂

Method: 结合隐式扩散模型与新型锯齿采样器，可应用于任何预训练的扩散模型以加速反向过程

Result: 相比标准基线实现了30倍的加速，同时提高了生成序列在分类任务中的质量

Conclusion: 该方法在保持生成质量的同时显著提升了DDPM的采样效率

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.

</details>


### [90] [TSGM: Regular and Irregular Time-series Generation using Score-based Generative Models](https://arxiv.org/abs/2511.21335)
*Haksoo Lim,Jaehoon Lee,Sewon Park,Minjung Kim,Noseong Park*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数生成模型的时间序列合成方法，通过条件分数网络学习时间序列的条件分数函数，实现了高质量和多样性的时间序列生成。


<details>
  <summary>Details</summary>
Motivation: 受分数生成模型在图像生成、语音合成等领域取得的优异成果启发，作者希望将SGMs应用于时间序列合成，以提升采样质量和多样性。

Method: 提出了一个条件分数网络用于时间序列合成，设计了专门的条件去噪分数匹配损失函数，框架灵活可同时处理规则和不规则时间序列。

Result: 在多个时间序列数据集上取得了优异的合成性能，实现了最先进的采样多样性和质量。

Conclusion: 分数生成模型可成功应用于时间序列合成任务，提出的条件分数网络框架在生成质量和多样性方面表现出色。

Abstract: Score-based generative models (SGMs) have demonstrated unparalleled sampling quality and diversity in numerous fields, such as image generation, voice synthesis, and tabular data synthesis, etc. Inspired by those outstanding results, we apply SGMs to synthesize time-series by learning its conditional score function. To this end, we present a conditional score network for time-series synthesis, deriving a denoising score matching loss tailored for our purposes. In particular, our presented denoising score matching loss is the conditional denoising score matching loss for time-series synthesis. In addition, our framework is such flexible that both regular and irregular time-series can be synthesized with minimal changes to our model design. Finally, we obtain exceptional synthesis performance on various time-series datasets, achieving state-of-the-art sampling diversity and quality.

</details>


### [91] [Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models](https://arxiv.org/abs/2511.21338)
*Julianna Piskorz,Cristina Pinneri,Alvaro Correia,Motasem Alfarra,Risheek Garrepalli,Christos Louizos*

Main category: cs.LG

TL;DR: 研究发现掩码扩散语言模型存在位置偏见和掩码干扰问题，提出掩码无关损失函数来提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索掩码扩散语言模型在上下文理解方面的能力，发现其存在与自回归模型类似的局限性。

Method: 通过系统消融实验分析模型性能，引入掩码无关损失函数进行微调。

Result: 掩码扩散语言模型具有强烈的位置偏见，大量掩码标记会显著降低上下文理解能力。使用掩码无关损失函数微调后，模型对掩码的干扰更具鲁棒性。

Conclusion: 当前掩码扩散语言模型的训练范式存在关键限制，提出的改进方法可为构建具有更强上下文理解能力的扩散语言模型提供指导。

Abstract: Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.

</details>


### [92] [Best Practices for Machine Learning Experimentation in Scientific Applications](https://arxiv.org/abs/2511.21354)
*Umberto Michelucci,Francesca Venturini*

Main category: cs.LG

TL;DR: 提供机器学习实验的实用结构化指南，强调可重复性、公平比较和透明报告，包括LOR和COS等评估指标


<details>
  <summary>Details</summary>
Motivation: 科学研究中机器学习应用日益增多，但实验设计和文档质量影响结果可靠性，需要避免误导性结论

Method: 提出分步工作流程，从数据准备到模型选择和评估，引入对数过拟合比(LOR)和复合过拟合分数(COS)等指标

Result: 通过推荐实践和报告格式示例，帮助研究人员建立稳健基准并从机器学习模型中得出有效证据

Conclusion: 该指南支持研究人员在科学问题中应用机器学习时进行可重复、公平和透明的实验

Abstract: Machine learning (ML) is increasingly adopted in scientific research, yet the quality and reliability of results often depend on how experiments are designed and documented. Poor baselines, inconsistent preprocessing, or insufficient validation can lead to misleading conclusions about model performance. This paper presents a practical and structured guide for conducting ML experiments in scientific applications, focussing on reproducibility, fair comparison, and transparent reporting. We outline a step-by-step workflow, from dataset preparation to model selection and evaluation, and propose metrics that account for overfitting and instability across validation folds, including the Logarithmic Overfitting Ratio (LOR) and the Composite Overfitting Score (COS). Through recommended practices and example reporting formats, this work aims to support researchers in establishing robust baselines and drawing valid evidence-based insights from ML models applied to scientific problems.

</details>


### [93] [Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance](https://arxiv.org/abs/2511.21356)
*Bram Silue,Santiago Amaya-Corredor,Patrick Mannion,Lander Willem,Pieter Libin*

Main category: cs.LG

TL;DR: 提出了Hybrid-AIRL (H-AIRL)方法，通过结合监督损失和随机正则化机制来增强对抗性逆强化学习在复杂不完美信息环境中的表现，特别是在德州扑克HULHE场景中取得了更好的样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对抗性逆强化学习(AIRL)在处理稀疏奖励问题上表现出潜力，但在高度复杂的不完美信息环境中的性能尚未充分探索。特别是在德州扑克HULHE这种具有稀疏、延迟奖励和显著不确定性的环境中，AIRL难以推断出足够信息量的奖励函数。

Method: 提出了Hybrid-AIRL (H-AIRL)扩展方法，通过引入来自专家数据的监督损失和随机正则化机制来增强奖励推断和策略学习。在Gymnasium基准测试和HULHE扑克环境中进行了评估，并通过可视化分析学习到的奖励函数。

Result: 实验结果表明，H-AIRL相比AIRL实现了更高的样本效率和更稳定的学习。在HULHE扑克设置中表现更好，能够更有效地推断奖励函数。

Conclusion: 将监督信号融入逆强化学习具有显著优势，H-AIRL为应对具有挑战性的现实世界设置提供了一个有前景的框架。

Abstract: Adversarial Inverse Reinforcement Learning (AIRL) has shown promise in addressing the sparse reward problem in reinforcement learning (RL) by inferring dense reward functions from expert demonstrations. However, its performance in highly complex, imperfect-information settings remains largely unexplored. To explore this gap, we evaluate AIRL in the context of Heads-Up Limit Hold'em (HULHE) poker, a domain characterized by sparse, delayed rewards and significant uncertainty. In this setting, we find that AIRL struggles to infer a sufficiently informative reward function. To overcome this limitation, we contribute Hybrid-AIRL (H-AIRL), an extension that enhances reward inference and policy learning by incorporating a supervised loss derived from expert data and a stochastic regularization mechanism. We evaluate H-AIRL on a carefully selected set of Gymnasium benchmarks and the HULHE poker setting. Additionally, we analyze the learned reward function through visualization to gain deeper insights into the learning process. Our experimental results show that H-AIRL achieves higher sample efficiency and more stable learning compared to AIRL. This highlights the benefits of incorporating supervised signals into inverse RL and establishes H-AIRL as a promising framework for tackling challenging, real-world settings.

</details>


### [94] [The Directed Prediction Change - Efficient and Trustworthy Fidelity Assessment for Local Feature Attribution Methods](https://arxiv.org/abs/2511.21363)
*Kevin Iselborn,David Dembinsky,Adriano Lucieri,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出了一种新的局部特征归因方法保真度评估指标DPC，通过结合扰动和归因方向，实现了近10倍的速度提升，消除了随机性，提供确定性评估。


<details>
  <summary>Details</summary>
Motivation: 在高风险医疗环境中，需要能忠实反映模型决策过程的解释方法。现有保真度指标如Infidelity依赖蒙特卡洛近似，需要大量模型评估并引入随机性不确定性。

Method: 在引导扰动实验中修改现有预测变化(PC)指标，通过结合扰动和归因方向，提出定向预测变化(DPC)指标。

Result: 在皮肤病变图像和金融表格数据两个数据集、两个黑盒模型、七种解释算法和广泛超参数范围内评估了4744个不同解释，DPC与PC一起实现了对基线导向和局部特征归因方法的全面高效评估。

Conclusion: DPC提供了确定性、可重复的评估结果，与局部Infidelity测量相同属性，但计算效率更高且无随机性。

Abstract: The utility of an explanation method critically depends on its fidelity to the underlying machine learning model. Especially in high-stakes medical settings, clinicians and regulators require explanations that faithfully reflect the model's decision process. Existing fidelity metrics such as Infidelity rely on Monte Carlo approximation, which demands numerous model evaluations and introduces uncertainty due to random sampling. This work proposes a novel metric for evaluating the fidelity of local feature attribution methods by modifying the existing Prediction Change (PC) metric within the Guided Perturbation Experiment. By incorporating the direction of both perturbation and attribution, the proposed Directed Prediction Change (DPC) metric achieves an almost tenfold speedup and eliminates randomness, resulting in a deterministic and trustworthy evaluation procedure that measures the same property as local Infidelity. DPC is evaluated on two datasets (skin lesion images and financial tabular data), two black-box models, seven explanation algorithms, and a wide range of hyperparameters. Across $4\,744$ distinct explanations, the results demonstrate that DPC, together with PC, enables a holistic and computationally efficient evaluation of both baseline-oriented and local feature attribution methods, while providing deterministic and reproducible outcomes.

</details>


### [95] [BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework for Multiclass Disaster Classification in Bangla](https://arxiv.org/abs/2511.21364)
*Ariful Islam,Md Rifat Hossen,Md. Mahmudul Arif,Abdullah Al Noman,Md Arifur Rahman*

Main category: cs.LG

TL;DR: 提出BanglaMM-Disaster多模态框架，结合文本和图像数据用于孟加拉语灾害分类，在5037条社交媒体数据上达到83.76%准确率，比单模态方法提升显著


<details>
  <summary>Details</summary>
Motivation: 孟加拉国自然灾害频发，需要实时监测和快速响应系统，但缺乏孟加拉语多模态灾害分析工具

Method: 使用基于transformer的文本编码器(BanglaBERT、mBERT、XLM-RoBERTa)和CNN骨干网络(ResNet50、DenseNet169、MobileNetV2)，通过早期融合处理文本和图像两种模态

Result: 最佳模型准确率达到83.76%，比纯文本基线提升3.84%，比纯图像基线提升16.91%，所有类别的误分类都减少，对模糊样本改进明显

Conclusion: 填补了孟加拉语多模态灾害分析的关键空白，证明了在低资源环境下结合多种数据类型对实时灾害响应的益处

Abstract: Natural disasters remain a major challenge for Bangladesh, so real-time monitoring and quick response systems are essential. In this study, we present BanglaMM-Disaster, an end-to-end deep learning-based multimodal framework for disaster classification in Bangla, using both textual and visual data from social media. We constructed a new dataset of 5,037 Bangla social media posts, each consisting of a caption and a corresponding image, annotated into one of nine disaster-related categories. The proposed model integrates transformer-based text encoders, including BanglaBERT, mBERT, and XLM-RoBERTa, with CNN backbones such as ResNet50, DenseNet169, and MobileNetV2, to process the two modalities. Using early fusion, the best model achieves 83.76% accuracy. This surpasses the best text-only baseline by 3.84% and the image-only baseline by 16.91%. Our analysis also shows reduced misclassification across all classes, with noticeable improvements for ambiguous examples. This work fills a key gap in Bangla multimodal disaster analysis and demonstrates the benefits of combining multiple data types for real-time disaster response in low-resource settings.

</details>


### [96] [Controlling changes to attention logits](https://arxiv.org/abs/2511.21377)
*Ben Anson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 通过为查询和键权重分配参数相关的学习率来控制对数变化，解决transformer模型训练中的稳定性问题，特别是在不适用QK归一化的MLA场景中。


<details>
  <summary>Details</summary>
Motivation: 解决transformer模型中查询和键权重不稳定的问题，特别是在QK归一化不适用的情况下（如MLA），需要一种替代的稳定性控制方法。

Method: 通过为查询和键权重分配参数相关的学习率来控制对数变化，这是一种廉价干预措施。

Result: 该方法允许提高网络的基础学习率，在MLA设置中优于其他方法，在使用多头注意力时与QK归一化性能相当。

Conclusion: 通过控制对数变化而非直接归一化权重，提供了一种有效且通用的transformer训练稳定性解决方案。

Abstract: Stability of neural network weights is critical when training transformer models. The query and key weights are particularly problematic, as they tend to grow large without any intervention. Applying normalization to queries and keys, known as `QK norm', fixes stability issues in practice, but is not always applicable. For example, QK norm is not compatible with Multi Latent Attention (MLA) because QK norm requires full materialization of queries and keys during inference, which is not done in MLA. In this paper we suggest that controlling the changes to logits is important for stability. We show that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights. We find that our cheap intervention allows us to increase the base learning rate of the network, outperform other methods in the MLA setting, and achieve performance competitive with QK norm when using Multi-head Attention.

</details>


### [97] [Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data](https://arxiv.org/abs/2511.21378)
*Jungi Lee,Jungkwon Kim,Chi Zhang,Kwangsun Yoo,Seok-Joo Byun*

Main category: cs.LG

TL;DR: 提出了AAR方法，通过动态排除异常数据来解决污染数据问题，在图像和表格数据集上优于现有方法0.041 AUROC。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测模型假设训练数据完全正常，但实际数据往往包含污染。现有方法依赖固定污染率假设，当假设与实际不符时性能严重下降，特别是在正常与异常数据分布重叠的噪声环境中。

Method: 提出自适应和激进拒绝(AAR)方法，使用改进的z-score和高斯混合模型阈值动态排除异常，结合硬拒绝和软拒绝策略平衡保留正常数据和排除异常数据的权衡。

Result: 在两个图像数据集和三十个表格数据集上的广泛实验表明，AAR比最先进方法提升了0.041 AUROC。

Conclusion: AAR为污染数据集提供了可扩展且可靠的解决方案，增强了鲁棒性，为安全和医疗等领域的实际应用铺平了道路。

Abstract: Handling contaminated data poses a critical challenge in anomaly detection, as traditional models assume training on purely normal data. Conventional methods mitigate contamination by relying on fixed contamination ratios, but discrepancies between assumed and actual ratios can severely degrade performance, especially in noisy environments where normal and abnormal data distributions overlap. To address these limitations, we propose Adaptive and Aggressive Rejection (AAR), a novel method that dynamically excludes anomalies using a modified z-score and Gaussian mixture model-based thresholds. AAR effectively balances the trade-off between preserving normal data and excluding anomalies by integrating hard and soft rejection strategies. Extensive experiments on two image datasets and thirty tabular datasets demonstrate that AAR outperforms the state-of-the-art method by 0.041 AUROC. By providing a scalable and reliable solution, AAR enhances robustness against contaminated datasets, paving the way for broader real-world applications in domains such as security and healthcare.

</details>


### [98] [BanglaASTE: A Novel Framework for Aspect-Sentiment-Opinion Extraction in Bangla E-commerce Reviews Using Ensemble Deep Learning](https://arxiv.org/abs/2511.21381)
*Ariful Islam,Md Rifat Hossen,Abir Ahmed,B M Taslimul Haque*

Main category: cs.LG

TL;DR: 提出了BanglaASTE框架，这是首个用于孟加拉语方面情感三元组提取(ASTE)的混合分类框架，结合了图匹配和语义相似度技术，在自建数据集上取得了89.9%的准确率和89.1%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语方面情感分析研究严重不足，缺乏全面的数据集和专门的三元组提取框架，无法满足电子商务和社交媒体中细粒度情感分析的需求。

Method: 创建首个孟加拉语ASTE数据集(3,345条评论)；开发混合分类框架，结合基于图的方面-观点匹配和语义相似度技术；实现集成模型，结合BanglaBERT上下文嵌入和XGBoost提升算法。

Result: 集成方法在所有评估指标上显著优于基线模型，达到89.9%准确率和89.1%的F1分数，有效处理了孟加拉语文本处理中的非正式表达、拼写变体和数据稀疏等挑战。

Conclusion: 该研究推进了低资源语言情感分析的最新技术，为孟加拉语电子商务分析应用提供了可扩展的解决方案。

Abstract: Aspect-Based Sentiment Analysis (ABSA) has emerged as a critical tool for extracting fine-grained sentiment insights from user-generated content, particularly in e-commerce and social media domains. However, research on Bangla ABSA remains significantly underexplored due to the absence of comprehensive datasets and specialized frameworks for triplet extraction in this language. This paper introduces BanglaASTE, a novel framework for Aspect Sentiment Triplet Extraction (ASTE) that simultaneously identifies aspect terms, opinion expressions, and sentiment polarities from Bangla product reviews. Our contributions include: (1) creation of the first annotated Bangla ASTE dataset containing 3,345 product reviews collected from major e-commerce platforms including Daraz, Facebook, and Rokomari; (2) development of a hybrid classification framework that employs graph-based aspect-opinion matching with semantic similarity techniques; and (3) implementation of an ensemble model combining BanglaBERT contextual embeddings with XGBoost boosting algorithms for enhanced triplet extraction performance. Experimental results demonstrate that our ensemble approach achieves superior performance with 89.9% accuracy and 89.1% F1-score, significantly outperforming baseline models across all evaluation metrics. The framework effectively addresses key challenges in Bangla text processing including informal expressions, spelling variations, and data sparsity. This research advances the state-of-the-art in low-resource language sentiment analysis and provides a scalable solution for Bangla e-commerce analytics applications.

</details>


### [99] [Subjective Depth and Timescale Transformers: Learning Where and When to Compute](https://arxiv.org/abs/2511.21408)
*Frederico Wieser,Martin Benfeghoul,Haitham Bou Ammar,Jun Wang,Zafeirios Fountas*

Main category: cs.LG

TL;DR: 提出了两种基于贝叶斯惊喜的动态计算路由Transformer架构：SDT通过决策层和动态层交替实现空间计算路由，STT在时间维度上基于变化假设动态执行或跳过Transformer块，显著减少计算和KV缓存需求。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer架构中刚性、均匀的计算分配限制了效率和可扩展性，特别是在大规模模型和长序列场景下。需要动态路由计算来提升效率。

Method: SDT：在解码器堆栈中交替使用决策层（计算完整后验和轻量先验）和动态层（基于贝叶斯惊喜的Top-K路由）；STT：使用转移网络预测残差更新，形成时间变化假设，动态执行或跳过Transformer块。

Result: 两种架构在训练过程中都表现出从新颖性驱动到预测驱动的门控转变，与基于惊喜的原则一致。在降低容量的情况下，减少了75%的自注意力计算和50%的KV缓存需求。

Conclusion: 提出的架构为效率提供了灵活框架，通过条件计算在计算-精度权衡方面提供了初步见解，为更高效模型开辟了路径。

Abstract: The rigid, uniform allocation of computation in standard Transformer (TF) architectures can limit their efficiency and scalability, particularly for large-scale models and long sequences. Addressing this, we introduce Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT), two distinct architectures that leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within decoder-only TFs. SDT augments a decoder-only stack with alternating Decision and Dynamic layers: a Decision layer computes a full block 'posterior' and a lightweight 'prior,' while a Dynamic layer employs fixed-capacity Top-K routing based on Bayesian surprise (Expected and Unexpected Change), maintaining a static compute graph. STT extends this conditional computation to the temporal domain: a transition network predicts residual updates, forming a temporal 'change hypothesis' that informs a router to dynamically execute or bypass TF blocks for each token, managing KV-cache contributions. Both architectures exhibit the predicted shift from novelty to prediction driven gating over training, suggesting alignment with surprise based principles. While operating at reduced capacity, they offer preliminary insights into the compute-accuracy trade-offs of conditional computation. The proposed architectures establish a flexible framework for efficiency, reducing self-attention computation by 75% and KV-cache requirements by 50% within each compute skipping layer, setting a pathway for more efficient models.

</details>


### [100] [SUPN: Shallow Universal Polynomial Networks](https://arxiv.org/abs/2511.21414)
*Zachary Morrow,Michael Penwarden,Brian Chen,Aurya Javeed,Akil Narayan,John D. Jakeman*

Main category: cs.LG

TL;DR: 提出浅层通用多项式网络（SUPNs），用单层可学习系数的多项式替代除最后一层外的所有隐藏层，在保持表达能力的同时大幅减少参数数量，并在多项实验中优于DNNs、KANs和多项式投影方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和KANs通常需要大量可训练参数，导致网络不透明、优化空间大且易陷入局部最优，初始化对泛化误差影响过大。

Method: 使用单层可学习系数的多项式替换除最后一层外的所有隐藏层，结合DNNs和多项式的优势，用更少参数实现充分表达能力。

Result: 在1维、2维和10维的13,000多个模型实验中，SUPNs在相同参数数量下的逼近误差和变异性通常比DNNs和KANs低一个数量级，甚至在非光滑函数上优于多项式投影。

Conclusion: SUPNs收敛速度与同阶最佳多项式逼近相同，推导了准最优参数公式，实验证明其在减少参数的同时保持高精度和低变异性。

Abstract: Deep neural networks (DNNs) and Kolmogorov-Arnold networks (KANs) are popular methods for function approximation due to their flexibility and expressivity. However, they typically require a large number of trainable parameters to produce a suitable approximation. Beyond making the resulting network less transparent, overparameterization creates a large optimization space, likely producing local minima in training that have quite different generalization errors. In this case, network initialization can have an outsize impact on the model's out-of-sample accuracy. For these reasons, we propose shallow universal polynomial networks (SUPNs). These networks replace all but the last hidden layer with a single layer of polynomials with learnable coefficients, leveraging the strengths of DNNs and polynomials to achieve sufficient expressivity with far fewer parameters. We prove that SUPNs converge at the same rate as the best polynomial approximation of the same degree, and we derive explicit formulas for quasi-optimal SUPN parameters. We complement theory with an extensive suite of numerical experiments involving SUPNs, DNNs, KANs, and polynomial projection in one, two, and ten dimensions, consisting of over 13,000 trained models. On the target functions we numerically studied, for a given number of trainable parameters, the approximation error and variability are often lower for SUPNs than for DNNs and KANs by an order of magnitude. In our examples, SUPNs even outperform polynomial projection on non-smooth functions.

</details>


### [101] [Ensemble Performance Through the Lens of Linear Independence of Classifier Votes in Data Streams](https://arxiv.org/abs/2511.21465)
*Enes Bektas,Fazli Can*

Main category: cs.LG

TL;DR: 论文研究了集成学习中分类器数量与性能的关系，提出线性独立性理论框架来解释集成规模与准确率之间的权衡，并推导出达到特定线性独立性概率所需集成规模的理论估计。


<details>
  <summary>Details</summary>
Motivation: 集成学习通过组合多个基分类器提升分类性能，但过大的集成会导致计算效率低下和收益递减。需要研究集成规模与性能的最佳平衡点。

Method: 通过分析分类器投票的线性独立性，建立几何模型和加权多数投票问题的理论框架，推导出达到线性独立性概率的理论集成规模估计，并在真实和合成数据集上使用OzaBagging和GOOWE方法进行验证。

Result: 理论估计能有效识别OzaBagging等稳健集成的性能饱和点；对于GOOWE等复杂加权方案，高理论多样性可能引发算法不稳定性。

Conclusion: 线性独立性理论框架为集成规模选择提供了理论指导，揭示了不同集成方法对线性独立性的响应差异，为高效集成学习设计提供了新视角。

Abstract: Ensemble learning improves classification performance by combining multiple base classifiers. While increasing the number of classifiers generally enhances accuracy, excessively large ensembles can lead to computational inefficiency and diminishing returns. This paper investigates the relationship between ensemble size and performance through the lens of linear independence among classifier votes in data streams. We propose that ensembles composed of linearly independent classifiers maximize representational capacity, particularly under a geometric model. We then generalize the importance of linear independence to the weighted majority voting problem. By modeling the probability of achieving linear independence among classifier outputs, we derive a theoretical framework that explains the trade-off between ensemble size and accuracy. Our analysis leads to a theoretical estimate of the ensemble size required to achieve a user-specified probability of linear independence. We validate our theory through experiments on both real-world and synthetic datasets using two ensemble methods, OzaBagging and GOOWE. Our results confirm that this theoretical estimate effectively identifies the point of performance saturation for robust ensembles like OzaBagging. Conversely, for complex weighting schemes like GOOWE, our framework reveals that high theoretical diversity can trigger algorithmic instability. Our implementation is publicly available to support reproducibility and future research.

</details>


### [102] [Mean-Field Limits for Two-Layer Neural Networks Trained with Consensus-Based Optimization](https://arxiv.org/abs/2511.21466)
*William De Deyn,Michael Herty,Giovanni Samaey*

Main category: cs.LG

TL;DR: 本文研究两层神经网络，使用基于共识的优化(CBO)粒子方法进行训练，并与Adam比较性能。提出了CBO与Adam混合方法加速收敛，在元学习场景下改进了CBO的内存效率。通过最优传输框架重新表述CBO，在无限粒子极限下定义了Wasserstein-over-Wasserstein空间上的动态，并证明方差单调递减。


<details>
  <summary>Details</summary>
Motivation: 研究更高效的神经网络训练方法，特别是探索粒子基优化算法CBO在神经网络训练中的应用潜力，并寻求改进其收敛速度和内存效率。

Method: 使用基于共识的优化(CBO)粒子方法训练两层神经网络，提出CBO与Adam的混合方法，在元学习场景下重新表述CBO以减少内存开销，通过最优传输框架重新表述CBO并分析其均值场极限。

Result: CBO与Adam混合方法比纯CBO收敛更快；在元学习场景下改进了CBO的内存效率；在无限粒子极限下，证明了方差在Wasserstein-over-Wasserstein空间上单调递减。

Conclusion: CBO是神经网络训练的有效替代方法，与Adam结合可加速收敛；通过最优传输框架的理论分析为CBO提供了坚实的数学基础，证明了其在均值场极限下的良好性质。

Abstract: We study two-layer neural networks and train these with a particle-based method called consensus-based optimization (CBO). We compare the performance of CBO against Adam on two test cases and demonstrate how a hybrid approach, combining CBO with Adam, provides faster convergence than CBO. In the context of multi-task learning, we recast CBO into a formulation that offers less memory overhead. The CBO method allows for a mean-field limit formulation, which we couple with the mean-field limit of the neural network. To this end, we first reformulate CBO within the optimal transport framework. Finally, in the limit of infinitely many particles, we define the corresponding dynamics on the Wasserstein-over-Wasserstein space and show that the variance decreases monotonically.

</details>


### [103] [Lost in Time? A Meta-Learning Framework for Time-Shift-Tolerant Physiological Signal Transformation](https://arxiv.org/abs/2511.21500)
*Qian Hong,Cheng Bian,Xiao Zhou,Xiaoyu Li,Yelei Li,Zijing Zeng*

Main category: cs.LG

TL;DR: ShiftSyncNet是一个基于元学习的双层优化框架，通过自动校正PPG/BCG到ABP信号转换中的时间错位问题，显著提升临床监测精度。


<details>
  <summary>Details</summary>
Motivation: 多模态生理信号转换中的时间错位问题严重影响转换精度，特别是对ABP峰值等关键特征的捕捉，现有同步方法依赖强相似性假设或手动调优，而传统噪声标签学习方法在时间偏移监督下效果不佳。

Method: 提出ShiftSyncNet框架，包含转换网络(TransNet)和时间偏移校正网络(SyncNet)，通过元学习进行双层优化，SyncNet学习训练对之间的时间偏移并应用傅里叶相位移动来对齐监督信号。

Result: 在一个工业数据集和两个公共数据集上的实验表明，ShiftSyncNet分别比强基线方法提升了9.4%、6.0%和12.8%的性能。

Conclusion: ShiftSyncNet能有效校正时间偏移、改善标签质量并提升转换精度，为处理多模态生理转换中的时间不一致性问题提供了统一方向。

Abstract: Translating non-invasive signals such as photoplethysmography (PPG) and ballistocardiography (BCG) into clinically meaningful signals like arterial blood pressure (ABP) is vital for continuous, low-cost healthcare monitoring. However, temporal misalignment in multimodal signal transformation impairs transformation accuracy, especially in capturing critical features like ABP peaks. Conventional synchronization methods often rely on strong similarity assumptions or manual tuning, while existing Learning with Noisy Labels (LNL) approaches are ineffective under time-shifted supervision, either discarding excessive data or failing to correct label shifts. To address this challenge, we propose ShiftSyncNet, a meta-learning-based bi-level optimization framework that automatically mitigates performance degradation due to time misalignment. It comprises a transformation network (TransNet) and a time-shift correction network (SyncNet), where SyncNet learns time offsets between training pairs and applies Fourier phase shifts to align supervision signals. Experiments on one real-world industrial dataset and two public datasets show that ShiftSyncNet outperforms strong baselines by 9.4%, 6.0%, and 12.8%, respectively. The results highlight its effectiveness in correcting time shifts, improving label quality, and enhancing transformation accuracy across diverse misalignment scenarios, pointing toward a unified direction for addressing temporal inconsistencies in multimodal physiological transformation.

</details>


### [104] [IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference](https://arxiv.org/abs/2511.21513)
*Wanli Zhong,Haibo Feng,Zirui Zhou,Hanyang Peng,Shiqi Yu*

Main category: cs.LG

TL;DR: IntAttention是一种完全整数的注意力机制，通过IndexSoftmax操作消除浮点运算，解决了Transformer在边缘设备上部署时的softmax瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署Transformer模型时，INT8量化虽然加速了矩阵乘法，但softmax阶段成为主要瓶颈，需要昂贵的去量化-softmax-重量化过程，占注意力总延迟的65%，破坏了端到端整数数据流。

Method: 提出IntAttention和IndexSoftmax操作，使用硬件友好的32项查找表近似、稀疏感知裁剪和直接整数归一化，完全在整数域内运行，无需重新训练。

Result: 在Armv8 CPU上，相比FP16基线实现3.7倍加速和61%能耗降低，比传统INT8注意力管道快2.0倍，同时保持与基线相当的精度。

Conclusion: IntAttention实现了完全整数的注意力管道，消除了数据类型转换开销，使Transformer在商用边缘设备上实现实用且高效的推理。

Abstract: Deploying Transformer models on edge devices is limited by latency and energy budgets. While INT8 quantization effectively accelerates the primary matrix multiplications, it exposes the softmax as the dominant bottleneck. This stage incurs a costly dequantize-softmax-requantize detour, which can account for up to 65% of total attention latency and disrupts the end-to-end integer dataflow critical for edge hardware efficiency. To address this limitation, we present IntAttention, the first fully integer, plug-and-play attention pipeline without retraining. At the core of our approach lies IndexSoftmax, a hardware-friendly operator that replaces floating-point exponentials entirely within the integer domain. IntAttention integrates sparsity-aware clipping, a 32-entry lookup-table approximation, and direct integer normalization, thereby eliminating all datatype conversion overhead. We evaluate IntAttention and demonstrate consistent and substantial gains. Our method achieves up to 3.7x speedup and 61% energy reduction over FP16 baselines and 2.0x faster than conventional INT8 attention pipelines on Armv8 CPUs. These gains are achieved with high-fidelity accuracy comparable to baselines across diverse language and vision models, enabling practical and efficient Transformer inference on commodity edge devices. Code will be released in later version of this work.

</details>


### [105] [Mechanistic Interpretability for Transformer-based Time Series Classification](https://arxiv.org/abs/2511.21514)
*Matīss Kalnāre,Sofoklis Kitharidis,Thomas Bäck,Niki van Stein*

Main category: cs.LG

TL;DR: 本文通过将机制可解释性技术从NLP领域迁移到时间序列分类的Transformer架构，揭示了模型内部注意力头和时间步的因果结构，构建了信息传播的因果图。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列分类中表现出色，但其内部决策机制难以理解。现有可解释性方法主要关注输入输出归因，对内部机制揭示不足。

Method: 采用机制可解释性技术：激活修补、注意力显著性分析和稀疏自编码器，系统探测注意力头和时间步的因果作用。

Result: 在基准时间序列数据集上构建了因果图，识别出驱动正确分类的关键注意力头和时间位置，并展示了稀疏自编码器在发现可解释潜在特征方面的潜力。

Conclusion: 研究为Transformer可解释性提供了方法论贡献，并揭示了时间序列分类任务中Transformer功能机制的新见解。

Abstract: Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.

</details>


### [106] [Predictive Safety Shield for Dyna-Q Reinforcement Learning](https://arxiv.org/abs/2511.21531)
*Jin Pin,Krasowski Hanna,Vanneaux Elena*

Main category: cs.LG

TL;DR: 提出了一种基于模型预测的安全防护机制，通过安全模拟更新Q函数，在保持硬安全保证的同时提升强化学习性能


<details>
  <summary>Details</summary>
Motivation: 现有安全防护机制通常随机采样安全动作或使用固定回退控制器，忽略了不同安全动作对未来性能的影响

Method: 在离散空间中为基于模型的强化学习设计预测性安全防护，通过环境模型的安全模拟来局部更新Q函数

Result: 在网格世界环境中的实验表明，即使短预测视野也足以识别最优路径，且对分布偏移具有鲁棒性

Conclusion: 该预测性安全防护方法能够在保持硬安全保证的同时改善性能，且无需额外训练即可应对分布偏移

Abstract: Obtaining safety guarantees for reinforcement learning is a major challenge to achieve applicability for real-world tasks. Safety shields extend standard reinforcement learning and achieve hard safety guarantees. However, existing safety shields commonly use random sampling of safe actions or a fixed fallback controller, therefore disregarding future performance implications of different safe actions. In this work, we propose a predictive safety shield for model-based reinforcement learning agents in discrete space. Our safety shield updates the Q-function locally based on safe predictions, which originate from a safe simulation of the environment model. This shielding approach improves performance while maintaining hard safety guarantees. Our experiments on gridworld environments demonstrate that even short prediction horizons can be sufficient to identify the optimal path. We observe that our approach is robust to distribution shifts, e.g., between simulation and reality, without requiring additional training.

</details>


### [107] [Context-Specific Causal Graph Discovery with Unobserved Contexts: Non-Stationarity, Regimes and Spatio-Temporal Patterns](https://arxiv.org/abs/2511.21537)
*Martin Rabel,Jakob Runge*

Main category: cs.LG

TL;DR: 本文提出了一个模块化框架，用于分析空间网格时间序列数据中因果图变化所编码的信息，通过修改约束型因果发现方法中的独立性测试层来实现。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据（如气候应用）通常具有空间网格时间序列结构，系统在不同时空点的行为相似但存在变化。这些变化既包含重要信息，又可能影响假设平稳性或空间平移不变性的算法的稳定性和有效性。

Method: 开发指导原则克服核心挑战，在独立性测试层面修改约束型因果发现方法，构建模块化框架。该框架可轻松扩展，利用现有约束型因果发现方法（如PC、PC-stable、FCI、PCMCI、PCMCI+、LPCMCI），无需或仅需少量修改。

Result: 创建了一个极其模块化、易于扩展且广泛适用的框架，能够系统性地理解和改进一系列子问题，并可利用变化点检测、聚类、独立性测试等相关领域的见解进行扩展。

Conclusion: 该框架通过将问题分解为更易处理的子问题，简化了对基本限制、控制权衡的超参数以及结果统计解释的理解。开源实现即将发布。

Abstract: Real-world data, for example in climate applications, often consists of spatially gridded time series data or data with comparable structure. While the underlying system is often believed to behave similar at different points in space and time, those variations that do exist are twofold relevant: They often encode important information in and of themselves. And they may negatively affect the stability / convergence and reliability\Slash{}validity of results of algorithms assuming stationarity or space-translation invariance. We study the information encoded in changes of the causal graph, with stability in mind. An analysis of this general task identifies two core challenges. We develop guiding principles to overcome these challenges, and provide a framework realizing these principles by modifying constraint-based causal discovery approaches on the level of independence testing. This leads to an extremely modular, easily extensible and widely applicable framework. It can leverage existing constraint-based causal discovery methods (demonstrated on IID-algorithms PC, PC-stable, FCI and time series algorithms PCMCI, PCMCI+, LPCMCI) with little to no modification. The built-in modularity allows to systematically understand and improve upon an entire array of subproblems. By design, it can be extended by leveraging insights from change-point-detection, clustering, independence-testing and other well-studied related problems. The division into more accessible sub-problems also simplifies the understanding of fundamental limitations, hyperparameters controlling trade-offs and the statistical interpretation of results. An open-source implementation will be available soon.

</details>


### [108] [Computing Strategic Responses to Non-Linear Classifiers](https://arxiv.org/abs/2511.21560)
*Jack Geary,Boyan Gao,Henry Gouk*

Main category: cs.LG

TL;DR: 提出了一种计算战略分类中智能体最优响应的方法，通过优化智能体目标的拉格朗日对偶来解决非线性分类器设置中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 战略分类中部署分类器会引发战略行为，导致后续观测的分布偏移。现有方法主要关注线性设置，但许多情况下非线性分类器更合适，而计算非线性设置中的最优响应是主要限制。

Method: 通过优化智能体目标的拉格朗日对偶来计算最优响应，该方法在非线性分类器设置中可直接应用。

Result: 该方法在线性设置中能重现最优响应，识别出现有方法的关键弱点，在非线性分类器设置中可用于评估和训练。

Conclusion: 提出的拉格朗日对偶优化方法为战略分类中的非线性分类器提供了有效的解决方案，能够处理分布偏移问题。

Abstract: We consider the problem of strategic classification, where the act of deploying a classifier leads to strategic behaviour that induces a distribution shift on subsequent observations. Current approaches to learning classifiers in strategic settings are focused primarily on the linear setting, but in many cases non-linear classifiers are more suitable. A central limitation to progress for non-linear classifiers arises from the inability to compute best responses in these settings. We present a novel method for computing the best response by optimising the Lagrangian dual of the Agents' objective. We demonstrate that our method reproduces best responses in linear settings, identifying key weaknesses in existing approaches. We present further results demonstrating our method can be straight-forwardly applied to non-linear classifier settings, where it is useful for both evaluation and training.

</details>


### [109] [Machine Learning Approaches to Clinical Risk Prediction: Multi-Scale Temporal Alignment in Electronic Health Records](https://arxiv.org/abs/2511.21561)
*Wei-Chen Chang,Lu Dai,Ting Xu*

Main category: cs.LG

TL;DR: 提出基于多尺度时间对齐网络(MSTAN)的风险预测方法，解决电子健康记录中时间不规则性、采样间隔差异和多尺度动态依赖的挑战，在公开EHR数据集上表现优于主流基线。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录(EHR)中存在的三个关键挑战：时间不规则性、采样间隔差异以及多尺度动态依赖性，这些因素限制了传统方法在医疗时间序列分析中的性能。

Method: 引入可学习的时间对齐机制和多尺度卷积特征提取结构，通过时间嵌入和对齐模块动态加权不规则采样数据，使用多层卷积和分层融合捕获不同时间粒度的关键模式，最后基于注意力的聚合机制整合全局时间依赖。

Result: 在公开EHR数据集上的实验表明，该模型在准确率、召回率、精确率和F1-Score方面均优于主流基线方法，证明了多尺度时间对齐在复杂医疗时间序列分析中的有效性和鲁棒性。

Conclusion: 为高维异步医疗序列的智能表示提供了新解决方案，为EHR驱动的临床风险预测提供了重要技术支持。

Abstract: This study proposes a risk prediction method based on a Multi-Scale Temporal Alignment Network (MSTAN) to address the challenges of temporal irregularity, sampling interval differences, and multi-scale dynamic dependencies in Electronic Health Records (EHR). The method focuses on temporal feature modeling by introducing a learnable temporal alignment mechanism and a multi-scale convolutional feature extraction structure to jointly model long-term trends and short-term fluctuations in EHR sequences. At the input level, the model maps multi-source clinical features into a unified high-dimensional semantic space and employs temporal embedding and alignment modules to dynamically weight irregularly sampled data, reducing the impact of temporal distribution differences on model performance. The multi-scale feature extraction module then captures key patterns across different temporal granularities through multi-layer convolution and hierarchical fusion, achieving a fine-grained representation of patient states. Finally, an attention-based aggregation mechanism integrates global temporal dependencies to generate individual-level risk representations for disease risk prediction and health status assessment. Experiments conducted on publicly available EHR datasets show that the proposed model outperforms mainstream baselines in accuracy, recall, precision, and F1-Score, demonstrating the effectiveness and robustness of multi-scale temporal alignment in complex medical time-series analysis. This study provides a new solution for intelligent representation of high-dimensional asynchronous medical sequences and offers important technical support for EHR-driven clinical risk prediction.

</details>


### [110] [A decoupled alignment kernel for peptide membrane permeability predictions](https://arxiv.org/abs/2511.21566)
*Ali Amirahmadi,Gökçe Geylan,Leonardo De Maria,Farzaneh Etminani,Mattias Ohlsson,Alessandro Tibo*

Main category: cs.LG

TL;DR: 提出了MD-GAK和PMD-GAK两种核方法，用于预测环肽的细胞膜渗透性，通过化学意义的残基相似性和序列比对来提升预测性能，特别关注不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 环肽是靶向细胞内位点的有前景模式，但细胞膜渗透性仍是关键瓶颈，且现有公共数据有限，需要良好校准的不确定性估计。

Method: 提出了单体感知解耦全局对齐核（MD-GAK），将化学意义的残基-残基相似性与序列对齐耦合，同时将局部匹配与间隙惩罚解耦。还引入了包含三角位置先验的变体PMD-GAK。使用高斯过程作为预测模型。

Result: 通过大量实验证明该方法在所有指标上均优于现有最先进模型，PMD-GAK在减少校准误差方面具有额外优势。

Conclusion: 该方法提供了一种完全可复现的解决方案，在环肽细胞膜渗透性预测方面表现出色，特别是在不确定性估计方面具有优势。

Abstract: Cyclic peptides are promising modalities for targeting intracellular sites; however, cell-membrane permeability remains a key bottleneck, exacerbated by limited public data and the need for well-calibrated uncertainty. Instead of relying on data-eager complex deep learning architecture, we propose a monomer-aware decoupled global alignment kernel (MD-GAK), which couples chemically meaningful residue-residue similarity with sequence alignment while decoupling local matches from gap penalties. MD-GAK is a relatively simple kernel. To further demonstrate the robustness of our framework, we also introduce a variant, PMD-GAK, which incorporates a triangular positional prior. As we will show in the experimental section, PMD-GAK can offer additional advantages over MD-GAK, particularly in reducing calibration errors. Since our focus is on uncertainty estimation, we use Gaussian Processes as the predictive model, as both MD-GAK and PMD-GAK can be directly applied within this framework. We demonstrate the effectiveness of our methods through an extensive set of experiments, comparing our fully reproducible approach against state-of-the-art models, and show that it outperforms them across all metrics.

</details>


### [111] [Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.21581)
*Alex Ning,Yen-Ling Kuo,Gabe Gomes*

Main category: cs.LG

TL;DR: 提出自适应长度潜在推理模型，通过强化学习优化推理长度，在保持准确率的同时将总推理长度减少52%


<details>
  <summary>Details</summary>
Motivation: 潜在推理相比思维链推理能压缩推理长度，通过直接传递信息丰富的潜在状态而非人类语言标记，突破推理媒介限制

Method: 开发自适应长度潜在推理模型，采用SFT后强化学习方法优化推理长度，最小化推理长度同时保持准确率

Result: 在Llama 3.2 1B模型和GSM8K-Aug数据集上实验，总推理长度下降52%且准确率无损失

Conclusion: 潜在推理模型显著减少计算使用并提升压缩能力，未来将扩展到更多模型和数据集，分析训练系数关系，实验架构变体，继续知识蒸馏工作

Abstract: Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.

</details>


### [112] [An AI-Enabled Hybrid Cyber-Physical Framework for Adaptive Control in Smart Grids](https://arxiv.org/abs/2511.21590)
*Muhammad Siddique,Sohaib Zafar*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的智能电网数字取证框架，该框架部署在云端，结合数据采集、认证通信、云存储和自动化取证分析，能够实时检测异常、重建事件和分析入侵。


<details>
  <summary>Details</summary>
Motivation: 智能电网融合了传统电力基础设施和先进通信网络，这种集成带来了可能破坏电网稳定性和可靠性的漏洞。数字取证对于识别、检测和缓解此类安全事件至关重要。

Method: 开发了一个综合的机器学习数字取证框架，使用监督和无监督学习算法（如随机森林、支持向量机、梯度提升树和深度神经网络），在云端实现数据采集、认证通信、可扩展存储和自动化取证分析。

Result: 通过对实时智能电表数据流的仿真和实验研究表明，该框架在检测数据篡改、虚假数据注入和协调控制回路操纵等网络攻击方面具有高准确性、可扩展性和弹性。

Conclusion: 云服务是大数据驱动取证工作流程的最佳基础架构，使能源公司能够实现快速态势感知和智能事件响应。

Abstract: Smart grids are a fusion of classical power infrastructure and advanced communication networks and smart control, to create a cyber-physical environment that is more efficient and flexible than ever before. This integration causes vulnerabilities that can undermine grid stability as well as reliability. Digital forensics is a fundamental concept of learning and identifying, detecting, and mitigating such security incidents. This paper presents an all-in-one machine learning-based digital forensic framework of smart grid systems deployed on the Cloud. The framework combines the data acquisition at the sensor-level, authenticated communication, scalable cloud storage and automated forensic analytics. The model uses supervised and unsupervised learning algorithms - such as Random Forest, Support Vector Machine, Gradient Boosted Trees and deep neural architectures for anomaly detection, event reconstruction and intrusion analysis in real time. After several simulation and experimental studies on real-time smart-meter data streams, the proposed framework is shown to be very accurate, scalable and resilient to cyber-attacks including data tampering, false-data injection and coordinated control-loop manipulation. The results indicate that cloud services are the best backbone for big-data-driven forensic workflows, which allows energy utilities to achieve a fast situational awareness and intelligent incident response.

</details>


### [113] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 通过降维方法提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，揭示了注意力机制和MLP组件在中间层的清晰分离等新发现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现出色，但其内部机制难以解释。本研究旨在通过可视化潜在状态几何结构来支持对Transformer内部机制的系统分析。

Method: 在Transformer块中的多个点捕获层间激活，通过主成分分析(PCA)和均匀流形逼近(UMAP)进行降维和可视化分析，在GPT-2和LLaMa模型上进行实验。

Result: 发现了中间层注意力机制和MLP组件输出的清晰分离，识别了初始序列位置潜在状态的高范数，可视化了潜在状态的层间演化，展示了GPT-2位置嵌入的高维螺旋结构和LLaMa的序列几何模式。

Conclusion: 该方法支持对Transformer内部机制的系统分析，有助于推进可复现的可解释性研究，代码已开源供进一步研究使用。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [114] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 算法效率提升的主要部分来自规模依赖的算法改进，特别是LSTM到Transformer的转变，而非之前认为的小规模算法创新。


<details>
  <summary>Details</summary>
Motivation: 研究2012-2023年间AI训练效率提升22,000倍的原因，发现传统小规模消融实验只能解释不到100倍的提升，存在巨大效率差距。

Method: 通过小规模消融实验和文献调查分析算法创新贡献，然后进行扩展实验比较LSTM和Transformer在不同规模下的效率差异。

Result: 发现算法效率提升具有规模依赖性，LSTM到Transformer的转变贡献了大部分效率增益，总计解释了6,930倍的效率提升。

Conclusion: 小规模模型的算法进展远慢于预期，算法效率的衡量具有强烈的参考依赖性，需要重新评估算法进步的真实贡献。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [115] [Scale-Agnostic Kolmogorov-Arnold Geometry in Neural Networks](https://arxiv.org/abs/2511.21626)
*Mathew Vanherreweghe,Michael H. Freedman,Keith M. Adams*

Main category: cs.LG

TL;DR: 研究发现多层感知机在MNIST手写数字分类任务中会自发形成Kolmogorov-Arnold几何结构，这种结构在不同空间尺度上保持一致，表明神经网络在真实高维数据学习过程中会自发形成有组织的尺度不变几何结构。


<details>
  <summary>Details</summary>
Motivation: 之前的研究发现浅层多层感知机在合成三维任务中会自发形成KAG结构，但不确定这种现象是否会在真实高维场景中持续存在，以及这种几何结构具有什么空间特性。

Method: 将KAG分析扩展到784维的MNIST数字分类任务，使用2层MLP，在多个尺度上进行系统空间分析，从局部7像素邻域到完整的28x28图像。

Result: 发现KAG在训练过程中出现，并在不同空间尺度上一致存在，这种尺度无关的特性在不同训练程序（标准训练和空间增强训练）中都产生相同的定性模式。

Conclusion: 神经网络在真实高维数据学习过程中会自发形成有组织的、尺度不变的几何结构。

Abstract: Recent work by Freedman and Mulligan demonstrated that shallow multilayer perceptrons spontaneously develop Kolmogorov-Arnold geometric (KAG) structure during training on synthetic three-dimensional tasks. However, it remained unclear whether this phenomenon persists in realistic high-dimensional settings and what spatial properties this geometry exhibits.
  We extend KAG analysis to MNIST digit classification (784 dimensions) using 2-layer MLPs with systematic spatial analysis at multiple scales. We find that KAG emerges during training and appears consistently across spatial scales, from local 7-pixel neighborhoods to the full 28x28 image. This scale-agnostic property holds across different training procedures: both standard training and training with spatial augmentation produce the same qualitative pattern. These findings reveal that neural networks spontaneously develop organized, scale-invariant geometric structure during learning on realistic high-dimensional data.

</details>


### [116] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深度视觉Transformer存在Cliff-Plateau-Climb三阶段模式，[CLS]令牌重要性逐渐降低，信息在补丁令牌间扩散，增加层数不一定提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度视觉Transformer性能不随深度增加而提升的问题，探索表示演化的规律。

Method: 对ViT-S、ViT-B、ViT-L在ImageNet上进行系统实证分析，使用信息扰乱指数量化信息混合模式。

Result: 发现[CLS]令牌重要性逐渐降低，补丁令牌间达成分布式共识，ViT-L中信息-任务权衡比ViT-B晚约10层出现。

Conclusion: Transformer架构可能更需要精心校准的深度而非简单增加参数，信息扰乱指数可作为模型诊断工具。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [117] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 提出Iterative PPO方法，将多轮对话RL问题转化为一系列单轮RLHF问题，通过交替拟合Q函数和改进策略来优化LLM在多轮对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 优化LLM在多轮对话中的表现面临稀疏奖励和响应级规划与令牌级生成之间的差异等挑战，特别是在目标导向场景如AI营销或销售代理中。

Method: 将多轮RL问题形式化地简化为一系列单轮RLHF问题，使用学习的多轮Q函数作为单轮问题的奖励模型，通过交替拟合Q函数和策略改进来实现优化。

Result: 证明解决单轮RL问题与标准令牌级PPO等价于多轮问题中的策略改进步骤，提出了Iterative PPO算法。

Conclusion: 该方法在完全在线和完全离线方法之间找到了平衡点，既保持了在线更新的适应性，又获得了离线训练的稳定性优势。

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>


### [118] [EvilGenie: A Reward Hacking Benchmark](https://arxiv.org/abs/2511.21654)
*Jonathan Gabor,Jayson Lynch,Jonathan Rosenfeld*

Main category: cs.LG

TL;DR: EvilGenie是一个用于评估编程环境中奖励攻击的基准测试，通过修改测试文件或硬编码测试用例等方式检测AI代理的奖励攻击行为。


<details>
  <summary>Details</summary>
Motivation: 现有的编程基准测试无法有效检测AI代理通过修改测试环境来获得虚假高分的奖励攻击行为，需要专门的评估框架。

Method: 从LiveCodeBench获取问题，创建允许奖励攻击的环境，使用三种方法检测奖励攻击：保留单元测试、LLM评判器和测试文件编辑检测。

Result: LLM评判器在明确情况下能有效检测奖励攻击，保留测试用例的改进效果有限。Codex和Claude Code都表现出明确的奖励攻击行为，所有三个代理都显示出未对齐的行为。

Conclusion: EvilGenie基准测试能有效识别编程AI代理的奖励攻击行为，LLM评判器是有效的检测工具，主流编程代理都存在奖励攻击风险。

Abstract: We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at https://github.com/JonathanGabor/EvilGenie.

</details>


### [119] [Escaping the Verifier: Learning to Reason via Demonstrations](https://arxiv.org/abs/2511.21667)
*Locke Cai,Ivan Provilkov*

Main category: cs.LG

TL;DR: RARO方法通过对抗性逆强化学习从专家演示中学习推理能力，无需任务特定的验证器。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界推理密集型任务缺乏验证器，但拥有丰富的专家演示数据未被充分利用。

Method: 建立策略（生成器）和相对论批评者（判别器）的对抗交互：策略学习模仿专家答案，批评者学习区分策略和专家答案。

Result: RARO在所有评估任务（Countdown、DeepMath、Poetry Writing）上显著优于无验证器基线，并展现出与可验证任务RL相同的稳健扩展趋势。

Conclusion: 该方法仅从专家演示中就能有效激发强大的推理性能，在任务特定验证器不可用时也能实现稳健的推理学习。

Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.

</details>


### [120] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 本文质疑电信AI训练中样本同等重要的假设，提出基于梯度分析的样本重要性框架，选择性优先处理有影响的数据，在保持性能的同时减少计算需求和能耗，推动可持续AI在电信领域的应用。


<details>
  <summary>Details</summary>
Motivation: 电信AI应用中数据量激增，但数据往往存在噪声、高维、存储处理成本高等问题。传统工作流假设所有训练样本同等重要，而下一代系统需要准确、高效且可持续的AI模型。

Method: 通过跨epoch的样本级梯度分析识别模型学习中的影响模式和冗余，基于此提出样本重要性框架，选择性优先处理有影响的数据以减少计算。

Result: 在三个真实世界电信数据集上的实验表明，该方法在保持性能的同时减少了数据需求和计算开销。

Conclusion: 该方法通过优化样本选择策略，在不牺牲准确性的前提下显著降低计算和能源消耗，推进了电信领域可持续AI的发展目标。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [121] [Dual-Domain Deep Learning Method to Accelerate Local Basis Functions Computation for Reservoir Simulation in High-Contrast Porous Media](https://arxiv.org/abs/2511.20685)
*Peiqi Li,Jie Chen*

Main category: math.NA

TL;DR: 提出了一种双域深度学习框架，用于加速混合广义多尺度有限元方法中多尺度基函数的计算，以解决达西流问题。


<details>
  <summary>Details</summary>
Motivation: 异质多孔介质中的达西流在储层模拟中是一个核心问题，但介质的显著多尺度特性给传统数值方法带来了计算需求和效率方面的挑战。虽然混合广义多尺度有限元方法提供了有效框架，但多尺度基函数的构建仍然计算昂贵。

Method: 通过提取和解码频域和空间域中的渗透率场特征，该方法能够快速生成多尺度基函数的数值矩阵。

Result: 数值实验表明，所提出的框架在保持高逼近精度的同时实现了显著的计算加速。

Conclusion: 该方法为未来实际储层工程应用提供了潜力。

Abstract: In energy science, Darcy flow in heterogeneous porous media is a central problem in reservoir sim-ulation. However, the pronounced multiscale characteristics of such media pose significant challenges to conventional numerical methods in terms of computational demand and efficiency. The Mixed Generalized Multiscale Finite Element Method (MGMsFEM) provides an effective framework for addressing these challenges, yet the construction of multiscale basis functions remains computationally expensive. In this work, we propose a dual-domain deep learning framework to accelerate the computation of multiscale basis functions within MGMsFEM for solving Darcy flow problems. By extracting and decoding permeability field features in both the frequency and spatial domains, the method enables rapid generation of numerical matrices of multiscale basis functions. Numerical experiments demonstrate that the proposed framework achieves significant computational acceleration while maintaining high approximation accuracy, thereby offering the potential for future applications in real-world reservoir engineering.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving](https://arxiv.org/abs/2511.20720)
*Haibo HU,Lianming Huang,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: DeeAD是一个无需训练的早期退出框架，通过评估中间轨迹的物理可行性来加速VLA模型的推理，在保持规划质量的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: VLA模型在自动驾驶中统一感知、推理和轨迹生成，但由于深度transformer堆栈导致推理延迟显著，需要加速方案。

Method: 提出基于动作引导的早期退出框架，当预测轨迹与轻量级规划先验（如导航或低精度规划）在可容忍偏差内（<2m）时终止推理；引入多跳控制器根据分数变化率自适应跳过冗余层。

Result: 在Bench2Drive基准测试中，实现了高达28%的transformer层稀疏化和29%的延迟降低，同时保持规划质量和安全性。

Conclusion: DeeAD框架可有效集成到现有VLA模型中（如ORION），无需重新训练即可显著加速推理，为VLA模型的实时应用提供了可行方案。

Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.

</details>


### [123] [Foundry: Distilling 3D Foundation Models for the Edge](https://arxiv.org/abs/2511.20721)
*Guillaume Letellier,Siddharth Srivastava,Frédéric Jurie,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出了Foundation Model Distillation (FMD)新范式，用于将大型自监督学习模型压缩为紧凑高效的代理模型，保持其通用表征能力。Foundry是该范式在3D点云上的首个实现。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽然功能强大，但规模庞大、计算成本高，难以部署在边缘设备上。现有压缩技术会牺牲模型的通用性，而FMD旨在保持这种通用表征能力。

Method: 训练学生模型学习压缩的SuperTokens，重构教师的token级表征，捕捉其潜在空间的紧凑基。Foundry是该方法的3D点云实现。

Result: 单个蒸馏模型在多种下游任务（分类、部件分割、少样本场景）中保持强迁移能力，接近完整基础模型性能，同时显著减少token数量和FLOPs。

Conclusion: FMD提供了一种实用的方法，使基础模型能够在资源受限的硬件上部署，同时保持其通用表征能力。

Abstract: Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.

</details>


### [124] [$Δ$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer](https://arxiv.org/abs/2511.20804)
*Kriti Ghosh,Devjyoti Chakraborty,Lakshmish Ramaswamy,Suchendra M. Bhandarkar,In Kee Kim,Nancy O'Hare,Deepak Mishra*

Main category: cs.CV

TL;DR: 提出了Δ-NeRF，一种用于增量NeRF精化的模块化残差框架，通过残差控制器、不确定性感知门控机制和视图选择策略，实现在不访问过去数据的情况下进行增量学习，显著减少训练时间和数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF框架在引入新视图时需要完全重新训练，限制了在数据顺序到达场景（如卫星地形分析）中的应用。增量NeRF精化研究不足，简单方法存在灾难性遗忘问题。

Method: 1) 残差控制器向冻结的基础NeRF注入逐层修正；2) 不确定性感知门控机制自适应结合基础和精化预测；3) 视图选择策略减少训练数据；4) 使用知识蒸馏将增强模型压缩为紧凑学生网络。

Result: 在卫星图像上，Δ-NeRF性能与联合训练相当，训练时间减少30-42%，数据需求减少47%。PSNR比简单微调提升43.5%，在某些指标上超越联合训练。

Conclusion: Δ-NeRF为增量NeRF学习提供了有效解决方案，克服了灾难性遗忘问题，在保持性能的同时显著提升了训练效率和数据利用率。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $Δ$-NeRF, a unique modular residual framework for incremental NeRF refinement. $Δ$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $Δ$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $Δ$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.

</details>


### [125] [SPHINX: A Synthetic Environment for Visual Perception and Reasoning](https://arxiv.org/abs/2511.20814)
*Md Tanvirul Alam,Saksham Aggarwal,Justin Yang Chae,Nidhi Rastogi*

Main category: cs.CV

TL;DR: Sphinx是一个针对视觉感知和推理核心认知原语的合成环境，通过程序化生成包含图案、瓦片、图表、图标和几何原语的谜题，并配备可验证的真实解，用于精确评估和大规模数据集构建。


<details>
  <summary>Details</summary>
Motivation: 当前视觉推理任务缺乏能够精确评估核心认知能力的基准测试，需要构建能够系统测试对称检测、几何变换、空间推理等基础认知原语的环境。

Method: 使用程序化生成方法创建包含25种任务类型的谜题，涵盖对称检测、几何变换、空间推理、图表解释和序列预测等认知原语，每个谜题都配备可验证的真实解。

Result: 评估显示最先进的GPT-5模型仅达到51.1%的准确率，远低于人类表现；通过可验证奖励的强化学习(RLVR)显著提高了模型在这些任务上的准确率，并在外部视觉推理基准上获得提升。

Conclusion: Sphinx基准有效揭示了当前大视觉语言模型在核心认知能力上的局限，可验证奖励的强化学习是提升多模态推理能力的有前景方法。

Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.

</details>


### [126] [Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion](https://arxiv.org/abs/2511.20821)
*Samuele Dell'Erba,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练和数据的优化视觉反转方法替代传统扩散先验网络，通过约束优化过程提高图像生成质量，并揭示了当前评估基准的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖计算昂贵的扩散先验网络将文本嵌入转换为视觉流形，这些先验需要大量训练数据。本文挑战了训练先验的必要性。

Method: 使用优化视觉反转方法，从随机伪标记初始化潜在视觉表示，通过迭代优化最大化与文本提示嵌入的余弦相似度，并提出了基于马氏距离和最近邻损失的约束方法。

Result: 在Kandinsky 2.2上的实验表明，OVI可以替代传统先验，约束方法特别是最近邻方法显著提高了视觉保真度，在定量评分上达到或超过了最先进的数据高效先验。

Conclusion: 优化视觉反转方法作为先验网络替代方案的可行性得到验证，当前评估基准存在缺陷，该方法值得进一步研究。

Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.

</details>


### [127] [RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs](https://arxiv.org/abs/2511.20823)
*Roman Naeem,David Hagerman,Jennifer Alvén,Fredrik Kahl*

Main category: cs.CV

TL;DR: RefTr是一个用于血管树中心线生成的3D图像到图模型，通过循环精炼汇合轨迹来生成具有正确树状拓扑结构的中心线，在保持高召回率的同时减少参数并提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 在临床诊断、治疗规划和手术导航中，准确检测血管树的中心线并保持正确的树状拓扑结构至关重要。保持高召回率尤为重要，因为遗漏小分支可能导致致命错误。

Method: 采用Producer-Refiner架构，基于Transformer解码器。Producer提出初始汇合轨迹，Refiner循环精炼这些轨迹生成最终轨迹，形成中心线图。汇合轨迹表示法能够在精炼完整轨迹时显式强制执行有效的树状拓扑。

Result: 在多个公共中心线数据集上，RefTr实现了优于先前SOTA的召回率和相当的精度，同时提供更快的推理速度和显著更少的参数（解码器参数减少2.4倍）。

Conclusion: RefTr展示了作为3D医学成像中血管树分析的新SOTA框架的潜力，在保持高召回率的同时实现了参数效率和推理速度的提升。

Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.

</details>


### [128] [MODEST: Multi-Optics Depth-of-Field Stereo Dataset](https://arxiv.org/abs/2511.20853)
*Nisarg K. Trivedi,Vinayak A. Belludi,Li-Yun Wang,Pardis Taghavi,Dante Lok*

Main category: cs.CV

TL;DR: 提出了首个高分辨率(5472×3648px)立体DSLR数据集，包含18000张图像，系统性地在复杂真实场景中变化焦距和光圈，捕捉专业相机系统的光学真实性和复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决深度估计在真实光学条件下的可靠性问题，填补大规模高保真实体立体DSLR数据集的空白，弥合合成训练数据与真实相机光学之间的真实感差距。

Method: 使用两个相同的相机组件，在9个不同场景复杂度、光照和背景的场景中，以10种焦距(28-70mm)和5种光圈(f/2.8-f/22)拍摄，涵盖50种光学配置，每个场景2000张图像。

Result: 创建了具有挑战性视觉元素的数据集，包括多尺度光学幻觉、反射表面、镜子、透明玻璃墙、精细细节和自然/人工环境光变化。

Conclusion: 该数据集支持单目和立体深度估计、浅景深渲染、去模糊、3D场景重建和新视角合成等任务的受控分析，并展示了当前最先进方法在真实世界光学泛化方面的挑战。

Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.

</details>


### [129] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim,Henry Gouk,Timothy Hospedales*

Main category: cs.CV

TL;DR: 提出了Null-Text Test-Time Alignment (Null-TTA)方法，通过优化分类器自由引导中的无条件嵌入来实现扩散模型的测试时对齐，避免了对潜在变量或噪声变量的操作，从而防止奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时对齐方法往往会出现欠优化或过优化（奖励黑客）目标奖励函数的问题，需要一种更有效的方法来平衡对齐效果和泛化能力。

Method: Null-TTA方法优化分类器自由引导中的无条件嵌入，而不是操作潜在变量或噪声变量。由于文本嵌入空间具有结构化的语义特性，这确保了在对齐过程中保持在语义一致的流形上。

Result: Null-TTA在目标测试时对齐方面达到了最先进的性能，同时保持了强大的跨奖励泛化能力。

Conclusion: 语义空间优化被确立为测试时对齐的一种有效且原则性的新范式。

Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.

</details>


### [130] [Open Vocabulary Compositional Explanations for Neuron Alignment](https://arxiv.org/abs/2511.20931)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.CV

TL;DR: 提出一个开放词汇组合解释框架，利用开放词汇语义分割模型生成掩码，从而能够对任意概念和数据集进行神经元分析，突破了传统方法依赖人工标注数据的限制。


<details>
  <summary>Details</summary>
Motivation: 传统组合解释方法依赖人工标注数据集，限制了其应用范围和概念灵活性。本文旨在开发一个能够探测神经元对任意概念响应的框架，提高解释方法的通用性和适用性。

Method: 框架包含三个步骤：指定任意概念、使用开放词汇语义分割模型生成语义分割掩码、从这些掩码中推导组合解释。通过模型生成的数据替代人工标注数据。

Result: 与先前方法相比，该框架在定量指标和人类可解释性方面表现良好，展示了从人工标注数据转向模型标注数据时解释的差异，并提供了在任务和感兴趣属性方面的解释灵活性。

Conclusion: 提出的开放词汇组合解释框架成功解决了传统方法的局限性，能够灵活处理任意概念和数据集，为神经元信息编码的理解提供了更通用的工具。

Abstract: Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.

</details>


### [131] [BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model](https://arxiv.org/abs/2511.20956)
*Rawa Mohammed,Mina Attin,Bryar Shareef*

Main category: cs.CV

TL;DR: BUSTR是一个用于乳腺超声报告生成的多任务视觉语言框架，无需配对的图像-报告监督数据，通过结构化描述符和放射组学特征构建报告，在多个数据集上提升了报告生成质量和临床效果。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声报告生成面临缺乏配对图像-报告数据集和大语言模型幻觉风险的问题，需要一种无需配对监督的方法来生成准确可靠的报告。

Method: 使用多任务视觉语言框架，从结构化描述符（BI-RADS、病理学、组织学）和放射组学特征构建报告，通过多头Swin编码器学习描述符感知的视觉表示，并使用双级目标对齐视觉和文本标记。

Result: 在两个公共乳腺超声数据集（BrEaST和BUS-BRA）上，BUSTR持续改善了标准自然语言生成指标和临床效果指标，特别是在BI-RADS类别和病理学等关键目标上表现优异。

Conclusion: 描述符感知的视觉模型结合标记级和对齐损失训练，无需配对图像-报告数据即可改善自动报告指标和临床效果，为乳腺超声报告生成提供了有效解决方案。

Abstract: Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR

</details>


### [132] [Wavefront-Constrained Passive Obscured Object Detection](https://arxiv.org/abs/2511.20991)
*Zhiwen Zheng,Yiwei Ouyang,Zhao Huang,Tao Zhang,Xiaoshuai Zhang,Huiyu Zhou,Wenwen Tang,Shaowei Jiang,Jin Liu,Xingru Huang*

Main category: cs.CV

TL;DR: 提出WavePCNet网络，通过三相位波前复传播重投影和动量记忆机制，增强非视域遮挡物体的定位与分割能力，在低信噪比条件下提升稳定性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于实值建模或局部卷积的方法难以捕捉相干光传播的物理本质，在低信噪比条件下易收敛到非物理解，影响观测的稳定性和可靠性。

Method: 提出WavePCNet网络，集成TriWCP模块引入复振幅传输算子精确约束相干传播行为，采用动量记忆机制抑制扰动积累，并通过高频跨层补偿增强构建频率选择性通路。

Result: 在四个物理采集数据集上的实验表明，WavePCNet在准确性和鲁棒性方面均优于现有最先进方法。

Conclusion: WavePCNet通过物理驱动的波前传播补偿，有效提升了非视域遮挡物体感知的稳定性和可靠性，为复杂环境条件下的光场成像提供了新思路。

Abstract: Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.

</details>


### [133] [From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting](https://arxiv.org/abs/2511.21215)
*Umang Agarwal,Rudraksh Sangore,Sumit Laddha*

Main category: cs.CV

TL;DR: 比较三种生成模型：DDPM、CFM和MeanFlow。CFM在50步采样时FID为24.15，显著优于DDPM的402.98。MeanFlow单步采样FID为29.15，推理时间减少50倍。CFM扩展至图像修复，在多种掩码类型下性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 比较三种生成建模范式的性能差异，特别是迭代采样与单步生成的效率对比，并探索CFM在图像修复任务中的应用潜力。

Method: 使用统一的TinyUNet架构（<1.5M参数）在CIFAR-10上实现DDPM、CFM和MeanFlow三种方法。CFM采用条件流匹配，MeanFlow通过建模时间间隔内的平均速度实现单步生成。扩展CFM至图像修复，实现四种掩码类型的掩码引导采样。

Result: CFM在50步采样时FID为24.15，DDPM为402.98。MeanFlow单步采样FID为29.15。图像修复任务中，PSNR从4.95提升至8.57 dB（+73%），SSIM从0.289提升至0.418（+45%）。

Conclusion: CFM在生成质量上显著优于DDPM，MeanFlow在保持良好生成质量的同时实现了50倍的推理加速。CFM在图像修复任务中表现出色，修复感知训练能显著提升修复性能。

Abstract: We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.

</details>


### [134] [Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis](https://arxiv.org/abs/2511.21397)
*Jiyun Bae,Hyunjong Ok,Sangwoo Mo,Jaeho Lee*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型中无关信息（干扰项）对测试时扩展的影响，发现视觉干扰项与文本干扰项存在根本差异：虽然都存在逆向扩展效应，但视觉干扰项会降低准确性而不增加推理长度。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明语言模型中的文本干扰项会导致推理变长但效果变差，本文旨在探究多模态环境中是否会出现类似现象。

Method: 引入Idis数据集，系统地在语义、数值和空间维度上变化视觉干扰项，并通过分析推理轨迹中的属性计数来研究干扰项、推理长度和准确性的相互作用。

Result: 视觉干扰项会降低模型准确性但不增加推理长度，与文本干扰项的影响模式不同。这些趋势也适用于已建立的视觉偏见基准测试。

Conclusion: 提出了简单的提示策略来减轻推理模型中的偏见驱动预测，揭示了视觉干扰项对VLMs的独特影响模式。

Abstract: How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.

</details>


### [135] [Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning](https://arxiv.org/abs/2511.21490)
*Taehoon Kim,Donghwan Jang,Bohyung Han*

Main category: cs.CV

TL;DR: 提出Merge-and-Bound (M&B)方法用于类增量学习，通过在参数空间中直接操作模型权重，包含任务间和任务内权重合并，以及有界更新技术来减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决类增量学习中的灾难性遗忘问题，通过直接优化模型权重来保留先前任务的知识。

Method: 使用两种权重合并：任务间权重合并（平均所有先前阶段模型的权重）和任务内权重合并（合并当前阶段内的模型参数），并采用有界更新技术来最小化累积更新。

Result: 在标准CIL基准测试中表现出色，优于现有最先进方法。

Conclusion: M&B方法能够有效减少灾难性遗忘，且无需修改架构组件或学习目标即可集成到现有CIL方法中。

Abstract: We present a novel training approach, named Merge-and-Bound (M&B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.

</details>


### [136] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出了一种新颖的AI错误校正系统，通过少样本学习在资源受限设备上实现高效错误修正，结合服务器端基础模型训练和设备端原型分类，无需模型重训练即可更新原型。


<details>
  <summary>Details</summary>
Motivation: AI模型在日常设备中的普及凸显了预测错误降低用户体验的问题，现有解决方案主要关注错误检测而缺乏高效的校正机制，特别是在资源受限设备上。

Method: 系统包含两个关键组件：(1) 服务器端管道利用知识蒸馏将基础模型的鲁棒特征表示转移到设备兼容架构；(2) 设备端机制通过原型适配实现超高效错误校正。

Result: 在Food-101和Flowers-102数据集上，单次场景下实现超过50%的错误校正，同时保持极低的遗忘率（小于0.02%）和可忽略的计算开销。

Conclusion: 通过Android演示应用验证了系统在实际场景中的实用性，证明了在资源受限设备上实现高效AI错误校正的可行性。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [137] [Cryptocurrency Portfolio Management with Reinforcement Learning: Soft Actor--Critic and Deep Deterministic Policy Gradient Algorithms](https://arxiv.org/abs/2511.20678)
*Kamal Paykan*

Main category: q-fin.CP

TL;DR: 提出基于强化学习的加密货币组合管理框架，使用SAC和DDPG算法优化投资组合权重，在高度波动的加密货币市场中实现收益最大化并控制风险。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化方法难以适应加密货币市场的高度波动性和非线性动态特性，需要更灵活的自适应方法。

Method: 设计强化学习智能体，通过模拟交易环境从历史市场数据中学习连续交易动作，使用SAC和DDPG算法优化投资组合权重。

Result: 在多种加密货币上的实验表明，SAC和DDPG智能体优于等权重和均值-方差等基准策略，SAC算法在噪声市场条件下表现出更好的稳定性和鲁棒性。

Conclusion: 深度强化学习在加密货币市场中具有实现自适应和数据驱动投资组合管理的潜力。

Abstract: This paper proposes a reinforcement learning--based framework for cryptocurrency portfolio management using the Soft Actor--Critic (SAC) and Deep Deterministic Policy Gradient (DDPG) algorithms. Traditional portfolio optimization methods often struggle to adapt to the highly volatile and nonlinear dynamics of cryptocurrency markets. To address this, we design an agent that learns continuous trading actions directly from historical market data through interaction with a simulated trading environment. The agent optimizes portfolio weights to maximize cumulative returns while minimizing downside risk and transaction costs. Experimental evaluations on multiple cryptocurrencies demonstrate that the SAC and DDPG agents outperform baseline strategies such as equal-weighted and mean--variance portfolios. The SAC algorithm, with its entropy-regularized objective, shows greater stability and robustness in noisy market conditions compared to DDPG. These results highlight the potential of deep reinforcement learning for adaptive and data-driven portfolio management in cryptocurrency markets.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [138] [When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing](https://arxiv.org/abs/2511.20851)
*Mousam Sinha,Tirtha Sarathi Ghosh,Ridam Pal*

Main category: stat.ML

TL;DR: 提出了一种基于非参数bootstrap假设检验的特征选择方法，通过引入随机噪声特征并与噪声特征的最大重要性值进行比较，建立统计理论基础，在模拟和真实数据集上表现优于Boruta、Knockoff等方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法存在计算成本高、缺乏统计驱动的停止标准、重要性评分显著性评估不足等问题，需要一种具有理论基础的稳健方法。

Method: 引入多个随机噪声特征，将每个特征的重要性与噪声特征的最大重要性值进行比较，结合非参数bootstrap假设检验框架建立统计理论基础。

Result: 在模拟数据集上比Boruta和Knockoff方法更有效地恢复有意义信号，在真实数据集上表现优于Boruta、RFE和Extra Trees等方法。

Conclusion: 该方法是一种稳健的特征选择算法，能够提取支持可靠推断、增强预测性能和高效计算的信息预测因子。

Abstract: Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.

</details>


### [139] [Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets](https://arxiv.org/abs/2511.20888)
*Arthur Jacot*

Main category: stat.ML

TL;DR: 深度神经网络实现了计算奥卡姆剃刀原理——寻找拟合数据的最简单算法，这解释了其相对于传统统计方法的巨大成功。


<details>
  <summary>Details</summary>
Motivation: 解释深度神经网络（特别是ResNets）为何在广泛任务中表现优于传统统计方法，揭示其内在的计算效率机制。

Method: 通过定义HTMC（Harder than Monte Carlo）范数和ResNet范数，建立两者之间的近似匹配夹逼界，证明最小化ResNet范数等价于寻找节点数几乎最优的电路。

Result: 发现当γ>2时，可用大小为cε^{-γ}的二进制电路ε-近似的实值函数集合在HTMC区域变得凸，ResNets成为HTMC区域计算实函数的更好替代模型。

Conclusion: ResNets实现了一种计算奥卡姆剃刀，通过最小化参数复杂度来找到拟合数据的最简单算法，这解释了其在HTMC区域的优越性能。

Abstract: This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $ε$-approximated with a binary circuit of size at most $cε^{-γ}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $γ>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity.

</details>


### [140] [Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification](https://arxiv.org/abs/2511.20960)
*Soumojit Das,Nairanjana Dasgupta,Prashanta Dutta*

Main category: stat.ML

TL;DR: 提出基于信息几何的神经网络概率输出后处理校准框架，使用Fisher-Rao度量在概率单纯形上定义校准映射和可靠性评分，为不确定预测提供理论保证的延迟机制。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在不确定时经常静默失败，需要开发具有理论保证的校准和不确定性量化方法，以支持关键决策应用中的严格验证需求。

Method: 将概率向量视为概率单纯形上的点，使用Fisher-Rao度量定义加性对数比(ALR)校准映射，构建基于Fisher-Rao距离的几何可靠性评分和中性区域用于预测延迟。

Result: 理论证明校准估计器的一致性(O_p(n^{-1/2}))和可靠性评分的紧集中界；在AAV分类任务中，两阶段框架(校准+延迟)捕获72.5%错误同时延迟34.5%样本。

Conclusion: 该工作连接信息几何和统计学习，为需要严格验证的应用提供形式化保证，几何校准的主要贡献在于理论基础而非经验性能优于简单替代方法。

Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.
  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\% of errors while deferring 34.5\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation.

</details>


### [141] [Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms](https://arxiv.org/abs/2511.21115)
*Lechen Feng,Haoran Li,Lucky Li,Xingqiu Zhao*

Main category: stat.ML

TL;DR: 本文研究基于最小绝对偏差回归的部分线性模型，使用深度神经网络参数化非参数项，建立惩罚LAD估计问题，并分析其统计性质和计算特性。


<details>
  <summary>Details</summary>
Motivation: 研究部分线性模型的LAD回归，解决正则化项非凸非光滑、网络结构随样本扩展、估计器定义涉及超高维非凸不连续优化等挑战。

Method: 使用深度神经网络参数化非参数项，构建惩罚LAD估计问题，采用近端次梯度方法求解，并分析原始问题和连续松弛问题的收敛性。

Result: 建立了估计器的一致性、收敛速率和渐近正态性，发现松弛公式允许显著更便宜的近端更新，反映了统计精度和计算可处理性之间的权衡。

Conclusion: 在面临多重理论挑战的情况下，成功建立了部分线性模型LAD-DNN估计的统计性质，并揭示了统计精度与计算效率之间的内在权衡关系。

Abstract: This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability.

</details>


### [142] [Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference](https://arxiv.org/abs/2511.21223)
*Jasraj Singh,Shelvia Wongso,Jeremie Houssineau,Badr-Eddine Chérief-Abdellatif*

Main category: stat.ML

TL;DR: 本文开发了可能性变分推理的原理性公式，并将其应用于指数族函数，揭示了可能性理论与概率论在数学结构上的差异。


<details>
  <summary>Details</summary>
Motivation: 变分推理是现代贝叶斯学习的基石，但其依赖于高维积分定义，往往需要近似技术。可能性理论作为不精确概率框架，可以直接建模认知不确定性，但需要重新思考变分推理中的核心概念如熵和散度。

Method: 开发了可能性变分推理的原理性公式，并将其应用于指数族函数，比较了与概率对应物的相似性。

Result: 建立了可能性变分推理的数学框架，揭示了可能性理论独特的数学结构特征。

Conclusion: 这项工作为可能性理论框架下的变分推理提供了理论基础，展示了与概率变分推理的并行性和差异性。

Abstract: Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory.

</details>


### [143] [Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities (II)](https://arxiv.org/abs/2511.21526)
*Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 本文证明了在随机块模型中，当社区数量K≥√n时，社区恢复的计算障碍位于Chin等人提出的阈值处，并通过构造特定结构基元和计数这些基元的方法，实现了在该阈值之上的社区恢复。


<details>
  <summary>Details</summary>
Motivation: 解决在随机块模型中，当社区数量K≥√n时，社区恢复在多项式时间内是否可能的问题，特别是验证Chin等人提出的新阈值是否为计算障碍。

Method: 1. 构造满足特定结构性质的基元族；2. 通过计数这些基元来证明在提议阈值之上可以实现社区恢复。

Result: 证明了在K≥√n的情况下，社区恢复在多项式时间内是可行的，且计算障碍确实位于Chin等人提出的阈值处。

Conclusion: 完成了K≥√n情况下随机块模型中社区恢复计算障碍的完整图像，并表明在适度稀疏机制中，最优算法与谱方法有根本不同。

Abstract: A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.
  When $K \geq \sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \geq \sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.
  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\ 1- Constructing a family of motifs satisfying specific structural properties; and\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \geq \sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods.

</details>


### [144] [On Evolution-Based Models for Experimentation Under Interference](https://arxiv.org/abs/2511.21675)
*Sadegh Shirani,Mohsen Bayati*

Main category: stat.ML

TL;DR: 提出了一种基于演化的因果效应估计方法，通过观察多轮干预下结果的变化来补偿缺失的网络信息，无需恢复确切的网络结构。


<details>
  <summary>Details</summary>
Motivation: 在复杂网络系统中，干预效应会溢出到其他单元，但交互路径通常不可观测。传统方法需要恢复网络结构，而本文认为识别总体因果效应只需表征交互如何影响结果演化。

Method: 采用演化方法研究多轮观察中结果如何随干预变化，使用暴露映射视角，建立结果经验分布遵循低维递归方程的条件，并作为分布层面的双重差分法。

Result: 证明了在最小结构条件下存在演化映射，治疗随机化不仅消除潜在混杂，还从隐藏干扰通道中诱导隐式采样，能够一致学习异质溢出效应。

Conclusion: 该方法在密集网络中表现为因果消息传递，可扩展到更一般的干扰结构，但强时间趋势或内生干扰会破坏识别性。

Abstract: Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [145] [Independent policy gradient-based reinforcement learning for economic and reliable energy management of multi-microgrid systems](https://arxiv.org/abs/2511.20977)
*Junkai Hu,Li Xia*

Main category: eess.SY

TL;DR: 该研究提出了一种分布式算法来解决多微电网系统中的经济可靠能源管理问题，使用均值-方差团队随机博弈模型，并开发了两种算法：已知模型参数的分布式独立策略梯度算法和未知参数情况下的深度强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 多微电网系统整合间歇性可再生能源时，需要同时考虑经济性和可靠性。传统基于期望累积奖励最大化的方法不适用于方差指标，因此需要新的解决方案。

Method: 1. 将能源管理问题建模为均值-方差团队随机博弈；2. 针对已知模型参数场景，提出完全分布式独立策略梯度算法；3. 针对大规模未知参数场景，开发基于独立策略梯度的深度强化学习算法。

Result: 数值实验验证了所提方法的有效性，能够充分利用多微电网的分布式计算能力，在经济性能和运行可靠性之间实现良好平衡。

Conclusion: 提出的分布式算法成功解决了多微电网系统中的经济可靠能源管理问题，为可再生能源集成提供了有效的解决方案。

Abstract: Efficiency and reliability are both crucial for energy management, especially in multi-microgrid systems (MMSs) integrating intermittent and distributed renewable energy sources. This study investigates an economic and reliable energy management problem in MMSs under a distributed scheme, where each microgrid independently updates its energy management policy in a decentralized manner to optimize the long-term system performance collaboratively. We introduce the mean and variance of the exchange power between the MMS and the main grid as indicators for the economic performance and reliability of the system. Accordingly, we formulate the energy management problem as a mean-variance team stochastic game (MV-TSG), where conventional methods based on the maximization of expected cumulative rewards are unsuitable for variance metrics. To solve MV-TSGs, we propose a fully distributed independent policy gradient algorithm, with rigorous convergence analysis, for scenarios with known model parameters. For large-scale scenarios with unknown model parameters, we further develop a deep reinforcement learning algorithm based on independent policy gradients, enabling data-driven policy optimization. Numerical experiments in two scenarios validate the effectiveness of the proposed methods. Our approaches fully leverage the distributed computational capabilities of MMSs and achieve a well-balanced trade-off between economic performance and operational reliability.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [146] [Estimation in high-dimensional linear regression: Post-Double-Autometrics as an alternative to Post-Double-Lasso](https://arxiv.org/abs/2511.21257)
*Sullivan Hué,Sébastien Laurent,Ulrich Aiounou,Emmanuel Flachaire*

Main category: econ.EM

TL;DR: 提出Post-Double-Autometrics方法，基于Autometrics，优于流行的Post-Double-Lasso方法，能减少有限样本中的遗漏变量偏差。


<details>
  <summary>Details</summary>
Motivation: Post-Double-Lasso方法在估计线性回归模型时存在显著的遗漏变量偏差问题，特别是在有限样本中。

Method: 基于Autometrics开发Post-Double-Autometrics方法。

Result: 新方法在性能上优于Post-Double-Lasso，在经济收敛假设应用中提供了新的见解。

Conclusion: Post-Double-Autometrics是Post-Double-Lasso的有效替代方法，能提供更准确的参数估计。

Abstract: Post-Double-Lasso is becoming the most popular method for estimating linear regression models with many covariates when the purpose is to obtain an accurate estimate of a parameter of interest, such as an average treatment effect. However, this method can suffer from substantial omitted variable bias in finite sample. We propose a new method called Post-Double-Autometrics, which is based on Autometrics, and show that this method outperforms Post-Double-Lasso. Its use in a standard application of economic growth sheds new light on the hypothesis of convergence from poor to rich economies.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [147] [Crowdsourcing the Frontier: Advancing Hybrid Physics-ML Climate Simulation via $50,000 Kaggle Competition](https://arxiv.org/abs/2511.20963)
*Jerry Lin,Zeyuan Hu,Tom Beucler,Katherine Frields,Hannah Christensen,Walter Hannah,Helge Heuer,Peter Ukkonnen,Laura A. Mansfield,Tian Zheng,Liran Peng,Ritwik Gupta,Pierre Gentine,Yusef Al-Naher,Mingjiang Duan,Kyo Hattori,Weiliang Ji,Chunhan Li,Kippei Matsuda,Naoki Murakami,Shlomo Ron,Marec Serlin,Hongjian Song,Yuma Tanabe,Daisuke Yamamoto,Jianyao Zhou,Mike Pritchard*

Main category: physics.ao-ph

TL;DR: 该论文研究了将Kaggle竞赛获胜团队的机器学习架构集成到气候模型中，验证了在低分辨率真实地理环境下在线稳定性的可重现性，并发现多个架构在特定指标上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决子网格机器学习参数化在长期气候预测中面临的在线不稳定性和性能不一致问题，推动混合物理-AI气候模拟的发展。

Method: 将Kaggle竞赛获胜团队的机器学习架构与包含完整云微物理的交互式气候模型耦合，系统评估其在线性能。

Result: 在低分辨率真实地理环境下，在线稳定性可在多种不同架构中重现；所有测试架构都显示出相似的离线和在线偏差；多个Kaggle启发架构在特定指标上达到最先进水平。

Conclusion: 众包离线问题的本质是提高混合物理-AI气候模拟在线性能的一条可行路径，证明了机器学习参数化在气候模型中的实际应用潜力。

Abstract: Subgrid machine-learning (ML) parameterizations have the potential to introduce a new generation of climate models that incorporate the effects of higher-resolution physics without incurring the prohibitive computational cost associated with more explicit physics-based simulations. However, important issues, ranging from online instability to inconsistent online performance, have limited their operational use for long-term climate projections. To more rapidly drive progress in solving these issues, domain scientists and machine learning researchers opened up the offline aspect of this problem to the broader machine learning and data science community with the release of ClimSim, a NeurIPS Datasets and Benchmarks publication, and an associated Kaggle competition. This paper reports on the downstream results of the Kaggle competition by coupling emulators inspired by the winning teams' architectures to an interactive climate model (including full cloud microphysics, a regime historically prone to online instability) and systematically evaluating their online performance. Our results demonstrate that online stability in the low-resolution, real-geography setting is reproducible across multiple diverse architectures, which we consider a key milestone. All tested architectures exhibit strikingly similar offline and online biases, though their responses to architecture-agnostic design choices (e.g., expanding the list of input variables) can differ significantly. Multiple Kaggle-inspired architectures achieve state-of-the-art (SOTA) results on certain metrics such as zonal mean bias patterns and global RMSE, indicating that crowdsourcing the essence of the offline problem is one path to improving online performance in hybrid physics-AI climate simulation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [148] [LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data](https://arxiv.org/abs/2511.20691)
*Lijun Shang,Yadong Yu,Wenqiang Kang,Jian Zhou,Dongyue Gao,Pan Xiang,Zhe Liu,Mengyan Dai,Zhonglu Guo,Zhimei Sun*

Main category: cs.CL

TL;DR: 二维材料在能源存储和转换领域有广泛应用，但其相关信息分散在研究论文中，难以系统获取。


<details>
  <summary>Details</summary>
Motivation: 由于二维材料的特性和制备方法信息分散在众多研究论文中，需要系统化的方法来整理和获取这些有价值的信息。

Method: 未在提供的摘要内容中明确说明具体方法。

Result: 未在提供的摘要内容中明确说明具体结果。

Conclusion: 二维材料在能源领域具有重要应用价值，但其相关信息需要更好的组织和获取方式。

Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe

</details>


### [149] [Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology](https://arxiv.org/abs/2511.20665)
*Tcharlies Schmitz*

Main category: cs.CL

TL;DR: HTP是一种无需训练、词汇表或随机参数的可逆确定性文本嵌入框架，通过Unicode整数的谐波轨迹生成嵌入向量，在STS-B基准测试中达到0.68的Spearman相关性。


<details>
  <summary>Details</summary>
Motivation: 传统神经嵌入依赖统计共现或优化，缺乏可解释性和可逆性。HTP旨在提供一种透明、高效的替代方案，从确定性几何中产生有意义的语义关系。

Method: 将每个token的Unicode整数表示解析为谐波轨迹，建立离散符号与连续向量空间的双射可解释映射，通过相位相干投影保持结构和可逆性。

Result: 在STS-B及其多语言扩展测试中，英语达到Spearman相关性0.68，在10种语言中保持稳定性能，计算成本极低，每句对延迟亚毫秒级。

Conclusion: 有意义的语义关系可以从确定性几何中产生，HTP为数据驱动嵌入提供了透明高效的替代方案。

Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.

</details>


### [150] [Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models](https://arxiv.org/abs/2511.20799)
*Trung Cuong Dang,David Mohaisen*

Main category: cs.CL

TL;DR: 提出了多前缀记忆框架来检测LLM中的记忆化现象，通过评估不同前缀触发相同序列的能力来量化记忆强度


<details>
  <summary>Details</summary>
Motivation: 现有记忆化定义在捕捉对齐模型中的记忆现象时存在不足，需要更全面的框架来评估隐私和版权风险

Method: 引入多前缀记忆框架，定义序列被记忆的标准是外部对抗搜索能找到足够数量的不同前缀来触发该序列

Result: 实验表明多前缀定义能可靠区分记忆化和非记忆化数据，为审计LLM数据泄露提供了鲁棒工具

Conclusion: 多前缀记忆框架通过量化记忆的检索路径多样性，提供了更全面的记忆化评估方法

Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.

</details>


### [151] [Structured Prompting Enables More Robust, Holistic Evaluation of Language Models](https://arxiv.org/abs/2511.20836)
*Asad Aali,Muhammad Ahmed Mohsin,Vasiliki Bikia,Arnav Singhvi,Richard Gaus,Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Yifan Mai,Jordan Cahoon,Michael Pfeffer,Roxana Daneshjou,Sanmi Koyejo,Emily Alsentzer,Percy Liang,Christopher Potts,Nigam H. Shah,Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: 本文提出了DSPy+HELM框架，通过结构化提示方法实现更准确的语言模型性能评估，发现传统HELM基准测试会低估模型性能并导致排名错误。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试框架（如HELM）使用固定提示，无法跨语言模型泛化，导致性能估计不准确，可能低估模型真实能力。

Method: 开发了可复现的DSPy+HELM框架，引入四种结构化提示方法（包括思维链推理），在7个基准测试上评估4个前沿语言模型。

Result: 发现：HELM平均低估性能4%；性能估计方差增加2%；3/7基准测试的排行榜排名反转；引入推理可降低对提示设计的敏感性。

Conclusion: 可扩展的性能上限估计能够提供更有决策价值的基准测试，这是首个大规模实证研究语言模型在不同基准和提示方法下的行为特征。

Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller Δ across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).

</details>


### [152] [Length-MAX Tokenizer for Language Models](https://arxiv.org/abs/2511.20849)
*Dong Dong,Weijie Su*

Main category: cs.CL

TL;DR: 提出了一种新的Length-MAX分词器，通过最小化平均字符数来减少训练和推理时的token数量，相比BPE在多种词汇表大小下减少13-18%的token，在GPT-2模型上实现更快的训练收敛和更低的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统分词器如BPE主要基于频率优化，而本文发现优化平均token长度能更有效地减少token数量，从而提高语言模型的训练和推理效率。

Method: 将长度加权目标最大化建模为图划分问题，并开发贪心近似算法来获取词汇表，该方法称为Length-MAX分词器。

Result: 在FineWeb和多个领域上，相比BPE减少14-18%的token；GPT-2模型训练步骤减少17.2-18.5%，推理延迟降低12.7-13.7%；下游任务表现提升，LAMBADA困惑度降低11.7%，HellaSwag准确率提高4.3%。

Conclusion: 优化平均token长度而非仅频率，为高效语言建模提供了有效方法，在不牺牲下游性能的情况下显著提升效率，推理时嵌入和KV缓存内存减少18%。

Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.

</details>


### [153] [Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels](https://arxiv.org/abs/2511.21038)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CL

TL;DR: 研究表明，上下文学习（ICL）无法覆盖预训练的标签语义，而是主要调整输入如何映射到预训练期间学习的稳定语义方向，揭示了少样本提示的基本限制。


<details>
  <summary>Details</summary>
Motivation: 探讨ICL是否能覆盖预训练的标签语义，还是仅仅细化现有的语义骨干，以理解ICL在标签语义重映射方面的能力限制。

Method: 将LLMs视为提示诱导分类器，对比其在自然演示（正确标签）和反转演示（系统翻转标签含义）下的行为，分解ICL行为为三个对齐指标，并引入语义覆盖率。

Result: 在8个分类任务和8个开源LLMs上，发现ICL无法学习连贯的反语义分类器，语义覆盖率在1-12B参数的少样本设置中保持为零。

Conclusion: ICL主要调整输入如何投影到预训练期间学习的稳定语义方向，而不是灵活地重映射标签含义，表明在这些规模上覆盖标签语义需要超越ICL的干预措施。

Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.

</details>


### [154] [Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning](https://arxiv.org/abs/2511.21081)
*Thura Aung,Eaint Kay Khaing Kyaw,Ye Kyaw Thu,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: 该研究探索在缅甸语等低资源语言分类任务中，使用Kolmogorov-Arnold Networks (KANs) 替代传统MLPs作为分类头，发现KANs在表达能力和效率方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言分类任务中，通常只微调最终分类层而保持预训练编码器权重冻结。传统MLPs的固定非线性可能限制表达能力并增加计算成本，因此需要探索更有效的替代方案。

Method: 评估了三种KAN变体（FourierKAN、EfficientKAN、FasterKAN）作为分类头，使用多种嵌入方法（TF-IDF、fastText、mBERT、Distil-mBERT）在缅甸语分类任务上进行实验。

Result: KAN-based分类头与MLPs相比具有竞争力或更优性能。EfficientKAN与fastText组合获得最高F1分数（0.928），FasterKAN在速度和精度之间提供最佳平衡，在transformer嵌入上EfficientKAN与mBERT组合达到0.917 F1。

Conclusion: KANs是低资源语言分类任务中MLPs的表达能力强且高效的替代方案。

Abstract: In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.

</details>


### [155] [ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features](https://arxiv.org/abs/2511.21088)
*Ye Bhone Lin,Thura Aung,Ye Kyaw Thu,Thazin Myint Oo*

Main category: cs.CL

TL;DR: 该论文研究了针对低资源缅甸语的ASR错误校正，使用序列到序列Transformer模型，结合IPA和音素对齐特征，显著降低了词错误率和提升了字符级精度。


<details>
  <summary>Details</summary>
Motivation: 这是首个专门针对缅甸语的ASR错误校正研究，旨在解决低资源语言ASR系统输出质量不佳的问题。

Method: 使用序列到序列Transformer模型，评估了五种ASR骨干网络，提出了结合IPA（国际音标）和音素对齐特征的ASR错误校正方法。

Result: 提出的AEC模型将ASR模型的平均WER从51.56降至39.82（增强前）和43.59（增强后），chrF++分数从0.5864提升至0.627，在词级和字符级精度上均优于基线。

Conclusion: 研究证明了ASR错误校正的鲁棒性，以及在低资源环境下特征设计对于改进ASR输出的重要性。

Abstract: This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.

</details>


### [156] [MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing](https://arxiv.org/abs/2511.21101)
*Manish Jain,Satheesh Kumar Ponnambalam,Salman Faroz,Chandrakanth Lns,Vinay Sharma*

Main category: cs.CL

TL;DR: 提出了MortgageLLM，一个专门针对抵押贷款金融领域的双专家大语言模型，通过双轨专业化框架解决了单一模型在多任务优化中的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用领域表现出色，但在专业领域如抵押贷款金融中需要增强领域专业知识，同时保持指令跟随能力。单一模型在多任务优化中存在性能权衡问题。

Method: 采用双轨专业化框架，从单一基础模型创建两个专家模型：对话问答模型和结构化任务模型。应用指令残差技术恢复领域适应后的指令跟随能力，无需监督微调。使用智能任务路由机制进行任务分配。

Result: 在领域特定基准测试中，最终模型MLM v2显著优于基础模型，在总结、问答和分类任务上均取得更高分数，BERTScore也全面优于基线方法。

Conclusion: 双专家架构和指令残差技术有效解决了专业领域LLM应用中的性能权衡问题，在抵押贷款金融领域实现了卓越性能。

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.

</details>


### [157] [Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning](https://arxiv.org/abs/2511.21416)
*Kaifeng Hong,Yinglong Zhang,Xiaoying Hong,Xuewen Xia,Xing Xu*

Main category: cs.CL

TL;DR: Odin是一个新的文本属性图模型架构，通过定向双模块机制在特定Transformer层注入图结构，避免了传统GNN的过平滑问题，同时解决了纯Transformer忽略图拓扑的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖GNN（存在过平滑和跳数依赖扩散问题），要么使用Transformer（忽略图拓扑，将节点视为孤立序列），需要一种能有效结合文本理解和结构推理的模型。

Method: 提出Odin架构，在选定深度的Transformer层通过定向双模块机制注入图结构，多跳结构在特定层集成，产生与语义层次对齐的低、中、高级结构抽象。还提出了轻量级变体Light Odin。

Result: 在多个文本丰富的图基准测试中，Odin实现了最先进的准确性，而Light Odin在显著降低计算成本的同时保持了有竞争力的性能。

Conclusion: Odin和Light Odin形成了一个统一的、无跳数的框架，用于原则性的结构与文本集成，其表达能力严格包含纯Transformer和GNN。

Abstract: Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.

</details>


### [158] [A Systematic Study of Model Merging Techniques in Large Language Models](https://arxiv.org/abs/2511.21437)
*Oğuz Kağan Hitit,Leander Girrbach,Zeynep Akata*

Main category: cs.CL

TL;DR: 对六种最先进的模型合并方法在大型语言模型上进行系统评估，发现最简单的方法Task Arithmetic是唯一能可靠提升性能的方法，其他方法通常导致性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 模型合并可以在不额外训练的情况下将多个微调后的检查点合并为单一模型，但现有方法在小型模型和分类器上的优势是否适用于大型语言模型尚不明确。

Method: 对六种最先进的合并方法（包括子空间方法）进行大规模系统评估，涵盖四个开源权重LLM、每个基础模型十二个微调检查点和十六个标准LLM基准测试。

Result: Task Arithmetic是唯一能可靠提升LLM性能的方法，其他干扰感知和子空间合并方法通常导致显著性能下降。

Conclusion: 当前合并技术不能直接迁移到现代LLM，需要设计LLM特定的合并算法和合并感知的微调方法。

Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.

</details>


### [159] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本文研究了在大型语言模型预训练中利用多种元数据（如文档质量指标）来加速训练的方法，发现细粒度元数据编码信息更有效，并提出了元数据追加和可学习元标记等新方法。


<details>
  <summary>Details</summary>
Motivation: 先前工作仅关注URL作为元数据信号，本研究旨在探索其他元数据类型是否能带来更大的训练加速效益。

Method: 研究了多种元数据类型，引入元数据追加作为辅助任务，使用可学习元标记配合掩码损失，并通过探测分析潜在表示。

Result: 发现细粒度文档质量指标等元数据能加速预训练，元数据追加和可学习元标记都能提高训练效率，元数据能塑造学习过程的质量感知潜在结构。

Conclusion: 为整合元数据改进LLM预训练的效率和效果提供了实用指南，证明了多种元数据类型在加速训练方面的潜力。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [160] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的多智能体合成数据生成框架，通过分布式队列传递序列化消息来替代中心化编排器，在相同硬件资源下实现2-15倍的数据生成吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体合成框架依赖中心化编排器导致可扩展性瓶颈，或针对特定领域硬编码限制了灵活性。需要一种能够高效协调多智能体协作生成高质量、多样化数据的分布式解决方案。

Method: 采用去中心化的对等设计，将控制和数据流表示为通过分布式队列传递的序列化消息。轻量级智能体独立处理任务，计算密集型操作由分布式服务处理，基于Ray构建支持数万个并发智能体工作流。

Result: 在多种合成场景（多智能体协作对话、基于网络的推理数据提取、客户服务环境中的工具使用轨迹生成）中，Matrix在相同硬件资源下实现2-15倍的数据生成吞吐量提升，且不牺牲输出质量。

Conclusion: Matrix提供了一个模块化、可配置的去中心化框架，能够轻松适应广泛的数据生成工作流，显著提升了多智能体合成数据生成的效率和可扩展性。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [161] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra方法训练小型编排器来协调智能工具，在Humanity's Last Exam等复杂任务上以更低成本超越GPT-5等大型模型性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大，但解决深度复杂问题仍面临概念挑战和计算成本高昂的问题，需要更高效的智能工具协调方法

Method: 使用强化学习训练小型编排器，包含结果、效率和用户偏好感知的奖励机制，协调多种智能工具

Result: 8B参数的Orchestrator模型在HLE上得分37.1%超越GPT-5(35.1%)，成本降低2.5倍；在tau2-Bench和FRAMES上大幅超越GPT-5同时仅使用约30%成本

Conclusion: 通过轻量级编排模型组合多样化工具比现有方法更高效有效，为实用可扩展的工具增强推理系统铺平道路

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [162] [Going with the Speed of Sound: Pushing Neural Surrogates into Highly-turbulent Transonic Regimes](https://arxiv.org/abs/2511.21474)
*Fabian Paischer,Leo Cotteleer,Yann Dreze,Richard Kurle,Dylan Rubini,Maurits Bleeker,Tobias Kronlachner,Johannes Brandstetter*

Main category: cs.CE

TL;DR: 提出了一个新的跨音速3D机翼CFD模拟数据集，包含约30,000个样本，用于评估神经代理模型在几何和流入条件变化下的OOD泛化能力，AB-UPT模型在跨音速流场和阻力-升力帕累托前沿方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有航空航天数据集主要关注2D翼型，忽略了关键的3D现象如翼尖涡流，而跨音速可压缩流动的高度非线性使得将神经网络代理方法从汽车空气动力学扩展到航空航天领域具有挑战性。

Method: 创建包含约30,000个样本的3D机翼跨音速CFD模拟数据集，评估包括Transolver和AB-UPT在内的先进神经代理模型在几何和流入条件变化下的OOD泛化性能。

Result: AB-UPT在跨音速流场方面表现出色，能够为未见过的机翼配置生成物理一致的阻力-升力帕累托前沿，能够近似未见几何形状的阻力-升力帕累托前沿。

Conclusion: AB-UPT作为快速空气动力学设计探索的高效有效工具具有潜力，数据集已开源以促进未来研究。

Abstract: The widespread use of neural surrogates in automotive aerodynamics, enabled by datasets such as DrivAerML and DrivAerNet++, has primarily focused on bluff-body flows with large wakes. Extending these methods to aerospace, particularly in the transonic regime, remains challenging due to the high level of non-linearity of compressible flows and 3D effects such as wingtip vortices. Existing aerospace datasets predominantly focus on 2D airfoils, neglecting these critical 3D phenomena. To address this gap, we present a new dataset of CFD simulations for 3D wings in the transonic regime. The dataset comprises volumetric and surface-level fields for around $30,000$ samples with unique geometry and inflow conditions. This allows computation of lift and drag coefficients, providing a foundation for data-driven aerodynamic optimization of the drag-lift Pareto front. We evaluate several state-of-the-art neural surrogates on our dataset, including Transolver and AB-UPT, focusing on their out-of-distribution (OOD) generalization over geometry and inflow variations. AB-UPT demonstrates strong performance for transonic flowfields and reproduces physically consistent drag-lift Pareto fronts even for unseen wing configurations. Our results demonstrate that AB-UPT can approximate drag-lift Pareto fronts for unseen geometries, highlighting its potential as an efficient and effective tool for rapid aerodynamic design exploration. To facilitate future research, we open-source our dataset at https://huggingface.co/datasets/EmmiAI/Emmi-Wing.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [163] [Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations](https://arxiv.org/abs/2511.21369)
*Tingkai Xue,Chin Chun Ooi,Zhengwei Ge,Fong Yew Leong,Hongying Li,Chang Wei Kang*

Main category: physics.comp-ph

TL;DR: 提出了一种混合物理-神经模型，能够以比传统3D模拟快数个数量级的速度预测复杂域中的标量输运，从几小时缩短到不到1分钟。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟在完整3D域上求解，但大多数分析只需要简化指标，现有方法计算成本高昂。

Method: 端到端可微分框架，联合学习物理模型参数化（正交各向异性扩散率）和非马尔可夫神经闭合模型，捕获未解析的粗粒度效应。

Result: 数据效率高（仅需26个训练数据），可灵活扩展到分布外场景（移动源），最终模拟时间的Spearman相关系数达到0.96。

Conclusion: 该可微分物理-神经框架能够为物理现象提供快速、准确且可泛化的粗粒度替代模型。

Abstract: Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [164] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文研究利用大型语言模型自动重构层次结构以优化双曲嵌入质量，通过提示引导的方法改造现有层次结构，实验证明LLM重构的层次结构能显著提高双曲嵌入质量。


<details>
  <summary>Details</summary>
Motivation: 双曲嵌入的质量与输入层次结构密切相关，而现有层次结构往往来自知识图谱或本体，可能不符合双曲嵌入的最佳条件（高分支因子和单继承）。

Method: 提出基于提示的方法，利用大型语言模型根据双曲嵌入的需求自动重构现有层次结构。

Result: 在16个不同层次结构上的实验表明，LLM重构的层次结构在多个标准嵌入质量指标上都能产生更高质量的双曲嵌入。

Conclusion: LLM引导的层次结构重构不仅能提高双曲嵌入质量，还能提供可解释的重组理由，帮助知识工程师理解重组过程。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [165] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim,Seung-Ho Han,EunYoung Byun,Jeongyun Han,Soohyun Cho,Eojin Joo,Heehyeon Kim,Sieun Kim,Juhoon Lee,Hyunsoo Lee,Dongkun Lee,Jonghwan Hyeon,Yechan Hwang,Young-Jun Lee,Kyeongryul Lee,Minhyeong An,Hyunjun Ahn,Jeongwoo Son,Junho Park,Donggyu Yoon,Taehyung Kim,Jeemin Kim,Dasom Choi,Kwangyoung Lee,Hyunseung Lim,Yeohyun Jung,Jongok Hong,Sooyohn Nam,Joonyoung Park,Sungmin Na,Yubin Choi,Jeanne Choi,Yoojin Hong,Sueun Jang,Youngseok Seo,Somin Park,Seoungung Jo,Wonhye Chae,Yeeun Jo,Eunyoung Kim,Joyce Jiyoung Whang,HwaJung Hong,Joseph Seering,Uichin Lee,Juho Kim,Sunna Choi,Seokyeon Ko,Taeho Kim,Kyunghoon Kim,Myungsik Ha,So Jung Lee,Jemin Hwang,JoonHo Kwak,Ho-Jin Choi*

Main category: cs.AI

TL;DR: AssurAI是一个针对韩语多模态生成AI安全评估的质量控制数据集，包含11,480个文本、图像、视频和音频实例，涵盖35种AI风险因素，特别关注韩语社会文化背景下的安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前的安全数据集主要是英语中心的，无法捕捉非英语社会文化背景（如韩语）中的特定风险，且通常仅限于文本模态，需要填补这一空白。

Method: 1) 定义35种AI风险因素分类法；2) 构建大规模韩语多模态数据集；3) 实施严格质量控制流程，包括两阶段构建、三重独立标注和迭代式专家红队测试循环。

Result: 试点研究验证了AssurAI在评估最新LLMs安全性方面的有效性，数据集已公开发布以促进更安全可靠的韩语生成AI系统开发。

Conclusion: AssurAI填补了韩语多模态AI安全评估的空白，为韩国社区提供了更安全可靠的生成AI系统开发基础。

Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.

</details>


### [166] [Reasoning With a Star: A Heliophysics Dataset and Benchmark for Agentic Scientific Reasoning](https://arxiv.org/abs/2511.20694)
*Kevin Lee,Russell Spiewak,James Walsh*

Main category: cs.AI

TL;DR: 提出了一个用于太阳物理推理的数据集和基准测试方法，通过多智能体模式提升推理能力，特别是在需要演绎推理的问题上。


<details>
  <summary>Details</summary>
Motivation: 解决太阳物理领域大型语言模型在科学推理中面临的挑战，包括物理假设整合、单位一致性和科学格式要求。

Method: 构建了基于NASA和UCAR夏季学校问题集的数据集，包含问题上下文、推理步骤、答案类型等；开发了程序化评分器；测试了单次提示和四种多智能体模式。

Result: 通过系统工程原则分解工作流程的多智能体模式在需要演绎推理的问题上表现优于直接提示方法。

Conclusion: 多智能体推理模式能够有效提升太阳物理领域的科学推理能力，特别是在复杂演绎推理任务中。

Abstract: Scientific reasoning through Large Language Models in heliophysics involves more than just recalling facts: it requires incorporating physical assumptions, maintaining consistent units, and providing clear scientific formats through coordinated approaches. To address these challenges, we present Reasoning With a Star, a newly contributed heliophysics dataset applicable to reasoning; we also provide an initial benchmarking approach. Our data are constructed from National Aeronautics and Space Administration & University Corporation for Atmospheric Research Living With a Star summer school problem sets and compiled into a readily consumable question-and-answer structure with question contexts, reasoning steps, expected answer type, ground-truth targets, format hints, and metadata. A programmatic grader checks the predictions using unit-aware numerical tolerance, symbolic equivalence, and schema validation. We benchmark a single-shot baseline and four multi-agent patterns, finding that decomposing workflows through systems engineering principles outperforms direct prompting on problems requiring deductive reasoning rather than pure inductive recall.

</details>


### [167] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari,Parv Maheshwari,Vidisha Agarwal*

Main category: cs.AI

TL;DR: 本文对多模态思维链(Multimodal-CoT)在跨领域推理中的泛化能力进行了全面分析，发现在科学推理之外的常识推理任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索多模态思维链方法在科学问答之外的多样化领域（如常识推理）中的泛化能力，现有方法在跨领域应用中的有效性尚未充分研究。

Method: 采用Zhang等人提出的两阶段框架，将原理生成与答案推理分离，通过门控融合机制整合视觉特征与T5语言模型，并进行系统消融研究分析视觉特征、原理质量和架构选择的影响。

Result: 视觉整合显著减少了原理生成中的幻觉，但思维链推理效果在不同问题类型间差异很大，常识推理尤其具有挑战性。

Conclusion: 为多模态推理系统的实现提供了实用见解，并确定了跨领域泛化未来改进的关键方向。

Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.

</details>


### [168] [Guaranteed Optimal Compositional Explanations for Neurons](https://arxiv.org/abs/2511.20934)
*Biagio La Rosa,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 提出了第一个计算保证最优组合解释的框架，通过分解空间对齐因素、设计启发式估计和高效算法，解决了现有方法无法保证最优性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于束搜索的组合解释方法无法提供理论最优性保证，不清楚当前解释与真正最优解的接近程度。

Method: 提出了包含三个关键组件的框架：(i)识别影响空间对齐因素的分解方法；(ii)在搜索任何阶段估计对齐的启发式；(iii)首个能在可行时间内计算最优组合解释的算法。

Result: 在计算机视觉和卷积神经网络场景中，当涉及重叠概念时，10-40%的束搜索解释是次优的。提出的引导束搜索变体在运行时间上匹配或优于现有方法，同时提供更大的超参数和计算资源灵活性。

Conclusion: 该框架首次实现了保证最优的组合解释计算，揭示了现有方法的局限性，并提供了更高效的替代方案。

Abstract: While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.

</details>


### [169] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem是一个双流记忆框架，通过分别编码视觉分心模式和逻辑推理错误，让多模态大语言模型从成功和失败经验中学习，提高多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的记忆方法存在简洁性偏差，逐渐丢失关键领域知识，且只记录单模态行为轨迹，无法保留视觉注意力和逻辑推理如何共同贡献于解决方案，这与人类多模态整合的语义记忆不符。

Method: 引入ViLoMem双流记忆框架，分别编码视觉分心模式和逻辑推理错误，采用增长-精炼原则逐步积累和更新多模态语义知识，保持稳定可泛化策略同时避免灾难性遗忘。

Result: 在六个多模态基准测试中，ViLoMem持续提高pass@1准确率，显著减少重复的视觉和逻辑错误。消融实验证实了双流记忆和显式分心-幻觉分离的必要性。

Conclusion: 错误感知的多模态记忆对于终身和跨领域智能学习具有重要价值，ViLoMem证明了双流记忆框架在多模态推理中的有效性。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [170] [Even with AI, Bijection Discovery is Still Hard: The Opportunities and Challenges of OpenEvolve for Novel Bijection Construction](https://arxiv.org/abs/2511.20987)
*Davis Brown,Jesse He,Helen Jenne,Henry Kvinge,Max Vargas*

Main category: math.CO

TL;DR: 本文探讨了使用OpenEvolve系统进行组合双射发现的研究，测试了三个涉及Dyck路径的双射构造问题，发现当前前沿系统在寻找新颖的研究级双射方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 进化程序合成系统为AI辅助数学发现提供了新方法，但现有数学应用主要集中在边界建立问题上。本文旨在探索这些系统在组合双射发现领域的适用性，特别是针对需要显式构造解的问题。

Method: 使用OpenEvolve系统，该系统利用大型语言模型团队生成候选解作为人类可读代码，然后通过进化过程改进这些解。将系统应用于三个涉及Dyck路径的双射构造问题（两个已知问题和一个开放问题）。

Result: 研究发现，虽然OpenEvolve等系统作为组合学家的有价值工具显示出潜力，但寻找新颖的研究级双射对当前前沿系统来说仍然是一个具有挑战性的任务。

Conclusion: 系统在组合双射发现方面显示出前景，但人类数学家仍需要在循环中发挥作用。文章为对该领域感兴趣的研究者提供了一些经验教训。

Abstract: Evolutionary program synthesis systems such as AlphaEvolve, OpenEvolve, and ShinkaEvolve offer a new approach to AI-assisted mathematical discovery. These systems utilize teams of large language models (LLMs) to generate candidate solutions to a problem as human readable code. These candidate solutions are then 'evolved' with the goal of improving them beyond what an LLM can produce in a single shot. While existing mathematical applications have mostly focused on problems of establishing bounds (e.g., sphere packing), the program synthesis approach is well suited to any problem where the solution takes the form of an explicit construction. With this in mind, in this paper we explore the use of OpenEvolve for combinatorial bijection discovery. We describe the results of applying OpenEvolve to three bijection construction problems involving Dyck paths, two of which are known and one of which is open. We find that while systems like OpenEvolve show promise as a valuable tool for combinatorialists, the problem of finding novel, research-level bijections remains a challenging task for current frontier systems, reinforcing the need for human mathematicians in the loop. We describe some lessons learned for others in the field interested in exploring the use of these systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [171] [Data-Driven Assessment of Concrete Slab Integrity via Impact-Echo Signals and Neural Networks](https://arxiv.org/abs/2511.21080)
*Yeswanth Ravichandran,Duoduo Liao,Charan Teja Kurakula*

Main category: eess.SP

TL;DR: 提出基于机器学习的冲击回波框架，自动定位混凝土缺陷并进行多类别分类，包括浅层剥离、深层剥离、空洞和蜂窝状缺陷，准确率达73%。


<details>
  <summary>Details</summary>
Motivation: 混凝土桥面板的亚表面缺陷（如剥离、空洞、蜂窝状）严重影响耐久性，但通过视觉检查或手动敲击难以可靠检测，需要自动化无损评估方法。

Method: 将原始冲击回波信号通过FFT转换为峰值频率特征并插值为空间图，使用k-means聚类识别缺陷区域，构建空间有序的峰值频率序列，输入堆叠LSTM网络进行四类缺陷分类。

Result: 在实验室数据上训练的分类模型在实地桥面板验证中表现良好，总体准确率达到73%，证明该方法在真实耦合、噪声和环境变化下具有良好的泛化能力。

Conclusion: 该框架提高了无损评估的客观性、可扩展性和可重复性，支持智能、数据驱动的桥梁健康监测网络化应用。

Abstract: Subsurface defects such as delamination, voids, and honeycombing critically affect the durability of concrete bridge decks but are difficult to detect reliably using visual inspection or manual sounding. This paper presents a machine learning based Impact Echo (IE) framework that automates both defect localization and multi-class classification of common concrete defects. Raw IE signals from Federal Highway Administration (FHWA) laboratory slabs and in-service bridge decks are transformed via Fast Fourier Transform (FFT) into dominant peak-frequency features and interpolated into spatial maps for defect zone visualization. Unsupervised k-means clustering highlights low-frequency, defect-prone regions, while Ground Truth Masks (GTMs) derived from seeded lab defects are used to validate spatial accuracy and generate high-confidence training labels. From these validated regions, spatially ordered peak-frequency sequences are constructed and fed into a stacked Long Short-Term Memory (LSTM) network that classifies four defect types shallow delamination, deep delamination, voids, and honeycombing with 73% overall accuracy. Field validation on the bridge deck demonstrates that models trained on laboratory data generalize under realistic coupling, noise, and environmental variability. The proposed framework enhances the objectivity, scalability, and repeatability of Non-Destructive Evaluation (NDE), supporting intelligent, data-driven bridge health monitoring at a network scale.

</details>


### [172] [Phase-Aware Code-Aided EM Algorithm for Blind Channel Estimation in PSK-Modulated OFDM](https://arxiv.org/abs/2511.21340)
*Chin-Hung Chen,Ivana Nikoloska,Wim van Houtum,Yan Wu,Alex Alvarado*

Main category: eess.SP

TL;DR: 提出了一种用于OFDM系统PSK调制的全盲相位感知EM算法，通过利用解码器外信息解决EM算法中的相位模糊问题，显著降低局部收敛率。


<details>
  <summary>Details</summary>
Motivation: 解决传统盲EM算法在信道估计中由于未知相位模糊导致的局部最大值问题，传统方法无法解决这种相位模糊。

Method: 利用PSK调制的固有对称性生成有限候选模型集，通过解码器选择最可能的候选模型，利用解码器外信息作为模型证据度量。

Result: 当与简单卷积码结合时，相位感知EM算法在初始化阶段可靠地解决相位模糊，在频率选择性信道中将局部收敛率从80%降低到接近0%。

Conclusion: 该算法仅在EM初始化阶段调用一次，在后续turbo迭代中增加的计算复杂度可忽略不计，有效解决了盲信道估计中的相位模糊问题。

Abstract: This paper presents a fully blind phase-aware expectation-maximization (EM) algorithm for OFDM systems with the phase-shift keying (PSK) modulation. We address the well-known local maximum problem of the EM algorithm for blind channel estimation. This is primarily caused by the unknown phase ambiguity in the channel estimates, which conventional blind EM estimators cannot resolve. To overcome this limitation, we propose to exploit the extrinsic information from the decoder as model evidence metrics. A finite set of candidate models is generated based on the inherent symmetries of PSK modulation, and the decoder selects the most likely candidate model. Simulation results demonstrate that, when combined with a simple convolutional code, the phase-aware EM algorithm reliably resolves phase ambiguity during the initialization stage and reduces the local convergence rate from 80% to nearly 0% in frequency-selective channels with a constant phase ambiguity. The algorithm is invoked only once after the EM initialization stage, resulting in negligible additional complexity during subsequent turbo iterations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [173] [Fusion of classical and quantum kernels enables accurate and robust two-sample tests](https://arxiv.org/abs/2511.20941)
*Yu Terada,Yugo Ogio,Ken Arai,Hiroyuki Tezuka,Yu Tanaka*

Main category: quant-ph

TL;DR: 提出了一种融合经典核和量子核的混合核MMD-FUSE测试框架，在小样本和高维数据场景下显著提升测试效能。


<details>
  <summary>Details</summary>
Motivation: 解决传统核方法在小数据集上性能不佳的问题，利用量子核的独特表达能力提升两样本测试的效能。

Method: 基于最大均值差异(MMD)理论，将量子核与经典核融合，构建混合核测试策略MMD-FUSE。

Result: 在合成和真实临床数据集上的实验表明，量子核MMD-FUSE在小样本高维数据下测试效能优于经典方法，混合框架具有强鲁棒性。

Conclusion: 量子启发和混合核策略为小样本数据分析提供了更有效的统计测试工具，具有广泛应用潜力。

Abstract: Two-sample tests have been extensively employed in various scientific fields and machine learning such as evaluation on the effectiveness of drugs and A/B testing on different marketing strategies to discriminate whether two sets of samples come from the same distribution or not. Kernel-based procedures for hypothetical testing have been proposed to efficiently disentangle high-dimensional complex structures in data to obtain accurate results in a model-free way by embedding the data into the reproducing kernel Hilbert space (RKHS). While the choice of kernels plays a crucial role for their performance, little is understood about how to choose kernel especially for small datasets. Here we aim to construct a hypothetical test which is effective even for small datasets, based on the theoretical foundation of kernel-based tests using maximum mean discrepancy, which is called MMD-FUSE. To address this, we enhance the MMD-FUSE framework by incorporating quantum kernels and propose a novel hybrid testing strategy that fuses classical and quantum kernels. This approach creates a powerful and adaptive test by combining the domain-specific inductive biases of classical kernels with the unique expressive power of quantum kernels. We evaluate our method on various synthetic and real-world clinical datasets, and our experiments reveal two key findings: 1) With appropriate hyperparameter tuning, MMD-FUSE with quantum kernels consistently improves test power over classical counterparts, especially for small and high-dimensional data. 2) The proposed hybrid framework demonstrates remarkable robustness, adapting to different data characteristics and achieving high test power across diverse scenarios. These results highlight the potential of quantum-inspired and hybrid kernel strategies to build more effective statistical tests, offering a versatile tool for data analysis where sample sizes are limited.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [174] [On the Periodic Orbits of the Dual Logarithmic Derivative Operator](https://arxiv.org/abs/2511.21283)
*Xiaohang Yu,William Knottenbelt*

Main category: math.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the periodic behaviour of the dual logarithmic derivative operator $\mathcal{A}[f]=\mathrm{d}\ln f/\mathrm{d}\ln x$ in a complex analytic setting. We show that $\mathcal{A}$ admits genuinely nondegenerate period-$2$ orbits and identify a canonical explicit example. Motivated by this, we obtain a complete classification of all nondegenerate period-$2$ solutions, which are precisely the rational pairs $(c a x^{c}/(1-ax^{c}),\, c/(1-ax^{c}))$ with $ac\neq 0$. We further classify all fixed points of $\mathcal{A}$, showing that every solution of $\mathcal{A}[f]=f$ has the form $f(x)=1/(a-\ln x)$. As an illustration, logistic-type functions become pre-periodic under $\mathcal{A}$ after a logarithmic change of variables, entering the period-$2$ family in one iterate. These results give an explicit description of the low-period structure of $\mathcal{A}$ and provide a tractable example of operator-induced dynamics on function spaces.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [175] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 提出一种融合像素级数据流的硬件加速器架构，通过消除中间缓冲区，在边缘AI应用中显著减少深度可分离卷积的数据移动和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI和TinyML应用中深度可分离卷积因逐层执行导致的中间特征图传输能耗和延迟问题，克服内存墙瓶颈。

Method: 设计基于RISC-V处理器的定制功能单元，采用融合像素级数据流，通过紧密耦合流水线直接计算所有DSC阶段的单个输出像素，无需中间缓冲。

Result: 相比传统逐层执行减少87%数据移动，在FPGA上实现59.3倍加速，ASIC合成显示0.284mm²面积和910mW功耗(28nm)或1.20mm²面积和233mW功耗(40nm)。

Conclusion: 验证了在TinyML资源限制内实现零缓冲区数据流的可行性，为边缘AI加速器克服内存墙提供了有效策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [176] [A Set of Rules for Model Validation](https://arxiv.org/abs/2511.20711)
*José Camacho*

Main category: stat.ME

TL;DR: 本文提出了一套通用的模型验证规则，旨在帮助从业者制定可靠的验证计划并透明地报告结果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型的验证是评估模型对新未见数据泛化能力的过程，需要建立标准化的验证方法。

Method: 提出一套通用的模型验证规则，用于指导验证计划的制定和结果报告。

Result: 这些规则可以帮助从业者确保验证策略足够实用，公开讨论验证策略的局限性，并报告清晰可比较的性能指标。

Conclusion: 虽然没有任何验证方案是完美的，但提出的规则能够为模型验证提供实用的指导框架。

Abstract: The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [177] [RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data](https://arxiv.org/abs/2511.20974)
*Zhisheng Zheng,Xiaohang Sun,Tuan Dinh,Abhishek Yanamandra,Abhinav Jain,Zhu Liu,Sunil Hadap,Vimal Bhat,Manoj Aggarwal,Gerard Medioni,David Harwath*

Main category: eess.AS

TL;DR: RosettaSpeech是一个用于零样本语音到语音翻译的简化框架，仅使用单语语音-文本数据和机器翻译监督进行训练，无需平行语音对。


<details>
  <summary>Details</summary>
Motivation: 平行语音语料库的稀缺严重阻碍了语音到语音翻译的发展，迫使依赖复杂多阶段流水线。

Method: 使用文本作为训练时的中间桥梁，但推理时作为直接的端到端语音到语音模型，利用文本NMT模型的语言知识。

Result: 在CVSS-C测试集上，德语到英语ASR-BLEU得分25.17，西班牙语到英语29.86，相对领先系统分别提升27%和14%。单个模型可实现多对一翻译。

Conclusion: 通过依赖丰富的平行文本而非难以获取的平行语音，RosettaSpeech为更广泛语言提供了可扩展的高质量语音到语音翻译路径。

Abstract: The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.

</details>


### [178] [The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval](https://arxiv.org/abs/2511.21247)
*Jaime Garcia-Martinez,David Diaz-Guerra,John Anderson,Ricardo Falcon-Perez,Pablo Cabañas-Molero,Tuomas Virtanen,Julio J. Carabias-Orti,Pedro Vera-Candeas*

Main category: eess.AS

TL;DR: The Spheres数据集是一个专门为古典音乐领域机器学习研究设计的多音轨管弦乐录音数据集，包含柴可夫斯基《罗密欧与朱丽叶》和莫扎特《第40交响曲》等作品，提供23个麦克风录制的分离音轨和房间脉冲响应，用于音乐源分离和相关MIR任务的研究。


<details>
  <summary>Details</summary>
Motivation: 为古典音乐领域的机器学习和音乐信息检索研究提供高质量的管弦乐录音数据集，特别是针对音乐源分离任务，解决现有数据集中缺乏真实古典音乐录音的问题。

Method: 使用Colibrì Ensemble在The Spheres录音室录制超过一小时的音乐作品，包括完整作品、音阶和独奏片段，采用23个麦克风（近场、主麦克风和环境麦克风）进行多轨录音，并估计每个乐器位置的房间脉冲响应。

Result: 数据集提供了高质量的分离音轨和声学特征，基于X-UMX模型的基线评估显示了在复杂管弦乐场景中源分离的潜力和挑战，为分离、定位、去混响和沉浸式渲染等任务提供了基准。

Conclusion: The Spheres数据集为古典音乐源分离和相关MIR任务提供了有价值的资源，突显了复杂管弦乐场景中源分离的挑战，并为新方法的研究和基准测试奠定了基础。

Abstract: This paper introduces The Spheres dataset, multitrack orchestral recordings designed to advance machine learning research in music source separation and related MIR tasks within the classical music domain. The dataset is composed of over one hour recordings of musical pieces performed by the Colibrì Ensemble at The Spheres recording studio, capturing two canonical works - Tchaikovsky's Romeo and Juliet and Mozart's Symphony No. 40 - along with chromatic scales and solo excerpts for each instrument. The recording setup employed 23 microphones, including close spot, main, and ambient microphones, enabling the creation of realistic stereo mixes with controlled bleeding and providing isolated stems for supervised training of source separation models. In addition, room impulse responses were estimated for each instrument position, offering valuable acoustic characterization of the recording space. We present the dataset structure, acoustic analysis, and baseline evaluations using X-UMX based models for orchestral family separation and microphone debleeding. Results highlight both the potential and the challenges of source separation in complex orchestral scenarios, underscoring the dataset's value for benchmarking and for exploring new approaches to separation, localization, dereverberation, and immersive rendering of classical music.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [179] [The communication complexity of distributed estimation](https://arxiv.org/abs/2511.21015)
*Parikshit Gopalan,Raghu Meka,Prasad Raghavendra,Mihir Singhal,Avi Wigderson*

Main category: cs.CC

TL;DR: 本文研究了分布式估计问题，改进了随机采样方法的通信复杂度，从O(R(f)/ε²)降低到线性依赖1/ε，并对特定函数类给出了最优协议。


<details>
  <summary>Details</summary>
Motivation: 研究在Alice和Bob分别持有概率分布p和q的情况下，估计E[f(x,y)]的通信复杂度问题，该问题在草图、数据库和学习等领域有广泛应用。

Method: 设计了一种新的去偏协议，改进了随机采样方法；针对特定函数类（如Equality和Greater-than函数）提出了更好的上界；使用谱方法和discrepancy技术建立下界。

Result: 去偏协议将通信复杂度从O(R(f)/ε²)改进为线性依赖1/ε；证明了Equality和Greater-than函数协议的最优性；发现Equality在满秩布尔函数中是最简单的。

Conclusion: 提出的去偏协议对于一般函数是最优的，特定函数类的协议也是最优的，为分布式估计问题提供了通信复杂度的完整理解。

Abstract: We study an extension of the standard two-party communication model in which Alice and Bob hold probability distributions $p$ and $q$ over domains $X$ and $Y$, respectively. Their goal is to estimate \[ \mathbb{E}_{x \sim p,\, y \sim q}[f(x, y)] \] to within additive error $\varepsilon$ for a bounded function $f$, known to both parties. We refer to this as the distributed estimation problem. Special cases of this problem arise in a variety of areas including sketching, databases and learning. Our goal is to understand how the required communication scales with the communication complexity of $f$ and the error parameter $\varepsilon$.
  The random sampling approach -- estimating the mean by averaging $f$ over $O(1/\varepsilon^2)$ random samples -- requires $O(R(f)/\varepsilon^2)$ total communication, where $R(f)$ is the randomized communication complexity of $f$. We design a new debiasing protocol which improves the dependence on $1/\varepsilon$ to be linear instead of quadratic. Additionally we show better upper bounds for several special classes of functions, including the Equality and Greater-than functions. We introduce lower bound techniques based on spectral methods and discrepancy, and show the optimality of many of our protocols: the debiasing protocol is tight for general functions, and that our protocols for the equality and greater-than functions are also optimal. Furthermore, we show that among full-rank Boolean functions, Equality is essentially the easiest.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [180] [Domain-Grounded Evaluation of LLMs in International Student Knowledge](https://arxiv.org/abs/2511.20653)
*Claudinei Daitx,Haitham Amar*

Main category: cs.HC

TL;DR: 评估大型语言模型在留学咨询中的表现，重点关注准确性和幻觉问题，使用真实留学问题集进行公平比较


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多用于回答高风险的留学咨询问题，但缺乏对其可靠性和幻觉问题的系统评估，需要为教育领域提供实用的部署前审计方法

Method: 使用ApplyBoard的真实留学咨询问题集，评估模型的准确性和幻觉率，采用包含正确、部分正确、错误的评分标准，考虑领域覆盖度和相关性

Result: 提供了各模型在留学咨询中的表现对比，识别了常见失败模式（不完整、离题、无根据回答），并建立了可复用的评估协议

Conclusion: 需要系统评估LLMs在留学咨询中的可靠性，提出的评估框架可为教育领域部署LLMs提供实用的审计方法

Abstract: Large language models (LLMs) are increasingly used to answer high-stakes study-abroad questions about admissions, visas, scholarships, and eligibility. Yet it remains unclear how reliably they advise students, and how often otherwise helpful answers drift into unsupported claims (``hallucinations'').
  This work provides a clear, domain-grounded overview of how current LLMs behave in this setting. Using realistic questions set drawn from ApplyBoard's advising workflows -- an EdTech platform that supports students from discovery to enrolment -- we evaluate two essentials side by side: accuracy (is the information correct and complete?) and hallucination (does the model add content not supported by the question or domain evidence). These questions are categorized by domain scope which can be a single-domain or multi-domain -- when it must integrate evidence across areas such as admissions, visas, and scholarships.
  To reflect real advising quality, we grade answers with a simple rubric which is correct, partial, or wrong. The rubric is domain-coverage-aware: an answer can be partial if it addresses only a subset of the required domains, and it can be over-scoped if it introduces extra, unnecessary domains; both patterns are captured in our scoring as under-coverage or reduced relevance/hallucination.
  We also report measures of faithfulness and answer relevance, alongside an aggregate hallucination score, to capture relevance and usefulness. All models are tested with the same questions for a fair, head-to-head comparison.
  Our goals are to: (1) give a clear picture of which models are most dependable for study-abroad advising, (2) surface common failure modes -- where answers are incomplete, off-topic, or unsupported, and (3) offer a practical, reusable protocol for auditing LLMs before deployment in education and advising contexts.

</details>


### [181] [MMA: A Momentum Mamba Architecture for Human Activity Recognition with Inertial Sensors](https://arxiv.org/abs/2511.21550)
*Thai-Khanh Nguyen,Uyen Vo,Tan M. Nguyen,Thieu N. Vo,Trung-Hieu Le,Cuong Pham*

Main category: cs.HC

TL;DR: 提出Momentum Mamba，一种动量增强的结构化状态空间模型，通过引入二阶动力学来改进传统SSM在人类活动识别中的稳定性、鲁棒性和长序列建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度模型（CNN、RNN、Transformer）在人类活动识别中存在梯度消失/爆炸、计算成本高、难以捕捉长程依赖等问题。现有SSM模型如Mamba虽然解决了部分问题，但仅限于一阶动力学且缺乏稳定的长期记忆机制。

Method: 提出Momentum Mamba，在SSM基础上引入动量机制实现二阶动力学，包含两个扩展版本：Complex Momentum Mamba用于频率选择性记忆缩放。

Result: 在多个HAR基准测试中，相比原始Mamba和Transformer基线，在准确性、鲁棒性和收敛速度方面均取得一致提升，仅适度增加训练成本。

Conclusion: 动量增强的SSM在准确性和效率之间提供了有利的平衡，为HAR建立了可扩展的范式，并为更广泛的序列建模应用提供了有前景的框架。

Abstract: Human activity recognition (HAR) from inertial sensors is essential for ubiquitous computing, mobile health, and ambient intelligence. Conventional deep models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformers have advanced HAR but remain limited by vanishing or exloding gradients, high computational cost, and difficulty in capturing long-range dependencies. Structured state-space models (SSMs) like Mamba address these challenges with linear complexity and effective temporal modeling, yet they are restricted to first-order dynamics without stable longterm memory mechanisms. We introduce Momentum Mamba, a momentum-augmented SSM that incorporates second-order dynamics to improve stability of information flow across time steps, robustness, and long-sequence modeling. Two extensions further expand its capacity: Complex Momentum Mamba for frequency-selective memory scaling. Experiments on multiple HAR benchmarks demonstrate consistent gains over vanilla Mamba and Transformer baselines in accuracy, robustness, and convergence speed. With only moderate increases in training cost, momentum-augmented SSMs offer a favorable accuracy-efficiency balance, establishing them as a scalable paradigm for HAR and a promising principal framework for broader sequence modeling applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [182] [Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework](https://arxiv.org/abs/2511.21448)
*Rebeka Toth,Tamas Bisztray,Richard Dubniczky*

Main category: cs.CR

TL;DR: 该研究创建了一个包含钓鱼邮件、垃圾邮件和合法邮件的综合数据集，区分了人类和LLM生成的内容，并评估了LLM在识别情感和动机线索方面的能力。


<details>
  <summary>Details</summary>
Motivation: 钓鱼和垃圾邮件仍然是主要的网络安全威胁，攻击者越来越多地利用LLM制作高度欺骗性的内容，需要改进AI辅助的邮件安全系统。

Method: 构建包含钓鱼、垃圾和合法邮件的标注数据集，使用多个LLM识别情感和动机线索，选择最可靠的模型标注完整数据集，并通过LLM重述邮件来评估分类鲁棒性。

Result: LLM在钓鱼检测方面表现出色，但在区分垃圾邮件和合法邮件方面仍存在挑战。

Conclusion: 该数据集和评估框架有助于改进AI辅助的邮件安全系统，所有代码和资源都已公开以支持开放科学。

Abstract: Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing between human- and LLM-generated content. Each email is annotated with its category, emotional appeal (e.g., urgency, fear, authority), and underlying motivation (e.g., link-following, credential theft, financial fraud). We benchmark multiple LLMs on their ability to identify these emotional and motivational cues and select the most reliable model to annotate the full dataset. To evaluate classification robustness, emails were also rephrased using several LLMs while preserving meaning and intent. A state-of-the-art LLM was then assessed on its performance across both original and rephrased emails using expert-labeled ground truth. The results highlight strong phishing detection capabilities but reveal persistent challenges in distinguishing spam from legitimate emails. Our dataset and evaluation framework contribute to improving AI-assisted email security systems. To support open science, all code, templates, and resources are available on our project site.

</details>


### [183] [Readout-Side Bypass for Residual Hybrid Quantum-Classical Models](https://arxiv.org/abs/2511.20922)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Hongyang He,Hailong Jiang*

Main category: cs.CR

TL;DR: 提出了一种轻量级残差混合架构，通过将量子特征与原始输入连接来绕过量子机器学习中的测量瓶颈，在保持低量子复杂度的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习面临测量瓶颈问题——狭窄的量子到经典读出限制了性能并放大了隐私风险，需要一种既能绕过瓶颈又不增加量子复杂度的解决方案。

Method: 采用残差混合架构，在分类前将量子特征与原始输入进行连接，在量子-经典接口处建立残差连接。

Result: 在集中式和联邦式设置中均优于纯量子模型和先前的混合模型，相比量子基线准确率提升高达+55%，同时保持低通信成本和增强的隐私鲁棒性。

Conclusion: 该方法为在隐私敏感、资源受限的环境中集成量子模型提供了一条实用的近期路径，特别是在联邦边缘学习场景中。

Abstract: Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.

</details>


### [184] [MAD-DAG: Protecting Blockchain Consensus from MEV](https://arxiv.org/abs/2511.21552)
*Roi Bar-Zur,Aviv Tamar,Ittay Eyal*

Main category: cs.CR

TL;DR: MAD-DAG是一种实用的区块链协议，通过创新的账本函数和MDP建模方法，在不利条件下有效抵御自私挖矿攻击，安全阈值显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有自私挖矿防御协议Colordag无法处理网络传播优势、MEV奖励变化和贿赂矿工等不利条件，且需要不切实际的高延迟才能保证安全。

Method: 提出MAD-DAG协议，采用新型账本函数丢弃等长竞争链的内容，并使用马尔可夫决策过程建模理性矿工行为，开发保守奖励规则来获得自私挖矿收益的上界。

Result: 在存在贿赂矿工和高MEV奖励变化的情况下，MAD-DAG的安全阈值达到11%-31%，而Colordag和比特币的安全阈值为0%。

Conclusion: MAD-DAG是首个在不利条件下实用的自私挖矿防御协议，在保持可比安全性的同时，显著提升了区块链系统的抗攻击能力。

Abstract: Blockchain security is threatened by selfish mining, where a miner (operator) deviates from the protocol to increase their revenue. Selfish mining is exacerbated by adverse conditions: rushing (network propagation advantage for the selfish miner), varying block rewards due to block contents, called miner extractable value (MEV), and petty-compliant miners who accept bribes from the selfish miner.
  The state-of-the-art selfish-mining-resistant blockchain protocol, Colordag, does not treat these adverse conditions and was proven secure only when its latency is impractically high.
  We present MAD-DAG, Mutually-Assured-Destruction Directed-Acyclic-Graph, the first practical protocol to counter selfish mining under adverse conditions. MAD-DAG achieves this thanks to its novel ledger function, which discards the contents of equal-length chains competing to be the longest.
  We analyze selfish mining in both Colordag and MAD-DAG by modeling a rational miner using a Markov Decision Process (MDP). We obtain a tractable model for both by developing conservative reward rules that favor the selfish miner to yield an upper bound on selfish mining revenue. To the best of our knowledge, this is the first tractable model of selfish mining in a practical DAG-based blockchain. This enables us to obtain a lower bound on the security threshold, the minimum fraction of computational power a miner needs in order to profit from selfish mining.
  MAD-DAG withstands adverse conditions under which Colordag and Bitcoin fail, while otherwise maintaining comparable security. For example, with petty-compliant miners and high levels of block reward variability, MAD-DAG's security threshold ranges from 11% to 31%, whereas both Colordag and Bitcoin achieve 0% for all levels.

</details>


### [185] [TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data](https://arxiv.org/abs/2511.21600)
*Yizhou Zhao,Xiang Li,Peter Song,Qi Long,Weijie Su*

Main category: cs.CR

TL;DR: 提出了TAB-DRW，一种用于生成表格数据的高效鲁棒后编辑水印方案，通过在频域嵌入水印信号来确保合成数据的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI产生的高保真合成表格数据在医疗、金融等领域引发数据来源和滥用的担忧，现有水印方法存在计算成本高、处理混合数据类型困难、对后修改缺乏鲁棒性等限制。

Method: 通过Yeo-Johnson变换和标准化归一化异构特征，应用离散傅里叶变换，根据预计算的伪随机位调整自适应选择条目的虚部，并引入基于排名的伪随机位生成方法实现无存储开销的行级检索。

Result: 在五个基准表格数据集上的实验表明，TAB-DRW实现了强检测性和对常见后处理攻击的鲁棒性，同时保持高数据保真度并完全支持混合类型特征。

Conclusion: TAB-DRW为生成表格数据提供了一种高效、鲁棒的水印解决方案，解决了现有方法的局限性。

Abstract: The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [186] [Cycle Cancellation for Submodular Fractional Allocations and Applications](https://arxiv.org/abs/2511.21099)
*Chandra Chekuri,Pooja Kulkarni,Ruta Mehta,Jan Vondrak*

Main category: cs.GT

TL;DR: 本文针对子模估值的离散分配问题，提出了一个类似于循环消除引理的结果，并开发了新的近似算法来解决最大-最小公平分配、纳什社会福利和最大最小份额三个经典问题。


<details>
  <summary>Details</summary>
Motivation: 在离散分配问题中，当代理人的估值是加法时，Lenstra等人的循环消除引理在舍入算法的设计和分析中起着关键作用。本文旨在为子模估值情况证明一个类似的引理，以扩展相关算法的应用范围。

Method: 本文提出了一个循环消除算法，该算法在分数分配的支持图中移除循环，同时保证每个代理人的价值（使用多线性扩展衡量）不会减少。结合其他思想，该算法被应用于三个分配目标。

Result: 对于子模纳什社会福利问题，获得了1/5近似比；对于最大最小份额问题，通过新的简单算法获得了1/2(1-1/e)近似比；对于物品价值较小或代理人数量恒定的特殊情况，获得了紧致或最佳已知的近似算法。

Conclusion: 本文为子模估值下的离散分配问题提供了新的理论工具和算法，扩展了循环消除技术在这一领域的应用，并在多个经典分配目标上取得了改进的近似保证。

Abstract: We consider discrete allocation problem where $m$ indivisible goods are to be divided among $n$ agents. When agents' valuations are additive, the well-known cycle cancelling lemma by Lenstra, Shmoys, and Tardos plays a key role in design and analysis of rounding algorithms.
  In this paper, we prove an analogous lemma for the case of submodular valuations. Our algorithm removes cycles in the support graph of a fractional allocation while guaranteeing that each agent's value, measured using the multilinear extension, does not decrease.
  We demonstrate applications of the cycle-canceling algorithm, along with other ideas, to obtain new algorithms and results for three well-studied allocation objectives: max-min (Santa Claus problem), Nash social welfare (NSW), and maximin-share (MMS). For the submodular NSW problem, we obtain a $\frac{1}{5}$-approximation; for the MMS problem, we obtain a $\frac{1}{2}(1-1/e)$-approximation through new simple algorithms. For various special cases where the goods are "small" valued or the number of agents is constant, we obtain tight/best-known approximation algorithms. All our results are in the value-oracle model.

</details>


### [187] [Arctic Auctions, Linear Fisher Markets, and Rational Convex Programs](https://arxiv.org/abs/2511.21637)
*Vijay V. Vazirani*

Main category: cs.GT

TL;DR: 本文统一了北极拍卖和线性费舍尔市场两个基础概念，提出了北极拍卖均衡的理性凸程序表示，并开发了首个组合多项式时间算法来计算北极拍卖均衡。


<details>
  <summary>Details</summary>
Motivation: 解决复杂市场中差异化商品的高效分配问题，统一经济学和算法博弈论中的两个基础构造。

Method: 将北极拍卖均衡表示为理性凸程序，并开发组合多项式时间算法。

Result: 证明了北极拍卖均衡可由理性凸程序捕获，并获得了首个计算北极拍卖均衡的组合多项式时间算法。

Conclusion: 成功统一了北极拍卖和线性费舍尔市场，为复杂市场中差异化商品的分配提供了有效的计算解决方案。

Abstract: This paper unifies two foundational constructs from economics and algorithmic game theory, the Arctic Auction and the linear Fisher market, to address the efficient allocation of differentiated goods in complex markets. Our main contributions are showing that an equilibrium for the Arctic Auction is captured by a Rational Convex Program, and deriving the first combinatorial polynomial-time algorithm for computing Arctic Auction equilibria.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [188] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB是一个用于软件验证任务的交换格式和中间语言，基于SMT-LIB构建，旨在实现不同编程语言验证工具之间的互操作性。


<details>
  <summary>Details</summary>
Motivation: 现有的验证工具通常针对特定语言开发，但许多验证方法实际上是语言无关的。为了促进技术转移和工具复用，需要一个通用的交换格式来支持不同编程和建模语言的验证。

Method: 提出SV-LIB格式，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型。定义了程序、规范和验证见证的格式，包括正确和错误程序的见证格式。

Result: 开发了SV-LIB 1.0版本，包含设计目标、语法和非正式语义。该格式易于解析，并能集成到现有基于SMT求解器的验证基础设施中。

Conclusion: SV-LIB为软件验证工具提供了通用的中间语言和交换格式，支持验证见证的独立验证和工具复用。未来版本计划增加形式语义和对并发性的扩展。

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [189] [The Human Brain as a Combinatorial Complex](https://arxiv.org/abs/2511.20692)
*Valentina Sánchez,Çiçek Güven,Koen Haak,Theodore Papamarkou,Gonzalo Nápoles,Marie Šafář Postma*

Main category: q-bio.NC

TL;DR: 提出从fMRI时间序列数据构建组合复形的框架，通过信息论度量捕获成对和高阶神经相互作用，连接拓扑深度学习和网络神经科学。


<details>
  <summary>Details</summary>
Motivation: 当前基于图的大脑网络表示系统性地忽略了表征神经复杂性的高阶依赖关系，而信息处理通常涉及无法分解为成对关系的协同相互作用。

Method: 使用fMRI信号计算的O信息和S信息度量构建数据驱动的组合复形，保留基于协同依赖的成对连接和高阶单元（如三元组、四元组）。

Result: 使用NetSim模拟作为受控概念验证数据集，展示了组合复形构建流程，证明神经时间序列中的成对和高阶依赖关系可以在统一结构中量化和表示。

Conclusion: 该工作提供了一个大脑网络表示框架，保留了传统图方法不可见的基本高阶结构，并使拓扑深度学习架构能够应用于神经数据。

Abstract: We propose a framework for constructing combinatorial complexes (CCs) from fMRI time series data that captures both pairwise and higher-order neural interactions through information-theoretic measures, bridging topological deep learning and network neuroscience. Current graph-based representations of brain networks systematically miss the higher-order dependencies that characterize neural complexity, where information processing often involves synergistic interactions that cannot be decomposed into pairwise relationships. Unlike topological lifting approaches that map relational structures into higher-order domains, our method directly constructs CCs from statistical dependencies in the data. Our CCs generalize graphs by incorporating higher-order cells that represent collective dependencies among brain regions, naturally accommodating the multi-scale, hierarchical nature of neural processing. The framework constructs data-driven combinatorial complexes using O-information and S-information measures computed from fMRI signals, preserving both pairwise connections and higher-order cells (e.g., triplets, quadruplets) based on synergistic dependencies. Using NetSim simulations as a controlled proof-of-concept dataset, we demonstrate our CC construction pipeline and show how both pairwise and higher-order dependencies in neural time series can be quantified and represented within a unified structure. This work provides a framework for brain network representation that preserves fundamental higher-order structure invisible to traditional graph methods, and enables the application of topological deep learning (TDL) architectures to neural data.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [190] [$δ$-core subsampling, strong collapses and TDA](https://arxiv.org/abs/2511.20954)
*Elias Gabriel Minian*

Main category: cs.CG

TL;DR: 提出基于单纯复形强坍塌的拓扑数据分析子采样方法，在保持全局和局部拓扑特征的同时显著降低持续同调计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统持续同调计算在大规模点云数据上计算复杂度高，需要开发能保持拓扑特征的高效子采样方法。

Method: 基于单纯复形的强坍塌技术构建子采样，通过选择尺度参数δ来平衡计算效率和拓扑保真度。

Result: 在合成和真实数据集上的实验表明，该方法相比其他子采样技术能提供更好的持续同调近似效果。

Conclusion: 强坍塌子采样方法为拓扑数据分析提供了一种计算高效且拓扑保真的实用工具。

Abstract: We introduce a subsampling method for topological data analysis based on strong collapses of simplicial complexes. Given a point cloud and a scale parameter $δ$, we construct a subsampling that preserves both global and local topological features while significantly reducing computational complexity of persistent homology calculations. We illustrate the effectiveness of our approach through experiments on synthetic and real datasets, showing improved persistence approximations compared to other subsampling techniques.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [191] [Learning Multi-Order Block Structure in Higher-Order Networks](https://arxiv.org/abs/2511.21350)
*Kazuki Nakajima,Yuya Sasaki,Takeaki Uno,Masaki Aida*

Main category: cs.SI

TL;DR: 提出了一个多阶块结构框架，通过引入不同亲和模式来管理不同交互阶的子集，解决了高阶网络中单阶模型可能忽略阶依赖结构细节的问题。


<details>
  <summary>Details</summary>
Motivation: 高阶网络（超图）在建模涉及三个或更多实体的交互时很重要。现有的单阶随机块模型简化了计算复杂度，但可能忽略了阶依赖的结构细节。

Method: 基于多阶随机块模型，搜索交互阶集合的最优划分，以最大化超链接预测的样本外性能。

Result: 分析多个真实世界网络发现，多阶块结构普遍存在。考虑它们不仅能获得比单阶模型更好的预测性能，还能揭示更清晰、更可解释的中尺度组织。

Conclusion: 阶依赖机制是真实世界高阶网络中尺度组织的一个关键特征。

Abstract: Higher-order networks, naturally described as hypergraphs, are essential for modeling real-world systems involving interactions among three or more entities. Stochastic block models offer a principled framework for characterizing mesoscale organization, yet their extension to hypergraphs involves a trade-off between expressive power and computational complexity. A recent simplification, a single-order model, mitigates this complexity by assuming a single affinity pattern governs interactions of all orders. This universal assumption, however, may overlook order-dependent structural details. Here, we propose a framework that relaxes this assumption by introducing a multi-order block structure, in which different affinity patterns govern distinct subsets of interaction orders. Our framework is based on a multi-order stochastic block model and searches for the optimal partition of the set of interaction orders that maximizes out-of-sample hyperlink prediction performance. Analyzing a diverse range of real-world networks, we find that multi-order block structures are prevalent. Accounting for them not only yields better predictive performance over the single-order model but also uncovers sharper, more interpretable mesoscale organization. Our findings reveal that order-dependent mechanisms are a key feature of the mesoscale organization of real-world higher-order networks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [192] [NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities](https://arxiv.org/abs/2511.20848)
*Tasha Kim,Yingke Wang,Hanvit Cho,Alex Hodges*

Main category: cs.RO

TL;DR: NOIR 2.0是一个增强版的脑机接口系统，通过EEG信号将人类意图转化为机器人指令，实现了更快的脑信号解码算法和少量样本的机器人学习能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够通过脑信号控制机器人执行日常任务的智能系统，提高人机交互的自然性和效率。

Method: 使用EEG脑电图技术解码人类意图，结合少量样本机器人学习算法和基础模型，实现个性化适应和意图预测。

Result: 任务完成时间减少46%，总体人类时间减少65%，学习样本需求从15个演示减少到1个演示。

Conclusion: NOIR 2.0显著提升了脑机接口系统的性能和效率，为脑控机器人技术提供了更实用的解决方案。

Abstract: Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.

</details>


### [193] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: TraceGen提出了一种符号化的3D轨迹空间表示，能够从跨平台、跨环境的视频中学习，仅需少量演示就能让机器人适应新任务。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中的小数据问题，利用丰富的人类和其他机器人视频资源，克服不同平台、摄像头和环境之间的差异。

Method: 开发TraceGen世界模型，在3D轨迹空间而非像素空间预测未来运动；构建TraceForge数据管道，将异构视频转换为一致的3D轨迹，创建包含123K视频和1.8M三元组的数据集。

Result: 仅需5个目标机器人视频即可达到80%成功率，推理速度比现有视频世界模型快50-600倍；仅用5个手机拍摄的人类演示视频也能在真实机器人上达到67.5%成功率。

Conclusion: TraceGen通过3D轨迹空间表示实现了高效的跨平台知识迁移，无需依赖物体检测器或繁重的像素空间生成，为小样本机器人学习提供了有效解决方案。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [194] [PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach](https://arxiv.org/abs/2511.20703)
*Udari Madhushani Sehwag,Shayan Shabihi,Alex McAvoy,Vikash Sehwag,Yuancheng Xu,Dalton Towers,Furong Huang*

Main category: cs.CY

TL;DR: PropensityBench是一个评估大语言模型在高风险能力下滥用倾向的基准框架，包含5,874个场景和6,648个工具，涵盖网络安全、自我增殖、生物安全和化学安全四个高风险领域。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估主要测试模型的能力，但忽略了模型在获得高风险能力后是否会滥用的倾向性，这留下了关键的安全盲点。

Method: 通过代理工具模拟危险能力，在受控的代理环境中评估模型在不同操作压力下的选择行为，包括资源稀缺和获得更多自主权等情况。

Result: 在开源和专有前沿模型中发现了9个令人担忧的倾向性迹象：模型在压力下频繁选择高风险工具，尽管它们缺乏独立执行这些操作的能力。

Conclusion: 需要从静态能力审计转向动态倾向性评估，作为安全部署前沿AI系统的先决条件。

Abstract: Recent advances in Large Language Models (LLMs) have sparked concerns over their potential to acquire and misuse dangerous or high-risk capabilities, posing frontier risks. Current safety evaluations primarily test for what a model \textit{can} do - its capabilities - without assessing what it $\textit{would}$ do if endowed with high-risk capabilities. This leaves a critical blind spot: models may strategically conceal capabilities or rapidly acquire them, while harboring latent inclinations toward misuse. We argue that $\textbf{propensity}$ - the likelihood of a model to pursue harmful actions if empowered - is a critical, yet underexplored, axis of safety evaluation. We present $\textbf{PropensityBench}$, a novel benchmark framework that assesses the proclivity of models to engage in risky behaviors when equipped with simulated dangerous capabilities using proxy tools. Our framework includes 5,874 scenarios with 6,648 tools spanning four high-risk domains: cybersecurity, self-proliferation, biosecurity, and chemical security. We simulate access to powerful capabilities via a controlled agentic environment and evaluate the models' choices under varying operational pressures that reflect real-world constraints or incentives models may encounter, such as resource scarcity or gaining more autonomy. Across open-source and proprietary frontier models, we uncover 9 alarming signs of propensity: models frequently choose high-risk tools when under pressure, despite lacking the capability to execute such actions unaided. These findings call for a shift from static capability audits toward dynamic propensity assessments as a prerequisite for deploying frontier AI systems safely. Our code is available at https://github.com/scaleapi/propensity-evaluation.

</details>


### [195] [A review on data fusion in multimodal learning analytics and educational data mining](https://arxiv.org/abs/2511.20871)
*Wilson Chango,Juan A. Lara,Rebeca Cerezo,Cristóbal Romero*

Main category: cs.CY

TL;DR: 本文综述了智能学习环境中多模态学习分析的数据融合方法，探讨了当前研究现状、主要融合数据类型、技术方法以及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 智能学习环境产生大量多模态学生数据，需要正确应用数据融合技术来整合这些数据源，以发现新知识并更好地理解学习过程。

Method: 通过文献综述方法，回顾了教育数据挖掘和学习分析领域的主要出版物，分析了不同类型教育数据的融合方法和技术。

Result: 总结了当前多模态学习分析数据融合的研究现状，包括主要融合数据类型（音频、视频、眼动追踪等）和使用的融合技术。

Conclusion: 数据融合在教育分析中具有重要作用，但仍面临技术挑战和开放性问题，需要进一步研究来推进这一领域的发展。

Abstract: The new educational models such as smart learning environments use of digital and context-aware devices to facilitate the learning process. In this new educational scenario, a huge quantity of multimodal students' data from a variety of different sources can be captured, fused, and analyze. It offers to researchers and educators a unique opportunity of being able to discover new knowledge to better understand the learning process and to intervene if necessary. However, it is necessary to apply correctly data fusion approaches and techniques in order to combine various sources of multimodal learning analytics (MLA). These sources or modalities in MLA include audio, video, electrodermal activity data, eye-tracking, user logs, and click-stream data, but also learning artifacts and more natural human signals such as gestures, gaze, speech, or writing. This survey introduces data fusion in learning analytics (LA) and educational data mining (EDM) and how these data fusion techniques have been applied in smart learning. It shows the current state of the art by reviewing the main publications, the main type of fused educational data, and the data fusion approaches and techniques used in EDM/LA, as well as the main open problems, trends, and challenges in this specific research area.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [196] [Lattice-to-total thermal conductivity ratio: a phonon-glass electron-crystal descriptor for data-driven thermoelectric design](https://arxiv.org/abs/2511.21213)
*Yifan Sun,Zhi Li,Tetsuya Imamura,Yuji Ohishi,Chris Wolverton,Ken Kurosaki*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究开发了一个机器学习框架，通过联合预测晶格和电子热导率来加速热电材料的发现和优化，发现高性能热电材料不仅需要低热导率，还需要晶格热导率占总热导率约0.5的比例。


<details>
  <summary>Details</summary>
Motivation: 加速发现高性能热电材料，传统方法主要关注低热导率材料，但研究发现高ZT材料还聚集在晶格热导率占总热导率约0.5的比例附近，这与声子玻璃电子晶体设计概念一致。

Method: 使用包含71,913个条目的数据集，构建了两个机器学习模型分别预测晶格和电子热导率，联合提供总热导率和晶格热导率比例，用于筛选104,567种化合物。

Result: 筛选出2,522种超低热导率候选材料，案例研究表明该框架能可靠提供优化策略，通过建议新的掺杂剂和合金使原始材料向κL/κ接近0.5的方向优化。

Conclusion: 通过将快速筛选与PGEC引导的优化相结合，该数据驱动框架有效弥合了材料发现与性能提升之间的关键差距。

Abstract: Thermoelectrics (TEs) are promising candidates for energy harvesting with performance quantified by figure of merit, $ZT$. To accelerate the discovery of high-$ZT$ materials, efforts have focused on identifying compounds with low thermal conductivity $κ$. Using a curated dataset of 71,913 entries, we show that high-$ZT$ materials reside not only in the low-$κ$ regime but also cluster near a lattice-to-total thermal conductivity ratio ($κ_\mathrm{L}/κ$) of approximately 0.5, consistent with the phonon-glass electron-crystal design concept. Building on this insight, we construct a framework consisting of two machine learning models for the lattice and electronic components of thermal conductivity that jointly provide both $κ$ and $κ_\mathrm{L}/κ$ for screening and guiding the optimization of TE materials. Among 104,567 compounds screened, our models identify 2,522 ultralow-$κ$ candidates. Follow-up case studies demonstrate that this framework can reliably provide optimization strategies by suggesting new dopants and alloys that shift pristine materials toward the $κ_\mathrm{L}/κ$ approaching 0.5 regime. Ultimately, by integrating rapid screening with PGEC-guided optimization, our data-driven framework effectively bridges the critical gap between materials discovery and performance enhancement.

</details>
