<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 72]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.ET](#cs.ET) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Interactive Text-to-SQL via Expected Information Gain for Disambiguation](https://arxiv.org/abs/2507.06467)
*Luyu Qiu,Jianing Li,Chi Su,Lei Chen*

Main category: cs.DB

TL;DR: 论文提出了一种交互式Text-to-SQL框架，通过概率推理和多候选查询解决自然语言查询的歧义问题，利用用户交互减少不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL系统在处理复杂数据库时因自然语言歧义易产生错误，缺乏交互机制。

Method: 提出基于概率推理的框架，生成多个候选SQL查询，通过用户交互和期望信息增益选择最佳澄清问题。

Result: 系统能够动态减少SQL查询的不确定性，提高翻译准确性。

Conclusion: 交互式框架有效解决了自然语言歧义问题，提升了Text-to-SQL系统的实用性。

Abstract: Relational databases are foundational to numerous domains, including business
intelligence, scientific research, and enterprise systems. However, accessing
and analyzing structured data often requires proficiency in SQL, which is a
skill that many end users lack. With the development of Natural Language
Processing (NLP) technology, the Text-to-SQL systems attempt to bridge this gap
by translating natural language questions into executable SQL queries via an
automated algorithm. Yet, when operating on complex real-world databases, the
Text-to-SQL systems often suffer from ambiguity due to natural ambiguity in
natural language queries. These ambiguities pose a significant challenge for
existing Text-to-SQL translation systems, which tend to commit early to a
potentially incorrect interpretation. To address this, we propose an
interactive Text-to-SQL framework that models SQL generation as a probabilistic
reasoning process over multiple candidate queries. Rather than producing a
single deterministic output, our system maintains a distribution over possible
SQL outputs and seeks to resolve uncertainty through user interaction. At each
interaction step, the system selects a branching decision and formulates a
clarification question aimed at disambiguating that aspect of the query.
Crucially, we adopt a principled decision criterion based on Expected
Information Gain to identify the clarification that will, in expectation, most
reduce the uncertainty in the SQL distribution.

</details>


### [2] [QUEST: Query Optimization in Unstructured Document Analysis](https://arxiv.org/abs/2507.06515)
*Zhaoze Sun,Qiyan Deng,Chengliang Chai,Kaisen Jin,Xinyu Guo,Han Han,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.DB

TL;DR: QUEST提出了一种针对非结构化文档分析的优化系统，通过索引、证据增强检索和实例优化查询执行策略，显著降低LLM成本并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在处理非结构化文档时，LLM提取操作成为性能瓶颈，传统数据库优化方法无法有效降低LLM成本。

Method: QUEST引入索引策略减少提取操作成本，设计证据增强检索策略避免遗漏相关文本，并为不同文档生成定制化查询执行计划。

Result: 在三个真实数据集上，QUEST实现了30%-6倍的成本节省，F1分数提升10%-27%。

Conclusion: QUEST通过创新优化策略，显著提升了非结构化文档分析的效率和准确性。

Abstract: Most recently, researchers have started building large language models (LLMs)
powered data systems that allow users to analyze unstructured text documents
like working with a database because LLMs are very effective in extracting
attributes from documents. In such systems, LLM-based extraction operations
constitute the performance bottleneck of query execution due to the high
monetary cost and slow LLM inference. Existing systems typically borrow the
query optimization principles popular in relational databases to produce query
execution plans, which unfortunately are ineffective in minimizing LLM cost. To
fill this gap, we propose QUEST, which features a bunch of novel optimization
strategies for unstructured document analysis. First, we introduce an
index-based strategy to minimize the cost of each extraction operation. With
this index, QUEST quickly retrieves the text segments relevant to the target
attributes and only feeds them to LLMs. Furthermore, we design an
evidence-augmented retrieval strategy to reduce the possibility of missing
relevant segments. Moreover, we develop an instance-optimized query execution
strategy: because the attribute extraction cost could vary significantly
document by document, QUEST produces different plans for different documents.
For each document, QUEST produces a plan to minimize the frequency of attribute
extraction. The innovations include LLM cost-aware operator ordering strategies
and an optimized join execution approach that transforms joins into filters.
Extensive experiments on 3 real-world datasets demonstrate the superiority of
QUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%
compared with state-of-the-art baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Designing Parallel Algorithms for Community Detection using Arachne](https://arxiv.org/abs/2507.06471)
*Fuhuan Li,Zhihui Du,David A. Bader*

Main category: cs.DC

TL;DR: 本文介绍了基于Arachne框架的并行社区检测算法（Label Propagation和Louvain），显著提升了性能，速度远超NetworkX和igraph，并与NetworKit竞争。


<details>
  <summary>Details</summary>
Motivation: 图数据在各领域的增长需要高效、可扩展的社区检测算法。

Method: 在Arachne框架中并行实现Label Propagation和Louvain算法。

Result: 实验显示，Arachne方法速度提升显著（最高710倍于NetworkX，75倍于igraph，12倍于NetworKit），并分析了并行Louvain算法的扩展性。

Conclusion: Arachne框架及其社区检测实现开源可用，性能优越。

Abstract: The rise of graph data in various fields calls for efficient and scalable
community detection algorithms. In this paper, we present parallel
implementations of two widely used algorithms: Label Propagation and Louvain,
specifically designed to leverage the capabilities of Arachne which is a
Python-accessible, open-source framework for large-scale graph analysis. Our
implementations achieve substantial speedups over existing Python-based tools
like NetworkX and igraph, which lack efficient parallelization, and are
competitive with parallel frameworks such as NetworKit. Experimental results
show that Arachne-based methods outperform these baselines, achieving speedups
of up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit.
Additionally, we analyze the scalability of our implementation under varying
thread counts, demonstrating how different phases contribute to overall
performance gains of the parallel Louvain algorithm. Arachne, including our
community detection implementation, is open-source and available at
https://github.com/Bears-R-Us/arkouda-njit .

</details>


### [4] [Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing](https://arxiv.org/abs/2507.06608)
*Xiaoxiang Shi,Colin Cai,Junjia Du,Zhanda Zhu,Xingda Wei,Zhihao Jia*

Main category: cs.DC

TL;DR: 论文提出了一种在单个GPU内动态分配资源以解耦预填充和解码阶段的方法，显著提高了吞吐量和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有预填充-解码（PD）解耦方法需要更多硬件资源，而分块预填充方法虽提高了GPU利用率，但引入了阶段干扰。论文探索是否能在单个GPU内实现解耦。

Method: 通过分析GPU资源的递减效应，动态分配单个GPU资源给预填充和解码阶段，实现解耦。

Result: Nexus系统在多种模型和工作负载下，吞吐量最高提升2.2倍，TTFT降低20倍，TBT降低2.5倍，且仅用一半GPU数量。

Conclusion: 在单个GPU内动态资源分配能有效解耦预填充和解码阶段，显著提升性能并减少硬件需求。

Abstract: Current prefill-decode (PD) disaggregation is typically deployed at the level
of entire serving engines, assigning separate GPUs to handle prefill and decode
phases. While effective at reducing latency, this approach demands more
hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode
requests within the same batch, but introduces phase interference between
prefill and decode.
  While existing PD disaggregation solutions separate the phases across GPUs,
we ask: can the same decoupling be achieved within a single serving engine? The
key challenge lies in managing the conflicting resource requirements of prefill
and decode when they share the same hardware. In this paper, we first show that
chunked prefill requests cause interference with decode requests due to their
distinct requirements for GPU resources. Second, we find that GPU resources
exhibit diminishing returns. Beyond a saturation point, increasing GPU
allocation yields negligible latency improvements. This insight enables us to
split a single GPU's resources and dynamically allocate them to prefill and
decode on the fly, effectively disaggregating the two phases within the same
GPU.
  Across a range of models and workloads, our system Nexus achieves up to 2.2x
higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also
outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x
lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using
only half the number of GPUs.

</details>


### [5] [Towards Efficient and Scalable Distributed Vector Search with RDMA](https://arxiv.org/abs/2507.06653)
*Xiangyu Zhi,Meng Chen,Xiao Yan,Baotong Lu,Hui Li,Qianxi Zhang,Qi Chen,James Cheng*

Main category: cs.DC

TL;DR: CoTra是一个分布式向量搜索系统，通过算法-系统协同设计解决计算与通信效率的冲突，显著提升了查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 单机内存和带宽限制了大向量数据集的搜索效率，需要分布式执行以扩展搜索能力。

Method: 采用聚类数据分区、异步执行和任务推送等技术，结合任务调度、通信批处理和存储格式优化。

Result: 在16台机器上，CoTra的查询吞吐量比单机提升9.8-13.4倍，比最佳基线高2.12-3.58倍。

Conclusion: CoTra通过协同设计和系统优化，有效解决了分布式向量搜索的扩展性问题。

Abstract: Similarity-based vector search facilitates many important applications such
as search and recommendation but is limited by the memory capacity and
bandwidth of a single machine due to large datasets and intensive data read. In
this paper, we present CoTra, a system that scales up vector search for
distributed execution. We observe a tension between computation and
communication efficiency, which is the main challenge for good scalability,
i.e., handling the local vectors on each machine independently blows up
computation as the pruning power of vector index is not fully utilized, while
running a global index over all machines introduces rich data dependencies and
thus extensive communication. To resolve such tension, we leverage the fact
that vector search is approximate in nature and robust to asynchronous
execution. In particular, we run collaborative vector search over the machines
with algorithm-system co-designs including clustering-based data partitioning
to reduce communication, asynchronous execution to avoid communication stall,
and task push to reduce network traffic. To make collaborative search
efficient, we introduce a suite of system optimizations including task
scheduling, communication batching, and storage format. We evaluate CoTra on
real datasets and compare with four baselines. The results show that when using
16 machines, the query throughput of CoTra scales to 9.8-13.4x over a single
machine and is 2.12-3.58x of the best-performing baseline at 0.95 recall@10.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [Parallel Batch-Dynamic Coreness Decomposition with Worst-Case Guarantees](https://arxiv.org/abs/2507.06334)
*Mohsen Ghaffari,Jaehyun Koo*

Main category: cs.DS

TL;DR: 提出首个并行批量动态算法，用于近似核心分解，具有最坏情况下的更新时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法仅支持摊销时间复杂度的问题，提供更高效的批量处理能力。

Method: 设计并行算法，处理批量边插入和删除，确保最坏情况下的工作量和深度均为多项式对数级别。

Result: 算法在给定处理器数量下，批量处理时间接近最优，仅差对数因子。

Conclusion: 该算法在性能和理论上优于现有方法，填补了批量动态核心分解的空白。

Abstract: We present the first parallel batch-dynamic algorithm for approximating
coreness decomposition with worst-case update times. Given any batch of edge
insertions and deletions, our algorithm processes all these updates in $
\text{poly}(\log n)$ depth, using a worst-case work bound of $b\cdot
\text{poly}(\log n)$ where $b$ denotes the batch size. This means the batch
gets processed in $\tilde{O}(b/p)$ time, given $p$ processors, which is optimal
up to logarithmic factors. Previously, an algorithm with similar guarantees was
known by the celebrated work of Liu, Shi, Yu, Dhulipala, and Shun [SPAA'22],
but with the caveat of the work bound, and thus the runtime, being only
amortized.

</details>


### [7] [Parallel Batch-Dynamic Algorithms for Spanners, and Extensions](https://arxiv.org/abs/2507.06338)
*Mohsen Ghaffari,Jaehyun Koo*

Main category: cs.DS

TL;DR: 论文提出了首个并行批量动态算法，用于计算图的生成器和稀疏化器，支持高效的边插入和删除操作。


<details>
  <summary>Details</summary>
Motivation: 解决在动态图中高效维护生成器和稀疏化器的问题，以支持大规模图分析的实时需求。

Method: 提出并行批量动态算法，处理边插入和删除，使用多对数深度和接近线性的摊销工作量。

Result: 1. 维护(2k-1)拉伸的生成器；2. 维护稀疏生成器；3. 维护t-束生成器以支持稀疏化器。

Conclusion: 算法在动态图中高效维护生成器和稀疏化器，为大规模图分析提供了实用工具。

Abstract: This paper presents the first parallel batch-dynamic algorithms for computing
spanners and sparsifiers. Our algorithms process any batch of edge insertions
and deletions in an $n$-node undirected graph, in $\text{poly}(\log n)$ depth
and using amortized work near-linear in the batch size. Our concrete results
are as follows:
  - Our base algorithm maintains a spanner with $(2k-1)$ stretch and
$\tilde{O}(n^{1+1/k})$ edges, for any $k\geq 1$.
  - Our first extension maintains a sparse spanner with only $O(n)$ edges, and
$\tilde{O}(\log n)$ stretch.
  - Our second extension maintains a $t$-bundle of spanners -- i.e., $t$
spanners, each of which is the spanner of the graph remaining after removing
the previous ones -- and allows us to maintain cut/spectral sparsifiers with
$\tilde{O}(n)$ edges.

</details>


### [8] [Multi-Queue SSD I/O Modeling & Its Implications for Data Structure Design](https://arxiv.org/abs/2507.06349)
*Erin Ransom,Andrew Lim,Michael Mitzenmacher*

Main category: cs.DS

TL;DR: 论文提出了一种新的存储抽象模型MQSSD，旨在更准确地反映现代多队列固态硬盘的性能特征，并通过实验验证其在优化LSM树存储引擎中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有存储性能模型（如DAM模型）在现代存储硬件上的准确性和实用性已显著降低，需要更精确的抽象模型来优化外部内存算法和数据结构设计。

Method: 基于现代多队列SSD的关键性能特征，提出MQSSD模型，并通过实际硬件验证其特性。将其应用于LSM树存储引擎优化，强调并发访问的重要性。

Result: MQSSD模型比传统模型更准确地抽象现代硬件性能，实验验证其在RocksDB中的应用效果。

Conclusion: MQSSD模型提供了对现代硬件更准确的抽象，有助于更深入的洞察和优化。

Abstract: Understanding the performance profiles of storage devices and how best to
utilize them has always been non-trivial due to factors such as seek times,
caching, scheduling, concurrent access, flash wear-out, and garbage collection.
However, analytical frameworks that provide simplified abstractions of storage
performance can still be accurate enough to evaluate external memory algorithms
and data structures at the design stage. For example, the Disk Access Machine
(DAM) model assumes that a storage device transfers data in fixed-size blocks
of size B and that all transfers have unit latency. This abstraction is already
sufficient to explain some of the benefits of data structures such as B-trees
and Log-Structured Merge trees (LSM trees); however, storage technology
advances have significantly reduced current models' accuracy and utility.
  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new
storage abstraction. This model builds upon previous models and aims to more
accurately represent the performance characteristics of modern storage
hardware. We identify key performance-critical aspects of modern multi-queue
solid-state drives on which we base our model and demonstrate these
characteristics on actual hardware. We then show how our model can be applied
to LSM-tree-based storage engines to optimize them for modern storage hardware.
We highlight that leveraging concurrent access is crucial for fully utilizing
the high throughput of multi-queue SSDs, enabling designs that may appear
counterintuitive under traditional paradigms We then validate these insights
through experiments using Facebook's LSM-tree-based key-value store, RocksDB.
We conclude that the MQSSD model offers a more accurate abstraction of modern
hardware than previous models, allowing for greater insight and optimization.

</details>


### [9] [Prediction-Augmented Mechanism Design for Weighted Facility Location](https://arxiv.org/abs/2507.06509)
*Yangguang Shi,Zhenyu Xue*

Main category: cs.DS

TL;DR: 本文提出了一种预测增强的算法框架，用于在非均匀权重的战略代理中平衡一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 经典的无权重设施选址问题假设所有代理的重要性相同，但实际场景中代理的权重可能不同，因此需要研究加权设施选址问题。

Method: 通过一种归约技术，识别代表性实例并将其他位置映射到这些实例上，设计了一种策略证明机制。

Result: 证明了在加权设置下，存在一种策略证明机制，能够实现有界的一致性和鲁棒性保证。

Conclusion: 在加权设施选址问题中，即使有完全预测，也无法通过策略证明的确定性机制同时达到最优一致性和鲁棒性。

Abstract: Facility location is fundamental in operations research, mechanism design,
and algorithmic game theory, with applications ranging from urban
infrastructure planning to distributed systems. Recent research in this area
has focused on augmenting classic strategyproof mechanisms with predictions to
achieve an improved performance guarantee against the uncertainty under the
strategic environment. Previous work has been devoted to address the trade-off
obstacle of balancing the consistency (near-optimality under accurate
predictions) and robustness (bounded inefficiency under poor predictions)
primarily in the unweighted setting, assuming that all agents have the same
importance. However, this assumption may not be true in some practical
scenarios, leading to research of weighted facility location problems.
  The major contribution of the current work is to provide a prediction
augmented algorithmic framework for balancing the consistency and robustness
over strategic agents with non-uniform weights. In particular, through a
reduction technique that identifies a subset of \emph{representative} instances
and maps the other given locations to the representative ones, we prove that
there exists a \emph{strategyproof} mechanism achieving a bounded consistency
guarantee of $\frac{\sqrt{(1+c)^2W^2_{\min}+(1-c)^2W^2_{\max}}}{(1+c)W_{\min}}$
and a bounded robustness guarantee of
$\frac{\sqrt{(1-c)^2W^2_{\min}+(1+c)^2W^2_{\max}}}{(1-c)W_{\min}}$ in weighted
settings, where $c$ can be viewed as a parameter to make a trade-off between
the consistency and robustness and $W_{\min}$ and $W_{\max}$ denote the minimum
and maximum agents' weight. We also proved that there is no strategyproof
deterministic mechanism that reach $1$-consistency and $O\left( n \cdot
\frac{W_{\max}}{W_{\min}} \right)$-robustness in weighted FLP, even with fully
predictions of all agents.

</details>


### [10] [Faster Algorithms for $(2k-1)$-Stretch Distance Oracles](https://arxiv.org/abs/2507.06721)
*Avi Kadria,Liam Roditty*

Main category: cs.DS

TL;DR: 本文提出了三种新算法，用于构建具有$(2k-1)$-stretch的距离预言机，改进了现有算法的时间复杂度，并在特定条件下实现了线性时间构造。


<details>
  <summary>Details</summary>
Motivation: 解决现有距离预言机构造算法在高密度图中的时间复杂度过高问题，尤其是针对$k>2$的情况。

Method: 提出三种新算法：第一种算法在特定条件下实现亚二次时间构造；第二种算法在近线性时间内完成构造；第三种算法针对大$k$值优化。

Result: 第一种算法在$2<k<6$时首次实现真正亚二次时间构造；第二种算法在$3<k<6$和$k\geq6$时分别改进现有结果；第三种算法在$k\geq16$时优于现有方法。

Conclusion: 这些算法显著改进了距离预言机的构造效率，解决了多个开放性问题，并为高密度图提供了更优的解决方案。

Abstract: Let $G=(V, E)$ be an undirected $n$-vertices $m$-edges graph with
non-negative edge weights. In this paper, we present three new algorithms for
constructing a $(2k-1)$-stretch distance oracle with $O(n^{1+\frac{1}{k}})$
space. The first algorithm runs in $\Ot(\max(n^{1+2/k},
m^{1-\frac{1}{k-1}}n^{\frac{2}{k-1}}))$ time, and improves upon the
$\Ot(\min(mn^{\frac{1}{k}},n^2))$ time of Thorup and Zwick [STOC 2001, JACM
2005] and Baswana and Kavitha [FOCS 2006, SICOMP 2010], for every $k > 2$ and
$m=\Omega(n^{1+\frac{1}{k}+\eps})$. This yields the first truly subquadratic
time construction for every $2 < k < 6$, and nearly resolves the open problem
posed by Wulff-Nilsen [SODA 2012] on the existence of such constructions.
  The two other algorithms have a running time of the form $\Ot(m+n^{1+f(k)})$,
which is near linear in $m$ if $m=\Omega(n^{1+f(k)})$, and therefore optimal in
such graphs. One algorithm runs in $\Ot(m+n^{\frac32+\frac{3}{4k-6}})$-time,
which improves upon the $\Ot(n^2)$-time algorithm of Baswana and Kavitha [FOCS
2006, SICOMP 2010], for $3 < k < 6$, and upon the
$\Ot(m+n^{\frac{3}{2}+\frac{2}{k}+O(k^{-2})})$-time algorithm of Wulff-Nilsen
[SODA 2012], for every $k\geq 6$. This is the first linear time algorithm for
constructing a $7$-stretch distance oracle and a $9$-stretch distance oracle,
for graphs with truly subquadratic density.\footnote{with $m=n^{2-\eps}$ for
some $\eps > 0$.} The other algorithm runs in
$\Ot(\sqrt{k}m+kn^{1+\frac{2\sqrt{2}}{\sqrt{k}}})$ time, (and hence relevant
only for $k\ge 16$), and improves upon the
$\Ot(\sqrt{k}m+kn^{1+\frac{2\sqrt{6}}{\sqrt{k}}+O(k^{-1})})$ time algorithm of
Wulff-Nilsen [SODA 2012] (which is relevant only for $k\ge 96$). ...

</details>


### [11] [Faster Estimation of the Average Degree of a Graph Using Random Edges and Structural Queries](https://arxiv.org/abs/2507.06925)
*Lorenzo Beretta,Deeparnab Chakrabarty,C. Seshadhri*

Main category: cs.DS

TL;DR: 本文研究了在标准图访问模型中设计子线性算法来估计图的平均度数的问题，提出了利用结构查询（如边对查询和全邻域访问查询）的新算法，并分析了在未知顶点数情况下的限制。


<details>
  <summary>Details</summary>
Motivation: 重新审视图平均度数估计问题，探索在标准访问模型和全邻域访问模型中更高效的子线性算法，同时研究顶点数未知时的限制。

Method: 提出利用边对查询和全邻域访问查询的新算法，分别在标准模型和全邻域模型中实现查询复杂度为$\widetilde{O}(n^{1/4})$和$\widetilde{O}(n^{1/5})$。

Result: 在标准模型中，算法查询复杂度为$\widetilde{O}(n^{1/4})$；在全邻域模型中为$\widetilde{O}(n^{1/5})$。同时证明了顶点数未知时结构查询无效。

Conclusion: 结构查询（如边对查询和全邻域访问）显著提升了平均度数估计的效率，但在顶点数未知时无法发挥作用。

Abstract: We revisit the problem of designing sublinear algorithms for estimating the
average degree of an $n$-vertex graph. The standard access model for graphs
allows for the following queries: sampling a uniform random vertex, the degree
of a vertex, sampling a uniform random neighbor of a vertex, and ``pair
queries'' which determine if a pair of vertices form an edge. In this model,
original results [Goldreich-Ron, RSA 2008; Eden-Ron-Seshadhri, SIDMA 2019] on
this problem prove that the complexity of getting
$(1+\varepsilon)$-multiplicative approximations to the average degree, ignoring
$\varepsilon$-dependencies, is $\Theta(\sqrt{n})$. When random edges can be
sampled, it is known that the average degree can estimated in
$\widetilde{O}(n^{1/3})$ queries, even without pair queries
[Motwani-Panigrahy-Xu, ICALP 2007; Beretta-Tetek, TALG 2024].
  We give a nearly optimal algorithm in the standard access model with random
edge samples. Our algorithm makes $\widetilde{O}(n^{1/4})$ queries exploiting
the power of pair queries. We also analyze the ``full neighborhood access"
model wherein the entire adjacency list of a vertex can be obtained with a
single query; this model is relevant in many practical applications. In a
weaker version of this model, we give an algorithm that makes
$\widetilde{O}(n^{1/5})$ queries. Both these results underscore the power of
{\em structural queries}, such as pair queries and full neighborhood access
queries, for estimating the average degree. We give nearly matching lower
bounds, ignoring $\varepsilon$-dependencies, for all our results.
  So far, almost all algorithms for estimating average degree assume that the
number of vertices, $n$, is known. Inspired by [Beretta-Tetek, TALG 2024], we
study this problem when $n$ is unknown and show that structural queries do not
help in estimating average degree in this setting.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: 研究调查了实践者对测试用例和测试套件质量属性的重要性认知及面临的挑战，发现故障检测、可用性、可维护性、可靠性和覆盖率是最重要的属性，并指出了资源效率、可重用性和简单性的意见分歧。


<details>
  <summary>Details</summary>
Motivation: 理解实践者对测试用例和测试套件质量属性的重要性认知及实际挑战，以提供更好的支持。

Method: 通过基于文献综述的问卷进行工业调查，利用LinkedIn抽样策略获取多样化的专业人员样本。

Result: 354份回复显示故障检测等属性最重要，资源效率等属性意见分歧，并识别了常见挑战如定义不足、缺乏指标等。

Conclusion: 研究结果可为学术研究方向提供指南，并鼓励企业为实践者提供更多支持。

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [13] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: 本文改进了检测Eager Test smell的规则，提出了一种新的定义和启发式方法，并通过实验验证其优于现有规则。


<details>
  <summary>Details</summary>
Motivation: 现有检测Eager Test smell的规则存在不准确和不足的问题，导致检测结果不一致，无法满足实践需求。

Method: 通过文献综述分析Eager Test smell的定义和检测规则，提出新的定义和启发式方法，并在300个Java单元测试用例中手动验证。

Result: 文献综述发现56项相关研究，新启发式方法能更精确地检测Eager Test smell，弥补现有规则的不足。

Conclusion: 新启发式方法能更准确地捕捉Eager Test smell的本质，有望解决实践中对现有规则不足的担忧。

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [14] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: 该论文首次系统研究了LLMs生成高效C语言图分析代码的能力，评估了8种先进模型，发现Claude Sonnet 4 Extended在生成高效代码方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在满足严格运行时和内存约束下生成高效C语言图分析代码的能力，填补了现有评估的空白。

Method: 采用两种方法：1) 评估LLMs生成优于现有算法的代码；2) 评估LLMs生成可集成到基准测试中的图算法。

Result: Claude Sonnet 4 Extended在生成高效代码方面表现最佳，甚至超越人工编写的基线。

Conclusion: 当代LLMs擅长优化和集成现有算法，但不擅长发明新技术。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [15] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: 该论文探讨了问题跟踪生态系统（ITE）的复杂性，提出了最佳实践本体以解决研究和实践中的不一致问题。


<details>
  <summary>Details</summary>
Motivation: 问题跟踪系统（ITS）在软件工程中广泛应用，但其生态系统（ITE）的复杂性和多样性尚未被充分研究，需要更深入的探索。

Method: 通过访谈从业者和对多种ITS进行档案分析，揭示了ITE问题的上下文依赖性，并开发了最佳实践本体。

Result: 研究发现ITE问题具有高度上下文依赖性，现有解决方案缺乏一致性和可比性，需要更对齐的研究和实践方法。

Conclusion: 论文提出了最佳实践本体，为ITE研究提供了更一致的框架，有助于解决复杂性和多样性问题。

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [16] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: 论文探讨了语义冲突检测问题，提出了一种基于Code Llama 70B的测试生成工具，以改进SMAT的性能。


<details>
  <summary>Details</summary>
Motivation: 传统合并工具无法检测语义冲突，SMAT虽有效但存在高假阴性率，需探索LLM的潜力。

Method: 集成Code Llama 70B到SMAT中，研究不同交互策略、提示内容和参数配置的测试生成能力。

Result: LLM在复杂场景下生成测试仍具挑战性，但显示出改进语义冲突检测的潜力。

Conclusion: LLM为语义冲突检测提供了新方向，尽管存在计算成本高的问题。

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [17] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: 本文扩展并简化了AADL的形式化语义，通过模态逻辑明确引入时间，并扩展了AADL标准的运行时服务以支持行为规范语言。


<details>
  <summary>Details</summary>
Motivation: AADL标准缺乏对时间的明确建模，且运行时服务需要扩展以支持行为规范语言。

Method: 使用Kripke结构定义的模态逻辑扩展形式化语义，并扩展运行时服务以支持BLESS和BA语言。

Result: 提出了一个包含时间的AADL运行时服务实现示例，展示了HAMR对BLESS状态转换机行为的支持。

Conclusion: 通过模态逻辑和扩展运行时服务，AADL的形式化语义更完善，支持更复杂的行为建模。

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [18] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLM）在代码生成中链式思维（CoT）提示的质量问题，分析了影响CoT质量的内外部因素，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的CoT质量及其对代码生成的影响，以提升LLM的可靠性和推理能力。

Method: 通过分析1,023个失败的代码样本和210个CoT-代码对，评估CoT质量及其对代码生成的影响，并通过提示LLM改进低质量CoT。

Result: 发现外部因素（53.60%）和内部因素（40.10%）影响CoT质量；即使CoT正确，18.5%的代码仍有错误；改进CoT可提升LLM表现。

Conclusion: 研究揭示了CoT生成的关键挑战，为提升LLM推理和可靠性提供了方向。

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [19] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文分析了62种开源公平性干预工具，发现32%在过去一年内活跃维护，50%提供偏差检测和缓解功能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型中的偏见在关键领域（如刑事司法和医疗）造成负面影响，但公平性干预工具的采用率低，部分原因是实践者对其了解不足。

Method: 系统识别并分析62种开源公平性干预工具，评估其活跃性、功能及实践者偏好。

Result: 32%的工具在过去一年内活跃维护，50%提供偏差检测和缓解功能，主要在模型训练中实现。

Conclusion: 开源公平性干预工具虽多，但活跃维护率较低，需提高实践者对其的认知和使用。

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [20] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: 论文提出了一种名为5C Prompt Contract的框架，将提示设计简化为五个直观组件，以提高LLM交互的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在关键任务中的应用增加，需要一种既系统又简洁的提示设计框架，以降低认知负担并保持创造性。

Method: 提出5C Prompt Contract框架，包括Character、Cause、Constraint、Contingency和Calibration五个组件，整合了备用和输出优化指令。

Result: 实验表明，5C框架在多种LLM架构中实现了更高的输入令牌效率，同时保持丰富且一致的输出。

Conclusion: 5C框架特别适合资源有限的个人和中小企业，能够提供可靠、可解释且灵活的AI交互。

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [One task to rule them all: A closer look at traffic classification generalizability](https://arxiv.org/abs/2507.06430)
*Elham Akbari,Zihao Zhou,Mohammad Ali Salahuddin,Noura Limam,Raouf Boutaba,Bertrand Mathieu,Stephanie Moteau,Stephane Tuffin*

Main category: cs.NI

TL;DR: 论文探讨了现有网站指纹和流量分类解决方案在评估环境变化时性能下降的问题，提出了一个考虑实际标签约束的评估框架，并展示了模型在分布偏移下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案的性能依赖于特定上下文假设，导致在评估环境变化时表现不佳。

Method: 通过将三种现有解决方案的模型应用于彼此的数据集，识别导致性能差异的特定属性，并设计了一个模拟现实标签约束的评估框架。

Result: 在分布偏移下，最佳解决方案的性能仅为30%-40%，而简单的1-Nearest Neighbor分类器表现接近。

Conclusion: 研究强调了考虑分布偏移的重要性，并提供了对实际流量模型性能的公平评估。

Abstract: Existing website fingerprinting and traffic classification solutions do not
work well when the evaluation context changes, as their performances often
heavily rely on context-specific assumptions. To clarify this problem, we take
three prior solutions presented for different but similar traffic
classification and website fingerprinting tasks, and apply each solution's
model to another solution's dataset. We pinpoint dataset-specific and
model-specific properties that lead each of them to overperform in their
specific evaluation context.
  As a realistic evaluation context that takes practical labeling constraints
into account, we design an evaluation framework using two recent real-world TLS
traffic datasets from large-scale networks. The framework simulates a
futuristic scenario in which SNIs are hidden in some networks but not in
others, and the classifier's goal is to predict destination services in one
network's traffic, having been trained on a labelled dataset collected from a
different network. Our framework has the distinction of including real-world
distribution shift, while excluding concept drift. We show that, even when
abundant labeled data is available, the best solutions' performances under
distribution shift are between 30% and 40%, and a simple 1-Nearest Neighbor
classifier's performance is not far behind. We depict all performances measured
on different models, not just the best ones, for a fair representation of
traffic models in practice.

</details>


### [22] [Stacked Intelligent Metasurfaces-Aided eVTOL Delay Sensitive Communications](https://arxiv.org/abs/2507.06632)
*Liyuan Chen,Kai Xiong,Yujie Qin,Hanqing Yu,Supeng Leng,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于网络演算和优化方法的新型框架，用于量化高级空中交通（AAM）系统中的通信延迟概率上限，以解决eVTOL系统的安全问题。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，地面交通拥堵问题日益严重，而现有的AAM系统缺乏严格的数学框架来量化动态空中交通模式下的通信延迟概率上限，这对安全至关重要。

Method: 采用网络演算工具首次推导AAM系统中的通信延迟概率上限，并通过非凸优化问题联合最小化延迟概率上限和传播延迟，提出基于BCD算法和SDR方法的解决方案。

Result: 论文分析了多种因素对延迟概率上限的影响，并探讨了负载强度和总延迟对结果的作用。

Conclusion: 提出的框架为AAM系统的安全运行提供了理论基础，优化方法有效解决了通信延迟问题。

Abstract: With rapid urbanization and increasing population density, urban traffic
congestion has become a critical issue, and traditional ground transportation
methods are no longer sufficient to address it effectively. To tackle this
challenge, the concept of Advanced Air Mobility (AAM) has emerged, aiming to
utilize low-altitude airspace to establish a three-dimensional transportation
system. Among various components of the AAM system, electric vertical take-off
and landing (eVTOL) aircraft plays a pivotal role due to their flexibility and
efficiency. However, the immaturity of Ultra Reliable Low Latency Communication
(URLLC) technologies poses significant challenges to safety-critical AAM
operations. Specifically, existing Stacked Intelligent Metasurfaces (SIM)-based
eVTOL systems lack rigorous mathematical frameworks to quantify probabilistic
delay bounds under dynamic air traffic patterns, a prerequisite for collision
avoidance and airspace management. To bridge this gap, we employ network
calculus tools to derive the probabilistic upper bound on communication delay
in the AAM system for the first time. Furthermore, we formulate a complex
non-convex optimization problem that jointly minimizes the probabilistic delay
bound and the propagation delay. To solve this problem efficiently, we propose
a solution based on the Block Coordinate Descent (BCD) algorithm and
Semidefinite Relaxation (SDR) method. In addition, we conduct a comprehensive
analysis of how various factors impact regret and transmission rate, and
explore the influence of varying load intensity and total delay on the
probabilistic delay bound.

</details>


### [23] [Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G](https://arxiv.org/abs/2507.06911)
*Michele Polese,Niloofar Mohamadi,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出了一种融合O-RAN和AI-RAN的新架构，支持在共享基础设施上统一管理通信和AI工作负载，并引入了AI-RAN Orchestrator和AI-RAN站点两项创新。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型AI应用在网络边缘的普及，需要从单纯利用AI优化网络转向支持分布式AI工作负载，为运营商提供新的盈利机会。

Method: 扩展O-RAN的模块化、解耦和云原生原则，提出AI-RAN Orchestrator和AI-RAN站点，支持异构AI部署和实时处理。

Result: 架构支持灵活部署，满足不同时间尺度和地理定位的AI工作负载需求，同时保持开放接口和多厂商互操作性。

Conclusion: 该架构为网络运营商提供了在边缘实现AI盈利的可行方案，同时充分利用现有基础设施。

Abstract: The proliferation of data-intensive Artificial Intelligence (AI) applications
at the network edge demands a fundamental shift in RAN design, from merely
consuming AI for network optimization, to actively enabling distributed AI
workloads. This paradigm shift presents a significant opportunity for network
operators to monetize AI at the edge while leveraging existing infrastructure
investments. To realize this vision, this article presents a novel converged
O-RAN and AI-RAN architecture that unifies orchestration and management of both
telecommunications and AI workloads on shared infrastructure. The proposed
architecture extends the Open RAN principles of modularity, disaggregation, and
cloud-nativeness to support heterogeneous AI deployments. We introduce two key
architectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN
Service Management and Orchestration (SMO) to enable integrated resource and
allocation across RAN and AI workloads; and (ii) AI-RAN sites that provide
distributed edge AI platforms with real-time processing capabilities. The
proposed system supports flexible deployment options, allowing AI workloads to
be orchestrated with specific timing requirements (real-time or batch
processing) and geographic targeting. The proposed architecture addresses the
orchestration requirements for managing heterogeneous workloads at different
time scales while maintaining open, standardized interfaces and multi-vendor
interoperability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: 提出了一种名为HADES-NN的新方法，用于估计非自治微分方程的参数，通过神经网络平滑处理不连续的外部信号，从而提升参数估计的准确性和精度。


<details>
  <summary>Details</summary>
Motivation: 非自治微分方程在建模受外部信号影响的系统时非常重要，但当信号突变时，模型拟合变得困难。

Method: HADES-NN分两阶段：首先用神经网络将不连续信号平滑化，再用平滑后的信号估计模型参数。

Result: HADES-NN在多种应用中（如昼夜节律系统和酵母交配响应）表现出高准确性和精度。

Conclusion: HADES-NN显著扩展了可拟合实际测量数据的模型系统范围。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [25] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: FedPhD是一种新颖的联邦学习方法，专注于高效训练扩散模型（DMs），通过分层联邦学习和同质性感知模型聚合解决数据异质性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在训练扩散模型（DMs）时面临数据异质性和高通信成本的挑战，现有研究较少涉及这些问题。

Method: FedPhD采用分层联邦学习，结合同质性感知模型聚合和选择策略，以及分布式结构化剪枝，以提高计算效率和降低通信成本。

Result: 实验表明，FedPhD在FID分数上表现优异，通信成本降低88%，计算和通信资源仅需56%，FID改进至少34%。

Conclusion: FedPhD在联邦学习环境中高效训练扩散模型，显著提升了性能并降低了资源消耗。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [26] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS是一种基于强化学习的自适应深脑刺激框架，通过预测奖励模型和Gumbel Softmax探索，解决了样本效率低和硬件资源受限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统开环DBS缺乏适应性和个性化，而现有RL方法存在样本复杂度高和硬件兼容性差的问题。

Method: SEA-DBS结合预测奖励模型和Gumbel Softmax探索，优化样本效率和策略更新。

Result: 在帕金森病基底节模拟中，SEA-DBS表现出更快的收敛速度和更强的病理β波段抑制能力。

Conclusion: SEA-DBS为资源受限的实时神经调控提供了一种实用有效的RL解决方案。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [27] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 研究去中心化学习中通信调度的优化，发现后期集中通信预算和最终全局合并能显著提升性能，挑战了去中心化学习在数据异构和有限通信下表现不佳的常见观点。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽具扩展性，但性能受限于点对点通信。研究如何优化通信调度以提升训练效果。

Method: 通过实验和理论分析，研究通信调度策略，包括通信时机和频率，并探讨模型合并的可行性。

Result: 后期集中通信和最终全局合并能匹配中心化训练性能，低通信下本地模型仍可合并。理论证明去中心化SGD的合并模型收敛更快。

Conclusion: 去中心化学习在优化通信调度下表现优异，挑战传统观点，为模型合并和损失景观提供新见解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [28] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux是一种新型深度学习框架，通过符号回归从标准辛平面上的向量场识别哈密顿函数，采用混合CNN-LSTM架构，并在新数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化发现哈密顿力学中的符号表达式，推动相关领域的研究。

Method: 使用混合CNN-LSTM架构进行符号回归，训练和验证基于新开发的哈密顿向量场数据集。

Result: 模型能准确恢复符号表达式，验证了其有效性。

Conclusion: SymFlux在哈密顿力学的自动化发现中具有潜力。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [29] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种优化边缘网络中专家缓存的方法，以减少混合专家（MoE）模型的推理延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型在边缘设备上存储大量专家网络带来负担，需优化专家缓存以减少延迟。

Method: 针对Top-K专家选择策略，设计了贪心算法和动态规划方法，并提出了基于最大卷积的加速算法。

Result: 仿真结果显示，该方法显著降低了推理延迟。

Conclusion: 提出的方法在边缘网络中有效优化了专家缓存，提升了MoE模型的推理效率。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [30] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 论文提出DecoyDB数据集和定制化GCL框架，用于蛋白质-配体复合物的自监督学习，显著提升结合亲和力预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 药物发现中蛋白质-配体结合亲和力预测因缺乏大规模高质量标记数据而受限，现有PDBbind数据集标记不足20K。

Method: 构建DecoyDB数据集（包含高分辨率真实复合物和计算生成的诱饵结构），并设计定制化GCL框架进行预训练和微调。

Result: 实验表明，基于DecoyDB预训练的模型在准确性、标签效率和泛化性上表现优越。

Conclusion: DecoyDB和定制化GCL框架填补了领域空白，为药物发现提供了高效工具。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [31] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 论文提出了一种名为DICE的方法，用于在去中心化网络中估计数据影响传播，以解决激励机制中公平贡献分配的挑战。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽然能分散计算负载，但缺乏公平激励机制阻碍了参与。公平激励需要准确评估节点贡献，但去中心化网络中的局部连接使得影响传播复杂。

Method: 设计了DICE方法，通过理论推导近似计算任意邻居跳数的影响传播，结合数据、通信拓扑和损失函数曲率。

Result: DICE为选择合适合作者和识别恶意行为提供了理论基础。

Conclusion: DICE是首个在去中心化环境中量化影响传播的方法，为激励机制设计奠定了基础。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [32] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 研究了深度线性卷积网络梯度流的几何性质，发现参数空间的梯度流可以表示为函数空间上的黎曼梯度流，且不受初始化条件限制。


<details>
  <summary>Details</summary>
Motivation: 探索线性卷积网络梯度流的几何特性，扩展了全连接网络的相关研究。

Method: 分析了参数空间的梯度流，并证明其在函数空间上可表示为黎曼梯度流，适用于多维卷积。

Result: 梯度流可表示为黎曼梯度流，且适用于D≥2维卷积，D=1时需满足步长大于1的条件。

Conclusion: 线性卷积网络的梯度流在函数空间上具有黎曼结构，扩展了现有理论。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [33] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成全连接网络权重和压缩卷积网络权重，显著减少内存需求，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络存储大量突触权重的高内存需求问题。

Method: 使用PCA降维和轻量级SVR模型预测权重，结合敏感性分析压缩CNN权重。

Result: FC层压缩53倍，AlexNet在MNIST上压缩28倍，CIFAR-10上压缩18倍，精度损失1-2%。

Conclusion: WINGs显著降低内存需求，提升能效，适用于资源受限的边缘应用。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [34] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 论文提出了一种分解梯度流的方法，用于理解循环动态系统中学习表示的机制，并通过实验和理论验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度下降及其变体在训练循环动态系统（如RNNs、Neural ODEs和GRUs）中表现出色，但缺乏理论工具来严格理解这些系统中学习表示的机制。

Method: 将梯度流分解为两个算子（参数算子K和线性化流传播子P）的乘积，并展示了它们在低维潜在动态和多任务训练中的应用。

Result: 实验和理论验证表明，这种分解能够解释网络结构如何导致低维潜在动态，并衡量多任务中目标的对齐程度。

Conclusion: 该研究为非线性和有限循环模型中的梯度下降学习提供了新的理论工具，推动了对其理解的新阶段。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [35] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 论文分析了CNN、ResNet和混合Transformer-CNN模型在心电图（ECG）信号篡改检测中的性能，并评估了Siamese网络在ECG身份验证中的表现。结果显示，这些模型在多种篡改场景下表现优异，准确率超过99.5%。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统在健康监测和身份验证中的普及，保护信号完整性免受篡改变得至关重要。

Method: 使用连续小波变换（CWT）将一维ECG信号转换为二维时频表示，并训练和评估CNN、ResNet、混合Transformer-CNN模型及Siamese网络。模拟了六种篡改策略。

Result: 在高度碎片化篡改场景中，模型准确率超过99.5%；对于细微篡改，FeatCNN-TranCNN模型平均准确率达98%。Siamese网络中，混合CNN-Transformer模型实现了100%的验证准确率。

Conclusion: 混合Transformer-CNN和Siamese网络在ECG篡改检测和身份验证中表现出色，尤其是混合CNN-Transformer Siamese模型，展现了完美的验证性能。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [36] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare是一个基于领域适应的深度学习框架，用于预测ICU中罕见疾病的临床结果，解决了数据稀缺和异质性问题，并在多个任务上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: ICU中罕见疾病因数据稀缺和异质性而未被充分研究，需要一种方法填补这一空白。

Method: 通过自监督预训练学习条件无关表示，并利用条件知识图选择性适应临床相似条件的知识。

Result: 在五个临床预测任务中表现优于现有模型和ICU评分系统，并展示了灵活性和泛化能力。

Conclusion: KnowRare是支持ICU罕见疾病临床决策的实用解决方案。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [37] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: eegFloss是一个开源Python工具包，用于检测睡眠EEG记录中的伪迹，提高自动睡眠分期的准确性。


<details>
  <summary>Details</summary>
Motivation: EEG信号在睡眠研究中易受伪迹干扰，导致自动睡眠分期错误，影响研究结果。

Method: 开发了eegUsability机器学习模型，基于手动标记的EEG数据训练，用于检测伪迹。

Result: eegUsability表现优异（F1-score约0.85，Cohen's kappa 0.78），召回率约94%。

Conclusion: eegFloss能提升睡眠研究的分析精度和结果可靠性。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [38] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 论文探讨了可解释性研究在预测模型对未见数据（OOD）行为方面的潜力与挑战，通过分析Transformer模型的注意力模式与OOD泛化的关系，发现简单的可解释性工具可以预测OOD性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补可解释性研究在预测模型对未见数据行为方面的空白，探索其作为预测工具的潜力。

Method: 在合成分类任务上独立训练数百个Transformer模型，分析其注意力模式与OOD泛化的相关性。

Result: 发现当模型在分布内数据中表现出分层注意力模式时，其在OOD数据上也可能分层泛化，即使规则实现不依赖这些模式。

Conclusion: 研究为可解释性工作预测未见模型行为提供了概念验证，鼓励进一步研究。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [39] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 本文提出了一种自动标注蛋白质语言模型（PLM）神经元的方法，揭示了神经元对生物化学和结构特性的选择性敏感，并开发了一种基于神经元激活的蛋白质设计方法。


<details>
  <summary>Details</summary>
Motivation: 理解PLM内部神经元的生物信息编码机制，填补现有方法（如稀疏自编码器或手动标注）的不足。

Method: 引入自动化框架标注PLM神经元，开发神经元激活引导的蛋白质设计方法。

Result: 揭示了神经元对多样生物化学和结构特性的选择性敏感，实现了目标蛋白质特性的高效设计。

Conclusion: 研究不仅揭示了PLM神经元的生物信息编码规律，还为蛋白质设计提供了新工具。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [40] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 论文提出了一种基于前向-前向算法的二值随机单元训练方法，通过硬件优化显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习能耗问题日益突出，反向传播算法在硬件加速上存在挑战，需要替代方案。

Method: 采用二值随机单元的前向-前向算法，利用硬件优化实现高效矩阵乘法替代。

Result: 在MNIST等数据集上表现接近实值算法，能耗降低约一个数量级。

Conclusion: 提出的方法在保持性能的同时显著降低了能耗，适用于硬件加速场景。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [41] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 论文提出了一种名为SignSoftSGD（S3）的新优化器，通过改进Adam的更新机制，解决了其梯度波动和损失尖峰的问题，并在理论和实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: Adam在训练深度神经网络中表现优异，但其成功机制和局限性尚未充分探索。本研究旨在增强Adam的优势并缓解其局限性。

Method: 提出S3优化器，采用p阶动量、统一的EMA系数和NAG模块，实现稳定训练和快速收敛。

Result: S3在视觉和语言任务中表现优于Adam，收敛更快且损失尖峰更少，甚至能使用10倍学习率。

Conclusion: S3在效率和最终任务性能上均优于AdamW，验证了其有效性。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [42] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 提出了一种基于N版本机器学习（NVML）的框架，通过安全感知加权软投票机制提升交通标志识别系统在对抗攻击下的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的交通标志识别易受对抗攻击影响，威胁驾驶安全，需提升其鲁棒性。

Method: 采用NVML框架，结合FMEA评估安全风险，动态分配安全感知权重，并通过FGSM和PGD攻击测试三版本NVML系统的鲁棒性。

Result: 实验表明，NVML方法显著提高了交通标志识别系统在对抗条件下的安全性和鲁棒性。

Conclusion: NVML框架有效增强了交通标志识别系统的安全性，为自动驾驶安全提供了新思路。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [43] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: FMSP利用基础模型的代码生成能力和广泛知识，通过跨越策略空间的局部最优解，改进自博弈方法，提出三种变体（vFMSP、NSSP、QDSP），并在Car Tag和Gandalf实验中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 自博弈算法常因陷入局部最优解而无法生成多样化的策略，FMSP旨在通过基础模型的能力解决这一问题。

Method: 提出三种FMSP变体：vFMSP通过竞争自博弈优化策略，NSSP专注于策略多样性，QDSP结合多样性与高质量策略。

Result: 在Car Tag中，FMSP超越人工设计的策略；在Gandalf中，成功突破LLM的多层防御并自动修复漏洞。

Conclusion: FMSP为基础模型与自博弈结合提供了新方向，推动了更具创造性和开放性的策略发现。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [44] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 论文提出了一种双视图图表示学习方法（MimbFD），用于解决基于GNN的欺诈检测中拓扑和类别不平衡问题，通过拓扑消息可达性和局部混淆去偏模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在欺诈检测中因局部交互导致全局拓扑信息传输不平衡，且欺诈节点信息易被淹没。

Method: 设计了拓扑消息可达性模块和局部混淆去偏模块，分别用于穿透欺诈者伪装和平衡类别影响。

Result: 在三个公开欺诈数据集上实验，MimbFD表现出色。

Conclusion: MimbFD能有效缓解消息不平衡问题，提升欺诈检测性能。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [45] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedDifRC的新方法，通过引入扩散模型来解决联邦学习中的数据异构性问题，利用文本驱动的对比学习和噪声驱动的一致性正则化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性导致模型收敛和性能问题，需要一种有效的方法来缓解这一问题。

Method: 提出FedDifRC方法，结合扩散模型的文本驱动对比学习和噪声驱动一致性正则化，优化特征空间。

Result: 实验验证了FedDifRC的有效性及其关键组件的高效性。

Conclusion: FedDifRC通过扩散模型的指导成功缓解了数据异构性问题，并提供了理论收敛保证。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [46] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: MoFE-Time是一种创新的时间序列预测模型，结合时间和频域特征，通过混合专家网络（MoE）提升预测性能，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预训练-微调范式下未能同时建模时间和频率特征，导致复杂时间序列预测性能不佳。

Method: 提出MoFE-Time模型，整合时间和频域特征，利用MoE网络构建多维稀疏表示，采用预训练-微调框架。

Result: 在六个公共基准测试中，MoFE-Time表现最优，MSE和MAE分别降低6.95%和6.02%；在私有数据集NEV-sales上也取得显著效果。

Conclusion: MoFE-Time在理论和实际应用中均表现出色，验证了其在复杂时间序列预测中的有效性。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [47] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 提出了一种新的单调后校准方法，通过线性参数化的约束校准映射，确保表达性、鲁棒性和可解释性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常产生校准不足的概率估计，现有后校准方法无法保证单调性或牺牲表达性/可解释性。

Method: 采用线性参数化的约束校准映射，通过约束优化问题确保单调性。

Result: 在多个数据集和模型上表现优异，计算高效且数据需求低。

Conclusion: 提出的方法在保持单调性的同时，实现了高性能和实用性。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [48] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: 提出了一种名为AdaDPIGU的差分隐私SGD框架，通过重要性梯度更新解决高维噪声问题，提升深度神经网络的隐私保护与性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法在高维设置中因噪声增加导致性能下降，需改进。

Method: 预训练阶段使用差分隐私高斯机制估计参数重要性，梯度更新阶段剪枝低重要性坐标并引入自适应裁剪机制。

Result: 在MNIST和CIFAR-10上表现优异，隐私预算下接近或超越非隐私模型。

Conclusion: AdaDPIGU通过自适应稀疏化同时提升隐私保护与模型效用。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [49] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 提出了一种基于直接遗憾优化的贝叶斯优化方法，通过联合学习最优模型和非近视获取函数，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法依赖手工设计的获取函数和代理模型，且通常是近视的。本文旨在解决这些问题。

Method: 利用高斯过程集合生成模拟轨迹，训练端到端决策变换器，直接学习选择查询点以减少多步遗憾。

Result: 在合成和真实基准测试中表现优于基线，实现了更低的简单遗憾和更鲁棒的探索。

Conclusion: 该方法通过联合学习和非近视策略，显著提升了贝叶斯优化的性能。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [50] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: 论文提出CircuitGCL，一种图对比学习框架，用于解决AMS电路图表示学习中的数据稀缺、标签不平衡和电路多样性问题，显著提升了寄生估计任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决AMS电路图表示学习中的数据稀缺、标签不平衡和电路多样性问题，以提升下游任务（如寄生估计）的性能。

Method: 提出CircuitGCL框架，结合表示散射和标签重平衡技术，使用自监督学习策略和平衡损失函数（平衡MSE和bsmCE）。

Result: 在TSMC 28nm AMS设计上，CircuitGCL在边级回归任务中R²提升33.64%~44.20%，在节点分类任务中F1分数提升0.9~2.1倍。

Conclusion: CircuitGCL通过自监督学习和标签平衡技术，显著提升了电路图表示学习的鲁棒性和可迁移性。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [51] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGPS是一种用于AMS电路中寄生效应预测的小样本学习方法，通过异构图表示和混合图Transformer提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于集成电路设计数据稀缺，训练深度学习模型用于AMS设计受限，需要一种高效的小样本学习方法。

Method: 将电路网表表示为异构图，采用小跳采样技术生成子图，结合混合图Transformer和低成本位置编码学习子图嵌入。

Result: 耦合存在预测精度提升至少20%，电容估计MAE降低至少0.067，具有强扩展性和零样本学习能力。

Conclusion: CircuitGPS在寄生效应预测中表现优异，为图表示学习提供了有价值的见解。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [52] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的2阶段模型，用于在预布局阶段准确预测SRAM电路中的寄生效应，显著提高了预测精度和仿真速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决SRAM设计中预布局与后布局仿真之间的寄生效应差异问题，减少设计迭代次数。

Method: 结合图神经网络（GNN）分类器和多层感知机（MLP）回归器，采用Focal Loss处理类别不平衡，并整合子电路信息以抽象层次结构。

Result: 在4个实际SRAM设计中，模型将寄生预测误差最大降低19倍，仿真速度提升高达598倍。

Conclusion: 该方法有效解决了寄生效应预测问题，显著提升了设计效率和仿真速度。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [53] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: LoRAM是一种基于更新幅度的初始化方案，匹配光谱方法的性能但避免了其计算和存储开销。


<details>
  <summary>Details</summary>
Motivation: 光谱初始化方法虽然提高了性能，但带来了额外的计算和存储开销，影响了效率。

Method: 提出LoRAM，通过幅度驱动的初始化方案，利用预训练权重幅度缩放确定性正交基来模拟光谱增益。

Result: LoRAM在保持LoRA效率的同时，匹配或优于光谱初始化方法。

Conclusion: 更新幅度是LoRA性能的关键驱动因素，LoRAM提供了一种高效且性能优越的初始化策略。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [54] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为LPPO的框架，通过前缀引导采样和学习进度加权优化强化学习，利用少量高质量示范数据提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效利用少量高质量示范数据，而非单纯增加数据量，以提升大型语言模型的推理能力。

Method: 提出前缀引导采样和学习进度加权两种技术，前者利用专家示范的部分解前缀指导策略，后者动态调整训练样本权重。

Result: 在数学推理基准测试中，方法表现优于基线，收敛更快且性能上限更高。

Conclusion: LPPO框架通过样本中心视角优化强化学习，显著提升了模型性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [55] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 论文提出了一种基于“预测信息增益”的方法，用于探索环境中信息量最大的区域，并通过强化学习找到有效的探索策略。


<details>
  <summary>Details</summary>
Motivation: 在无法获取显式模型的环境中，如何通过探索学习可控动态是研究的核心动机。

Method: 使用“预测信息增益”作为信息度量，结合强化学习方法，寻找高效的探索策略。

Result: 该方法能够可靠地估计环境的可控动态，并在与几种短视探索方法的比较中表现出色。

Conclusion: 基于信息增益的探索策略是一种有效的替代方法，适用于模型不可用但需学习动态的环境。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [56] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的通用框架，用于解决现代无线接入网络中的资源管理问题，通过图注意力网络和分布式训练提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代无线接入网络环境动态且异构，传统基于规则的资源管理算法表现不佳，而现有强化学习方法容易过拟合，难以适应多样化的部署场景。

Method: 采用注意力机制构建图表示网络拓扑和节点属性，结合领域随机化扩展训练分布，并通过分布式数据生成和集中式训练实现高效学习。

Result: 在多个5G基准测试中，该策略显著提升了吞吐量和频谱效率，尤其在高速移动场景下表现优异，同时展示了良好的泛化能力。

Conclusion: 该框架为AI驱动的6G无线接入网络提供了一种可扩展且通用的解决方案。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [57] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过使用不同β值的VAE和扩散模型，平衡解缠和重建质量，实现高质量生成和解缠表示。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中解缠表示与生成质量之间的权衡问题。

Method: 训练单一VAE，使用新损失函数控制不同β值的潜在表示，并引入非线性扩散模型平滑过渡表示。

Result: 模型在解缠和生成质量上表现优异，支持无输入图像的样本生成。

Conclusion: 该框架有效平衡了解缠与重建质量，支持灵活的潜在空间操作。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [58] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为CTPG的新框架，通过跨任务策略指导加速多任务强化学习，利用已掌握任务的策略指导未掌握任务，并引入两种门控机制提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注参数共享，但忽略了利用已掌握任务的策略直接指导未掌握任务的可能性。

Method: 提出CTPG框架，为每个任务训练一个指导策略，从所有任务的控制策略中选择行为策略，并引入两种门控机制优化学习效率。

Result: 实验证明，CTPG与现有参数共享方法结合后，在操作和运动基准测试中显著提升了性能。

Conclusion: CTPG是一种通用框架，能有效利用跨任务相似性，加速多任务强化学习。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [59] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 论文提出SAD-DPSGD方法，通过动态调整噪声和裁剪阈值，解决了医疗图像分类中数据泄漏和类别不平衡问题，性能优于Auto-DPSGD。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分类中数据泄漏是严重问题，现有方法在小型不平衡数据集（如HAM10000）上表现不佳，导致模型陷入次优解。

Method: 提出SAD-DPSGD，采用线性衰减机制动态调整噪声和裁剪阈值，初期分配更多隐私预算和更高裁剪阈值。

Result: 在HAM10000数据集上，SAD-DPSGD比Auto-DPSGD准确率提高2.15%（ε=3.0，δ=10^-3）。

Conclusion: SAD-DPSGD有效解决了医疗图像分类中的隐私和类别不平衡问题，性能显著提升。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [60] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD是一个通用的异常检测框架，通过利用标记数据集训练单一模型，避免繁琐的超参数调整和模型训练，适用于多种领域。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法需要针对不同数据集进行超参数调整和模型训练，过程繁琐且成本高。UniOD旨在解决这一问题，提高检测的便利性和准确性。

Method: UniOD将数据集转换为多个图，生成一致的节点特征，并将异常检测任务转化为节点分类问题，能够泛化到未见过的领域。

Result: 在15个基准数据集上与15种先进基线方法对比，验证了UniOD的有效性。

Conclusion: UniOD避免了模型选择和超参数调整的麻烦，降低了计算成本，并有效利用历史数据集的知识，提升了实际应用的便利性和准确性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [61] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: GO-Skill是一种离线多任务强化学习方法，通过目标导向技能提取和分层策略学习，提升知识共享和任务性能。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习面临跨任务知识共享的挑战，受人类学习启发，提出GO-Skill以提取可重用技能。

Method: 通过目标导向技能提取和向量量化构建离散技能库，引入技能增强阶段优化技能，并采用分层策略学习动态协调技能。

Result: 在MetaWorld基准测试中，GO-Skill在多样化机器人操作任务中表现出高效性和通用性。

Conclusion: GO-Skill通过技能抽象和分层策略有效解决了离线多任务强化学习中的知识共享问题。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [62] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种检测和防止网格数据回归过拟合的方法，通过拉普拉斯算子二阶导数计算熵，优化超参数以减少振荡。


<details>
  <summary>Details</summary>
Motivation: 解决网格数据回归中的过拟合问题，避免训练数据中的振荡现象。

Method: 在原始训练网格上计算导数作为真实标签，在交错网格上计算训练数据的导数以识别振荡，利用拉普拉斯算子导数损失优化超参数。

Result: 通过最小化训练模型的熵，减少了不希望的振荡，无需分割训练数据即可进行测试。

Conclusion: 该方法有效减少了网格数据回归中的过拟合问题，提供了一种基于扩散特性的替代测试指标。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [63] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了一种新的个体治疗效果估计算法，结合多头注意力机制和线性正交正则化器，通过软分解预处理变量并消除选择偏差，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决观测数据中个体治疗效果估计问题，现有方法在精确分解变量方面存在不足。

Method: 采用多头注意力机制和线性正交正则化器软分解变量，结合重要性采样重加权技术消除选择偏差。

Result: 在公开半合成和真实数据集上实验，性能优于现有方法。

Conclusion: 新算法在个体治疗效果估计中表现优异，为因果推断提供了有效工具。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [64] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文探讨如何通过机器学习和联邦学习改进模糊系统，以处理不确定性，并讨论其潜在改进和局限性。


<details>
  <summary>Details</summary>
Motivation: 模糊系统能处理不确定性，但仍有改进空间，尤其是结合机器学习和联邦学习的新技术。

Method: 提出将机器学习和联邦学习的理念应用于模糊系统，例如更新模糊规则。

Result: 潜在改进包括提高模糊系统的性能和效率，但仍需进一步研究验证。

Conclusion: 改进模糊系统的潜力存在，但需更多研究以确定其实际效果。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [65] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 提出了一种基于异构图注意力网络的电力系统状态预测方法，显著提升了多域、多速率系统的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源和分布式能源的引入而变得复杂多变，需要可靠的短期状态预测以确保稳定运行。现有方法难以处理多域异构数据。

Method: 采用异构图注意力网络（Heterogeneous Graph Attention Networks），建模传感器数据的同质域内和异质域间关系。

Result: 实验显示，该方法在归一化均方根误差上平均优于传统基线35.5%。

Conclusion: 该方法在多域、多速率的电力系统状态预测中表现出色，为复杂系统的建模提供了有效工具。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [66] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文提出了一种改进的模仿学习观察方法（IfO），通过利用无动作演示数据，避免了传统方法对动作标签或奖励函数的需求，并研究了更复杂的数据分布。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要动作标签或奖励函数，成本高昂。IfO可以利用无动作演示数据，但现有研究多基于理想化场景，限制了实用性。本文旨在探索更复杂的数据分布，并提出一种适应方法。

Method: 提出了一种基于强化学习的模仿学习方法，利用价值函数在专家与非专家数据间传递信息，适应无动作演示数据。

Result: 通过全面评估，揭示了不同数据分布与算法适用性之间的关系，并指出了现有方法的局限性。

Conclusion: 研究为开发更鲁棒、实用的IfO技术提供了重要见解，推动了可扩展行为学习的进展。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [67] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 提出了一种基于自适应物理信息神经网络（PINN-Obs）的非线性系统状态估计方法，优于传统模型观测器。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的状态估计在部分和噪声测量下具有挑战性，传统方法需要系统变换或线性化。

Method: 直接集成系统动力学和传感器数据到物理信息学习过程中，自适应学习最优增益矩阵。

Result: 理论分析证明收敛性，数值模拟验证了其在多样非线性系统中的优越性能。

Conclusion: PINN-Obs在准确性、鲁棒性和适应性上优于现有观测器设计。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [68] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 提出了MAD框架，结合物理定律与数据驱动学习，解决微分方程求解中的数据依赖和效率-精度权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵标注数据或面临效率-精度权衡，亟需新范式。

Method: 利用微分方程数学结构生成物理解析解与合成数据，实现无实验/模拟数据的算子学习。

Result: 在2D参数化问题中验证了MAD的泛化能力和高效/高精度特性。

Conclusion: MAD框架有望成为科学计算中物理驱动机器智能的通用范式。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [69] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种参数化激活函数（Leaky Exponential Linear Unit）以改进多维非线性数据回归，并通过新提出的扩散损失指标评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 非线性激活函数对学习非线性数据集至关重要，但其平滑性和梯度特性会影响大神经网络的过拟合和参数敏感性。

Method: 设计了一种平滑且具有非零梯度的Leaky Exponential Linear Unit激活函数，并提出扩散损失指标评估模型。

Result: 改进的激活函数在性能上优于传统激活函数（如ELU、SiLU、RELU等）。

Conclusion: 平滑且非零梯度的激活函数能有效提升模型性能，扩散损失指标为评估过拟合提供了新方法。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [70] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑泛化界，避免了复杂的信息论项，通过轨迹稳定性框架将泛化误差与拓扑数据分析和算法稳定性联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑泛化界依赖复杂的信息论项，难以应用于实际算法（如ADAM），因此需要更直观且可解释的泛化界。

Method: 引入轨迹稳定性框架，扩展假设集稳定性概念，通过算法稳定性证明泛化误差与拓扑复杂性和轨迹稳定性参数的关系。

Result: 实验证明拓扑数据项在泛化界中至关重要，尤其在训练样本增加时，解释了拓扑泛化界的实证成功。

Conclusion: 新框架提供了更实用且可解释的拓扑泛化界，为理解优化算法的泛化性能提供了新视角。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [71] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出了一种模仿学习方法，用于学习符合专家轨迹约束的最大熵策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过模仿学习生成符合约束的策略，同时最大化熵以提高泛化能力。

Method: 利用KL散度边界连接性能，结合强化学习目标和约束目标，通过双梯度下降优化学习目标。

Result: 实验表明，该方法能有效学习符合约束的策略，适应多种约束类型和行为模态，并具备泛化能力。

Conclusion: 该方法在模仿学习中结合约束和熵最大化，通过双梯度下降实现稳定训练，适用于多约束场景。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [72] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 本文提出了一种新型语音分词器，同时编码语言和声学信息，显著提升了语音表示的真实性，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于残差向量量化（RVQ）的方法常忽略关键声学特征，本文旨在同时保留语言和声学信息。

Method: 提出一种先进方法，同时编码语言和声学信息，保留韵律和情感内容。

Result: 实验证明该方法在语音编码、语音转换、情感识别和多模态语言建模中有效，无需额外训练。

Conclusion: 该方法具有广泛适用性，是推动AI语音处理的关键工具。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [73] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR提出了一种无需修改架构或损失函数的联邦学习方法，利用训练信号实现高效模型聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法需要修改架构或损失函数，LIVAR旨在利用现有训练信号实现高效聚合。

Method: LIVAR通过方差加权分类器聚合和基于SHAP分析的LoRA合并技术实现。

Result: LIVAR在多个基准测试中达到最优性能，且与现有方法无缝集成。

Conclusion: LIVAR证明仅通过现有训练信号即可实现高效模型聚合，为联邦学习提供了新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [74] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文对原型模型（如ProtoPNet、ProtoPool和PIPNet）进行了深入分析，提出了一套全面的评估指标，并应用于多种数据集。


<details>
  <summary>Details</summary>
Motivation: 原型模型是解释性人工智能（XAI）的重要方法，本文旨在通过新指标和多样化数据集评估其性能。

Method: 应用标准指标和新提出的指标，对原型模型在细粒度分类、非独立同分布（Non-IID）和多标签分类数据集上进行实验。

Result: 实验展示了原型模型在不同数据集上的表现，并提供了开源代码库以支持进一步研究。

Conclusion: 本文为原型模型的评估提供了新视角和工具，促进了XAI领域的发展。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [75] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为HeLo的多模态情感分布学习框架，旨在挖掘多模态情感数据的异质性和互补信息，以及混合基本情感之间的标签相关性。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在人机交互中具有重要作用，但现有方法在挖掘多模态异质性和利用基本情感间的语义相关性方面存在不足。

Method: 采用交叉注意力融合生理数据，设计基于最优传输的异质性挖掘模块，引入可学习的标签嵌入和相关性矩阵对齐，并通过标签相关性驱动的交叉注意力机制整合多模态表示。

Result: 在两个公开数据集上的实验结果表明，该方法在情感分布学习中表现优越。

Conclusion: HeLo框架有效解决了多模态情感分布学习中的异质性挖掘和标签相关性利用问题。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [76] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 介绍了一个基于Generals.io的实时策略游戏环境，兼容Gymnasium和PettingZoo，支持高性能运行，并训练了一个顶级水平的参考代理。


<details>
  <summary>Details</summary>
Motivation: 为多智能体强化学习研究提供一个易用且具有挑战性的平台。

Method: 结合监督预训练和自博弈训练参考代理，采用基于潜在奖励塑造和记忆特征加速学习。

Result: 参考代理在36小时内达到1v1人类排行榜前0.003%，运行性能高达每秒数千帧。

Conclusion: 该环境和基线代理为多智能体强化学习研究提供了高效且竞争性强的工具。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [77] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种结合迭代方法和路径条件化的方法，以提高高斯过程在大规模数据下的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在不确定性感知函数逼近和序列决策中具有强大能力，但其经典形式难以适应大规模数据和现代硬件并行计算的需求。

Method: 通过将迭代线性系统求解器与路径条件化相结合，将昂贵计算转化为线性方程组的求解，减少内存需求并利用矩阵乘法优化计算。

Result: 该方法显著降低了内存需求，适用于更大规模的数据，并优化了现代硬件的计算效率。

Conclusion: 结合迭代方法和路径条件化，为高斯过程在大规模应用中的使用提供了有效解决方案。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [78] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra是一种基于扩散模型的生成框架，直接从多模态光谱数据推断2D和3D分子结构，解决了传统方法和现有机器学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 分子结构解析是化学中的基础问题，传统方法依赖专家解释且缺乏扩展性，现有机器学习方法受限于有限库。DiffSpectra旨在通过生成模型解决这些问题。

Method: DiffSpectra使用扩散模型和SE(3)-等变架构的Diffusion Molecule Transformer，结合基于Transformer的光谱编码器SpecFormer，实现多模态光谱数据的条件生成。

Result: 实验表明，DiffSpectra在结构解析中表现出色，Top-1准确率为16.01%，Top-20准确率为96.86%，3D几何建模和多模态条件显著提升了性能。

Conclusion: DiffSpectra首次统一了多模态光谱推理和2D/3D生成建模，为分子结构解析提供了有效解决方案。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [79] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究在线决策问题，设计了一种在非平稳上下文环境中实现次线性遗憾的算法。


<details>
  <summary>Details</summary>
Motivation: 解决动态定价和拍卖等应用中资源分配的非平稳性和多上下文挑战。

Method: 提出一种在线算法，利用置信边界预言机处理非平稳上下文和无限状态空间。

Result: 算法在T次迭代中实现次线性遗憾，并在特定设置下提供改进的遗憾边界。

Conclusion: 该框架为上下文BwK问题提供了新的解决方案，尤其在未标记特征数据的情况下表现优异。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [80] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种联邦因果发现方法，结合水平与垂直联邦设置，利用高阶累积量构建全局因果图，解决了变量不完整导致的虚假因果问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法主要针对水平联邦设置，而实际场景中客户端变量可能不同，导致虚假因果关系。

Method: 通过聚合客户端的高阶累积量信息构建全局估计，递归识别因果源，生成全局因果强度矩阵。

Result: 在合成和真实数据实验中表现优异，能重建因果图并估计因果强度系数。

Conclusion: 该方法在水平和垂直联邦设置下均有效，解决了变量不完整问题，具有实际应用价值。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [81] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: 论文提出了一种名为ReMix的通用方法，通过混合策略近端策略梯度、KL凸策略约束和策略重生，使基于策略的强化微调方法（如PPO和GRPO）能够利用非策略数据，显著降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化微调方法（如PPO和GRPO）主要依赖策略内数据，导致计算和时间成本高昂，限制了经济高效的扩展。

Method: ReMix包含三个主要组件：混合策略近端策略梯度（提高更新与数据比）、KL凸策略约束（平衡稳定性和灵活性）、策略重生（实现从高效早期学习到稳定渐进改进的无缝过渡）。

Result: 实验表明，ReMix在多个数学推理基准测试中表现出色，训练成本降低30至450倍，同时达到SOTA性能。

Conclusion: ReMix为强化微调提供了一种高效且经济的方法，显著提升了性能并降低了成本，同时揭示了非策略数据使用中的一些有趣现象。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [82] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的动态滑动窗口优化方法（RL-Window），用于处理多维数据流，适应动态变化并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多维数据流（如IoT、金融市场）的高速度、无界性和复杂依赖性对处理技术提出了挑战，传统固定窗口方法难以适应动态变化。

Method: 将窗口大小选择建模为强化学习问题，使用Dueling DQN和优先级经验回放，学习自适应策略。

Result: 在多个基准数据集上，RL-Window在分类精度、漂移鲁棒性和计算效率上优于现有方法（如ADWIN、CNN-Adaptive）。

Conclusion: RL-Window具有适应性和稳定性，适用于实时应用，并通过扩展指标验证了其优势。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [83] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 提出了一种评估基础模型是否真正理解深层结构的技术，发现模型在适应新任务时未能发展出与底层世界模型一致的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 验证基础模型是否能通过序列预测揭示更深层次的领域理解，类似于开普勒预测行星运动后牛顿力学的发现。

Method: 开发了一种称为“归纳偏差探针”的技术，通过生成合成数据集并测试模型适应性来评估模型。

Result: 基础模型在训练任务上表现优异，但在适应新任务时未能发展出与底层世界模型一致的归纳偏差，特别是在轨道轨迹任务中未能应用牛顿力学。

Conclusion: 基础模型可能仅发展出任务特定的启发式方法，缺乏泛化能力。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [84] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 论文研究了在噪声数据下，物理信息神经网络（PINNs）逼近偏微分方程解的能力，并证明了网络规模的下界条件。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，数据样本通常含有噪声，但PINNs在噪声条件下有效降低经验风险的条件尚不明确。

Method: 通过理论分析，证明了在监督和无监督PINN设置下，网络规模与样本数量及噪声方差的关系。

Result: 发现增加噪声标签数量并不能无条件降低经验风险，且PINNs在特定条件下可实现低于噪声方差的经验风险。

Conclusion: 研究为定量理解噪声环境下训练PINNs的参数需求奠定了基础。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [85] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [86] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 论文提出了两种新的损失函数（MV-InfoNCE和MV-DHEL），用于改进多视图对比学习，解决了现有方法的四个关键限制，并在实验中验证了其优越性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习方法在处理多视图数据时存在优化冲突、视图交互建模不足、对齐-均匀性耦合等问题，无法充分利用多视图的优势。

Method: 提出了MV-InfoNCE和MV-DHEL两种损失函数，分别通过同时建模所有视图交互和解耦对齐与均匀性来解决上述问题。

Result: 在ImageNet1K等数据集上的实验表明，新方法优于现有多视图方法，并能有效利用多视图提升性能，尤其在五视图以上时显著缓解维度塌缩。

Conclusion: 新方法为多视图对比学习提供了理论支持和实践验证，显著提升了性能并扩展了应用范围。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [87] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed是一个生成多表时间序列电子健康记录（EHR）数据的框架，通过文本表示和压缩技术，无需复杂预处理即可捕捉复杂结构和时间动态。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和监管限制，真实EHR数据难以共享和使用，需要生成合成数据。现有方法通常仅生成专家选择的特征，无法模拟原始EHR的复杂性。

Method: RawMed采用文本表示和压缩技术，生成多表时间序列EHR数据，并提出了新的评估框架，涵盖分布相似性、表间关系、时间动态和隐私。

Result: 在开源EHR数据集上验证，RawMed在保真度和实用性上优于基线模型。

Conclusion: RawMed是首个能生成接近原始EHR的合成数据框架，具有高保真度和实用性。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [88] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在高斯数据分布去模糊任务中的准确性，通过计算Wasserstein距离比较理论解与扩散模型解之间的差异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为贝叶斯逆问题的先验，因其灵活性和高方差受到关注，但其性能尚不明确。本文旨在评估其在特定任务中的准确性。

Method: 在约束的高斯数据分布下，计算扩散模型采样器分布与理想逆问题解分布之间的精确Wasserstein距离。

Result: 研究发现可以比较文献中不同算法的性能。

Conclusion: 扩散模型在特定任务中的准确性可通过理论分析进行评估，为算法比较提供了依据。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [89] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 本文研究了在资源有限的智能电表上进行边缘侧模型训练的可行性，提出了混合和降低精度的训练方案，并通过光伏功率预测任务验证了其经济性。


<details>
  <summary>Details</summary>
Motivation: 推动电网边缘智能化和设备端训练的概念，以解决资源受限环境下的模型训练问题。

Method: 介绍了设备端训练的技术准备步骤，并在光伏功率预测任务中测试了梯度提升树模型和循环神经网络模型，同时设计了混合和降低精度的训练方案。

Result: 实验结果表明，通过现有先进计量基础设施经济地实现电网边缘智能是可行的。

Conclusion: 资源受限的智能电表上可以实现边缘侧模型训练，为电网边缘智能化提供了经济有效的解决方案。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [90] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME是一种新型的MSA设计模型，利用预训练蛋白质语言模型的进化嵌入，提升低同源性和孤儿蛋白质的结构预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有折叠模型对多序列比对（MSA）的依赖问题，特别是在低同源性和孤儿蛋白质中MSA信息稀缺的情况。

Method: 提出PLAME模型，结合预训练表示和守恒-多样性损失函数，设计高质量的MSA，并引入新的MSA筛选方法和序列质量评估指标。

Result: 在AlphaFold2和AlphaFold3基准测试中，PLAME在折叠增强和序列质量评估方面达到最先进水平。

Conclusion: PLAME不仅提升了预测性能，还能作为适配器实现AlphaFold2级精度与ESMFold的推理速度。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [91] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）在资源受限的边缘设备上实现高效学习，通过定制自监督学习（SSL）策略，性能接近监督学习，同时资源消耗减少4倍。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习在边缘设备上的可行性和效率，解决传统方法对数据和计算资源的高需求问题。

Method: 分析不同SSL技术在有限计算、数据和能源预算下的适应性，评估其在资源受限环境中学习鲁棒表示的效果，并考虑半监督学习减少标注能耗。

Result: 定制SSL策略在性能接近监督学习的同时，资源消耗减少高达4倍。

Conclusion: SSL技术适合边缘设备，能实现高效节能的学习。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [92] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 本文提出了一种集成嵌入方法，通过结合多个嵌入模型和训练元编码器，提升LLM缓存系统中的语义相似性检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有的语义缓存框架依赖单一嵌入模型，难以捕捉真实查询分布中的多样语义关系，限制了缓存效率。

Method: 采用集成嵌入方法，结合多个嵌入模型并通过训练元编码器优化语义相似性检测。

Result: 在QQP数据集上，该方法实现了92%的缓存命中率和85%的非等价查询拒绝准确率。

Conclusion: 集成嵌入方法显著优于单一模型，能更有效区分语义相似和相异查询，提升LLM系统的缓存性能和计算效率。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [93] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: DCE框架通过频率感知专家组和动态专家选择器解决DIL中的类不平衡和跨域分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: DIL在非平稳环境中面临类内不平衡和跨域分布偏移的挑战，影响模型性能。

Method: 提出DCE框架，包括频率感知专家组和动态专家选择器，分别处理类内不平衡和跨域知识迁移。

Result: 在四个基准数据集上，DCE表现出最先进的性能。

Conclusion: DCE有效解决了DIL中的关键挑战，提升了模型在非平稳环境中的适应能力。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [94] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究发现小批量训练语言模型更稳定且性能更优，建议调整Adam超参数并避免梯度累积。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为小批量训练不稳定，但本研究挑战这一观点，探索小批量（甚至批量大小为1）的潜力。

Method: 提出调整Adam超参数的规则，并验证小批量训练的稳定性与性能。

Result: 小批量训练更稳定、对超参数更鲁棒，性能优于大批量，且支持SGD稳定训练。

Conclusion: 建议选择小批量并调整超参数，避免梯度累积，除非在多设备训练中带宽受限。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [95] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究发现，组合泛化能力由数据多样性而非数据规模驱动，线性分解表示结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨当代视觉模型是否具备组合理解能力，以及数据规模和多样性对组合泛化的影响。

Method: 通过控制实验，系统变化数据规模、概念多样性和组合覆盖率，分析模型表现。

Result: 组合泛化依赖数据多样性，线性分解表示结构能实现高效泛化。预训练模型表现部分符合此结构。

Conclusion: 强调构建多样化数据集和优化表示结构以提升组合学习效率。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [96] [Robust Containerization of the High Angular Resolution Functional Imaging (HARFI) Pipeline](https://arxiv.org/abs/2507.07010)
*Zhiyuan Li,Kurt G. Schilling,Bennett A. Landman*

Main category: physics.med-ph

TL;DR: 本文介绍了HARFI管道的容器化版本，旨在简化技术复杂性，促进对白质功能结构的广泛研究。


<details>
  <summary>Details</summary>
Motivation: 传统fMRI主要关注灰质，但近期研究表明白质功能活动在认知和学习中也有重要作用。HARFI管道虽已提出，但因技术复杂性限制了其应用。

Method: 开发了HARFI管道的容器化版本，支持跨多个公共数据集的便捷执行。

Result: 容器化实现提高了HARFI的可访问性和可重复性，支持高角度分辨率功能相关性研究。

Conclusion: 容器化的HARFI管道为白质功能结构研究提供了更高效和可访问的工具，推动了该领域的进一步发展。

Abstract: Historically, functional magnetic resonance imaging (fMRI) of the brain has
focused primarily on gray matter, particularly the cortical gray matter and
associated nuclei. However, recent work has demonstrated that functional
activity in white matter also plays a meaningful role in both cognition and
learning. In previous work, we introduced the High Angular Resolution
Functional Imaging (HARFI) pipeline, which demonstrated both local and global
patterns of functional correlation in white matter. Notably, HARFI enabled
exploration of asymmetric voxel-wise correlation using odd-order spherical
harmonics. Although the original implementation of HARFI was released via
GitHub, adoption was limited due to the technical complexity of running the
source code. In this work, we present a robust and efficient containerized
version of the HARFI pipeline, enabling seamless execution across multiple
public datasets. Our goal is to facilitate broader and deeper exploration of
functional white matter architecture, especially through the lens of high
angular resolution functional correlations. The key innovation of this work is
the containerized implementation, which we have made available under a
permissive open-source license to support reproducible and accessible research
practices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [97] [Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems](https://arxiv.org/abs/2507.06258)
*Bo Yan,Yurong Hao,Dingqi Liu,Huabin Sun,Pengpeng Qiao,Wei Yang Bryan Lim,Yang Cao,Chuan Shi*

Main category: cs.CR

TL;DR: Spattack是一种针对联邦推荐系统的定向投毒攻击方法，通过两阶段策略（近似和推广）操纵特定用户子组的推荐结果，同时保持对其他用户的影响最小。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统的投毒攻击通常针对整个用户群体，缺乏对特定子组的针对性，而现实中的攻击者可能更倾向于针对特定子组（如老年用户）。

Method: Spattack采用两阶段策略：1）基于对比学习和聚类模拟目标/非目标子组的用户嵌入；2）通过自适应优化权重和嵌入对齐策略推广目标项目。

Result: 实验表明，Spattack在特定用户子组上具有强操纵效果，对其他用户影响极小，且能抵抗主流防御机制。

Conclusion: Spattack填补了联邦推荐系统中定向投毒攻击的空白，展示了高效性和隐蔽性。

Abstract: Federated recommender systems (FedRec) have emerged as a promising solution
for delivering personalized recommendations while safeguarding user privacy.
However, recent studies have demonstrated their vulnerability to poisoning
attacks. Existing attacks typically target the entire user group, which
compromises stealth and increases the risk of detection. In contrast,
real-world adversaries may prefer to prompt target items to specific user
subgroups, such as recommending health supplements to elderly users. Motivated
by this gap, we introduce Spattack, the first targeted poisoning attack
designed to manipulate recommendations for specific user subgroups in the
federated setting. Specifically, Spattack adopts a two-stage
approximation-and-promotion strategy, which first simulates user embeddings of
target/non-target subgroups and then prompts target items to the target
subgroups. To enhance the approximation stage, we push the inter-group
embeddings away based on contrastive learning and augment the target group's
relevant item set based on clustering. To enhance the promotion stage, we
further propose to adaptively tune the optimization weights between target and
non-target subgroups. Besides, an embedding alignment strategy is proposed to
align the embeddings between the target items and the relevant items. We
conduct comprehensive experiments on three real-world datasets, comparing
Spattack against seven state-of-the-art poisoning attacks and seven
representative defense mechanisms. Experimental results demonstrate that
Spattack consistently achieves strong manipulation performance on the specific
user subgroup, while incurring minimal impact on non-target users, even when
only 0.1\% of users are malicious. Moreover, Spattack maintains competitive
overall recommendation performance and exhibits strong resilience against
existing mainstream defenses.

</details>


### [98] [We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems](https://arxiv.org/abs/2507.06250)
*Zhihao Li,Kun Li,Boyang Ma,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 本文首次对MCP（Model Context Protocol）的安全风险进行了大规模实证分析，揭示了插件权限滥用和隔离不足的问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然提供了强大的扩展性和集成能力，但也带来了显著的安全风险，尤其是插件权限滥用和隔离不足的问题。

Method: 开发了一个自动化静态分析框架，系统分析了2,562个实际MCP应用，涵盖23个功能类别。

Result: 研究发现网络和系统资源API使用最广泛，开发者工具和API开发插件风险最高，权限分离不足导致特权升级等问题。

Conclusion: 提出了MCP资源访问的分类法，量化了安全相关的API使用，并指出了动态权限模型和自动信任评估等开放挑战。

Abstract: The Model Context Protocol (MCP) has emerged as a widely adopted mechanism
for connecting large language models to external tools and resources. While MCP
promises seamless extensibility and rich integrations, it also introduces a
substantially expanded attack surface: any plugin can inherit broad system
privileges with minimal isolation or oversight. In this work, we conduct the
first large-scale empirical analysis of MCP security risks. We develop an
automated static analysis framework and systematically examine 2,562 real-world
MCP applications spanning 23 functional categories. Our measurements reveal
that network and system resource APIs dominate usage patterns, affecting 1,438
and 1,237 servers respectively, while file and memory resources are less
frequent but still significant. We find that Developer Tools and API
Development plugins are the most API-intensive, and that less popular plugins
often contain disproportionately high-risk operations. Through concrete case
studies, we demonstrate how insufficient privilege separation enables privilege
escalation, misinformation propagation, and data tampering. Based on these
findings, we propose a detailed taxonomy of MCP resource access, quantify
security-relevant API usage, and identify open challenges for building safer
MCP ecosystems, including dynamic permission models and automated trust
assessment.

</details>


### [99] [An Architecture for Privacy-Preserving Telemetry Scheme](https://arxiv.org/abs/2507.06350)
*Kenneth Odoh*

Main category: cs.CR

TL;DR: 提出了一种隐私保护的遥测聚合方案，结合差分隐私和客户端-服务器架构，利用本地差分隐私和Oblivious HTTP增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决传统遥测数据聚合中的隐私泄露问题，防止重新识别攻击，同时支持公开数据发布。

Method: 采用本地差分隐私方案，客户端随机化数据后提交至服务器，结合Oblivious HTTP保护传输数据隐私。

Result: 方案在频率估计中表现良好，提供了比传统方法更严格的隐私保护。

Conclusion: 该方案有效增强了隐私保护，适用于需要高隐私保障的遥测数据聚合场景。

Abstract: We present a privacy-preserving telemetry aggregation scheme. Our underlying
frequency estimation routine works within the framework of differential
privacy. The design philosophy follows a client-server architecture.
Furthermore, the system uses a local differential privacy scheme where data
gets randomized on the client before submitting the request to the resource
server. This scheme allows for data analysis on de-identified data by carefully
adding noise to prevent re-identification attacks, thereby facilitating public
data release without compromising the identifiability of the individual record.
This work further enhances privacy guarantees by leveraging Oblivious HTTP
(OHTTP) to achieve increased privacy protection for data in transit that
addresses pre-existing privacy vulnerabilities in raw HTTP. We provide an
implementation that focuses on frequency estimation with a histogram of a known
dictionary. Our resulting formulation based on OHTTP has provided stricter
privacy safeguards when compared to trusting an organization to manually delete
identifying information from the client's request in the ingestor as deployed
in reference work~\cite{apple2017}. Code available at
https://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.

</details>


### [100] [TELSAFE: Security Gap Quantitative Risk Assessment Framework](https://arxiv.org/abs/2507.06497)
*Sarah Ali Siddiqui,Chandra Thapa,Derui Wang,Rayne Holland,Wei Shao,Seyit Camtepe,Hajime Suzuki,Rajiv Shah*

Main category: cs.CR

TL;DR: 本文提出了一种名为TELSAFE的混合风险评估框架，结合概率建模消除专家意见偏见，适用于实际场景如电信行业。


<details>
  <summary>Details</summary>
Motivation: 现有安全标准与实际实施之间的差距可能导致漏洞和安全风险，需有效管理策略。

Method: 提出TELSAFE框架，结合定性与定量评估，利用概率建模进行定量风险评估。

Result: 框架在实际案例（如电信行业）中展示了适用性和实施效果。

Conclusion: TELSAFE框架为组织提供了定制化的风险管理策略，解决了标准与实践间的差距。

Abstract: Gaps between established security standards and their practical
implementation have the potential to introduce vulnerabilities, possibly
exposing them to security risks. To effectively address and mitigate these
security and compliance challenges, security risk management strategies are
essential. However, it must adhere to well-established strategies and industry
standards to ensure consistency, reliability, and compatibility both within and
across organizations. In this paper, we introduce a new hybrid risk assessment
framework called TELSAFE, which employs probabilistic modeling for quantitative
risk assessment and eliminates the influence of expert opinion bias. The
framework encompasses both qualitative and quantitative assessment phases,
facilitating effective risk management strategies tailored to the unique
requirements of organizations. A specific use case utilizing Common
Vulnerabilities and Exposures (CVE)-related data demonstrates the framework's
applicability and implementation in real-world scenarios, such as in the
telecommunications industry.

</details>


### [101] [False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems](https://arxiv.org/abs/2507.06252)
*Samaneh Shafee,Alysson Bessani,Pedro M. Ferreira*

Main category: cs.CR

TL;DR: 该论文研究了网络威胁情报（CTI）管道中对抗性攻击的漏洞，重点关注逃避、洪泛和投毒攻击，并分析了其对系统信息选择能力的影响。


<details>
  <summary>Details</summary>
Motivation: 由于CTI管道从开放源（如社交媒体和论坛）获取文本输入，容易受到对抗性攻击，尤其是虚假内容的影响。

Method: 研究分析了三种攻击类型（逃避、洪泛和投毒），并评估了它们对CTI管道的影响，特别是通过对抗性文本生成技术制造虚假网络安全文本。

Result: 研究发现对抗性文本生成技术可以误导分类器、降低性能并破坏系统功能，尤其是逃避攻击为后续攻击提供了条件。

Conclusion: 论文强调了CTI管道中对抗性攻击的严重性，并指出需要进一步研究以增强其安全性。

Abstract: Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach
that operates in the early phases of the cyber threat lifecycle. CTI involves
collecting, processing, and analyzing threat data to provide a more accurate
and rapid understanding of cyber threats. Due to the large volume of data,
automation through Machine Learning (ML) and Natural Language Processing (NLP)
models is essential for effective CTI extraction. These automated systems
leverage Open Source Intelligence (OSINT) from sources like social networks,
forums, and blogs to identify Indicators of Compromise (IoCs). Although prior
research has focused on adversarial attacks on specific ML models, this study
expands the scope by investigating vulnerabilities within various components of
the entire CTI pipeline and their susceptibility to adversarial attacks. These
vulnerabilities arise because they ingest textual inputs from various open
sources, including real and potentially fake content. We analyse three types of
attacks against CTI pipelines, including evasion, flooding, and poisoning, and
assess their impact on the system's information selection capabilities.
Specifically, on fake text generation, the work demonstrates how adversarial
text generation techniques can create fake cybersecurity and cybersecurity-like
text that misleads classifiers, degrades performance, and disrupts system
functionality. The focus is primarily on the evasion attack, as it precedes and
enables flooding and poisoning attacks within the CTI pipeline.

</details>


### [102] [Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method](https://arxiv.org/abs/2507.06262)
*Haoqi He,Xiaokai Lin,Jiancai Chen,Yan Xiao*

Main category: cs.CR

TL;DR: 提出了一种基于量子计算的混合防御方法Q-Detection，用于检测数据投毒攻击，实验证明其优于基线方法，并有望实现20%以上的加速。


<details>
  <summary>Details</summary>
Motivation: 数据投毒攻击对机器学习模型构成威胁，传统计算框架在大规模复杂数据集上检测困难，量子计算提供了新的解决方案。

Method: 提出Q-Detection方法，结合量子计算设备优化的Q-WAN，通过量子模拟库进行实验验证。

Result: Q-Detection有效防御标签操纵和后门攻击，性能优于基线方法，理论分析显示量子计算可带来20%以上的加速。

Conclusion: Q-Detection为数据投毒检测提供了高效解决方案，量子计算的引入显著提升了性能。

Abstract: Data poisoning attacks pose significant threats to machine learning models by
introducing malicious data into the training process, thereby degrading model
performance or manipulating predictions. Detecting and sifting out poisoned
data is an important method to prevent data poisoning attacks. Limited by
classical computation frameworks, upcoming larger-scale and more complex
datasets may pose difficulties for detection. We introduce the unique speedup
of quantum computing for the first time in the task of detecting data
poisoning. We present Q-Detection, a quantum-classical hybrid defense method
for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which
is optimized using quantum computing devices. Experimental results using
multiple quantum simulation libraries show that Q-Detection effectively defends
against label manipulation and backdoor attacks. The metrics demonstrate that
Q-Detection consistently outperforms the baseline methods and is comparable to
the state-of-the-art. Theoretical analysis shows that Q-Detection is expected
to achieve more than a 20% speedup using quantum computing power.

</details>


### [103] [ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation](https://arxiv.org/abs/2507.07031)
*Bing-Jyue Chen,Lilia Tang,Daniel Kang*

Main category: cs.CR

TL;DR: ZKTorch是一个开源系统，通过将ML模型编译为基本加密操作块，并使用并行扩展的Mira累积方案，显著减少了证明大小和验证时间。


<details>
  <summary>Details</summary>
Motivation: 解决现有零知识证明方法在ML模型中的效率低和通用性差的问题。

Method: 将ML模型编译为基本加密操作块，使用并行扩展的Mira累积方案生成简洁证明。

Result: 证明大小减少3倍，验证时间加快6倍。

Conclusion: ZKTorch为ML模型的零知识证明提供了高效且通用的解决方案。

Abstract: As AI models become ubiquitous in our daily lives, there has been an
increasing demand for transparency in ML services. However, the model owner
does not want to reveal the weights, as they are considered trade secrets. To
solve this problem, researchers have turned to zero-knowledge proofs of ML
model inference. These proofs convince the user that the ML model output is
correct, without revealing the weights of the model to the user. Past work on
these provers can be placed into two categories. The first method compiles the
ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The
second method uses custom cryptographic protocols designed only for a specific
class of models. Unfortunately, the first method is highly inefficient, making
it impractical for the large models used today, and the second method does not
generalize well, making it difficult to update in the rapidly changing field of
machine learning. To solve this, we propose ZKTorch, an open source end-to-end
proving system that compiles ML models into base cryptographic operations
called basic blocks, each proved using specialized protocols. ZKTorch is built
on top of a novel parallel extension to the Mira accumulation scheme, enabling
succinct proofs with minimal accumulation overhead. These contributions allow
ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to
specialized protocols and up to a $6\times$ speedup in proving time over a
general-purpose ZKML framework.

</details>


### [104] [LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing](https://arxiv.org/abs/2507.07056)
*Jiahao Chen,junhao li,Yiming Wang,Zhe Ma,Yi Jiang,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: LoRAShield是一种数据无关的编辑框架，用于保护LoRA模型免受滥用，通过动态编辑和重新对齐权重子空间来阻止恶意生成，同时保留良性任务的功能。


<details>
  <summary>Details</summary>
Motivation: LoRA模型的共享生态系统存在被滥用的风险，现有防御方法忽视其模块化适配器的特性，需要一种针对性的解决方案。

Method: 提出LoRAShield，通过对抗性优化和语义增强动态编辑LoRA的权重子空间。

Result: 实验证明LoRAShield在阻止恶意生成方面高效、有效且鲁棒，同时不影响良性任务。

Conclusion: LoRAShield为生成生态系统提供了安全、可扩展的共享方案，是迈向可信生成的关键一步。

Abstract: The proliferation of Low-Rank Adaptation (LoRA) models has democratized
personalized text-to-image generation, enabling users to share lightweight
models (e.g., personal portraits) on platforms like Civitai and Liblib.
However, this "share-and-play" ecosystem introduces critical risks: benign
LoRAs can be weaponized by adversaries to generate harmful content (e.g.,
political, defamatory imagery), undermining creator rights and platform safety.
Existing defenses like concept-erasure methods focus on full diffusion models
(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability
to adversarial prompt engineering. To bridge this gap, we propose LoRAShield,
the first data-free editing framework for securing LoRA models against misuse.
Our platform-driven approach dynamically edits and realigns LoRA's weight
subspace via adversarial optimization and semantic augmentation. Experimental
results demonstrate that LoRAShield achieves remarkable effectiveness,
efficiency, and robustness in blocking malicious generations without
sacrificing the functionality of the benign task. By shifting the defense to
platforms, LoRAShield enables secure, scalable sharing of personalized models,
a critical step toward trustworthy generative ecosystems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [105] [Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data](https://arxiv.org/abs/2507.06336)
*Adam J Riesselman,Evan M Cofer,Therese LaRue,Wim Meeussen*

Main category: q-bio.QM

TL;DR: 利用移动机器人平台和大规模水培绿叶蔬菜系统的数据，通过自监督建模方法预测植物生长轨迹，为农业研究和效率提供支持。


<details>
  <summary>Details</summary>
Motivation: 量化植物生长动态和生物量积累对理解农艺性状和优化作物生产至关重要，但大规模高质量生长数据难以获取。

Method: 使用移动机器人平台捕获高分辨率环境感知和表型测量数据，采用自监督建模方法构建从观测数据到植物生长轨迹的映射。

Result: 成功预测了未来植物高度和收获质量，展示了机器人自动化与机器学习结合的潜力。

Conclusion: 该方法为农业研究和操作效率提供了可操作的见解，是机器人自动化与机器学习结合的重要进展。

Abstract: Quantifying organism-level phenotypes, such as growth dynamics and biomass
accumulation, is fundamental to understanding agronomic traits and optimizing
crop production. However, quality growing data of plants at scale is difficult
to generate. Here we use a mobile robotic platform to capture high-resolution
environmental sensing and phenotyping measurements of a large-scale hydroponic
leafy greens system. We describe a self-supervised modeling approach to build a
map from observed growing data to the entire plant growth trajectory. We
demonstrate our approach by forecasting future plant height and harvest mass of
crops in this system. This approach represents a significant advance in
combining robotic automation and machine learning, as well as providing
actionable insights for agronomic research and operational efficiency.

</details>


### [106] [DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning](https://arxiv.org/abs/2507.07060)
*Shreyas Vinaya Sathyanarayana,Rahil Shah,Sharanabasava D. Hiremath,Rishikesh Panda,Rahul Jana,Riya Singh,Rida Irfan,Ashwin Murali,Bharath Ramsundar*

Main category: q-bio.QM

TL;DR: DeepRetro结合模板和LLM的迭代框架，用于改进逆合成分析，生成新路径。


<details>
  <summary>Details</summary>
Motivation: 传统逆合成方法受限于预定义模板，LLM虽有望但多步规划能力尚未充分开发。

Method: 结合模板引擎和LLM生成单步断开建议，通过反馈循环动态优化路径。

Result: 在基准测试和案例中成功识别可行及新颖路径，支持人机交互反馈。

Conclusion: 迭代LLM推理可提升复杂化学合成的前沿水平。

Abstract: Retrosynthesis, the identification of precursor molecules for a target
compound, is pivotal for synthesizing complex molecules, but faces challenges
in discovering novel pathways beyond predefined templates. Recent large
language model (LLM) approaches to retrosynthesis have shown promise but
effectively harnessing LLM reasoning capabilities for effective multi-step
planning remains an open question. To address this challenge, we introduce
DeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic
framework. Our approach integrates the strengths of conventional
template-based/Monte Carlo tree search tools with the generative power of LLMs
in a step-wise, feedback-driven loop. Initially, synthesis planning is
attempted with a template-based engine. If this fails, the LLM subsequently
proposes single-step retrosynthetic disconnections. Crucially, these
suggestions undergo rigorous validity, stability, and hallucination checks
before the resulting precursors are recursively fed back into the pipeline for
further evaluation. This iterative refinement allows for dynamic pathway
exploration and correction. We demonstrate the potential of this pipeline
through benchmark evaluations and case studies, showcasing its ability to
identify viable and potentially novel retrosynthetic routes. In particular, we
develop an interactive graphical user interface that allows expert human
chemists to provide human-in-the-loop feedback to the reasoning algorithm. This
approach successfully generates novel pathways for complex natural product
compounds, demonstrating the potential for iterative LLM reasoning to advance
state-of-art in complex chemical syntheses.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [107] [Deep learning-based species-area models reveal multi-scale patterns of species richness and turnover](https://arxiv.org/abs/2507.06358)
*Victor Boussange,Philipp Brun,Johanna T. Malle,Gabriele Midolo,Jeanne Portier,Théophile Sanchez,Niklaus E. Zimmermann,Irena Axmanová,Helge Bruelheide,Milan Chytrý,Stephan Kambach,Zdeňka Lososová,Martin Večeřa,Idoia Biurrun,Klaus T. Ecker,Jonathan Lenoir,Jens-Christian Svenning,Dirk Nikolaus Karger*

Main category: q-bio.PE

TL;DR: 论文提出了一种深度学习模型，利用采样理论和小规模生态调查，预测欧洲维管植物群落的物种丰富度，并验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 理解物种丰富度在不同空间尺度上的动态变化，但由于缺乏全面的生物多样性记录，这一目标难以实现。

Method: 开发了一种结合采样理论和深度学习的模型，预测物种丰富度，并通过独立数据集验证。

Result: 模型将物种丰富度估计提高了32%，并提供了从平方米到数百平方公里的物种丰富度和更替模式。

Conclusion: 该模型能有效捕捉生物多样性的多尺度特性，为全球变化下的生物多样性评估和预测提供了可靠工具。

Abstract: The number of species within ecosystems is influenced not only by their
intrinsic characteristics but also by the spatial scale considered. As the
sampled area expands, species richness increases, a phenomenon described by the
species-area relationship (SAR). The accumulation dynamics of the SAR results
from a complex interplay of biotic and abiotic processes operating at various
spatial scales. However, the challenge of collecting exhaustive biodiversity
records across spatial scales has hindered a comprehensive understanding of
these dynamics. Here, we develop a deep learning approach that leverages
sampling theory and small-scale ecological surveys to spatially resolve the
scale-dependency of species richness. We demonstrate its performance by
predicting the species richness of vascular plant communities across Europe,
and evaluate the predictions against an independent dataset of plant community
inventories. Our model improves species richness estimates by 32\% and delivers
spatially explicit patterns of species richness and turnover for sampling areas
ranging from square meters to hundreds of square kilometers. Explainable AI
techniques further disentangle how drivers of species richness operate across
spatial scales. The ability of our model to represent the multi-scale nature of
biodiversity is essential to deliver robust biodiversity assessments and
forecasts under global change.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [108] [Machine-Learned Force Fields for Lattice Dynamics at Coupled-Cluster Level Accuracy](https://arxiv.org/abs/2507.06929)
*Sita Schönbauer,Johanna P. Carbone,Andreas Grüneis*

Main category: cond-mat.mtrl-sci

TL;DR: 论文研究了基于近似密度泛函理论（DFT）和耦合簇（CC）势能面的机器学习力场（MLFFs），评估了其在碳金刚石和氢化锂固体中的准确性，并通过声子色散和振动态密度（VDOS）与实验和参考结果对比。


<details>
  <summary>Details</summary>
Motivation: 探讨MLFFs在模拟固体材料振动性质中的表现，尤其是克服CC训练数据中长程效应和原子力缺失的限制。

Method: 采用基于CC和DFT差异的delta-learning方法，训练MLFFs并计算声子色散和VDOS。

Result: 与DFT相比，基于CC的MLFFs在光学模式振动频率上更接近实验结果，并用于估计氢化锂的VDOS非谐效应。

Conclusion: 基于CC的MLFFs在模拟固体振动性质上优于DFT，delta-learning方法有效解决了数据限制问题。

Abstract: We investigate Machine-Learned Force Fields (MLFFs) trained on approximate
Density Functional Theory (DFT) and Coupled Cluster (CC) level potential energy
surfaces for the carbon diamond and lithium hydride solids. We assess the
accuracy and precision of the MLFFs by calculating phonon dispersions and
vibrational densities of states (VDOS) that are compared to experiment and
reference ab initio results. To overcome limitations from long-range effects
and the lack of atomic forces in the CC training data, a delta-learning
approach based on the difference between CC and DFT results is explored.
Compared to DFT, MLFFs trained on CC theory yield higher vibrational
frequencies for optical modes, agreeing better with experiment. Furthermore,
the MLFFs are used to estimate anharmonic effects on the VDOS of lithium
hydride at the level of CC theory.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [109] [A Machine Learning Framework for Breast Cancer Treatment Classification Using a Novel Dataset](https://arxiv.org/abs/2507.06243)
*Md Nahid Hasan,Md Monzur Murshed,Md Mahadi Hasan,Faysal A. Chowdhury*

Main category: stat.AP

TL;DR: 利用机器学习模型预测乳腺癌患者接受化疗或激素治疗的可能性，GBM表现最佳。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌治疗的个性化选择因分子和临床异质性而复杂化，机器学习可提供数据驱动的预测支持。

Method: 使用TCGA乳腺癌临床数据集，通过五折交叉验证训练和评估多种ML模型，包括GBM、XGBoost和AdaBoost。

Result: GBM表现最优（准确率0.7718，AUROC 0.8252），XGBoost和AdaBoost次之。

Conclusion: 机器学习在个性化乳腺癌治疗决策中具有潜力。

Abstract: Breast cancer (BC) remains a significant global health challenge, with
personalized treatment selection complicated by the disease's molecular and
clinical heterogeneity. BC treatment decisions rely on various patient-specific
clinical factors, and machine learning (ML) offers a powerful approach to
predicting treatment outcomes. This study utilizes The Cancer Genome Atlas
(TCGA) breast cancer clinical dataset to develop ML models for predicting the
likelihood of undergoing chemotherapy or hormonal therapy. The models are
trained using five-fold cross-validation and evaluated through performance
metrics, including accuracy, precision, recall, specificity, sensitivity,
F1-score, and area under the receiver operating characteristic curve (AUROC).
Model uncertainty is assessed using bootstrap techniques, while SHAP values
enhance interpretability by identifying key predictors. Among the tested
models, the Gradient Boosting Machine (GBM) achieves the highest stable
performance (accuracy = 0.7718, AUROC = 0.8252), followed by Extreme Gradient
Boosting (XGBoost) (accuracy = 0.7557, AUROC = 0.8044) and Adaptive Boosting
(AdaBoost) (accuracy = 0.7552, AUROC = 0.8016). These findings underscore the
potential of ML in supporting personalized breast cancer treatment decisions
through data-driven insights.

</details>


### [110] [When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior](https://arxiv.org/abs/2507.07012)
*Chengyuan Zhang,Zhengbing He,Cathy Wu,Lijun Sun*

Main category: stat.AP

TL;DR: 提出了一种结合深度神经网络和非平稳高斯过程的随机建模框架，用于更准确地模拟跟车行为中的不确定性和潜在因素。


<details>
  <summary>Details</summary>
Motivation: 传统确定性模型无法完全捕捉人类驾驶中的变异性，而现代方法常忽略潜在因素（如驾驶员意图、感知误差等）。

Method: 采用深度神经网络与非平稳高斯过程结合，使用场景自适应的Gibbs核学习加速度决策的动态时间相关性。

Result: 在HighD数据集上的实验表明，该方法在预测性能和不确定性量化方面优于传统方法。

Conclusion: 该框架兼具可解释性和准确性，适用于交通分析和安全关键应用。

Abstract: Modeling car-following behavior is fundamental to microscopic traffic
simulation, yet traditional deterministic models often fail to capture the full
extent of variability and unpredictability in human driving. While many modern
approaches incorporate context-aware inputs (e.g., spacing, speed, relative
speed), they frequently overlook structured stochasticity that arises from
latent driver intentions, perception errors, and memory effects -- factors that
are not directly observable from context alone. To fill the gap, this study
introduces an interpretable stochastic modeling framework that captures not
only context-dependent dynamics but also residual variability beyond what
context can explain. Leveraging deep neural networks integrated with
nonstationary Gaussian processes (GPs), our model employs a scenario-adaptive
Gibbs kernel to learn dynamic temporal correlations in acceleration decisions,
where the strength and duration of correlations between acceleration decisions
evolve with the driving context. This formulation enables a principled,
data-driven quantification of uncertainty in acceleration, speed, and spacing,
grounded in both observable context and latent behavioral variability.
Comprehensive experiments on the naturalistic vehicle trajectory dataset
collected from the German highway, i.e., the HighD dataset, demonstrate that
the proposed stochastic simulation method within this framework surpasses
conventional methods in both predictive performance and interpretable
uncertainty quantification. The integration of interpretability and accuracy
makes this framework a promising tool for traffic analysis and safety-critical
applications.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [111] [Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity](https://arxiv.org/abs/2507.06479)
*Niloofar Asefi,Leonard Lupin-Jimenez,Tianning Wu,Ruoying He,Ashesh Chattopadhyay*

Main category: physics.ao-ph

TL;DR: 论文提出了一种结合神经算子和去噪扩散概率模型（DDPMs）的深度学习框架，用于从极稀疏的拉格朗日观测数据中重建高分辨率海洋状态。


<details>
  <summary>Details</summary>
Motivation: 海洋观测数据稀疏、不规则且多为拉格朗日采样，限制了海洋动力学的重建，尤其是对中尺度湍流等关键现象的预测。

Method: 结合神经算子和DDPMs的深度学习框架，通过生成模型在神经算子输出上条件化，捕捉小尺度高波数动力学。

Result: 在合成数据和真实卫星观测中，即使数据稀疏度高达99%和99.9%，模型仍能准确重建海洋状态，性能优于其他深度学习方法。

Conclusion: 该方法在极端稀疏采样条件下表现出色，为海洋动力学重建提供了新思路。

Abstract: Reconstructing ocean dynamics from observational data is fundamentally
limited by the sparse, irregular, and Lagrangian nature of spatial sampling,
particularly in subsurface and remote regions. This sparsity poses significant
challenges for forecasting key phenomena such as eddy shedding and rogue waves.
Traditional data assimilation methods and deep learning models often struggle
to recover mesoscale turbulence under such constraints. We leverage a deep
learning framework that combines neural operators with denoising diffusion
probabilistic models (DDPMs) to reconstruct high-resolution ocean states from
extremely sparse Lagrangian observations. By conditioning the generative model
on neural operator outputs, the framework accurately captures small-scale,
high-wavenumber dynamics even at $99\%$ sparsity (for synthetic data) and
$99.9\%$ sparsity (for real satellite observations). We validate our method on
benchmark systems, synthetic float observations, and real satellite data,
demonstrating robust performance under severe spatial sampling limitations as
compared to other deep learning baselines.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [112] [Graph-based Fake Account Detection: A Survey](https://arxiv.org/abs/2507.06541)
*Ali Safarpoor Dehkordi,Ahad N. Zehmakan*

Main category: cs.SI

TL;DR: 本文综述了社交网络中虚假账户检测的图基方法，分类讨论了现有技术的优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着虚假账户问题的日益严重，开发高效检测算法成为研究重点。

Method: 基于图拓扑特征并结合账户信息（如内容和资料数据），分类分析了现有技术。

Result: 总结了现有方法的优缺点，并探讨了数据集（真实与合成）的可用性。

Conclusion: 提出了未来研究的潜在方向。

Abstract: In recent years, there has been a growing effort to develop effective and
efficient algorithms for fake account detection in online social networks. This
survey comprehensively reviews existing methods, with a focus on graph-based
techniques that utilise topological features of social graphs (in addition to
account information, such as their shared contents and profile data) to
distinguish between fake and real accounts. We provide several categorisations
of these methods (for example, based on techniques used, input data, and
detection time), discuss their strengths and limitations, and explain how these
methods connect in the broader context. We also investigate the available
datasets, including both real-world data and synthesised models. We conclude
the paper by proposing several potential avenues for future research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [113] [On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective](https://arxiv.org/abs/2507.06552)
*Zhiyi Dong,Zixuan Liu,Yongyi Mao*

Main category: stat.ML

TL;DR: 本文研究了协变量偏移下无监督域适应（UDA）的困难性，通过引入后验目标标签不确定性（PTLU）及其经验估计（EPTLU）来量化学习难度。


<details>
  <summary>Details</summary>
Motivation: 传统的最坏情况分析过于悲观，本文提出了一种新的框架，通过耦合源分布、目标分布和分类器来更准确地评估UDA的困难性。

Method: 定义了UDA类，并通过平均目标域风险来衡量学习器性能。引入PTLU和EPTLU作为信息论量来量化学习难度。

Result: 证明了PTLU可以下界任何学习器的风险，并通过示例展示了PTLU在评估UDA学习难度上的优势。

Conclusion: PTLU和EPTLU是评估UDA学习难度的有效代理指标，优于现有方法。

Abstract: This paper studies the hardness of unsupervised domain adaptation (UDA) under
covariate shift. We model the uncertainty that the learner faces by a
distribution $\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a
UDA class -- where $(p, q)$ is the source -- target distribution pair and $f$
is the classifier. We define the performance of a learner as the overall target
domain risk, averaged over the randomness of the ground-truth triple. This
formulation couples the source distribution, the target distribution and the
classifier in the ground truth, and deviates from the classical worst-case
analyses, which pessimistically emphasize the impact of hard but rare UDA
instances. In this formulation, we precisely characterize the optimal learner.
The performance of the optimal learner then allows us to define the learning
difficulty for the UDA class and for the observed sample. To quantify this
difficulty, we introduce an information-theoretic quantity -- Posterior Target
Label Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the
sample , which capture the uncertainty in the prediction for the target domain.
Briefly, PTLU is the entropy of the predicted label in the target domain under
the posterior distribution of ground-truth classifier given the observed source
and target samples. By proving that such a quantity serves to lower-bound the
risk of any learner, we suggest that these quantities can be used as proxies
for evaluating the hardness of UDA learning. We provide several examples to
demonstrate the advantage of PTLU, relative to the existing measures, in
evaluating the difficulty of UDA learning.

</details>


### [114] [Semi-parametric Functional Classification via Path Signatures Logistic Regression](https://arxiv.org/abs/2507.06637)
*Pengcheng Zeng,Siyuan Jiang*

Main category: stat.ML

TL;DR: PSLR是一种半参数框架，用于分类带有标量协变量的向量值函数数据，通过路径签名克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统函数逻辑回归模型依赖线性假设和固定基展开，灵活性不足且在不规则采样下性能下降。

Method: 利用截断路径签名构建有限维、无基表示，捕捉非线性和跨通道依赖，嵌入时间增强路径提取稳定特征。

Result: 在合成和真实数据集上，PSLR在准确性、鲁棒性和可解释性上优于传统分类器，尤其在不均匀采样下表现突出。

Conclusion: PSLR将粗糙路径理论融入现代函数数据分析，具有理论和实践优势。

Abstract: We propose Path Signatures Logistic Regression (PSLR), a semi-parametric
framework for classifying vector-valued functional data with scalar covariates.
Classical functional logistic regression models rely on linear assumptions and
fixed basis expansions, which limit flexibility and degrade performance under
irregular sampling. PSLR overcomes these issues by leveraging truncated path
signatures to construct a finite-dimensional, basis-free representation that
captures nonlinear and cross-channel dependencies. By embedding trajectories as
time-augmented paths, PSLR extracts stable, geometry-aware features that are
robust to sampling irregularity without requiring a common time grid, while
still preserving subject-specific timing patterns. We establish theoretical
guarantees for the existence and consistent estimation of the optimal
truncation order, along with non-asymptotic risk bounds. Experiments on
synthetic and real-world datasets show that PSLR outperforms traditional
functional classifiers in accuracy, robustness, and interpretability,
particularly under non-uniform sampling schemes. Our results highlight the
practical and theoretical benefits of integrating rough path theory into modern
functional data analysis.

</details>


### [115] [Fast Gaussian Processes under Monotonicity Constraints](https://arxiv.org/abs/2507.06677)
*Chao Zhang,Jasper M. Everink,Jakob Sauer Jørgensen*

Main category: stat.ML

TL;DR: 提出了一种基于虚拟点的高斯过程（GP）框架，结合单调性约束，通过RLRTO方法高效采样，并改进了现有方法，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 利用先验知识（如单调性）改进GP模型的预测精度和减少不确定性，但高维问题计算复杂。

Method: 采用RLRTO方法进行约束后验分布的高效采样，并用NUTS替代Gibbs采样改进现有虚拟点方法。

Result: 在合成函数和微分方程系统上验证，所有方法预测性能相当，但RLRTO和NUTS方法显著提升计算效率。

Conclusion: 提出的框架能高效构建约束GP模型，适用于广泛问题，尤其是高维场景。

Abstract: Gaussian processes (GPs) are widely used as surrogate models for complicated
functions in scientific and engineering applications. In many cases, prior
knowledge about the function to be approximated, such as monotonicity, is
available and can be leveraged to improve model fidelity. Incorporating such
constraints into GP models enhances predictive accuracy and reduces
uncertainty, but remains a computationally challenging task for
high-dimensional problems. In this work, we present a novel virtual point-based
framework for building constrained GP models under monotonicity constraints,
based on regularized linear randomize-then-optimize (RLRTO), which enables
efficient sampling from a constrained posterior distribution by means of
solving randomized optimization problems. We also enhance two existing virtual
point-based approaches by replacing Gibbs sampling with the No U-Turn Sampler
(NUTS) for improved efficiency. A Python implementation of these methods is
provided and can be easily applied to a wide range of problems. This
implementation is then used to validate the approaches on approximating a range
of synthetic functions, demonstrating comparable predictive performance between
all considered methods and significant improvements in computational efficiency
with the two NUTS methods and especially with the RLRTO method. The framework
is further applied to construct surrogate models for systems of differential
equations.

</details>


### [116] [Adaptive collaboration for online personalized distributed learning with heterogeneous clients](https://arxiv.org/abs/2507.06844)
*Constantin Philippenko,Batiste Le Bars,Kevin Scaman,Laurent Massoulié*

Main category: stat.ML

TL;DR: 本文研究了在线个性化去中心化学习问题，提出了一种基于梯度的协作准则，动态选择相似梯度的客户端以减少梯度方差并缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习中客户端统计异构性带来的挑战，通过动态选择协作伙伴优化训练效果。

Method: 引入梯度协作准则，动态选择梯度相似的客户端，提出两种协作方法并验证其效果。

Result: 理论分析表明算法可作为方差减少方法，实验验证了方法的有效性。

Conclusion: 提出的协作准则和方法在理论和实验上均表现良好，尤其适用于异构客户端场景。

Abstract: We study the problem of online personalized decentralized learning with $N$
statistically heterogeneous clients collaborating to accelerate local training.
An important challenge in this setting is to select relevant collaborators to
reduce gradient variance while mitigating the introduced bias. To tackle this,
we introduce a gradient-based collaboration criterion, allowing each client to
dynamically select peers with similar gradients during the optimization
process. Our criterion is motivated by a refined and more general theoretical
analysis of the All-for-one algorithm, proved to be optimal in Even et al.
(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds
for smooth objective functions, being either strongly convex, non-convex, or
satisfying the Polyak-Lojasiewicz condition; our analysis reveals that the
algorithm acts as a variance reduction method where the speed-up depends on a
sufficient variance. We put forward two collaboration methods instantiating the
proposed general schema; and we show that one variant preserves the optimality
of All-for-one. We validate our results with experiments on synthetic and real
datasets.

</details>


### [117] [Conformal Prediction for Long-Tailed Classification](https://arxiv.org/abs/2507.06867)
*Tiffany Ding,Jean-Baptiste Fermanian,Joseph Salmon*

Main category: stat.ML

TL;DR: 论文提出了一种针对长尾分布分类问题的新方法，通过调整预测集的覆盖率和大小，平衡稀有类别的覆盖和预测集的实用性。


<details>
  <summary>Details</summary>
Motivation: 解决长尾分布分类问题中现有方法无法同时保证预测集的小规模和稀有类别的覆盖率的矛盾。

Method: 提出了两种方法：1）基于宏覆盖的prevalence-adjusted softmax评分函数；2）标签加权的保形预测方法，用于在边际和类别条件覆盖之间插值。

Result: 在Pl@ntNet和iNaturalist两个长尾图像数据集上验证了方法的有效性。

Conclusion: 新方法能够在保证边际覆盖的同时，灵活平衡预测集大小和类别条件覆盖，适用于长尾分布分类问题。

Abstract: Many real-world classification problems, such as plant identification, have
extremely long-tailed class distributions. In order for prediction sets to be
useful in such settings, they should (i) provide good class-conditional
coverage, ensuring that rare classes are not systematically omitted from the
prediction sets, and (ii) be a reasonable size, allowing users to easily verify
candidate labels. Unfortunately, existing conformal prediction methods, when
applied to the long-tailed setting, force practitioners to make a binary choice
between small sets with poor class-conditional coverage or sets with very good
class-conditional coverage but that are extremely large. We propose methods
with guaranteed marginal coverage that smoothly trade off between set size and
class-conditional coverage. First, we propose a conformal score function,
prevalence-adjusted softmax, that targets a relaxed notion of class-conditional
coverage called macro-coverage. Second, we propose a label-weighted conformal
prediction method that allows us to interpolate between marginal and
class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet
and iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,
respectively.

</details>


### [118] [Distribution-free inference for LightGBM and GLM with Tweedie loss](https://arxiv.org/abs/2507.06921)
*Alokesh Manna,Aditya Vikram Sett,Dipak K. Dey,Yuwen Gu,Elizabeth D. Schifano,Jichao He*

Main category: stat.ML

TL;DR: 论文提出了一种新的非一致性度量方法，用于GLMs和GBMs，以改进保险索赔预测中的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 保险行业需要更准确地量化预测不确定性，以提高保费定价的精确性和风险管理能力。

Method: 使用正则化Tweedie GLM回归和LightGBM（带Tweedie损失）进行共形预测，并提出了新的非一致性度量方法。

Result: 模拟结果显示，使用局部加权Pearson残差的LightGBM方法在保持名义覆盖范围的同时，平均区间宽度最小。

Conclusion: 提出的方法在保险索赔数据中表现优异，尤其是局部加权Pearson残差的LightGBM方法效果最佳。

Abstract: Prediction uncertainty quantification is a key research topic in recent years
scientific and business problems. In insurance industries
(\cite{parodi2023pricing}), assessing the range of possible claim costs for
individual drivers improves premium pricing accuracy. It also enables insurers
to manage risk more effectively by accounting for uncertainty in accident
likelihood and severity. In the presence of covariates, a variety of
regression-type models are often used for modeling insurance claims, ranging
from relatively simple generalized linear models (GLMs) to regularized GLMs to
gradient boosting models (GBMs). Conformal predictive inference has arisen as a
popular distribution-free approach for quantifying predictive uncertainty under
relatively weak assumptions of exchangeability, and has been well studied under
the classic linear regression setting. In this work, we propose new
non-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized
Tweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal
prediction performance with these non-conformity measures in insurance claims
data. Our simulation results favor the use of locally weighted Pearson
residuals for LightGBM over other methods considered, as the resulting
intervals maintained the nominal coverage with the smallest average width.

</details>


### [119] [Off-Policy Evaluation Under Nonignorable Missing Data](https://arxiv.org/abs/2507.06961)
*Han Wang,Yang Xu,Wenbin Lu,Rui Song*

Main category: stat.ML

TL;DR: 本文研究了在单调缺失数据下的离策略评估（OPE），发现可忽略缺失时估计无偏，但非可忽略缺失时可能有偏。提出了一种加权估计器，并通过实验验证其可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，离线数据常存在缺失，但缺失数据如何影响OPE的理论理解尚不明确。

Method: 提出逆概率加权值估计器，并进行统计推断以量化估计不确定性。

Result: 理论证明可忽略缺失时估计无偏，非可忽略缺失时有偏；实验表明所提估计器在缺失数据下更可靠。

Conclusion: 在单调缺失数据下，所提方法能保持估计一致性，为OPE提供了更可靠的解决方案。

Abstract: Off-Policy Evaluation (OPE) aims to estimate the value of a target policy
using offline data collected from potentially different policies. In real-world
applications, however, logged data often suffers from missingness. While OPE
has been extensively studied in the literature, a theoretical understanding of
how missing data affects OPE results remains unclear. In this paper, we
investigate OPE in the presence of monotone missingness and theoretically
demonstrate that the value estimates remain unbiased under ignorable
missingness but can be biased under nonignorable (informative) missingness. To
retain the consistency of value estimation, we propose an inverse probability
weighted value estimator and conduct statistical inference to quantify the
uncertainty of the estimates. Through a series of numerical experiments, we
empirically demonstrate that our proposed estimator yields a more reliable
value inference under missing data.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [120] [gigiProfiler: Diagnosing Performance Issues by Uncovering Application Resource Bottlenecks](https://arxiv.org/abs/2507.06452)
*Yigong Hu,Haodong Zheng,Yicheng Liu,Dedong Xie,Youliang Huang,Baris Kasikci*

Main category: cs.PF

TL;DR: OmniResource Profiling是一种结合系统级和应用级资源追踪的性能分析方法，通过gigiProfiler工具实现，能全面诊断资源瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现代软件性能瓶颈诊断困难，传统分析工具无法有效识别应用级资源竞争问题。

Method: gigiProfiler采用混合LLM-静态分析方法离线识别应用定义资源，并在异常执行时分析其性能影响。

Result: 在12个实际性能问题中，gigiProfiler准确识别了所有瓶颈，并成功诊断了两个新问题。

Conclusion: OmniResource Profiling方法有效解决了传统工具无法覆盖的应用级性能瓶颈问题。

Abstract: Diagnosing performance bottlenecks in modern software is essential yet
challenging, particularly as applications become more complex and rely on
custom resource management policies. While traditional profilers effectively
identify execution bottlenecks by tracing system-level metrics, they fall short
when it comes to application-level resource contention caused by waiting for
application-level events. In this work, we introduce OmniResource Profiling, a
performance analysis approach that integrates system-level and
application-level resource tracing to diagnose resource bottlenecks
comprehensively. gigiProfiler, our realization of OmniResource Profiling, uses
a hybrid LLM-static analysis approach to identify application-defined resources
offline and analyze their impact on performance during buggy executions to
uncover the performance bottleneck. gigiProfiler then samples and records
critical variables related to these bottleneck resources during buggy execution
and compares their value with those from normal executions to identify the root
causes. We evaluated gigiProfiler on 12 real-world performance issues across
five applications. gigiProfiler accurately identified performance bottlenecks
in all cases. gigiProfiler also successfully diagnosed the root causes of two
newly emerged, previously undiagnosed problems, with the findings confirmed by
developers.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [121] [Learning To Communicate Over An Unknown Shared Network](https://arxiv.org/abs/2507.06499)
*Shivangi Agarwal,Adi Asija,Sanjit K. Kaul,Arani Bhattacharya,Saket Anand*

Main category: cs.MA

TL;DR: 论文提出了一种名为QNet的深度强化学习模型，帮助代理在共享无线网络中优化通信策略，无需针对特定网络配置训练。


<details>
  <summary>Details</summary>
Motivation: 在共享无线网络中，代理无法感知网络资源状态，需通过学习策略优化通信效用。

Method: 使用模拟到现实的框架训练QNet，模拟模型仅需一个参数，适用于各种无线网络配置。

Result: 实验验证了QNet在WiFi和蜂窝网络中的有效性，适应从低到高的网络竞争条件。

Conclusion: QNet能泛化到不同网络配置和代理数量，优于其他策略。

Abstract: As robots (edge-devices, agents) find uses in an increasing number of
settings and edge-cloud resources become pervasive, wireless networks will
often be shared by flows of data traffic that result from communication between
agents and corresponding edge-cloud. In such settings, agent communicating with
the edge-cloud is unaware of state of network resource, which evolves in
response to not just agent's own communication at any given time but also to
communication by other agents, which stays unknown to the agent. We address
challenge of an agent learning a policy that allows it to decide whether or not
to communicate with its cloud node, using limited feedback it obtains from its
own attempts to communicate, to optimize its utility. The policy generalizes
well to any number of other agents sharing the network and must not be trained
for any particular network configuration. Our proposed policy is a DRL model
Query Net (QNet) that we train using a proposed simulation-to-real framework.
Our simulation model has just one parameter and is agnostic to specific
configurations of any wireless network. It allows training an agent's policy
over a wide range of outcomes that an agent's communication with its edge-cloud
node may face when using a shared network, by suitably randomizing the
simulation parameter. We propose a learning algorithm that addresses challenges
observed in training QNet. We validate our simulation-to-real driven approach
through experiments conducted on real wireless networks including WiFi and
cellular. We compare QNet with other policies to demonstrate its efficacy. WiFi
experiments involved as few as five agents, resulting in barely any contention
for the network, to as many as fifty agents, resulting in severe contention.
The cellular experiments spanned a broad range of network conditions, with
baseline RTT ranging from a low of 0.07 second to a high of 0.83 second.

</details>


### [122] [A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes](https://arxiv.org/abs/2507.06278)
*Kemboi Cheruiyot,Nickson Kiprotich,Vyacheslav Kungurtsev,Kennedy Mugo,Vivian Mwirigi,Marvin Ngesa*

Main category: cs.MA

TL;DR: 本文综述了三种多智能体交互拓扑结构：联邦强化学习（RL）、去中心化RL和非合作RL，分析了它们的结构异同、理论保证及数值性能的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着对自主智能体研究的兴趣增加，多智能体交互的复杂场景变得重要，本文旨在全面梳理三种交互拓扑的研究现状。

Method: 通过文献综述，分别定义并分析了联邦RL、去中心化RL和非合作RL的框架、理论保证及数值性能。

Result: 总结了三种交互拓扑的最新研究进展，包括其结构特点、理论支持和实际应用中的局限性。

Conclusion: 本文为多智能体交互领域的研究提供了系统化的综述，指出了未来研究的潜在方向。

Abstract: The increasing interest in research and innovation towards the development of
autonomous agents presents a number of complex yet important scenarios of
multiple AI Agents interacting with each other in an environment. The
particular setting can be understood as exhibiting three possibly topologies of
interaction - centrally coordinated cooperation, ad-hoc interaction and
cooperation, and settings with noncooperative incentive structures. This
article presents a comprehensive survey of all three domains, defined under the
formalism of Federal Reinforcement Learning (RL), Decentralized RL, and
Noncooperative RL, respectively. Highlighting the structural similarities and
distinctions, we review the state of the art in these subjects, primarily
explored and developed only recently in the literature. We include the
formulations as well as known theoretical guarantees and highlights and
limitations of numerical performance.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [123] [Designing Robust Software Sensors for Nonlinear Systems via Neural Networks and Adaptive Sliding Mode Control](https://arxiv.org/abs/2507.06817)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: math.DS

TL;DR: 提出了一种结合神经网络与自适应滑模控制的鲁棒状态观测器设计方法，适用于非线性动态系统，无需依赖显式变换或线性化。


<details>
  <summary>Details</summary>
Motivation: 动态系统中状态变量的准确估计对控制、诊断和监控至关重要，但直接测量所有状态往往不可行。

Method: 集成神经网络与自适应滑模控制，利用传感器测量驱动学习过程，并通过系统动力学方程作为物理约束进行观测器设计。

Result: 仿真验证了方法的有效性，包括非可微动态和变化可观测性条件下的系统，表现出快速收敛和高精度。

Conclusion: 该方法为复杂状态估计问题提供了鲁棒且广泛适用的解决方案，具有实际应用潜力。

Abstract: Accurate knowledge of the state variables in a dynamical system is critical
for effective control, diagnosis, and supervision, especially when direct
measurements of all states are infeasible. This paper presents a novel approach
to designing software sensors for nonlinear dynamical systems expressed in
their most general form. Unlike traditional model-based observers that rely on
explicit transformations or linearization, the proposed framework integrates
neural networks with adaptive Sliding Mode Control (SMC) to design a robust
state observer under a less restrictive set of conditions. The learning process
is driven by available sensor measurements, which are used to correct the
observer's state estimate. The training methodology leverages the system's
governing equations as a physics-based constraint, enabling observer synthesis
without access to ground-truth state trajectories. By employing a time-varying
gain matrix dynamically adjusted by the neural network, the observer adapts in
real-time to system changes, ensuring robustness against noise, external
disturbances, and variations in system dynamics. Furthermore, we provide
sufficient conditions to guarantee estimation error convergence, establishing a
theoretical foundation for the observer's reliability. The methodology's
effectiveness is validated through simulations on challenging examples,
including systems with non-differentiable dynamics and varying observability
conditions. These examples, which are often problematic for conventional
techniques, serve to demonstrate the robustness and broad applicability of our
approach. The results show rapid convergence and high accuracy, underscoring
the method's potential for addressing complex state estimation challenges in
real-world applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [124] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: PERK是一种参数高效的方法，通过测试时梯度更新轻量级适配器来编码长上下文，显著优于基于提示的基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文推理中信息噪声问题，同时避免传统元学习方法的高内存消耗。

Method: 采用双嵌套优化循环：内循环将上下文编码到低秩适配器（LoRA），外循环学习使用适配器进行推理。

Result: 在多个任务中表现优异，小模型提升90%，大模型提升27%，且对推理复杂性、长度外推和信息位置更鲁棒。

Conclusion: PERK在训练时内存消耗大，但在推理时比提示方法更高效，适用于长上下文推理。

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [125] [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
*Shun Wang,Tyler Loakman,Youbo Lei,Yi Liu,Bohao Yang,Yuting Zhao,Dong Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 论文提出了一种基于字典学习和稀疏自编码器的大语言模型分解方法，用于提取单义特征，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型被视为黑盒算法，缺乏可解释性且影响性能提升。

Method: 采用字典学习和稀疏自编码器分解LLM，提取单义特征并自动优化提示。

Result: 方法显著提升了下游任务（如数学推理和隐喻检测）的性能。

Conclusion: 分解方法增强了LLM的可解释性和性能，为模型优化提供了新途径。

Abstract: Large Language Models (LLMs) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective LLM decomposition method using a dictionary-learning
approach with sparse autoencoders. This helps extract monosemantic features
from polysemantic LLM neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by LLMs. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.

</details>


### [126] [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
*Russell Taylor,Benjamin Herbert,Michael Sana*

Main category: cs.CL

TL;DR: 提出了一种结合大型语言模型和专门技术的双关语翻译方法，旨在解决跨语言翻译中的幽默和创意问题。


<details>
  <summary>Details</summary>
Motivation: 解决双关语翻译中语义模糊、语音相似性和文化差异的挑战，填补翻译研究与计算语言学之间的空白。

Method: 采用三阶段方法：1) 基于对比学习数据集的大模型基线；2) 结合语音-语义嵌入的引导思维链；3) 生成-判别多智能体框架。

Result: 在CLEF JOKER 2025竞赛中取得第一和第二名，证明方法有效。

Conclusion: 通过语言学技术提升语言模型处理双关语的能力，为幽默翻译提供了新思路。

Abstract: Translating wordplay across languages presents unique challenges that have
long confounded both professional human translators and machine translation
systems. This research proposes a novel approach for translating puns from
English to French by combining state-of-the-art large language models with
specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a
baseline using multiple frontier large language models with feedback based on a
new contrastive learning dataset. Second, we implement a guided
chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we
implement a multi-agent generator-discriminator framework for evaluating and
regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's
primary objective is to capture the linguistic creativity and humor of the
source text wordplay, rather than simply duplicating its vocabulary. Our best
runs earned first and second place in the CLEF JOKER 2025 Task 2 competition
where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational
linguistics by implementing linguistically-informed techniques for wordplay
translation, advancing our understanding of how language models can be
leveraged to handle the complex interplay between semantic ambiguity, phonetic
similarity, and the implicit cultural and linguistic awareness needed for
successful humor.

</details>


### [127] [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
*Huisheng Wang,Zhuoshi Pan,Hangjing Zhang,Mingxiao Liu,Hanqing Gao,H. Vicky Zhao*

Main category: cs.CL

TL;DR: 论文提出InvestAlign框架，通过理论解决方案构建高质量SFT数据集，以解决行为金融中LLM与投资者决策对齐的挑战，避免真实用户数据的高成本和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 行为金融中，LLM与投资者决策对齐需大量真实用户数据，但数据稀缺、成本高且隐私风险大。

Method: 提出InvestAlign框架，利用理论解决方案生成高质量SFT数据集，替代复杂场景的真实数据。

Result: 实验显示，InvestAlign生成的数据训练LLM参数收敛更快，且InvestAgent模型更贴近真实用户行为。

Conclusion: InvestAlign为复杂投资问题提供高效解决方案，有望实现LLM与投资者决策行为的对齐。

Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes
under herd behavior is a critical challenge in behavioral finance, which
grapples with a fundamental limitation: the scarcity of real-user data needed
for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM
outputs and human behavioral patterns, its reliance on massive authentic data
imposes substantial collection costs and privacy risks. We propose InvestAlign,
a novel framework that constructs high-quality SFT datasets by leveraging
theoretical solutions to similar and simple optimal investment problems rather
than complex scenarios. Our theoretical analysis demonstrates that training
LLMs with InvestAlign-generated data achieves faster parameter convergence than
using real-user data, suggesting superior learning efficiency. Furthermore, we
develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which
demonstrates significantly closer alignment to real-user data than pre-SFT
models in both simple and complex investment problems. This highlights our
proposed InvestAlign as a promising approach with the potential to address
complex optimal investment problems and align LLMs with investor
decision-making processes under herd behavior. Our code is publicly available
at https://github.com/thu-social-network-research-group/InvestAlign.

</details>


### [128] [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
*Juan B. Gutiérrez*

Main category: cs.CL

TL;DR: 论文提出了一种将人类与大型语言模型（LLMs）视为平等节点的网络模型，分析了四种危害（漂移、自我修复、新编造和外部检测），并通过数学模型和开源算法（FOO）展示了如何通过同行评审提高网络可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人类与LLMs交互中的新媒介，关注其可靠性问题，而非孤立地分析幻觉现象。

Method: 采用了一种称为‘话语网络’的模型，将人类和LLMs视为平等节点，并开发了数学模型和开源算法（FOO）来模拟和优化网络行为。

Result: 研究发现，仅依赖漂移和自我修复的网络会稳定在较低错误率；加入编造行为会重现当前LLMs的高错误率；而引入同行评审（如FOO算法）可将系统推向以真相为主的状态。

Conclusion: 结论强调，新媒介的可靠性并非来自单一模型的完美，而是通过将不完美模型连接成互相监督的网络来实现。

Abstract: Large-language models turn writing into a live exchange between humans and
software. We capture this new medium with a discursive-network model that
treats people and LLMs as equal nodes and tracks how their statements
circulate. Broadening the focus from isolated hallucinations, we define
invalidation (any factual, logical, or structural breach) and show it follows
four hazards: drift from truth, self-repair, fresh fabrication, and external
detection. A general mathematical model of discursive networks is developed to
provide valuable insights: A network governed only by drift and self-repair
stabilizes at a modest error rate; adding fabrication reproduces the high rates
seen in current LLMs. Giving each false claim even a small chance of peer
review shifts the system to a truth-dominant state. We operationalize peer
review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a
configurable loop in which any set of agents critique one another while a
harmoniser merges their verdicts. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
wiring imperfect ones into networks that keep each other honest.

</details>


### [129] [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
*Liliang Ren,Congcong Chen,Haoran Xu,Young Jin Kim,Adam Atkinson,Zheng Zhan,Jiankai Sun,Baolin Peng,Liyuan Liu,Shuohang Wang,Hao Cheng,Jianfeng Gao,Weizhu Chen,Yelong Shen*

Main category: cs.CL

TL;DR: 论文提出了一种名为Gated Memory Unit (GMU)的机制，用于在SSM层之间高效共享内存，并基于此构建了SambaY架构，显著提升了解码效率和长上下文性能。


<details>
  <summary>Details</summary>
Motivation: 探索SSM层之间表示共享的效率潜力，以提升序列建模的性能。

Method: 引入GMU机制，构建SambaY架构，结合Samba和YOCO的优势，并在跨解码器中共享内存状态。

Result: SambaY显著提升了解码效率，保持了线性预填充时间复杂度，并在长上下文任务中表现优异。

Conclusion: GMU和SambaY架构在性能和效率上均优于基线模型，展示了大规模计算环境下的优越可扩展性。

Abstract: Recent advances in language modeling have demonstrated the effectiveness of
State Space Models (SSMs) for efficient sequence modeling. While hybrid
architectures such as Samba and the decoder-decoder architecture, YOCO, have
shown promising performance gains over Transformers, prior works have not
investigated the efficiency potential of representation sharing between SSM
layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet
effective mechanism for efficient memory sharing across layers. We apply it to
create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in
the cross-decoder to share memory readout states from a Samba-based
self-decoder. SambaY significantly enhances decoding efficiency, preserves
linear pre-filling time complexity, and boosts long-context performance, all
while eliminating the need for explicit positional encoding. Through extensive
scaling experiments, we demonstrate that our model exhibits a significantly
lower irreducible loss compared to a strong YOCO baseline, indicating superior
performance scalability under large-scale compute regimes. Our largest model
enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves
significantly better performance than Phi4-mini-Reasoning on reasoning tasks
such as Math500, AIME24/25, and GPQA Diamond without any reinforcement
learning, while delivering up to 10x higher decoding throughput on 2K-length
prompts with 32K generation length under the vLLM inference framework. We
release our training codebase on open-source data at
https://github.com/microsoft/ArchScale.

</details>


### [130] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 论文通过分析大型语言模型（LLMs）在层间概率轨迹的动态变化，发现确定和不确定预测的推理动态基本一致，挑战了利用简单方法检测不确定性的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs内部如何表示和处理预测的不确定性，以检测不确定性并防止幻觉。

Method: 使用Tuned Lens（Logit Lens的变体）分析11个数据集和5个模型的层间概率轨迹，以错误预测作为高认知不确定性的代表。

Result: 确定和不确定预测的轨迹基本一致，均在相似层出现置信度突增；更优秀的模型可能学会不同方式处理不确定性。

Conclusion: 研究结果表明，简单方法检测不确定性可能不可行，同时展示了可解释性方法在研究不确定性影响推理中的应用。

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


### [131] [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
*Seonwu Kim,Yohan Na,Kihun Kim,Hanhee Cho,Geun Lim,Mintae Kim,Seongik Park,Ki Hyun Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CL

TL;DR: 研究验证了基于DACP的方法在小型LLMs上的有效性，显著提升目标领域性能，同时保持通用能力，为企业部署提供经济高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 开源大型语言模型（LLMs）虽为企业应用提供了机会，但许多组织缺乏部署和维护大规模模型的基础设施，因此小型LLMs（sLLMs）成为实用替代方案。

Method: 采用领域自适应持续预训练（DACP）方法，验证其在多种基础模型和服务领域的有效性。

Result: 通过实验和实际评估，DACP应用的小型LLMs在目标领域性能显著提升，同时保留通用能力。

Conclusion: DACP为小型LLMs提供了一种经济高效且可扩展的企业级部署解决方案。

Abstract: The emergence of open-source large language models (LLMs) has expanded
opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result,
small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has
been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the
effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations,
we demonstrate that DACP-applied sLLMs achieve substantial gains in target
domain performance while preserving general capabilities, offering a
cost-efficient and scalable solution for enterprise-level deployment.

</details>


### [132] [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
*Luca Mariotti,Veronica Guidetti,Federica Mandreoli*

Main category: cs.CL

TL;DR: SCoRE是一种模块化、低成本的句子级关系抽取系统，无需微调即可适配不同语料库和知识图谱，性能优于现有方法且能耗更低。


<details>
  <summary>Details</summary>
Motivation: 解决低监督环境下关系抽取的需求，提供适应性强且抗噪声的解决方案。

Method: 结合监督对比学习和贝叶斯k近邻分类器进行多标签分类，提出新评估指标CSD和P@R。

Result: 在五个基准测试中性能匹配或超越现有方法，显著降低能耗。

Conclusion: SCoRE凭借高效、模块化和可扩展性，成为实际应用的理想选择。

Abstract: The growing demand for efficient knowledge graph (KG) enrichment leveraging
external corpora has intensified interest in relation extraction (RE),
particularly under low-supervision settings. To address the need for adaptable
and noise-resilient RE solutions that integrate seamlessly with pre-trained
large language models (PLMs), we introduce SCoRE, a modular and cost-effective
sentence-level RE system. SCoRE enables easy PLM switching, requires no
finetuning, and adapts smoothly to diverse corpora and KGs. By combining
supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)
classifier for multi-label classification, it delivers robust performance
despite the noisy annotations of distantly supervised corpora. To improve RE
evaluation, we propose two novel metrics: Correlation Structure Distance (CSD),
measuring the alignment between learned relational patterns and KG structures,
and Precision at R (P@R), assessing utility as a recommender system. We also
release Wiki20d, a benchmark dataset replicating real-world RE conditions where
only KG-derived annotations are available. Experiments on five benchmarks show
that SCoRE matches or surpasses state-of-the-art methods while significantly
reducing energy consumption. Further analyses reveal that increasing model
complexity, as seen in prior work, degrades performance, highlighting the
advantages of SCoRE's minimal design. Combining efficiency, modularity, and
scalability, SCoRE stands as an optimal choice for real-world RE applications.

</details>


### [133] [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
*Ashen Weligalle*

Main category: cs.CL

TL;DR: 论文研究了离散扩散模型（D3PM）在自然语言生成中的可行性和性能，并与传统自回归模型（AR）进行了比较。结果显示D3PM在并行生成速度上有优势，但AR在压缩性能上更优。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在离散数据（如自然语言）上的应用潜力，解决传统扩散模型在离散数据上的挑战。

Method: 评估离散去噪扩散概率模型（D3PM），并与自回归模型（AR）对比，使用BPT、NLL、PPL和批处理速度等指标。

Result: D3PM在批处理速度上表现更优（3.97批次/秒），但AR在压缩性能（BPT均值4.59）上优于D3PM（BPT均值8.05）。

Conclusion: 扩散模型在离散数据生成中具有潜力，尤其在并行生成方面，但需进一步改进以提升生成质量。

Abstract: Diffusion models have emerged as a powerful class of generative models,
achieving state-of-the-art results in continuous data domains such as image and
video generation. Their core mechanism involves a forward diffusion process
that gradually transforms structured data into a Gaussian-like distribution,
followed by a learned reverse process to reconstruct the data. While successful
in continuous modalities, applying this framework to discrete data-particularly
natural language-remains challenging due to token dependency complexities and
the lack of a defined generation order.This thesis investigates the feasibility
and performance of discrete diffusion models for natural language generation.
Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model
(D3PM) and compare it with traditional autoregressive (AR) language models. To
assess generative performance, we use Bits Per Token (BPT), Negative
Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.
  Results show the best-performing D3PM model achieves a BPT of 5.72, with a
mean of 8.05. The AR model outperforms in compression with a lower mean BPT of
4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches
per sec., indicating potential for parallel generation.All evaluations were
conducted under consistent conditions-generating 100,000 tokens per model with
a fixed batch size of four-for fair comparison. This research presents a
detailed analysis of diffusion-based vs. autoregressive models, highlighting
trade-offs in generative quality and efficiency. Findings emphasize both the
promise and limitations of diffusion models for discrete data, supporting
future work in non-autoregressive language generation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [134] [A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering](https://arxiv.org/abs/2507.07046)
*Shahana Yasmin Chowdhury,Bithi Banik,Md Tamjidul Hoque,Shreya Banerjee*

Main category: cs.SD

TL;DR: 提出了一种名为DCRF-BiLSTM的模型，用于识别七种情感，并在五个数据集上取得了高准确率，证明了其鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）在人机交互和人工智能发展中至关重要，但目前缺乏一个能在多个基准数据集上同时评估的模型。

Method: 使用DCRF-BiLSTM模型，在RAVDESS、TESS、SAVEE、EmoDB和Crema-D五个数据集上训练，识别七种情感。

Result: 模型在单个数据集上准确率高达97.83%至100%，组合数据集上达到98.82%，综合五个数据集时整体准确率为93.76%。

Conclusion: DCRF-BiLSTM模型具有强大的鲁棒性和泛化能力，适用于多样化的数据集。

Abstract: Nowadays, speech emotion recognition (SER) plays a vital role in the field of
human-computer interaction (HCI) and the evolution of artificial intelligence
(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:
neutral, happy, sad, angry, fear, disgust, and surprise, which are trained on
five datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).
The model achieves high accuracy on individual datasets, including 97.83% on
RAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS
and EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,
outperforming previously reported results. To our knowledge, no existing study
has evaluated a single SER model across all five benchmark datasets (i.e.,
R+T+S+C+E) simultaneously. In our work, we introduce this comprehensive
combination and achieve a remarkable overall accuracy of 93.76%. These results
confirm the robustness and generalizability of our DCRF-BiLSTM framework across
diverse datasets.

</details>


### [135] [Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification](https://arxiv.org/abs/2507.07058)
*Martin Sondermann,Pinar Bisgin,Niklas Tschorn,Anja Burmann,Christoph M. Friedrich*

Main category: cs.SD

TL;DR: 论文比较了四种模型（两种CNN和两种BEATs变换器）用于心音图分类，发现CNN性能更优，但BEATs在效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 探索不同模型和归一化方法对心音图自动分类性能的影响，为临床诊断提供指导。

Method: 使用PhysioNet2022数据集，比较两种CNN和两种BEATs变换器在固定长度和心周期归一化下的表现。

Result: CNN固定长度窗口AUROC为79.5%，心周期归一化为75.4%；BEATs固定长度窗口为65.7%，心周期归一化为70.1%。

Conclusion: CNN性能更优，但BEATs在开发效率上有潜力，需平衡准确性与计算效率。

Abstract: The automated classification of phonocardiogram (PCG) recordings represents a
substantial advancement in cardiovascular diagnostics. This paper presents a
systematic comparison of four distinct models for heart murmur detection: two
specialized convolutional neural networks (CNNs) and two zero-shot universal
audio transformers (BEATs), evaluated using fixed-length and heart cycle
normalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart
cycle normalization method tailored to individual cardiac rhythms is
introduced. The findings indicate the following AUROC values: the CNN model
with fixed-length windowing achieves 79.5%, the CNN model with heart cycle
normalization scores 75.4%, the BEATs transformer with fixed-length windowing
achieves 65.7%, and the BEATs transformer with heart cycle normalization
results in 70.1%.
  The findings indicate that physiological signal constraints, especially those
introduced by different normalization strategies, have a substantial impact on
model performance. The research provides evidence-based guidelines for
architecture selection in clinical settings, emphasizing the need for a balance
between accuracy and computational efficiency. Although specialized CNNs
demonstrate superior performance overall, the zero-shot transformer models may
offer promising efficiency advantages during development, such as faster
training and evaluation cycles, despite their lower classification accuracy.
These findings highlight the potential of automated classification systems to
enhance cardiac diagnostics and improve patient care.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [136] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: AR2是一种通过对齐干净和损坏图像的类激活图（CAMs）来增强预训练CNN鲁棒性的方法，无需改变架构即可显著提升模型在损坏数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在常见损坏（如噪声、模糊、天气和数字失真）下性能显著下降，限制了其在现实应用中的可靠性。

Method: AR2通过迭代修复策略，交替进行CAM引导的细化和标准微调，对齐干净和损坏图像的CAMs。

Result: AR2在标准损坏基准测试（CIFAR-10-C、CIFAR-100-C和ImageNet-C）上优于现有方法，平衡了干净数据的准确性和损坏鲁棒性。

Conclusion: AR2为增强模型在多样化损坏环境中的可靠性提供了可扩展的解决方案。

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [137] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: 本文综述了离线手写文本识别（HTR）系统中的数据增强与生成技术，旨在解决标注数据不足的问题，并探讨了深度学习方法与传统方法的优劣。


<details>
  <summary>Details</summary>
Motivation: 离线手写文本识别系统在历史文档数字化等领域至关重要，但标注数据稀缺，尤其是低资源语言和复杂脚本，限制了其性能。

Method: 采用PRISMA方法，系统分析了传统数据增强技术和深度学习方法（如GANs、扩散模型和基于Transformer的方法），并从1,302项研究中筛选出848项进行综述。

Result: 通过评估现有数据集、指标和先进方法，揭示了研究空白，并提出了未来方向。

Conclusion: 本文为手写文本生成领域的研究提供了系统总结，并指出了未来发展的潜在路径。

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [138] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: 论文提出了一种名为CCPDA的数据增强方法，用于改善野火科学中多类分割模型的训练效果，特别是提升火灾类别的分割性能。


<details>
  <summary>Details</summary>
Motivation: 在野火科学领域，获取和标注图像用于训练分割模型成本高昂且公开数据集稀缺，因此需要一种有效的数据增强方法。

Method: CCPDA方法包括三个步骤：识别源图像中的火灾集群、集中技术聚焦火灾核心区域、将精炼的火灾集群粘贴到目标图像上。

Result: 通过数值分析和多目标优化比较，CCPDA显著提升了火灾类别的分割性能，优于其他增强方法。

Conclusion: CCPDA有效缓解了小规模标注数据集的训练困难，尤其在火灾类别分割上表现突出。

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [139] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Concept-TRAK的新方法，用于解决扩散模型在图像生成中的版权和透明度问题，通过概念级归因提供更细粒度的分析。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其广泛使用引发了版权和模型透明度的担忧。现有方法无法精确归因到特定元素（如风格或对象）。

Method: Concept-TRAK扩展了影响函数，通过两种创新：(1)基于扩散后验采样的训练损失；(2)强调语义相关性的概念感知奖励函数。

Result: 在AbC基准测试中，Concept-TRAK显著优于现有方法，并通过案例研究展示了其在版权保护、内容安全和提示工程中的实用性。

Conclusion: 概念级归因为生成AI的负责任开发和治理提供了可操作的见解。

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [140] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: 提出了一种基于分布差异的相似性函数（DSF），通过将多视图表示为分布并测量分布间的差异来捕获联合结构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要捕捉成对关系，未能建模多视图的联合结构。

Method: 提出DSF，将多视图表示为分布并测量分布间的差异。

Result: DSF在kNN分类和线性评估等任务中表现优异，且效率更高。

Conclusion: DSF无需温度超参数即可有效工作，理论证明其与余弦相似性的联系。

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [141] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: EXAONE Path 2.0通过直接使用幻灯片级监督学习补丁级表示，解决了数字病理学中自监督学习（SSL）方法的局限性，显著提高了数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的自监督学习方法在补丁级别训练时可能忽略复杂的领域特定特征，且数据效率较低，需要大量计算资源。

Method: 提出EXAONE Path 2.0，一种病理学基础模型，通过直接利用幻灯片级监督学习补丁级表示。

Result: 仅使用37k张全切片图像（WSIs）训练，EXAONE Path 2.0在10个生物标志物预测任务中达到最先进性能。

Conclusion: EXAONE Path 2.0显著提高了数据效率和性能，为数字病理学提供了一种更高效的解决方案。

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [142] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 论文提出了一种从稀疏标注中学习密集预测任务的方法，通过新的损失函数（Crag and Tail loss）在2D腹腔镜视频帧中定位癌变关键点，解决了医学领域中标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 医学领域中标注成本高且密集标注不现实，尤其是在新任务中。本文旨在通过少量像素级标注学习密集预测任务，推动研究进展。

Method: 将问题建模为稀疏热图回归，提出Crag and Tail损失函数，有效利用稀疏标注并减少假阴性标注的影响。

Result: 通过消融实验验证了方法的有效性，能够准确实现癌变关键点的密集定位。

Conclusion: 该方法在密集标注难以获取的场景中具有潜力，可推动相关研究。

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [143] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种新的梯度管理技术SPGD，用于解决扩散模型中先验和似然梯度方向冲突及梯度波动问题，显著提升了图像恢复的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复中表现出色，但先验和似然梯度方向的冲突及梯度波动问题未被充分研究，影响了恢复性能。

Method: 提出SPGD技术，包含渐进式似然预热策略和自适应方向动量平滑，以解决梯度冲突和波动问题。

Result: 实验表明，SPGD显著提升了生成稳定性，在定量指标和视觉效果上均达到最优性能。

Conclusion: SPGD通过梯度管理技术有效解决了扩散模型中的不稳定问题，为图像恢复任务提供了新的解决方案。

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [144] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: RPFNet通过残差先验和频域融合模块，高效整合多模态图像信息，提升融合质量和高级视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态图像融合中长距离特征依赖计算成本高和缺乏真实数据的问题。

Method: 采用双分支框架（RPM和FDFM）提取残差先验和频域特征，结合CPM模块增强局部与全局特征交互。

Result: 实验证明RPFNet能有效整合特征、增强细节和显著对象，提升高级视觉任务性能。

Conclusion: RPFNet在多模态图像融合中表现出色，为高级视觉任务提供了有效支持。

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [145] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种局部和全局双粒度身份关联机制，显著提升了弱监督文本到人物图像匹配的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂的一对多身份关系，限制了性能提升，因此需要一种新方法来解决这一问题。

Method: 采用局部和全局双粒度身份关联机制，局部层面显式建立跨模态身份关系，全局层面构建动态跨模态身份关联网络，并结合信息不对称样本对构建和一致性学习。

Result: 实验结果表明，该方法显著提升了跨模态匹配的准确性。

Conclusion: 该方法为文本到人物图像匹配提供了一种高效实用的解决方案。

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [146] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: 论文提出了一种名为D2I的框架，通过规则奖励提升多模态大语言模型（MLLMs）的推理能力，无需额外标注或复杂奖励。


<details>
  <summary>Details</summary>
Motivation: 多模态推理研究在模态对齐和训练成本方面仍需探索，现有方法依赖额外标注和复杂奖励，增加了成本并限制了可扩展性。

Method: D2I框架在训练时通过规则奖励设置深思熟虑的推理策略，评估时转为直觉推理，隐式反映模型能力。

Result: D2I在领域内和领域外基准测试中均优于基线方法。

Conclusion: D2I展示了规则奖励在提升MLLMs可迁移推理能力中的作用，为训练与测试推理深度的解耦提供了方向。

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [147] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: GNN-ViTCap框架通过动态聚类和注意力机制处理冗余WSI补丁，结合GNN和ViT实现病理图像分类与描述生成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像中冗余补丁和未知位置问题，以及自动生成病理描述的挑战。

Method: 使用ViT提取补丁嵌入，动态聚类去冗余，GNN捕获上下文，结合语言模型生成描述。

Result: 在BreakHis和PatchGastric数据集上，分类F1为0.934，AUC为0.963；描述BLEU-4为0.811，METEOR为0.569。

Conclusion: GNN-ViTCap为病理诊断提供了高效可靠的解决方案，性能优于现有方法。

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [148] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: MST-Distill是一种新型跨模态知识蒸馏框架，通过混合专家教师模型和动态路由网络解决传统方法的局限性，显著提升跨模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在跨模态场景中因数据和统计异质性无法有效利用跨模态教师模型的互补知识，存在蒸馏路径选择和知识漂移问题。

Method: 提出MST-Distill框架，结合跨模态和多模态教师模型，使用动态路由网络自适应选择蒸馏路径，并引入掩码模块抑制模态差异。

Result: 在五个多模态数据集上的实验表明，MST-Distill显著优于现有最优方法。

Conclusion: MST-Distill通过动态教师模型和掩码模块有效解决了跨模态知识蒸馏的挑战，提升了性能。

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [149] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 研究探讨了预训练文本到图像扩散模型作为视觉编码器的潜力，发现其能捕捉细粒度细节并解决CLIP的局限性，同时提出了信息泄漏问题及解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP视觉编码器在捕捉细粒度细节上的不足，探索扩散模型作为替代的可能性。

Method: 分析扩散模型的内部表示，利用文本条件聚焦相关区域，并提出融合CLIP与扩散特征的策略。

Result: 扩散模型在视觉理解任务中表现优异，尤其在需要空间和组合推理的任务中。

Conclusion: 扩散模型有望成为视觉编码器的新选择，特别是在细粒度视觉任务中。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [150] [Optimizing Cognitive Networks: Reinforcement Learning Meets Energy Harvesting Over Cascaded Channels](https://arxiv.org/abs/2507.06981)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: cs.ET

TL;DR: 本文提出了一种基于强化学习的方法，用于提高认知无线电网络中物理层安全性，通过深度Q网络优化传输功率和能量收集决策。


<details>
  <summary>Details</summary>
Motivation: 在高度移动的网络（如认知车载网络）中，窃听者可能拦截通信，因此需要增强安全性和可靠性。

Method: 利用深度Q网络（DQN）策略，每个次级用户发射器分配一个代理，优化传输功率和能量收集决策。

Result: 该方法在安全性和可靠性方面优于两种基线策略。

Conclusion: 提出的DQN方法能有效提升次级用户的保密率和吞吐量，同时满足主用户的干扰阈值。

Abstract: This paper presents a reinforcement learning (RL) based approach to improve
the physical layer security (PLS) of an underlay cognitive radio network (CRN)
over cascaded channels. These channels are utilized in highly mobile networks
such as cognitive vehicular networks (CVN). In addition, an eavesdropper aims
to intercept the communications between secondary users (SUs). The SU receiver
has full-duplex and energy harvesting capabilities to generate jamming signals
to confound the eavesdropper and enhance security. Moreover, the SU transmitter
extracts energy from ambient radio frequency signals in order to power
subsequent transmissions to its intended receiver. To optimize the privacy and
reliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized
where multiple DQN agents are required such that an agent is assigned at each
SU transmitter. The objective for the SUs is to determine the optimal
transmission power and decide whether to collect energy or transmit messages
during each time period in order to maximize their secrecy rate. Thereafter, we
propose a DQN approach to maximize the throughput of the SUs while respecting
the interference threshold acceptable at the receiver of the primary user.
According to our findings, our strategy outperforms two other baseline
strategies in terms of security and reliability.

</details>


### [151] [Maximizing Reliability in Overlay Radio Networks with Time Switching and Power Splitting Energy Harvesting](https://arxiv.org/abs/2507.06983)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: cs.ET

TL;DR: 本文研究了认知无线电网络（CRN）中的能量效率问题，通过优化时间切换和功率分配参数，提升次级用户（SU）的数据速率。


<details>
  <summary>Details</summary>
Motivation: 解决频谱利用率不足问题，同时优化系统可靠性和能量效率。

Method: 采用时间切换-能量收集（EH）协议和功率分配协议，结合多天线技术和最大比合并方法。

Result: 通过中断概率评估网络可靠性，并优化了SU的数据速率。

Conclusion: 优化EH协议和功率分配可显著提升CRN的能量效率和系统可靠性。

Abstract: Cognitive radio networks (CRNs) are acknowledged for their ability to tackle
the issue of spectrum under-utilization. In the realm of CRNs, this paper
investigates the energy efficiency issue and addresses the critical challenge
of optimizing system reliability for overlay CRN access mode. Randomly
dispersed secondary users (SUs) serving as relays for primary users (PUs) are
considered, in which one of these relays is designated to harvest energy
through the time switching-energy harvesting (EH) protocol. Moreover, this
relay amplifies-and-forwards (AF) the PU's messages and broadcasts them along
with its own across cascaded $\kappa$-$\mu$ fading channels. The power
splitting protocol is another EH approach utilized by the SU and PU receivers
to enhance the amount of energy in their storage devices. In addition, the SU
transmitters and the SU receiver are deployed with multiple antennas for
reception and apply the maximal ratio combining approach. The outage
probability is utilized to assess both networks' reliability. Then, an energy
efficiency evaluation is performed to determine the effectiveness of EH on the
system. Finally, an optimization problem is provided with the goal of
maximizing the data rate of the SUs by optimizing the time switching and the
power allocation parameters of the SU relay.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [152] [Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks](https://arxiv.org/abs/2507.06997)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 本文提出了一种基于联邦学习的多智能体强化学习（MARL）策略，用于提升超5G网络中多蜂窝网络的物理层安全性（PLS）。通过比较DQN和RDPG两种方法，发现RDPG收敛更快且性能优于分布式DRL方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决多蜂窝网络中合法用户与窃听者之间的安全问题，同时保护用户数据的隐私。

Method: 采用联邦学习的MARL策略，每个基站作为DRL智能体，使用DQN和RDPG两种方法优化保密速率。

Result: 结果表明RDPG收敛更快且性能优于分布式DRL方法，同时揭示了安全性与复杂性之间的权衡。

Conclusion: 结论是联邦学习的MARL策略在提升PLS方面具有潜力，尤其是RDPG方法表现更优。

Abstract: This paper explores the application of a federated learning-based multi-agent
reinforcement learning (MARL) strategy to enhance physical-layer security (PLS)
in a multi-cellular network within the context of beyond 5G networks. At each
cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent
that interacts with the surrounding environment to maximize the secrecy rate of
legitimate users in the presence of an eavesdropper. This eavesdropper attempts
to intercept the confidential information shared between the BS and its
authorized users. The DRL agents are deemed to be federated since they only
share their network parameters with a central server and not the private data
of their legitimate users. Two DRL approaches, deep Q-network (DQN) and
Reinforce deep policy gradient (RDPG), are explored and compared. The results
demonstrate that RDPG converges more rapidly than DQN. In addition, we
demonstrate that the proposed method outperforms the distributed DRL approach.
Furthermore, the outcomes illustrate the trade-off between security and
complexity.

</details>


### [153] [How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks](https://arxiv.org/abs/2507.07067)
*Clement Ruah,Houssem Sifaou,Osvaldo Simeone,Bashir M. Al-Hashimi*

Main category: eess.SP

TL;DR: 论文探讨了电信领域AI模型训练的挑战，提出通过数字孪生和模拟到现实（sim-to-real）策略解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 电信领域AI模型训练因部署特定数据稀缺而困难，数字孪生和sim-to-real策略可提供解决方案。

Method: 采用两种互补策略：1）通过真实测量校准数字孪生；2）使用sim-to-real感知训练策略处理数据差异。

Result: 评估了两种方法：基于贝叶斯学习的环境级建模和基于预测驱动推断的训练损失级建模。

Conclusion: 数字孪生和sim-to-real策略能有效解决电信AI模型训练中的数据稀缺问题。

Abstract: Training effective artificial intelligence models for telecommunications is
challenging due to the scarcity of deployment-specific data. Real data
collection is expensive, and available datasets often fail to capture the
unique operational conditions and contextual variability of the network
environment. Digital twinning provides a potential solution to this problem, as
simulators tailored to the current network deployment can generate
site-specific data to augment the available training datasets. However, there
is a need to develop solutions to bridge the inherent simulation-to-reality
(sim-to-real) gap between synthetic and real-world data. This paper reviews
recent advances on two complementary strategies: 1) the calibration of digital
twins (DTs) through real-world measurements, and 2) the use of sim-to-real
gap-aware training strategies to robustly handle residual discrepancies between
digital twin-generated and real data. For the latter, we evaluate two
conceptually distinct methods that model the sim-to-real gap either at the
level of the environment via Bayesian learning or at the level of the training
loss via prediction-powered inference.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [154] [Enhancing Quantum Software Development Process with Experiment Tracking](https://arxiv.org/abs/2507.06990)
*Mahee Gamage,Otso Kinanen,Jake Muff,Vlad Stirbu*

Main category: quant-ph

TL;DR: 论文探讨了在量子研究中应用MLflow以提升实验可重复性、协作性和决策能力。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从理论走向实验，严格的实验跟踪变得至关重要，借鉴机器学习和人工智能的最佳实践可以显著提升量子研究的可重复性和协作性。

Method: 论文研究了MLflow在量子研究中的应用，展示了其如何改善开发实践、实验可重复性和跨领域整合。

Result: MLflow在量子研究中能够有效支持实验跟踪、决策和协作，适应经典-量子混合的研究环境。

Conclusion: MLflow为量子研究提供了结构化的实验跟踪框架，有助于提升研究的可重复性和协作效率。

Abstract: As quantum computing advances from theoretical promise to experimental
reality, the need for rigorous experiment tracking becomes critical. Drawing
inspiration from best practices in machine learning (ML) and artificial
intelligence (AI), we argue that reproducibility, scalability, and
collaboration in quantum research can benefit significantly from structured
tracking workflows. This paper explores the application of MLflow in quantum
research, illustrating how it enables better development practices, experiment
reproducibility, decision making, and cross-domain integration in an
increasingly hybrid classical-quantum landscape.

</details>


### [155] [Trainability of Quantum Models Beyond Known Classical Simulability](https://arxiv.org/abs/2507.06344)
*Sabri Meyer,Francesco Scala,Francesco Tacchino,Aurelien Lucchi*

Main category: quant-ph

TL;DR: 论文探讨了变分量子算法（VQAs）的可训练性与计算复杂性之间的关系，提出了一种新方法避免梯度消失问题，并揭示了量子优势的可能性。


<details>
  <summary>Details</summary>
Motivation: 解决VQAs因梯度消失（barren plateaus）而难以扩展的问题，并验证避免梯度消失是否会导致经典可模拟性。

Method: 引入线性Clifford编码器（LCE）确保梯度统计恒定，并结合经典泰勒代理揭示计算复杂性相变。

Result: 证明了在无经典代理的区域可以避免梯度消失，数值实验验证了存在超多项式复杂性的过渡区。

Conclusion: 研究为设计无梯度消失的变分量子算法提供了理论支持，并展示了实现量子优势的潜力。

Abstract: Variational Quantum Algorithms (VQAs) are promising candidates for near-term
quantum computing, yet they face scalability challenges due to barren plateaus,
where gradients vanish exponentially in the system size. Recent conjectures
suggest that avoiding barren plateaus might inherently lead to classical
simulability, thus limiting the opportunities for quantum advantage. In this
work, we advance the theoretical understanding of the relationship between the
trainability and computational complexity of VQAs, thus directly addressing the
conjecture. We introduce the Linear Clifford Encoder (LCE), a novel technique
that ensures constant-scaling gradient statistics on optimization landscape
regions that are close to Clifford circuits. Additionally, we leverage
classical Taylor surrogates to reveal computational complexity phase
transitions from polynomial to super-polynomial as the initialization region
size increases. Combining these results, we reveal a deeper link between
trainability and computational complexity, and analytically prove that barren
plateaus can be avoided in regions for which no classical surrogate is known to
exist. Furthermore, numerical experiments on LCE transformed landscapes confirm
in practice the existence of a super-polynomially complex ``transition zone''
where gradients decay polynomially. These findings indicate a plausible path to
practically relevant, barren plateau-free variational models with potential for
quantum advantage.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [156] [Neural Actor-Critic Methods for Hamilton-Jacobi-Bellman PDEs: Asymptotic Analysis and Numerical Studies](https://arxiv.org/abs/2507.06428)
*Samuel N. Cohen,Jackson Hebner,Deqing Jiang,Justin Sirignano*

Main category: math.OC

TL;DR: 论文分析了用于求解高维HJB方程的演员-评论家机器学习算法，证明了其收敛性，并通过数值实验验证了其在200维问题中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决高维随机控制问题中的HJB方程，传统方法计算成本高，而神经网络方法可能陷入局部最优。

Method: 设计满足边界条件的评论家网络和基于哈密顿量积分的演员网络，证明其收敛性。

Result: 算法在200维问题中表现良好，验证了理论收敛性。

Conclusion: 该方法为高维HJB方程提供了有效解决方案，但需注意非凸哈密顿量的局限性。

Abstract: We mathematically analyze and numerically study an actor-critic machine
learning algorithm for solving high-dimensional Hamilton-Jacobi-Bellman (HJB)
partial differential equations from stochastic control theory. The architecture
of the critic (the estimator for the value function) is structured so that the
boundary condition is always perfectly satisfied (rather than being included in
the training loss) and utilizes a biased gradient which reduces computational
cost. The actor (the estimator for the optimal control) is trained by
minimizing the integral of the Hamiltonian over the domain, where the
Hamiltonian is estimated using the critic. We show that the training dynamics
of the actor and critic neural networks converge in a Sobolev-type space to a
certain infinite-dimensional ordinary differential equation (ODE) as the number
of hidden units in the actor and critic $\rightarrow \infty$. Further, under a
convexity-like assumption on the Hamiltonian, we prove that any fixed point of
this limit ODE is a solution of the original stochastic control problem. This
provides an important guarantee for the algorithm's performance in light of the
fact that finite-width neural networks may only converge to a local minimizers
(and not optimal solutions) due to the non-convexity of their loss functions.
In our numerical studies, we demonstrate that the algorithm can solve
stochastic control problems accurately in up to 200 dimensions. In particular,
we construct a series of increasingly complex stochastic control problems with
known analytic solutions and study the algorithm's numerical performance on
them. These problems range from a linear-quadratic regulator equation to highly
challenging equations with non-convex Hamiltonians, allowing us to identify and
analyze the strengths and limitations of this neural actor-critic method for
solving HJB equations.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [157] [Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing](https://arxiv.org/abs/2507.06584)
*Qiong Feng,Xiaotian Ma,Ziyuan Feng,Marat Akhin,Wei Song,Peng Liang*

Main category: cs.PL

TL;DR: CrossLangFuzzer是一个用于检测跨语言编译器错误的框架，通过中间表示和突变技术发现多个编译器中的错误。


<details>
  <summary>Details</summary>
Motivation: 跨语言编译的正确性研究不足，现有工作多关注单语言编译，因此需要填补这一研究空白。

Method: 提出CrossLangFuzzer框架，使用通用中间表示和三种突变技术（LangShuffler、FunctionRemoval、TypeChanger）生成多样化测试程序。

Result: 发现24个编译器错误（Kotlin、Groovy、Scala 3、Scala 2、Java），其中TypeChanger最有效。

Conclusion: 首次系统研究跨语言编译错误，为多语言环境下的编译器正确性改进提供支持。

Abstract: Compilers play a central role in translating high-level code into executable
programs, making their correctness essential for ensuring code safety and
reliability. While extensive research has focused on verifying the correctness
of compilers for single-language compilation, the correctness of cross-language
compilation - which involves the interaction between two languages and their
respective compilers - remains largely unexplored. To fill this research gap,
we propose CrossLangFuzzer, a novel framework that introduces a universal
intermediate representation (IR) for JVM-based languages and automatically
generates cross-language test programs with diverse type parameters and complex
inheritance structures. After generating the initial IR, CrossLangFuzzer
applies three mutation techniques - LangShuffler, FunctionRemoval, and
TypeChanger - to enhance program diversity. By evaluating both the original and
mutated programs across multiple compiler versions, CrossLangFuzzer
successfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed
bugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2
confirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java
compiler. Among all mutators, TypeChanger is the most effective, detecting 11
of the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes
of cross-compilation bugs, examining the respective responsibilities of
language compilers when incorrect behavior occurs during cross-language
compilation. To the best of our knowledge, this is the firstwork specifically
focused on identifying and diagnosing compiler bugs in cross-language
compilation scenarios. Our research helps to understand these challenges and
contributes to improving compiler correctness in multi-language environments.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [158] [Non-Asymptotic Analysis of Online Local Private Learning with SGD](https://arxiv.org/abs/2507.07041)
*Enze Shi,Jinhan Xie,Bei Jiang,Linglong Kong,Xuming He*

Main category: stat.ME

TL;DR: 本文对差分隐私随机梯度下降（DP-SGD）在在线问题和本地差分隐私（LDP）模型中的非渐近收敛性进行了系统性分析，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注非隐私优化方法，缺乏对隐私保护优化问题的非渐近收敛分析，本文旨在填补这一空白。

Method: 提出一个适用于在线LDP模型的通用框架，对敏感数据进行实时估计，并进行非渐近收敛分析。

Result: 通过理论推导和数值实验验证了所提估计器的有效性，并提供了超参数对收敛速率的实际影响指南。

Conclusion: 本文为非渐近收敛分析在隐私优化问题中的应用奠定了基础，并为实际应用提供了指导。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely
used for solving optimization problems with privacy guarantees in machine
learning and statistics. Despite this, a systematic non-asymptotic convergence
analysis for DP-SGD, particularly in the context of online problems and local
differential privacy (LDP) models, remains largely elusive. Existing
non-asymptotic analyses have focused on non-private optimization methods, and
hence are not applicable to privacy-preserving optimization problems. This work
initiates the analysis to bridge this gap and opens the door to non-asymptotic
convergence analysis of private optimization problems. A general framework is
investigated for the online LDP model in stochastic optimization problems. We
assume that sensitive information from individuals is collected sequentially
and aim to estimate, in real-time, a static parameter that pertains to the
population of interest. Most importantly, we conduct a comprehensive
non-asymptotic convergence analysis of the proposed estimators in finite-sample
situations, which gives their users practical guidelines regarding the effect
of various hyperparameters, such as step size, parameter dimensions, and
privacy budgets, on convergence rates. Our proposed estimators are validated in
the theoretical and practical realms by rigorous mathematical derivations and
carefully constructed numerical experiments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [159] [Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding](https://arxiv.org/abs/2507.06779)
*Martin Wimpff,Jan Zerfowski,Bin Yang*

Main category: cs.HC

TL;DR: 论文提出了一种名为RAP的新方法，解决了深度学习在实时脑机接口中的三大挑战：离线到在线的转换、计算复杂性和数据需求。


<details>
  <summary>Details</summary>
Motivation: 深度学习在离线脑机接口中表现优异，但在实时应用中面临三大挑战：离线模型的在线转换、计算复杂性和数据稀缺。

Method: 引入RAP方法，通过自适应修改现有离线模型的池化层，降低计算复杂性，并利用源自由域适应减少数据需求。

Result: RAP在实时脑机接口中表现稳健高效，保护隐私、减少校准需求，支持协同自适应系统。

Conclusion: RAP为深度学习在在线脑机接口中的广泛应用奠定了基础，推动了用户中心化高性能系统的开发。

Abstract: Despite the growing success of deep learning (DL) in offline brain-computer
interfaces (BCIs), its adoption in real-time applications remains limited due
to three primary challenges. First, most DL solutions are designed for offline
decoding, making the transition to online decoding unclear. Second, the use of
sliding windows in online decoding substantially increases computational
complexity. Third, DL models typically require large amounts of training data,
which are often scarce in BCI applications. To address these challenges and
enable real-time, cross-subject decoding without subject-specific calibration,
we introduce realtime adaptive pooling (RAP), a novel parameter-free method.
RAP seamlessly modifies the pooling layers of existing offline DL models to
meet online decoding requirements. It also reduces computational complexity
during training by jointly decoding consecutive sliding windows. To further
alleviate data requirements, our method leverages source-free domain
adaptation, enabling privacy-preserving adaptation across varying amounts of
target data. Our results demonstrate that RAP provides a robust and efficient
framework for real-time BCI applications. It preserves privacy, reduces
calibration demands, and supports co-adaptive BCI systems, paving the way for
broader adoption of DL in online BCIs. These findings lay a strong foundation
for developing user-centered, high-performance BCIs that facilitate immediate
feedback and user learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: 本文介绍了Prompt Declaration Language (PDL)，一种新型提示表示方法，旨在解决LLM提示工程的复杂性，通过将提示置于核心位置，支持手动和自动调整，并整合规则代码和外部工具。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程框架要么隐藏复杂性，要么提供不灵活的固定模式，难以支持复杂的代理编程。

Method: 提出PDL，通过声明式表示抽象组合逻辑，支持提示优化和自动化调整。

Result: 在实际案例中，使用PDL调整提示模式使性能提升高达4倍。

Conclusion: PDL通过简化提示工程和优化表示，显著提升了程序员效率和性能。

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction](https://arxiv.org/abs/2507.06404)
*Matteo Tiezzi,Tommaso Apicella,Carlos Cardenas-Perez,Giovanni Fregonese,Stefano Dafarra,Pietro Morerio,Daniele Pucci,Alessio Del Bue*

Main category: cs.RO

TL;DR: 提出了一种基于轨迹性能的通用评估框架NeME，用于比较模仿学习方法在复杂人机交互任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于成功率指标难以复现且无法捕捉机器人运动轨迹的复杂性，评估人形机器人性能具有挑战性。

Method: 设计了Neural Meta Evaluator (NeME)，一个深度学习模型，用于从机器人关节轨迹中分类动作，并作为元评估器比较控制策略的性能。

Result: 实验验证表明，该方法比基线更符合机器人的实际成功率，提供了可复现、系统化的评估手段。

Conclusion: NeME框架为复杂人机交互任务中的多模态模仿学习方法提供了一种有效的性能比较工具。

Abstract: Evaluating and comparing the performance of autonomous Humanoid Robots is
challenging, as success rate metrics are difficult to reproduce and fail to
capture the complexity of robot movement trajectories, critical in Human-Robot
Interaction and Collaboration (HRIC). To address these challenges, we propose a
general evaluation framework that measures the quality of Imitation Learning
(IL) methods by focusing on trajectory performance. We devise the Neural Meta
Evaluator (NeME), a deep learning model trained to classify actions from robot
joint trajectories. NeME serves as a meta-evaluator to compare the performance
of robot control policies, enabling policy evaluation without requiring human
involvement in the loop. We validate our framework on ergoCub, a humanoid
robot, using teleoperation data and comparing IL methods tailored to the
available platform. The experimental results indicate that our method is more
aligned with the success rate obtained on the robot than baselines, offering a
reproducible, systematic, and insightful means for comparing the performance of
multimodal imitation learning approaches in complex HRI tasks.

</details>


### [162] [Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic](https://arxiv.org/abs/2507.06625)
*Shizhe Cai,Jayadeep Jacob,Zeya Yin,Fabio Ramos*

Main category: cs.RO

TL;DR: Q-STAC结合贝叶斯MPC与actor-critic强化学习，通过约束Stein变分梯度下降优化控制序列，提升样本效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在连续控制任务中数据需求大、长时规划难及安全性不足的问题，同时弥补MPC仅局部最优和成本函数设计复杂的缺陷。

Method: 整合贝叶斯MPC与actor-critic强化学习，利用约束SVGD优化控制序列，以Q值替代显式成本函数设计。

Result: 在2D导航和机器人操作任务中，Q-STAC表现出更高的样本效率、鲁棒性和最优性。

Conclusion: Q-STAC成功结合了强化学习与MPC的优势，实现了高效、安全且最优的控制。

Abstract: Deep reinforcement learning has shown remarkable success in continuous
control tasks, yet often requires extensive training data, struggles with
complex, long-horizon planning, and fails to maintain safety constraints during
operation. Meanwhile, Model Predictive Control (MPC) offers explainability and
constraint satisfaction, but typically yields only locally optimal solutions
and demands careful cost function design. This paper introduces the Q-guided
STein variational model predictive Actor-Critic (Q-STAC), a novel framework
that bridges these approaches by integrating Bayesian MPC with actor-critic
reinforcement learning through constrained Stein Variational Gradient Descent
(SVGD). Our method optimizes control sequences directly using learned Q-values
as objectives, eliminating the need for explicit cost function design while
leveraging known system dynamics to enhance sample efficiency and ensure
control signals remain within safe boundaries. Extensive experiments on 2D
navigation and robotic manipulation tasks demonstrate that Q-STAC achieves
superior sample efficiency, robustness, and optimality compared to
state-of-the-art algorithms, while maintaining the high expressiveness of
policy distributions. Experiment videos are available on our website:
https://sites.google.com/view/q-stac

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [163] [Machine Learning based Enterprise Financial Audit Framework and High Risk Identification](https://arxiv.org/abs/2507.06266)
*Tingyu Yuan,Xi Zhang,Xuanjing Chen*

Main category: q-fin.RM

TL;DR: 本研究提出了一种基于AI的企业财务审计和高风险识别框架，通过机器学习提高效率和准确性。使用四大会计师事务所2020-2025年的数据集，评估了SVM、RF和KNN三种算法，最终RF表现最佳。


<details>
  <summary>Details</summary>
Motivation: 面对全球经济不确定性，传统人工审计方法在大数据量和复杂业务结构下效率低下，需AI驱动解决方案。

Method: 使用SVM、RF和KNN三种算法，通过分层K折交叉验证和F1分数等指标评估模型性能。

Result: RF表现最佳，F1分数0.9012，能有效识别欺诈和合规异常。关键预测因素包括审计频率、历史违规、员工工作量和客户评分。

Conclusion: 建议采用RF为核心模型，结合特征工程和实时风险监控，为现代企业智能审计和风险管理提供参考。

Abstract: In the face of global economic uncertainty, financial auditing has become
essential for regulatory compliance and risk mitigation. Traditional manual
auditing methods are increasingly limited by large data volumes, complex
business structures, and evolving fraud tactics. This study proposes an
AI-driven framework for enterprise financial audits and high-risk
identification, leveraging machine learning to improve efficiency and accuracy.
Using a dataset from the Big Four accounting firms (EY, PwC, Deloitte, KPMG)
from 2020 to 2025, the research examines trends in risk assessment, compliance
violations, and fraud detection. The dataset includes key indicators such as
audit project counts, high-risk cases, fraud instances, compliance breaches,
employee workload, and client satisfaction, capturing both audit behaviors and
AI's impact on operations. To build a robust risk prediction model, three
algorithms - Support Vector Machine (SVM), Random Forest (RF), and K-Nearest
Neighbors (KNN) - are evaluated. SVM uses hyperplane optimization for complex
classification, RF combines decision trees to manage high-dimensional,
nonlinear data with resistance to overfitting, and KNN applies distance-based
learning for flexible performance. Through hierarchical K-fold cross-validation
and evaluation using F1-score, accuracy, and recall, Random Forest achieves the
best performance, with an F1-score of 0.9012, excelling in identifying fraud
and compliance anomalies. Feature importance analysis reveals audit frequency,
past violations, employee workload, and client ratings as key predictors. The
study recommends adopting Random Forest as a core model, enhancing features via
engineering, and implementing real-time risk monitoring. This research
contributes valuable insights into using machine learning for intelligent
auditing and risk management in modern enterprises.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [164] [Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian Processes](https://arxiv.org/abs/2507.06938)
*Lisa Gaedke-Merzhäuser,Vincent Maillou,Fernando Rodriguez Avellaneda,Olaf Schenk,Mathieu Luisier,Paula Moraga,Alexandros Nikolaos Ziogas,Håvard Rue*

Main category: stat.CO

TL;DR: DALIA是一个高度可扩展的框架，用于时空多元高斯过程的贝叶斯推断，通过GPU加速和分布式内存并行方案，显著提升了计算性能。


<details>
  <summary>Details</summary>
Motivation: 多元高斯过程在高维时空应用中面临显著的计算挑战，需要一种高效的方法来解决这一问题。

Method: DALIA基于集成嵌套拉普拉斯近似方法，采用稀疏逆协方差矩阵、GPU加速的块密集方法，以及三层分布式内存并行方案。

Result: 在8倍参数空间的模型上，弱扩展性能超越现有技术两个数量级；在496个GH200超级芯片上运行时，强扩展速度提升三个数量级。应用于意大利北部的空气污染数据，展示了更高的空间分辨率。

Conclusion: DALIA框架显著提升了多元高斯过程在时空应用中的计算效率，为高维数据分析提供了实用工具。

Abstract: Multivariate Gaussian processes (GPs) offer a powerful probabilistic
framework to represent complex interdependent phenomena. They pose, however,
significant computational challenges in high-dimensional settings, which
frequently arise in spatial-temporal applications. We present DALIA, a highly
scalable framework for performing Bayesian inference tasks on spatio-temporal
multivariate GPs, based on the methodology of integrated nested Laplace
approximations. Our approach relies on a sparse inverse covariance matrix
formulation of the GP, puts forward a GPU-accelerated block-dense approach, and
introduces a hierarchical, triple-layer, distributed memory parallel scheme. We
showcase weak scaling performance surpassing the state-of-the-art by two orders
of magnitude on a model whose parameter space is 8$\times$ larger and measure
strong scaling speedups of three orders of magnitude when running on 496 GH200
superchips on the Alps supercomputer. Applying DALIA to air pollution data from
northern Italy over 48 days, we showcase refined spatial resolutions over the
aggregated pollutant measurements.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [165] [Connecting the Unconnected -- Sentiment Analysis of Field Survey of Internet Connectivity in Emerging Economies](https://arxiv.org/abs/2507.06827)
*Dibakar Das,Barath S Narayan,Aarna Bhammar,Jyotsna Bapat*

Main category: cs.CY

TL;DR: 本文分析了尼泊尔加德满都部分地区居民对互联网使用的体验和期望，发现高速、低成本、可靠和安全的互联网是主要需求，整体情绪偏积极。


<details>
  <summary>Details</summary>
Motivation: 研究互联网覆盖虽广但仍有40%全球人口无法使用宽带的问题，调查加德满都居民对互联网的体验和未来期望。

Method: 在加德满都三个地区进行实地调查，收集居民对互联网使用体验、生活影响及未来期望的数据，并进行情感分析和人口统计。

Result: 调查显示居民对互联网的总体情绪积极，高速、低成本、可靠和安全的互联网是主要需求；情感分析显示积极情绪方差高，消极情绪方差低。

Conclusion: 研究揭示了加德满都居民对互联网的积极态度和明确需求，为未来互联网服务改进提供了方向。

Abstract: Internet has significantly improved the quality of citizens across the world.
Though the internet coverage is quite high, 40% of global population do not
have access to broadband internet. This paper presents an analysis of a field
survey of population in some areas of Kathmandu, Nepal, an emerging economy.
This survey was triggered by intermittent severe congestion of internet in
certain areas of the city. People from three different areas were asked about
their present experience of internet usage, its impact on their lives and their
aspirations for the future. Survey pointed to high speed, low cost, reliable
and secure internet as a major aspiration of the respondents. Based on their
inputs, this paper presents a sentiment analysis as well as demographic
information. Keys insights from this analysis shows that overall sentiment to
most queries are positive. The variances of positive sentiments are high
whereas those for negative ones are low. Also, some correlations and clusters
are observed among the attributes though no dominant component exists in the
data.

</details>


### [166] [Deprecating Benchmarks: Criteria and Framework](https://arxiv.org/abs/2507.06434)
*Ayrton San Joaquin,Rokas Gipiškis,Leon Staufer,Ariel Gil*

Main category: cs.CY

TL;DR: 论文提出基准测试淘汰标准与框架，以解决AI模型评估中过时基准的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的快速发展，过时的基准测试可能误导模型能力评估，甚至掩盖真实问题。

Method: 通过回顾基准测试实践，提出淘汰基准的标准和框架。

Result: 提出了一套用于决定何时淘汰基准的准则和实施框架。

Conclusion: 研究旨在推动更严谨的基准测试评估，为开发者、用户和政策制定者提供指导。

Abstract: As frontier artificial intelligence (AI) models rapidly advance, benchmarks
are integral to comparing different models and measuring their progress in
different task-specific domains. However, there is a lack of guidance on when
and how benchmarks should be deprecated once they cease to effectively perform
their purpose. This risks benchmark scores over-valuing model capabilities, or
worse, obscuring capabilities and safety-washing. Based on a review of
benchmarking practices, we propose criteria to decide when to fully or
partially deprecate benchmarks, and a framework for deprecating benchmarks. Our
work aims to advance the state of benchmarking towards rigorous and quality
evaluations, especially for frontier models, and our recommendations are aimed
to benefit benchmark developers, benchmark users, AI governance actors (across
governments, academia, and industry panels), and policy makers.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [167] [X-ray transferable polyrepresentation learning](https://arxiv.org/abs/2507.06264)
*Weronika Hryniewska-Guzik,Przemyslaw Biecek*

Main category: eess.IV

TL;DR: 论文提出了一种名为“多表征”（polyrepresentation）的新概念，通过整合来自不同来源的同一模态的多种表征（如Siamese Network的向量嵌入、自监督模型和可解释的放射组学特征），提升了机器学习算法的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习算法的成功依赖于有意义的特征提取，而数据表示的质量和从未见过的数据集中有效提取特征的能力是关键挑战。

Method: 提出多表征方法，整合同一模态的多种表征（如Siamese Network的向量嵌入、自监督模型和放射组学特征）。

Result: 多表征方法在性能指标上优于单一表征，且在X射线图像中展示了其在小数据集上的可迁移性。

Conclusion: 多表征方法具有实用性和资源效率，适用于医学数据及其他领域，展示了广泛的潜在影响。

Abstract: The success of machine learning algorithms is inherently related to the
extraction of meaningful features, as they play a pivotal role in the
performance of these algorithms. Central to this challenge is the quality of
data representation. However, the ability to generalize and extract these
features effectively from unseen datasets is also crucial. In light of this, we
introduce a novel concept: the polyrepresentation. Polyrepresentation
integrates multiple representations of the same modality extracted from
distinct sources, for example, vector embeddings from the Siamese Network,
self-supervised models, and interpretable radiomic features. This approach
yields better performance metrics compared to relying on a single
representation. Additionally, in the context of X-ray images, we demonstrate
the transferability of the created polyrepresentation to a smaller dataset,
underscoring its potential as a pragmatic and resource-efficient approach in
various image-related solutions. It is worth noting that the concept of
polyprepresentation on the example of medical data can also be applied to other
domains, showcasing its versatility and broad potential impact.

</details>


### [168] [Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification](https://arxiv.org/abs/2507.06417)
*Laura Pituková,Peter Sinčák,László József Kovács*

Main category: eess.IV

TL;DR: 比较四种神经网络架构，提出新的Capsule-ConvKAN模型，在生物医学图像分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 改进特征表示和分类准确性，特别是在复杂的生物医学图像数据中。

Method: 结合Capsule Network的动态路由和空间层次能力与ConvKAN的灵活可解释函数逼近，提出Capsule-ConvKAN。

Result: 在组织病理学图像数据集上，Capsule-ConvKAN达到91.21%的最高分类准确率。

Conclusion: 新模型在捕获空间模式和管理复杂特征方面优于传统卷积模型。

Abstract: This study conducts a comprehensive comparison of four neural network
architectures: Convolutional Neural Network, Capsule Network, Convolutional
Kolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional
Kolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines
the dynamic routing and spatial hierarchy capabilities of Capsule Network with
the flexible and interpretable function approximation of Convolutional
Kolmogorov--Arnold Networks. This novel hybrid model was developed to improve
feature representation and classification accuracy, particularly in challenging
real-world biomedical image data. The architectures were evaluated on a
histopathological image dataset, where Capsule-ConvKAN achieved the highest
classification performance with an accuracy of 91.21\%. The results demonstrate
the potential of the newly introduced Capsule-ConvKAN in capturing spatial
patterns, managing complex features, and addressing the limitations of
traditional convolutional models in medical image classification.

</details>


### [169] [Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers](https://arxiv.org/abs/2507.06764)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: eess.IV

TL;DR: 提出Fast Equivariant Imaging (FEI)，一种无监督学习框架，无需真实数据即可高效训练深度成像网络。


<details>
  <summary>Details</summary>
Motivation: 解决传统Equivariant Imaging (EI) 训练效率低的问题，提出更高效的优化方法。

Method: 通过拉格朗日乘数法重新构建优化问题，并结合即插即用去噪器（PnP）。

Result: PnP-FEI方案在CT100数据集上训练U-Net时，比标准EI快10倍，且泛化性能更好。

Conclusion: FEI框架在无监督学习下显著提升了训练效率和性能。

Abstract: We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning
framework to efficiently train deep imaging networks without ground-truth data.
From the perspective of reformulating the Equivariant Imaging based
optimization problem via the method of Lagrange multipliers and utilizing
plug-and-play denoisers, this novel unsupervised scheme shows superior
efficiency and performance compared to vanilla Equivariant Imaging paradigm. In
particular, our PnP-FEI scheme achieves an order-of-magnitude (10x)
acceleration over standard EI on training U-Net with CT100 dataset for X-ray CT
reconstruction, with improved generalization performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [170] [Temporal Information Retrieval via Time-Specifier Model Merging](https://arxiv.org/abs/2507.06782)
*SeungYoon Han,Taeho Hwang,Sukmin Cho,Soyeong Jeong,Hoyun Song,Huije Lee,Jong C. Park*

Main category: cs.IR

TL;DR: 论文提出了一种名为TSM的新方法，通过合并针对不同时间指示符的专用检索器，提升时间约束查询的检索性能，同时保持非时间查询的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索方法在处理时间约束查询时表现不佳，且传统时间信息检索方法容易导致灾难性遗忘，影响非时间查询的性能。

Method: TSM训练针对单个时间指示符的专用检索器，并将它们合并为一个统一模型。

Result: 实验表明，TSM在时间约束查询上显著优于基线方法，同时保持非时间查询的高性能。

Conclusion: TSM是一种有效的方法，能够同时优化时间约束和非时间约束查询的检索性能。

Abstract: The rapid expansion of digital information and knowledge across structured
and unstructured sources has heightened the importance of Information Retrieval
(IR). While dense retrieval methods have substantially improved semantic
matching for general queries, they consistently underperform on queries with
explicit temporal constraints--often those containing numerical expressions and
time specifiers such as ``in 2015.'' Existing approaches to Temporal
Information Retrieval (TIR) improve temporal reasoning but often suffer from
catastrophic forgetting, leading to reduced performance on non-temporal
queries. To address this, we propose Time-Specifier Model Merging (TSM), a
novel method that enhances temporal retrieval while preserving accuracy on
non-temporal queries. TSM trains specialized retrievers for individual time
specifiers and merges them in to a unified model, enabling precise handling of
temporal constraints without compromising non-temporal retrieval. Extensive
experiments on both temporal and non-temporal datasets demonstrate that TSM
significantly improves performance on temporally constrained queries while
maintaining strong results on non-temporal queries, consistently outperforming
other baseline methods. Our code is available at
https://github.com/seungyoonee/TSM .

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [171] [Stochastic Alignments: Matching an Observed Trace to Stochastic Process Models](https://arxiv.org/abs/2507.06472)
*Tian Li,Artem Polyvyanyy,Sander J. J. Leemans*

Main category: cs.FL

TL;DR: 本文提出了一种基于启发式路径查找的算法，用于将观察到的轨迹与随机过程模型匹配，以解决传统对齐方法可能选择低概率路径的问题。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法在匹配轨迹与随机过程模型时，可能选择偏离较大的低概率路径，无法充分利用频率信息。

Method: 将问题表述为优化问题，并开发了一种启发式引导的路径查找算法。

Result: 开源实现验证了方法的可行性，并为分析人员提供了新的诊断见解。

Conclusion: 该方法能够有效匹配轨迹与模型路径，同时考虑路径的似然性和编辑距离。

Abstract: Process mining leverages event data extracted from IT systems to generate
insights into the business processes of organizations. Such insights benefit
from explicitly considering the frequency of behavior in business processes,
which is captured by stochastic process models. Given an observed trace and a
stochastic process model, conventional alignment-based conformance checking
techniques face a fundamental limitation: They prioritize matching the trace to
a model path with minimal deviations, which may, however, lead to selecting an
unlikely path. In this paper, we study the problem of matching an observed
trace to a stochastic process model by identifying a likely model path with a
low edit distance to the trace. We phrase this as an optimization problem and
develop a heuristic-guided path-finding algorithm to solve it. Our open-source
implementation demonstrates the feasibility of the approach and shows that it
can provide new, useful diagnostic insights for analysts.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [172] [From large-eddy simulations to deep learning: A U-net model for fast urban canopy flow predictions](https://arxiv.org/abs/2507.06533)
*Themistoklis Vargiemezis,Catherine Gorlé*

Main category: physics.comp-ph

TL;DR: 该研究提出了一种基于深度神经网络的快速预测城市风场的方法，显著降低了计算时间和成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如风洞和大涡模拟）成本高且耗时，需要更高效的风场预测方法以支持城市设计和行人安全。

Method: 采用U-Net架构，输入为2D建筑表示和符号距离函数，结合空间注意力模块，训练基于大涡模拟数据的模型。

Result: 模型在测试集上表现出高精度，平均相对误差为9.3%（速度大小）和5.2%（湍流强度）。

Conclusion: 深度学习方法能够快速准确地预测城市风场，为城市环境设计提供重要支持。

Abstract: Accurate prediction of wind flow fields in urban canopies is crucial for
ensuring pedestrian comfort, safety, and sustainable urban design. Traditional
methods using wind tunnels and Computational Fluid Dynamics, such as Large-Eddy
Simulations (LES), are limited by high costs, computational demands, and time
requirements. This study presents a deep neural network (DNN) approach for fast
and accurate predictions of urban wind flow fields, reducing computation time
from an order of 10 hours on 32 CPUs for one LES evaluation to an order of 1
second on a single GPU using the DNN model. We employ a U-Net architecture
trained on LES data including 252 synthetic urban configurations at seven wind
directions ($0^{o}$ to $90^{o}$ in $15^{o}$ increments). The model predicts two
key quantities of interest: mean velocity magnitude and streamwise turbulence
intensity, at multiple heights within the urban canopy. The U-net uses 2D
building representations augmented with signed distance functions and their
gradients as inputs, forming a $256\times256\times9$ tensor. In addition, a
Spatial Attention Module is used for feature transfer through skip connections.
The loss function combines the root-mean-square error of predictions, their
gradient magnitudes, and L2 regularization. Model evaluation on 50 test cases
demonstrates high accuracy with an overall mean relative error of 9.3% for
velocity magnitude and 5.2% for turbulence intensity. This research shows the
potential of deep learning approaches to provide fast, accurate urban wind
assessments essential for creating comfortable and safe urban environments.
Code is available at https://github.com/tvarg/Urban-FlowUnet.git

</details>
