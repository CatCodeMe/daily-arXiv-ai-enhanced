<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 67]
- [math.OC](#math.OC) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 11]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.IT](#cs.IT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Triadic First-Order Logic Queries in Temporal Networks](https://arxiv.org/abs/2507.17215)
*Omkar Bhalerao,Yunjie Pan,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: 本文介绍了一种新的时间网络分析方法——阈值一阶逻辑(FOL)模体分析，设计了FOLTY算法来挖掘更丰富的时间网络模式，在包含近7000万条边的图上能在一小时内完成查询。


<details>
  <summary>Details</summary>
Motivation: 传统的模体计数只能处理简单的存在性查询（如"找到在一小时内形成三角形的三个顶点"），无法挖掘网络中更丰富的信息。受逻辑学和数据库理论启发，作者希望引入阈值一阶逻辑来表达更复杂的查询语义，如"找到所有满足条件的顶点对u,v：对于与u相连的一半顶点w，v和w也在一小时内形成边"。

Method: 提出了阈值一阶逻辑(FOL)模体分析框架，结合存在量词和阈值全称量词来表达复杂查询。设计了FOLTY算法，这是首个用于挖掘阈值FOL三元查询的算法，使用专门的时间数据结构进行高效实现。

Result: FOLTY算法的理论运行时间与稀疏图中时间三角形计数的最佳已知运行时间相匹配。在实验中表现优异，能够在普通硬件上一小时内处理包含近7000万条边的图的三元FOL查询。

Conclusion: 本工作为经典的模体分析问题开辟了新的研究方向，通过引入阈值FOL查询语义，能够从大规模时间网络中挖掘更丰富的结构信息，为网络分析提供了更强大的工具。

Abstract: Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

</details>


### [2] [Unfolding Data Quality Dimensions in Practice: A Survey](https://arxiv.org/abs/2507.17507)
*Vasileios Papastergios,Lisa Ehrlinger,Anastasios Gounaris*

Main category: cs.DB

TL;DR: 本文通过系统分析七个开源数据质量工具，建立了数据质量工具功能与高层次质量维度之间的映射关系，为理论与实践搭建桥梁


<details>
  <summary>Details</summary>
Motivation: 数据质量理论研究丰富且有标准化尝试（如ISO/IEC 25012），但其实际应用往往不明确；同时存在大量检测特定数据质量问题的工具，但理论与实践之间存在鸿沟，需要将低层次工具功能与高层次质量维度系统性连接

Method: 系统性调研七个开源数据质量工具，分析其功能实现，建立工具功能与数据质量维度（如准确性、完整性、一致性、及时性等）之间的多对多映射关系

Result: 提供了数据质量工具功能与质量维度之间的全面映射，展示了单个功能及其变体如何部分贡献于单一维度的评估，揭示了数据质量检查领域的碎片化现状

Conclusion: 为从业者和研究人员提供了数据质量检查碎片化领域的统一视角，为跨多个维度的质量评估提供了可操作的见解，成功连接了数据质量理论与实践

Abstract: Data quality describes the degree to which data meet specific requirements
and are fit for use by humans and/or downstream tasks (e.g., artificial
intelligence). Data quality can be assessed across multiple high-level concepts
called dimensions, such as accuracy, completeness, consistency, or timeliness.
While extensive research and several attempts for standardization (e.g.,
ISO/IEC 25012) exist for data quality dimensions, their practical application
often remains unclear. In parallel to research endeavors, a large number of
tools have been developed that implement functionalities for the detection and
mitigation of specific data quality issues, such as missing values or outliers.
With this paper, we aim to bridge this gap between data quality theory and
practice by systematically connecting low-level functionalities offered by data
quality tools with high-level dimensions, revealing their many-to-many
relationships. Through an examination of seven open-source data quality tools,
we provide a comprehensive mapping between their functionalities and the data
quality dimensions, demonstrating how individual functionalities and their
variants partially contribute to the assessment of single dimensions. This
systematic survey provides both practitioners and researchers with a unified
view on the fragmented landscape of data quality checks, offering actionable
insights for quality assessment across multiple dimensions.

</details>


### [3] [SHINE: A Scalable HNSW Index in Disaggregated Memory](https://arxiv.org/abs/2507.17647)
*Manuel Widmoser,Daniel Kocher,Nikolaus Augsten*

Main category: cs.DB

TL;DR: 本文提出了一种适用于分解内存架构的可扩展HNSW索引方法，通过保持图结构完整性和高效缓存机制，在不损失精度的情况下实现了大规模高维向量的近似最近邻搜索。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式ANN搜索方法通过分割图结构来扩展，但这会损失精度。在分解内存架构中，计算和存储分离，通过RDMA网络连接，从远程内存连续获取高维向量数据会面临严重的网络带宽限制问题。

Method: 提出了一种图保持的HNSW索引方法，不分割图结构以保持与单机HNSW相同的精度。采用高效的缓存机制克服网络带宽限制，并逻辑上组合多个计算节点的缓存以提高整体缓存效率。

Result: 实验评估确认了该方法的效率和可扩展性，能够在分解内存架构中实现高效的大规模向量搜索，同时保持与单机HNSW相同的搜索精度。

Conclusion: 通过图保持索引和智能缓存策略的结合，成功解决了分解内存架构中ANN搜索的精度和性能挑战，为大规模高维向量搜索提供了有效的解决方案。

Abstract: Approximate nearest neighbor (ANN) search is a fundamental problem in
computer science for which in-memory graph-based methods, such as Hierarchical
Navigable Small World (HNSW), perform exceptionally well. To scale beyond
billions of high-dimensional vectors, the index must be distributed. The
disaggregated memory architecture physically separates compute and memory into
two distinct hardware units and has become popular in modern data centers. Both
units are connected via RDMA networks that allow compute nodes to directly
access remote memory and perform all the computations, posing unique challenges
for disaggregated indexes.
  In this work, we propose a scalable HNSW index for ANN search in
disaggregated memory. In contrast to existing distributed approaches, which
partition the graph at the cost of accuracy, our method builds a
graph-preserving index that reaches the same accuracy as a single-machine HNSW.
Continuously fetching high-dimensional vector data from remote memory leads to
severe network bandwidth limitations, which we overcome by employing an
efficient caching mechanism. Since answering a single query involves processing
numerous unique graph nodes, caching alone is not sufficient to achieve high
scalability. We logically combine the caches of the compute nodes to increase
the overall cache effectiveness and confirm the efficiency and scalability of
our method in our evaluation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs](https://arxiv.org/abs/2507.17087)
*Anjiang Wei,Rohan Yadav,Hang Song,Wonchan Lee,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: 论文提出了Mapple，一个高级声明式编程接口，用于简化分布式异构系统中并行程序的映射优化，通过提供转换原语（特别是decompose原语）来解决迭代空间和处理器空间之间的维度不匹配问题，显著减少了映射器代码量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 分布式异构系统中并行程序的优化仍然是一个复杂任务，通常需要大量代码修改。虽然基于任务的编程系统通过将性能决策与核心应用逻辑分离来提高模块化，但它们的映射接口往往过于底层，难以使用。

Method: 提出Mapple，一个高级声明式编程接口，用于分布式应用程序的映射。Mapple提供转换原语来解决迭代空间和处理器空间之间的维度不匹配，包括关键的decompose原语来最小化通信量。通过将Mapple映射器翻译成Legion运行时的底层C++接口来实现该系统。

Result: 在九个应用程序（包括六种矩阵乘法算法和三个科学计算工作负载）上的评估显示，Mapple将映射器代码大小减少了14倍，相比专家编写的C++映射器实现了高达1.34倍的性能提升。decompose原语相比现有的维度解析启发式方法实现了高达1.83倍的改进。

Conclusion: Mapple成功简化了分布式应用程序高性能映射器的开发过程，通过提供高级声明式接口和有效的转换原语，既减少了开发复杂度又提升了性能表现，证明了高级映射接口在分布式计算中的价值。

Abstract: Optimizing parallel programs for distributed heterogeneous systems remains a
complex task, often requiring significant code modifications. Task-based
programming systems improve modularity by separating performance decisions from
core application logic, but their mapping interfaces are often too low-level.
In this work, we introduce Mapple, a high-level, declarative programming
interface for mapping distributed applications. Mapple provides transformation
primitives to resolve dimensionality mismatches between iteration and processor
spaces, including a key primitive, decompose, that helps minimize communication
volume. We implement Mapple on top of the Legion runtime by translating Mapple
mappers into its low-level C++ interface. Across nine applications, including
six matrix multiplication algorithms and three scientific computing workloads,
Mapple reduces mapper code size by 14X and enables performance improvements of
up to 1.34X over expert-written C++ mappers. In addition, the decompose
primitive achieves up to 1.83X improvement over existing
dimensionality-resolution heuristics. These results demonstrate that Mapple
simplifies the development of high-performance mappers for distributed
applications.

</details>


### [5] [PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.17094)
*Sukjin Kim,Seongyeon Park,Si Ung Noh,Junguk Hong,Taehee Kwon,Hunseong Lim,Jinho Lee*

Main category: cs.DC

TL;DR: PathWeaver是一个新颖的多GPU框架，通过流水线路径扩展、幽灵暂存和方向引导选择三个核心技术，显著提升了大规模数据集上基于图的近似最近邻搜索(ANNS)性能，相比现有多GPU ANNS框架实现了3.24倍的几何平均加速和最高5.30倍的加速


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速的ANNS工作虽然显著提升了性能，但随着数据集规模不断增长，需要高效的多GPU解决方案。然而现有工作的设计忽视了多GPU可扩展性，采用简单的方法将额外GPU仅作为扩展内存容量的手段，通过分割数据集并在每个GPU上独立搜索导致效率低下

Method: 提出PathWeaver多GPU框架，包含三个核心技术：1)基于流水线的路径扩展 - GPU感知的流水线机制，通过利用GPU间通信减少冗余搜索迭代；2)幽灵暂存 - 利用代表性数据集识别最优查询起始点，减少困难查询的搜索空间；3)方向引导选择 - 数据选择技术，在搜索过程早期过滤无关点，最小化不必要的内存访问和距离计算

Result: 在多个多样化数据集上的综合评估显示，PathWeaver在95%召回率下相比最先进的多GPU ANNS框架实现了3.24倍的几何平均加速，最高可达5.30倍的加速

Conclusion: PathWeaver通过三个创新技术成功解决了现有多GPU ANNS框架的可扩展性问题，为大规模数据集的近似最近邻搜索提供了高效的多GPU解决方案，显著提升了搜索性能

Abstract: Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in
numerous applications, such as recommendation systems, natural language
processing, and computer vision. While recent works on GPU-based acceleration
have significantly advanced ANNS performance, the ever-growing scale of
datasets now demands efficient multi-GPU solutions. However, the design of
existing works overlooks multi-GPU scalability, resulting in naive approaches
that treat additional GPUs as a means to extend memory capacity for large
datasets. This inefficiency arises from partitioning the dataset and
independently searching for data points similar to the queries in each GPU. We
therefore propose PathWeaver, a novel multi-GPU framework designed to scale and
accelerate ANNS for large datasets. First, we propose pipelining-based path
extension, a GPU-aware pipelining mechanism that reduces prior work's redundant
search iterations by leveraging GPU-to-GPU communication. Second, we design
ghost staging that leverages a representative dataset to identify optimal query
starting points, reducing the search space for challenging queries. Finally, we
introduce direction-guided selection, a data selection technique that filters
irrelevant points early in the search process, minimizing unnecessary memory
accesses and distance computations. Comprehensive evaluations across diverse
datasets demonstrate that PathWeaver achieves 3.24$\times$ geomean speedup and
up to 5.30$\times$ speedup on 95% recall rate over state-of-the-art
multi-GPU-based ANNS frameworks.

</details>


### [6] [BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving](https://arxiv.org/abs/2507.17120)
*Wanyi Zheng,Minxian Xu,Shengye Song,Kejiang Ye*

Main category: cs.DC

TL;DR: 本文提出了BucketServe，一个基于桶的动态批处理框架，通过将请求按序列长度分组到同质桶中来优化大语言模型推理性能，显著提升了吞吐量并减少了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型服务系统使用静态或连续批处理策略，在异构工作负载下会导致GPU内存利用率低下和延迟增加，难以适应动态工作负载波动，造成吞吐量次优和服务水平目标违反。

Method: 提出BucketServe框架，根据序列长度将请求分组到大小同质的桶中，最小化填充开销，通过实时批量大小调整优化GPU内存使用，引入自适应桶分割/合并和优先级感知调度机制。

Result: BucketServe在吞吐量方面显著优于UELLM，实现高达3.58倍的改进；在80% SLO达成率下可处理比DistServe多1.93倍的请求负载；相比UELLM展现出1.975倍更高的系统负载能力。

Conclusion: BucketServe通过基于桶的动态批处理有效解决了大语言模型推理服务中的资源利用和性能问题，在保证服务质量的同时显著提升了系统吞吐量和负载处理能力。

Abstract: Large language models (LLMs) have become increasingly popular in various
areas, traditional business gradually shifting from rule-based systems to
LLM-based solutions. However, the inference of LLMs is resource-intensive or
latency-sensitive, posing significant challenges for serving systems. Existing
LLM serving systems often use static or continuous batching strategies, which
can lead to inefficient GPU memory utilization and increased latency,
especially under heterogeneous workloads. These methods may also struggle to
adapt to dynamic workload fluctuations, resulting in suboptimal throughput and
potential service level objective (SLO) violations. In this paper, we introduce
BucketServe, a bucket-based dynamic batching framework designed to optimize LLM
inference performance. By grouping requests into size-homogeneous buckets based
on sequence length, BucketServe minimizes padding overhead and optimizes GPU
memory usage through real-time batch size adjustments preventing out-of-memory
(OOM) errors. It introduces adaptive bucket splitting/merging and
priority-aware scheduling to mitigate resource fragmentation and ensure SLO
compliance. Experiment shows that BucketServe significantly outperforms UELLM
in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more
request load under the SLO attainment of 80% compared with DistServe and
demonstrates 1.975x higher system load capacity compared to the UELLM.

</details>


### [7] [Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy](https://arxiv.org/abs/2507.17128)
*Minxian Xu,Linfeng Wen,Junhan Liao,Huaming Wu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文系统综述了2020年以来云原生应用自动扩缩容的最新方法，提出了从基础设施、架构、扩缩容方法、优化目标和行为建模五个维度的分类体系，并指出了大模型应用、微服务依赖管理和元学习技术等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 云原生应用内部交互复杂，服务数量和负载不断变化，对自动扩缩容方法提出了更高要求，涉及微服务依赖分析、性能分析、异常检测、工作负载特征化和任务协同定位等多项挑战，需要系统性的综述研究来梳理现有方法并指导未来发展。

Method: 采用系统文献综述方法，收集并分析2020年以来云原生应用自动扩缩容领域的研究文献，建立包含基础设施、架构、扩缩容方法、优化目标和行为建模五个维度的详细分类体系，对各种方法进行全面比较和深入讨论。

Result: 提出了云原生应用自动扩缩容方法的五维分类体系，全面比较了各种方法的关键特征、优势、局限性和应用场景，识别了当前研究领域的空白和未解决的挑战，特别是在不同环境下的性能表现方面。

Conclusion: 总结了该领域的研究现状，强调了大模型应用、微服务依赖管理和元学习技术等有前景的未来研究方向，这些技术有望增强模型在不同环境中的适用性和适应性，为云原生应用自动扩缩容提供更好的解决方案。

Abstract: The interactions within cloud-native applications are complex, with a
constantly changing number of services and loads, posing higher demands on
auto-scaling approach. This mainly involves several challenges such as
microservices dependency analysis, performance profiling, anomaly detection,
workload characterization and task co-location. Therefore, some advanced
algorithms have been investigated into auto-scaling cloud-native applications
to optimize system and application performance. These algorithms can learn from
historical data and appropriately adjust resource allocation based on the
current environment and load conditions to optimize resource utilization and
system performance. In this paper, we systematically review the literature on
state-of-the-art auto-scaling approaches for cloud-native applications from
2020, and further explore the technological evolution. Additionally, we propose
a detailed taxonomy to categorize current research from five perspectives,
including infrastructure, architecture, scaling methods, optimization
objectives, and behavior modeling. Then, we provide a comprehensive comparison
and in-depth discussion of the key features, advantages, limitations, and
application scenarios of each approach, considering their performance in
diverse environments and under various conditions. Finally, we summarize the
current state of research in this field, identify the gaps and unresolved
challenges, and emphasize promising directions for future exploration,
particularly in areas such as the application of large models, microservice
dependency management, and the use of meta-learning techniques to enhance model
applicability and adaptability across different environments.

</details>


### [8] [BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs](https://arxiv.org/abs/2507.17133)
*Jianmin Hu,Minxian Xu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: BrownoutServe是一个针对混合专家（MoE）大语言模型的新型推理服务框架，通过"联合专家"和动态brownout机制优化推理效率，在保证服务质量的同时实现了2.07倍的吞吐量提升和90.28%的SLO违规减少。


<details>
  <summary>Details</summary>
Motivation: 现有MoE大语言模型系统存在静态模型部署和缺乏动态工作负载适应能力的问题，导致资源利用率低下、延迟增加，特别是在突发请求期间表现不佳。

Method: 提出BrownoutServe框架，包含两个核心技术：（1）"联合专家"机制，整合多个专家的知识以减少专家访问次数和推理延迟；（2）动态brownout机制，自适应调整特定token的处理过程，在保证服务水平目标（SLO）的前提下优化推理性能。

Result: 在各种工作负载下的评估显示，BrownoutServe相比vLLM实现了高达2.07倍的吞吐量提升，SLO违规减少了90.28%，在突发流量下展现了良好的鲁棒性，同时保持了可接受的推理准确性。

Conclusion: BrownoutServe成功解决了MoE大语言模型在动态计算需求和流量条件下的推理效率和服务可靠性问题，为MoE模型的实际部署提供了有效的解决方案。

Abstract: In recent years, the Mixture-of-Experts (MoE) architecture has been widely
applied to large language models (LLMs), providing a promising solution that
activates only a subset of the model's parameters during computation, thereby
reducing overall memory requirements and allowing for faster inference compared
to dense models. Despite these advantages, existing systems still face issues
of low efficiency due to static model placement and lack of dynamic workloads
adaptation. This leads to suboptimal resource utilization and increased
latency, especially during bursty requests periods.
  To address these challenges, this paper introduces BrownoutServe, a novel
serving framework designed to optimize inference efficiency and maintain
service reliability for MoE-based LLMs under dynamic computational demands and
traffic conditions. BrownoutServe introduces "united experts" that integrate
knowledge from multiple experts, reducing the times of expert access and
inference latency. Additionally, it proposes a dynamic brownout mechanism to
adaptively adjust the processing of certain tokens, optimizing inference
performance while guaranteeing service level objectives (SLOs) are met. Our
evaluations show the effectiveness of BrownoutServe under various workloads: it
achieves up to 2.07x throughput improvement compared to vLLM and reduces SLO
violations by 90.28%, showcasing its robustness under bursty traffic while
maintaining acceptable inference accuracy.

</details>


### [9] [Efficient Column-Wise N:M Pruning on RISC-V CPU](https://arxiv.org/abs/2507.17301)
*Chi-Wei Chu,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 本文提出了一种列级N:M剪枝策略，结合XNNPACK修改和操作融合优化，在RISC-V向量架构上实现了高效的CNN模型推理，相比密集基线模型实现了4.0倍的推理吞吐量提升，同时ImageNet top-1准确率仅下降2.1%。


<details>
  <summary>Details</summary>
Motivation: 深度学习中权重剪枝是提高计算效率的重要技术，特别是对于CNN中的卷积算子性能瓶颈问题。然而剪枝的有效性很大程度上取决于具体实现方式，不同的方法会显著影响计算性能和内存占用。因此需要针对特定架构设计高效的剪枝实现方案。

Method: 提出了瓦片级别的列级N:M剪枝策略，修改XNNPACK以支持在RISC-V向量架构上高效执行剪枝模型。此外，融合im2col和数据打包操作来减少冗余内存访问和内存开销。结合AITemplate的性能分析技术来为每个卷积算子识别最优实现方案。

Result: 所提出的方法有效地将ResNet推理吞吐量提升了高达4.0倍，同时在ImageNet数据集上的top-1准确率相比密集基线模型仅下降2.1%，实现了性能和精度的良好平衡。

Conclusion: 通过列级N:M剪枝策略和针对RISC-V架构的优化实现，成功在保持较高模型精度的同时显著提升了CNN模型的推理性能，为在资源受限的RISC-V平台上部署高效深度学习模型提供了有效解决方案。

Abstract: In deep learning frameworks, weight pruning is a widely used technique for
improving computational efficiency by reducing the size of large models. This
is especially critical for convolutional operators, which often act as
performance bottlenecks in convolutional neural networks (CNNs). However, the
effectiveness of pruning heavily depends on how it is implemented, as different
methods can significantly impact both computational performance and memory
footprint. In this work, we propose a column-wise N:M pruning strategy applied
at the tile level and modify XNNPACK to enable efficient execution of pruned
models on the RISC-V vector architecture. Additionally, we propose fusing the
operations of im2col and data packing to minimize redundant memory accesses and
memory overhead. To further optimize performance, we incorporate AITemplate's
profiling technique to identify the optimal implementation for each
convolutional operator. Our proposed approach effectively increases ResNet
inference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy
within 2.1\% of the dense baseline.

</details>


### [10] [Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions](https://arxiv.org/abs/2507.17411)
*Pál András Papp,Toni Böhnlein,A. N. Yzelman*

Main category: cs.DC

TL;DR: 本文研究在两级内存层次结构的多处理器环境中调度计算DAG的问题，提出了基于整数线性规划的整体调度算法，实验表明该方法比传统的分离式调度和内存管理方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的调度算法通常将并行化和内存管理分开优化，但这种分离式方法可能导致解决方案与最优解存在线性倍数的差距。因此需要一种能够同时考虑工作负载平衡、通信和缓存大小限制导致的数据移动的整体调度方法。

Method: 将问题建模为整数线性规划(ILP)问题，开发了一个基于ILP的整体调度算法，该算法能够同时优化并行化和内存管理，而不是将两者分离处理。

Result: 实验结果表明，基于ILP的方法能够找到比结合经典调度算法和内存管理策略的基线方法显著更好的解决方案。同时从理论角度分析了该问题的基本性质，包括计算复杂性。

Conclusion: 在两级内存层次结构的多处理器环境中，整体优化调度和内存管理比分离式方法更有效。基于ILP的调度算法能够获得更好的性能，证明了同时考虑并行化和内存管理的重要性。

Abstract: We study the problem of scheduling a general computational DAG on multiple
processors in a 2-level memory hierarchy. This setting is a natural
generalization of several prominent models in the literature, and it
simultaneously captures workload balancing, communication, and data movement
due to cache size limitations. We first analyze the fundamental properties of
this problem from a theoretical perspective, such as its computational
complexity. We also prove that optimizing parallelization and memory management
separately, as done in many applications, can result in a solution that is a
linear factor away from the optimum.
  On the algorithmic side, we discuss a natural technique to represent and
solve the problem as an Integer Linear Program (ILP). We develop a holistic
scheduling algorithm based on this approach, and we experimentally study its
performance and properties on a small benchmark of computational tasks. Our
results confirm that the ILP-based method can indeed find considerably better
solutions than a baseline which combines classical scheduling algorithms and
memory management policies.

</details>


### [11] [Distributed P2P quantile tracking with relative value error](https://arxiv.org/abs/2507.17458)
*Marco Pulimeno,Italo Epicoco,Massimo Cafaro*

Main category: cs.DC

TL;DR: 本文提出了DUDDSketch算法，这是UDDSketch算法的分布式版本，用于在非结构化P2P网络中准确跟踪分位数，采用完全去中心化的gossip协议


<details>
  <summary>Details</summary>
Motivation: 现有的分位数跟踪算法主要针对单机环境设计，在分布式P2P网络环境下缺乏有效的去中心化解决方案，需要一个能够在非结构化网络中准确跟踪分位数的分布式算法

Method: 设计了DUDDSketch分布式算法，基于UDDSketch的扩展，采用完全去中心化的gossip协议在非结构化P2P网络中工作，通过节点间的信息交换来实现分布式分位数跟踪

Result: 通过大量实验验证，DUDDSketch算法能够收敛到与顺序算法相同的结果，证明了算法的有效性和准确性，实现了分布式环境下的精确分位数跟踪

Conclusion: DUDDSketch算法成功实现了在非结构化P2P网络中的准确分位数跟踪，具有完全去中心化特性，算法正确性得到形式化证明，实验结果表明其能收敛到顺序算法的结果，为分布式环境下的分位数计算提供了可靠解决方案

Abstract: In this paper we present \textsc{DUDDSketch}, a distributed version of the
\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm
is a fully decentralized, gossip-based distributed protocol working in the
context of unstructured P2P networks. We discuss the algorithm's design and
formally prove its correctness. We also show, through extensive experimental
results, that the algorithm converges to the results provided by the sequential
algorithm, which is a fundamental and highly desirable property.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [12] [Optimal Pure Differentially Private Sparse Histograms in Near-Linear Deterministic Time](https://arxiv.org/abs/2507.17017)
*Florian Kerschbaum,Steven Lee,Hao Wu*

Main category: cs.DS

TL;DR: 本文提出了一种纯差分隐私稀疏直方图算法，在包含n个参与者和域大小d≫n的设置下，实现了最优的ℓ∞估计误差，运行时间严格为O(n ln ln d)，突破了之前O(n²)的二次时间复杂度障碍。


<details>
  <summary>Details</summary>
Motivation: 解决纯差分隐私稀疏直方图算法的时间复杂度瓶颈问题。之前最好的确定性时间界限为Õ(n²)，存在二次时间复杂度的障碍，这是一个由Balcer和Vadhan在2019年提出的开放问题，需要突破这个二次障碍。

Method: 核心方法是提出了一种新颖的"私有项目覆盖技术"（private item blanket technique）结合目标长度填充（target-length padding），将基于稳定性的近似差分隐私直方图算法转换为纯差分隐私算法。

Result: 算法在word-RAM模型下运行时间严格为O(n ln ln d)，达到了最优的ℓ∞估计误差，成功突破了之前的二次时间复杂度障碍，从Õ(n²)改进到O(n ln ln d)。

Conclusion: 成功解决了纯差分隐私稀疏直方图的时间复杂度开放问题，通过创新的私有项目覆盖技术实现了显著的性能提升，为差分隐私算法的效率优化提供了新的技术路径。

Abstract: We introduce an algorithm that releases a pure differentially private sparse
histogram over $n$ participants drawn from a domain of size $d \gg n$. Our
method attains the optimal $\ell_\infty$-estimation error and runs in strictly
$O(n \ln \ln d)$ time in the word-RAM model, thereby improving upon the
previous best known deterministic-time bound of $\tilde{O}(n^2)$ and resolving
the open problem of breaking this quadratic barrier (Balcer and Vadhan, 2019).
Central to our algorithm is a novel private item blanket technique with
target-length padding, which transforms the approximate differentially private
stability-based histogram algorithm into a pure differentially private one.

</details>


### [13] [Compatibility of Max and Sum Objectives for Committee Selection and $k$-Facility Location](https://arxiv.org/abs/2507.17063)
*Yue Han,Elliot Anshelevich*

Main category: cs.DS

TL;DR: 该论文研究了度量设施位置问题的多目标优化版本，证明了可以找到同时接近多个目标最优解的解决方案，而不需要为了优化一个目标而牺牲另一个目标。


<details>
  <summary>Details</summary>
Motivation: 在设施位置问题或委员会选择问题中，通常需要在多个不同的优化目标之间做出权衡。现有研究主要关注单一目标优化，但实际应用中往往需要同时考虑多个目标。因此，研究这些目标之间的兼容性，寻找能够同时满足多个目标的解决方案具有重要意义。

Method: 论文考虑了四种不同的目标函数：每个客户试图最小化其到所选设施距离的总和或最大值，而总体目标考虑个体客户成本的总和或最大值。通过分析这些目标之间的兼容性，研究任意一对目标是否存在同时接近最优的解决方案。

Result: 研究结果表明，对于上述任意一对目标，都存在同时接近最优的解决方案。这意味着在选择设施集合或代表性委员会时，通常可以找到对多个目标都表现良好的解决方案。

Conclusion: 在设施位置问题和委员会选择问题中，不同的优化目标之间具有良好的兼容性。可以找到同时满足多个目标的解决方案，而不需要为了实现一个目标而完全牺牲另一个目标，这为实际应用中的多目标决策提供了理论支持。

Abstract: We study a version of the metric facility location problem (or, equivalently,
variants of the committee selection problem) in which we must choose $k$
facilities in an arbitrary metric space to serve some set of clients $C$. We
consider four different objectives, where each client $i\in C$ attempts to
minimize either the sum or the maximum of its distance to the chosen
facilities, and where the overall objective either considers the sum or the
maximum of the individual client costs. Rather than optimizing a single
objective at a time, we study how compatible these objectives are with each
other, and show the existence of solutions which are simultaneously
close-to-optimum for any pair of the above objectives. Our results show that
when choosing a set of facilities or a representative committee, it is often
possible to form a solution which is good for several objectives at the same
time, instead of sacrificing one desideratum to achieve another.

</details>


### [14] [Advancing Quantum State Preparation using LimTDD](https://arxiv.org/abs/2507.17170)
*Xin Hong,Aochu Dai,Chenjian Li,Sanjiang Li,Shenggang Ying,Mingsheng Ying*

Main category: cs.DS

TL;DR: 本文提出了基于LimTDD(局部可逆映射张量决策图)的量子态制备算法族，针对不同数量的辅助量子比特场景设计了高效算法，在运行时间和门复杂度方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量子态制备是量子计算和量子信息处理中的基础任务，对量子机器学习等量子算法的执行至关重要。现有的量子态制备算法在处理大规模量子态时存在效率和可扩展性问题，需要开发更高效的算法来降低量子电路复杂度。

Method: 提出了基于局部可逆映射张量决策图(LimTDD)的新型决策图方法，该方法结合了张量网络和决策图来实现量子态的高度紧凑表示。针对不同的辅助量子比特数量场景(无辅助、单个辅助、大量辅助)设计了相应的量子态制备算法族。

Result: 大量实验表明，所提出的方法在运行时间和门复杂度方面显著优于现有方法，对大规模量子态展现出更好的可扩展性。在最佳情况下，该方法显示出指数级的改进效果。

Conclusion: 基于LimTDD的量子态制备算法为不同辅助量子比特场景提供了高效的解决方案，在性能和可扩展性方面取得了显著突破，为量子计算中的量子态制备任务提供了新的有效途径。

Abstract: Quantum state preparation (QSP) is a fundamental task in quantum computing
and quantum information processing. It is critical to the execution of many
quantum algorithms, including those in quantum machine learning. In this paper,
we propose a family of efficient QSP algorithms tailored to different numbers
of available ancilla qubits - ranging from no ancilla qubits, to a single
ancilla qubit, to a sufficiently large number of ancilla qubits. Our algorithms
are based on a novel decision diagram that is fundamentally different from the
approaches used in previous QSP algorithms. Specifically, our approach exploits
the power of Local Invertible Map Tensor Decision Diagrams (LimTDDs) - a highly
compact representation of quantum states that combines tensor networks and
decision diagrams to reduce quantum circuit complexity. Extensive experiments
demonstrate that our methods significantly outperform existing approaches and
exhibit better scalability for large-scale quantum states, both in terms of
runtime and gate complexity. Furthermore, our method shows exponential
improvement in best-case scenarios. This paper is an extended version of [1],
with three more algorithms proposed.

</details>


### [15] [RLZ-r and LZ-End-r: Enhancing Move-r](https://arxiv.org/abs/2507.17300)
*Patrick Dinklage,Johannes Fischer,Lukas Nalbach,Jan Zumbrink*

Main category: cs.DS

TL;DR: 本文通过集成压缩后缀数组来增强r-index和Move-r索引，使用Relative Lempel-Ziv和LZ-End两种压缩方案，在空间和查询性能之间提供新的权衡选择，显著提升了locate查询的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的r-index在回答locate查询时存在实际瓶颈，需要为每个匹配位置计算函数Φ，这促使研究者通过压缩后缀数组的高效随机访问来增强索引，以空间换取更快的locate查询速度。

Method: 采用两种压缩方案来构建压缩后缀数组：1) Relative Lempel-Ziv压缩方案，改进了Puglisi和Zhukova的工作；2) LZ-End压缩方案，提供了比Relative Lempel-Ziv更好的压缩率但访问时间较慢的权衡。将这些压缩后缀数组集成到r-index和Move-r中。

Result: 实验结果显示，在r-index和Move-r中集成压缩后缀数组后，locate查询性能得到显著提升，特别是在查询模式有很多匹配位置的情况下。两种不同的压缩方案在索引大小与查询性能之间提供了新的权衡选择。

Conclusion: 通过将压缩后缀数组集成到现有的压缩自索引中，可以有效解决locate查询的性能瓶颈。两种压缩方案各有优势，为实际应用提供了灵活的空间-时间权衡选择，特别适用于需要处理大量匹配位置的场景。

Abstract: In pattern matching on strings, a locate query asks for an enumeration of all
the occurrences of a given pattern in a given text. The r-index [Gagie et al.,
2018] is a recently presented compressed self index that stores the text and
auxiliary information in compressed space. With some modifications, locate
queries can be answered in optimal time [Nishimoto & Tabei, 2021], which has
recently been proven relevant in practice in the form of Move-r [Bertram et
al., 2024]. However, there remains the practical bottleneck of evaluating
function $\Phi$ for every occurrence to report. This motivates enhancing the
index by a compressed representation of the suffix array featuring efficient
random access, trading off space for faster answering of locate queries
[Puglisi & Zhukova, 2021]. In this work, we build upon this idea considering
two suitable compression schemes: Relative Lempel-Ziv [Kuruppu et al., 2010],
improving the work by Puglisi and Zhukova, and LZ-End [Kreft & Navarro, 2010],
introducing a different trade-off where compression is better than for Relative
Lempel-Ziv at the cost of slower access times. We enhance both the r-index and
Move-r by the compressed suffix arrays and evaluate locate query performance in
an experiment. We show that locate queries can be sped up considerably in both
the r-index and Move-r, especially if the queried pattern has many occurrences.
The choice between two different compression schemes offers new trade-offs
regarding index size versus query performance.

</details>


### [16] [Residual Prophet Inequalities](https://arxiv.org/abs/2507.17391)
*Jose Correa,Sebastian Perez-Salazar,Dana Pizarro,Bruno Ziliotto*

Main category: cs.DS

TL;DR: 本文提出了经典先知不等式的一个变体——剩余先知不等式(RPI)，在该问题中最大的k个值被预先移除，赌徒需要从剩余的n-k个值中在线选择，并给出了在不同信息模型下的竞争比算法。


<details>
  <summary>Details</summary>
Motivation: 经典先知不等式假设所有值都可供选择，但现实中可能存在高优先级代理已经分配了最优值的情况。因此需要研究在部分最优值被预先移除后的在线选择问题，这更符合实际应用场景。

Method: 研究了两种信息模型：FI模型(赌徒知道观察到的变量身份)和NI模型(不知道变量身份)。设计了数据驱动的随机化算法，仅需要访问单个样本中k+1个最大值。对于独立同分布的特殊情况(k=1)，构建了单阈值算法。

Result: 在FI模型中得到竞争比至少为1/(k+2)的算法，并证明这是紧的。在NI模型中提供竞争比为1/(2k+2)的算法。对于k=1的独立同分布情况，单阈值算法的竞争比至少为0.4901，并证明任何单阈值策略的竞争比不能超过0.5464。

Conclusion: 成功解决了剩余先知不等式问题，在不同信息条件下给出了最优或接近最优的算法，为在线选择问题在部分信息缺失环境下的应用提供了理论基础。算法的数据驱动特性使其在实际应用中更具可操作性。

Abstract: We introduce a variant of the classic prophet inequality, called
\emph{residual prophet inequality} (RPI). In the RPI problem, we consider a
finite sequence of $n$ nonnegative independent random values with known
distributions, and a known integer $0\leq k\leq n-1$. Before the gambler
observes the sequence, the top $k$ values are removed, whereas the remaining
$n-k$ values are streamed sequentially to the gambler. For example, one can
assume that the top $k$ values have already been allocated to a higher-priority
agent. Upon observing a value, the gambler must decide irrevocably whether to
accept or reject it, without the possibility of revisiting past values.
  We study two variants of RPI, according to whether the gambler learns online
of the identity of the variable that he sees (FI model) or not (NI model). Our
main result is a randomized algorithm in the FI model with \emph{competitive
ratio} of at least $1/(k+2)$, which we show is tight. Our algorithm is
data-driven and requires access only to the $k+1$ largest values of a single
sample from the $n$ input distributions. In the NI model, we provide a similar
algorithm that guarantees a competitive ratio of $1/(2k+2)$. We further analyze
independent and identically distributed instances when $k=1$. We build a
single-threshold algorithm with a competitive ratio of at least 0.4901, and
show that no single-threshold strategy can get a competitive ratio greater than
0.5464.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 本文针对视觉语言动作(VLA)模型提出了8个不确定性指标和5个质量指标，通过大规模实证研究验证这些指标与人类专家评估的相关性，挑战了仅依赖二元成功率的传统评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型评估主要依赖任务成功率，无法捕捉任务执行质量和模型决策置信度，需要更全面的评估指标来改进机器人系统的实时监控和自适应增强。

Method: 提出了8个不确定性指标和5个质量指标专门用于评估机器人操作任务中的VLA模型；通过908次成功任务执行的大规模实证研究，涉及3个最先进的VLA模型和4个代表性机器人操作任务；人类领域专家手动标注任务质量，分析提出指标与专家判断的相关性。

Result: 多个指标与人类评估呈现中等到强相关性，证明了其在评估任务质量和模型置信度方面的实用性；部分指标能够区分高、中、低质量执行与不成功任务，在缺乏测试预言的情况下具有价值。

Conclusion: 研究挑战了仅依赖二元成功率的当前评估实践的充分性，为改进VLA驱动机器人系统的实时监控和自适应增强铺平了道路，推动了更全面的模型评估方法发展。

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [18] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: 本文研究模糊测试中可达性估计器的可靠性问题，提出了合成程序生成框架和真实程序可靠性检验方法来评估当前估计器的准确性。


<details>
  <summary>Details</summary>
Motivation: 模糊测试通常由覆盖率引导，因此估计最大可达覆盖率是关键问题。然而现有的静态可达性分析不够准确，而基于生物统计学的统计估计方法缺乏可靠的标准基准和真实标签，限制了对其准确性的严格评估。

Method: 提出双轴评估方法：(1)构建评估框架，合成生成具有复杂控制流的大型程序，确保明确定义的可达性并提供评估的真实标签；(2)针对缺乏真实标签的现实程序，通过改变采样单元大小来检验可达性估计器的可靠性（理论上不应影响估计结果）。

Result: 两项研究共同回答了当前可达性估计器是否可靠的问题，并为评估未来可达性估计改进定义了协议。

Conclusion: 该研究建立了评估可达性估计器可靠性的完整框架，解决了缺乏真实标签基准的问题，为未来改进可达性估计方法提供了评估标准。

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [19] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: 这篇论文评估了六个大语言模型(LLMs)从自然语言描述生成GitHub Actions配置的能力，发现零样本提示最高只能达到69%的相似度和3%的完美匹配率，揭示了LLMs在CI配置生成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 持续集成(CI)服务需要开发者编写基于YAML的配置文件，这个过程繁琐且容易出错。尽管大语言模型在软件工程任务自动化方面应用日益增加，但它们生成CI配置的能力仍未得到充分探索。

Method: 研究评估了六个LLMs生成GitHub Actions配置的能力：三个通用基础模型(GPT-4o、Llama、Gemma)和三个代码预训练模型(GPT-4.1、Code Llama、CodeGemma)。构建了第一个此类标注数据集，从GitHub Actions文档中提取描述与对应的最佳实践YAML配置配对。使用零样本提示进行评估。

Result: 零样本提示最高达到69%的与真实标准的相似度，但只有3%的完美匹配。代码预训练模型在基于YAML的CI任务中表现略逊于通用模型。GPT-4o输出分析显示存在缺失或重命名步骤、误解描述、不必要添加等问题，影响结构和上下文正确性。

Conclusion: 研究揭示了LLMs在CI配置生成方面的局限性，生成质量与可执行CI配置所需精度之间存在差距。研究为改进LLM与配置语言的对齐提供了洞察，并为未来CI自动化和工具支持工作提供指导。

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [20] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: 本研究比较了传统统计方法与量子电路专用测试方法在量子软件单元测试中的效果，通过对179万个变异量子电路的实证研究，发现量子中心测试（特别是状态向量测试和逆测试）在精度和效率方面具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂性的不断增加，传统的软件验证和验证方法在量子软件单元测试中面临重大挑战，需要开发专门针对量子电路特性的测试方法来提高测试的可靠性和效率。

Method: 研究采用大规模实证分析方法，对1,796,880个变异量子电路进行测试，比较了传统统计测试与量子中心测试的性能，包括仅在经典计算机上运行的状态向量测试，以及可在量子硬件上执行的交换测试和新颖的逆测试。

Result: 实验结果表明量子中心测试（特别是状态向量测试和逆测试）在检测量子电路期望状态与实际状态之间的细微差异方面表现更优，相比统计测试能够减少假阳性和假阴性，同时在精度和效率方面具有明显优势。

Conclusion: 该研究为量子软件测试提供了更加鲁棒和可扩展的策略，支持容错量子计算机的未来应用，并促进量子软件工程中更可靠实践的发展，为量子软件验证领域做出了重要贡献。

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [21] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: 研究了提示编程（prompt programming）中开发者的需求和挑战，通过访谈、观察和调研构建了包含25个任务和51个问题的分类体系，发现现有工具对提示编程支持不足，并提出了工具改进的机会。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型等基础模型的普及，开发者越来越多地在软件中嵌入提示程序，但提示编程过程中开发者需要执行的任务和遇到的问题尚不明确，现有的研究和商业工具是否能充分满足提示编程者的需求也不清楚。

Method: 采用混合研究方法：(1)访谈16名提示编程者；(2)观察8名开发者进行提示修改过程；(3)调研50名开发者；(4)构建包含25个任务和51个问题的分类体系并评估其重要性；(5)将该分类体系与48个研究和商业工具进行对比分析。

Result: 构建了提示编程任务和问题的完整分类体系，发现提示编程支持不足：所有任务都需要手动完成，51个问题中有16个（包括大多数最重要的问题）仍然没有得到解答。现有工具无法充分支持提示编程者的核心需求。

Conclusion: 提示编程目前缺乏有效的工具支持，特别是在回答开发者最关心的核心问题方面。基于研究发现，论文提出了提示编程工具发展的重要机会和方向，为未来工具开发提供了指导。

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [22] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: 本文分析了一个分布式实时分析系统项目的失败案例，该项目采用大爆炸式集成方法导致系统仅运行6分钟而非预期的40分钟，并提出了基于模拟和早期集成的改进建议


<details>
  <summary>Details</summary>
Motivation: 分析一个为期一年的分布式实时分析系统项目失败的原因，该项目使用边缘计算和机器学习技术，但由于采用大爆炸式集成方法导致严重挫折，需要找出技术和组织层面的障碍并提出解决方案

Method: 通过根本原因分析方法，对项目失败进行深入剖析，识别技术和组织障碍（如沟通不良、缺乏早期集成测试、抗拒自上而下规划），并考虑心理因素（如偏向完全开发的组件而非模型），提出早期基于模拟的部署、强化沟通基础设施和采用自上而下思维的解决方案

Result: 项目集成努力仅实现了6分钟的系统功能，远低于预期的40分钟。研究识别出了导致失败的关键因素：沟通不良、缺乏早期集成测试、抗拒自上而下规划以及心理偏见。发现传统敏捷方法在此类复杂分布式项目中存在局限性

Conclusion: 传统敏捷方法在复杂分布式项目中存在局限性，建议采用早期基于模拟的部署、建立强健的沟通基础设施、采用自上而下思维来管理复杂性和降低风险。提出仿真驱动工程和结构化集成周期作为未来成功的关键推动因素

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [23] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: 本文提出Seed&Steer方法，通过解耦前缀生成和断言生成来改进基于大语言模型的单元测试自动生成，结合传统测试工具和LLM能力，显著提升编译成功率和测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的单元测试生成方法在编译成功率和测试覆盖率方面仍存在挑战。作者从新角度重新审视这一问题，发现前缀生成和断言生成面临不同的困难：初始化复杂性主要影响编译成功，而圈复杂度影响测试覆盖率。

Method: 提出Seed&Steer两步法：1) 利用传统单元测试工具(如EvoSuite)生成高编译成功率的方法调用作为种子，指导LLM构建有效的测试上下文；2) 引入分支提示帮助LLM探索多样化的执行路径(正常、边界、异常情况)并生成高覆盖率的断言。

Result: 在五个真实Java项目上的评估显示：编译通过率提升约7%，在两个LLM上成功编译了792和887个之前失败的案例；在不同复杂度的目标方法上达到约73%的分支和行覆盖率，覆盖率改进范围从1.09倍到1.26倍。

Conclusion: Seed&Steer通过结合传统单元测试技术和大语言模型能力，有效解决了LLM在单元测试生成中的编译和覆盖率问题，为自动化测试生成提供了新的有效途径。

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [24] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: 本文提出了一个数据虚拟化服务的设计和实现，用于支持多个并发的机器学习工作流，解决了ML团队在数据存储、处理和维护方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习团队需要同时管理多个ML工作流，每个工作流涉及大量实验、迭代和协作活动，通常需要数月甚至数年时间，产生了大量需要存储、处理和维护的中间数据，因此需要数据虚拟化技术来支撑ML基础设施。

Method: 设计和实现了一个数据虚拟化服务，重点关注其服务架构和服务操作，通过虚拟化技术来管理和处理ML工作流中的数据需求。

Result: 该基础设施目前支持六个ML应用，每个应用包含多个ML工作流，数据虚拟化服务能够支持未来应用和工作流数量的增长。

Conclusion: 数据虚拟化服务成功解决了多并发ML工作流的数据管理问题，为ML基础设施提供了可扩展的解决方案，能够适应未来业务增长的需求。

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [25] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: 该研究通过分析约200万个Scratch项目，探索计算思维（CT）能力与代码异味之间的关系，旨在为编程教育提供实证基础并改进自动化反馈系统。


<details>
  <summary>Details</summary>
Motivation: 代码异味在专业代码中已被广泛研究，但在初学者创建的块式编程项目中的重要性仍然未知。块式编程环境如Scratch提供了独特的数据丰富环境来研究设计问题如何与计算思维技能培养相交叉。

Method: 随机抽样约200万个公开的Scratch项目进行挖掘。使用开源检查工具从项目中提取9个计算思维分数和40个代码异味指标。经过严格的预处理后，应用描述性分析、稳健相关性测试、分层交叉验证和探索性机器学习模型；定性抽查用于情境化定量模式。

Result: 该研究将提供首个大规模、细粒度的映射，将特定的计算思维能力与具体的设计缺陷和反模式联系起来。结果将为基于证据的课程设计和自动化反馈系统提供信息，为未来教育干预提供效应大小基准，并为研究社区提供开放的匿名化数据集和可重现的分析管道。

Conclusion: 通过阐明编程习惯如何影响早期技能习得，该工作推进了计算教育理论和可持续软件维护与演进的实用工具开发。研究成果将有助于改进编程教育质量并为学习者提供更好的反馈机制。

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [26] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: 本文介绍了Roseau，一个用于检测Java库API演化和破坏性变更的静态分析工具，相比传统工具JApiCmp和Revapi具有更高的准确性(F1=0.99)和更好的大规模纵向分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Java API破坏性变更检测工具(如JApiCmp和Revapi)依赖二进制JAR文件，限制了其在大规模纵向研究和细粒度分析(如提交级别的BC检测)中的应用。需要一个能够从源代码构建API模型并支持大规模API演化分析的工具。

Method: 开发了Roseau静态分析工具，构建技术无关的API模型，配备丰富的语义分析。该工具可以从源代码或字节码构建API模型，针对大规模纵向分析进行优化，能够检测任意两个版本之间的破坏性变更。

Result: 在扩展的BC基准测试中，Roseau达到了0.99的F1分数，超过JApiCmp(0.86)和Revapi(0.91)。对60个流行Maven库的分析显示，Roseau在2秒内完成版本间BC检测，包括数十万行代码的库。在Google Guava的14年6839次提交分析中，将分析时间从数天缩短到数分钟。

Conclusion: Roseau为API演化研究提供了一个准确、高效的解决方案，克服了传统工具在大规模纵向分析中的局限性，为软件库演化的实证研究开辟了新的可能性。

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [27] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: 这篇论文对代码大语言模型中的训练数据检测（TDD）方法进行了全面的实证研究，提出了CodeSnitch基准数据集，并评估了七种最先进TDD方法在八个代码大语言模型上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型可能输出包含专有或敏感代码片段的内容，引发对训练数据合规使用的担忧，并对隐私和知识产权构成风险。现有的TDD方法主要针对自然语言，在代码数据上的有效性尚未充分探索，而代码具有结构化语法和与自然语言不同的相似性标准。

Method: 对七种最先进的TDD方法在八个代码大语言模型上进行了全面的实证研究。引入了CodeSnitch函数级基准数据集，包含三种编程语言的9000个代码样本。设计了基于Type-1到Type-4代码克隆检测分类法的目标突变策略，在三种不同设置下测试TDD方法的鲁棒性。

Result: 提供了当前TDD技术在代码领域的系统性评估结果，展示了不同方法在原始CodeSnitch数据集和突变测试下的性能表现。研究揭示了现有方法在代码数据上的局限性和挑战。

Conclusion: 本研究为代码领域的TDD技术提供了系统性评估，为未来开发更有效和鲁棒的检测方法提供了指导见解。研究强调了专门针对代码数据特点的TDD方法的重要性。

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [28] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip是一种利用大语言模型自动生成Bug复现测试的新技术，通过先生成通过测试再反转为失败测试的方式，在SWT-Bench基准测试中取得了43.6%的成功率，超越了所有已知技术。


<details>
  <summary>Details</summary>
Motivation: 在开源和工业环境中，大多数Bug在报告时缺乏可执行的复现测试，这使得Bug诊断和修复变得更加困难和耗时。现有的直接生成失败测试的方法效果不佳，需要一种更有效的自动生成Bug复现测试的技术。

Method: AssertFlip采用了一种新颖的两步法：首先使用大语言模型在有Bug的行为上生成通过的测试，然后将这些测试反转，使其在存在Bug时失败。这种方法基于假设：大语言模型更擅长编写通过的测试，而不是故意崩溃或失败的测试。

Result: AssertFlip在SWT-Bench基准测试的排行榜中超越了所有已知技术。具体而言，在SWT-Bench-Verified子集上实现了43.6%的fail-to-pass成功率。

Conclusion: AssertFlip通过创新的"先通过后反转"策略，有效解决了自动生成Bug复现测试的挑战，显著提高了Bug复现测试的生成成功率，为软件调试和修复过程提供了更有效的工具。

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [29] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: 提出了CodeReasoner框架，通过改进数据集构建和两阶段训练（指令微调+强化学习）来增强大语言模型的代码推理能力，在代码推理基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的代码推理方法主要依赖监督微调，但存在两个核心问题：训练数据质量低和监督微调在教授通用推理技能方面的局限性，导致性能提升有限且难以在不同场景中泛化。

Method: 提出CodeReasoner框架，包含：1）构建专注于Python程序核心执行逻辑的数据集；2）两阶段训练过程：先通过指令微调注入从强大教师模型中蒸馏的执行特定知识，然后通过GRPO强化学习增强推理和泛化能力。

Result: 在三个广泛使用的代码推理基准测试中，7B模型相比先前方法提升27.1%到40.2%的性能，在输入/输出和覆盖率预测等关键任务上与GPT-4o性能相当；14B模型在所有基准测试中都超越了GPT-4o。

Conclusion: CodeReasoner通过改进数据集质量和采用两阶段训练策略有效提升了大语言模型的代码推理能力，消融研究证实了每个训练阶段的有效性并强调了推理链的重要性。

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [30] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出了C3Gen方法，通过检索代码库中相关代码片段来增强提交信息生成，解决了仅依赖代码差异无法捕获完整上下文的问题，生成更全面和实用的提交信息。


<details>
  <summary>Details</summary>
Motivation: 现有的提交信息生成方法仅依赖代码差异作为输入，但原始代码差异无法捕获生成高质量和信息丰富的提交信息所需的完整上下文信息，这种局限性影响了生成质量。

Method: 提出了基于上下文代码检索的C3Gen方法，通过从代码库中检索与提交相关的代码片段，并将其融入模型输入中，在代码库范围内提供更丰富的上下文信息来增强提交信息生成。

Result: 实验结果表明，通过在输入中融入上下文代码，C3Gen使模型能够有效利用额外信息生成更全面、信息丰富且在实际开发场景中具有更大实用价值的提交信息。人工评估显示开发者对C3Gen生成的提交信息评价良好。

Conclusion: C3Gen通过引入代码库级别的上下文信息有效改善了提交信息生成质量，进一步分析还揭示了基于相似性度量可靠性的担忧，为提交信息生成领域提供了实证见解。

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [31] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: 本文介绍了CASCADE，一种结合Gemini大语言模型和JavaScript中间表示(JSIR)的混合JavaScript去混淆方法，能够有效识别关键前导函数并恢复原始程序行为，已在Google生产环境中部署并显著提升去混淆效率。


<details>
  <summary>Details</summary>
Motivation: JavaScript代码混淆技术广泛存在，严重阻碍了代码理解和分析，给软件测试、静态分析和恶意软件检测带来重大挑战。现有的静态和动态去混淆技术存在局限性，需要大量硬编码规则，缺乏可靠性和灵活性。

Method: 提出CASCADE混合方法，结合Gemini大语言模型的高级编码能力和编译器中间表示(IR)的确定性转换能力。使用Gemini识别关键前导函数（最常见混淆技术的基础组件），然后利用JavaScript IR(JSIR)进行后续代码转换，从而恢复语义元素如原始字符串和API名称。

Result: CASCADE成功克服了现有静态和动态去混淆技术的局限性，消除了数百到数千条硬编码规则，同时实现了可靠性和灵活性。该方法已在Google生产环境中部署，在JavaScript去混淆效率方面取得了显著改进，并减少了逆向工程工作量。

Conclusion: CASCADE通过将大语言模型的智能识别能力与编译器IR的确定性转换相结合，为JavaScript去混淆提供了一种有效的解决方案，不仅提高了去混淆的准确性和效率，还展现了在实际生产环境中的实用价值，为软件安全分析和恶意代码检测提供了强有力的工具支持。

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [32] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: 本研究探索了面向对象编程中代码问题指标与学习困难之间的关系，通过定性分析和文献综述建立了连接代码气味、SOLID原则违规与学习挑战的概念模型


<details>
  <summary>Details</summary>
Motivation: 面向对象编程对计算机科学本科生来说具有挑战性，特别是在理解封装、继承和多态等抽象概念方面。虽然现有文献描述了通过源代码分析识别设计和编码问题的方法，但很少有研究探索这些代码级问题与面向对象编程学习困难之间的关系

Method: 使用定性分析方法识别学习困难的主要类别，通过文献综述建立学习困难、代码气味和SOLID原则违规之间的联系，开发了将代码相关问题与面向对象编程学习挑战联系起来的概念地图

Result: 成功开发了一个概念模型，该模型将代码相关问题与面向对象编程中的特定学习挑战相关联。专家对该模型进行了评估，并将其应用于学生代码分析中

Conclusion: 研究建立了代码问题指标与面向对象编程学习困难之间的关联性，开发的概念模型在教育环境中具有相关性和适用性，为理解和解决学生在面向对象编程学习中遇到的困难提供了新的视角

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [33] [LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks](https://arxiv.org/abs/2507.17188)
*Lijie Zheng,Ji He,Shih Yu Chang,Yulong Shen,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出了一种分层优化框架来解决异构无人机网络中在推进能耗约束下的物理层安全保密速率最大化问题，结合了半定松弛算法和大语言模型引导的多智能体强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设无人机能力均匀或忽略能耗-安全权衡，本文考虑具有不同载荷和计算资源的异构无人机在窃听者存在时协作服务地面终端的现实场景，需要解决无人机运动与通信之间的复杂耦合问题。

Method: 提出分层优化框架：内层使用基于半定松弛(SDR)的S2DC算法结合惩罚函数和凸差规划解决固定无人机位置下的保密预编码问题；外层引入大语言模型引导的启发式多智能体强化学习方法(LLM-HeMARL)进行轨迹优化，有效融合LLM生成的专家启发式策略。

Result: 仿真结果显示该方法在保密速率和能效方面优于现有基线方法，在不同无人机群规模和随机种子下表现出一致的鲁棒性。

Conclusion: 所提出的分层优化框架能够有效解决异构无人机网络中的物理层安全问题，在保密速率最大化的同时考虑能耗约束，LLM-HeMARL方法使无人机能够学习到能耗感知的安全驱动轨迹。

Abstract: This work tackles the physical layer security (PLS) problem of maximizing the
secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy
constraints. Unlike prior studies that assume uniform UAV capabilities or
overlook energy-security trade-offs, we consider a realistic scenario where
UAVs with diverse payloads and computation resources collaborate to serve
ground terminals in the presence of eavesdroppers. To manage the complex
coupling between UAV motion and communication, we propose a hierarchical
optimization framework. The inner layer uses a semidefinite relaxation
(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex
(d.c.) programming to solve the secrecy precoding problem with fixed UAV
positions. The outer layer introduces a Large Language Model (LLM)-guided
heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for
trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics
policy generated by the LLM, enabling UAVs to learn energy-aware,
security-driven trajectories without the inference overhead of real-time LLM
calls. The simulation results show that our method outperforms existing
baselines in secrecy rate and energy efficiency, with consistent robustness
across varying UAV swarm sizes and random seeds.

</details>


### [34] [Closed-Form and Boundary Expressions for Task-Success Probability in Status-Driven Systems](https://arxiv.org/abs/2507.17195)
*Jianpeng Qi,Chao Liu,Rui Wang,Junyu Dong,Yanwei Yu*

Main category: cs.NI

TL;DR: 本文提出了一个统一的分析框架，用于建模计算优先网络系统中任务成功概率，通过拉普拉斯变换处理网络延迟，并提供了准确的理论上下界


<details>
  <summary>Details</summary>
Motivation: 在计算优先网络系统中，用户任务动态到达且计算资源有限且随机，需要及时高效地传播服务器状态。现有方法难以准确建模随机到达、有限服务器容量和双向链路延迟等因素对任务成功概率的影响

Method: 提出统一分析框架，将接入点转发规则抽象为单一概率，通过拉普拉斯变换建模所有网络和等待延迟。该方法产生端到端任务成功概率的闭式表达式，并提供捕获Erlang损失阻塞、信息陈旧性和随机上下行延迟的上下界

Result: 通过大范围参数的仿真验证，理论预测和界限始终包围观察到的成功率。框架仅需两个可互换输入（转发概率和延迟变换），易于适应其他转发策略和延迟分布。上界精度达到0.01，下界精度达到0.016

Conclusion: 该框架成功解决了计算优先网络中任务成功概率建模的复杂性问题，提供了准确且实用的理论分析工具，为不同转发策略和延迟分布提供了良好的适应性

Abstract: Timely and efficient dissemination of server status is critical in
compute-first networking systems, where user tasks arrive dynamically and
computing resources are limited and stochastic. In such systems, the access
point plays a key role in forwarding tasks to a server based on its latest
received server status. However, modeling the task-success probability
suffering the factors of stochastic arrivals, limited server capacity, and
bidirectional link delays. Therefore, we introduce a unified analytical
framework that abstracts the AP forwarding rule as a single probability and
models all network and waiting delays via their Laplace transforms. This
approach yields a closed form expression for the end to end task success
probability, together with upper and lower bounds that capture Erlang loss
blocking, information staleness, and random uplink/downlink delays. We validate
our results through simulations across a wide range of parameters, showing that
theoretical predictions and bounds consistently enclose observed success rates.
Our framework requires only two interchangeable inputs (the forwarding
probability and the delay transforms), making it readily adaptable to
alternative forwarding policies and delay distributions. Experiments
demonstrate that our bounds are able to achieve accuracy within 0.01 (upper
bound) and 0.016 (lower bound) of the empirical task success probability.

</details>


### [35] [Custody Transfer and Compressed Status Reporting for Bundle Protocol Version 7](https://arxiv.org/abs/2507.17403)
*Alice Le Bihan,Felix Flentge,Juan A. Fraire*

Main category: cs.NI

TL;DR: 本文提出了一种新的BPv7版本的托管传输机制，通过序列编号和压缩信号来提高空间网络中DTN通信的可靠性和效率


<details>
  <summary>Details</summary>
Motivation: 随着空间任务增加，需要用高效可靠的网络中心化通信方式替代点对点通信。DTN的Bundle协议被选为LunaNet互操作规范中的网络协议，但BPv7版本移除了原有的托管传输可靠性机制，需要重新定义可靠性扩展

Method: 设计了三个核心特性：(1)通过序列编号高效识别数据包集合的策略；(2)新的托管传输扩展块和压缩托管信号管理记录；(3)新的压缩报告扩展块，使用序列编号请求数据包处理步骤报告

Result: 在ESA BP实现中构建了原型，并在地球观测和月球通信仿真场景中进行了测试，验证了机制的有效性

Conclusion: 成功为BPv7开发了新的托管传输过程，预计将于2025年由CCSDS作为实验规范发布，为DTN可靠传输领域的未来工作奠定了基础

Abstract: As space missions increase, there is a growing need to replace point-to-point
communication with an efficient and reliable network-centric communication
approach. Disruption/Delay Tolerant Networking (DTN) with the Bundle Protocol
(BP) has been selected as an interoperable network protocol in the LunaNet
Interoperability Specification. It is also considered for future Earth
Observation and Mars communication scenarios. In a DTN, the "bundle" -- the
fundamental data unit of BP -- requires dedicated mechanisms to ensure
reliability due to the challenges posed by intermittent connectivity and long
delays. The previous version of BP, BPv6, contained a mechanism for reliable
transfer between "custodial nodes" called "custody transfer". However, this
approach has been removed from the core protocol specification for BPv7, which
requires a corresponding BP reliability extension to be defined separately.
This paper introduces a new custody transfer process for BPv7 (expected to be
published by CCSDS as an experimental specification in 2025). The core features
of this new custody transfer method for BPv7 are: (1) A strategy to efficiently
identify sets of bundles by sequence numbering (2) A new Custody Transfer
Extension Block and a corresponding administrative record, Compressed Custody
Signal, to efficiently report on the acceptance or rejection of custody using
sequence numbering (3) A new Compressed Reporting Extension Block requesting
reporting on bundle processing steps using a corresponding administrative
record with sequence numbering for efficiency. The paper will describe those
concepts and their design, specification, and implementation in detail. These
mechanisms have been prototyped in the ESA BP implementation and tested in
Earth Observation and Lunar communication simulation scenarios. The results
will be presented, as will an outlook on future work in the DTN reliable
transfer domain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: 本文提出了一种以数据为中心的绿色联邦学习方法，通过减少训练数据量和选择环境影响最小的节点来降低AI模型训练的碳排放。研究开发了一个交互式推荐系统，可以优化联邦学习配置，在时间序列分类任务中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 人工智能和机器学习的广泛应用带来了显著的环境影响，特别是在能耗和碳排放方面。联邦学习虽然能减少数据传输成本并增强隐私保护，但由于数据源异构性、计算节点能力差异等因素，仍面临环境影响挑战。因此需要创新解决方案来减轻AI的生态足迹。

Method: 采用以数据为中心的绿色联邦学习方法，包括：1）分析联邦数据集的特征；2）基于质量指标选择最优数据子集；3）选择环境影响最低的联邦节点；4）开发综合方法论检验数据质量、数据量等因素对FL训练性能和碳排放的影响；5）构建交互式推荐系统，通过数据减少优化FL配置。

Result: 在时间序列分类任务上的应用显示了该方法在减少联邦学习任务环境影响方面的良好效果。通过数据减少和节点优化选择，成功降低了训练过程中的碳排放。

Conclusion: 提出的以数据为中心的绿色联邦学习方法能够有效减少AI模型训练的环境影响。通过优化数据选择和节点配置，该方法为推进绿色AI发展提供了可行的解决方案，为可持续的机器学习实践奠定了基础。

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [37] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: 研究使用人工智能方法标准化经胫假肢接受腔设计，通过对118名患者数据的分析，发现随机森林模型在预测假肢师适配调整方面表现最佳，中位误差仅1.24毫米


<details>
  <summary>Details</summary>
Motivation: 经胫假肢接受腔的质量高度依赖假肢师的手工技能和专业经验，缺乏标准化流程。研究旨在通过人工智能方法帮助标准化经胫假肢接受腔设计，减少对个人技能的依赖

Method: 收集118名患者的残肢3D扫描数据和对应的假肢师设计的接受腔3D模型；进行数据预处理包括对齐、标准化和可选的形变模型及主成分分析压缩；开发三种算法（3D神经网络、前馈神经网络、随机森林）分别预测最终接受腔形状或假肢师的适配调整

Result: 所有算法中，预测适配调整的方法都优于直接预测最终接受腔形状。随机森林模型在适配预测方面表现最佳，中位表面距离误差为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米

Conclusion: 通过人工智能方法可以有效辅助经胫假肢接受腔设计的标准化，其中基于适配调整预测的随机森林模型达到了最高精度，为假肢设计的自动化和标准化提供了可行的技术路径

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [38] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 本研究开发了一个自动化工作流程来检测和修正自驱动实验室中的噪声特征，系统研究了数据集大小、噪声强度和特征分布对噪声检测和恢复的影响，旨在提高自动化材料发现中的数据质量和实验精度。


<details>
  <summary>Details</summary>
Motivation: 自驱动实验室虽然在加速材料发现方面前景广阔，但输入参数捕获错误可能损坏用于建模系统性能的特征，从而影响当前和未来的实验活动。需要开发系统性方法来检测和修正噪声特征，以确保数据质量。

Method: 开发了一个自动化工作流程，包括：1）系统性检测噪声特征；2）确定可以修正的样本-特征配对；3）恢复正确的特征值。使用kNN插补方法进行数据修复，并系统研究了数据集大小、噪声强度和特征值分布对检测和恢复性能的影响。

Result: 发现高强度噪声和大型训练数据集有利于噪声特征的检测和修正。低强度噪声降低了检测和恢复效果，但可通过更大的清洁训练数据集来补偿。连续和分散特征分布比离散或窄分布特征表现出更好的可恢复性。该框架在噪声、有限数据和不同特征分布条件下均有效。

Conclusion: 建立了一个与模型无关的理性数据恢复框架，为材料数据集中的kNN插补提供了切实的基准。该研究最终旨在提高自动化材料发现中的数据质量和实验精度，为自驱动实验室的可靠运行提供了重要支持。

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [39] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan Ničković,Radu Grosu*

Main category: cs.LG

TL;DR: 本文介绍了TD-Interpreter，一个基于多模态大语言模型的专门工具，帮助工程师理解和分析复杂的时序图，通过微调LLaVA模型和合成数据生成workflow来解决训练数据不足问题，在基准测试中表现优于未调优的GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 工程师在设计和验证过程中需要理解来自第三方的复杂时序图，缺乏专门的ML工具来协助分析这些时序图并回答相关的设计和验证查询问题。

Method: 开发TD-Interpreter作为视觉问答环境，通过微调轻量级7B多模态大语言模型LLaVA实现多模态学习。为解决训练数据不足问题，开发了合成数据生成workflow，将视觉信息与文本解释对齐。

Result: 实验评估显示TD-Interpreter在评估基准上大幅超越了未调优的GPT-4o，证明了该工具的有效性和实用性。

Conclusion: TD-Interpreter成功为工程师提供了理解复杂时序图的专业化ML工具，通过多模态学习和合成数据生成有效解决了领域特定应用中的数据稀缺问题，在实际应用中展现出优异性能。

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [40] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种将双曲几何Transformer集成到强化学习中的新框架，用于解决多步推理问题，在FrontierMath和非线性最优控制基准测试中显著提升了准确性和计算效率


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理复杂的多步推理任务时面临信用分配、高维状态表示和稳定性等挑战，需要新的解决方案来有效建模层次结构并提升推理能力

Method: 提出了一个将双曲Transformer集成到强化学习中的新框架，利用双曲嵌入来有效建模层次结构，结合了Transformer架构和双曲几何的优势

Result: 与使用普通Transformer的强化学习相比，双曲强化学习在FrontierMath基准上准确率提升32%-44%，在非线性最优控制基准上提升43%-45%；同时计算时间分别减少16%-32%和16%-17%

Conclusion: 双曲Transformer在强化学习中具有巨大潜力，特别适用于涉及层次结构的多步推理任务，为人工智能中的复杂推理问题提供了新的有效解决方案

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [41] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: 本文提出了DiffCarl算法，这是一个基于扩散模型的碳感知和风险感知强化学习算法，用于多微网系统的智能运行。该算法通过将扩散模型集成到深度强化学习框架中，实现了不确定性下的自适应能源调度，并明确考虑了碳排放和运营风险。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的日益集成和系统复杂性的增加，微网社区在不确定性条件下的实时能源调度和优化面临重大挑战。现有方法缺乏对碳排放和运营风险的明确考虑，且在动态不确定环境下的适应性不足。

Method: DiffCarl将扩散模型集成到深度强化学习(DRL)框架中，通过去噪生成过程学习动作分布，增强DRL策略的表达能力。该方法能够在动态不确定的微网环境中实现碳感知和风险感知的调度。

Result: 广泛的实验研究表明，DiffCarl优于经典算法和最先进的DRL解决方案，运营成本降低2.3-30.1%。与不考虑碳排放的变体相比，碳排放减少28.7%，并减少了性能变异性。

Conclusion: DiffCarl是一个实用且前瞻性的解决方案，其灵活的设计允许高效适应不同的系统配置和目标，支持在不断发展的能源系统中的实际部署。该算法在多微网系统的智能运行方面具有显著优势。

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [42] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe Fré,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: 本文详细阐述了Cartan神经网络的数学基础，通过非紧对称空间U/H构建几何一致的神经网络理论，实现协变性和几何可解释性


<details>
  <summary>Details</summary>
Motivation: 现有神经网络缺乏几何一致性和可解释性，需要基于群论结构和齐次流形理论构建更具几何意义的神经网络架构

Method: 利用非紧对称空间U/H作为齐次流形，构建Cartan神经网络的数学框架，详细分析层间映射与几何结构的相互作用机制

Result: 成功建立了Cartan神经网络的完整数学理论基础，展示了层的几何性质以及层间映射如何保持协变性和几何可解释性

Conclusion: 该工作为基于群论结构的完全几何可解释神经网络理论奠定了重要基础，与配套论文共同构成了这一新兴领域的开创性贡献

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [43] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: 提出了一种置信度优化概率编码(CPE)方法，通过置信度感知机制和L2正则化来改善概率编码中的距离测量问题，在BERT和RoBERTa模型上显著提升了自然语言分类任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 概率编码虽然能够实现从确定性到不确定性状态的平滑过渡并增强泛化能力，但高斯噪声的随机性会扭曲分类任务中基于点的距离测量，影响分类效果。现有的KL散度方差正则化方法依赖于不可靠的先验假设。

Method: 提出置信度优化概率编码(CPE)方法，包含两个关键策略：1)引入置信度感知机制来调整距离计算，确保概率编码分类任务中的一致性和可靠性；2)用更简单的L2正则化项替代传统的基于KL散度的方差正则化，直接约束方差而不依赖不可靠的先验假设。

Result: 在自然语言分类任务上进行的大量实验表明，该方法在BERT和RoBERTa模型上都显著提升了性能和泛化能力。该方法具有模型无关性，可以广泛应用。

Conclusion: CPE方法成功解决了概率编码中距离测量的可靠性问题，通过置信度感知机制和L2正则化改进了表征学习，为概率编码在分类任务中的应用提供了有效的解决方案。

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [44] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: 本文提出了SplitMeanFlow，一种基于区间分割一致性的新方法，用于训练平均速度场进行快速生成建模。相比MeanFlow的微分方法，该方法使用纯代数恒等式，实现更高效的训练和更好的硬件兼容性，在大规模语音合成中实现了20倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型如Flow Matching虽然性能优异，但受限于计算昂贵的迭代采样过程。虽然MeanFlow等方法通过学习平均速度场实现少步或单步生成，但其基于微分恒等式的方法存在局限性，需要JVP计算，导致实现复杂、训练不稳定且硬件兼容性差。

Method: 从平均速度的第一性原理出发，利用定积分的可加性，推导出新的纯代数恒等式"区间分割一致性"(Interval Splitting Consistency)。该恒等式在不同时间区间建立平均速度场的自参考关系，无需微分算子。基于此原理构建SplitMeanFlow训练框架，直接将代数一致性作为学习目标。

Result: 理论上证明了当区间分割趋于无穷小时，SplitMeanFlow的代数一致性能恢复MeanFlow的微分恒等式，建立了更一般的理论基础。实践上消除了JVP计算需求，实现更简单的实现、更稳定的训练和更广泛的硬件兼容性。单步和两步SplitMeanFlow模型已成功部署在大规模语音合成产品中。

Conclusion: SplitMeanFlow提供了学习平均速度场的更直接和通用的理论基础，通过纯代数方法避免了微分计算的复杂性。该方法不仅在理论上更加优雅，在实践中也显著提升了效率和可部署性，为快速生成建模提供了新的解决方案。

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [45] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: 本文提出P3SL框架，解决异构边缘设备环境下分割学习的个性化隐私保护问题，通过双层优化技术让客户端自主确定最优分割点，在保护隐私的同时平衡能耗和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习方法在异构环境中忽略了个性化隐私需求和本地模型定制化，无法很好地适应不同设备的计算资源、通信能力、环境条件和隐私要求的差异。

Method: 设计个性化顺序分割学习管道，允许每个客户端实现定制化隐私保护和个性化本地模型；采用双层优化技术，使客户端能够在不向服务器共享敏感信息的情况下自主确定最优个性化分割点。

Result: 在包含7个设备（4个Jetson Nano P3450、2个树莓派、1台笔记本电脑）的测试平台上进行实验验证，使用多样化的模型架构和数据集，在不同环境条件下测试了框架的有效性。

Conclusion: P3SL框架成功实现了异构资源受限边缘设备系统中的个性化隐私保护分割学习，能够在保持高模型精度的同时平衡能耗和隐私泄露风险。

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>


### [46] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 该论文提出了一种简单有效的量化感知训练方法，在增加不到0.1%训练成本的情况下，在多个现代基准测试中大幅超越了现有主流量化方法的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型量化可以减少推理延迟、模型大小和能耗，从而以更低成本提供更好的用户体验。现有挑战是在合理时间内交付精度损失最小的量化模型，特别是在不需要与专用推理加速器不兼容的机制的前提下。

Method: 提出了一种简单的端到端量化感知训练方法，该方法可以轻松推广到不同的模型架构，适用于激活值、缓存和权重，除了量化本身外不需要向模型引入任何额外操作。

Result: 在增加总模型训练预算不到0.1%的情况下，该方法在多个现代基准测试中大幅超越了主流发布的量化方法，对基础模型和指令模型变体都有效。

Conclusion: 该研究证明了一种简单有效的量化感知训练方法，能够在极小的额外训练成本下显著提升量化模型性能，且具有良好的通用性和实用性。

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [47] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: 本研究使用分层强化学习(HRL)开发下肢外骨骼的自适应控制策略，通过生物感觉运动处理模型启发的方法，结合通用价值函数(GVFs)预测信息来提高外骨骼在不同地形行走时的决策能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了增强运动障碍人群的移动能力和自主性，需要开发能够适应不同地形的下肢外骨骼自适应控制策略。现有控制系统在复杂地形转换时存在决策困难和分类错误的问题。

Method: 采用分层强化学习(HRL)方法，将外骨骼控制适应任务分解为两个层次：高层地形策略适应框架和低层预测信息提供框架。低层框架通过持续学习通用价值函数(GVFs)实现，利用多种可穿戴下肢传感器(肌电图、压力鞋垫、测角仪)生成未来信号值的时间抽象。研究了两种将实际和预测传感器信号整合到策略网络中的方法。

Result: GVFs预测信息的加入显著提高了网络整体准确性。在平地、不平地面、上下坡道和转弯等地形上都观察到地形特定的性能提升，这些地形在没有预测信息时经常被误分类。预测信息在不确定性决策中发挥了重要作用，特别是在容易被误分类的地形上。

Conclusion: 预测信息能够在不确定情况下辅助决策制定，特别是在易误分类的地形上。该工作为分层强化学习的细节理解和未来外骨骼开发提供了新见解，有助于实现在不同行走环境中的安全转换和穿越。

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [48] [Enhancing Quantum Federated Learning with Fisher Information-Based Optimization](https://arxiv.org/abs/2507.17580)
*Amandeep Singh Bhatia,Sabre Kais*

Main category: cs.LG

TL;DR: 本文提出了一种基于Fisher信息的量子联邦学习算法，通过识别和保留对量子模型性能有重要影响的关键参数来解决联邦学习中的通信成本、数据异构性和隐私威胁等挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临高通信成本、异构客户端数据、处理时间长和隐私威胁增加等挑战。量子联邦学习的兴起为医疗和金融等领域带来了新的机遇，但仍需解决上述问题。Fisher信息能够量化量子态在参数变化下携带的信息量，可以用来识别对模型性能有重要影响的关键参数。

Method: 提出了一种量子联邦学习(QFL)算法，利用在本地客户端模型上计算的Fisher信息，处理分布在异构分区上的数据。该方法识别对量子模型性能有重要影响的关键参数，并确保这些参数在聚合过程中得到保留。

Result: 在ADNI和MNIST数据集上的实验结果表明，与量子联邦平均方法相比，该方法在性能和鲁棒性方面都取得了更好的效果，证明了在量子联邦学习环境中融入Fisher信息的有效性。

Conclusion: 基于Fisher信息的量子联邦学习算法成功解决了传统联邦学习的关键挑战，通过保留关键参数提高了模型性能和鲁棒性，为量子联邦学习在实际应用中的部署提供了有效的解决方案。

Abstract: Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.

</details>


### [49] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,Blaž Stojanovič,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文介绍了PyTorch Geometric框架的2.0版本更新，该版本在可扩展性和实际应用能力方面进行了重大改进，支持异构图和时序图，并在关系深度学习和大语言模型等领域得到广泛应用。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络领域的快速发展，需要一个更强大、更可扩展的框架来支持大规模图学习问题和实际应用需求，特别是在处理异构图、时序图等复杂场景方面。

Method: 通过架构增强实现PyG 2.0的全面升级，包括：支持异构图和时序图、可扩展的特征/图存储系统、各种性能优化措施，以及在关系深度学习和大语言建模等重要领域的深入集成。

Result: PyG 2.0成功提升了框架的可扩展性和实际应用能力，使研究人员和从业者能够高效地解决大规模图学习问题，并在多个应用领域得到广泛支持和应用。

Conclusion: PyG 2.0的发布标志着图神经网络框架的重要进步，通过架构改进和功能扩展，为大规模图学习研究和实际应用提供了更强大的技术支撑，特别是在关系深度学习和大语言模型领域展现出重要价值。

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [50] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个新的框架，通过策略性地利用偏差来补充不变表示，从而改进模型在分布外(OOD)域上的适应性能，而不是简单地消除所有偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的分布外域适应方法主要依赖不变表示学习来消除偏差特征的影响，但作者质疑是否应该总是消除偏差，以及何时应该保留偏差并如何有效利用偏差来提升模型性能。

Method: 提出了一个包含两个关键组件的新框架：(1)使用不变性作为指导从偏差中提取预测性成分；(2)利用识别出的偏差来估计环境条件，然后使用它来探索适当的偏差感知预测器以缓解环境差距。该方法以直接和间接的方式利用偏差来补充不变表示。

Result: 在合成数据集和标准域泛化基准上的实验结果表明，该方法始终优于现有方法，证明了其鲁棒性和适应性。理论分析也支持了在特定条件下可以识别和有效利用偏差特征的观点。

Conclusion: 偏差不应该总是被消除，而应该在适当的条件下被策略性地利用。通过理论分析确定利用偏差的条件，并设计相应的框架来同时利用不变表示和偏差信息，可以显著提升模型在分布外域上的泛化性能。

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [51] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,Bálint Mucsányi,Lenard Rommel,Thomas Christie,Lars Kasüschke,Marvin Pförtner,Philipp Hennig*

Main category: cs.LG

TL;DR: 本文介绍了laplax，一个基于JAX的开源Python包，用于在深度神经网络中执行拉普拉斯近似以量化权重空间的不确定性


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习缺乏高效可扩展的不确定性量化方法，需要一个灵活易用的工具来应用贝叶斯方法如预测不确定性和模型选择

Method: 开发了laplax包，采用模块化和纯函数式架构，基于JAX实现拉普拉斯近似，具有最小的外部依赖和研究友好的设计

Result: 提供了一个可扩展高效的框架，能够量化深度神经网络的权重空间不确定性，支持贝叶斯工具的应用

Conclusion: laplax为贝叶斯神经网络研究、深度学习不确定性量化和改进拉普拉斯近似技术提供了便利的实验平台

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [52] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: 本文提出了CGF-LLM框架，首次将GPT-2与模糊时间序列和因果图结合用于多变量时间序列预测，通过模糊化和因果分析将数值时间序列转换为可解释的文本表示，为LLM在时间序列预测领域开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在时间序列预测中的应用日益受到关注，现有方法缺乏将数值时间序列转换为可解释形式的有效架构。研究者希望结合模糊时间序列和因果图的优势，使LLM能够更好地理解时间序列的复杂动态和结构关系。

Method: 提出CGF-LLM框架，结合GPT-2、模糊时间序列(FTS)和因果图。通过并行应用模糊化和因果分析，将数值时间序列转换为可解释的文本表示形式，使预训练的GPT-2模型能够同时获得语义理解和结构洞察作为输入。

Result: 在四个不同的多变量时间序列数据集上验证了所提出的基于LLM的时间序列预测模型的有效性。实验结果确认了CGF-LLM框架在时间序列预测任务中的良好性能。

Conclusion: CGF-LLM是文献中首个结合GPT-2、模糊时间序列和因果图的时间序列预测架构。该方法成功地将复杂的数值时间序列转换为更具可解释性的文本表示，为基于模糊时间序列的LLM时间序列预测领域开辟了有前景的未来发展方向。

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [53] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: 本文提出了一种基于双层局部算子学习(BiLO)的贝叶斯推理框架，用于解决偏微分方程约束的不确定性量化和反问题，通过梯度MCMC方法和低秩适应实现高效采样，避免了在高维神经网络权重空间中采样的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于贝叶斯神经网络的方法在处理PDE约束优化问题时面临高维权重空间采样困难和需要指定神经网络解的先验分布等挑战，需要一种更有效的不确定性量化方法来处理由偏微分方程控制的科学工程应用。

Method: 将双层局部算子学习(BiLO)扩展到贝叶斯推理框架：下层通过最小化局部算子损失训练网络逼近局部解算子；上层从后验分布中采样PDE参数；使用梯度马尔可夫链蒙特卡罗(MCMC)方法和低秩适应(LoRA)实现高效采样；通过强化PDE约束让不确定性从数据自然传播。

Result: 通过多种PDE模型的数值实验验证，该方法在保持高计算效率的同时，能够准确推理和量化不确定性；分析了MCMC采样器中梯度的动态误差和下层问题不精确最小化导致的后验分布静态误差；建立了下层问题求解容差与不确定性量化精度之间的直接联系。

Conclusion: 提出的方法成功绕过了在高维神经网络权重空间采样的挑战，无需为神经网络解指定先验分布，通过强化PDE约束提高了参数推理和不确定性量化的准确性，在各种PDE模型上展现出准确的推理能力和高效的计算性能。

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [54] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种基于树模型的可解释离线强化学习方法，通过分析患者状态下最频繁选择的治疗行为来制定治疗策略，在类风湿关节炎和脓毒症护理中展现出优于现有实践的效果。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习在安全关键领域面临两大挑战：1）黑盒策略缺乏可解释性；2）离策略评估对行为策略偏差敏感，特别是使用重要性采样方法时。这些问题限制了离线强化学习在医疗等关键领域的实际应用。

Method: 提出使用树模型估计行为策略，从每个患者状态下最频繁选择的行为中推导治疗策略。该方法通过树结构天然保证可解释性，通过控制考虑的行为数量来调节与行为策略的重叠程度，实现可靠的离策略评估，并标准化频繁的治疗模式。

Result: 在类风湿关节炎和脓毒症护理的真实世界案例中，该框架推导的策略能够超越当前的临床实践，提供了相比传统离线强化学习方法更具可解释性的替代方案。

Conclusion: 这种实用的策略开发方法通过标准化频繁的治疗模式，捕获了数据中蕴含的集体临床判断，为离线强化学习在医疗领域的应用提供了一个既可解释又有效的解决方案。

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


### [55] [Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation](https://arxiv.org/abs/2507.17066)
*Jessup Byun,Xiaofeng Lin,Joshua Ward,Guang Cheng*

Main category: cs.LG

TL;DR: 该研究首次对基础模型在表格数据合成中的隐私风险进行基准测试，发现大型预训练模型虽然在小数据集上表现良好，但存在严重的成员推理攻击风险，并提出了三种零成本的提示优化策略来改善隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在小数据集上容易过拟合和泄露敏感信息，而基础模型通过上下文学习可以避免重训练，但会逐字重复种子行，在表格合成中引入新的隐私风险。由于表格数据中单行可能识别个人身份，这种风险的严重程度尚不明确，需要系统性评估。

Method: 构建首个基准测试，使用三个基础模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）与四个基线模型在35个来自健康、金融和政策领域的真实表格上进行对比。评估指标包括统计保真度、下游效用和成员推理泄露。同时进行因子研究，测试小批量大小、低温度和使用摘要统计等提示优化策略。

Result: 基础模型始终具有最高的隐私风险，LLaMA 3.3 70B在1%假阳性率下的真阳性率比最安全基线高出54个百分点。CTGAN和GPT-4o-mini提供更好的隐私-效用权衡。三种零成本提示优化策略可将最坏情况AUC降低14个点，稀有类泄露减少39个点，同时保持90%以上的保真度。

Conclusion: 基础模型在小数据表格合成中虽然有效，但存在显著隐私风险。通过合适的提示工程策略可以改善隐私-效用权衡，为在小数据环境中更安全地使用基础模型提供实用指导。该基准测试为评估和改进表格数据合成的隐私保护提供了重要工具。

Abstract: Synthetic tabular data is essential for machine learning workflows,
especially for expanding small or imbalanced datasets and enabling
privacy-preserving data sharing. However, state-of-the-art generative models
(GANs, VAEs, diffusion models) rely on large datasets with thousands of
examples. In low-data settings, often the primary motivation for synthetic
data, these models can overfit, leak sensitive records, and require frequent
retraining. Recent work uses large pre-trained transformers to generate rows
via in-context learning (ICL), which needs only a few seed examples and no
parameter updates, avoiding retraining. But ICL repeats seed rows verbatim,
introducing a new privacy risk that has only been studied in text. The severity
of this risk in tabular synthesis-where a single row may identify a
person-remains unclear. We address this gap with the first benchmark of three
foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four
baselines on 35 real-world tables from health, finance, and policy. We evaluate
statistical fidelity, downstream utility, and membership inference leakage.
Results show foundation models consistently have the highest privacy risk.
LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at
1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly
vulnerable. We plot the privacy-utility frontier and show that CTGAN and
GPT-4o-mini offer better tradeoffs. A factorial study finds that three
zero-cost prompt tweaks-small batch size, low temperature, and using summary
statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up
to 39 points while maintaining over 90% fidelity. Our benchmark offers a
practical guide for safer low-data synthesis with foundation models.

</details>


### [56] [Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach](https://arxiv.org/abs/2507.17070)
*Adithya Mohan,Dominik Rößle,Daniel Cremers,Torsten Schön*

Main category: cs.LG

TL;DR: 本文提出了一种集成防御架构来提高自动驾驶中深度强化学习模型对抗攻击的鲁棒性，在FGSM攻击下显著提升了模型性能，平均奖励提升213%，碰撞率降低82%。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习模型在面对对抗性攻击时缺乏鲁棒性，虽然存在对抗训练和蒸馏等防御机制，但在自动驾驶场景中缺乏多重防御集成的研究，存在重要的研究空白。

Method: 提出了一种新颖的集成防御架构，通过整合多种防御策略来缓解自动驾驶中的对抗性攻击，该方法专门针对自动驾驶场景中的深度强化学习模型设计。

Result: 在FGSM攻击下，集成方法在高速公路和并道场景中将平均奖励从5.87提升到18.38（提升超过213%），将平均碰撞率从0.50降低到0.09（降低82%），表现优于所有单独的防御策略。

Conclusion: 所提出的集成防御架构显著增强了深度强化学习模型的鲁棒性，在自动驾驶场景中有效缓解了对抗性攻击的影响，为自动驾驶系统的安全性提供了重要保障。

Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.

</details>


### [57] [Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees](https://arxiv.org/abs/2507.17453)
*Guanqin Zhang,Kota Fukuda,Zhenya Zhang,H. M. N. Dilum Bandara,Shiping Chen,Jianjun Zhao,Yulei Sui*

Main category: cs.LG

TL;DR: 本文提出了Oliva框架，通过优先探索更可能包含反例的子问题来提高神经网络形式化验证的效率，在MNIST和CIFAR10数据集上分别实现了最高25倍和80倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的分支定界（BaB）神经网络验证方法采用"先来先服务"的朴素策略探索子问题空间，导致验证效率低下，难以快速得出验证结论。

Method: 提出Oliva验证框架，基于子问题包含反例的可能性对其进行排序，优先探索更可能找到反例的子问题。包含两个变体：贪心策略Oliva^GR和受模拟退火启发的平衡策略Oliva^SA。

Result: 在涵盖5个模型、690个验证问题的MNIST和CIFAR10数据集上进行实验评估，相比最先进方法在MNIST上实现最高25倍加速，在CIFAR10上实现最高80倍加速。

Conclusion: 通过智能地排序和优先处理子问题，Oliva能够显著提高神经网络形式化验证的效率，即使未找到反例也不会导致性能下降，为神经网络验证提供了更高效的解决方案。

Abstract: The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.

</details>


### [58] [Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation](https://arxiv.org/abs/2507.17071)
*Juntao Lin,Xianghao Zhan*

Main category: cs.LG

TL;DR: 本研究提出了基于知识蒸馏(KD)的新方法来解决电子鼻系统中的传感器漂移问题，在UCI气体传感器阵列漂移数据集上相比现有最先进的DRCA方法，准确率提升18%，F1分数提升15%。


<details>
  <summary>Details</summary>
Motivation: 环境变化和传感器老化导致的传感器漂移严重影响电子鼻系统在实际部署中的气体分类性能。以往研究缺乏稳健的统计实验验证，且可能存在过度补偿传感器漂移而丢失类别相关方差的问题。

Method: 基于UCI气体传感器阵列漂移数据集设计了两个领域适应任务，系统性测试了三种方法：提出的新颖知识蒸馏(KD)方法、基准方法领域正则化成分分析(DRCA)和混合方法KD-DRCA，在30个随机测试集分区上进行验证。

Result: 知识蒸馏方法在漂移补偿方面表现出卓越性能，相比DRCA和KD-DRCA方法，准确率提升达18%，F1分数提升达15%，在所有测试中均表现出一致的优越性。

Conclusion: 首次将知识蒸馏应用于电子鼻漂移缓解，显著超越了之前最先进的DRCA方法，提高了传感器漂移补偿在真实环境中的可靠性，为电子鼻系统的实际应用提供了更有效的解决方案。

Abstract: Due to environmental changes and sensor aging, sensor drift challenges the
performance of electronic nose systems in gas classification during real-world
deployment. Previous studies using the UCI Gas Sensor Array Drift Dataset
reported promising drift compensation results but lacked robust statistical
experimental validation and may overcompensate for sensor drift, losing
class-related variance.To address these limitations and improve sensor drift
compensation with statistical rigor, we first designed two domain adaptation
tasks based on the same electronic nose dataset: using the first batch to
predict the remaining batches, simulating a controlled laboratory setting; and
predicting the next batch using all prior batches, simulating continuous
training data updates for online training. We then systematically tested three
methods: our proposed novel Knowledge Distillation (KD) method, the benchmark
method Domain Regularized Component Analysis (DRCA), and a hybrid method
KD-DRCA, across 30 random test set partitions on the UCI dataset. We showed
that KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%
improvement in accuracy and 15% in F1-score, demonstrating KD's superior
effectiveness in drift compensation. This is the first application of KD for
electronic nose drift mitigation, significantly outperforming the previous
state-of-the-art DRCA method and enhancing the reliability of sensor drift
compensation in real-world environments.

</details>


### [59] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: 提出了ZORMS-LfD方法，这是一种零阶随机矩阵搜索算法，用于从专家演示中学习受约束最优控制问题的成本、约束和动力学，无需平滑性假设，在连续和离散时间问题上都表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶方法需要计算成本、约束、动力学和学习损失相对于状态、控制和参数的梯度，且要求学习损失景观具有平滑性。大多数现有方法主要针对离散时间问题，对连续时间约束问题关注不足。需要一种不依赖梯度信息且能处理连续时间约束问题的学习方法。

Method: 提出零阶随机矩阵搜索学习演示方法(ZORMS-LfD)，这是一种无梯度的优化算法，能够从专家演示中学习受约束最优控制问题的参数，适用于连续时间和离散时间系统，不需要学习损失景观的平滑性假设。

Result: 在各种基准问题上，ZORMS-LfD在学习损失和计算时间方面都达到或超越了现有最先进方法的性能。在无约束连续时间基准问题上，实现了与一阶方法相似的损失性能，但计算时间减少了80%以上。在受约束连续时间基准问题上，优于常用的无梯度Nelder-Mead优化方法。

Conclusion: ZORMS-LfD为从演示中学习受约束最优控制问题提供了一个有效的解决方案，特别适用于梯度信息难以获得或计算成本高昂的场景，在连续时间约束问题上具有明显优势，为学习演示领域提供了新的技术选择。

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


### [60] [Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models](https://arxiv.org/abs/2507.17107)
*Andrii Balashov*

Main category: cs.LG

TL;DR: 研究发现强化学习微调大语言模型时，只会修改模型中5-30%的参数子网络，而非传统认为的需要更新大部分参数，这种稀疏性现象在多种RL算法和模型中都存在，且具有可转移性。


<details>
  <summary>Details</summary>
Motivation: 挑战强化学习微调需要更新大语言模型大部分参数的传统假设，探索RL微调过程中参数更新的真实模式，以提高微调效率并深入理解RL如何适应模型。

Method: 通过分析多种RL算法（PPO、DPO、SimPO、PRIME）在不同模型家族（OpenAI、Meta及开源LLM）上的参数更新模式，识别和验证RL诱导的参数更新稀疏性现象，并测试仅微调稀疏子网络的性能。

Result: 发现RL微调只修改5-30%的模型参数，这些更新的子网络在不同种子、数据集和算法间显示出显著重叠性，仅微调稀疏子网络即可恢复完整模型性能，产生与全量微调几乎相同的参数。

Conclusion: RL适应模型的方式不是通过改变所有权重，而是专注于训练一个小而一致更新的子网络。这种稀疏性源于RL在模型原始分布附近操作，只需要有针对性的改变。这一发现为更高效的RL方法提供了可能，并从彩票假说的角度重新审视了稀疏性。

Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (LLMs) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update sparsity. It arises naturally, without any sparsity
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show
substantial overlap across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this sparse subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this sparsity emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the sparsity pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes sparsity through the lens of the lottery ticket hypothesis.

</details>


### [61] [Probabilistic Graphical Models: A Concise Tutorial](https://arxiv.org/abs/2507.17116)
*Jacqueline Maasch,Willie Neiswanger,Stefano Ermon,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 这是一篇关于概率图模型的教程论文，介绍了该建模框架的形式化方法、算法和应用，涵盖了图表示、参数学习和推理算法三个核心主题。


<details>
  <summary>Details</summary>
Motivation: 概率图模型作为机器学习的重要分支，能够在不确定性环境下进行预测和决策支持，但需要一个系统性的教程来介绍其理论基础、方法和应用。

Method: 通过回顾基础概率论和图论知识，然后围绕三个核心主题展开：(1)用直观的图形语言表示多元分布；(2)从数据中学习模型参数和图结构的算法；(3)精确和近似推理算法。

Result: 提供了概率图模型框架的简明介绍，涵盖了该领域的主要形式化方法、算法技术和实际应用，为读者建立了完整的知识体系。

Conclusion: 概率图模型通过结合概率论和图论，为概率推理提供了强大的生成模型，本教程为该建模框架提供了全面而简洁的入门指导。

Abstract: Probabilistic graphical modeling is a branch of machine learning that uses
probability distributions to describe the world, make predictions, and support
decision-making under uncertainty. Underlying this modeling framework is an
elegant body of theory that bridges two mathematical traditions: probability
and graph theory. This framework provides compact yet expressive
representations of joint probability distributions, yielding powerful
generative models for probabilistic reasoning.
  This tutorial provides a concise introduction to the formalisms, methods, and
applications of this modeling framework. After a review of basic probability
and graph theory, we explore three dominant themes: (1) the representation of
multivariate distributions in the intuitive visual language of graphs, (2)
algorithms for learning model parameters and graphical structures from data,
and (3) algorithms for inference, both exact and approximate.

</details>


### [62] [Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems](https://arxiv.org/abs/2507.17123)
*Jacob M. Delgado-López,Ricardo A. Morell-Rodriguez,Sebastián O. Espinosa-Del Rosario,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究开发了一个基于AI的猴痘诊断工具，在NVIDIA Jetson Orin Nano上部署MobileNetV2模型，通过TensorRT优化实现了93.07%的F1分数，同时降低了功耗和模型大小，适合资源受限环境下的传染病快速诊断。


<details>
  <summary>Details</summary>
Motivation: 传染病(如猴痘)的快速诊断对于有效控制和治疗至关重要，特别是在资源受限的环境中。现有诊断方法在欠发达地区面临设备、能耗和连接性挑战，需要开发适合低资源医疗环境的高效、可扩展且节能的诊断解决方案。

Method: 使用预训练的MobileNetV2架构进行二元分类，在开源猴痘皮肤病变数据集上训练模型。采用TensorRT框架对模型进行优化，包括FP32推理加速以及FP16和INT8格式的训练后量化。部署系统包含Wi-Fi接入点热点和基于Web的界面，支持通过移动设备直接上传和分析图像。

Result: 模型达到93.07%的F1分数，在精确度和召回率之间实现良好平衡。TensorRT优化将模型大小减少约一半，推理速度提升约一倍，功耗降低约一半，同时保持原始准确性。功耗分析确认优化模型在推理过程中显著降低能耗。

Conclusion: 该诊断工具作为高效、可扩展且节能的解决方案，成功解决了服务不足地区的诊断挑战，为在低资源医疗环境中更广泛采用铺平了道路。系统的简单访问性和无缝连接性使其在实际应用中具有实用价值。

Abstract: The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for
effective containment and treatment, particularly in resource-constrained
environments. This study presents an AI-driven diagnostic tool developed for
deployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained
MobileNetV2 architecture for binary classification. The model was trained on
the open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,
which reflects a well-balanced performance in precision and recall. To optimize
the model, the TensorRT framework was used to accelerate inference for FP32 and
to perform post-training quantization for FP16 and INT8 formats. TensorRT's
mixed-precision capabilities enabled these optimizations, which reduced the
model size, increased inference speed, and lowered power consumption by
approximately a factor of two, all while maintaining the original accuracy.
Power consumption analysis confirmed that the optimized models used
significantly less energy during inference, reinforcing their suitability for
deployment in resource-constrained environments. The system was deployed with a
Wi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to
upload and analyze images directly through connected devices such as mobile
phones. This setup ensures simple access and seamless connectivity, making the
tool practical for real-world applications. These advancements position the
diagnostic tool as an efficient, scalable, and energy-conscious solution to
address diagnosis challenges in underserved regions, paving the way for broader
adoption in low-resource healthcare settings.

</details>


### [63] [Model Compression Engine for Wearable Devices Skin Cancer Diagnosis](https://arxiv.org/abs/2507.17125)
*Jacob M. Delgado-López,Andrea P. Seda-Hernandez,Juan D. Guadalupe-Rosado,Luis E. Fernandez Ramirez,Miguel Giboyeaux-Camilo,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究开发了一个基于MobileNetV2和TensorRT优化的皮肤癌AI诊断工具，可在NVIDIA Jetson Orin Nano等嵌入式设备上高效运行，实现了87.18%的F1分数，同时显著降低了模型大小和能耗，为资源受限地区提供了可行的皮肤癌早期检测解决方案。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是最常见且可预防的癌症类型之一，但其早期检测仍然是一个挑战，特别是在医疗资源有限的地区，专业医疗服务稀缺。因此需要开发一种适用于嵌入式系统的AI驱动诊断工具来解决这一问题。

Method: 采用迁移学习方法，使用MobileNetV2架构进行皮肤病变的二分类（"皮肤癌"和"其他"）。利用TensorRT框架对模型进行压缩和优化，以便在NVIDIA Jetson Orin Nano上部署，在性能和能效之间取得平衡。对模型大小、推理速度、吞吐量和功耗等多个基准进行了全面评估。

Result: 优化后的模型保持了良好性能，F1分数达到87.18%，精确率93.18%，召回率81.91%。压缩后模型大小减少了0.41倍，推理速度和吞吐量得到提升，INT8精度下能耗降低了0.93倍。

Conclusion: 研究验证了在资源受限的边缘设备上部署高性能、高能效诊断工具的可行性。该方法不仅适用于皮肤癌检测，还可广泛应用于其他医疗诊断和需要可访问、高效AI解决方案的领域。优化的AI系统有潜力革命性地改变医疗诊断，弥合先进技术与服务不足地区之间的差距。

Abstract: Skin cancer is one of the most prevalent and preventable types of cancer, yet
its early detection remains a challenge, particularly in resource-limited
settings where access to specialized healthcare is scarce. This study proposes
an AI-driven diagnostic tool optimized for embedded systems to address this
gap. Using transfer learning with the MobileNetV2 architecture, the model was
adapted for binary classification of skin lesions into "Skin Cancer" and
"Other." The TensorRT framework was employed to compress and optimize the model
for deployment on the NVIDIA Jetson Orin Nano, balancing performance with
energy efficiency. Comprehensive evaluations were conducted across multiple
benchmarks, including model size, inference speed, throughput, and power
consumption. The optimized models maintained their performance, achieving an
F1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.
Post-compression results showed reductions in model size of up to 0.41, along
with improvements in inference speed and throughput, and a decrease in energy
consumption of up to 0.93 in INT8 precision. These findings validate the
feasibility of deploying high-performing, energy-efficient diagnostic tools on
resource-constrained edge devices. Beyond skin cancer detection, the
methodologies applied in this research have broader applications in other
medical diagnostics and domains requiring accessible, efficient AI solutions.
This study underscores the potential of optimized AI systems to revolutionize
healthcare diagnostics, thereby bridging the divide between advanced technology
and underserved regions.

</details>


### [64] [Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance](https://arxiv.org/abs/2507.17131)
*Yufei He,Ruoyu Li,Alex Chen,Yue Liu,Yulin Chen,Yuan Sui,Cheng Chen,Yi Zhu,Luca Luo,Frank Yang,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出了ARIA框架，一个能够在测试时持续学习更新领域知识的大语言模型智能体，通过结构化自我对话评估不确定性，主动识别知识缺口并向人类专家请求指导，系统性更新内部知识库


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型智能体在规则和领域知识频繁变化的环境中表现不佳，如监管合规和用户风险筛查。离线微调和标准提示等方法无法在实际运行中有效适应新知识

Method: 提出ARIA（自适应反思交互智能体）框架，通过结构化自我对话评估不确定性，主动识别知识缺口并请求人类专家的针对性解释或纠正，系统性更新带时间戳的内部知识库，通过比较和澄清查询检测和解决冲突或过时的知识

Result: 在TikTok Pay的客户尽职调查姓名筛查任务和公开动态知识任务上的评估显示，与使用标准离线微调和现有自改进智能体的基线相比，ARIA在适应性和准确性方面有显著改进。已在服务超过1.5亿月活用户的TikTok Pay中部署

Conclusion: ARIA框架成功解决了大语言模型智能体在快速变化环境中的适应性问题，通过测试时学习机制实现了持续的知识更新，在实际业务场景中展现了良好的实用性和有效性

Abstract: Large language model (LLM) agents often struggle in environments where rules
and required domain knowledge frequently change, such as regulatory compliance
and user risk screening. Current approaches, like offline fine-tuning and
standard prompting, are insufficient because they cannot effectively adapt to
new knowledge during actual operation. To address this limitation, we propose
the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework
designed specifically to continuously learn updated domain knowledge at test
time. ARIA assesses its own uncertainty through structured self-dialogue,
proactively identifying knowledge gaps and requesting targeted explanations or
corrections from human experts. It then systematically updates an internal,
timestamped knowledge repository with provided human guidance, detecting and
resolving conflicting or outdated knowledge through comparisons and
clarification queries. We evaluate ARIA on the realistic customer due diligence
name screening task on TikTok Pay, alongside publicly available dynamic
knowledge tasks. Results demonstrate significant improvements in adaptability
and accuracy compared to baselines using standard offline fine-tuning and
existing self-improving agents. ARIA is deployed within TikTok Pay serving over
150 million monthly active users, confirming its practicality and effectiveness
for operational use in rapidly evolving environments.

</details>


### [65] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: 本文提出了SADA（稳定性引导的自适应扩散加速）方法，通过统一的稳定性准则来指导步骤级和token级的稀疏性决策，实现了扩散模型的高效加速，在保持生成质量的同时获得了1.8倍以上的速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但由于迭代采样过程和二次注意力成本导致计算开销高。现有的无训练加速策略虽然能减少采样时间，但与原始基线相比保真度较低。作者认为这种保真度差距源于：(a)不同提示对应不同的去噪轨迹，(b)现有方法没有考虑底层ODE公式及其数值解。

Method: 提出SADA方法，通过单一稳定性准则统一步骤级和token级稀疏性决策来加速基于ODE的生成模型。针对问题(a)，SADA基于采样轨迹自适应分配稀疏性；针对问题(b)，SADA引入利用数值ODE求解器精确梯度信息的原理性近似方案。

Result: 在SD-2、SDXL和Flux上使用EDM和DPM++求解器的综合评估显示，与未修改的基线相比，SADA实现了一致的≥1.8倍速度提升，同时保持最小的保真度下降（LPIPS ≤ 0.10，FID ≤ 4.5），显著优于先前方法。此外，SADA无缝适配其他管道和模态：无需修改即可加速ControlNet，并以约0.01的频谱图LPIPS将MusicLDM加速1.8倍。

Conclusion: SADA方法成功解决了扩散模型加速中的保真度-效率权衡问题，通过稳定性引导的自适应策略实现了显著的性能提升，并展现出良好的通用性和可扩展性。

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [66] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: PICore是一个无监督的核心集选择框架，通过物理信息损失识别最有信息量的训练样本，无需访问真实PDE解，显著提高神经算子训练效率并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 神经算子在求解偏微分方程时面临两个主要瓶颈：需要大量训练数据学习函数空间映射，且数据需要通过昂贵的数值求解器仿真获得标签，导致训练成本高昂。

Method: 提出PICore无监督核心集选择框架，利用物理信息损失来评估未标记输入样本对算子学习的潜在贡献，选择紧凑的信息量最大的输入子集，仅对选中样本进行数值仿真生成标签，然后在缩减的标记数据集上训练神经算子。

Result: 在四个不同的PDE基准测试和多种核心集选择策略中，PICore相比有监督核心集选择方法平均训练效率提升高达78%，同时准确性变化最小，显著减少了训练时间和标注成本。

Conclusion: PICore框架通过物理信息指导的无监督样本选择，成功解决了神经算子训练中的数据需求大和标注成本高的双重问题，为PDE求解的神经算子训练提供了高效的解决方案。

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [67] [Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection](https://arxiv.org/abs/2507.17161)
*Vinura Galwaduge,Jagath Samarabandu*

Main category: cs.LG

TL;DR: 本文提出了一个基于扩散模型的反事实解释框架，用于网络入侵检测系统，能够提供可操作的解释并生成全局规则来有效过滤攻击查询。


<details>
  <summary>Details</summary>
Motivation: 现代网络入侵检测系统使用复杂的深度学习模型，但其"黑盒"特性阻碍了对检测决策的理解和信任，现有的可解释AI方法难以转化为可操作的对策。

Method: 提出了一种新颖的基于扩散模型的反事实解释框架，通过生成最小化、多样化的反事实解释，并将其总结为全局规则。

Result: 在3个现代网络入侵数据集上评估，相比其他反事实解释算法，该方法能更高效地生成最小化、多样化的反事实解释，并能创建可操作的全局规则。

Conclusion: 所提出的方法不仅在实例级别提供可操作解释，还能在全局级别生成规则来有效过滤入侵攻击查询，这对高效的入侵检测和防御机制至关重要。

Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the
predictive power of complex deep learning models. However, the "black-box"
nature of such deep learning methods adds a layer of opaqueness that hinders
the proper understanding of detection decisions, trust in the decisions and
prevent timely countermeasures against such attacks. Explainable AI (XAI)
methods provide a solution to this problem by providing insights into the
causes of the predictions. The majority of the existing XAI methods provide
explanations which are not convenient to convert into actionable
countermeasures. In this work, we propose a novel diffusion-based
counterfactual explanation framework that can provide actionable explanations
for network intrusion attacks. We evaluated our proposed algorithm against
several other publicly available counterfactual explanation algorithms on 3
modern network intrusion datasets. To the best of our knowledge, this work also
presents the first comparative analysis of existing counterfactual explanation
algorithms within the context of network intrusion detection systems. Our
proposed method provide minimal, diverse counterfactual explanations out of the
tested counterfactual explanation algorithms in a more efficient manner by
reducing the time to generate explanations. We also demonstrate how
counterfactual explanations can provide actionable explanations by summarizing
them to create a set of global rules. These rules are actionable not only at
instance level but also at the global level for intrusion attacks. These global
counterfactual rules show the ability to effectively filter out incoming attack
queries which is crucial for efficient intrusion detection and defense
mechanisms.

</details>


### [68] [Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://arxiv.org/abs/2507.17189)
*Shaohan Li,Hao Yang,Min Chen,Xiaolin Qin*

Main category: cs.LG

TL;DR: 该论文提出了Met2Net，一种用于天气预测的隐式两阶段训练方法，通过为每个变量配置独立的编码器和解码器，以及引入自注意力机制来改善多变量融合，在近地面气温和相对湿度预测上分别减少了28.82%和23.39%的MSE误差。


<details>
  <summary>Details</summary>
Motivation: 由于全球气候变化导致极端天气事件频发，迫切需要准确的天气预测。现有的端到端深度学习方法存在多变量集成中的表示不一致性问题，难以有效捕获复杂天气系统中变量间的依赖关系。虽然多模态模型的两阶段训练方法可部分缓解此问题，但由于两个阶段训练任务不一致，结果往往不够理想。

Method: 提出了一种隐式两阶段训练方法，为每个变量配置独立的编码器和解码器。第一阶段冻结翻译器，让编码器和解码器学习共享的潜在空间；第二阶段冻结编码器和解码器，让翻译器捕获变量间的交互进行预测。此外，在潜在空间中引入自注意力机制进行多变量融合，进一步提升性能。

Result: 大量实验表明该方法达到了最先进的性能。具体而言，在近地面气温和相对湿度预测任务上，MSE误差分别减少了28.82%和23.39%。源代码已在GitHub上公开。

Conclusion: Met2Net通过隐式两阶段训练和自注意力机制有效解决了天气预测中的多变量表示不一致性和变量依赖捕获问题，在天气预测任务上取得了显著的性能提升，为准确天气预测提供了新的解决方案。

Abstract: The increasing frequency of extreme weather events due to global climate
change urges accurate weather prediction. Recently, great advances have been
made by the \textbf{end-to-end methods}, thanks to deep learning techniques,
but they face limitations of \textit{representation inconsistency} in
multivariable integration and struggle to effectively capture the dependency
between variables, which is required in complex weather systems. Treating
different variables as distinct modalities and applying a \textbf{two-stage
training approach} from multimodal models can partially alleviate this issue,
but due to the inconformity in training tasks between the two stages, the
results are often suboptimal. To address these challenges, we propose an
implicit two-stage training method, configuring separate encoders and decoders
for each variable. In detailed, in the first stage, the Translator is frozen
while the Encoders and Decoders learn a shared latent space, in the second
stage, the Encoders and Decoders are frozen, and the Translator captures
inter-variable interactions for prediction. Besides, by introducing a
self-attention mechanism for multivariable fusion in the latent space, the
performance achieves further improvements. Empirically, extensive experiments
show the state-of-the-art performance of our method. Specifically, it reduces
the MSE for near-surface air temperature and relative humidity predictions by
28.82\% and 23.39\%, respectively. The source code is available at
https://github.com/ShremG/Met2Net.

</details>


### [69] [Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation](https://arxiv.org/abs/2507.17204)
*Zixuan Wang,Jinghao Shi,Hanzhong Liang,Xiang Shen,Vera Wen,Zhiqian Chen,Yifan Wu,Zhixin Zhang,Hongyu Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种基于多模态大语言模型(MLLM)的视频内容审核系统,通过路由器-排序级联架构实现了高效的工业级部署,在提升审核效果的同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统视频分类模型在处理隐含有害内容和上下文歧义等复杂场景时表现不佳,而多模态大语言模型虽然具有优秀的跨模态推理能力,但面临计算成本高和生成模型适配分类任务困难的挑战,阻碍了其在工业界的广泛应用。

Method: 首先提出了一种高效方法,使用最少的判别训练数据将生成式MLLM转换为多模态分类器;然后设计了路由器-排序级联系统,将MLLM与轻量级路由模型集成,实现工业规模的部署。

Result: 离线实验显示,基于MLLM的方法相比传统分类器F1分数提升66.50%,同时仅需要2%的微调数据;在线评估表明系统将自动内容审核量提升41%,级联部署将计算成本降低至直接全规模部署的1.5%。

Conclusion: 通过巧妙的系统设计,成功解决了MLLM在视频内容审核中的部署难题,实现了效果提升与成本控制的双重目标,为多模态大模型的工业化应用提供了可行的解决方案。

Abstract: Effective content moderation is essential for video platforms to safeguard
user experience and uphold community standards. While traditional video
classification models effectively handle well-defined moderation tasks, they
struggle with complicated scenarios such as implicit harmful content and
contextual ambiguity. Multimodal large language models (MLLMs) offer a
promising solution to these limitations with their superior cross-modal
reasoning and contextual understanding. However, two key challenges hinder
their industrial adoption. First, the high computational cost of MLLMs makes
full-scale deployment impractical. Second, adapting generative models for
discriminative classification remains an open research problem. In this paper,
we first introduce an efficient method to transform a generative MLLM into a
multimodal classifier using minimal discriminative training data. To enable
industry-scale deployment, we then propose a router-ranking cascade system that
integrates MLLMs with a lightweight router model. Offline experiments
demonstrate that our MLLM-based approach improves F1 score by 66.50% over
traditional classifiers while requiring only 2% of the fine-tuning data. Online
evaluations show that our system increases automatic content moderation volume
by 41%, while the cascading deployment reduces computational cost to only 1.5%
of direct full-scale deployment.

</details>


### [70] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: 本文提出了一种联合速率-效用优化的数据集蒸馏方法，通过将合成样本参数化为可优化的潜在编码并使用轻量级网络解码，在保持准确性的同时实现了比标准蒸馏方法高达170倍的压缩率。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习受"规模决定一切"范式驱动，对越来越大的数据集和模型的需求导致了过高的计算和存储要求。现有的数据集蒸馏方法要么在固定存储预算下最大化性能，要么追求合适的合成数据表示来去除冗余，但没有联合优化这两个目标。

Method: 提出了一种联合速率-效用优化的数据集蒸馏方法。将合成样本参数化为由极轻量级网络解码的可优化潜在编码，使用量化潜在变量的香农熵作为速率度量，将任何现有的蒸馏损失作为效用度量，通过拉格朗日乘数在两者之间进行权衡。引入了每类比特数(bpc)作为精确的存储度量标准。

Result: 在CIFAR-10、CIFAR-100和ImageNet-128数据集上，该方法在相当精度下实现了比标准蒸馏高达170倍的压缩率。在不同的bpc预算、蒸馏损失和骨干架构下，该方法始终建立了更好的速率-效用权衡。

Conclusion: 通过联合优化存储成本和模型性能，提出的方法在数据集蒸馏领域取得了显著的压缩效果，为在资源受限环境下进行高效机器学习提供了新的解决方案。该方法在多个数据集和不同设置下都表现出了一致的优越性能。

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [71] [DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs](https://arxiv.org/abs/2507.17245)
*Haolin Jin,Mengbai Xiao,Yuan Yuan,Xiao Zhang,Dongxiao Yu,Guanghui Zhang,Haoliang Wang*

Main category: cs.LG

TL;DR: 本文提出了DistrAttention，一种高效灵活的自注意力机制，通过在嵌入维度上对数据进行分组来保持完整上下文信息，同时降低计算复杂度，在多个任务上实现了速度和精度的良好平衡。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的自注意力机制具有相对于输入序列长度的二次时间复杂度，严重阻碍了Transformer的可扩展性。现有的自注意力优化方法要么丢弃了完整的上下文信息，要么缺乏灵活性，无法在保持全上下文的同时实现高效计算。

Method: 设计了DistrAttention机制，通过在嵌入维度d上对数据进行分组来保持完整上下文。采用轻量级的采样和融合方法，利用局部敏感哈希来分组相似数据。进一步设计了块级分组框架来限制局部敏感哈希引入的误差。通过优化块大小的选择，使DistrAttention能够轻松与FlashAttention-2集成。

Result: 实验结果显示，DistrAttention在自注意力计算上比FlashAttention-2快37%。在ViT推理中，DistrAttention在近似自注意力机制中速度最快且精度最高。在Llama3-1B上，DistrAttention实现了最低的推理时间，精度损失仅为1%。

Conclusion: DistrAttention成功解决了传统自注意力机制的计算效率问题，在保持完整上下文信息的同时显著提升了计算速度，为大规模Transformer模型的部署提供了一个实用且高效的解决方案。

Abstract: The Transformer architecture has revolutionized deep learning, delivering the
state-of-the-art performance in areas such as natural language processing,
computer vision, and time series prediction. However, its core component,
self-attention, has the quadratic time complexity relative to input sequence
length, which hinders the scalability of Transformers. The exsiting approaches
on optimizing self-attention either discard full-contextual information or lack
of flexibility. In this work, we design DistrAttention, an effcient and
flexible self-attention mechanism with the full context. DistrAttention
achieves this by grouping data on the embedding dimensionality, usually
referred to as $d$. We realize DistrAttention with a lightweight sampling and
fusion method that exploits locality-sensitive hashing to group similar data. A
block-wise grouping framework is further designed to limit the errors
introduced by locality sensitive hashing. By optimizing the selection of block
sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining
high-performance on modern GPUs. We evaluate DistrAttention with extensive
experiments. The results show that our method is 37% faster than
FlashAttention-2 on calculating self-attention. In ViT inference,
DistrAttention is the fastest and the most accurate among approximate
self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the
lowest inference time with only 1% accuray loss.

</details>


### [72] [Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions](https://arxiv.org/abs/2507.17255)
*Songxuan Shi*

Main category: cs.LG

TL;DR: 本文探索了自编码器的生成能力，提出了一种新的VAE训练方法来增强数据紧凑性，并揭示了VAE和VQ-VAE之间的内在联系，强调了编码空间紧凑性在生成建模中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在生成任务中存在编码空间未定义区域的问题，限制了其生成能力。现有VAE和VQ-VAE方法各有局限性，需要探索它们之间的内在联系并找到改进方案。

Method: 提出了一种新的类VAE训练方法，通过引入聚类中心来增强数据紧凑性，确保潜在空间的良好定义，无需依赖传统的KL散度或重参数化技术。将该方法扩展到多个可学习向量，观察向VQ-VAE模型的自然演进。

Result: 在MNIST、CelebA和FashionMNIST数据集上的实验显示了平滑的插值过渡效果，但仍存在模糊性问题。当编码器输出多个向量时，模型退化为离散自编码器(VQ-AE)，只能组合图像片段而无法学习语义表示。

Conclusion: 研究强调了编码空间紧凑性和分散性在生成建模中的关键作用，为VAE和VQ-VAE之间的内在联系提供了新见解，为这些模型的设计和局限性分析提供了新的视角。

Abstract: This paper explores the generative capabilities of Autoencoders (AEs) and
establishes connections between Variational Autoencoders (VAEs) and Vector
Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training
framework. We demonstrate that AEs exhibit generative potential via latent
space interpolation and perturbation, albeit limited by undefined regions in
the encoding space. To address this, we propose a new VAE-like training method
that introduces clustering centers to enhance data compactness and ensure
well-defined latent spaces without relying on traditional KL divergence or
reparameterization techniques. Experimental results on MNIST, CelebA, and
FashionMNIST datasets show smooth interpolative transitions, though blurriness
persists. Extending this approach to multiple learnable vectors, we observe a
natural progression toward a VQ-VAE-like model in continuous space. However,
when the encoder outputs multiple vectors, the model degenerates into a
discrete Autoencoder (VQ-AE), which combines image fragments without learning
semantic representations. Our findings highlight the critical role of encoding
space compactness and dispersion in generative modeling and provide insights
into the intrinsic connections between VAEs and VQ-VAEs, offering a new
perspective on their design and limitations.

</details>


### [73] [Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance](https://arxiv.org/abs/2507.17273)
*Rishi Parekh,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 本文提出了一个结合知识图谱(KG)和大语言模型(LLM)的框架，用于分析仓库离散事件仿真(DES)的复杂输出数据，自动识别瓶颈和低效问题，显著提升了分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 分析仓库离散事件仿真产生的大型复杂输出数据集以识别瓶颈和低效问题是一项关键但具有挑战性的任务，通常需要大量人工努力或专门的分析工具，缺乏自动化和智能化的解决方案。

Method: 构建了一个集成知识图谱和LLM代理的框架：1)将原始DES数据转换为语义丰富的知识图谱，捕捉仿真事件和实体之间的关系；2)使用基于LLM的代理进行迭代推理，生成相互依赖的子问题；3)为每个子问题创建Cypher查询与KG交互，提取信息并进行自我反思以纠正错误；4)通过自适应、迭代和自我纠错过程模拟人类分析识别运营问题。

Result: 在设备故障和流程异常的仓库瓶颈识别测试中，该方法优于基线方法。对于运营问题，在精确定位低效问题方面达到了接近完美的通过率。对于复杂的调查性问题，展现了卓越的诊断能力，能够发现微妙的、相互关联的问题。

Conclusion: 该工作成功桥接了仿真建模和人工智能技术(KG+LLM)，为获取可操作洞察提供了更直观的方法，减少了洞察获取时间，实现了仓库低效评估和诊断的自动化，为仓库运营优化提供了新的技术路径。

Abstract: Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.

</details>


### [74] [Decentralized Federated Learning of Probabilistic Generative Classifiers](https://arxiv.org/abs/2507.17285)
*Aritz Pérez,Carlos Echegoyen,Guzmán Santafé*

Main category: cs.LG

TL;DR: 本文提出了一种在去中心化架构下协作学习概率生成分类器的新方法，通过节点间共享局部统计信息来训练全局模型，无需中央服务器


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习主要依赖中央服务器，而去中心化架构可以避免单点故障和隐私泄露风险，但缺乏有效的协作学习概率生成分类器的方法

Method: 设计了一个由通信网络和局部更新规则组成的框架，各节点与邻居节点共享局部统计信息，聚合邻居信息后迭代学习自己的局部分类器，最终收敛到全局模型

Result: 大量实验表明，该算法在各种网络拓扑、网络规模、局部数据集大小和极端非独立同分布数据分布下都能一致收敛到具有全局竞争力的模型

Conclusion: 提出的去中心化联邦学习方法能够有效训练概率生成分类器，在保护数据隐私的同时实现良好的模型性能，适用于各种实际应用场景

Abstract: Federated learning is a paradigm of increasing relevance in real world
applications, aimed at building a global model across a network of
heterogeneous users without requiring the sharing of private data. We focus on
model learning over decentralized architectures, where users collaborate
directly to update the global model without relying on a central server. In
this context, the current paper proposes a novel approach to collaboratively
learn probabilistic generative classifiers with a parametric form. The
framework is composed by a communication network over a set of local nodes,
each of one having its own local data, and a local updating rule. The proposal
involves sharing local statistics with neighboring nodes, where each node
aggregates the neighbors' information and iteratively learns its own local
classifier, which progressively converges to a global model. Extensive
experiments demonstrate that the algorithm consistently converges to a globally
competitive model across a wide range of network topologies, network sizes,
local dataset sizes, and extreme non-i.i.d. data distributions.

</details>


### [75] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: 本文提出R-Stitch框架，通过在小语言模型和大语言模型之间基于置信度进行动态切换，实现思维链推理的高效加速，在数学推理任务上实现了85%的推理延迟减少且准确率几乎无损失。


<details>
  <summary>Details</summary>
Motivation: 思维链推理虽然能增强大语言模型的问题解决能力，但由于需要自回归解码长序列而带来巨大的计算开销。现有的推测解码方法在大小模型一致性低时加速有限，且未能充分利用小模型生成简洁中间推理的潜在优势。

Method: 提出R-Stitch，一个基于置信度的混合解码框架。默认使用小语言模型生成tokens，仅在小模型置信度低于阈值时切换到大语言模型。该方法避免全序列回滚，选择性地在不确定步骤调用大模型，是模型无关、无需训练且兼容标准解码管道的框架。

Result: 在数学推理基准测试中，R-Stitch实现了高达85%的推理延迟减少，同时准确率几乎无损失，证明了其在加速思维链推理方面的实际有效性。

Conclusion: R-Stitch通过token级别的置信度判断实现大小模型间的智能切换，有效平衡了推理效率和答案质量，为思维链推理加速提供了一种实用且高效的解决方案。

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [76] [Confounded Causal Imitation Learning with Instrumental Variables](https://arxiv.org/abs/2507.17309)
*Yan Zeng,Shenglan Nie,Feng Xie,Libo Huang,Peng Wu,Zhi Geng*

Main category: cs.LG

TL;DR: 本文提出了一个混淆因果模仿学习(C2L)模型，通过工具变量方法解决模仿学习中未测量混淆变量导致的策略估计偏差问题


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习容易受到未测量变量（混淆因子）对状态和动作的影响，如果忽略这些因素会导致策略估计产生偏差，需要找到一种方法来消除这种混淆效应

Method: 提出混淆因果模仿学习(C2L)模型，利用工具变量(IV)的强大能力，开发了一个两阶段框架：第一阶段基于定义的伪变量构建测试准则来识别有效的工具变量；第二阶段利用识别出的工具变量提出两种策略学习方法（基于模拟器和离线方法）

Result: 通过大量实验验证了识别有效工具变量和学习策略的有效性，证明了该方法能够处理跨多个时间步影响动作的混淆因子，而不仅限于即时时间依赖关系

Conclusion: C2L模型成功解决了模仿学习中的混淆变量问题，通过工具变量方法实现了有效的策略学习，为处理因果关系复杂的模仿学习任务提供了新的解决方案

Abstract: Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.

</details>


### [77] [EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents](https://arxiv.org/abs/2507.17311)
*Zijie Guo,Jiong Wang,Xiaoyu Yue,Wangxu Wei,Zhe Jiang,Wanghan Xu,Ben Fei,Wenlong Zhang,Xinyu Gu,Lijing Cheng,Jing-Jia Luo,Chao Li,Yaqiang Wang,Tao Chen,Wanli Ouyang,Fenghua Ling,Lei Bai*

Main category: cs.LG

TL;DR: EarthLink是首个专为地球科学家设计的AI智能体，通过自动化端到端研究流程和交互式学习，显著提升了地球系统研究的效率和可信度。


<details>
  <summary>Details</summary>
Motivation: 现代地球科学面临数据庞大、分散且复杂的挑战，加上日益复杂的分析需求，这为快速科学发现创造了重大瓶颈。现有的静态诊断工具无法满足动态、交互式的研究需求。

Method: 开发了EarthLink AI智能体，具备交互式副驾驶功能，能够自动化从规划、代码生成到多场景分析的端到端研究工作流程。系统通过动态反馈循环从用户交互中学习，持续改进其能力。采用透明、可审计的工作流程和自然语言接口。

Result: 在气候变化核心科学任务验证中表现良好，包括模型-观测比较和复杂现象诊断。多专家评估显示EarthLink能产生科学上合理的分析，其分析能力在特定方面可与人类初级研究员的工作流程相媲美。

Conclusion: EarthLink标志着地球系统研究向高效、可信和协作范式的关键转变，使科学家能够从繁重的手动执行转向战略监督和假设生成，为全球变化加速时代的地球科学研究提供了新的解决方案。

Abstract: Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change.

</details>


### [78] [A Learning-based Domain Decomposition Method](https://arxiv.org/abs/2507.17328)
*Rui Wu,Nikola Kovachki,Burigede Liu*

Main category: cs.LG

TL;DR: 提出了一种基于学习的域分解方法(L-DDM)，使用预训练的神经算子作为代理模型来高效求解复杂几何域上的偏微分方程，在保持分辨率不变性的同时显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法如有限元法在处理大规模复杂几何问题时面临计算成本高和可扩展性差的挑战，而现有的神经网络方法主要局限于简单域，难以应用于涉及复杂几何的实际偏微分方程问题。

Method: 提出基于学习的域分解方法(L-DDM)，将在简单域上预训练的单一神经算子作为代理模型嵌入域分解框架中，使用物理预训练神经算子(PPNO)来处理复杂几何域上的椭圆型偏微分方程。

Result: 在具有不连续微结构的复杂几何域上准确逼近椭圆型偏微分方程解，性能超越当前最先进方法，同时具备分辨率不变性和对训练期间未见微结构模式的强泛化能力。

Conclusion: L-DDM方法成功解决了神经网络方法在复杂几何域应用中的局限性，为大规模复杂结构的高效建模和分析提供了新的解决方案，在理论上证明了域分解框架下神经算子逼近的存在性。

Abstract: Recent developments in mechanical, aerospace, and structural engineering have
driven a growing need for efficient ways to model and analyse structures at
much larger and more complex scales than before. While established numerical
methods like the Finite Element Method remain reliable, they often struggle
with computational cost and scalability when dealing with large and
geometrically intricate problems. In recent years, neural network-based methods
have shown promise because of their ability to efficiently approximate
nonlinear mappings. However, most existing neural approaches are still largely
limited to simple domains, which makes it difficult to apply to real-world PDEs
involving complex geometries. In this paper, we propose a learning-based domain
decomposition method (L-DDM) that addresses this gap. Our approach uses a
single, pre-trained neural operator-originally trained on simple domains-as a
surrogate model within a domain decomposition scheme, allowing us to tackle
large and complicated domains efficiently. We provide a general theoretical
result on the existence of neural operator approximations in the context of
domain decomposition solution of abstract PDEs. We then demonstrate our method
by accurately approximating solutions to elliptic PDEs with discontinuous
microstructures in complex geometries, using a physics-pretrained neural
operator (PPNO). Our results show that this approach not only outperforms
current state-of-the-art methods on these challenging problems, but also offers
resolution-invariance and strong generalization to microstructural patterns
unseen during training.

</details>


### [79] [DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD](https://arxiv.org/abs/2507.17346)
*Rongwei Lu,Jingyan Jiang,Chunyang Li,Haotian Dong,Xingguang Wei,Delin Cai,Zhi Wang*

Main category: cs.LG

TL;DR: 该论文提出了DeCo-SGD算法，通过动态调整梯度压缩比和延迟同步参数来解决分布式机器学习在高延迟、低带宽网络环境下的吞吐量退化问题，相比传统方法实现了最高5.07倍的加速。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习在高端到端延迟和低变化带宽网络环境中面临严重的吞吐量退化问题。现有方法通常采用梯度压缩和延迟聚合来分别缓解低带宽和高延迟问题，但将这些策略结合时会产生压缩比、延迟性和模型收敛率之间复杂的三方权衡。由于缺乏理论指导，现有工作依赖静态启发式策略，无法在变化的带宽条件下实现最佳平衡。

Method: 引入新的理论工具，将联合优化问题分解为传统收敛率分析与多个可分析噪声项的组合。首次揭示延迟性会指数级放大梯度压缩对训练性能的负面影响。通过将收敛率与网络感知的时间最小化条件相结合，提出DeCo-SGD算法，该算法根据实时网络条件和训练任务动态调整压缩比和延迟性参数。

Result: DeCo-SGD在高延迟和低变化带宽网络中分别比D-SGD和静态策略实现了最高5.07倍和1.37倍的加速。实验验证了该方法在challenging网络环境下的有效性。

Conclusion: 通过理论分析揭示了压缩梯度和延迟梯度对训练的影响机制，特别是延迟性对梯度压缩负面影响的指数级放大效应。提出的DeCo-SGD算法成功解决了分布式SGD在复杂网络环境下的性能问题，为分布式机器学习在资源受限网络中的应用提供了有效解决方案。

Abstract: Distributed machine learning in high end-to-end latency and low, varying
bandwidth network environments undergoes severe throughput degradation. Due to
its low communication requirements, distributed SGD (D-SGD) remains the
mainstream optimizer in such challenging networks, but it still suffers from
significant throughput reduction. To mitigate these limitations, existing
approaches typically employ gradient compression and delayed aggregation to
alleviate low bandwidth and high latency, respectively. To address both
challenges simultaneously, these strategies are often combined, introducing a
complex three-way trade-off among compression ratio, staleness (delayed
synchronization steps), and model convergence rate. To achieve the balance
under varying bandwidth conditions, an adaptive policy is required to
dynamically adjust these parameters. Unfortunately, existing works rely on
static heuristic strategies due to the lack of theoretical guidance, which
prevents them from achieving this goal. This study fills in this theoretical
gap by introducing a new theoretical tool, decomposing the joint optimization
problem into a traditional convergence rate analysis with multiple analyzable
noise terms. We are the first to reveal that staleness exponentially amplifies
the negative impact of gradient compression on training performance, filling a
critical gap in understanding how compressed and delayed gradients affect
training. Furthermore, by integrating the convergence rate with a network-aware
time minimization condition, we propose DeCo-SGD, which dynamically adjusts the
compression ratio and staleness based on the real-time network condition and
training task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and
static strategy in high-latency and low, varying bandwidth networks,
respectively.

</details>


### [80] [TOC-UCO: a comprehensive repository of tabular ordinal classification datasets](https://arxiv.org/abs/2507.17348)
*Rafael Ayllón-Gavilán,David Guijo-Rubio,Antonio Manuel Gómez-Orellana,David Guijo-Rubio,Francisco Bérchez-Moreno,Víctor Manuel Vargas-Yun,Pedro A. Gutiérrez*

Main category: cs.LG

TL;DR: 科尔多瓦大学创建了TOC-UCO数据集库，包含46个表格式有序分类数据集，为有序分类方法提供标准化的基准测试平台


<details>
  <summary>Details</summary>
Motivation: 有序分类领域缺乏全面的数据集库来对新方法进行基准测试，这阻碍了该领域的发展

Method: 构建了一个公开可用的表格式有序分类数据集库TOC-UCO，包含46个经过预处理的数据集，确保合理的样本数量和适当的类别分布，并提供30个随机训练-测试划分的索引

Result: 成功建立了TOC-UCO库，为有序分类研究提供了标准化的基准测试平台，包含详细的数据来源、预处理步骤和基准测试指南

Conclusion: TOC-UCO数据集库填补了有序分类领域缺乏标准化基准数据集的空白，为该领域的方法验证和比较提供了重要资源，有助于推动有序分类研究的发展

Abstract: An ordinal classification (OC) problem corresponds to a special type of
classification characterised by the presence of a natural order relationship
among the classes. This type of problem can be found in a number of real-world
applications, motivating the design and development of many ordinal
methodologies over the last years. However, it is important to highlight that
the development of the OC field suffers from one main disadvantage: the lack of
a comprehensive set of datasets on which novel approaches to the literature
have to be benchmarked. In order to approach this objective, this manuscript
from the University of C\'ordoba (UCO), which have previous experience on the
OC field, provides the literature with a publicly available repository of
tabular data for a robust validation of novel OC approaches, namely TOC-UCO
(Tabular Ordinal Classification repository of the UCO). Specifically, this
repository includes a set of $46$ tabular ordinal datasets, preprocessed under
a common framework and ensured to have a reasonable number of patterns and an
appropriate class distribution. We also provide the sources and preprocessing
steps of each dataset, along with details on how to benchmark a novel approach
using the TOC-UCO repository. For this, indices for $30$ different randomised
train-test partitions are provided to facilitate the reproducibility of the
experiments.

</details>


### [81] [DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2507.17365)
*Chuzhan Hao,Wenfeng Feng,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 本文提出DynaSearcher，一个基于动态知识图谱和多奖励强化学习的搜索代理，用于解决基于大语言模型的多步检索系统在复杂信息搜索任务中面临的事实不一致和搜索效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的多步代理检索系统在复杂信息搜索任务中表现出色，但仍面临生成事实不一致的中间查询和低效搜索轨迹等问题，这会导致推理偏差或冗余计算。

Method: 提出DynaSearcher系统，结合动态知识图谱和多奖励强化学习。利用知识图谱作为外部结构化知识指导搜索过程，通过显式建模实体关系确保中间查询的事实一致性；采用多奖励强化学习框架对检索准确性、效率和响应质量等训练目标进行细粒度控制。

Result: 在六个多跳问答数据集上达到了最先进的答案准确性，仅使用小规模模型和有限计算资源就能匹配前沿大语言模型的性能。该方法在不同检索环境和大规模模型上展现出强泛化性和鲁棒性。

Conclusion: DynaSearcher通过结合动态知识图谱和多奖励强化学习，有效解决了多步检索系统中的事实一致性和搜索效率问题，在保持高准确性的同时显著提升了计算效率，具有广泛的应用前景。

Abstract: Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.

</details>


### [82] [ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning](https://arxiv.org/abs/2507.17368)
*Hao Dai,Chong Tang,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 本文提出ViRN框架，通过变分推理和分布三角测量技术解决持续学习中长尾数据分布的挑战，在六个基准测试中相比现有最佳方法平均准确率提升10.24%。


<details>
  <summary>Details</summary>
Motivation: 现实世界AI系统面临持续学习中长尾数据分布的关键挑战：模型需要在严重类别不平衡的情况下顺序适应新类别，同时保持对旧类别的知识。现有方法难以平衡稳定性和可塑性，在极端样本稀缺情况下容易崩溃。

Method: 提出ViRN框架，将变分推理与分布三角测量相结合：1) 通过变分自编码器建模类条件分布以减轻对头部类别的偏差；2) 通过基于Wasserstein距离的邻域检索和几何融合重构尾部类别分布，实现尾部类别表示的样本高效对齐。

Result: 在六个长尾分类基准测试（包括语音任务如稀有声学事件、口音识别和图像任务）上进行评估，ViRN相比现有最先进方法平均准确率提升10.24%。

Conclusion: ViRN框架通过变分推理和分布三角测量的创新结合，有效解决了持续学习中长尾数据分布的挑战，显著提升了模型在类别不平衡场景下的性能表现。

Abstract: Continual learning (CL) with long-tailed data distributions remains a
critical challenge for real-world AI systems, where models must sequentially
adapt to new classes while retaining knowledge of old ones, despite severe
class imbalance. Existing methods struggle to balance stability and plasticity,
often collapsing under extreme sample scarcity. To address this, we propose
ViRN, a novel CL framework that integrates variational inference (VI) with
distributional trilateration for robust long-tailed learning. First, we model
class-conditional distributions via a Variational Autoencoder to mitigate bias
toward head classes. Second, we reconstruct tail-class distributions via
Wasserstein distance-based neighborhood retrieval and geometric fusion,
enabling sample-efficient alignment of tail-class representations. Evaluated on
six long-tailed classification benchmarks, including speech (e.g., rare
acoustic events, accents) and image tasks, ViRN achieves a 10.24% average
accuracy gain over state-of-the-art methods.

</details>


### [83] [Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective](https://arxiv.org/abs/2507.17382)
*Hao Dai,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 本文提出了变分贝叶斯持续广义类别发现方法(VB-CGCD)，通过贝叶斯视角分析遗忘动力学，解决了在无标签数据流中增量学习新类别的同时保持旧类别知识的挑战，在标准基准上实现了+15.21%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 持续广义类别发现面临关键挑战：需要从无标签数据流中增量学习新类别，同时保持对旧类别的知识。现有方法在灾难性遗忘方面存在困难，特别是当无标签数据混合已知和新颖类别时。

Method: 通过贝叶斯视角分析C-GCD的遗忘动力学，发现新旧类别间的协方差错位导致性能下降。基于此洞察，提出变分贝叶斯C-GCD框架，将变分推理与协方差感知的最近类均值分类相结合，通过随机变分更新自适应对齐类分布并抑制伪标签噪声。

Result: VB-CGCD在标准基准测试中超越现有方法+15.21%的最终会话总体准确率。在新的挑战性基准(仅10%标签数据和扩展在线阶段)中，VB-CGCD达到67.86%的最终准确率，显著高于最先进方法的38.55%。

Conclusion: VB-CGCD通过变分推理和协方差感知分类有效解决了持续广义类别发现中的遗忘问题，在多种场景下展现出强大的适用性和显著的性能改进，为无标签数据流中的增量学习提供了新的解决方案。

Abstract: Continual Generalized Category Discovery (C-GCD) faces a critical challenge:
incrementally learning new classes from unlabeled data streams while preserving
knowledge of old classes. Existing methods struggle with catastrophic
forgetting, especially when unlabeled data mixes known and novel categories. We
address this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,
revealing that covariance misalignment between old and new classes drives
performance degradation. Building on this insight, we propose Variational Bayes
C-GCD (VB-CGCD), a novel framework that integrates variational inference with
covariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns
class distributions while suppressing pseudo-label noise via stochastic
variational updates. Experiments show VB-CGCD surpasses prior art by +15.21%
with the overall accuracy in the final session on standard benchmarks. We also
introduce a new challenging benchmark with only 10% labeled data and extended
online phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher
than state-of-the-art (38.55%), demonstrating its robust applicability across
diverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD

</details>


### [84] [A Comprehensive Evaluation on Quantization Techniques for Large Language Models](https://arxiv.org/abs/2507.17417)
*Yutong Liu,Cairong Zhao,Guosheng Hu*

Main category: cs.LG

TL;DR: 本文对大语言模型的训练后量化(PTQ)方法进行了全面的理论分析和实验评估，将量化方法解耦为预量化变换和量化误差缓解两个步骤，并评估了最新的MXFP4数据格式性能。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法缺乏公平的对比评估，且理论联系分析不足。不同论文的实验环境不统一，难以准确比较各方法的真实性能，同时缺乏对现有方法理论连接的深入理解。

Method: 将现有量化方法解耦为两个关键步骤：1)预量化变换-在量化前应用预处理步骤来减少异常值影响，使数据分布更平坦适合量化；2)量化误差缓解-采用技术来抵消量化过程中引入的误差。在统一实验环境下评估不同组件的影响，并分析最新MXFP4数据格式。

Result: 实验结果表明：1)优化的旋转和缩放在预量化变换中表现最佳；2)低秩补偿与GPTQ的结合在量化误差缓解中偶尔优于单独使用GPTQ；3)针对INT4的最优预量化变换策略不能很好地泛化到MXFP4格式。

Conclusion: 通过系统的理论分析和公平的实验比较，揭示了现有量化方法的理论联系和性能差异。发现了不同量化格式需要不同的优化策略，为未来量化方法的发展提供了重要见解和研究方向。

Abstract: For large language models (LLMs), post-training quantization (PTQ) can
significantly reduce memory footprint and computational overhead. Model
quantization is a rapidly evolving research field. Though many papers have
reported breakthrough performance, they may not conduct experiments on the same
ground since one quantization method usually contains multiple components. In
addition, analyzing the theoretical connections among existing methods is
crucial for in-depth understanding. To bridge these gaps, we conduct an
extensive review of state-of-the-art methods and perform comprehensive
evaluations on the same ground to ensure fair comparisons. To our knowledge,
this fair and extensive investigation remains critically important yet
underexplored. To better understand the theoretical connections, we decouple
the published quantization methods into two steps: pre-quantization
transformation and quantization error mitigation. We define the former as a
preprocessing step applied before quantization to reduce the impact of
outliers, making the data distribution flatter and more suitable for
quantization. Quantization error mitigation involves techniques that offset the
errors introduced during quantization, thereby enhancing model performance. We
evaluate and analyze the impact of different components of quantization
methods. Additionally, we analyze and evaluate the latest MXFP4 data format and
its performance. Our experimental results demonstrate that optimized rotation
and scaling yield the best performance for pre-quantization transformation, and
combining low-rank compensation with GPTQ occasionally outperforms using GPTQ
alone for quantization error mitigation. Furthermore, we explore the potential
of the latest MXFP4 quantization and reveal that the optimal pre-quantization
transformation strategy for INT4 does not generalize well to MXFP4, inspiring
further investigation.

</details>


### [85] [Persistent Patterns in Eye Movements: A Topological Approach to Emotion Recognition](https://arxiv.org/abs/2507.17450)
*Arsha Niksa,Hooman Zare,Ali Shahrabi,Hanieh Hatami,Mohammadreza Razvan*

Main category: cs.LG

TL;DR: 本文提出了一种基于拓扑数据分析的眼动轨迹情感识别方法，通过持久同调分析延迟嵌入的注视轨迹，提取形状特征并使用随机森林分类器实现了75.6%的四类情感识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的眼动数据情感识别方法可能无法充分捕捉注视轨迹的复杂动态特征，因此需要探索新的数学方法来更好地理解和分析眼动模式中蕴含的情感信息。

Method: 采用拓扑数据分析流水线：首先对注视轨迹进行延迟嵌入，然后使用持久同调分析得到持久图，从持久图中提取基于形状的特征（如平均持久性、最大持久性和熵），最后使用随机森林分类器进行情感分类。

Result: 在情感环形模型的四个象限对应的四类情感分类任务中，该方法达到了75.6%的分类准确率，证明了持久图几何特征能够有效编码具有判别性的注视动态信息。

Conclusion: 持久图几何特征能够有效捕捉和编码眼动轨迹中的判别性动态信息，为情感计算和人类行为分析提供了一种有前景的拓扑学方法，为该领域开辟了新的研究方向。

Abstract: We present a topological pipeline for automated multiclass emotion
recognition from eye-tracking data. Delay embeddings of gaze trajectories are
analyzed using persistent homology. From the resulting persistence diagrams, we
extract shape-based features such as mean persistence, maximum persistence, and
entropy. A random forest classifier trained on these features achieves up to
$75.6\%$ accuracy on four emotion classes, which are the quadrants the
Circumplex Model of Affect. The results demonstrate that persistence diagram
geometry effectively encodes discriminative gaze dynamics, suggesting a
promising topological approach for affective computing and human behavior
analysis.

</details>


### [86] [C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning](https://arxiv.org/abs/2507.17454)
*Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.LG

TL;DR: 提出C3RL框架，通过对比学习联合建模通道混合(CM)和通道独立(CI)策略，在多变量时间序列预测中实现了显著的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法存在局限性：CM策略能捕获变量间依赖但无法识别变量特定的时间模式；CI策略改善了这一点但无法充分利用跨变量依赖；基于特征融合的混合策略泛化性和可解释性有限

Method: 提出C3RL表示学习框架，受计算机视觉中对比学习启发，将CM和CI策略的输入视为转置视图，构建孪生网络架构：一个策略作为主干网络，另一个作为补充；通过自适应加权联合优化对比损失和预测损失

Result: 在七个模型上的大量实验表明，C3RL将基于CI策略的模型最佳性能率提升至81.4%，将基于CM策略的模型最佳性能率提升至76.3%，展现出强大的泛化性和有效性

Conclusion: C3RL框架成功解决了现有方法的局限性，通过联合建模CM和CI策略实现了表示学习和预测性能的平衡，在多变量时间序列预测任务中取得了显著改进

Abstract: Multivariate time series forecasting has drawn increasing attention due to
its practical importance. Existing approaches typically adopt either
channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can
capture inter-variable dependencies but fails to discern variable-specific
temporal patterns. CI strategy improves this aspect but fails to fully exploit
cross-variable dependencies like CM. Hybrid strategies based on feature fusion
offer limited generalization and interpretability. To address these issues, we
propose C3RL, a novel representation learning framework that jointly models
both CM and CI strategies. Motivated by contrastive learning in computer
vision, C3RL treats the inputs of the two strategies as transposed views and
builds a siamese network architecture: one strategy serves as the backbone,
while the other complements it. By jointly optimizing contrastive and
prediction losses with adaptive weighting, C3RL balances representation and
forecasting performance. Extensive experiments on seven models show that C3RL
boosts the best-case performance rate to 81.4\% for models based on CI strategy
and to 76.3\% for models based on CM strategy, demonstrating strong
generalization and effectiveness. The code will be available once the paper is
accepted.

</details>


### [87] [BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles](https://arxiv.org/abs/2507.17472)
*Junhua Liu,Roy Ka-Wei Lee,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出BGM-HAN模型，一个增强的字节对编码门控多头分层注意力网络，用于改善大学招生等高风险决策领域中的决策制定过程，通过建模半结构化申请者数据来提高决策的公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 人类在高风险决策领域（如大学招生）中容易受到难以检测的认知偏见影响，威胁决策的公平性和长期结果。需要一种新方法来增强复杂决策工作流程，减少偏见并提高决策质量。

Method: 提出BGM-HAN（增强的字节对编码门控多头分层注意力网络），通过分层学习和各种增强技术的集成来有效建模半结构化申请者数据。该模型能够捕获多层次表示，对细致评估至关重要，同时提高可解释性和预测性能。

Result: 在真实招生数据上的实验结果表明，所提出的BGM-HAN模型显著优于从传统机器学习到大型语言模型的最先进基线方法，在预测性能和可解释性方面都有显著提升。

Conclusion: BGM-HAN为在结构、上下文和公平性都很重要的领域中增强决策制定提供了一个有前景的框架，特别适用于大学招生等高风险决策场景，能够有效减少认知偏见并提高决策质量。

Abstract: Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.

</details>


### [88] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了深度归一化Transformer（DNT），通过在关键位置集成归一化技术，使Transformer能够用vanilla momentum SGD训练而不需要AdamW等自适应优化器，同时保持相当的性能


<details>
  <summary>Details</summary>
Motivation: 现有Transformer训练通常需要AdamW等自适应学习率优化器，而无法使用momentum SGD，这主要是由于梯度的重尾分布特性造成的。研究旨在解决这一限制，使Transformer能够用更简单的优化器训练

Method: 设计了深度归一化Transformer（DNT），在Transformer的适当位置策略性地集成归一化技术，有效调节各层的雅可比矩阵，平衡权重、激活函数及其交互的影响，从而使梯度分布更加集中

Result: 在两种流行的Transformer架构上进行了广泛实验验证：a) DNT在性能上超越了对应的模型（如ViT和GPT）；b) DNT能够有效地使用vanilla momentum SGD进行训练

Conclusion: 通过精心设计的归一化技术，DNT成功解决了Transformer训练中对自适应优化器的依赖问题，实现了用vanilla momentum SGD训练的目标，同时提供了理论证明和实验验证

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [89] [HOTA: Hamiltonian framework for Optimal Transport Advection](https://arxiv.org/abs/2507.17513)
*Nazar Buzun,Daniil Shlenskii,Maxim Bobrin,Dmitry V. Dylov*

Main category: cs.LG

TL;DR: 本文提出了HOTA（哈密顿最优传输对流）方法，通过Hamilton-Jacobi-Bellman方程和Kantorovich势函数解决双动态最优传输问题，避免了显式密度建模，在标准基准和非可微成本数据集上都优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型大多假设简单几何结构（如欧几里得几何）并依赖强密度估计假设，导致生成的轨迹不符合底层流形的真正最优性原则。需要一种能够在复杂流形上进行有效最优传输的方法。

Method: 提出了基于Hamilton-Jacobi-Bellman方程的哈密顿最优传输对流（HOTA）方法，通过Kantorovich势函数显式处理双动态最优传输问题，实现高效可扩展的轨迹优化，无需显式密度建模。

Result: HOTA在标准基准测试和具有非可微成本的自定义数据集上均优于所有基线方法，在可行性和最优性方面都表现出色，即使在成本函数非光滑的情况下也能有效工作。

Conclusion: HOTA方法成功解决了传统最优传输方法在复杂流形上的局限性，通过避免显式密度建模和处理非光滑成本函数，为生成模型提供了更加灵活和有效的最优传输框架。

Abstract: Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.

</details>


### [90] [Generalized Low-Rank Matrix Contextual Bandits with Graph Information](https://arxiv.org/abs/2507.17528)
*Yao Wang,Jiannan Li,Yue Kang,Shanxing Gao,Zhenxin Xiao*

Main category: cs.LG

TL;DR: 提出了一个新的矩阵上下文赌博机算法框架，能够同时利用低秩结构和图信息来改进序列决策制定，在理论分析和实验中都表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的矩阵上下文赌博机方法只利用了低秩结构，忽略了现实场景中存在的图信息（如在线广告和推荐系统中用户/物品之间的相似关系），这使得它们难以生成有效的决策策略。

Method: 提出了基于上置信界（UCB）框架的新型矩阵上下文赌博机算法，通过求解联合核范数和矩阵拉普拉斯正则化问题，然后实施基于图的广义线性UCB算法，统一整合低秩结构和图信息。

Result: 理论分析证明该方法在累积遗憾界方面优于几种流行的替代方法，合成数据和真实数据实验进一步验证了该方法的优越性。

Conclusion: 通过有效利用图信息，所提出的矩阵上下文赌博机框架能够显著改善序列决策制定的性能，为在线广告和推荐系统等应用场景提供了更好的解决方案。

Abstract: The matrix contextual bandit (CB), as an extension of the well-known
multi-armed bandit, is a powerful framework that has been widely applied in
sequential decision-making scenarios involving low-rank structure. In many
real-world scenarios, such as online advertising and recommender systems,
additional graph information often exists beyond the low-rank structure, that
is, the similar relationships among users/items can be naturally captured
through the connectivity among nodes in the corresponding graphs. However,
existing matrix CB methods fail to explore such graph information, and thereby
making them difficult to generate effective decision-making policies. To fill
in this void, we propose in this paper a novel matrix CB algorithmic framework
that builds upon the classical upper confidence bound (UCB) framework. This new
framework can effectively integrate both the low-rank structure and graph
information in a unified manner. Specifically, it involves first solving a
joint nuclear norm and matrix Laplacian regularization problem, followed by the
implementation of a graph-based generalized linear version of the UCB
algorithm. Rigorous theoretical analysis demonstrates that our procedure
outperforms several popular alternatives in terms of cumulative regret bound,
owing to the effective utilization of graph information. A series of synthetic
and real-world data experiments are conducted to further illustrate the merits
of our procedure.

</details>


### [91] [Generalized Advantage Estimation for Distributional Policy Gradients](https://arxiv.org/abs/2507.17530)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出了分布式广义优势估计（DGAE），通过最优传输理论和Wasserstein-like方向性度量来扩展传统GAE，使其能够处理分布式强化学习中的价值分布，从而在保持低方差的同时提高对系统噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的广义优势估计（GAE）虽然能有效降低策略梯度估计的方差，但无法处理分布式强化学习中的价值分布。分布式强化学习能够捕获系统固有的随机性，对系统噪声更加鲁棒，因此需要开发能够处理价值分布的优势估计方法。

Method: 基于最优传输理论，引入Wasserstein-like方向性度量来衡量概率分布之间的距离和方向差异。结合指数加权估计，利用这个度量推导出分布式GAE（DGAE）。该方法能够在控制偏差的同时提供低方差的优势估计，适用于依赖优势估计进行策略更新的策略梯度算法。

Result: 将DGAE集成到三种不同的策略梯度方法中，在多个OpenAI Gym环境中进行评估，与使用传统GAE的基线方法进行比较，验证了算法的性能表现。

Conclusion: DGAE成功地将GAE扩展到分布式强化学习领域，通过处理价值分布而不仅仅是期望值，提供了更鲁棒的优势估计方法，同时保持了传统GAE的低方差特性和可控偏差。

Abstract: Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.

</details>


### [92] [Federated Majorize-Minimization: Beyond Parameter Aggregation](https://arxiv.org/abs/2507.17534)
*Aymeric Dieuleveut,Gersende Fort,Mahmoud Hegazy,Hoi-To Wai*

Main category: cs.LG

TL;DR: 本文提出了一个统一的随机优化算法框架，用于解决联邦学习中的Majorize-Minimization问题，通过学习和聚合代理函数信息而非原始参数来应对数据异质性、部分参与和通信约束等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习算法在面对数据异质性、部分参与和通信约束等实际问题时缺乏统一的理论框架，需要一种能够鲁棒扩展到联邦学习环境的随机优化算法设计方法。

Method: 研究具有线性参数化代理函数族的Majorize-Minimization问题类别，提出统一算法SSMM（随机近似随机代理MM），然后扩展到联邦设置中形成QSMM算法。QSMM的核心创新是学习并聚合描述代理函数的信息，而非传统的聚合原始参数。

Result: SSMM框架成功统一了多种现有随机MM算法，QSMM算法有效处理了联邦学习中的关键挑战。作为灵活性展示，该方法还被应用于联邦环境下的最优传输映射计算问题。

Conclusion: 提出的统一框架为联邦学习中的随机优化提供了新的理论基础和实用方法，通过聚合代理函数信息而非参数的创新思路，为解决联邦学习实际挑战提供了有效途径。

Abstract: This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.

</details>


### [93] [XStacking: Explanation-Guided Stacked Ensemble Learning](https://arxiv.org/abs/2507.17650)
*Moncef Garouani,Ayah Barhrhouj,Olivier Teste*

Main category: cs.LG

TL;DR: 本文提出XStacking框架，通过集成动态特征转换和Shapley可加性解释，解决了集成机器学习（特别是堆叠方法）缺乏可解释性的问题，在保持预测准确性的同时提供固有的可解释性。


<details>
  <summary>Details</summary>
Motivation: 集成机器学习技术（尤其是堆叠方法）虽然能通过组合多个基础模型来提高预测性能，但经常因缺乏可解释性而受到批评。现有的堆叠方法在提升预测准确性的同时，往往牺牲了模型的透明度和可理解性。

Method: 提出XStacking框架，将动态特征转换与模型无关的Shapley可加性解释相结合。该框架使堆叠模型在保持预测准确性的同时，能够提供固有的可解释性，为负责任的机器学习提供实用且可扩展的解决方案。

Result: 在29个数据集上验证了框架的有效性，在学习空间的预测效果和结果模型的可解释性两个方面都取得了改进。XStacking成功地在保持预测性能的同时增强了模型的可解释性。

Conclusion: XStacking框架有效地解决了集成学习方法缺乏可解释性的关键问题，为实现既准确又可解释的机器学习模型提供了实用的解决方案，推进了负责任机器学习的发展。

Abstract: Ensemble Machine Learning (EML) techniques, especially stacking, have been
shown to improve predictive performance by combining multiple base models.
However, they are often criticized for their lack of interpretability. In this
paper, we introduce XStacking, an effective and inherently explainable
framework that addresses this limitation by integrating dynamic feature
transformation with model-agnostic Shapley additive explanations. This enables
stacked models to retain their predictive accuracy while becoming inherently
explainable. We demonstrate the effectiveness of the framework on 29 datasets,
achieving improvements in both the predictive effectiveness of the learning
space and the interpretability of the resulting models. XStacking offers a
practical and scalable solution for responsible ML.

</details>


### [94] [How Should We Meta-Learn Reinforcement Learning Algorithms?](https://arxiv.org/abs/2507.17668)
*Alexander David Goldie,Zilin Wang,Jakob Nicolaus Foerster,Shimon Whiteson*

Main category: cs.LG

TL;DR: 本文对不同元学习算法在强化学习中的应用进行了首次全面的实证比较，包括进化优化和大语言模型代码生成等方法，并提出了元学习新RL算法的指导原则。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法多数从监督或无监督学习改编而来，存在次优性问题。虽然元学习在RL中显示出潜力，但缺乏对不同元学习算法（如进化优化黑盒函数vs大语言模型生成代码）的系统性比较研究。

Method: 对多种元学习算法进行实证比较，这些算法针对RL流水线的不同部分。评估指标包括元训练和元测试性能，以及可解释性、样本成本和训练时间等因素。

Result: 通过全面的实验比较，揭示了不同元学习方法在强化学习任务中的性能表现和各自的优缺点，为选择合适的元学习算法提供了实证依据。

Conclusion: 基于实验发现，提出了几个用于元学习新RL算法的指导原则，这些原则有助于确保未来学习到的算法能够达到尽可能好的性能。

Abstract: The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.

</details>


### [95] [Generalized Dual Discriminator GANs](https://arxiv.org/abs/2507.17684)
*Penukonda Naga Chandana,Tejas Srivastava,Gowtham R. Kurri,V. Lalitha*

Main category: cs.LG

TL;DR: 本文提出了双判别器α-GANs（D2α-GANs）和广义双判别器GANs，通过结合双判别器架构与可调损失函数来解决模式坍塌问题，并在理论上证明了优化目标可简化为f-散度和逆f-散度的线性组合。


<details>
  <summary>Details</summary>
Motivation: 现有的双判别器GANs（D2GANs）虽然能缓解模式坍塌问题，但缺乏损失函数的灵活性。作者希望结合双判别器的优势与可调损失函数的灵活性，进一步推广到更广泛的函数类别，以获得更好的生成效果。

Method: 首先引入双判别器α-GANs（D2α-GANs），将双判别器架构与α-loss可调损失函数结合；然后将此方法推广到定义在正实数上的任意函数，形成广义双判别器GANs；对每个提出的模型进行理论分析，证明min-max优化可简化为f-散度和逆f-散度的线性组合。

Result: 理论分析表明，提出的模型的优化目标可以简化为f-散度和逆f-散度的线性组合，这推广了D2-GANs中KL散度和逆KL散度线性组合的已知简化结果。在2D合成数据上的实验验证了方法的有效性，使用多个性能指标展示了所提GANs的各种优势。

Conclusion: 双判别器α-GANs和广义双判别器GANs成功结合了双判别器架构的优势与损失函数的灵活性，理论分析揭示了其与f-散度的深层联系，实验结果证明了方法在解决模式坍塌问题方面的有效性。

Abstract: Dual discriminator generative adversarial networks (D2 GANs) were introduced
to mitigate the problem of mode collapse in generative adversarial networks. In
D2 GANs, two discriminators are employed alongside a generator: one
discriminator rewards high scores for samples from the true data distribution,
while the other favors samples from the generator. In this work, we first
introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines
the strengths of dual discriminators with the flexibility of a tunable loss
function, $\alpha$-loss. We further generalize this approach to arbitrary
functions defined on positive reals, leading to a broader class of models we
refer to as generalized dual discriminator generative adversarial networks. For
each of these proposed models, we provide theoretical analysis and show that
the associated min-max optimization reduces to the minimization of a linear
combination of an $f$-divergence and a reverse $f$-divergence. This generalizes
the known simplification for D2-GANs, where the objective reduces to a linear
combination of the KL-divergence and the reverse KL-divergence. Finally, we
perform experiments on 2D synthetic data and use multiple performance metrics
to capture various advantages of our GANs.

</details>


### [96] [Towards Effective Open-set Graph Class-incremental Learning](https://arxiv.org/abs/2507.17687)
*Jiazhen Chen,Zheng Ma,Sichao Fu,Mingbin Feng,Tony S. Wirjanto,Weihua Ou*

Main category: cs.LG

TL;DR: 提出了OGCIL框架，解决开放集图类增量学习中的灾难性遗忘和未知类检测问题，通过原型条件变分自编码器生成伪样本嵌入和基于混合的策略生成分布外样本，实现对已知类知识的保持和未知类的鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 现有的图类增量学习方法主要关注封闭集假设，假设所有测试样本都属于已知类别，这限制了它们在真实世界场景中的适用性。在实际应用中，未知类别会在推理过程中自然出现，但在训练时却不存在。因此需要解决开放集图类增量学习中的两个相互关联的挑战：对旧类的灾难性遗忘和不充分的开放集识别。

Method: 提出OGCIL框架，包含三个关键组件：1）设计原型条件变分自编码器来合成旧类的节点嵌入，实现知识重放而无需存储原始图数据；2）采用基于混合的策略，从伪分布内和当前节点嵌入生成分布外(OOD)样本来处理未知类；3）提出新颖的原型超球分类损失，将分布内嵌入锚定到各自的类原型，同时排斥OOD嵌入，通过原型感知的拒绝区域将未知样本明确建模为异常值。

Result: 在五个基准数据集上进行的广泛实验表明，OGCIL相比现有的图类增量学习和开放集图神经网络方法具有显著的有效性，能够有效缓解灾难性遗忘并实现对未知类的鲁棒检测。

Conclusion: OGCIL框架成功解决了开放集图类增量学习中的核心挑战，通过伪样本嵌入生成有效缓解了灾难性遗忘，并通过原型超球分类损失实现了对未知类的鲁棒识别。该方法为图神经网络在动态环境中的实际应用提供了更实用的解决方案。

Abstract: Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.

</details>


### [97] [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/abs/2507.17692)
*Jialiang Wang,Xianming Liu,Xiong Zhou,Gangfeng Hu,Deming Zhai,Junjun Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了联合非对称损失(JAL)框架，通过将新的非对称均方误差(AMSE)损失函数集成到主动被动损失(APL)中，有效解决了带噪声标签学习中的欠拟合问题，在标签噪声缓解方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有对称损失函数在处理噪声标签时存在欠拟合问题，而理论分析表明非对称损失具有更优的性质，但现有非对称损失无法与APL等先进优化框架兼容，限制了其应用潜力。

Method: 将非对称损失扩展到更复杂的被动损失场景，提出了新的非对称均方误差(AMSE)损失函数，并严格建立了AMSE满足非对称条件的充要条件。通过用AMSE替换APL中的传统对称被动损失，构建了联合非对称损失(JAL)框架。

Result: 广泛的实验证明了该方法在缓解标签噪声方面的有效性，JAL框架相比现有方法在带噪声标签学习任务中表现更优。

Conclusion: 通过将非对称损失理论与APL框架结合，成功开发了JAL框架，为处理噪声标签问题提供了新的解决方案，既保持了非对称损失的优越性质，又避免了对称损失的欠拟合问题。

Abstract: Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss

</details>


### [98] [HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging](https://arxiv.org/abs/2507.17706)
*Taha Ceritli,Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: HydraOpt是一种新的模型合并技术，通过利用低秩适配器矩阵间的相似性，在显著减少存储需求（减少48%）的同时保持竞争性能（仅下降0.2-1.8%），解决了为每个任务存储单独适配器的内存挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型使用适配器（如低秩适配器）在下游任务上表现良好，但为每个任务存储单独的适配器会显著增加内存需求，这在移动设备等资源受限环境中构成挑战。现有的模型合并技术虽能减少存储成本，但通常导致性能大幅下降。

Method: HydraOpt是一种新的模型合并技术，它利用低秩适配器矩阵之间的内在相似性。与现有方法产生存储大小和性能之间固定权衡不同，HydraOpt允许在效率和性能的范围内进行灵活导航。

Result: 实验表明，与存储所有适配器相比，HydraOpt显著减少了存储大小（减少48%），同时实现了竞争性性能（仅下降0.2-1.8%）。在相同或略差的存储效率下，它在性能方面优于现有的合并技术。

Conclusion: HydraOpt成功解决了适配器存储的内存挑战，通过利用低秩适配器矩阵的相似性，在大幅减少存储需求的同时保持了良好的性能，为资源受限环境中的大语言模型部署提供了有效解决方案。

Abstract: Large language models (LLMs) often leverage adapters, such as low-rank-based
adapters, to achieve strong performance on downstream tasks. However, storing a
separate adapter for each task significantly increases memory requirements,
posing a challenge for resource-constrained environments such as mobile
devices. Although model merging techniques can reduce storage costs, they
typically result in substantial performance degradation. In this work, we
introduce HydraOpt, a new model merging technique that capitalizes on the
inherent similarities between the matrices of low-rank adapters. Unlike
existing methods that produce a fixed trade-off between storage size and
performance, HydraOpt allows us to navigate this spectrum of efficiency and
performance. Our experiments show that HydraOpt significantly reduces storage
size (48% reduction) compared to storing all adapters, while achieving
competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing
merging techniques in terms of performance at the same or slightly worse
storage efficiency.

</details>


### [99] [On the Interaction of Compressibility and Adversarial Robustness](https://arxiv.org/abs/2507.17725)
*Melih Barsbey,Antônio H. Ribeiro,Umut Şimşekli,Tolga Birdal*

Main category: cs.LG

TL;DR: 本文研究了神经网络压缩性与对抗鲁棒性之间的根本冲突，发现不同形式的压缩会在表示空间中产生高度敏感的方向，使对手能够构造有效的扰动攻击


<details>
  <summary>Details</summary>
Motivation: 现代神经网络需要同时满足多个要求：准确拟合训练数据、泛化到未见输入、参数和计算效率以及对抗扰动的鲁棒性。尽管压缩性和鲁棒性各自被广泛研究，但对它们相互作用的统一理解仍然缺乏

Method: 开发了一个原则性框架来分析不同形式的压缩性（如神经元级稀疏性和谱压缩性）如何影响对抗鲁棒性。分析了这些压缩形式如何在表示空间中诱导少数高度敏感的方向，并推导出简单而有指导意义的鲁棒性界限

Result: 理论分析表明压缩会产生敏感方向供对手利用，且这种脆弱性与压缩实现方式无关。通过合成和现实任务的实证评估证实了理论预测，发现这些脆弱性在对抗训练和迁移学习下持续存在，并导致通用对抗扰动的出现

Conclusion: 研究揭示了结构化压缩性与鲁棒性之间存在根本张力，为设计既高效又安全的模型提供了新的路径。无论通过正则化、架构偏置还是隐式学习动力学实现压缩，都会产生可被对手利用的脆弱性

Abstract: Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.

</details>


### [100] [Flow Matching Meets Biology and Life Science: A Survey](https://arxiv.org/abs/2507.17731)
*Zihao Li,Zhichen Zeng,Xiao Lin,Feihao Fang,Yanru Qu,Zhe Xu,Zhining Liu,Xuying Ning,Tianxin Wei,Ge Liu,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: 本文是首个关于流匹配(flow matching)在生物学领域应用的综合性综述，系统回顾了流匹配的基础理论及其在生物序列建模、分子生成设计、肽和蛋白质生成等三大生物学领域的最新进展。


<details>
  <summary>Details</summary>
Motivation: 随着生成对抗网络、掩码自编码器和扩散模型等生成建模技术的发展，这些方法在分子设计、蛋白质生成、药物发现等生物学研究中取得了重大突破。流匹配作为扩散模型的强大且高效的替代方案，在生物学和生命科学领域的应用日益增长，但缺乏系统性的综述研究。

Method: 作者采用综合性文献综述的方法，首先系统回顾流匹配的基础理论和变体，然后将其在生物学领域的应用分为三个主要类别：生物序列建模、分子生成和设计、肽和蛋白质生成，并对每个领域的最新进展进行深入分析。

Result: 文章提供了流匹配在生物学领域应用的全面概览，总结了常用的数据集和软件工具，并建立了相应的资源库。同时识别出流匹配在三大生物学应用领域中的关键进展和技术特点。

Conclusion: 流匹配作为一种新兴的生成建模技术，在生物学领域展现出巨大潜力，为分子设计、蛋白质生成等关键生物学问题提供了新的解决方案。文章讨论了该领域的未来发展方向，并提供了开源资源以促进进一步研究。

Abstract: Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.

</details>


### [101] [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
*Anisha Gunjal,Anthony Wang,Elaine Lau,Vaskar Nath,Bing Liu,Sean Hendryx*

Main category: cs.LG

TL;DR: 本文提出了"评分标准作为奖励"(RaR)框架，通过使用结构化的检查清单式评分标准作为可解释的奖励信号来改进强化学习，在HealthBench-1k上相比传统李克特量表方法获得了28%的相对提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务中强化学习需要平衡客观和主观评估标准，但许多任务缺乏明确的真实标准，难以为语言模型定义可靠的奖励信号。传统偏好方法依赖不透明且容易产生虚假关联的奖励函数。

Method: 引入"评分标准作为奖励"(RaR)框架，使用结构化的检查清单式评分标准作为可解释的奖励信号，结合GRPO进行在线策略训练。将评分标准视为结构化奖励信号。

Result: 最佳RaR方法在HealthBench-1k上相比简单李克特量表方法获得高达28%的相对改进，性能匹配或超越基于专家参考答案的奖励信号。小规模判断模型能更好地与人类偏好对齐。

Conclusion: RaR框架通过结构化评分标准提供了可解释的奖励信号，使小规模判断模型能够更好地与人类偏好对齐，并在不同模型规模上保持稳健性能，为强化学习提供了有效的改进方案。

Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.

</details>


### [102] [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/abs/2507.17748)
*Melih Barsbey,Lucas Prieto,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.LG

TL;DR: 本文发现大学习率能够同时实现模型对虚假相关性的鲁棒性和网络压缩性，并且在多种数据集、模型和优化器上都表现出色，揭示了大学习率在标准分类任务中成功的原因可能是其能够处理训练数据中隐藏的稀有虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型需要同时具备鲁棒性和资源效率两个重要特性，但同时实现这两个目标仍然是一个挑战。研究者希望找到一种简单有效的方法来同时提升模型的鲁棒性（对虚假相关性的抵抗能力）和可压缩性。

Method: 本文将大学习率作为同时实现鲁棒性和网络压缩性的促进因子。研究方法包括：1）在多种虚假相关数据集上测试大学习率的效果；2）分析大学习率对表征属性的影响，包括不变特征利用、类别分离和激活稀疏性；3）将大学习率与其他超参数和正则化方法进行比较；4）在不同模型和优化器上验证大学习率的一致性效果。

Result: 研究结果表明：1）大学习率能够产生理想的表征特性，包括不变特征利用、类别分离和激活稀疏性；2）在满足鲁棒性和压缩性方面，大学习率比其他超参数和正则化方法表现更好且更一致；3）大学习率在多种虚假相关数据集、模型和优化器上都显示出积极效果；4）提供了强有力的证据表明大学习率在标准分类任务中的成功很可能是由于其处理训练数据中隐藏/稀有虚假相关性的能力。

Conclusion: 大学习率是一个简单而有效的解决方案，能够同时实现机器学习模型的鲁棒性和资源效率。它不仅在对抗虚假相关性方面表现出色，还能促进网络压缩。更重要的是，这一发现为理解大学习率在标准分类任务中的成功提供了新的视角，即其能够处理训练数据中潜在的虚假相关性问题。

Abstract: Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [103] [High-dimensional multidisciplinary design optimization for aircraft eco-design / Optimisation multi-disciplinaire en grande dimension pour l'éco-conception avion en avant-projet](https://arxiv.org/abs/2402.04711)
*Paul Saves*

Main category: math.OC

TL;DR: 本博士论文提出了一种高效的方法来优化涉及大量混合整数设计变量（通常100个变量）的多学科黑盒模型，通过改进EGO（基于高斯过程的贝叶斯优化）方法，结合参数化工具如偏最小二乘回归来降维，并将贝叶斯优化适配到离散变量和高维空间，最终应用于创新飞机概念如"DRAGON"混合动力飞机的优化设计以减少气候影响。


<details>
  <summary>Details</summary>
Motivation: 现有的EGO全局优化方法在处理高维问题时存在维数灾难问题，特别是在涉及连续和分类变量的现实飞机设计问题中，设计变量数量通常超过100个，直接使用EGO方法无法有效求解这类大规模混合整数约束优化问题。

Method: 研究有效的参数化工具（包括偏最小二乘回归等技术）来显著减少设计变量数量；将贝叶斯优化方法适配到离散变量和高维空间；基于自适应代理模型的序列丰富化方法，使用高斯过程代理模型来近似耗时的高保真度模型。

Result: 成功将改进的贝叶斯优化方法应用于创新飞机概念优化，特别是"DRAGON"混合动力飞机的设计优化，能够在减少评估次数的同时处理高维混合整数设计空间，实现了对复杂航空器设计问题的有效求解。

Conclusion: 通过结合降维技术和改进的贝叶斯优化方法，成功解决了高维混合整数约束优化问题中的维数灾难，为复杂航空器设计提供了有效的优化工具，并在减少气候影响的创新飞机概念设计中得到了实际应用验证。

Abstract: The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an
efficient approach for optimizing a multidisciplinary black-box model when the
optimization problem is constrained and involves a large number of mixed
integer design variables (typically 100 variables). The targeted optimization
approach, called EGO, is based on a sequential enrichment of an adaptive
surrogate model and, in this context, GP surrogate models are one of the most
widely used in engineering problems to approximate time-consuming high fidelity
models. EGO is a heuristic BO method that performs well in terms of solution
quality. However, like any other global optimization method, EGO suffers from
the curse of dimensionality, meaning that its performance is satisfactory on
lower dimensional problems, but deteriorates as the dimensionality of the
optimization search space increases. For realistic aircraft design problems,
the typical size of the design variables can even exceed 100 and, thus, trying
to solve directly the problems using EGO is ruled out. The latter is especially
true when the problems involve both continuous and categorical variables
increasing even more the size of the search space. In this Ph.D thesis,
effective parameterization tools are investigated, including techniques like
partial least squares regression, to significantly reduce the number of design
variables. Additionally, Bayesian optimization is adapted to handle discrete
variables and high-dimensional spaces in order to reduce the number of
evaluations when optimizing innovative aircraft concepts such as the "DRAGON"
hybrid airplane to reduce their climate impact.

</details>


### [104] [Scalable DC Optimization via Adaptive Frank-Wolfe Algorithms](https://arxiv.org/abs/2507.17545)
*Sebastian Pokutta*

Main category: math.OC

TL;DR: 本文研究在紧致凸集上最小化光滑凸函数差的问题，通过结合Blended Pairwise Conditional Gradients算法与热启动和自适应误差界，提出了一种高效可扩展的无投影算法来解决约束DC优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统的约束DC（差凸）优化问题计算开销较大，需要开发更高效的算法来减少计算复杂度，特别是在处理大规模问题时需要无投影的可扩展解决方案。

Method: 将Blended Pairwise Conditional Gradients (BPCG)算法与热启动技术和Maskan等人提出的自适应误差界相结合，构建了一个基于Frank-Wolfe变体的框架来解决约束DC优化问题。

Result: 实证研究表明，所提出的算法组合能够高效求解约束DC问题，实现了计算开销的显著降低，并且具有良好的可扩展性。

Conclusion: 通过整合先进的Frank-Wolfe变体，成功开发出一种高效且可扩展的无投影算法，为约束DC优化提供了实用的解决方案，在保持算法性能的同时大幅减少了计算复杂度。

Abstract: We consider the problem of minimizing a difference of (smooth) convex
functions over a compact convex feasible region $P$, i.e., $\min_{x \in P} f(x)
- g(x)$, with smooth $f$ and Lipschitz continuous $g$. This computational study
builds upon and complements the framework of Maskan et al. [2025] by
integrating advanced Frank-Wolfe variants to reduce computational overhead. We
empirically show that constrained DC problems can be efficiently solved using a
combination of the Blended Pairwise Conditional Gradients (BPCG) algorithm
[Tsuji et al., 2022] with warm-starting and the adaptive error bound from
Maskan et al. [2025]. The result is a highly efficient and scalable
projection-free algorithm for constrained DC optimization.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [105] [Avoiding spectral pollution for transfer operators using residuals](https://arxiv.org/abs/2507.16915)
*April Herwig,Matthew J. Colbrook,Oliver Junge,Péter Koltai,Julia Slipantschuk*

Main category: math.DS

TL;DR: 本文提出了计算传递算子频谱性质的新算法，解决了有限维近似中的频谱污染问题，并扩展到Hardy-Hilbert空间，通过多个案例研究验证了方法的准确性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的Koopman算子和传递算子的有限维近似容易产生频谱污染，引入虚假特征值，影响频谱计算的准确性。虽然Koopman算子已有可证明收敛的方法，但一般传递算子缺乏类似的工具。

Method: 开发了无频谱污染的传递算子频谱性质计算算法，包括扩展到Hardy-Hilbert空间的方法。通过函数解析的方法定义"真实"的Koopman频谱，处理特征函数位于选择空间之外时的频谱特征问题。

Result: 通过多个案例研究验证了方法的有效性，包括已知频谱的Blaschke映射族和蛋白质折叠的分子动力学模型。成功展示了即使相应的特征函数位于所选空间之外，频谱特征仍可能出现的现象。

Conclusion: 提出的方法为广泛应用领域的频谱估计提供了鲁棒的工具，有效解决了传递算子频谱计算中的污染问题，并揭示了定义"真实"Koopman频谱时的函数解析微妙之处。

Abstract: Koopman operator theory enables linear analysis of nonlinear dynamical
systems by lifting their evolution to infinite-dimensional function spaces.
However, finite-dimensional approximations of Koopman and transfer
(Frobenius--Perron) operators are prone to spectral pollution, introducing
spurious eigenvalues that can compromise spectral computations. While recent
advances have yielded provably convergent methods for Koopman operators,
analogous tools for general transfer operators remain limited. In this paper,
we present algorithms for computing spectral properties of transfer operators
without spectral pollution, including extensions to the Hardy-Hilbert space.
Case studies--ranging from families of Blaschke maps with known spectrum to a
molecular dynamics model of protein folding--demonstrate the accuracy and
flexibility of our approach. Notably, we demonstrate that spectral features can
arise even when the corresponding eigenfunctions lie outside the chosen space,
highlighting the functional-analytic subtleties in defining the "true" Koopman
spectrum. Our methods offer robust tools for spectral estimation across a broad
range of applications.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [106] [Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS and ScaNN](https://arxiv.org/abs/2507.16978)
*Mohammad Saleh Refahi,Gavin Hearne,Harrison Muller,Kieran Lynch,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: q-bio.GN

TL;DR: 本文探索了基于嵌入的DNA序列相似性搜索方法，系统评估了FAISS和ScaNN两个向量搜索库在生物学基因嵌入上的性能，相比传统的BLAST等工具在计算效率和检索质量上都有显著提升


<details>
  <summary>Details</summary>
Motivation: DNA测序数据呈指数级增长，传统基于启发式的方法（如BLAST）在扩展性、计算成本和处理分化序列方面存在局限性，急需高效的计算方法来支持大规模相似性搜索这一生物信息学基础任务

Method: 采用基于嵌入的相似性搜索方法，学习能够捕获深层结构和功能模式的潜在表示，系统评估FAISS和ScaNN两个最先进的向量搜索库在生物学有意义的基因嵌入上的性能，重点关注生物信息学特定嵌入及其在检测新序列方面的效用

Result: 结果显示基于嵌入的方法在计算方面具有优势（内存和运行时效率更高），同时检索质量也有改善，为传统的重对齐工具提供了有前景的替代方案

Conclusion: 基于嵌入的相似性搜索方法相比传统方法在处理大规模DNA序列数据时表现出更好的计算效率和检索性能，特别是在检测来自未表征分类群或缺乏已知同源物的新序列方面具有优势

Abstract: The exponential growth of DNA sequencing data has outpaced traditional
heuristic-based methods, which struggle to scale effectively. Efficient
computational approaches are urgently needed to support large-scale similarity
search, a foundational task in bioinformatics for detecting homology,
functional similarity, and novelty among genomic and proteomic sequences.
Although tools like BLAST have been widely used and remain effective in many
scenarios, they suffer from limitations such as high computational cost and
poor performance on divergent sequences.
  In this work, we explore embedding-based similarity search methods that learn
latent representations capturing deeper structural and functional patterns
beyond raw sequence alignment. We systematically evaluate two state-of-the-art
vector search libraries, FAISS and ScaNN, on biologically meaningful gene
embeddings. Unlike prior studies, our analysis focuses on
bioinformatics-specific embeddings and benchmarks their utility for detecting
novel sequences, including those from uncharacterized taxa or genes lacking
known homologs. Our results highlight both computational advantages (in memory
and runtime efficiency) and improved retrieval quality, offering a promising
alternative to traditional alignment-heavy tools.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 本文研究了在本体上的受控查询评估(CQE)问题，通过结合认知依赖(EDs)和最优GA审查器来安全地回答布尔合取查询联合(BUCQs)，并提出了一种基于交集的方法来确保强安全保证和良好的计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有的受控查询评估框架需要在保护敏感信息的同时允许安全的信息披露。传统方法在安全性和计算效率之间存在权衡问题，需要一种既能提供强安全保证又具有良好计算性能的新方法。

Method: 将认知依赖(EDs)与最优GA审查器相结合，采用基于所有最优GA审查器交集的方法来回答布尔合取查询联合(BUCQs)。针对EDs的子类和DL-Lite_R本体，设计了详细的一阶重写算法来实现AC^0数据复杂度的查询回答。

Result: 证明了基于交集的方法对于完全EDs类别是安全的；对于EDs子类和DL-Lite_R本体，BUCQ回答的数据复杂度为AC^0；通过两种不同评估场景的实验验证了重写函数的实际可行性。

Conclusion: 提出的基于交集的CQE方法成功地在保证强安全性的同时实现了良好的计算性能，为本体上的受控查询评估提供了一个理论上可靠且实践上可行的解决方案。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [108] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 本文提出将AI集成到车辆中作为感知平台，实现从被动维护到主动维护的转变，并开发能够同时与机器和驾驶员交流的AI副驾驶系统


<details>
  <summary>Details</summary>
Motivation: 传统车辆维护模式是被动响应式的，缺乏预测性和智能化。随着AI技术的发展，有必要将车辆转变为智能感知平台，实现主动式预测性维护，并建立人机协作的智能交互系统

Method: 将AI技术集成到车辆系统中，开发具备双语能力的AI副驾驶系统，使其能够同时理解机器语言（传感器数据、系统状态等）和人类语言（驾驶员指令、需求等），构建智能车辆感知平台

Result: 提出了智能车辆系统的概念框架和技术视角，为跨学科对话提供了基础，并为未来智能车辆系统、预测性维护和AI驱动的用户交互研究指明了方向

Conclusion: AI副驾驶系统的集成将彻底改变车辆维护模式，从被动响应转向主动预测，同时实现更智能的人机交互。这一技术发展将推动智能车辆系统、预测性维护和AI用户交互领域的进步

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [109] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型(LLM)与实时优化算法的共生智能体范式，用于构建可信赖的6G网络自主管理系统，实现了从专用AI向通用人工智能(AGI)驱动网络的转变。


<details>
  <summary>Details</summary>
Motivation: 当前6G网络需要从处理孤立任务的专用AI算法转向具备更广泛推理能力、能管理多样化网络功能的AGI驱动网络，以实现实时决策和端用户服务供应的自主管理。

Method: 设计了一种新颖的共生智能体范式，将LLM与实时优化算法相结合：输入层优化器为数值精确任务提供有界不确定性引导，输出层优化器在LLM监督下实现自适应实时控制。实现了两种智能体类型：无线接入网优化器和服务级别协议多智能体协商器。

Result: 在5G测试平台上的实验显示，共生智能体相比独立LLM智能体减少了5倍的决策错误；小语言模型(SLM)在保持相似精度的同时，GPU资源开销减少99.9%，实现了82毫秒的近实时循环；多智能体协作RAN演示显示RAN过度利用率降低约44%。

Conclusion: 共生范式为下一代AGI驱动的网络系统奠定了基础，这些系统即使在LLM不断发展的情况下也能保持适应性、高效性和可信赖性，为6G网络的智能化管理提供了有效解决方案。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [110] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 提出了Ctx2TrajGen框架，这是一个基于GAIL的上下文感知轨迹生成模型，能够合成真实的城市驾驶行为，解决了微观交通建模中的非线性相互依赖和训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对交通行为分析和自动驾驶系统至关重要，现有方法存在数据稀缺和领域偏移问题，需要一个能够生成真实、多样化且符合上下文的轨迹生成方法

Method: 提出Ctx2TrajGen框架，结合GAIL（生成对抗模仿学习）、PPO（近端策略优化）和WGAN-GP（带梯度惩罚的Wasserstein生成对抗网络），通过明确调节周围车辆和道路几何信息来生成交互感知的轨迹

Result: 在无人机捕获的DRIFT数据集上的实验表明，该方法在真实性、行为多样性和上下文保真度方面均优于现有方法，有效解决了数据稀缺和领域偏移问题

Conclusion: Ctx2TrajGen为微观交通轨迹生成提供了一个鲁棒的解决方案，能够在不依赖仿真的情况下生成高质量的上下文感知轨迹，为交通行为分析和自动驾驶系统提供了有价值的工具

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [111] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 该研究系统性地探索了带有可验证奖励的强化学习(RLVR)框架下的多领域推理能力，重点关注数学推理、代码生成和逻辑谜题求解三个主要领域，通过广泛实验揭示了领域间相互作用的动态机制和影响专业化与通用推理性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在孤立的推理领域（如数学问题求解、编程任务或逻辑推理），但现实世界的推理场景本质上需要多种认知技能的综合应用。然而，强化学习框架下这些推理技能之间的相互作用仍然理解不足，需要系统性研究多领域推理在RLVR框架中的表现。

Method: 采用GRPO算法和Qwen-2.5-7B模型系列，进行四个关键组成部分的综合研究：(1)评估单领域数据集训练时模型的领域内改进和跨领域泛化能力；(2)检查联合跨领域训练中出现的相互增强和冲突等复杂交互；(3)分析比较基础模型和指令模型在相同RL配置下的性能差异；(4)深入探讨课程学习策略、奖励设计变化和语言特定因素等关键RL训练细节的影响。

Result: 通过广泛实验，研究结果对领域交互的动态机制提供了重要洞察，揭示了影响专业化和通用推理性能的关键因素。实验涵盖了单领域训练的跨领域泛化效果、多领域联合训练中的相互作用模式、不同模型基础对RL效果的影响，以及各种训练策略对性能的具体影响。

Conclusion: 研究为优化强化学习方法提供了宝贵指导，以促进大语言模型的全面多领域推理能力发展。通过系统性分析领域间相互作用和关键影响因素，为构建更强大的多领域推理系统奠定了理论和实践基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [112] [Learning from Scratch: Structurally-masked Transformer for Next Generation Lib-free Simulation](https://arxiv.org/abs/2507.17396)
*Junlang Huang,Hao Chen,Zhong Guan*

Main category: eess.SP

TL;DR: 本文提出了一个基于神经网络的多级数据路径功耗和时序预测框架，使用预训练的波形预测和延迟估算模型，直接从SPICE网表推断瞬态波形和传播延迟，在工业电路上达到SPICE级精度。


<details>
  <summary>Details</summary>
Motivation: 传统的基于库的分析方法依赖于驱动器特性描述和负载简化，存在精度限制。现有方法难以准确捕获本征延迟和耦合引起的延迟效应，且需要简化或插值处理。因此需要一种更精确、可扩展的神经网络替代方案。

Method: 采用混合CNN-Transformer架构和网表感知的节点级编码；使用两个预训练神经模型进行波形预测和延迟估算；实施递归传播策略，将每级预测的波形输入到后续级；专门的子网络分别处理主要延迟估算和串扰校正；基于负载电容、输入斜率、门尺寸等关键物理参数。

Result: 在多样化工业电路上实现SPICE级精度，RMSE始终低于0.0098；能够准确捕获本征和耦合引起的延迟效应；确保复杂信号路径中的精确时序对齐和完整波形可见性；提供可扩展、结构自适应的解决方案。

Conclusion: 提出的神经框架为传统功耗和时序引擎提供了高保真度的替代方案，能够准确反映物理电路行为，在多级数据路径的功耗和时序预测方面表现出色，为集成电路设计提供了新的分析工具。

Abstract: This paper proposes a neural framework for power and timing prediction of
multi-stage data path, distinguishing itself from traditional lib-based
analytical methods dependent on driver characterization and load
simplifications. To the best of our knowledge, this is the first
language-based, netlist-aware neural network designed explicitly for standard
cells. Our approach employs two pre-trained neural models of waveform
prediction and delay estimation that directly infer transient waveforms and
propagation delays from SPICE netlists, conditioned on critical physical
parameters such as load capacitance, input slew, and gate size. This method
accurately captures both intrinsic and coupling-induced delay effects without
requiring simplification or interpolation. For multi-stage timing prediction,
we implement a recursive propagation strategy where predicted waveforms from
each stage feed into subsequent stages, cumulatively capturing delays across
the logic chain. This approach ensures precise timing alignment and complete
waveform visibility throughout complex signal pathways. The waveform prediction
utilizes a hybrid CNN-Transformer architecture with netlist-aware node-level
encoding, addressing traditional Transformers' fixed input dimensionality
constraints. Additionally, specialized subnetworks separately handle primary
delay estimation and crosstalk correction. Experimental results demonstrate
SPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse
industrial circuits. The proposed framework provides a scalable, structurally
adaptable neural alternative to conventional power and timing engines,
demonstrating high fidelity to physical circuit behaviors.

</details>


### [113] [Joint Multi-Target Detection-Tracking in Cognitive Massive MIMO Radar via POMCP](https://arxiv.org/abs/2507.17506)
*Imad Bouhou,Stefano Fortunati,Leila Gharsalli,Alexandre Renaux*

Main category: eess.SP

TL;DR: 本文提出了一种基于POMCP的认知雷达框架，通过自适应波形设计和功率分配来实现多目标检测与跟踪，特别针对不同信噪比目标的检测性能进行了优化。


<details>
  <summary>Details</summary>
Motivation: 传统的均匀功率分配在面对不同信噪比目标时往往次优，需要开发一种能够自适应分配发射能量的认知雷达系统，以提高弱目标或远距离目标的可检测性，同时确保高信噪比目标有足够的功率。

Method: 将单目标POMCP算法扩展到多目标场景，为每个目标分配独立的POMCP树；基于目标的估计距离和雷达散射截面预测未来角位置和期望接收功率；通过约束优化问题进行自适应波形设计；修改POMDP中的奖励函数以优先考虑空间和功率估计的准确性。

Result: 仿真结果显示，相比使用均匀或正交波形的方法，该框架提高了低信噪比目标的检测概率，实现了更准确的跟踪性能。多目标不同信噪比场景下的仿真验证了方法的有效性。

Conclusion: 基于POMCP的认知雷达框架在自适应、高效的多目标雷达系统中展现出巨大潜力，能够有效改善多目标检测与跟踪性能，特别是在处理不同信噪比目标时具有明显优势。

Abstract: This correspondence presents a power-aware cognitive radar framework for
joint detection and tracking of multiple targets in a massive multiple-input
multiple-output (MIMO) radar environment. Building on a previous single-target
algorithm based on Partially Observable Monte Carlo Planning (POMCP), we extend
it to the multi-target case by assigning each target an independent POMCP tree,
enabling scalable and efficient planning.
  Departing from uniform power allocation-which is often suboptimal with
varying signal-to-noise ratios (SNRs)-our approach predicts each target's
future angular position and expected received power, based on its estimated
range and radar cross-section (RCS). These predictions guide adaptive waveform
design via a constrained optimization problem that allocates transmit energy to
enhance the detectability of weaker or distant targets, while ensuring
sufficient power for high-SNR targets. The reward function in the underlying
partially observable Markov decision process (POMDP) is also modified to
prioritize accurate spatial and power estimation.
  Simulations involving multiple targets with different SNRs confirm the
effectiveness of our method. The proposed framework for the cognitive radar
improves detection probability for low-SNR targets and achieves more accurate
tracking compared to approaches using uniform or orthogonal waveforms. These
results demonstrate the potential of the POMCP-based framework for adaptive,
efficient multi-target radar systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [114] [Shared Control of Holonomic Wheelchairs through Reinforcement Learning](https://arxiv.org/abs/2507.17055)
*Jannis Bähler,Diego Paez-Granados,Jorge Peña-Queralta*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的智能电动轮椅共享控制方法，能够将用户的2D输入转换为3D运动，同时确保用户舒适度并减少认知负荷，实现了全向移动平台的首个真实世界强化学习共享控制实现。


<details>
  <summary>Details</summary>
Motivation: 现有的全向系统共享控制方法往往导致用户体验不直观，且无法充分利用全向驾驶的潜力。虽然非全向机器人的共享控制在提高导航安全性方面已显示出潜力，但全向系统仍缺乏有效的解决方案。

Method: 提出了一种基于强化学习的共享控制方法，该方法接收2D用户输入并输出3D运动。在Isaac Gym中训练RL智能体，在Gazebo中进行仿真测试，比较了不同的RL智能体架构和基于认知负荷与用户舒适度指标的奖励函数。

Result: 该方法确保了无碰撞导航，能够智能地调整轮椅方向，与之前的非学习方法相比显示出更好或具有竞争力的平滑性。成功实现了从仿真到真实环境的迁移，并在真实世界中展示了全向移动平台的RL共享控制。

Conclusion: 成功开发了首个基于强化学习的全向电动轮椅共享控制系统，该系统能够在确保安全导航的同时提高用户舒适度并减少认知负荷，为智能辅助移动设备的发展提供了新的解决方案。

Abstract: Smart electric wheelchairs can improve user experience by supporting the
driver with shared control. State-of-the-art work showed the potential of
shared control in improving safety in navigation for non-holonomic robots.
However, for holonomic systems, current approaches often lead to unintuitive
behavior for the user and fail to utilize the full potential of omnidirectional
driving. Therefore, we propose a reinforcement learning-based method, which
takes a 2D user input and outputs a 3D motion while ensuring user comfort and
reducing cognitive load on the driver. Our approach is trained in Isaac Gym and
tested in simulation in Gazebo. We compare different RL agent architectures and
reward functions based on metrics considering cognitive load and user comfort.
We show that our method ensures collision-free navigation while smartly
orienting the wheelchair and showing better or competitive smoothness compared
to a previous non-learning-based method. We further perform a sim-to-real
transfer and demonstrate, to the best of our knowledge, the first real-world
implementation of RL-based shared control for an omnidirectional mobility
platform.

</details>


### [115] [Deformable Cluster Manipulation via Whole-Arm Policy Learning](https://arxiv.org/abs/2507.17085)
*Jayadeep Jacob,Wenzheng Zhang,Houston Warren,Paulo Borges,Tirthankar Bandyopadhyay,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文提出了一个结合3D点云和本体感觉触觉指示器的无模型强化学习框架，用于操控可变形物体集群，实现了全身接触感知的操作，并成功应用于电力线清理任务。


<details>
  <summary>Details</summary>
Motivation: 操控可变形物体集群是一个重大挑战，需要接触丰富的全臂交互。现有方法在真实模型合成能力有限、感知不确定性高、缺乏高效空间抽象等方面存在问题，传统的末端执行器模式无法满足需求。

Method: 提出了一个融合3D点云和本体感觉触觉指示器两种模态的无模型策略学习框架，强调具有全身接触感知的操作。采用分布式状态表示和核均值嵌入的强化学习框架，并提出了一种新的上下文无关遮挡启发式方法来清理目标区域的可变形物体。

Result: 在电力线清理场景中部署该框架，观察到智能体能够生成利用多个手臂链节进行去遮挡的创新策略。实现了零样本仿真到现实的策略迁移，使机械臂能够清理具有未知遮挡模式、未见拓扑结构和不确定动力学的真实树枝。

Conclusion: 该框架成功解决了可变形物体集群操控的关键挑战，通过多模态感知和分布式状态表示实现了高效的全身接触操作，并在实际应用中展现了良好的泛化能力和创新性策略生成能力。

Abstract: Manipulating clusters of deformable objects presents a substantial challenge
with widespread applicability, but requires contact-rich whole-arm
interactions. A potential solution must address the limited capacity for
realistic model synthesis, high uncertainty in perception, and the lack of
efficient spatial abstractions, among others. We propose a novel framework for
learning model-free policies integrating two modalities: 3D point clouds and
proprioceptive touch indicators, emphasising manipulation with full body
contact awareness, going beyond traditional end-effector modes. Our
reinforcement learning framework leverages a distributional state
representation, aided by kernel mean embeddings, to achieve improved training
efficiency and real-time inference. Furthermore, we propose a novel
context-agnostic occlusion heuristic to clear deformables from a target region
for exposure tasks. We deploy the framework in a power line clearance scenario
and observe that the agent generates creative strategies leveraging multiple
arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy
transfer, allowing the arm to clear real branches with unknown occlusion
patterns, unseen topology, and uncertain dynamics.

</details>


### [116] [JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction](https://arxiv.org/abs/2507.17152)
*Fangze Lin,Ying He,Fei Yu,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了一个两阶段多智能体交互预测框架JAM，通过分类感知的边际提议和关键点引导的联合预测来解决自动驾驶中低概率模式生成质量差的问题，在Waymo数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中多智能体联合预测存在低概率模式生成质量差的问题，现有方法难以有效学习所有类别的轨迹并捕获关键信息用于联合预测。

Method: 提出两阶段框架JAM：第一阶段进行边际预测，通过轨迹类型分类鼓励模型学习所有类别轨迹；第二阶段进行联合预测，结合场景上下文和边际提议学习最终联合分布，并引入关键路径点指导联合预测模块更好地捕获和利用初始预测轨迹的关键信息。

Result: 在Waymo开放运动数据集的交互预测基准测试中取得了竞争性能，框架对比实验显示JAM超越了其他预测框架，在交互轨迹预测中达到了最先进的性能。

Conclusion: 所提出的JAM框架通过两阶段设计和关键点引导有效解决了多智能体联合预测中的低质量生成问题，在实际数据集上验证了方法的有效性，为交互轨迹预测提供了新的解决方案。

Abstract: Predicting the future motion of road participants is a critical task in
autonomous driving. In this work, we address the challenge of low-quality
generation of low-probability modes in multi-agent joint prediction. To tackle
this issue, we propose a two-stage multi-agent interactive prediction framework
named \textit{keypoint-guided joint prediction after classification-aware
marginal proposal} (JAM). The first stage is modeled as a marginal prediction
process, which classifies queries by trajectory type to encourage the model to
learn all categories of trajectories, providing comprehensive mode information
for the joint prediction module. The second stage is modeled as a joint
prediction process, which takes the scene context and the marginal proposals
from the first stage as inputs to learn the final joint distribution. We
explicitly introduce key waypoints to guide the joint prediction module in
better capturing and leveraging the critical information from the initial
predicted trajectories. We conduct extensive experiments on the real-world
Waymo Open Motion Dataset interactive prediction benchmark. The results show
that our approach achieves competitive performance. In particular, in the
framework comparison experiments, the proposed JAM outperforms other prediction
frameworks and achieves state-of-the-art performance in interactive trajectory
prediction. The code is available at https://github.com/LinFunster/JAM to
facilitate future research.

</details>


### [117] [Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning](https://arxiv.org/abs/2507.17275)
*Po-Yen Wu,Cheng-Yu Kuo,Yuki Kadokawa,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本文提出了一种强化学习框架，通过将工具寿命作为策略优化因子，学习既能完成任务又能延长工具寿命的机器人工具使用策略，在仿真和真实环境中实现了高达8.01倍的工具寿命延长。


<details>
  <summary>Details</summary>
Motivation: 在不可达环境中，机器人使用通用工具时面临挑战：如何学习既能完成任务又能延长工具寿命的策略。通用工具缺乏预定义使用策略，其寿命对使用方式高度敏感，因此需要考虑工具寿命的智能使用策略。

Method: 提出融合工具寿命的强化学习框架：1）利用有限元分析(FEA)和Miner规则基于累积应力估计剩余使用寿命(RUL)；2）将RUL集成到RL奖励中指导策略学习；3）引入自适应奖励归一化(ARN)机制，根据估计的RUL动态调整奖励缩放，确保稳定的学习信号。

Result: 在仿真和真实世界的工具使用任务中验证方法有效性，包括物体移动和开门任务。学习的策略在仿真中可将工具寿命延长高达8.01倍，并能有效迁移到真实世界环境中，证明了寿命导向工具使用策略的实用价值。

Conclusion: 该框架成功解决了机器人在使用通用工具时的寿命优化问题，通过将工具寿命集成到强化学习中，实现了任务完成与工具保护的平衡，为机器人在复杂环境中的可持续工具使用提供了有效解决方案。

Abstract: In inaccessible environments with uncertain task demands, robots often rely
on general-purpose tools that lack predefined usage strategies. These tools are
not tailored for particular operations, making their longevity highly sensitive
to how they are used. This creates a fundamental challenge: how can a robot
learn a tool-use policy that both completes the task and prolongs the tool's
lifespan? In this work, we address this challenge by introducing a
reinforcement learning (RL) framework that incorporates tool lifespan as a
factor during policy optimization. Our framework leverages Finite Element
Analysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based
on accumulated stress, and integrates the RUL into the RL reward to guide
policy learning toward lifespan-guided behavior. To handle the fact that RUL
can only be estimated after task execution, we introduce an Adaptive Reward
Normalization (ARN) mechanism that dynamically adjusts reward scaling based on
estimated RULs, ensuring stable learning signals. We validate our method across
simulated and real-world tool use tasks, including Object-Moving and
Door-Opening with multiple general-purpose tools. The learned policies
consistently prolong tool lifespan (up to 8.01x in simulation) and transfer
effectively to real-world settings, demonstrating the practical value of
learning lifespan-guided tool use strategies.

</details>


### [118] [VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback](https://arxiv.org/abs/2507.17294)
*Jianxin Bi,Kevin Yuchen Ma,Ce Hao,Mike Zheng Shou,Harold Soh*

Main category: cs.RO

TL;DR: VLA-Touch是一种在不微调基础VLA模型的情况下，通过双层次触觉反馈集成来增强通用机器人策略的方法，包括用于高级任务规划的触觉-语言模型和用于接触丰富操作的基于扩散的控制器。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作(VLA)模型缺乏解释和使用触觉信号的能力，限制了它们在接触丰富任务中的有效性。触觉反馈对于与物理世界的有效交互至关重要，但由于缺乏大型多模态数据集，将触觉反馈整合到这些系统中具有挑战性。

Method: 提出了两个关键创新：(1)利用预训练的触觉-语言模型的管道，为高级任务规划提供语义触觉反馈；(2)基于扩散的控制器，使用触觉信号来细化VLA生成的动作，用于接触丰富的操作。该方法在不微调基础VLA的情况下增强通用机器人策略。

Result: 通过真实世界实验证明，双层次触觉反馈集成提高了任务规划效率，同时增强了执行精度。代码已开源。

Conclusion: VLA-Touch成功地将触觉感知整合到通用机器人策略中，在不需要微调基础VLA模型的情况下，通过语义触觉反馈和基于扩散的动作细化实现了任务规划和执行性能的双重提升。

Abstract: Tactile feedback is generally recognized to be crucial for effective
interaction with the physical world. However, state-of-the-art
Vision-Language-Action (VLA) models lack the ability to interpret and use
tactile signals, limiting their effectiveness in contact-rich tasks.
Incorporating tactile feedback into these systems is challenging due to the
absence of large multi-modal datasets. We present VLA-Touch, an approach that
enhances generalist robot policies with tactile sensing \emph{without
fine-tuning} the base VLA. Our method introduces two key innovations: (1) a
pipeline that leverages a pretrained tactile-language model that provides
semantic tactile feedback for high-level task planning, and (2) a
diffusion-based controller that refines VLA-generated actions with tactile
signals for contact-rich manipulation. Through real-world experiments, we
demonstrate that our dual-level integration of tactile feedback improves task
planning efficiency while enhancing execution precision. Code is open-sourced
at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.

</details>


### [119] [Confidence Calibration in Vision-Language-Action Models](https://arxiv.org/abs/2507.17383)
*Thomas P Zollo,Richard Zemel*

Main category: cs.RO

TL;DR: 这是第一个系统性研究视觉-语言-动作(VLA)基础模型中置信度校准的工作，提出了提示集成和动作维度Platt缩放等方法来提高机器人行为的可信度和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 可信赖的机器人行为不仅需要高任务成功率，还需要机器人能可靠地量化其成功概率。现有VLA模型在置信度校准方面缺乏系统性研究，这限制了其在实际应用中的可信度。

Method: 1) 对多个数据集和VLA变体进行广泛基准测试，分析任务成功率与校准误差的关系；2) 提出提示集成算法，通过对改写指令的置信度进行平均来改善校准；3) 分析任务时间轴上的校准情况；4) 提出动作维度Platt缩放方法，对每个动作维度独立重新校准。

Result: 发现任务性能与校准不存在冲突；提示集成算法能持续改善校准效果；置信度在取得一定进展后往往最可靠，为风险感知干预提供了自然的时机；不同动作维度存在差异化的误校准现象。

Conclusion: 通过开发必要的工具和概念理解，使VLA模型既具有高性能又具有高可信度，实现可靠的不确定性量化。该研究为构建更可信赖的机器人系统奠定了基础。

Abstract: Trustworthy robot behavior requires not only high levels of task success but
also that the robot can reliably quantify how likely it is to succeed. To this
end, we present the first systematic study of confidence calibration in
vision-language-action (VLA) foundation models, which map visual observations
and natural-language instructions to low-level robot motor commands. We begin
with extensive benchmarking to understand the critical relationship between
task success and calibration error across multiple datasets and VLA variants,
finding that task performance and calibration are not in tension. Next, we
introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm
that averages confidence across paraphrased instructions and consistently
improves calibration. We further analyze calibration over the task time
horizon, showing that confidence is often most reliable after making some
progress, suggesting natural points for risk-aware intervention. Finally, we
reveal differential miscalibration across action dimensions and propose
action-wise Platt scaling, a method to recalibrate each action dimension
independently to produce better confidence estimates. Our aim in this study is
to begin to develop the tools and conceptual understanding necessary to render
VLAs both highly performant and highly trustworthy via reliable uncertainty
quantification.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [120] [On Temporal Guidance and Iterative Refinement in Audio Source Separation](https://arxiv.org/abs/2507.17297)
*Tobias Morocutti,Jonathan Greif,Paul Primus,Florian Schmid,Gerhard Widmer*

Main category: cs.SD

TL;DR: 本文提出了一种新的空间语义音场分割(S5)方法，通过结合预训练Transformer进行音频标记和声音事件检测，以及迭代优化机制来改进声源分离性能，在DCASE Challenge 2025 Task 4中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 传统的空间语义音场分割系统采用两阶段流水线（音频标记+标签条件声源分离），但缺乏有效分离所需的细粒度时间信息，限制了系统性能。

Method: 1) 微调预训练Transformer进行活跃声音类别检测；2) 使用另一个微调Transformer实例进行声音事件检测(SED)，为分离模块提供详细的时变指导；3) 实现迭代优化机制，递归重用前一次迭代的分离器输出来逐步提升分离质量。

Result: 该系统在DCASE Challenge 2025的Task 4中获得第二名，在音频标记和声源分离性能方面都取得了显著改进。

Conclusion: 通过增强事件检测和声源分离阶段之间的协同作用，提出的方法有效解决了传统S5系统缺乏时间信息的局限性，显著提升了空间语义音场分割的整体性能。

Abstract: Spatial semantic segmentation of sound scenes (S5) involves the accurate
identification of active sound classes and the precise separation of their
sources from complex acoustic mixtures. Conventional systems rely on a
two-stage pipeline - audio tagging followed by label-conditioned source
separation - but are often constrained by the absence of fine-grained temporal
information critical for effective separation. In this work, we address this
limitation by introducing a novel approach for S5 that enhances the synergy
between the event detection and source separation stages. Our key contributions
are threefold. First, we fine-tune a pre-trained Transformer to detect active
sound classes. Second, we utilize a separate instance of this fine-tuned
Transformer to perform sound event detection (SED), providing the separation
module with detailed, time-varying guidance. Third, we implement an iterative
refinement mechanism that progressively enhances separation quality by
recursively reusing the separator's output from previous iterations. These
advancements lead to significant improvements in both audio tagging and source
separation performance, as demonstrated by our system's second-place finish in
Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints
are available in our GitHub repository: https://github.com/theMoro/dcase25task4 .

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [121] [SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure](https://arxiv.org/abs/2507.17064)
*Nafisa Anjum,Tasnuva Farheen*

Main category: cs.CR

TL;DR: 该论文对太空网络攻击向量进行了全面分析，提出了风险评分框架，并为太空基础设施的网络安全防护提供了系统性的威胁评估和缓解措施。


<details>
  <summary>Details</summary>
Motivation: 随着现代技术的发展，关键基础设施、通信和国家安全越来越依赖太空资产，这些资产面临严重的网络攻击威胁。以往研究虽然发现了离散的漏洞并提出了具体解决方案，但缺乏对太空网络攻击向量的全面审查和缓解技术有效性的严格评估。

Method: 采用综合性方法分析各种可能的太空网络攻击向量，包括地面、太空、卫星和卫星星座等层面；评估与太空基础设施相关的缓解措施的有效性；提出风险评分框架来量化和评估威胁。

Result: 识别了涵盖地面、太空、卫星和卫星星座的全方位网络攻击向量；评估了相应缓解措施的有效性；建立了风险评分框架；确定了发展和测试前沿技术解决方案的潜在研究挑战。

Conclusion: 该研究为太空基础设施的网络安全提供了系统性分析框架，强调了建立强大网络安全措施的必要性，并为未来太空网络安全技术的发展指明了研究方向。

Abstract: With the advent of modern technology, critical infrastructure,
communications, and national security depend increasingly on space-based
assets. These assets, along with associated assets like data relay systems and
ground stations, are, therefore, in serious danger of cyberattacks. Strong
security defenses are essential to ensure data integrity, maintain secure
operations, and protect assets in space and on the ground against various
threats. Previous research has found discrete vulnerabilities in space systems
and suggested specific solutions to address them. Such research has yielded
valuable insights, but lacks a thorough examination of space cyberattack
vectors and a rigorous assessment of the efficacy of mitigation techniques.
This study tackles this issue by taking a comprehensive approach to analyze the
range of possible space cyber-attack vectors, which include ground, space,
satellite, and satellite constellations. In order to address the particular
threats, the study also assesses the efficacy of mitigation measures that are
linked with space infrastructures and proposes a Risk Scoring Framework. Based
on the analysis, this paper identifies potential research challenges for
developing and testing cutting-edge technology solutions, encouraging robust
cybersecurity measures needed in space.

</details>


### [122] [Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond](https://arxiv.org/abs/2507.17074)
*Sanzida Hoque,Abdullah Aydeger,Engin Zeydan,Madhusanka Liyanage*

Main category: cs.CR

TL;DR: 本文在5G网络环境中实现并评估了NIST选定的后量子密码学算法，发现ML-KEM配合ML-DSA在延迟敏感应用中效率最高，而SPHINCS+和HQC组合的计算和传输开销较大。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现威胁传统公钥密码系统安全，需要向后量子密码学过渡，但其在实际无线通信环境中的性能表现尚未充分探索，特别是在5G网络中的用户设备间通信场景。

Method: 使用完整的5G仿真协议栈（Open5GS和UERANSIM）以及支持后量子密码的TLS 1.3（通过BoringSSL和liboqs），在真实网络条件下测试密钥封装机制和数字签名方案，评估握手延迟、CPU和内存使用、带宽及重传率等性能指标。

Result: ML-KEM与ML-DSA组合在延迟敏感应用中表现最佳，而SPHINCS+和HQC组合产生更高的计算和传输开销，不适合对安全性要求高但时间敏感的5G应用场景。

Conclusion: 在5G网络的用户设备间通信中，不同后量子密码算法的性能差异显著，需要根据具体应用场景的安全性和延迟要求选择合适的算法组合，ML-KEM配合ML-DSA是延迟敏感场景的最佳选择。

Abstract: The advent of quantum computing threatens the security of classical
public-key cryptographic systems, prompting the transition to post-quantum
cryptography (PQC). While PQC has been analyzed in theory, its performance in
practical wireless communication environments remains underexplored. This paper
presents a detailed implementation and performance evaluation of NIST-selected
PQC algorithms in user equipment (UE) to UE communications over 5G networks.
Using a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3
via BoringSSL and liboqs, we examine key encapsulation mechanisms and digital
signature schemes across realistic network conditions. We evaluate performance
based on handshake latency, CPU and memory usage, bandwidth, and retransmission
rates, under varying cryptographic configurations and client loads. Our
findings show that ML-KEM with ML-DSA offers the best efficiency for
latency-sensitive applications, while SPHINCS+ and HQC combinations incur
higher computational and transmission overheads, making them unsuitable for
security-critical but time-sensitive 5G scenarios.

</details>


### [123] [Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement](https://arxiv.org/abs/2507.17491)
*Nazatul H. Sultan,Xinlong Guan,Josef Pieprzyk,Wei Ni,Sharif Abuadbba,Hajime Suzuki*

Main category: cs.CR

TL;DR: 本文提出了增强版5G认证协议，解决了现有5G-AKA协议在安全性和性能方面的局限性，包括对主动攻击的脆弱性、序列号同步问题和缺乏完美前向保密性等问题。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络扩展到关键基础设施，现有的5G-AKA协议存在多个问题：容易受到主动攻击、依赖序列号机制导致同步问题和通信开销、缺乏完美前向保密性(PFS)，这些问题在面对复杂威胁时会暴露过去的通信内容。

Method: 研究者提出了两种增强协议：1）无状态版本协议，移除对序列号的依赖，降低复杂性并保持与现有SIM卡和基础设施的兼容性；2）在无状态设计基础上扩展添加PFS功能，使用最小的加密开销。使用ProVerif工具进行严格的安全性分析验证。

Result: 通过ProVerif分析确认两种协议都符合所有主要安全要求，包括抗被动和主动攻击能力，以及3GPP和学术研究定义的安全标准。性能评估显示，与5G-AKA和5G-AKA'相比，提议的协议在提供更强安全性的同时仅产生轻微的计算开销。

Conclusion: 提出的增强认证协议成功解决了5G-AKA的安全和性能局限性，提供了更强的安全保障（包括PFS），同时保持了实用性和与现有基础设施的兼容性，为5G及未来网络提供了可行的解决方案。

Abstract: As 5G networks expand into critical infrastructure, secure and efficient user
authentication is more important than ever. The 5G-AKA protocol, standardized
by 3GPP in TS 33.501, is central to authentication in current 5G deployments.
It provides mutual authentication, user privacy, and key secrecy. However,
despite its adoption, 5G-AKA has known limitations in both security and
performance. While it focuses on protecting privacy against passive attackers,
recent studies show its vulnerabilities to active attacks. It also relies on a
sequence number mechanism to prevent replay attacks, requiring perfect
synchronization between the device and the core network. This stateful design
adds complexity, causes desynchronization, and incurs extra communication
overhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing
past communications if long-term keys are compromised-an increasing concern
amid sophisticated threats. This paper proposes an enhanced authentication
protocol that builds on 5G-AKA's design while addressing its shortcomings.
First, we introduce a stateless version that removes sequence number reliance,
reducing complexity while staying compatible with existing SIM cards and
infrastructure. We then extend this design to add PFS with minimal
cryptographic overhead. Both protocols are rigorously analyzed using ProVerif,
confirming their compliance with all major security requirements, including
resistance to passive and active attacks, as well as those defined by 3GPP and
academic studies. We also prototype both protocols and evaluate their
performance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the
proposed protocols offer stronger security with only minor computational
overhead, making them practical, future-ready solutions for 5G and beyond.

</details>


### [124] [Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses](https://arxiv.org/abs/2507.17655)
*Shams Shaikh,Trima P. Fernandes e Fizardo*

Main category: cs.CR

TL;DR: 随着组织快速迁移到云端，传统的硬件安全模块(HSM)和可信平台模块(TPM)在云原生环境中面临新的安全挑战。本文分析了云部署中HSM/TPM的安全失效案例，识别常见攻击向量，并探索包括机密计算、后量子密码学和去中心化密钥管理在内的替代方案。


<details>
  <summary>Details</summary>
Motivation: 云迁移加速导致密钥管理安全性日益受到关注。传统被视为加密密钥和数字信任黄金标准的HSM和TPM在云原生威胁面前面临挑战。现实世界的安全漏洞暴露了云部署中的弱点，包括配置错误、API滥用和权限提升，使攻击者能够访问敏感密钥材料并绕过保护机制。

Method: 分析涉及HSM和TPM的重大安全失效案例，识别常见攻击向量，质疑其在分布式环境中有效性的长期假设。探索机密计算、后量子密码学和去中心化密钥管理等替代方法。评估当前弱点和新兴模型。

Result: 研究发现虽然硬件本身仍然安全，但周围的云生态系统引入了系统性漏洞。HSM和TPM虽然仍发挥作用，但现代云安全需要更具适应性的分层架构。识别了云环境中密钥管理的主要威胁和漏洞。

Conclusion: 传统HSM和TPM在云环境中需要重新评估其安全有效性。现代云安全需要采用更加适应性强的分层架构来应对不断演变的威胁环境。研究为云架构师和安全工程师提供了在新威胁环境中加强密码学信任的策略。

Abstract: As organizations rapidly migrate to the cloud, the security of cryptographic
key management has become a growing concern. Hardware Security Modules (HSMs)
and Trusted Platform Modules (TPMs), traditionally seen as the gold standard
for securing encryption keys and digital trust, are increasingly challenged by
cloud-native threats. Real-world breaches have exposed weaknesses in cloud
deployments, including misconfigurations, API abuse, and privilege escalations,
allowing attackers to access sensitive key material and bypass protections.
These incidents reveal that while the hardware remains secure, the surrounding
cloud ecosystem introduces systemic vulnerabilities. This paper analyzes
notable security failures involving HSMs and TPMs, identifies common attack
vectors, and questions longstanding assumptions about their effectiveness in
distributed environments. We explore alternative approaches such as
confidential computing, post-quantum cryptography, and decentralized key
management. Our findings highlight that while HSMs and TPMs still play a role,
modern cloud security requires more adaptive, layered architectures. By
evaluating both current weaknesses and emerging models, this research equips
cloud architects and security engineers with strategies to reinforce
cryptographic trust in the evolving threat landscape.

</details>


### [125] [Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks](https://arxiv.org/abs/2507.16540)
*Radowanul Haque,Aftab Ali,Sally McClean,Naveed Khan*

Main category: cs.CR

TL;DR: ExplainVulD是一个基于图的C/C++代码漏洞检测框架，通过双通道嵌入和边感知注意力机制提高检测性能，同时提供可解释性输出，在ReVeal数据集上达到88.25%准确率和48.23% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的漏洞检测方法存在三个主要问题：1）真实数据集中类别不平衡导致易受攻击函数代表性不足；2）过度优化召回率导致高假阳性率，降低了在开发工作流中的可用性；3）缺乏可解释性，限制了在安全工作流中的集成应用。

Method: 构建代码属性图(Code Property Graphs)，使用双通道嵌入表示节点以捕获语义和结构信息，采用边感知注意力机制结合边类型嵌入来区分程序关系，使用类加权交叉熵损失来解决类别不平衡问题，并通过识别最具影响力的代码区域提供可解释性输出。

Result: 在ReVeal数据集上经过30次独立运行，ExplainVulD达到平均88.25%的准确率和48.23%的F1分数。相比ReVeal模型，准确率相对提升4.6%，F1分数相对提升16.9%。相比静态分析工具，准确率相对提升14.0-14.1%，F1分数相对提升132.2-201.2%。

Conclusion: ExplainVulD框架不仅在漏洞检测性能上超越了现有方法，还通过识别函数内最具影响力的代码区域提供了可解释的输出，支持安全分类中的透明度和信任度，为安全工作流的集成提供了更好的解决方案。

Abstract: Detecting security vulnerabilities in source code remains challenging,
particularly due to class imbalance in real-world datasets where vulnerable
functions are under-represented. Existing learning-based methods often optimise
for recall, leading to high false positive rates and reduced usability in
development workflows. Furthermore, many approaches lack explainability,
limiting their integration into security workflows. This paper presents
ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.
The method constructs Code Property Graphs and represents nodes using
dual-channel embeddings that capture both semantic and structural information.
These are processed by an edge-aware attention mechanism that incorporates
edge-type embeddings to distinguish among program relations. To address class
imbalance, the model is trained using class-weighted cross-entropy loss.
ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23
percent across 30 independent runs on the ReVeal dataset. These results
represent relative improvements of 4.6 percent in accuracy and 16.9 percent in
F1 score compared to the ReVeal model, a prior learning-based method. The
framework also outperforms static analysis tools, with relative gains of 14.0
to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond
improved detection performance, ExplainVulD produces explainable outputs by
identifying the most influential code regions within each function, supporting
transparency and trust in security triage.

</details>


### [126] [Revisiting Pre-trained Language Models for Vulnerability Detection](https://arxiv.org/abs/2507.16887)
*Youpeng Li,Weiliang Qi,Xuyu Wang,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 本文对17个预训练语言模型在漏洞检测任务上进行了全面评估，发现专门针对代码语法语义模式设计的模型表现更好，但在复杂依赖、代码变换和实际场景中仍面临挑战


<details>
  <summary>Details</summary>
Motivation: 现有对预训练语言模型在漏洞检测方面的评估研究在数据准备、评估设置和实验配置方面存在不足，影响了评估的准确性和全面性，需要更全面的评估来了解模型在真实世界漏洞检测中的有效性

Method: 构建新的数据集，对17个预训练语言模型进行评估，包括小型代码专用模型和大规模模型；比较微调和提示工程两种方法的性能；评估模型在不同训练测试设置下的有效性和泛化能力；分析模型对代码规范化、抽象化和语义保持变换的鲁棒性

Result: 专门设计用于捕获代码语法语义模式的预训练任务的模型优于通用模型和仅在大型代码语料库上预训练的模型；但模型在检测复杂依赖漏洞、处理代码规范化抽象化扰动、识别语义保持的脆弱代码变换方面存在显著挑战；模型的有限上下文窗口导致的截断会造成不可忽视的标注错误

Conclusion: 研究强调了在实际场景中全面评估模型性能的重要性，并为提高预训练语言模型在现实漏洞检测应用中的有效性提出了未来发展方向

Abstract: The rapid advancement of pre-trained language models (PLMs) has demonstrated
promising results for various code-related tasks. However, their effectiveness
in detecting real-world vulnerabilities remains a critical challenge. % for the
security community. While existing empirical studies evaluate PLMs for
vulnerability detection (VD), their inadequate consideration in data
preparation, evaluation setups, and experimental settings undermines the
accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,
an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and
large-scale PLMs using newly constructed datasets. Specifically, we compare the
performance of PLMs under both fine-tuning and prompt engineering, assess their
effectiveness and generalizability across various training and testing
settings, and analyze their robustness against code normalization, abstraction,
and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks
designed to capture the syntactic and semantic patterns of code outperform both
general-purpose PLMs and those solely pre-trained or fine-tuned on large code
corpora. However, these models face notable challenges in real-world scenarios,
such as difficulties in detecting vulnerabilities with complex dependencies,
handling perturbations introduced by code normalization and abstraction, and
identifying semantic-preserving vulnerable code transformations. Also, the
truncation caused by the limited context windows of PLMs can lead to a
non-negligible amount of labeling errors. This study underscores the importance
of thorough evaluations of model performance in practical scenarios and
outlines future directions to help enhance the effectiveness of PLMs for
realistic VD applications.

</details>


### [127] [Enabling Cyber Security Education through Digital Twins and Generative AI](https://arxiv.org/abs/2507.17518)
*Vita Santa Barletta,Vito Bavaro,Miriana Calvano,Antonio Curci,Antonio Piccinno,Davide Pio Posa*

Main category: cs.CR

TL;DR: 本研究探索了将数字孪生(DT)与渗透测试工具和大语言模型(LLM)结合，通过开发Red Team Knife(RTK)工具包来增强网络安全教育和实战准备能力的方法。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全教育存在理论与实践脱节的问题，需要一个能够模拟真实网络环境、提供实时监控和威胁分析的实用交互框架，以提升网络安全教育的有效性和相关性。

Method: 开发了一个结合数字孪生、渗透测试工具和大语言模型的综合框架。核心组件是Red Team Knife(RTK)定制渗透测试工具包，基于网络杀伤链模型设计，指导学习者进行侦察、攻击和响应等关键阶段的实践。同时集成LLM提供智能实时反馈、自然语言威胁解释和自适应学习支持。

Result: 在学术环境中的试点应用显示，该综合框架显著提高了网络安全培训的有效性和相关性，成功缩小了理论知识与实际应用之间的差距，为漏洞评估、威胁检测和安全运营技能的实践培训提供了有效支持。

Conclusion: 数字孪生与大语言模型的结合能够有效改革网络安全教育，通过提供真实的网络环境模拟和智能化学习支持，满足不断演变的行业需求，为培养具备实战能力的网络安全人才提供了新的解决方案。

Abstract: Digital Twins (DTs) are gaining prominence in cybersecurity for their ability
to replicate complex IT (Information Technology), OT (Operational Technology),
and IoT (Internet of Things) infrastructures, allowing for real time
monitoring, threat analysis, and system simulation. This study investigates how
integrating DTs with penetration testing tools and Large Language Models (LLMs)
can enhance cybersecurity education and operational readiness. By simulating
realistic cyber environments, this approach offers a practical, interactive
framework for exploring vulnerabilities and defensive strategies. At the core
of this research is the Red Team Knife (RTK), a custom penetration testing
toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide
learners through key phases of cyberattacks, including reconnaissance,
exploitation, and response within a DT powered ecosystem. The incorporation of
Large Language Models (LLMs) further enriches the experience by providing
intelligent, real-time feedback, natural language threat explanations, and
adaptive learning support during training exercises. This combined DT LLM
framework is currently being piloted in academic settings to develop hands on
skills in vulnerability assessment, threat detection, and security operations.
Initial findings suggest that the integration significantly improves the
effectiveness and relevance of cybersecurity training, bridging the gap between
theoretical knowledge and real-world application. Ultimately, the research
demonstrates how DTs and LLMs together can transform cybersecurity education to
meet evolving industry demands.

</details>


### [128] [SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping](https://arxiv.org/abs/2507.16852)
*Álvaro Ruiz-Ródenas,Jaime Pujante Sáez,Daniel García-Algora,Mario Rodríguez Béjar,Jorge Blasco,José Luis Hernández-Ramos*

Main category: cs.CR

TL;DR: 该论文提出了SynthCTI，一个数据增强框架，通过生成高质量的合成网络威胁情报(CTI)句子来解决MITRE ATT&CK技术映射中的数据稀缺和类别不平衡问题，在多个数据集上显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 网络威胁情报挖掘中将威胁描述映射到MITRE ATT&CK技术的过程通常需要手动完成，需要专家知识和大量工作。自动化方法面临两个主要挑战：高质量标注CTI数据稀缺和类别不平衡（许多技术只有很少的样本）。虽然领域特定的大语言模型如SecureBERT表现有所改善，但大多数研究关注模型架构而非解决数据限制问题。

Method: 提出SynthCTI数据增强框架，使用基于聚类的策略从训练数据中提取语义上下文，指导LLM生成词汇多样且语义忠实的合成CTI句子，专门针对代表性不足的MITRE ATT&CK技术。

Result: 在CTI-to-MITRE和TRAM两个公开CTI数据集上评估，使用不同容量的LLM。融入合成数据后宏观F1分数持续改善：ALBERT从0.35提升到0.52（相对增长48.6%），SecureBERT达到0.6558（从0.4412提升）。使用SynthCTI增强的小模型超越了未使用增强的大模型性能。

Conclusion: SynthCTI框架有效解决了CTI分类中的数据稀缺和类别不平衡问题，通过生成高质量合成数据显著提升了模型性能。该方法证明了数据生成方法在构建高效有效的CTI分类系统中的价值，为网络威胁情报自动化处理提供了新的解决方案。

Abstract: Cyber Threat Intelligence (CTI) mining involves extracting structured
insights from unstructured threat data, enabling organizations to understand
and respond to evolving adversarial behavior. A key task in CTI mining is
mapping threat descriptions to MITRE ATT\&CK techniques. However, this process
is often performed manually, requiring expert knowledge and substantial effort.
Automated approaches face two major challenges: the scarcity of high-quality
labeled CTI data and class imbalance, where many techniques have very few
examples. While domain-specific Large Language Models (LLMs) such as SecureBERT
have shown improved performance, most recent work focuses on model architecture
rather than addressing the data limitations. In this work, we present SynthCTI,
a data augmentation framework designed to generate high-quality synthetic CTI
sentences for underrepresented MITRE ATT\&CK techniques. Our method uses a
clustering-based strategy to extract semantic context from training data and
guide an LLM in producing synthetic CTI sentences that are lexically diverse
and semantically faithful. We evaluate SynthCTI on two publicly available CTI
datasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.
Incorporating synthetic data leads to consistent macro-F1 improvements: for
example, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\%), and
SecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented
with SynthCTI outperform larger models trained without augmentation,
demonstrating the value of data generation methods for building efficient and
effective CTI classification systems.

</details>


### [129] [Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs](https://arxiv.org/abs/2507.17010)
*H M Mohaimanul Islam,Huynh Q. N. Vo,Aditya Rane*

Main category: cs.CR

TL;DR: 本文提出了TrustDefender框架，结合轻量级CNN和零知识证明协议，在扩展现实(XR)环境中实时检测深度伪造内容，同时保护用户隐私，达到95.3%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 在合成媒体时代，深度伪造技术对信息完整性构成重大威胁，需要一个既能在XR平台的计算约束下工作，又能满足敏感环境中严格隐私要求的检测系统。

Method: 提出两阶段TrustDefender框架：(1)使用轻量级卷积神经网络(CNN)在XR流中实时检测深度伪造图像；(2)集成简洁零知识证明(ZKP)协议验证检测结果，同时不泄露原始用户数据。

Result: 在多个基准深度伪造数据集上实现95.3%的检测准确率，同时具备高效的证明生成能力，确保与高性能AI系统的无缝集成。

Conclusion: 通过融合先进的计算机视觉模型和可证明的安全机制，该工作为沉浸式和隐私敏感应用中的可靠AI奠定了基础。

Abstract: In the era of synthetic media, deepfake manipulations pose a significant
threat to information integrity. To address this challenge, we propose
TrustDefender, a two-stage framework comprising (i) a lightweight convolutional
neural network (CNN) that detects deepfake imagery in real-time extended
reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof
(ZKP) protocol that validates detection results without disclosing raw user
data. Our design addresses both the computational constraints of XR platforms
while adhering to the stringent privacy requirements in sensitive settings.
Experimental evaluations on multiple benchmark deepfake datasets demonstrate
that TrustDefender achieves 95.3% detection accuracy, coupled with efficient
proof generation underpinned by rigorous cryptography, ensuring seamless
integration with high-performance artificial intelligence (AI) systems. By
fusing advanced computer vision models with provable security mechanisms, our
work establishes a foundation for reliable AI in immersive and
privacy-sensitive applications.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [130] [Doubly robust outlier resistant inference on causal treatment effect](https://arxiv.org/abs/2507.17439)
*Joonsung Kang*

Main category: stat.ME

TL;DR: 本文提出了一种在包含异常值的观察性研究中进行因果效应估计的双重稳健点估计方法，通过稳健估计方程和协变量平衡倾向得分在惩罚经验似然框架下实现，并推导出有限样本置信区间。


<details>
  <summary>Details</summary>
Motivation: 异常值会严重扭曲观察性研究中的因果效应估计，特别是在小样本情况下检测和移除异常值变得困难，因此需要在不排除这些有影响力数据点的情况下稳健地估计治疗效果。

Method: 提出了一种在污染模型下估计平均治疗效果的双重稳健点估计器：通过稳健估计方程实现结果回归的稳健性，通过协变量平衡倾向得分(CBPS)确保倾向得分建模的弹性，并结合变量选择防止模型过拟合，所有组件统一在惩罚经验似然框架下。

Result: 通过仿真和包含异常值的高血压数据的实际应用验证，该方法在准确性和稳健性方面始终优于现有方法，并成功推导出不受异常值影响的最优有限样本置信区间。

Conclusion: 提出的双重稳健估计方法能够有效处理观察性研究中的异常值问题，在准确性和稳健性方面表现优异，为含异常值数据的因果推断提供了可靠的解决方案。

Abstract: Outliers can severely distort causal effect estimation in observational
studies, yet this issue has received limited attention in the literature. Their
influence is especially pronounced in small sample sizes, where detecting and
removing outliers becomes increasingly difficult. Therefore, it is essential to
estimate treatment effects robustly without excluding these influential data
points. To address this, we propose a doubly robust point estimator for the
average treatment effect under a contaminated model that includes outliers.
Robustness in outcome regression is achieved through a robust estimating
equation, while covariate balancing propensity scores (CBPS) ensure resilience
in propensity score modeling.
  To prevent model overfitting due to the inclusion of numerous parameters, we
incorporate variable selection. All these components are unified under a
penalized empirical likelihood framework. For confidence interval estimation,
most existing approaches rely on asymptotic properties, which may be unreliable
in finite samples. We derive an optimal finite-sample confidence interval for
the average treatment effect using our proposed estimating equation, ensuring
that the interval bounds remain unaffected by outliers. Through simulations and
a real-world application involving hypertension data with outliers, we
demonstrate that our method consistently outperforms existing approaches in
both accuracy and robustness.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [131] [GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP](https://arxiv.org/abs/2507.17174)
*Myeongwon Jung,Takanori Fujiwara,Jaemin Jo*

Main category: cs.GR

TL;DR: 本文针对UMAP降维方法的随机优化过程导致结果不稳定的问题，提出了(r,d)-稳定性框架来分析和评估数据点在投影空间中的随机定位，并开发了可视化工具来交互式探索数据点的稳定性。


<details>
  <summary>Details</summary>
Motivation: UMAP作为广泛使用的降维方法，其随机优化过程对结果的影响尚未得到充分探索。研究发现UMAP经常产生不稳定的结果，数据点的投影位置更多由随机性决定而非反映真实的邻域结构，这一局限性需要被解决。

Method: 引入(r,d)-稳定性框架分析UMAP中数据点的随机定位。通过创建数据点的"幽灵"副本来表示由随机性导致的潜在位置变化，评估初始投影位置和负采样等随机元素的影响。定义如果数据点的幽灵在初始投影中被扰动在半径r的圆内，最终位置仍保持在半径d的圆内，则该点为(r,d)-稳定的。开发了自适应丢弃方案来高效计算幽灵投影。

Result: 开发的自适应丢弃方案相比未优化基线减少了高达60%的运行时间，同时保持约90%的不稳定点检测准确性。提供了支持交互式探索数据点(r,d)-稳定性的可视化工具。通过真实数据集验证了框架的有效性。

Conclusion: 成功建立了评估UMAP投影稳定性的(r,d)-稳定性框架，解决了UMAP结果不稳定的问题。该框架不仅提供了理论分析工具，还包含高效的计算方法和可视化工具，为UMAP的有效使用提供了指导原则。

Abstract: Despite the widespread use of Uniform Manifold Approximation and Projection
(UMAP), the impact of its stochastic optimization process on the results
remains underexplored. We observed that it often produces unstable results
where the projections of data points are determined mostly by chance rather
than reflecting neighboring structures. To address this limitation, we
introduce (r,d)-stability to UMAP: a framework that analyzes the stochastic
positioning of data points in the projection space. To assess how stochastic
elements, specifically initial projection positions and negative sampling,
impact UMAP results, we introduce "ghosts", or duplicates of data points
representing potential positional variations due to stochasticity. We define a
data point's projection as (r,d)-stable if its ghosts perturbed within a circle
of radius r in the initial projection remain confined within a circle of radius
d for their final positions. To efficiently compute the ghost projections, we
develop an adaptive dropping scheme that reduces a runtime up to 60% compared
to an unoptimized baseline while maintaining approximately 90% of unstable
points. We also present a visualization tool that supports the interactive
exploration of the (r,d)-stability of data points. Finally, we demonstrate the
effectiveness of our framework by examining the stability of projections of
real-world datasets and present usage guidelines for the effective use of our
framework.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [132] [Time Deep Gradient Flow Method for pricing American options](https://arxiv.org/abs/2507.17606)
*Jasper Rou*

Main category: q-fin.CP

TL;DR: 本研究探索了基于神经网络的方法来定价多维美式看跌期权，采用Time Deep Gradient Flow (TDGF)和Deep Galerkin Method (DGM)两种方法，在Black-Scholes和Heston模型下处理最多五维问题，通过精心设计的采样策略提升性能，相比传统蒙特卡洛方法实现更高精度和计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统的蒙特卡洛方法在处理多维美式期权定价时存在计算效率低下的问题，特别是在处理美式期权固有的自由边界偏微分方程时面临挑战，需要开发更高效的神经网络方法来解决这一问题。

Method: 采用两种神经网络方法：1) 扩展Time Deep Gradient Flow (TDGF)方法来处理美式期权的自由边界偏微分方程；2) 使用Deep Galerkin Method (DGM)；在训练过程中精心设计采样策略以提升性能，在Black-Scholes和Heston模型框架下处理最多五维的美式看跌期权定价问题。

Result: TDGF和DGM两种方法都实现了高精度的定价结果，在计算速度方面明显超越了传统的蒙特卡洛方法。特别地，TDGF方法在训练阶段比DGM方法表现出更快的收敛速度。

Conclusion: 基于神经网络的方法（TDGF和DGM）为多维美式期权定价提供了有效的解决方案，不仅保持了高精度，还显著提升了计算效率，其中TDGF方法在训练效率方面表现更优，为复杂金融衍生品定价提供了新的技术路径。

Abstract: In this research, we explore neural network-based methods for pricing
multidimensional American put options under the BlackScholes and Heston model,
extending up to five dimensions. We focus on two approaches: the Time Deep
Gradient Flow (TDGF) method and the Deep Galerkin Method (DGM). We extend the
TDGF method to handle the free-boundary partial differential equation inherent
in American options. We carefully design the sampling strategy during training
to enhance performance. Both TDGF and DGM achieve high accuracy while
outperforming conventional Monte Carlo methods in terms of computational speed.
In particular, TDGF tends to be faster during training than DGM.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [133] [HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery](https://arxiv.org/abs/2507.17209)
*Haoran Jiang,Shaohan Shi,Yunjie Yao,Chang Jiang,Quan Li*

Main category: cs.HC

TL;DR: HypoChainer是一个协作可视化框架，整合人类专业知识、大语言模型推理和知识图谱，通过三阶段流程（探索与情境化、假设链形成、验证优先级排序）来增强科学假设的生成和验证，解决了传统研究方法的认知局限性和深度学习模型输出难以筛选的问题。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现面临整合庞大异构知识的挑战，传统假设驱动研究受限于人类认知能力、生物系统复杂性和试验成本；深度学习模型虽能加速预测但输出量巨大难以人工筛选；大语言模型存在幻觉问题且缺乏结构化知识支撑，可靠性有限。

Method: 提出HypoChainer协作可视化框架，采用三阶段方法：1）探索与情境化阶段-专家使用检索增强大语言模型和降维技术导航大规模图神经网络预测；2）假设链形成阶段-专家迭代检查知识图谱关系并通过大语言模型和知识图谱建议完善假设；3）验证优先级排序阶段-基于知识图谱支持的证据过滤假设，识别高优先级实验候选。

Result: 通过两个领域的案例研究和专家访谈验证了HypoChainer的有效性，展示了其在支持可解释、可扩展和知识基础科学发现方面的潜力。

Conclusion: HypoChainer成功整合了人类专业知识、大语言模型推理能力和知识图谱的结构化信息，为科学假设生成和验证提供了一个可解释、可扩展且以知识为基础的解决方案，有望显著提升生物医学和药物开发等领域的科学发现效率。

Abstract: Modern scientific discovery faces growing challenges in integrating vast and
heterogeneous knowledge critical to breakthroughs in biomedicine and drug
development. Traditional hypothesis-driven research, though effective, is
constrained by human cognitive limits, the complexity of biological systems,
and the high cost of trial-and-error experimentation. Deep learning models,
especially graph neural networks (GNNs), have accelerated prediction
generation, but the sheer volume of outputs makes manual selection for
validation unscalable. Large language models (LLMs) offer promise in filtering
and hypothesis generation, yet suffer from hallucinations and lack grounding in
structured knowledge, limiting their reliability. To address these issues, we
propose HypoChainer, a collaborative visualization framework that integrates
human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance
hypothesis generation and validation. HypoChainer operates in three stages:
First, exploration and contextualization -- experts use retrieval-augmented
LLMs (RAGs) and dimensionality reduction to navigate large-scale GNN
predictions, assisted by interactive explanations. Second, hypothesis chain
formation -- experts iteratively examine KG relationships around predictions
and semantically linked entities, refining hypotheses with LLM and KG
suggestions. Third, validation prioritization -- refined hypotheses are
filtered based on KG-supported evidence to identify high-priority candidates
for experimentation, with visual analytics further strengthening weak links in
reasoning. We demonstrate HypoChainer's effectiveness through case studies in
two domains and expert interviews, highlighting its potential to support
interpretable, scalable, and knowledge-grounded scientific discovery.

</details>


### [134] [Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills](https://arxiv.org/abs/2507.17688)
*Mohammad Nur Hossain Khan,David creswell,Jordan Albert,Patrick O'Connell,Shawn Fallon,Mathew Polowitz,Xuhai "orson" Xu,Bashima islam*

Main category: cs.HC

TL;DR: 本研究开发了基于智能手机加速度计的呼吸追踪算法和正念技能评估框架，通过生物信号反馈提升数字冥想应用的用户体验和技能发展效果


<details>
  <summary>Details</summary>
Motivation: 数字冥想应用虽然提高了正念训练的可及性，但长期用户参与度仍是挑战。现有方法难以准确捕捉正念冥想中的慢呼吸模式，且缺乏量化的正念技能评估框架

Method: 开发基于智能手机加速度计的呼吸追踪算法，无需额外可穿戴设备；构建首个基于加速度计呼吸数据的正念技能量化评估框架，评估专注力、感知清晰度和平静心三个维度；在261个正念训练会话中测试算法性能

Result: 呼吸追踪模型达到1.6次/分钟的平均绝对误差，与真实数据高度一致；正念技能评估在追踪技能进展方面达到80-84%的F1分数；用户研究显示呼吸反馈显著提升了系统可用性

Conclusion: 通过将呼吸追踪和正念技能评估集成到商业应用中，证明了智能手机传感器在增强数字正念训练方面的潜力，为提升用户参与度和技能发展提供了有效解决方案

Abstract: Mindfulness training is widely recognized for its benefits in reducing
depression, anxiety, and loneliness. With the rise of smartphone-based
mindfulness apps, digital meditation has become more accessible, but sustaining
long-term user engagement remains a challenge. This paper explores whether
respiration biosignal feedback and mindfulness skill estimation enhance system
usability and skill development. We develop a smartphone's accelerometer-based
respiration tracking algorithm, eliminating the need for additional wearables.
Unlike existing methods, our approach accurately captures slow breathing
patterns typical of mindfulness meditation. Additionally, we introduce the
first quantitative framework to estimate mindfulness skills-concentration,
sensory clarity, and equanimity-based on accelerometer-derived respiration
data. We develop and test our algorithms on 261 mindfulness sessions in both
controlled and real-world settings. A user study comparing an experimental
group receiving biosignal feedback with a control group using a standard app
shows that respiration feedback enhances system usability. Our respiration
tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,
closely aligning with ground truth data, while our mindfulness skill estimation
attains F1 scores of 80-84% in tracking skill progression. By integrating
respiration tracking and mindfulness estimation into a commercial app, we
demonstrate the potential of smartphone sensors to enhance digital mindfulness
training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [135] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 本文研究了文本到图像扩散模型中的数据记忆问题，发现现有的权重剪枝防护方法存在脆弱性，并提出了一种对抗性微调方法来增强模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型可能会无意中记忆和复制训练数据，引发数据隐私和知识产权问题。现有的缓解方法主要基于权重剪枝来抑制数据复制，但这些方法的鲁棒性尚未得到充分评估。

Method: 研究评估了基于剪枝方法的鲁棒性，通过微调文本嵌入来测试是否能重新触发数据复制。同时挑战了记忆局部化的基本假设，展示复制可从文本嵌入空间的不同位置触发。最后提出了一种新颖的对抗性微调方法，迭代搜索复制触发器并更新模型以增强鲁棒性。

Result: 研究发现即使在剪枝后，对输入提示的文本嵌入进行微小调整仍足以重新触发数据复制，证明了这些防护方法的脆弱性。记忆并非局部化的，复制可以从文本嵌入空间的多个位置触发，并在模型中遵循不同路径。

Conclusion: 现有的缓解策略是不充分的，需要真正移除记忆内容而非仅仅抑制其检索的方法。研究为理解文本到图像扩散模型中记忆的本质提供了新见解，并为构建更可靠和合规的生成式AI奠定了基础。

Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.

</details>


### [136] [AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation](https://arxiv.org/abs/2507.16940)
*Nima Fathi,Amar Kumar,Tal Arbel*

Main category: cs.CV

TL;DR: 本文介绍了AURA，首个专门用于医学影像分析的视觉语言可解释性代理系统，通过动态交互、上下文解释和假设测试，将静态预测转变为交互式决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然在多个领域展现出代理AI的潜力，但在医学影像领域的应用仍处于起步阶段。医学影像分析需要从静态预测系统向能够推理、与工具交互、适应复杂任务的代理AI系统转变，以提供更透明、适应性更强、与临床更好对齐的AI系统。

Method: 基于Qwen-32B大语言模型架构构建AURA系统，集成模块化工具箱包括：(1)分割套件：包含相位定位、病理分割和解剖分割来定位临床相关区域；(2)反事实图像生成模块：通过图像级解释支持推理；(3)评估工具集：包括像素级差异图分析、分类和先进组件来评估诊断相关性和视觉可解释性。

Result: AURA实现了医学影像的动态交互、上下文解释和假设测试功能，成功将医学影像分析从静态预测转变为交互式决策支持系统，在医学影像的综合分析、解释和评估方面取得了显著进展。

Conclusion: AURA代表了在医学影像分析领域向更透明、适应性更强、与临床更好对齐的AI系统发展的重要进步，展示了代理AI在将医学影像分析从静态预测转变为交互式决策支持方面的巨大潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.

</details>


### [137] [Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks](https://arxiv.org/abs/2507.17000)
*Jacob Piland,Chris Sweet,Adam Czajka*

Main category: cs.CV

TL;DR: 本文提出了一种新的显著性引导训练方法，通过同时利用真类和假类的类激活图（CAM）来改进深度学习模型的泛化能力，在多个二分类任务上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的显著性引导训练方法只关注真类（正确标签）的类激活图与人类参考显著性图的比较，而忽略了假类（错误标签）的CAM。作者假设在二分类任务中，真类和假类的CAM应该在人类识别的重要分类特征上产生分歧，基于这一假设提出改进方法。

Method: 提出了三种新的显著性引导训练方法，将真类和假类的模型CAM都纳入训练策略中，并开发了一个新的事后工具来识别重要特征。这些方法利用真假类CAM之间的分歧来指导模型训练。

Result: 在多个不同的二分类任务上进行评估，包括合成人脸检测、生物特征呈现攻击检测和胸部X光异常分类等闭集和开集分类任务。结果表明，所提出的方法相比传统的（仅使用真类CAM的）显著性引导训练方法，能够改善深度学习模型的泛化能力。

Conclusion: 通过同时考虑真类和假类的类激活图，新提出的显著性引导训练方法能够更好地改善模型的泛化性能。作者提供了源代码和模型权重以支持可重复研究。

Abstract: Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model's class activation map (CAM)
for a sample's true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model's saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model's CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.

</details>


### [138] [Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation](https://arxiv.org/abs/2507.17121)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

TL;DR: 本文提出了一个基于深度学习的糖尿病视网膜病变分类框架，通过迁移学习和数据增强技术，在二分类任务中达到98.9%的准确率，在五分类任务中达到84.6%的准确率，为糖尿病视网膜病变的自动化筛查提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球视力丧失的主要原因，通过自动化视网膜图像分析进行早期诊断可以显著降低失明风险。现有方法面临类别不平衡和训练数据有限的挑战，需要开发更加鲁棒的深度学习框架来提高诊断准确性。

Method: 采用迁移学习和大量数据增强技术构建深度学习框架，评估多种预训练卷积神经网络架构（包括ResNet和EfficientNet变体），使用APTOS 2019数据集进行训练和验证。通过类别平衡增强与迁移学习相结合的方式解决数据不平衡问题。

Result: 在二分类任务中，模型达到98.9%的准确率、98.6%的精确率、99.3%的召回率、98.9%的F1分数和99.4%的AUC值。在五分类严重程度分类任务中，模型获得84.6%的准确率和94.1%的AUC值，超越了多个现有方法。EfficientNet-B0和ResNet34在两个任务中都表现出准确率和计算效率的最佳平衡。

Conclusion: 研究结果证明了类别平衡增强与迁移学习相结合在高性能糖尿病视网膜病变诊断中的有效性。所提出的框架为糖尿病视网膜病变筛查提供了可扩展且准确的解决方案，具有在真实临床环境中部署的潜力。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and
early diagnosis through automated retinal image analysis can significantly
reduce the risk of blindness. This paper presents a robust deep learning
framework for both binary and five-class DR classification, leveraging transfer
learning and extensive data augmentation to address the challenges of class
imbalance and limited training data. We evaluate a range of pretrained
convolutional neural network architectures, including variants of ResNet and
EfficientNet, on the APTOS 2019 dataset.
  For binary classification, our proposed model achieves a state-of-the-art
accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of
98.9%, and an AUC of 99.4%. In the more challenging five-class severity
classification task, our model obtains a competitive accuracy of 84.6% and an
AUC of 94.1%, outperforming several existing approaches. Our findings also
demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between
accuracy and computational efficiency across both tasks.
  These results underscore the effectiveness of combining class-balanced
augmentation with transfer learning for high-performance DR diagnosis. The
proposed framework provides a scalable and accurate solution for DR screening,
with potential for deployment in real-world clinical environments.

</details>


### [139] [ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation](https://arxiv.org/abs/2507.17149)
*Bo Fang,Jianan Fan,Dongnan Liu,Hang Chang,Gerald J. Shami,Filip Braet,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出ScSAM方法，通过融合预训练SAM模型和MAE引导的细胞先验知识来解决亚细胞器官分割中的形态变异性和分布偏差问题，在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的细胞器分割模型面临亚细胞组件形态和分布变异性大的挑战，容易产生偏向性特征学习。虽然SAM模型提供了丰富的特征表示，但在亚细胞场景中存在两个关键问题：(1)亚细胞形态和分布的变异性在标签空间中产生间隙，导致模型学习虚假或偏向特征；(2)SAM专注于全局上下文理解，常忽略细粒度空间细节，难以捕获微妙的结构变化并应对数据分布偏斜。

Method: 提出ScSAM方法，通过融合预训练SAM与MAE引导的细胞先验知识来增强特征鲁棒性，缓解数据不平衡导致的训练偏差。具体包括：(1)设计特征对齐和融合模块，将预训练嵌入对齐到相同特征空间并有效结合不同表示；(2)提出基于余弦相似矩阵的类别提示编码器，激活类别特定特征以识别亚细胞类别。

Result: 在多个不同的亚细胞图像数据集上进行的广泛实验表明，ScSAM方法的性能优于现有的最先进方法。

Conclusion: ScSAM成功解决了亚细胞器官分割中的形态变异性和分布偏差问题，通过融合SAM和MAE的优势，实现了更鲁棒的特征学习和更准确的分割性能，为亚细胞组件分析提供了有效的解决方案。

Abstract: The significant morphological and distributional variability among
subcellular components poses a long-standing challenge for learning-based
organelle segmentation models, significantly increasing the risk of biased
feature learning. Existing methods often rely on single mapping relationships,
overlooking feature diversity and thereby inducing biased training. Although
the Segment Anything Model (SAM) provides rich feature representations, its
application to subcellular scenarios is hindered by two key challenges: (1) The
variability in subcellular morphology and distribution creates gaps in the
label space, leading the model to learn spurious or biased features. (2) SAM
focuses on global contextual understanding and often ignores fine-grained
spatial details, making it challenging to capture subtle structural alterations
and cope with skewed data distributions. To address these challenges, we
introduce ScSAM, a method that enhances feature robustness by fusing
pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge
to alleviate training bias from data imbalance. Specifically, we design a
feature alignment and fusion module to align pre-trained embeddings to the same
feature space and efficiently combine different representations. Moreover, we
present a cosine similarity matrix-based class prompt encoder to activate
class-specific features to recognize subcellular categories. Extensive
experiments on diverse subcellular image datasets demonstrate that ScSAM
outperforms state-of-the-art methods.

</details>


### [140] [A Low-Cost Machine Learning Approach for Timber Diameter Estimation](https://arxiv.org/abs/2507.17219)
*Fatemeh Hasanzadeh Fard,Sanaz Hasanzadeh Fard,Mehdi Jonoobi*

Main category: cs.CV

TL;DR: 本研究提出了一种基于YOLOv5的木材原木直径自动估算方法，使用标准RGB图像在真实工业环境下实现木材检测和厚度估算，为木材加工行业提供了实用且成本效益高的机器学习解决方案。


<details>
  <summary>Details</summary>
Motivation: 木材加工行业（如锯木厂和MDF生产线）需要准确高效地识别木材种类和厚度，但传统方法依赖人工专家，速度慢、不一致且容易出错，特别是在处理大批量木材时。因此需要开发实用且成本效益高的自动化解决方案。

Method: 采用YOLOv5目标检测算法，在公开数据集TimberSeg 1.0上进行微调，通过边界框尺寸检测单个木材原木并估算厚度。模型使用在典型工业棚屋中木材交付期间拍摄的标准RGB图像进行训练，无需昂贵的传感器或受控环境。

Result: 实验结果显示，模型在平均精度mAP@0.5指标上达到0.64，即使在有限的计算资源下也能实现可靠的原木检测。该解决方案轻量化且可扩展。

Conclusion: 该轻量化、可扩展的解决方案有望实际集成到现有工作流程中，包括现场库存管理和初步分拣，特别适用于中小型操作。为木材加工行业提供了一种实用的自动化检测方案。

Abstract: The wood processing industry, particularly in facilities such as sawmills and
MDF production lines, requires accurate and efficient identification of species
and thickness of the wood. Although traditional methods rely heavily on expert
human labor, they are slow, inconsistent, and prone to error, especially when
processing large volumes. This study focuses on practical and cost-effective
machine learning frameworks that automate the estimation of timber log diameter
using standard RGB images captured under real-world working conditions. We
employ the YOLOv5 object detection algorithm, fine-tuned on a public dataset
(TimberSeg 1.0), to detect individual timber logs and estimate thickness
through bounding-box dimensions. Unlike previous methods that require expensive
sensors or controlled environments, this model is trained on images taken in
typical industrial sheds during timber delivery. Experimental results show that
the model achieves a mean Average Precision (mAP@0.5) of 0.64, demonstrating
reliable log detection even with modest computing resources. This lightweight,
scalable solution holds promise for practical integration into existing
workflows, including on-site inventory management and preliminary sorting,
particularly in small and medium-sized operations.

</details>


### [141] [Principled Multimodal Representation Learning](https://arxiv.org/abs/2507.17343)
*Xiaohao Liu,Xiaobo Xia,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了PMRL框架，通过优化表示矩阵的主奇异值实现多模态数据的同步对齐，无需依赖固定锚点模态，解决了传统方法的局限性


<details>
  <summary>Details</summary>
Motivation: 传统多模态表示学习方法依赖成对对比学习和预定义锚点模态，限制了所有模态间的对齐效果；现有多模态同步对齐方法存在固定锚点限制和奇异值乘积优化不稳定的问题

Method: 基于完全对齐对应秩-1格拉姆矩阵的理论洞察，PMRL通过优化表示矩阵的主奇异值使模态沿共享主方向对齐；采用基于softmax的损失函数将奇异值视为logits来优先考虑最大奇异值；使用主特征向量的实例级对比正则化保持实例间可分性并防止表示坍塌

Result: 在多个不同任务上的广泛实验表明，PMRL相比基线方法表现出优越性能

Conclusion: PMRL框架成功实现了无锚点依赖的稳定多模态同步对齐，为多模态表示学习提供了更有效的解决方案

Abstract: Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.

</details>


### [142] [SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving](https://arxiv.org/abs/2507.17479)
*Chuang Chen,Xiaolin Qin,Jing Hu,Wenyi Ge*

Main category: cs.CV

TL;DR: 提出了SRMambaV2方法，通过仿生2D选择性扫描自注意力机制和双分支网络架构，有效解决了自动驾驶场景中LiDAR点云上采样的稀疏性和复杂3D结构重建问题


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中LiDAR点云上采样面临重大挑战，主要由于数据固有的稀疏性和复杂的3D结构。现有方法将3D空间场景转换为2D图像超分辨率任务，但由于距离图像的稀疏和模糊特征表示，准确重建详细和复杂的空间拓扑结构仍然困难

Method: 提出SRMambaV2稀疏点云上采样方法，包括：1）受人类驾驶员视觉感知启发，设计仿生2D选择性扫描自注意力(2DSSA)机制来建模远距离稀疏区域的特征分布；2）引入双分支网络架构增强稀疏特征表示；3）引入渐进自适应损失(PAL)函数进一步细化上采样过程中细粒度细节的重建

Result: 实验结果表明，SRMambaV2在定性和定量评估中都取得了优异性能，在提升长距离稀疏区域上采样精度的同时保持了整体几何重建质量

Conclusion: SRMambaV2方法有效解决了汽车稀疏点云上采样任务的关键问题，展现了其在实际应用中的有效性和实用价值，为自动驾驶场景中的LiDAR点云处理提供了新的解决方案

Abstract: Upsampling LiDAR point clouds in autonomous driving scenarios remains a
significant challenge due to the inherent sparsity and complex 3D structures of
the data. Recent studies have attempted to address this problem by converting
the complex 3D spatial scenes into 2D image super-resolution tasks. However,
due to the sparse and blurry feature representation of range images, accurately
reconstructing detailed and complex spatial topologies remains a major
difficulty. To tackle this, we propose a novel sparse point cloud upsampling
method named SRMambaV2, which enhances the upsampling accuracy in long-range
sparse regions while preserving the overall geometric reconstruction quality.
Specifically, inspired by human driver visual perception, we design a
biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the
feature distribution in distant sparse areas. Meanwhile, we introduce a
dual-branch network architecture to enhance the representation of sparse
features. In addition, we introduce a progressive adaptive loss (PAL) function
to further refine the reconstruction of fine-grained details during the
upsampling process. Experimental results demonstrate that SRMambaV2 achieves
superior performance in both qualitative and quantitative evaluations,
highlighting its effectiveness and practical value in automotive sparse point
cloud upsampling tasks.

</details>


### [143] [Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors](https://arxiv.org/abs/2507.17577)
*Chen Ma,Xinjie Xu,Shuyu Cheng,Qi Xuan*

Main category: cs.CV

TL;DR: 本文提出了一种先验引导的硬标签黑盒对抗攻击方法，通过利用代理模型的迁移先验信息来改进射线搜索效率，在查询效率方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 硬标签黑盒对抗攻击中，现有的射线搜索方法需要大量查询来进行二分搜索和梯度估计，查询效率低下。虽然已有方法使用"符号技巧"来减少查询次数，但梯度估计质量仍有改进空间。

Method: 提出先验引导的射线搜索方法：1）利用代理模型的迁移先验信息；2）设计新的梯度估计器，通过近似真实梯度在先验向量和随机方向张成的子空间上的投影来整合先验信息；3）以查询高效的方式实现梯度估计。

Result: 在ImageNet和CIFAR-10数据集上的实验表明，该方法在查询效率方面显著优于11种最先进的方法。理论分析证明了所提出的梯度估计器与真实梯度之间的期望余弦相似度有所改善。

Conclusion: 通过理论分析梯度估计质量并引入先验信息，成功改进了硬标签黑盒对抗攻击的射线搜索效率，为实际应用中的对抗攻击提供了更高效的解决方案。

Abstract: One of the most practical and challenging types of black-box adversarial
attacks is the hard-label attack, where only the top-1 predicted label is
available. One effective approach is to search for the optimal ray direction
from the benign image that minimizes the $\ell_p$-norm distance to the
adversarial region. The unique advantage of this approach is that it transforms
the hard-label attack into a continuous optimization problem. The objective
function value is the ray's radius, which can be obtained via binary search at
a high query cost. Existing methods use a "sign trick" in gradient estimation
to reduce the number of queries. In this paper, we theoretically analyze the
quality of this gradient estimation and propose a novel prior-guided approach
to improve ray search efficiency both theoretically and empirically.
Specifically, we utilize the transfer-based priors from surrogate models, and
our gradient estimators appropriately integrate them by approximating the
projection of the true gradient onto the subspace spanned by these priors and
random directions, in a query-efficient manner. We theoretically derive the
expected cosine similarities between the obtained gradient estimators and the
true gradient, and demonstrate the improvement achieved by incorporating
priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that
our approach significantly outperforms 11 state-of-the-art methods in terms of
query efficiency.

</details>


### [144] [PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.17596)
*Maciej K. Wozniak,Lianhang Liu,Yixi Cai,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出了PRIX，一个仅使用相机数据的高效端到端自动驾驶架构，通过Context-aware Recalibration Transformer (CaRT)模块直接从原始像素预测安全轨迹，在保持性能的同时显著提升了推理速度和模型效率。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型面临模型尺寸大、依赖昂贵LiDAR传感器、BEV特征表示计算密集等问题，限制了其在仅配备相机的量产车辆上的可扩展性和实际部署。

Method: 提出PRIX架构，仅使用相机数据，无需显式BEV表示和LiDAR。核心组件包括视觉特征提取器、生成式规划头以及Context-aware Recalibration Transformer (CaRT)模块，该模块能有效增强多层次视觉特征以实现更鲁棒的路径规划。

Result: 在NavSim和nuScenes基准测试中达到最先进性能，与更大的多模态扩散规划器性能相当，但在推理速度和模型尺寸方面显著更高效，使其成为实际部署的实用解决方案。

Conclusion: PRIX成功解决了端到端自动驾驶模型的部署挑战，通过仅使用相机数据和创新的CaRT模块，实现了高性能、高效率的自动驾驶规划，为量产车辆的实际应用提供了可行方案。

Abstract: While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.

</details>


### [145] [Vision Transformer attention alignment with human visual perception in aesthetic object evaluation](https://arxiv.org/abs/2507.17616)
*Miguel Carrasco,César González-Martín,José Aranda,Luis Oliveros*

Main category: cs.CV

TL;DR: 研究比较了人类视觉注意力与视觉Transformer(ViT)注意力机制在评估手工艺品时的相关性，发现ViT的某些注意力头可以近似人类视觉行为，但整体表现出更全局的注意力模式


<details>
  <summary>Details</summary>
Motivation: 视觉注意力机制在人类感知和美学评价中起关键作用，虽然ViT在计算机视觉任务中表现出色，但其与人类视觉注意力模式的对齐程度，特别是在美学背景下，仍未得到充分探索

Method: 使用眼动追踪实验记录30名参与者观看20件手工艺品（篮编包和姜罐）时的凝视模式并生成热图；同时使用预训练的ViT-DINO模型分析相同物体，提取12个注意力头的注意力图；使用Kullback-Leibler散度比较人类和ViT注意力分布

Result: 在sigma=2.4±0.03时发现最优相关性，注意力头#12与人类视觉模式对齐最强；注意力头间存在显著差异，头#7和#9与人类注意力偏差最大(p<0.05)；ViT表现出更全局的注意力模式，但某些注意力头能近似人类视觉行为

Conclusion: 研究表明ViT注意力机制在产品设计和美学评估中具有潜在应用价值，同时突出了人类感知与当前AI模型在注意力策略上的根本差异

Abstract: Visual attention mechanisms play a crucial role in human perception and
aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have
demonstrated remarkable capabilities in computer vision tasks, yet their
alignment with human visual attention patterns remains underexplored,
particularly in aesthetic contexts. This study investigates the correlation
between human visual attention and ViT attention mechanisms when evaluating
handcrafted objects. We conducted an eye-tracking experiment with 30
participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal
objects comprising basketry bags and ginger jars. Using a Pupil Labs
eye-tracker, we recorded gaze patterns and generated heat maps representing
human visual attention. Simultaneously, we analyzed the same objects using a
pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting
attention maps from each of the 12 attention heads. We compared human and ViT
attention distributions using Kullback-Leibler divergence across varying
Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal
correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest
alignment with human visual patterns. Significant differences were found
between attention heads, with heads #7 and #9 demonstrating the greatest
divergence from human attention (p< 0.05, Tukey HSD test). Results indicate
that while ViTs exhibit more global attention patterns compared to human focal
attention, certain attention heads can approximate human visual behavior,
particularly for specific object features like buckles in basketry items. These
findings suggest potential applications of ViT attention mechanisms in product
design and aesthetic evaluation, while highlighting fundamental differences in
attention strategies between human perception and current AI models.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [146] [Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling](https://arxiv.org/abs/2507.17526)
*Leandro Von Krannichfeldt,Kristina Orehounig,Olga Fink*

Main category: eess.SY

TL;DR: 该论文评估了五种代表性的混合方法在概率建筑能耗建模中的表现，重点关注建筑热力学的分位数预测，发现残差学习方法表现最佳且在分布外数据上具有物理直觉性。


<details>
  <summary>Details</summary>
Motivation: 现有的建筑能耗建模混合方法主要集中在确定性建模上，往往忽略了天气波动和居住者行为等因素带来的固有不确定性，且缺乏在概率建模框架内的系统性比较研究。

Method: 评估五种代表性的混合方法进行概率建筑能耗建模，包括物理模型代理学习、模拟与观测数据间的残差建模、用真实测量数据微调代理模型、将物理输出作为数据驱动模型的额外输入，以及将物理输出集成到损失函数中。重点关注建筑热力学的分位数预测。

Result: 发现混合方法的性能在不同建筑房间类型中有所差异，但使用前馈神经网络的残差学习方法平均表现最佳。残差方法是唯一在分布外测试数据上产生物理直觉预测的模型。分位数保形预测在室内温度建模的分位数预测校准中是有效的程序。

Conclusion: 在概率建筑能耗建模中，残差学习方法不仅具有最佳的平均性能，还能在分布外数据上提供物理上合理的预测。分位数保形预测为室内温度建模的不确定性量化提供了有效的校准方法。

Abstract: Building energy modeling is a key tool for optimizing the performance of
building energy systems. Historically, a wide spectrum of methods has been
explored -- ranging from conventional physics-based models to purely
data-driven techniques. Recently, hybrid approaches that combine the strengths
of both paradigms have gained attention. These include strategies such as
learning surrogates for physics-based models, modeling residuals between
simulated and observed data, fine-tuning surrogates with real-world
measurements, using physics-based outputs as additional inputs for data-driven
models, and integrating the physics-based output into the loss function the
data-driven model. Despite this progress, two significant research gaps remain.
First, most hybrid methods focus on deterministic modeling, often neglecting
the inherent uncertainties caused by factors like weather fluctuations and
occupant behavior. Second, there has been little systematic comparison within a
probabilistic modeling framework. This study addresses these gaps by evaluating
five representative hybrid approaches for probabilistic building energy
modeling, focusing on quantile predictions of building thermodynamics in a
real-world case study. Our results highlight two main findings. First, the
performance of hybrid approaches varies across different building room types,
but residual learning with a Feedforward Neural Network performs best on
average. Notably, the residual approach is the only model that produces
physically intuitive predictions when applied to out-of-distribution test data.
Second, Quantile Conformal Prediction is an effective procedure for calibrating
quantile predictions in case of indoor temperature modeling.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [147] [Spintronic Bayesian Hardware Driven by Stochastic Magnetic Domain Wall Dynamics](https://arxiv.org/abs/2507.17193)
*Tianyi Wang,Bingqian Dai,Kin Wong,Yaochen Li,Yang Cheng,Qingyuan Shu,Haoran He,Puyang Huang,Hanshen Huang,Kang L. Wang*

Main category: physics.app-ph

TL;DR: 研究者提出了一种磁性概率计算(MPC)平台，利用自旋电子学系统的内在随机性来高效实现概率神经网络，为AI系统提供不确定性量化能力


<details>
  <summary>Details</summary>
Motivation: 传统神经网络缺乏不确定性估计能力，限制了其在安全关键领域的可靠性。概率神经网络虽然能提供不确定性量化，但在传统CMOS架构上实现时面临巨大的计算开销，因为CMOS架构天然抑制随机性

Method: 提出磁性概率计算平台，基于磁畴壁动力学的自旋电子学系统。集成三个关键机制：热诱导磁畴壁随机性、电压控制磁各向异性(VCMA)和隧道磁阻(TMR)，在器件级别实现完全电控和可调的概率功能

Result: 在CIFAR-10分类任务上验证了贝叶斯神经网络推理结构。与标准28nm CMOS实现相比，整体性能指标提升了7个数量级，在面积效率、能耗和速度方面都有显著提升

Conclusion: MPC平台通过利用物理系统的内在随机性，为实现可靠和可信的物理AI系统提供了新的解决方案，展现了在概率计算领域的巨大潜力

Abstract: As artificial intelligence (AI) advances into diverse applications, ensuring
reliability of AI models is increasingly critical. Conventional neural networks
offer strong predictive capabilities but produce deterministic outputs without
inherent uncertainty estimation, limiting their reliability in safety-critical
domains. Probabilistic neural networks (PNNs), which introduce randomness, have
emerged as a powerful approach for enabling intrinsic uncertainty
quantification. However, traditional CMOS architectures are inherently designed
for deterministic operation and actively suppress intrinsic randomness. This
poses a fundamental challenge for implementing PNNs, as probabilistic
processing introduces significant computational overhead. To address this
challenge, we introduce a Magnetic Probabilistic Computing (MPC) platform-an
energy-efficient, scalable hardware accelerator that leverages intrinsic
magnetic stochasticity for uncertainty-aware computing. This physics-driven
strategy utilizes spintronic systems based on magnetic domain walls (DWs) and
their dynamics to establish a new paradigm of physical probabilistic computing
for AI. The MPC platform integrates three key mechanisms: thermally induced DW
stochasticity, voltage controlled magnetic anisotropy (VCMA), and tunneling
magnetoresistance (TMR), enabling fully electrical and tunable probabilistic
functionality at the device level. As a representative demonstration, we
implement a Bayesian Neural Network (BNN) inference structure and validate its
functionality on CIFAR-10 classification tasks. Compared to standard 28nm CMOS
implementations, our approach achieves a seven orders of magnitude improvement
in the overall figure of merit, with substantial gains in area efficiency,
energy consumption, and speed. These results underscore the MPC platform's
potential to enable reliable and trustworthy physical AI systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [148] [Fundamental limits of distributed covariance matrix estimation via a conditional strong data processing inequality](https://arxiv.org/abs/2507.16953)
*Mohammad Reza Rahmani,Mohammad Hossein Yassaee,Mohammad Reza Aref*

Main category: stat.ML

TL;DR: 本文研究了在特征分割设置下分布式协方差估计的理论极限，提出了条件强数据处理不等式(C-SDPI)系数，建立了协方差矩阵估计的极小极大下界，并设计了近似最优的估计协议。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，每个智能体只能观察到高维随机向量的不同分量，需要在通信受限的情况下估计完整的协方差矩阵。现有理论框架多假设无限样本或高斯分布，缺乏更一般的理论分析。

Method: 引入条件强数据处理不等式(C-SDPI)系数作为主要技术工具，该系数量化了状态依赖信道中的平均收缩，具有张量化等关键性质。利用Geng-Nair的倍增技巧和算子Jensen不等式计算高斯混合信道的C-SDPI系数，建立估计误差的极小极大下界。

Result: 获得了算子范数和Frobenius范数下协方差矩阵估计的近紧极小极大下界，揭示了样本大小、通信成本和数据维度之间的权衡关系。设计的估计协议在样本和通信要求上与下界匹配至对数因子。交互式协议相比非交互式方案能显著降低通信需求。

Conclusion: 本文建立了分布式协方差估计问题的理论基础，提出的C-SDPI系数为分析通信受限下的统计推断提供了新工具。框架不依赖无限样本或高斯分布假设，具有广泛适用性，为实际应用中的分布式统计学习提供了理论指导。

Abstract: Estimating high-dimensional covariance matrices is a key task across many
fields. This paper explores the theoretical limits of distributed covariance
estimation in a feature-split setting, where communication between agents is
constrained. Specifically, we study a scenario in which multiple agents each
observe different components of i.i.d. samples drawn from a sub-Gaussian random
vector. A central server seeks to estimate the complete covariance matrix using
a limited number of bits communicated by each agent. We obtain a nearly tight
minimax lower bound for covariance matrix estimation under operator norm and
Frobenius norm. Our main technical tool is a novel generalization of the strong
data processing inequality (SDPI), termed the Conditional Strong Data
Processing Inequality (C-SDPI) coefficient, introduced in this work. The C-SDPI
coefficient shares key properties such as tensorization with the conventional
SDPI. Crucially, it quantifies the average contraction in a state-dependent
channel and can be significantly lower than the worst-case SDPI coefficient
over the state input.
  Utilizing the doubling trick of Geng-Nair and an operator Jensen inequality,
we compute this coefficient for Gaussian mixture channels. We then employ it to
establish minimax lower bounds on estimation error, capturing the trade-offs
among sample size, communication cost, and data dimensionality. Building on
this, we present a nearly optimal estimation protocol whose sample and
communication requirements match the lower bounds up to logarithmic factors.
Unlike much of the existing literature, our framework does not assume infinite
samples or Gaussian distributions, making it broadly applicable. Finally, we
extend our analysis to interactive protocols, showing interaction can
significantly reduce communication requirements compared to non-interactive
schemes.

</details>


### [149] [Bayesian preference elicitation for decision support in multiobjective optimization](https://arxiv.org/abs/2507.16999)
*Felix Huber,Sebastian Rojas Gonzalez,Raul Astudillo*

Main category: stat.ML

TL;DR: 本文提出了一种基于贝叶斯模型的交互式方法，通过成对比较来估计决策者的效用函数，从而高效地从多目标优化的帕累托集中识别偏好解决方案。


<details>
  <summary>Details</summary>
Motivation: 在多目标优化问题中，决策者面临从帕累托集中选择偏好解决方案的挑战。现有方法往往效率不高，需要大量查询才能找到高效用的解决方案，因此需要一种更高效的交互式方法来帮助决策者快速识别满足其偏好的最优解。

Method: 使用贝叶斯模型基于成对比较来估计决策者的效用函数；采用原则性的启发策略交互式地选择查询，平衡探索和利用；该方法既可以交互式使用，也可以在通过标准多目标优化技术估计帕累托前沿后事后使用；在启发阶段结束时生成高质量解决方案的精简菜单。

Result: 在最多九个目标的测试问题上的实验表明，该方法在用少量查询找到高效用解决方案方面表现优异；提供了开源实现以支持更广泛社区的采用。

Conclusion: 该方法成功地解决了多目标优化中的偏好识别问题，通过贝叶斯建模和智能查询策略显著提高了决策效率，为决策者提供了一个实用且高效的工具来从复杂的帕累托集中识别最佳解决方案。

Abstract: We present a novel approach to help decision-makers efficiently identify
preferred solutions from the Pareto set of a multi-objective optimization
problem. Our method uses a Bayesian model to estimate the decision-maker's
utility function based on pairwise comparisons. Aided by this model, a
principled elicitation strategy selects queries interactively to balance
exploration and exploitation, guiding the discovery of high-utility solutions.
The approach is flexible: it can be used interactively or a posteriori after
estimating the Pareto front through standard multi-objective optimization
techniques. Additionally, at the end of the elicitation phase, it generates a
reduced menu of high-quality solutions, simplifying the decision-making
process. Through experiments on test problems with up to nine objectives, our
method demonstrates superior performance in finding high-utility solutions with
a small number of queries. We also provide an open-source implementation of our
method to support its adoption by the broader community.

</details>


### [150] [The surprising strength of weak classifiers for validating neural posterior estimates](https://arxiv.org/abs/2507.17026)
*Vansh Bansal,Tianyu Chen,James G. Scott*

Main category: stat.ML

TL;DR: 该论文提出了Conformal C2ST方法，通过将任何训练分类器的输出转换为精确的有限样本p值，解决了神经后验估计验证中弱分类器的可靠性问题，证明即使是弱分类器也能提供强大且可靠的测试结果。


<details>
  <summary>Details</summary>
Motivation: 神经后验估计(NPE)在贝叶斯推断中应用广泛，但验证其准确性仍具挑战性。现有的分类器双样本测试(C2ST)方法需要近似贝叶斯最优分类器，这一要求在实践中很难满足且难以验证。因此，一个重要的开放问题是：弱分类器是否仍能用于神经后验验证？

Method: 基于Hu和Lei的工作，提出Conformal C2ST的保形变体方法。该方法将任何训练分类器的分数（即使是弱分类器或过拟合模型）转换为精确的有限样本p值，建立了保形C2ST的两个关键理论性质：有限样本I型错误控制和与分类器误差相关的非平凡功效。

Result: 理论上证明了Conformal C2ST具有有限样本I型错误控制和随分类器误差温和退化的非平凡功效。实验结果显示，Conformal C2ST在多个基准测试中优于经典判别测试，证明了即使是弱的、有偏的或过拟合的分类器仍能产生强大且可靠的测试结果。

Conclusion: 该研究揭示了弱分类器在验证神经后验估计方面被低估的优势，将Conformal C2ST确立为现代基于仿真推断的实用且理论基础扎实的诊断工具，为神经后验估计验证提供了新的解决方案。

Abstract: Neural Posterior Estimation (NPE) has emerged as a powerful approach for
amortized Bayesian inference when the true posterior $p(\theta \mid y)$ is
intractable or difficult to sample. But evaluating the accuracy of neural
posterior estimates remains challenging, with existing methods suffering from
major limitations. One appealing and widely used method is the classifier
two-sample test (C2ST), where a classifier is trained to distinguish samples
from the true posterior $p(\theta \mid y)$ versus the learned NPE approximation
$q(\theta \mid y)$. Yet despite the appealing simplicity of the C2ST, its
theoretical and practical reliability depend upon having access to a
near-Bayes-optimal classifier -- a requirement that is rarely met and, at best,
difficult to verify. Thus a major open question is: can a weak classifier still
be useful for neural posterior validation? We show that the answer is yes.
Building on the work of Hu and Lei, we present several key results for a
conformal variant of the C2ST, which converts any trained classifier's scores
-- even those of weak or over-fitted models -- into exact finite-sample
p-values. We establish two key theoretical properties of the conformal C2ST:
(i) finite-sample Type-I error control, and (ii) non-trivial power that
degrades gently in tandem with the error of the trained classifier. The upshot
is that even weak, biased, or overfit classifiers can still yield powerful and
reliable tests. Empirically, the Conformal C2ST outperforms classical
discriminative tests across a wide range of benchmarks. These results reveal
the under appreciated strength of weak classifiers for validating neural
posterior estimates, establishing the conformal C2ST as a practical,
theoretically grounded diagnostic for modern simulation-based inference.

</details>


### [151] [CoLT: The conditional localization test for assessing the accuracy of neural posterior estimates](https://arxiv.org/abs/2507.17030)
*Tianyu Chen,Vansh Bansal,James G. Scott*

Main category: stat.ML

TL;DR: 该论文提出了条件定位测试(CoLT)方法，用于验证神经后验估计q(θ|x)是否准确逼近真实后验分布p(θ|x)，通过学习定位函数自适应选择神经后验偏差最大的点，在仿真推理设置中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的神经后验估计(NPE)质量评估方法主要基于分类器测试或散度度量，存在实际应用中的诸多缺陷。需要一种原则性的方法来检测神经后验估计与真实后验分布在所有条件输入范围内的差异，特别是在仅能从真实后验获得单次采样但可从神经后验进行任意次采样的仿真推理场景中。

Method: 提出条件定位测试(CoLT)方法，学习一个定位函数来自适应选择点θ_l(x)，这些点是神经后验q相对于真实后验p在给定x下偏差最大的位置。该方法避免了在每个x处进行穷尽比较或密度估计，而是专注于识别最显著的差异区域。

Result: 理论上建立了评估所有x上分布相等性的充分必要条件，提供了严格的保证和实际的可扩展性。实证实验表明，CoLT在比较p和q方面不仅优于现有方法，还能精确定位显著分歧的区域，为模型改进提供可操作的见解。

Conclusion: CoLT作为验证神经后验估计的最先进解决方案，不仅在性能上超越现有方法，还能提供关于模型偏差位置的详细信息，这对于神经后验估计的验证和改进具有重要价值，特别适用于仿真推理场景。

Abstract: We consider the problem of validating whether a neural posterior estimate \(
q(\theta \mid x) \) is an accurate approximation to the true, unknown true
posterior \( p(\theta \mid x) \). Existing methods for evaluating the quality
of an NPE estimate are largely derived from classifier-based tests or
divergence measures, but these suffer from several practical drawbacks. As an
alternative, we introduce the \emph{Conditional Localization Test} (CoLT), a
principled method designed to detect discrepancies between \( p(\theta \mid x)
\) and \( q(\theta \mid x) \) across the full range of conditioning inputs.
Rather than relying on exhaustive comparisons or density estimation at every \(
x \), CoLT learns a localization function that adaptively selects points
$\theta_l(x)$ where the neural posterior $q$ deviates most strongly from the
true posterior $p$ for that $x$. This approach is particularly advantageous in
typical simulation-based inference settings, where only a single draw \( \theta
\sim p(\theta \mid x) \) from the true posterior is observed for each
conditioning input, but where the neural posterior \( q(\theta \mid x) \) can
be sampled an arbitrary number of times. Our theoretical results establish
necessary and sufficient conditions for assessing distributional equality
across all \( x \), offering both rigorous guarantees and practical
scalability. Empirically, we demonstrate that CoLT not only performs better
than existing methods at comparing $p$ and $q$, but also pinpoints regions of
significant divergence, providing actionable insights for model refinement.
These properties position CoLT as a state-of-the-art solution for validating
neural posterior estimates.

</details>


### [152] [Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler Divergence with High Probability](https://arxiv.org/abs/2507.17316)
*Dirk van der Hoeven,Julia Olkhovskaia,Tim van Erven*

Main category: stat.ML

TL;DR: 本文研究离散分布估计问题，提供了KL散度下的上界和下界，并提出了一个基于在线到批处理转换的高效估计器，在理论上接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 在统计学习中，准确估计离散分布是一个基础且重要的问题。现有方法在KL散度意义下的理论保证不够充分，需要建立更精确的上界和下界，并设计在理论和计算上都优秀的估计算法。

Method: 1) 建立了离散分布估计在KL散度下的理论下界；2) 提出了基于在线到批处理转换(Online to Batch conversion)和后缀平均的计算高效估计器p^OTB；3) 分析了最大似然估计器在χ²散度和KL散度下的性能保证。

Result: 证明了任何估计器在最坏情况下的KL散度下界为C·max{K,ln(K)ln(1/δ)}/n；提出的p^OTB估计器达到上界C(K·log(log(K)) + ln(K)ln(1/δ))/n，在理论上接近最优；最大似然估计器在充足样本下也能获得良好的KL散度界。

Conclusion: 本文为离散分布估计问题提供了几乎最优的理论界，所提出的OTB估计器在保持计算效率的同时达到了接近理论下界的性能，为实际应用提供了理论指导和有效算法。

Abstract: We consider the problem of estimating a discrete distribution $p$ with
support of size $K$ and provide both upper and lower bounds with high
probability in KL divergence. We prove that in the worst case, for any
estimator $\widehat{p}$, with probability at least $\delta$, $\text{KL}(p \|
\widehat{p}) \geq C\max\{K,\ln(K)\ln(1/\delta) \}/n $, where $n$ is the sample
size and $C > 0$ is a constant. We introduce a computationally efficient
estimator $p^{\text{OTB}}$, based on Online to Batch conversion and suffix
averaging, and show that with probability at least $1 - \delta$ $\text{KL}(p \|
\widehat{p}) \leq C(K\log(\log(K)) + \ln(K)\ln(1/\delta)) /n$.
  Furthermore, we also show that with sufficiently many observations relative
to $\log(1/\delta)$, the maximum likelihood estimator $\bar{p}$ guarantees that
with probability at least $1-\delta$ $$
  1/6 \chi^2(\bar{p}\|p) \leq 1/4 \chi^2(p\|\bar{p}) \leq \text{KL}(p|\bar{p})
\leq C(K + \log(1/\delta))/n\,, $$ where $\chi^2$ denotes the
$\chi^2$-divergence.

</details>


### [153] [To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks](https://arxiv.org/abs/2507.17494)
*Rashika Raina,Nidhi Simmons,David E. Simmons,Michel Daoud Yacoub,Trung Q. Duong*

Main category: stat.ML

TL;DR: 研究了下一代通信网络中机器学习模型的校准性能，重点分析了在单用户多资源分配框架下基于ML的中断预测器的校准表现，建立了理论框架并通过仿真验证。


<details>
  <summary>Details</summary>
Motivation: 下一代通信网络需要机器学习模型不仅提供准确预测，还要提供反映真实正确决策可能性的良好校准置信度分数，因此需要研究ML中断预测器的校准性能以满足系统可靠性要求。

Method: 建立单用户多资源分配框架下的理论模型，推导完美校准预测器的中断概率条件，分析准确性-置信度函数的单调性条件，使用Platt缩放和等温回归等后处理校准技术，在考虑接收器移动性的Rayleigh衰落信道上进行仿真验证。

Result: 证明了随着资源数量增长，完美校准预测器的中断概率趋向于低于分类阈值条件下的期望输出；单资源时中断概率等于模型整体期望输出；后处理校准无法改善系统最小可达到的中断概率；良好校准的模型属于能够改善中断概率的更广泛预测器类别。

Conclusion: 建立了ML中断预测器校准性能的完整理论框架，为系统设计者选择分类阈值以达到期望中断概率提供指导，证明了校准模型在改善通信系统可靠性方面的重要性，为下一代通信网络中ML模型的实际应用提供了理论基础。

Abstract: In next-generation communications and networks, machine learning (ML) models
are expected to deliver not only accurate predictions but also well-calibrated
confidence scores that reflect the true likelihood of correct decisions. This
paper studies the calibration performance of an ML-based outage predictor
within a single-user, multi-resource allocation framework. We first establish
key theoretical properties of this system's outage probability (OP) under
perfect calibration. Importantly, we show that as the number of resources
grows, the OP of a perfectly calibrated predictor approaches the expected
output conditioned on it being below the classification threshold. In contrast,
when only one resource is available, the system's OP equals the model's overall
expected output. We then derive the OP conditions for a perfectly calibrated
predictor. These findings guide the choice of the classification threshold to
achieve a desired OP, helping system designers meet specific reliability
requirements. We also demonstrate that post-processing calibration cannot
improve the system's minimum achievable OP, as it does not introduce new
information about future channel states. Additionally, we show that
well-calibrated models are part of a broader class of predictors that
necessarily improve OP. In particular, we establish a monotonicity condition
that the accuracy-confidence function must satisfy for such improvement to
occur. To demonstrate these theoretical properties, we conduct a rigorous
simulation-based analysis using post-processing calibration techniques: Platt
scaling and isotonic regression. As part of this framework, the predictor is
trained using an outage loss function specifically designed for this system.
Furthermore, this analysis is performed on Rayleigh fading channels with
temporal correlation captured by Clarke's 2D model, which accounts for receiver
mobility.

</details>


### [154] [Optimal differentially private kernel learning with random projection](https://arxiv.org/abs/2507.17544)
*Bonwoo Lee,Cheolwoo Park,Jeongyoun Ahn*

Main category: stat.ML

TL;DR: 本文提出了一种基于随机投影的差分隐私核学习算法，在保证隐私的同时实现了最优的泛化性能，并证明了现有方法的次优性。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私核学习算法在隐私保护和学习性能之间的权衡不够理想，需要开发能够在经验风险最小化框架下实现最优泛化性能的差分隐私算法。

Method: 提出了一种基于再生核希尔伯特空间中随机投影的新型差分隐私核经验风险最小化算法，使用高斯过程进行降维处理。

Result: 该算法在局部强凸条件下对平方损失和Lipschitz光滑凸损失函数都达到了minimax最优的超额风险；证明了基于随机傅里叶特征映射或L2正则化的现有方法表现次优；首次推导出基于目标扰动的私有线性ERM的无维度泛化界。

Conclusion: 随机投影技术能够实现统计高效且最优隐私的核学习，为差分隐私算法设计提供了新见解，强调了降维技术在平衡隐私和效用方面的核心作用。

Abstract: Differential privacy has become a cornerstone in the development of
privacy-preserving learning algorithms. This work addresses optimizing
differentially private kernel learning within the empirical risk minimization
(ERM) framework. We propose a novel differentially private kernel ERM algorithm
based on random projection in the reproducing kernel Hilbert space using
Gaussian processes. Our method achieves minimax-optimal excess risk for both
the squared loss and Lipschitz-smooth convex loss functions under a local
strong convexity condition. We further show that existing approaches based on
alternative dimension reduction techniques, such as random Fourier feature
mappings or $\ell_2$ regularization, yield suboptimal generalization
performance. Our key theoretical contribution also includes the derivation of
dimension-free generalization bounds for objective perturbation-based private
linear ERM -- marking the first such result that does not rely on noisy
gradient-based mechanisms. Additionally, we obtain sharper generalization
bounds for existing differentially private kernel ERM algorithms. Empirical
evaluations support our theoretical claims, demonstrating that random
projection enables statistically efficient and optimally private kernel
learning. These findings provide new insights into the design of differentially
private algorithms and highlight the central role of dimension reduction in
balancing privacy and utility.

</details>


### [155] [Debiased maximum-likelihood estimators for hazard ratios under machine-learning adjustment](https://arxiv.org/abs/2507.17686)
*Takashi Hayakawa,Satoshi Asai*

Main category: stat.ML

TL;DR: 该研究提出了一种基于机器学习的新方法来解决Cox模型中风险比不可解释的问题，通过显式建模风险集变化并使用Neyman正交性计算去偏最大似然估计器，为现代流行病学中观察性数据的因果推断提供了新的解决方案。


<details>
  <summary>Details</summary>
Motivation: Cox模型估计的治疗组间风险比存在不可解释性问题，因为模型的不确定基线风险无法识别由治疗分配和未观察因素导致的风险集组成的时间变化，特别是在基于观察性数据、具有不受控动态治疗和实时测量多个协变量的研究中，这一问题更加突出。

Method: 研究者提出放弃基线风险的方法，使用机器学习显式建模风险集的变化（包含或不包含潜在变量）。基于Neyman正交性原理开发了一种计算去偏最大似然估计器的方法，并阐明了风险比可以进行因果解释的背景条件。

Result: 数值模拟结果证实所提出的方法能够以最小偏差识别真实值。该方法在计算效率上优于基于边际结构Cox模型的加权回归方法构建的估计器。

Conclusion: 研究为现代流行病学中使用不受控观察性数据进行因果推断奠定了基础，提供了一种有用的替代方法，解决了传统Cox模型在处理复杂观察性数据时的局限性。

Abstract: Previous studies have shown that hazard ratios between treatment groups
estimated with the Cox model are uninterpretable because the indefinite
baseline hazard of the model fails to identify temporal change in the risk set
composition due to treatment assignment and unobserved factors among multiple,
contradictory scenarios. To alleviate this problem, especially in studies based
on observational data with uncontrolled dynamic treatment and real-time
measurement of many covariates, we propose abandoning the baseline hazard and
using machine learning to explicitly model the change in the risk set with or
without latent variables. For this framework, we clarify the context in which
hazard ratios can be causally interpreted, and then develop a method based on
Neyman orthogonality to compute debiased maximum-likelihood estimators of
hazard ratios. Computing the constructed estimators is more efficient than
computing those based on weighted regression with marginal structural Cox
models. Numerical simulations confirm that the proposed method identifies the
ground truth with minimal bias. These results lay the foundation for developing
a useful, alternative method for causal inference with uncontrolled,
observational data in modern epidemiology.

</details>


### [156] [Sequential Bayesian Design for Efficient Surrogate Construction in the Inversion of Darcy Flows](https://arxiv.org/abs/2507.17713)
*Hongji Wang,Hongqiao Wang,Jinyong Ying,Qingping Zhou*

Main category: stat.ML

TL;DR: 本文提出了一种基于序贯贝叶斯设计的局部精确代理模型方法(SBD-LAS)，用于解决由偏微分方程控制的反问题，特别是达西流方程相关问题，在保证反演精度的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法求解偏微分方程反问题需要大量计算昂贵的正向求解器评估，而构建全局精确的代理模型需要高模型复杂度和大量训练数据，因此需要开发一种计算成本更低但仍能保证精度的替代方法。

Method: 提出了一种专注于反问题真实似然高概率区域的局部精确代理模型，具有相对较低的模型复杂度和较少的训练数据需求；同时引入序贯贝叶斯设计策略来获取该代理模型，将序贯贝叶斯设计的后验演化过程视为高斯过程，通过一步前瞻先验实现算法加速。

Result: 通过三个基于达西流方程的实验验证，所提出的SBD-LAS方法在反演精度和计算速度方面都表现出明显优势。

Conclusion: SBD-LAS方法成功解决了高维复杂偏微分方程反问题中代理模型构建的挑战，在保持较高反演精度的同时大幅降低了计算成本，为相关领域的实际应用提供了有效的解决方案。

Abstract: Inverse problems governed by partial differential equations (PDEs) play a
crucial role in various fields, including computational science, image
processing, and engineering. Particularly, Darcy flow equation is a fundamental
equation in fluid mechanics, which plays a crucial role in understanding fluid
flow through porous media. Bayesian methods provide an effective approach for
solving PDEs inverse problems, while their numerical implementation requires
numerous evaluations of computationally expensive forward solvers. Therefore,
the adoption of surrogate models with lower computational costs is essential.
However, constructing a globally accurate surrogate model for high-dimensional
complex problems demands high model capacity and large amounts of data. To
address this challenge, this study proposes an efficient locally accurate
surrogate that focuses on the high-probability regions of the true likelihood
in inverse problems, with relatively low model complexity and few training data
requirements. Additionally, we introduce a sequential Bayesian design strategy
to acquire the proposed surrogate since the high-probability region of the
likelihood is unknown. The strategy treats the posterior evolution process of
sequential Bayesian design as a Gaussian process, enabling algorithmic
acceleration through one-step ahead prior. The complete algorithmic framework
is referred to as Sequential Bayesian design for locally accurate surrogate
(SBD-LAS). Finally, three experiments based the Darcy flow equation demonstrate
the advantages of the proposed method in terms of both inversion accuracy and
computational speed.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [157] [Does Language Matter for Early Detection of Parkinson's Disease from Speech?](https://arxiv.org/abs/2507.16832)
*Peter Plantinga,Briac Cordelle,Dominique Louër,Mirco Ravanelli,Denise Klein*

Main category: eess.AS

TL;DR: 研究发现语言在帕金森病早期检测中起关键作用，文本模型与语音特征模型性能相当，多语言Whisper模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 帕金森病语音生物标志物检测存在数据收集和分析方法的争议，需要评估语言在PD检测中的作用并比较不同模型性能

Method: 使用不同数据类型和预训练目标的预训练模型进行测试，包括文本模型、语音特征模型、多语言和单语言Whisper模型，以及AudioSet预训练模型

Result: 1）纯文本模型与语音特征模型性能相当；2）多语言Whisper优于自监督模型，单语言Whisper表现较差；3）AudioSet预训练在持续元音发音任务上有效，但在自发语音上无效

Conclusion: 语言在帕金森病早期检测中发挥关键作用，为基于语音的PD检测提供了新的研究方向和方法选择指导

Abstract: Using speech samples as a biomarker is a promising avenue for detecting and
monitoring the progression of Parkinson's disease (PD), but there is
considerable disagreement in the literature about how best to collect and
analyze such data. Early research in detecting PD from speech used a sustained
vowel phonation (SVP) task, while some recent research has explored recordings
of more cognitively demanding tasks. To assess the role of language in PD
detection, we tested pretrained models with varying data types and pretraining
objectives and found that (1) text-only models match the performance of
vocal-feature models, (2) multilingual Whisper outperforms self-supervised
models whereas monolingual Whisper does worse, and (3) AudioSet pretraining
improves performance on SVP but not spontaneous speech. These findings together
highlight the critical role of language for the early detection of Parkinson's
disease.

</details>


### [158] [From Black Box to Biomarker: Sparse Autoencoders for Interpreting Speech Models of Parkinson's Disease](https://arxiv.org/abs/2507.16836)
*Peter Plantinga,Jen-Kai Chen,Roozbeh Sattari,Mirco Ravanelli,Denise Klein*

Main category: eess.AS

TL;DR: 该研究使用稀疏自编码器(SAE)来解释基于语音的帕金森病检测深度学习系统，发现了与帕金森病语音特征相关的可解释内部表示，并证明了这些特征与脑部MRI扫描的临床指标存在关联。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统虽然能从原始音频中发现手工特征无法捕获的细微信号来检测帕金森病，但其黑盒性质阻碍了临床应用。需要提高模型的可解释性以促进临床采用。

Method: 将稀疏自编码器(SAE)应用于语音帕金森病检测系统，引入新颖的基于掩码的激活方法来适应小规模生物医学数据集，创建稀疏解耦的字典表示。

Result: 发现字典条目与帕金森病语音的特征性发音缺陷有强关联，如模型注意力突出的低能量区域中频谱通量降低和频谱平坦度增加。频谱通量与MRI扫描中壳核的体积测量相关。

Conclusion: 稀疏自编码器能够揭示临床相关的生物标志物，为疾病监测和诊断提供了潜在价值。该方法成功提高了深度学习语音分析系统的可解释性，有助于临床应用。

Abstract: Speech holds promise as a cost-effective and non-invasive biomarker for
neurological conditions such as Parkinson's disease (PD). While deep learning
systems trained on raw audio can find subtle signals not available from
hand-crafted features, their black-box nature hinders clinical adoption. To
address this, we apply sparse autoencoders (SAEs) to uncover interpretable
internal representations from a speech-based PD detection system. We introduce
a novel mask-based activation for adapting SAEs to small biomedical datasets,
creating sparse disentangled dictionary representations. These dictionary
entries are found to have strong associations with characteristic articulatory
deficits in PD speech, such as reduced spectral flux and increased spectral
flatness in the low-energy regions highlighted by the model attention. We
further show that the spectral flux is related to volumetric measurements of
the putamen from MRI scans, demonstrating the potential of SAEs to reveal
clinically relevant biomarkers for disease monitoring and diagnosis.

</details>


### [159] [Enhancing Lung Disease Diagnosis via Semi-Supervised Machine Learning](https://arxiv.org/abs/2507.16845)
*Xiaoran Xua,In-Ho Rab,Ravi Sankarc*

Main category: eess.AS

TL;DR: 该研究使用半监督学习方法结合MFCC+CNN模型进行肺音信号检测，通过引入Mix Match、Co-Refinement和Co Refurbishing等半监督模块，将检测准确率提升至92.9%，相比基线模型提高了3.8%，为肺部疾病声音检测提供了新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的肺部疾病诊断方法成本高、耗时长且具有侵入性，同时面临个体差异大、标注数据不足等挑战，因此需要开发更高效、准确的肺音检测方法来辅助肺癌和COPD等肺部疾病的诊断。

Method: 采用MFCC+CNN作为基础模型，并引入半监督学习模块包括Mix Match、Co-Refinement和Co Refurbishing，通过半监督学习方法减少对人工标注的依赖，提升肺音信号检测性能。

Result: 添加半监督学习模块后，MFCC+CNN模型的准确率达到92.9%，相比基线模型提升了3.8%，显著改善了肺音检测的性能表现。

Conclusion: 半监督学习方法能够有效提升肺音信号检测的准确性，为肺部疾病的声音检测领域做出贡献，解决了个体差异和标注数据不足等关键挑战，为临床诊断提供了更好的辅助工具。

Abstract: Lung diseases, including lung cancer and COPD, are significant health
concerns globally. Traditional diagnostic methods can be costly,
time-consuming, and invasive. This study investigates the use of semi
supervised learning methods for lung sound signal detection using a model
combination of MFCC+CNN. By introducing semi supervised learning modules such
as Mix Match, Co-Refinement, and Co Refurbishing, we aim to enhance the
detection performance while reducing dependence on manual annotations. With the
add-on semi-supervised modules, the accuracy rate of the MFCC+CNN model is
92.9%, an increase of 3.8% to the baseline model. The research contributes to
the field of lung disease sound detection by addressing challenges such as
individual differences, feature insufficient labeled data.

</details>


### [160] [Technical report: Impact of Duration Prediction on Speaker-specific TTS for Indian Languages](https://arxiv.org/abs/2507.16875)
*Isha Pandey,Pranav Gaikwad,Amruta Parulekar,Ganesh Ramakrishnan*

Main category: eess.AS

TL;DR: 研究针对印度等低资源语言的高质量语音生成问题，通过训练基于连续标准化流(CNF)的非自回归语音模型，比较了多种持续时间预测策略在零样本说话人特定生成中的效果。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如许多印度语言）由于数据有限和语言结构多样化，高质量语音生成仍然是一个重大挑战。持续时间预测是语音生成管道中的关键组件，在建模韵律和语音节奏方面发挥重要作用。虽然一些生成方法选择省略显式持续时间建模，但往往以更长的训练时间为代价。

Method: 使用公开可用的印度语言数据训练基于连续标准化流(CNF)的非自回归语音模型，并评估多种持续时间预测策略用于零样本、说话人特定生成。通过语音填充任务进行比较分析。

Result: 比较分析揭示了细微的权衡：基于填充的预测器在某些语言中提高了可懂度，而说话人提示预测器在其他语言中更好地保持了说话人特征。

Conclusion: 研究结果为针对特定语言和任务定制持续时间策略的设计和选择提供了指导，强调了在将先进生成架构适应低资源、多语言环境时，持续时间预测等可解释组件的持续价值。

Abstract: High-quality speech generation for low-resource languages, such as many
Indian languages, remains a significant challenge due to limited data and
diverse linguistic structures. Duration prediction is a critical component in
many speech generation pipelines, playing a key role in modeling prosody and
speech rhythm. While some recent generative approaches choose to omit explicit
duration modeling, often at the cost of longer training times. We retain and
explore this module to better understand its impact in the linguistically rich
and data-scarce landscape of India. We train a non-autoregressive Continuous
Normalizing Flow (CNF) based speech model using publicly available Indian
language data and evaluate multiple duration prediction strategies for
zero-shot, speaker-specific generation. Our comparative analysis on
speech-infilling tasks reveals nuanced trade-offs: infilling based predictors
improve intelligibility in some languages, while speaker-prompted predictors
better preserve speaker characteristics in others. These findings inform the
design and selection of duration strategies tailored to specific languages and
tasks, underscoring the continued value of interpretable components like
duration prediction in adapting advanced generative architectures to
low-resource, multilingual settings.

</details>


### [161] [Clustering-based hard negative sampling for supervised contrastive speaker verification](https://arxiv.org/abs/2507.17540)
*Piotr Masztalski,Michał Romaniuk,Jakub Żak,Mateusz Matuszewski,Konrad Kowalczyk*

Main category: eess.AS

TL;DR: 本文提出了CHNS方法，通过聚类相似说话人嵌入并调整批次组成来优化对比学习中的困难负样本采样，在VoxCeleb数据集上相比基线方法实现了18%的相对EER和minDCF改进。


<details>
  <summary>Details</summary>
Motivation: 在说话人验证中，对比学习作为传统分类方法的替代方案日益流行。对比方法可以从有效使用困难负样本对中受益，这些是由于相似性而对验证模型特别具有挑战性的不同类别样本。现有方法在困难负样本采样方面还有改进空间。

Method: 提出CHNS（基于聚类的困难负样本采样）方法，专门用于监督对比说话人表示学习。该方法对相似说话人的嵌入进行聚类，并调整批次组成以在对比损失计算期间获得困难和简单负样本的最优比例。

Result: 在VoxCeleb数据集上使用两种轻量级模型架构，CHNS方法在相对EER和minDCF指标上比基线监督对比方法（无论是否使用基于损失的困难负样本采样）以及最先进的基于分类的说话人验证方法都有高达18%的改进。

Conclusion: CHNS通过聚类和批次组成调整有效改进了监督对比学习中的困难负样本采样，显著提升了说话人验证性能，证明了该方法在轻量级模型架构上的有效性。

Abstract: In speaker verification, contrastive learning is gaining popularity as an
alternative to the traditionally used classification-based approaches.
Contrastive methods can benefit from an effective use of hard negative pairs,
which are different-class samples particularly challenging for a verification
model due to their similarity. In this paper, we propose CHNS - a
clustering-based hard negative sampling method, dedicated for supervised
contrastive speaker representation learning. Our approach clusters embeddings
of similar speakers, and adjusts batch composition to obtain an optimal ratio
of hard and easy negatives during contrastive loss calculation. Experimental
evaluation shows that CHNS outperforms a baseline supervised contrastive
approach with and without loss-based hard negative sampling, as well as a
state-of-the-art classification-based approach to speaker verification by as
much as 18 % relative EER and minDCF on the VoxCeleb dataset using two
lightweight model architectures.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [162] [Symmetric Private Information Retrieval (SPIR) on Graph-Based Replicated Systems](https://arxiv.org/abs/2507.17736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 本文研究了基于图模型的复制数据库上的对称私密信息检索(SPIR)问题，建立了下界并推导出路径图和正则图的精确SPIR容量


<details>
  <summary>Details</summary>
Motivation: 现有的私密信息检索研究主要集中在传统的数据库模型上，缺乏对复制数据库环境下对称私密信息检索的理论分析，特别是在图模型框架下考虑消息特定公共随机性的情况

Method: 提出了基于简单图的复制数据库SPIR模型，其中顶点对应服务器，边表示消息复制关系；设计了可达到的SPIR方案来建立容量下界；分析了消息特定公共随机性的最小尺寸要求；为路径图和正则图提供了匹配的上界证明

Result: 建立了一般图的SPIR容量下界；证明了任何可行SPIR方案的消息特定随机性最小尺寸必须等于消息尺寸；推导出路径图和正则图的精确SPIR容量

Conclusion: 在图模型的复制数据库环境下，成功建立了SPIR的理论框架，确定了消息特定公共随机性的必要条件，并为特定图类（路径图和正则图）获得了精确的容量结果，为该领域提供了重要的理论基础

Abstract: We introduce the problem of symmetric private information retrieval (SPIR) on
replicated databases modeled by a simple graph. In this model, each vertex
corresponds to a server, and a message is replicated on two servers if and only
if there is an edge between them. We consider the setting where the server-side
common randomness necessary to accomplish SPIR is also replicated at the
servers according to the graph, and we call this as message-specific common
randomness. In this setting, we establish a lower bound on the SPIR capacity,
i.e., the maximum download rate, for general graphs, by proposing an achievable
SPIR scheme. Next, we prove that, for any SPIR scheme to be feasible, the
minimum size of message-specific randomness should be equal to the size of a
message. Finally, by providing matching upper bounds, we derive the exact SPIR
capacity for the class of path and regular graphs.

</details>


### [163] [Fast One-Pass Sparse Approximation of the Top Eigenvectors of Huge Low-Rank Matrices? Yes, $MAM^*$!](https://arxiv.org/abs/2507.17036)
*Edem Boahen,Simone Brugiapaglia,Hung-Hsu Chou,Mark Iwen,Felix Krahmer*

Main category: cs.IT

TL;DR: 本文提出了基于压缩感知的单次扫描算法，用于超大规模矩阵的稀疏主特征向量近似，能够在有限内存下处理无法完全存储的巨型矩阵


<details>
  <summary>Details</summary>
Motivation: 针对稀疏PCA等应用场景，需要处理极其庞大的矩阵（如10^16个元素），这些矩阵无法完全存储在内存中，因此需要开发能够在单次扫描下高效计算稀疏主特征向量近似的算法

Method: 采用基于压缩感知的方法，通过单个紧凑的线性素描（linear sketch）来近似超大规模近似低秩矩阵的主特征向量，压缩感知恢复算法的运行时间主要取决于所求稀疏近似的大小，使其运行时间相对于大矩阵尺寸呈亚线性关系

Result: 在包含约10^16个元素的超大矩阵上进行的初步实验验证了所提出理论的有效性，展示了该方法的实际应用潜力，能够在内存占用仅为稀疏特征向量近似大小的情况下完成计算

Conclusion: 提出的基于压缩感知的单次扫描算法能够有效处理无法完全存储的超大规模矩阵的稀疏主特征向量近似问题，在理论上提供了准确性保证，在实践中展现了处理极大规模数据的潜力

Abstract: Motivated by applications such as sparse PCA, in this paper we present
provably-accurate one-pass algorithms for the sparse approximation of the top
eigenvectors of extremely massive matrices based on a single compact linear
sketch. The resulting compressive-sensing-based approaches can approximate the
leading eigenvectors of huge approximately low-rank matrices that are too large
to store in memory based on a single pass over its entries while utilizing a
total memory footprint on the order of the much smaller desired sparse
eigenvector approximations. Finally, the compressive sensing recovery algorithm
itself (which takes the gathered compressive matrix measurements as input, and
then outputs sparse approximations of its top eigenvectors) can also be
formulated to run in a time which principally depends on the size of the sought
sparse approximations, making its runtime sublinear in the size of the large
matrix whose eigenvectors one aims to approximate. Preliminary experiments on
huge matrices having $\sim 10^{16}$ entries illustrate the developed theory and
demonstrate the practical potential of the proposed approach.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [164] [A Virtual Quantum Network Prototype for Open Access](https://arxiv.org/abs/2507.17495)
*Raj Kamleshkumar Madhu,Visuttha Manthamkarn,Zheshen Zhang,Jianqing Liu*

Main category: quant-ph

TL;DR: 本文提出了一个开放访问的基于软件的量子网络虚拟化平台，通过云应用虚拟化实验室规模量子网络测试平台的核心硬件组件，使用匈牙利算法确保用户间公平的资源分配，解决了量子网络扩展中的专业人才短缺、基础设施访问受限和高成本等问题。


<details>
  <summary>Details</summary>
Motivation: 当前量子网络系统规模有限、高度应用特定化（如量子密钥分发）、缺乏全球扩展的清晰路线图。这些限制主要由专业人才短缺、量子基础设施访问受限以及构建和运营量子硬件的高复杂性和成本驱动。

Method: 提出一个开放访问的基于软件的量子网络虚拟化平台，通过云应用虚拟化实验室规模量子网络测试平台的核心硬件组件（包括时间标记器和光开关），使用户能够远程执行光子纠缠的符合计数，并采用匈牙利算法确保用户间公平的资源分配。

Result: 从硬件、软件和云平台的角度提供了实现细节和性能分析，证明了所开发原型的功能性和效率。该系统能够实现远程量子硬件交互，并通过匈牙利算法在用户间分配几乎相等的有效纠缠率。

Conclusion: 开发的量子网络虚拟化平台原型成功解决了量子网络扩展中的关键挑战，通过软件虚拟化和云平台实现了可扩展的远程量子硬件访问，为量子网络的全球化发展提供了可行的技术路径。

Abstract: The rise of quantum networks has revolutionized domains such as
communication, sensing, and cybersecurity. Despite this progress, current
quantum network systems remain limited in scale, are highly
application-specific (e.g., for quantum key distribution), and lack a clear
road map for global expansion. These limitations are largely driven by a
shortage of skilled professionals, limited accessibility to quantum
infrastructure, and the high complexity and cost associated with building and
operating quantum hardware. To address these challenges, this paper proposes an
open-access software-based quantum network virtualization platform designed to
facilitate scalable and remote interaction with quantum hardware. The system is
built around a cloud application that virtualizes the core hardware components
of a lab-scale quantum network testbed, including the time tagger and optical
switch, enabling users to perform coincidence counts of the photon
entanglements while ensuring fair resource allocation. The fairness is ensured
by employing the Hungarian Algorithm to allocate nearly equal effective
entanglement rates among users. We provide implementation details and
performance analysis from the perspectives of hardware, software, and cloud
platform, which demonstrates the functionality and efficiency of the developed
prototype.

</details>


### [165] [Comparing performance of variational quantum algorithm simulations on HPC systems](https://arxiv.org/abs/2507.17614)
*Marco De Pascale,Tobias Valentin Bauer,Yaknan John Gambo,Mario Hernández Vera,Stefan Huber,Burak Mete,Amit Jamadagni,Amine Bentellis,Marita Oliv,Luigi Iapichino,Jeanette Miriam Lorenz*

Main category: quant-ph

TL;DR: 本文开发了一个通用工具链，用于在不同量子模拟器之间一致地移植变分量子算法问题定义，并通过三个实际案例验证了该工具的有效性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法在NISQ设备上具有重要应用价值，但其大量参数空间使得不同方法和软件模拟器之间的结果比较变得复杂且容易出错，缺乏统一的问题描述和移植方法。

Method: 采用哈密顿量和拟设的通用描述方法，开发了一个解析器工具链，能够在不同模拟器之间一致地移植问题定义。选择氢分子基态计算、MaxCut和旅行商问题三个用例，在多个HPC系统和软件模拟器上进行测试。

Result: 工具链成功实现了问题定义在不同模拟器间的转换。变分算法在扩展性方面受到长运行时间的限制，相对于内存占用暴露出有限的并行性，但可通过作业数组等技术部分缓解这一问题。

Conclusion: 开发的解析器工具在探索HPC性能和比较变分算法模拟结果方面展现出巨大潜力，为量子算法的跨平台研究提供了有效的解决方案。

Abstract: Variational quantum algorithms are of special importance in the research on
quantum computing applications because of their applicability to current Noisy
Intermediate-Scale Quantum (NISQ) devices. The main building blocks of these
algorithms (among them, the definition of the Hamiltonian and of the ansatz,
the optimizer) define a relatively large parameter space, making the comparison
of results and performance between different approaches and software simulators
cumbersome and prone to errors. In this paper, we employ a generic description
of the problem, in terms of both Hamiltonian and ansatz, to port a problem
definition consistently among different simulators. Three use cases of
relevance for current quantum hardware (ground state calculation for the
Hydrogen molecule, MaxCut, Travelling Salesman Problem) have been run on a set
of HPC systems and software simulators to study the dependence of performance
on the runtime environment, the scalability of the simulation codes and the
mutual agreement of the physical results, respectively. The results show that
our toolchain can successfully translate a problem definition between different
simulators. On the other hand, variational algorithms are limited in their
scaling by the long runtimes with respect to their memory footprint, so they
expose limited parallelism to computation. This shortcoming is partially
mitigated by using techniques like job arrays. The potential of the parser tool
for exploring HPC performance and comparisons of results of variational
algorithm simulations is highlighted.

</details>


### [166] [Demonstration of Efficient Predictive Surrogates for Large-scale Quantum Processors](https://arxiv.org/abs/2507.17470)
*Wei-You Liao,Yuxuan Du,Xinbiao Wang,Tian-Ci Tian,Yong Luo,Bo Du,Dacheng Tao,He-Liang Huang*

Main category: quant-ph

TL;DR: 研究者提出了预测性代理模型概念，使用经典学习模型来模拟量子处理器的平均值行为，大幅减少对量子处理器的依赖，并在数字量子模拟中展现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 大规模量子处理器制造成本高昂，在可预见的未来仍将稀缺，限制了量子计算的广泛应用。需要找到减少量子处理器使用需求的方法。

Method: 提出预测性代理模型概念，使用经典学习模型来模拟给定量子处理器的平均值行为，具有可证明的计算效率。开发了两种预测性代理模型来大幅减少实际场景中对量子处理器的需求。

Result: 成功模拟了多达20个可编程超导量子比特的量子处理器，实现了横向场伊辛模型族变分量子本征求解器的高效预训练，识别了非平衡Floquet对称保护拓扑相。预测性代理不仅将测量开销降低了几个数量级，还能超越传统量子资源密集型方法的性能。

Conclusion: 预测性代理模型为扩大先进量子处理器影响力提供了实用路径，能够显著降低量子计算成本并提高效率，为量子计算的广泛应用奠定了基础。

Abstract: The ongoing development of quantum processors is driving breakthroughs in
scientific discovery. Despite this progress, the formidable cost of fabricating
large-scale quantum processors means they will remain rare for the foreseeable
future, limiting their widespread application. To address this bottleneck, we
introduce the concept of predictive surrogates, which are classical learning
models designed to emulate the mean-value behavior of a given quantum processor
with provably computational efficiency. In particular, we propose two
predictive surrogates that can substantially reduce the need for quantum
processor access in diverse practical scenarios. To demonstrate their potential
in advancing digital quantum simulation, we use these surrogates to emulate a
quantum processor with up to 20 programmable superconducting qubits, enabling
efficient pre-training of variational quantum eigensolvers for families of
transverse-field Ising models and identification of non-equilibrium Floquet
symmetry-protected topological phases. Experimental results reveal that the
predictive surrogates not only reduce measurement overhead by orders of
magnitude, but can also surpass the performance of conventional,
quantum-resource-intensive approaches. Collectively, these findings establish
predictive surrogates as a practical pathway to broadening the impact of
advanced quantum processors.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [167] [Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review](https://arxiv.org/abs/2507.16876)
*Charlotte Jennings,Andrew Broad,Lucy Godson,Emily Clarke,David Westhead,Darren Treanor*

Main category: q-bio.QM

TL;DR: 这是一项系统性综述，分析了结合病理切片图像(WSI)和高通量组学数据进行癌症生存预测的多模态机器学习研究。研究发现该领域发展迅速且结果有前景，但存在方法学偏倚、外部验证不足等问题。


<details>
  <summary>Details</summary>
Motivation: 癌症预后预测对临床决策至关重要。多模态机器学习结合组织病理学和分子数据在癌症预后方面显示出潜力，但缺乏系统性评估。因此需要全面回顾和分析结合全切片图像和高通量组学数据预测总生存期的研究现状。

Method: 采用系统性综述方法，检索EMBASE、PubMed和Cochrane CENTRAL数据库(截至2024年12月8日)，加上引文筛选。使用CHARMS进行数据提取，用PROBAST+AI评估偏倚，按照SWiM和PRISMA 2020指南进行综合分析。研究方案已在PROSPERO注册。

Result: 共纳入48项研究(均自2017年以来)，涵盖19种癌症类型，全部使用癌症基因组图谱数据。研究方法包括正则化Cox回归(4项)、经典机器学习(13项)和深度学习(31项)。报告的c指数范围为0.550-0.857，多模态模型通常优于单模态模型。但所有研究均存在不明确或高偏倚风险，外部验证有限，对临床实用性关注不足。

Conclusion: 多模态WSI-组学生存预测是一个快速发展的领域，结果令人鼓舞，但需要改进方法学严谨性、扩大数据集范围并进行临床评估。该领域具有巨大潜力，但在临床应用前还需要解决现有的方法学和验证问题。

Abstract: Multimodal machine learning integrating histopathology and molecular data
shows promise for cancer prognostication. We systematically reviewed studies
combining whole slide images (WSIs) and high-throughput omics to predict
overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL
(12/08/2024), plus citation screening, identified eligible studies. Data
extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed
SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).
  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all
used The Cancer Genome Atlas. Approaches included regularised Cox regression
(n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged
0.550-0.857; multimodal models typically outperformed unimodal ones. However,
all studies showed unclear/high bias, limited external validation, and little
focus on clinical utility.
  Multimodal WSI-omics survival prediction is a fast-growing field with
promising results but needs improved methodological rigor, broader datasets,
and clinical evaluation.
  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687),
supported by UKRI Industrial Strategy Challenge Fund.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [168] [Stable Iterative Solvers for Ill-conditioned Linear Systems](https://arxiv.org/abs/2507.17673)
*Vasileios Kalantzis,Mark S. Squillante,Chai Wah Wu*

Main category: math.NA

TL;DR: 本文提出了通用算法框架来修改Krylov子空间迭代求解方法，确保算法在病态线性系统中保持稳定且不发散，并在SciPy中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模线性系统的迭代求解器（如Krylov子空间方法）在处理病态线性系统时容易发散，这严重限制了这些迭代方法在高性能计算中求解大规模线性系统的实际应用性。

Method: 提出通用算法框架来修改Krylov子空间迭代求解方法，确保算法稳定且不发散。然后将这些通用框架应用到SciPy中相应迭代方法的现有实现中。

Result: 通过在广泛的合成和真实世界病态线性系统上进行数值实验，证明了稳定迭代方法的有效性。实验结果显示该方法能够有效防止算法发散。

Conclusion: 所提出的通用算法框架成功解决了Krylov子空间方法在病态线性系统中的发散问题，提高了这些迭代方法在高性能计算中的实用性和可靠性。

Abstract: Iterative solvers for large-scale linear systems such as Krylov subspace
methods can diverge when the linear system is ill-conditioned, thus
significantly reducing the applicability of these iterative methods in practice
for high-performance computing solutions of such large-scale linear systems. To
address this fundamental problem, we propose general algorithmic frameworks to
modify Krylov subspace iterative solution methods which ensure that the
algorithms are stable and do not diverge. We then apply our general frameworks
to current implementations of the corresponding iterative methods in SciPy and
demonstrate the efficacy of our stable iterative approach with respect to
numerical experiments across a wide range of synthetic and real-world
ill-conditioned linear systems.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [169] [OkadaTorch: A Differentiable Programming of Okada Model to Calculate Displacements and Strains from Fault Parameters](https://arxiv.org/abs/2507.17126)
*Masayoshi Someya,Taisuke Yamada,Tomohisa Okazaki*

Main category: physics.geo-ph

TL;DR: 本文提出了OkadaTorch，一个基于PyTorch的可微分Okada模型实现，用于计算3D弹性半空间中位错源引起的位移和应变，支持自动微分以便于断层参数反演和科学机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 传统的Okada模型虽然广泛用于计算地震断层引起的地表变形，但缺乏可微分性，限制了其在基于梯度的优化、贝叶斯推断和科学机器学习中的应用。因此需要开发一个可微分的Okada模型实现。

Method: 将原始Okada模型直接翻译到PyTorch框架中，使整个代码具有可微分性，并提供便捷的包装接口来高效计算相对于观测站坐标或断层参数的梯度和Hessian矩阵，利用自动微分技术实现。

Result: 成功开发了OkadaTorch工具，实现了完全可微分的Okada模型，能够通过自动微分轻松计算输入参数的梯度，为断层参数反演提供了有效的计算框架。

Conclusion: OkadaTorch提供了一个强大的可微分框架，特别适用于断层参数反演，包括基于梯度的优化、贝叶斯推断以及与科学机器学习模型的集成，为地震学研究提供了新的计算工具。

Abstract: The Okada model is a widely used analytical solution for displacements and
strains caused by a point or rectangular dislocation source in a 3D elastic
half-space. We present OkadaTorch, a PyTorch implementation of the Okada model,
where the entire code is differentiable; gradients with respect to input can be
easily computed using automatic differentiation (AD). Our work consists of two
components: a direct translation of the original Okada model into PyTorch, and
a convenient wrapper interface for efficiently computing gradients and Hessians
with respect to either observation station coordinates or fault parameters.
This differentiable framework is well suited for fault parameter inversion,
including gradient-based optimization, Bayesian inference, and integration with
scientific machine learning (SciML) models. Our code is available here:
https://github.com/msomeya1/OkadaTorch

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [170] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出了WSM框架，通过模型融合技术替代传统学习率衰减，在多个基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统学习率衰减方法存在局限性，而无衰减方法和模型融合技术显示出潜力，需要建立学习率衰减与模型融合之间的理论联系

Method: 提出WSM（Warmup-Stable and Merge）框架，将学习率衰减策略（如余弦衰减、线性衰减、逆平方根衰减）转化为原则性的模型平均方案，兼容多种优化方法

Result: WSM在多个基准测试中超越WSD方法：MATH上提升3.5%，HumanEval上提升2.9%，MMLU-Pro上提升5.5%。发现融合持续时间是影响性能的最关键因素

Conclusion: WSM框架成功建立了学习率衰减与模型融合的统一理论基础，在多种场景下表现优异，为长期模型优化提供了新的解决方案

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [171] [A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion](https://arxiv.org/abs/2507.16955)
*Yalda Zafari,Roaa Elalfy,Mohamed Mabrok,Somaya Al-Maadeed,Tamer Khattab,Essam A. Rashed*

Main category: eess.IV

TL;DR: 本文提出了一种新颖的多视图、多任务混合深度学习框架，用于乳腺X线摄影筛查。该框架结合CNN和视觉状态空间模型(VSSM)处理四个标准视图，同时预测诊断标签和BI-RADS评分，在各种分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在乳腺癌筛查中存在局限性，主要集中在单视图输入或单任务输出上，限制了临床实用性。乳腺X线摄影解读因成像发现微妙和诊断模糊性而具有挑战性，需要更全面的方法来处理多视图信息并执行多个相关任务。

Method: 提出了一种多视图、多任务混合深度学习框架，包含：1)混合CNN-VSSM骨干网络，结合卷积编码器进行局部特征提取和视觉状态空间模型捕获全局上下文依赖；2)基于门控注意力的融合模块，动态加权跨视图信息并处理缺失数据；3)同时处理四个标准乳腺X线摄影视图并联合预测诊断标签和BI-RADS评分。

Result: 在各种诊断任务中，混合模型始终优于基线CNN架构和VSSM模型。在二分类BI-RADS 1 vs. 5任务中，共享混合模型达到AUC 0.9967和F1分数0.9830。在更具挑战性的三分类中达到F1分数0.7790，在五分类BI-RADS任务中最佳F1分数达到0.4904。

Conclusion: 提出的混合框架有效性得到验证，展示了多任务学习在改善诊断性能和实现临床有意义的乳腺X线摄影分析方面的潜力和局限性。该方法通过整合多视图信息和多任务学习，为乳腺癌早期检测提供了更robust和可解释的解决方案。

Abstract: Early and accurate interpretation of screening mammograms is essential for
effective breast cancer detection, yet it remains a complex challenge due to
subtle imaging findings and diagnostic ambiguity. Many existing AI approaches
fall short by focusing on single view inputs or single-task outputs, limiting
their clinical utility. To address these limitations, we propose a novel
multi-view, multitask hybrid deep learning framework that processes all four
standard mammography views and jointly predicts diagnostic labels and BI-RADS
scores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,
combining convolutional encoders for rich local feature extraction with Visual
State Space Models (VSSMs) to capture global contextual dependencies. To
improve robustness and interpretability, we incorporate a gated attention-based
fusion module that dynamically weights information across views, effectively
handling cases with missing data. We conduct extensive experiments across
diagnostic tasks of varying complexity, benchmarking our proposed hybrid models
against baseline CNN architectures and VSSM models in both single task and
multi task learning settings. Across all tasks, the hybrid models consistently
outperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,
the shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.
For the more challenging ternary classification, it attains an F1 score of
0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.
These results highlight the effectiveness of the proposed hybrid framework and
underscore both the potential and limitations of multitask learning for
improving diagnostic performance and enabling clinically meaningful mammography
analysis.

</details>


### [172] [Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with Sequential Mixture of Experts for Multi-View Mammography](https://arxiv.org/abs/2507.17662)
*Farnoush Bayatmakou,Reza Taleei,Nicole Simone,Arash Mohammadi*

Main category: eess.IV

TL;DR: 本文提出了Mammo-Mamba框架，通过结合选择性状态空间模型、Transformer注意力机制和专家驱动的特征优化，实现了高效准确的多视角乳腺X光图像分类，在CBIS-DDSM数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌仍是女性癌症相关死亡的主要原因之一。虽然基于Transformer的多视角乳腺X光图像分类模型表现出色，但其计算复杂度随图像块数量呈二次增长，需要更高效的替代方案来实现早期检测。

Method: 提出Mammo-Mamba框架，将选择性状态空间模型(SSMs)、基于Transformer的注意力机制和专家驱动的特征优化整合到统一架构中。通过定制的SecMamba块引入序列专家混合(SeqMoE)机制扩展MambaVision骨干网络，实现内容自适应特征优化和动态专家门控。

Result: 在CBIS-DDSM基准数据集上，Mammo-Mamba在所有关键指标上都实现了优异的分类性能，同时保持了计算效率。

Conclusion: Mammo-Mamba成功解决了传统Transformer模型在乳腺X光图像分析中的计算复杂度问题，通过创新的架构设计实现了高效准确的多视角乳腺癌检测，为计算机辅助诊断系统提供了新的解决方案。

Abstract: Breast cancer (BC) remains one of the leading causes of cancer-related
mortality among women, despite recent advances in Computer-Aided Diagnosis
(CAD) systems. Accurate and efficient interpretation of multi-view mammograms
is essential for early detection, driving a surge of interest in Artificial
Intelligence (AI)-powered CAD models. While state-of-the-art multi-view
mammogram classification models are largely based on Transformer architectures,
their computational complexity scales quadratically with the number of image
patches, highlighting the need for more efficient alternatives. To address this
challenge, we propose Mammo-Mamba, a novel framework that integrates Selective
State-Space Models (SSMs), transformer-based attention, and expert-driven
feature refinement into a unified architecture. Mammo-Mamba extends the
MambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)
mechanism through its customized SecMamba block. The SecMamba is a modified
MambaVision block that enhances representation learning in high-resolution
mammographic images by enabling content-adaptive feature refinement. These
blocks are integrated into the deeper stages of MambaVision, allowing the model
to progressively adjust feature emphasis through dynamic expert gating,
effectively mitigating the limitations of traditional Transformer models.
Evaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior
classification performance across all key metrics while maintaining
computational efficiency.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [173] [Graph Neural Network Approach to Predicting Magnetization in Quasi-One-Dimensional Ising Systems](https://arxiv.org/abs/2507.17509)
*V. Slavin,O. Kryvchikov,D. Laptev*

Main category: cond-mat.dis-nn

TL;DR: 提出了一个基于图神经网络的深度学习框架，通过将晶格几何编码为图结构来预测准一维伊辛自旋系统的磁性质，能够准确重现磁化曲线的关键特征并实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 传统的蒙特卡罗模拟方法预测磁性材料的磁化行为计算成本高昂，需要开发一种能够直接从结构连接性推断磁性行为的高效方法，避免额外的蒙特卡罗模拟。

Method: 将晶格几何编码为图结构，使用图神经网络(GNN)处理图数据，然后通过全连接层进行预测。模型在蒙特卡罗模拟数据上进行训练，能够捕获局部模式和全局对称性。

Result: 模型能够准确重现磁化曲线的关键特征，包括平台、临界转变点和几何阻挫效应。证明了图神经网络可以直接从结构连接性推断磁性行为。

Conclusion: 基于图神经网络的深度学习框架能够实现准一维伊辛自旋系统磁性质的高效预测，为磁性材料的计算研究提供了一种新的有效方法，无需进行额外的蒙特卡罗模拟。

Abstract: We present a graph-based deep learning framework for predicting the magnetic
properties of quasi-one-dimensional Ising spin systems. The lattice geometry is
encoded as a graph and processed by a graph neural network (GNN) followed by
fully connected layers. The model is trained on Monte Carlo simulation data and
accurately reproduces key features of the magnetization curve, including
plateaus, critical transition points, and the effects of geometric frustration.
It captures both local motifs and global symmetries, demonstrating that GNNs
can infer magnetic behavior directly from structural connectivity. The proposed
approach enables efficient prediction of magnetization without the need for
additional Monte Carlo simulations.

</details>


### [174] [Deep Generative Learning of Magnetic Frustration in Artificial Spin Ice from Magnetic Force Microscopy Images](https://arxiv.org/abs/2507.17726)
*Arnab Neogi,Suryakant Mishra,Prasad P Iyer,Tzu-Ming Lu,Ezra Bussmann,Sergei Tretiak,Andrew Crandall Jones,Jian-Xin Zhu*

Main category: cond-mat.dis-nn

TL;DR: 本文提出了一种基于机器学习的方法来自动分析蜂窝晶格自旋冰样品的显微图像，通过变分自编码器生成合成磁力显微图像并提取特征，实现对磁矩、取向和挫败顶点的精确识别和预测。


<details>
  <summary>Details</summary>
Motivation: 随着原子分辨率显微图像数据集的增长，需要开发机器学习方法来识别和分析图像中的微妙物理现象。传统的自旋冰配置分析方法存在实验和分割误差，需要更精确的自动化分析工具。

Method: 采用两阶段工作流程：第一阶段训练机器学习模型预测自旋冰结构中的磁矩和方向，使用变分自编码器（VAEs）生成高质量的合成磁力显微图像并提取潜在特征表示；第二阶段实现对挫败顶点和纳米磁性片段的精确识别和预测。

Result: 成功实现了对蜂窝晶格自旋冰样品净磁矩和方向取向的自动化计算，有效关联了显微图像的结构和功能特征，能够精确识别挫败顶点和纳米磁性片段。

Conclusion: 该方法能够设计具有受控挫败模式的优化自旋冰配置，为按需合成提供了可能性，为自旋冰材料的研究和应用开辟了新的途径。

Abstract: Increasingly large datasets of microscopic images with atomic resolution
facilitate the development of machine learning methods to identify and analyze
subtle physical phenomena embedded within the images. In this work, microscopic
images of honeycomb lattice spin-ice samples serve as datasets from which we
automate the calculation of net magnetic moments and directional orientations
of spin-ice configurations. In the first stage of our workflow, machine
learning models are trained to accurately predict magnetic moments and
directions within spin-ice structures. Variational Autoencoders (VAEs), an
emergent unsupervised deep learning technique, are employed to generate
high-quality synthetic magnetic force microscopy (MFM) images and extract
latent feature representations, thereby reducing experimental and segmentation
errors. The second stage of proposed methodology enables precise identification
and prediction of frustrated vertices and nanomagnetic segments, effectively
correlating structural and functional aspects of microscopic images. This
facilitates the design of optimized spin-ice configurations with controlled
frustration patterns, enabling potential on-demand synthesis.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [175] [EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media](https://arxiv.org/abs/2507.16847)
*Ismail Hossain,Sai Puppala,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.SI

TL;DR: 本研究提出了一种结合开源大语言模型（Llama-3-Instruct、Mistral-7B-Instruct、Gemma-7B-IT）和传统语言模型（GPT-2、BERT、RoBERTa）的联合嵌入技术，用于分析和预测社交媒体用户行为的终身演化模式。


<details>
  <summary>Details</summary>
Motivation: 社交媒体用户行为会随着时间推移而演化，受到人口统计学属性和社交网络的影响。研究者希望能够预测用户未来的社交演化阶段，包括网络变化、未来连接和活动转变，以解决好友推荐和活动预测等关键挑战，并为潜在负面结果提供早期预警。

Method: 采用提示工程技术结合开源大语言模型（Llama-3-Instruct、Mistral-7B-Instruct、Gemma-7B-IT），并与传统语言模型（GPT-2、BERT、RoBERTa）使用联合嵌入技术相结合，构建跨模态配置来分析用户社交媒体行为演化模式。

Result: 实验结果显示，在跨模态配置下GPT-2取得了最低的困惑度（8.21），优于RoBERTa（9.11）和BERT模型。研究证明了跨模态配置对于获得优异性能的重要性，并展示了这些模型预测用户社交演化未来阶段的潜力。

Conclusion: 该方法有效解决了社交媒体中好友推荐和活动预测等关键挑战，能够预测未来的交互和活动，为用户行为轨迹提供洞察。通过预测潜在的负面结果，该研究旨在帮助用户做出明智决策并降低长期风险。

Abstract: Social media platforms serve as a significant medium for sharing personal
emotions, daily activities, and various life events, ensuring individuals stay
informed about the latest developments. From the initiation of an account,
users progressively expand their circle of friends or followers, engaging
actively by posting, commenting, and sharing content. Over time, user behavior
on these platforms evolves, influenced by demographic attributes and the
networks they form. In this study, we present a novel approach that leverages
open-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through
prompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint
embedding technique, to analyze and predict the evolution of user behavior on
social media over their lifetime. Our experiments demonstrate the potential of
these models to forecast future stages of a user's social evolution, including
network changes, future connections, and shifts in user activities.
Experimental results highlight the effectiveness of our approach, with GPT-2
achieving the lowest perplexity (8.21) in a Cross-modal configuration,
outperforming RoBERTa (9.11) and BERT, and underscoring the importance of
leveraging Cross-modal configurations for superior performance. This approach
addresses critical challenges in social media, such as friend recommendations
and activity predictions, offering insights into the trajectory of user
behavior. By anticipating future interactions and activities, this research
aims to provide early warnings about potential negative outcomes, enabling
users to make informed decisions and mitigate risks in the long term.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [176] [Citation Recommendation using Deep Canonical Correlation Analysis](https://arxiv.org/abs/2507.17603)
*Conor McNamara,Effirul Ramlan*

Main category: cs.IR

TL;DR: 提出了一种基于深度典型相关分析(DCCA)的引文推荐算法，通过捕获文本和图结构表示之间的非线性关系来改进多视图学习，在DBLP数据集上相比传统CCA方法实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的引文推荐方法虽然通过多视图表示学习提高了准确性，但在有效融合多个数据视图时需要能够捕获互补信息并保持每种模态独特特征的融合技术。传统的线性典型相关分析(CCA)方法存在局限性，无法捕获复杂的非线性关系。

Method: 提出了一种新颖的引文推荐算法，通过应用深度典型相关分析(DCCA)来改进线性CCA方法。DCCA是一种神经网络扩展，能够捕获科学文章的分布式文本表示和基于图的表示之间复杂的非线性关系。

Result: 在大规模DBLP引文网络数据集上的实验表明，该方法优于最先进的基于CCA的方法，在Mean Average Precision@10上实现了超过11%的相对改进，在Precision@10上实现了5%的改进，在Recall@10上实现了7%的改进。

Conclusion: 这些性能提升反映了更相关的引文推荐和增强的排序质量，表明DCCA的非线性变换比CCA的线性投影产生了更具表达力的潜在表示，验证了深度学习方法在多模态学术文档分析中的有效性。

Abstract: Recent advances in citation recommendation have improved accuracy by
leveraging multi-view representation learning to integrate the various
modalities present in scholarly documents. However, effectively combining
multiple data views requires fusion techniques that can capture complementary
information while preserving the unique characteristics of each modality. We
propose a novel citation recommendation algorithm that improves upon linear
Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a
neural network extension capable of capturing complex, non-linear relationships
between distributed textual and graph-based representations of scientific
articles. Experiments on the large-scale DBLP (Digital Bibliography & Library
Project) citation network dataset demonstrate that our approach outperforms
state-of-the-art CCA-based methods, achieving relative improvements of over 11%
in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These
gains reflect more relevant citation recommendations and enhanced ranking
quality, suggesting that DCCA's non-linear transformations yield more
expressive latent representations than CCA's linear projections.

</details>
