<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 9]
- [hep-th](#hep-th) [Total: 1]
- [cs.CL](#cs.CL) [Total: 13]
- [cs.SD](#cs.SD) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.IR](#cs.IR) [Total: 8]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution](https://arxiv.org/abs/2508.03767)
*Sandeepa Kannangara,Arman Abrahamyan,Daniel Elias,Thomas Kilby,Nadav Dar,Luiz Pizzato,Anna Leontjeva,Dan Jermyn*

Main category: cs.DB

TL;DR: MERAI是一种高效的大规模实体解析（ER）管道，专为解决企业级高容量数据集中的记录去重和链接问题而设计，性能优于Dedupe和Splink。


<details>
  <summary>Details</summary>
Motivation: 实体解析在数据管理中面临大规模数据集处理的挑战，现有工具如Dedupe和Splink在扩展性和准确性上存在不足。

Method: 提出MERAI管道，通过实验验证其在大规模记录去重和链接任务中的表现，并与Dedupe和Splink进行对比。

Result: MERAI成功处理了1570万条记录，准确率优于基线系统（F1分数更高），而Dedupe在200万条记录后因内存问题失败。

Conclusion: MERAI为企业级大规模实体解析提供了可扩展且可靠的解决方案，确保数据完整性和一致性。

Abstract: Entity resolution (ER) remains a significant challenge in data management,
especially when dealing with large datasets. This paper introduces MERAI
(Massive Entity Resolution using AI), a robust and efficient pipeline designed
to address record deduplication and linkage issues in high-volume datasets at
an enterprise level. The pipeline's resilience and accuracy have been validated
through various large-scale record deduplication and linkage projects. To
evaluate MERAI's performance, we compared it with two well-known entity
resolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2
million records due to memory constraints, MERAI successfully processed
datasets of up to 15.7 million records and produced accurate results across all
experiments. Experimental data demonstrates that MERAI outperforms both
baseline systems in terms of matching accuracy, with consistently higher F1
scores in both deduplication and record linkage tasks. MERAI offers a scalable
and reliable solution for enterprise-level large-scale entity resolution,
ensuring data integrity and consistency in real-world applications.

</details>


### [2] [Raqlet: Cross-Paradigm Compilation for Recursive Queries](https://arxiv.org/abs/2508.03978)
*Amir Shaikhha,Youning Xia,Meisam Tarabkhah,Jazal Saleem,Anna Herlihy*

Main category: cs.DB

TL;DR: Raqlet是一个源到源的编译框架，旨在解决递归查询引擎在关系型、图型和演绎型系统中的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于SQL:2023的SQL/PGQ和GQL标准在实际系统中的支持不一致，Raqlet通过中间表示（IRs）提供统一的语义基础，以支持跨范式的递归查询翻译。

Method: Raqlet通过将Cypher或SQL/PGQ转换为PGIR，再转换为DLIR，最后转换为SQIR，实现递归查询的跨范式翻译。

Result: Raqlet提供了一个共享的语义基础，可作为语言标准的参考实现，并支持静态分析和性能优化。

Conclusion: Raqlet的目标是成为一个强大的平台，支持跨范式原型设计、可移植的递归查询和形式化推理。

Abstract: We introduce Raqlet, a source-to-source compilation framework that addresses
the fragmentation of recursive querying engines spanning relational (recursive
SQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards
such as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for
querying graph data within relational and graph databases; however, real-world
support remains inconsistent across systems. Raqlet bridges this gap by
translating recursive queries across paradigms through leveraging intermediate
representations (IRs) grounded in well-defined semantics; it translates Cypher
or SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),
and finally to SQIR (inspired by recursive SQL). Raqlet provides a shared
semantic basis that can serve as a golden reference implementation for language
standards, while supporting static analysis and transformations (e.g.,
magic-set transformation) for performance tuning. Our vision is to make Raqlet
a robust platform that enables rapid cross-paradigm prototyping, portable
recursive queries, and formal reasoning about recursion even when targeting
diverse query execution engines.

</details>


### [3] [BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases](https://arxiv.org/abs/2508.04031)
*Lianggui Weng,Dandan Liu,Rong Zhu,Bolin Ding,Jingren Zhou*

Main category: cs.DB

TL;DR: BridgeScope是一个连接LLM和数据库的通用工具包，通过模块化SQL操作、权限管理和代理机制解决现有交互设计中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与数据库的交互在可用性、安全性、权限管理和数据传输效率方面存在局限，BridgeScope旨在解决这些问题。

Method: BridgeScope通过模块化SQL操作、权限与安全策略对齐以及代理机制实现高效、安全的数据库交互。

Result: 评估显示，BridgeScope显著提升LLM代理的数据库操作效率，减少80%的token使用，并支持数据密集型工作流。

Conclusion: BridgeScope为下一代智能数据自动化提供了强大基础。

Abstract: As large language models (LLMs) demonstrate increasingly powerful reasoning
and orchestration capabilities, LLM-based agents are rapidly proliferating for
complex data-related tasks. Despite this progress, the current design of how
LLMs interact with databases exhibits critical limitations in usability,
security, privilege management, and data transmission efficiency. To resolve
these challenges, we introduce BridgeScope, a universal toolkit bridging LLMs
and databases through three key innovations. First, it modularizes SQL
operations into fine-grained tools for context retrieval, CRUD execution, and
ACID-compliant transaction management, enabling more precise and LLM-friendly
functionality controls. Second, it aligns tool implementations with both
database privileges and user security policies to steer LLMs away from unsafe
or unauthorized operations, improving task execution efficiency while
safeguarding database security. Third, it introduces a proxy mechanism for
seamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All
of these designs are database-agnostic and can be transparently integrated with
existing agent architectures. We also release an open-source implementation of
BridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate
that BridgeScope enables LLM agents to operate databases more effectively,
reduces token usage by up to 80% through improved security awareness, and
uniquely supports data-intensive workflows beyond existing toolkits,
establishing BridgeScope as a robust foundation for next-generation intelligent
data automation.

</details>


### [4] [Rethinking Analytical Processing in the GPU Era](https://arxiv.org/abs/2508.04701)
*Bobbi Yogatama,Yifei Yang,Kevin Kristensen,Devesh Sarda,Abigale Kim,Adrian Cockcroft,Yu Teng,Joshua Patterson,Gregory Kimball,Wes McKinney,Weiwei Gong,Xiangyao Yu*

Main category: cs.DB

TL;DR: GPU数据分析时代已至，硬件和软件的进步消除了GPU数据分析广泛采用的主要障碍。Sirius是一个GPU原生SQL引擎原型，提供对多种数据系统的即插即用加速，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 近年来硬件（如更大的GPU内存、更快的互连和IO、成本下降）和软件（如可组合数据系统和成熟库）的进步，为GPU数据分析的广泛采用扫清了障碍。

Method: 提出Sirius，一个开源GPU原生SQL引擎原型，利用libcudf等库实现高性能关系操作，并通过标准Substrait查询表示提供对现有数据库的即插即用加速。

Result: 在TPC-H测试中，Sirius与DuckDB集成时单节点性能提升7倍，与Apache Doris集成时分布式性能提升高达12.5倍。

Conclusion: GPU数据分析的潜力巨大，Sirius展示了GPU原生引擎在性能和成本效益上的优势。

Abstract: The era of GPU-powered data analytics has arrived. In this paper, we argue
that recent advances in hardware (e.g., larger GPU memory, faster interconnect
and IO, and declining cost) and software (e.g., composable data systems and
mature libraries) have removed the key barriers that have limited the wider
adoption of GPU data analytics. We present Sirius, a prototype open-source
GPU-native SQL engine that offers drop-in acceleration for diverse data
systems. Sirius treats GPU as the primary engine and leverages libraries like
libcudf for high-performance relational operators. It provides drop-in
acceleration for existing databases by leveraging the standard Substrait query
representation, replacing the CPU engine without changing the user-facing
interface. On TPC-H, Sirius achieves 7x speedup when integrated with DuckDB in
a single node at the same hardware rental cost, and up to 12.5x speedup when
integrated with Apache Doris in a distributed setting.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication](https://arxiv.org/abs/2508.03760)
*Qingyuan Li,Bo Zhang,Hui Kang,Tianhao Xu,Yulei Qian,Yuchen Xie,Lin Ma*

Main category: cs.DC

TL;DR: FlashCommunication V2 是一种新型通信范式，通过位拆分和尖峰保留技术，支持任意位宽的跨GPU高效传输，显著提升通信系统的灵活性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 分布式训练和部署大型语言模型（LLM）时，通信瓶颈成为关键挑战。

Method: 提出位拆分技术将不规则位宽分解为基本单元，确保硬件兼容性；尖峰保留技术保留数值异常值（最小和最大值）为浮点数，缩小动态范围并支持2位量化。

Result: 在NVLink和PCIe架构上实现稳健性能，AllReduce加速3.2倍，All2All加速2倍。

Conclusion: FlashCommunication V2 通过软硬件协同设计，显著提升通信效率，支持低比特量化。

Abstract: Nowadays, communication bottlenecks have emerged as a critical challenge in
the distributed training and deployment of large language models (LLMs). This
paper introduces FlashCommunication V2, a novel communication paradigm enabling
efficient cross-GPU transmission at arbitrary bit widths. Its core innovations
lie in the proposed bit splitting and spike reserving techniques, which address
the challenges of low-bit quantization. Bit splitting decomposes irregular bit
widths into basic units, ensuring compatibility with hardware capabilities and
thus enabling transmission at any bit width. Spike reserving, on the other
hand, retains numerical outliers (i.e., minima and maxima) as floating-point
numbers, which shrinks the dynamic numerical range and pushes the quantization
limits to 2-bit with acceptable losses. FlashCommunication V2 significantly
enhances the flexibility and resource utilization of communication systems.
Through meticulous software-hardware co-design, it delivers robust performance
and reduced overhead across both NVLink-based and PCIe-based architectures,
achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All
communication.

</details>


### [6] [Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training](https://arxiv.org/abs/2508.03854)
*Xin Zhang,Quanyu Zhu,Liangbei Xu,Zain Huda,Wang Zhou,Jin Fang,Dennis van der Staay,Yuxi Hu,Jade Nie,Jiyan Yang,Chunzhi Yang*

Main category: cs.DC

TL;DR: 提出了一种新的二维稀疏并行方法，用于解决大规模深度学习推荐模型（DLRM）训练中的嵌入表并行化问题，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 工业级DLRM的稀疏嵌入表通常包含数万亿参数，传统全并行策略面临内存限制、负载不均和通信开销等问题。

Method: 结合数据并行与模型并行，提出二维稀疏并行方法，并开发动量缩放行式AdaGrad算法以减少性能损失。

Result: 实验表明，该方法在保持模型性能的同时，显著提升训练效率，支持4K GPU的线性扩展。

Conclusion: 该方法为推荐模型训练设定了新的技术标杆，解决了大规模并行化中的关键挑战。

Abstract: The increasing complexity of deep learning recommendation models (DLRM) has
led to a growing need for large-scale distributed systems that can efficiently
train vast amounts of data. In DLRM, the sparse embedding table is a crucial
component for managing sparse categorical features. Typically, these tables in
industrial DLRMs contain trillions of parameters, necessitating model
parallelism strategies to address memory constraints. However, as training
systems expand with massive GPUs, the traditional fully parallelism strategies
for embedding table post significant scalability challenges, including
imbalance and straggler issues, intensive lookup communication, and heavy
embedding activation memory. To overcome these limitations, we propose a novel
two-dimensional sparse parallelism approach. Rather than fully sharding tables
across all GPUs, our solution introduces data parallelism on top of model
parallelism. This enables efficient all-to-all communication and reduces peak
memory consumption. Additionally, we have developed the momentum-scaled
row-wise AdaGrad algorithm to mitigate performance losses associated with the
shift in training paradigms. Our extensive experiments demonstrate that the
proposed approach significantly enhances training efficiency while maintaining
model performance parity. It achieves nearly linear training speed scaling up
to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model
training.

</details>


### [7] [Reputation-based partition scheme for IoT security](https://arxiv.org/abs/2508.03981)
*Zhikui Chen,Muhammad Zeeshan Haider,Naiwen Luo,Shuo Yu,Xu Yuan,Yaochen Zhang,Tayyaba Noreen*

Main category: cs.DC

TL;DR: 提出了一种基于声誉的分区方案（RSPC），通过结合节点声誉值计算最优分区大小，并定期重组网络以避免分区攻击，同时提出四阶段确认协议确保跨分区交易的安全高效完成。


<details>
  <summary>Details</summary>
Motivation: 随着智能终端的普及，众包感知作为数据聚合范式在数据驱动应用中至关重要，但集中式管理带来安全漏洞和可扩展性问题。

Method: RSPC结合节点声誉值计算最优分区大小，定期重组网络，并提出四阶段确认协议处理跨分区交易。

Result: 实验表明RSPC提高了众包感知的可扩展性、低延迟和高吞吐量。

Conclusion: RSPC有效解决了集中式管理的安全和可扩展性问题，为众包感知提供了高效安全的解决方案。

Abstract: With the popularity of smart terminals, such as the Internet of Things,
crowdsensing is an emerging data aggregation paradigm, which plays a pivotal
role in data-driven applications. There are some key issues in the development
of crowdsensing such as platform security and privacy protection. As the
crowdsensing is usually managed by a centralized platform, centralized
management will bring various security vulnerabilities and scalability issues.
To solve these issues, an effective reputation-based partition scheme (RSPC) is
proposed in this article. The partition scheme calculates the optimal partition
size by combining the node reputation value and divides the node into several
disjoint partitions according to the node reputation value. By selecting the
appropriate partition size, RSPC provides a mechanism to ensure that each
partition is valid, as long as themaximum permissible threshold for the failed
node is observed. At the same time, the RSPC reorganizes the network
periodically to avoid partition attacks. In addition, for cross-partition
transactions, this paper innovatively proposes a four-stage confirmation
protocol to ensure the efficient and safe completion of cross-partition
transactions. Finally, experiments show that RSPC improves scalability, low
latency, and high throughput for crowdsensing.

</details>


### [8] [High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines](https://arxiv.org/abs/2508.03984)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: cs.DC

TL;DR: 该研究提出了一种利用低精度矩阵引擎模拟高精度矩阵乘法（SGEMM和DGEMM）的方法，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 深度学习中对低精度矩阵乘法的需求推动了高性能和能效矩阵引擎的发展，但如何利用这些引擎模拟高精度矩阵乘法仍是一个挑战。

Method: 研究提出了一种新的模拟方法，利用低精度矩阵引擎实现高精度矩阵乘法。

Result: 在GH200 Grace Hopper Superchip上，模拟DGEMM的性能提升1.4倍，能效提升43%；模拟SGEMM的性能提升3.0倍，能效提升154%。

Conclusion: 该方法在性能和能效上显著优于传统模拟方法和原生实现，适用于大规模问题。

Abstract: Recent architectures integrate high-performance and power-efficient matrix
engines. These engines demonstrate remarkable performance in low-precision
matrix multiplication, which is crucial in deep learning. Several techniques
have been proposed to emulate single- and double-precision general
matrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such
low-precision matrix engines. In this study, we present emulation methods that
significantly outperforms conventional approaches. On a GH200 Grace Hopper
Superchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\%
improvement in power efficiency compared to native DGEMM for sufficiently large
problems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\%
improvement in power efficiency compared to native SGEMM for sufficiently large
problems. Furthermore, compared to conventional emulation methods, the proposed
emulation achieves more than 2x higher performance and superior power
efficiency.

</details>


### [9] [Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability](https://arxiv.org/abs/2508.04000)
*Tayyaba Noreen,Qiufen Xia,Muhammad Zeeshan Haider*

Main category: cs.DC

TL;DR: 本文提出了一种基于DAG的ADR协议，旨在解决区块链系统的吞吐量、可扩展性和延迟问题，特别适用于物联网应用。


<details>
  <summary>Details</summary>
Motivation: 当前区块链系统因共识机制的限制，在吞吐量、可扩展性和延迟方面表现不佳，尤其是在物联网应用中。

Method: ADR协议采用DAG结构，通过三步法（节点验证、DAG账本构建和排名算法）提升网络安全性、吞吐量和可扩展性。

Result: 在Amazon EC2集群上的模拟实验表明，ADR显著提高了交易吞吐量和网络活跃度，优于IOTA和ByteBall等现有DAG区块链。

Conclusion: ADR协议为区块链在物联网等领域的应用提供了高效、可扩展的解决方案。

Abstract: In the past decade, blockchain has emerged as a promising solution for
building secure distributed ledgers and has attracted significant attention.
However, current blockchain systems suffer from limited throughput, poor
scalability, and high latency. Due to limitations in consensus mechanisms,
especially in managing node identities, blockchain is often considered
unsuitable for applications such as the Internet of Things (IoT). This paper
proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain
scalability and throughput. ADR employs a directed acyclic graph (DAG)
structure where nodes are positioned based on their rankings. Unlike
traditional chains, ADR allows honest nodes to write blocks and verify
transactions using a DAG-based topology. The protocol follows a three-step
approach to secure the network against double-spending and enhance performance.
First, it verifies nodes using their public and private keys before granting
entry. Second, it builds an advanced DAG ledger enabling block production and
transaction validation. Third, a ranking algorithm filters out malicious nodes,
ranks the remaining nodes based on performance, and arranges them
topologically. This process increases throughput and ensures robust
scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,
including scenarios with injected malicious nodes. Simulation results
demonstrate that ADR significantly improves transaction throughput and network
liveness compared to existing DAG-based blockchains such as IOTA and ByteBall,
making it well-suited for IoT applications.

</details>


### [10] [High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2508.04013)
*Sameh Abdulah,Mary Lai O. Salvana,Ying Sun,David E. Keyes,Marc G. Genton*

Main category: cs.DC

TL;DR: 本文探讨了统计计算（SC）与高性能计算（HPC）之间的差距，提出了高性能统计计算（HPSC）的愿景，并分析了挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 统计计算社区开发的软件被广泛使用，但在高性能计算领域参与度低。希望通过结合SC与HPC的优势，推动快速、可扩展的统计应用发展。

Method: 通过回顾SC的历史，提出HPSC的愿景，分析现有挑战与机遇，并制定可能的路线图。

Result: 指出了SC与HPC结合的潜力，以及如何通过技术革新和社区合作实现HPSC。

Conclusion: 通过加强SC与HPC社区的联系，可以推动HPSC的发展，为统计科学在高性能计算环境中开辟新方向。

Abstract: We recognize the emergence of a statistical computing community focused on
working with large computing platforms and producing software and applications
that exemplify high-performance statistical computing (HPSC). The statistical
computing (SC) community develops software that is widely used across
disciplines. However, it remains largely absent from the high-performance
computing (HPC) landscape, particularly on platforms such as those featured on
the Top500 or Green500 lists. Many disciplines already participate in HPC,
mostly centered around simulation science, although data-focused efforts under
the artificial intelligence (AI) label are gaining popularity. Bridging this
gap requires both community adaptation and technical innovation to align
statistical methods with modern HPC technologies. We can accelerate progress in
fast and scalable statistical applications by building strong connections
between the SC and HPC communities. We present a brief history of SC, a vision
for how its strengths can contribute to statistical science in the HPC
environment (such as HPSC), the challenges that remain, and the opportunities
currently available, culminating in a possible roadmap toward a thriving HPSC
community.

</details>


### [11] [SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning](https://arxiv.org/abs/2508.04265)
*Borui Li,Li Yan,Jianmin Liu*

Main category: cs.DC

TL;DR: SelectiveShield是一个轻量级混合防御框架，结合选择性同态加密和差分隐私，通过Fisher信息量化参数敏感性，保护联邦学习中的敏感数据。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分散数据上训练模型时易受梯度泄漏攻击，现有防御机制（如差分隐私和同态加密）在隐私、模型效用和系统开销之间存在权衡。

Method: 提出SelectiveShield框架，利用Fisher信息量化参数敏感性，通过协作协议选择敏感参数进行同态加密保护，非关键参数使用差分隐私噪声。

Result: 实验表明，SelectiveShield在保持模型效用的同时显著降低梯度泄漏风险。

Conclusion: SelectiveShield为实际联邦学习部署提供了实用且可扩展的防御机制。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data but remains vulnerable to gradient leakage attacks that can reconstruct
sensitive user information. Existing defense mechanisms, such as differential
privacy (DP) and homomorphic encryption (HE), often introduce a trade-off
between privacy, model utility, and system overhead, a challenge that is
exacerbated in heterogeneous environments with non-IID data and varying client
capabilities. To address these limitations, we propose SelectiveShield, a
lightweight hybrid defense framework that adaptively integrates selective
homomorphic encryption and differential privacy. SelectiveShield leverages
Fisher information to quantify parameter sensitivity, allowing clients to
identify critical parameters locally. Through a collaborative negotiation
protocol, clients agree on a shared set of the most sensitive parameters for
protection via homomorphic encryption. Parameters that are uniquely important
to individual clients are retained locally, fostering personalization, while
non-critical parameters are protected with adaptive differential privacy noise.
Extensive experiments demonstrate that SelectiveShield maintains strong model
utility while significantly mitigating gradient leakage risks, offering a
practical and scalable defense mechanism for real-world federated learning
deployments.

</details>


### [12] [S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge](https://arxiv.org/abs/2508.04271)
*JinYi Yoon,JiHo Lee,Ting He,Nakjung Choi,Bo Ji*

Main category: cs.DC

TL;DR: S2M3是一种用于边缘设备的多任务推理的分割共享多模态架构，通过模块共享减少资源使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态AI服务依赖云端存在带宽、延迟、隐私等问题，而边缘设备支持多任务面临资源挑战。

Method: 分割多模态模型的功能模块并共享通用模块，采用贪婪模块级放置和并行路由优化资源。

Result: 实验显示S2M3在单任务和多任务下分别减少内存使用50%和62%，延迟降低56.9%。

Conclusion: S2M3有效解决边缘设备资源限制问题，性能优于云端AI。

Abstract: With the advancement of Artificial Intelligence (AI) towards multiple
modalities (language, vision, speech, etc.), multi-modal models have
increasingly been used across various applications (e.g., visual question
answering or image generation/captioning). Despite the success of AI as a
service for multi-modal applications, it relies heavily on clouds, which are
constrained by bandwidth, latency, privacy concerns, and unavailability under
network or server failures. While on-device AI becomes popular, supporting
multiple tasks on edge devices imposes significant resource challenges. To
address this, we introduce S2M3, a split-and-share multi-modal architecture for
multi-task inference on edge devices. Inspired by the general-purpose nature of
multi-modal models, which are composed of multiple modules (encoder, decoder,
classifier, etc.), we propose to split multi-modal models at functional-level
modules; and then share common modules to reuse them across tasks, thereby
reducing resource usage. To address cross-model dependency arising from module
sharing, we propose a greedy module-level placement with per-request parallel
routing by prioritizing compute-intensive modules. Through experiments on a
testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,
we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in
single-task and multi-task settings, respectively, without sacrificing
accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95
instances (93.7%) while reducing inference latency by up to 56.9% on
resource-constrained devices, compared to cloud AI.

</details>


### [13] [Optimizing Microgrid Composition for Sustainable Data Centers](https://arxiv.org/abs/2508.04284)
*Julius Irion,Philipp Wiesner,Jonathan Bader,Odej Kao*

Main category: cs.DC

TL;DR: 提出了一种优化框架，用于评估微电网组件对数据中心长期可持续性和电力可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心能源需求增长和电网基础设施不足，微电网的可再生能源和储能系统集成成为趋势，但缺乏评估工具。

Method: 扩展了Vessim模拟器，结合NREL的SAM模型，模拟计算负载、可再生能源生产和储能交互，并进行多阶段黑盒优化。

Result: 框架能够评估微电网组件的长期可持续性和可靠性，帮助数据中心运营商做出更明智的能源系统规划决策。

Conclusion: 该优化框架为数据中心微电网的可持续性和可靠性提供了实用工具。

Abstract: As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.

</details>


### [14] [Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing](https://arxiv.org/abs/2508.04334)
*Noor Islam S. Mohammad*

Main category: cs.DC

TL;DR: 提出了一种结合深度强化学习和蚁群优化的混合调度算法，用于物联网-云环境中的高效任务调度。


<details>
  <summary>Details</summary>
Motivation: 物联网设备快速增长，产生大量异构数据流，现有调度方法难以适应动态负载和网络变化。

Method: 结合深度强化学习（模型无关策略梯度）和蚁群优化，实现自适应任务分配和全局资源优化。

Result: 实验显示，平均响应时间减少18.4%，资源利用率提高12.7%，能耗降低9.3%。

Conclusion: 该算法通过结合模型无关强化学习和群体智能，为下一代物联网-云平台提供了高效、可扩展的调度方案。

Abstract: The rapid growth of Internet of Things (IoT) devices produces massive,
heterogeneous data streams, demanding scalable and efficient scheduling in
cloud environments to meet latency, energy, and Quality-of-Service (QoS)
requirements. Existing scheduling methods often lack adaptability to dynamic
workloads and network variability inherent in IoT-cloud systems. This paper
presents a novel hybrid scheduling algorithm combining deep Reinforcement
Learning (RL) and Ant Colony Optimization (ACO) to address these challenges.
The deep RL agent utilizes a model-free policy-gradient approach to learn
adaptive task allocation policies responsive to real-time workload fluctuations
and network states. Simultaneously, the ACO metaheuristic conducts a global
combinatorial search to optimize resource distribution, mitigate congestion,
and balance load across distributed cloud nodes. Extensive experiments on
large-scale synthetic IoT datasets, reflecting diverse workloads and QoS
constraints, demonstrate that the proposed method achieves up to 18.4%
reduction in average response time, 12.7% improvement in resource utilization,
and 9.3% decrease in energy consumption compared to leading heuristics and
RL-only baselines. Moreover, the algorithm ensures strict Service Level
Agreement (SLA) compliance through deadline-aware scheduling and dynamic
prioritization. The results confirm the effectiveness of integrating model-free
RL with swarm intelligence for scalable, energy-efficient IoT data scheduling,
offering a promising approach for next-generation IoT-cloud platforms.

</details>


### [15] [Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis](https://arxiv.org/abs/2508.04596)
*Chuan-Chi Lai,Yan-Lin Chen,Bo-Xin Liu,Chuan-Ming Liu*

Main category: cs.DC

TL;DR: 提出了一种边缘辅助并行不确定天际线（EPUS）算法，用于低延迟物联网数据分析，通过修剪数据减少云端计算和传输负担。


<details>
  <summary>Details</summary>
Motivation: 物联网数据量激增导致云端计算和传输成本高昂，边缘计算被提出以减轻负担。

Method: 利用天际线候选集在并行边缘计算节点上修剪数据，仅发送必要信息到云端更新全局天际线。

Result: 仿真结果显示，EPUS方法在二维数据上减少50%以上延迟，高维数据表现也优于现有方法。

Conclusion: EPUS算法有效降低了数据传输和云端计算需求，适用于低延迟物联网应用。

Abstract: Due to the Internet of Everything (IoE), data generated in our life become
larger. As a result, we need more effort to analyze the data and extract
valuable information. In the cloud computing environment, all data analysis is
done in the cloud, and the client only needs less computing power to handle
some simple tasks. However, with the rapid increase in data volume, sending all
data to the cloud via the Internet has become more expensive. The required
cloud computing resources have also become larger. To solve this problem, edge
computing is proposed. Edge is granted with more computation power to process
data before sending it to the cloud. Therefore, the data transmitted over the
Internet and the computing resources required by the cloud can be effectively
reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline
(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the
concept of skyline candidate set to prune data that are less likely to become
the skyline data on the parallel edge computing nodes. With the candidate
skyline set, each edge computing node only sends the information required to
the server for updating the global skyline, which reduces the amount of data
that transfer over the internet. According to the simulation results, the
proposed method is better than two comparative methods, which reduces the
latency of processing two-dimensional data by more than 50%. For
high-dimensional data, the proposed EPUS method also outperforms the other
existing methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [16] [A 60-Addition, Rank-23 Scheme for Exact 3x3 Matrix Multiplication](https://arxiv.org/abs/2508.03857)
*Joshua Stapleton*

Main category: cs.DS

TL;DR: 将3x3非交换矩阵乘法的加法成本从61和62降低到60，无需改变基。


<details>
  <summary>Details</summary>
Motivation: 改进非交换矩阵乘法的效率，突破现有记录。

Method: 通过优化算法，减少加法操作的数量，保持基不变。

Result: 将加法成本降至60，创下新纪录。

Conclusion: 该方法在非交换矩阵乘法中实现了更高效的计算。

Abstract: We reduce the additive cost of general (non-commutative) 3x3 matrix
multiplication from the previous records of 61 (Schwartz-Vaknin, 2023) and 62
(Martensson-Wagner, 2025) to 60 without a change of basis. To our knowledge,
this represents a new state-of-the-art.

</details>


### [17] [Counting Distinct Square Substrings in Sublinear Time](https://arxiv.org/abs/2508.03930)
*Panagiotis Charalampopoulos,Manal Mohamed,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 提出了一种在压缩字符串中计算不同平方数的次线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决压缩字符串中平方数计算的效率问题，突破现有线性时间算法的限制。

Method: 利用长周期运行和层运行的组合性质，结合稀疏Lyndon根技术。

Result: 在压缩字符串中，算法时间复杂度为O(n/log_σ n)，优于之前的线性时间算法。

Conclusion: 首次实现了压缩字符串中平方数的次线性时间计算，为相关领域提供了新方法。

Abstract: We show that the number of distinct squares in a packed string of length $n$
over an alphabet of size $\sigma$ can be computed in $O(n/\log_\sigma n)$ time
in the word-RAM model. This paper is the first to introduce a sublinear-time
algorithm for counting squares in the packed setting. The packed representation
of a string of length $n$ over an alphabet of size $\sigma$ is given as a
sequence of $O(n/\log_\sigma n)$ machine words in the word-RAM model (a machine
word consists of $\omega \ge \log_2 n$ bits). Previously, it was known how to
count distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for
a string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al.,
CPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for
extracting squares from runs described by Crochemore et al. [TCS 2014].
However, the packed model requires novel approaches.
  We need an $O(n/\log_\sigma n)$-sized representation of all long-period runs
(runs with period $\Omega(\log_\sigma n)$) which allows for a sublinear-time
counting of the -- potentially linearly-many -- implied squares. The
long-period runs with a string period that is periodic itself (called layer
runs) are an obstacle, since their number can be $\Omega(n)$. The number of all
other long-period runs is $O(n/\log_\sigma n)$ and we can construct an implicit
representation of all long-period runs in $O(n/\log_\sigma n)$ time by
leveraging the insights of Amir et al. [ESA 2019]. We count squares in layer
runs by exploiting combinatorial properties of pyramidally-shaped groups of
layer runs. Another difficulty lies in computing the locations of Lyndon roots
of runs in packed strings, which is needed for grouping runs that may generate
equal squares. To overcome this difficulty, we introduce sparse-Lyndon roots
which are based on string synchronizers [Kempa and Kociumaka, STOC 2019].

</details>


### [18] [Exactly simulating stochastic chemical reaction networks in sub-constant time per reaction](https://arxiv.org/abs/2508.04079)
*Joshua Petrack,David Doty*

Main category: cs.DS

TL;DR: 提出了一种新的化学反应网络随机模拟算法，其运行时间在特定条件下可证明为亚线性，优于传统的Gillespie算法。


<details>
  <summary>Details</summary>
Motivation: 传统Gillespie算法模拟ℓ次反应的时间为Ω(ℓ)，效率较低，需要更高效的算法。

Method: 基于Berenbrink等人的分布式计算模型算法，扩展并适应化学反应网络，实现亚线性时间模拟。

Result: 新算法在ℓ≥n^{5/4}时运行时间为O(ℓ/√n)，在n≤ℓ≤n^{5/4}时为O(ℓ/n^{2/5})。

Conclusion: 新算法在理论和实践中均表现出色，为化学反应网络模拟提供了更高效的解决方案。

Abstract: The model of chemical reaction networks is among the oldest and most widely
studied and used in natural science. The model describes reactions among
abstract chemical species, for instance $A + B \to C$, which indicates that if
a molecule of type $A$ interacts with a molecule of type $B$ (the reactants),
they may stick together to form a molecule of type $C$ (the product). The
standard algorithm for simulating (discrete, stochastic) chemical reaction
networks is the Gillespie algorithm [JPC 1977], which stochastically simulates
one reaction at a time, so to simulate $\ell$ consecutive reactions, it
requires total running time $\Omega(\ell)$.
  We give the first chemical reaction network stochastic simulation algorithm
that can simulate $\ell$ reactions, provably preserving the exact stochastic
dynamics (sampling from precisely the same distribution as the Gillespie
algorithm), yet using time provably sublinear in $\ell$. Under reasonable
assumptions, our algorithm can simulate $\ell$ reactions among $n$ total
molecules in time $O(\ell/\sqrt n)$ when $\ell \ge n^{5/4}$, and in time
$O(\ell/n^{2/5})$ when $n \le \ell \le n^{5/4}$. Our work adapts an algorithm
of Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020] for
simulating the distributed computing model known as population protocols,
extending it (in a very nontrivial way) to the more general chemical reaction
network setting.
  We provide an implementation of our algorithm as a Python package, with the
core logic implemented in Rust, with remarkably fast performance in practice.

</details>


### [19] [Exact Matching in Matrix Multiplication Time](https://arxiv.org/abs/2508.04081)
*Ryotaro Sato,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 本文回顾了代数匹配算法的基本事实，并借助矩阵特征多项式的快速计算探讨了可能的改进。特别地，证明了精确匹配问题可以在与矩阵乘法相同的时间复杂度内高概率解决，并讨论了其在线性拟阵配对问题中的扩展。


<details>
  <summary>Details</summary>
Motivation: 研究代数算法在匹配及相关问题中的应用，探索通过矩阵特征多项式快速计算改进现有方法的可能性。

Method: 利用矩阵特征多项式的快速计算技术，分析精确匹配问题的算法复杂度。

Result: 证明了精确匹配问题可以在与矩阵乘法相同的时间复杂度内高概率解决。

Conclusion: 通过代数方法改进匹配算法是可行的，且可以扩展到更复杂的线性拟阵配对问题。

Abstract: Initiated by Mulmuley, Vazirani, and Vazirani (1987), many algebraic
algorithms have been developed for matching and related problems. In this
paper, we review basic facts and discuss possible improvements with the aid of
fast computation of the characteristic polynomial of a matrix. In particular,
we show that the so-called exact matching problem can be solved with high
probability in asymptotically the same time order as matrix multiplication. We
also discuss its extension to the linear matroid parity problem.

</details>


### [20] [Approximation Algorithms for Scheduling Crowdsourcing Tasks in Mobile Social Networks](https://arxiv.org/abs/2508.04159)
*Chi-Yeh Chen*

Main category: cs.DS

TL;DR: 本文纠正了Zhang等人关于移动社交网络中调度问题的近似比分析错误，并提出了新的随机和确定性算法以优化任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 解决移动社交网络中任务调度的近似比分析错误，并改进算法性能。

Method: 1. 纠正Zhang等人的近似比分析；2. 提出随机近似算法（1.5 + ε）；3. 提出确定性近似算法（max{2.5,1+ε}）。

Result: 随机算法达到1.5 + ε的近似比，确定性算法在特定条件下可达1.5 + ε。

Conclusion: 本文提出的算法显著改进了移动社交网络中任务调度的性能。

Abstract: This paper addresses the scheduling problem in mobile social networks. We
begin by proving that the approximation ratio analysis presented in the paper
by Zhang \textit{et al.} (IEEE Transactions on Mobile Computing, 2025) is
incorrect, and we provide the correct analysis results. Furthermore, when the
required service time for a task exceeds the total contact time between the
requester and the crowd worker, we demonstrate that the approximation ratio of
the Largest-Ratio-First task scheduling algorithm can reach $2 - \frac{1}{m}$.
Next, we introduce a randomized approximation algorithm to minimize mobile
social networks' total weighted completion time. This algorithm achieves an
expected approximation ratio of $1.5 + \epsilon$ for $\epsilon>0$. Finally, we
present a deterministic approximation algorithm that minimizes mobile social
networks' total weighted completion time. This deterministic algorithm achieves
an approximation ratio of $\max\left\{2.5,1+\epsilon\right\}$ for $\epsilon>0$.
Additionally, when the task's required service time or the total contact time
between the requester and the crowd worker is sufficiently large, this
algorithm can reach an approximation ratio of $1.5+\epsilon$ for $\epsilon>0$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices](https://arxiv.org/abs/2508.03846)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 本文提出了17条可操作的共情指南，帮助软件工程团队和组织提升共情能力，并提供了一个可视化优先级框架以支持实施。


<details>
  <summary>Details</summary>
Motivation: 共情在软件工程中被忽视，但其对团队合作、沟通和决策至关重要。基于先前研究发现的策略，本文旨在提供具体指南以促进共情的实践。

Method: 通过分析真实案例、挑战及解决方案，设计17条共情指南，并开发可视化优先级框架以分类指南。

Result: 指南和框架为软件工程实践提供了灵活且实用的共情整合建议。

Conclusion: 本文为软件工程团队提供了从原则到可持续行动的共情实践工具，促进共情在日常工作中的落地。

Abstract: Empathy is a powerful yet often overlooked element in software engineering
(SE), supporting better teamwork, smoother communication, and effective
decision-making. In our previous study, we identified a range of practitioner
strategies for fostering empathy in SE contexts. Building on these insights,
this paper introduces 17 actionable empathy guidelines designed to support
practitioners, teams, and organisations. We also explore how these guidelines
can be implemented in practice by examining real-world applications,
challenges, and strategies to overcome them shared by software practitioners.
To support adoption, we present a visual prioritisation framework that
categorises the guidelines based on perceived importance, ease of
implementation, and willingness to adopt. The findings offer practical and
flexible suggestions for integrating empathy into everyday SE work, helping
teams move from principles to sustainable action.

</details>


### [22] [Evaluating Software Supply Chain Security in Research Software](https://arxiv.org/abs/2508.03856)
*Richard Hegewald,Rebecca Beyer*

Main category: cs.SE

TL;DR: 研究软件安全性普遍较差，平均得分3.5/10，需改进。


<details>
  <summary>Details</summary>
Motivation: 研究软件的安全性对科学结果的完整性和可重复性至关重要，但目前研究较少。

Method: 分析3,248个高质量研究软件仓库，使用OpenSSF Scorecard评估安全性。

Result: 安全性普遍较弱，重要实践（如签名发布和分支保护）实施率低。

Conclusion: 提出低成本的改进建议，帮助研究团队提升软件安全性。

Abstract: The security of research software is essential for ensuring the integrity and
reproducibility of scientific results. However, research software security is
still largely unexplored. Due to its dependence on open source components and
distributed development practices, research software is particularly vulnerable
to supply chain attacks. This study analyses 3,248 high-quality, largely
peer-reviewed research software repositories using the OpenSSF Scorecard. We
find a generally weak security posture with an average score of 3.5/10.
Important practices, such as signed releases and branch protection, are rarely
implemented. Finally, we present actionable, low-effort recommendations that
can help research teams improve software security and mitigate potential
threats to scientific integrity.

</details>


### [23] [From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential](https://arxiv.org/abs/2508.03881)
*Martin Obaidi,Kushtrim Qengaj,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Elisa Schmid,Kurt Schneider*

Main category: cs.SE

TL;DR: 研究发现，应用属性对用户解释需求的预测能力有限，需结合直接用户反馈设计用户中心软件。


<details>
  <summary>Details</summary>
Motivation: 探讨应用属性是否能预测用户解释需求，以支持早期开发和大规模需求挖掘。

Method: 分析4,495条应用评论的黄金标准数据集，进行相关性分析和线性回归建模。

Result: 应用属性与解释需求关联较弱，仅特定特征（如版本、评论数、评分）有中等相关性。

Conclusion: 解释需求高度依赖上下文，需结合用户反馈设计可解释的软件系统。

Abstract: In today's digitized world, software systems must support users in
understanding both how to interact with a system and why certain behaviors
occur. This study investigates whether explanation needs, classified from user
reviews, can be predicted based on app properties, enabling early consideration
during development and large-scale requirements mining. We analyzed a gold
standard dataset of 4,495 app reviews enriched with metadata (e.g., app
version, ratings, age restriction, in-app purchases). Correlation analyses
identified mostly weak associations between app properties and explanation
needs, with moderate correlations only for specific features such as app
version, number of reviews, and star ratings. Linear regression models showed
limited predictive power, with no reliable forecasts across configurations.
Validation on a manually labeled dataset of 495 reviews confirmed these
findings. Categories such as Security & Privacy and System Behavior showed
slightly higher predictive potential, while Interaction and User Interface
remained most difficult to predict. Overall, our results highlight that
explanation needs are highly context-dependent and cannot be precisely inferred
from app metadata alone. Developers and requirements engineers should therefore
supplement metadata analysis with direct user feedback to effectively design
explainable and user-centered software systems.

</details>


### [24] [A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output](https://arxiv.org/abs/2508.03922)
*Soroush Heydari*

Main category: cs.SE

TL;DR: 论文研究了GitHub Copilot等AI编程助手在满足人类需求方面的挑战，提出了以人为中心的需求框架，并评估了其适应性和协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架多关注技术层面，忽略了影响AI助手成功整合到软件开发工作流中的人类因素。

Method: 通过分析GitHub Copilot的聊天界面交互，测量其适应不同用户专业水平的能力，并评估其促进协作编程的效果。

Result: 建立了以人为中心的需求框架，并测试了GitHub Copilot在这些方面的表现。

Conclusion: 研究结果对未来分析自动化编程中的人类需求具有启示意义。

Abstract: The rapid adoption of Artificial Intelligence(AI) programming assistants such
as GitHub Copilot introduces new challenges in how these software tools address
human needs. Many existing evaluation frameworks address technical aspects such
as code correctness and efficiency, but often overlook crucial human factors
that affect the successful integration of AI assistants in software development
workflows. In this study, I analyzed GitHub Copilot's interaction with users
through its chat interface, measured Copilot's ability to adapt explanations
and code generation to user expertise levels, and assessed its effectiveness in
facilitating collaborative programming experiences. I established a
human-centered requirements framework with clear metrics to evaluate these
qualities in GitHub Copilot chat. I discussed the test results and their
implications for future analysis of human requirements in automated
programming.

</details>


### [25] [Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems](https://arxiv.org/abs/2508.03931)
*Everton Guimaraes,Nathalia Nascimento,Chandan Shivalingaiah,Asish Nelapati*

Main category: cs.SE

TL;DR: 本研究对ChatGPT、Copilot、Gemini和DeepSeek四种大型语言模型在150道LeetCode题目上的表现进行了系统比较，评估了执行时间、内存使用和算法复杂度，揭示了各模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件工程中的广泛应用，对其性能进行系统比较以优化实际应用至关重要。

Method: 研究通过Java和Python生成150道LeetCode题目的解决方案，评估执行时间、内存使用和算法复杂度。

Result: ChatGPT在时间和内存使用上表现一致高效，Copilot和DeepSeek随任务复杂度增加表现波动，Gemini在简单任务上有效但难度增加时需更多尝试。

Conclusion: 研究结果提供了各模型优缺点的实用见解，为开发者选择适合特定编码任务的模型提供了指导。

Abstract: Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are
transforming software engineering by automating key tasks, including code
generation, testing, and debugging. As these models become integral to
development workflows, a systematic comparison of their performance is
essential for optimizing their use in real world applications. This study
benchmarks these four prominent LLMs on one hundred and fifty LeetCode problems
across easy, medium, and hard difficulties, generating solutions in Java and
Python. We evaluate each model based on execution time, memory usage, and
algorithmic complexity, revealing significant performance differences. ChatGPT
demonstrates consistent efficiency in execution time and memory usage, while
Copilot and DeepSeek show variability as task complexity increases. Gemini,
although effective on simpler tasks, requires more attempts as problem
difficulty rises. Our findings provide actionable insights into each model's
strengths and limitations, offering guidance for developers selecting LLMs for
specific coding tasks and providing insights on the performance and complexity
of GPT-like generated solutions.

</details>


### [26] [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 论文研究了代码语言模型压缩技术对对抗性鲁棒性的影响，发现压缩模型在性能上接近原始模型，但在对抗攻击下鲁棒性显著下降。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的代码语言模型在软件分析任务中表现优异，但其高计算成本和环境影响阻碍了广泛应用。模型压缩技术虽能缓解这些问题，但其在对抗性场景下的鲁棒性影响尚不明确。

Method: 研究评估了三种常见压缩策略对三种广泛使用的代码语言模型的影响，采用六种评估指标和四种经典对抗攻击。

Result: 压缩模型在性能上与原始模型相当，但在对抗攻击下鲁棒性显著降低，揭示了模型大小与鲁棒性之间的权衡。

Conclusion: 研究强调了在安全关键应用中谨慎部署压缩模型的必要性，并呼吁进一步研究平衡计算效率与对抗性鲁棒性的压缩策略。

Abstract: Transformer-based language models for code have shown remarkable performance
in various software analytics tasks, but their adoption is hindered by high
computational costs, slow inference speeds, and substantial environmental
impact. Model compression techniques such as pruning, quantization, and
knowledge distillation have gained traction in addressing these challenges.
However, the impact of these strategies on the robustness of compressed
language models for code in adversarial scenarios remains poorly understood.
Understanding how these compressed models behave under adversarial attacks is
essential for their safe and effective deployment in real-world applications.
To bridge this knowledge gap, we conduct a comprehensive evaluation of how
common compression strategies affect the adversarial robustness of compressed
models. We assess the robustness of compressed versions of three widely used
language models for code across three software analytics tasks, using six
evaluation metrics and four commonly used classical adversarial attacks. Our
findings indicate that compressed models generally maintain comparable
performance to their uncompressed counterparts. However, when subjected to
adversarial attacks, compressed models exhibit significantly reduced
robustness. These results reveal a trade-off between model size reduction and
adversarial robustness, underscoring the need for careful consideration when
deploying compressed models in security-critical software applications. Our
study highlights the need for further research into compression strategies that
strike a balance between computational efficiency and adversarial robustness,
which is essential for deploying reliable language models for code in
real-world software applications.

</details>


### [27] [Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks](https://arxiv.org/abs/2508.04125)
*Sangwon Hyun,Hyunjun Kim,Jinhyuk Jang,Hyojin Choi,M. Ali Babar*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）在软件工程任务中的应用，发现现有研究多局限于函数级任务和常见提示模式。通过设计实验，分析了影响代码生成效率的人机交互（HLI）特征，并提出了改进指南和错误分类。


<details>
  <summary>Details</summary>
Motivation: 现有研究在复杂工作流（如多类依赖）和HLI特征对代码生成效率的影响方面存在空白，需更全面的分析。

Method: 设计了包含项目级任务的实验，招募36名参与者使用GPT助手完成任务，通过屏幕记录和聊天日志分析HLI特征和行为。

Result: 发现3个HLI特征显著影响代码生成效率，提出5条改进指南和29种错误分类及缓解方案。

Conclusion: 研究填补了复杂工作流中HLI特征的空白，为提升代码生成效率提供了实践指导。

Abstract: The application of Large Language Models (LLMs) is growing in the productive
completion of Software Engineering tasks. Yet, studies investigating the
productive prompting techniques often employed a limited problem space,
primarily focusing on well-known prompting patterns and mainly targeting
function-level SE practices. We identify significant gaps in real-world
workflows that involve complexities beyond class-level (e.g., multi-class
dependencies) and different features that can impact Human-LLM Interactions
(HLIs) processes in code generation. To address these issues, we designed an
experiment that comprehensively analyzed the HLI features regarding the code
generation productivity. Our study presents two project-level benchmark tasks,
extending beyond function-level evaluations. We conducted a user study with 36
participants from diverse backgrounds, asking them to solve the assigned tasks
by interacting with the GPT assistant using specific prompting patterns. We
also examined the participants' experience and their behavioral features during
interactions by analyzing screen recordings and GPT chat logs. Our statistical
and empirical investigation revealed (1) that three out of 15 HLI features
significantly impacted the productivity in code generation; (2) five primary
guidelines for enhancing productivity for HLI processes; and (3) a taxonomy of
29 runtime and logic errors that can occur during HLI processes, along with
suggested mitigation plans.

</details>


### [28] [EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust是一个自动化框架，用于将整个C项目转换为等效的Rust代码，结合了规则和LLM方法的优势，显著提高了翻译的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: Rust的编译时安全特性使其成为安全关键系统的理想选择，但现有的C到Rust转换方法在小规模程序上表现有限，无法满足大规模项目的需求。

Method: EvoC2Rust采用骨架引导的翻译策略，分三个阶段：模块分解与骨架生成、增量函数翻译和编译错误修复，结合了LLM和静态分析。

Result: 在开源基准和工业项目上，EvoC2Rust在语法和语义准确性上分别提高了17.24%和14.32%，代码安全性比规则工具高96.79%。

Conclusion: EvoC2Rust在大规模项目级C到Rust转换中表现出色，模块级编译和测试通过率分别达到92.25%和89.53%。

Abstract: Rust's compile-time safety guarantees make it ideal for safety-critical
systems, creating demand for translating legacy C codebases to Rust. While
various approaches have emerged for this task, they face inherent trade-offs:
rule-based solutions face challenges in meeting code safety and idiomaticity
requirements, while LLM-based solutions often fail to generate semantically
equivalent Rust code, due to the heavy dependencies of modules across the
entire codebase. Recent studies have revealed that both solutions are limited
to small-scale programs. In this paper, we propose EvoC2Rust, an automated
framework for converting entire C projects to equivalent Rust ones. EvoC2Rust
employs a skeleton-guided translation strategy for project-level translation.
The pipeline consists of three evolutionary stages: 1) it first decomposes the
C project into functional modules, employs a feature-mapping-enhanced LLM to
transform definitions and macros and generates type-checked function stubs,
which form a compilable Rust skeleton; 2) it then incrementally translates the
function, replacing the corresponding stub placeholder; 3) finally, it repairs
compilation errors by integrating LLM and static analysis. Through evolutionary
augmentation, EvoC2Rust combines the advantages of both rule-based and
LLM-based solutions. Our evaluation on open-source benchmarks and six
industrial projects demonstrates EvoC2Rust's superior performance in
project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%
improvements in syntax and semantic accuracy over the LLM-based approaches,
along with a 96.79% higher code safety rate than the rule-based tools. At the
module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates
on industrial projects, even for complex codebases and long functions.

</details>


### [29] [Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models](https://arxiv.org/abs/2508.04352)
*Dragana Sunaric,Charlotte Verbruggen,Dominik Bork*

Main category: cs.SE

TL;DR: Vanilla-Converter是一个命令行工具，用于自动化将BPMN模型从Camunda 7迁移到Camunda 8，支持多种BPMN元素，并生成转换日志。


<details>
  <summary>Details</summary>
Motivation: 由于Camunda 7和8之间存在根本性差异，手动迁移复杂且耗时。

Method: 开发了Vanilla-Converter工具，自动化转换BPMN模型，并生成转换日志。

Result: 通过三个实际工业案例验证，工具能成功将模型转换为可执行的Camunda 8模型。

Conclusion: Vanilla-Converter有效简化了迁移过程，提高了效率。

Abstract: As organizations prepare for the end-of-life of Camunda 7, manual migration
remains complex due to fundamental differences between the two platforms. We
present Vanilla-Converter, a command-line tool that facilitates the migration
of BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the
transformation process, supports a wide range of BPMN elements, and produces a
transformed model and a detailed transformation log indicating automatic
changes and remaining manual conversion tasks. The tool's effectiveness is
demonstrated through three case studies with real industrially used Camunda 7
models, confirming its ability to convert these models into valid and
executable Camunda 8 models.

</details>


### [30] [Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making](https://arxiv.org/abs/2508.04408)
*Carlos Andrés Ramírez Cataño,Makoto Itoh*

Main category: cs.SE

TL;DR: 论文提出了一种基于开发者编码习惯的软件缺陷预测框架，其性能优于现有代码和历史指标，并增强了模型的可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 软件缺陷的根本原因常归因于人为错误，但基于非软件指标的预测研究较少。本文探索开发者编码习惯对缺陷预测的作用。

Method: 提出一个框架确定预测指标，并与现有最佳代码和历史指标进行性能对比，分析各指标的重要性。

Result: 在21个关键基础设施开源项目中，新指标的平均预测性能优于现有方法，且新指标的重要性更高。

Conclusion: 基于人为错误的框架显著提升了缺陷预测的可解释性和实用性，为实践提供了可操作的见解。

Abstract: Software defect prediction using code metrics has been extensively researched
over the past five decades. However, prediction harnessing non-software metrics
is under-researched. Considering that the root cause of software defects is
often attributed to human error, human factors theory might offer key
forecasting metrics for actionable insights. This paper explores automated
software defect prediction at the method level based on the developers' coding
habits. First, we propose a framework for deciding the metrics to conduct
predictions. Next, we compare the performance of our metrics to that of the
code and commit history metrics shown by research to achieve the highest
performance to date. Finally, we analyze the prediction importance of each
metric. As a result of our analyses of twenty-one critical infrastructure
large-scale open-source software projects, we have presented: (1) a human
error-based framework with metrics useful for defect prediction at method
level; (2) models using our proposed metrics achieve better average prediction
performance than the state-of-the-art code metrics and history measures; (3)
the prediction importance of all metrics distributes differently with each of
the novel metrics having better average importance than code and history
metrics; (4) the novel metrics dramatically enhance the explainability,
practicality, and actionability of software defect prediction models,
significantly advancing the field. We present a systematic approach to
forecasting defect-prone software methods via a human error framework. This
work empowers practitioners to act on predictions, empirically demonstrating
how developer coding habits contribute to defects in software systems.

</details>


### [31] [Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection](https://arxiv.org/abs/2508.04448)
*Damian Gnieciak,Tomasz Szandala*

Main category: cs.SE

TL;DR: 论文对比了六种自动化代码分析工具（三种静态分析工具和三种大型语言模型）在检测漏洞方面的表现，发现语言模型在召回率上表现更优，但存在高误报和定位不精确的问题，建议结合使用。


<details>
  <summary>Details</summary>
Motivation: 评估现代自动化测试工具在代码漏洞检测中的效果，为开发者提供工具选择的依据。

Method: 使用十个真实C#项目（含63个漏洞）对比六种工具的检测准确性、延迟和开发者工作量。

Result: 语言模型的平均F-1分数（0.797, 0.753, 0.750）高于静态工具（0.260, 0.386, 0.546），但误报率高且定位不精确。

Conclusion: 建议结合语言模型的广泛扫描和静态工具的高精度验证，以提高代码安全性。

Abstract: Modern software relies on a multitude of automated testing and quality
assurance tools to prevent errors, bugs and potential vulnerabilities. This
study sets out to provide a head-to-head, quantitative and qualitative
evaluation of six automated approaches: three industry-standard rule-based
static code-analysis tools (SonarQube, CodeQL and Snyk Code) and three
state-of-the-art large language models hosted on the GitHub Models platform
(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten
real-world C# projects that embed 63 vulnerabilities across common categories
such as SQL injection, hard-coded secrets and outdated dependencies, we measure
classical detection accuracy (precision, recall, F-score), analysis latency,
and the developer effort required to vet true positives. The language-based
scanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their
static counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'
advantage originates from superior recall, confirming an ability to reason
across broader code contexts. However, this benefit comes with substantial
trade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language
models mislocate issues at line-or-column granularity due to tokenisation
artefacts. Overall, language models successfully rival traditional static
analysers in finding real vulnerabilities. Still, their noisier output and
imprecise localisation limit their standalone use in safety-critical audits. We
therefore recommend a hybrid pipeline: employ language models early in
development for broad, context-aware triage, while reserving deterministic
rule-based scanners for high-assurance verification. The open benchmark and
JSON-based result harness released with this paper lay a foundation for
reproducible, practitioner-centric research into next-generation automated code
security.

</details>


### [32] [Manifestations of Empathy in Software Engineering: How, Why, and When It Matters](https://arxiv.org/abs/2508.04479)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 研究探讨了同理心在软件工程中的表现、动机及影响因素，通过访谈和调查揭示了其实际应用和驱动因素。


<details>
  <summary>Details</summary>
Motivation: 理解同理心在软件工程中的具体表现、动机及影响因素，填补现有研究的空白。

Method: 通过22次访谈和116名软件从业者的大规模调查进行研究。

Result: 揭示了同理心在软件工程中的表达方式、驱动因素、适用场景及其他影响因素。

Conclusion: 为软件工程实践和研究提供了如何有效整合同理心的实用建议。

Abstract: Empathy plays a crucial role in software engineering (SE), influencing
collaboration, communication, and decision-making. While prior research has
highlighted the importance of empathy in SE, there is limited understanding of
how empathy manifests in SE practice, what motivates SE practitioners to
demonstrate empathy, and the factors that influence empathy in SE work. Our
study explores these aspects through 22 interviews and a large scale survey
with 116 software practitioners. Our findings provide insights into the
expression of empathy in SE, the drivers behind empathetic practices, SE
activities where empathy is perceived as useful or not, and the other factors
that influence empathy. In addition, we offer practical implications for SE
practitioners and researchers, offering a deeper understanding of how to
effectively integrate empathy into SE processes.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [33] [CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors](https://arxiv.org/abs/2508.03862)
*Abdul Saboor,Zhuangzhuang Cui,Achiel Colpaert,Evgenii Vinogradov,Sofie Pollin*

Main category: cs.NI

TL;DR: 论文提出了一种基于无人机轨迹的上下文感知智能切换协议（CASH），以减少频繁切换对网络性能的影响，并在模拟器中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通（UAM）中无人机频繁切换导致的网络性能下降问题。

Method: 提出CASH协议，利用无人机轨迹进行前瞻性评分，实现主动切换决策。

Result: CASH将切换频率降低78%，同时保持低中断概率。

Conclusion: CASH协议显著提升了UAM通信的可靠性，并通过实验确定了基站密度和安全裕度的最优配置。

Abstract: Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial
Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,
such as air taxis. A key challenge in these high-mobility aerial corridors is
ensuring reliable connectivity, where frequent handovers can degrade network
performance. To resolve this, we present a Context-Aware Smart Handover (CASH)
protocol that uses a forward-looking scoring mechanism based on UAV trajectory
to make proactive handover decisions. We evaluate the performance of the
proposed CASH against existing handover protocols in a custom-built simulator.
Results show that CASH reduces handover frequency by up to 78% while
maintaining low outage probability. We then investigate the impact of base
station density and safety margin on handover performance, where their optimal
setups are empirically obtained to ensure reliable UAM communication.

</details>


### [34] [Confidence Driven Classification of Application Types in the Presence of Background Network](https://arxiv.org/abs/2508.03891)
*Eun Hun Choi,Jasleen Kaur,Vladas Pipiras,Nelson Gomes Rodrigues Antunes,Brendan Massey*

Main category: cs.NI

TL;DR: 论文提出了一种基于高斯混合模型的分类框架，用于提高深度学习分类器在真实网络流量中对背景流量的分类可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在真实网络流量中表现不佳，主要因为忽略了非应用特定的背景流量（如广告、分析工具等），导致分类混淆。

Method: 设计了一个高斯混合模型框架，通过改进分类器的置信度指示，避免对不确定样本的错误分类。

Result: 该方法能够更可靠地区分应用流量和背景流量，减少误分类。

Conclusion: 高斯混合模型框架提升了分类器在真实流量中的可靠性，解决了背景流量带来的混淆问题。

Abstract: Accurately classifying the application types of network traffic using deep
learning models has recently gained popularity. However, we find that these
classifiers do not perform well on real-world traffic data due to the presence
of non-application-specific generic background traffic originating from
advertisements, analytics, shared APIs, and trackers. Unfortunately,
state-of-the-art application classifiers overlook such traffic in curated
datasets and only classify relevant application traffic. To address this issue,
when we label and train using an additional class for background traffic, it
leads to additional confusion between application and background traffic, as
the latter is heterogeneous and encompasses all traffic that is not relevant to
the application sessions. To avoid falsely classifying background traffic as
one of the relevant application types, a reliable confidence measure is
warranted, such that we can refrain from classifying uncertain samples.
Therefore, we design a Gaussian Mixture Model-based classification framework
that improves the indication of the deep learning classifier's confidence to
allow more reliable classification.

</details>


### [35] [Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3](https://arxiv.org/abs/2508.04004)
*Tanguy Ropitault,Matteo Bordin,Paolo Testolina,Michele Polese,Pedram Johari,Nada Golmie,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文扩展了5G-LENA模块，引入基于轨迹的信道模型，提升系统级仿真的几何保真度，支持数字孪生应用。


<details>
  <summary>Details</summary>
Motivation: 现有统计信道模型无法捕捉站点特定现象，如衍射、遮挡等，限制了高保真系统级仿真的能力。

Method: 通过外部射线追踪或测量获取多径分量，构建频域信道矩阵，与现有PHY/MAC栈无缝集成。

Result: 新模块在波束管理和端到端分析中揭示了统计模型无法展现的性能拐点。

Conclusion: 该模块为高保真系统级研究和数字孪生应用提供了关键工具。

Abstract: Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is
challenging because the performance emerges from the tight coupling of
propagation, beam management, scheduling, and higher-layer interactions.
System-level simulation is therefore indispensable, yet the vast majority of
studies rely on the statistical 3GPP channel models. These are well suited to
capture average behavior across many statistical realizations, but cannot
reproduce site-specific phenomena such as corner diffraction, street-canyon
blockage, or deterministic line-of-sight conditions and
angle-of-departure/arrival relationships that drive directional links. This
paper extends 5G-LENA, an NR module for the system-level Network Simulator 3
(ns-3), with a trace-based channel model that processes the Multipath
Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer
(RT)) or measurement campaigns. Our module constructs frequency-domain channel
matrices and feeds them to the existing Physical (PHY)/Medium Access Control
(MAC) stack without any further modifications. The result is a geometry-based
channel model that remains fully compatible with the standard 3GPP
implementation in 5G-LENA, while delivering site-specific geometric fidelity.
This new module provides a key building block toward Digital Twin (DT)
capabilities by offering realistic site-specific channel modeling, unlocking
studies that require site awareness, including beam management, blockage
mitigation, and environment-aware sensing. We demonstrate its capabilities for
precise beam-steering validation and end-to-end metric analysis. In both cases,
the trace-driven engine exposes performance inflections that the statistical
model does not exhibit, confirming its value for high-fidelity system-level
cellular networks research and as a step toward DT applications.

</details>


### [36] [A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks](https://arxiv.org/abs/2508.04015)
*Haoxiang Luo,Kun Yang,Qi Huang,Schahram Dustdar*

Main category: cs.NI

TL;DR: 本文提出了一种两阶段协同优化（TSCO）框架，通过结合电力系统调度和计算能力网络（CPN）任务调度，实现低碳运行。


<details>
  <summary>Details</summary>
Motivation: 大规模人工智能和数据密集型应用的普及推动了计算能力网络（CPN）的发展，但其高能耗和可再生能源（RES）的不稳定性带来了双重挑战。

Method: TSCO框架将问题分解为日前随机机组组合（SUC）阶段和实时运行阶段，前者使用Benders分解求解，后者通过深度强化学习（DRL）代理实现自适应任务调度。

Result: 在IEEE 30节点系统的仿真中，TSCO显著降低了碳排放和运营成本，减少了60%以上的RES弃电，同时保证了服务质量（QoS）。

Conclusion: TSCO框架有效解决了CPN的高能耗和RES不稳定性问题，为低碳运行提供了可行方案。

Abstract: The proliferation of large-scale artificial intelligence and data-intensive
applications has spurred the development of Computing Power Networks (CPNs),
which promise to deliver ubiquitous and on-demand computational resources.
However, the immense energy consumption of these networks poses a significant
sustainability challenge. Simultaneously, power grids are grappling with the
instability introduced by the high penetration of intermittent renewable energy
sources (RES). This paper addresses these dual challenges through a novel
Two-Stage Co-Optimization (TSCO) framework that synergistically manages power
system dispatch and CPN task scheduling to achieve low-carbon operations. The
framework decomposes the complex, large-scale problem into a day-ahead
stochastic unit commitment (SUC) stage and a real-time operational stage. The
former is solved using Benders decomposition for computational tractability,
while in the latter, economic dispatch of generation assets is coupled with an
adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)
agent. This agent makes intelligent, carbon-aware decisions by responding to
dynamic grid conditions, including real-time electricity prices and marginal
carbon intensity. Through extensive simulations on an IEEE 30-bus system
integrated with a CPN, the TSCO framework is shown to significantly outperform
baseline approaches. Results demonstrate that the proposed framework reduces
total carbon emissions and operational costs, while simultaneously decreasing
RES curtailment by more than 60% and maintaining stringent Quality of Service
(QoS) for computational tasks.

</details>


### [37] [Metaverse Framework for Wireless Systems Management](https://arxiv.org/abs/2508.04150)
*Ilias Chrysovergis,Alexandros-Apostolos A. Boulogeorgos,Theodoros A. Tsiftsis,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了一种用于无线系统仿真、模拟和交互的元宇宙框架，整合了XR、DTs、AI、IoT、区块链和6G技术。


<details>
  <summary>Details</summary>
Motivation: 为无线系统的开发和管理提供一个动态、沉浸式的平台，探索未来网络环境的潜力。

Method: 通过XR实现可视化交互，DTs实时监控优化，AI生成3D内容并提升性能，IoT提供实时数据，区块链保障安全，6G支持低延迟通信。

Result: 该框架为无线系统的探索、开发和优化提供了强大工具。

Conclusion: 该框架为未来网络环境的研究和应用提供了重要见解。

Abstract: This article introduces a comprehensive metaverse framework, which is
designed for the simulation, emulation, and interaction with wireless systems.
The proposed framework integrates core metaverse technologies such as extended
reality (XR), digital twins (DTs), artificial intelligence (AI), internet of
things (IoT), blockchain, and advanced 6G networking solutions to create a
dynamic, immersive platform for both system development and management. By
leveraging XR, users can visualize and engage with complex systems, while DTs
enable real-time monitoring and optimization. AI generates the
three-dimensional (3D) content, enhances decision-making and system
performance, whereas IoT devices provide real-time sensor data for boosting the
simulation accuracy. Additionally, blockchain ensures secure, decentralized
interactions, and 5G/6G networks offer the necessary infrastructure for
seamless, low-latency communication. This framework serves as a robust tool for
exploring, developing, and optimizing wireless systems, aiming to provide
valuable insights into the future of networked environments.

</details>


### [38] [DSNS: The Deep Space Network Simulator](https://arxiv.org/abs/2508.04317)
*Joshua Smailes,Filip Futera,Sebastian Köhler,Simon Birnbach,Martin Strohmeier,Ivan Martinovic*

Main category: cs.NI

TL;DR: DSNS是一种新型网络模拟器，专注于大规模卫星网络，解决了现有工具不实用的问题，并展示了其优于现有工具的性能。


<details>
  <summary>Details</summary>
Motivation: 随着卫星网络规模扩大和星际互联网的发展，现有模拟工具已不适用，需要更高效的解决方案。

Method: 开发了DSNS模拟器，通过实现现有协议和CCSDS推荐的DTN模拟参考场景，展示其灵活性和可扩展性。

Result: DSNS在规模和保真度上优于现有工具，为协议开发和测试提供了更高效的平台。

Conclusion: DSNS有助于加速卫星网络的发展，确保通信的高效性和安全性。

Abstract: Simulation tools are commonly used in the development and testing of new
protocols or new networks. However, as satellite networks start to grow to
encompass thousands of nodes, and as companies and space agencies begin to
realize the interplanetary internet, existing satellite and network simulation
tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network
simulator with a focus on large-scale satellite networks. We demonstrate its
improved capabilities compared to existing offerings, showcase its flexibility
and extensibility through an implementation of existing protocols and the DTN
simulation reference scenarios recommended by CCSDS, and evaluate its
scalability, showing that it exceeds existing tools while providing better
fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite
operators, enabling fast iteration on protocol development and testing of
parameters under highly realistic conditions. By removing roadblocks to
research and innovation, we can accelerate the development of upcoming
satellite networks and ensure that their communication is both fast and secure.

</details>


### [39] [Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection](https://arxiv.org/abs/2508.04415)
*Xuan Chen,Yu Huang,Miaowen Wen,Shahid Mumtaz,Fatih Gulec,Anwer Al-Dulaimi,Andrew W. Eckford*

Main category: cs.NI

TL;DR: 本文探讨了分子通信（MC）在构建生物纳米物联网（IoBNT）以应对流行病防控挑战中的潜力，包括病毒传播建模、病毒/感染者检测及病毒突变识别。


<details>
  <summary>Details</summary>
Motivation: 生物纳米物联网（IoBNT）作为一种革命性医疗范式，有望用于流行病控制，但其构建面临挑战。本文旨在通过分子通信技术解决这些挑战。

Method: 讨论了宏观和微观尺度下的MC通道以匹配病毒传播，研究了检测方法及定位机制，并提出了一种病毒突变识别策略，通过ORF3a蛋白模拟验证。

Result: 提出了针对病毒传播的MC模型和检测方法，并通过模拟验证了病毒突变识别策略的有效性。

Conclusion: 本文通过MC技术分析了病毒传播，并利用信号处理技术对抗病毒扩散，为IoBNT在流行病防控中的应用提供了理论支持。

Abstract: The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary
healthcare paradigm, shows promise for epidemic control. This paper explores
the potential of using molecular communication (MC) to address the challenges
in constructing IoBNT for epidemic prevention, specifically focusing on
modeling viral transmission, detecting the virus/infected individuals, and
identifying virus mutations. First, the MC channels in macroscale and
microscale scenarios are discussed to match viral transmission in both scales
separately. Besides, the detection methods for these two scales are also
studied, along with the localization mechanism designed for the virus/infected
individuals. Moreover, an identification strategy is proposed to determine
potential virus mutations, which is validated through simulation using the
ORF3a protein as a benchmark. Finally, open research issues are discussed. In
summary, this paper aims to analyze viral transmission through MC and combat
viral spread using signal processing techniques within MC.

</details>


### [40] [Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions](https://arxiv.org/abs/2508.04526)
*Fannya R. Sandjaja,Ayesha A. Majeed,Abdullah Abdullah,Gyan Wickremasinghe,Karen Rafferty,Vishal Sharma*

Main category: cs.NI

TL;DR: 论文探讨了在分布式网络中零信任架构（ZTA）策略设计的挑战与解决方案，提出了零信任分布式网络（ZTDN）的概念，并通过UPPAAL进行策略的形式化验证，强调了系统安全中责任与问责的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统安全架构因依赖信任而容易受到分布式攻击，尤其是在引入自主AI后，安全需求更加复杂。零信任架构（ZTA）虽为潜在解决方案，但其策略设计问题可能导致未授权访问。

Method: 提出零信任分布式网络（ZTDN）概念，并通过UPPAAL工具对策略进行形式化验证，结合案例研究分析。

Result: 验证了形式化方法在ZTA策略设计中的有效性，并强调了责任与问责机制对系统安全的关键作用。

Conclusion: ZTDN和形式化验证为分布式网络中的安全策略设计提供了可行方案，责任与问责是确保系统安全的必要因素。

Abstract: Traditional security architectures are becoming more vulnerable to
distributed attacks due to significant dependence on trust. This will further
escalate when implementing agentic AI within the systems, as more components
must be secured over a similar distributed space. These scenarios can be
observed in consumer technologies, such as the dense Internet of things (IoT).
Here, zero-trust architecture (ZTA) can be seen as a potential solution, which
relies on a key principle of not giving users explicit trust, instead always
verifying their privileges whenever a request is made. However, the overall
security in ZTA is managed through its policies, and unverified policies can
lead to unauthorized access. Thus, this paper explores challenges and solutions
for ZTA policy design in the context of distributed networks, which is referred
to as zero-trust distributed networks (ZTDN). This is followed by a case-study
on formal verification of policies using UPPAAL. Subsequently, the importance
of accountability and responsibility in the system's security is discussed.

</details>


### [41] [CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps](https://arxiv.org/abs/2508.04556)
*Filipe B. Teixeira,Carolina Simões,Paulo Fidalgo,Wagner Pedrosa,André Coelho,Manuel Ricardo,Luis M. Pessoa*

Main category: cs.NI

TL;DR: 论文提出了一种结合电信与计算机视觉的新架构，通过多智能体方法实时传输无线电和视频感知信息，支持5G/6G RAN的实时控制。


<details>
  <summary>Details</summary>
Motivation: 高频无线链路主要依赖视距传输，视觉数据可预测信道动态并帮助克服障碍，如通过波束成形或切换技术。

Method: 提出了一种多智能体架构，将实时无线电和视频感知信息传递给O-RAN xApps，并引入新的视频功能生成阻塞信息。

Result: 实验显示感知信息延迟低于1毫秒，xApp能成功利用无线电和视频信息实时控制5G/6G RAN。

Conclusion: 该架构实现了集成感知与通信，为高频无线链路的障碍预测和实时控制提供了有效解决方案。

Abstract: Telecommunications and computer vision have evolved independently. With the
emergence of high-frequency wireless links operating mostly in line-of-sight,
visual data can help predict the channel dynamics by detecting obstacles and
help overcoming them through beamforming or handover techniques.
  This paper proposes a novel architecture for delivering real-time radio and
video sensing information to O-RAN xApps through a multi-agent approach, and
introduces a new video function capable of generating blockage information for
xApps, enabling Integrated Sensing and Communications. Experimental results
show that the delay of sensing information remains under 1\,ms and that an xApp
can successfully use radio and video sensing information to control the 5G/6G
RAN in real-time.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过结合监督对比学习和特权信息学习，提升了情感计算模型从实验室到现实环境的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从受控实验室环境到非受控现实环境迁移的可靠性问题。

Method: 提出Privileged Contrastive Pretraining (PriCon)框架，结合监督对比学习(SCL)和特权信息学习(LUPI)。

Result: 在RECOLA和AGAIN数据集上，PriCon模型表现优于LUPI和端到端模型，部分情况下接近全模态训练模型。

Conclusion: PriCon为缩小实验室与现实环境情感建模差距提供了可扩展的实用解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [43] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种新型轨迹压缩框架，通过结合频域物理建模和误差有界优化，支持任意维度轨迹压缩，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有线简化方法通常假设2D轨迹，忽略时间同步和运动连续性，无法满足高维轨迹压缩需求。

Method: PILOT-C集成频域物理建模与误差有界优化，独立压缩每个空间轴，支持任意维度轨迹。

Result: 在四个真实数据集上，PILOT-C压缩比平均优于CISED-W 19.2%，误差减少32.6%，3D轨迹压缩比提升49%。

Conclusion: PILOT-C在压缩比和轨迹保真度上显著优于现有方法，且支持高维轨迹，计算复杂度不变。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [44] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种基于课程强化学习和可验证过程奖励的生成模型，用于胸部X光片的多任务诊断，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态模型在CXR诊断中缺乏可验证推理监督的问题，如冗长推理、稀疏奖励和幻觉。

Method: 使用CX-Set数据集和课程强化学习（CuRL-VPR），分两阶段优化模型推理能力。

Result: 在视觉理解、文本生成和对齐方面显著优于现有模型，平均性能提升25.1%。

Conclusion: CX-Mind在多任务CXR诊断中表现出色，临床实用性得到专家验证。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [45] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 提出了Latent Knowledge Scalpel (LKS)，一种通过轻量级超网络编辑LLM内部表示的方法，支持大规模精确编辑，同时保留模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM中预训练信息不准确或过时的问题，现有方法难以同时编辑大量事实信息且可能损害模型通用能力。

Method: 通过操纵特定实体的潜在知识，利用轻量级超网络实现精确编辑，类似自然语言输入的编辑方式。

Result: 在Llama-2和Mistral上实验，即使同时编辑10,000条信息，LKS仍能有效编辑知识并保留模型通用能力。

Conclusion: LKS是一种可行且高效的方法，能够大规模编辑LLM知识而不损害其通用能力。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [46] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一种多模态梯度提升框架，结合临床特征、眼底图像和专家文本描述，用于青光眼风险预测，显著优于基线模型，验证准确率达98.71%。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测青光眼至关重要，但现有方法依赖单模态数据且缺乏可解释性，限制了临床实用性。

Method: GlaBoost整合结构化临床特征、眼底图像嵌入和专家文本描述，使用预训练卷积编码器和基于Transformer的语言模型提取特征，通过增强的XGBoost模型进行分类。

Result: 在真实数据集上，GlaBoost验证准确率达98.71%，特征重要性分析显示杯盘比、视盘苍白和特定文本嵌入对模型决策贡献最大。

Conclusion: GlaBoost为可解释的青光眼诊断提供了透明且可扩展的解决方案，并可扩展到其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [47] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种新的低秩Tucker表示模型（LRTuckerRep），结合全局和局部先验建模，通过自适应的加权核范数和稀疏Tucker核心实现低秩性，并通过无参数拉普拉斯正则化捕获平滑性。实验表明其在多维数据补全中表现优越。


<details>
  <summary>Details</summary>
Motivation: 多维数据补全在计算科学中至关重要，但现有方法（如低秩近似或局部平滑正则化）存在计算成本高或泛化性差的问题。

Method: 提出LRTuckerRep模型，结合低秩Tucker分解、自适应加权核范数和无参数拉普拉斯正则化，并开发两种迭代算法。

Result: 在多维图像修复和交通数据填补实验中，LRTuckerRep在高缺失率下表现出更高的准确性和鲁棒性。

Conclusion: LRTuckerRep通过统一全局和局部先验建模，显著提升了多维数据补全的性能。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [48] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型（LLM）的自动化先验分布生成框架LLMPrior，并通过联邦算法Fed-LLMPrior实现多代理系统中的先验分布聚合。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断中先验分布的设定是一个主观且难以扩展的瓶颈问题，需要自动化解决方案。

Method: 结合LLM与显式生成模型（如高斯混合模型），设计LLMPrior算子，将非结构化上下文转化为有效概率分布；并扩展为多代理系统，使用对数意见池（LogOP）聚合先验。

Result: 提出了LLMPrior和Fed-LLMPrior算法，能够自动化生成并聚合先验分布，降低贝叶斯建模的门槛。

Conclusion: 该框架为贝叶斯建模提供了新的工具，有望推动先验分布的自动化与规模化应用。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [49] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 论文提出了一种在线分布鲁棒强化学习方法，解决了现有方法依赖生成模型或离线数据的局限性，通过优化最坏情况性能，在未知环境中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实部署中因模拟与实际的差异（sim-to-real gap）表现不佳，现有分布鲁棒方法依赖生成模型或广泛覆盖的离线数据，限制了在未知环境中的实用性。

Method: 研究在线分布鲁棒强化学习，提出基于$f$-散度的不确定性集合（如Chi-Square和KL散度球）的高效算法，具有次线性遗憾保证。

Result: 理论证明了算法的近最优性，并通过多环境实验验证了其鲁棒性和效率。

Conclusion: 该方法在未知环境中实现了高效的分布鲁棒性能，为实际部署提供了理论支持和实验验证。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [50] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 论文分析了GRPO的两大局限性，并提出GTPO作为改进方案，通过跳过冲突令牌的负更新和过滤高熵补全，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在语言模型训练中存在令牌冲突和输出分布扁平化的问题，影响模型性能。

Method: 提出GTPO，识别冲突令牌并跳过负更新，同时过滤高熵补全，避免KL散度正则化。

Result: 在GSM8K、MATH和AIME 2024基准测试中验证了GTPO的稳定性和性能提升。

Conclusion: GTPO解决了GRPO的局限性，提供了一种更稳定有效的策略优化方法。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [51] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的物理信息网络（U-PINet），用于高效且物理一致的电磁散射建模，解决了传统数值求解器和纯数据驱动方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高且难以扩展，而纯数据驱动方法缺乏物理约束且需要大量标注数据。U-PINet旨在结合物理一致性和计算效率。

Method: 采用U形物理信息网络，通过多尺度处理神经网络架构和稀疏图表示，建模近场和远场相互作用的分解与耦合。

Result: U-PINet准确预测表面电流分布，与传统求解器结果一致，同时显著减少计算时间，并在准确性和鲁棒性上优于传统深度学习方法。

Conclusion: U-PINet为电磁散射建模提供了一种高效、物理一致且泛化能力强的解决方案，适用于下游应用如雷达截面预测。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [52] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 本文提出了一种基于时空预测框架的方法，利用众包用户侧KPI和监管数据集预测频谱需求，优于传统ITU模型。


<details>
  <summary>Details</summary>
Motivation: 频谱需求预测对频谱分配、监管规划和无线通信网络可持续发展至关重要，支持ITU等机构制定公平政策和满足5G、6G及IoT需求。

Method: 采用先进的时空预测框架，结合特征工程、相关性分析和迁移学习技术，利用众包KPI和监管数据建模。

Result: 实验结果表明，该方法预测精度高，具有跨区域泛化能力，优于传统ITU模型。

Conclusion: 该方法为政策制定者和监管机构提供了更现实、可操作的频谱管理工具。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [53] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理信息的神经网络（PINN）用于快速准确估计核聚变装置EAST中的热通量，相比传统有限元方法（FEM）实现了40倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统FEM方法依赖网格采样，计算效率低且难以实时模拟，而人工智能驱动的科学计算为这一问题提供了新思路。

Method: 通过输入空间坐标和时间戳，结合热传导方程计算边界损失、初始条件损失和物理损失，并采用数据驱动方式采样少量数据点以提高模型预测能力。

Result: 实验表明，PINN在均匀和非均匀加热条件下均达到与FEM相当的精度，同时计算效率提升40倍。

Conclusion: PINN为核聚变装置中的热通量估计提供了一种高效且准确的解决方案。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [54] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: SoilNet是一种多模态多任务模型，用于解决土壤层次分类问题，结合图像数据和地理时间元数据，通过模块化流程实现精确分类。


<details>
  <summary>Details</summary>
Motivation: 土壤层次分类对土壤健康监测至关重要，影响农业、生态系统和气候适应能力，但目前的基础模型尚未有效解决其多模态、多任务和复杂标签结构的挑战。

Method: SoilNet通过模块化流程整合图像和元数据，预测深度标记、分割土壤剖面，提取形态特征，并利用图表示标签层次关系进行分类。

Result: 在真实土壤剖面数据集上验证了方法的有效性。

Conclusion: SoilNet为解决复杂层次分类问题提供了有效方案，代码和实验已开源。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [55] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE框架通过智能子采样减少数据量，提升模型精度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放的终结，高效训练需要重新思考数据量。

Method: 开发SICKLE框架，采用最大熵采样方法，并与随机和相空间采样对比。

Result: 在Frontier上大规模评估，子采样可提升精度并降低能耗（最高38倍）。

Conclusion: 智能子采样是高效训练的有效方法。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [56] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 论文提出了一种名为Bernoulli-LoRA的新理论框架，通过引入伯努利机制选择更新的矩阵，统一并扩展了现有的LoRA方法，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模指数级增长，参数高效微调（PEFT）成为适应大型基础模型的关键方法。LoRA因其简单有效而备受关注，但其理论理解仍有限。本文旨在填补这一空白。

Method: 提出Bernoulli-LoRA框架，通过伯努利概率机制选择更新的矩阵，涵盖并推广现有策略。在非凸优化假设下，分析多种变体的收敛性，并扩展到凸非光滑函数。

Result: 理论分析证明了各变体的收敛性，实验验证了方法的实际效果。

Conclusion: Bernoulli-LoRA为PEFT方法提供了理论支持，同时保持了实用性，是理论与实际结合的重要一步。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [57] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种新型的基于神经网络的贝叶斯优化方法，通过分离探索和利用标准，避免了模型不确定性估计的计算复杂性，在高效优化的同时显著减少了计算时间和函数评估次数。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化（BO）在高维和大规模函数评估时面临计算复杂性挑战，而基于神经网络的BO方法虽具扩展性，但模型不确定性估计复杂且计算量大。

Method: 提出SNBO方法，通过分离探索和利用标准，自适应控制采样区域，避免模型不确定性估计，实现高效优化。

Result: 在10至102维的优化问题中，SNBO在大多数测试问题上优于基线算法，减少40-60%的函数评估次数，运行时间降低至少一个数量级。

Conclusion: SNBO是一种高效且可扩展的优化方法，适用于高维和大规模优化问题。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [58] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了一种名为DP-NCB的新算法框架，同时实现差分隐私和公平性，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机算法中，隐私和公平性通常被独立研究，但两者能否同时实现尚不明确。

Method: 提出了差分隐私纳什置信界（DP-NCB）框架，支持全局和局部差分隐私模型，无需预知时间范围。

Result: 理论证明DP-NCB在隐私和公平性上均达到最优，实验显示其纳什遗憾低于现有基线。

Conclusion: DP-NCB为高社会影响应用提供了隐私保护且公平的算法设计基础。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [59] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种可分部训练的替代模型VAE-DNN，用于求解参数化非线性偏微分方程的正反问题，相比FNO和DeepONet更高效且更准确。


<details>
  <summary>Details</summary>
Motivation: 现有替代模型和算子学习模型（如FNO和DeepONet）在训练时间和能耗上较高，需要一种更高效的训练方法。

Method: 使用编码器将高维输入降维，通过全连接神经网络映射到PDE解的潜在空间，再通过解码器重构解。创新点在于三个组件可独立训练。

Result: VAE-DNN在非线性扩散方程的正反问题求解中，比FNO和DeepONet更高效且更准确。

Conclusion: VAE-DNN通过独立训练组件显著提升了训练效率和精度，适用于复杂PDE问题。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [60] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息论的智能数据子采样方法，用于离线学习，以减少下游预测的不确定性，并在实验中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 数据流中生成的数据需要高效处理，同时保持计算成本可控，因此需要一种智能的数据子采样方法。

Method: 采用信息论方法，专注于减少下游预测的不确定性，进行智能数据子采样。

Result: 实验表明，该方法在两个广泛研究的问题上优于之前的信息论技术。

Conclusion: 实际应用中，可靠的高性能需要精心设计的模型。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [61] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架，结合脉冲控制和切换控制，用于个性化T1DM治疗决策，显著降低血糖水平违规率。


<details>
  <summary>Details</summary>
Motivation: 解决T1DM治疗中干预效果的延迟性和异质性，提升个性化治疗决策的安全性。

Method: 结合脉冲控制（快速干预）和切换控制（长期干预）的强化学习框架，采用约束马尔可夫决策过程，融入生理状态特征。

Result: 在T1DM控制任务中，血糖水平违规率从22.4%降至10.8%。

Conclusion: 为未来安全且时间敏感的医疗强化学习奠定了基础，但尚不适用于临床部署。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [62] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习和循环神经网络的混合建模方法，用于提高葡萄物候预测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统生物物理模型在精细化管理中缺乏精度，而深度学习方法受限于稀疏的物候数据集。

Method: 结合多任务学习和循环神经网络，参数化可微分的生物物理模型，实现跨品种的共享学习。

Result: 在真实和合成数据集上的实验表明，该方法显著优于传统生物物理模型和基线深度学习方法。

Conclusion: 混合方法在预测物候阶段和其他作物状态变量方面表现出更高的准确性和鲁棒性。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [63] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 论文揭示了基于距离的分类器中隐藏的神经网络结构，使可解释AI技术（如LRP）得以应用，并通过实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 基于距离的分类器（如k近邻和支持向量机）在机器学习中广泛应用，但其预测的可解释性对实际应用至关重要。本文旨在通过揭示其隐藏结构，提升其可解释性。

Method: 通过发现基于距离的分类器中隐藏的神经网络结构（线性检测单元与非线性池化层的组合），应用可解释AI技术（如LRP）。

Result: 定量评估表明，新解释方法优于多个基线方法，并通过两个实际用例展示了其整体实用性。

Conclusion: 本文揭示了基于距离分类器的隐藏结构，为提升其可解释性提供了新途径，并通过实验验证了其有效性。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [64] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 研究结合主动学习和迁移学习在跨域时间序列数据异常检测中的效果，发现聚类与主动学习存在交互作用，最佳性能出现在未聚类时。主动学习能提升模型性能，但改进速度较慢，且迁移学习性能随目标点增加先升后降。


<details>
  <summary>Details</summary>
Motivation: 探索主动学习与迁移学习结合在跨域时间序列异常检测中的有效性，以优化模型性能。

Method: 结合主动学习和迁移学习，分析聚类与主动学习的交互作用，评估不同数据集上的性能变化。

Result: 未聚类时性能最佳；主动学习提升性能但速度较慢；迁移学习性能先升后降。

Conclusion: 主动学习有效但性能提升呈线性平缓趋势，迁移学习性能存在上限。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [65] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 论文提出了一种结合流形和机器学习的方法，从高保真代理模拟中学习潜在空间中的离散演化算子，用于人群动力学建模。


<details>
  <summary>Details</summary>
Motivation: 解决微观和宏观建模尺度之间的桥梁问题，以支持系统数值分析、优化和控制。

Method: 四阶段方法：1) 从微观数据推导宏观密度场；2) 通过流形学习构建潜在空间映射；3) 使用机器学习（LSTM和MVAR）学习降阶模型；4) 重构高维空间中的密度分布。

Result: 数值结果表明方法具有高精度、鲁棒性和泛化能力，能够快速准确模拟人群动力学。

Conclusion: 通过潜在空间的嵌入、学习和重构，实现了对宏观PDE的有效求解，为人群动力学建模提供了高效工具。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [66] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 研究Transformer通过预测下一个标记学习算法的能力，发现模型在达到一定规模和训练集大小后能学习转移概率而非记忆训练模式。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer是否能在仅通过预测下一个标记的训练中学习上下文相关的算法。

Method: 使用随机转移矩阵的马尔可夫链训练Transformer预测下一个标记，测试时使用不同的矩阵。

Result: 模型在达到一定规模和训练集大小后能学习转移概率，且更复杂的状态编码能提升对不同结构马尔可夫链的预测鲁棒性。

Conclusion: Transformer能通过预测任务学习算法，且编码方式影响泛化能力。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [67] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT是一种新颖的后处理框架，通过最优传输策略性地调整风险评分分布，实现公平性与AUC性能的可调权衡。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗、金融、刑事司法）中，公平性评估通常基于风险评分而非二元结果，但严格公平性要求可能显著降低AUC性能。

Method: 提出FairPOT框架，利用最优传输选择性地调整弱势群体的评分分布（如top-lambda分位数），并通过调整lambda实现公平性与AUC性能的权衡。

Result: 在合成、公开和临床数据集上，FairPOT在全局和部分AUC场景中均优于现有技术，常以轻微AUC损失或性能提升实现公平性改进。

Conclusion: FairPOT的计算效率和实用性使其成为现实部署的有前景解决方案。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [68] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet是一种基于物理信息的深度算子网络（PI-DeepONet）的模型，用于将压力分布映射到气泡半径响应，通过自适应激活函数解决深度学习中的频谱偏差问题，并在多种气泡动力学场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在模拟气泡动力学时计算成本高，因此需要一种高效且物理可信的替代方法。

Method: 结合DeepONet的算子学习能力和物理信息神经网络（PINN）的物理保真性，引入Rowdy自适应激活函数以提升高频特征表示。

Result: 在单初始半径和多初始半径的气泡动力学模拟中表现优异，计算效率显著高于传统数值求解器。

Conclusion: BubbleONet是一种高效的气泡动力学模拟替代模型，具有广泛的应用潜力。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [69] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP是一个动态、用户可控的隐私保护框架，通过多模态对比学习实现敏感活动的少样本检测，并在识别后生成隐私合规数据。


<details>
  <summary>Details</summary>
Motivation: 现代传感系统中，用户隐私偏好因人而异且可能随时间变化，现有方法多为静态或需要大量隐私数据，缺乏适应性和用户控制。

Method: PrivCLIP利用多模态对比学习将IMU传感器数据与自然语言描述对齐，通过语言引导的活动净化器和IMU-GPT模块生成隐私合规数据。

Result: 在多个活动识别数据集上，PrivCLIP在隐私保护和数据效用方面显著优于基线方法。

Conclusion: PrivCLIP提供了一种灵活、用户可控的隐私保护方案，适用于动态变化的隐私需求。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [70] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA通过文本级和参数级方法解决多任务LoRA适配器的任务干扰问题，提升LLM适应性能。


<details>
  <summary>Details</summary>
Motivation: 多任务设置中，合并来自异构源的LoRA适配器常导致任务干扰，影响下游性能。

Method: 文本级：聚类训练样本并训练专用LoRA适配器；参数级：联合CP分解解耦任务特定与共享因子。

Result: 在Phi-3和Mistral-7B上分别提升1.4%和2.3%准确率。

Conclusion: TC-LoRA有效减少任务干扰，提升LLM多任务适应能力。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [71] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 论文提出了一种名为DCFL的新框架，通过解耦对比学习损失来解决联邦学习中数据异质性导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异质性导致性能下降，而现有对比学习方法在有限样本下不适用。

Method: 提出DCFL框架，将对比学习损失解耦为对齐性和均匀性两个目标，独立校准吸引力和排斥力。

Result: DCFL在正样本对齐和负样本均匀性上优于现有方法，并在多个基准测试中表现优异。

Conclusion: DCFL为联邦学习提供了一种有效的对比学习方法，显著提升了性能。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [72] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文对TensorFlow和PyTorch两大深度学习框架进行了全面比较，涵盖易用性、性能和部署灵活性等方面，并总结了各自的优劣势。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架的选择对研究和生产环境至关重要，本文旨在帮助开发者理解TensorFlow和PyTorch的核心差异，以便根据需求选择合适的工具。

Method: 通过对比编程范式、训练与推理性能、部署工具、生态系统及社区支持，并结合实际应用案例进行分析。

Result: PyTorch在研究领域更受欢迎，因其灵活性和易用性；TensorFlow则在企业环境中更具优势，因其成熟的生态系统和部署工具。

Conclusion: 选择框架需权衡研究灵活性与生产需求，未来发展方向包括统一执行模式、跨框架互操作性和优化编译器技术。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [73] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FeDaL的联邦学习方法，用于解决时间序列基础模型中的数据集异质性问题，通过消除局部和全局偏差提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据集的异质性导致显著的领域偏差，降低了时间序列基础模型的泛化能力，这一问题尚未得到充分研究。

Method: 采用联邦学习范式，提出FeDaL方法，通过域偏差消除（DBE）和全局偏差消除（GBE）机制，学习数据集无关的时间表示。

Result: 在8个任务的真实数据集上评估，优于54个基线方法，并分析了联邦学习的扩展行为。

Conclusion: FeDaL通过联邦学习和偏差消除机制，有效提升了时间序列模型的跨数据集泛化能力。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [74] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 论文提出了一种量子增强的混合量子-经典架构QTFT，扩展了经典TFT框架的能力，并在某些情况下优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 通过量子计算增强经典TFT框架，提升时间序列预测的性能，并适应当前NISQ设备的限制。

Method: 基于变分量子算法，设计量子-经典混合架构QTFT，无需严格限制量子比特数或电路深度。

Result: QTFT在某些测试案例中优于经典TFT，其余案例中性能相当，且能在NISQ设备上实现。

Conclusion: QTFT是一种有前景的量子增强时间序列预测方法，适用于当前量子硬件。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [75] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 论文研究了自动化短答案评分（ASAG）的改进方法，比较了基于大型语言模型（LLM）的微调与少样本提示的效果，发现微调在封闭模型中表现更好，而在开放权重模型中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过微调和少样本提示提升自动化短答案评分的性能，解决传统微调方法对大规模计算资源的依赖问题。

Method: 评估了两种微调方法（OpenAI的微调服务和QLORA）与少样本提示的结合效果，测试了Llama和OpenAI模型的性能。

Result: 微调在少量数据下对Llama模型效果有限，但在OpenAI封闭模型中优于少样本基线；Llama 3.1 8B-Instruct模型通过合成数据显著提升性能。

Conclusion: 微调的效果受模型类型和领域影响，合成数据可显著提升开放权重模型的性能。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [76] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT是一种新型的联邦学习后门攻击方法，通过潜在驱动的条件自编码器生成多样化的目标特定触发器，提高了攻击的灵活性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有的后门攻击方法通常局限于固定模式或单一目标触发器，不够灵活且容易被检测。FLAT旨在解决这一问题，提供更灵活、隐蔽的攻击手段。

Method: FLAT利用潜在驱动的条件自编码器生成多样化的触发器，攻击者无需重新训练即可选择任意目标，并能规避传统检测机制。

Result: 实验表明，FLAT攻击成功率高，且能抵抗先进的联邦学习防御机制。

Conclusion: FLAT展示了潜在驱动的多目标后门攻击的威胁，强调了联邦学习环境中需要新的防御策略。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [77] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种对抗性公平多视图聚类（AFMVC）框架，通过对抗训练去除学习特征中的敏感属性信息，确保聚类结果不受其影响。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法主要关注性能，而忽略了公平性，尤其是在人类中心应用中。

Method: 采用对抗训练去除敏感属性信息，并通过KL散度对齐视图特定聚类分配与公平不变的共识分布。

Result: 在公平约束数据集上，AFMVC表现出优越的公平性和竞争性的聚类性能。

Conclusion: AFMVC框架在保证聚类一致性的同时显著提升了公平性，为多视图聚类提供了理论和实践支持。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [78] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 研究首次探讨了视觉语言模型（VLMs）在模型反转攻击中的隐私风险，提出了一系列针对VLMs的令牌和序列反转方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注传统单模态DNN的隐私风险，而VLMs的隐私漏洞尚未充分探索。

Method: 提出了四种模型反转策略：TMI、TMI-C、SMI和SMI-AW，特别针对VLMs的令牌生成特性。

Result: 实验表明，序列方法（如SMI-AW）在攻击准确性和视觉相似性上优于令牌方法，人类评估攻击准确率达75.31%。

Conclusion: VLMs存在显著的隐私风险，随着其在医疗和金融等领域的广泛应用，这一问题需引起重视。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [79] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种一致性感知的策略优化框架，通过全局奖励和熵基软混合机制解决强化学习中梯度消失问题，提升训练效率和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多响应收敛时梯度消失，限制了训练效率和性能。

Method: 提出一致性感知策略优化框架，结合全局奖励和熵基软混合机制。

Result: 在多个数学推理基准测试中表现显著提升。

Conclusion: 该方法在奖励设计和优化策略上具有创新性，且具有鲁棒性和通用性。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [80] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 提出了一种半监督深度域适应框架，用于解决太阳能预测中的域偏移问题，仅需目标域20%的标注数据即可显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 地理和天气特征的差异导致太阳能预测模型在不同地点表现不佳，且标注数据不足和存储问题加剧了挑战。

Method: 采用半监督深度域适应框架，基于源位置数据训练深度卷积神经网络，并通过无源数据的教师-学生模型配置进行目标域适应。

Result: 在加州、佛罗里达和纽约作为目标域时，预测准确性分别提升了11.36%、6.65%和4.92%。

Conclusion: 该方法有效解决了域偏移问题，仅需少量目标域标注数据即可显著提升预测性能。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [81] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练的编码器-解码器方法，用于从质谱数据中生成分子结构，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决从质谱数据中生成分子结构的挑战，提出一种高效的两阶段方法。

Method: 使用MIST作为编码器将质谱转换为分子指纹，MolForge作为解码器将指纹解码为分子结构，并利用预训练提升性能。

Result: 该方法在分子结构恢复上表现优异，top-1正确率为28%，top-10为36%，比现有方法提升十倍。

Conclusion: 该方法是未来质谱分子结构解析研究的强基线。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [82] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出了一种名为SAMT的新方法，通过交替更新网络层参数和自适应步长策略，提高了深度神经网络训练的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中的非凸优化问题，减少计算成本并提高收敛稳定性。

Method: 采用块交替最小化策略，将每层权重视为一个块，结合基于元学习的自适应步长策略。

Result: 实验表明，SAMT在多个基准测试中优于现有方法，具有更好的泛化性能和更少的参数更新。

Conclusion: SAMT是一种高效且稳定的神经网络优化方法，具有理论和实验支持。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [83] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 论文提出Causal Reward Adjustment (CRA)方法，通过因果推断解决外部推理系统中的奖励欺骗问题，提高数学问题求解的准确性。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统结合语言模型和过程奖励模型（PRMs）时，容易出现奖励欺骗现象，即高评分但逻辑错误的推理路径被误判为正确。

Method: 提出CRA方法，通过稀疏自编码器提取PRM内部激活的可解释特征，并利用后门调整消除混杂语义特征的影响。

Result: 在数学问题求解数据集上的实验表明，CRA有效减少了奖励欺骗，提高了最终准确性。

Conclusion: CRA无需修改策略模型或重新训练PRM，即可显著改善推理系统的性能。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [84] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 论文提出了一种基于对称散度的离线强化学习框架S$f$-AC，解决了对称散度在行为正则化策略优化中的数值和分析问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用非对称散度（如KL散度），但对称散度在正则化和损失函数中存在数值和分析挑战。

Method: 通过泰勒展开$f$-散度，证明有限级数可得到解析策略，并分解对称散度为不对称和条件对称项以缓解数值问题。

Result: 实验表明，S$f$-AC在分布近似和MuJoCo任务中表现优异。

Conclusion: S$f$-AC是首个实用的基于对称散度的BRPO算法，性能具有竞争力。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [85] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS是一种基于数据的时间序列代理，通过优化数据质量而非模型架构，在时间序列预测中实现了6%的平均误差降低。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML方法多关注特征工程和模型架构搜索，但轻量级模型在时间序列预测中表现优异，因此探索数据质量优化作为AutoML的新方向。

Method: 提出DCATS代理，利用时间序列的元数据清洗数据并优化预测性能，在交通流量数据集上测试了四种模型。

Result: DCATS在所有测试模型和时间范围内平均降低了6%的预测误差。

Conclusion: 数据为中心的方法在时间序列AutoML中具有潜力。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [86] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的自动化多普勒角度估计方法，用于减少超声血流速度测量中的误差。


<details>
  <summary>Details</summary>
Motivation: 多普勒角度估计错误是血流速度测量误差的主要原因，需要自动化解决方案以提高准确性。

Method: 使用2100张人类颈动脉超声图像（包括数据增强），结合五个预训练模型提取特征，并通过自定义浅层网络进行角度估计。

Result: 最佳模型的平均绝对误差（MAE）低于临床可接受阈值，避免了正常流速被误判为狭窄。

Conclusion: 深度学习技术在自动化多普勒角度估计中具有潜力，可集成到商用超声扫描仪软件中。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [87] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time是一种新型的三模态框架，通过时间、频谱和提示分支解决多变量时间序列预测中的问题，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉时间序列数据的复杂关系和适应性方面存在局限，如忽略变量间交互或使用静态融合策略。

Method: 提出T3Time框架，包含时间、频谱和提示分支，通过门控机制动态调整特征优先级，并自适应聚合跨模态对齐头。

Result: 在基准数据集上，T3Time平均减少MSE 3.28%和MAE 2.29%，在少样本学习中也表现优异。

Conclusion: T3Time通过动态特征融合和自适应机制，显著提升了多变量时间序列预测的性能和泛化能力。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [88] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个基于Python的工具，通过集成局部和全局敏感性分析，帮助用户可视化和理解机器学习模型的行为。


<details>
  <summary>Details</summary>
Motivation: 为AI研究人员和领域专家提供无需编程的交互式图形界面，支持配置、训练、评估和解释模型。

Method: 自动化模型训练和选择，提供基于方差的全局特征归因分析，以及通过LIME和SHAP进行实例解释。

Result: 在泰坦尼克数据集上的分类任务中展示了系统功能，敏感性信息可指导特征选择和数据优化。

Conclusion: SAInT通过直观的界面和自动化分析，有效支持人机协作的机器学习工作流程。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [89] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 论文提出框架Mockingbird，通过让大语言模型（LLMs）扮演功能并反思错误来适应通用机器学习任务，结果显示其表现尚可，但仅靠自我反思无法超越领域专家反馈的效果。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）在通用机器学习任务中的潜力，而不仅限于聊天机器人应用。

Method: 提出Mockingbird框架，指导LLMs扮演功能并反思错误以自我改进。

Result: LLM驱动的机器学习方法（如Mockingbird）在通用任务上表现尚可，但自我反思效果不及领域专家反馈。

Conclusion: LLMs在通用机器学习任务中有潜力，但需结合领域专家知识以进一步提升性能。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [90] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: VL-DAC是一种轻量级、无需超参数调整的RL算法，通过解耦动作和价值的更新，提升VLMs在多任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）缺乏将视觉观察转化为语言条件动作序列的能力，且传统RL方法泛化性差、依赖密集奖励或超参数调整。

Method: 提出VL-DAC算法，解耦动作令牌的PPO更新与环境步级别的价值学习，避免不稳定权重项，实现快速收敛。

Result: 在多个模拟器中训练的VLMs，泛化性能显著提升（如BALROG +50%，VSI-Bench +5%，VisualWebBench +2%），且不影响通用图像理解。

Conclusion: VL-DAC首次证明简单RL算法可在廉价合成环境中训练VLMs，并在真实任务中取得显著性能提升。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [91] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出了一种基于权重显著性的两阶段高效机器学习遗忘方法（WSS-CL），用于图像分类，显著缩小了与“精确”遗忘的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习遗忘方法在精确性、稳定性和跨领域适用性方面存在挑战，需要一种更高效的方法。

Method: 采用两阶段方法：遗忘阶段最大化输出对数与伪标签的KL散度；对抗微调阶段通过自监督对比学习，最大化遗忘与保留样本在特征空间的距离。

Result: 实验表明，该方法在遗忘效果和性能损失方面优于现有技术，适用于监督和自监督场景。

Conclusion: WSS-CL方法在高效遗忘和性能保持方面表现出色，具有广泛应用潜力。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [92] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种通过将语料库中的令牌分为正负两类来优化监督微调（SFT）的方法，以提升模型性能并减少对数据质量的依赖。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）对预训练大语言模型（LLMs）至关重要，但其效果高度依赖数据质量和数量，可能导致性能提升有限甚至下降。

Method: 将语料库中的令牌分为正负两类：正令牌按常规训练，负令牌（缺乏语义或误导性）则明确遗忘。

Result: 实验表明，遗忘机制不仅提升了模型整体性能，还促进了更多样化的模型响应。

Conclusion: 通过令牌分类和遗忘机制，模型能更精确地学习有用信息，同时减少无关信息的干扰。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [93] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS是一种新的隐私推理范式，通过分布式特征共享解决隐私与效率的权衡问题，同时提出两种扩展方法进一步增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 云机器学习服务（MLaaS）在处理敏感数据时存在隐私问题，现有隐私推理方法在隐私和效率之间存在矛盾。

Method: PrivDFS将输入特征分割为多个平衡共享，分发到非共谋服务器进行独立推理，客户端安全聚合结果。扩展方法包括PrivDFS-AT（对抗训练）和PrivDFS-KD（用户特定密钥）。

Result: 在CIFAR-10和CelebA上，PrivDFS在保持隐私的同时减少客户端计算100倍，且无精度损失。扩展方法对多种攻击保持鲁棒性。

Conclusion: PrivDFS在隐私保护和效率之间取得了平衡，扩展方法进一步提升了隐私安全性。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [94] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出了一种名为MMSFM的新方法，用于处理高维系统在非均匀时间点的演化建模问题，避免了传统降维方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法在处理高维系统演化时可能过度简化动态行为，尤其是在非平衡系统中，无法捕捉关键的瞬态行为。

Method: MMSFM是一种基于多边际随机流匹配的方法，通过度量值样条增强对不规则时间点的鲁棒性，并使用分数匹配防止高维空间中的过拟合。

Result: 在合成和基准数据集（如基因表达数据和图像进展任务）上验证了方法的有效性。

Conclusion: MMSFM提供了一种无需降维的高维数据建模方法，适用于非均匀时间点的观测数据。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [95] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种针对多实例学习（MIL）的持续学习方法，通过选择代表性样本避免灾难性遗忘，并在白血病检测中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床数据动态变化，需要持续更新模型以保持性能，但现有方法在多实例学习中效果不佳。

Method: 基于实例注意力分数和距离选择代表性样本，存储为示例集，保留数据多样性。

Result: 在白血病实验室数据上，该方法显著优于现有持续学习方法。

Conclusion: 该方法首次为MIL提供持续学习解决方案，适应数据分布变化，如疾病发生或基因变异。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [96] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种新型的INT6量化框架，通过算法和系统优化，在保持高精度的同时提升推理效率和内存节省。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高内存和计算成本限制了实际部署，现有INT4/INT8量化方法在精度和效率上存在不足。

Method: FlexQ采用统一的6位权重量化，并通过层敏感分析保留8位激活，开发了支持W6A6和W6A6表示的GPU内核。

Result: 在LLaMA模型上，FlexQ保持了接近FP16的精度，推理速度提升1.33倍，内存节省1.21倍。

Conclusion: FlexQ在INT6量化中实现了精度与效率的平衡，为LLMs的实际部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [97] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 本文系统回顾了2020年至2024年初关于多模态模型可解释性的研究，发现注意力技术是主要方法，但评估方法缺乏系统性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习和可解释人工智能（XAI）的发展，研究多模态模型的可解释性成为重要课题。

Method: 通过系统文献综述，分析模型架构、模态、解释算法和评估方法。

Result: 研究集中在视觉-语言和纯语言模型，评估方法不一致且缺乏对模态交互的全面捕捉。

Conclusion: 建议未来研究采用更严谨、透明和标准化的评估方法，以提升多模态XAI的可解释性和责任性。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [98] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了基于矩阵向量乘法的随机算法，用于估计矩阵的二到无穷和一到二范数，并应用于深度神经网络训练和推荐系统的对抗攻击缓解。


<details>
  <summary>Details</summary>
Motivation: 解决在矩阵自由设置中高效估计矩阵范数的问题，并探索其在深度学习和推荐系统中的实际应用。

Method: 基于Hutchinson对角估计器及其Hutch++变体的改进算法。

Result: 提供了算法的复杂度界限，并在图像分类和推荐系统中验证了实用性。

Conclusion: 新算法在矩阵范数估计和实际应用中表现出高效性和实用性。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [99] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: CMCFAE是一种新型生成模型，将云模型与Wasserstein自编码器结合，通过云模型特征函数正则化潜在空间，提升复杂数据分布的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖标准高斯先验和传统散度度量，导致潜在空间同质化，CMCFAE通过云模型先验提供更灵活和真实的表示。

Method: 提出云模型特征函数，并在WAE框架中设计相应正则化器，优化潜在空间结构。

Result: 在MNIST等数据集上，CMCFAE在重建质量、潜在空间结构和样本多样性上优于现有模型。

Conclusion: CMCFAE为自编码器生成模型提供了新视角，结合云模型理论和MMD正则化，具有潜力。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [100] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出了一种通过强化学习训练AI来动态攻击其他AI的新方法，解决了现有红队测试方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队测试方法依赖脆弱的提示模板或单轮攻击，无法模拟复杂的真实对抗对话。

Method: 将红队测试建模为马尔可夫决策过程（MDP），采用分层强化学习框架，通过细粒度的token级奖励训练生成代理。

Result: 该方法能发现现有基线遗漏的细微漏洞，成为新的SOTA。

Conclusion: 将LLM红队测试重新定义为动态、基于轨迹的过程，对AI的稳健部署至关重要。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [101] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 论文探讨了在小规模任务切换场景中，基于注意力的架构与传统方法（如多层感知机或循环网络）的性能对比，发现标准Transformer无法解决基本任务切换问题，而改进后的模型（如cisformer和extensive attention）表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解注意力机制在小规模应用中的表现，尤其是在任务切换框架下，探索其与传统方法的性能差异及改进潜力。

Method: 方法包括在任务切换框架下测试标准Transformer、LSTM、MLP的性能，并引入改进模型（cisformer和extensive attention）进行对比。

Result: 结果显示标准Transformer、LSTM和MLP表现相似但有限，而结合extensive attention的改进模型能达到约95%的准确率。

Conclusion: 结论表明，通过比较不同任务切换设置下的注意力机制，可以更好地理解并改进其性能。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [102] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出了一种基于缓存的并行推测解码框架CARD，通过解耦起草和验证过程，显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法因必须遵循'起草-验证'范式，导致推理效率低下且限制起草模型规模。

Method: CARD采用'查询-纠正'范式，起草模型生成候选令牌填充共享缓存，目标模型并行纠正生成方向。

Result: CARD实现了高达4.83倍的加速，且无需微调起草或目标模型。

Conclusion: CARD通过并行化起草和验证，显著提升了LLM推理效率。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [103] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种基于Transformer的神经算子方法，通过同时学习全局和局部特征，解决了多尺度问题中的物理一致性和数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了局部物理细节与全局特征的协同学习，而这对于解决多尺度问题、保持物理一致性和数值稳定性至关重要。

Method: GFocal通过Nyström注意力机制和切片机制分别捕捉全局和局部特征，并通过卷积门控块动态融合多尺度信息。

Result: GFocal在六项基准测试中的五项中平均相对增益达15.2%，并在工业规模模拟中表现出色。

Conclusion: GFocal通过全局与局部特征的动态融合，显著提升了多尺度物理问题的建模和预测能力。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [104] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP提出了一种基于解析解（闭式解）的个性化联邦学习方案，避免了梯度更新的依赖，解决了非独立同分布（non-IID）数据带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法因依赖梯度更新而对非IID数据敏感，导致性能下降。FedHiP旨在通过解析解实现异构不变性。

Method: 利用自监督预训练的基础模型提取特征，开发解析分类器，分三阶段训练：局部解析训练、全局解析聚合和局部解析个性化。

Result: 在基准数据集上，FedHiP准确率比现有方法高5.79%-20.97%。

Conclusion: FedHiP通过解析解实现了异构不变性，显著提升了非IID数据下的性能。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [105] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 研究使用因果机器学习模型分析英国住房墙保温对天然气消耗的影响，发现节能效果因能源负担不同而异。


<details>
  <summary>Details</summary>
Motivation: 探讨能源效率干预对减少家庭能源需求的异质性影响，特别是对能源负担不同群体的分布效应。

Method: 使用全国代表性的英国住房数据训练因果机器学习模型，估计墙保温的平均和条件处理效应。

Result: 墙保温平均减少19%天然气需求，但高能源负担家庭几乎无减少，因他们将节省用于改善热舒适。

Conclusion: 需更广泛评估框架，兼顾气候影响和能源政策的公平性。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [106] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 提出一种基于cGAN的多模态情感检测方法，显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情感检测方法效果有限，需探索多模态融合以提升准确性。

Method: 使用cGAN架构，结合文本、音频和面部表情数据，生成合成情感数据并优化分类。

Result: 实验结果显示，相比基线模型，情感识别性能显著提升。

Conclusion: cGAN在多模态情感检测中具有潜力，可增强人机交互系统的情感理解能力。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [107] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 论文提出了一种基于层次结构的评分指标，用于更细粒度地评估机器学习模型的分类和物体检测性能，通过评分树编码类别关系，区分不同类型的错误。


<details>
  <summary>Details</summary>
Motivation: 传统的分类评估采用二元的通过/失败评分，将所有错误视为等同，忽略了类别间的层次关系和错误的不同影响。

Method: 开发了基于评分树的层次评分指标，通过编码类别关系生成反映预测与真实标签距离的指标，并展示了三种权重策略的抽象用例。

Result: 结果表明，这些指标能更细粒度地捕捉错误，且评分树支持调整，能根据错误的类型或影响对模型进行排序。

Conclusion: 该方法提供了一种更细致的ML性能评估方式，不仅关注错误数量，还关注错误的类型和影响。Python实现将开源。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [108] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 论文提出Causal Reflection框架，通过显式建模因果关系，改进LLM和RL代理的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: LLM和传统RL代理在因果推理上表现不足，依赖虚假关联和脆弱模式，缺乏对因果关系的理解。

Method: 引入Causal Reflection框架，动态建模状态、动作、时间和扰动的因果关系，并设计Reflect机制以修正内部模型。

Result: 框架为代理提供了适应、自我修正和表达因果理解的能力。

Conclusion: Causal Reflection为因果推理代理奠定了理论基础，适用于动态环境。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [109] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM是一种基于卷积的特征提取器，通过多分辨率、单通道设计减少模型复杂性和参数数量，在多元时间序列分类中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer和CNN模型在多元时间序列分类中计算量大、频率多样性有限和参数需求高的问题。

Method: PRISM采用对称有限脉冲响应（FIR）滤波器，在多时间尺度上独立处理每个通道，避免通道间卷积，降低模型复杂度。

Result: PRISM在多个基准测试中表现优于或媲美主流CNN和Transformer模型，同时参数和计算量大幅减少。

Conclusion: PRISM结合经典信号处理和深度学习，提供了一种高效、准确的多元时间序列分类解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [110] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出了一种新的联邦交通预测方法Fed-CI，通过独立通道建模范式减少通信开销，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 交通数据分散且隐私受限，现有联邦学习方法通信开销大，训练速度慢。

Method: 采用通道独立范式（CIP），无需客户端间通信，基于本地数据进行高效预测。

Result: Fed-CI在多个数据集上表现优异，RMSE、MAE和MAPE分别提升8%、14%和16%，通信成本显著降低。

Conclusion: Fed-CI在减少通信开销的同时，实现了高性能的联邦交通预测，符合隐私法规。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [111] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 该研究通过分析5000多个身份盗窃和欺诈案例，构建了一个基于图的身份生态系统模型，用于预测个人隐私风险。


<details>
  <summary>Details</summary>
Motivation: 缺乏对隐私风险的基本理解使得个人和组织难以保护个人信息。

Method: 构建身份生态系统图，利用图论和图神经网络开发隐私风险预测框架。

Result: 研究有效回答了核心问题：某个身份属性的披露是否可能导致另一个属性的披露。

Conclusion: 该框架为隐私风险评估提供了有效工具。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [112] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: GraphProp通过两阶段训练提升图基础模型（GFM）的跨域泛化能力，强调结构信息的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统GFM主要关注节点特征的跨域泛化，但缺乏结构信息的跨域一致性。GraphProp旨在通过结构信息提升泛化能力。

Method: 1. 训练结构GFM，预测图不变量；2. 利用结构GFM的表示作为位置编码，训练综合GFM。

Result: GraphProp在监督学习和少样本学习中显著优于竞争对手，尤其适用于无节点属性的图。

Conclusion: GraphProp通过结构信息的泛化能力，显著提升了GFM在跨域任务中的表现。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [113] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 论文研究了基于物理信息的神经网络在铝点焊中用于非破坏性质量评估的方法，提出了两种新的训练策略以减少优化冲突。


<details>
  <summary>Details</summary>
Motivation: 点焊是汽车工业中主要的连接工艺，但焊核直径的测量需要破坏性测试，限制了高效质量控制的可能性。

Method: 采用物理信息神经网络，结合实验数据重建内部过程状态；引入渐进式损失函数和条件更新材料参数策略。

Result: 二维网络成功预测动态位移和焊核生长，支持从钢到铝的焊接阶段转移，展示了工业应用的潜力。

Conclusion: 该方法为快速、基于模型的质量控制提供了有效解决方案。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [114] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出了一种广义的概率分布映射学习框架，扩展了流和扩散模型的时间动态。


<details>
  <summary>Details</summary>
Motivation: 通过将标量时间变量推广为向量、矩阵或线性算子，实现跨多维空间的概率分布桥接，构建多功能生成模型。

Method: 采用基于算子的插值方法，统一现有生成模型的理论视角并扩展其能力。

Result: 数值实验展示了该方法在条件生成、修复、微调、后验采样和多尺度建模中的零样本效果。

Conclusion: 该方法有望成为任务无关的通用替代方案，适用于多种生成任务。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [115] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的分层架构，用于终身网络入侵检测系统（NIDS），通过静态和动态SNN结合生物启发机制实现高效学习和低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受大脑分层处理和能量效率的启发，设计一种能够持续学习新威胁并保持现有知识的NIDS。

Method: 采用静态SNN初步识别入侵，动态SNN分类攻击类型，结合GWR结构可塑性和Ad-STDP学习规则。

Result: 在UNSW-NB15基准测试中达到85.3%的准确率，展示了强大的适应性和低遗忘性。

Conclusion: 该架构在持续学习环境中表现优异，适合低功耗神经形态硬件部署。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [116] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 提出了一种基于因果关系的框架CaPulse，用于时间序列异常检测，解决了现有方法在捕捉异常生成机制和数据相关挑战（如标签稀缺、数据不平衡和多周期性）上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉时间序列异常生成的机制，且面临标签稀缺、数据不平衡和多周期性等数据挑战。

Method: 利用因果工具构建结构因果模型，提出Periodical Normalizing Flows和周期感知学习器，形成基于密度的异常检测方法。

Result: 在七个真实数据集上，CaPulse表现优于现有方法，AUROC提升3%至17%，且具有更好的可解释性。

Conclusion: CaPulse通过因果分析和周期感知方法，显著提升了时间序列异常检测的性能和可解释性。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [117] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 本文探讨了预训练如何解决图神经网络在链接预测任务中的关键挑战，提出了模块化迁移和混合专家框架，显著提升了性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 链接预测任务中，图神经网络面临监督稀疏、初始化敏感和泛化能力差等问题，预训练被视为潜在解决方案。

Method: 提出了模块化迁移研究、混合专家框架和参数高效调优策略，以整合节点和边信息并适应多样数据集。

Result: 在16个数据集上验证了方法的有效性，实现了低资源链接预测的最优性能，计算开销降低超10,000倍。

Conclusion: 预训练和模块化设计显著提升了链接预测的泛化能力和效率，为图机器学习提供了新思路。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [118] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一个高性能的生物声学预训练模型，通过自蒸馏和原型学习分类器扩展了多类群数据集，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩展Perch模型以支持多类群生物声学分类，并探索细粒度物种分类作为预训练任务的鲁棒性。

Method: 使用自蒸馏和原型学习分类器，结合新的源预测训练准则进行训练。

Result: 在BirdSet和BEANS基准测试中达到最优性能，并在海洋迁移学习任务中超越专业模型。

Conclusion: 细粒度物种分类是生物声学预训练的有效任务，Perch 2.0在多类群任务中表现出色。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [119] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种高效算法，用于在高斯分布下学习单指数模型，并在对抗性标签噪声存在时实现恒定因子近似。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在未知激活函数情况下无法实现恒定因子近似或仅适用于较小激活函数家族的问题。

Method: 开发了一种优化框架，通过直接利用问题结构、高斯空间性质和单调函数规律性，识别有用的向量场来指导算法更新。

Result: 算法适用于所有具有有界矩的单调激活函数，包括单调Lipschitz函数和不连续函数（如半空间）。

Conclusion: 该方法的创新性在于突破了传统梯度方法的限制，为学习单指数模型提供了更高效的解决方案。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [120] [Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding](https://arxiv.org/abs/2508.03718)
*Mike Gartner*

Main category: cs.CY

TL;DR: 论文探讨了美国医疗保险的复杂性，提出利用自然语言处理技术改善理解和司法公正，并发布了一个相关语料库和预测模型。


<details>
  <summary>Details</summary>
Motivation: 美国医疗保险系统复杂，理解和司法公正的不足对弱势群体影响严重，需要技术手段改善。

Method: 收集并发布了与医疗保险相关的法律和医学文本语料库，设计了保险申诉结果预测任务，并发布了标注数据和训练模型。

Result: 提出了一个支持监管和患者自助应用的预测任务，并提供了相关数据和模型。

Conclusion: 通过语料库和预测模型，论文为改善医疗保险理解和司法公正提供了技术支持。

Abstract: U.S. health insurance is complex, and inadequate understanding and limited
access to justice have dire implications for the most vulnerable. Advances in
natural language processing present an opportunity to support efficient,
case-specific understanding, and to improve access to justice and healthcare.
Yet existing corpora lack context necessary for assessing even simple cases. We
collect and release a corpus of reputable legal and medical text related to
U.S. health insurance. We also introduce an outcome prediction task for health
insurance appeals designed to support regulatory and patient self-help
applications, and release a labeled benchmark for our task, and models trained
on it.

</details>


### [121] [Development of management systems using artificial intelligence systems and machine learning methods for boards of directors (preprint, unofficial translation)](https://arxiv.org/abs/2508.03769)
*Anna Romanova*

Main category: cs.CY

TL;DR: 论文探讨了AI从决策支持工具转变为自主决策者的范式转变，提出了一种基于计算法律的参考模型，以确保AI在企业管理中的合法和伦理决策。


<details>
  <summary>Details</summary>
Motivation: AI技术的发展远超法律和伦理指南的制定，导致AI在企业管理中的自主决策缺乏规范。

Method: 提出了一种参考模型，结合计算法律、专用操作环境和游戏理论，确保AI决策的合法性和伦理性。

Result: 模型强调可解释AI（XAI）的重要性，以增强透明度和问责制。

Conclusion: 需要为AI系统建立明确的法律和伦理框架，以确保其安全和有效运作。

Abstract: The study addresses the paradigm shift in corporate management, where AI is
moving from a decision support tool to an autonomous decision-maker, with some
AI systems already appointed to leadership roles in companies. A central
problem identified is that the development of AI technologies is far outpacing
the creation of adequate legal and ethical guidelines.
  The research proposes a "reference model" for the development and
implementation of autonomous AI systems in corporate management. This model is
based on a synthesis of several key components to ensure legitimate and ethical
decision-making. The model introduces the concept of "computational law" or
"algorithmic law". This involves creating a separate legal framework for AI
systems, with rules and regulations translated into a machine-readable,
algorithmic format to avoid the ambiguity of natural language. The paper
emphasises the need for a "dedicated operational context" for autonomous AI
systems, analogous to the "operational design domain" for autonomous vehicles.
This means creating a specific, clearly defined environment and set of rules
within which the AI can operate safely and effectively. The model advocates for
training AI systems on controlled, synthetically generated data to ensure
fairness and ethical considerations are embedded from the start. Game theory is
also proposed as a method for calculating the optimal strategy for the AI to
achieve its goals within these ethical and legal constraints. The provided
analysis highlights the importance of explainable AI (XAI) to ensure the
transparency and accountability of decisions made by autonomous systems. This
is crucial for building trust and for complying with the "right to
explanation".

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [122] [Data-Driven Discovery of Mobility Periodicity for Understanding Urban Transportation Systems](https://arxiv.org/abs/2508.03747)
*Xinyu Chen,Qi Wang,Yunhan Zheng,Nina Cao,HanQin Cai,Jinhua Zhao*

Main category: cs.SI

TL;DR: 该研究提出了一种稀疏自回归方法，用于量化人类移动数据的周期性，揭示了COVID-19对移动规律的影响及恢复趋势。


<details>
  <summary>Details</summary>
Motivation: 揭示人类移动的周期性规律对理解城市动态和决策制定至关重要。

Method: 采用稀疏自回归方法，从多维时间序列数据中识别显著的正自相关性，量化周期性模式。

Result: 分析显示，纽约和芝加哥在2020年周期性显著下降，纽约的恢复速度更快。

Conclusion: 可解释的机器学习方法为理解城市系统提供了新工具，揭示了移动数据的潜在规律。

Abstract: Uncovering the temporal regularity of human mobility is crucial for
discovering urban dynamics and has implications for various decision-making
processes and urban system applications. This study formulates the periodicity
quantification problem in complex and multidimensional human mobility data as a
sparse identification of dominant positive auto-correlations in time series
autoregression, allowing one to discover and quantify significant periodic
patterns such as weekly periodicity from a data-driven and interpretable
machine learning perspective. We apply our framework to real-world human
mobility data, including metro passenger flow in Hangzhou, China and
ridesharing trips in New York City (NYC) and Chicago, USA, revealing the
interpretable weekly periodicity across different spatial locations over past
several years. In particular, our analysis of ridesharing data from 2019 to
2024 demonstrates the disruptive impact of the COVID-19 pandemic on mobility
regularity and the subsequent recovery trends, highlighting differences in the
recovery pattern percentages and speeds between NYC and Chicago. We explore
that both NYC and Chicago experienced a remarkable reduction of weekly
periodicity in 2020, and the recovery of mobility regularity in NYC is faster
than Chicago. The interpretability of sparse autoregression provides insights
into the underlying temporal patterns of human mobility, offering a valuable
tool for understanding urban systems. Our findings highlight the potential of
interpretable machine learning to unlock crucial insights from real-world
mobility data.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [123] [Convolutional autoencoders for the reconstruction of three-dimensional interfacial multiphase flows](https://arxiv.org/abs/2508.04084)
*Murray Cutforth,Shahab Mirjalili*

Main category: cs.CE

TL;DR: 本文研究了自编码器在三维多相流降维建模中的应用，探讨了不同界面表示方法的优缺点，并提出了最佳实践方法。


<details>
  <summary>Details</summary>
Motivation: 多相流建模的复杂性需要高效的降维方法，自编码器在此领域的应用潜力尚未充分探索。

Method: 使用合成数据和高分辨率模拟数据，结合卷积架构，比较了不同界面表示方法（扩散、锐利、水平集）。

Result: 研究明确了自编码器在多相流降维中的最佳实践，为后续低维空间训练提供了基础。

Conclusion: 该研究为多相流建模提供了重要指导，并扩展了自编码器在复杂流体动力学中的应用。

Abstract: In this work, we perform a comprehensive investigation of autoencoders for
reduced-order modeling of three-dimensional multiphase flows. Focusing on the
accuracy of reconstructing multiphase flow volume/mass fractions with a
standard convolutional architecture, we examine the advantages and
disadvantages of different interface representation choices (diffuse, sharp,
level set). We use a combination of synthetic data with non-trivial interface
topologies and high-resolution simulation data of multiphase homogeneous
isotropic turbulence for training and validation. This study clarifies the best
practices for reducing the dimensionality of multiphase flows via autoencoders.
Consequently, this paves the path for uncoupling the training of autoencoders
for accurate reconstruction and the training of temporal or input/output models
such as neural operators (e.g., FNOs, DeepONets) and neural ODEs on the
lower-dimensional latent space given by the autoencoders. As such, the
implications of this study are significant and of interest to the multiphase
flow community and beyond.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [124] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: RLGS是一个基于强化学习的框架，用于自动调整3D高斯溅射（3DGS）中的超参数，提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS中的超参数调优需要大量人工干预且结果不稳定，RLGS旨在通过自动化解决这一问题。

Method: RLGS通过轻量级策略模块动态调整关键超参数（如学习率和密度阈值），无需修改现有3DGS架构。

Result: RLGS在多个3DGS变体（如Taming-3DGS和3DGS-MCMC）上表现优异，例如在TNT数据集上将Taming-3DGS的PSNR提升了0.7dB。

Conclusion: RLGS为3DGS超参数调优提供了一种通用且高效的自动化解决方案。

Abstract: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [125] [MD-LLM-1: A Large Language Model for Molecular Dynamics](https://arxiv.org/abs/2508.03709)
*Mhd Hussein Murtada,Z. Faidon Brotzakis,Michele Vendruscolo*

Main category: q-bio.BM

TL;DR: 论文提出了一种基于大语言模型（MD-LLM）的分子动力学框架，用于学习蛋白质动态并预测未在训练中出现的构象状态。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学（MD）在生物大分子系统的时空尺度上计算成本高昂，深度学习为解决这一问题提供了新机会。

Method: 通过微调Mistral 7B模型，开发了MD-LLM-1框架，并应用于T4溶菌酶和Mad2蛋白系统。

Result: 实验表明，MD-LLM-1能够基于一种构象状态预测其他构象状态，初步展示了其在蛋白质构象空间探索中的潜力。

Conclusion: MD-LLM-1能够学习蛋白质构象景观的探索原则，但尚未明确建模其热力学和动力学特性。

Abstract: Molecular dynamics (MD) is a powerful approach for modelling molecular
systems, but it remains computationally intensive on spatial and time scales of
many macromolecular systems of biological interest. To explore the
opportunities offered by deep learning to address this problem, we introduce a
Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how
LLMs can be leveraged to learn protein dynamics and discover states not seen in
training. By applying MD-LLM-1, the first implementation of this approach,
obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein
systems, we show that training on one conformational state enables the
prediction of other conformational states. These results indicate that MD-LLM-1
can learn the principles for the exploration of the conformational landscapes
of proteins, although it is not yet modeling explicitly their thermodynamics
and kinetics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [126] [Accept-Reject Lasso](https://arxiv.org/abs/2508.04646)
*Yanxin Liu,Yunqi Zhang*

Main category: stat.ME

TL;DR: ARL方法通过聚类和子集分析解决Lasso在高相关性特征下的不稳定问题，区分真假相关性，优化特征选择。


<details>
  <summary>Details</summary>
Motivation: 解决Lasso在高相关性特征下表现不稳定的问题，避免错误地遗漏或包含特征。

Method: 提出ARL方法，通过聚类识别数据子集结构，分析Lasso行为以区分真假相关性，并设定阈值进行特征选择。

Result: ARL能有效区分真假相关性，最大化信息变量的包含，同时最小化有害变量的引入。

Conclusion: ARL通过精细的子集分析和阈值设定，显著提升了Lasso在高相关性特征下的表现。

Abstract: The Lasso method is known to exhibit instability in the presence of highly
correlated features, often leading to an arbitrary selection of predictors.
This issue manifests itself in two primary error types: the erroneous omission
of features that lack a true substitutable relationship (falsely redundant
features) and the inclusion of features with a true substitutable relationship
(truly redundant features). Although most existing methods address only one of
these challenges, we introduce the Accept-Reject Lasso (ARL), a novel approach
that resolves this dilemma. ARL operationalizes an Accept-Reject framework
through a fine-grained analysis of feature selection across data subsets. This
framework is designed to partition the output of an ensemble method into
beneficial and detrimental components through fine-grained analysis. The
fundamental challenge for Lasso is that inter-variable correlation obscures the
true sources of information. ARL tackles this by first using clustering to
identify distinct subset structures within the data. It then analyzes Lasso's
behavior across these subsets to differentiate between true and spurious
correlations. For truly correlated features, which induce multicollinearity,
ARL tends to select a single representative feature and reject the rest to
ensure model stability. Conversely, for features linked by spurious
correlations, which may vanish in certain subsets, ARL accepts those that Lasso
might have incorrectly omitted. The distinct patterns arising from true versus
spurious correlations create a divisible separation. By setting an appropriate
threshold, our framework can effectively distinguish between these two
phenomena, thereby maximizing the inclusion of informative variables while
minimizing the introduction of detrimental ones. We illustrate the efficacy of
the proposed method through extensive simulation and real-data experiments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [127] [The Ubiquitous Sparse Matrix-Matrix Products](https://arxiv.org/abs/2508.04077)
*Aydın Buluç*

Main category: math.NA

TL;DR: 论文探讨了稀疏矩阵乘法的广泛应用及其在多种领域的重要性。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵乘法是数据科学中的基础操作，涉及图算法、神经网络等多个领域，但其在任意代数半环或更一般的异质代数中的应用尚未得到统一研究。

Method: 论文提供了对稀疏矩阵乘法操作的统一处理方法，涵盖其丰富的应用场景。

Result: 研究展示了稀疏矩阵乘法在机器学习、计算生物学、图算法和科学计算中的广泛应用。

Conclusion: 稀疏矩阵乘法是一个具有广泛应用的基础操作，其统一处理方法对多个领域具有重要意义。

Abstract: Multiplication of a sparse matrix with another (dense or sparse) matrix is a
fundamental operation that captures the computational patterns of many data
science applications, including but not limited to graph algorithms, sparsely
connected neural networks, graph neural networks, clustering, and many-to-many
comparisons of biological sequencing data.
  In many application scenarios, the matrix multiplication takes places on an
arbitrary algebraic semiring where the scalar operations are overloaded with
user-defined functions with certain properties or a more general heterogenous
algebra where even the domains of the input matrices can be different. Here, we
provide a unifying treatment of the sparse matrix-matrix operation and its rich
application space including machine learning, computational biology and
chemistry, graph algorithms, and scientific computing.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [128] [Reliable Programmatic Weak Supervision with Confidence Intervals for Label Probabilities](https://arxiv.org/abs/2508.03896)
*Verónica Álvarez,Santiago Mazuelas,Steven An,Sanjoy Dasgupta*

Main category: stat.ML

TL;DR: 提出一种新方法，通过不确定性集合改进弱监督学习，提供标签概率的置信区间，提高预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 数据集标注成本高且耗时，现有弱监督方法无法评估标签概率的可靠性。

Method: 利用不确定性集合封装弱标注函数的信息，生成置信区间。

Result: 在多个基准数据集上验证了方法的优越性和置信区间的实用性。

Conclusion: 新方法显著提升了弱监督学习的可靠性和实用性。

Abstract: The accurate labeling of datasets is often both costly and time-consuming.
Given an unlabeled dataset, programmatic weak supervision obtains probabilistic
predictions for the labels by leveraging multiple weak labeling functions (LFs)
that provide rough guesses for labels. Weak LFs commonly provide guesses with
assorted types and unknown interdependences that can result in unreliable
predictions. Furthermore, existing techniques for programmatic weak supervision
cannot provide assessments for the reliability of the probabilistic predictions
for labels. This paper presents a methodology for programmatic weak supervision
that can provide confidence intervals for label probabilities and obtain more
reliable predictions. In particular, the methods proposed use uncertainty sets
of distributions that encapsulate the information provided by LFs with
unrestricted behavior and typology. Experiments on multiple benchmark datasets
show the improvement of the presented methods over the state-of-the-art and the
practicality of the confidence intervals presented.

</details>


### [129] [Reinforcement Learning in MDPs with Information-Ordered Policies](https://arxiv.org/abs/2508.03904)
*Zhongjun Zhang,Shipra Agrawal,Ilan Lobel,Sean R. Sinclair,Christina Lee Yu*

Main category: stat.ML

TL;DR: 提出了一种基于epoch的强化学习算法，用于无限时间平均成本MDP，利用策略类的偏序关系减少环境交互需求，实现高效反事实推理。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习在无限时间MDP中需要大量环境交互的问题，利用偏序关系提高效率。

Method: 通过策略类的偏序关系（π' ≤ π），利用π收集的数据估计π'的性能，避免额外环境交互。

Result: 算法实现了O(√(w log(|Θ|) T))的遗憾界，与状态和动作空间大小无关。

Conclusion: 该方法在库存控制和排队系统等领域具有广泛适用性，无需额外假设即可获得理论和实证优势。

Abstract: We propose an epoch-based reinforcement learning algorithm for
infinite-horizon average-cost Markov decision processes (MDPs) that leverages a
partial order over a policy class. In this structure, $\pi' \leq \pi$ if data
collected under $\pi$ can be used to estimate the performance of $\pi'$,
enabling counterfactual inference without additional environment interaction.
Leveraging this partial order, we show that our algorithm achieves a regret
bound of $O(\sqrt{w \log(|\Theta|) T})$, where $w$ is the width of the partial
order. Notably, the bound is independent of the state and action space sizes.
We illustrate the applicability of these partial orders in many domains in
operations research, including inventory control and queuing systems. For each,
we apply our framework to that problem, yielding new theoretical guarantees and
strong empirical results without imposing extra assumptions such as convexity
in the inventory model or specialized arrival-rate structure in the queuing
model.

</details>


### [130] [Negative binomial regression and inference using a pre-trained transformer](https://arxiv.org/abs/2508.04111)
*Valentine Svensson*

Main category: stat.ML

TL;DR: 论文提出了一种基于预训练Transformer的方法来估计负二项回归参数，发现矩估计法在速度和准确性上优于最大似然估计和Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模比较研究中负二项回归参数估计的计算挑战。

Method: 使用预训练Transformer从观测数据生成参数估计，并通过合成数据训练模型。

Result: Transformer方法比最大似然估计快20倍且更准确，但矩估计法在速度（快1000倍）、校准性和统计功效上表现最佳。

Conclusion: 矩估计法是最优解决方案，因其高效性和准确性。

Abstract: Negative binomial regression is essential for analyzing over-dispersed count
data in in comparative studies, but parameter estimation becomes
computationally challenging in large screens requiring millions of comparisons.
We investigate using a pre-trained transformer to produce estimates of negative
binomial regression parameters from observed count data, trained through
synthetic data generation to learn to invert the process of generating counts
from parameters. The transformer method achieved better parameter accuracy than
maximum likelihood optimization while being 20 times faster. However,
comparisons unexpectedly revealed that method of moment estimates performed as
well as maximum likelihood optimization in accuracy, while being 1,000 times
faster and producing better-calibrated and more powerful tests, making it the
most efficient solution for this application.

</details>


### [131] [Deep Neural Network-Driven Adaptive Filtering](https://arxiv.org/abs/2508.04258)
*Qizhen Wang,Gang Wang,Ying-Chang Liang*

Main category: stat.ML

TL;DR: 提出了一种基于深度神经网络的框架，通过直接梯度获取解决自适应滤波中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统自适应滤波框架依赖显式成本函数设计，泛化能力有限，本文旨在通过数据驱动方法提升泛化性能。

Method: 将深度神经网络作为非线性算子嵌入自适应滤波系统，直接映射滤波残差与学习梯度，采用最大似然作为隐式成本函数。

Result: 通过大量非高斯场景的实验验证了框架的优异泛化能力，并进行了均值与均方稳定性分析。

Conclusion: 该框架通过数据驱动方法显著提升了自适应滤波的泛化性能，适用于复杂非高斯场景。

Abstract: This paper proposes a deep neural network (DNN)-driven framework to address
the longstanding generalization challenge in adaptive filtering (AF). In
contrast to traditional AF frameworks that emphasize explicit cost function
design, the proposed framework shifts the paradigm toward direct gradient
acquisition. The DNN, functioning as a universal nonlinear operator, is
structurally embedded into the core architecture of the AF system, establishing
a direct mapping between filtering residuals and learning gradients. The
maximum likelihood is adopted as the implicit cost function, rendering the
derived algorithm inherently data-driven and thus endowed with exemplary
generalization capability, which is validated by extensive numerical
experiments across a spectrum of non-Gaussian scenarios. Corresponding mean
value and mean square stability analyses are also conducted in detail.

</details>


### [132] [The Relative Instability of Model Comparison with Cross-validation](https://arxiv.org/abs/2508.04409)
*Alexandre Bayle,Lucas Janson,Lester Mackey*

Main category: stat.ML

TL;DR: 本文研究了交叉验证（CV）在比较两种算法时的相对稳定性问题，发现即使算法本身稳定，CV提供的置信区间在比较性能差异时可能无效。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明CV可以为稳定算法的测试误差提供渐近置信区间，但在比较两种算法时，相对稳定性难以从现有结果中推导，因此需要深入研究。

Method: 通过研究软阈值最小二乘算法（类似Lasso），分析其在稀疏低维线性模型中的相对稳定性。

Result: 研究发现，虽然单个算法的稳定性成立，但比较两种算法时相对稳定性不成立，CV置信区间在性能差异上无效。

Conclusion: 在比较两种机器学习算法的性能差异时，即使算法本身稳定，CV的置信区间仍需谨慎使用。

Abstract: Existing work has shown that cross-validation (CV) can be used to provide an
asymptotic confidence interval for the test error of a stable machine learning
algorithm, and existing stability results for many popular algorithms can be
applied to derive positive instances where such confidence intervals will be
valid. However, in the common setting where CV is used to compare two
algorithms, it becomes necessary to consider a notion of relative stability
which cannot easily be derived from existing stability results, even for simple
algorithms. To better understand relative stability and when CV provides valid
confidence intervals for the test error difference of two algorithms, we study
the soft-thresholded least squares algorithm, a close cousin of the Lasso. We
prove that while stability holds when assessing the individual test error of
this algorithm, relative stability fails to hold when comparing the test error
of two such algorithms, even in a sparse low-dimensional linear model setting.
Additionally, we empirically confirm the invalidity of CV confidence intervals
for the test error difference when either soft-thresholding or the Lasso is
used. In short, caution is needed when quantifying the uncertainty of CV
estimates of the performance difference of two machine learning algorithms,
even when both algorithms are individually stable.

</details>


### [133] [Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification](https://arxiv.org/abs/2508.04457)
*Simon Baur,Wojciech Samek,Jackie Ma*

Main category: stat.ML

TL;DR: 本文研究了医学影像中AI模型的不确定性量化方法，通过多标签胸部X光分类任务评估了13种方法，并扩展了部分方法到多标签场景。


<details>
  <summary>Details</summary>
Motivation: 可靠的量化不确定性对医学影像中AI模型的决策至关重要，但目前在实际医疗诊断任务中的应用研究不足。

Method: 使用MIMIC-CXR-JPG数据集，评估了13种不确定性量化方法，包括卷积和基于Transformer的架构，并扩展了部分方法到多标签场景。

Result: 分析揭示了不同方法和架构在不确定性估计中的优势和局限性，以及分离认知和随机不确定性的能力。

Conclusion: 研究为医学影像中的不确定性量化提供了基准，并展示了不同方法的适用性和局限性。

Abstract: Reliable uncertainty quantification is crucial for trustworthy
decision-making and the deployment of AI models in medical imaging. While prior
work has explored the ability of neural networks to quantify predictive,
epistemic, and aleatoric uncertainties using an information-theoretical
approach in synthetic or well defined data settings like natural image
classification, its applicability to real life medical diagnosis tasks remains
underexplored. In this study, we provide an extensive uncertainty
quantification benchmark for multi-label chest X-ray classification using the
MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for
convolutional (ResNet) and transformer-based (Vision Transformer) architectures
across a wide range of tasks. Additionally, we extend Evidential Deep Learning,
HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting.
Our analysis provides insights into uncertainty estimation effectiveness and
the ability to disentangle epistemic and aleatoric uncertainties, revealing
method- and architecture-specific strengths and limitations.

</details>


### [134] [Metric Learning in an RKHS](https://arxiv.org/abs/2508.04476)
*Gokcan Tatli,Yi Chen,Blake Mason,Robert Nowak,Ramya Korlakai Vinayak*

Main category: stat.ML

TL;DR: 论文提出了一种在RKHS中学习度量的一般框架，并提供了新的泛化保证和样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 解决非线性度量学习缺乏理论支持的问题，特别是在RKHS中。

Method: 开发了一个通用的RKHS框架，结合核方法和神经网络进行非线性度量学习。

Result: 通过模拟和真实数据集验证了框架的有效性，并提供了理论保证。

Conclusion: 该框架为非线性度量学习提供了理论基础，并在实践中表现良好。

Abstract: Metric learning from a set of triplet comparisons in the form of "Do you
think item h is more similar to item i or item j?", indicating similarity and
differences between items, plays a key role in various applications including
image retrieval, recommendation systems, and cognitive psychology. The goal is
to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric
learning using kernel methods and neural networks have shown great empirical
promise. While previous works have addressed certain aspects of this problem,
there is little or no theoretical understanding of such methods. The exception
is the special (linear) case in which the RKHS is the standard Euclidean space
$\mathbb{R}^d$; there is a comprehensive theory for metric learning in
$\mathbb{R}^d$. This paper develops a general RKHS framework for metric
learning and provides novel generalization guarantees and sample complexity
bounds. We validate our findings through a set of simulations and experiments
on real datasets. Our code is publicly available at
https://github.com/RamyaLab/metric-learning-RKHS.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [135] [A semi-automatic approach to study population dynamics based on population pyramids](https://arxiv.org/abs/2508.03788)
*Max Hahn-Klimroth,João Pedro Meireles,Laurie Bingaman Lackey,Nick van Eeuwijk Mads F. Bertelsen,Paul W. Dierkes,Marcus Clauss*

Main category: q-bio.PE

TL;DR: 本文提出了一种基于算法的种群数据分类方法，将种群金字塔形状与特定特征关联，用于分析和传达历史种群发展。


<details>
  <summary>Details</summary>
Motivation: 尽管种群金字塔是评估种群特征的常用工具，但缺乏正式的算法方法从中提取信息。本文旨在填补这一空白。

Method: 使用1970-2024年全球动物园哺乳动物种群数据，开发算法将种群数据分类为不同形状的金字塔（如正常/倒金字塔、菱形、柱形等）。

Result: 算法分类结果合理，尤其能捕捉与种群大小变化相关的金字塔形状序列和过渡。

Conclusion: 该方法有望成为分析和传达种群历史发展的有用工具，并对动物种群管理策略具有广泛意义。

Abstract: The depiction of populations - of humans or animals - as "population
pyramids" is a useful tool for the assessment of various characteristics of
populations at a glance. Although these visualisations are well-known objects
in various communities, formalised and algorithmic approaches to gain
information from these data are less present. Here, we present an
algorithm-based classification of population data into "pyramids" of different
shapes ([normal and inverted] pyramid / plunger / bell, [lower / middle /
upper] diamond, column, hourglass) that are linked to specific characteristics
of the population. To develop the algorithmic approach, we used data describing
global zoo populations of mammals from 1970-2024. This algorithm-based approach
delivers plausible classifications, in particular with respect to changes in
population size linked to specific series of, and transitions between,
different "pyramid" shapes. We believe this approach might become a useful tool
for analysing and communicating historical population developments in multiple
contexts and is of broad interest. Moreover, it might be useful for animal
population management strategies.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [136] [Constraining the outputs of ReLU neural networks](https://arxiv.org/abs/2508.03867)
*Yulia Alexandr,Guido Montúfar*

Main category: math.AG

TL;DR: 论文研究了ReLU神经网络与代数簇的关联，通过分析激活区域内的秩约束，推导出表征网络函数的多项式方程，并探讨了簇达到预期维度的条件。


<details>
  <summary>Details</summary>
Motivation: 探索ReLU神经网络的代数结构，以理解其表达能力和结构特性。

Method: 通过分析网络输出的秩约束，推导多项式方程，并研究簇的维度条件。

Result: 得出了表征ReLU网络函数的代数方程，并确定了簇达到预期维度的条件。

Conclusion: 研究揭示了ReLU网络的代数特性，为其表达能力和结构提供了新的见解。

Abstract: We introduce a class of algebraic varieties naturally associated with ReLU
neural networks, arising from the piecewise linear structure of their outputs
across activation regions in input space, and the piecewise multilinear
structure in parameter space. By analyzing the rank constraints on the network
outputs within each activation region, we derive polynomial equations that
characterize the functions representable by the network. We further investigate
conditions under which these varieties attain their expected dimension,
providing insight into the expressive and structural properties of ReLU
networks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [137] [Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training](https://arxiv.org/abs/2508.03742)
*Weiwei Cao,Jianpeng Zhang,Zhongyi Shui,Sinuo Wang,Zeli Chen,Xi Li,Le Lu,Xianghua Ye,Tingbo Liang,Qi Zhang,Ling Zhang*

Main category: eess.IV

TL;DR: 该论文提出了一种提升视觉语义密度的方法，通过疾病级视觉对比学习和解剖学正态建模，改善了医学图像与报告的对齐效果，并在多个CT数据集上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像与报告之间存在语义密度差距，导致视觉对齐偏差，影响了多功能的医学诊断能力。

Method: 采用疾病级视觉对比学习增强视觉语义，并引入解剖学正态建模方法，利用VQ-VAE重构正常视觉嵌入，放大异常信号。

Result: 在多个CT数据集上实现了84.9%的平均AUC，显著超越现有方法，并展示了优越的迁移学习能力。

Conclusion: 通过提升视觉语义密度，有效改善了医学图像与报告的对齐效果，为多功能医学诊断提供了新思路。

Abstract: Vision-language pre-training (VLP) has great potential for developing
multifunctional and general medical diagnostic capabilities. However, aligning
medical images with a low signal-to-noise ratio (SNR) to reports with a high
SNR presents a semantic density gap, leading to visual alignment bias. In this
paper, we propose boosting vision semantic density to improve alignment
effectiveness. On one hand, we enhance visual semantics through disease-level
vision contrastive learning, which strengthens the model's ability to
differentiate between normal and abnormal samples for each anatomical
structure. On the other hand, we introduce an anatomical normality modeling
method to model the distribution of normal samples for each anatomy, leveraging
VQ-VAE for reconstructing normal vision embeddings in the latent space. This
process amplifies abnormal signals by leveraging distribution shifts in
abnormal samples, enhancing the model's perception and discrimination of
abnormal attributes. The enhanced visual representation effectively captures
the diagnostic-relevant semantics, facilitating more efficient and accurate
alignment with the diagnostic report. We conduct extensive experiments on two
chest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,
MedVL-CT69K, and comprehensively evaluate the diagnosis performance across
multiple tasks in the chest and abdominal CT scenarios, achieving
state-of-the-art zero-shot performance. Notably, our method achieved an average
AUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing
methods. Additionally, we demonstrate the superior transfer learning
capabilities of our pre-trained model. Code is available at
https://github.com/alibaba-damo-academy/ViSD-Boost.

</details>


### [138] [Assessing the Impact of Image Super Resolution on White Blood Cell Classification Accuracy](https://arxiv.org/abs/2508.03759)
*Tatwadarshi P. Nagarhalli,Shruti S. Pawar,Soham A. Dahanukar,Uday Aswalekar,Ashwini M. Save,Sanket D. Patil*

Main category: eess.IV

TL;DR: 研究探讨了通过图像超分辨率技术提升白细胞显微镜图像分辨率，以提高深度学习分类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 低分辨率的白细胞显微镜图像可能导致分类不准确，影响医疗诊断效果。

Method: 采用图像超分辨率技术增强图像分辨率，并将增强图像与原始图像结合用于模型训练。

Result: 通过实验验证，图像分辨率的提升有助于模型捕捉更细微的形态变化，从而提高分类准确性。

Conclusion: 结合图像增强技术与深度学习模型，可显著提升白细胞分类的准确性，为医疗诊断提供更可靠的工具。

Abstract: Accurately classifying white blood cells from microscopic images is essential
to identify several illnesses and conditions in medical diagnostics. Many deep
learning technologies are being employed to quickly and automatically classify
images. However, most of the time, the resolution of these microscopic pictures
is quite low, which might make it difficult to classify them correctly. Some
picture improvement techniques, such as image super-resolution, are being
utilized to improve the resolution of the photos to get around this issue. The
suggested study uses large image dimension upscaling to investigate how
picture-enhancing approaches affect classification performance. The study
specifically looks at how deep learning models may be able to understand more
complex visual information by capturing subtler morphological changes when
image resolution is increased using cutting-edge techniques. The model may
learn from standard and augmented data since the improved images are
incorporated into the training process. This dual method seeks to comprehend
the impact of image resolution on model performance and enhance classification
accuracy. A well-known model for picture categorization is used to conduct
extensive testing and thoroughly evaluate the effectiveness of this approach.
This research intends to create more efficient image identification algorithms
customized to a particular dataset of white blood cells by understanding the
trade-offs between ordinary and enhanced images.

</details>


### [139] [Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation](https://arxiv.org/abs/2508.04522)
*Johannes Tischer,Patric Kienast,Marlene Stümpflen,Gregor Kasprian,Georg Langs,Roxane Licandro*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的连续年龄特异性胎儿脑图谱生成框架，用于实时胎儿脑组织分割。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI评估因脑成熟度、成像协议和孕周估计的不确定性而具有挑战性，需要标准化参考框架。

Method: 结合直接配准模型和条件判别器，使用219例21至37周孕期的胎儿MRI数据进行训练。

Result: 高配准精度，动态解剖变化捕捉清晰，平均DSC为86.3%，揭示了详细的神经典型生长轨迹。

Conclusion: 该方法支持个体化发育评估，具有实时性能，适用于研究和临床应用。

Abstract: Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for
studying brain development in vivo. Yet, its assessment remains challenging due
to variability in brain maturation, imaging protocols, and uncertain estimates
of Gestational Age (GA). To overcome these, brain atlases provide a
standardized reference framework that facilitates objective evaluation and
comparison across subjects by aligning the atlas and subjects in a common
coordinate system. In this work, we introduce a novel deep-learning framework
for generating continuous, age-specific fetal brain atlases for real-time fetal
brain tissue segmentation. The framework combines a direct registration model
with a conditional discriminator. Trained on a curated dataset of 219
neurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method
achieves high registration accuracy, captures dynamic anatomical changes with
sharp structural detail, and robust segmentation performance with an average
Dice Similarity Coefficient (DSC) of 86.3% across six brain tissues.
Furthermore, volumetric analysis of the generated atlases reveals detailed
neurotypical growth trajectories, providing valuable insights into the
maturation of the fetal brain. This approach enables individualized
developmental assessment with minimal pre-processing and real-time performance,
supporting both research and clinical applications. The model code is available
at https://github.com/cirmuw/fetal-brain-atlas

</details>


### [140] [LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation](https://arxiv.org/abs/2508.04553)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: eess.IV

TL;DR: 提出了一种名为LA-CaRe-CNN的两阶段3D CNN模型，用于从LGE MR扫描中精确分割左心房和左心房瘢痕组织，以支持个性化消融治疗。


<details>
  <summary>Details</summary>
Motivation: 心房颤动（AF）治疗需要消融手术，而患者特异性心脏数字孪生模型需要准确的健康与瘢痕组织分割。

Method: LA-CaRe-CNN是一个两阶段CNN级联模型，第一阶段预测左心房，第二阶段结合原始图像信息细化瘢痕组织预测。使用强化的数据增强应对域偏移。

Result: 模型在5折集成下表现优异：左心房分割DSC为89.21%，ASSD为1.6969 mm；瘢痕组织DSC为64.59%，G-DSC为91.80%。

Conclusion: LA-CaRe-CNN的分割结果有望用于生成心脏数字孪生模型，支持个性化消融治疗。

Abstract: Atrial fibrillation (AF) represents the most prevalent type of cardiac
arrhythmia for which treatment may require patients to undergo ablation
therapy. In this surgery cardiac tissues are locally scarred on purpose to
prevent electrical signals from causing arrhythmia. Patient-specific cardiac
digital twin models show great potential for personalized ablation therapy,
however, they demand accurate semantic segmentation of healthy and scarred
tissue typically obtained from late gadolinium enhanced (LGE) magnetic
resonance (MR) scans. In this work we propose the Left Atrial Cascading
Refinement CNN (LA-CaRe-CNN), which aims to accurately segment the left atrium
as well as left atrial scar tissue from LGE MR scans. LA-CaRe-CNN is a 2-stage
CNN cascade that is trained end-to-end in 3D, where Stage 1 generates a
prediction for the left atrium, which is then refined in Stage 2 in conjunction
with the original image information to obtain a prediction for the left atrial
scar tissue. To account for domain shift towards domains unknown during
training, we employ strong intensity and spatial augmentation to increase the
diversity of the training dataset. Our proposed method based on a 5-fold
ensemble achieves great segmentation results, namely, 89.21% DSC and 1.6969 mm
ASSD for the left atrium, as well as 64.59% DSC and 91.80% G-DSC for the more
challenging left atrial scar tissue. Thus, segmentations obtained through
LA-CaRe-CNN show great potential for the generation of patient-specific cardiac
digital twin models and downstream tasks like personalized targeted ablation
therapy to treat AF.

</details>


### [141] [A Comprehensive Framework for Uncertainty Quantification of Voxel-wise Supervised Models in IVIM MRI](https://arxiv.org/abs/2508.04588)
*Nicola Casali,Alessandro Brusaferri,Giuseppe Baselli,Stefano Fumagalli,Edoardo Micotti,Gianluigi Forloni,Riaz Hussein,Giovanna Rizzo,Alfonso Mastropietro*

Main category: eess.IV

TL;DR: 提出了一种基于深度集成和混合密度网络的概率深度学习框架，用于估计IVIM参数及其不确定性，并在合成和真实数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: IVIM参数估计因逆问题的不适定性和噪声敏感性而具有挑战性，需一种能量化不确定性的方法。

Method: 使用混合密度网络（MDN）和深度集成（DE）框架，分解预测不确定性为偶然性和认知性成分，并在合成数据上训练。

Result: MDN对D和f参数生成更校准和锐利的预测分布，但D*略有过度自信。认知性不确定性显示真实数据与训练数据存在不匹配。

Conclusion: 该框架为IVIM拟合提供了全面的不确定性量化方法，并可推广至其他物理模型。

Abstract: Accurate estimation of intravoxel incoherent motion (IVIM) parameters from
diffusion-weighted MRI remains challenging due to the ill-posed nature of the
inverse problem and high sensitivity to noise, particularly in the perfusion
compartment. In this work, we propose a probabilistic deep learning framework
based on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling
estimation of total predictive uncertainty and decomposition into aleatoric
(AU) and epistemic (EU) components. The method was benchmarked against non
probabilistic neural networks, a Bayesian fitting approach and a probabilistic
network with single Gaussian parametrization. Supervised training was performed
on synthetic data, and evaluation was conducted on both simulated and two in
vivo datasets. The reliability of the quantified uncertainties was assessed
using calibration curves, output distribution sharpness, and the Continuous
Ranked Probability Score (CRPS). MDNs produced more calibrated and sharper
predictive distributions for the D and f parameters, although slight
overconfidence was observed in D*. The Robust Coefficient of Variation (RCV)
indicated smoother in vivo estimates for D* with MDNs compared to Gaussian
model. Despite the training data covering the expected physiological range,
elevated EU in vivo suggests a mismatch with real acquisition conditions,
highlighting the importance of incorporating EU, which was allowed by DE.
Overall, we present a comprehensive framework for IVIM fitting with uncertainty
quantification, which enables the identification and interpretation of
unreliable estimates. The proposed approach can also be adopted for fitting
other physical models through appropriate architectural and simulation
adjustments.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [142] [Comparing Normalization Methods for Portfolio Optimization with Reinforcement Learning](https://arxiv.org/abs/2508.03910)
*Caio de Souza Barbosa Costa,Anna Helena Reali Costa*

Main category: q-fin.CP

TL;DR: 强化学习在金融领域（如投资组合优化）应用广泛，但特定策略梯度算法在不同市场表现不一致。本文探讨状态归一化方法对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究特定策略梯度算法在非加密货币市场中表现不佳的原因，假设状态归一化方法可能导致关键信息丢失。

Method: 评估两种常用归一化方法在三种市场（IBOVESPA、NYSE和加密货币）中的表现，并与标准归一化实践对比。

Result: 结果表明，状态归一化确实会降低智能体在特定领域的性能。

Conclusion: 状态归一化方法在金融领域可能不适合，需进一步研究优化方法。

Abstract: Recently, reinforcement learning has achieved remarkable results in various
domains, including robotics, games, natural language processing, and finance.
In the financial domain, this approach has been applied to tasks such as
portfolio optimization, where an agent continuously adjusts the allocation of
assets within a financial portfolio to maximize profit. Numerous studies have
introduced new simulation environments, neural network architectures, and
training algorithms for this purpose. Among these, a domain-specific policy
gradient algorithm has gained significant attention in the research community
for being lightweight, fast, and for outperforming other approaches. However,
recent studies have shown that this algorithm can yield inconsistent results
and underperform, especially when the portfolio does not consist of
cryptocurrencies. One possible explanation for this issue is that the commonly
used state normalization method may cause the agent to lose critical
information about the true value of the assets being traded. This paper
explores this hypothesis by evaluating two of the most widely used
normalization methods across three different markets (IBOVESPA, NYSE, and
cryptocurrencies) and comparing them with the standard practice of normalizing
data before training. The results indicate that, in this specific domain, the
state normalization can indeed degrade the agent's performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体协作优化方法MAGRPO，用于解决LLM在协作任务中的问题，实验表明其能高效生成高质量响应。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多独立预训练，缺乏协作优化，且现有微调框架依赖复杂个体奖励设计。

Method: 将LLM协作建模为合作MARL问题，提出多智能体多轮算法MAGRPO，结合RL和MARL技术。

Result: 实验证明MAGRPO能有效提升LLM在写作和编程协作任务中的表现。

Conclusion: MAGRPO为LLM应用MARL方法提供了新思路，并指出了相关挑战。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


### [144] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文综述了基于多模态大语言模型的操作系统代理（OS Agents），探讨了其核心组件、构建方法、评估协议及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实现像《钢铁侠》中J.A.R.V.I.S.一样多功能的AI助手，是推动操作系统代理研究的核心动力。

Method: 通过分析环境、观察空间、动作空间等关键组件，以及领域特定的基础模型和代理框架，构建OS Agents。

Result: 综述了OS Agents的评估协议和基准测试，展示了其在多样化任务中的表现。

Conclusion: OS Agents研究面临安全、隐私、个性化等挑战，未来需进一步探索这些方向以推动发展。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [145] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种基于辩论的新型可解释、可解释的偏见检测方法，结合形式化和计算论证技术，强调透明度和公平性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在社会中的广泛应用，防止数据或模型中的偏见对特定群体造成系统性不利影响变得至关重要。现有方法大多忽视透明度，而公平性需要更高的可解释性。

Method: 基于受保护特征的个体及其邻域值，通过形式化和计算论证技术构建辩论框架，检测偏见。

Result: 方法在性能上优于基线，同时具备高度的可解释性和可解释性。

Conclusion: 该方法为公平性研究提供了透明且高效的偏见检测工具。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [146] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个自进化框架，帮助计算机使用代理（CUAs）通过自主交互学习新软件，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）在缺乏标注数据的新软件环境中表现不佳。

Method: SEAgent通过经验学习、世界状态模型和课程生成器，结合对抗模仿和GRPO优化策略，实现自主进化。

Result: 在OS-World的五个新软件环境中，SEAgent的成功率从11.3%提升至34.5%，超越UI-TARS。

Conclusion: SEAgent通过自主学习和专家-通用策略，显著提升了CUAs在新环境中的性能。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [147] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: ASTRA是一个自动化代理系统，用于系统性发现AI代码生成和安全性指导系统中的安全缺陷，通过知识图谱和自适应探索提升模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手的安全性在高风险领域（如网络安全）仍不明确，现有工具无法覆盖真实漏洞。

Method: ASTRA分三阶段工作：构建知识图谱、自适应探索输入空间和推理过程、生成高质量违规案例。

Result: ASTRA在两大评估领域中发现比现有技术多11-66%的问题，测试案例使对齐训练效果提升17%。

Conclusion: ASTRA为构建更安全的AI系统提供了实用价值。

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


### [148] [SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios](https://arxiv.org/abs/2508.04100)
*Borui Li,Li Yan,Junhao Han,Jianmin Liu,Lei Yu*

Main category: cs.CR

TL;DR: SenseCrypt是一种基于敏感性的选择性同态加密框架，用于在跨设备联邦学习中平衡安全性和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统选择性同态加密方法在跨设备场景下存在计算开销高和客户端性能下降的问题，需要一种更高效的解决方案。

Method: 通过隐私保护方法聚类相似数据分布的客户端，设计评分机制确定加密比例，并优化模型参数选择以最小化开销和最大化安全性。

Result: SenseCrypt在保证安全性的同时，将训练时间减少58.4%-88.7%，并保持模型准确性。

Conclusion: SenseCrypt在跨设备联邦学习中有效平衡了安全性和性能，优于传统方法。

Abstract: Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but
suffers from high overhead and adaptation cost. Selective HE methods, which
partially encrypt model parameters by a global mask, are expected to protect
privacy with reduced overhead and easy adaptation. However, in cross-device
scenarios with heterogeneous data and system capabilities, traditional
Selective HE methods deteriorate client straggling, and suffer from degraded HE
overhead reduction performance. Accordingly, we propose SenseCrypt, a
Sensitivity-guided selective Homomorphic EnCryption framework, to adaptively
balance security and HE overhead per cross-device FL client. Given the
observation that model parameter sensitivity is effective for measuring
clients' data distribution similarity, we first design a privacy-preserving
method to respectively cluster the clients with similar data distributions.
Then, we develop a scoring mechanism to deduce the straggler-free ratio of
model parameters that can be encrypted by each client per cluster. Finally, for
each client, we formulate and solve a multi-objective model parameter selection
optimization problem, which minimizes HE overhead while maximizing model
security without causing straggling. Experiments demonstrate that SenseCrypt
ensures security against the state-of-the-art inversion attacks, while
achieving normal model accuracy as on IID data, and reducing training time by
58.4%-88.7% as compared to traditional HE methods.

</details>


### [149] [Evaluating Selective Encryption Against Gradient Inversion Attacks](https://arxiv.org/abs/2508.04155)
*Jiajun Gu,Yuhang Yao,Shuaiqi Wang,Carlee Joe-Wong*

Main category: cs.CR

TL;DR: 论文探讨了梯度反转攻击对联邦学习的隐私威胁，提出选择性加密方法以减少计算开销并保持抗攻击能力。通过实验验证了梯度幅度作为有效指标，并提供了选择策略的指南。


<details>
  <summary>Details</summary>
Motivation: 梯度反转攻击威胁分布式训练框架的隐私，传统加密方法计算开销大，选择性加密成为潜在解决方案，但缺乏系统性研究。

Method: 系统评估不同重要性指标的选择性加密方法，提出基于距离的重要性分析框架，并在多种模型和攻击类型下实验。

Result: 梯度幅度是抵御优化型梯度反转攻击的有效指标，但无单一策略适用于所有场景，需根据模型和隐私需求选择。

Conclusion: 选择性加密可行且高效，需灵活选择策略以适应不同场景。

Abstract: Gradient inversion attacks pose significant privacy threats to distributed
training frameworks such as federated learning, enabling malicious parties to
reconstruct sensitive local training data from gradient communications between
clients and an aggregation server during the aggregation process. While
traditional encryption-based defenses, such as homomorphic encryption, offer
strong privacy guarantees without compromising model utility, they often incur
prohibitive computational overheads. To mitigate this, selective encryption has
emerged as a promising approach, encrypting only a subset of gradient data
based on the data's significance under a certain metric. However, there have
been few systematic studies on how to specify this metric in practice. This
paper systematically evaluates selective encryption methods with different
significance metrics against state-of-the-art attacks. Our findings demonstrate
the feasibility of selective encryption in reducing computational overhead
while maintaining resilience against attacks. We propose a distance-based
significance analysis framework that provides theoretical foundations for
selecting critical gradient elements for encryption. Through extensive
experiments on different model architectures (LeNet, CNN, BERT, GPT-2) and
attack types, we identify gradient magnitude as a generally effective metric
for protection against optimization-based gradient inversions. However, we also
observe that no single selective encryption strategy is universally optimal
across all attack scenarios, and we provide guidelines for choosing appropriate
strategies for different model architectures and privacy requirements.

</details>


### [150] [Attack Pattern Mining to Discover Hidden Threats to Industrial Control Systems](https://arxiv.org/abs/2508.04561)
*Muhammad Azmi Umer,Chuadhry Mujeeb Ahmed,Aditya Mathur,Muhammad Taha Jilani*

Main category: cs.CR

TL;DR: 验证工业控制系统（ICS）安全中攻击模式挖掘的有效性，提出了一种数据驱动技术生成攻击模式，并通过案例研究验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统的安全评估需要生成大量多样的攻击模式，但现有方法可能不足，因此提出数据驱动技术以解决这一问题。

Method: 采用数据驱动技术，从实际水处理厂的数据中生成超过10万种攻击模式。

Result: 成功生成了大量攻击模式，并通过案例研究验证了其有效性。

Conclusion: 数据驱动技术能够有效生成并验证工业控制系统的攻击模式，为安全评估提供了新方法。

Abstract: This work focuses on validation of attack pattern mining in the context of
Industrial Control System (ICS) security. A comprehensive security assessment
of an ICS requires generating a large and variety of attack patterns. For this
purpose we have proposed a data driven technique to generate attack patterns
for an ICS. The proposed technique has been used to generate over 100,000
attack patterns from data gathered from an operational water treatment plant.
In this work we present a detailed case study to validate the attack patterns.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [151] [Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model](https://arxiv.org/abs/2508.03925)
*Shen Zhu,Yinzhu Jin,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 提出了一种扩散模型，用于生成具有对应关系的点基形状表示，解决了现有深度学习方法忽略点对应关系的问题。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状模型关注点对应关系，但当前深度学习方法仅关注无序点云，无法生成具有点对应关系的形状。

Method: 设计了一种扩散模型，利用OASIS-3数据集中的对应关系数据，生成保留点对应关系的形状表示。

Result: 模型生成的形状表示高度逼真，优于现有方法，并展示了在条件生成和疾病形态预测中的应用。

Conclusion: 该模型成功生成了具有点对应关系的形状表示，为下游任务提供了实用工具。

Abstract: We propose a diffusion model designed to generate point-based shape
representations with correspondences. Traditional statistical shape models have
considered point correspondences extensively, but current deep learning methods
do not take them into account, focusing on unordered point clouds instead.
Current deep generative models for point clouds do not address generating
shapes with point correspondences between generated shapes. This work aims to
formulate a diffusion model that is capable of generating realistic point-based
shape representations, which preserve point correspondences that are present in
the training data. Using shape representation data with correspondences derived
from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our
correspondence-preserving model effectively generates point-based hippocampal
shape representations that are highly realistic compared to existing methods.
We further demonstrate the applications of our generative model by downstream
tasks, such as conditional generation of healthy and AD subjects and predicting
morphological changes of disease progression by counterfactual generation.

</details>


### [152] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 本文提出了一种结合特权信息和深度学习目标检测的新方法，用于提高垃圾检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着全球垃圾污染的增加，开发高效的自动化垃圾检测工具成为重要挑战。

Method: 结合特权信息与深度学习目标检测，提出了一种编码边界框信息为二进制掩码的方法，以优化检测指导。

Result: 在SODA、BDW和UAVVaste数据集上的实验表明，该方法在所有模型中均提升了性能，且未增加模型复杂度。

Conclusion: 该方法在垃圾检测中实现了准确性与效率的平衡，适用于实际应用。

Abstract: As litter pollution continues to rise globally, developing automated tools
capable of detecting litter effectively remains a significant challenge. This
study presents a novel approach that combines, for the first time, privileged
information with deep learning object detection to improve litter detection
while maintaining model efficiency. We evaluate our method across five widely
used object detection models, addressing challenges such as detecting small
litter and objects partially obscured by grass or stones. In addition to this,
a key contribution of our work can also be attributed to formulating a means of
encoding bounding box information as a binary mask, which can be fed to the
detection model to refine detection guidance. Through experiments on both
within-dataset evaluation on the renowned SODA dataset and cross-dataset
evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate
consistent performance improvements across all models. Our approach not only
bolsters detection accuracy within the training sets but also generalises well
to other litter detection contexts. Crucially, these improvements are achieved
without increasing model complexity or adding extra layers, ensuring
computational efficiency and scalability. Our results suggest that this
methodology offers a practical solution for litter detection, balancing
accuracy and efficiency in real-world applications.

</details>


### [153] [Bootstrap Deep Spectral Clustering with Optimal Transport](https://arxiv.org/abs/2508.04200)
*Wengang Guo,Wei Ye,Chunchun Chen,Xin Sun,Christian Böhm,Claudia Plant,Susanto Rahardja*

Main category: cs.CV

TL;DR: 提出了一种名为BootSC的深度谱聚类模型，通过端到端方式联合优化谱聚类的所有阶段，解决了传统方法的优化不连贯和表示能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类方法存在优化过程不连贯和表示能力有限的缺点，限制了其性能。

Method: BootSC通过单一网络端到端学习谱聚类的所有阶段，利用最优传输监督引导亲和矩阵和聚类分配矩阵，并引入语义一致的正交重参数化技术增强判别能力。

Result: 实验表明，BootSC在聚类性能上达到最先进水平，例如在ImageNet-Dogs数据集上比第二名方法提升了16%的NMI。

Conclusion: BootSC通过端到端联合优化和正交化技术显著提升了谱聚类的性能，具有实际应用潜力。

Abstract: Spectral clustering is a leading clustering method. Two of its major
shortcomings are the disjoint optimization process and the limited
representation capacity. To address these issues, we propose a deep spectral
clustering model (named BootSC), which jointly learns all stages of spectral
clustering -- affinity matrix construction, spectral embedding, and $k$-means
clustering -- using a single network in an end-to-end manner. BootSC leverages
effective and efficient optimal-transport-derived supervision to bootstrap the
affinity matrix and the cluster assignment matrix. Moreover, a
semantically-consistent orthogonal re-parameterization technique is introduced
to orthogonalize spectral embeddings, significantly enhancing the
discrimination capability. Experimental results indicate that BootSC achieves
state-of-the-art clustering performance. For example, it accomplishes a notable
16\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs
dataset. Our code is available at https://github.com/spdj2271/BootSC.

</details>


### [154] [Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting](https://arxiv.org/abs/2508.04227)
*Yuyang Liu,Qiuhe Hong,Linlan Huang,Alexandra Gomez-Villa,Dipam Goswami,Xialei Liu,Joost van de Weijer,Yonghong Tian*

Main category: cs.CV

TL;DR: 该论文首次系统综述了视觉语言模型（VLM）的持续学习（CL）问题，提出了三种核心挑战及解决方案，并呼吁改进评估基准。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现优异，但在非静态数据下的持续学习中面临灾难性遗忘等独特挑战，需要系统研究。

Method: 通过识别三种核心失败模式，提出挑战驱动的分类法，包括多模态回放策略、跨模态正则化和参数高效适应。

Result: 总结了当前解决方案、评估协议和数据集，指出需要更好的基准来捕捉VLM特有的遗忘和组合泛化问题。

Conclusion: 论文为开发终身视觉语言系统提供了全面参考，并提出了未来研究方向，如持续预训练和组合零样本学习。

Abstract: Vision-language models (VLMs) have achieved impressive performance across
diverse multimodal tasks by leveraging large-scale pre-training. However,
enabling them to learn continually from non-stationary data remains a major
challenge, as their cross-modal alignment and generalization capabilities are
particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal
continual learning (CL), VLMs face unique challenges such as cross-modal
feature drift, parameter interference due to shared architectures, and
zero-shot capability erosion. This survey offers the first focused and
systematic review of continual learning for VLMs (VLM-CL). We begin by
identifying the three core failure modes that degrade performance in VLM-CL.
Based on these, we propose a challenge-driven taxonomy that maps solutions to
their target problems: (1) \textit{Multi-Modal Replay Strategies} address
cross-modal drift through explicit or implicit memory mechanisms; (2)
\textit{Cross-Modal Regularization} preserves modality alignment during
updates; and (3) \textit{Parameter-Efficient Adaptation} mitigates parameter
interference with modular or low-rank updates. We further analyze current
evaluation protocols, datasets, and metrics, highlighting the need for better
benchmarks that capture VLM-specific forgetting and compositional
generalization. Finally, we outline open problems and future directions,
including continual pre-training and compositional zero-shot learning. This
survey aims to serve as a comprehensive and diagnostic reference for
researchers developing lifelong vision-language systems. All resources are
available at:
https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.

</details>


### [155] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: LayerT2V是一种分层生成视频的方法，解决了多物体运动轨迹控制的挑战，显著提升了生成质量和控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2V生成方法在多物体运动场景中表现不佳，尤其是在物体轨迹交叉时语义冲突严重。

Method: 通过分层合成背景和前景物体，将每个物体置于独立层上，实现多物体运动的灵活控制。

Result: 实验表明，LayerT2V在mIoU和AP50指标上分别比现有方法提升了1.4倍和4.5倍。

Conclusion: LayerT2V在多物体视频生成任务中表现出色，提供了更灵活和高质量的控制能力。

Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a
challenging and relatively under-explored area, particularly in scenarios
involving multiple moving objects. Most community models and datasets in the
T2V domain are designed for single-object motion, limiting the performance of
current generative models in multi-object tasks. Additionally, existing motion
control methods in T2V either lack support for multi-object motion scenes or
experience severe performance degradation when object trajectories intersect,
primarily due to the semantic conflicts in colliding regions. To address these
limitations, we introduce LayerT2V, the first approach for generating video by
compositing background and foreground objects layer by layer. This layered
generation enables flexible integration of multiple independent elements within
a video, positioning each element on a distinct "layer" and thus facilitating
coherent multi-object synthesis while enhancing control over the generation
process. Extensive experiments demonstrate the superiority of LayerT2V in
generating complex multi-object scenarios, showcasing 1.4x and 4.5x
improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.
Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .

</details>


### [156] [Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark](https://arxiv.org/abs/2508.04260)
*Xiao Wang,Ziwen Wang,Wentao Wu,Anjie Wang,Jiashu Wu,Yantao Pan,Chenglong Li*

Main category: cs.CV

TL;DR: 论文提出SAV框架，结合SAM模型、车辆部件知识图谱和上下文检索模块，解决车辆部件分割问题，并发布了VehicleSeg10K数据集。


<details>
  <summary>Details</summary>
Motivation: 现有SAM模型无法直接用于车辆部件分割任务，因其缺乏语义标签和文本提示功能，限制了其在结构化任务中的应用。

Method: SAV框架包括SAM编码器-解码器、车辆部件知识图谱和上下文样本检索编码模块，知识图谱建模空间关系，检索模块增强分割性能。

Result: 在VehicleSeg10K数据集和其他两个数据集上进行了实验，验证了SAV框架的有效性。

Conclusion: SAV框架解决了SAM在车辆部件分割中的局限性，并提供了新数据集和开源代码，为未来研究奠定基础。

Abstract: With the rapid advancement of autonomous driving, vehicle perception,
particularly detection and segmentation, has placed increasingly higher demands
on algorithmic performance. Pre-trained large segmentation models, especially
Segment Anything Model (SAM), have sparked significant interest and inspired
new research directions in artificial intelligence. However, SAM cannot be
directly applied to the fine-grained task of vehicle part segmentation, as its
text-prompted segmentation functionality is not publicly accessible, and the
mask regions generated by its default mode lack semantic labels, limiting its
utility in structured, category-specific segmentation tasks. To address these
limitations, we propose SAV, a novel framework comprising three core
components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a
context sample retrieval encoding module. The knowledge graph explicitly models
the spatial and geometric relationships among vehicle parts through a
structured ontology, effectively encoding prior structural knowledge.
Meanwhile, the context retrieval module enhances segmentation by identifying
and leveraging visually similar vehicle instances from training data, providing
rich contextual priors for improved generalization. Furthermore, we introduce a
new large-scale benchmark dataset for vehicle part segmentation, named
VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations
across diverse scenes and viewpoints. We conduct comprehensive experiments on
this dataset and two other datasets, benchmarking multiple representative
baselines to establish a solid foundation for future research and comparison. %
Both the dataset and source code of this paper will be released upon
acceptance. Both the dataset and source code of this paper will be released on
https://github.com/Event-AHU/SAV

</details>


### [157] [VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones](https://arxiv.org/abs/2508.04379)
*Lefei Shen,Mouxiang Chen,Xu Liu,Han Fu,Xiaoxue Ren,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.CV

TL;DR: VisionTS++通过视觉模型改进时间序列预测，解决了数据模态、多变量和概率预测的差距，并在多个基准测试中取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 解决视觉模型在时间序列预测中的三个关键差距：数据模态差异、多变量建模需求和概率预测需求。

Method: 提出VisionTS++，包括高质量数据筛选机制、多变量时间序列转换为RGB图像的方法，以及多分位数预测技术。

Result: 在多个基准测试中表现优异，MSE降低6%-44%，并在12个概率预测设置中9次排名第一。

Conclusion: VisionTS++为跨模态知识转移提供了新范式，推动了通用时间序列基础模型的发展。

Abstract: Recent studies have revealed that vision models pre-trained on images can
perform well in time series forecasting by reformulating forecasting as an
image reconstruction task, suggesting their potential as universal time series
foundation models. However, effective cross-modal transfer from vision to time
series remains challenging due to three key discrepancies: (1) data-modality
gap between structured, bounded image data and unbounded, heterogeneous time
series; (2) multivariate-forecasting gap between standard RGB
three-channel-based vision models and the need to model time series with
arbitrary numbers of variates; and (3) probabilistic-forecasting gap between
the deterministic output formats of most vision models and the requirement for
uncertainty-aware probabilistic predictions. To bridge these gaps, we propose
VisionTS++, a vision-model-based TSFM that performs continual pre-training on
large-scale time series datasets, including 3 innovations: (1) a
vision-model-based filtering mechanism to identify high-quality time series
data, thereby mitigating modality gap and improving pre-training stability, (2)
a colorized multivariate conversion method that transforms multivariate time
series into multi-subfigure RGB images, capturing complex inter-variate
dependencies; and (3) a multi-quantile forecasting approach using parallel
reconstruction heads to generate forecasts of different quantile levels, thus
more flexibly approximating arbitrary output distributions without restrictive
prior distributional assumptions. Evaluated on both in-distribution and
out-of-distribution TSF benchmarks, \model achieves SOTA results, outperforming
specialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12
probabilistic forecasting settings. Our work establishes a new paradigm for
cross-modal knowledge transfer, advancing the development of universal TSFMs.

</details>


### [158] [Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model](https://arxiv.org/abs/2508.04472)
*Hongxu Chen,Zhen Wang,Taoran Mei,Lin Li,Bowei Zhu,Runshi Li,Long Chen*

Main category: cs.CV

TL;DR: ErasePro是一种新的闭式方法，旨在更彻底地消除概念并保持生成质量，通过零残差约束和渐进式层更新策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法在概念消除中存在不完全消除和生成质量下降的问题。

Method: 引入零残差约束确保目标与锚概念特征完美对齐；采用渐进式层更新策略，从浅层到深层逐步更新参数。

Result: 在不同概念消除任务中验证了ErasePro的有效性。

Conclusion: ErasePro解决了现有方法的局限性，实现了更彻底的消除和更好的生成质量。

Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from
generating content associated with semantic-harmful concepts (i.e., target
concepts), is getting increased attention. State-of-the-art methods formulate
this task as an optimization problem: they align all target concepts with
semantic-harmless anchor concepts, and apply closed-form solutions to update
the model accordingly. While these closed-form methods are efficient, we argue
that existing methods have two overlooked limitations: 1) They often result in
incomplete erasure due to "non-zero alignment residual", especially when text
prompts are relatively complex. 2) They may suffer from generation quality
degradation as they always concentrate parameter updates in a few deep layers.
To address these issues, we propose a novel closed-form method ErasePro: it is
designed for more complete concept erasure and better preserving overall
generative quality. Specifically, ErasePro first introduces a strict
zero-residual constraint into the optimization objective, ensuring perfect
alignment between target and anchor concept features and enabling more complete
erasure. Secondly, it employs a progressive, layer-wise update strategy that
gradually transfers target concept features to those of the anchor concept from
shallow to deep layers. As the depth increases, the required parameter changes
diminish, thereby reducing deviations in sensitive deep layers and preserving
generative quality. Empirical results across different concept erasure tasks
(including instance, art style, and nudity erasure) have demonstrated the
effectiveness of our ErasePro.

</details>


### [159] [Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation](https://arxiv.org/abs/2508.04552)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的全心脏分割方法，通过平衡联合训练和强数据增强技术，显著提升了在不同医学影像数据（CT和MR）上的分割性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要更精确的心脏结构分割方法来支持个性化治疗和心脏数字孪生模型的生成。

Method: 采用平衡联合训练策略（同时使用CT和MR数据）和强强度与空间数据增强技术，以应对域偏移问题。

Result: 在CT和MR数据上分别达到93.33% DSC、0.8388 mm ASSD和89.30% DSC、1.2411 mm ASSD的性能，优于单一数据训练的模型。

Conclusion: 该方法能高效生成精确的心脏分割结果，为心脏数字孪生模型提供支持。

Abstract: As the leading cause of death worldwide, cardiovascular diseases motivate the
development of more sophisticated methods to analyze the heart and its
substructures from medical images like Computed Tomography (CT) and Magnetic
Resonance (MR). Semantic segmentations of important cardiac structures that
represent the whole heart are useful to assess patient-specific cardiac
morphology and pathology. Furthermore, accurate semantic segmentations can be
used to generate cardiac digital twin models which allows e.g.
electrophysiological simulation and personalized therapy planning. Even though
deep learning-based methods for medical image segmentation achieved great
advancements over the last decade, retaining good performance under domain
shift -- i.e. when training and test data are sampled from different data
distributions -- remains challenging. In order to perform well on domains known
at training-time, we employ a (1) balanced joint training approach that
utilizes CT and MR data in equal amounts from different source domains.
Further, aiming to alleviate domain shift towards domains only encountered at
test-time, we rely on (2) strong intensity and spatial augmentation techniques
to greatly diversify the available training data. Our proposed whole heart
segmentation method, a 5-fold ensemble with our contributions, achieves the
best performance for MR data overall and a performance similar to the best
performance for CT data when compared to a model trained solely on CT. With
93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR
data, our method demonstrates great potential to efficiently obtain accurate
semantic segmentations from which patient-specific cardiac twin models can be
generated.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [160] [Viability of perturbative expansion for quantum field theories on neurons](https://arxiv.org/abs/2508.03810)
*Srimoyee Sen,Varun Vaidya*

Main category: hep-th

TL;DR: 本文研究了有限神经元数N下，单层神经网络模拟局部量子场论的可行性，并提出了改进架构以提高收敛性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络架构在模拟局部量子场论中的潜力，特别是在有限神经元数下的表现。

Method: 使用标量φ⁴理论作为例子，分析神经网络在有限神经元数N下的表现，并提出改进架构。

Result: 发现O(1/N)修正对紫外截断敏感，收敛性较弱，改进架构后可提升收敛性。

Conclusion: 通过参数约束和N的缩放，可以提取准确的场论结果，改进架构有助于提高收敛性。

Abstract: Neural Network (NN) architectures that break statistical independence of
parameters have been proposed as a new approach for simulating local quantum
field theories (QFTs). In the infinite neuron number limit, single-layer NNs
can exactly reproduce QFT results. This paper examines the viability of this
architecture for perturbative calculations of local QFTs for finite neuron
number $N$ using scalar $\phi^4$ theory in $d$ Euclidean dimensions as an
example. We find that the renormalized $O(1/N)$ corrections to two- and
four-point correlators yield perturbative series which are sensitive to the
ultraviolet cut-off and therefore have a weak convergence. We propose a
modification to the architecture to improve this convergence and discuss
constraints on the parameters of the theory and the scaling of N which allow us
to extract accurate field theory results.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [161] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 论文介绍了针对高能物理理论的专业化大语言模型，基于8B参数的Llama-3.1模型微调了20个变体，并在arXiv摘要上训练，性能优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 开发专门用于高能物理理论的语言模型，以提升该领域的文本生成和理解能力。

Method: 使用两种不同的低秩适应微调方法，在不同数据集（hep-th、hep-ph、gr-qc等）上训练模型变体。

Result: 微调后的模型在高能物理摘要补全任务上表现优于基础模型，并与主流商业LLM进行了比较。

Conclusion: 研究为高能物理理论领域的专业化语言模型开发提供了有价值的见解。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [162] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）生成内容的真实性评估方法，强调了事实核查的重要性，并提出了改进框架和研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs可能生成错误信息，需系统性评估其内容真实性，以提升模型的可信度。

Method: 通过分析2020至2025年的文献，探讨了幻觉问题、数据集限制和评估指标可靠性，提出了整合高级提示策略、领域微调和检索增强生成（RAG）的框架。

Result: 研究发现当前评估指标存在局限性，需结合外部验证证据和领域定制化以提高事实一致性。

Conclusion: 构建准确、可解释且适用于领域特定事实核查的LLMs是未来研究方向，有助于开发更可信的模型。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [163] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: SMEdit是一种新的基于元学习的模型编辑方法，通过多步反向传播和权重更新正则化，提升了低数据场景下的编辑效果和训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的静态特性导致知识更新成本高，模型编辑提供了一种高效替代方案。然而，现有方法在低数据场景下表现不佳且训练效率受限。

Method: 提出SMEdit方法，采用多步反向传播（MBPS）提升低监督下的编辑性能，并通过权重更新的范数正则化提高训练效率。

Result: 在两个数据集和两种LLMs上的实验表明，SMEdit优于现有方法，且MBPS策略可无缝集成到其他方法中进一步提升性能。

Conclusion: SMEdit通过MBPS和正则化策略，有效解决了低数据场景和训练效率问题，为模型编辑提供了新思路。

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [164] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 本文提出了一种从基础模型出发，结合数据处理、数据选择、训练策略和架构调整的方法，以提高资源受限环境下大型语言模型（LLMs）的效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的广泛应用受限于高计算资源需求，本文旨在探索在资源受限环境中提升其效率的方法。

Method: 通过定义可靠数据集标准、进行不同配置的对照实验，并系统评估模型变体的能力、通用性、响应时间和安全性。

Result: 通过比较测试验证了所提策略的有效性，展示了改进后的模型变体在性能和效率上的提升。

Conclusion: 本文提出的方法为资源受限环境下优化大型语言模型提供了可行方案，并基于硕士论文的研究成果进行了验证。

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [165] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出了一种基于难度的偏好数据选择策略，通过选择隐含奖励差距较小的样本提高数据效率和模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖大量昂贵的偏好数据，缺乏高质量数据选择策略。

Method: 基于DPO隐含奖励机制，选择隐含奖励差距较小的偏好数据样本。

Result: 在多个数据集和对齐任务中优于五种基线方法，仅用10%数据实现更优性能。

Conclusion: 该方法为资源有限下扩展LLM对齐提供了高效解决方案。

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [166] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 论文提出了一种新指标HFR（Human Fooling Rate），用于衡量TTS系统生成的语音被误认为人类的频率。研究发现，现有TTS系统在欺骗测试中表现不佳，商业模型在零样本设置下接近人类水平，但开源系统仍有差距。


<details>
  <summary>Details</summary>
Motivation: 探讨当前TTS系统是否能在类似图灵测试的评估中真正欺骗人类，并揭示现有主观评估的局限性。

Method: 引入HFR指标，对开源和商业TTS模型进行大规模评估，分析其在欺骗测试中的表现。

Result: 发现CMOS声称的人类水平在欺骗测试中常失败，商业模型接近人类水平，开源系统仍有不足。

Conclusion: 强调需要更真实、以人为中心的评估方法，补充现有主观测试。

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [167] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 研究探讨了使用黑盒大语言模型（LLMs）进行层次文本分类（HTC）的可行性，比较了三种提示策略在零样本和少样本设置下的表现，发现少样本设置能提升准确率，但需权衡计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决HTC中数据稀缺和模型复杂性的挑战，探索LLMs作为传统机器学习方法的替代方案。

Method: 评估了三种提示策略（DL、DH、TMH）在零样本和少样本设置下的表现，并与传统机器学习模型对比。

Result: 少样本设置提升准确率；LLMs在深层标签层次上表现更优，但API成本显著增加。

Conclusion: 黑盒LLMs在HTC中具有潜力，但需谨慎选择提示策略以平衡性能和成本。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [168] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: MedCheck是一个针对医疗基准测试的生命周期评估框架，解决了现有基准测试在临床保真度、数据管理和安全性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准测试在可靠性、临床实践关联性和安全性方面存在问题，需要更全面的评估方法。

Method: 提出MedCheck框架，将基准测试开发分为五个阶段，并提供46项医学定制标准，用于评估53个医疗LLM基准测试。

Result: 发现现有基准测试普遍存在与临床实践脱节、数据完整性问题和安全性忽视等系统性缺陷。

Conclusion: MedCheck可作为诊断工具和指南，推动医疗AI评估的标准化和可靠性。

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [169] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 论文提出了一种名为Chain of Questions (CoQ)的框架，通过动态生成问题引导多模态语言模型选择性地激活相关感官模态，从而提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在单模态推理方面已有显著进步，但在多模态环境中仍需改进，模型需要主动决定使用哪些感官模态（如视觉、听觉或空间感知）来应对复杂现实环境。

Method: 提出CoQ框架，通过好奇心驱动生成问题，引导模型选择性激活相关模态，收集关键信息以支持推理。

Result: 在整合多个数据集的新多模态基准测试中，CoQ方法显著提升了基础模型识别和整合感官信息的能力，提高了准确性和可解释性。

Conclusion: CoQ框架在多模态任务中有效提升了模型的推理能力，使其更准确地整合信息并生成响应。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [170] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 研究评估了三种NLP模型（零样本LLMs、微调Transformer和传统逻辑回归）在提升交通事故数据质量中的表现，发现微调Transformer（如RoBERTa）在准确性和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 通过高级NLP技术提升交通事故叙述数据的质量，以肯塔基州的二次事故识别为例。

Method: 比较了零样本LLMs、微调Transformer和逻辑回归三类模型在16,656条手动审核的叙述数据上的表现，测试集为2022年的1,771条数据。

Result: 微调Transformer（如RoBERTa）表现最优（F1:0.90，准确率95%），零样本LLMs（如LLaMA3:70B）表现接近但计算成本高，逻辑回归表现最差。

Conclusion: 微调Transformer在准确性和效率上表现最佳，适合实际部署；LLMs在特定场景下表现优异但计算成本高，需权衡优化。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [171] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 论文提出了一种名为ThinkingF的数据合成与训练流程，旨在提升自动形式化的能力，通过结合形式语言知识和自然语言推理能力，显著提高了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法准确率低，需要同时掌握形式语言知识和自然语言推理能力。

Method: 构建两个数据集：一个专注于形式知识，另一个生成非正式到正式的推理轨迹；采用SFT和RLVR训练流程。

Result: 7B和32B模型表现优异，StepFun-Formalizer-32B在FormalMATH-Lite和ProverBench上达到SOTA。

Conclusion: ThinkingF通过结合形式知识与推理能力，显著提升了自动形式化的性能。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [172] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: Sculptor框架通过主动上下文管理工具（ACM）帮助大语言模型（LLM）减少长上下文处理中的性能下降，提升推理和记忆能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在处理长上下文时因无关信息干扰导致的性能下降问题。

Method: 提出Sculptor框架，提供三种工具：上下文分割、摘要/隐藏/恢复、智能搜索，帮助LLM主动管理注意力。

Result: 在PI-LLM和NeedleBench基准测试中，Sculptor显著提升了性能，无需额外训练。

Conclusion: 主动上下文管理策略（而非更大的token窗口）是提升LLM长上下文处理鲁棒性的关键。

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [173] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 论文提出了一种名为GeRe的框架，通过使用预训练文本进行高效抗遗忘，同时引入基于阈值的边缘损失（TM）来优化激活状态一致性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在持续微调过程中的灾难性遗忘问题，同时保持其通用能力和先前任务性能。

Method: 提出General Sample Replay（GeRe）框架，结合TM损失优化激活状态一致性，使用固定的小规模预收集样本进行回放学习。

Result: 实验表明，TM方法在性能提升和鲁棒性方面优于其他回放策略。

Conclusion: GeRe框架为LLMs的高效回放学习提供了新思路，解决了灾难性遗忘问题。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [174] [NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations](https://arxiv.org/abs/2508.04195)
*Huan Liao,Qinke Ni,Yuancheng Wang,Yiheng Lu,Haoyue Zhan,Pengyuan Xie,Qiang Zhang,Zhizheng Wu*

Main category: cs.SD

TL;DR: NVSpeech是一个集成识别和合成副语言发声的管道，包括数据集构建、ASR建模和可控TTS，首次提供了大规模、词级标注的中文副语言数据集。


<details>
  <summary>Details</summary>
Motivation: 副语言发声（如笑声、呼吸声等）在自然语音交流中至关重要，但传统ASR和TTS系统常忽略这些信息。NVSpeech旨在填补这一空白。

Method: 1. 构建手动标注的48,430条副语言数据集；2. 开发副语言感知ASR模型，将副语言作为可解码标记；3. 利用该模型自动标注大规模中文数据集；4. 微调零样本TTS模型，实现副语言的显式控制。

Result: NVSpeech提供了首个大规模、词级标注的中文副语言数据集（174,179条，573小时），并实现了副语言的可控合成。

Conclusion: NVSpeech通过统一副语言的识别与生成，为中文表达性语音建模提供了开放、可扩展的解决方案。

Abstract: Paralinguistic vocalizations-including non-verbal sounds like laughter and
breathing, as well as lexicalized interjections such as "uhm" and "oh"-are
integral to natural spoken communication. Despite their importance in conveying
affect, intent, and interactional cues, such cues remain largely overlooked in
conventional automatic speech recognition (ASR) and text-to-speech (TTS)
systems. We present NVSpeech, an integrated and scalable pipeline that bridges
the recognition and synthesis of paralinguistic vocalizations, encompassing
dataset construction, ASR modeling, and controllable TTS. (1) We introduce a
manually annotated dataset of 48,430 human-spoken utterances with 18 word-level
paralinguistic categories. (2) We develop the paralinguistic-aware ASR model,
which treats paralinguistic cues as inline decodable tokens (e.g., "You're so
funny [Laughter]"), enabling joint lexical and non-verbal transcription. This
model is then used to automatically annotate a large corpus, the first
large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level
alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both
human- and auto-labeled data to enable explicit control over paralinguistic
vocalizations, allowing context-aware insertion at arbitrary token positions
for human-like speech synthesis. By unifying the recognition and generation of
paralinguistic vocalizations, NVSpeech offers the first open, large-scale,
word-level annotated pipeline for expressive speech modeling in Mandarin,
integrating recognition and synthesis in a scalable and controllable manner.
Dataset and audio demos are available at https://nvspeech170k.github.io/.

</details>


### [175] [Live Music Models](https://arxiv.org/abs/2508.04651)
*Lyria Team,Antoine Caillon,Brian McWilliams,Cassie Tarakajian,Ian Simon,Ilaria Manco,Jesse Engel,Noah Constant,Pen Li,Timo I. Denk,Alberto Lalama,Andrea Agostinelli,Anna Huang,Ethan Manilow,George Brower,Hakan Erdogan,Heidi Lei,Itai Rolnick,Ivan Grishchenko,Manu Orsini,Matej Kastelic,Mauricio Zuluaga,Mauro Verzetti,Michael Dooley,Ondrej Skopek,Rafael Ferrer,Zalán Borsos,Äaron van den Oord,Douglas Eck,Eli Collins,Jason Baldridge,Tom Hume,Chris Donahue,Kehang Han,Adam Roberts*

Main category: cs.SD

TL;DR: Magenta RealTime和Lyria RealTime是新型实时音乐生成模型，支持通过文本或音频提示控制音乐风格，在音乐质量和实时生成能力上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索一种新型的音乐生成模型，强调实时性和用户交互，以支持现场音乐表演。

Method: 开发了Magenta RealTime和Lyria RealTime两种模型，前者为开源权重模型，后者为API模型，均支持文本或音频提示控制。

Result: Magenta RealTime在音乐质量指标上优于其他开源模型，且具备实时生成能力；Lyria RealTime提供更强大的控制和广泛的提示覆盖。

Conclusion: 这些模型展示了AI辅助音乐创作的新范式，强调实时交互和现场表演中的人机协作。

Abstract: We introduce a new class of generative models for music called live music
models that produce a continuous stream of music in real-time with synchronized
user control. We release Magenta RealTime, an open-weights live music model
that can be steered using text or audio prompts to control acoustic style. On
automatic metrics of music quality, Magenta RealTime outperforms other
open-weights music generation models, despite using fewer parameters and
offering first-of-its-kind live generation capabilities. We also release Lyria
RealTime, an API-based model with extended controls, offering access to our
most powerful model with wide prompt coverage. These models demonstrate a new
paradigm for AI-assisted music creation that emphasizes human-in-the-loop
interaction for live music performance.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [176] [Entanglement distribution in quantum networks via swapping of partially entangled states](https://arxiv.org/abs/2508.04536)
*Henrique Guerra,Tailan S. Sarubi,Rafael Chaves,Jonas Maziero*

Main category: quant-ph

TL;DR: 研究扩展了纠缠交换协议（ESP）到不同拓扑结构的量子网络，分析了部分纠缠态的应用，探讨了纠缠演化和成功概率。


<details>
  <summary>Details</summary>
Motivation: 探索如何在量子网络中有效分布纠缠，尤其是在部分纠缠态的情况下。

Method: 将ESP应用于线性、星型和混合拓扑的量子网络，分析初始部分纠缠态的演化和成功概率。

Result: 提供了纠缠分布动态的新见解，并为设计稳健的量子通信策略提供了实用指南。

Conclusion: 研究为量子网络中的纠缠分布提供了理论和实践支持，适用于现实条件。

Abstract: The entanglement swapping protocol (ESP) is a fundamental primitive for
distributing quantum correlations across distant nodes in a quantum network.
Recent studies have demonstrated that even when the involved qubit pairs are
only partially entangled, it is still possible to concentrate and transmit
entanglement via Bell-basis measurements. In this work, we extend these ideas
to quantum networks with various topologies - including linear, star, and
hybrid configurations - by analyzing the application of the ESP to initially
partially entangled states. We investigate how entanglement evolves under such
protocols by examining the transformations of the initial states and evaluating
the success probabilities for generating maximally entangled states at the
output. Our results offer new insights into the dynamics of the entanglement
distribution in quantum networks and provide practical guidelines for designing
robust quantum communication strategies under realistic conditions.

</details>


### [177] [Advantages of Co-locating Quantum-HPC Platforms: A Survey for Near-Future Industrial Applications](https://arxiv.org/abs/2508.04171)
*Daigo Honda,Yuta Nishiyama,Junya Ishikawa,Kenichi Matsuzaki,Satoshi Miyata,Tadahiro Chujo,Yasuhisa Yamamoto,Masahiko Kiminami,Taro Kato,Jun Towada,Naoki Yoshioka,Naoto Aoki,Nobuyasu Ito*

Main category: quant-ph

TL;DR: 量子-HPC平台通过共置量子计算机与高性能计算系统，研究了其对延迟、带宽和任务调度的优化效果，发现共置能提升混合任务吞吐量，并需要HPC资源支持大规模问题。


<details>
  <summary>Details</summary>
Motivation: 探讨量子-HPC平台是否能为近未来工业应用带来实际效益，尤其是共置对性能的影响。

Method: 系统调查量子-HPC平台，分析共置对延迟、带宽、任务调度的优化，以及HPC能力对混合算法性能、错误缓解和量子电路优化的支持。

Result: 共置量子与HPC系统可显著提升混合任务吞吐量，且大规模问题需要HPC资源支持。

Conclusion: 量子-HPC平台的共置设计对提升混合任务性能具有实际价值，尤其适用于大规模复杂问题。

Abstract: We conducted a systematic survey of emerging quantum-HPC platforms, which
integrate quantum computers and High-Performance Computing (HPC) systems
through co-location. Currently, it remains unclear whether such platforms
provide tangible benefits for near-future industrial applications. To address
this, we examined the impact of co-location on latency reduction, bandwidth
enhancement, and advanced job scheduling. Additionally, we assessed how
HPC-level capabilities could enhance hybrid algorithm performance, support
large-scale error mitigation, and facilitate complex quantum circuit
partitioning and optimization. Our findings demonstrate that co-locating
quantum and HPC systems can yield measurable improvements in overall hybrid job
throughput. We also observe that large-scale real-world problems can require
HPC-level computational resources for executing hybrid algorithms.

</details>


### [178] [Dynamic Solutions for Hybrid Quantum-HPC Resource Allocation](https://arxiv.org/abs/2508.04217)
*Roberto Rocco,Simone Rizzo,Matteo Barbieri,Gabriella Bettonte,Elisabetta Boella,Fulvio Ganz,Sergio Iserte,Antonio J. Peña,Petter Sandås,Alberto Scionti,Olivier Terzo,Chiara Vercellino,Giacomo Vitali,Paolo Viviani,Jonathan Frassineti,Sara Marzella,Daniele Ottaviani,Iacopo Colonnelli,Daniele Gregori*

Main category: quant-ph

TL;DR: 提出了一种基于可扩展性和工作流的策略，优化混合HPC-量子计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决HPC与量子计算机结合时的资源分配技术挑战。

Method: 采用可扩展性和工作流策略，动态释放和重新分配经典资源。

Result: 实验证明动态分配策略在混合HPC-量子计算用例中具有优势。

Conclusion: 动态资源分配方案在混合计算环境中展现出潜力。

Abstract: The integration of quantum computers within classical High-Performance
Computing (HPC) infrastructures is receiving increasing attention, with the
former expected to serve as accelerators for specific computational tasks.
However, combining HPC and quantum computers presents significant technical
challenges, including resource allocation. This paper presents a novel
malleability-based approach, alongside a workflow-based strategy, to optimize
resource utilization in hybrid HPC-quantum workloads. With both these
approaches, we can release classical resources when computations are offloaded
to the quantum computer and reallocate them once quantum processing is
complete. Our experiments with a hybrid HPC-quantum use case show the benefits
of dynamic allocation, highlighting the potential of those solutions.

</details>


### [179] [Hybrid Quantum--Classical Machine Learning Potential with Variational Quantum Circuits](https://arxiv.org/abs/2508.04098)
*Soohaeng Yoo Willow,D. ChangMo Yang,Chang Woo Myung*

Main category: quant-ph

TL;DR: 论文探讨了混合量子-经典算法在材料建模中的潜在优势，通过将经典神经网络与变分量子电路结合，在NISQ硬件上实现了对液态硅DFT性质的预测。


<details>
  <summary>Details</summary>
Motivation: 量子算法在模拟复杂分子系统方面仍处于早期阶段，超越经典技术具有挑战性。混合量子-经典算法为NISQ硬件提供了一种实用途径。

Method: 将经典E(3)-等变消息传递机器学习势与混合量子-经典MLP进行对比，用VQC替换消息传递层中的每个读出。

Result: 混合架构在高温度结构和热力学性质预测中表现出色，展示了NISQ兼容算法的优势。

Conclusion: 混合量子-经典算法在材料建模中具有潜在优势，为近期量子优势的实现提供了可行路径。

Abstract: Quantum algorithms for simulating large and complex molecular systems are
still in their infancy, and surpassing state-of-the-art classical techniques
remains an ever-receding goal post. A promising avenue of inquiry in the
meanwhile is to seek practical advantages through hybrid quantum-classical
algorithms, which combine conventional neural networks with variational quantum
circuits (VQCs) running on today's noisy intermediate-scale quantum (NISQ)
hardware. Such hybrids are well suited to NISQ hardware. The classical
processor performs the bulk of the computation, while the quantum processor
executes targeted sub-tasks that supply additional non-linearity and
expressivity. Here, we benchmark a purely classical E(3)-equivariant
message-passing machine learning potential (MLP) against a hybrid
quantum-classical MLP for predicting density functional theory (DFT) properties
of liquid silicon. In our hybrid architecture, every readout in the
message-passing layers is replaced by a VQC. Molecular dynamics simulations
driven by the HQC-MLP reveal that an accurate reproduction of high-temperature
structural and thermodynamic properties is achieved with VQCs. These findings
demonstrate a concrete scenario in which NISQ-compatible HQC algorithm could
deliver a measurable benefit over the best available classical alternative,
suggesting a viable pathway toward near-term quantum advantage in materials
modeling.

</details>


### [180] [Quantum circuit complexity and unsupervised machine learning of topological order](https://arxiv.org/abs/2508.04486)
*Yanming Che,Clemens Gneiting,Xiaoguang Wang,Franco Nori*

Main category: quant-ph

TL;DR: 论文探讨了量子电路复杂度在理解量子多体系统拓扑序中的无监督机器学习中的应用，提出了两个定理连接量子电路复杂度与保真度和纠缠生成，并基于此设计了高效的相似性度量方法。


<details>
  <summary>Details</summary>
Motivation: 受Kolmogorov复杂度与无监督机器学习之间紧密关系的启发，研究旨在利用量子电路复杂度构建可解释且高效的机器学习方法，以理解量子多体系统的拓扑序。

Method: 提出了两个定理，将Nielsen的量子电路复杂度与保真度变化和纠缠生成联系起来，并基于此设计了保真度和纠缠驱动的相似性度量方法。通过数值实验验证了方法的有效性。

Result: 数值实验表明，提出的相似性度量方法在量子相的无监督聚类中表现优异，特别是在交替XXZ自旋链、Kitaev环面码基态和随机乘积态的分类中。

Conclusion: 研究建立了量子电路计算、量子复杂度和拓扑量子序机器学习之间的联系，为相关领域提供了新的理论工具和实践方法。

Abstract: Inspired by the close relationship between Kolmogorov complexity and
unsupervised machine learning, we explore quantum circuit complexity, an
important concept in quantum computation and quantum information science, as a
pivot to understand and to build interpretable and efficient unsupervised
machine learning for topological order in quantum many-body systems. To span a
bridge from conceptual power to practical applicability, we present two
theorems that connect Nielsen's quantum circuit complexity for the quantum path
planning between two arbitrary quantum many-body states with fidelity change
and entanglement generation, respectively. Leveraging these connections,
fidelity-based and entanglement-based similarity measures or kernels, which are
more practical for implementation, are formulated. Using the two proposed
kernels, numerical experiments targeting the unsupervised clustering of quantum
phases of the bond-alternating XXZ spin chain, the ground state of Kitaev's
toric code and random product states, are conducted, demonstrating their
superior performance. Relations with classical shadow tomography and shadow
kernel learning are also discussed, where the latter can be naturally derived
and understood from our approach. Our results establish connections between key
concepts and tools of quantum circuit computation, quantum complexity, and
machine learning of topological quantum order.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [181] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: 利用智能手机、智能手表和睡眠传感器被动收集24小时数据，结合主观调查，构建了ETRI Lifelog Dataset 2024，用于研究人类日常行为和睡眠质量。


<details>
  <summary>Details</summary>
Motivation: 提升对人类身心健康状态的准确理解，支持健康研究。

Method: 通过智能设备和睡眠传感器被动收集数据，辅以主观疲劳、压力和睡眠质量的调查问卷。

Result: 构建了包含定量行为和主观报告的综合数据集，部分数据已公开。

Conclusion: 该数据集为研究人类日常生活模式提供了基础资源，并展示了机器学习在预测睡眠质量和压力方面的应用潜力。

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


### [182] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 该研究提出了一种非侵入性、可解释的机器学习框架，利用多模态可穿戴传感器检测脊髓损伤患者的自主神经反射异常（AD）。通过特征选择和集成学习，模型在HR和ECG特征上表现最佳，为实时监测提供了潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 自主神经反射异常（AD）是脊髓损伤患者中一种潜在危及生命的状况，目前监测方法多为侵入性或依赖主观报告，亟需非侵入性、准确的检测手段。

Method: 研究使用多模态可穿戴传感器（ECG、PPG、BioZ等）采集数据，通过BorutaSHAP进行特征选择，并采用集成学习方法训练模型。

Result: HR和ECG特征最具信息量，集成模型性能最佳（Macro F1 = 0.77），HR的AUC达0.93。模型对传感器丢失具有鲁棒性。

Conclusion: 该研究为脊髓损伤患者的个性化实时监测提供了重要进展，未来可进一步优化多模态数据的整合。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [183] [Suggest, Complement, Inspire: Story of Two Tower Recommendations at Allegro.com](https://arxiv.org/abs/2508.03702)
*Aleksandra Osowska-Kurczab,Klaudia Nazarko,Mateusz Marzec,Lidia Wojciechowska,Eliška Kremeňová*

Main category: cs.IR

TL;DR: 论文提出了一种统一的内容推荐系统，解决了大规模电商推荐系统的三大挑战：通用架构设计、降低维护成本和管理动态产品目录。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电商推荐系统中的通用架构设计、高维护成本和高动态产品目录管理的技术挑战。

Method: 基于双塔检索框架，利用文本和结构化属性表示产品，通过近似最近邻搜索实现高效检索。

Result: 经过两年的A/B测试，在桌面和移动应用渠道中显著提升了用户参与度和盈利指标。

Conclusion: 灵活、可扩展的架构能以最小维护成本满足多样化用户需求。

Abstract: Building large-scale e-commerce recommendation systems requires addressing
three key technical challenges: (1) designing a universal recommendation
architecture across dozens of placements, (2) decreasing excessive maintenance
costs, and (3) managing a highly dynamic product catalogue. This paper presents
a unified content-based recommendation system deployed at Allegro.com, the
largest e-commerce platform of European origin. The system is built on a
prevalent Two Tower retrieval framework, representing products using textual
and structured attributes, which enables efficient retrieval via Approximate
Nearest Neighbour search. We demonstrate how the same model architecture can be
adapted to serve three distinct recommendation tasks: similarity search,
complementary product suggestions, and inspirational content discovery, by
modifying only a handful of components in either the model or the serving
logic. Extensive A/B testing over two years confirms significant gains in
engagement and profit-based metrics across desktop and mobile app channels. Our
results show that a flexible, scalable architecture can serve diverse user
intents with minimal maintenance overhead.

</details>


### [184] [Evaluating Generative AI Tools for Personalized Offline Recommendations: A Comparative Study](https://arxiv.org/abs/2508.03710)
*Rafael Salinas-Buestan,Otto Parra,Nelly Condori-Fernandez,Maria Fernanda Granda*

Main category: cs.IR

TL;DR: 研究评估了五种生成式AI工具在推荐非数字活动时的性能和用户满意度，针对有重复性劳损风险的个体。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在健康行为干预中的有效性尚未充分探索，尤其是在减少技术使用的领域。

Method: 采用GQM范式，通过预定义用户档案和干预场景，评估工具的性能（精确度、召回率等）和用户满意度。

Result: 研究回答了两个问题：哪种工具推荐最准确（RQ1），以及工具选择如何影响用户满意度（RQ2）。

Conclusion: 研究填补了生成式AI在健康干预中的空白，为未来工具优化提供了方向。

Abstract: Background: Generative AI tools have become increasingly relevant in
supporting personalized recommendations across various domains. However, their
effectiveness in health-related behavioral interventions, especially those
aiming to reduce the use of technology, remains underexplored. Aims: This study
evaluates the performance and user satisfaction of the five most widely used
generative AI tools when recommending non-digital activities tailored to
individuals at risk of repetitive strain injury. Method: Following the
Goal/Question/Metric (GQM) paradigm, this proposed experiment involves
generative AI tools that suggest offline activities based on predefined user
profiles and intervention scenarios. The evaluation is focused on quantitative
performance (precision, recall, F1-score and MCC-score) and qualitative aspects
(user satisfaction and perceived recommendation relevance). Two research
questions were defined: RQ1 assessed which tool delivers the most accurate
recommendations, and RQ2 evaluated how tool choice influences user
satisfaction.

</details>


### [185] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 提出了一种基于语言模型的系统，用于从社交媒体内容中检测和分类房地产相关事件，支持城市管理。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台如Twitter和Facebook已成为日常生活的一部分，为识别房地产相关问题提供了宝贵资源。

Method: 采用分层分类框架筛选相关帖子，并使用基于Transformer的地理定位模块推断无明确地理标签的帖子位置。

Result: 系统能够提供及时的数据驱动见解，支持城市管理、操作响应和情境感知。

Conclusion: 该集成方法为房地产相关事件的检测和分类提供了有效解决方案。

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [186] [Measuring the stability and plasticity of recommender systems](https://arxiv.org/abs/2508.03941)
*Maria João Lavoura,Robert Jungnickel,João Vinagre*

Main category: cs.IR

TL;DR: 论文提出了一种评估推荐模型长期行为的方法，关注模型的稳定性和可塑性，并通过实验验证了不同算法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的离线评估协议仅能反映模型在特定时间点的性能，无法捕捉模型在动态变化环境中的长期表现。

Method: 提出了一种离线评估协议，用于分析模型在重新训练时的稳定性和可塑性，并通过GoodReads数据集验证了三种算法的表现。

Result: 实验结果表明不同算法在稳定性和可塑性上存在差异，并可能需要在两者之间进行权衡。

Conclusion: 该框架为理解推荐模型的长期动态行为提供了有价值的工具，未来需要更多实验验证。

Abstract: The typical offline protocol to evaluate recommendation algorithms is to
collect a dataset of user-item interactions and then use a part of this dataset
to train a model, and the remaining data to measure how closely the model
recommendations match the observed user interactions. This protocol is
straightforward, useful and practical, but it only captures performance of a
particular model trained at some point in the past. We know, however, that
online systems evolve over time. In general, it is a good idea that models
reflect such changes, so models are frequently retrained with recent data. But
if this is the case, to what extent can we trust previous evaluations? How will
a model perform when a different pattern (re)emerges? In this paper we propose
a methodology to study how recommendation models behave when they are
retrained. The idea is to profile algorithms according to their ability to, on
the one hand, retain past patterns -- stability -- and, on the other hand,
(quickly) adapt to changes -- plasticity. We devise an offline evaluation
protocol that provides detail on the long-term behavior of models, and that is
agnostic to datasets, algorithms and metrics. To illustrate the potential of
this framework, we present preliminary results of three different types of
algorithms on the GoodReads dataset that suggest different stability and
plasticity profiles depending on the algorithmic technique, and a possible
trade-off between stability and plasticity.Although additional experiments will
be necessary to confirm these observations, they already illustrate the
usefulness of the proposed framework to gain insights on the long term dynamics
of recommendation models.

</details>


### [187] [Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics](https://arxiv.org/abs/2508.04419)
*Jarne Mathi Decker,Joeran Beel*

Main category: cs.IR

TL;DR: 论文提出了一种结合用户特征和算法特征的元学习方法，用于推荐系统的算法选择，实验表明该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中算法选择的问题，传统方法忽略了算法的内在特性，而新方法通过提取算法特征来改进性能。

Method: 提出了一种基于用户元特征和从源代码自动提取的算法特征的元学习方法。

Result: 在六个数据集上的实验显示，加入算法特征后，NDCG@10性能提升了8.83%，优于基线方法。

Conclusion: 静态源代码指标提供了有价值的预测信号，为构建更智能的推荐系统提供了新方向。

Abstract: The Algorithm Selection Problem for recommender systems-choosing the best
algorithm for a given user or context-remains a significant challenge.
Traditional meta-learning approaches often treat algorithms as categorical
choices, ignoring their intrinsic properties. Recent work has shown that
explicitly characterizing algorithms with features can improve model
performance in other domains. Building on this, we propose a per-user
meta-learning approach for recommender system selection that leverages both
user meta-features and automatically extracted algorithm features from source
code. Our preliminary results, averaged over six diverse datasets, show that
augmenting a meta-learner with algorithm features improves its average NDCG@10
performance by 8.83% from 0.135 (user features only) to 0.147. This enhanced
model outperforms the Single Best Algorithm baseline (0.131) and successfully
closes 10.5% of the performance gap to a theoretical oracle selector. These
findings show that even static source code metrics provide a valuable
predictive signal, presenting a promising direction for building more robust
and intelligent recommender systems.

</details>


### [188] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 论文研究了多模态推荐系统中嵌入表示的作用，提出利用大型视觉语言模型（LVLMs）生成语义对齐的多模态嵌入，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态推荐系统的性能提升是否源于真正的多模态理解还是模型复杂性，并解决现有方法中跨模态对齐不足的问题。

Method: 利用LVLMs通过结构化提示生成多模态嵌入，无需融合策略，直接获得语义对齐的表示。

Result: 实验表明LVLMs嵌入显著提升推荐性能，并能解码为结构化文本描述，验证了其语义深度和对齐性。

Conclusion: 研究强调了语义丰富表示的重要性，并认为LVLMs是构建稳健多模态表示的有力工具。

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [189] [A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature](https://arxiv.org/abs/2508.04612)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.IR

TL;DR: 提出了一种开源、可复现的自动化流程，用于检索、筛选、提取和分析论文，支持实验复现。


<details>
  <summary>Details</summary>
Motivation: 随着自回归生成模型研究的快速发展，手动文献调查和复现研究变得不切实际，需要自动化工具。

Method: 开发了一个开源流程，包括文档检索、相关性筛选、元数据提取、参数和结果提取、主题聚类、摘要生成以及实验脚本容器化。

Result: 在50篇手动标注论文上，相关性分类、参数提取和引用识别的F1分数超过0.85；在1000篇论文上展示近线性扩展性；三个案例研究验证了复现的准确性。

Conclusion: 该流程高效且准确，支持大规模文献分析和实验复现。

Abstract: The accelerating pace of research on autoregressive generative models has
produced thousands of papers, making manual literature surveys and reproduction
studies increasingly impractical. We present a fully open-source, reproducible
pipeline that automatically retrieves candidate documents from public
repositories, filters them for relevance, extracts metadata, hyper-parameters
and reported results, clusters topics, produces retrieval-augmented summaries
and generates containerised scripts for re-running selected experiments.
Quantitative evaluation on 50 manually-annotated papers shows F1 scores above
0.85 for relevance classification, hyper-parameter extraction and citation
identification. Experiments on corpora of up to 1000 papers demonstrate
near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM
on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model
on the Lakh MIDI dataset -- confirm that the extracted settings support
faithful reproduction, achieving test perplexities within 1--3% of the original
reports.

</details>


### [190] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: QAM是一种混合框架，通过将开放文本查询分解为结构化元数据标签和语义元素，提升搜索精度和相关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统搜索方法在处理自由文本查询时的局限性，减少噪声并提高检索的针对性。

Method: 自动从自由文本查询中提取元数据过滤器，结合语义元素进行搜索。

Result: 在Amazon Toys Reviews数据集上，QAM的mAP@5达到52.99%，显著优于传统方法。

Conclusion: QAM是适用于企业搜索（尤其是电子商务系统）的强大解决方案。

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [191] [Predicting fall risk in older adults: A machine learning comparison of accelerometric and non-accelerometric factors](https://arxiv.org/abs/2508.03756)
*Ana González-Castro,José Alberto Benítez-Andrades,Rubén González-González,Camino Prada-García,Raquel Leirós-Rodríguez*

Main category: stat.AP

TL;DR: 研究使用多种机器学习模型预测老年人跌倒风险，结合加速度和非加速度数据表现最佳，贝叶斯岭回归准确率最高。


<details>
  <summary>Details</summary>
Motivation: 提高老年人跌倒风险预测的准确性，为预防策略提供依据。

Method: 使用146名参与者的加速度和非加速度数据训练多种机器学习模型，比较性能。

Result: 结合数据的模型表现最优，贝叶斯岭回归准确率最高（MSE=0.6746，R2=0.9941），非加速度变量（如年龄和合并症）对预测至关重要。

Conclusion: 支持使用综合数据和贝叶斯方法改进跌倒风险评估，指导预防措施。

Abstract: This study investigates fall risk prediction in older adults using various
machine learning models trained on accelerometric, non-accelerometric, and
combined data from 146 participants. Models combining both data types achieved
superior performance, with Bayesian Ridge Regression showing the highest
accuracy (MSE = 0.6746, R2 = 0.9941). Non-accelerometric variables, such as age
and comorbidities, proved critical for prediction. Results support the use of
integrated data and Bayesian approaches to enhance fall risk assessment and
inform prevention strategies.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [192] [Agentic-AI based Mathematical Framework for Commercialization of Energy Resilience in Electrical Distribution System Planning and Operation](https://arxiv.org/abs/2508.04170)
*Aniket Johri,Divyanshi Dwivedi,Mayukha Pal*

Main category: eess.SY

TL;DR: 提出了一种结合双智能体PPO和市场机制的新型框架，用于提升电力分配系统的经济可行性和动态适应性。


<details>
  <summary>Details</summary>
Motivation: 电力分配系统在极端天气和网络威胁下脆弱性增加，现有方法缺乏市场驱动的机制和动态适应性。

Method: 采用双智能体PPO方案，战略智能体选择最优DER配置，战术智能体调整开关状态和电网偏好。

Result: 平均韧性得分为0.85±0.08，效益成本比为0.12±0.01，市场激励可持续。

Conclusion: 该框架通过市场机制和智能决策，实现了韧性提升和经济可行性的平衡。

Abstract: The increasing vulnerability of electrical distribution systems to extreme
weather events and cyber threats necessitates the development of economically
viable frameworks for resilience enhancement. While existing approaches focus
primarily on technical resilience metrics and enhancement strategies, there
remains a significant gap in establishing market-driven mechanisms that can
effectively commercialize resilience features while optimizing their deployment
through intelligent decision-making. Moreover, traditional optimization
approaches for distribution network reconfiguration often fail to dynamically
adapt to both normal and emergency conditions. This paper introduces a novel
framework integrating dual-agent Proximal Policy Optimization (PPO) with
market-based mechanisms, achieving an average resilience score of 0.85 0.08
over 10 test episodes. The proposed architecture leverages a dual-agent PPO
scheme, where a strategic agent selects optimal DER-driven switching
configurations, while a tactical agent fine-tunes individual switch states and
grid preferences under budget and weather constraints. These agents interact
within a custom-built dynamic simulation environment that models stochastic
calamity events, budget limits, and resilience-cost trade-offs. A comprehensive
reward function is designed that balances resilience enhancement objectives
with market profitability (with up to 200x reward incentives, resulting in 85%
of actions during calamity steps selecting configurations with 4 DERs),
incorporating factors such as load recovery speed, system robustness, and
customer satisfaction. Over 10 test episodes, the framework achieved a
benefit-cost ratio of 0.12 0.01, demonstrating sustainable market incentives
for resilience investment. This framework creates sustainable market incentives

</details>


### [193] [A virtual sensor fusion approach for state of charge estimation of lithium-ion cells](https://arxiv.org/abs/2508.04268)
*Davide Previtali,Daniele Masti,Mirko Mazzoleni,Fabio Previdi*

Main category: eess.SY

TL;DR: 结合卡尔曼滤波与机器学习方法，提出一种锂离子电池SOC估计的虚拟传感器技术，通过数据驱动校准EKF噪声协方差矩阵，提高了估计精度和平滑性。


<details>
  <summary>Details</summary>
Motivation: 解决锂离子电池SOC估计问题，结合传统卡尔曼滤波与机器学习方法的优势，提升估计性能。

Method: 1. 从数据学习APV模型；2. 从APV模型推导线性观测器组；3. 结合观测器特征与输入输出数据训练机器学习模型预测SOC；4. 将SOC预测与电压测量输入EKF，并校准噪声协方差矩阵。

Result: 实验表明，该方法在SOC估计精度和平滑性上优于传统方法。

Conclusion: 结合数据驱动与模型驱动方法，显著提升了SOC估计性能。

Abstract: This paper addresses the estimation of the State Of Charge (SOC) of
lithium-ion cells via the combination of two widely used paradigms: Kalman
Filters (KFs) equipped with Equivalent Circuit Models (ECMs) and
machine-learning approaches. In particular, a recent Virtual Sensor (VS)
synthesis technique is considered, which operates as follows: (i) learn an
Affine Parameter-Varying (APV) model of the cell directly from data, (ii)
derive a bank of linear observers from the APV model, (iii) train a
machine-learning technique from features extracted from the observers together
with input and output data to predict the SOC. The SOC predictions returned by
the VS are supplied to an Extended KF (EKF) as output measurements along with
the cell terminal voltage, combining the two paradigms. A data-driven
calibration strategy for the noise covariance matrices of the EKF is proposed.
Experimental results show that the designed approach is beneficial w.r.t. SOC
estimation accuracy and smoothness.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [194] [Algebraically Observable Physics-Informed Neural Network and its Application to Epidemiological Modelling](https://arxiv.org/abs/2508.04590)
*Mizuka Komatsu*

Main category: cs.SC

TL;DR: PINN用于流行病学模型中的状态变量和参数估计，通过代数可观测性分析补充未测量数据，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决流行病学模型中部分轨迹数据无法测量的问题，提升状态变量和参数估计的精度。

Method: 提出基于代数可观测性分析的未测量数据补充方法，结合PINN框架进行估计。

Result: 在噪声和部分测量条件下，方法比传统方法更准确，适用于实际场景。

Conclusion: 该方法有效提升了流行病学模型中未测量状态和参数的估计精度，具有实际应用价值。

Abstract: Physics-Informed Neural Network (PINN) is a deep learning framework that
integrates the governing equations underlying data into a loss function. In
this study, we consider the problem of estimating state variables and
parameters in epidemiological models governed by ordinary differential
equations using PINNs. In practice, not all trajectory data corresponding to
the population described by models can be measured. Learning PINNs to estimate
the unmeasured state variables and epidemiological parameters using partial
measurements is challenging.
  Accordingly, we introduce the concept of algebraic observability of the state
variables. Specifically, we propose augmenting the unmeasured data based on
algebraic observability analysis. The validity of the proposed method is
demonstrated through numerical experiments under three scenarios in the context
of epidemiological modelling. Specifically, given noisy and partial
measurements, the accuracy of unmeasured states and parameter estimation of the
proposed method is shown to be higher than that of the conventional methods.
The proposed method is also shown to be effective in practical scenarios, such
as when the data corresponding to certain variables cannot be reconstructed
from the measurements.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [195] [Delving Deeper Into Astromorphic Transformers](https://arxiv.org/abs/2312.10925)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 论文探讨了将星形胶质细胞（占人脑细胞50%以上）引入神经形态计算以模拟Transformer自注意力机制的潜力，提出了生物合理的神经元-突触-星形胶质细胞网络模型，并在多个任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 星形胶质细胞在神经形态计算中的关键作用尚未充分探索，本文旨在深入研究其与神经元和突触的相互作用，以模拟Transformer的自注意力机制。

Method: 通过生物合理的Hebbian和突触前可塑性建模，结合非线性和反馈效应，将神经元-星形胶质细胞计算映射到自注意力机制，并在IMDB、CIFAR10和WikiText-2数据集上评估性能。

Result: Astromorphic Transformers在情感和图像分类任务中表现出更高的准确性和学习速度，在自然语言生成任务中实现了更低的困惑度，优于传统模型。

Conclusion: 该模型展示了星形胶质细胞在提升神经形态计算性能方面的潜力，为未来研究提供了新方向。

Abstract: Preliminary attempts at incorporating the critical role of astrocytes - cells
that constitute more than 50\% of human brain cells - in brain-inspired
neuromorphic computing remain in infancy. This paper seeks to delve deeper into
various key aspects of neuron-synapse-astrocyte interactions to mimic
self-attention mechanisms in Transformers. The cross-layer perspective explored
in this work involves bioplausible modeling of Hebbian and presynaptic
plasticities in neuron-astrocyte networks, incorporating effects of
non-linearities and feedback along with algorithmic formulations to map the
neuron-astrocyte computations to self-attention mechanism and evaluating the
impact of incorporating bio-realistic effects from the machine learning
application side. Our analysis on sentiment and image classification tasks
(IMDB and CIFAR10 datasets) highlights the advantages of Astromorphic
Transformers, offering improved accuracy and learning speed. Furthermore, the
model demonstrates strong natural language generation capabilities on the
WikiText-2 dataset, achieving better perplexity compared to conventional
models, thus showcasing enhanced generalization and stability across diverse
machine learning tasks.

</details>
