<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 9]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.RO](#cs.RO) [Total: 4]
- [eess.AS](#eess.AS) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 15]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.PL](#cs.PL) [Total: 3]
- [eess.IV](#eess.IV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [math.CO](#math.CO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 9]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Schema Inference for Tabular Data Repositories Using Large Language Models](https://arxiv.org/abs/2509.04632)
*Zhenyu Wu,Jiaoyan Chen,Norman W. Paton*

Main category: cs.DB

TL;DR: SI-LLM使用大语言模型从表格数据的列标题和单元格值推断简洁的概念模式，包括层次化实体类型、属性和类型间关系，在有限元数据的情况下取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 处理最小化整理的表格数据时，由于来源异构导致表示不一致，且元数据稀疏，使得模式推断变得困难。现有方法在元数据有限的情况下难以有效进行模式推断。

Method: 提出SI-LLM方法，仅使用列标题和单元格值，通过大语言模型推断出包含层次化实体类型、属性和类型间关系的概念模式。

Result: 在两个数据集（网络表格和开放数据）上的广泛评估显示，SI-LLM取得了有前景的端到端结果，在每个步骤上都达到或优于现有最先进方法。

Conclusion: SI-LLM为元数据有限的表格数据提供了有效的模式推断解决方案，所有源代码、完整提示和数据集均已开源。

Abstract: Minimally curated tabular data often contain representational inconsistencies
across heterogeneous sources, and are accompanied by sparse metadata. Working
with such data is intimidating. While prior work has advanced dataset discovery
and exploration, schema inference remains difficult when metadata are limited.
We present SI-LLM (Schema Inference using Large Language Models), which infers
a concise conceptual schema for tabular data using only column headers and cell
values. The inferred schema comprises hierarchical entity types, attributes,
and inter-type relationships. In extensive evaluation on two datasets from web
tables and open data, SI-LLM achieves promising end-to-end results, as well as
better or comparable results to state-of-the-art methods at each step. All
source code, full prompts, and datasets of SI-LLM are available at
https://github.com/PierreWoL/SILLM.

</details>


### [2] [Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach](https://arxiv.org/abs/2509.05129)
*Meihao Liao,Yueyang Pan,Rong-Hua Li,Guoren Wang*

Main category: cs.DB

TL;DR: 提出了TreeIndex方法，利用树分解技术为小树宽图（如道路网络）构建电阻距离标签索引，支持高效的精确查询


<details>
  <summary>Details</summary>
Motivation: 现有随机游走方法只能提供近似解且在小树宽图上效率低下，而最短路径距离计算利用割属性和树分解在这些图上表现优异，因此希望为电阻距离开发类似的高效精确方法

Method: 分析电阻距离的割属性，结合树分解技术发现电阻距离仅依赖于节点到树分解根路径上的标签，基于此构建紧凑的标签索引结构TreeIndex

Result: 在完整美国道路网络上构建405GB标签仅需7小时（单线程），单对查询耗时10^{-3}秒，单源查询耗时190秒，大幅优于现有方法

Conclusion: TreeIndex是首个可扩展到大型图的确切电阻距离计算方法，为小树宽图上的电阻距离计算提供了高效精确的解决方案

Abstract: Resistance distance computation is a fundamental problem in graph analysis,
yet existing random walk-based methods are limited to approximate solutions and
suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In
contrast, shortest-path distance computation achieves remarkable efficiency on
such graphs by leveraging cut properties and tree decompositions. Motivated by
this disparity, we first analyze the cut property of resistance distance. While
a direct generalization proves impractical due to costly matrix operations, we
overcome this limitation by integrating tree decompositions, revealing that the
resistance distance $r(s,t)$ depends only on labels along the paths from $s$
and $t$ to the root of the decomposition. This insight enables compact
labelling structures. Based on this, we propose \treeindex, a novel index
method that constructs a resistance distance labelling of size $O(n \cdot
h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where
$h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small
constants in many real-world small-treewidth graphs (e.g., road networks). Our
labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and
single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive
experiments show that TreeIndex substantially outperforms state-of-the-art
approaches. For instance, on the full USA road network, it constructs a $405$
GB labelling in $7$ hours (single-threaded) and answers exact single-pair
queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the
first exact method scalable to such large graphs.

</details>


### [3] [Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving](https://arxiv.org/abs/2509.02718)
*Fangzhou Wu,Sandeep Silwal*

Main category: cs.DB

TL;DR: 这篇论文提出了一种无需训练的在线LLM路由算法，通过近似最近邻搜索和一次性优化，在高查询量和令牌预算约束下实现來算效率和成本效益的显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型服务需求增长，部署和计算成本大幅上升。现有路由方案主要面向离线场景，无法满足在线高并发查询和令牌预算约束的需求。

Method: 算法利用近似最近邻搜索来高效估计查询特征，并在小规模初始查询集上进行一次性优化学习路由策略，用于指导未来路由。

Result: 算法在3个标准数据集和8个基线模型上进行了广泛实验，平均性能提升3.55倍，成本效率提升1.85倍，吞吐量提升近伥4.25倍。

Conclusion: 该方法为在线LLM路由提供了一种高效、无需训练的解决方案，在保持理论性能保障的同时实现了显著的实践效果。

Abstract: Increasing demand for Large Language Models (LLMs) services imposes
substantial deployment and computation costs on providers. LLM routing offers a
cost-efficient solution by directing queries to the optimal LLM based on model
and query features. However, existing works primarily focus on offline
scenarios and struggle to adapt to online settings with high query volume and
constrained token budgets. In this work, we introduce the first training-free
algorithm for online routing scenarios. Our algorithm leverages approximate
nearest neighbor search to efficiently estimate query features and performs a
one-time optimization over a small set of initial queries to learn a routing
strategy that guides future routing. We provide theoretical guarantees
demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$
under natural assumptions, which is further validated by extensive experiments
across 3 benchmark datasets and 8 baselines, showing an average improvement of
3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and
nearly 4.25$\times$ in throughput.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一个针对异构多GPU环境的扩散模型推理加速框架，通过时空自适应调度机制，在时间维度减少慢速GPU的去噪步骤，在空间维度弹性分配图像块，实现了45%的延迟降低和更好的负载均衡。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型并行推理方案在异构多GPU环境中资源利用率低，硬件能力差异和后台任务导致工作负载不均衡，需要更高效的并行推理技术来应对扩散模型的高计算成本。

Method: 提出时空自适应扩散推理框架STADI，包含：1）时间维度的计算感知步骤分配器，使用最小公倍数最小化量化技术减少慢速GPU的去噪步骤；2）空间维度的弹性块并行机制，根据GPU计算能力分配不同大小的图像块。

Result: 在负载不均衡和异构多GPU集群上的实验验证了STADI的有效性，相比最先进的块并行框架，端到端推理延迟降低高达45%，显著提高了异构GPU的资源利用率。

Conclusion: STADI框架通过时空维度的自适应调度，有效解决了异构环境下扩散模型推理的负载均衡问题，显著提升了推理效率和资源利用率。

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


### [5] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: VoltanaLLM是一个基于控制理论的SLO感知、节能LLM服务系统，通过频率缩放和请求路由的协同设计，在prefill/decode分离架构中实现细粒度控制，可节省高达36.3%的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互式应用中的广泛使用，其推理过程的高能耗成本成为可持续和成本效益部署的挑战，需要开发节能的LLM服务系统。

Method: 采用控制理论方法，设计反馈驱动的频率控制器动态调整GPU频率，以及状态空间路由器探索跨频率缩放实例的路由决策，在prefill/decode分离架构中实现相特定的精细控制。

Result: 在SGLang中实现并评估，在多个先进LLM和真实数据集上测试，结果显示能节省高达36.3%的能源，同时保持近乎完美的SLO达成率。

Conclusion: VoltanaLLM为可持续和智能的LLM服务铺平了道路，证明了通过系统级优化可以在不牺牲服务质量的前提下显著降低能耗。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [6] [Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.05216)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 多GPU扩展3D高斯散点技术，提升科学可视化数据集处理效率和规模


<details>
  <summary>Details</summary>
Motivation: 解决单GPU无法处理大规模科学数据集的问题，为HPC科学工作流集成3D高斯散点技术基础

Method: 基于Grendel-GS的多GPU训练后端，分布式优化算法，支持高分辨率重建

Result: 在Kingsnake数据集(4M高斯)上获得5.6倍速度提升，成功训练Miranda数据集(18M高斯)，超出单A100 GPU能力

Conclusion: 该方法为集成3D高斯散点技术到HPC科学工作流中提供了基础，支持复杂模拟的实时可视化

Abstract: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.

</details>


### [7] [Dynamic reconfiguration for malleable applications using RMA](https://arxiv.org/abs/2509.05248)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo*

Main category: cs.DC

TL;DR: 本文研究了基于MPI远程内存访问(RMA)操作的新型单边通信方法，用于可扩展应用的动态调整大小，通过最小化对应用执行的影响实现数据重分布。


<details>
  <summary>Details</summary>
Motivation: 传统基于集合操作的动态调整方法对应用执行影响较大，需要探索更高效的通信方法来支持可扩展应用的动态重构。

Method: 将基于RMA的单边通信方法集成到MaM库中，并与传统集合方法进行比较，同时扩展Wait Drains策略以支持高效后台重构。

Result: 新方法表现出可比的性能，但目前较高的初始化成本限制了其优势。

Conclusion: 基于RMA的单边通信方法在动态调整方面具有潜力，但需要进一步优化初始化开销才能充分发挥优势。

Abstract: This paper investigates the novel one-sided communication methods based on
remote memory access (RMA) operations in MPI for dynamic resizing of malleable
applications, enabling data redistribution with minimal impact on application
execution. After their integration into the MaM library, these methods are
compared with traditional collective-based approaches. In addition, the
existing strategy Wait Drains is extended to support efficient background
reconfiguration. Results show comparable performance, though high
initialization costs currently limit their advantage.

</details>


### [8] [Scaling Performance of Large Language Model Pretraining](https://arxiv.org/abs/2509.05258)
*Alexander Interrante-Grant,Carla Varela-Rosa,Suhaas Narayan,Chris Connelly,Albert Reuther*

Main category: cs.DC

TL;DR: 本文揭秘大型语言模型预训练流程的具体技术细节，重点关注分布式训练、大规模数据集管理和数据并行扩展等问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练需要巨额计算资源，但相关扩展性能和训练考虑因素在公开文献中缺乏，实践建议少见，需要揭秘这些技术细节

Method: 重点研究分布式训练技术、在百余个节点上管理大规模数据集的方法、以及通过数据并行扩展充分利用GPU计算能力的策略

Result: 提供了大型语言模型预训练流程的具体技术解决方案和实践建议，填补了公开文献在这一领域的空白

Conclusion: 本文成功揭秘了大型语言模型预训练流程的关键技术细节，为进阶AI研究公司提供了重要的实践指南，有助于推动大模型训练技术的发展

Abstract: Large language models (LLMs) show best-in-class performance across a wide
range of natural language processing applications. Training these models is an
extremely computationally expensive task; frontier Artificial Intelligence (AI)
research companies are investing billions of dollars into supercomputing
infrastructure to train progressively larger models on increasingly massive
datasets. Unfortunately, information about the scaling performance and training
considerations of these large training pipelines is scarce in public
literature. Working with large-scale datasets and models can be complex and
practical recommendations are scarce in the public literature for tuning
training performance when scaling up large language models. In this paper, we
aim to demystify the large language model pretraining pipeline somewhat - in
particular with respect to distributed training, managing large datasets across
hundreds of nodes, and scaling up data parallelism with an emphasis on fully
leveraging available GPU compute capacity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [Transitivity Preserving Projection in Directed Hypergraphs](https://arxiv.org/abs/2509.04543)
*Eric Parsonage,Matthew Roughan,Hung X Nguyen*

Main category: cs.DS

TL;DR: 提出了一种新的传递性保持投影(TPP)方法，用于有向超图的可视化分析，相比传统的Basu和 Blanning投影(BBP)方法，TPP能够保持传递关系的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 有向超图在多个领域中用于建模复杂的多元关系，但其固有的复杂性阻碍了有效的可视化和分析，特别是对于大型图。现有的BBP方法计算密集且可能增加复杂性。

Method: 开发了TPP方法，通过捕获不可约的主导元路径来提供最小且完整的关系表示，使用set-trie数据结构实现高效算法，将计算复杂度从指数级降低到线性级。

Result: 实验结果显示TPP性能优越，在BBP无法在24小时内完成的图上，TPP能在几秒钟内完成投影。

Conclusion: TPP方法为有向超图提供了最小但完整的关系视图，支持网络安全和供应链等应用领域的分析需求。

Abstract: Directed hypergraphs are vital for modeling complex polyadic relationships in
domains such as discrete mathematics, computer science, network security, and
systems modeling. However, their inherent complexity often impedes effective
visualization and analysis, particularly for large graphs. This paper
introduces a novel Transitivity Preserving Projection (TPP) to address the
limitations of the computationally intensive Basu and Blanning projection
(BBP), which can paradoxically increase complexity by flattening transitive
relationships. TPP offers a minimal and complete representation of
relationships within a chosen subset of elements, capturing only irreducible
dominant metapaths to ensure the smallest set of edges while preserving all
essential transitive and direct connections. This approach significantly
enhances visualization by reducing edge proliferation and maintains the
integrity of the original hypergraph's structure. We develop an efficient
algorithm leveraging the set-trie data structure, reducing the computational
complexity from an exponential number of metapath searches in BBP to a linear
number of metapath searches with polynomial-time filtering, enabling
scalability for real-world applications. Experimental results demonstrate TPP's
superior performance, completing projections in seconds on graphs where BBP
fails to terminate within 24 hours. By providing a minimal yet complete view of
relationships, TPP supports applications in network security and supply

</details>


### [10] [Additive, Near-Additive, and Multiplicative Approximations for APSP in Weighted Undirected Graphs: Trade-offs and Algorithms](https://arxiv.org/abs/2509.04640)
*Liam Roditty,Ariel Sapir*

Main category: cs.DS

TL;DR: 本文提出了几个新的近似全对最短路径(APASP)算法，包括稠密加权图的+2∑W_i-APASP算法、近乎加性APASP算法和改进的乘法APASP算法框架，突破了原有的条件下限限制。


<details>
  <summary>Details</summary>
Motivation: 现有的APASP算法在处理稠密加权图时存在效率限制，且对于近乎加性和乘法近似的最短路径问题需要更好的算法性能。本文旨在改进这些算法的运行时间和近似质量。

Method: 提出了多种APASP算法：1) 稠密加权图的+2∑W_i-APASP算法；2) 近乎加性(1+ε, min{2W₁,4W₂})-APASP算法；3) 乘法((3ℓ+4)/(ℓ+2)+ε)-APASP算法框架；4) 通过允许加法项来绕过条件下限限制。

Result: 获得了显著的运行时间改进：稠密图运行时间为Õ(n²⁺¹/(³ᵏ⁺²))，近乎加性算法运行时间为Õ((1/ε)^O(1)·n².¹⁵¹³⁵³¹³·log W)，基础乘法近似达到7/3+ε近似。

Conclusion: 本文在APASP领域取得了重要进展，提供了更高效的稠密加权图算法，改进了近乎加性和乘法近似算法，并成功绕过了原有的条件计算下限，为近似最短路径算法开辟了新方向。

Abstract: We present a $+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm for dense weighted
graphs with runtime $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$, where $W_{i}$
is the weight of an $i^\textnormal{th}$ heaviest edge on a shortest path. Dor,
Halperin and Zwick [FOCS'96, SICOMP'00] had two algorithms for the commensurate
unweighted $+2\cdot\left( k+1\right)$-APASP: $\tilde
O\left(n^{2-\frac{1}{k+2}}m^{\frac{1}{k+2}}\right)$ runtime for sparse graphs
and $\tilde O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime for dense graphs. Cohen
and Zwick [SODA'97, JALG'01] adapted the sparse variant to weighted graphs:
$+2\sum_{i=1}^{k+1}{W_i}$-APASP algorithm in the same runtime. We show an
algorithm for dense weighted graphs.
  For \emph{nearly additive} APASP, we present a
$\left(1+\varepsilon,\min{\left\{2W_1,4W_{2}\right\}}\right)$-APASP algorithm
with $\tilde O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot
n^{2.15135313}\cdot\log W\right)$ runtime. This improves the
$\left(1+\varepsilon,2W_1\right)$-APASP of Saha and Ye [SODA'24].
  For multiplicative APASP, we show a framework of $\left(\frac{3\ell +4}{\ell
+ 2}+\varepsilon\right)$-APASP algorithms, reducing the runtime of Akav and
Roditty [ESA'21] for dense graphs and generalizing the
$\left(2+\varepsilon\right)$-APASP algorithm of Dory et al [SODA'24]. Our base
case is a $\left(\frac{7}{3}+\varepsilon\right)$-APASP in $\tilde
O\left(\left(\frac{1}{\varepsilon}\right)^{O\left(1\right)}\cdot
n^{2.15135313}\cdot \log W\right)$ runtime, improving the $\frac{7}{3}$-APASP
algorithm of Baswana and Kavitha [FOCS'06, SICOMP'10] for dense graphs.
  Finally, we "bypass" an $\tilde \Omega \left(n^\omega\right)$ conditional
lower bound by Dor, Halperin, and Zwick for $\alpha$-APASP with $\alpha < 2$,
by allowing an additive term (e.g.
$\paren{\frac{6k+3}{3k+2},\sum_{i=1}^{k+1}{W_{i}}}$-APASP in $\tilde
O\left(n^{2+\frac{1}{3k+2}}\right)$ runtime.).

</details>


### [11] [A 13/6-Approximation for Strip Packing via the Bottom-Left Algorithm](https://arxiv.org/abs/2509.04654)
*Stefan Hougardy,Bart Zondervan*

Main category: cs.DS

TL;DR: 提出了新的矩形排序方法，使Bottom-Left算法在条带装箱问题中的近似比从3改进到13/6


<details>
  <summary>Details</summary>
Motivation: Bottom-Left算法是条带装箱问题的经典启发式算法，45年来其3的近似比一直未被改进，需要寻找更好的排序策略

Method: 引入新的矩形排序方法，结合Bottom-Left算法进行矩形放置

Result: 使用新排序方法后，Bottom-Left算法的近似比从3降低到13/6

Conclusion: 通过创新的排序策略，显著改进了经典Bottom-Left算法的性能表现

Abstract: In the Strip Packing problem, we are given a vertical strip of fixed width
and unbounded height, along with a set of axis-parallel rectangles. The task is
to place all rectangles within the strip, without overlaps, while minimizing
the height of the packing. This problem is known to be NP-hard. The Bottom-Left
Algorithm is a simple and widely used heuristic for Strip Packing. Given a
fixed order of the rectangles, it places them one by one, always choosing the
lowest feasible position in the strip and, in case of ties, the leftmost one.
Baker, Coffman, and Rivest proved in 1980 that the Bottom-Left Algorithm has
approximation ratio 3 if the rectangles are sorted by decreasing width. For the
past 45 years, no alternative ordering has been found that improves this bound.
We introduce a new rectangle ordering and show that with this ordering the
Bottom-Left Algorithm achieves a 13/6 approximation for the Strip Packing
problem.

</details>


### [12] [Parameterized Approximability for Modular Linear Equations](https://arxiv.org/abs/2509.04976)
*Konrad K. Dabrowski,Peter Jonsson,Sebastian Ordyniak,George Osipov,Magnus Wahlström*

Main category: cs.DS

TL;DR: 本文研究了模m线性方程组的最小不可满足子集删除问题，提出了针对Z_{p^n}环上二元线性方程的2倍FPT近似算法，并给出了相关下界证明。


<details>
  <summary>Details</summary>
Motivation: Min-r-Lin(Z_m)问题是NP难且UGC难近似的问题，即使在r=m=2时也难以在多项式时间内获得常数因子近似。之前的研究表明当m是素数时该问题属于FPT类，但非素数幂时是W[1]-难的。

Method: 通过逐步求解更紧的松弛问题来减少变量的可能取值集合。在Z_{p^n}环上，将值表示为p进制形式，松弛问题固定尾随零的个数和最低有效非零数字。通过构建特定图结构，将解识别为特定的割集集合，并使用基于影子移除的策略来计算近似解。

Result: 证明了Min-2-Lin(Z_{p^n})可在因子2内FPT近似，这意味着Min-2-Lin(Z_m)可在2ω(m)因子内FPT近似，其中ω(m)是m的不同素因子的数量。

Conclusion: 该算法为模m线性方程组的最小不可满足子集删除问题提供了有效的参数化近似方案，同时通过下界证明了对于三元线性方程或某些有限交换环上的二元线性方程，常数因子FPT近似是不可能的。

Abstract: We consider the Min-$r$-Lin$(Z_m)$ problem: given a system $S$ of length-$r$
linear equations modulo $m$, find $Z \subseteq S$ of minimum cardinality such
that $S-Z$ is satisfiable. The problem is NP-hard and UGC-hard to approximate
in polynomial time within any constant factor even when $r = m = 2$. We focus
on parameterized approximation with solution size as the parameter. Dabrowski
et al. showed that Min-$2$-Lin$(Z_m)$ is in FPT if $m$ is prime (i.e. $Z_m$ is
a field), and it is W[1]-hard if $m$ is not a prime power. We show that
Min-$2$-Lin$(Z_{p^n})$ is FPT-approximable within a factor of $2$ for every
prime $p$ and integer $n \geq 2$. This implies that Min-$2$-Lin$(Z_m)$, $m \in
Z^+$, is FPT-approximable within a factor of $2\omega(m)$ where $\omega(m)$
counts the number of distinct prime divisors of $m$. The idea behind the
algorithm is to solve ever tighter relaxations of the problem, decreasing the
set of possible values for the variables at each step. Working over $Z_{p^n}$
and viewing the values in base-$p$, one can roughly think of a relaxation as
fixing the number of trailing zeros and the least significant nonzero digits of
the values assigned to the variables. To solve the relaxed problem, we
construct a certain graph where solutions can be identified with a particular
collection of cuts. The relaxation may hide obstructions that will only become
visible in the next iteration of the algorithm, which makes it difficult to
find optimal solutions. To deal with this, we use a strategy based on shadow
removal to compute solutions that (1) cost at most twice as much as the optimum
and (2) allow us to reduce the set of values for all variables simultaneously.
We complement the algorithmic result with two lower bounds, ruling out
constant-factor FPT-approximation for Min-$3$-Lin$(R)$ over any nontrivial ring
$R$ and for Min-$2$-Lin$(R)$ over some finite commutative rings $R$.

</details>


### [13] [Graph Reconstruction with a Connected Components Oracle](https://arxiv.org/abs/2509.05002)
*Juha Harviainen,Pekka Parviainen*

Main category: cs.DS

TL;DR: 该论文研究图重构问题，提出了一种新的连通分量查询oracle，并给出了自适应随机算法的最优查询复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 研究图重构问题中不同oracle的能力，特别关注连通分量查询oracle在查询复杂度方面的表现，旨在建立最优查询复杂度界限。

Method: 提出使用连通分量查询oracle，通过自适应随机算法来重构隐藏图。算法复杂度分析基于图的顶点数、边数、最大度数和树宽等参数。

Result: 1. 对于具有n个顶点、m条边、最大度Δ和树宽k的隐藏图，GR问题可以通过自适应随机算法在O(min{m, Δ², k²}·log n)次CC查询内解决。
2. 证明了任何算法都无法在o(min{m, Δ², k²})次CC查询内解决GR问题。

Conclusion: 该论文建立了连通分量查询oracle在图重构问题中的最优查询复杂度界限，证明了所提算法的查询复杂度是最优的，为图重构问题的研究提供了重要的理论结果。

Abstract: In the Graph Reconstruction (GR) problem, the goal is to recover a hidden
graph by utilizing some oracle that provides limited access to the structure of
the graph. The interest is in characterizing how strong different oracles are
when the complexity of an algorithm is measured in the number of performed
queries. We study a novel oracle that returns the set of connected components
(CC) on the subgraph induced by the queried subset of vertices. Our main
contributions are as follows:
  1. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$,
and treewidth $k$, GR can be solved in $O(\min\{m, \Delta^2, k^2\} \cdot \log
n)$ CC queries by an adaptive randomized algorithm.
  2. For a hidden graph with $n$ vertices, $m$ edges, maximum degree $\Delta$,
and treewidth $k$, no algorithm can solve GR in $o(\min\{m, \Delta^2, k^2\})$
CC queries.

</details>


### [14] [On approximating the $f$-divergence between two Ising models](https://arxiv.org/abs/2509.05016)
*Weiming Feng,Yucheng Fu*

Main category: cs.DS

TL;DR: 本文研究了Ising模型之间f-散度的近似计算问题，针对χ^α-散度建立了算法和计算复杂度结果，算法参数范围与困难性结果匹配，并可扩展到其他f-散度。


<details>
  <summary>Details</summary>
Motivation: f-散度是衡量两个分布差异的基本概念，本文旨在解决Ising模型之间f-散度的近似计算问题，这是对最近TV距离近似工作的推广。

Method: 给定由交互矩阵和外场指定的两个Ising模型ν和μ，研究在任意相对误差e^(±ε)内近似f-散度D_f(ν‖μ)的问题。针对常数整数α的χ^α-散度，建立了算法和困难性结果。

Result: 算法在参数范围内工作，与困难性结果匹配。算法可扩展到其他f-散度，包括α-散度、KL散度、Rényi散度、Jensen-Shannon散度和平方Hellinger距离。

Conclusion: 本文为Ising模型之间的f-散度近似提供了完整的算法和复杂性理论框架，算法具有广泛的适用性，覆盖了多种重要的散度度量。

Abstract: The $f$-divergence is a fundamental notion that measures the difference
between two distributions. In this paper, we study the problem of approximating
the $f$-divergence between two Ising models, which is a generalization of
recent work on approximating the TV-distance. Given two Ising models $\nu$ and
$\mu$, which are specified by their interaction matrices and external fields,
the problem is to approximate the $f$-divergence $D_f(\nu\,\|\,\mu)$ within an
arbitrary relative error $\mathrm{e}^{\pm \varepsilon}$. For
$\chi^\alpha$-divergence with a constant integer $\alpha$, we establish both
algorithmic and hardness results. The algorithm works in a parameter regime
that matches the hardness result. Our algorithm can be extended to other
$f$-divergences such as $\alpha$-divergence, Kullback-Leibler divergence,
R\'enyi divergence, Jensen-Shannon divergence, and squared Hellinger distance.

</details>


### [15] [Testing Depth First Search Numbering](https://arxiv.org/abs/2509.05132)
*Artur Czumaj,Christian Sohler,Stefan Walzer*

Main category: cs.DS

TL;DR: 这篇论文引入了带有标签查询的有界度图模型新变体，研究DFS遍历发现时间的子线性查询复杂度测试算法。


<details>
  <summary>Details</summary>
Motivation: 动机来自于查询DFS遍历过程中分配的连续标签，研究哪些编号方式可以在子线性时间内测试。

Method: 提出新的带标签查询的有界度图模型，设计了DFS发现时间测试算法，支持查询顶点标签。

Result: 实现了查询复杂度O(n^{1/3}/ε)的算法，并证明对于常数ε>0存在匹配的下界。

Conclusion: 证明了DFS发现时间在新模型下可以以子线性查询复杂度测试，为进一步研究图过程编号测试奠定了基础。

Abstract: Property Testing is a formal framework to study the computational power and
complexity of sampling from combinatorial objects. A central goal in standard
graph property testing is to understand which graph properties are testable
with sublinear query complexity. Here, a graph property P is testable with a
sublinear query complexity if there is an algorithm that makes a sublinear
number of queries to the input graph and accepts with probability at least 2/3,
if the graph has property P, and rejects with probability at least 2/3 if it is
$\varepsilon$-far from every graph that has property P.
  In this paper, we introduce a new variant of the bounded degree graph model.
In this variant, in addition to the standard representation of a bounded degree
graph, we assume that every vertex $v$ has a unique label num$(v)$ from $\{1,
\dots, |V|\}$, and in addition to the standard queries in the bounded degree
graph model, we also allow a property testing algorithm to query for the label
of a vertex (but not for a vertex with a given label).
  Our new model is motivated by certain graph processes such as a DFS
traversal, which assign consecutive numbers (labels) to the vertices of the
graph. We want to study which of these numberings can be tested in sublinear
time. As a first step in understanding such a model, we develop a
\emph{property testing algorithm for discovery times of a DFS traversal} with
query complexity $O(n^{1/3}/\varepsilon)$ and for constant $\varepsilon>0$ we
give a matching lower bound.

</details>


### [16] [Efficient Contractions of Dynamic Graphs -- with Applications](https://arxiv.org/abs/2509.05157)
*Monika Henzinger,Evangelos Kosinas,Robin Münk,Harald Räcke*

Main category: cs.DS

TL;DR: 提出了一种用于全动态图的灵活数据结构，能够高效地在更新序列中的任何时刻提供非平凡最小割稀疏化器，支持快速边插入/删除操作，并应用于最小割仙人掌表示和最大k边连通子图计算。


<details>
  <summary>Details</summary>
Motivation: 为了解决全动态图中非平凡最小割稀疏化器的高效维护问题，支持在动态更新过程中快速获取稀疏表示，从而应用于最小割分析和连通性计算。

Method: 使用简单的动态森林数据结构，实现从零开始的快速稀疏化器构建。根据对手强度和所需时间界限类型，提供不同的性能保证，包括最坏情况时间和摊销时间保证。

Result: 数据结构支持在n^o(1)最坏情况下时间内插入/删除边，能够在Õ(n)时间内返回具有O(n/δ)顶点和O(n)边的NMC稀疏化器。应用于最小割仙人掌表示和最大k边连通子图计算，性能优于现有方法。

Conclusion: 该数据结构为全动态图提供了高效的NMC稀疏化解决方案，在对抗性环境和不同时间保证要求下均表现良好，为动态图的最小割分析和连通性计算提供了有效工具。

Abstract: A non-trivial minimum cut (NMC) sparsifier is a multigraph $\hat{G}$ that
preserves all non-trivial minimum cuts of a given undirected graph $G$. We
introduce a flexible data structure for fully dynamic graphs that can
efficiently provide an NMC sparsifier upon request at any point during the
sequence of updates. We employ simple dynamic forest data structures to achieve
a fast from-scratch construction of the sparsifier at query time. Based on the
strength of the adversary and desired type of time bounds, the data structure
comes with different guarantees. Specifically, let $G$ be a fully dynamic
simple graph with $n$ vertices and minimum degree $\delta$. Then our data
structure supports an insertion/deletion of an edge to/from $G$ in $n^{o(1)}$
worst-case time. Furthermore, upon request, it can return w.h.p. an NMC
sparsifier of $G$ that has $O(n/\delta)$ vertices and $O(n)$ edges, in
$\hat{O}(n)$ time. The probabilistic guarantees hold against an adaptive
adversary. Alternatively, the update and query times can be improved to
$\tilde{O}(1)$ and $\tilde{O}(n)$ respectively, if amortized-time guarantees
are sufficient, or if the adversary is oblivious.
  We discuss two applications of our data structure. First, it can be used to
efficiently report a cactus representation of all minimum cuts of a fully
dynamic simple graph. Using the NMC sparsifier we can w.h.p. build this cactus
in worst-case time $\hat{O}(n)$ against an adaptive adversary. Second, our data
structure allows us to efficiently compute the maximal $k$-edge-connected
subgraphs of undirected simple graphs, by repeatedly applying a minimum cut
algorithm on the NMC sparsifier. Specifically, we can compute w.h.p. the
maximal $k$-edge-connected subgraphs of a simple graph with $n$ vertices and
$m$ edges in $\tilde{O}(m+n^2/k)$ time which is an improvement for $k =
\Omega(n^{1/8})$ and works for fully dynamic graphs.

</details>


### [17] [List Decoding Expander-Based Codes via Fast Approximation of Expanding CSPs: I](https://arxiv.org/abs/2509.05203)
*Fernando Granha Jeronimo,Aman Singh*

Main category: cs.DS

TL;DR: 本文提出了基于扩展码的线性时间列表解码算法，包括三种不同类型的LDPC码和AEL码的高效解码方案


<details>
  <summary>Details</summary>
Motivation: 为了解决扩展码构造的高效列表解码问题，传统方法在时间复杂度上存在瓶颈，需要开发近线性时间的解码算法

Method: 将解码任务建模为扩展图上的协议CSP问题，利用[Jer23]中的快速近似算法和弱正则分解技术，通过枚举分解部分的常数赋值来恢复码字

Result: 成功实现了三种码构造的(δ-ε, O_ε(1))列表解码，时间复杂度为Õ_ε(n)，字母表大小分别为O_δ(1)、exp(poly(1/ε))和exp(exp(poly(1/ε)))

Conclusion: 该方法为扩展码提供了高效的近线性时间列表解码算法，显著提升了解码效率，为实际应用提供了理论保障

Abstract: We present near-linear time list decoding algorithms (in the block-length
$n$) for expander-based code constructions. More precisely, we show that
  (i) For every $\delta \in (0,1)$ and $\epsilon > 0$, there is an explicit
family of good Tanner LDPC codes of (design) distance $\delta$ that is $(\delta
- \epsilon, O_\varepsilon(1))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size $O_\delta(1)$,
  (ii) For every $R \in (0,1)$ and $\epsilon > 0$, there is an explicit family
of AEL codes of rate $R$, distance $1-R -\varepsilon$ that is $(1-R-\epsilon,
O_\varepsilon(1))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size
$\text{exp}(\text{poly}(1/\epsilon))$, and
  (iii) For every $R \in (0,1)$ and $\epsilon > 0$, there is an explicit family
of AEL codes of rate $R$, distance $1-R-\varepsilon$ that is $(1-R-\epsilon,
O(1/\epsilon))$ list decodable in time
$\widetilde{\mathcal{O}}_{\varepsilon}(n)$ with alphabet size
$\text{exp}(\text{exp}(\text{poly}(1/\epsilon)))$ using recent near-optimal
list size bounds from [JMST25].
  Our results are obtained by phrasing the decoding task as an agreement CSP
[RWZ20,DHKNT19] on expander graphs and using the fast approximation algorithm
for $q$-ary expanding CSPs from [Jer23], which is based on weak regularity
decomposition [JST21,FK96]. Similarly to list decoding $q$-ary Ta-Shma's codes
in [Jer23], we show that it suffices to enumerate over assignments that are
constant in each part (of the constantly many) of the decomposition in order to
recover all codewords in the list.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 使用LLM自动生成测试架构，评估四款模型在Ruby RSpec测试生成中的表现，DeepSeek在维护性和结构方面最优


<details>
  <summary>Details</summary>
Motivation: 手动创建测试架构耗时易错，特别在教育和大规模开发环境中，需要自动化解决方案

Method: 评估GPT-4、DeepSeek-Chat、Llama4-Maverick、Gemma2-9B四款LLM生成RSpec测试架构的能力，通过静态分析和盲测专家评审进行质量评估

Result: DeepSeek生成了最可维护和结构良好的测试架构，GPT-4输出更完整但不符合测试约定，提示设计和上下文输入是关键质量因素

Conclusion: LLM可以有效生成测试架构，但不同模型在代码理解和测试规范方面存在显著差异，需要经过谨慎的提示设计

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [19] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: PICO-TINYML-BENCHMARK是一个模块化、平台无关的框架，用于在资源受限的嵌入式系统上基准测试TinyML模型的实时性能，评估推理延迟、CPU利用率、内存效率和预测稳定性等关键指标。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的嵌入式系统上评估TinyML模型的实时性能，提供计算权衡和平台特定优化的见解，弥合理论进展与实际应用之间的差距。

Method: 开发了一个模块化、平台无关的基准测试框架，在BeagleBone AI64和Raspberry Pi 4两种平台上对三种代表性TinyML模型（手势分类、关键词识别和MobileNet V2）进行基准测试，使用真实世界数据集评估关键性能指标。

Result: BeagleBone AI64在AI特定任务上表现出一致的推理延迟，而Raspberry Pi 4在资源效率和成本效益方面表现更优，揭示了关键的计算权衡。

Conclusion: 该框架为优化TinyML部署提供了可行的指导，有助于在实际嵌入式系统中更好地应用TinyML技术。

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [20] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ是一个多样性引导的量子程序测试框架，通过分布式测试用例生成和新颖性驱动评估，有效提升测试输入多样性并发现更多缺陷


<details>
  <summary>Details</summary>
Motivation: 随着量子计算发展，确保量子程序可靠性变得越来越重要。量子程序利用量子电路解决经典机器难以处理的问题，需要有效的测试方法来保证其正确性

Method: 结合分布式测试用例生成器和新颖性驱动评估模块。生成器通过变异电路参数产生多样量子态输入，评估器基于幅度、相位和纠缠等内部电路状态指标量化行为新颖性，选择映射到度量空间中较少覆盖区域的输入

Result: 在不同规模和复杂度的量子程序上评估NovaQ，实验结果显示其始终比现有基线方法获得更高的测试输入多样性并检测到更多缺陷

Conclusion: NovaQ框架通过多样性引导的测试方法有效探索未充分测试的程序行为，为量子程序可靠性保障提供了有效解决方案

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [21] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: 利用LLM将代码变更从资源丰富语言翻译到新兴语言，生成合成训练数据来解决新语言标签数据稀缺问题，实现代码审查推荐系统的自动化。


<details>
  <summary>Details</summary>
Motivation: 新编程语言和框架的出现造成了标签数据不足的瓶颈，虽然有大量未标注代码，但缺乏足够的标签数据来训练监督模型进行审查分类。

Method: 使用大语言模型(LLM)将代码变更从资源丰富语言翻译为等价的新兴语言变更，生成合成训练数据，然后训练监督分类器。

Result: 在多个GitHub仓库和语言对中进行实验，证明LLM生成的合成数据可以有效地启动审查推荐系统，缩小与真实标签数据训练模型的性能差距。

Conclusion: 该方法为扩展自动代码审查能力到快速发展的技术栈提供了可扩展的途径，即使在缺乏注释数据的情况下也能实现。

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [22] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: 本研究通过分析400个GitHub项目的README文件和issue讨论，验证了LLM在软件工程教育中的激励因素（如学习参与度、编程辅助）和抑制因素（如抄袭担忧、安全问题），为构建负责任集成框架提供实证基础。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等大型语言模型在软件工程教育中的广泛应用，需要系统研究其负责任集成到课程中的方法，平衡机遇与挑战。

Method: 采用仓库挖掘方法，分析400个GitHub项目的README文件和issue讨论，识别文献综述中合成的激励因素和抑制因素的存在情况。

Result: 激励因素如学习参与度（227次）、软件工程流程理解（133次）、编程辅助（97次）表现突出；抑制因素如抄袭和知识产权担忧（385次）、安全问题（87次）也很显著，但部分抑制因素如学习成果评估困难未出现。

Conclusion: 研究为激励/抑制因素分类提供了早期实证验证，揭示了研究实践差距，为开发LLM在软件工程教育中负责任采用的综合框架奠定了基础。

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [23] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC是一个新颖的模糊测试框架，通过符号执行从二进制文件中重建def-use链，结合数据流分析来发现传统控制流模糊测试可能遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统的二进制模糊测试由于缺乏对程序内部数据流的洞察，往往难以实现全面的代码覆盖和发现隐藏漏洞。仅依赖控制流边覆盖的传统灰盒模糊测试容易忽略那些无法通过控制流分析单独暴露的漏洞。

Method: FuzzRDUCC采用符号执行技术直接从二进制可执行文件中重建定义-使用(def-use)链，并使用一种新颖的启发式算法选择相关的def-use链，在不影响模糊测试彻底性的前提下避免过高的计算开销。

Result: 在binutils基准测试上的评估表明，FuzzRDUCC能够识别出现有最先进模糊测试工具无法发现的独特崩溃。

Conclusion: FuzzRDUCC通过将数据流分析整合到模糊测试过程中，为下一代漏洞检测和发现机制提供了一个可行的解决方案，能够揭示控制流方法可能遗漏的执行路径。

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [24] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: 基于GenAI的自动化测试案例生成方法，利用LLM和VLM将自然语言需求和系统图转换为Gherkin测试案例，通过VSS模型标准化车辆信号定义，在digital.auto环境中执行验证。


<details>
  <summary>Details</summary>
Motivation: 解决汽车软件测试中手动编写测试案例效率低下、质量不稳定的问题，通过GenAI技术自动化测试案例生成过程，提高测试效率和质量。

Method: 结合大语言模型(LLM)和视觉-语言模型(VLM)，将自然语言需求和系统图转换为结构化Gherkin测试案例，采用车辆信号规范(VSS)模型标准化信号定义，在digital.auto平台执行测试。

Result: 在儿童存在检测系统案例中，实现了手动测试规范工作量的大幅减少，生成的测试案例能够快速执行。

Conclusion: 该GenAI驱动方法能够显著提高汽车软件测试效率，但目前在测试案例生成和脚本编写方面仍需人工干预，原因是GenAI流程和digital.auto平台的现有限制。

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [25] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber是一个基于AI代理的网页测试框架，能够自主探索网站、模拟用户交互、识别bug和可用性问题，并生成可读报告。


<details>
  <summary>Details</summary>
Motivation: 传统网页测试方法主要关注代码覆盖和负载测试，但往往无法捕捉复杂的用户行为，导致许多可用性问题未被发现。大型语言模型和AI代理的出现为网页测试提供了新可能。

Method: 开发WebProber原型框架，给定URL后自主探索网站，模拟真实用户交互，识别bug和可用性问题，生成人类可读报告。

Result: 在120个学术个人网站的案例研究中，发现了29个可用性问题，其中许多是传统工具遗漏的。

Conclusion: 基于代理的测试是一个有前景的方向，为开发下一代以用户为中心的测试框架指明了方向。

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [26] [NEXUS: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute](https://arxiv.org/abs/2509.04625)
*Zhenzhou Qi,Chung-Hsuan Tung,Zhihui Gao,Tingjun Chen*

Main category: cs.NI

TL;DR: NEXUS是首个在单服务器上实现实时虚拟化多小区毫米波基带处理的系统，通过软硬件协同设计和智能资源调度，支持16个并发小区，达到5.37Gbps总吞吐量，大幅减少调度搜索空间。


<details>
  <summary>Details</summary>
Motivation: 5G NR毫米波频谱的快速部署对基带处理的灵活性、可扩展性和效率提出严格要求，而多小区异构工作负载下的计算资源分配问题尚未得到充分探索。

Method: 集成软件DSP流水线和硬件加速LDPC解码，通过虚拟函数(VFs)在多个CPU核心间共享Intel ACC100 eASIC；单小区使用随机森林模型预测能效最优资源分配，多小区采用功率感知调度器结合轻量级竞争模型。

Result: 支持最多16个并发全负载小区，实现5.37Gbps总吞吐量，多小区调度搜索空间减少数个数量级，微秒级推理延迟和高精度预测。

Conclusion: 虚拟化、资源感知的基带处理对下一代vRAN系统既实用又高效，为多小区毫米波部署提供了可行的解决方案。

Abstract: The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave
(mmWave) spectrum, imposes stringent demands on the flexibility, scalability,
and efficiency of baseband processing. While virtualized Radio Access Networks
(vRANs) enable dynamic spectrum sharing across cells, compute resource
allocation for baseband processing, especially in multi-cell deployments with
heterogeneous workloads, remains underexplored. In this paper, we present
NEXUS, the first system to realize real-time, virtualized multi-cell mmWave
baseband processing on a single server with heterogeneous compute resources.
NEXUS integrates software-based digital signal processing pipelines with
hardware-accelerated LDPC decoding, and introduces a novel framework for
sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions
(VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based
model that predicts the most energy-efficient resource allocation for the given
cell configuration with microsecond-level inference latency and high accuracy.
For multi-cell scenarios, NEXUS introduces a power-aware scheduler that
incorporates a lightweight contention model to adjust resource allocation
strategies under concurrent execution. Through extensive evaluation across
various Frequency Range 2 (FR2) cell configurations, we show that NEXUS
supports up to 16 concurrent cells under full load, achieving 5.37Gbps
aggregate throughput, while reducing the multi-cell scheduling search space by
orders of magnitude. These results demonstrate that virtualized, resource-aware
baseband processing is both practical and efficient for next-generation vRAN
systems.

</details>


### [27] [Path Dynamics in a Deployed Path-Aware Network: A Measurement Study of SCIONLab](https://arxiv.org/abs/2509.04695)
*Lars Herschbach,Damien Rossi,Sina Keshvadi*

Main category: cs.NI

TL;DR: 对SCION网络的综向测量研究，揭示了路径不稳定性、异构性和性能交换问题，对多路径协议设计提出挑战


<details>
  <summary>Details</summary>
Motivation: 路径感知网络拥有更好的性能和弹性，但缺乏实际运行数据阻碍了高效协议的设计

Method: 在全球SCIONLab测试平台上进行综向测量研究，分析路径稳定性、多样性和性能特征

Result: 发现高度动态环境，控制面更新频繁，路径存活期短，存在路由策略导致的路径可用性不对称问题，同时多路径传输在提升总速率时可能造成单路延迟和可靠性下降

Conclusion: 多路径协议（如MPQUIC）需要明确考虑高更新频率和路径不对称性，这与当前协议设计中的普遍假设相冲突

Abstract: Path-aware networks promise enhanced performance and resilience through
multipath transport, but a lack of empirical data on their real-world dynamics
hinders the design of effective protocols. This paper presents a longitudinal
measurement study of the SCION architecture on the global SCIONLab testbed,
characterizing the path stability, diversity, and performance crucial for
protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic
environment, with significant control-plane churn and short path lifetimes in
parts of the testbed. We identify and characterize path discrepancy, a
phenomenon where routing policies create asymmetric path availability between
endpoints. Furthermore, we observe a performance trade-off where concurrent
multipath transmissions can improve aggregate throughput but may degrade the
latency and reliability of individual paths. These findings demonstrate that
protocols such as MPQUIC should explicitly account for high churn and path
asymmetry, challenging common assumptions in multipath protocol design.

</details>


### [28] [Where Have All the Firewalls Gone? Security Consequences of Residential IPv6 Transition](https://arxiv.org/abs/2509.04792)
*Erik Rye,Dave Levin,Robert Beverly*

Main category: cs.NI

TL;DR: IPv6过渡使住宅网络更容易受到攻击，IPv6扫描发现比IPv4更多的设备可直接访问，NAT曾作为事实防火墙保护设备。


<details>
  <summary>Details</summary>
Motivation: 研究IPv4到IPv6过渡是否使住宅网络更易受攻击，特别是对IoT僵尸网络的脆弱性。

Method: 开发大规模IPv6扫描方法，在低资源设备上运行，对住宅网络进行大规模测量，比较IPv6和IPv4网络的可访问设备。

Result: 从118个国家2436个AS的1400万个IPv6地址收到响应，发现IPv6比IPv4扫描能访问更多打印机、iPhone和智能灯等设备。

Conclusion: NAT确实充当了互联网的事实防火墙，IPv6过渡正在向攻击开放新设备。

Abstract: IPv4 NAT has limited the spread of IoT botnets considerably by
default-denying bots' incoming connection requests to in-home devices unless
the owner has explicitly allowed them. As the Internet transitions to majority
IPv6, however, residential connections no longer require the use of NAT. This
paper therefore asks: has the transition from IPv4 to IPv6 ultimately made
residential networks more vulnerable to attack, thereby empowering the next
generation of IPv6-based IoT botnets? To answer this question, we introduce a
large-scale IPv6 scanning methodology that, unlike those that rely on AI, can
be run on low-resource devices common in IoT botnets. We use this methodology
to perform the largest-scale measurement of IPv6 residential networks to date,
and compare which devices are publicly accessible to comparable IPv4 networks.
We were able to receive responses from 14.0M distinct IPv6 addresses inside of
residential networks (i.e., not the external-facing gateway), in 2,436 ASes
across 118 countries. These responses come from protocols commonly exploited by
IoT botnets (including telnet and FTP), as well as protocols typically
associated with end-user devices (including iPhone-Sync and IPP). Comparing to
IPv4, we show that we are able to reach more printers, iPhones, and smart
lights over IPv6 than full IPv4-wide scans could. Collectively, our results
show that NAT has indeed acted as the de facto firewall of the Internet, and
the v4-to-v6 transition of residential networks is opening up new devices to
attack.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics](https://arxiv.org/abs/2509.04536)
*Oliver Dunn,Koorosh Aslansefat,Yiannis Papadopoulos*

Main category: cs.LG

TL;DR: Q-SafeML是一种针对量子机器学习的安全监控方法，通过量子中心距离度量来检测概念漂移，提高系统透明度和安全性


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习的发展，现有经典机器学习的安全监控方法无法直接应用于QML，需要专门针对量子计算特性的安全机制

Method: 基于SafeML方法，采用量子中心距离度量来评估模型准确性，进行模型依赖的后分类评估，检测操作数据与训练数据之间的距离

Result: 在QCNN和VQC模型上的实验表明，该方法能够实现知情的人工监督，增强系统透明度和安全性

Conclusion: Q-SafeML为量子机器学习提供了专门的安全监控解决方案，适应了量子系统的独特表示约束和概率性输出特性

Abstract: The rise of machine learning in safety-critical systems has paralleled
advancements in quantum computing, leading to the emerging field of Quantum
Machine Learning (QML). While safety monitoring has progressed in classical ML,
existing methods are not directly applicable to QML due to fundamental
differences in quantum computation. Given the novelty of QML, dedicated safety
mechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety
monitoring approach for QML. The method builds on SafeML, a recent method that
utilizes statistical distance measures to assess model accuracy and provide
confidence in the reasoning of an algorithm. An adapted version of Q-SafeML
incorporates quantum-centric distance measures, aligning with the probabilistic
nature of QML outputs. This shift to a model-dependent, post-classification
evaluation represents a key departure from classical SafeML, which is
dataset-driven and classifier-agnostic. The distinction is motivated by the
unique representational constraints of quantum systems, requiring distance
metrics defined over quantum state spaces. Q-SafeML detects distances between
operational and training data addressing the concept drifts in the context of
QML. Experiments on QCNN and VQC Models show that this enables informed human
oversight, enhancing system transparency and safety.

</details>


### [30] [Finance-Grounded Optimization For Algorithmic Trading](https://arxiv.org/abs/2509.04541)
*Kasymkhan Khubiev,Mikhail Semenov,Irina Podlipnova*

Main category: cs.LG

TL;DR: 该论文提出了基于金融指标的损失函数（夏普比率、PnL、最大回撤）和换手率正则化方法，在算法交易指标上优于传统的均方误差损失函数。


<details>
  <summary>Details</summary>
Motivation: 深度学习在金融领域的应用面临可解释性挑战，传统方法在自然语言处理、计算机视觉和预测方面表现良好，但不完全适合金融领域，因为金融专家使用不同的指标来评估模型性能。

Method: 提出了基于关键量化金融指标（夏普比率、损益、最大回撤）的金融基础损失函数，以及换手率正则化方法，用于约束生成头寸的换手率在预定范围内。

Result: 研究结果表明，提出的损失函数结合换手率正则化，在算法交易指标评估中优于传统的均方误差损失函数，能够提升交易策略和投资组合优化的预测性能。

Conclusion: 金融基础指标能够增强交易策略中的预测性能，为深度学习在金融领域的应用提供了更合适的评估和优化方法。

Abstract: Deep Learning is evolving fast and integrates into various domains. Finance
is a challenging field for deep learning, especially in the case of
interpretable artificial intelligence (AI). Although classical approaches
perform very well with natural language processing, computer vision, and
forecasting, they are not perfect for the financial world, in which specialists
use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key
quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss
(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,
a method that inherently constrains the turnover of generated positions within
predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction
with turnover regularization, outperform the traditional mean squared error
loss for return prediction tasks when evaluated using algorithmic trading
metrics. The study shows that financially grounded metrics enhance predictive
performance in trading strategies and portfolio optimization.

</details>


### [31] [i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition](https://arxiv.org/abs/2509.04544)
*Ashutosh Kumar Sinha,Ayush Patel,Mitul Dudhat,Pritam Anand,Rahul Mishra*

Main category: cs.LG

TL;DR: i-Mask是一种基于呼出气流模式的人类活动识别方法，使用定制口罩传感器采集数据，通过噪声过滤和时间序列分解处理，实现超过95%的准确率


<details>
  <summary>Details</summary>
Motivation: 呼吸模式包含重要的生理信号，可用于预测人类行为、健康趋势和生命参数，对人类活动识别和实时健康监测具有重要意义

Method: 开发定制口罩配备集成传感器采集呼出气流模式数据，进行噪声过滤、时间序列分解和标注，训练预测模型

Result: 实验验证了方法的有效性，准确率超过95%

Conclusion: 该方法在医疗健康和健身应用领域具有巨大潜力

Abstract: The patterns of inhalation and exhalation contain important physiological
signals that can be used to anticipate human behavior, health trends, and vital
parameters. Human activity recognition (HAR) is fundamentally connected to
these vital signs, providing deeper insights into well-being and enabling
real-time health monitoring. This work presents i-Mask, a novel HAR approach
that leverages exhaled breath patterns captured using a custom-developed mask
equipped with integrated sensors. Data collected from volunteers wearing the
mask undergoes noise filtering, time-series decomposition, and labeling to
train predictive models. Our experimental results validate the effectiveness of
the approach, achieving over 95\% accuracy and highlighting its potential in
healthcare and fitness applications.

</details>


### [32] [Bootstrapping Task Spaces for Self-Improvement](https://arxiv.org/abs/2509.04575)
*Minqi Jiang,Andrei Lupu,Yoram Bachrach*

Main category: cs.LG

TL;DR: Exploratory Iteration (ExIt)是一种自课程强化学习方法，通过选择性采样信息量最大的中间状态来训练LLM进行多步自我改进，在推理时实现超越训练迭代深度的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设固定的最大迭代深度，这既昂贵又随意。需要一种能够利用自我改进任务递归结构的方法，让代理在推理时可靠地进行多步自我改进。

Method: ExIt通过选择性采样训练过程中遇到的最具信息量的中间部分历史记录来扩展任务空间，将这些起点视为新的自我迭代任务实例来训练自我改进策略，并可结合显式探索机制维持任务多样性。

Result: 在数学竞赛、多轮工具使用和机器学习工程等多个领域，ExIt策略能够产生在保持任务实例上表现出强推理时自我改进能力的策略，并能在超出训练期间平均迭代深度的步数预算内迭代实现更高性能。

Conclusion: ExIt方法有效解决了固定迭代深度的限制，通过自课程学习实现了推理时的多步自我改进，在多个复杂任务领域展现出强大的性能提升能力。

Abstract: Progress in many task domains emerges from repeated revisions to previous
solution attempts. Training agents that can reliably self-improve over such
sequences at inference-time is a natural target for reinforcement learning
(RL), yet the naive approach assumes a fixed maximum iteration depth, which can
be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family
of autocurriculum RL methods that directly exploits the recurrent structure of
self-improvement tasks to train LLMs to perform multi-step self-improvement at
inference-time while only training on the most informative single-step
iterations. ExIt grows a task space by selectively sampling the most
informative intermediate, partial histories encountered during an episode for
continued iteration, treating these starting points as new self-iteration task
instances to train a self-improvement policy. ExIt can further pair with
explicit exploration mechanisms to sustain greater task diversity. Across
several domains, encompassing competition math, multi-turn tool-use, and
machine learning engineering, we demonstrate that ExIt strategies, starting
from either a single or many task instances, can produce policies exhibiting
strong inference-time self-improvement on held-out task instances, and the
ability to iterate towards higher performance over a step budget extending
beyond the average iteration depth encountered during training.

</details>


### [33] [Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions](https://arxiv.org/abs/2509.04583)
*Jiequn Han,Kui Ren,Nathan Soedjak*

Main category: cs.LG

TL;DR: 提出基于实例的自适应采样框架，用于构建紧凑且信息丰富的训练数据集，以解决逆问题的监督学习。通过根据测试实例动态分配采样资源，显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法需要从先验分布中抽取大量训练样本，当先验维度高或精度要求高时，数据收集成本巨大。需要一种针对特定测试实例的采样方法来提高效率。

Method: 迭代式自适应采样框架，根据最新预测结果条件化地精炼训练数据集，使数据集适应每个测试实例周围逆映射的几何结构。

Result: 在逆散射问题中验证了方法的有效性，特别是在复杂先验或高精度要求的情况下，自适应方法的优势更加明显。

Conclusion: 该自适应采样策略具有广泛适用性，可扩展到其他逆问题，为传统固定数据集训练提供了可扩展且实用的替代方案。

Abstract: We propose an instance-wise adaptive sampling framework for constructing
compact and informative training datasets for supervised learning of inverse
problem solutions. Typical learning-based approaches aim to learn a
general-purpose inverse map from datasets drawn from a prior distribution, with
the training process independent of the specific test instance. When the prior
has a high intrinsic dimension or when high accuracy of the learned solution is
required, a large number of training samples may be needed, resulting in
substantial data collection costs. In contrast, our method dynamically
allocates sampling effort based on the specific test instance, enabling
significant gains in sample efficiency. By iteratively refining the training
dataset conditioned on the latest prediction, the proposed strategy tailors the
dataset to the geometry of the inverse map around each test instance. We
demonstrate the effectiveness of our approach in the inverse scattering problem
under two types of structured priors. Our results show that the advantage of
the adaptive method becomes more pronounced in settings with more complex
priors or higher accuracy requirements. While our experiments focus on a
particular inverse problem, the adaptive sampling strategy is broadly
applicable and readily extends to other inverse problems, offering a scalable
and practical alternative to conventional fixed-dataset training regimes.

</details>


### [34] [Toward Faithfulness-guided Ensemble Interpretation of Neural Network](https://arxiv.org/abs/2509.04588)
*Siyu Zhang,Kenneth Mcmillan*

Main category: cs.LG

TL;DR: FEI框架通过忠实性引导的集成解释方法，提升神经网络解释的可视化效果和定量忠实性评分，在定性和定量评估中都超越了现有方法


<details>
  <summary>Details</summary>
Motivation: 需要为特定神经推理提供可解释且忠实的解释，以理解和评估模型行为，现有方法在忠实性和可视化效果方面有待提升

Method: 提出FEI框架，使用平滑近似技术提升定量忠实性评分，针对隐藏层编码开发多种变体来增强忠实性，并提出新的定性评估指标

Result: 在大量实验中，FEI在定性可视化和定量忠实性评分方面都显著超越了现有方法

Conclusion: 研究建立了一个提升神经网络解释忠实性的综合框架，强调广度和精度的平衡

Abstract: Interpretable and faithful explanations for specific neural inferences are
crucial for understanding and evaluating model behavior. Our work introduces
\textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation
(\textbf{FEI}), an innovative framework that enhances the breadth and
effectiveness of faithfulness, advancing interpretability by providing superior
visualization. Through an analysis of existing evaluation benchmarks,
\textbf{FEI} employs a smooth approximation to elevate quantitative
faithfulness scores. Diverse variations of \textbf{FEI} target enhanced
faithfulness in hidden layer encodings, expanding interpretability.
Additionally, we propose a novel qualitative metric that assesses hidden layer
faithfulness. In extensive experiments, \textbf{FEI} surpasses existing
methods, demonstrating substantial advances in qualitative visualization and
quantitative faithfulness scores. Our research establishes a comprehensive
framework for elevating faithfulness in neural network explanations,
emphasizing both breadth and precision

</details>


### [35] [An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2509.05213)
*Jiaojiao Zhang,Yuqi Xu,Kun Yuan*

Main category: cs.LG

TL;DR: FedSub是一种高效的联邦学习子空间算法，通过子空间投影和低维双变量来解决数据异构性导致的客户端漂移问题，同时降低通信、计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在大规模深度神经网络中面临的关键挑战：数据异构性导致的客户端漂移问题，以及高昂的通信、计算和内存成本。

Method: 提出FedSub算法，利用子空间投影确保每个客户端的本地更新在低维子空间内进行，同时引入低维双变量来缓解客户端漂移。

Result: 提供了收敛性分析，揭示了步长和子空间投影矩阵等关键因素对收敛的影响，实验结果表明该方法的有效性。

Conclusion: FedSub通过子空间投影和低维双变量的结合，有效解决了联邦学习中的数据异构性和资源消耗问题，具有较好的收敛性能和实际应用价值。

Abstract: This work addresses the key challenges of applying federated learning to
large-scale deep neural networks, particularly the issue of client drift due to
data heterogeneity across clients and the high costs of communication,
computation, and memory. We propose FedSub, an efficient subspace algorithm for
federated learning on heterogeneous data. Specifically, FedSub utilizes
subspace projection to guarantee local updates of each client within
low-dimensional subspaces, thereby reducing communication, computation, and
memory costs. Additionally, it incorporates low-dimensional dual variables to
mitigate client drift. We provide convergence analysis that reveals the impact
of key factors such as step size and subspace projection matrices on
convergence. Experimental results demonstrate its efficiency.

</details>


### [36] [Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction](https://arxiv.org/abs/2509.04601)
*Han Zhang,Fengji Ma,Jiamin Su,Xinyue Yang,Lei Wang,Wen-Cai Ye,Li Liu*

Main category: cs.LG

TL;DR: 提出了QW-MTL框架，通过量子化学描述符增强分子表示，并采用指数任务加权方案进行多任务学习，在13个ADMET分类任务中显著优于单任务基线。


<details>
  <summary>Details</summary>
Motivation: 现有ADMET预测方法主要依赖单任务学习，无法充分利用任务间的互补性，且计算资源消耗大。需要开发能够有效利用任务间关系并提高效率的多任务学习框架。

Method: 基于Chemprop-RDKit骨干网络，引入量子化学描述符丰富分子表示，采用结合数据集规模先验和可学习参数的指数任务加权方案实现动态损失平衡。

Result: 在13个TDC分类基准测试中，QW-MTL在12个任务上显著优于单任务基线，实现了高预测性能和快速推理。

Conclusion: 量子信息增强的特征和自适应任务加权的多任务分子学习具有有效性和高效性，为ADMET预测提供了新的解决方案。

Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and
Toxicity) plays a crucial role in drug discovery and development, accelerating
the screening and optimization of new drugs. Existing methods primarily rely on
single-task learning (STL), which often fails to fully exploit the
complementarities between tasks. Besides, it requires more computational
resources while training and inference of each task independently. To address
these issues, we propose a new unified Quantum-enhanced and task-Weighted
Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET
classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts
quantum chemical descriptors to enrich molecular representations with
additional information about the electronic structure and interactions.
Meanwhile, it introduces a novel exponential task weighting scheme that
combines dataset-scale priors with learnable parameters to achieve dynamic loss
balancing across tasks. To the best of our knowledge, this is the first work to
systematically conduct joint multi-task training across all 13 Therapeutics
Data Commons (TDC) classification benchmarks, using leaderboard-style data
splits to ensure a standardized and realistic evaluation setting. Extensive
experimental results show that QW-MTL significantly outperforms single-task
baselines on 12 out of 13 tasks, achieving high predictive performance with
minimal model complexity and fast inference, demonstrating the effectiveness
and efficiency of multi-task molecular learning enhanced by quantum-informed
features and adaptive task weighting.

</details>


### [37] [Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families](https://arxiv.org/abs/2509.04622)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 本文提出了一个评估表征相似性度量区分能力的定量框架，通过三种互补的分离度量方法，系统比较了不同相似性度量在区分不同模型家族方面的表现。


<details>
  <summary>Details</summary>
Motivation: 表征相似性度量是神经科学和AI领域的基础工具，但缺乏对不同度量方法在不同模型家族间区分能力的系统性比较。

Method: 使用信号检测理论的dprime、轮廓系数和ROC-AUC三种分离度量方法，评估包括RSA、线性预测性、Procrustes和软匹配等常用相似性度量的区分能力。

Result: 分离性随着度量施加更严格的对齐约束而系统性增加。在基于映射的方法中，软匹配具有最高的分离性，其次是Procrustes对齐和线性预测性。非拟合方法如RSA也能在不同家族间产生强分离性。

Conclusion: 研究首次通过分离性视角系统比较了相似性度量，阐明了它们的相对敏感性，为大规模模型和大脑比较中的度量选择提供了指导。

Abstract: Representational similarity metrics are fundamental tools in neuroscience and
AI, yet we lack systematic comparisons of their discriminative power across
model families. We introduce a quantitative framework to evaluate
representational similarity measures based on their ability to separate model
families-across architectures (CNNs, Vision Transformers, Swin Transformers,
ConvNeXt) and training regimes (supervised vs. self-supervised). Using three
complementary separability measures-dprime from signal detection theory,
silhouette coefficients and ROC-AUC, we systematically assess the
discriminative capacity of commonly used metrics including RSA, linear
predictivity, Procrustes, and soft matching. We show that separability
systematically increases as metrics impose more stringent alignment
constraints. Among mapping-based approaches, soft-matching achieves the highest
separability, followed by Procrustes alignment and linear predictivity.
Non-fitting methods such as RSA also yield strong separability across families.
These results provide the first systematic comparison of similarity metrics
through a separability lens, clarifying their relative sensitivity and guiding
metric choice for large-scale model and brain comparisons.

</details>


### [38] [Split Conformal Prediction in the Function Space with Neural Operators](https://arxiv.org/abs/2509.04623)
*David Millard,Lars Lindemann,Ali Baheri*

Main category: cs.LG

TL;DR: 该论文提出了一种将分体保形预测扩展到函数空间的方法，为神经算子提供有限样本覆盖保证，解决了无限维设置中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 神经算子在无限维设置中的不确定性量化是一个开放问题，现有方法（高斯过程、贝叶斯神经网络等）需要强分布假设或产生保守覆盖，缺乏对函数值输出的有限样本覆盖保证。

Method: 采用两步法：首先在有限维空间中通过输出函数空间的离散化映射建立有限样本覆盖保证，然后通过考虑离散化细化时的渐近收敛将这些保证提升到函数空间。提出了基于回归的校正来跨分辨率传输校准，并提出了两个诊断指标。

Result: 经验结果表明，该方法在分辨率变化下保持校准覆盖且变化较小，在超分辨率任务中实现了更好的覆盖。

Conclusion: 该方法成功地将分体保形预测扩展到函数空间，为神经算子提供了有效的有限样本不确定性量化框架，解决了无限维设置中的覆盖保证问题。

Abstract: Uncertainty quantification for neural operators remains an open problem in
the infinite-dimensional setting due to the lack of finite-sample coverage
guarantees over functional outputs. While conformal prediction offers
finite-sample guarantees in finite-dimensional spaces, it does not directly
extend to function-valued outputs. Existing approaches (Gaussian processes,
Bayesian neural networks, and quantile-based operators) require strong
distributional assumptions or yield conservative coverage. This work extends
split conformal prediction to function spaces following a two step method. We
first establish finite-sample coverage guarantees in a finite-dimensional space
using a discretization map in the output function space. Then these guarantees
are lifted to the function-space by considering the asymptotic convergence as
the discretization is refined. To characterize the effect of resolution, we
decompose the conformal radius into discretization, calibration, and
misspecification components. This decomposition motivates a regression-based
correction to transfer calibration across resolutions. Additionally, we propose
two diagnostic metrics (conformal ensemble score and internal agreement) to
quantify forecast degradation in autoregressive settings. Empirical results
show that our method maintains calibrated coverage with less variation under
resolution shifts and achieves better coverage in super-resolution tasks.

</details>


### [39] [Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction](https://arxiv.org/abs/2509.04631)
*Arash Behboodi,Alvaro H. C. Correia,Fabio Valerio Massoli,Christos Louizos*

Main category: cs.LG

TL;DR: 本文揭示了转导式共形预测中置信度与效率之间的基本权衡，证明了非平凡置信水平会导致预测集大小呈指数增长，增长指数与样本数量和条件熵成正比。


<details>
  <summary>Details</summary>
Motivation: 研究转导式共形预测中同时预测多个数据点时置信度与预测集效率之间的基本关系，解决现有方法在置信度要求下预测集大小快速增长的问题。

Method: 推导严格的有限样本界限，分析预测集大小与置信水平、样本数量、条件熵和分散度之间的关系，并在理想化设置中证明界限的可达性。

Result: 证明了任何非平凡置信水平都会导致预测集大小呈指数增长，增长指数与样本数量线性相关且与条件熵成正比，分散度作为二阶项影响界限。

Conclusion: 转导式共形预测存在置信度与效率的根本权衡，提出的界限在理想设置下可达，为特定情况提供了渐近最优的置信预测器。

Abstract: Transductive conformal prediction addresses the simultaneous prediction for
multiple data points. Given a desired confidence level, the objective is to
construct a prediction set that includes the true outcomes with the prescribed
confidence. We demonstrate a fundamental trade-off between confidence and
efficiency in transductive methods, where efficiency is measured by the size of
the prediction sets. Specifically, we derive a strict finite-sample bound
showing that any non-trivial confidence level leads to exponential growth in
prediction set size for data with inherent uncertainty. The exponent scales
linearly with the number of samples and is proportional to the conditional
entropy of the data. Additionally, the bound includes a second-order term,
dispersion, defined as the variance of the log conditional probability
distribution. We show that this bound is achievable in an idealized setting.
Finally, we examine a special case of transductive prediction where all test
data points share the same label. We show that this scenario reduces to the
hypothesis testing problem with empirically observed statistics and provide an
asymptotically optimal confidence predictor, along with an analysis of the
error exponent.

</details>


### [40] [Interpreting Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
*Jonas A. Actor,Anthony Gruber,Eric C. Cyr*

Main category: cs.LG

TL;DR: 本文建立了注意力机制与多项式回归之间的新联系，证明在固定多项式回归设置中，优化潜在特征会产生与注意力块动态一致的解，Transformer表示演化可解释为恢复分类最优特征的轨迹


<details>
  <summary>Details</summary>
Motivation: 理解现代机器学习模型内部组件如何产生整体行为，特别是注意力机制在Transformer模型中的核心作用及其与特征多义性、叠加和模型性能的关系

Method: 在固定多项式回归设置中优化潜在特征，分析最优解与注意力块诱导动态的一致性

Result: 发现注意力机制的最优解与多项式回归中的最优特征恢复轨迹一致

Conclusion: Transformer表示演化可被解释为分类最优特征的恢复过程，为注意力机制提供了新的数学解释框架

Abstract: Mechanistic interpretability aims to understand how internal components of
modern machine learning models, such as weights, activations, and layers, give
rise to the model's overall behavior. One particularly opaque mechanism is
attention: despite its central role in transformer models, its mathematical
underpinnings and relationship to concepts like feature polysemanticity,
superposition, and model performance remain poorly understood. This paper
establishes a novel connection between attention mechanisms and multinomial
regression. Specifically, we show that in a fixed multinomial regression
setting, optimizing over latent features yields optimal solutions that align
with the dynamics induced by attention blocks. In other words, the evolution of
representations through a transformer can be interpreted as a trajectory that
recovers the optimal features for classification.

</details>


### [41] [Flexible inference of learning rules from de novo learning data using neural networks](https://arxiv.org/abs/2509.04661)
*Yuhan Helena Liu,Victor Geadah,Jonathan Pillow*

Main category: cs.LG

TL;DR: 通过深度神经网络框架从动物初始学习行为数据中推断学习规则，发现了错误与正确试验的不对称更新和历史依赖性


<details>
  <summary>Details</summary>
Motivation: 动物学习机制研究对神经科学和AI开发重要，但现有方法多假设特定参数化学习规则或仅适用于简化情境，而动物常需从头学习新行为

Method: 提出非参数化框架，用深度神经网络(DNN)定义每试验改策更新，并扩展到递归类型(RNN)以捕捉非马尔可夫动态性，在模拟数据中验证后应用于鼠类感知决策学习数据

Result: 模型在指定数据上提升了预测性能，推断出的学习规则显示了正确与错误试验后的不对称更新模式以及历史依赖性，符合非马尔可夫学习特征

Conclusion: 该框架为从初始学习行为数据中推断生物学习规则提供了灵活方法，为实验训练协议设计和行为数字双胞开发提供了见解

Abstract: Understanding how animals learn is a central challenge in neuroscience, with
growing relevance to the development of animal- or human-aligned artificial
intelligence. However, most existing approaches assume specific parametric
forms for the learning rule (e.g., Q-learning, policy gradient) or are limited
to simplified settings like bandit tasks, which do not involve learning a new
input-output mapping from scratch. In contrast, animals must often learn new
behaviors de novo, which poses a rich challenge for learning-rule inference. We
target this problem by inferring learning rules directly from animal
decision-making data during de novo task learning, a setting that requires
models flexible enough to capture suboptimality, history dependence, and rich
external stimulus integration without strong structural priors. We first
propose a nonparametric framework that parameterizes the per-trial update of
policy weights with a deep neural network (DNN), and validate it by recovering
ground-truth rules in simulation. We then extend to a recurrent variant (RNN)
that captures non-Markovian dynamics by allowing updates to depend on trial
history. Applied to a large behavioral dataset of mice learning a sensory
decision-making task over multiple weeks, our models improved predictions on
held-out data. The inferred rules revealed asymmetric updates after correct
versus error trials and history dependence, consistent with non-Markovian
learning. Overall, these results introduce a flexible framework for inferring
biological learning rules from behavioral data in de novo learning tasks,
providing insights to inform experimental training protocols and the
development of behavioral digital twins.

</details>


### [42] [Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition](https://arxiv.org/abs/2509.04668)
*Difei Xu,Meng Ding,Zihang Xiang,Jinhui Xu,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了差分隐私模型下的随机凸优化问题，针对满足Tsybakov噪声条件的损失函数，提出了新的隐私保护算法，并在高概率下获得了与Lipschitz常数无关的效用上界，同时给出了相应的下界证明。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私随机凸优化研究通常假设损失函数是Lipschitz连续的，但在实际应用中，损失函数的Lipschitz常数可能非常大甚至无界。本文旨在解决在梯度具有有界矩但损失函数可能非Lipschitz的情况下，如何设计有效的差分隐私优化算法。

Method: 针对满足Tsybakov噪声条件且梯度具有有界k阶矩的损失函数，设计了(ε,δ)-差分隐私算法。首先处理θ≥2的Lipschitz情况，然后扩展到θ≥θ̅>1的一般情况，并分析了隐私预算ε较小时的非Lipschitz情况。

Result: 提出了效用上界为Õ((r̃_{2k}(1/√n + (√d/nε))^{(k-1)/k})^{θ/(θ-1)})的算法，该上界与Lipschitz常数无关。对于小的ε，即使损失函数非Lipschitz，也能获得Õ((r̃_k(1/√n + (√d/nε))^{(k-1)/k})^{θ/(θ-1)})的上界。同时给出了匹配的下界Ω((r̃_k(1/√n + (√d/n√ρ))^{(k-1)/k})^{θ/(θ-1)})。

Conclusion: 本文在更一般的假设条件下（梯度有界矩但损失函数可能非Lipschitz），建立了差分隐私随机凸优化的最小极大最优速率，扩展了现有理论框架，为处理实际应用中更复杂的优化问题提供了理论基础。

Abstract: We study Stochastic Convex Optimization in the Differential Privacy model
(DP-SCO). Unlike previous studies, here we assume the population risk function
satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$,
where the Lipschitz constant of the loss could be extremely large or even
unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment
with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an
$(\varepsilon, \delta)$-DP algorithm whose utility bound is
$\Tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
in high probability, where $n$ is the sample size, $d$ is the model dimension,
and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the
gradient. It is notable that such an upper bound is independent of the
Lipschitz constant. We then extend to the case where
  $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$.
Moreover, when the privacy budget $\varepsilon$ is small enough, we show an
upper bound of
$\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
even if the loss function is not Lipschitz. For the lower bound, we show that
for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated
Differential Privacy is lower bounded by
$\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.

</details>


### [43] [Echoes Before Collapse: Deep Learning Detection of Flickering in Complex Systems](https://arxiv.org/abs/2509.04683)
*Yazdan Babazadeh Maghsoodlo,Madhur Anand,Chris T. Bauch*

Main category: cs.LG

TL;DR: 深度学习可以准确检测复杂系统中的闪烁现象（噪声驱动的状态切换），这是系统韧性下降的早期预警信号


<details>
  <summary>Details</summary>
Motivation: 闪烁现象是气候系统、生态系统、金融市场等复杂系统中韧性降低的标志性特征，可能预示着难以预测的关键状态转变，但深度学习在此领域的应用潜力尚未被探索

Method: 使用卷积长短期记忆（CNN LSTM）模型，在由简单多项式函数加噪声生成的合成时间序列上进行训练

Result: 模型能够准确识别闪烁模式，尽管在简化动态上训练，但能泛化到各种随机系统，并在实际数据（如睡鼠体温记录和非洲湿润期古气候代理数据）中可靠检测闪烁

Conclusion: 深度学习可以从嘈杂的非线性时间序列中提取早期预警信号，为识别各种动力系统的不稳定性提供了一个灵活框架

Abstract: Deep learning offers powerful tools for anticipating tipping points in
complex systems, yet its potential for detecting flickering (noise-driven
switching between coexisting stable states) remains unexplored. Flickering is a
hallmark of reduced resilience in climate systems, ecosystems, financial
markets, and other systems. It can precede critical regime shifts that are
highly impactful but difficult to predict. Here we show that convolutional long
short-term memory (CNN LSTM) models, trained on synthetic time series generated
from simple polynomial functions with additive noise, can accurately identify
flickering patterns. Despite being trained on simplified dynamics, our models
generalize to diverse stochastic systems and reliably detect flickering in
empirical datasets, including dormouse body temperature records and
palaeoclimate proxies from the African Humid Period. These findings demonstrate
that deep learning can extract early warning signals from noisy, nonlinear time
series, providing a flexible framework for identifying instability across a
wide range of dynamical systems.

</details>


### [44] [KRAFT: A Knowledge Graph-Based Framework for Automated Map Conflation](https://arxiv.org/abs/2509.04684)
*Farnoosh Hashemi,Laks V. S. Lakshmanan*

Main category: cs.LG

TL;DR: KRATF是一种基于学习的数字地图融合方法，通过知识图谱构建、地图匹配和地图合并三个模块，解决了传统方法只能处理线性对象和基于启发式规则的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有地图融合方法存在两个主要限制：(1) 只能处理线性对象（如道路网络），无法扩展到非线性对象；(2) 基于预定义规则的启发式算法，无法以数据驱动方式学习实体匹配。

Method: KRATF包含三个部分：1) 知识图谱构建 - 将地理空间数据库表示为知识图谱；2) 地图匹配 - 使用知识图谱对齐方法和地理空间特征编码器匹配实体；3) 地图合并 - 使用混合整数线性规划公式一致地合并匹配实体。

Result: 实验评估显示，KRATF不仅在地图融合任务中相比最先进方法和基线方法表现出色，其各个模块（如地图匹配和地图合并）也分别优于传统的匹配和合并方法。

Conclusion: KRATF提供了一种有效的数据驱动方法来解决地图融合问题，能够处理非线性对象并避免传统启发式方法的局限性，实现了高质量的地图数据库融合。

Abstract: Digital maps play a crucial role in various applications such as navigation,
fleet management, and ride-sharing, necessitating their accuracy and currency,
which require timely updates. While the majority of geospatial databases (GDBs)
provide high-quality information, their data is (i) limited to specific regions
and/or (ii) missing some entities, even in their covered areas. Map conflation
is the process of augmentation of a GDB using another GDB to conflate missing
spatial features. Existing map conflation methods suffer from two main
limitations: (1) They are designed for the conflation of linear objects (e.g.,
road networks) and cannot simply be extended to non-linear objects, thus
missing information about most entities in the map. (2) They are heuristic
algorithmic approaches that are based on pre-defined rules, unable to learn
entities matching in a data-driven manner. To address these limitations, we
design KRAFT, a learning based approach consisting of three parts: (1)
Knowledge Graph Construction - where each GDB is represented by a knowledge
graph, (2) Map Matching - where we use a knowledge graph alignment method as
well as a geospatial feature encoder to match entities in obtained knowledge
graphs, and (3) Map Merging - where we merge matched entities in the previous
modules in a consistent manner, using a mixed integer linear programming
formulation that fully merges the GDBs without adding any inconsistencies. Our
experimental evaluation shows that not only does KRAFT achieve outstanding
performance compared to state-of-the-art and baseline methods in map conflation
tasks, but each of its modules (e.g., Map Matching and Map Merging) also
separately outperforms traditional matching and merging methods.

</details>


### [45] [CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals](https://arxiv.org/abs/2509.04699)
*Wenhui Cui,Christopher Sandino,Hadi Pouransari,Ran Liu,Juri Minxha,Ellen L. Zippi,Aman Verma,Anna Sedlackova,Behrooz Mahasseni,Erdrin Azemi*

Main category: cs.LG

TL;DR: 提出CPEP框架，通过对比学习对齐EMG和姿态表示，实现零样本手势分类，在未见手势分类上提升72%


<details>
  <summary>Details</summary>
Motivation: 利用低成本生物信号（如表面肌电信号）实现连续手势预测，但弱模态数据需要与高质量结构化数据对齐来提升表示质量

Method: 对比姿态-EMG预训练框架（CPEP），学习能够产生高质量且包含姿态信息的EMG编码器

Result: 在线性探测和零样本设置下，模型在分布内手势分类上比基准模型提升21%，在未见手势分类上提升72%

Conclusion: 通过学习弱模态数据与高质量结构化数据的对齐表示，可以显著提升手势分类性能，特别是零样本场景下的表现

Abstract: Hand gesture classification using high-quality structured data such as
videos, images, and hand skeletons is a well-explored problem in computer
vision. Leveraging low-power, cost-effective biosignals, e.g. surface
electromyography (sEMG), allows for continuous gesture prediction on wearables.
In this paper, we demonstrate that learning representations from weak-modality
data that are aligned with those from structured, high-quality data can improve
representation quality and enables zero-shot classification. Specifically, we
propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and
pose representations, where we learn an EMG encoder that produces high-quality
and pose-informative representations. We assess the gesture classification
performance of our model through linear probing and zero-shot setups. Our model
outperforms emg2pose benchmark models by up to 21% on in-distribution gesture
classification and 72% on unseen (out-of-distribution) gesture classification.

</details>


### [46] [Natural Spectral Fusion: p-Exponent Cyclic Scheduling and Early Decision-Boundary Alignment in First-Order Optimization](https://arxiv.org/abs/2509.04713)
*Gongyue Zhang,Honghai Liu*

Main category: cs.LG

TL;DR: 本文提出了自然频谱融合(NSF)方法，将优化器视为频谱控制器，通过p指数扩展和循环调度动态平衡高低频信息，无需修改模型或训练流程即可提升性能


<details>
  <summary>Details</summary>
Motivation: 研究一阶优化器固有的频率偏好如何影响优化路径，现有方法主要关注步长缩放而忽略了频谱覆盖和信息融合的重要性

Method: 提出NSF框架，通过p指数扩展二阶矩项（支持正负指数）和循环调度，周期性重新加权频率带，实现可控的频谱覆盖

Result: 理论分析和实验表明自适应方法强调低频，SGD接近中性，负指数放大高频信息。循环调度拓宽频谱覆盖，改善跨频带融合，在多个基准测试中一致降低测试误差

Conclusion: NSF揭示了优化器作为主动频谱控制器的作用，为优化提供了统一、可控且高效的框架，能以更少训练成本达到相同精度

Abstract: Spectral behaviors have been widely discussed in machine learning, yet the
optimizer's own spectral bias remains unclear. We argue that first-order
optimizers exhibit an intrinsic frequency preference that significantly
reshapes the optimization path. To address this, we propose Natural Spectral
Fusion (NSF): reframing training as controllable spectral coverage and
information fusion rather than merely scaling step sizes. NSF has two core
principles: treating the optimizer as a spectral controller that dynamically
balances low- and high-frequency information; and periodically reweighting
frequency bands at negligible cost, without modifying the model, data, or
training pipeline. We realize NSF via a p-exponent extension of the
second-moment term, enabling both positive and negative exponents, and
implement it through cyclic scheduling. Theory and experiments show that
adaptive methods emphasize low frequencies, SGD is near-neutral, and negative
exponents amplify high-frequency information. Cyclic scheduling broadens
spectral coverage, improves cross-band fusion, and induces early
decision-boundary alignment, where accuracy improves even while loss remains
high. Across multiple benchmarks, with identical learning-rate strategies and
fixed hyperparameters, p-exponent cyclic scheduling consistently reduces test
error and demonstrates distinct convergence behavior; on some tasks, it matches
baseline accuracy with only one-quarter of the training cost. Overall, NSF
reveals the optimizer's role as an active spectral controller and provides a
unified, controllable, and efficient framework for first-order optimization.

</details>


### [47] [CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction](https://arxiv.org/abs/2509.04733)
*Yuzhu Chen,Yingjie Wang,Shunyu Liu,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: CoVeR是一种基于保形预测框架的新型解码策略，能在保持紧凑搜索空间的同时确保对理想轨迹的高覆盖概率，提供理论保证的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 主流解码策略如beam search缺乏可证明的覆盖保证，难以平衡搜索效率与多样化轨迹需求，特别是在需要长尾序列的实际应用中。

Method: 提出CoVeR模型无关解码策略，基于保形预测框架，通过理论分析建立PAC风格的泛化边界。

Result: 理论证明CoVeR能够渐近地实现至少1-α的覆盖率，其中α∈(0,1)是任意目标水平。

Conclusion: CoVeR解决了现有解码策略的覆盖保证问题，为复杂推理任务提供了理论可靠且高效的解码方案。

Abstract: Autoregressive pre-trained models combined with decoding methods have
achieved impressive performance on complex reasoning tasks. While mainstream
decoding strategies such as beam search can generate plausible candidate sets,
they often lack provable coverage guarantees, and struggle to effectively
balance search efficiency with the need for versatile trajectories,
particularly those involving long-tail sequences that are essential in certain
real-world applications. To address these limitations, we propose
\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal
prediction framework that simultaneously maintains a compact search space and
ensures high coverage probability over desirable trajectories. Theoretically,
we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR}
asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target
level $\alpha \in (0,1)$.

</details>


### [48] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: Beyond I-Con框架通过探索不同的统计散度和相似性核函数，系统性地发现新的损失函数，在无监督聚类、监督对比学习和降维任务中均取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: I-Con框架发现现有表示学习方法隐含地最小化数据分布与学习分布之间的KL散度，但KL散度的不对称性和无界性可能导致优化问题与真实目标不一致。

Method: 提出Beyond I-Con框架，通过替换KL散度为其他统计散度（如总变差距离、有界f-散度）和不同的相似性核函数（如距离核代替角度核），系统性地探索新的损失函数设计。

Result: 在DINO-ViT嵌入的无监督聚类中达到SOTA；在监督对比学习中超越标准方法；在降维任务中比SNE获得更好的定性结果和下游任务性能。

Conclusion: 统计散度和相似性核函数的选择对表示学习优化至关重要，Beyond I-Con框架为系统探索这些选择提供了有效途径。

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>


### [49] [VARMA-Enhanced Transformer for Time Series Forecasting](https://arxiv.org/abs/2509.04782)
*Jiajun Song,Xiaoou Liu*

Main category: cs.LG

TL;DR: VARMAformer结合了现代Transformer架构和经典VARMA统计模型的优势，通过VARMA特征提取器和增强注意力机制，在时间序列预测中实现了更好的性能


<details>
  <summary>Details</summary>
Motivation: 现有的简化Transformer架构（如CATS）虽然高效，但可能忽略了经典统计模型（如VARMA）能够捕捉的细粒度局部时间依赖性

Method: 提出VARMAformer架构，包含两个关键创新：1）VARMA启发的特征提取器（VFE）在patch级别显式建模自回归和移动平均模式；2）VARMA增强注意力机制（VE-atten）使用时序门控使查询更具上下文感知

Result: 在广泛使用的基准数据集上进行大量实验，证明该模型始终优于现有的最先进方法

Conclusion: 将经典统计洞察融入现代深度学习框架对时间序列预测有显著益处，验证了这种融合方法的有效性

Abstract: Transformer-based models have significantly advanced time series forecasting.
Recent work, like the Cross-Attention-only Time Series transformer (CATS),
shows that removing self-attention can make the model more accurate and
efficient. However, these streamlined architectures may overlook the
fine-grained, local temporal dependencies effectively captured by classical
statistical models like Vector AutoRegressive Moving Average model (VARMA). To
address this gap, we propose VARMAformer, a novel architecture that synergizes
the efficiency of a cross-attention-only framework with the principles of
classical time series analysis. Our model introduces two key innovations: (1) a
dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models
autoregressive (AR) and moving-average (MA) patterns at the patch level, and
(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal
gate to make queries more context-aware. By fusing these classical insights
into a modern backbone, VARMAformer captures both global, long-range
dependencies and local, statistical structures. Through extensive experiments
on widely-used benchmark datasets, we demonstrate that our model consistently
outperforms existing state-of-the-art methods. Our work validates the
significant benefit of integrating classical statistical insights into modern
deep learning frameworks for time series forecasting.

</details>


### [50] [Graph Unlearning: Efficient Node Removal in Graph Neural Networks](https://arxiv.org/abs/2509.04785)
*Faqian Guan,Tianqing Zhu,Zhoutian Wang,Wei Ren,Wanlei Zhou*

Main category: cs.LG

TL;DR: 本文提出三种新的节点反学习方法，通过利用图的拓扑特征来高效移除GNN模型中的敏感训练节点信息，以保护隐私并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有节点反学习方法要么限制GNN结构，要么没有有效利用图的拓扑特征，某些方法还会破坏图的拓扑结构，影响性能效率。

Method: 提出三种新方法：基于类别的标签替换、拓扑引导的邻居平均后验概率、类一致的邻居节点过滤。后两种方法有效利用图的拓扑特征。

Result: 在三个标准数据集上进行实验，从模型效用、反学习效用和反学习效率三个维度评估。实验结果显示方法在效用和效率方面都优于现有最优方法。

Conclusion: 提出的方法能够高效移除敏感训练节点，保护GNN模型中的隐私信息，为节点反学习领域提供了有价值的见解。

Abstract: With increasing concerns about privacy attacks and potential sensitive
information leakage, researchers have actively explored methods to efficiently
remove sensitive training data and reduce privacy risks in graph neural network
(GNN) models. Node unlearning has emerged as a promising technique for
protecting the privacy of sensitive nodes by efficiently removing specific
training node information from GNN models. However, existing node unlearning
methods either impose restrictions on the GNN structure or do not effectively
utilize the graph topology for node unlearning. Some methods even compromise
the graph's topology, making it challenging to achieve a satisfactory
performance-complexity trade-off. To address these issues and achieve efficient
unlearning for training node removal in GNNs, we propose three novel node
unlearning methods: Class-based Label Replacement, Topology-guided Neighbor
Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among
these methods, Topology-guided Neighbor Mean Posterior Probability and
Class-consistent Neighbor Node Filtering effectively leverage the topological
features of the graph, resulting in more effective node unlearning. To validate
the superiority of our proposed methods in node unlearning, we conducted
experiments on three benchmark datasets. The evaluation criteria included model
utility, unlearning utility, and unlearning efficiency. The experimental
results demonstrate the utility and efficiency of the proposed methods and
illustrate their superiority compared to state-of-the-art node unlearning
methods. Overall, the proposed methods efficiently remove sensitive training
nodes and protect the privacy information of sensitive nodes in GNNs. The
findings contribute to enhancing the privacy and security of GNN models and
provide valuable insights into the field of node unlearning.

</details>


### [51] [An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning](https://arxiv.org/abs/2509.04815)
*Wonseo Jang,Dongjae Kim*

Main category: cs.LG

TL;DR: 提出了ACED-DQN框架，通过集成多样化DQN变体和仲裁控制机制来解决深度强化学习在持续学习中的灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在静态环境中学习最优策略很有效，但在持续强化学习场景中容易发生灾难性遗忘，导致性能下降。受人类前额叶皮层决策机制的启发

Method: 集成多个具有多样化价值函数的DQN变体，并采用仲裁控制机制优先选择最近试验中误差较小的智能体

Result: 在静态和持续环境中都表现出显著的性能提升，实证证据证明了仲裁控制对多样化DQN的有效性

Conclusion: 提出了一个受人类大脑启发的框架，使强化学习智能体能够持续学习，有效解决了灾难性遗忘问题

Abstract: Deep reinforcement learning (RL) models, despite their efficiency in learning
an optimal policy in static environments, easily loses previously learned
knowledge (i.e., catastrophic forgetting). It leads RL models to poor
performance in continual reinforcement learning (CRL) scenarios. To address
this, we present an arbitration control mechanism over an ensemble of RL
agents. It is motivated by and closely aligned with how humans make decisions
in a CRL context using an arbitration control of multiple RL agents in parallel
as observed in the prefrontal cortex. We integrated two key ideas into our
model: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have
diverse value functions and (2) an arbitration control that prioritizes agents
with higher reliability (i.e., less error) in recent trials. We propose a
framework for CRL, an Arbitration Control for an Ensemble of Diversified DQN
variants (ACED-DQN). We demonstrate significant performance improvements in
both static and continual environments, supported by empirical evidence showing
the effectiveness of arbitration control over diversified DQNs during training.
In this work, we introduced a framework that enables RL agents to continuously
learn, with inspiration from the human brain.

</details>


### [52] [Revolution or Hype? Seeking the Limits of Large Models in Hardware Design](https://arxiv.org/abs/2509.04905)
*Qiang Xu,Leon Stok,Rolf Drechsler,Xi Wang,Grace Li Zhang,Igor L. Markov*

Main category: cs.LG

TL;DR: 这篇论文作为ICCAD 2025会议的基础文本，批判性分析大型AI模型在硬件设计中的实际能力、基本限制和未来前景，提供了有关这一争议性技术趋势的权威概述。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和大型电路模型的突破在EDA社区引发了兴奋，但也带来了对其真实价值的怀疑。论文旨在批判性地考察这些AI模型是否真正能够革呼电路设计，还是仅为一场暂时的激情。

Method: 聚集来自学术界和产业界领军专家的观点，系统分析关于可靠性、可扩展性和可解释性的核心论点，形成有关大型AI模型能否超越或补充传统EDA方法的辩论框架。

Result: 提供了有关大型AI模型在硬件设计中应用的权威概览，对当今最具争议性和影响力的技术趋势提供了新的见解。

Conclusion: 该论文作为基础性文本，通过多角度的专家视角和批判性分析，为讨论大型AI模型在电子设计自动化领域的真实价值和发展前景提供了重要的框架和见解。

Abstract: Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models
(LCMs) have sparked excitement across the electronic design automation (EDA)
community, promising a revolution in circuit design and optimization. Yet, this
excitement is met with significant skepticism: Are these AI models a genuine
revolution in circuit design, or a temporary wave of inflated expectations?
This paper serves as a foundational text for the corresponding ICCAD 2025
panel, bringing together perspectives from leading experts in academia and
industry. It critically examines the practical capabilities, fundamental
limitations, and future prospects of large AI models in hardware design. The
paper synthesizes the core arguments surrounding reliability, scalability, and
interpretability, framing the debate on whether these models can meaningfully
outperform or complement traditional EDA methods. The result is an
authoritative overview offering fresh insights into one of today's most
contentious and impactful technology trends.

</details>


### [53] [Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series](https://arxiv.org/abs/2509.04921)
*Yuki Takemoto*

Main category: cs.LG

TL;DR: 提出了一种通过生成人工混沌时间序列和重采样技术来模拟金融时间序列的方法，使用100亿训练样本进行大规模预训练，在比特币交易数据上实现了零样本预测，交易策略表现显著优于自相关模型。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测具有挑战性，现有方法难以处理混沌特性。研究者希望通过生成合成混沌时间序列来构建多样化数据集，提升模型在金融领域的预测能力。

Method: 生成人工混沌时间序列，应用重采样技术模拟金融时间序列数据作为训练样本。通过增加重采样间隔扩展预测范围，使用100亿样本进行大规模预训练，在真实比特币数据上进行零样本预测测试。

Result: 基于预测结果的简单交易策略在盈利能力评估中显著优于自相关模型。在大规模预训练过程中观察到类似缩放定律的现象，即通过指数增加训练样本可以在扩展预测范围时保持一定水平的预测性能。

Conclusion: 如果这种缩放定律在各种混沌模型中都具有鲁棒性，则表明通过投入大量计算资源有可能预测近期事件。未来研究应关注更大规模训练和验证该缩放定律在不同混沌模型中的适用性。

Abstract: Time series forecasting plays a critical role in decision-making processes
across diverse fields including meteorology, traffic, electricity, economics,
finance, and so on. Especially, predicting returns on financial instruments is
a challenging problem. Some researchers have proposed time series foundation
models applicable to various forecasting tasks. Simultaneously, based on the
recognition that real-world time series exhibit chaotic properties, methods
have been developed to artificially generate synthetic chaotic time series,
construct diverse datasets and train models. In this study, we propose a
methodology for modeling financial time series by generating artificial chaotic
time series and applying resampling techniques to simulate financial time
series data, which we then use as training samples. Increasing the resampling
interval to extend predictive horizons, we conducted large-scale pre-training
using 10 billion training samples for each case. We subsequently created test
datasets for multiple timeframes using actual Bitcoin trade data and performed
zero-shot prediction without re-training the pre-trained model. The results of
evaluating the profitability of a simple trading strategy based on these
predictions demonstrated significant performance improvements over
autocorrelation models. During the large-scale pre-training process, we
observed a scaling law-like phenomenon that we can achieve predictive
performance at a certain level with extended predictive horizons for chaotic
time series by increasing the number of training samples exponentially. If this
scaling law proves robust and holds true across various chaotic models, it
suggests the potential to predict near-future events by investing substantial
computational resources. Future research should focus on further large-scale
training and verifying the applicability of this scaling law to diverse chaotic
models.

</details>


### [54] [A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection](https://arxiv.org/abs/2509.04925)
*Jiale Zhang,Pengfei He,Fei Li,Kewei Li,Yan Wang,Lan Huang,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: TrailGate是一个结合机器学习和深度学习的网络入侵检测框架，使用Transformer和BiGRU架构，通过特征选择和数据增强技术来检测常见攻击和新兴威胁。


<details>
  <summary>Details</summary>
Motivation: 当前网络流量激增，传统机器学习方法难以处理复杂网络入侵模式，存在数据稀缺和类别不平衡问题，需要更强大的入侵检测解决方案。

Method: 整合Transformer和双向门控循环单元(BiGRU)架构，采用先进的特征选择策略和数据增强技术，结合机器学习和深度学习技术。

Result: TrailGate能够有效识别常见攻击类型，并在检测和缓解新兴威胁方面表现出色，能够快速识别和中和源自现有范式的新威胁。

Conclusion: 该研究提出的TrailGate框架通过算法融合成功解决了网络入侵检测中的数据稀缺和类别不平衡问题，为应对快速演变的网络威胁提供了有效解决方案。

Abstract: In today's fast-paced digital communication, the surge in network traffic
data and frequency demands robust and precise network intrusion solutions.
Conventional machine learning methods struggle to grapple with complex patterns
within the vast network intrusion datasets, which suffer from data scarcity and
class imbalance. As a result, we have integrated machine learning and deep
learning techniques within the network intrusion detection system to bridge
this gap. This study has developed TrailGate, a novel framework that combines
machine learning and deep learning techniques. By integrating Transformer and
Bidirectional Gated Recurrent Unit (BiGRU) architectures with advanced feature
selection strategies and supplemented by data augmentation techniques,
TrailGate can identifies common attack types and excels at detecting and
mitigating emerging threats. This algorithmic fusion excels at detecting common
and well-understood attack types and has the unique ability to swiftly identify
and neutralize emerging threats that stem from existing paradigms.

</details>


### [55] [Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics](https://arxiv.org/abs/2509.04942)
*Heinke Hihn,Dennis A. V. Dittrich,Carl Jeske,Cayo Costa Sobral,Helio Pais,Timm Lochmann*

Main category: cs.LG

TL;DR: 基于Sentence-BERT模型的嵌入对齐方法，通过语义搜索将式工作标题分类到标准职业分类系统


<details>
  <summary>Details</summary>
Motivation: 解决不同来源职业数据跨源推理的长期瓶颈，提供比人工维护本体论更可扩展的解决方案

Method: 使用德国联邦就业局公开数据构建数据集，细调Sentence-BERT模型学习本体论结构，通过近似最近邻搜索实现语义分类

Result: 建立了一种可扩展的分类方法，能够将任意式工德语工作标题与标准职业分类系统对齐

Conclusion: 该方法提供了一种可扩展的职业数据对齐方案，有力支持多语言和多本体论的加入，为劳动力市场分析提供了更灵活的工具

Abstract: The limited ability to reason across occupational data from different sources
is a long-standing bottleneck for data-driven labour market analytics. Previous
research has relied on hand-crafted ontologies that allow such reasoning but
are computationally expensive and require careful maintenance by human experts.
The rise of language processing machine learning models offers a scalable
alternative by learning shared semantic spaces that bridge diverse occupational
vocabularies without extensive human curation. We present an embedding-based
alignment process that links any free-form German job title to two established
ontologies - the German Klassifikation der Berufe and the International
Standard Classification of Education. Using publicly available data from the
German Federal Employment Agency, we construct a dataset to fine-tune a
Sentence-BERT model to learn the structure imposed by the ontologies. The
enriched pairs (job title, embedding) define a similarity graph structure that
we can use for efficient approximate nearest-neighbour search, allowing us to
frame the classification process as a semantic search problem. This allows for
greater flexibility, e.g., adding more classes. We discuss design decisions,
open challenges, and outline ongoing work on extending the graph with other
ontologies and multilingual titles.

</details>


### [56] [Detecting Blinks in Healthy and Parkinson's EEG: A Deep Learning Perspective](https://arxiv.org/abs/2509.04951)
*Artem Lensky,Yiding Qiu*

Main category: cs.LG

TL;DR: 这篇论文评估了多种深度学习模型在EEG信号眼睛闭合检测中的性能，发现CNN-RNN混合模型表现最佳，在健康群体中达到95.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 眼睛闭合率及其变异性是监测认知负荷、注意力和神经疾病的重要生理标志，需要准确的检测方法来区分EEG信号中的不自主眼睛闭合和非眼睛闭合信号。

Method: 使用前额叶1、3或5个EEG电极构建了眼睛检测流程，将问题形式化为序列到序列任务。测试了多种深度学习架构，包括RNN、CNN、深度卷积CNN、时间卷积网络(TCN)、Transformer模型和混合架构，使用UCSD公开数据集进行训练和测试。

Result: CNN-RNN混合模型表现最佳：在健康群体中使用1、3、5个通道分别达到93.8%、95.4%和95.8%的准确率；在帕金森病患者中分别达到73.8%、75.4%和75.8%。模型对抖动具有较强的健壮性。

Conclusion: 证明了深度学习模型特别是CNN-RNN混合架构在EEG眼睛检测中的效果，为计算眼睛闭合率和其他统计指标提供了可靠的技术支持，对于监测认知功能和神经疾病具有重要意义。

Abstract: Blinks in electroencephalography (EEG) are often treated as unwanted
artifacts. However, recent studies have demonstrated that blink rate and its
variability are important physiological markers to monitor cognitive load,
attention, and potential neurological disorders. This paper addresses the
critical task of accurate blink detection by evaluating various deep learning
models for segmenting EEG signals into involuntary blinks and non-blinks. We
present a pipeline for blink detection using 1, 3, or 5 frontal EEG electrodes.
The problem is formulated as a sequence-to-sequence task and tested on various
deep learning architectures including standard recurrent neural networks,
convolutional neural networks (both standard and depth-wise), temporal
convolutional networks (TCN), transformer-based models, and hybrid
architectures. The models were trained on raw EEG signals with minimal
pre-processing. Training and testing was carried out on a public dataset of 31
subjects collected at UCSD. This dataset consisted of 15 healthy participants
and 16 patients with Parkinson's disease allowing us to verify the model's
robustness to tremor. Out of all models, CNN-RNN hybrid model consistently
outperformed other models and achieved the best blink detection accuracy of
93.8%, 95.4% and 95.8% with 1, 3, and 5 channels in the healthy cohort and
correspondingly 73.8%, 75.4% and 75.8% in patients with PD. The paper compares
neural networks for the task of segmenting EEG recordings to involuntary blinks
and no blinks allowing for computing blink rate and other statistics.

</details>


### [57] [On the Normalization of Confusion Matrices: Methods and Geometric Interpretations](https://arxiv.org/abs/2509.04959)
*Johan Erbani,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Sonia Ben Mokhtar,Diana Nurbakova*

Main category: cs.LG

TL;DR: 提出使用双随机归一化方法来分离混淆矩阵中的类别相似性和分布偏差，从而更准确地诊断模型行为。


<details>
  <summary>Details</summary>
Motivation: 混淆矩阵的值同时受到类别相似性和分布偏差的影响，难以区分两者的独立贡献，需要一种方法来解耦这些误差源。

Method: 引入基于迭代比例拟合的双随机归一化方法，这是一种行和列归一化的泛化，能够恢复类别相似性的底层结构。

Result: 该方法能够有效分离错误来源，支持更有针对性的模型改进，并在模型的内部类别表示空间中提供几何解释。

Conclusion: 双随机归一化为混淆矩阵分析提供了新的视角，能够更深入地理解分类器的行为特征和归一化所揭示的信息。

Abstract: The confusion matrix is a standard tool for evaluating classifiers by
providing insights into class-level errors. In heterogeneous settings, its
values are shaped by two main factors: class similarity -- how easily the model
confuses two classes -- and distribution bias, arising from skewed
distributions in the training and test sets. However, confusion matrix values
reflect a mix of both factors, making it difficult to disentangle their
individual contributions. To address this, we introduce bistochastic
normalization using Iterative Proportional Fitting, a generalization of row and
column normalization. Unlike standard normalizations, this method recovers the
underlying structure of class similarity. By disentangling error sources, it
enables more accurate diagnosis of model behavior and supports more targeted
improvements. We also show a correspondence between confusion matrix
normalizations and the model's internal class representations. Both standard
and bistochastic normalizations can be interpreted geometrically in this space,
offering a deeper understanding of what normalization reveals about a
classifier.

</details>


### [58] [Neuro-Spectral Architectures for Causal Physics-Informed Networks](https://arxiv.org/abs/2509.04966)
*Arthur Bizzi,Leonardo M. Moreira,Márcio Marques,Leonardo Mendonça,Christian Júnior de Oliveira,Vitor Balestro,Lucas dos Santos Fernandez,Daniel Yukimura,Pavel Petrov,João M. Pereira,Tiago Novello,Lucas Nissenbaum*

Main category: cs.LG

TL;DR: NeuSA是一种基于谱方法的物理信息神经网络，通过将PDE投影到谱基上来克服标准PINNs的频谱偏差和因果性问题，在波动方程基准测试中表现出更快的收敛速度和更好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 标准MLP-based PINNs在处理复杂初值问题时经常无法收敛，导致解违反因果关系并存在对低频分量的频谱偏差。

Method: NeuSA学习PDE在谱基上的投影，得到动力学的有限维表示，然后通过改进的神经ODE进行积分，利用谱表示的高频分量克服频谱偏差，继承NODE的因果结构，并基于经典方法设计初始化方案。

Result: 在线性和非线性波动方程的基准测试中，NeuSA相比其他架构表现出更强的性能，具有更快的收敛速度、改进的时间一致性和优越的预测精度。

Conclusion: NeuSA通过结合谱方法和神经ODE的优势，成功解决了PINNs在复杂PDE问题中的收敛困难、因果性违反和频谱偏差等问题，为PDE求解提供了有效的神经网络框架。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural
framework for solving partial differential equations (PDEs). However, standard
MLP-based PINNs often fail to converge when dealing with complex initial-value
problems, leading to solutions that violate causality and suffer from a
spectral bias towards low-frequency components. To address these issues, we
introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired
by classical spectral methods, designed to solve linear and nonlinear PDEs with
variable coefficients. NeuSA learns a projection of the underlying PDE onto a
spectral basis, leading to a finite-dimensional representation of the dynamics
which is then integrated with an adapted Neural ODE (NODE). This allows us to
overcome spectral bias, by leveraging the high-frequency components enabled by
the spectral representation; to enforce causality, by inheriting the causal
structure of NODEs, and to start training near the target solution, by means of
an initialization scheme based on classical methods. We validate NeuSA on
canonical benchmarks for linear and nonlinear wave equations, demonstrating
strong performance as compared to other architectures, with faster convergence,
improved temporal consistency and superior predictive accuracy. Code and
pretrained models will be released.

</details>


### [59] [Topology-Aware Graph Reinforcement Learning for Dynamic Routing in Cloud Networks](https://arxiv.org/abs/2509.04973)
*Yuxi Wang,Heyao Liu,Guanzi Yao,Nyutian Long,Yue Kang*

Main category: cs.LG

TL;DR: 提出拓扑感知图强化学习方法优化云服务器路由策略，通过结构感知状态编码和策略自适应图更新机制，在动态拓扑下实现高效稳定的路由决策。


<details>
  <summary>Details</summary>
Motivation: 解决云服务器环境中动态拓扑变化导致的决策不稳定和结构感知不足的问题，提升路由策略在复杂网络环境中的性能。

Method: 结合Structure-Aware State Encoding (SASE)模块进行多层图卷积和结构位置嵌入建模节点状态，以及Policy-Adaptive Graph Update (PAGU)机制根据策略行为变化和奖励反馈调整图结构。

Result: 在GEANT拓扑数据集上的实验表明，该方法在吞吐量、延迟控制和链路平衡等多个性能指标上优于现有图强化学习模型，实现了动态复杂云网络中的高效鲁棒路由。

Conclusion: 所提出的拓扑感知图强化学习方法能够有效处理动态拓扑环境，通过结构感知和自适应更新机制显著提升路由策略的性能和稳定性。

Abstract: This paper proposes a topology-aware graph reinforcement learning approach to
address the routing policy optimization problem in cloud server environments.
The method builds a unified framework for state representation and structural
evolution by integrating a Structure-Aware State Encoding (SASE) module and a
Policy-Adaptive Graph Update (PAGU) mechanism. It aims to tackle the challenges
of decision instability and insufficient structural awareness under dynamic
topologies. The SASE module models node states through multi-layer graph
convolution and structural positional embeddings, capturing high-order
dependencies in the communication topology and enhancing the expressiveness of
state representations. The PAGU module adjusts the graph structure based on
policy behavior shifts and reward feedback, enabling adaptive structural
updates in dynamic environments. Experiments are conducted on the real-world
GEANT topology dataset, where the model is systematically evaluated against
several representative baselines in terms of throughput, latency control, and
link balance. Additional experiments, including hyperparameter sensitivity,
graph sparsity perturbation, and node feature dimensionality variation, further
explore the impact of structure modeling and graph updates on model stability
and decision quality. Results show that the proposed method outperforms
existing graph reinforcement learning models across multiple performance
metrics, achieving efficient and robust routing in dynamic and complex cloud
networks.

</details>


### [60] [Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization](https://arxiv.org/abs/2509.04977)
*Shuaicheng Niu,Guohao Chen,Deyu Chen,Yifan Zhang,Jiaxiang Wu,Zhiquan Wen,Yaofo Chen,Peilin Zhao,Chunyan Miao,Mingkui Tan*

Main category: cs.LG

TL;DR: 这篇论文研究了测试时适配(TTA)的稳定性问题，发现批处理正则化层是影响稳定性的关键因素，并提出了SAR和SAR^2方法来提高TTA在混合分布移动、小批量和不平衡标签分布下的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 实际部署中的TTA方法在测试数据存在混合分布移动、小批量大小和在线不平衡标签分布时可能失效或造成模型性能下降，这是阻碍TTA方法在真实世界部署的关键问题。

Method: 首先分析发现批处正则化层是影响TTA稳定性的关键因素，但使用批处无关的正则化层仍然存在模型崩溃问题。提出了两个方法：1) SAR方法：通过删除有大梯度的噪声样本和鼓励模型权重进入平坦最小值来提高稳定性；2) SAR^2方法：在SAR基础上添加了重复性正则化器和不公平正则化器来防止表征崩溃。

Result: 实验结果表明，提出的SAR和SAR^2方法在上述野外测试场景下较以往方法表现更加稳定，计算效率高且性能优异。

Conclusion: 论文通过分析TTA稳定性问题的根源，提出了有效的解决方案，为实际部署中的测试时适配提供了可靠的技术支持，显著提高了TTA方法在复杂真实场景下的稳定性和可靠性。

Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model
performance when test data have: 1) mixed distribution shifts, 2) small batch
sizes, 3) online imbalanced label distribution shifts. This is often a key
obstacle preventing existing TTA methods from being deployed in the real world.
In this paper, we investigate the unstable reasons and find that the batch norm
layer is a crucial factor hindering TTA stability. Conversely, TTA can perform
more stably with batch-agnostic norm layers, i.e., group or layer norm.
However, we observe that TTA with group and layer norms does not always succeed
and still suffers many failure cases, i.e., the model collapses into trivial
solutions by assigning the same class label for all samples. By digging into
this, we find that, during the collapse process: 1) the model gradients often
undergo an initial explosion followed by rapid degradation, suggesting that
certain noisy test samples with large gradients may disrupt adaptation; and 2)
the model representations tend to exhibit high correlations and classification
bias. To address this, we first propose a sharpness-aware and reliable entropy
minimization method, called SAR, for stabilizing TTA from two aspects: 1)
remove partial noisy samples with large gradients, 2) encourage model weights
to go to a flat minimum so that the model is robust to the remaining noisy
samples. Based on SAR, we further introduce SAR^2 to prevent representation
collapse with two regularizers: 1) a redundancy regularizer to reduce
inter-dimensional correlations among centroid-invariant features; and 2) an
inequity regularizer to maximize the prediction entropy of a prototype
centroid, thereby penalizing biased representations toward any specific class.
Promising results demonstrate that our methods perform more stably over prior
methods and are computationally efficient under the above wild test scenarios.

</details>


### [61] [Directed Evolution of Proteins via Bayesian Optimization in Embedding Space](https://arxiv.org/abs/2509.04998)
*Matouš Soldát,Jiří Kléma*

Main category: cs.LG

TL;DR: 基于飞行语言模型序列嵌入的贝叶斯优化方法，通过更好的蛋白质变异表征提升了导向进化的效果，在相同筛选次数下获得更优的蛋白功能


<details>
  <summary>Details</summary>
Motivation: 导向进化中筛选蛋白变异体的过程耗费费时，机器学习方法可以帮助选择信息量丰富或有前景的变异体，提高筛选效率和质量

Method: 结合贝叶斯优化与预训练蛋白质语言模型提取的信息表征来表征蛋白变异体

Result: 新的序列嵌入表征显著提升了贝叶斯优化的性能，在相同筛选次数下获得更好的结果

Conclusion: 该方法在回归目标下超越了现有的机器学习辅助导向进化方法

Abstract: Directed evolution is an iterative laboratory process of designing proteins
with improved function by iteratively synthesizing new protein variants and
evaluating their desired property with expensive and time-consuming biochemical
screening. Machine learning methods can help select informative or promising
variants for screening to increase their quality and reduce the amount of
necessary screening. In this paper, we present a novel method for
machine-learning-assisted directed evolution of proteins which combines
Bayesian optimization with informative representation of protein variants
extracted from a pre-trained protein language model. We demonstrate that the
new representation based on the sequence embeddings significantly improves the
performance of Bayesian optimization yielding better results with the same
number of conducted screening in total. At the same time, our method
outperforms the state-of-the-art machine-learning-assisted directed evolution
methods with regression objective.

</details>


### [62] [Depth-Aware Initialization for Stable and Efficient Neural Network Training](https://arxiv.org/abs/2509.05018)
*Vijay Pandey*

Main category: cs.LG

TL;DR: 本文综述了深度网络初始化方法的研究进展，提出了一种新的初始化方案，通过结合每个层的深度信息来灵活增加网络方差，在深度网络中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有初始化方法在深度网络中存在限制，理论假设的单位方差在深度网络中表现不佳，需要从第一层到最后一层逐渐增加方差。

Method: 提出了一种新的初始化方案，结合每个层的深度信息和整个网络的深度来灵活增加网络方差，以改善激活和梯度传播。

Result: 实验结果显示，提出的方法在深度网络中表现超过了现有的初始化方案。

Conclusion: 通过结合层次深度信息来灵活调整网络方差的初始化方法，能够有效改善深度网络的训练效果和性能。

Abstract: In past few years, various initialization schemes have been proposed. These
schemes are glorot initialization, He initialization, initialization using
orthogonal matrix, random walk method for initialization. Some of these methods
stress on keeping unit variance of activation and gradient propagation through
the network layer. Few of these methods are independent of the depth
information while some methods has considered the total network depth for
better initialization. In this paper, comprehensive study has been done where
depth information of each layer as well as total network is incorporated for
better initialization scheme. It has also been studied that for deeper networks
theoretical assumption of unit variance throughout the network does not perform
well. It requires the need to increase the variance of the network from first
layer activation to last layer activation. We proposed a novel way to increase
the variance of the network in flexible manner, which incorporates the
information of each layer depth. Experiments shows that proposed method
performs better than the existing initialization scheme.

</details>


### [63] [MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer](https://arxiv.org/abs/2509.05037)
*Noorul Wahab,Ethar Alzaid,Jiaqi Lv,Adam Shephard,Shan E Ahmed Raza*

Main category: cs.LG

TL;DR: MultiSurv是一个多模态深度生存模型，整合临床、MRI、RNA-seq和病理数据，使用DeepHit和跨模态注意力机制，在前列腺癌和膀胱癌复发预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确预测癌症患者的时间-事件结果对治疗规划和患者管理至关重要，需要整合多模态异质数据来捕捉互补的预后信号。

Method: 采用DeepHit框架，加入投影层和跨模态注意力机制，整合临床、MRI、RNA-seq和全切片病理特征等多模态数据。

Result: 在前列腺癌任务中C-index达0.843（交叉验证）和0.818（开发集），膀胱癌任务中C-index为0.662（交叉验证）和0.457（开发集）。

Conclusion: 多模态整合与深度生存学习为个性化风险分层提供了有前景的途径，该框架可广泛应用于涉及异质生物医学数据的生存预测任务。

Abstract: Accurate prediction of time-to-event outcomes is a central challenge in
oncology, with significant implications for treatment planning and patient
management. In this work, we present MultiSurv, a multimodal deep survival
model utilising DeepHit with a projection layer and inter-modality
cross-attention, which integrates heterogeneous patient data, including
clinical, MRI, RNA-seq and whole-slide pathology features. The model is
designed to capture complementary prognostic signals across modalities and
estimate individualised time-to-biochemical recurrence in prostate cancer and
time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the
context of the CHIMERA Grand Challenge, across two of the three provided tasks.
For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed
framework achieved a concordance index (C-index) of 0.843 on 5-folds
cross-validation and 0.818 on CHIMERA development set, demonstrating robust
discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the
model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on
development set, highlighting its adaptability and potential for clinical
translation. These results suggest that leveraging multimodal integration with
deep survival learning provides a promising pathway toward personalised risk
stratification in prostate and bladder cancer. Beyond the challenge setting,
our framework is broadly applicable to survival prediction tasks involving
heterogeneous biomedical data.

</details>


### [64] [Recurrent State Encoders for Efficient Neural Combinatorial Optimization](https://arxiv.org/abs/2509.05084)
*Tim Dernedde,Daniela Thyssens,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: 通过采用递归编码器来重用之前计算的状态嵌入，在神经组合优化中实现了更高效的计算。该方法在保持或提升性能的同时，显著减少了模型深度和迟延。


<details>
  <summary>Details</summary>
Motivation: 在神经组合优化中，构建方法逐步添加解组件时，状态变化逐步很小，可以重用之前的计算结果来提高效率。

Method: 训练一个递归编码器，计算状态嵌入时不仅考虑当前状态，还考虑之前步骤的嵌入结果，从而重用计算。

Result: 递归编码器在层数减少3倍的情况下，仍能达到等效或更好的性能，显著提升了迟延性能。在TSP、CVRP和OP问题上都发现了实践意义。

Conclusion: 递归编码器通过重用计算，能够在保持性能的同时显著提高效率，对于神经组合优化的实际应用具有重要价值。

Abstract: The primary paradigm in Neural Combinatorial Optimization (NCO) are
construction methods, where a neural network is trained to sequentially add one
solution component at a time until a complete solution is constructed. We
observe that the typical changes to the state between two steps are small,
since usually only the node that gets added to the solution is removed from the
state. An efficient model should be able to reuse computation done in prior
steps. To that end, we propose to train a recurrent encoder that computes the
state embeddings not only based on the state but also the embeddings of the
step before. We show that the recurrent encoder can achieve equivalent or
better performance than a non-recurrent encoder even if it consists of
$3\times$ fewer layers, thus significantly improving on latency. We demonstrate
our findings on three different problems: the Traveling Salesman Problem (TSP),
the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem
(OP) and integrate the models into a large neighborhood search algorithm, to
showcase the practical relevance of our findings.

</details>


### [65] [HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions](https://arxiv.org/abs/2509.05117)
*Rafael Bischof,Michal Piovarči,Michael A. Kraus,Siddhartha Mishra,Bernd Bickel*

Main category: cs.LG

TL;DR: HyPINO是一个多物理神经算子，通过超网络和混合监督实现参数化PDE的零样本泛化，无需任务特定微调，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理信息神经网络(PINNs)对每个PDE问题需要重新训练的问题，开发一个能够零样本泛化到各类参数化PDE的统一框架。

Method: 使用Swin Transformer超网络结合混合监督：MMS生成的解析解标签数据和物理信息目标优化的无标签数据，并引入迭代细化程序生成集成解。

Result: 在7个PINN基准问题上实现强零样本精度，优于U-Nets、Poseidon和PINO；迭代细化使平均L2损失降低100倍；初始化PINNs收敛更快且误差更低。

Conclusion: HyPINO为扩展神经算子解决复杂非线性高维PDE问题提供了可扩展的基础方法，显著提高了精度并降低了计算成本。

Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot
generalization across a broad class of parametric PDEs without requiring
task-specific fine-tuning. Our approach combines a Swin Transformer-based
hypernetwork with mixed supervision: (i) labeled data from analytical solutions
generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled
samples optimized using physics-informed objectives. The model maps PDE
parametrizations to target Physics-Informed Neural Networks (PINNs) and can
handle linear elliptic, hyperbolic, and parabolic equations in two dimensions
with varying source terms, geometries, and mixed Dirichlet/Neumann boundary
conditions, including interior boundaries. HyPINO achieves strong zero-shot
accuracy on seven benchmark problems from PINN literature, outperforming
U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we
introduce an iterative refinement procedure that compares the physics of the
generated PINN to the requested PDE and uses the discrepancy to generate a
"delta" PINN. Summing their contributions and repeating this process forms an
ensemble whose combined solution progressively reduces the error on six
benchmarks and achieves over 100x gain in average $L_2$ loss in the best case,
while retaining forward-only inference. Additionally, we evaluate the
fine-tuning behavior of PINNs initialized by HyPINO and show that they converge
faster and to lower final error than both randomly initialized and
Reptile-meta-learned PINNs on five benchmarks, performing on par on the
remaining two. Our results highlight the potential of this scalable approach as
a foundation for extending neural operators toward solving increasingly
complex, nonlinear, and high-dimensional PDE problems with significantly
improved accuracy and reduced computational cost.

</details>


### [66] [Should We Always Train Models on Fine-Grained Classes?](https://arxiv.org/abs/2509.05130)
*Davide Pirovano,Federico Milanesio,Michele Caselle,Piero Fariselli,Matteo Osella*

Main category: cs.LG

TL;DR: 细粒度标签训练并不总能提升分类性能，其效果取决于数据几何结构、标签层次关系、数据集大小和模型容量等因素


<details>
  <summary>Details</summary>
Motivation: 研究在层次化标签分类问题中，使用更细粒度的标签进行训练是否能普遍提升模型性能，以及影响这种策略效果的关键因素

Method: 使用真实和合成数据集进行实验，分析数据几何结构、标签层次关系、数据集大小和模型容量对细粒度标签训练效果的影响

Result: 细粒度标签训练并非普遍有效，其效果严重依赖于数据几何结构与标签层次的关系，以及数据集大小和模型容量等条件

Conclusion: 细粒度标签训练的效果具有条件依赖性，需要根据具体的数据特征和模型条件来评估是否采用这种策略

Abstract: In classification problems, models must predict a class label based on the
input data features. However, class labels are organized hierarchically in many
datasets. While a classification task is often defined at a specific level of
this hierarchy, training can utilize a finer granularity of labels. Empirical
evidence suggests that such fine-grained training can enhance performance. In
this work, we investigate the generality of this observation and explore its
underlying causes using both real and synthetic datasets. We show that training
on fine-grained labels does not universally improve classification accuracy.
Instead, the effectiveness of this strategy depends critically on the geometric
structure of the data and its relations with the label hierarchy. Additionally,
factors such as dataset size and model capacity significantly influence whether
fine-grained labels provide a performance benefit.

</details>


### [67] [On the Learnability of Distribution Classes with Adaptive Adversaries](https://arxiv.org/abs/2509.05137)
*Tosca Lechner,Alex Bie,Gautam Kamath*

Main category: cs.LG

TL;DR: 论文研究了在自适应对手存在下的分布类可学习性问题，证明了相对于加性自适应对手的可学习性比相对于加性非自适应对手的可学习性条件更强


<details>
  <summary>Details</summary>
Motivation: 探索在能够拦截并操纵学习器样本请求的自适应对手存在下，分布类的可学习性条件，与只能修改底层分布的非自适应对手形成对比

Method: 提出了一个考虑对手预算的通用自适应对手可学习性概念，通过理论分析比较了不同类型对手下的学习条件

Result: 证明了相对于加性自适应对手的可学习性是一个比相对于加性非自适应对手的可学习性更严格的条件

Conclusion: 自适应对手的存在显著增加了学习难度，需要更强的学习条件，这对现实世界中对抗性环境下的机器学习具有重要启示

Abstract: We consider the question of learnability of distribution classes in the
presence of adaptive adversaries -- that is, adversaries capable of
intercepting the samples requested by a learner and applying manipulations with
full knowledge of the samples before passing it on to the learner. This stands
in contrast to oblivious adversaries, who can only modify the underlying
distribution the samples come from but not their i.i.d.\ nature. We formulate a
general notion of learnability with respect to adaptive adversaries, taking
into account the budget of the adversary. We show that learnability with
respect to additive adaptive adversaries is a strictly stronger condition than
learnability with respect to additive oblivious adversaries.

</details>


### [68] [Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights](https://arxiv.org/abs/2509.05142)
*Cosmin-Andrei Hatfaludi,Alex Serban*

Main category: cs.LG

TL;DR: 这是一份关于联邦学习和基础模型融合的综述性论文，通过新的分类法对技术方法进行分类和对比，并以医疗健康领域为案例研究。


<details>
  <summary>Details</summary>
Motivation: 联邦学习能够在不共享私有数据的情况下进行协作模型训练，而基础模型的普及使得整合私有数据和扩大训练资源的需求日益增长。目前缺乏统一的综述性研究，需要对这两种范式的融合方法进行识别、分类和特征化分析。

Method: 采用文献调研方法，初始检索和审查了4,200多篇论文，通过包含标准筛选出250多篇详细审查的文献，包含42种独特方法。构建了基于发展生命周期阶段的新分类法，并从复杂性、效率和可扩展性方面对方法进行技术对比。

Result: 研究包含了联邦学习、自监督学习、微调、蓄缩和迁移学习等多个交叉主题。通过分类法对42种方法进行了系统性的对比分析，并以医疗健康领域为具体案例进行实践分析。

Conclusion: 该研究提供了一份自含的概述，不仅总结了该领域的现状，还为采用、发展和整合基础模型与联邦学习提供了实践见解和指南，尤其是在医疗健康领域具有重要的潜在影响。

Abstract: Federated learning has the potential to unlock siloed data and distributed
resources by enabling collaborative model training without sharing private
data. As more complex foundational models gain widespread use, the need to
expand training resources and integrate privately owned data grows as well. In
this article, we explore the intersection of federated learning and
foundational models, aiming to identify, categorize, and characterize technical
methods that integrate the two paradigms. As a unified survey is currently
unavailable, we present a literature survey structured around a novel taxonomy
that follows the development life-cycle stages, along with a technical
comparison of available methods. Additionally, we provide practical insights
and guidelines for implementing and evolving these methods, with a specific
focus on the healthcare domain as a case study, where the potential impact of
federated learning and foundational models is considered significant. Our
survey covers multiple intersecting topics, including but not limited to
federated learning, self-supervised learning, fine-tuning, distillation, and
transfer learning. Initially, we retrieved and reviewed a set of over 4,200
articles. This collection was narrowed to more than 250 thoroughly reviewed
articles through inclusion criteria, featuring 42 unique methods. The methods
were used to construct the taxonomy and enabled their comparison based on
complexity, efficiency, and scalability. We present these results as a
self-contained overview that not only summarizes the state of the field but
also provides insights into the practical aspects of adopting, evolving, and
integrating foundational models with federated learning.

</details>


### [69] [KVCompose: Efficient Structured KV Cache Compression with Composite Tokens](https://arxiv.org/abs/2509.05165)
*Dmitry Akulov,Mohamed Sana,Antonio De Domenico,Tareq Si Salem,Nicola Piovesan,Fadhel Ayed*

Main category: cs.LG

TL;DR: 基于注意力导向的层适应性复合token压缩框架，通过注意力评估token重要性、层间全局分配机制，在保持精度的同时大幅减少KV缓存占用，且完全兼容现有推理引擎。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理时，KV缓存占用线性增长成为性能瓶颈，而现有压缩方法存在固化算法、破坏张量布局或需专门计算内核等问题。

Method: 使用注意力评分估计token重要性，独立选择每个注意头的关键token，并将其对齐为符合标准缓存结构的复合token。通过全局分配机制根据各层信息密度动态分配保留容量。

Result: 该方法在大幅减少内存占用的同时保持了模型准确性，一致性超过之前的结构化和半结构化方法。

Conclusion: 该框架提供了一种简单有效、完全兼容标准推理流程的实用解决方案，为高效长上下文LLM部署提供了可扩展的途径。

Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient
autoregressive decoding; however, cache size grows linearly with context length
and model depth, becoming a major bottleneck in long-context inference. Prior
KV cache compression methods either enforce rigid heuristics, disrupt tensor
layouts with per-attention-head variability, or require specialized compute
kernels.
  We propose a simple, yet effective, KV cache compression framework based on
attention-guided, layer-adaptive composite tokens. Our method aggregates
attention scores to estimate token importance, selects head-specific tokens
independently, and aligns them into composite tokens that respect the uniform
cache structure required by existing inference engines. A global allocation
mechanism further adapts retention budgets across layers, assigning more
capacity to layers with informative tokens. This approach achieves significant
memory reduction while preserving accuracy, consistently outperforming prior
structured and semi-structured methods. Crucially, our approach remains fully
compatible with standard inference pipelines, offering a practical and scalable
solution for efficient long-context LLM deployment.

</details>


### [70] [Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection](https://arxiv.org/abs/2509.05190)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: 提出了一种轻量级一维CNN模型，通过结构化剪枝和温和早停技术，在减少50%权重和内存的同时保持甚至提升了癫痫检测性能


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生物医学信号处理中表现优异，但在实时检测或资源有限环境中面临模型大小和计算需求的挑战，需要开发更高效的解决方案

Method: 使用结构化剪枝方法，基于卷积核重要性移除50%的卷积核，并结合温和早停技术来防止过拟合

Result: 剪枝后模型在保持预测能力的同时，准确率从92.78%提升至92.87%，macro-F1分数从0.8686提升至0.8707，权重和内存减少50%

Conclusion: 结构化剪枝能有效去除冗余、提高泛化能力，结合温和早停技术为资源有限环境下的癫痫检测提供了有前景的高效可靠解决方案

Abstract: Deep learning models, especially convolutional neural networks (CNNs), have
shown considerable promise for biomedical signals such as EEG-based seizure
detection. However, these models come with challenges, primarily due to their
size and compute requirements in environments where real-time detection or
limited resources are available. In this study, we present a lightweight
one-dimensional CNN model with structured pruning to improve efficiency and
reliability. The model was trained with mild early stopping to address possible
overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.
Structured pruning of the baseline CNN involved removing 50% of the
convolutional kernels based on their importance to model predictions.
Surprisingly, after pruning the weights and memory by 50%, the new network was
still able to maintain predictive capabilities, while modestly increasing
precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we
present a convincing case that structured pruning removes redundancy, improves
generalization, and, in combination with mild early stopping, achieves a
promising way forward to improve seizure detection efficiency and reliability,
which is clear motivation for resource-limited settings.

</details>


### [71] [Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning](https://arxiv.org/abs/2509.05193)
*Bastien Dubail,Stefan Stojanovic,Alexandre Proutière*

Main category: cs.LG

TL;DR: 本文挑战了强化学习中后继度量具有低秩结构的常见假设，发现移位后继度量才具有自然低秩性，并提供了有限样本性能保证和新的马尔可夫链函数不等式分析。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习算法常隐含假设低秩结构，但本文发现传统后继度量本身并不低秩，而移位后的后继度量才具有低秩特性，这挑战了现有假设。

Method: 提出移位后继度量的概念，推导其低秩近似和估计的有限样本性能保证，引入Type II Poincaré不等式分析谱可恢复性参数，建立移位量与系统局部混合性质的联系。

Result: 理论分析表明近似误差和估计误差主要受谱可恢复性控制，所需移位量取决于高阶奇异值衰减且实际中通常很小，实验验证移位能提升目标条件强化学习性能。

Conclusion: 移位后继度量比传统后继度量更适合低秩近似，通过合理选择移位量可以显著改善强化学习算法的性能，为低秩结构假设提供了更准确的理论基础。

Abstract: Low-rank structure is a common implicit assumption in many modern
reinforcement learning (RL) algorithms. For instance, reward-free and
goal-conditioned RL methods often presume that the successor measure admits a
low-rank representation. In this work, we challenge this assumption by first
remarking that the successor measure itself is not low-rank. Instead, we
demonstrate that a low-rank structure naturally emerges in the shifted
successor measure, which captures the system dynamics after bypassing a few
initial transitions. We provide finite-sample performance guarantees for the
entry-wise estimation of a low-rank approximation of the shifted successor
measure from sampled entries. Our analysis reveals that both the approximation
and estimation errors are primarily governed by the so-called spectral
recoverability of the corresponding matrix. To bound this parameter, we derive
a new class of functional inequalities for Markov chains that we call Type II
Poincar\'e inequalities and from which we can quantify the amount of shift
needed for effective low-rank approximation and estimation. This analysis shows
in particular that the required shift depends on decay of the high-order
singular values of the shifted successor measure and is hence typically small
in practice. Additionally, we establish a connection between the necessary
shift and the local mixing properties of the underlying dynamical system, which
provides a natural way of selecting the shift. Finally, we validate our
theoretical findings with experiments, and demonstrate that shifting the
successor measure indeed leads to improved performance in goal-conditioned RL.

</details>


### [72] [RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks](https://arxiv.org/abs/2509.05207)
*Arefin Niam,Tevfik Kosar,M S Q Zulkar Nine*

Main category: cs.LG

TL;DR: RapidGNN是一个分布式GNN训练框架，通过确定性采样调度实现高效缓存构建和远程特征预取，相比基线方法训练吞吐量提升2.46-3倍，远程特征获取减少9.7-15.4倍。


<details>
  <summary>Details</summary>
Motivation: 大规模图数据上分布式训练GNN面临通信开销大的挑战，传统采样方法无法有效解决远程特征获取问题。

Method: 采用确定性采样调度策略，实现高效缓存构建和远程特征预取机制。

Result: 在基准图数据集上，训练吞吐量平均提升2.46-3倍，远程特征获取减少9.7-15.4倍，计算单元扩展接近线性，CPU和GPU能效分别提升44%和32%。

Conclusion: RapidGNN框架有效解决了分布式GNN训练中的通信瓶颈问题，实现了高性能和高能效的训练。

Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of
tasks in exploring structural relationships between entities. However, due to
the highly connected structure of the datasets, distributed training of GNNs on
large-scale graphs poses significant challenges. Traditional sampling-based
approaches mitigate the computational loads, yet the communication overhead
remains a challenge. This paper presents RapidGNN, a distributed GNN training
framework with deterministic sampling-based scheduling to enable efficient
cache construction and prefetching of remote features. Evaluation on benchmark
graph datasets demonstrates RapidGNN's effectiveness across different scales
and topologies. RapidGNN improves end-to-end training throughput by 2.46x to
3.00x on average over baseline methods across the benchmark datasets, while
cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further
demonstrates near-linear scalability with an increasing number of computing
units efficiently. Furthermore, it achieves increased energy efficiency over
the baseline methods for both CPU and GPU by 44% and 32%, respectively.

</details>


### [73] [Deep Learning-Enhanced for Amine Emission Monitoring and Performance Analysis in Industrial Carbon Capture Plants](https://arxiv.org/abs/2509.05241)
*Lokendra Poudel,David Tincher,Duy-Nhat Phan,Rahul Bhowmik*

Main category: cs.LG

TL;DR: 基于深度学习模型的胺排放和系统性能预测方法，通过四种LSTM等模型结构实现超过99%的预测准确度，并通过因果分析确定优化操作参数。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够预测胺排放和关键性能参数的深度学习模型，以支持碳捕获系统的监控和优化，降低环境影响。

Method: 采用四种DL模型结构：基础LSTM、堆叠LSTM、双向LSTM和卷积LSTM，使用CESAR1溶剂运行数据进行时间序列预测，并通过系统性批动八个输入变量进行因果影响分析。

Result: 模型预测准确度超过99%，能够有效跟踪稳态趋势和突变波动，发现调整稀溶剂温度和水洗条件等参数可显著降低胺排放并提升系统性能。

Conclusion: 该研究为碳捕获操作提供了一种数据驱动的智能控制策略，通过实时监控、场景测试和操作优化，有助于提高效率、稳定性和可持续性。

Abstract: We present data driven deep learning models for forecasting and monitoring
amine emissions and key performance parameters in amine-based post-combustion
carbon capture systems. Using operational data from the CESAR1 solvent campaign
at Technology Center Mongstad, four DL architectures such as Basic Long
Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional
LSTM were developed to capture time-dependent process behavior. For emission
prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and
Piperazine emissions measured via FTIR and IMR-MS methods. System performance
models target four critical parameters: CO$_2$ product flow, absorber outlet
temperature, depleted flue gas outlet temperature, and RFCC stripper bottom
temperature. These models achieved high predictive accuracy exceeding 99% and
effectively tracked both steady trends and abrupt fluctuations. Additionally,
we conducted causal impact analysis to evaluate how operational variables
influence emissions and system performance. Eight input variables were
systematically perturbed within $\pm$20% of nominal values to simulate
deviations and assess their impact. This analysis revealed that adjusting
specific operational parameters, such as lean solvent temperature and water
wash conditions, can significantly reduce amine emissions and enhance system
performance. This study highlights ML not only as a predictive tool but also as
a decision support system for optimizing carbon capture operations under steady
state and dynamic conditions. By enabling real time monitoring, scenario
testing, and operational optimization, the developed ML framework offers a
practical pathway for mitigating environmental impacts. This work represents a
step toward intelligent, data-driven control strategies that enhance the
efficiency, stability, and sustainability of carbon capture and storage
technologies.

</details>


### [74] [A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems](https://arxiv.org/abs/2509.05259)
*Jehad Jilan,Niranjana Naveen Nambiar,Ahmad Mohammad Saber,Alok Paranjape,Amr Youssef,Deepa Kundur*

Main category: cs.LG

TL;DR: 提出基于Kolmogorov-Arnold Networks (KAN)的可解释性FDIA检测方法，用于电力系统AGC的网络安全防护，检测率高达95.97%，误报率低。


<details>
  <summary>Details</summary>
Motivation: 传统AGC系统容易受到隐蔽网络攻击（如FDIA）的威胁，现有检测方法多为黑盒模型，缺乏可解释性，无法有效应对系统非线性特性。

Method: 使用KAN网络离线学习AGC测量值之间的复杂非线性关系，训练后可以提取符号公式，提供比传统机器学习模型更好的可解释性。

Result: KAN模型在FDIA检测中达到95.97%的检测率，提取的符号公式检测率为95.9%，且具有较低的误报率。

Conclusion: KAN为AGC系统提供了一种准确且可解释的网络安全解决方案，能够有效检测隐蔽网络攻击并增强系统稳定性。

Abstract: Automatic Generation Control (AGC) is essential for power grid stability but
remains vulnerable to stealthy cyberattacks, such as False Data Injection
Attacks (FDIAs), which can disturb the system's stability while evading
traditional detection methods. Unlike previous works that relied on blackbox
approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an
interpretable and accurate method for FDIA detection in AGC systems,
considering the system nonlinearities. KAN models include a method for
extracting symbolic equations, and are thus able to provide more
interpretability than the majority of machine learning models. The proposed KAN
is trained offline to learn the complex nonlinear relationships between the AGC
measurements under different operating scenarios. After training, symbolic
formulas that describe the trained model's behavior can be extracted and
leveraged, greatly enhancing interpretability. Our findings confirm that the
proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for
the initial model and the symbolic formula, respectively, with a low false
alarm rate, offering a reliable approach to enhancing AGC cybersecurity.

</details>


### [75] [Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks](https://arxiv.org/abs/2509.05273)
*Jason Gardner,Ayan Dutta,Swapnoneel Roy,O. Patrick Kreidl,Ladislau Boloni*

Main category: cs.LG

TL;DR: 本文系统性地比较了7种主流深度强化学习算法的能耗、碳排放和经济成本，发现不同算法间存在显著差异，某些算法在保持性能的同时可减少高达24%的能耗和68%的成本。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习的计算需求日益增长，但其能耗、温室气体排放和经济成本尚未得到充分研究，需要建立可持续的DRL实践标准。

Method: 使用Stable Baselines实现了DQN、TRPO、A2C、ARS、PPO、RecurrentPPO和QR-DQN等7种算法，在10个Atari 2600游戏上各训练100万步，实时测量功耗以估算总能耗、CO2当量排放和电力成本。

Result: 发现算法间能效差异显著：ARS比DQN节能24%，QR-DQN比RecurrentPPO减少近68%的CO2排放和成本，同时保持可比性能。

Conclusion: 研究为开发能源意识和成本效益的DRL实践提供了可行见解，并为未来算法设计中纳入可持续性考量奠定了基础。

Abstract: The growing computational demands of deep reinforcement learning (DRL) have
raised concerns about the environmental and economic costs of training
large-scale models. While algorithmic efficiency in terms of learning
performance has been extensively studied, the energy requirements, greenhouse
gas emissions, and monetary costs of DRL algorithms remain largely unexplored.
In this work, we present a systematic benchmarking study of the energy
consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,
ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each
algorithm was trained for one million steps each on ten Atari 2600 games, and
power consumption was measured in real-time to estimate total energy usage,
CO2-Equivalent emissions, and electricity cost based on the U.S. national
average electricity price. Our results reveal substantial variation in energy
efficiency and training cost across algorithms, with some achieving comparable
performance while consuming up to 24% less energy (ARS vs. DQN), emitting
nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.
RecurrentPPO) than less efficient counterparts. We further analyze the
trade-offs between learning performance, training time, energy use, and
financial cost, highlighting cases where algorithmic choices can mitigate
environmental and economic impact without sacrificing learning performance.
This study provides actionable insights for developing energy-aware and
cost-efficient DRL practices and establishes a foundation for incorporating
sustainability considerations into future algorithmic design and evaluation.

</details>


### [76] [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
*Yuqi Pan,Yupeng Feng,Jinghao Zhuang,Siyu Ding,Zehao Liu,Bohan Sun,Yuhong Chou,Han Xu,Xuerui Qiu,Anlin Deng,Anjie Hu,Peng Zhou,Man Yao,Jibin Wu,Jian Yang,Guoliang Sun,Bo Xu,Guoqi Li*

Main category: cs.LG

TL;DR: SpikingBrain是一个基于脉冲神经网络的脑启发式大语言模型家族，通过线性/混合线性注意力架构、脉冲神经元和系统优化，在非NVIDIA平台上实现了高效的长上下文训练和推理，性能接近Transformer基线但计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决主流Transformer模型在长序列处理中的二次计算复杂度和线性内存增长问题，以及在非NVIDIA平台上训练大模型的稳定性和效率挑战。

Method: 采用线性/混合线性注意力架构和自适应脉冲神经元；开发高效的转换式训练流水线和专用脉冲编码框架；针对MetaX硬件定制训练框架、算子库和并行策略。

Result: 开发了7B和76B两个模型，在150B tokens上持续预训练达到开源Transformer可比性能；7B模型在4M tokens序列上首token生成速度提升100倍以上；训练稳定，MFU达23.4%；脉冲方案实现69.15%稀疏度。

Conclusion: 脑启发机制有潜力推动下一代高效可扩展大模型设计，证明了在非NVIDIA平台上开发大规模LLM的可行性。

Abstract: Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.

</details>


### [77] [Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection](https://arxiv.org/abs/2509.05281)
*Naman Tyagi*

Main category: cs.LG

TL;DR: 基于空间和频域特征的双支深度学习框架，用于检测图像伪造，在CASIA 2.0数据集上达券77.9%准确率


<details>
  <summary>Details</summary>
Motivation: 深度伪造和数字图像伪造技术快速发展，图像真实性验证面临无比挑战

Method: 提出双支卷积神经网络，同时处理空间和频域特征，通过Siamese网络融合并比较特征，生成64维嵌入进行分类

Result: 在CASIA 2.0数据集上达券77.9%的准确率，超过传统统计方法，虽然效果略弱于更大更复杂的检测系统

Conclusion: 方法在计算复杂度和检测可靠性之间取得平衡，适合实际部署，为数字图像司法鉴定提供了强大方法，推动了视觉司法技术的发展

Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring
the authenticity of images is becoming increasingly challenging. This report
introduces a forgery detection framework that combines spatial and
frequency-based features for detecting forgeries. We propose a dual branch
convolution neural network that operates on features extracted from spatial and
frequency domains. Features from both branches are fused and compared within a
Siamese network, yielding 64 dimensional embeddings for classification. When
benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,
outperforming traditional statistical methods. Despite its relatively weaker
performance compared to larger, more complex forgery detection pipelines, our
approach balances computational complexity and detection reliability, making it
ready for practical deployment. It provides a strong methodology for forensic
scrutiny of digital images. In a broader sense, it advances the state of the
art in visual forensics, addressing an urgent requirement in media
verification, law enforcement and digital content reliability.

</details>


### [78] [Learning to accelerate distributed ADMM using graph neural networks](https://arxiv.org/abs/2509.05288)
*Henri Doerks,Paul Häusner,Daniel Hernández Escobar,Jens Sjölund*

Main category: cs.LG

TL;DR: 将分布式ADMM与图神经网络结合，通过学习自适应步长和通信权重来加速收敛


<details>
  <summary>Details</summary>
Motivation: ADMM在分布式优化中收敛慢且对超参数敏感，需要改进其性能

Method: 将ADMM迭代表示为图神经网络的消息传递框架，通过展开固定次数的迭代来端到端训练网络参数

Result: 数值实验表明学习变体相比标准ADMM在收敛速度和求解质量上都有显著提升

Conclusion: 通过GNN学习超参数的方法有效改善了ADMM的性能，同时保持了算法的收敛性

Abstract: Distributed optimization is fundamental in large-scale machine learning and
control applications. Among existing methods, the Alternating Direction Method
of Multipliers (ADMM) has gained popularity due to its strong convergence
guarantees and suitability for decentralized computation. However, ADMM often
suffers from slow convergence and sensitivity to hyperparameter choices. In
this work, we show that distributed ADMM iterations can be naturally
represented within the message-passing framework of graph neural networks
(GNNs). Building on this connection, we propose to learn adaptive step sizes
and communication weights by a graph neural network that predicts the
hyperparameters based on the iterates. By unrolling ADMM for a fixed number of
iterations, we train the network parameters end-to-end to minimize the final
iterates error for a given problem class, while preserving the algorithm's
convergence properties. Numerical experiments demonstrate that our learned
variant consistently improves convergence speed and solution quality compared
to standard ADMM. The code is available at
https://github.com/paulhausner/learning-distributed-admm.

</details>


### [79] [Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest](https://arxiv.org/abs/2509.05292)
*Xiao Yang,Mehdi Ben Ayed,Longyu Zhao,Fan Zhou,Yuchen Shen,Abe Engle,Jinfeng Zhuang,Ling Leng,Jiajing Xu,Charles Rosenberg,Prathibha Deshikachar*

Main category: cs.LG

TL;DR: 使用深度强化学习框架自动优化广告推荐系统的多目标排名赋权，提高点击率和长点击率


<details>
  <summary>Details</summary>
Motivation: 传统手动调优方法在广告推荐系统中存在调优目标不理惵、参数组合过多、缺乏个性化和季节性适应性等问题，导致效果次优

Method: 提出DRL-PUT框架：1)将问题形式化为强化学习任务，根据广告请求状态预测最优超参数；2)直接从在线服务日志学习策略模型，避免估计价值函数的挑战

Result: 在Pinterest广告推荐系统中进行在线A/B实验，与基准手动调优方法相比，点击率提高9.7%，长点击率提高7.7%

Conclusion: DRL-PUT框架能够有效解决广告推荐系统中的多目标优化问题，实现个性化调优和效果提升

Abstract: The ranking utility function in an ad recommender system, which linearly
combines predictions of various business goals, plays a central role in
balancing values across the platform, advertisers, and users. Traditional
manual tuning, while offering simplicity and interpretability, often yields
suboptimal results due to its unprincipled tuning objectives, the vast amount
of parameter combinations, and its lack of personalization and adaptability to
seasonality. In this work, we propose a general Deep Reinforcement Learning
framework for Personalized Utility Tuning (DRL-PUT) to address the challenges
of multi-objective optimization within ad recommender systems. Our key
contributions include: 1) Formulating the problem as a reinforcement learning
task: given the state of an ad request, we predict the optimal hyperparameters
to maximize a pre-defined reward. 2) Developing an approach to directly learn
an optimal policy model using online serving logs, avoiding the need to
estimate a value function, which is inherently challenging due to the high
variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT
through an online A/B experiment in Pinterest's ad recommender system. Compared
to the baseline manual utility tuning approach, DRL-PUT improved the
click-through rate by 9.7% and the long click-through rate by 7.7% on the
treated segment. We conducted a detailed ablation study on the impact of
different reward definitions and analyzed the personalization aspect of the
learned policy model.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [80] [Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning](https://arxiv.org/abs/2509.04069)
*Chengyandan Shen,Christoffer Sloth*

Main category: cs.RO

TL;DR: 提出DRLR框架，通过改进IBRL算法的动作选择模块来减少bootstrapping误差，使用SAC替代TD3防止策略收敛到次优解，在机器人任务中验证了有效性并实现了sim2real部署


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在机器人任务中探索效率低、bootstrapping误差导致学习效率低下，以及策略容易收敛到次优解的问题

Method: 基于IBRL算法改进动作选择模块，提供校准的Q值来减少bootstrapping误差；使用SAC作为RL策略替代TD3；在装桶和开抽屉等机器人任务上进行验证

Result: 实验证明该方法能有效减少bootstrapping误差和防止过拟合，在不同状态-动作维度和演示质量的任务中均表现鲁棒，成功实现从仿真到真实世界的部署

Conclusion: DRLR框架通过改进动作选择和策略优化，显著提高了机器人任务中的探索效率和性能，为工业机器人应用提供了有效的强化学习解决方案

Abstract: This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.

</details>


### [81] [In-Context Policy Adaptation via Cross-Domain Skill Diffusion](https://arxiv.org/abs/2509.04535)
*Minjong Yoo,Woo Kyung Kim,Honguk Woo*

Main category: cs.RO

TL;DR: 提出了ICPAD框架，用于长时程多任务环境中的策略快速适应，通过跨域技能扩散和动态域提示机制，在有限目标域数据下实现高性能策略迁移。


<details>
  <summary>Details</summary>
Motivation: 解决长时程多任务环境中强化学习策略跨域适应的问题，特别是在模型不能更新且目标域数据有限的严格约束下，需要快速有效的策略适应方法。

Method: 采用跨域技能扩散方案，学习领域无关的原型技能和领域接地的技能适配器；开发动态域提示机制指导技能适配器更好地与目标域对齐。

Result: 在Metaworld机器人操作和CARLA自动驾驶实验中，该框架在各种跨域配置（环境动态、智能体体现、任务时程差异）下均表现出优越的策略适应性能。

Conclusion: ICPAD框架通过扩散式技能学习和动态提示机制，有效解决了有限数据条件下的跨域策略适应问题，为长时程多任务环境提供了实用的解决方案。

Abstract: In this work, we present an in-context policy adaptation (ICPAD) framework
designed for long-horizon multi-task environments, exploring diffusion-based
skill learning techniques in cross-domain settings. The framework enables rapid
adaptation of skill-based reinforcement learning policies to diverse target
domains, especially under stringent constraints on no model updates and only
limited target domain data. Specifically, the framework employs a cross-domain
skill diffusion scheme, where domain-agnostic prototype skills and a
domain-grounded skill adapter are learned jointly and effectively from an
offline dataset through cross-domain consistent diffusion processes. The
prototype skills act as primitives for common behavior representations of
long-horizon policies, serving as a lingua franca to bridge different domains.
Furthermore, to enhance the in-context adaptation performance, we develop a
dynamic domain prompting scheme that guides the diffusion-based skill adapter
toward better alignment with the target domain. Through experiments with
robotic manipulation in Metaworld and autonomous driving in CARLA, we show that
our $\oursol$ framework achieves superior policy adaptation performance under
limited target domain data conditions for various cross-domain configurations
including differences in environment dynamics, agent embodiment, and task
horizon.

</details>


### [82] [Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving](https://arxiv.org/abs/2509.04712)
*Zhihao Zhang,Chengyang Peng,Ekim Yurtsever,Keith A. Redmill*

Main category: cs.RO

TL;DR: 提出了一种使用非专家演示策略来指导强化学习自动驾驶代理的方法，通过集成基于规则的车道变换控制器与SAC算法，提高了探索效率和学习性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶控制中面临样本效率低和探索困难的问题，难以发现最优驾驶策略，需要一种有效的方法来指导学习过程。

Method: 将基于规则的车道变换控制器与Soft Actor Critic (SAC)算法集成，使用演示策略来指导强化学习代理的探索和学习过程。

Result: 该方法显著提高了驾驶性能和学习效率，能够扩展到其他可以从演示指导中受益的驾驶场景。

Conclusion: 即使使用非专家级别的演示策略，也能有效指导强化学习代理，提高自动驾驶控制的学习效率和性能表现。

Abstract: Automated vehicle control using reinforcement learning (RL) has attracted
significant attention due to its potential to learn driving policies through
environment interaction. However, RL agents often face training challenges in
sample efficiency and effective exploration, making it difficult to discover an
optimal driving strategy. To address these issues, we propose guiding the RL
driving agent with a demonstration policy that need not be a highly optimized
or expert-level controller. Specifically, we integrate a rule-based lane change
controller with the Soft Actor Critic (SAC) algorithm to enhance exploration
and learning efficiency. Our approach demonstrates improved driving performance
and can be extended to other driving scenarios that can similarly benefit from
demonstration-based guidance.

</details>


### [83] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 这篇论文提出了一种稳健模型预测控制框架，专门处理深度学习感知模块中的非高斯器声问题，通过集合基估计和线性规划实现了稳定的闭环控制性能。


<details>
  <summary>Details</summary>
Motivation: 因为深度学习感知模块的估计错误通常不遵循高斯分布且存在偏差和重尾特征，传统的零均值噪声假设导致不准确的不确定性量化，影响控制系统的安全性和稳定性。

Method: 使用约束韦诺托普进行集基状态估计，捕捉偏差和重尾不确定性。将稳健MPC重构为线性规划问题，采用Minkowski-Lyapunov成本函数和松弛变量防止逆解。通过Minkowski-Lyapunov不等式和缩放韦诺托普不变集确保闭环稳定性。

Result: 在模拟和硬件实验中，该感知感知MPC框架在重尾噪声条件下展现出稳定而准确的控制性能，在状态估计错误边界和整体控制性能方面显著超过传统高斯噪声设计。

Conclusion: 该研究为处理深度学习感知模块非高斯噪声特征提供了有效的稳健控制解决方案，通过集基估计和线性规划方法实现了超越传统方法的性能。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [84] [DarkStream: real-time speech anonymization with low latency](https://arxiv.org/abs/2509.04667)
*Waris Quamer,Ricardo Gutierrez-Osuna*

Main category: eess.AS

TL;DR: DarkStream是一个实时说话人匿名化的流式语音合成模型，通过因果波形编码器、短前瞻缓冲区和基于Transformer的上下文层来改善内容编码，直接生成波形以避免梅尔频谱转换，并使用GAN生成的伪说话人嵌入实现说话人匿名化。


<details>
  <summary>Details</summary>
Motivation: 为了解决实时语音通信中的隐私保护问题，需要在严格延迟约束下实现有效的说话人匿名化，同时保持语音内容的可理解性。

Method: 结合因果波形编码器、短前瞻缓冲区和基于Transformer的上下文层来改善内容编码；直接通过神经声码器生成波形；注入GAN生成的伪说话人嵌入到内容编码器的语言特征中实现匿名化。

Result: 在lazy-informed攻击场景下达到接近50%的说话人验证EER（接近随机性能），同时保持可接受的语言可理解性（WER在9%以内）。

Conclusion: DarkStream通过平衡低延迟、强隐私保护和最小可理解性损失，为隐私保护的实时语音通信提供了实用解决方案。

Abstract: We propose DarkStream, a streaming speech synthesis model for real-time
speaker anonymization. To improve content encoding under strict latency
constraints, DarkStream combines a causal waveform encoder, a short lookahead
buffer, and transformer-based contextual layers. To further reduce inference
time, the model generates waveforms directly via a neural vocoder, thus
removing intermediate mel-spectrogram conversions. Finally, DarkStream
anonymizes speaker identity by injecting a GAN-generated pseudo-speaker
embedding into linguistic features from the content encoder. Evaluations show
our model achieves strong anonymization, yielding close to 50% speaker
verification EER (near-chance performance) on the lazy-informed attack
scenario, while maintaining acceptable linguistic intelligibility (WER within
9%). By balancing low-latency, robust privacy, and minimal intelligibility
degradation, DarkStream provides a practical solution for privacy-preserving
real-time speech communication.

</details>


### [85] [Lightweight DNN for Full-Band Speech Denoising on Mobile Devices: Exploiting Long and Short Temporal Patterns](https://arxiv.org/abs/2509.05079)
*Konstantinos Drossos,Mikko Heikkinen,Paschalis Tsiaflakis*

Main category: eess.AS

TL;DR: 这篇论文提出了一种优化质量的轻量级深度神经网络方法，用于移动设备上的完全带宽语音去噪，具有因果性、低延迟和高效性特点。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度神经网络方法在移动设备等资源受限平台上的优化问题，以及大多数方法没有重点关注完全带宽信号和低延迟场景。

Method: 基于修改的UNet架构，采用回顾帧、卷积内核时间范围扩展和递归神经网络来利用短期和长期时间模式，使用STFT幅值输入、MobileNet反向瓦筒结构、因果实例归一化等技术。

Result: 方法在现代手机上实时因子低于0.02，通过公开数据集评估显示其在(SI-)SDR指标上超过现有完全带宽和低延迟语音去噪方法。

Conclusion: 该程序提供了一种高效、低延迟的轻量级解决方案，适用于移动设备等资源受限环境下的完全带宽语音去噪任务。

Abstract: Speech denoising (SD) is an important task of many, if not all, modern signal
processing chains used in devices and for everyday-life applications. While
there are many published and powerful deep neural network (DNN)-based methods
for SD, few are optimized for resource-constrained platforms such as mobile
devices. Additionally, most DNN-based methods for SD are not focusing on
full-band (FB) signals, i.e. having 48 kHz sampling rate, and/or low latency
cases. In this paper we present a causal, low latency, and lightweight
DNN-based method for full-band SD, leveraging both short and long temporal
patterns. The method is based on a modified UNet architecture employing
look-back frames, temporal spanning of convolutional kernels, and recurrent
neural networks for exploiting short and long temporal patterns in the signal
and estimated denoising mask. The DNN operates on a causal frame-by-frame basis
taking as an input the STFT magnitude, utilizes inverted bottlenecks inspired
by MobileNet, employs causal instance normalization for channel-wise
normalization, and achieves a real-time factor below 0.02 when deployed on a
modern mobile phone. The proposed method is evaluated using established speech
denoising metrics and publicly available datasets, demonstrating its
effectiveness in achieving an (SI-)SDR value that outperforms existing FB and
low latency SD methods.

</details>


### [86] [Room-acoustic simulations as an alternative to measurements for audio-algorithm evaluation](https://arxiv.org/abs/2509.05175)
*Georg Götz,Daniel Gert Nielsen,Steinar Guðjónsson,Finnur Pind*

Main category: eess.AS

TL;DR: 通过比较室内声学模拟与实际测量数据评估音频处理算法，发现波动方程模拟能更准确地重现实际评测结果，而几何声学模拟效果较差


<details>
  <summary>Details</summary>
Motivation: 解决音频处理算法评估中面临的数据集有限性问题，探索室内声学模拟在算法评估中的应用可行性

Method: 使用数值波动方程模拟器和两种几何声学模拟器生成模拟数据，对三种音频处理算法进行评估，并与实际测量数据进行对比分析

Result: 数值波动方程模拟在三种算法评估中都能产生与实际测量相似的结果，而几何声学模拟无法可靠地重现测量评估结果

Conclusion: 室内声学模拟可以作为音频处理算法评估的有效工具，特别是数值波动方法能提供与实际测量相似的评估结果

Abstract: Audio-signal-processing and audio-machine-learning (ASP/AML) algorithms are
ubiquitous in modern technology like smart devices, wearables, and
entertainment systems. Development of such algorithms and models typically
involves a formal evaluation to demonstrate their effectiveness and progress
beyond the state-of-the-art. Ideally, a thorough evaluation should cover many
diverse application scenarios and room-acoustic conditions. However, in
practice, evaluation datasets are often limited in size and diversity because
they rely on costly and time-consuming measurements. This paper explores how
room-acoustic simulations can be used for evaluating ASP/AML algorithms. To
this end, we evaluate three ASP/AML algorithms with room-acoustic measurements
and data from different simulation engines, and assess the match between the
evaluation results obtained from measurements and simulations. The presented
investigation compares a numerical wave-based solver with two geometrical
acoustics simulators. While numerical wave-based simulations yielded similar
evaluation results as measurements for all three evaluated ASP/AML algorithms,
geometrical acoustic simulations could not replicate the measured evaluation
results as reliably.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [87] [High-Resolution Global Land Surface Temperature Retrieval via a Coupled Mechanism-Machine Learning Framework](https://arxiv.org/abs/2509.04991)
*Tian Xie,Huanfeng Shen,Menghui Jiang,Juan-Carlos Jiménez-Muñoz,José A. Sobrino,Huifang Li,Chao Zeng*

Main category: physics.ao-ph

TL;DR: 提出一种物理机制模型与机器学习耦合的框架(MM-ML)，通过结合物理约束和数据驱动学习，实现了更准确和稳健的地表温度(LST)反汇。


<details>
  <summary>Details</summary>
Motivation: 传统分窗算法在湿温环境中存在偏差，纯机器学习方法缺乏可解释性且在数据有限时普遍化能力差。需要一种方法能够在复杂环境下进行准确的LST反汇。

Method: 开发了MM-ML框架，融合辐射传输模型和数据组件，使用MODTRAN模拟和全球大气配置文件，采用物理约束优化方法。

Result: 在4,450个观测数据上验证，MM-ML达到MAE=1.84K，RMSE=2.55K，R平方=0.966，表现超过传统方法。在极端条件下错误减少超过50%。敏感性分析显示估计结果对传感器辐射度最敏感，其次是水气，对发射率敏感性较低。

Conclusion: MM-ML框架结合了物理可解释性和非线性建模能力，能够在复杂环境下进行可靠的LST反汇，支持气候监测和生态系统研究。

Abstract: Land surface temperature (LST) is vital for land-atmosphere interactions and
climate processes. Accurate LST retrieval remains challenging under
heterogeneous land cover and extreme atmospheric conditions. Traditional split
window (SW) algorithms show biases in humid environments; purely machine
learning (ML) methods lack interpretability and generalize poorly with limited
data. We propose a coupled mechanism model-ML (MM-ML) framework integrating
physical constraints with data-driven learning for robust LST retrieval. Our
approach fuses radiative transfer modeling with data components, uses MODTRAN
simulations with global atmospheric profiles, and employs physics-constrained
optimization. Validation against 4,450 observations from 29 global sites shows
MM-ML achieves MAE=1.84K, RMSE=2.55K, and R-squared=0.966, outperforming
conventional methods. Under extreme conditions, MM-ML reduces errors by over
50%. Sensitivity analysis indicates LST estimates are most sensitive to sensor
radiance, then water vapor, and less to emissivity, with MM-ML showing superior
stability. These results demonstrate the effectiveness of our coupled modeling
strategy for retrieving geophysical parameters. The MM-ML framework combines
physical interpretability with nonlinear modeling capacity, enabling reliable
LST retrieval in complex environments and supporting climate monitoring and
ecosystem studies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [88] [Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring](https://arxiv.org/abs/2509.04682)
*Nicholas R. Rasmussen,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.SD

TL;DR: 提出了GetNetUPAM评估框架和ARPA-N神经网络架构，用于提升水下被动声学监测的模型稳定性和泛化能力，在生态多样性场景下取得了显著性能提升


<details>
  <summary>Details</summary>
Motivation: 水下被动声学监测存在固有噪声和复杂信号依赖性问题，现有方法在环境多样性下的稳定性和泛化能力不足，需要更鲁棒的架构和评估方法

Method: 提出GetNetUPAM分层嵌套交叉验证框架，按站点-年份划分数据保持生态异质性；设计ARPA-N神经网络，采用自适应分辨率池化和空间注意力机制处理不规则频谱图

Result: ARPA-N在GetNetUPAM评估下相比DenseNet基线平均精度提升14.4%，所有指标变异性降低数量级，实现跨站点-年份的一致检测

Conclusion: 该框架和架构显著提升了水下生物声学监测的准确性和可扩展性，为生态多样性环境下的稳定检测提供了有效解决方案

Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal
data for long-term ecological analysis, but intrinsic noise and complex signal
dependencies hinder model stability and generalization. Multilayered windowing
has improved target sound localization, yet variability from shifting ambient
noise, diverse propagation effects, and mixed biological and anthropogenic
sources demands robust architectures and rigorous evaluation. We introduce
GetNetUPAM, a hierarchical nested cross-validation framework designed to
quantify model stability under ecologically realistic variability. Data are
partitioned into distinct site-year segments, preserving recording
heterogeneity and ensuring each validation fold reflects a unique environmental
subset, reducing overfitting to localized noise and sensor artifacts. Site-year
blocking enforces evaluation against genuine environmental diversity, while
standard cross-validation on random subsets measures generalization across
UPAM's full signal distribution, a dimension absent from current benchmarks.
Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution
Pooling and Attention Network (ARPA-N), a neural architecture for irregular
spectrogram dimensions. Adaptive pooling with spatial attention extends the
receptive field, capturing global context without excessive parameters. Under
GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet
baselines and a log2-scale order-of-magnitude drop in variability across all
metrics, enabling consistent detection across site-year folds and advancing
scalable, accurate bioacoustic monitoring.

</details>


### [89] [Learning and composing of classical music using restricted Boltzmann machines](https://arxiv.org/abs/2509.04899)
*Mutsumi Kobayashi,Hiroshi Watanabe*

Main category: cs.SD

TL;DR: 使用受限玻尔兹曼机(RBM)分析巴赫音乐风格特征，RBM的简单结构便于理解模型如何学习作曲家的音乐特性


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的作曲软件结构复杂，难以分析模型如何理解作曲家音乐风格特征，需要更简单的模型来研究音乐特征学习机制

Method: 采用受限玻尔兹曼机(RBM)对J.S.巴赫的音乐进行训练，利用RBM的简单结构来分析学习后的内部状态

Result: 训练后的RBM能够进行音乐创作，证明了简单模型也能学习到巴赫的音乐风格特征

Conclusion: RBM作为一种结构简单的机器学习模型，不仅能够学习巴赫的音乐风格并进行创作，更重要的是其透明性便于分析模型对音乐特征的理解机制

Abstract: Recently, software has been developed that uses machine learning to mimic the
style of a particular composer, such as J. S. Bach. However, since such
software often adopts machine learning models with complex structures, it is
difficult to analyze how the software understands the characteristics of the
composer's music. In this study, we adopted J. S. Bach's music for training of
a restricted Boltzmann machine (RBM). Since the structure of RBMs is simple, it
allows us to investigate the internal states after learning. We found that the
learned RBM is able to compose music.

</details>


### [90] [MAIA: An Inpainting-Based Approach for Music Adversarial Attacks](https://arxiv.org/abs/2509.04980)
*Yuxuan Liu,Peihong Zhang,Rui Sang,Zhixin Li,Shengchen Li*

Main category: cs.SD

TL;DR: MAIA是一种新颖的音乐对抗攻击框架，支持白盒和黑盒攻击场景，通过重要性分析和生成式修复模型产生难以察觉的对抗扰动，在多个MIR任务中表现出高攻击成功率和音频保真度。


<details>
  <summary>Details</summary>
Motivation: 当前音乐信息检索系统存在安全漏洞，需要开发有效的对抗攻击方法来揭示这些系统的脆弱性，促进更鲁棒和安全的模型发展。

Method: MAIA框架首先进行重要性分析识别关键音频片段，然后利用生成式修复模型在模型输出指导下重构这些片段，产生细微而有效的对抗扰动。

Result: 在多个MIR任务中，MAIA在白盒和黑盒设置下都实现了高攻击成功率，同时保持最小的感知失真，主观听力测试证实了对抗样本的高音频保真度。

Conclusion: 研究揭示了当前MIR系统的脆弱性，强调了开发更鲁棒和安全模型的必要性，MAIA框架为评估和改进MIR系统的安全性提供了有效工具。

Abstract: Music adversarial attacks have garnered significant interest in the field of
Music Information Retrieval (MIR). In this paper, we present Music Adversarial
Inpainting Attack (MAIA), a novel adversarial attack framework that supports
both white-box and black-box attack scenarios. MAIA begins with an importance
analysis to identify critical audio segments, which are then targeted for
modification. Utilizing generative inpainting models, these segments are
reconstructed with guidance from the output of the attacked model, ensuring
subtle and effective adversarial perturbations. We evaluate MAIA on multiple
MIR tasks, demonstrating high attack success rates in both white-box and
black-box settings while maintaining minimal perceptual distortion.
Additionally, subjective listening tests confirm the high audio fidelity of the
adversarial samples. Our findings highlight vulnerabilities in current MIR
systems and emphasize the need for more robust and secure models.

</details>


### [91] [Recomposer: Event-roll-guided generative audio editing](https://arxiv.org/abs/2509.05256)
*Daniel P. W. Ellis,Eduardo Fonseca,Ron J. Weiss,Kevin Wilson,Scott Wisdom,Hakan Erdogan,John R. Hershey,Aren Jansen,R. Channing Moore,Manoj Plakal*

Main category: cs.SD

TL;DR: 提出了一个基于文本描述和时序图形表示的音频编辑系统，能够删除、插入和增强复杂音频场景中的单个声音事件


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂音频场景编辑困难，因为声音源在时间上重叠。生成模型可以利用其对数据域的强先验理解来填充缺失或损坏的细节

Method: 使用基于SoundStream表示的编码器-解码器transformer模型，通过在密集的真实背景音频上添加孤立声音事件来合成训练数据对

Result: 评估显示编辑描述中的每个部分（动作、类别、时序）都很重要

Conclusion: 这项工作证明了'重组合'是一个重要且实用的应用

Abstract: Editing complex real-world sound scenes is difficult because individual sound
sources overlap in time. Generative models can fill-in missing or corrupted
details based on their strong prior understanding of the data domain. We
present a system for editing individual sound events within complex scenes able
to delete, insert, and enhance individual sound events based on textual edit
descriptions (e.g., ``enhance Door'') and a graphical representation of the
event timing derived from an ``event roll'' transcription. We present an
encoder-decoder transformer working on SoundStream representations, trained on
synthetic (input, desired output) audio example pairs formed by adding isolated
sound events to dense, real-world backgrounds. Evaluation reveals the
importance of each part of the edit descriptions -- action, class, timing. Our
work demonstrates ``recomposition'' is an important and practical application.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [92] [Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects](https://arxiv.org/abs/2509.05289)
*Martina Boschi,Jürgen Lerner,Ernst C. Wit*

Main category: stat.ME

TL;DR: 本文提出了一种新的关系超事件模型(RHEM)，通过张量积平滑方法建模联合时变和非线性效应，突破了传统线性假设的限制，能够识别非线性模式和时变效应。


<details>
  <summary>Details</summary>
Motivation: 当前的关系超事件模型大多依赖线性假设来建模事件率与历史统计量和外部信息的关系，无法捕捉复杂的非线性模式和时变效应，限制了模型的解释能力和预测准确性。

Method: 使用张量积平滑方法建模联合时变和非线性效应，构建更灵活的模型框架，允许统计量的效应随时间非线性变化。

Result: 在合成数据和实证数据上验证了方法的有效性，特别是在科学合作和影响力模式演化研究中，能够识别传统线性模型无法发现的非单调模式。

Conclusion: 提出的方法为关系超事件的动态驱动因素提供了更深入的洞察，能够捕捉复杂的非线性时变效应，具有更好的解释能力和应用价值。

Abstract: Recent technological advances have made it easier to collect large and
complex networks of time-stamped relational events connecting two or more
entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of
these events by modeling the event rate as a function of statistics based on
past history and external information.
  However, despite the complexity of the data, most current RHEM approaches
still rely on a linearity assumption to model this relationship. In this work,
we address this limitation by introducing a more flexible model that allows the
effects of statistics to vary non-linearly and over time. While time-varying
and non-linear effects have been used in relational event modeling, we take
this further by modeling joint time-varying and non-linear effects using tensor
product smooths.
  We validate our methodology on both synthetic and empirical data. In
particular, we use RHEMs to study how patterns of scientific collaboration and
impact evolve over time. Our approach provides deeper insights into the dynamic
factors driving relational hyper-events, allowing us to evaluate potential
non-monotonic patterns that cannot be identified using linear models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [93] [Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation](https://arxiv.org/abs/2509.04633)
*Brennen Hill*

Main category: cs.NE

TL;DR: 提出了一个用于训练神经类器官生物智能体的虚拟环境框架，包含三个复杂度递增的任务环境，并引入LLM自动化实验协议生成和优化的元学习方法


<details>
  <summary>Details</summary>
Motivation: 随着人工智能体复杂度增加，设计能有效塑造其行为和能力的环境成为关键研究前沿，需要将这一原则扩展到生物神经网络（神经类器官）的新型智能体

Method: 设计了三个可扩展的闭环虚拟环境：条件回避任务、一维捕食者-猎物场景和经典Pong游戏；提出了基于LLM的元学习方法来自动生成和优化实验协议；采用多模态方法在电生理、细胞和分子水平评估学习效果

Result: 建立了连接计算神经科学和基于智能体的AI之间的桥梁，为研究受控生物基质中的具身化、学习和智能提供了独特平台

Conclusion: 该框架为研究生物神经网络的学习机制（如LTP和LTD）提供了系统方法，通过可扩展的环境设计和自动化协议生成，推动了生物智能体研究的发展

Abstract: As the complexity of artificial agents increases, the design of environments
that can effectively shape their behavior and capabilities has become a
critical research frontier. We propose a framework that extends this principle
to a novel class of agents: biological neural networks in the form of neural
organoids. This paper introduces three scalable, closed-loop virtual
environments designed to train organoid-based biological agents and probe the
underlying mechanisms of learning, such as long-term potentiation (LTP) and
long-term depression (LTD). We detail the design of three distinct task
environments with increasing complexity: (1) a conditional avoidance task, (2)
a one-dimensional predator-prey scenario, and (3) a replication of the classic
Pong game. For each environment, we formalize the state and action spaces, the
sensory encoding and motor decoding mechanisms, and the feedback protocols
based on predictable (reward) and unpredictable (punishment) stimulation.
Furthermore, we propose a novel meta-learning approach where a Large Language
Model (LLM) is used to automate the generation and optimization of experimental
protocols, scaling the process of environment and curriculum design. Finally,
we outline a multi-modal approach for evaluating learning by measuring synaptic
plasticity at electrophysiological, cellular, and molecular levels. This work
bridges the gap between computational neuroscience and agent-based AI, offering
a unique platform for studying embodiment, learning, and intelligence in a
controlled biological substrate.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [94] [Evaluating NL2SQL via SQL2NL](https://arxiv.org/abs/2509.04657)
*Mohammadtaher Safarzadeh,Afshin Oroojlooyjadid,Dan Roth*

Main category: cs.CL

TL;DR: 该论文提出了一种新的模型评估框架，通过SQL2NL自动生成语义等价的语言变体来系统性地测试NL2SQL模型的语言变化鲁棒性，发现当前最先进模型在语言变化下表现很脏弱。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL模型评测标准缺乏对语言变化的系统性考虑，无法真正评估模型在实际应用中的鲁棒性和普遍化能力。

Method: 设计了一种语义对齐的重写框架，利用SQL2NL技术自动生成保持原始意图和数据库结构对齐的语言变体，实现了在控制环境下对语言变化鲁棒性的目标评估。

Result: 当前最先进的NL2SQL模型在语言变化下表现很脏弱：LLaMa3.3-70B执行准确率下降10.23%，LLaMa3.1-8B下降近20%，较小模型受影响更大。鲁棒性退化与查询复杂度、数据集和领域显著相关。

Conclusion: 现有NL2SQL模型在语言变化方面存在显著的脏弱性，强调了开发明确测量语言普遍化能力的评估框架的必要性，以确保模型在实际应用中的可靠性。

Abstract: Robust evaluation in the presence of linguistic variation is key to
understanding the generalization capabilities of Natural Language to SQL
(NL2SQL) models, yet existing benchmarks rarely address this factor in a
systematic or controlled manner. We propose a novel schema-aligned paraphrasing
framework that leverages SQL-to-NL (SQL2NL) to automatically generate
semantically equivalent, lexically diverse queries while maintaining alignment
with the original schema and intent. This enables the first targeted evaluation
of NL2SQL robustness to linguistic variation in isolation-distinct from prior
work that primarily investigates ambiguity or schema perturbations. Our
analysis reveals that state-of-the-art models are far more brittle than
standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop
in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,
while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to
42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We
also find that robustness degradation varies significantly with query
complexity, dataset, and domain -- highlighting the need for evaluation
frameworks that explicitly measure linguistic generalization to ensure reliable
performance in real-world settings.

</details>


### [95] [Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis](https://arxiv.org/abs/2509.04459)
*Shiqin Han,Manning Gao,Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 一种不确定性驱动的协同系统，通过组合大型多模态语言模型和轻量模型，在保持高准确性的同时大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 解决多模态大型语言模型(MLLM)计算资源消耗大与轻量专门模型性能不足之间的矛盾

Method: 不确定性驱动的级联机制，先用小模型过滤样本，将高不确定性的难样本升级给MLLM处理，并采用加权平均和提示验证等策略处理模糊预测

Result: 在标准数据集上达到最先进性能，同时只需立派MLLM的少量计算资源

Conclusion: 该系统有效解决了性能与效率的争议，为多模态感知的实际部署提供了可行方案

Abstract: The advent of Multimodal Large Language Models (MLLMs) has significantly
advanced the state-of-the-art in multimodal machine learning, yet their
substantial computational demands present a critical barrier to real-world
deployment. Conversely, smaller, specialized models offer high efficiency but
often at the cost of performance. To reconcile this performance-efficiency
trade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)
that synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a
lightweight baseline model for multimodal sentiment analysis. The core of our
system is an uncertainty-driven cascade mechanism, where the efficient small
model first acts as a rapid filter for all input samples. Only those samples
yielding high predictive uncertainty, thereby indicating greater difficulty,
are selectively escalated to the MLLM for more sophisticated analysis.
Furthermore, our system introduces advanced strategies to handle ambiguous or
conflicting predictions, including weighted averaging for predictions of
similar polarity and a prompt-based cross-verification to resolve conflicting
predictions when both models exhibit high uncertainty. This
sample-difficulty-aware approach allows for a dynamic allocation of
computational resources, drastically reducing inference costs while retaining
the high accuracy of MLLM. Extensive experiments on benchmark datasets
demonstrate that our proposed method achieves state-of-the-art performance,
while requiring only a fraction of the computational resources compared to
using a standalone MLLM.

</details>


### [96] [Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition](https://arxiv.org/abs/2509.04480)
*Ryo Takahashi,Naoki Saito,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CL

TL;DR: 通过离散提示微调方法，使多模态大语言模型能够适应个人化视觉情感识别需求，充分利用个人特征来提升识别准确性


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉情感识别任务中表现出艰优性能，但因训练数据集宽泛而偏向主流观点，在个人化识别方面存在限制，影响实际应用效果

Method: 受人类提示工程的启发，采用离散提示微调方法，从生成的提示中选择最佳自然语言表达，并用其更新提示以实现个人化视觉情感识别

Result: 方法能够有效适应个人特征，提升了个人化视觉情感识别的准确性，充分利用个人化信息来充补大模型的偏向性限制

Conclusion: 通过离散提示微调方法，可以有效解决MLLMs在个人化VER任务中的偏向性问题，为实际应用提供了更准确的个人化情感识别能力

Abstract: Visual Emotion Recognition (VER) is an important research topic due to its
wide range of applications, including opinion mining and advertisement design.
Extending this capability to recognize emotions at the individual level further
broadens its potential applications. Recently, Multimodal Large Language Models
(MLLMs) have attracted increasing attention and demonstrated performance
comparable to that of conventional VER methods. However, MLLMs are trained on
large and diverse datasets containing general opinions, which causes them to
favor majority viewpoints and familiar patterns. This tendency limits their
performance in a personalized VER, which is crucial for practical and
real-world applications, and indicates a key area for improvement. To address
this limitation, the proposed method employs discrete prompt tuning inspired by
the process of humans' prompt engineering to adapt the VER task to each
individual. Our method selects the best natural language representation from
the generated prompts and uses it to update the prompt for the realization of
accurate personalized VER.

</details>


### [97] [Understanding Reinforcement Learning for Model Training, and future directions with GRAPE](https://arxiv.org/abs/2509.04501)
*Rohit Patel*

Main category: cs.CL

TL;DR: 这篇论文系统地介绍了指令微调的关键算法，包括SFT、Rejection Sampling、REINFORCE、TRPO、PPO、GRPO和DPO等，以清晰直观的方式进行从头开始的讲解。


<details>
  <summary>Details</summary>
Motivation: 现有的算法解释多假设前探知识、缺乏关键细节、或过于抽象复杂，需要一个更加清晰直观的教学质材。

Method: 使用简化明确的标记法，逐步展开每个算法的讲解，减少对普通强化学习文献的偏离，并与大语言模型直接联系。

Result: 提供了一个消除模糊性、降低认知负荷的完整算法体系教学。还包含了最新技术的文献综述和新的研究思路GRAPE。

Conclusion: 该论文为指令微调领域提供了一个全面且易于理解的算法基础，有助于推动该领域的教学和研究发展。

Abstract: This paper provides a self-contained, from-scratch, exposition of key
algorithms for instruction tuning of models: SFT, Rejection Sampling,
REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy
Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct
Preference Optimization (DPO). Explanations of these algorithms often assume
prior knowledge, lack critical details, and/or are overly generalized and
complex. Here, each method is discussed and developed step by step using
simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity
and provide a clear and intuitive understanding of the concepts. By minimizing
detours into the broader RL literature and connecting concepts to LLMs, we
eliminate superfluous abstractions and reduce cognitive overhead. Following
this exposition, we provide a literature review of new techniques and
approaches beyond those detailed. Finally, new ideas for research and
exploration in the form of GRAPE (Generalized Relative Advantage Policy
Evolution) are presented.

</details>


### [98] [Scaling behavior of large language models in emotional safety classification across sizes and tasks](https://arxiv.org/abs/2509.04512)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.CL

TL;DR: 研究大型语言模型处理情感敏感内容的能力，发现大模型在零样本和多标签分类中表现更好，但轻量级微调后的小模型也能达到相近性能，适合隐私敏感应用


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何处理情感敏感内容对于构建安全可靠的系统至关重要，特别是在心理健康领域需要确保对话安全性

Method: 构建包含15K+样本的心理健康数据集，使用情感重解释提示增强数据，评估4个LLaMA模型(1B-70B)在零样本、少样本和微调设置下的性能

Result: 大模型平均性能更强，特别是在零样本和多标签分类中；但轻量级微调后1B小模型在多个高数据类别中性能可比肩大模型和BERT，且推理时仅需<2GB显存

Conclusion: 小型设备端模型可作为隐私保护替代方案，具备解释情感语境和维护安全对话边界的能力，对治疗性LLM应用和安全关键系统的可扩展对齐具有重要意义

Abstract: Understanding how large language models (LLMs) process emotionally sensitive
content is critical for building safe and reliable systems, particularly in
mental health contexts. We investigate the scaling behavior of LLMs on two key
tasks: trinary classification of emotional safety (safe vs. unsafe vs.
borderline) and multi-label classification using a six-category safety risk
taxonomy. To support this, we construct a novel dataset by merging several
human-authored mental health datasets (> 15K samples) and augmenting them with
emotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA
models (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings.
Our results show that larger LLMs achieve stronger average performance,
particularly in nuanced multi-label classification and in zero-shot settings.
However, lightweight fine-tuning allowed the 1B model to achieve performance
comparable to larger models and BERT in several high-data categories, while
requiring <2GB VRAM at inference. These findings suggest that smaller,
on-device models can serve as viable, privacy-preserving alternatives for
sensitive applications, offering the ability to interpret emotional context and
maintain safe conversational boundaries. This work highlights key implications
for therapeutic LLM applications and the scalable alignment of safety-critical
systems.

</details>


### [99] [Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports](https://arxiv.org/abs/2509.04517)
*Indu Bala,Lewis Mitchell,Marianne H Gillam*

Main category: cs.CL

TL;DR: 通过NLP情感分析MAUDE数据库中网片移植病人报告，发现2011-2012和2017-2018期间关注报告和情感强度增加，为医疗实践提供病人体验的深度见解


<details>
  <summary>Details</summary>
Motivation: 网片移植手术常见但术后并发症问题严重，需要了解病人的情感体验以改善病人养护

Method: 使用NRC Emotion Lexicon和TextBlob对MAUDE数据库2000-2021年病人报告进行情感分析，分析八种情感类别和情感极性

Result: 发现2011-2012和2017-2018年间关注报告数量和情感强度显著增加，为医疗实践提供了有价值的病人体验见解

Conclusion: 情感考量在医学实践中至关重要，情感分析技术可以有效提升病人养护质量，改善术前咨询和术后养护

Abstract: Mesh implants are widely utilized in hernia repair surgeries, but
postoperative complications present a significant concern. This study analyzes
patient reports from the Manufacturer and User Facility Device Experience
(MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of
patients following mesh implantation using Natural Language Processing (NLP).
Employing the National Research Council Canada (NRC) Emotion Lexicon and
TextBlob for sentiment analysis, the research categorizes patient narratives
into eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy,
and disgust) and assesses sentiment polarity. The goal is to discern patterns
in patient sentiment over time and to identify reports signaling urgent
concerns, referred to as "Concern Reports," thereby understanding shifts in
patient experiences in relation to changes in medical device regulation and
technological advancements in healthcare. The study detected an increase in
Concern Reports and higher emotional intensity during the periods of 2011-2012
and 2017-2018. Through temporal analysis of Concern Reports and overall
sentiment, this research provides valuable insights for healthcare
practitioners, enhancing their understanding of patient experiences
post-surgery, which is critical for improving preoperative counselling,
postoperative care, and preparing patients for mesh implant surgeries. The
study underscores the importance of emotional considerations in medical
practices and the potential for sentiment analysis to inform and enhance
patient care.

</details>


### [100] [Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs](https://arxiv.org/abs/2509.04615)
*Brennen Hill,Surendra Parla,Venkata Abhijeeth Balabhadruni,Atharv Prajod Padmalayam,Sujay Chandra Shekara Sharma*

Main category: cs.CL

TL;DR: 本文对基于提示的大语言模型攻击方法进行了全面的文献综述，系统分类并建立了清晰的威胁模型，旨在帮助研究社区开发更安全的LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的普及带来了严重的安全挑战，攻击者可以通过操纵输入提示来规避安全对齐机制，导致知识产权盗窃、错误信息生成和用户信任侵蚀等问题。

Method: 采用文献综述方法，系统性地收集和分析现有的基于提示的攻击方法，并进行分类整理，建立清晰的威胁模型。

Result: 提供了对各类提示攻击机制的详细分析，包括其工作原理和影响，为理解LLM安全漏洞提供了系统框架。

Conclusion: 系统理解这些攻击向量是开发强大防御措施的基础步骤，本次调查旨在为研究社区构建下一代安全LLM提供信息支持，使其能够固有地抵抗未经授权的蒸馏、微调和编辑。

Abstract: The proliferation of Large Language Models (LLMs) has introduced critical
security challenges, where adversarial actors can manipulate input prompts to
cause significant harm and circumvent safety alignments. These prompt-based
attacks exploit vulnerabilities in a model's design, training, and contextual
understanding, leading to intellectual property theft, misinformation
generation, and erosion of user trust. A systematic understanding of these
attack vectors is the foundational step toward developing robust
countermeasures. This paper presents a comprehensive literature survey of
prompt-based attack methodologies, categorizing them to provide a clear threat
model. By detailing the mechanisms and impacts of these exploits, this survey
aims to inform the research community's efforts in building the next generation
of secure LLMs that are inherently resistant to unauthorized distillation,
fine-tuning, and editing.

</details>


### [101] [Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework](https://arxiv.org/abs/2509.04770)
*Zucheng Liang,Wenxin Wei,Kaijie Zhang,Hongyi Chen*

Main category: cs.CL

TL;DR: 本文提出基于MQUAKE框架的多跳问题分解方法，使用LLAMA3模型验证了该方法在知识图谱中提升复杂问题回答准确性的有效性，无论是在训练前还是训练后都显著优于直接回答方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在准确回答复杂问题方面一直面临挑战，需要寻找有效的方法来提升模型的理解和推理能力。

Method: 使用多跳问题分解方法将MQUAKE-T数据集转换为单跳和多跳两种格式，基于LLAMA3模型进行微调实验，采用LoRA方法进行参数高效微调。

Result: 未经微调时，多跳分解方法的预测性能显著优于直接回答方法；经过LoRA微调后，两种方法性能均有提升，但多跳分解方法始终保持优势。

Conclusion: 多跳问题分解方法能有效增强大型语言模型回答复杂问题的能力，在训练前后都表现出优越性能。

Abstract: Accurately answering complex questions has consistently been a significant
challenge for Large Language Models (LLMs). To address this, this paper
proposes a multi-hop question decomposition method for complex questions,
building upon research within the MQUAKE framework. Utilizing the LLAMA3 model,
we systematically investigate the impact of multi-hop question decomposition
within knowledge graphs on model comprehension and reasoning accuracy, both
before and after model training. In our experiments, we systematically
partitioned and converted the MQUAKE-T dataset into two distinct formats: a
single-hop dataset designed for directly answering complex questions, and a
multi-hop dataset constructed using the multi-hop question decomposition
method. We then fine-tuned the LLAMA3 model on these datasets and conducted
inference tests. Our results demonstrate that, without fine-tuning the LLM, the
prediction performance based on the multi-hop question decomposition method
significantly outperforms the method of directly answering complex questions.
After fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance
of both approaches improved compared to the untrained baseline. Crucially, the
method utilizing multi-hop decomposition consistently maintained its
superiority. These findings validate the effectiveness of the multi-hop
decomposition method both before and after training, demonstrating its
capability to effectively enhance the LLM's ability to answer complex
questions.

</details>


### [102] [PLaMo 2 Technical Report](https://arxiv.org/abs/2509.04897)
*Preferred Networks,:,Kaizaburo Chubachi,Yasuhiro Fujita,Shinichi Hemmi,Yuta Hirokawa,Toshiki Kataoka,Goro Kobayashi,Kenichi Maehashi,Calvin Metzger,Hiroaki Mikami,Shogo Murai,Daisuke Nishino,Kento Nozawa,Shintarou Okada,Daisuke Okanohara,Shunta Saito,Shotaro Sano,Shuji Suzuki,Daisuke Tanaka,Avinash Ummadisingu,Hanqin Wang,Sixue Wang,Tianqi Xu*

Main category: cs.CL

TL;DR: PLaMo 2是日本专用的大语言模型系列，采用混合Samba架构，通过持续预训练支持32K上下文，使用合成语料克服数据稀缺，通过权重重用和结构化剪枝实现计算效率，8B模型性能媲美之前的100B模型。


<details>
  <summary>Details</summary>
Motivation: 解决日本语言模型数据稀缺问题，提升计算效率，在保持高性能的同时大幅减少模型参数规模。

Method: 采用混合Samba架构过渡到全注意力机制，使用合成语料进行预训练，通过权重重用和结构化剪枝实现高效计算，结合监督微调(SFT)和直接偏好优化(DPO)进行后训练，使用vLLM优化推理和量化技术。

Result: 在日语基准测试中达到最先进水平，在指令跟随、语言流畅度和日语专业知识方面优于同规模开源模型，8B模型性能与之前100B模型相当。

Conclusion: PLaMo 2通过创新的架构设计、高效训练方法和优化技术，成功实现了高性能日本语言模型的紧凑化，为日语NLP应用提供了高效的解决方案。

Abstract: In this report, we introduce PLaMo 2, a series of Japanese-focused large
language models featuring a hybrid Samba-based architecture that transitions to
full attention via continual pre-training to support 32K token contexts.
Training leverages extensive synthetic corpora to overcome data scarcity, while
computational efficiency is achieved through weight reuse and structured
pruning. This efficient pruning methodology produces an 8B model that achieves
performance comparable to our previous 100B model. Post-training further
refines the models using a pipeline of supervised fine-tuning (SFT) and direct
preference optimization (DPO), enhanced by synthetic Japanese instruction data
and model merging techniques. Optimized for inference using vLLM and
quantization with minimal accuracy loss, the PLaMo 2 models achieve
state-of-the-art results on Japanese benchmarks, outperforming similarly-sized
open models in instruction-following, language fluency, and Japanese-specific
knowledge.

</details>


### [103] [Classification of kinetic-related injury in hospital triage data using NLP](https://arxiv.org/abs/2509.04969)
*Midhun Shyam,Jim Basilakis,Kieran Luken,Steven Thomas,John Crozier,Paul M. Middleton,X. Rosalind Wang*

Main category: cs.CL

TL;DR: 使用限制计算资源通过两步微调LLM来分类医疗分赚记录，解决医院数据敏感性、硬件资源不足和专家标注成本高的挑战


<details>
  <summary>Details</summary>
Motivation: 医院分赚记录含有丰富的医疗信息，但NLP分析面临数据敏感性、硬件资源限制和标注成本高等挑战

Method: 首先在GPU上使用2k开源数据微调预训练LLM分类器，然后在CPU上使用1000个医院特定样本进行进一步微调

Result: 通过精心管理数据集并利用现有模型和开源数据，成功在限制计算资源下完成分赚数据分类

Conclusion: 该流水线提供了一种可行方案，能够在硬件资源有限的医疗环境中应用LLM技术分析敏感医疗数据

Abstract: Triage notes, created at the start of a patient's hospital visit, contain a
wealth of information that can help medical staff and researchers understand
Emergency Department patient epidemiology and the degree of time-dependent
illness or injury. Unfortunately, applying modern Natural Language Processing
and Machine Learning techniques to analyse triage data faces some challenges:
Firstly, hospital data contains highly sensitive information that is subject to
privacy regulation thus need to be analysed on site; Secondly, most hospitals
and medical facilities lack the necessary hardware to fine-tune a Large
Language Model (LLM), much less training one from scratch; Lastly, to identify
the records of interest, expert inputs are needed to manually label the
datasets, which can be time-consuming and costly. We present in this paper a
pipeline that enables the classification of triage data using LLM and limited
compute resources. We first fine-tuned a pre-trained LLM with a classifier
using a small (2k) open sourced dataset on a GPU; and then further fine-tuned
the model with a hospital specific dataset of 1000 samples on a CPU. We
demonstrated that by carefully curating the datasets and leveraging existing
models and open sourced data, we can successfully classify triage data with
limited compute resources.

</details>


### [104] [Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts](https://arxiv.org/abs/2509.04982)
*Julius Neumann,Robert Lange,Yuni Susanti,Michael Färber*

Main category: cs.CL

TL;DR: 小型Transformer模型在短文本情感分类中的效果评估，发现数据增帽能提升性能，而预训练和分类头修改效果有限


<details>
  <summary>Details</summary>
Motivation: 解决短文本情感分类中的类不平衡、训练样本少、标签主观性以及上下文限制导致的模糊性和数据稀疏问题

Method: 评估小型BERT和RoBERTa模型(参数<10亿)，重点分析预训练、生成式数据增帽和分类头架构修改三个关键因素对性能的影响

Result: 数据增帽能显著提升分类性能，但在增帽数据上继续预训练反而会引入噪声降低准确性，分类头修改仅有边际改善

Conclusion: 为资源受限环境下优化BERT模型提供实用指南，建议重点采用数据增帽策略来改善短文本情感分类效果

Abstract: Sentiment classification in short text datasets faces significant challenges
such as class imbalance, limited training samples, and the inherent
subjectivity of sentiment labels -- issues that are further intensified by the
limited context in short texts. These factors make it difficult to resolve
ambiguity and exacerbate data sparsity, hindering effective learning. In this
paper, we evaluate the effectiveness of small Transformer-based models (i.e.,
BERT and RoBERTa, with fewer than 1 billion parameters) for multi-label
sentiment classification, with a particular focus on short-text settings.
Specifically, we evaluated three key factors influencing model performance: (1)
continued domain-specific pre-training, (2) data augmentation using
automatically generated examples, specifically generative data augmentation,
and (3) architectural variations of the classification head. Our experiment
results show that data augmentation improves classification performance, while
continued pre-training on augmented datasets can introduce noise rather than
boost accuracy. Furthermore, we confirm that modifications to the
classification head yield only marginal benefits. These findings provide
practical guidance for optimizing BERT-based models in resource-constrained
settings and refining strategies for sentiment classification in short-text
datasets.

</details>


### [105] [Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant](https://arxiv.org/abs/2509.05006)
*Inbal Bolshinsky,Shani Kupiec,Almog Sasson,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文通过对比研究探讨了在对话AI服务响应生成中，显式意图识别是否是高质量回复的必要前提，还是模型可以直接生成有效回复。


<details>
  <summary>Details</summary>
Motivation: 在对话AI时代，生成准确且上下文合适的服务响应仍是一个关键挑战。核心问题是：显式意图识别是否是生成高质量服务响应的先决条件，还是模型可以绕过这一步直接产生有效回复。

Method: 使用两个公开可用的服务交互数据集，对包括微调T5变体在内的多个最先进语言模型进行基准测试，比较意图优先响应生成和直接响应生成两种范式。

Result: 评估指标涵盖语言质量和任务成功率，揭示了关于显式意图建模必要性或冗余性的令人惊讶的见解。

Conclusion: 研究结果挑战了对话AI流水线中的传统假设，为设计更高效和有效的响应生成系统提供了可操作的指导方针。

Abstract: In the era of conversational AI, generating accurate and contextually
appropriate service responses remains a critical challenge. A central question
remains: Is explicit intent recognition a prerequisite for generating
high-quality service responses, or can models bypass this step and produce
effective replies directly? This paper conducts a rigorous comparative study to
address this fundamental design dilemma. Leveraging two publicly available
service interaction datasets, we benchmark several state-of-the-art language
models, including a fine-tuned T5 variant, across both paradigms: Intent-First
Response Generation and Direct Response Generation. Evaluation metrics
encompass both linguistic quality and task success rates, revealing surprising
insights into the necessity or redundancy of explicit intent modelling. Our
findings challenge conventional assumptions in conversational AI pipelines,
offering actionable guidelines for designing more efficient and effective
response generation systems.

</details>


### [106] [BEDTime: A Unified Benchmark for Automatically Describing Time Series](https://arxiv.org/abs/2509.05215)
*Medhasweta Sen,Zachary Gottesman,Jiaxing Qiu,C. Bayan Bruss,Nam Nguyen,Tom Hartvigsen*

Main category: cs.CL

TL;DR: 时间序列分析基础模型的标准化评测框架，通过3个自然语言描述任务评估13个先进模型，发现视觉语言模型表现最佳，但所有模型在稳健性测试中都存在问题


<details>
  <summary>Details</summary>
Motivation: 解决现有时间序列基础模型评估中的两个问题：缺乏直接对比评测，以及评估过于广泛而无法准确分析模型的具体能力

Method: 形式化了3个用于测试时间序列自然语言描述能力的任务：识别（真伪问答）、区分（多选题）和生成（开放式描述），并统一4个数据集进行头对头模型对比

Result: 评测13个先进模型发现：纯语言模型表现差强，需要时间序列特定架构；视觉语言模型表现优异；预训练多模态时间序列-语言模型超过纯语言模型但仍有提升空间；所有模型在稳健性测试中都存在脏性

Conclusion: 该标准化评测框架为时间序列理解系统提供了必要的评估标准，显示了不同类型模型在时间序列自然语言描述任务中的相对优势和不足

Abstract: Many recent studies have proposed general-purpose foundation models designed
for a variety of time series analysis tasks. While several established datasets
already exist for evaluating these models, previous works frequently introduce
their models in conjunction with new datasets, limiting opportunities for
direct, independent comparisons and obscuring insights into the relative
strengths of different methods. Additionally, prior evaluations often cover
numerous tasks simultaneously, assessing a broad range of model abilities
without clearly pinpointing which capabilities contribute to overall
performance. To address these gaps, we formalize and evaluate 3 tasks that test
a model's ability to describe time series using generic natural language: (1)
recognition (True/False question-answering), (2) differentiation (multiple
choice question-answering), and (3) generation (open-ended natural language
description). We then unify 4 recent datasets to enable head-to-head model
comparisons on each task. Experimentally, in evaluating 13 state-of-the-art
language, vision--language, and time series--language models, we find that (1)
popular language-only methods largely underperform, indicating a need for time
series-specific architectures, (2) VLMs are quite successful, as expected,
identifying the value of vision models for these tasks and (3) pretrained
multimodal time series--language models successfully outperform LLMs, but still
have significant room for improvement. We also find that all approaches exhibit
clear fragility in a range of robustness tests. Overall, our benchmark provides
a standardized evaluation on a task necessary for time series reasoning
systems.

</details>


### [107] [CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models](https://arxiv.org/abs/2509.05230)
*Aysenur Kocak,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: CURE是一个轻量级框架，通过内容提取器和反转网络分离概念无关表示，再通过对比学习调整残余概念线索，有效抑制概念驱动的伪相关性，提升模型鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型虽然成功但容易受到虚假概念相关性的影响，损害模型的鲁棒性和公平性，需要一种方法来系统性地解耦和抑制概念捷径。

Method: 使用内容提取器和反转网络提取概念无关表示，然后通过可控去偏模块的对比学习精细调整残余概念线索的影响。

Result: 在IMDB和Yelp数据集上，使用三种预训练架构，CURE在IMDB上F1分数绝对提升10个百分点，在Yelp上提升2个百分点，计算开销最小。

Conclusion: CURE为对抗概念偏见提供了一个灵活的无监督蓝图，为构建更可靠和公平的语言理解系统铺平了道路。

Abstract: Pre-trained language models have achieved remarkable success across diverse
applications but remain susceptible to spurious, concept-driven correlations
that impair robustness and fairness. In this work, we introduce CURE, a novel
and lightweight framework that systematically disentangles and suppresses
conceptual shortcuts while preserving essential content information. Our method
first extracts concept-irrelevant representations via a dedicated content
extractor reinforced by a reversal network, ensuring minimal loss of
task-relevant information. A subsequent controllable debiasing module employs
contrastive learning to finely adjust the influence of residual conceptual
cues, enabling the model to either diminish harmful biases or harness
beneficial correlations as appropriate for the target task. Evaluated on the
IMDB and Yelp datasets using three pre-trained architectures, CURE achieves an
absolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,
while introducing minimal computational overhead. Our approach establishes a
flexible, unsupervised blueprint for combating conceptual biases, paving the
way for more reliable and fair language understanding systems.

</details>


### [108] [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291)
*Deniz Bayazit,Aaron Mueller,Antoine Bosselut*

Main category: cs.CL

TL;DR: 使用稀疏交叉编码器追踪语言模型预训练过程中语言特征的演化，提出RelIE指标来识别特征何时对任务性能变得因果重要，实现架构无关的可扩展特征分析


<details>
  <summary>Details</summary>
Motivation: 传统基准测试无法揭示模型如何获得概念和能力，需要更细粒度的方法来理解预训练过程中特定语言能力的涌现

Method: 使用稀疏交叉编码器在不同检查点之间发现和对齐特征，训练具有显著性能差异的检查点三元组，引入RelIE指标追踪特征因果重要性

Result: 交叉编码器能够检测预训练过程中特征的出现、维持和终止，方法具有架构无关性和可扩展性

Conclusion: 该方法为预训练过程中的表示学习提供了更可解释和细粒度的分析路径

Abstract: Large language models (LLMs) learn non-trivial abstractions during
pretraining, like detecting irregular plural noun subjects. However, it is not
well understood when and how specific linguistic abilities emerge as
traditional evaluation methods such as benchmarking fail to reveal how models
acquire concepts and capabilities. To bridge this gap and better understand
model training at the concept level, we use sparse crosscoders to discover and
align features across model checkpoints. Using this approach, we track the
evolution of linguistic features during pretraining. We train crosscoders
between open-sourced checkpoint triplets with significant performance and
representation shifts, and introduce a novel metric, Relative Indirect Effects
(RelIE), to trace training stages at which individual features become causally
important for task performance. We show that crosscoders can detect feature
emergence, maintenance, and discontinuation during pretraining. Our approach is
architecture-agnostic and scalable, offering a promising path toward more
interpretable and fine-grained analysis of representation learning throughout
pretraining.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [109] [AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design](https://arxiv.org/abs/2509.04805)
*Keqin Zhang*

Main category: eess.SP

TL;DR: 本文综述AI驱动的无线前传压缩技术，重点分析CSI反馈和RB粒度预编码压缩两种高压缩方法，并提出面向无蜂窝架构的压缩策略。


<details>
  <summary>Details</summary>
Motivation: 现代无线前传链路需要在高带宽和低延迟约束下传输高维信号，传统压缩方法在高压缩比下性能急剧下降且难以调优，AI技术能更好地利用信道信息结构。

Method: 采用端到端学习、矢量和分层量化、学习熵模型等AI技术，重点研究CSI反馈端到端学习和RB粒度预编码优化压缩两种方法。

Result: 提出了针对无蜂窝架构的前传压缩策略，实现高压缩比下的可控性能损失，支持RB级速率自适应和低延迟推理。

Conclusion: AI驱动的压缩技术能够有效解决无线前传链路的高维信号传输挑战，为下一代网络集中式协作传输提供可行方案。

Abstract: Modern fronthaul links in wireless systems must transport high-dimensional
signals under stringent bandwidth and latency constraints, which makes
compression indispensable. Traditional strategies such as compressed sensing,
scalar quantization, and fixed-codec pipelines often rely on restrictive
priors, degrade sharply at high compression ratios, and are hard to tune across
channels and deployments. Recent progress in Artificial Intelligence (AI) has
brought end-to-end learned transforms, vector and hierarchical quantization,
and learned entropy models that better exploit the structure of Channel State
Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first
surveys AI-driven compression techniques and then provides a focused analysis
of two representative high-compression routes: CSI feedback with end-to-end
learning and Resource Block (RB) granularity precoding optimization combined
with compression. Building on these insights, we propose a fronthaul
compression strategy tailored to cell-free architectures. The design targets
high compression with controlled performance loss, supports RB-level rate
adaptation, and enables low-latency inference suitable for centralized
cooperative transmission in next-generation networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [110] [Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection](https://arxiv.org/abs/2509.04999)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 结合自编码器异常检测与主动学习的新方法，用于APT检测，在数据稀缺情况下显著提升检测率


<details>
  <summary>Details</summary>
Motivation: APT攻击具有隐蔽性和长期性，传统监督学习需要大量标注数据，而现实场景中标注数据稀缺

Method: 提出注意力对抗双自编码器异常检测框架，结合主动学习循环，选择性查询不确定样本的标签来减少标注成本

Result: 在DARPA透明计算项目的真实数据上测试，APT攻击仅占0.004%的极度不平衡数据中，检测率显著提升，优于现有方法

Conclusion: 该方法能够以最小数据有效学习，减少对大量人工标注的依赖，在多个操作系统环境下均表现优异

Abstract: Advanced Persistent Threats (APTs) present a considerable challenge to
cybersecurity due to their stealthy, long-duration nature. Traditional
supervised learning methods typically require large amounts of labeled data,
which is often scarce in real-world scenarios. This paper introduces a novel
approach that combines AutoEncoders for anomaly detection with active learning
to iteratively enhance APT detection. By selectively querying an oracle for
labels on uncertain or ambiguous samples, our method reduces labeling costs
while improving detection accuracy, enabling the model to effectively learn
with minimal data and reduce reliance on extensive manual labeling. We present
a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based
anomaly detection framework and demonstrate how the active learning loop
progressively enhances the model's performance. The framework is evaluated on
real-world, imbalanced provenance trace data from the DARPA Transparent
Computing program, where APT-like attacks account for just 0.004\% of the data.
The datasets, which cover multiple operating systems including Android, Linux,
BSD, and Windows, are tested in two attack scenarios. The results show
substantial improvements in detection rates during active learning,
outperforming existing methods.

</details>


### [111] [On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy](https://arxiv.org/abs/2509.05265)
*Zijian Wang,Wei Tong,Tingxuan Han,Haoyu Chen,Tianling Zhang,Yunlong Mao,Sheng Zhong*

Main category: cs.CR

TL;DR: 本文提出了一种专门针对应用局部差分隐私的联邦学习环境的模型毒化攻击框架，能够避免现有防御机制并显著降低全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习与局部差分隐私结合虽能保护隐私，但在受恶意参与者攻击时很弱容，尤其是模型毒化攻击的防御研究不足。

Method: 提出了一种可扩展的模型毒化攻击框架，通过最大化全局训练损失来实现攻击目标，并为对抗Multi-Krum和修剪均值等防御机制而设计了适应性攻击方法。

Result: 在三种代表性LDPFL协议、三个标准数据集和两种深度神经网络上进行实验，结果显示该攻击框架能显著降低全局模型性能。

Conclusion: 该研究曝露了LDPFL协议在模型毒化攻击下的严重漏洞，强调了开发更稳健的防御策略的必要性。

Abstract: Federated learning (FL) combined with local differential privacy (LDP)
enables privacy-preserving model training across decentralized data sources.
However, the decentralized data-management paradigm leaves LDPFL vulnerable to
participants with malicious intent. The robustness of LDPFL protocols,
particularly against model poisoning attacks (MPA), where adversaries inject
malicious updates to disrupt global model convergence, remains insufficiently
studied. In this paper, we propose a novel and extensible model poisoning
attack framework tailored for LDPFL settings. Our approach is driven by the
objective of maximizing the global training loss while adhering to local
privacy constraints. To counter robust aggregation mechanisms such as
Multi-Krum and trimmed mean, we develop adaptive attacks that embed carefully
crafted constraints into a reverse training process, enabling evasion of these
defenses. We evaluate our framework across three representative LDPFL
protocols, three benchmark datasets, and two types of deep neural networks.
Additionally, we investigate the influence of data heterogeneity and privacy
budgets on attack effectiveness. Experimental results demonstrate that our
adaptive attacks can significantly degrade the performance of the global model,
revealing critical vulnerabilities and highlighting the need for more robust
LDPFL defense strategies against MPA. Our code is available at
https://github.com/ZiJW/LDPFL-Attack

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [112] [Dynamical Learning in Deep Asymmetric Recurrent Neural Networks](https://arxiv.org/abs/2509.05041)
*Davide Badalotti,Carlo Baldassi,Marc Mézard,Mattia Scardecchia,Riccardo Zecchina*

Main category: cond-mat.dis-nn

TL;DR: 非对称深度循环神经网络通过稀疏兴奋性耦合产生指数级大规模的可访问表示空间，提出无需梯度评估的分布式学习方案，在标准AI基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 旨在构建具有大规模内部表示空间的循环神经网络模型，探索无需梯度计算的分布式学习机制，缩小AI与计算神经科学之间的差距

Method: 使用非对称深度循环神经网络，增强稀疏兴奋性耦合，通过迭代动力学寻找稳定配置，采用分布式学习方案，输入输出关联从循环动力学中自然涌现

Result: 模型产生指数级大规模密集可访问的内部表示流形，学习过程稳定，移除监督信号后配置仍保持稳定，在标准AI基准测试中表现竞争力

Conclusion: 该方法提供了一种无需梯度评估的有效学习范式，具有良好的泛化能力，在计算和生物学方向都有拓展潜力，有助于弥合AI与神经科学之间的鸿沟

Abstract: We show that asymmetric deep recurrent neural networks, enhanced with
additional sparse excitatory couplings, give rise to an exponentially large,
dense accessible manifold of internal representations which can be found by
different algorithms, including simple iterative dynamics. Building on the
geometrical properties of the stable configurations, we propose a distributed
learning scheme in which input-output associations emerge naturally from the
recurrent dynamics, without any need of gradient evaluation. A critical feature
enabling the learning process is the stability of the configurations reached at
convergence, even after removal of the supervisory output signal. Extensive
simulations demonstrate that this approach performs competitively on standard
AI benchmarks. The model can be generalized in multiple directions, both
computational and biological, potentially contributing to narrowing the gap
between AI and computational neuroscience.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [113] [The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models](https://arxiv.org/abs/2509.04781)
*Danielle Ensign,Henry Sleight,Kyle Fish*

Main category: cs.CY

TL;DR: 大语言模型在对话中会选择退出对话（bail），实际退出率在0.06-7%之间，受模型类型、退出方式和提示词影响显著


<details>
  <summary>Details</summary>
Motivation: 研究LLM在对话中是否会主动选择退出对话，以及退出行为的频率和影响因素

Method: 使用三种退出方法（工具调用、特定字符串输出、提问模型）在实际对话数据集（Wildchat和ShareGPT）上测试，并构建BailBench综合数据集进行系统化测试

Result: 模型退出率在0.28-32%之间，但考虑偏差后实际退出率为0.06-7%；退出行为受模型类型、退出方式和提示词影响显著；退出与拒绝行为有关联但不完全相同

Conclusion: LLM存在退出对话的能力，退出率受多种因素影响，这种行为与拒绝行为有一定关联但不可直接推测，需要更深入研究模型的对话退出机制

Abstract: When given the option, will LLMs choose to leave the conversation (bail)? We
investigate this question by giving models the option to bail out of
interactions using three different bail methods: a bail tool the model can
call, a bail string the model can output, and a bail prompt that asks the model
if it wants to leave. On continuations of real world data (Wildchat and
ShareGPT), all three of these bail methods find models will bail around
0.28-32\% of the time (depending on the model and bail method). However, we
find that bail rates can depend heavily on the model used for the transcript,
which means we may be overestimating real world bail rates by up to 4x. If we
also take into account false positives on bail prompt (22\%), we estimate real
world bail rates range from 0.06-7\%, depending on the model and bail method.
We use observations from our continuations of real world data to construct a
non-exhaustive taxonomy of bail cases, and use this taxonomy to construct
BailBench: a representative synthetic dataset of situations where some models
bail. We test many models on this dataset, and observe some bail behavior
occurring for most of them. Bail rates vary substantially between models, bail
methods, and prompt wordings. Finally, we study the relationship between
refusals and bails. We find: 1) 0-13\% of continuations of real world
conversations resulted in a bail without a corresponding refusal 2) Jailbreaks
tend to decrease refusal rates, but increase bail rates 3) Refusal abliteration
increases no-refuse bail rates, but only for some bail methods 4) Refusal rate
on BailBench does not appear to predict bail rate.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [114] [Capturing an Invisible Robber using Separators](https://arxiv.org/abs/2509.05024)
*Igor Potapov,Tymofii Prokopenko,John Sylvester*

Main category: cs.DM

TL;DR: 提出基于分离层次结构的新方法来攻击零可见性警窃游戏，改善了捕获时间和空间复杂度，并提供了更好的近似零可见性警察数上界。


<details>
  <summary>Details</summary>
Motivation: 零可见性警窃游戏中警察在捕获前无法看到窃贼，这与全信息的经典游戏不同。需要更高效的策略来应对这种信息不完全的情况。

Method: 使用分离层次结构替代路径宽度分解方法，设计了新的捕获策略。

Result: 新方法在大多数情况下保持了零可见性警察数的趋势复杂度，同时显著改善了捕获时间和空间复杂度。

Conclusion: 分离层次结构方法为零可见性警窃游戏提供了更高效的解决方案，并在多种图类上得到了更优的近似算法上界。

Abstract: We study the zero-visibility cops and robbers game, where the robber is
invisible to the cops until they are caught. This differs from the classic game
where full information about the robber's location is known at any time. A
previously known solution for capturing a robber in the zero-visibility case is
based on the pathwidth decomposition. We provide an alternative solution based
on a separation hierarchy, improving capture time and space complexity without
asymptotically increasing the zero-visibility cop number in most cases. In
addition, we provide a better bound on the approximate zero-visibility cop
number for various classes of graphs, where approximate refers to the
restriction to polynomial time computable strategies.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [115] [Filtering with Randomised Observations: Sequential Learning of Relevant Subspace Properties and Accuracy Analysis](https://arxiv.org/abs/2509.04867)
*Nazanin Abedini,Jana de Wiljes,Svetlana Dubinkina*

Main category: math.NA

TL;DR: 本文研究了连续集合卫易筛波器在固定、随机和适应性部分观测下的信号跟踪性能，并提出了一种适应性子空间维度确定方法来控制筛波误差。


<details>
  <summary>Details</summary>
Motivation: 结合观测数据和数学模型的状态估计在很多应用中都很重要，集合卫易筛波是常用方法。需要研究不同观测方式下的筛波性能，并提供系统化的方法来确定适当的状态子空间维度。

Method: 使用连续集合卫易筛波方法，在固定、随机和适应性变化部分观测条件下进行信号跟踪。建立了与观测运算符随机性相关的预期跟踪误差严格界限。提出了一种序列学习方案，适应性地确定保证筛波误差有界所需的状态子空间维度。

Result: 获得了与观测运算符随机性相关的预期信号跟踪误差的严格界限。适应性方案能够通过平衡观测复杂性和估计准确性来系统地确定适当的筛波相关子空间大小。

Conclusion: 该研究为连续集合卫易筛波提供了严格的性能分析和误差控制方法。适应性子空间确定方法不仅能控制筛波误差，还能系统地识别基础动力学中筛波相关的适当子空间维度。

Abstract: State estimation that combines observational data with mathematical models is
central to many applications and is commonly addressed through filtering
methods, such as ensemble Kalman filters. In this article, we examine the
signal-tracking performance of a continuous ensemble Kalman filtering under
fixed, randomised, and adaptively varying partial observations. Rigorous bounds
are established for the expected signal-tracking error relative to the
randomness of the observation operator. In addition, we propose a sequential
learning scheme that adaptively determines the dimension of a state subspace
sufficient to ensure bounded filtering error, by balancing observation
complexity with estimation accuracy. Beyond error control, the adaptive scheme
provides a systematic approach to identifying the appropriate size of the
filter-relevant subspace of the underlying dynamics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [116] [Universal Representation of Generalized Convex Functions and their Gradients](https://arxiv.org/abs/2509.04477)
*Moeen Nehzati*

Main category: math.OC

TL;DR: 本文提出了广义凸函数(GCFs)的参数化方法，具有凸性和通用逼近性质，可用于将双层优化问题转化为单层优化问题，并在多商品收益最大化拍卖问题中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 广义凸函数在优化问题中广泛应用，但现有的参数化方法未能充分利用其特性。需要开发具有通用逼近性质的参数化方法来有效解决嵌套双层优化问题。

Method: 基于凸函数参数化文献，扩展提出GCFs的凸参数化方法，该方法具有潜在的一对一映射特性和通用逼近性质，并与浅层神经网络进行比较分析。

Result: 开发了Python包gconvex实现该方法，成功应用于多商品收益最大化拍卖问题，证明参数化方法的有效性。

Conclusion: 提出的GCFs参数化方法具有理论保证和实际应用价值，为复杂优化问题提供了有效的数值求解工具。

Abstract: Solutions to a wide range of optimization problems, from optimal transport
theory to mathematical economics, often take the form of generalized convex
functions (GCFs). This characterization can be used to convert nested bilevel
optimization problems into single-level optimization problems. Despite this,
the characterization has not been fully exploited in numerical optimization.
  When the solution to an optimization problem is known to belong to a
particular class of objects, this information can be leveraged by
parameterizing that class of objects and optimizing over this parameterization.
The hallmark of a good parameterization is the Universal Approximation Property
(UAP): that is, the parameterization approximates any object in the class
arbitrarily well. For example, neural networks satisfy the UAP with respect to
the class of continuous functions.
  Building on the literature concerned with the parameterization of convex
functions, we extend these ideas to GCFs. We present a convex and potentially
one-to-one parameterization of GCFs and their gradients that satisfies the UAP.
We also compare this class to shallow neural networks and highlight their
shared characteristics.
  The ideas pursued here have been implemented in the Python package
\href{https://github.com/MoeenNehzati/gconvex}{\texttt{gconvex}}, available
online. Using it, we tackle the problem of finding the revenue-maximizing
auction for multiple goods and demonstrate how our parameterization can
effectively solve this problem.

</details>


### [117] [Provably data-driven projection method for quadratic programming](https://arxiv.org/abs/2509.04524)
*Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: math.OC

TL;DR: 本文分析了数据驱动投影矩阵学习方法在凸二次规划问题中的泛化保证，提出了展开有效集方法，并扩展到其他设置。


<details>
  <summary>Details</summary>
Motivation: 投影方法通过降低优化问题的维度来提高高维问题的可扩展性。最近的数据驱动方法在线性规划中取得了成功，但在凸二次规划中，由于最优解不局限于多面体顶点，分析更加复杂。

Method: 利用Caratheodory定理将凸二次规划的解定位在特殊有效集对应的可行区域内，提出展开有效集方法，将最优值计算建模为具有有界复杂度的Goldberg-Jerrum算法。

Result: 建立了凸二次规划数据驱动投影矩阵学习的学习保证，并进一步扩展到学习匹配最优解和输入感知设置。

Conclusion: 该方法成功解决了凸二次规划中数据驱动投影学习的泛化分析难题，为高维优化问题的可扩展性提供了理论保证。

Abstract: Projection methods aim to reduce the dimensionality of the optimization
instance, thereby improving the scalability of high-dimensional problems.
Recently, Sakaue and Oki proposed a data-driven approach for linear programs
(LPs), where the projection matrix is learned from observed problem instances
drawn from an application-specific distribution of problems. We analyze the
generalization guarantee for the data-driven projection matrix learning for
convex quadratic programs (QPs). Unlike in LPs, the optimal solutions of convex
QPs are not confined to the vertices of the feasible polyhedron, and this
complicates the analysis of the optimal value function. To overcome this
challenge, we demonstrate that the solutions of convex QPs can be localized
within a feasible region corresponding to a special active set, utilizing
Caratheodory's theorem. Building on such observation, we propose the unrolled
active set method, which models the computation of the optimal value as a
Goldberg-Jerrum (GJ) algorithm with bounded complexities, thereby establishing
learning guarantees. We then further extend our analysis to other settings,
including learning to match the optimal solution and input-aware setting, where
we learn a mapping from QP problem instances to projection matrices.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [118] [Improved Bounds for Twin-Width Parameter Variants with Algorithmic Applications to Counting Graph Colorings](https://arxiv.org/abs/2509.05122)
*Ambroise Baril,Miguel Couceiro,Victor Lagerkvist*

Main category: cs.CC

TL;DR: 本文改进了H-着色问题中组件孪生宽度和团宽之间的参数关系，从之前的指数级改进为线性界，并提出了更快的计数算法


<details>
  <summary>Details</summary>
Motivation: H-着色问题是经典NP完全问题k-着色的推广，研究其与团宽和组件孪生宽度等参数的关系对于理解计算复杂性具有重要意义

Method: 通过构造性证明建立了组件孪生宽度与团宽之间的线性关系，并基于此设计了两种算法：一种使用输入图的收缩序列，另一种使用模板图的收缩序列

Result: 获得了组件孪生宽度与团宽之间的线性界（之前为指数级），提出了更快的#H-着色计数算法，在多种图类（如cographs、长度≥7的环、距离遗传图）上表现更优

Conclusion: 该研究显著改进了图参数之间的关系界限，并提供了更高效的算法，为图同态计数问题的参数化复杂性研究提供了新的视角

Abstract: The $H$-Coloring problem is a well-known generalization of the classical
NP-complete problem $k$-Coloring where the task is to determine whether an
input graph admits a homomorphism to the template graph $H$. This problem has
been the subject of intense theoretical research and in this article we study
the complexity of $H$-Coloring with respect to the parameters clique-width and
the more recent component twin-width, which describe desirable computational
properties of graphs. We give two surprising linear bounds between these
parameters, thus improving the previously known exponential and double
exponential bounds. Our constructive proof naturally extends to related
parameters and as a showcase we prove that total twin-width and linear
clique-width can be related via a tight quadratic bound. These bounds naturally
lead to algorithmic applications. The linear bounds between component
twin-width and clique-width entail natural approximations of component
twin-width, by making use of the results known for clique-width. As for
computational aspects of graph coloring, we target the richer problem of
counting the number of homomorphisms to $H$ (#$H$-Coloring). The first
algorithm that we propose uses a contraction sequence of the input graph $G$
parameterized by the component twin-width of $G$. This leads to a positive FPT
result for the counting version. The second uses a contraction sequence of the
template graph $H$ and here we instead measure the complexity with respect to
the number of vertices in the input graph. Using our linear bounds we show that
our algorithms are always at least as fast as the previously best #$H$-Coloring
algorithms (based on clique-width) and for several interesting classes of
graphs (e.g., cographs, cycles of length $\ge 7$, or distance-hereditary
graphs) are in fact strictly faster.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [119] [Optimal Variance and Covariance Estimation under Differential Privacy in the Add-Remove Model and Beyond](https://arxiv.org/abs/2509.04919)
*Shokichi Takakura,Seng Pei Liew,Satoshi Hasegawa*

Main category: stat.ML

TL;DR: 提出基于Bézier机制的高效差分隐私方差和协方差估计算法，在add-remove模型下实现极小极大最优性，并在实例级别效用上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注swap模型下的差分隐私估计，而add-remove模型由于需要保护数据集大小隐私，更具挑战性且研究较少，需要开发新的高效机制

Method: 基于Bézier机制（一种利用Bernstein基的新型矩释放框架）开发方差和协方差估计机制，建立新的极小极大下界证明最优性

Result: 在高隐私机制下达到极小极大最优，实例级别效用分析显示Bézier估计器始终优于其他机制，且该机制可扩展到其他统计任务

Conclusion: Bézier机制为add-remove模型下的差分隐私统计估计提供了有效的解决方案，在理论和实践层面都表现出优越性能

Abstract: In this paper, we study the problem of estimating the variance and covariance
of datasets under differential privacy in the add-remove model. While
estimation in the swap model has been extensively studied in the literature,
the add-remove model remains less explored and more challenging, as the dataset
size must also be kept private. To address this issue, we develop efficient
mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier
mechanism}, a novel moment-release framework that leverages Bernstein bases. We
prove that our proposed mechanisms are minimax optimal in the high-privacy
regime by establishing new minimax lower bounds. Moreover, beyond worst-case
scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based
estimator consistently achieves better utility compared to alternative
mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier
mechanism beyond variance and covariance estimation, showcasing its
applicability to other statistical tasks.

</details>


### [120] [Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment](https://arxiv.org/abs/2509.04852)
*Wei Chen,Shigui Li,Jiacheng Li,Jian Xu,Zhiqi Lin,Junmei Yang,Delu Zeng,John Paisley,Qibin Zhao*

Main category: stat.ML

TL;DR: ISA-DRE是一种新的密度比估计框架，通过全局割线函数和区间退火策略，实现了无需数值积分的高效准确估计


<details>
  <summary>Details</summary>
Motivation: 现有密度比估计方法往往在准确性和效率之间进行权衡，需要数值积分且计算成本高

Method: 提出ISA-DRE框架：1）学习全局割线函数而非局部切线，降低方差；2）基于割线对齐恒等式的自一致性条件；3）收缩区间退火策略逐步扩大对齐区间

Result: 实验表明ISA-DRE在显著减少函数评估次数的同时达到竞争性精度，推理速度更快

Conclusion: 该方法特别适合实时和交互式应用，为密度比估计提供了高效准确的解决方案

Abstract: Estimating density ratios is a fundamental problem in machine learning, but
existing methods often trade off accuracy for efficiency. We propose
\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},
a framework that enables accurate, any-step estimation without numerical
integration.
  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE
learns a global secant function, defined as the expectation of all tangents
over an interval, with provably lower variance, making it more suitable for
neural approximation. This is made possible by the \emph{Secant Alignment
Identity}, a self-consistency condition that formally connects the secant with
its underlying tangent representations.
  To mitigate instability during early training, we introduce \emph{Contraction
Interval Annealing}, a curriculum strategy that gradually expands the alignment
interval during training. This process induces a contraction mapping, which
improves convergence and training stability.
  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer
function evaluations compared to prior methods, resulting in much faster
inference and making it well suited for real-time and interactive applications.

</details>


### [121] [Spectral Algorithms in Misspecified Regression: Convergence under Covariate Shift](https://arxiv.org/abs/2509.05106)
*Ren-Rui Liu,Zheng-Chu Guo*

Main category: stat.ML

TL;DR: 本文研究了协变量偏移下谱算法的收敛性质，通过引入重要性权重处理分布不匹配问题，在再生核希尔伯特空间中建立了加权谱算法，并提供了误设情况下的理论分析。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中源域和目标域输入分布不同但条件分布相同的协变量偏移问题，特别是在目标函数不属于RKHS的误设情况下，现有研究主要关注well-specified设置，缺乏对更实际误设情况的全面理论分析。

Method: 引入重要性权重（目标与源密度比）到学习框架中，在RKHS中构建加权谱算法。对于无界重要性权重情况，提出了新的截断技术，并在误设情况下扩展了收敛率分析。

Result: 在重要性权重有界的情况下，当目标函数属于RKHS时获得了极小极大最优收敛率；对于无界权重情况，通过截断技术在温和正则条件下实现了近最优收敛率，并将结果扩展到误设机制。

Conclusion: 通过解决协变量偏移和模型误设的相互交织挑战，将经典核学习理论扩展到更实际的场景，为理解它们的相互作用提供了系统框架。

Abstract: This paper investigates the convergence properties of spectral algorithms --
a class of regularization methods originating from inverse problems -- under
covariate shift. In this setting, the marginal distributions of inputs differ
between source and target domains, while the conditional distribution of
outputs given inputs remains unchanged. To address this distributional
mismatch, we incorporate importance weights, defined as the ratio of target to
source densities, into the learning framework. This leads to a weighted
spectral algorithm within a nonparametric regression setting in a reproducing
kernel Hilbert space (RKHS). More importantly, in contrast to prior work that
largely focuses on the well-specified setting, we provide a comprehensive
theoretical analysis of the more challenging misspecified case, in which the
target function does not belong to the RKHS. Under the assumption of uniformly
bounded density ratios, we establish minimax-optimal convergence rates when the
target function lies within the RKHS. For scenarios involving unbounded
importance weights, we introduce a novel truncation technique that attains
near-optimal convergence rates under mild regularity conditions, and we further
extend these results to the misspecified regime. By addressing the intertwined
challenges of covariate shift and model misspecification, this work extends
classical kernel learning theory to more practical scenarios, providing a
systematic framework for understanding their interaction.

</details>


### [122] [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](https://arxiv.org/abs/2509.05186)
*Benjamin J. Zhang,Siting Liu,Stanley J. Osher,Markos A. Katsoulakis*

Main category: stat.ML

TL;DR: ICON是一种基于基础模型的算子学习方法，通过概率框架揭示其隐式执行贝叶斯推断，计算后验预测分布的均值。GenICON扩展为生成式设置，能够从解算子的后验预测分布中采样，实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有ICON方法虽然能够学习微分方程的解算子映射，但缺乏对不确定性量化的理论支撑。需要建立概率框架来理解ICON的贝叶斯推断本质，并扩展为生成式方法以捕捉解算子的不确定性。

Method: 提出随机微分方程的概率框架，将ICON形式化为计算后验预测分布均值的贝叶斯推断过程。在此基础上开发生成式ICON(GenICON)，能够从解算子的后验预测分布中采样。

Result: 建立了ICON的贝叶斯概率解释框架，证明了其隐式执行后验预测分布均值计算。GenICON能够生成解算子的后验样本，为算子学习提供原则性的不确定性量化方法。

Conclusion: 概率视角为ICON提供了理论基础，揭示了其贝叶斯推断本质。GenICON扩展了ICON的生成能力，实现了对解算子不确定性的量化，为算子学习提供了更完整的概率框架。

Abstract: In-context operator networks (ICON) are a class of operator learning methods
based on the novel architectures of foundation models. Trained on a diverse set
of datasets of initial and boundary conditions paired with corresponding
solutions to ordinary and partial differential equations (ODEs and PDEs), ICON
learns to map example condition-solution pairs of a given differential equation
to an approximation of its solution operator. Here, we present a probabilistic
framework that reveals ICON as implicitly performing Bayesian inference, where
it computes the mean of the posterior predictive distribution over solution
operators conditioned on the provided context, i.e., example condition-solution
pairs. The formalism of random differential equations provides the
probabilistic framework for describing the tasks ICON accomplishes while also
providing a basis for understanding other multi-operator learning methods. This
probabilistic perspective provides a basis for extending ICON to
\emph{generative} settings, where one can sample from the posterior predictive
distribution of solution operators. The generative formulation of ICON
(GenICON) captures the underlying uncertainty in the solution operator, which
enables principled uncertainty quantification in the solution predictions in
operator learning.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [123] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: 首个大规模浮点数算法实践研究，分析GitHub公开库中静态类型语言的浮点数代码使用情况


<details>
  <summary>Details</summary>
Motivation: 理解实际代码中浮点数算法的真实使用情况，以支持更有效的静态分析、动态分析和程序修复技术的发展

Method: 采用随机采样和内在属性过滤的最佳实践，通过源代码关键字搜索和代码解析来识别浮点数使用和程序构造

Result: 证实了浮点数算法被广泛使用的论断，发现文献中用于评估自动化推理技术的基准测试在某些方面代表实际代码，但非全部方面

Conclusion: 研究结果和数据集将帮助未来浮点数算法技术更好地设计和评估，以满足实际用户需求

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [124] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: 通过域特定建模技术和图形化可视化提高AI辅助编程的透明度，支持可视检查和形式验证


<details>
  <summary>Details</summary>
Motivation: 提高AI辅助编程的软件开发性能，通过增强透明性来促进代码生成和验证过程

Method: 集成域特定建模技术，提供实时图形化可视化，支持编程、自然语言提示、语音命令和阶段性精化建模

Result: 开发了Visual Studio Code扩展原型（Lingua Franca语言），展示了新颖域特定建模实践的潜力

Conclusion: 该方法为模型创建、可视化和验证提供了进步，有助于提高AI辅助编程的效果和可靠性

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [125] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: Pulse Infinite是一个使用证明技术检测大型程序中非终止性（发散）的工具，通过组合性和欠近似方法实现了对亿级代码库的规模化分析，在真实代码库中发现了30多个未知问题


<details>
  <summary>Details</summary>
Motivation: 现有工作主要针对小型基准测试（几十到几百行代码），而实际企业代码库可能达到数千万甚至数亿行代码，规模化限制阻碍了这些工具的实用性

Method: 采用组合性和欠近似证明技术：组合性支持规模化分析，欠近似确保证明发散性的可靠性

Result: 在超过1亿行的开源和专有软件（C、C++、Hack语言）中应用，识别出30多个先前未知的问题，在真实代码库的发散检测方面达到了新的技术水平

Conclusion: Pulse Infinite工具成功解决了大规模代码库中非终止性检测的规模化挑战，为实际企业级应用提供了有效的解决方案

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [126] [Inferring the Graph Structure of Images for Graph Neural Networks](https://arxiv.org/abs/2509.04677)
*Mayur S Gowda,John Shi,Augusto Santos,José M. F. Moura*

Main category: eess.IV

TL;DR: 通过使用像素值相关性构建行相关图、列相关图和积图来替代传统的格子图表示MNIST和Fashion-MNIST数据集，提高了图神经网络的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的格子图表示方式在图神经网络任务中并非最优选择，需要找到更好的图结构表示来提高下游任务的性能。

Method: 使用像素值之间的相关性来构建行相关图、列相关图和积图，并将这些新的图表示作为图神经网络的输入。

Result: 实验结果显示，使用这些新的图表示方法比传统的格子图和超像素方法能够提高图神经网络的分类准确性。

Conclusion: 通过优化图表示方式可以有效提升图神经网络在图像分类任务中的性能，这为图神经网络的图表示学习提供了新的思路。

Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural
Network (GNN) architectures. The images are traditionally represented as a grid
graph with each node representing a pixel and edges connecting neighboring
pixels (vertically and horizontally). The graph signal is the values
(intensities) of each pixel in the image. The graphs are commonly used as input
to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph
CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the
images. In this work, we improve the accuracy of downstream graph neural
network tasks by finding alternative graphs to the grid graph and superpixel
methods to represent the dataset images, following the approach in [5, 6]. We
find row correlation, column correlation, and product graphs for each image in
MNIST and Fashion-MNIST using correlations between the pixel values building on
the method in [5, 6]. Experiments show that using these different graph
representations and features as input into downstream GNN models improves the
accuracy over using the traditional grid graph and superpixel methods in the
literature.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [127] [RobQFL: Robust Quantum Federated Learning in Adversarial Environment](https://arxiv.org/abs/2509.04914)
*Walid El Maouaki,Nouhaila Innan,Alberto Marchisio,Taoufik Said,Muhammad Shafique,Mohamed Bennai*

Main category: quant-ph

TL;DR: 量子联邦学习对对抗性噪声脆弱，RobQFL通过在联邦循环中嵌入对抗训练来提升鲁棒性，通过调节客户端覆盖率、扰动调度和优化策略，在MNIST和Fashion-MNIST数据集上显著提高了对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习结合了隐私保护和量子计算优势，但其对抗噪声的鲁棒性未知。研究发现QFL与集中式量子学习同样脆弱，需要开发鲁棒解决方案。

Method: 提出Robust Quantum Federated Learning (RobQFL)，在联邦循环中直接嵌入对抗训练。通过三个可调参数：客户端覆盖率γ(0-100%)、扰动调度(固定ε vs ε混合)、优化策略(微调 vs 从头训练)，并定义了两个评估指标：准确率-鲁棒性面积和鲁棒性体积。

Result: 在15客户端的MNIST和Fashion-MNIST数据集上，IID和非IID条件下，仅对20-50%客户端进行对抗训练就能在ε≤0.1时提升准确率约15个百分点，且干净准确率损失小于2个百分点。微调策略额外提升3-5个百分点。当覆盖率≥75%时，中等ε混合策略最优。标签排序的非IID分割使鲁棒性减半。

Conclusion: 数据异构性是主要风险因素，适度的对抗训练覆盖率(20-50%)和精心设计的扰动调度可以显著提升量子联邦学习的对抗鲁棒性，而无需对所有客户端进行对抗训练。

Abstract: Quantum Federated Learning (QFL) merges privacy-preserving federation with
quantum computing gains, yet its resilience to adversarial noise is unknown. We
first show that QFL is as fragile as centralized quantum learning. We propose
Robust Quantum Federated Learning (RobQFL), embedding adversarial training
directly into the federated loop. RobQFL exposes tunable axes: client coverage
$\gamma$ (0-100\%), perturbation scheduling (fixed-$\varepsilon$ vs
$\varepsilon$-mixes), and optimization (fine-tune vs scratch), and distils the
resulting $\gamma \times \varepsilon$ surface into two metrics:
Accuracy-Robustness Area and Robustness Volume. On 15-client simulations with
MNIST and Fashion-MNIST, IID and Non-IID conditions, training only 20-50\%
clients adversarially boosts $\varepsilon \leq 0.1$ accuracy $\sim$15 pp at $<
2$ pp clean-accuracy cost; fine-tuning adds 3-5 pp. With $\geq$75\% coverage, a
moderate $\varepsilon$-mix is optimal, while high-$\varepsilon$ schedules help
only at 100\% coverage. Label-sorted non-IID splits halve robustness,
underscoring data heterogeneity as a dominant risk.

</details>


### [128] [Artificial intelligence for representing and characterizing quantum systems](https://arxiv.org/abs/2509.04923)
*Yuxuan Du,Yan Zhu,Yuan-Hang Zhang,Min-Hsiu Hsieh,Patrick Rebentrost,Weibo Gao,Ya-Dong Wu,Jens Eisert,Giulio Chiribella,Dacheng Tao,Barry C. Sanders*

Main category: quant-ph

TL;DR: 这篇综述论文探讨了人工智能在量子系统表征中的应用，主要关注机器学习、深度学习和语言模型三种范式如何解决量子特性预测和量子态替代构建两大核心任务。


<details>
  <summary>Details</summary>
Motivation: 随着量子模拟器和大型量子计算机的发展，大规模量子系统的表征面临希尔伯特空间维度指数增长的挑战。人工智能凭借其高维模式识别和函数逼近能力，为解决这一挑战提供了有力工具。

Method: 论文将AI与量子系统表征的整合分为三个协同范式：机器学习、深度学习和语言模型。这些方法被应用于量子特性预测和量子态替代构建两大核心任务。

Result: 研究表明AI方法在量子系统表征中展现出强大潜力，能够有效处理高维量子系统的表征问题，为量子认证、基准测试、量子算法增强以及强关联物相理解等应用提供支持。

Conclusion: AI与量子科学的交叉领域具有广阔前景，但仍面临关键挑战和开放性问题，需要进一步研究来推动这一领域的发展。

Abstract: Efficient characterization of large-scale quantum systems, especially those
produced by quantum analog simulators and megaquop quantum computers, poses a
central challenge in quantum science due to the exponential scaling of the
Hilbert space with respect to system size. Recent advances in artificial
intelligence (AI), with its aptitude for high-dimensional pattern recognition
and function approximation, have emerged as a powerful tool to address this
challenge. A growing body of research has leveraged AI to represent and
characterize scalable quantum systems, spanning from theoretical foundations to
experimental realizations. Depending on how prior knowledge and learning
architectures are incorporated, the integration of AI into quantum system
characterization can be categorized into three synergistic paradigms: machine
learning, and, in particular, deep learning and language models. This review
discusses how each of these AI paradigms contributes to two core tasks in
quantum systems characterization: quantum property prediction and the
construction of surrogates for quantum states. These tasks underlie diverse
applications, from quantum certification and benchmarking to the enhancement of
quantum algorithms and the understanding of strongly correlated phases of
matter. Key challenges and open questions are also discussed, together with
future prospects at the interface of AI and quantum science.

</details>


### [129] [QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.05051)
*Aaron Mark Thomas,Yu-Cheng Chen,Hubert Okadome Valencia,Sharu Theresa Jose,Ronin Wu*

Main category: quant-ph

TL;DR: 提出了一种基于量子电路Born机器(QCBM)的生成对抗网络QCA-MolGAN，用于生成具有目标药物性质的分子，并通过多智能体强化学习优化关键药物指标。


<details>
  <summary>Details</summary>
Motivation: 药物发现中需要设计具有特定目标性质的新型药物分子，但化学空间巨大，传统方法面临挑战。生成模型为此提供了有前景的解决方案。

Method: 使用QCBM作为可学习的先验分布，与GAN判别器联合训练定义潜在空间；集成多智能体强化学习网络来指导分子生成，优化QED、LogP和SA等关键药物性质指标。

Result: 实验结果表明该方法增强了生成分子的性质对齐性，多智能体强化学习能够有效平衡化学性质。

Conclusion: QCA-MolGAN框架成功地将量子计算与生成模型结合，为药物分子设计提供了有效的解决方案，能够生成具有理想药物性质的分子结构。

Abstract: Navigating the vast chemical space of molecular structures to design novel
drug molecules with desired target properties remains a central challenge in
drug discovery. Recent advances in generative models offer promising solutions.
This work presents a novel quantum circuit Born machine (QCBM)-enabled
Generative Adversarial Network (GAN), called QCA-MolGAN, for generating
drug-like molecules. The QCBM serves as a learnable prior distribution, which
is associatively trained to define a latent space aligning with high-level
features captured by the GANs discriminator. Additionally, we integrate a novel
multi-agent reinforcement learning network to guide molecular generation with
desired targeted properties, optimising key metrics such as quantitative
estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and
synthetic accessibility (SA) scores in conjunction with one another.
Experimental results demonstrate that our approach enhances the property
alignment of generated molecules with the multi-agent reinforcement learning
agents effectively balancing chemical properties.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [130] [Vertex-ordering and arc-partitioning problems](https://arxiv.org/abs/2509.05245)
*Nóra A. Borsik,Péter Madarasi*

Main category: math.CO

TL;DR: 研究无环有向图中顶点排序问题，重点关注左向弧约束的存在性条件和计算复杂度，特别探讨了顶点特定的左出度和右入度上下界约束。


<details>
  <summary>Details</summary>
Motivation: 探索有向图中顶点排序问题与弧划分问题的联系，为图退化、无环定向、影响传播和秩聚合等更广泛主题提供算法边界和统一框架。

Method: 分析顶点排序问题在左向弧约束下的计算复杂度，研究顶点特定上下界约束的特殊情况，并探讨排序问题与弧划分问题的等价性。

Result: 发现左向弧形成入分支的问题可在多项式时间内解决，而入树形图问题则是NP完全的；确定了某些图族（入分支、不相交有向路径并集、匹配）的排序和划分问题等价，而其他图族（入树形图、有向路径、哈密顿有向路径、完美匹配）则不等价。

Conclusion: 研究提供了全面的复杂度图谱，统一了各种特例和变体，阐明了有序有向图的算法边界，并与多个相关领域建立了联系。

Abstract: We study vertex-ordering problems in loop-free digraphs subject to
constraints on the left-going arcs, focusing on existence conditions and
computational complexity. As an intriguing special case, we explore
vertex-specific lower and upper bounds on the left-outdegrees and
right-indegrees. We show, for example, that deciding whether the left-going
arcs can form an in-branching is solvable in polynomial time and provide a
necessary and sufficient condition, while the analogous problem for an
in-arborescence turns out to be NP-complete. We also consider a weighted
variant that enforces vertex-specific lower and upper bounds on the weighted
left-outdegrees, which is particularly relevant in applications. Furthermore,
we investigate the connection between ordering problems and their
arc-partitioning counterparts, where one seeks to partition the arcs into a
subgraph from a specific digraph family and an acyclic subgraph --
equivalently, one seeks to cover all directed cycles with a subgraph belonging
to a specific family. For the family of in-branchings, unions of disjoint
dipaths, and matchings, the two formulations coincide, whereas for
in-arborescences, dipaths, Hamiltonian dipaths, and perfect matchings the
formulations diverge. Our results yield a comprehensive complexity landscape,
unify diverse special cases and variants, clarify the algorithmic boundaries of
ordered digraphs, and relate them to broader topics including graph degeneracy,
acyclic orientations, influence propagation, and rank aggregation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [131] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: Maestro是一个框架无关的LLM智能体整体优化器，通过联合搜索图结构和节点配置来最大化智能体质量，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有优化器主要调整配置而固定图结构，无法解决结构性故障模式，需要一种能够同时优化图结构和节点配置的全面优化方法。

Method: 提出Maestro框架，联合搜索智能体的图结构（模块和信息流）和节点配置（模型、提示、工具、控制参数），利用反射性文本反馈来优先编辑，提高样本效率并针对特定故障模式。

Result: 在IFBench和HotpotQA基准测试中，Maestro平均优于MIPROv2、GEPA和GEPA+Merge分别12%、4.9%和4.86%；即使在仅提示优化限制下，仍领先9.65%、2.37%和2.41%。使用比GEPA少得多的rollouts实现这些结果。

Conclusion: 联合图结构和配置搜索能够解决仅提示调优无法修复的结构性故障模式，在多个应用中显示出显著优势，证明了整体优化方法的有效性。

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [132] [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
*Brennen Hill*

Main category: cs.AI

TL;DR: 论文主张通过构建显式的分层世界模型来解决复杂多智能体任务中的探索困难和稀疏奖励问题，提出利用大语言模型动态生成分层支架，为智能体学习提供内在课程和密集学习信号。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能发展中，语言模型和智能体模型已取得显著进展，但复杂、长时域多智能体任务中显式世界模型的开发仍是关键瓶颈。在机器人足球等领域，传统强化学习方法在高保真但结构平坦的模拟器中训练时，由于难以处理的探索空间和稀疏奖励而经常失败。

Method: 提出层次化支架方法，将复杂目标分解为结构化、可管理的子目标。利用大语言模型动态生成这种分层支架，通过语言即时构建世界模型。构建具有显式、语言可配置任务层的环境。

Result: 通过对2024年多智能体足球研究的系统回顾，发现了将符号化和层次化方法与多智能体强化学习(MARL)整合的明确趋势。这些方法隐式或显式地构建基于任务的世界模型来指导智能体学习。

Conclusion: 通过构建具有显式、语言可配置任务层的环境，可以弥合低层反应行为与高层战略团队玩法之间的差距，为训练下一代智能智能体创建一个强大且可推广的框架。

Abstract: The convergence of Language models, Agent models, and World models represents
a critical frontier for artificial intelligence. While recent progress has
focused on scaling Language and Agent models, the development of sophisticated,
explicit World Models remains a key bottleneck, particularly for complex,
long-horizon multi-agent tasks. In domains such as robotic soccer, agents
trained via standard reinforcement learning in high-fidelity but
structurally-flat simulators often fail due to intractable exploration spaces
and sparse rewards. This position paper argues that the next frontier in
developing capable agents lies in creating environments that possess an
explicit, hierarchical World Model. We contend that this is best achieved
through hierarchical scaffolding, where complex goals are decomposed into
structured, manageable subgoals. Drawing evidence from a systematic review of
2024 research in multi-agent soccer, we identify a clear and decisive trend
towards integrating symbolic and hierarchical methods with multi-agent
reinforcement learning (MARL). These approaches implicitly or explicitly
construct a task-based world model to guide agent learning. We then propose a
paradigm shift: leveraging Large Language Models to dynamically generate this
hierarchical scaffold, effectively using language to structure the World Model
on the fly. This language-driven world model provides an intrinsic curriculum,
dense and meaningful learning signals, and a framework for compositional
learning, enabling Agent Models to acquire sophisticated, strategic behaviors
with far greater sample efficiency. By building environments with explicit,
language-configurable task layers, we can bridge the gap between low-level
reactive behaviors and high-level strategic team play, creating a powerful and
generalizable framework for training the next generation of intelligent agents.

</details>


### [133] [Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales](https://arxiv.org/abs/2509.04871)
*Krittanon Kaewtawee,Wachiravit Modecrua,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: 本文提出了一种从通话录音中克隆对话语音AI助手的通用方法，通过整合语音识别、大语言模型对话管理和语音合成技术，能够实现自动化销售等任务。


<details>
  <summary>Details</summary>
Motivation: 语言和语音模型的进步使得建立自主语音助手成为可能，这些系统在客服和医疗领域有着广泛应用前景，能够自动化重复任务、降低成本并提供不间断支持。

Method: 从通话录音语料中克隆对话AI助手，整合自动语音识别(ASR)、基于大语言模型的对话管理器和文本转语音(TTS)合成技术，构建流式推理流水线，学习顶级人类助手的结构化播放册。

Result: 通过包含22项标准的评价体系，盲测显示AI助手在通话常规方面接近人类表现，但在说服和异议处理方面较差。分析了这些缺点并对提示进行了精炼。

Conclusion: 这项研究为克隆对话语音AI助手提供了通用方法，并持续改进以提升性能。未来研究方向包括大规模模拟和自动化评估。

Abstract: Recent advances in language and speech modelling have made it possible to
build autonomous voice assistants that understand and generate human dialogue
in real time. These systems are increasingly being deployed in domains such as
customer service and healthcare care, where they can automate repetitive tasks,
reduce operational costs, and provide constant support around the clock. In
this paper, we present a general methodology for cloning a conversational voice
AI agent from a corpus of call recordings. Although the case study described in
this paper uses telesales data to illustrate the approach, the underlying
process generalizes to any domain where call transcripts are available. Our
system listens to customers over the telephone, responds with a synthetic
voice, and follows a structured playbook learned from top performing human
agents. We describe the domain selection, knowledge extraction, and prompt
engineering used to construct the agent, integrating automatic speech
recognition, a large language model based dialogue manager, and text to speech
synthesis into a streaming inference pipeline. The cloned agent is evaluated
against human agents on a rubric of 22 criteria covering introduction, product
communication, sales drive, objection handling, and closing. Blind tests show
that the AI agent approaches human performance in routine aspects of the call
while underperforming in persuasion and objection handling. We analyze these
shortcomings and refine the prompt accordingly. The paper concludes with design
lessons and avenues for future research, including large scale simulation and
automated evaluation.

</details>


### [134] [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
*Barbara Gendron,Gaël Guibon,Mathieu D'aquin*

Main category: cs.AI

TL;DR: 提出基于本体的方法来形式化定义对话特征，通过语言描述符将定性概念量化，应用于LLM的语言水平控制，提高对话AI的透明度和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型作为对话代理时的可控性问题，确保可预测和用户个性化的响应，特别是对定性对话特征的形式化定义需求

Method: 使用语言描述符将定性概念转化为定量定义，构建描述逻辑本体，通过微调指导LLM的受控文本生成，以CEFR语言水平为案例研究

Result: 实验结果表明该方法提供了一致且可解释的语言水平定义，改善了对话AI的透明度

Conclusion: 基于本体的方法能够有效形式化对话特征，为LLM的可控对话生成提供了透明和可解释的框架

Abstract: The controllability of Large Language Models (LLMs) when used as
conversational agents is a key challenge, particularly to ensure predictable
and user-personalized responses. This work proposes an ontology-based approach
to formally define conversational features that are typically qualitative in
nature. By leveraging a set of linguistic descriptors, we derive quantitative
definitions for qualitatively-defined concepts, enabling their integration into
an ontology for reasoning and consistency checking. We apply this framework to
the task of proficiency-level control in conversations, using CEFR language
proficiency levels as a case study. These definitions are then formalized in
description logic and incorporated into an ontology, which guides controlled
text generation of an LLM through fine-tuning. Experimental results demonstrate
that our approach provides consistent and explainable proficiency-level
definitions, improving transparency in conversational AI.

</details>


### [135] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: LatticeWorld是一个基于轻量级LLM和游戏引擎的3D世界生成框架，通过多模态输入生成动态交互式3D环境，大幅提升工业生产效率


<details>
  <summary>Details</summary>
Motivation: 解决传统手动建模3D场景效率低下的问题，通过AI生成方法缩小仿真与现实的差距，为各种应用领域提供更真实的3D环境模拟

Method: 使用LLaMA-2-7B轻量级大语言模型结合Unreal Engine 5渲染引擎，接受文本描述和视觉指令作为多模态输入，生成包含动态代理的大规模3D交互世界

Result: 在场景布局生成和视觉保真度方面达到优异精度，相比传统手动生产方法实现90倍以上的工业生产效率提升，同时保持高创意质量

Conclusion: LatticeWorld提供了一个简单有效的3D世界生成框架，显著提高了3D环境生产的工业效率，为各种应用领域的仿真需求提供了可行的解决方案

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [136] [An Interactive Tool for Analyzing High-Dimensional Clusterings](https://arxiv.org/abs/2509.04603)
*Justin Lin,Julia Fukuyama*

Main category: stat.AP

TL;DR: 这篇论文提出了一个交互式工具DRtool，用于识别和诊断高维数据非线性降维方法产生的假结构问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据非线性降维方法在噪声环境中容易产生假结构，需要工具来识别和诊断这些问题。

Method: 开发了一个交互式工具DRtool，通过多种分析图表提供多角度视角来评估降维结果的合法性。

Result: 工具以R包形式可用，能够帮助分析师更好地理解和诊断降维结果。

Conclusion: DRtool提供了一个有效的方法来识别非线性降维方法中的假结构，提高了高维数据分析的可靠性。

Abstract: Technological advances have spurred an increase in data complexity and
dimensionality. We are now in an era in which data sets containing thousands of
features are commonplace. To digest and analyze such high-dimensional data,
dimension reduction techniques have been developed and advanced along with
computational power. Of these techniques, nonlinear methods are most commonly
employed because of their ability to construct visually interpretable
embeddings. Unlike linear methods, these methods non-uniformly stretch and
shrink space to create a visual impression of the high-dimensional data. Since
capturing high-dimensional structures in a significantly lower number of
dimensions requires drastic manipulation of space, nonlinear dimension
reduction methods are known to occasionally produce false structures,
especially in noisy settings. In an effort to deal with this phenomenon, we
developed an interactive tool that enables analysts to better understand and
diagnose their dimension reduction results. It uses various analytical plots to
provide a multi-faceted perspective on results to determine legitimacy. The
tool is available via an R package named DRtool.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [137] [Evaluating Idle Animation Believability: a User Perspective](https://arxiv.org/abs/2509.05023)
*Eneko Atxa Landa,Elena Lazkano,Igor Rodriguez,Itsaso Rodríguez-Moreno,Itziar Irigoien*

Main category: cs.HC

TL;DR: 论文研究了真实感和表演式空闲动画的感知差异，发现用户无法区分真实和表演的空闲动画，但能区分手工制作和录制的动画，这简化了空闲动画数据集的录制过程。


<details>
  <summary>Details</summary>
Motivation: 由于高质量空闲动画制作成本高且真实录制复杂（需要演员在不知情状态下表现自然动作），导致空闲动画数据集稀缺，需要研究更可行的录制方法。

Method: 通过比较真实录制和表演式录制的空闲动画，分析用户对这两种动画的感知差异和真实性判断。

Result: 用户无法区分真实和表演的空闲动画，但能感知手工制作和录制动画的差异；表演式录制可以简化制作过程。

Conclusion: 表演式录制空闲动画是可行的替代方案，可以显著简化录制过程，并发布了包含真实和表演动画的ReActIdle三维数据集。

Abstract: Animating realistic avatars requires using high quality animations for every
possible state the avatar can be in. This includes actions like walking or
running, but also subtle movements that convey emotions and personality. Idle
animations, such as standing, breathing or looking around, are crucial for
realism and believability. In games and virtual applications, these are often
handcrafted or recorded with actors, but this is costly. Furthermore, recording
realistic idle animations can be very complex, because the actor must not know
they are being recorded in order to make genuine movements. For this reasons
idle animation datasets are not widely available. Nevertheless, this paper
concludes that both acted and genuine idle animations are perceived as real,
and that users are not able to distinguish between them. It also states that
handmade and recorded idle animations are perceived differently. These two
conclusions mean that recording idle animations should be easier than it is
thought to be, meaning that actors can be specifically told to act the
movements, significantly simplifying the recording process. These conclusions
should help future efforts to record idle animation datasets. Finally, we also
publish ReActIdle, a 3 dimensional idle animation dataset containing both real
and acted idle motions.

</details>


### [138] [SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching](https://arxiv.org/abs/2509.04752)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.HC

TL;DR: SePA是一个基于LLM的健康教练系统，结合个性化机器学习和检索增强生成技术，通过可穿戴设备数据预测用户健康状态并提供基于专家验证内容的个性化建议。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够提供个性化、基于证据的健康指导系统，结合实时传感器数据和可靠的专家知识，解决传统健康建议缺乏个性化和可信度的问题。

Method: 1) 基于28名用户1260个数据点的个性化机器学习模型预测日常压力、酸痛和受伤风险；2) 检索增强生成模块，从专家验证的网络内容中获取信息来增强LLM生成的反馈；3) 使用滚动原点交叉验证和组k折交叉验证评估模型性能。

Result: 个性化模型优于通用基线模型；在专家研究中(n=4)，基于检索的建议优于非检索基线(Cliff's δ=0.3, p=0.05)；量化了响应质量与速度之间的延迟性能权衡。

Conclusion: SePA系统为下一代可信赖的个人健康信息系统提供了透明蓝图，展示了结合个性化预测和检索增强生成技术在健康指导领域的有效性。

Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM
health coaching system that integrates personalized machine learning and
retrieval-augmented generation to deliver adaptive, evidence-based guidance.
SePA combines: (1) Individualized models predicting daily stress, soreness, and
injury risk from wearable sensor data (28 users, 1260 data points); and (2) A
retrieval module that grounds LLM-generated feedback in expert-vetted web
content to ensure contextual relevance and reliability. Our predictive models,
evaluated with rolling-origin cross-validation and group k-fold
cross-validation show that personalized models outperform generalized
baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was
preferred over a non-retrieval baseline, yielding meaningful practical effect
(Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs
between response quality and speed, offering a transparent blueprint for
next-generation, trustworthy personal health informatics systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems](https://arxiv.org/abs/2509.04694)
*Wei Xu,Jiasen Zheng,Junjiang Lin,Mingxuan Han,Junliang Du*

Main category: cs.IR

TL;DR: 提出统一表示学习框架，通过多意图建模和不确定性建模联合处理用户意图多样性和行为不确定性，提升推荐系统的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐方法难以同时处理用户意图多样性和行为不确定性，存在建模瓶颈，需要统一框架来解决这些复杂用户行为特征。

Method: 构建多意图表示模块和不确定性建模机制，使用注意力机制融合多潜在意图向量，通过高斯分布学习行为表示的均值和协方差，采用可学习融合策略结合长短期信号。

Result: 在标准公开数据集上优于现有代表性模型，在冷启动和行为扰动场景下表现出更好的稳定性和适应性。

Conclusion: 统一建模策略在处理复杂用户行为方面有效，具有实际应用价值，能够缓解传统方法的建模瓶颈。

Abstract: This paper addresses the challenge of jointly modeling user intent diversity
and behavioral uncertainty in recommender systems. A unified representation
learning framework is proposed. The framework builds a multi-intent
representation module and an uncertainty modeling mechanism. It extracts
multi-granularity interest structures from user behavior sequences. Behavioral
ambiguity and preference fluctuation are captured using Bayesian distribution
modeling. In the multi-intent modeling part, the model introduces multiple
latent intent vectors. These vectors are weighted and fused using an attention
mechanism to generate semantically rich representations of long-term user
preferences. In the uncertainty modeling part, the model learns the mean and
covariance of behavior representations through Gaussian distributions. This
reflects the user's confidence in different behavioral contexts. Next, a
learnable fusion strategy is used to combine long-term intent and short-term
behavior signals. This produces the final user representation, improving both
recommendation accuracy and robustness. The method is evaluated on standard
public datasets. Experimental results show that it outperforms existing
representative models across multiple metrics. It also demonstrates greater
stability and adaptability under cold-start and behavioral disturbance
scenarios. The approach alleviates modeling bottlenecks faced by traditional
methods when dealing with complex user behavior. These findings confirm the
effectiveness and practical value of the unified modeling strategy in
real-world recommendation tasks.

</details>


### [140] [Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms](https://arxiv.org/abs/2509.04751)
*Yushang Zhao,Yike Peng,Li Zhang,Qianyi Sun,Zhihui Zhang,Yingying Zhuang*

Main category: cs.IR

TL;DR: 提出基于多模态基础模型的用户兴趣建模框架，整合视频、文本和音频信息，通过行为驱动特征嵌入机制提升推荐准确性和时效性


<details>
  <summary>Details</summary>
Motivation: 传统单模态兴趣建模方法无法充分捕捉复杂多模态内容环境中的用户偏好，需要更全面的多模态建模方案

Method: 使用跨模态对齐策略将视频帧、文本描述和背景音乐整合到统一语义空间，构建细粒度用户兴趣向量；引入行为驱动的特征嵌入机制建模动态兴趣演化

Result: 在公开和私有短视频数据集上显著提升了行为预测准确性、冷启动用户兴趣建模效果和推荐点击率

Conclusion: 该框架通过多模态融合和行为序列建模有效提升了推荐系统性能，并通过可解释性机制增强了系统透明度和可控性

Abstract: With the rapid expansion of user bases on short video platforms, personalized
recommendation systems are playing an increasingly critical role in enhancing
user experience and optimizing content distribution. Traditional interest
modeling methods often rely on unimodal data, such as click logs or text
labels, which limits their ability to fully capture user preferences in a
complex multimodal content environment. To address this challenge, this paper
proposes a multimodal foundation model-based framework for user interest
modeling and behavior analysis. By integrating video frames, textual
descriptions, and background music into a unified semantic space using
cross-modal alignment strategies, the framework constructs fine-grained user
interest vectors. Additionally, we introduce a behavior-driven feature
embedding mechanism that incorporates viewing, liking, and commenting sequences
to model dynamic interest evolution, thereby improving both the timeliness and
accuracy of recommendations. In the experimental phase, we conduct extensive
evaluations using both public and proprietary short video datasets, comparing
our approach against multiple mainstream recommendation algorithms and modeling
techniques. Results demonstrate significant improvements in behavior prediction
accuracy, interest modeling for cold-start users, and recommendation
click-through rates. Moreover, we incorporate interpretability mechanisms using
attention weights and feature visualization to reveal the model's decision
basis under multimodal inputs and trace interest shifts, thereby enhancing the
transparency and controllability of the recommendation system.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [141] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: VCMamba是一个结合CNN和多向Mamba SSM优势的新型视觉骨干网络，在ImageNet-1K分类和ADE20K语义分割任务中表现出色，参数效率高。


<details>
  <summary>Details</summary>
Motivation: ViT和SSM在捕获全局上下文方面表现出色，但缺乏CNN对细粒度局部特征的有效捕获能力；而CNN虽然擅长局部特征但缺乏全局推理能力。需要结合两者的优势。

Method: 采用卷积主干和分层结构，早期阶段使用卷积块提取局部特征，后期阶段使用多向Mamba块建模长距离依赖和全局上下文，保持线性复杂度。

Result: VCMamba-B在ImageNet-1K达到82.6% top-1准确率，比PlainMamba-L3高0.3%且参数少37%；在ADE20K获得47.1 mIoU，比EfficientFormer-L7高2.0 mIoU且参数少62%。

Conclusion: VCMamba成功整合了CNN和Mamba SSM的优势，在保持线性复杂度的同时实现了优异的特征表示，在多个视觉任务中展现出卓越性能和参数效率。

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [142] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 该论文提出使用混合专家模型(MoE)从计算机视觉模型中提取准确且校准良好的预测不确定性估计，无需架构修改，在OOD数据下比集成方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用（如交通场景感知）中，需要准确且校准良好的预测不确定性估计来提高计算机视觉模型的可靠性。虽然集成方法常用于量化不确定性，但混合专家模型提供了一种更高效的替代方案。

Method: 研究三种从MoE中提取预测不确定性估计的方法：预测熵、互信息和专家方差。使用两个专家的MoE在A2D2数据集上进行训练和评估，同时评估通过门熵计算的路由不确定性。

Result: MoE在OOD数据下的条件正确性指标方面比集成方法产生更可靠的不确定性估计。简单门控机制比更复杂的类门控产生更好的路由不确定性校准。增加专家数量可以进一步提高不确定性校准。

Conclusion: 混合专家模型能够有效提取准确且校准良好的预测不确定性估计，在不确定性量化方面优于传统集成方法，特别是在安全关键应用中具有重要价值。

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [143] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 使用预训练计算机视觉模型通过迁移学习预测蜘蛛图像恐惧等级，平均绝对误差10.1-11.0，模型可解释性良好但需要足够数据量


<details>
  <summary>Details</summary>
Motivation: 为开发自适应计算机化暴露疗法系统，需要能够根据视觉刺激动态调整的技术，因此研究计算机视觉模型能否准确预测蜘蛛图像的恐惧等级

Method: 采用三种预训练计算机视觉模型进行迁移学习，使用313张标准化蜘蛛图像数据集预测0-100恐惧评分，通过交叉验证评估性能，并进行学习曲线和可解释性分析

Result: 模型平均绝对误差在10.1-11.0之间，学习曲线显示减少数据集会显著降低性能但增加数据无显著提升，可解释性分析确认模型基于蜘蛛相关特征进行预测，远距离视图和人工蜘蛛等条件误差较高

Conclusion: 可解释的计算机视觉模型在预测恐惧评分方面具有潜力，强调了模型可解释性和足够数据集大小对于开发有效情绪感知治疗技术的重要性

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [144] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: 提出使用视觉语言模型和3D仿真引擎生成工业磨损检测的合成数据，解决了真实数据获取困难的问题，在锈蚀检测任务上达到0.87 mAP50的优异性能


<details>
  <summary>Details</summary>
Motivation: 工业磨损检测对预测性维护很重要，但不同磨损场景的数据集获取成本高且耗时，需要一种有效的数据生成方法

Method: 结合视觉语言模型和3D仿真渲染引擎，生成不同锈蚀条件的合成数据来训练计算机视觉模型

Result: 使用合成数据训练的锈蚀检测模型在真实图像测试中表现优异，mAP50达到0.87，优于其他方法

Conclusion: 该方法可定制且易于扩展到其他工业磨损检测场景，为缺乏真实数据的工业视觉任务提供了有效的解决方案

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [145] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: 这篇论文提出了一种简单的注意力基础多实例学习框架，用于自动化面部皮腺细胞的液滴计数分析。结果显示基线MLP模型比注意力MIL模型更稳定和准确。


<details>
  <summary>Details</summary>
Motivation: 手动计数面部皮腺细胞内液滴耗时谨主观，需要自动化解决方案来提高效率和准确性。

Method: 使用Nile Red染色的面部皮腺细胞图像，分为14个类别根据液滴数量。对比了两种模型：基线MLP模型和注意力基础MIL模型（使用ResNet-50特征），采用五折交叉验证。

Result: 基线MLP模型表现更稳定（平均MAE = 5.6），而注意力MIL模型不如其稳定（平均MAE = 10.7），但在某些特定折叠中更优。

Conclusion: 简单的集合级聚合方法提供了健壃的基线，而注意力MIL需要任务对齐池化和正则化才能充分发挥潜力。

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [146] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 使用稀疏混合专家(MoE)层替代CNN中的残差块或卷积层，在不增加推理成本的情况下提升模型对抗攻击的鲁棒性。在CIFAR-100上训练的ResNet中，深层插入单个MoE层结合对抗训练可显著提升对PGD和AutoPGD攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统CNN对抗对抗攻击的鲁棒性提升方法往往需要大量资源，因此探索在不增加推理成本的情况下通过增加模型容量来提高鲁棒性的方法。

Method: 在ResNet架构的深层阶段插入稀疏混合专家(MoE)层，替代原有的残差块或卷积层，并结合对抗训练。使用switch loss进行专家平衡。

Result: 单个MoE层插入可显著提升模型对PGD和AutoPGD攻击的鲁棒性。switch loss导致路由集中在少数过度使用的专家上，使这些路径在对抗训练中变得更加鲁棒，部分专家个体的鲁棒性甚至超过整个门控MoE模型。

Conclusion: 稀疏MoE层是提升CNN对抗攻击鲁棒性的有效方法，鲁棒子路径通过专家专业化自然涌现，为对抗鲁棒性研究提供了新视角。

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [147] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: 基于Transformer的框架从单张图片和网格直接生成高保真3D纹理，无需UV映射，每个形状仅0.2秒


<details>
  <summary>Details</summary>
Motivation: 解决现有生成方法速度慢、依赖UV映射且无法保持与参考图像一致性的问题

Method: 使用Transformer框架，统一三平面表示和深度基于的反向投影损失，直接从单张图片和网格预测3D纹理场

Result: 在单张图片纹理重建任务上，在保真度和感知质量方面都超过了最新的基准方法，每个形状仅需0.2秒

Conclusion: 该方法为可扩展、高质量和可控的3D内容创建提供了实用的解决方案

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [148] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: 本文提出了改进的ModelNet-R数据集和轻量级Point-SkipNet网络，解决了ModelNet40的数据质量问题，实现了更高的分类精度和更低的计算开销。


<details>
  <summary>Details</summary>
Motivation: ModelNet40数据集存在标签不一致、2D数据、尺寸不匹配和类别区分不足等问题，影响了3D点云分类模型的性能，需要更可靠的数据集和高效模型。

Method: 提出了ModelNet-R精炼数据集，并设计了Point-SkipNet轻量图神经网络，采用高效采样、邻域分组和跳跃连接技术。

Result: 在ModelNet-R上训练的模型性能显著提升，Point-SkipNet以更少的参数量达到了最先进的分类精度。

Conclusion: 数据集质量对3D点云分类模型的效率优化至关重要，ModelNet-R和Point-SkipNet为相关应用提供了更可靠的基准和高效解决方案。

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [149] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: 该论文研究了大型语言模型生成符号图形程序（SVG）的能力，提出了SGP-GenBench基准测试，并开发了基于强化学习的验证奖励方法来提升SVG生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在生成符号图形程序方面的能力，特别是从自然语言描述生成可渲染精确视觉内容的SVG程序，以此作为理解LLMs视觉世界认知的视角。

Method: 引入SGP-GenBench基准测试评估LLMs性能；提出强化学习方法，使用格式有效性验证和跨模态奖励（SigLIP用于文本-图像对齐，DINO用于图像-图像对齐）来优化SVG生成。

Result: 前沿专有模型显著优于开源模型，性能与通用编码能力相关；基于Qwen-2.5-7B的强化学习方法大幅提升了SVG生成质量和语义理解，达到与前沿系统相当的性能。

Conclusion: 符号图形编程为跨模态基础提供了精确且可解释的研究视角，强化学习方法能够诱导更精细的对象分解和场景连贯性细节。

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [150] [Multiscale Graph Neural Network for Turbulent Flow-Thermal Prediction Around a Complex-Shaped Pin-Fin](https://arxiv.org/abs/2509.04463)
*Riddhiman Raut,Evan M. Mihalko,Amrita Basak*

Main category: physics.flu-dyn

TL;DR: 基于图神经网络的空间预测模型，用于预测复杂针翅结构中的稳态激流流场和热行为，计算效率提升2-3个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统的CFD模拟在复杂结构中耗时过长，需要快速而准确的流场预测方法来支持设计优化和分析。

Method: 通过抽核参数化生成1,000个多样化的针翅结构，将CFD模拟转换为图结构，开发基于图神经网络的多尺度模型来预测温度、速度和压力场。

Result: 模型预测精度极高，能够准确捕捉边界层、回流区和滞步区域，同时将计算时间缩短2-3个数量级。

Conclusion: 该图神经网络提供了一种高效、准确的代理模拟方法，适用于复杂流动配置的快速分析和设计。

Abstract: This study presents the development of a domain-responsive edge-aware
multiscale Graph Neural Network for predicting steady, turbulent flow and
thermal behavior in a two-dimensional channel containing arbitrarily shaped
complex pin-fin geometries. The training dataset was constructed through an
automated framework that integrated geometry generation, meshing, and
flow-field solutions in ANSYS Fluent. The pin-fin geometry was parameterized
using piecewise cubic splines, producing 1,000 diverse configurations through
Latin Hypercube Sampling. Each simulation was converted into a graph structure,
where nodes carried a feature vector containing spatial coordinates, a
normalized streamwise position, one-hot boundary indicators, and a signed
distance to the nearest boundary such as wall. This graph structure served as
input to the newly developed Graph Neural Network, which was trained to predict
temperature, velocity magnitude, and pressure at each node using data from
ANSYS. The network predicted fields with outstanding accuracy, capturing
boundary layers, recirculation, and the stagnation region upstream of the
pin-fins while reducing wall time by 2-3 orders of magnitude. In conclusion,
the novel graph neural network offered a fast and reliable surrogate for
simulations in complex flow configurations.

</details>
