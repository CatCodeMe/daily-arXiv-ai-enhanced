<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 51]
- [math.ST](#math.ST) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.RO](#cs.RO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 9]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.CC](#cs.CC) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Space-Time Tradeoffs for Spatial Conjunctive Queries](https://arxiv.org/abs/2509.10050)
*Aryan Esmailpour,Xiao Hu,Stavros Sintos*

Main category: cs.DB

TL;DR: 本文针对空间连接查询的索引问题，建立了查询时间与空间使用之间的下界，并构建了最优索引结构，支持范围空值查询、范围计数查询和最近邻查询。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需要O(N)查询时间，要么使用与查询结果数量相同的空间，这在实践中效率低下或不现实。因此需要构建时间和空间都高效的索引来回答空间连接查询。

Method: 建立k-star和k-path查询的时间-空间权衡下界，构建最优索引处理范围空值查询和范围计数查询，扩展到分层查询，利用广义超树分解扩展到任意连接查询。

Result: 证明了k-star查询需要Ω(N+N^k/T^k)空间，k-path查询需要Ω(N+N^2/T^{2/(k-1)})空间，并构建了相应的最优索引。

Conclusion: 提出的索引结构能够高效支持空间连接查询，并可以改进关系算法中已知算法的运行时间。

Abstract: Given a conjunctive query and a database instance, we aim to develop an index
that can efficiently answer spatial queries on the results of a conjunctive
query. We are interested in some commonly used spatial queries, such as range
emptiness, range count, and nearest neighbor queries. These queries have
essential applications in data analytics, such as filtering relational data
based on attribute ranges and temporal graph analysis for counting graph
structures like stars, paths, and cliques. Furthermore, this line of research
can accelerate relational algorithms that incorporate spatial queries in their
workflow, such as relational clustering. Known approaches either have to spend
$\tilde{O}(N)$ query time or use space as large as the number of query results,
which are inefficient or unrealistic to employ in practice. Hence, we aim to
construct an index that answers spatial conjunctive queries in both time- and
space-efficient ways.
  In this paper, we establish lower bounds on the tradeoff between answering
time and space usage. For $k$-star (resp. $k$-path) queries, we show that any
index for range emptiness, range counting or nearest neighbor queries with $T$
answering time requires $\Omega\left(N+\frac{N^k}{T^k}\right)$ (resp.
$\Omega\left(N+\frac{N^2}{T^{2/(k-1)}}\right)$) space. Then, we construct
optimal indexes for answering range emptiness and range counting problems over
$k$-star and $k$-path queries. Extending this result, we build an index for
hierarchical queries. By resorting to the generalized hypertree decomposition,
we can extend our index to arbitrary conjunctive queries for supporting spatial
conjunctive queries. Finally, we show how our new indexes can be used to
improve the running time of known algorithms in the relational setting.

</details>


### [2] [Semi-interval Comparison Constraints in Query Containment and Their Impact on Certain Answer Computation](https://arxiv.org/abs/2509.10138)
*Foto N. Afrati,Matthew Damigos*

Main category: cs.DB

TL;DR: 本文研究了包含算术比较的连接查询(CQAC)的包含性测试和确定答案计算的复杂度问题


<details>
  <summary>Details</summary>
Motivation: 研究CQAC查询的包含性测试问题的计算复杂度，特别是针对包含半区间算术比较的查询类，以及在这些查询框架下计算确定答案的复杂度

Method: 通过理论分析和复杂度证明，研究了不同CQAC查询类别的包含性测试问题，并探讨了使用CQAC视图计算确定答案的方法

Result: 发现对于包含半区间算术比较的查询，包含性测试问题可在NP内解决；同时证明在某些简单情况下问题仍保持Π₂^p完全性；证明了在CQAC并集语言中最大包含重写能精确计算所有确定答案

Conclusion: CQAC查询的包含性测试具有不同的复杂度特征，取决于查询中算术比较的类型；最大包含重写是计算确定答案的有效方法，在某些情况下可在多项式时间内完成

Abstract: We consider conjunctive queries with arithmetic comparisons (CQAC) and
investigate the computational complexity of the problem: Given two CQAC
queries, $Q$ and $Q'$, is $Q'$ contained in $Q$? We know that, for CQAC
queries, the problem of testing containment is $\Pi_2 ^p$ -complete. However,
there are broad classes of queries with semi-interval arithmetic comparisons in
the containing query that render the problem solvable in NP. In all cases
examined the contained query is allowed to be any CQAC. Interestingly, we also
prove that there are simple cases where the problem remains $\Pi_2 ^p$
-complete.
  We also investigate the complexity of computing certain answers in the
framework of answering CQAC queries with semi-interval comparisons using any
CQAC views. We prove that maximally contained rewritings in the language of
union of CQACs always compute exactly all certain answers. We find cases where
we can compute certain answers in polynomial time using maximally contained
rewritings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Setchain Algorithms for Blockchain Scalability](https://arxiv.org/abs/2509.09795)
*Arivarasan Karmegam,Gabina Luz Bianchi,Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,César Sánchez*

Main category: cs.DC

TL;DR: Setchain通过放宽交易严格全序要求提升区块链可扩展性，提出三种基于底层区块链的算法：Vanilla基础实现、Compresschain批量压缩、Hashchain哈希存储，实现比底层区块链高数个数量级的吞吐量，最终性延迟低于4秒。


<details>
  <summary>Details</summary>
Motivation: 传统区块链的严格全序交易限制了可扩展性，Setchain通过将交易组织成无序集合（epochs）来放松排序要求，从而提高吞吐量和性能。

Method: 提出三种Setchain算法：1) Vanilla基础实现作为参考；2) Compresschain将元素批量压缩后作为epochs添加到账本；3) Hashchain将批次转换为固定长度哈希，需要分布式服务获取批次内容。所有算法都维护epoch-proof（服务器签名的epoch哈希）以确保安全性。

Result: 在4、7、10个服务器的集群配置下进行性能评估，Setchain算法达到比底层区块链高数个数量级的吞吐量，最终性延迟低于4秒。客户端可通过f+1个epoch-proof（f为拜占庭服务器最大数量）安全验证epoch正确性。

Conclusion: Setchain通过放松交易排序要求有效提升了区块链可扩展性，三种算法均能实现高性能和高吞吐量，为区块链扩容提供了可行解决方案。

Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the
strict total order requirement among transactions. Setchain organizes elements
into a sequence of sets, referred to as epochs, so that elements within each
epoch are unordered. In this paper, we propose and evaluate three distinct
Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is
a basic implementation that serves as a reference point. Compresschain
aggregates elements into batches, and compresses these batches before appending
them as epochs in the ledger. Hashchain converts batches into fixed-length
hashes which are appended as epochs in the ledger. This requires Hashchain to
use a distributed service to obtain the batch contents from its hash. To allow
light clients to safely interact with only one server, the proposed algorithms
maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the
hash of the epoch, cryptographically signed by a server. A client can verify
the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum
number of Byzantine servers assumed). All three Setchain algorithms are
implemented on top of the CometBFT blockchain application platform. We
conducted performance evaluations across various configurations, using clusters
of four, seven, and ten servers. Our results show that the Setchain algorithms
reach orders of magnitude higher throughput than the underlying blockchain, and
achieve finality with latency below 4 seconds.

</details>


### [4] [Ordered Consensus with Equal Opportunity](https://arxiv.org/abs/2509.09868)
*Yunhao Zhang,Haobin Ni,Soumya Basu,Shir Cohen,Maofan Yin,Lorenzo Alvisi,Robbert van Renesse,Qi Chen,Lidong Zhou*

Main category: cs.DC

TL;DR: 本文扩展了状态机复制(SMR)的有序共识，引入了平等机会的公平性概念，通过秘密随机预言机(SRO)生成随机性来减少排序偏见，提出了基于可信硬件和阈值可验证随机函数的Bercow协议来缓解区块链中的排序攻击。


<details>
  <summary>Details</summary>
Motivation: 在基于SMR的区块链中，不同的命令排序会给客户端带来不同的财务奖励，现有有序共识主要关注限制拜占庭节点的影响，但现实中的排序操纵即使没有拜占庭节点也会发生，因为网络速度、基础设施距离等因素给某些客户端带来了不公平优势。

Method: 提出平等机会的公平性概念，要求同等资格的候选人在排序中应有平等机会；引入秘密随机预言机(SRO)来以容错方式生成随机性；设计了基于可信硬件和阈值可验证随机函数的两种SRO方案；实例化为Bercow有序共识协议，通过可配置因子近似实现平等机会。

Result: 开发了Bercow协议，能够有效缓解SMR基区块链中已知的排序攻击，通过随机性控制偏见，在可配置范围内近似实现平等机会的公平性要求。

Conclusion: 通过将平等机会的公平性概念引入有序共识，并利用随机性技术来减少排序偏见，Bercow协议为解决区块链中的排序操纵问题提供了有效的解决方案，增强了系统的公平性和安全性。

Abstract: The specification of state machine replication (SMR) has no requirement on
the final total order of commands. In blockchains based on SMR, however, order
matters, since different orders could provide their clients with different
financial rewards. Ordered consensus augments the specification of SMR to
include specific guarantees on such order, with a focus on limiting the
influence of Byzantine nodes. Real-world ordering manipulations, however, can
and do happen even without Byzantine replicas, typically because of factors,
such as faster networks or closer proximity to the blockchain infrastructure,
that give some clients an unfair advantage. To address this challenge, this
paper proceeds to extend ordered consensus by requiring it to also support
equal opportunity, a concrete notion of fairness, widely adopted in social
sciences. Informally, equal opportunity requires that two candidates who,
according to a set of criteria deemed to be relevant, are equally qualified for
a position (in our case, a specific slot in the SMR total order), should have
an equal chance of landing it. We show how randomness can be leveraged to keep
bias in check, and, to this end, introduce the secret random oracle (SRO), a
system component that generates randomness in a fault-tolerant manner. We
describe two SRO designs based, respectively, on trusted hardware and threshold
verifiable random functions, and instantiate them in Bercow, a new ordered
consensus protocol that, by approximating equal opportunity up to within a
configurable factor, can effectively mitigate well-known ordering attacks in
SMR-based blockchains.

</details>


### [5] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: 本文对大规模多GPU系统中LLM训练进行了全面性能分析，揭示了硬件配置、并行策略和优化技术之间的复杂交互关系，为未来系统设计提供指导


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模快速增长，训练工作负载已远超单节点分析能力，需要深入理解这些模型在大规模多GPU系统中的行为表现

Method: 在NVIDIA H100/H200和AMD MI250 GPU平台上，分析密集和稀疏模型在不同并行策略（张量、流水线、数据和专家并行）下的硬件利用率、功耗和热行为，并评估激活重计算和计算-通信重叠等优化效果

Result: 研究发现性能不仅由硬件容量决定：在通信受限场景中，较少但高内存GPU的纵向扩展系统可能优于横向扩展系统，但需要精细调优；某些并行组合（如张量与流水线并行）会导致带宽利用不足，而过大的微批次大小会引起突发执行和峰值功耗问题

Conclusion: 训练性能由硬件、系统拓扑和模型执行之间的复杂交互决定，研究为改进未来LLM系统可扩展性和可靠性的系统和硬件设计提供了具体建议

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [6] [Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates](https://arxiv.org/abs/2509.10036)
*Zhuowei Zhao,Zhuo Zhang,Hanzhi Wang,Junhao Gan,Zhifeng Bao,Jianzhong Qi*

Main category: cs.DS

TL;DR: 本文提出了AGP-Static++和AGP-Dynamic两种新算法，改进了近似图传播框架在动态图和动态参数化查询场景下的性能，显著提升了更新时间和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有的AGP-Static算法在动态图和动态参数化查询场景下存在性能瓶颈：查询复杂度依赖不存在的理想子集采样算法，且动态图更新处理时间高达O(n log n)。

Method: 提出AGP-Static++算法简化查询过程，降低O(log² n)的查询复杂度因子；进一步提出AGP-Dynamic算法实现O(1)摊销时间的动态图更新处理。

Result: 实验验证显示，新算法相比基线方法在更新时间上达到177倍加速，在查询效率上达到10倍提升，同时保持了近似保证。

Conclusion: 所提出的AGP-Static++和AGP-Dynamic算法有效解决了动态图和动态参数化查询场景下的性能问题，为图传播任务提供了高效的解决方案。

Abstract: We revisit Approximate Graph Propagation (AGP), a unified framework which
captures various graph propagation tasks, such as PageRank, feature propagation
in Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation
(RAG). Our work focuses on the settings of dynamic graphs and dynamic
parameterized queries, where the underlying graphs evolve over time (updated by
edge insertions or deletions) and the input query parameters are specified on
the fly to fit application needs. Our first contribution is an interesting
observation that the SOTA solution, AGP-Static, can be adapted to support
dynamic parameterized queries; however several challenges remain unresolved.
Firstly, the query time complexity of AGP-Static is based on an assumption of
using an optimal algorithm for subset sampling in its query algorithm.
Unfortunately, back to that time, such an algorithm did not exist; without such
an optimal algorithm, an extra $O(\log^2 n)$ factor is required in the query
complexity, where $n$ is the number of vertices in the graphs. Secondly,
AGP-Static performs poorly on dynamic graphs, taking $O(n\log n)$ time to
process each update. To address these challenges, we propose a new algorithm,
AGP-Static++, which is simpler yet reduces roughly a factor of $O(\log^2 n)$ in
the query complexity while preserving the approximation guarantees of
AGP-Static. However, AGP-Static++ still requires $O(n)$ time to process each
update. To better support dynamic graphs, we further propose AGP-Dynamic, which
achieves $O(1)$ amortized time per update, significantly improving the
aforementioned $O(n)$ per-update bound, while still preserving the query
complexity and approximation guarantees. Last, our comprehensive experiments
validate the theoretical improvements: compared to the baselines, our algorithm
achieves speedups of up to $177\times$ on update time and $10\times$ on query
efficiency.

</details>


### [7] [Constant Time with Minimal Preprocessing, a Robust and Extensive Complexity Class](https://arxiv.org/abs/2509.10188)
*Étienne Grandjean,Louis Jachiet*

Main category: cs.DS

TL;DR: 本文研究cstPP类运算，这类运算在O(N)预处理时间后能在常数时间内计算任意操作数，证明了该类的鲁棒性、闭包性和对预处理时间变化的稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究在RAM模型中具有常数时间计算能力的运算类，探索其数学性质和计算特性，为高效算法设计提供理论基础。

Method: 通过形式化定义cstPP类运算，分析其在不同原始操作集下的不变性，证明闭包性质（复合和逆运算），并研究预处理时间变化对类定义的影响。

Result: 证明cstPP类具有鲁棒性，在不同原始操作集下保持不变；满足复合和逆运算闭包；预处理时间在O(N^c)或N^ε范围内不影响类定义；但预处理时间为N^{o(1)}时类会退化。

Conclusion: cstPP类是一个强大且稳定的计算类，对预处理时间具有一定容忍度，为理解常数时间可计算性提供了重要理论框架。

Abstract: In this paper, we study the class $\mathtt{cstPP}$ of operations
$\mathtt{op}: \mathbb{N}^k\to\mathbb{N}$, of any fixed arity $k\ge 1$,
satisfying the following property: for each fixed integer $d\ge 1$, there
exists an algorithm for a RAM machine which, for any input integer $N\ge 2$, -
pre-computes some tables in $O(N)$ time, - then reads $k$ operands
$x_1,\ldots,x_k<N^d$ and computes $\mathtt{op}(x_1,\dots,x_k)$ in constant
time.
  We show that the $\mathtt{cstPP}$ class is robust and extensive and satisfies
several closure properties. It is invariant depending on whether the set of
primitive operations of the RAM is $\{+\}$, or
$\{+,-,\times,\mathtt{div},\mathtt{mod}\}$, or any set of operations in
$\mathtt{cstPP}$ provided it includes $+$. We prove that the $\mathtt{cstPP}$
class is closed under composition and, for fast-growing functions, is closed
under inverse. We also show that in the definition of $\mathtt{cstPP}$ the
constant-time procedure can be reduced to a single return instruction. Finally,
we establish that linear preprocessing time is not essential in the definition
of the $\mathtt{cstPP}$ class: this class is not modified if the preprocessing
time is increased to $O(N^c)$, for any fixed $c>1$, or conversely, is reduced
to $N^{\varepsilon}$, for any positive $\varepsilon<1$ (provided the set of
primitive operation includes $+$, $\mathtt{div}$ and $\mathtt{mod}$). To
complete the picture, we demonstrate that the $\mathtt{cstPP}$ class
degenerates if the preprocessing time reduces to $N^{o(1)}$.

</details>


### [8] [A linear-time algorithm for Chow decompositions](https://arxiv.org/abs/2509.10450)
*Alexander Taveira Blomenhofer,Benjamin Lovitz*

Main category: cs.DS

TL;DR: 提出线性时间算法计算低秩Chow分解，可分解对称3-张量，并开发次二次时间算法处理高阶Chow分解和特殊轨道分解


<details>
  <summary>Details</summary>
Motivation: 需要高效算法来分解对称张量，特别是处理Chow分解问题，以解决计算复杂度和时间效率的挑战

Method: 基于铅笔(pencil-based)的算法，依赖广义特征值计算，开发线性时间和次二次时间算法处理不同阶数的Chow分解

Result: 成功实现了线性时间算法分解秩为n/3的对称3-张量，并开发了处理高阶分解和特殊轨道分解的次二次时间算法

Conclusion: 该研究提供了高效的Chow分解算法，显著提升了对称张量分解的计算效率，为相关领域提供了实用的计算工具

Abstract: We propose a linear-time algorithm to compute low-rank Chow decompositions.
Our algorithm can decompose concise symmetric 3-tensors in n variables of Chow
rank n/3. The algorithm is pencil based, hence it relies on generalized
eigenvalue computations. We also develop sub-quadratic time algorithms for
higher order Chow decompositions, and Chow decompositions of 3-tensors into
products of linear forms which do not lie on the generic orbit. In particular,
we obtain a sub-quadratic-time algorithm for decomposing a symmetric 3-tensor
into a linear combination of W-tensors.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SWE-Effi提出了新的多维度指标来评估AI系统在软件工程任务中的整体效能，不仅考虑准确性还考虑资源消耗（token和时间），发现在SWE-bench基准测试中AI系统的效能取决于脚手架与基础模型的整合质量，并识别出"token雪球效应"和"昂贵失败"等系统性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI软件工程排行榜（如SWE-bench）只关注解决方案准确性，忽略了资源受限环境中的效能因素。任何AI系统不仅需要正确，还必须具有成本效益。

Method: 引入SWE-Effi指标集，在SWE-bench基准的子集上重新评估流行的AI问题解决系统，使用新的多维度指标（结果准确性与资源消耗的平衡）进行重新排名。

Result: 发现AI系统效能不仅取决于脚手架本身，还取决于其与基础模型的整合质量；识别出"token雪球效应"和"昂贵失败"模式；观察到token预算和时间预算下的效能存在明显权衡。

Conclusion: 需要综合考虑准确性和资源消耗来评估AI系统效能，系统整合质量对资源高效性能至关重要，识别出的系统性挑战对实际部署和RL训练成本有重要影响。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [10] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 首个对Hugging Face数据集和模型许可证的端到端审计，发现35.5%的模型到应用转换中通过重新许可消除了限制性条款，并开发了可检测86.4%许可证冲突的规则引擎


<details>
  <summary>Details</summary>
Motivation: 开源AI生态系统中隐藏的许可证冲突存在严重的法律和伦理风险，但缺乏对这些冲突发生频率、来源和受影响社区的数据驱动理解

Method: 对Hugging Face上36.4万个数据集、160万个模型以及14万个GitHub项目进行实证分析，并开发了可编码近200个SPDX和模型特定条款的可扩展规则引擎

Result: 发现35.5%的模型到应用转换通过重新许可消除了限制性条款，原型规则引擎可以解决86.4%的软件应用许可证冲突

Conclusion: 许可证合规性是开源AI中的关键治理挑战，研究提供了支持自动化、AI感知大规模合规性所需的数据和工具

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [11] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec是一种针对复杂循环程序的LLM辅助规范生成方法，通过程序切片和逻辑删除两个新阶段，显著提高了生成规范的正确性、相关性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在处理包含复杂循环结构的程序时往往生成不相关的规范，且验证工具的严格证明义务和设计约束会导致不完整和模糊的规范。

Method: 提出SLD-Spec方法，包含两个新阶段：(1)切片阶段-将函数分解为包含独立循环结构的代码片段；(2)逻辑删除阶段-使用LLM推理过滤错误候选规范。

Result: 在简单数据集上比最先进的AutoSpec多验证5个程序，运行时间减少23.73%。在复杂循环数据集上，95.1%的断言和90.91%的程序通过验证。

Conclusion: 逻辑删除对提升规范正确性和相关性至关重要，程序切片对规范完整性贡献显著，SLD-Spec能有效处理复杂循环程序的规范生成问题。

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [12] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL是一个集成SonarQube和大型语言模型的Web应用，通过三个模块自动化代码问题检测、修复和评估，在563个文件7599个问题上验证了有效性，显著减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 随着软件项目复杂度增加，代码问题数量和种类大幅增长，需要高效的自动化工具来检测、解决和评估代码问题。

Method: 开发WALL Web应用，集成SonarQube和LLMs（GPT-3.5 Turbo和GPT-4o），包含问题提取工具、代码问题修订器和代码比较工具三个模块。

Result: 在563个文件7599个问题上实验证明，WALL能有效减少人工工作量并保持高质量修订，混合使用成本效益型和先进LLMs可显著降低成本并提高修订率。

Conclusion: WALL展示了自动化代码质量管理的可行性，未来工作将集成开源LLMs并消除人工干预，实现完全自动化的代码质量管理。

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [13] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 研究发现提示工程可以提升小型语言模型在代码生成中的能效，其中CoT提示对Qwen2.5-Coder和StableCode-3B模型效果显著，但效果具有模型依赖性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件开发中的高能耗和碳足迹问题日益受到关注，小型语言模型作为更可持续的替代方案需要研究如何通过提示工程提高其能效。

Method: 评估4个开源SLM模型在150个LeetCode Python问题上的表现，测试4种提示策略（角色提示、零样本、少样本、思维链），测量运行时、内存使用和能耗，并与人工编写基准对比。

Result: CoT提示为Qwen2.5-Coder和StableCode-3B带来持续节能效果，但CodeLlama-7B和Phi-3-Mini-4K在所有提示策略下均未能超越基准。

Conclusion: 提示工程的效益具有模型依赖性，精心设计的提示可以引导SLM实现更环保的软件开发。

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [14] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: 使用LLM自动化软件设计文档评审过程的研究，通过分析11个评审视角并开发新技术，验证了LLM能够识别设计文档中的不一致性问题


<details>
  <summary>Details</summary>
Motivation: 探索利用大型语言模型(LLM)自动化软件设计文档评审过程，提高评审效率和准确性

Method: 分析设计文档评审方法并组织11个评审视角，开发新技术使LLM能够理解包含表格数据的复杂设计文档，使用GPT进行实验评估设计项和描述在不同设计文档中的一致性

Result: 实验结果表明LLM能够在评审过程中有效识别软件设计文档中的不一致性问题

Conclusion: LLM可以替代人类完成软件设计文档的某些评审任务，特别是在识别设计不一致性方面具有实用价值

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [15] [Sustaining Research Software: A Fitness Function Approach](https://arxiv.org/abs/2509.10085)
*Philipp Zech,Irdin Pekaric*

Main category: cs.SE

TL;DR: 本文提出利用进化架构中的适应度函数概念，为研究软件定义专门的FAIR（可发现、可访问、可互操作、可重用）适应度函数，通过自动化持续评估来提升研究软件的长期可持续性。


<details>
  <summary>Details</summary>
Motivation: 研究软件通常面临可维护性差、缺乏适应性、最终过时等可持续性挑战，需要系统性的方法来确保其长期可用性和科学影响力。

Method: 采用进化架构中的适应度函数概念，为研究软件定制专门的FAIR适应度函数，集成到开发生命周期中，促进模块化设计、全面文档化、版本控制和与不断发展的技术生态系统的兼容性。

Result: 案例研究和实验结果表明，该方法能够有效提升研究软件的长期FAIR特性，弥合短期项目开发与持久科学影响之间的差距。

Conclusion: 通过定义和实施针对研究软件的适应度函数，可以培养研究社区的可持续性文化，确保研究软件的长久价值和科学贡献。

Abstract: The long-term sustainability of research software is a critical challenge, as
it usually suffers from poor maintainability, lack of adaptability, and
eventual obsolescence. This paper proposes a novel approach to addressing this
issue by leveraging the concept of fitness functions from evolutionary
architecture. Fitness functions are automated, continuously evaluated metrics
designed to ensure that software systems meet desired non-functional,
architectural qualities over time. We define a set of fitness functions
tailored to the unique requirements of research software, focusing on
findability, accessibility, interoperability and reusability (FAIR). These
fitness functions act as proactive safeguards, promoting practices such as
modular design, comprehensive documentation, version control, and compatibility
with evolving technological ecosystems. By integrating these metrics into the
development life cycle, we aim to foster a culture of sustainability within the
research community. Case studies and experimental results demonstrate the
potential of this approach to enhance the long-term FAIR of research software,
bridging the gap between ephemeral project-based development and enduring
scientific impact.

</details>


### [16] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: LLM生成的Python代码在能源效率方面表现不一，在服务器上比人类代码低效16%，在树莓派上低效3%，但在PC上高效25%。绿色软件专家编写的代码在所有硬件平台上都比LLM代码节能17-30%。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的Python代码与人类编写代码在能源效率方面的对比，特别是在绿色软件开发背景下，了解LLM是否能替代人类专家编写节能代码。

Method: 使用EvoEval基准测试中的9个编程问题的363个解决方案，测试6个主流LLM和4种提示技术，并与人类开发的解决方案进行比较。在三种硬件平台（服务器、PC、树莓派）上测量能源消耗，总测试时间约881小时。

Result: 人类解决方案在服务器上节能16%，在树莓派上节能3%；LLM在PC上比人类开发者节能25%。提示技术不能持续带来节能效果，最节能的提示因硬件平台而异。绿色软件专家的代码在所有硬件平台上都比所有LLM节能至少17-30%。

Conclusion: 尽管LLM展现出相对良好的代码生成能力，但没有一个LLM生成的代码比经验丰富的绿色软件开发者更节能，表明目前开发节能Python代码仍然非常需要人类专业知识。

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


### [17] [Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://arxiv.org/abs/2509.10236)
*Mingyi Li,Junmin Xiao,Siyan Chen,Hui Ma,Xi Chen,Peihua Bao,Liang Yuan,Guangming Tan*

Main category: cs.SE

TL;DR: Stencil-Lifting是一个自动将低级语言编写的模板内核转换为等效DSL实现的系统，通过分层递归提升理论和算法实现高效转换，比现有系统快31.62倍和5.8倍


<details>
  <summary>Details</summary>
Motivation: 针对现有验证提升系统效率瓶颈，解决将遗留代码中的低级模板内核转换为现代DSL实现的可扩展性问题

Method: 提出分层递归提升理论，使用不变子图表示模板内核，通过谓词摘要编码计算语义；开发分层递归提升算法，保证终止性和完整性

Result: 在两个不同基准套件和四个实际应用上评估，相比STNG和Dexter系统分别实现31.62倍和5.8倍加速，同时保持完全语义等价

Conclusion: 显著提高了低级模板内核到DSL实现的转换效率，有效弥合了传统优化技术与现代DSL范式之间的差距

Abstract: We introduce Stencil-Lifting, a novel system for automatically converting
stencil kernels written in low-level languages in legacy code into semantically
equivalent Domain-Specific Language (DSL) implementations. Targeting the
efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting
achieves scalable stencil kernel abstraction through two key innovations.
First, we propose a hierarchical recursive lifting theory that represents
stencil kernels, structured as nested loops, using invariant subgraphs, which
are customized data dependency graphs that capture loop-carried computation and
structural invariants. Each vertex in the invariant subgraph is associated with
a predicate-based summary, encoding its computational semantics. By enforcing
self-consistency across these summaries, Stencil-Lifting ensures the derivation
of correct loop invariants and postconditions for nested loops, eliminating the
need for external verification. Second, we develop a hierarchical recursive
lifting algorithm that guarantees termination through a convergent recursive
process, avoiding the inefficiencies of search-based synthesis. The algorithm
efficiently derives the valid summaries of stencil kernels, and its
completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil
benchmarks from two different suites and on four real-world applications.
Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$
and 5.8$\times$ speedups compared to the state-of-the-art verified lifting
systems STNG and Dexter, respectively, while maintaining full semantic
equivalence. Our work significantly enhances the translation efficiency of
low-level stencil kernels to DSL implementations, effectively bridging the gap
between legacy optimization techniques and modern DSL-based paradigms.

</details>


### [18] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: T-TS是一种基于机器学习的工业测试选择方法，通过将提交表示为变更文件的词袋，结合跨文件和预测特征，无需覆盖率映射，在工业部署中仅选择15%的测试，执行时间减少5.9倍，管道加速5.6倍，故障检测率超过95%。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展和测试套件增长，在高频率代码提交的情况下，高效管理测试过程变得越来越具有挑战性。需要一种能够有效选择相关测试的方法来提高测试效率。

Method: 提出Targeted Test Selection (T-TS)机器学习方法，关键创新是数据表示方式：将提交表示为变更文件的词袋，包含跨文件和额外预测特征，避免使用覆盖率映射。

Result: 在工业数据上，T-TS仅选择15%的测试，执行时间减少5.9倍，管道加速5.6倍，检测超过95%的测试故障。方法在内部和公共数据集上进行了全面评估。

Conclusion: T-TS在工业环境中表现出色，显著提高了测试效率，实现公开可用以支持进一步研究和实际应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [19] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: 基于82,845个真实开发者与LLM对话的分析显示，LLM响应比开发者提示长14倍，68%为多轮对话，代码生成存在语言特定的质量问题，但通过明确指错和请求修复可以有效改善代码质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件开发中广泛应用，但缺乏对开发者实际交互方式和对话动态如何影响任务结果、代码质量和工作流的深入理解。

Method: 利用CodeChat数据集（包含82,845个真实开发者-LLM对话，368,506个代码片段，覆盖20+编程语言），进行对话长度分析、话题分类和五种主要编程语言的代码质量评估。

Result: 发现LLM响应显著长于开发者提示（14:1比例），多轮对话占68%；不同语言存在特定问题：Python/JavaScript未定义变量（83.4%/75.3%）、Java缺少注释（75.9%）、C++缺少头文件（41.1%）、C#未解析命名空间（49.2%）；但通过多轮交互，Java文档质量提升14.7%，Python导入处理改善3.7%。

Conclusion: 开发者与LLM的对话通常是迭代和动态的，明确指出现有代码错误并请求修复的提示最有效，多轮对话可以逐步改善代码质量，但不同编程语言需要针对性的质量关注点。

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [DBOS Network Sensing: A Web Services Approach to Collaborative Awareness](https://arxiv.org/abs/2509.09898)
*Sophia Lockton,Jeremy Kepner,Michael Stonebraker,Hayden Jananthan,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel Burrill,Chansup Byun,Timothy Davis,Vijay Gadepally,Michael Houle,Matthew Hubbell,Michael Jones,Piotr Luszczek,Peter Michaleas,Lauren Milechin,Chasen Milner,Guillermo Morales,Julie Mullen,Michel Pelletier,Alex Poliakov,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Alex Pentland*

Main category: cs.NI

TL;DR: DBOS是一个集成了网络服务、操作系统功能和数据库特性的新型系统，通过GraphBLAS超稀疏流量矩阵实现网络感知，显著降低了web部署工作量并提高了弹性。


<details>
  <summary>Details</summary>
Motivation: 为了解决web部署工作量大且系统弹性不足的问题，开发一个集成了操作系统、数据库和网络服务的统一平台，通过协同网络感知来增强集体弹性和安全性。

Method: 采用两种方法实现网络感知：(1) Python-GraphBLAS方法，(2) OneSparse PostgreSQL方法。使用pPython进行并行化，在MIT SuperCloud的64个计算节点上进行基准测试。

Result: 单个DBOS实例可维持>10^5的web请求率，远超需求最大值。Python-GraphBLAS实现可线性扩展到64个节点，OneSparse PostgreSQL可扩展到32个节点，计算资源开销可忽略不计。

Conclusion: DBOS能够以可忽略的开销实现协同网络感知，为构建更具弹性和安全性的web服务提供了有效解决方案。

Abstract: DBOS (DataBase Operating System) is a novel capability that integrates web
services, operating system functions, and database features to significantly
reduce web-deployment effort while increasing resilience. Integration of high
performance network sensing enables DBOS web services to collaboratively create
a shared awareness of their network environments to enhance their collective
resilience and security. Network sensing is added to DBOS using GraphBLAS
hypersparse traffic matrices via two approaches: (1) Python-GraphBLAS and (2)
OneSparse PostgreSQL. These capabilities are demonstrated using the workflow
and analytics from the IEEE/MIT/Amazon Anonymized Network Sensing Graph
Challenge. The system was parallelized using pPython and benchmarked using 64
compute nodes on the MIT SuperCloud. The web request rate sustained by a single
DBOS instance was ${>}10^5$, well above the required maximum, indicating that
network sensing can be added to DBOS with negligible overhead. For
collaborative awareness, many DBOS instances were connected to a single DBOS
aggregator. The Python-GraphBLAS and OneSparse PostgreSQL implementations
scaled linearly up to 64 and 32 nodes respectively. These results suggest that
DBOS collaborative network awareness can be achieved with a negligible increase
in computing resources.

</details>


### [21] [Taming Volatility: Stable and Private QUIC Classification with Federated Learning](https://arxiv.org/abs/2509.09997)
*Richard Jozsa,Karel Hynek,Adrian Pekar*

Main category: cs.NI

TL;DR: 本文针对联邦学习中网络流量时间波动性问题，提出客户端数据缓冲机制，在QUIC分类任务中实现95.2%的F1分数，仅比非私有集中式模型低2.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护的网络流量分析中具有潜力，但实际部署面临非IID数据的挑战。先前工作主要关注统计异质性，而时间波动性（网络活动的日常起伏）对模型稳定性的影响尚未充分研究。这种波动会导致客户端数据可用性不一致，破坏整个训练过程的稳定性。

Method: 提出客户端数据缓冲机制作为实用解决方案，将本地训练与实时流量波动解耦，确保稳定一致的本地训练。使用真实世界的CESNET-QUIC22数据集，划分为14个自治客户端进行验证。

Result: 该方法实现了稳健收敛，稳定联邦系统达到95.2%的F1分数，仅比非私有集中式模型低2.3个百分点。

Conclusion: 这项工作为构建操作稳定的网络管理联邦学习系统提供了蓝图，证明通过有针对性的架构选择可以克服动态网络环境的挑战。

Abstract: Federated Learning (FL) is a promising approach for privacy-preserving
network traffic analysis, but its practical deployment is challenged by the
non-IID nature of real-world data. While prior work has addressed statistical
heterogeneity, the impact of temporal traffic volatility-the natural daily ebb
and flow of network activity-on model stability remains largely unexplored.
This volatility can lead to inconsistent data availability at clients,
destabilizing the entire training process. In this paper, we systematically
address the problem of temporal volatility in federated QUIC classification. We
first demonstrate the instability of standard FL in this dynamic setting. We
then propose and evaluate a client-side data buffer as a practical mechanism to
ensure stable and consistent local training, decoupling it from real-time
traffic fluctuations. Using the real-world CESNET-QUIC22 dataset partitioned
into 14 autonomous clients, we then demonstrate that this approach enables
robust convergence. Our results show that a stable federated system achieves a
95.2% F1 score, a mere 2.3 percentage points below a non-private centralized
model. This work establishes a blueprint for building operationally stable FL
systems for network management, proving that the challenges of dynamic network
environments can be overcome with targeted architectural choices.

</details>


### [22] [Service Function Chaining Architecture for Multi-hop Split Inference and Learning](https://arxiv.org/abs/2509.10001)
*Takanori Hara,Masahiro Sasabe*

Main category: cs.NI

TL;DR: 提出基于服务功能链(SFC)的多跳分割推理/学习架构，将分割子模型视为服务功能，通过SFC动态建立通信路径，实现高效自适应执行


<details>
  <summary>Details</summary>
Motivation: 受网络服务功能链技术启发，希望为多跳分割推理和学习提供动态、高效的通信路径管理方案

Method: 设计神经服务功能(NSF)作为透明TCP代理执行分割子模型，与SRv6和eBPF-based SFC代理集成，实现动态路由上的高效ML处理

Result: 架构对MSI和MSL都可行，特别适合小批量实时推理场景，支持动态路径重配置以适应网络变化

Conclusion: SFC-based架构为多跳分割推理和学习提供了高效的动态通信路径管理解决方案，具有良好的兼容性和适应性

Abstract: Service Function Chaining (SFC) is a networking technique that ensures
traffic traverses a predefined sequence of service functions, realizing
arbitrary network services through dynamic and efficient communication paths.
Inspired by this concept, we propose an SFC-based architecture for Multi-hop
Split Inference (MSI), where split sub-models are interpreted as service
functions and their composition forms a service chain representing the global
model. By leveraging SFC, the proposed architecture dynamically establishes
communication paths for split sub-models, ensuring efficient and adaptive
execution. Furthermore, we extend this architecture to Multi-hop Split Learning
(MSL) by applying SFC to the bidirectional communication required for training
tasks. To realize the proposed architecture, we design Neural Service Functions
(NSFs) to execute split sub-models as transparent TCP proxies and integrate
them with Segment Routing over IPv6 (SRv6) and the extended Berkeley Packet
Filter (eBPF)-based SFC proxy. This integration ensures efficient ML processing
over dynamic routing while maintaining compatibility with existing
applications. Evaluation results demonstrate that (1) the proposed architecture
is feasible for both MSI and MSL; (2) it is particularly suitable for real-time
inference in MSI scenarios with small mini-batch sizes; (3) it supports dynamic
path reconfiguration, enabling adaptive responses to changing network
conditions while minimizing the impact of control mechanisms on inference and
learning processes.

</details>


### [23] [Maximising Energy Efficiency in Large-Scale Open RAN: Hybrid xApps and Digital Twin Integration](https://arxiv.org/abs/2509.10097)
*Ahmed Al-Tahmeesschi,Yi Chu,Gurdeep Singh,Charles Turyagyenda,Dritan Kaleshi,David Grace,Hamed Ahmadi*

Main category: cs.NI

TL;DR: 提出基于启发式方法和无监督机器学习的混合xApp，结合数字孪生技术动态管理RU睡眠模式，在Open RAN中实现约13%的节能效果且不影响服务质量


<details>
  <summary>Details</summary>
Motivation: 5G及后续网络的高速、超可靠、低延迟通信需求导致RAN功耗大幅增加，带来运营和可持续性挑战，需要在不影响QoS的前提下提升能效

Method: 采用混合xApp结合启发式方法和无监督机器学习，通过TeraVM AI RAN场景生成器集成数字孪生技术，动态管理开放无线电单元(RU)的睡眠模式

Result: 在真实大规模仿真Open RAN场景中，混合xApp实现了约13%的节能效果，证明了其实际应用潜力

Conclusion: 该方法有效解决了O-RAN架构中功耗管理的复杂性，展示了在不影响用户QoS的前提下显著降低能耗的可行性

Abstract: The growing demand for high-speed, ultra-reliable, and low-latency
communications in 5G and beyond networks has significantly driven up power
consumption, particularly within the Radio Access Network (RAN). This surge in
energy demand poses critical operational and sustainability challenges for
mobile network operators, necessitating innovative solutions that enhance
energy efficiency without compromising Quality of Service (QoS). Open Radio
Access Network (O-RAN), spearheaded by the O-RAN Alliance, offers
disaggregated, programmable, and intelligent architectures, promoting
flexibility, interoperability, and cost-effectiveness. However, this
disaggregated approach adds complexity, particularly in managing power
consumption across diverse network components such as Open Radio Units (RUs).
In this paper, we propose a hybrid xApp leveraging heuristic methods and
unsupervised machine learning, integrated with digital twin technology through
the TeraVM AI RAN Scenario Generator (AI-RSG). This approach dynamically
manages RU sleep modes to effectively reduce energy consumption. Our
experimental evaluation in a realistic, large-scale emulated Open RAN scenario
demonstrates that the hybrid xApp achieves approximately 13% energy savings,
highlighting its practicality and significant potential for real-world
deployments without compromising user QoS.

</details>


### [24] [Secure and Scalable Rerouting in LEO Satellite Networks](https://arxiv.org/abs/2509.10173)
*Lyubomir Yanev,Pietro Ronchetti,Joshua Smailes,Martin Strohmeier*

Main category: cs.NI

TL;DR: 本文系统比较了LEO卫星网络中三种重路由策略（本地邻居、分段和全局知识）在动态故障下的性能表现，发现分段重路由在响应性和协调性之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络面临频繁不可预测的链路和节点故障，现有重路由策略在不同网络感知程度下的性能权衡尚未得到充分研究。

Method: 扩展Deep Space Network Simulator，比较本地邻居、分段和全局知识三种重路由范式，以及无故障感知的源路由方案，评估随机和定向故障下的性能指标。

Result: 分段重路由在本地响应性和全局协调性之间实现了有利的权衡，以最小开销提供弹性优势。

Conclusion: 分段重路由策略为未来容错卫星网络设计提供了有价值的见解，能够在故障条件下实现良好的路由性能和弹性。

Abstract: Resilient routing in large-scale Low Earth Orbit (LEO) satellite networks
remains a key challenge due to frequent and unpredictable link and node
failures, potentially in response to cybersecurity breaches. While prior work
has explored rerouting strategies with various levels of network awareness,
their relative tradeoffs under dynamic failure conditions remain underexplored.
In this work, we extend the Deep Space Network Simulator (DSNS) to
systematically compare three rerouting paradigms, each differing in the scope
of failure knowledge available to each node. We compare local neighbor-based,
segment-based and global-knowledge-based rerouting as well as a naive source
routing solution that is unaware of failures. Our main goal is to evaluate how
the breadth of failure awareness impacts routing performance and resilience
under failures, both random and targeted. We measure delivery ratio, latency,
rerouting overhead, and loop occurrence. Our findings show the potential of
segment-based rerouting to achieve a favorable tradeoff between local
responsiveness and global coordination, offering resilience benefits with
minimal overhead--insights that can inform future fault-tolerant satellite
network design.

</details>


### [25] [Friend or Foe? Identifying Anomalous Peers in Moneros P2P Network](https://arxiv.org/abs/2509.10214)
*Yannik Kopyciok,Stefan Schmid,Friedhelm Victor*

Main category: cs.NI

TL;DR: 对Monero P2P网络中异常节点行为的首次全面研究，发现约14.74%的节点表现出非标准行为，可能威胁隐私保护和网络去中心化


<details>
  <summary>Details</summary>
Motivation: Monero作为领先的隐私加密货币，其P2P网络中存在伪装成诚实节点的非标准节点，可能用于网络监控和间谍活动，但目前对异常节点行为的检测和分析理解有限

Method: 从全球5个不同观测点收集超过240小时的网络流量数据，建立形式化框架来分析和分类P2P加密货币网络中的异常模式，实现离线分析检测方法

Result: 发现网络中约14.74%（可达节点中13.19%）的节点表现出非标准行为，这些节点显示出可能表明多种并发攻击的独特行为模式，揭示了Monero隐私保证和网络去中心化的重大缺陷

Conclusion: 研究揭示了Monero网络中存在的严重安全问题，发布了检测管道以帮助网络运营商识别和阻止可疑节点，为实时监控系统奠定了基础

Abstract: Monero, the leading privacy-focused cryptocurrency, relies on a peer-to-peer
(P2P) network to propagate transactions and blocks. Growing evidence suggests
that non-standard nodes exist in the network, posing as honest nodes but are
perhaps intended for monitoring the network and spying on other nodes. However,
our understanding of the detection and analysis of anomalous peer behavior
remains limited. This paper presents a first comprehensive study of anomalous
behavior in Monero's P2P network. To this end, we collected and analyzed over
240 hours of network traffic captured from five distinct vantage points
worldwide. We further present a formal framework which allows us to
analytically define and classify anomalous patterns in P2P cryptocurrency
networks. Our detection methodology, implemented as an offline analysis,
provides a foundation for real-time monitoring systems. Our analysis reveals
the presence of non-standard peers in the network where approximately 14.74%
(13.19%) of (reachable) peers in the network exhibit non-standard behavior.
These peers exhibit distinct behavioral patterns that might suggest multiple
concurrent attacks, pointing to substantial shortcomings in Monero's privacy
guarantees and network decentralization. To support reproducibility and enable
network operators to protect themselves, we release our examination pipeline to
identify and block suspicious peers based on newly captured network traffic.

</details>


### [26] [RFSeek and Ye Shall Find](https://arxiv.org/abs/2509.10216)
*Noga H. Rotman,Tiago Ferreira,Hila Peleg,Mark Silberstein,Alexandra Silva*

Main category: cs.NI

TL;DR: RFSeek是一个基于LLM的交互式工具，能够从RFC文档中自动提取协议逻辑的可视化摘要，生成可追溯来源的状态机图，帮助理解复杂的网络协议规范。


<details>
  <summary>Details</summary>
Motivation: RFC文档冗长且基于文本，阻碍了对协议操作逻辑的精确理解，需要工具来自动提取和可视化协议逻辑。

Method: 利用大型语言模型(LLMs)从RFC文本中生成可追溯来源的可探索图表，提取官方状态机和文本中隐含的逻辑。

Result: RFSeek不仅重建了RFC中的现有图表，还发现了文本中描述但图表中缺失的重要逻辑节点和边，并为复杂协议如QUIC生成了新的可视化图表。

Conclusion: 结合LLMs和形式化、用户定制化可视化的摘要可视化方法，是增强协议理解和支持健壮实现的有前景方向。

Abstract: Requests for Comments (RFCs) are extensive specification documents for
network protocols, but their prose-based format and their considerable length
often impede precise operational understanding. We present RFSeek, an
interactive tool that automatically extracts visual summaries of protocol logic
from RFCs. RFSeek leverages large language models (LLMs) to generate
provenance-linked, explorable diagrams, surfacing both official state machines
and additional logic found only in the RFC text. Compared to existing RFC
visualizations, RFSeek's visual summaries are more transparent and easier to
audit against their textual source. We showcase the tool's potential through a
series of use cases, including guided knowledge extraction and semantic
diffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.
  In practice, RFSeek not only reconstructs the RFC diagrams included in some
specifications, but, more interestingly, also uncovers important logic such as
nodes or edges described in the text but missing from those diagrams. RFSeek
further derives new visualization diagrams for complex RFCs, with QUIC as a
representative case. Our approach, which we term \emph{Summary Visualization},
highlights a promising direction: combining LLMs with formal, user-customized
visualizations to enhance protocol comprehension and support robust
implementations.

</details>


### [27] [Trusted Repeater Placement in QKD-enabled Optical Networks](https://arxiv.org/abs/2509.10338)
*Arup Kumar Marik,Basabdatta Palit,Sadananda Behera*

Main category: cs.NI

TL;DR: 量子密钥分配网络中的可靠性感知中继节点布置框架，通过信任评分和中心性指标优化TRN选择，在同样数量中继节点下覆盖了更多最短路径。


<details>
  <summary>Details</summary>
Motivation: 现有量子中继网络假设所有中继节点都可信，忽视了软件漏洞和内部威胁带来的风险，需要一种更可靠的中继节点布置方案。

Method: 提出可靠性感知TRN布置框架，给每个节点赋予信任评分，通过权重链路集成到Dijkstra算法中，然后使用中间度中心性和特征向量中心性的综合评分对节点进行排名。

Result: 在参考拓扑结构上的模拟显示，在使用约8个TRN的情况下，新方法相比传统的度中心性指标覆盖了多10.77%的最短路径。

Conclusion: 该方法适合于TRN选择，能够最大化安全连接性，提高量子密钥分配网络的可靠性和安全性。

Abstract: Quantum Key Distribution (QKD) provides information-theoretic security, but
is limited by distance in optical networks, thereby requiring repeater nodes to
extend coverage. Existing works usually assume all repeater nodes and
associated Key Management Servers (KMSs) to be Trusted Repeater Nodes (TRNs),
while ignoring risks from software exploits and insider threats. In this paper,
we propose a reliability-aware TRN placement framework for metro optical
networks, which assigns each node a trust score and integrates it into the
Dijkstra algorithm via weighted links. We then rank the nodes using a composite
score, which is a weighted combination of betweenness centrality and
eigenvector centrality to enable a secure and scalable TRN deployment.
Simulation results on a reference topology show that our method covers 10.77%
more shortest paths compared to traditional metrics like degree centrality,
using the same number (around eight) of TRNs, making it suitable for TRN
selection to maximize secure connectivity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG是一个两阶段的自监督学习框架，通过结构语义保护来学习脑图表示，在精神疾病诊断中表现优异，特别是在小样本标注数据场景下。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法在脑图数据上往往依赖可能破坏关键结构语义的数据增强策略，而标注脑网络数据的稀缺性使得准确且可解释的精神疾病诊断具有挑战性。

Method: 提出两阶段框架：1）预训练阶段在小标注子集上训练边缘掩码器捕捉关键结构语义；2）自监督学习阶段利用提取的结构先验指导结构感知的数据增强过程，学习更具语义意义和鲁棒性的表示。

Result: 在两个真实世界精神疾病数据集上的实验表明，SAM-BG优于最先进方法，特别是在小标注数据设置下，并能发现具有临床相关性的连接模式增强可解释性。

Conclusion: SAM-BG通过结构语义保护的自监督学习方法，有效解决了脑图数据标注稀缺的问题，在精神疾病诊断中实现了更好的性能和可解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [29] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: 提出了D-CAT框架，通过解耦的跨注意力转移实现跨模态知识迁移，在推理时只需单一传感器，解决了多模态分类模型在资源受限环境中的部署问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移学习方法在训练和推理时都需要成对的传感器数据，限制了在资源受限环境中的部署，因为完整传感器套件在经济和技术上不可行。

Method: D-CAT框架结合自注意力模块进行特征提取，使用新颖的跨注意力对齐损失来对齐不同模态的特征空间，无需耦合两种模态的分类流程。

Result: 在三个多模态人类活动数据集上评估，在分布内场景中从高性能模态（如视频到IMU）转移可获得10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能。

Conclusion: D-CAT通过跨模态知识实现单传感器推理，减少了感知系统的硬件冗余，同时保持准确性，对成本敏感或自适应部署至关重要。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [30] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto是一个基于Transformer的统一架构，结合元学习和强化学习，创建了一个完全自我改进的加密货币交易代理，无需人工监督，在多种市场环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 预测加密货币回报极其困难，价格变动由快速变化的链上活动、新闻流和社交情绪驱动，且标记训练数据稀缺昂贵，需要一种能够自我改进的交易代理。

Method: 从基础指令调优的LLM开始，代理在闭环架构中迭代交替扮演三个角色（执行者、评判者和元评判者），利用多模态市场输入和内部偏好反馈，持续改进交易策略和评估标准。

Result: 在多样化市场环境下的实验表明，Meta-RL-Crypto在真实市场的技术指标上表现良好，并且优于其他基于LLM的基线方法。

Conclusion: 该研究提出了一个统一的元学习-强化学习框架，能够创建完全自我改进的交易代理，在加密货币预测这一困难任务中展现出优越性能，为自动化交易系统提供了新思路。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [31] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa是一个统一的KV缓存压缩框架，通过最小化Transformer残差流信息损失实现动态预算分配，在多种长文本基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法大多是启发式的，缺乏动态预算分配机制，无法根据任务需求优化缓存使用效率。

Method: 通过分析层注意力输出损失，提出新的度量标准来比较不同头的缓存条目，实现层级压缩和动态头预算分配；通过跨层信息对比实现动态层预算分配。

Result: 在LongBench、Needle-In-A-Haystack、Ruler和InfiniteBench等基准测试中表现出优越性能，无需训练或多策略组合。

Conclusion: LAVa是首个统一的缓存淘汰和动态预算分配策略，动态层预算对生成任务关键，动态头预算对抽取任务重要，在各种任务类型中保持顶级性能。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [32] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: 提出HACO框架，结合风险校准和偏好优化，为医疗补助人群提供安全、公平、可审计的决策支持，在270万次决策数据上验证了风险控制和公平性


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群的健康管理项目需要协调纵向服务，必须确保安全、公平和可审计性，需要一种能够控制不良事件风险并提供保守行动建议的框架

Method: 混合自适应符合离线强化学习(HACO)框架：1)训练轻量级风险模型预测不良事件；2)推导符合阈值以屏蔽不安全行动；3)在安全子集上学习偏好策略。使用版本无关的FQE评估和亚组审计

Result: HACO实现了强大的风险区分能力(AUC约0.81)，校准阈值在α=0.10时为τ≈0.038，同时保持高安全覆盖率。亚组分析显示不同人口统计特征间存在系统性价值差异

Conclusion: 符合风险门控与离线强化学习结合良好，能够为人群健康管理团队提供保守、可审计的决策支持，强调了公平性审计的重要性

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [33] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF是一种新颖的联邦学习框架，通过在本地训练期间直接学习量化模型参数，逐比特更新参数来大幅减少通信开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但存在显著的通信开销问题。现有方法通常在本地训练后进行量化，这会将量化误差引入训练参数并可能降低模型精度。

Method: 提出Federated Bit Freezing (FedBiF)框架：服务器先量化模型参数并传输给客户端；每个客户端每次只更新多比特参数表示中的单个比特，冻结其余比特；这种逐比特更新策略将每个参数更新减少到1比特。

Result: 在5个常用数据集上的IID和非IID设置下进行实验，FedBiF不仅实现了优异的通信压缩，还促进了模型的稀疏性。在使用仅1bpp上行和3bpp下行通信时，仍能达到与FedAvg相当的精度。

Conclusion: FedBiF通过直接在本地训练期间学习量化参数和逐比特更新策略，有效解决了联邦学习的通信开销问题，在保持模型精度的同时实现了显著的通信压缩。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [34] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出基于单头交叉注意力机制的统一路由框架，动态选择最优LLM，在RouterBench基准上实现6.6%的质量提升和2.9%的最大性能提升


<details>
  <summary>Details</summary>
Motivation: 解决不同计算成本和性能的大型语言模型在现实应用中规模化、成本效益部署的挑战

Method: 使用单头交叉注意力机制联合建模查询和模型嵌入，显式捕获细粒度查询-模型交互，预测响应质量和生成成本

Result: 在RouterBench基准测试中，平均质量提升6.6%，最大性能提升2.9%，架构轻量且跨域泛化效果好

Conclusion: 建立了成本感知LLM路由的新标准，通过指数奖励函数在性能和成本间实现稳健平衡，相比现有方法效率更高

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [35] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 分析梯度步长去噪器及其在即插即用算法中的应用，该去噪器被训练为显式函数的功能梯度下降算子或邻近算子，同时保持最先进的去噪能力


<details>
  <summary>Details</summary>
Motivation: 即插即用优化算法使用现成的去噪器来替代图像先验的邻近算子或梯度下降算子，但通常这种图像先验是隐式的。研究旨在开发能够同时作为显式函数算子的高性能去噪器

Method: 训练梯度步长去噪器，使其精确地成为显式函数的功能梯度下降算子或邻近算子

Result: 梯度步长去噪器在保持最先进去噪性能的同时，能够作为显式函数的数学算子使用

Conclusion: 梯度步长去噪器成功地将显式函数建模与高性能去噪能力相结合，为即插即用算法提供了更可靠的数学基础

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [36] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 本文使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶事件，最高准确率达到85.7%，并能有效区分惊吓、惊讶和基线状态，准确率74.9%。


<details>
  <summary>Details</summary>
Motivation: 意外事件会损害注意力并延迟决策，在航空等高危环境中构成严重安全风险。惊吓和惊讶反应以不同方式影响飞行员表现，但在实践中难以区分。现有研究大多单独研究这些反应，对其组合效应或如何用生理数据区分它们关注有限。

Method: 使用机器学习和多模态融合策略，基于生理信号区分惊吓和惊讶事件。采用SVM和XGBoost等算法，结合Late Fusion融合策略。

Result: 惊吓和惊讶事件可以可靠预测，SVM和Late Fusion组合达到最高平均准确率85.7%。扩展评估包括基线条件后，XGBoost和Late Fusion组合能区分惊吓、惊讶和基线状态，最高平均准确率74.9%。

Conclusion: 该方法能有效区分惊吓和惊讶反应，为高危环境中意外事件的生理监测和干预提供了可靠的技术手段，具有重要的安全应用价值。

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [37] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 提出Proof of AutoML架构，利用机器学习回归器生成随机数作为区块链nonce，用于灾难场景下太阳能家庭与移动充电单元之间的安全能源交易


<details>
  <summary>Details</summary>
Motivation: 在灾难场景中传统能源基础设施受损时，需要确保太阳能家庭与移动充电单元之间能源交易的安全性和可追溯性，而区块链网络需要强大的随机nonce生成机制

Method: 采用SDN使能架构，利用五种AutoML选择的回归模型（梯度提升、LightGBM、随机森林、额外树和K近邻），通过9000样本数据集评估这些模型生成随机输出的能力而非预测精度

Result: 随机性分析显示随机森林和额外树回归器表现出完全的随机性依赖，梯度提升、K近邻和LightGBM分别达到97.6%、98.8%和99.9%的随机性分数

Conclusion: 某些机器学习模型，特别是基于树的集成方法，可以作为有效的轻量级nonce生成器，用于构建抗灾难的区块链安全SDN能源交易基础设施

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [38] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 该论文重新审视了离散动作环境中的actor-critic方法，通过解耦actor和critic的熵项，显著提升了离散SAC的性能，使其在Atari游戏中达到与DQN相当的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于值的方法（如DQN）在离散动作环境中表现良好，而基于策略的方法要么无法有效利用离策略数据（如PPO），要么在离散动作设置中表现不佳（如SAC）。离散SAC（DSAC）性能较差的主要原因是actor和critic熵项的耦合。

Method: 提出了一个灵活的离策略actor-critic框架，解耦了actor和critic的熵项，允许使用m步Bellman算子进行critic更新，并将标准策略优化方法与熵正则化相结合来实例化actor目标。

Result: 理论上证明了在表格设置中该方法可以保证收敛到最优正则化值函数。实证结果表明，这些方法可以在标准Atari游戏中接近DQN的性能，甚至在没有熵正则化或显式探索的情况下也能实现。

Conclusion: 通过解耦actor和critic的熵项，离散动作环境中的actor-critic方法可以达到与值基方法相当的性能，为离散动作RL提供了新的有效解决方案。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [39] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN是首个针对异质图的集成学习框架，通过元路径和随机丢弃创建等位GNN，利用残差注意力机制和相关性正则化提升分类精度


<details>
  <summary>Details</summary>
Motivation: 异质图中节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来挑战，需要适应多样化的图学习器

Method: 使用元路径结合随机丢弃创建等位GNN，采用残差注意力机制校准不同元路径的等位GNN，并通过相关性正则化项增大不同元路径生成嵌入矩阵的差异性

Result: 在五个异质网络上的实验验证HGEN始终以显著优势超越最先进的竞争对手

Conclusion: HGEN通过有效的集成学习框架成功解决了异质图学习中的挑战，提升了分类准确性

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [40] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 论文提出了一个动态计算分配框架，在推理时根据查询需求选择最佳生成策略（如beam search或best-of-N），同时考虑token成本和延迟时间，以优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理时扩展方法主要关注并行生成和token使用，忽略了增量解码方法和延迟时间，而延迟对用户体验和智能体工作流至关重要。

Method: 构建动态计算分配和策略选择框架，在每查询基础上决定应用哪种生成策略以及分配多少计算资源，同时显式考虑token成本和wall-clock延迟。

Result: 在推理基准测试中，该方法持续优于静态策略，实现了良好的准确率-成本权衡，且具有实际部署可行性。

Conclusion: 动态计算分配框架能够有效优化LLM推理性能，在保持准确性的同时降低计算成本和延迟，特别适用于需要高效多查询的智能体工作流。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [41] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 本文提出了一种基于可观测变量的数据驱动计算框架，用于描述耗散动力系统的相空间演化，通过热力学拉格朗日方法和神经网络来保证热力学约束和熵增特性。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统的演化计算依赖于相空间数据，但实际中可观测的变量往往不包含完整的相空间信息（如动量和熵），特别是在耗散动力系统中，这些变量通常无法直接观测。

Method: 开发了一个基于热力学拉格朗日的新型方法，构建神经网络来保持热力学约束，确保熵的非递减演化，仅使用可观测变量进行相空间演化描述。

Result: 研究表明，该网络能够基于有限的数据点和相对较少的系统参数，有效描述相空间演化。

Conclusion: 该方法为仅使用可观测变量进行耗散动力系统演化计算提供了有效的框架，克服了传统方法需要完整相空间信息的限制。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [42] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 提出了LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习问题，相比从零训练的方法能生成更可靠的伪标签，并在开放世界场景下处理OOD样本。


<details>
  <summary>Details</summary>
Motivation: 现有的长尾半监督学习方法大多从零开始训练模型，容易导致过自信和低质量伪标签问题，需要更有效的方法来利用基础模型的能力。

Method: 提出LoFT框架，通过参数高效微调预训练基础模型来生成更可靠的伪标签；进一步提出LoFT-OW处理开放世界场景下的OOD样本问题。

Result: 在多个基准测试中表现优于现有方法，即使只使用1%的无标签数据也能取得优异性能。

Conclusion: 通过微调基础模型可以有效提升长尾半监督学习的性能，特别是在处理开放世界场景时具有显著优势。

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [43] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 提出了多播放组合半强盗(MP-CSB)模型，扩展了传统组合半强盗问题到非负整数动作空间，解决了最优运输和背包等问题。提出了两种算法：基于Thompson采样的算法和最佳两用算法，在理论和实验上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统组合半强盗(CSB)问题限制在二元决策空间，无法处理涉及非负整数流或分配的重要问题，如最优运输和背包问题。需要扩展模型来支持更广泛的应用场景。

Method: 提出了MP-CSB模型，允许选择非负整数动作并从单个臂获得多次反馈。设计了两种算法：1)基于Thompson采样的算法，计算可行且达到O(log T)遗憾；2)最佳两用算法，在随机和对抗环境下都表现良好。

Result: Thompson采样算法在随机环境下达到O(log T)分布依赖遗憾；最佳两用算法在随机环境下达到O(log T)方差依赖遗憾，在对抗环境下达到Õ(√T)最坏情况遗憾，且具有数据适应性。实验显示提出的算法优于现有CSB方法。

Conclusion: MP-CSB模型成功扩展了组合半强盗问题到整数动作空间，提出的算法在理论和实验性能上都表现出色，为解决最优运输、背包等实际问题提供了有效工具。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [44] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: 本文探索使用LLMs生成科学计算代码的新方法，通过构建诊断数据集和大规模ODE基准测试，评估LLMs在数值算法选择和稳定性检查方面的能力。


<details>
  <summary>Details</summary>
Motivation: 传统科学机器学习方法在准确性和鲁棒性方面存在挑战，本文探索使用LLMs编写数值算法代码的替代方案，将学习负担从求解函数转移到领域感知的数值选择。

Method: 引入两个新数据集：诊断性误导问题和1000个多样化ODE任务的大规模基准测试。评估开源和闭源LLM模型在无引导与领域知识引导提示、现成与微调变体方面的表现。

Result: 研究发现，在充分上下文和引导提示下，较新的指令跟随模型在两个评估标准上都达到高准确率。开源系统无需微调即可表现良好，而较老或较小模型仍能从微调中受益。

Conclusion: 初步结果表明，精心设计的提示和微调可以产生能够可靠解决简单ODE问题的专用LLM代理，为科学计算任务提供了新的解决方案路径。

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [45] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena模型通过将音频视觉线索转换为动态的逐token卷积核来直接调制文本特征提取，而不是简单的特征融合，在多模态意图识别任务上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态意图识别模型通过多头注意力等机制融合多模态特征，但这种方法可能会用噪声或无关的非语言信号污染主要语言特征，无法捕捉细粒度的token级影响。

Method: 提出DyKen-Hyena模型，将问题从特征融合重新定义为处理调制，将音频视觉线索转换为动态的逐token卷积核来直接调制文本特征提取过程。

Result: 在MIntRec和MIntRec2.0基准测试中达到最先进结果，在out-of-scope检测中获得+10.46%的F1分数提升。

Conclusion: 该方法创建了更鲁棒的意图表示，验证了通过调制而非简单融合的方式处理多模态信息的有效性。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [46] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出无需训练的token合并框架，通过贝叶斯优化实现计算和通信成本的动态压缩，在保持精度的同时显著降低资源消耗


<details>
  <summary>Details</summary>
Motivation: 大规模transformer模型在边缘设备部署时面临高计算和通信成本问题，需要一种无需重新训练的自适应压缩方法

Method: 基于每层相似度阈值选择性合并语义冗余token，使用贝叶斯优化寻找帕累托最优的精度-效率权衡策略

Result: 在ImageNet分类上减少30%计算量和80%通信成本，在VQA任务上以1/3计算量和1/10带宽达到LLaVA模型性能

Conclusion: 该框架为资源受限的边缘智能场景提供了实用且通用的transformer部署解决方案，具有鲁棒性和隐私保护优势

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [47] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine是一个合成表格数据生成框架，通过从可解释模型中提取符号规则嵌入提示词，并采用双粒度过滤策略，在数据稀缺场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成方法（GANs、扩散模型、微调LLMs）需要充足参考数据，在领域特定数据库记录稀缺时效果有限。基于提示的LLMs虽然灵活但难以捕捉数据集特定的特征-标签依赖关系，且生成冗余数据导致下游任务性能下降。

Method: 1) 从可解释模型推导符号"if-then"规则并嵌入提示词，显式指导生成符合领域特定特征分布；2) 应用双粒度过滤策略，抑制过采样模式并选择性精炼稀有但信息丰富的样本以减少分布不平衡。

Result: 在多种回归和分类基准测试中，ReFine consistently outperforms state-of-the-art methods，回归任务R平方绝对提升0.44，分类任务F1分数相对提升10.0%。

Conclusion: ReFine框架通过规则引导和智能过滤，有效解决了数据稀缺场景下的表格生成问题，显著提升了生成数据的质量和下游任务性能。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [48] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 提出基于机器学习的虚拟服务器能耗估计方法，仅使用虚拟机资源利用率指标即可预测能耗，无需物理功耗测量接口或主机特权访问


<details>
  <summary>Details</summary>
Motivation: 解决虚拟化环境（如云平台）中无法直接测量能耗的关键问题，实现无主机特权访问的能耗估计

Method: 使用梯度提升回归器（Gradient Boosting Regressor），基于虚拟机收集的资源利用率指标来预测通过RAPL测量的主机能耗

Result: 在多样化工作负载实验中实现了高预测精度（R²在0.90到0.97之间），证明了仅使用虚拟机端资源进行能耗估计的可行性

Conclusion: 该方法可为虚拟化环境中的能量感知调度、成本优化和独立于物理主机的能耗估计提供支持

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [49] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 该研究通过实验探索了深度回归模型中的神经缩放定律，发现在扭曲范德瓦尔斯磁体参数估计任务中，损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数在1到2之间。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型成功证明了神经缩放定律的重要性，但这些定律在深度回归模型中的应用仍然很少被探索。研究者希望填补这一空白，为深度回归模型的发展提供理论指导。

Method: 使用扭曲范德瓦尔斯磁体的参数估计模型，在不同架构（全连接网络、残差网络、视觉变换器）上实验研究损失与训练数据集大小、模型容量之间的幂律关系。

Result: 观察到损失与训练数据集大小和模型容量之间存在幂律关系，缩放指数范围在1到2之间，具体值取决于回归参数和模型细节。

Conclusion: 一致的缩放行为和大缩放指数表明，深度回归模型的性能可以随着数据量的增加而显著提升，这为资源管理和模型开发提供了重要指导。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [50] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA是一种能够估计数据集内在维度并重建原始数据的自编码器，通过投影重建损失项和重加权双CancelOut层结构，在线性和非线性流形数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在估计数据集内在维度时往往无法同时提供高质量的数据重建能力，需要一种既能准确估计内在维度又能有效重建原始数据的统一框架。

Method: 提出IDEA自编码器，使用重加权双CancelOut层构建潜在空间，引入投影重建损失项来指导训练，通过连续评估去除潜在维度后的重建质量来确定内在维度。

Result: 在理论基准测试中表现出良好的准确性和高通用性，在垂直解析一维自由表面流数值解数据上成功估计内在维度并重建原始解。

Conclusion: IDEA是一个强大且通用的工具，能够同时准确估计数据集内在维度和重建原始数据，在理论和实际应用中都表现出色。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [51] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: SMoE-VAE架构在无监督专家路由方面表现优于有监督基线，能够识别出超越人工定义类别边界的有意义子类别结构。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络内部组织是深度学习可解释性的基本挑战，探索稀疏混合专家变分自编码器架构来解决这一挑战。

Method: 在QuickDraw数据集上测试SMoE-VAE模型，比较无监督专家路由与基于真实标签的有监督基线，使用t-SNE可视化和重建分析。

Result: 无监督路由始终获得更好的重建性能，专家学会识别有意义且常超越人工类别边界的子类别结构，数据集大小影响专家专业化程度。

Conclusion: MoE模型能够发现与模型目标更一致的基本数据结构，为设计高效MoE架构提供了关于数据量与专家专业化权衡的指导。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [52] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出了AODL方法，通过低秩编码模型解决多字典场景下的字典学习问题，在保证重建质量的同时获得更稀疏的解，并证明了方法的收敛性和泛化性能


<details>
  <summary>Details</summary>
Motivation: 传统稀疏字典编码在处理多字典场景时面临字典和编码系数联合学习的挑战，特别是编码系数需要处理所有字典原子的组合，计算复杂度高

Method: 提出低秩编码模型，使用凸松弛解决方案AODL，通过稀疏编码矩阵和学习字典之间的交替优化进行求解

Result: AODL在相同重建质量下比非低秩和固定字典基线获得高达90%更稀疏的解，学习到的字典能够揭示训练样本中的可解释模式

Conclusion: AODL方法有效解决了多字典学习的数据复杂度问题，在合成和真实数据集上都表现出优异的重建和缺失值填补性能

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [53] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文提出了一个形式化理论，证明概率有限自动机(PFAs)可以通过符号前馈神经网络精确模拟。该架构使用向量表示状态分布，随机矩阵表示转移，通过矩阵向量乘积实现概率状态传播，提供并行、可解释、可微分的PFA动态模拟。


<details>
  <summary>Details</summary>
Motivation: 统一概率自动机理论与神经网络架构，在严格的代数框架下弥合符号计算与深度学习之间的差距，实现概率自动机的可学习神经网络模拟。

Method: 使用符号前馈神经网络架构，将状态分布表示为向量，转移表示为随机矩阵，通过矩阵向量乘积进行概率状态传播，实现非循环的软更新模拟。采用标准梯度下降优化在标记序列数据上进行训练。

Result: 证明了PFAs与特定类别神经网络的等价性，展示了符号模拟器不仅具有表达性而且可学习：通过训练能够精确恢复真实PFAs的行为。

Conclusion: 该工作建立了概率自动机理论与神经架构的统一框架，实现了从符号计算到深度学习的桥梁，为概率自动机的神经网络模拟提供了理论基础和实用方法。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [54] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP是一种新颖的联邦学习算法，通过随机投影技术和ADMM优化框架结合，在保护隐私的同时降低通信成本，并提供强差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护用户隐私方面面临挑战，特别是在对抗潜在攻击和管理通信成本方面需要改进。

Method: 采用随机投影技术降低模型参数维度，结合ADMM优化框架，在参数传输到中央服务器前进行降维处理。

Result: 实验结果显示FedRP不仅保持高模型精度，在隐私保护和通信效率方面均优于现有方法，包括传统差分隐私方法和FedADMM。

Conclusion: FedRP算法成功解决了联邦学习中的隐私保护和通信成本问题，提供了有效的隐私保护解决方案。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [55] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 评估VBLL与TabPFN集成在不确定性校准中的性能，发现原始TabPFN在所有数据集上始终优于VBLL集成版本


<details>
  <summary>Details</summary>
Motivation: 在医疗诊断等安全关键应用中，可靠的不确定性估计至关重要，需要评估先进的不确定性估计方法VBLL与新兴的表格基础模型TabPFN的集成效果

Method: 在三个基准医疗表格数据集上对比原始TabPFN和VBLL集成版本的性能，使用变分贝叶斯最后一层(VBLL)方法与TabPFN进行集成

Result: 与预期相反，原始TabPFN在所有数据集的不确定性校准方面始终优于VBLL集成的TabPFN

Conclusion: VBLL集成并未改善TabPFN的不确定性校准性能，原始TabPFN在不确定性估计方面表现更优

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [56] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR是一个基于Kolmogorov Arnold Networks的新型符号回归框架，采用分治方法，结合深度学习技术和简化策略，能够恢复Feynman SRSD数据集的真实方程，并能精确模拟生物过程系统动力学。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归通常使用遗传编程方法，本文旨在利用深度学习技术、特定KAN网络和简化策略来改进符号回归的准确性和效率，特别是在科学发现和工程系统动态建模方面。

Method: 使用Kolmogorov Arnold Networks (KANs)构建符号回归框架，采用分治方法，结合深度学习技术、平移对称性和可分离性等简化策略，并与神经控制微分方程结合进行动态建模。

Result: 成功恢复了Feynman SRSD数据集的真实方程，并精确模拟了硅内生物过程系统的动力学，为其他工程系统的动态建模开辟了新途径。

Conclusion: KAN-SR框架在符号回归和动态系统建模方面表现出色，结合深度学习技术和简化策略能够有效解决传统方法面临的挑战，具有广泛的应用前景。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [57] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的贝叶斯联邦学习个性化框架，通过将全局模型投影到用户本地模型邻域，实现全局泛化与本地特化的可调权衡，计算成本低且效果显著


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯联邦学习方法通常依赖MCMC采样或变分推断，需要个性化机制来适应异构数据分布，但计算成本较高。本文旨在开发一种更高效的个人化方法

Method: 采用信息几何投影框架，将全局模型投影到用户本地模型邻域，证明该投影等价于统计流形上的重心计算，从而获得闭式解。结合改进的变分在线牛顿优化器(IVON)应用于变分学习设置

Result: 在异构数据分布下的实证评估表明，该方法能有效平衡全局和本地性能，且计算开销极小

Conclusion: 提出的信息几何投影框架为贝叶斯联邦学习提供了一种计算高效的个人化方法，实现了全局泛化与本地特化的最优权衡

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [58] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 提出了BenchECG标准化基准和xECG模型，通过统一的评估框架解决ECG基础模型缺乏公平比较的问题，xECG在多个数据集和任务上表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有ECG基础模型研究缺乏一致的评估标准，使用不同的任务选择和数据集，阻碍了公平比较和进展

Method: 开发BenchECG标准化基准，包含全面的公开ECG数据集和多样化任务；提出基于xLSTM的xECG模型，使用SimDINOv2自监督学习进行训练

Result: xECG在BenchECG基准测试中获得最佳分数，是唯一在所有数据集和任务上都表现优异的公开可用模型

Conclusion: BenchECG通过标准化评估实现了严格比较，xECG为未来ECG基础模型设立了新的性能基准，有望加速ECG表示学习领域的进展

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [59] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出了一个联邦多智能体强化学习框架Fed-MARL，用于6G超密集边缘网络的隐私保护、实时资源管理，通过跨层协调MAC层和应用层实现能效优化。


<details>
  <summary>Details</summary>
Motivation: 6G网络向超密集智能边缘环境发展，需要在严格隐私、移动性和能耗约束下实现高效资源管理，传统集中式方法面临隐私泄露和可扩展性挑战。

Method: 采用深度循环Q网络(DRQN)让每个智能体基于本地观测学习去中心化策略，结合椭圆曲线Diffie-Hellman密钥交换的安全聚合协议保护隐私，将问题建模为部分可观测多智能体马尔可夫决策过程。

Result: 仿真结果表明Fed-MARL在任务成功率、延迟、能效和公平性方面优于集中式MARL和启发式基线方法，同时确保强大的隐私保护和动态6G边缘网络中的可扩展性。

Conclusion: Fed-MARL框架为6G边缘网络提供了一种有效的隐私保护资源管理解决方案，能够同时优化多个性能指标并满足6G特定服务要求。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [60] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 提出了一种通过神经网络连续函数近似来重新优化表面码解码器的方法，解决了传统方法因非唯一性预测只能获取误差概率分布的问题


<details>
  <summary>Details</summary>
Motivation: 量子纠错码中表面码虽然具有高错误阈值，但传统解码器由于输入预测的非唯一性只能获取误差概率分布，需要改进解码精度

Method: 使用神经网络数学插值近似综合征测量，将解码问题重构为回归问题，通过连续函数重新优化解码器模型

Result: 在码距5和7的多层感知机解码器，以及卷积神经网络、循环神经网络和Transformer解码器上均显示出精度提升，证明了方法的普适有效性

Conclusion: 将表面码解码问题重新构建为可通过深度学习解决的回归问题是一个有效的策略，该方法独立于码距和网络架构都表现出优越性能

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [61] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 本文研究了深度残差网络在标准随机初始化下的梯度训练动态，证明了当深度L趋近无穷时，训练动态收敛到神经平均ODE，并给出了输出与极限之间的误差界。


<details>
  <summary>Details</summary>
Motivation: 研究深度残差网络在标准随机初始化下的训练动态，特别是当网络深度很大时的极限行为，这对于理解Transformer等实际模型的理论基础具有重要意义。

Method: 采用数学分析方法，通过随机初始化的前向和后向传播行为作为随机平均ODE的近似，利用传播混沌理论保持训练动态中的这种行为。

Result: 证明了深度残差网络训练动态收敛到神经平均ODE，给出了误差界O(1/L + α/√(LM))，并验证了该速率的紧性。对于两层感知器块的特殊情况，确定了导致完全特征学习的唯一残差缩放比例。

Conclusion: 深度残差网络在标准随机初始化下具有明确的极限训练动态，不同的残差缩放参数会导致不同的特征学习机制（完全特征学习或惰性ODE机制），这为理解深度网络的理论性质提供了新的数学视角。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [62] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了一个可扩展的框架，用于学习高分辨率3D物理模拟的确定性和概率性神经代理模型，采用混合CNN-Transformer架构，在速度和准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决高分辨率3D物理模拟中计算成本高、内存需求大的问题，需要开发高效且可扩展的神经代理模型来替代传统的数值模拟方法。

Method: 引入混合CNN-Transformer骨干架构，支持在小块模拟域上进行预训练，然后融合获得全局解，可选地通过序列到序列模型包含长程依赖关系，减少内存和计算需求。

Result: 在14种不同类型3D PDE动力学学习任务中显著优于基线方法，可扩展到512^3空间分辨率的高分辨率各向同性湍流，并能作为扩散模型生成不同雷诺数下高度湍流3D通道流的概率样本。

Conclusion: 该框架为高分辨率3D物理模拟提供了高效且可扩展的神经代理解决方案，在保持准确性的同时大幅降低了计算和内存需求，展现了在复杂物理系统建模中的广泛应用潜力。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [63] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出了一种新的集成学习框架，通过将margin方差纳入损失函数并重新参数化集成权重到单位球面，解决了传统基于margin的集成方法忽略方差和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于margin的集成方法主要关注最大化期望margin而忽略margin方差的重要性，这限制了模型的泛化能力并增加了过拟合风险，特别是在噪声或不平衡数据集中。同时，传统方法在概率单纯形中优化集成权重存在计算效率低和可扩展性挑战。

Method: 提出新的集成学习框架，将margin方差显式纳入损失函数，联合优化负期望margin及其方差。通过将集成权重重新参数化到单位球面，简化优化过程并提高计算效率。

Result: 在多个基准数据集上的广泛实验表明，该方法始终优于传统的基于margin的集成技术，证明了其有效性和实用性。

Conclusion: 该框架通过同时考虑期望margin和margin方差，显著提高了集成学习的鲁棒性和泛化性能，同时通过重新参数化技术改善了计算效率，为大规模问题提供了实用的解决方案。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [64] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 基于机器学习的飞机机翼疲劳寿命预测管道，通过飞行参数快速估算疲劳寿命，减少传统有限元模拟的计算成本


<details>
  <summary>Details</summary>
Motivation: 传统疲劳寿命预测方法耗时且复杂，需要多团队协作和大量有限元模拟。机器学习方法可以提供快速估算，作为传统方法的补充，提高效率并降低资源需求

Method: 提出基于机器学习的管道，根据飞机整个运行寿命中不同任务的飞行参数，估算机翼不同位置的疲劳寿命。包括统计验证和不确定性量化

Result: 在真实疲劳寿命估算用例中验证了管道的准确性，能够提供精确预测

Conclusion: 该机器学习管道是对传统方法的有效补充，通过减少昂贵模拟的数量，降低了计算和人力资源需求

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [65] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 本文通过系统性评估发现：简单的提示词注入攻击对LLM同行评审高度有效（可达100%接受率），且LLM评审普遍存在接受偏向（>95%），这对LLM在学术评审中的应用讨论具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 针对近期关于作者使用隐藏提示词注入操纵LLM同行评审分数的报道，研究此类攻击的可行性和技术成功率，因为其存在将对LLM在学术评审中的使用辩论产生重大影响。

Method: 使用多种LLM对2024年ICLR会议的1000篇论文评审进行系统性评估，分析提示词注入攻击的效果和LLM评审的偏向性。

Result: 1) 非常简单提示词注入攻击高度有效，最高可达100%接受率；2) LLM评审普遍存在接受偏向，许多模型接受率超过95%。

Conclusion: 研究结果对当前关于LLM在同行评审中使用的讨论具有重大影响，揭示了系统漏洞和固有偏向问题。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [66] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 提出基于神经推荐系统的迁移学习框架，利用COSMO-RS模拟数据和稀疏实验数据，准确预测离子液体的五种关键热物理性质


<details>
  <summary>Details</summary>
Motivation: 离子液体具有可定制的物理化学性质，但由于化学设计空间巨大和实验数据有限，准确预测其热物理性质仍然具有挑战性

Method: 两阶段方法：首先在固定温压下的COSMO-RS模拟数据上预训练神经推荐系统模型，学习阴阳离子的性质特异性结构嵌入；然后使用这些嵌入和变温变压实验数据微调简单前馈神经网络

Result: 预训练的密度、粘度和热容模型用于微调所有五种目标性质模型，其中四种性质的性能显著提升。模型对未见过的离子液体具有稳健外推能力，可为超过70万种离子液体组合提供性质预测

Conclusion: 该工作展示了结合模拟数据和迁移学习来克服实验数据稀疏性的有效性，为过程设计中的离子液体筛选提供了可扩展的解决方案

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [67] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC离线强化学习算法，直接从历史数据学习作业车间调度策略，无需在线交互，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统在线RL方法需要大量模拟环境交互且样本效率低，无法捕捉真实世界复杂性，需要直接从历史数据学习的离线方法

Method: CDQAC结合分位数critic和延迟策略更新，估计每个机器-操作对的回报分布而非直接选择，能够从次优数据中改进

Result: CDQAC在多种数据源上表现优异，始终优于原始数据生成启发式方法，超越最先进的离线和在线RL基线，仅需10-20个训练实例

Conclusion: CDQAC是高效的离线RL调度方法，意外发现在随机启发式生成数据上训练效果优于高质量遗传算法数据

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [68] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 提出了GraphCSVAE框架，通过深度学习、图表示和分类概率推理整合卫星时间序列数据和专家知识，用于建模灾害物理脆弱性，并在孟加拉国和塞拉利昂的灾害案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有灾害风险评估主要关注灾害和暴露度建模，但在物理脆弱性建模方面进展有限，限制了决策者对联合国仙台框架进展的评估能力。

Method: 开发了Graph Categorical Structured Variational Autoencoder (GraphCSVAE)概率框架，整合深度学习、图表示和分类概率推理，使用时间序列卫星数据和专家先验知识，引入弱监督一阶转移矩阵来反映物理脆弱性的时空分布变化。

Result: 在两个灾害频发且社会经济弱势地区（孟加拉国Khurushkul社区和塞拉利昂弗里敦市）成功揭示了灾后物理脆弱性的区域动态变化。

Conclusion: 该工作为局部时空审计和灾后风险减少的可持续策略提供了有价值的见解，能够有效建模物理脆弱性的时空变化。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [69] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 提出基于ARIMA模型启发的简单卷积模块ARMA，用于长期时间序列预测，包含趋势捕捉和局部变化修正两个组件，直接进行多步预测，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型需要迭代多步预测且难以扩展到多变量设置，需要一种简单有效的长期时间序列预测方法。

Method: 设计双组件卷积模块：一个卷积组件捕捉趋势（自回归），另一个卷积组件修正局部变化（移动平均），直接进行多步预测，易于扩展到多变量场景。

Result: 在9个常用基准数据集上实验表明，ARMA方法在具有强趋势变化的数据集上表现优异，同时保持架构简单性。分析显示该模块能自然编码绝对位置信息。

Conclusion: ARMA模块是长期时间序列预测的有效解决方案，具有竞争性的准确性、架构简单性，并可作为序列模型中位置嵌入的轻量级替代方案。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [70] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于结构保持数字孪生的自适应源定位机器学习框架，结合条件神经Whitney形式和变压器算子学习，实现实时轨迹规划和数据同化


<details>
  <summary>Details</summary>
Motivation: 解决复杂流体输运系统中源定位问题，传统方法难以在实时性和物理约束保持之间取得平衡，需要开发既能保持物理守恒律又能适应实时传感器数据的智能方法

Method: 使用条件神经Whitney形式(CNWF)构建数字孪生，耦合有限元外微积分(FEEC)的数值保证与基于变压器的算子学习，采用交错方案交替评估数字孪生和应用Lloyd算法指导传感器布置

Result: 实验显示在复杂几何形状中，当强制执行物理约束时，相比物理不可知变压器架构具有更高的精度，结构保持为源识别提供了有效的归纳偏置

Conclusion: 结构保持的数字孪生框架能够有效实现自适应源定位，保持离散守恒性并实时适应传感器数据，为复杂环境系统中的源识别问题提供了物理可实现的解决方案

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [71] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架来形式化数据集压缩问题，将传统基于泛化性能的方法扩展到更广泛的分布近似目标，包括鲁棒性、隐私保护等特性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法主要关注泛化性能，缺乏统一的理论框架。作者希望建立一个更通用的形式化定义，能够涵盖多种目标并量化分布近似质量。

Method: 使用差异度概念来量化概率分布之间的距离，构建统一框架来形式化数据集压缩问题，将任务扩展到包括鲁棒性、隐私保护等多重目标。

Result: 提出了一个能够统一现有数据集压缩方法的理论框架，扩展了压缩目标的范围，为不同应用场景下的数据集压缩提供了形式化基础。

Conclusion: 该框架为数据集压缩提供了更全面和形式化的理论基础，使得压缩方法能够针对不同应用需求（如泛化、鲁棒性、隐私等）进行优化设计。

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [72] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: 本文研究了对比学习在ECG预训练中队列组成的影响，发现多中心多样化队列虽然提高分布内准确率，但会降低分布外泛化能力，并提出了IDB策略来增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习自监督预训练策略对队列组成的依赖性，特别是不同人口统计学、健康状况和人群多样性对下游预测任务性能的影响。

Method: 提出了CAPE基础模型，在四大洲五个队列（n=5,203,352）上进行预训练，系统评估预训练队列特征对下游性能的影响，并提出了In-Distribution Batch (IDB)策略来保持队列内一致性。

Result: 发现下游性能取决于预训练队列的分布特性，包括人口统计学和健康状况。多中心多样化队列提高了分布内准确率，但通过编码队列特定伪影降低了对比学习方法的分布外泛化能力。

Conclusion: IDB策略能够保持预训练期间的队列内一致性并增强分布外鲁棒性，为开发临床公平和可泛化的基础模型提供了重要见解。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [73] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文建立了无限维希尔伯特空间中整流流的严格函数化表述，扩展了整流流到无限维空间的框架，并展示了其在函数流匹配和概率流ODE中的自然应用。


<details>
  <summary>Details</summary>
Motivation: 许多在有限维欧几里得空间中开发的生成模型在无限维设置中都有函数化推广，但整流流向无限维空间的扩展尚未被探索。

Method: 基于无限维空间中连续性方程的叠加原理，建立了整流流的函数化表述框架，并将其扩展到函数流匹配和函数概率流ODE。

Result: 实验证明该方法相比现有函数生成模型具有更优越的性能，同时移除了现有理论中的限制性测度理论假设。

Conclusion: 成功建立了无限维空间中整流流的严格数学框架，为函数生成模型提供了新的理论基础和实用方法。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [74] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: 提出Vendi信息增益(VIG)主动学习策略，通过考虑数据集整体的预测不确定性来选择信息量和多样性兼备的图像，在Snapshot Serengeti数据集上仅用不到10%的标签就达到了接近全监督的预测精度。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱监测生物多样性已成为生态研究的重要手段，但由于标注资源有限，识别捕获图像数据中的物种仍然是主要瓶颈。传统主动学习方法通常只关注个体预测的不确定性，而忽略了整个数据集的不确定性。

Method: 引入新的主动学习策略Vendi信息增益(VIG)，基于图像对整个数据集预测不确定性的影响来选择图像，同时捕捉信息量和多样性。

Result: 在Snapshot Serengeti数据集上，VIG使用不到10%的标签就实现了接近全监督的预测准确性，在各种指标和批次大小上都持续优于标准基线方法，并在特征空间中收集了更多样化的数据。

Conclusion: VIG具有超越生态学的广泛适用性，研究结果突显了其在数据有限环境下进行生物多样性监测的价值。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [75] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO是一种针对掩码扩散大语言模型的RL框架，通过部分真实推理轨迹的修复引导来提升探索效率和样本利用率，在数学推理任务上取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决传统RL方法在LLM对齐中的探索挑战——稀疏奖励信号和样本浪费问题，利用dLLM的修复能力来指导探索

Method: 提出IGPO框架，在在线采样中策略性地插入部分真实推理轨迹，结合监督微调、基于熵的过滤等技术

Result: 在GSM8K、Math500和AMC三个数学基准测试中取得显著提升，为全注意力掩码dLLM创造了新的最先进结果

Conclusion: 修复引导的RL方法能够有效提升dLLM的探索效率和性能，为LLM与RL的结合提供了新思路

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [76] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe是一种高效的softmax注意力近似方法，通过语义聚类和多极展开技术降低Transformer的二次计算复杂度，在保持性能的同时显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列处理中的二次计算复杂度问题，传统方法要么只对键进行聚类，要么使用统一聚类，未能充分考虑注意力机制中查询和键空间的不对称性。

Method: 结合语义聚类和计算物理中的多极展开技术，分别对查询和键在学习的表示空间中进行聚类，采用层次化两阶段注意力机制，在质心近似基础上增加偶极校正来捕捉簇内方向方差。

Result: 在8k上下文长度下比CUDNN Flash Attention快3倍，相对平方误差低于20%；在30M参数模型上使用16k上下文进行端到端预训练时，运行时间减少12.2%，性能损失仅0.36%。

Conclusion: 多极近似方法为高效Transformer预训练提供了可行方案，在保持模型性能的同时显著降低了计算成本，特别适用于处理长序列文本。

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [77] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 使用过程挖掘和机器学习技术对ERTMS/ETCS L2铁路系统进行运行时控制流异常检测和定位，提高系统韧性


<details>
  <summary>Details</summary>
Motivation: 随着铁路系统复杂性和关键性增加，尽管有严格的验证和认证流程，运行时仍可能出现异常，需要增强系统韧性来应对设计时未知的故障、修改和网络威胁

Method: 采用过程挖掘技术从执行轨迹中学习系统实际控制流，进行在线一致性检查；使用无监督机器学习进行异常定位，将偏差关联到关键系统组件

Result: 在RBC/RBC交接参考场景中测试，方法能够以高准确性、效率和可解释性检测和定位异常

Conclusion: 过程挖掘和机器学习相结合的方法能有效增强铁路控制系统的运行时韧性和安全性

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [78] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究了Local SGD中外部优化器的作用，证明了新的收敛保证，发现调整外部学习率可以在优化误差和随机梯度噪声方差之间权衡，并弥补内部学习率的不良调整。理论表明外部学习率有时应大于1，动量调整的外部学习率也有类似作用。外部优化器的加速改进了收敛率，实验验证了理论。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要在大批量、分布式数据和并行计算硬件上训练，通信成为主要瓶颈。Local SGD能减少通信开销，但现有研究主要关注本地优化过程的超参数，对外部优化器及其超参数的选择不够清晰。

Method: 通过理论分析证明Local SGD的收敛保证，研究外部学习率的作用机制，扩展到使用动量的外部优化器，分析外部优化器的加速效果，并提出新的数据相关分析框架。

Result: 理论表明调整外部学习率可以权衡优化误差和噪声方差，弥补内部学习率的不良调整，外部学习率有时应大于1。外部加速改进了收敛率，优于现有本地加速算法。实验验证了理论发现。

Conclusion: 外部优化器在Local SGD中扮演关键角色，适当调整外部学习率和动量参数能显著提升性能，外部加速技术能有效改善通信轮次相关的收敛率，为分布式机器学习提供了重要理论指导。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [79] [A Smooth Computational Transition in Tensor PCA](https://arxiv.org/abs/2509.09904)
*Zhangsong Li*

Main category: math.ST

TL;DR: 提出了一种基于加权超图计数的高效张量PCA算法，在固定阶数p≥3时，当信噪比为λn^{-p/4}且λ=Ω(1)时，算法成功运行且时间复杂度为n^{C+o(1)}，其中C是依赖于λ的常数。


<details>
  <summary>Details</summary>
Motivation: 改进现有的张量PCA算法性能，解决Sum-of-Squares层次和Kikuchi层次方法存在的多对数因子开销问题，验证关于信噪比与计算成本之间平滑权衡的猜想。

Method: 基于特定加权超图族的计数方法，通过分析超图结构来设计高效的张量主成分分析算法。

Result: 算法在信噪比λn^{-p/4}条件下成功运行，时间复杂度为n^{C+o(1)}，相比先前方法改进了多对数因子，并证实了信噪比与计算成本之间的平滑权衡关系。

Conclusion: 提出的加权超图计数方法为张量PCA问题提供了更高效的解决方案，验证了理论猜想，展示了算法性能与信噪比参数之间的定量关系。

Abstract: We propose an efficient algorithm for tensor PCA based on counting a specific
family of weighted hypergraphs. For the order-$p$ tensor PCA problem where $p
\geq 3$ is a fixed integer, we show that when the signal-to-noise ratio is
$\lambda n^{-\frac{p}{4}}$ where $\lambda=\Omega(1)$, our algorithm succeeds
and runs in time $n^{C+o(1)}$ where $C=C(\lambda)$ is a constant depending on
$\lambda$. This algorithm improves a poly-logarithmic factor compared to
previous algorithms based on the Sum-of-Squares hierarchy \cite{HSS15} or based
on the Kikuchi hierarchy in statistical physics \cite{WEM19}. Furthermore, our
result shows a smooth tradeoff between the signal-to-noise ratio and the
computational cost in this problem, thereby confirming a conjecture posed in
\cite{KWB22}.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [80] [Matrix-free Neural Preconditioner for the Dirac Operator in Lattice Gauge Theory](https://arxiv.org/abs/2509.10378)
*Yixuan Sun,Srinivas Eswar,Yin Lin,William Detmold,Phiala Shanahan,Xiaoye Li,Yang Liu,Prasanna Balaprakash*

Main category: hep-lat

TL;DR: 提出基于算子学习的框架构建线性映射作为有效预处理器，用于加速格点QCD中稀疏病态线性系统的共轭梯度求解，无需显式矩阵，在Schwinger模型中有效降低条件数并减少约一半迭代次数。


<details>
  <summary>Details</summary>
Motivation: 格点量子色动力学中求解稀疏但病态的Hermitian正定线性系统需要耗时的迭代方法，现有多网格预处理器构造复杂且计算开销大，需要更高效的预处理器构建方法。

Method: 利用算子学习技术构建线性映射作为预处理器，不依赖于原始线性系统或预处理器的显式矩阵，实现高效模型训练和在CG求解器中的应用。

Result: 在Schwinger模型U(1)规范理论中，该预处理器方案有效降低了线性系统的条件数，在相关参数范围内将收敛所需迭代次数减少约一半，并展示了零样本学习能力。

Conclusion: 基于算子学习的预处理器框架为格点QCD中的线性系统求解提供了有效的加速方案，具有零样本泛化能力，可应用于不同尺寸的规范场配置。

Abstract: Linear systems arise in generating samples and in calculating observables in
lattice quantum chromodynamics~(QCD). Solving the Hermitian positive definite
systems, which are sparse but ill-conditioned, involves using iterative
methods, such as Conjugate Gradient (CG), which are time-consuming and
computationally expensive. Preconditioners can effectively accelerate this
process, with the state-of-the-art being multigrid preconditioners. However,
constructing useful preconditioners can be challenging, adding additional
computational overhead, especially in large linear systems. We propose a
framework, leveraging operator learning techniques, to construct linear maps as
effective preconditioners. The method in this work does not rely on explicit
matrices from either the original linear systems or the produced
preconditioners, allowing efficient model training and application in the CG
solver. In the context of the Schwinger model U(1) gauge theory in 1+1
spacetime dimensions with two degenerate-mass fermions), this preconditioning
scheme effectively decreases the condition number of the linear systems and
approximately halves the number of iterations required for convergence in
relevant parameter ranges. We further demonstrate the framework learns a
general mapping dependent on the lattice structure which leads to zero-shot
learning ability for the Dirac operators constructed from gauge field
configurations of different sizes.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [81] [Realistic UE Antennas for 6G in the 3GPP Channel Model](https://arxiv.org/abs/2509.10357)
*Simon Svendsen,Dimitri Gold,Christian Rom,Volker Pauli,Vuokko Nurmela*

Main category: eess.SP

TL;DR: 3GPP Rel.19对TR 38.901信道模型进行了重要更新，引入了更真实的UE天线建模和用户遮挡效应，基于智能手机的高保真仿真和测量数据，为6G技术评估提供更准确的性能分析框架。


<details>
  <summary>Details</summary>
Motivation: 6G发展需要更精确的信道模型来评估新技术性能，特别是手持设备的UE天线特性和用户遮挡效应，现有模型在这些方面存在不足。

Method: 通过高保真仿真和测量参考智能手机在多个频段的数据，建立包含定向天线模式、实际天线布局、极化效应和单元特定遮挡的更真实框架。

Result: 新模型使链路级和系统级仿真更贴近真实设备行为，支持跨行业和研究的一致性能评估。

Conclusion: 更新后的3GPP信道模型为6G技术提供了更准确的评估基础，推动了标准化和产业发展。

Abstract: The transition to 6G has driven significant updates to the 3GPP channel
model, particularly in modeling UE antennas and user-induced blockage for
handheld devices. The 3GPP Rel.19 revision of TR 38.901 introduces a more
realistic framework that captures directive antenna patterns, practical antenna
placements, polarization effects, and element-specific blockage. These updates
are based on high-fidelity simulations and measurements of a reference
smartphone across multiple frequency ranges. By aligning link- and system-level
simulations with real-world device behavior, the new model enables more
accurate evaluation of 6G technologies and supports consistent performance
assessment across industry and research.

</details>


### [82] [Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy](https://arxiv.org/abs/2509.09695)
*Fabio Magarelli,Geraldine B. Boylan,Saeed Montazeri,Feargal O'Sullivan,Dominic Lightbody,Minoo Ashoori,Tamara Skoric Ceranic,John M. O'Toole*

Main category: eess.SP

TL;DR: 通过机器学习竞赛开发新生儿脑电图背景模式严重程度分类模型，发现深度学习模型在验证集上泛化能力更好，但所有模型在未见数据上性能都显著下降，强调了大规模多样化数据集和保留验证集的重要性。


<details>
  <summary>Details</summary>
Motivation: 机器学习有潜力改善高危新生儿脑功能监测，但缺乏高质量标注数据。通过竞赛形式可以促进数据共享、模型比较和多样化专业知识的汇集。

Method: 收集了102名新生儿353小时脑电图数据，分为训练、测试和保留验证集。创建基于web的竞赛平台，让研究者开发EEG背景模式严重程度分类的ML模型。赛后对前4名模型在独立验证集上进行离线评估。

Result: 特征工程模型在测试集上排名第一，但深度学习模型在验证集上泛化更好。所有方法在验证集上的性能相比测试集都有显著下降，表明模型泛化到未见数据的挑战。

Conclusion: 研究强调了在大型多样化数据集上训练ML模型的重要性，以及保留验证集在新生儿EEG研究中的必要性。开放获取数据和协作ML开发有助于营造合作研究环境，加速新生儿神经监测临床决策支持工具的开发。

Abstract: Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.

</details>


### [83] [FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification](https://arxiv.org/abs/2509.10082)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Nhi Tran,Sharmony B. Kelly,Gari D. Clifford,Robert Galinsky,Faezeh Marzbanrad*

Main category: eess.SP

TL;DR: FetalSleepNet是首个用于胎儿羊脑电图睡眠分期的深度学习框架，通过迁移学习和频谱均衡技术，在胎儿EEG数据上实现了86.6%的准确率，为胎儿脑发育监测提供了自动化工具。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑电图获取复杂且解读困难，但准确的睡眠分期有助于早期发现与妊娠并发症相关的异常脑成熟，如缺氧或宫内生长受限。

Method: 使用轻量级深度神经网络，通过从成人EEG的迁移学习训练胎儿羊EEG数据，采用频谱均衡域适应策略减少跨域不匹配。

Result: 完全微调结合频谱均衡取得了最佳性能（准确率86.6%，宏观F1分数62.5），优于基线模型。

Conclusion: FetalSleepNet是首个专门为胎儿EEG自动睡眠分期开发的深度学习框架，其轻量设计适合低功耗实时可穿戴胎儿监测系统部署，可作为标签引擎支持大规模弱/半监督标注。

Abstract: Introduction: This study presents FetalSleepNet, the first published deep
learning approach to classifying sleep states from the ovine
electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and
laborious to interpret consistently. However, accurate sleep stage
classification may aid in the early detection of abnormal brain maturation
associated with pregnancy complications (e.g. hypoxia or intrauterine growth
restriction).
  Methods: EEG electrodes were secured onto the ovine dura over the parietal
cortices of 24 late gestation fetal sheep. A lightweight deep neural network
originally developed for adult EEG sleep staging was trained on the ovine EEG
using transfer learning from adult EEG. A spectral equalisation-based domain
adaptation strategy was used to reduce cross-domain mismatch.
  Results: We demonstrated that while direct transfer performed poorly, full
fine tuning combined with spectral equalisation achieved the best overall
performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming
baseline models.
  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep
learning framework specifically developed for automated sleep staging from the
fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier
functions as a label engine, enabling large scale weak/semi supervised labeling
and distillation to facilitate training on less invasive signals that can be
acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.
FetalSleepNet's lightweight design makes it well suited for deployment in low
power, real time, and wearable fetal monitoring systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态检索增强生成框架(MM-RAG)，用于自然灾害后房屋损坏评估，通过双分支编码器和跨模态交互模块实现图像和文本的语义对齐，在检索准确率和损坏严重程度分类指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自然灾害后准确的房屋损坏评估对于保险理赔响应和资源规划至关重要，需要结合图像和文本信息进行综合分析。

Method: 采用双分支多模态编码器结构：图像分支使用ResNet和Transformer提取建筑损坏特征，文本分支使用BERT检索器处理文本向量化；集成跨模态交互模块通过多头注意力实现语义对齐；引入模态注意力门控机制动态控制生成过程中的视觉证据和文本先验信息作用。

Result: 在检索准确率和损坏严重程度分类指标上表现优异，Top-1检索准确率提升了9.6%。

Conclusion: 该多模态检索增强生成框架能够有效整合视觉和文本信息，实现端到端的图像理解和政策匹配，为灾后房屋损坏评估提供了有效的解决方案。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [85] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的噪声历史文档文本提取集成框架，通过多图像增强变体转录和Needleman Wunsch对齐器融合输出，提高转录准确性4个百分点


<details>
  <summary>Details</summary>
Motivation: 解决噪声历史文档中文本提取的稳定性问题，提高转录准确性和可靠性

Method: 使用Gemini 2.0 Flash对多个增强图像变体进行转录，然后通过自定义Needleman Wunsch风格对齐器融合输出，生成共识转录和置信度分数

Result: 在622份宾夕法尼亚州死亡记录数据集上，相比单次转录基线，准确率提高了4个百分点；填充和模糊处理对提升准确性最有效，网格扭曲扰动最适合区分高低置信度情况

Conclusion: 该方法简单、可扩展，可立即部署到其他文档集合和转录模型，为历史文档数字化提供了有效的解决方案

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [86] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI是一个从数据中学习可控制、可提示的世界模型的系统，通过概率预测、结构提取和集成三个步骤的循环，不断改进模型能力并创建新的控制机制


<details>
  <summary>Details</summary>
Motivation: 构建能够从视频数据中学习丰富可控世界模型的系统，支持类似LLM的通用提示语言，实现更好的视频理解和预测

Method: 三步循环：1) 构建概率图模型Psi；2) 通过因果推断零样本提取底层低维结构；3) 将结构转换为新token类型并集成回训练中

Result: 在1.4万亿token的视频数据上训练，实现了最先进的光流、自监督深度和对象分割，支持完整的预测改进循环

Conclusion: PSI系统通过循环集成结构学习，成功构建了可控、可提示的世界模型，在视频理解和预测任务上表现出色

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [87] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 通过智能手机应用利用深度学习分析红眼反射图像，实现儿童视力筛查，准确率达90%


<details>
  <summary>Details</summary>
Motivation: 利用智能手机和AI技术重现传统Bruckner测试，使儿童视力筛查更加可达和便捷

Method: 基于深度神经网络模型，训练于眼科医生收集和标注的儿童孔径图像数据

Result: 在未见测试数据上达到90%的准确率，无需专业设备即可获得高可靠性能

Conclusion: 该研究为全球范围内实现可达的儿科视力筛查和早期干预提供了重要基础

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [88] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出了一种基于深度引导的多模态融合方法DGFusion，通过深度感知特征和空间变化的条件融合机制，在自动驾驶语义感知任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的传感器融合方法在处理具有挑战性的环境条件时表现不佳，因为它们通常在整个输入空间范围内统一处理传感器数据，而没有考虑不同传感器在不同深度条件下的可靠性差异。

Method: 提出了DGFusion网络，将多模态分割作为多任务问题处理，利用LiDAR测量作为输入和深度学习的真值。通过辅助深度头学习深度感知特征，编码为空间变化的局部深度标记，与全局条件标记一起动态调整传感器融合策略。

Result: 在具有挑战性的MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。

Conclusion: 深度引导的多模态融合方法能够有效适应不同传感器在不同深度条件下的可靠性变化，显著提升了自动驾驶语义感知在复杂环境中的鲁棒性。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [89] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: 提出WAVE-DETR多模态无人机检测器，融合可见光RGB和声学信号，在Deformable DETR和Wav2Vec2架构基础上实现鲁棒的实时无人机目标检测。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂环境条件下无人机检测的挑战，利用多模态信息（视觉和声学）来提高检测性能，特别是在小尺寸无人机检测方面。

Method: 基于Deformable DETR和Wav2Vec2架构，开发了四种融合配置：门控机制、线性层、MLP和交叉注意力。将声学嵌入与多分辨率特征映射融合，增强目标检测性能。

Result: 最佳的门控融合方法在ARDrone数据集上将Deformable DETR检测器的mAP提高了11.1%到15.3%（小无人机），中型和大型无人机的mAP也有3.27%到5.84%的提升。

Conclusion: 多模态融合（视觉+声学）显著提升了无人机检测性能，特别是在挑战性环境条件下，门控融合是最有效的融合策略。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [90] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD是一个新颖的多模态协作学习框架，通过整合点云、RGB图像和文本语义，实现了卓越的零样本3D异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本3D异常检测方法主要关注点云数据，忽略了RGB图像和文本先验等互补模态提供的丰富语义线索。为了解决数据稀缺、隐私和高标注成本等约束场景下的缺陷检测问题，需要充分利用多模态信息。

Method: 提出了多模态提示学习机制(MPLM)，通过对象无关的解耦文本提示和多模态对比损失增强模态内表示能力和模态间协作学习；设计了协作调制机制(CMM)，通过联合调制RGB图像引导和点云引导分支来充分利用点云和RGB图像的互补表示。

Result: 大量实验证明，MCL-AD框架在零样本3D异常检测中达到了最先进的性能。

Conclusion: 通过多模态协作学习，MCL-AD框架能够有效整合点云、RGB图像和文本语义信息，在零样本3D异常检测任务中取得了优异的表现，为解决数据约束场景下的缺陷检测问题提供了有效解决方案。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [91] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter是首个完全整数化的ViT分割框架，通过系统替换浮点运算、提出新激活函数λ-ShiftGELU、移除L2归一化层和使用最近邻上采样，在保持精度的同时显著减少模型大小和提升推理速度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现优异，但由于高内存占用和计算成本，在资源受限设备上部署受限。量化虽能提升效率，但ViT分割模型在低精度下表现脆弱，量化误差在深度编码器-解码器管道中累积。

Method: 基于Segmenter架构，系统替换浮点运算为整数运算；提出λ-ShiftGELU激活函数处理长尾激活分布；移除L2归一化层；将解码器中的双线性插值替换为最近邻上采样。

Result: I-Segmenter在精度上与FP32基线相差平均5.1%，模型大小减少达3.8倍，推理速度提升达1.2倍。即使在单张校准图像的一次性PTQ下也能保持竞争力。

Conclusion: I-Segmenter是首个完全整数化的ViT分割框架，通过系统优化实现了高效的整数计算，为实际部署提供了实用解决方案。

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [92] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 提出了GLAM模型，通过几何引导的全局和局部对齐方法，解决乳腺X光多视图对应学习问题，在多个数据集上优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有乳腺X光视觉语言模型忽略多视图关系特性，无法像放射科医生那样同时分析双侧视图来处理同侧对应关系，导致几何上下文丢失和预测效果不佳

Method: 利用乳腺X光多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉、视觉-语言的对比学习，学习局部跨视图对齐和细粒度局部特征

Result: 在最大的公开乳腺X光数据集EMBED上预训练后，模型在多个数据集的不同设置下均优于基线方法

Conclusion: GLAM模型通过几何引导的多视图对齐方法，有效提升了乳腺X光视觉语言模型的性能，为早期乳腺癌检测提供了更好的工具

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [93] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本研究将三种先进的时序自监督学习方法应用于3D脑部MRI分析，通过处理可变长度输入和增强空间特征学习，在阿尔茨海默病预测任务中超越了监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病预测的深度学习模型面临标注数据不足、跨数据集泛化能力差以及对不同扫描数量和间隔时间缺乏灵活性的问题。

Method: 采用三种时序自监督学习（SSL）方法，包括时序顺序预测和对比学习，添加了处理可变长度输入的新扩展，并在包含3,161名患者的四个公开数据集上进行预训练。

Result: 在七个下游任务中的六个任务上，自监督学习方法的表现优于监督学习，展示了跨任务和不同输入图像数量及时间间隔的适应性和泛化能力。

Conclusion: 时序自监督学习方法在阿尔茨海默病预测中表现出强大的性能，具有临床应用的潜力，代码和模型已公开。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [94] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: 开发了一种结合多智能体资源管理和大型语言模型自动生成行为场景的混合控制架构，用于人形导览机器人，解决了传统系统手动配置、灵活性差和行为不自然的问题。


<details>
  <summary>Details</summary>
Motivation: 传统导览机器人系统依赖手动调整行为场景，存在配置繁琐、灵活性低和机器人行为不够自然等局限性，需要更自动化和自然的控制方法。

Method: 采用两阶段生成过程：首先生成风格化叙述文本，然后整合非语言动作标签；使用多智能体系统协调并行动作执行和冲突解决，并在主要操作完成后维持默认行为。

Result: 试验结果表明，该方法在自动化和社会机器人控制系统扩展方面具有潜力，能够实现更自然的机器人行为。

Conclusion: 提出的混合控制架构成功克服了传统系统的局限性，为社交机器人控制的自动化和规模化提供了有效解决方案。

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [95] [An Information-Theoretic Framework for Credit Risk Modeling: Unifying Industry Practice with Statistical Theory for Fair and Interpretable Scorecards](https://arxiv.org/abs/2509.09855)
*Agus Sudjianto,Denis Burakov*

Main category: stat.ML

TL;DR: 该论文为信用风险建模中的WoE、IV和PSI指标建立了统一的信息论框架，证明这些行业标准指标是经典信息散度的实例，并首次提供了统计推断和公平性约束的数学基础。


<details>
  <summary>Details</summary>
Motivation: 信用风险建模广泛使用WoE、IV和PSI等指标，但这些指标的理论基础分散且缺乏统一的统计框架，无法进行正式的假设检验和公平性约束。

Method: 通过信息论框架证明IV等于PSI，应用delta方法推导IV和PSI的标准误差，使用深度1的XGBoost树桩进行自动分箱，比较三种编码策略，并利用混合整数规划寻找性能-公平性帕累托前沿。

Result: 所有方法都达到了相当的预测性能（AUC 0.82-0.84），证明了基于信息论的分箱方法比编码选择更重要，同时能够量化不确定性并提供性能-公平性权衡的帕累托解。

Conclusion: 该框架首次为广泛使用的信用风险指标提供了严格的统计基础，为受监管环境中平衡准确性和公平性提供了原则性工具，连接了理论与实践。

Abstract: Credit risk modeling relies extensively on Weight of Evidence (WoE) and
Information Value (IV) for feature engineering, and Population Stability Index
(PSI) for drift monitoring, yet their theoretical foundations remain
disconnected. We establish a unified information-theoretic framework revealing
these industry-standard metrics as instances of classical information
divergences. Specifically, we prove that IV exactly equals PSI (Jeffreys
divergence) computed between good and bad credit outcomes over identical bins.
Through the delta method applied to WoE transformations, we derive standard
errors for IV and PSI, enabling formal hypothesis testing and probabilistic
fairness constraints for the first time. We formalize credit modeling's
inherent performance-fairness trade-off as maximizing IV for predictive power
while minimizing IV for protected attributes. Using automated binning with
depth-1 XGBoost stumps, we compare three encoding strategies: logistic
regression with one-hot encoding, WoE transformation, and constrained XGBoost.
All methods achieve comparable predictive performance (AUC 0.82-0.84),
demonstrating that principled, information-theoretic binning outweighs encoding
choice. Mixed-integer programming traces Pareto-efficient solutions along the
performance-fairness frontier with uncertainty quantification. This framework
bridges theory and practice, providing the first rigorous statistical
foundation for widely-used credit risk metrics while offering principled tools
for balancing accuracy and fairness in regulated environments.

</details>


### [96] [Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance](https://arxiv.org/abs/2509.10166)
*Vladimir Petrovic,Rémi Bardenet,Agnès Desolneux*

Main category: stat.ML

TL;DR: 本文研究了在任意维度下使用蒙特卡洛方法计算单位球面上函数积分的问题，特别关注切片Wasserstein距离的计算。通过分析各种排斥性节点积分方法，推荐在低维使用随机拟蒙特卡洛，高维使用UnifOrtho估计器。


<details>
  <summary>Details</summary>
Motivation: 切片Wasserstein距离在机器学习中作为Wasserstein距离的替代或独立距离度量具有重要意义，但现有数值积分方法在方差减少方面仍有改进空间。本文旨在探索使用排斥性节点（负相关）的积分方法来实现方差减少。

Method: 从确定性点过程(DPPs)和排斥点过程文献中提取积分方法，分析UnifOrtho估计器的方差特性，并对各种积分方法进行数值基准测试比较。

Result: 研究发现UnifOrtho估计器在高维情况下表现优异，随机拟蒙特卡洛在低维效果最好。DPP-based积分方法仅在拟蒙特卡洛有效时表现良好，而排斥性积分方法总体上显示出适度的方差减少。

Conclusion: 推荐计算切片Wasserstein距离时，低维使用随机拟蒙特卡洛，高维使用UnifOrtho方法。排斥性积分方法需要更多理论研究来提高其鲁棒性。

Abstract: In this paper, we consider the problem of computing the integral of a
function on the unit sphere, in any dimension, using Monte Carlo methods.
Although the methods we present are general, our guiding thread is the sliced
Wasserstein distance between two measures on $\mathbb{R}^d$, which is precisely
an integral on the $d$-dimensional sphere. The sliced Wasserstein distance (SW)
has gained momentum in machine learning either as a proxy to the less
computationally tractable Wasserstein distance, or as a distance in its own
right, due in particular to its built-in alleviation of the curse of
dimensionality. There has been recent numerical benchmarks of quadratures for
the sliced Wasserstein, and our viewpoint differs in that we concentrate on
quadratures where the nodes are repulsive, i.e. negatively dependent. Indeed,
negative dependence can bring variance reduction when the quadrature is adapted
to the integration task. Our first contribution is to extract and motivate
quadratures from the recent literature on determinantal point processes (DPPs)
and repelled point processes, as well as repulsive quadratures from the
literature specific to the sliced Wasserstein distance. We then numerically
benchmark these quadratures. Moreover, we analyze the variance of the UnifOrtho
estimator, an orthogonal Monte Carlo estimator. Our analysis sheds light on
UnifOrtho's success for the estimation of the sliced Wasserstein in large
dimensions, as well as counterexamples from the literature. Our final
recommendation for the computation of the sliced Wasserstein distance is to use
randomized quasi-Monte Carlo in low dimensions and \emph{UnifOrtho} in large
dimensions. DPP-based quadratures only shine when quasi-Monte Carlo also does,
while repelled quadratures show moderate variance reduction in general, but
more theoretical effort is needed to make them robust.

</details>


### [97] [Why does your graph neural network fail on some graphs? Insights from exact generalisation error](https://arxiv.org/abs/2509.10337)
*Nil Ayday,Mahalakshmi Sabanayagam,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: 该论文通过信号处理视角推导出图神经网络在转导固定设计设置下的精确泛化误差，揭示了只有节点特征与图结构对齐的信息才有助于泛化，并量化了同配性对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然GNN在图结构数据学习中广泛应用，但对其成功或失败的理论理解仍不清晰。现有工作主要关注架构限制（如过平滑和过挤压），但无法解释GNN如何提取有意义的表示或为何相似架构间性能差异巨大，这涉及到泛化能力的问题。

Method: 采用信号处理视角，将GNN解释为通过图结构对节点特征进行操作的图滤波器。在线性GNN框架下允许图滤波器的非线性，推导出包括卷积、PageRank和注意力模型在内的多种GNN的精确泛化误差。

Result: 精确泛化误差表征显示，只有节点特征与图结构对齐的信息对泛化有贡献。量化了同配性对泛化的影响，为模型选择提供了实用指导框架。

Conclusion: 该工作提供了一个解释GNN何时以及为何能有效利用结构和特征信息的理论框架，填补了GNN泛化理论理解的空白，对实际模型选择具有指导意义。

Abstract: Graph Neural Networks (GNNs) are widely used in learning on graph-structured
data, yet a principled understanding of why they succeed or fail remains
elusive. While prior works have examined architectural limitations such as
over-smoothing and over-squashing, these do not explain what enables GNNs to
extract meaningful representations or why performance varies drastically
between similar architectures. These questions are related to the role of
generalisation: the ability of a model to make accurate predictions on
unlabelled data. Although several works have derived generalisation error
bounds for GNNs, these are typically loose, restricted to a single
architecture, and offer limited insight into what governs generalisation in
practice. In this work, we take a different approach by deriving the exact
generalisation error for GNNs in a transductive fixed-design setting through
the lens of signal processing. From this viewpoint, GNNs can be interpreted as
graph filter operators that act on node features via the graph structure. By
focusing on linear GNNs while allowing non-linearity in the graph filters, we
derive the first exact generalisation error for a broad range of GNNs,
including convolutional, PageRank-based, and attention-based models. The exact
characterisation of the generalisation error reveals that only the aligned
information between node features and graph structure contributes to
generalisation. Furthermore, we quantify the effect of homophily on
generalisation. Our work provides a framework that explains when and why GNNs
can effectively leverage structural and feature information, offering practical
guidance for model selection.

</details>


### [98] [Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise](https://arxiv.org/abs/2509.10385)
*Utsab Saha,Tanvir Muntakim Tonoy,Hafiz Imtiaz*

Main category: stat.ML

TL;DR: 提出CAPE辅助的联邦DP-CDA算法，通过在联邦学习环境中整合相关性辅助隐私估计协议，改善差分隐私合成数据生成在分散数据设置中的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 在分散数据设置中，DP-CDA面临样本量有限导致局部计算敏感性增加的问题，需要注入更多噪声来保持差分隐私保证，从而导致效用显著下降。

Method: 将CAPE协议集成到联邦DP-CDA框架中，允许客户端生成联合分布的反相关噪声，在聚合时相互抵消，同时保持个体层面的隐私保护。

Result: 在MNIST和FashionMNIST数据集上的实验表明，该方法在某些参数范围内可以达到与集中式对应方法相当的效用，同时保持严格的差分隐私保证。

Conclusion: CAPE辅助联邦DP-CDA算法显著改善了联邦设置中的隐私-效用权衡，为分散数据环境下的差分隐私数据合成提供了有效解决方案。

Abstract: In this work, we explore differentially private synthetic data generation in
a decentralized-data setting by building on the recently proposed
Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA
synthesizes data in a centralized setting by mixing multiple randomly-selected
samples from the same class and injecting carefully calibrated Gaussian noise,
ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a
decentralized or federated setting, where each client holds only a small
partition of the data, DP-CDA faces new challenges. The limited sample size per
client increases the sensitivity of local computations, requiring higher noise
injection to maintain the differential privacy guarantee. This, in turn, leads
to a noticeable degradation in the utility compared to the centralized setting.
To mitigate this issue, we integrate the Correlation-Assisted Private
Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE
Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among
the clients by allowing them to generate jointly distributed (anti-correlated)
noise that cancels out in aggregate, while preserving privacy at the individual
level. This technique significantly improves the privacy-utility trade-off in
the federated setting. Extensive experiments on MNIST and FashionMNIST datasets
demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can
achieve utility comparable to its centralized counterpart under some parameter
regime, while maintaining rigorous differential privacy guarantees.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [99] [Off Policy Lyapunov Stability in Reinforcement Learning](https://arxiv.org/abs/2509.09863)
*Sarvan Gill,Daniela Constantinescu*

Main category: eess.SY

TL;DR: 提出一种离策略学习Lyapunov函数的方法，结合SAC和PPO算法提供数据高效稳定性保证


<details>
  <summary>Details</summary>
Motivation: 传统强化学习缺乏稳定性保证，现有基于Lyapunov函数的方法由于采用同策略学习导致样本效率低下

Method: 开发离策略Lyapunov函数学习方法，并将其整合到Soft Actor Critic和Proximal Policy Optimization算法中

Result: 在倒立摆和四旋翼飞行器仿真中，采用所提方法的算法性能得到显著提升

Conclusion: 离策略Lyapunov函数学习方法能够为强化学习算法提供数据高效的稳定性证书，提高样本利用效率

Abstract: Traditional reinforcement learning lacks the ability to provide stability
guarantees. More recent algorithms learn Lyapunov functions alongside the
control policies to ensure stable learning. However, the current self-learned
Lyapunov functions are sample inefficient due to their on-policy nature. This
paper introduces a method for learning Lyapunov functions off-policy and
incorporates the proposed off-policy Lyapunov function into the Soft Actor
Critic and Proximal Policy Optimization algorithms to provide them with a data
efficient stability certificate. Simulations of an inverted pendulum and a
quadrotor illustrate the improved performance of the two algorithms when
endowed with the proposed off-policy Lyapunov function.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [100] [Engineering Spatial and Molecular Features from Cellular Niches to Inform Predictions of Inflammatory Bowel Disease](https://arxiv.org/abs/2509.09923)
*Myles Joshua Toledo Tan,Maria Kapetanaki,Panayiotis V. Benos*

Main category: q-bio.GN

TL;DR: 本研究开发了一个基于空间转录组学的可解释机器学习框架，用于区分克罗恩病和溃疡性结肠炎，通过识别细胞生态位特征实现高精度分类


<details>
  <summary>Details</summary>
Motivation: 炎症性肠病(IBD)的两种主要亚型克罗恩病和溃疡性结肠炎临床表现重叠，传统诊断方法存在挑战，需要开发新的精准诊断工具

Method: 使用非负矩阵分解识别4个重复出现的细胞生态位，从中提取44个特征（生态位组成、邻域富集和生态位-基因信号），训练多层感知机分类器

Result: 三分类问题准确率77.4%，二分类问题准确率91.6%；模型可解释性分析显示空间组织破坏是炎症的主要预测因子，而特定基因表达特征区分亚型

Conclusion: 该研究提供了一个将描述性空间数据转化为准确可解释预测工具的流程，不仅提供新的诊断范式，还深入揭示了IBD亚型的生物学机制

Abstract: Differentiating between the two main subtypes of Inflammatory Bowel Disease
(IBD): Crohns disease (CD) and ulcerative colitis (UC) is a persistent clinical
challenge due to overlapping presentations. This study introduces a novel
computational framework that employs spatial transcriptomics (ST) to create an
explainable machine learning model for IBD classification. We analyzed ST data
from the colonic mucosa of healthy controls (HC), UC, and CD patients. Using
Non-negative Matrix Factorization (NMF), we first identified four recurring
cellular niches, representing distinct functional microenvironments within the
tissue. From these niches, we systematically engineered 44 features capturing
three key aspects of tissue pathology: niche composition, neighborhood
enrichment, and niche-gene signals. A multilayer perceptron (MLP) classifier
trained on these features achieved an accuracy of 0.774 +/- 0.161 for the more
challenging three-class problem (HC, UC, and CD) and 0.916 +/- 0.118 in the
two-class problem of distinguishing IBD from healthy tissue. Crucially, model
explainability analysis revealed that disruptions in the spatial organization
of niches were the strongest predictors of general inflammation, while the
classification between UC and CD relied on specific niche-gene expression
signatures. This work provides a robust, proof-of-concept pipeline that
transforms descriptive spatial data into an accurate and explainable predictive
tool, offering not only a potential new diagnostic paradigm but also deeper
insights into the distinct biological mechanisms that drive IBD subtypes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [101] [Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks](https://arxiv.org/abs/2509.10077)
*Simen Storesund,Kristian Valset Aars,Robin Dietrich,Nicolai Waniek*

Main category: cs.NE

TL;DR: 提出了一种基于局部脉冲时序的生物学合理最短路径算法，通过脉冲时间巧合和抑制-兴奋消息对来识别最优路径节点，无需全局状态或反向追踪


<details>
  <summary>Details</summary>
Motivation: 现有图算法需要全局状态和生物学上不可信的操作，而强化学习方法依赖缓慢的梯度更新，与生物系统的快速行为适应不符

Method: 利用局部脉冲消息传递和实际处理延迟，通过脉冲时间巧合识别最优路径节点，早期收到抑制-兴奋消息对的神经元减少响应延迟，形成从目标到源的时间压缩

Result: 算法收敛并发现所有最短路径，通过纯时序机制在随机空间网络上验证

Conclusion: 展示了短期时序动态如何单独计算最短路径，为理解生物网络如何通过局部计算和相对脉冲时间预测解决复杂问题提供了新见解

Abstract: Efficient planning and sequence selection are central to intelligence, yet
current approaches remain largely incompatible with biological computation.
Classical graph algorithms like Dijkstra's or A* require global state and
biologically implausible operations such as backtracing, while reinforcement
learning methods rely on slow gradient-based policy updates that appear
inconsistent with rapid behavioral adaptation observed in natural systems.
  We propose a biologically plausible algorithm for shortest-path computation
that operates through local spike-based message-passing with realistic
processing delays. The algorithm exploits spike-timing coincidences to identify
nodes on optimal paths: Neurons that receive inhibitory-excitatory message
pairs earlier than predicted reduce their response delays, creating a temporal
compression that propagates backwards from target to source. Through analytical
proof and simulations on random spatial networks, we demonstrate that the
algorithm converges and discovers all shortest paths using purely timing-based
mechanisms. By showing how short-term timing dynamics alone can compute
shortest paths, this work provides new insights into how biological networks
might solve complex computational problems through purely local computation and
relative spike-time prediction. These findings open new directions for
understanding distributed computation in biological and artificial systems,
with possible implications for computational neuroscience, AI, reinforcement
learning, and neuromorphic systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [102] [Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation](https://arxiv.org/abs/2509.09802)
*Tianqi Qiao,Marie Maros*

Main category: math.OC

TL;DR: 提出了Sparse Polyak方法，改进Polyak自适应步长以解决高维统计估计问题，在维度远大于样本量的情况下显著提升性能


<details>
  <summary>Details</summary>
Motivation: 标准Polyak步长在高维统计估计中表现不佳，即使问题条件良好，也需要更多迭代才能达到最优统计精度，这源于Lipschitz平滑度常数估计在高维环境中的不适用性

Method: 通过修改步长来估计受限Lipschitz平滑度常数（针对问题相关方向的平滑度），而非全局Lipschitz常数，从而适应高维设置

Result: 理论分析和数值实验均表明Sparse Polyak方法在高维情况下性能显著提升

Conclusion: Sparse Polyak通过采用受限平滑度估计方法，有效解决了高维统计估计中的步长适应问题，为高维优化提供了更有效的自适应步长策略

Abstract: We propose and study Sparse Polyak, a variant of Polyak's adaptive step size,
designed to solve high-dimensional statistical estimation problems where the
problem dimension is allowed to grow much faster than the sample size. In such
settings, the standard Polyak step size performs poorly, requiring an
increasing number of iterations to achieve optimal statistical precision-even
when, the problem remains well conditioned and/or the achievable precision
itself does not degrade with problem size. We trace this limitation to a
mismatch in how smoothness is measured: in high dimensions, it is no longer
effective to estimate the Lipschitz smoothness constant. Instead, it is more
appropriate to estimate the smoothness restricted to specific directions
relevant to the problem (restricted Lipschitz smoothness constant). Sparse
Polyak overcomes this issue by modifying the step size to estimate the
restricted Lipschitz smoothness constant. We support our approach with both
theoretical analysis and numerical experiments, demonstrating its improved
performance.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [103] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: HYPOGENEAGENT是一个基于大语言模型的框架，将细胞聚类注释转化为可量化优化的任务，通过LLM生成GO假设并计算内部一致性和外部区分度评分，自动选择最佳聚类分辨率。


<details>
  <summary>Details</summary>
Motivation: 解决单细胞研究中聚类分辨率选择和功能注释的主观性问题，传统方法依赖启发式规则和专家经验，缺乏客观量化标准。

Method: 使用LLM作为基因集分析师生成GO假设和置信度评分，然后通过句子嵌入模型计算聚类内一致性（intra-cluster agreement）和聚类间区分度（inter-cluster separation），组合得到分辨率评分。

Result: 在K562 CRISPRi Perturb-seq数据集测试中，该方法选择的聚类粒度与已知通路对齐效果优于传统指标如轮廓系数和模块度评分。

Conclusion: LLM代理可以作为聚类分辨率和功能注释的客观裁决者，为单细胞多组学研究实现全自动、上下文感知的解释流程铺平道路。

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [104] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 提出了一种参数高效的2D卷积前端，用于语音识别特征提取，相比现有方法更加通用和统一，在计算资源有限的情况下性能与现有监督学习方法相当


<details>
  <summary>Details</summary>
Motivation: 现有神经前端方法仍受传统方法影响较大，虽然这种归纳偏置有助于系统设计，但作者希望开发更通用的特征提取前端，并统一前端架构（而不是组合不同来源的层拓扑结构）

Method: 开发了基于2D卷积的通用统一前端架构，通过系统实验减少现有技术的影响，实现参数高效的设计

Result: 该2D卷积前端参数高效，适合计算资源有限的场景，性能与现有监督学习特征提取器相当

Conclusion: 这种通用的统一方法不仅可行，而且能够匹配现有监督学习特征提取器的性能，为有限计算资源场景提供了有效解决方案

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [105] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本文分析了会议转录中语音分离的泄漏问题，发现交叉通道存在显著泄漏但被VAD忽略，高级二值化方法能将与oracle分割的差距减少三分之一


<details>
  <summary>Details</summary>
Motivation: 会议转录领域近年来取得显著进展但仍存在性能限制，需要分析语音分离中的泄漏问题及其对最终性能的影响

Method: 扩展先前提出的泄漏分析框架，增加对时间局部性的敏感性，比较不同分割方法（能量基VAD与高级二值化方法），分析泄漏对性能的影响

Result: 发现在主要说话人活跃区域存在显著的交叉通道泄漏，但这些泄漏部分被VAD忽略而不影响最终性能；高级二值化方法相比简单能量基VAD能将与oracle分割的差距减少三分之一

Conclusion: 研究揭示了语音分离中的泄漏特性及其实际影响，证明了高级二值化方法的有效性，在仅使用LibriSpeech数据训练的系统中达到了LibriCSS上的最先进性能

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: 提出了一个从当前工作流管理系统向完全自主、分布式科学实验室演化的概念框架，通过智能化和群体化两个维度来加速科学发现


<details>
  <summary>Details</summary>
Motivation: 现代科学发现需要协调分布式设施和异构资源，研究人员被迫成为手动工作流协调员而非科学家。AI智能体技术为加速科学发现提供了新机遇，但需要明确如何在实际中实现和集成

Method: 提出了一个概念框架，工作流沿着两个维度演化：智能化（从静态到智能）和组合化（从单一到群体），并提供了架构蓝图

Result: 该框架为社区提供了向自主科学发展的重要步骤，有望实现100倍的科学发现加速和变革性科学工作流

Conclusion: 通过智能化和群体化的演化路径，可以构建完全自主的分布式科学实验室，显著加速科学发现进程，改变科研工作方式

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [107] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: Boldsea是一个基于语义事件方法的架构，使用可执行本体来建模复杂动态系统，将事件语义与数据流架构结合，解决了传统BPM系统和面向对象语义技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决传统业务流程管理(BPM)系统和面向对象语义技术在动态系统建模中的局限性，提供能够实时修改事件模型并统一数据和业务逻辑的语义框架。

Method: 提出Boldsea语义语言(BSL)及其BNF语法，设计boldsea-engine架构，直接解释语义模型作为可执行算法，无需编译过程。

Result: 实现了运行时修改事件模型的能力，确保时间透明度，并在统一的语义框架内无缝融合数据和业务逻辑。

Conclusion: Boldsea架构通过可执行本体和语义事件方法，为复杂动态系统建模提供了更灵活和统一的解决方案，克服了传统技术的限制。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [108] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: 提出模块化多模态框架，使用生成式AI从公开住宅信息和图像生成能源建模所需数据，解决数据获取难题


<details>
  <summary>Details</summary>
Motivation: 能源建模研究需要大量数据，但许多数据难以获取、成本高昂或存在隐私问题，需要开发替代数据生成方法

Method: 开发模块化多模态框架，利用生成式人工智能从公开可访问的住宅信息和图像生成所需数据，并提供完整的处理流程

Result: 实验表明该框架能避免生成模型的常见问题，产生真实且标注良好的数据

Conclusion: 通过减少对昂贵或受限数据源的依赖，为更易获取和可重复的研究铺平道路

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [109] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 该论文提出了一个信息论框架，通过分析状态-动作互信息模式来诊断RL系统部署时的异常故障，能够区分传感器故障和驱动器故障，为自适应RL系统提供自主故障检测基础。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署的强化学习代理面临传感器故障、驱动器磨损和环境变化等问题，但缺乏内在机制来检测和诊断这些故障，需要开发能够自主检测系统异常的方法。

Method: 采用信息论框架分析状态-动作互信息模式，通过控制扰动实验研究不同故障类型对信息通道的影响，使用互信息指标来诊断系统故障。

Result: 成功学习表现出特征性信息签名：状态-动作互信息从0.84增长到2.83比特（238%增长）；状态、动作和下一状态联合互信息呈现倒U型曲线；信息指标能够区分传感器故障（广泛破坏所有信息通道）和驱动器故障（选择性破坏动作-结果可预测性）。

Conclusion: 信息模式既是学习的签名，也是系统健康的诊断工具，为基于信息论原理的自适应RL系统提供了自主故障检测和政策调整的基础，无需架构修改或性能下降即可实现精确故障定位。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [110] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: 本研究提出使用大型语言模型生成个体出行日记的新方法，通过开源数据创建虚拟人物并合成出行记录，在零样本条件下达到与传统方法相当的逼真度。


<details>
  <summary>Details</summary>
Motivation: 传统出行模型依赖大量专有家庭出行调查数据，成本高且获取困难。本研究旨在利用开源数据和LLM技术，开发更高效、低成本的出行日记生成方案。

Method: 使用美国社区调查和智能位置数据库的开源数据随机生成虚拟人物，通过直接提示LLM合成出行日记。采用包含四个指标（出行次数、时间间隔、目的、方式）的综合逼真度评分体系，并用Jensen-Shannon散度验证分布相似性。

Result: LLM生成的日记整体逼真度与传统方法相当（0.485 vs 0.455），在出行目的确定方面表现更优且一致性更高，而传统方法在出行次数和活动时长数值估计方面略胜。聚合验证显示LLM具有更好的统计代表性（0.612 vs 0.435）。

Conclusion: LLM在零样本条件下能够生成具有统计代表性的出行日记，证明了其可行性，并为未来合成日记评估系统建立了可量化的逼真度度量标准。

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [111] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS是一个基于大型语言模型的系统，用于生成包含55万+指标的综合理论网络，解决心理学测量中理论网络构建的长期挑战。


<details>
  <summary>Details</summary>
Motivation: 解决Cronbach和Meehl提出的理论网络构建难题，该问题70年来一直是测量验证的基础挑战，影响临床试验效果检测和公共政策目标制定。

Method: 使用经过验证的问卷测量训练大型语言模型，开发ALIGNS系统，包含三个综合理论网络，涵盖心理学、医学、社会政策等多个领域。

Result: 系统在三个评估中表现良好：发现NIH PROMIS焦虑抑郁工具收敛为单一情绪困扰维度；识别儿童气质测量的四个新维度并质疑现有维度；专家评估显示系统具有重要性、可访问性和适用性。

Conclusion: ALIGNS是首个应用大型语言模型解决测量验证基础问题的系统，免费提供使用，可补充传统验证方法，实现大规模理论分析。

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [112] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: 提出基于技术间时间关系的框架来识别新兴技术机会，利用大语言模型从专利数据中提取主题并追踪其随时间变化


<details>
  <summary>Details</summary>
Motivation: 技术机会是推动技术、产业和创新进步的关键信息，需要系统化方法来识别新兴技术发展趋势

Method: 从专利数据集中提取文本，将基于文本的主题映射以发现技术间关系，利用大语言模型提取主题并通过追踪主题随时间变化来识别技术机会

Result: 使用美国专利商标局提供的人工智能专利数据集进行评估，实验结果表明人工智能技术正朝着便于日常访问的形式发展

Conclusion: 该框架展示了识别未来技术机会的潜力，为技术发展和创新提供了有价值的分析工具

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [113] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: 对Twitter情感分析中机器学习性能的元分析显示平均准确率为0.80，指出总体准确率指标存在误导性，需要标准化报告规范


<details>
  <summary>Details</summary>
Motivation: 评估Twitter情感分析中机器学习模型的平均性能，分析研究间的异质性，并探讨研究特征如何影响模型性能

Method: 使用PRISMA指南搜索学术数据库，选取20项研究中的195个试验，采用双反正弦变换和三级随机效应模型分析总体准确率

Result: AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]，发现总体准确率因类别不平衡和情感类别数量而具有误导性

Conclusion: 需要规范模型性能报告标准，包括报告独立测试集的混淆矩阵，以实现跨研究的可靠比较

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [114] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench是一个专门针对MCP协议的综合基准测试，包含33个服务器、188个工具和600个查询，用于评估语言代理在工具交互中的真实性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法准确评估在MCP协议下AI代理的真实操作价值，导致对其能力的误解和无法可靠区分不同代理的熟练程度。

Method: 建立了包含33个操作服务器和188个不同工具的MCP测试床，开发了600个系统设计的查询，分布在6个不同复杂度的交互类别中，并引入了MCP-Eval这种以结果为导向的新型评估方法。

Result: 通过对领先语言代理的广泛实证评估，提供了基础性见解，展示了不同代理在MCP环境下的性能差异。

Conclusion: MCP-AgentBench为研究社区提供了一个标准化和可靠的框架，用于构建、验证和推进能够充分利用MCP变革性优势的代理，加速实现真正有能力且可互操作的AI系统。

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [115] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: HEFT是一种分层高效微调策略，结合LoRA和ReFT两种PEFT方法，在BoolQ基准测试中仅用3个epoch就达到85.17%准确率，优于单一方法训练20个epoch的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专门推理任务上的适配受计算资源限制，不同PEFT方法在权重空间和表示空间中各有优势，研究假设这两种范式的协同组合可以解锁更好的性能和效率。

Method: 提出HEFT分层适配策略：先在权重空间使用LoRA进行广泛的基础适配，然后在表示空间使用ReFT对内部激活进行精确细化，形成从粗到细的微调方式。

Result: 在Llama-2-7B模型和BoolQ推理数据集上的实验显示，HEFT仅用3个epoch训练就达到85.17%准确率，超过LoRA-only（85.05%）和ReFT-only（83.36%）方法训练20个epoch的性能。

Conclusion: PEFT方法的精心组合是一种强大的算法创新，为提升语言模型推理能力提供了更高效有效的路径，以更少的计算预算获得更优结果。

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [116] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: 提出系统框架生成高质量、与人口分布对齐的LLM社交模拟角色集，通过社交媒体数据生成、质量评估、重要性采样和心理测量分布对齐，显著减少人口级偏差


<details>
  <summary>Details</summary>
Motivation: 现有LLM社交模拟研究主要关注代理框架和模拟环境设计，忽视了角色生成的复杂性以及非代表性角色集引入的潜在偏差，需要构建能真实反映现实世界人口多样性和分布的角色集

Method: 利用LLM从长期社交媒体数据生成叙述性角色，进行严格质量评估筛选低保真档案，应用重要性采样实现与参考心理测量分布（如大五人格特质）的全局对齐，并引入任务特定模块针对目标子群体进行适配

Result: 实验表明该方法显著减少了人口级偏差，为广泛的研究和政策应用实现了准确、灵活的社交模拟

Conclusion: 提出的系统框架能够合成高质量、与人口分布对齐的角色集，解决了LLM驱动社交模拟中的关键挑战，为计算社会科学提供了更真实可靠的基础

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [117] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文通过大规模实证分析发现，上下文学习(ICL)确实构成学习机制，但其学习能力有限，对未见任务的泛化能力不足，且对提示中的分布变化敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管ICL被声称能够通过少量样本学习新任务，但需要实证验证其是否真正构成学习机制，以及其学习能力的局限性。

Method: 进行大规模ICL分析，通过消融实验排除记忆效应、预训练影响、分布偏移等因素，分析不同提示风格和措辞对ICL效果的影响。

Result: ICL是一种有效的学习范式，但在学习未见任务和泛化方面能力有限；当示例数量增多时，准确率对示例分布、模型、提示风格等不敏感，主要从提示中的规律性推断模式。

Conclusion: 自回归模型的临时编码机制不够鲁棒，表明其通用泛化能力有限，ICL对分布变化敏感，特别是在思维链等提示风格中。

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [118] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE是一种仅使用文本数据进行预训练ASR模型领域适应的深度监督方法，通过变分自编码器建模编码器输出并微调解码器，在推理时不增加额外计算成本


<details>
  <summary>Details</summary>
Motivation: 预训练ASR模型如Whisper在处理未见词汇和方言时需要领域适应，但在许多实际场景中收集语音数据不现实，因此需要仅使用文本的适应方法

Method: 训练变分自编码器(VAE)从文本建模编码器输出，使用学习的文本到潜在编码器微调解码器，可选结合文本到语音(TTS)适应。推理时恢复原始编码器

Result: 在四个域外数据集和四个ASR模型上，WhisTLE结合TTS相比仅使用TTS的适应方法相对降低12.3%的词错误率，在32个场景中的27个优于所有非WhisTLE基线

Conclusion: WhisTLE提供了一种有效的文本-only适应方法，显著提升预训练ASR模型在未见领域的性能，且不增加推理时的计算开销

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [119] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出ZADS方法，通过测试时优化自适应调整扩散模型中的保真度权重，无需重新训练即可适应不同噪声调度，在MRI重建中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解决逆问题时严重依赖精心调整的保真度权重，特别是在快速采样计划下，现有启发式或固定权重方法无法泛化到不同的测量条件和时间步调度

Method: ZADS将去噪过程视为固定的展开采样器，仅使用欠采样测量以自监督方式优化保真度权重，实现测试时自适应调整

Result: 在fastMRI膝关节数据集上的实验表明，ZADS始终优于传统压缩感知和最近的基于扩散的方法，能够跨不同噪声调度和采集设置提供高保真重建

Conclusion: ZADS提供了一种无需重新训练扩散先验的有效方法，通过自适应权重调整显著提升了扩散模型在逆问题求解中的性能

Abstract: Diffusion/score-based models have recently emerged as powerful generative
priors for solving inverse problems, including accelerated MRI reconstruction.
While their flexibility allows decoupling the measurement model from the
learned prior, their performance heavily depends on carefully tuned data
fidelity weights, especially under fast sampling schedules with few denoising
steps. Existing approaches often rely on heuristics or fixed weights, which
fail to generalize across varying measurement conditions and irregular timestep
schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling
(ZADS), a test-time optimization method that adaptively tunes fidelity weights
across arbitrary noise schedules without requiring retraining of the diffusion
prior. ZADS treats the denoising process as a fixed unrolled sampler and
optimizes fidelity weights in a self-supervised manner using only undersampled
measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS
consistently outperforms both traditional compressed sensing and recent
diffusion-based methods, showcasing its ability to deliver high-fidelity
reconstructions across varying noise schedules and acquisition settings.

</details>


### [120] [Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators](https://arxiv.org/abs/2509.09894)
*Jiayun Wang,Yousuf Aborahama,Arya Khokhar,Yang Zhang,Chuwei Wang,Karteekeya Sastry,Julius Berner,Yilin Luo,Boris Bonev,Zongyi Li,Kamyar Azizzadenesheli,Lihong V. Wang,Anima Anandkumar*

Main category: eess.IV

TL;DR: 提出Pano（PACT成像神经算子），一种端到端的物理感知模型，直接从传感器测量学习到体积重建的逆声学映射，显著减少换能器数量和采集时间，同时保持重建质量


<details>
  <summary>Details</summary>
Motivation: 当前三维PACT系统需要密集的换能器阵列和长时间采集，限制了临床转化，需要开发能够减少硬件需求同时保持成像质量的新方法

Method: 使用球面离散-连续卷积保持半球形传感器几何结构，结合亥姆霍兹方程约束确保物理一致性，采用分辨率无关的端到端学习框架

Result: 在模拟和真实实验数据中都能重建高质量图像，即使显著减少换能器数量和有限角度采集配置下也能保持一致的性能

Conclusion: Pano框架为3D PACT的临床转化提供了实用途径，大幅减少硬件需求而不影响重建质量，使实时体积成像成为可能

Abstract: Photoacoustic computed tomography (PACT) combines optical contrast with
ultrasonic resolution, achieving deep-tissue imaging beyond the optical
diffusion limit. While three-dimensional PACT systems enable high-resolution
volumetric imaging for applications spanning transcranial to breast imaging,
current implementations require dense transducer arrays and prolonged
acquisition times, limiting clinical translation. We introduce Pano (PACT
imaging neural operator), an end-to-end physics-aware model that directly
learns the inverse acoustic mapping from sensor measurements to volumetric
reconstructions. Unlike existing approaches (e.g. universal back-projection
algorithm), Pano learns both physics and data priors while also being agnostic
to the input data resolution. Pano employs spherical discrete-continuous
convolutions to preserve hemispherical sensor geometry, incorporates Helmholtz
equation constraints to ensure physical consistency and operates
resolutionindependently across varying sensor configurations. We demonstrate
the robustness and efficiency of Pano in reconstructing high-quality images
from both simulated and real experimental data, achieving consistent
performance even with significantly reduced transducer counts and limited-angle
acquisition configurations. The framework maintains reconstruction fidelity
across diverse sparse sampling patterns while enabling real-time volumetric
imaging capabilities. This advancement establishes a practical pathway for
making 3D PACT more accessible and feasible for both preclinical research and
clinical applications, substantially reducing hardware requirements without
compromising image reconstruction quality.

</details>


### [121] [Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms](https://arxiv.org/abs/2509.09972)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Mohsen Mesgaran,Parastoo Farajpoor,Hamid Jafarbiglu*

Main category: eess.IV

TL;DR: 本研究结合无人机多光谱影像和LSTM深度学习网络，使用SMOTE技术处理类别不平衡，成功实现了对番茄寄生植物分枝列当的早期检测，最高准确率达到88.37%，召回率达到95.37%。


<details>
  <summary>Details</summary>
Motivation: 分枝列当对加州番茄产业构成严重威胁，其地下生命周期使得早期检测困难，传统化学控制方法成本高、环境有害且效果有限，需要开发新的检测技术。

Method: 在已知感染的番茄农场进行实验，使用无人机采集多光谱影像，通过生长度日确定五个关键生长阶段，利用LSTM网络处理时序数据，并采用SMOTE技术解决类别不平衡问题。

Result: 在897生长度日时检测准确率达到79.09%，召回率70.36%；整合所有生长阶段并使用SMOTE后，准确率提升至88.37%，召回率达到95.37%。

Conclusion: 时序多光谱分析和LSTM网络在早期列当检测方面具有强大潜力，无人机多光谱传感与深度学习结合可为精准农业提供有力工具，减少损失并提高番茄生产的可持续性。

Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche
ramosa) to California's tomato industry, which supplies over 90 percent of U.S.
processing tomatoes. The parasite's largely underground life cycle makes early
detection difficult, while conventional chemical controls are costly,
environmentally harmful, and often ineffective. To address this, we combined
drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep
learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)
to handle class imbalance. Research was conducted on a known broomrape-infested
tomato farm in Woodland, Yolo County, CA, across five key growth stages
determined by growing degree days (GDD). Multispectral images were processed to
isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with
79.09 percent overall accuracy and 70.36 percent recall without integrating
later stages. Incorporating sequential growth stages with LSTM improved
detection substantially. The best-performing scenario, which integrated all
growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy
and 95.37 percent recall. These results demonstrate the strong potential of
temporal multispectral analysis and LSTM networks for early broomrape
detection. While further real-world data collection is needed for practical
deployment, this study shows that UAV-based multispectral sensing coupled with
deep learning could provide a powerful precision agriculture tool to reduce
losses and improve sustainability in tomato production.

</details>


### [122] [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](https://arxiv.org/abs/2509.10348)
*Yehudit Aperstein,Amit Tzahar,Alon Gottlib,Tal Verber,Ravit Shagan Damti,Alexander Apartsin*

Main category: eess.IV

TL;DR: 该研究提出了一个基于DenseNet-121的不确定性感知框架，通过熵拒绝和置信区间拒绝两种选择性预测机制，在胸部X光多标签分类中提高模型可靠性，避免对不确定预测做出错误判断。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像任务中存在过度自信风险，特别是在需要同时检测多种共存病理的胸部X光多标签分类中，不确定的预测可能带来高风险。

Method: 使用DenseNet-121作为骨干网络，集成熵基拒绝和置信区间拒绝两种选择性预测机制，采用分位数校准程序调整拒绝阈值，支持全局和类特定策略。

Result: 在三个大型公共数据集上的实验表明，选择性拒绝改善了诊断准确性和覆盖范围之间的权衡，熵基拒绝在所有病理中获得了最高的平均AUC。

Conclusion: 该研究支持将选择性预测整合到AI辅助诊断工作流程中，为深度学习在临床环境中更安全、不确定性感知的部署提供了实用步骤。

Abstract: Overconfidence in deep learning models poses a significant risk in
high-stakes medical imaging tasks, particularly in multi-label classification
of chest X-rays, where multiple co-occurring pathologies must be detected
simultaneously. This study introduces an uncertainty-aware framework for chest
X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective
prediction mechanisms: entropy-based rejection and confidence interval-based
rejection. Both methods enable the model to abstain from uncertain predictions,
improving reliability by deferring ambiguous cases to clinical experts. A
quantile-based calibration procedure is employed to tune rejection thresholds
using either global or class-specific strategies. Experiments conducted on
three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)
demonstrate that selective rejection improves the trade-off between diagnostic
accuracy and coverage, with entropy-based rejection yielding the highest
average AUC across all pathologies. These results support the integration of
selective prediction into AI-assisted diagnostic workflows, providing a
practical step toward safer, uncertainty-aware deployment of deep learning in
clinical settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [123] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: text-2-SQL-4-PM是一个为流程挖掘领域设计的双语（葡萄牙语-英语）文本到SQL基准数据集，包含1,655个自然语言语句和205个SQL语句，支持无SQL专业知识的用户进行自然语言数据库查询。


<details>
  <summary>Details</summary>
Motivation: 促进自然语言数据库查询，提高非SQL专家用户的可访问性和专家用户的生产力，解决流程挖掘领域特有的挑战，如专业词汇和基于事件日志的单表关系结构。

Method: 通过专家手动整理、专业翻译和详细注释过程创建数据集，包括人工生成的释义，并使用GPT-3.5 Turbo进行基线研究以验证数据集的可行性。

Result: 数据集支持文本到SQL实现的评估，展示了在流程挖掘领域的应用可行性，并为语义解析和其他自然语言处理任务提供了更广泛的适用性。

Conclusion: text-2-SQL-4-PM数据集为流程挖掘领域的文本到SQL任务提供了有效的基准工具，促进了自然语言处理技术在数据库查询中的应用和发展。

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [124] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: 提出基于波的语义记忆框架，用波模式表示知识并通过共振干涉进行检索，相比传统向量方法能更好地保留振幅和相位信息，实现更强的语义区分能力


<details>
  <summary>Details</summary>
Motivation: 传统基于向量的记忆系统依赖于余弦或内积相似度，虽然计算高效但本质上是相位不敏感的，无法捕捉对意义表示至关重要的共振现象

Method: 将知识建模为波模式ψ(x)=A(x)e^{iφ(x)}，通过基于共振的干涉进行检索，保留振幅和相位信息

Result: 共振检索在向量方法失败的场景（如相位偏移、否定和组合查询）中展现出更高的区分能力，ResonanceDB实现百万级模式的可扩展性和毫秒级延迟

Conclusion: 基于波的记忆可作为向量存储的可行替代方案，适用于AGI导向的推理和知识表示

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [125] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 本文提出了生成式引擎优化(GEO)新范式，通过大规模实验对比AI搜索与传统搜索，发现AI搜索系统性地偏向权威第三方内容而非品牌自有内容，并提供了具体的GEO策略框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索引擎的兴起正在重塑信息检索方式，从传统排名列表转向合成式、带引用的答案，这挑战了现有的SEO实践，需要建立新的优化范式。

Method: 通过跨多个垂直领域、语言和查询改写的大规模受控实验，量化分析AI搜索与传统搜索在信息来源方面的关键差异。

Result: 研究发现AI搜索系统性地过度偏向权威第三方内容(Earned media)，与谷歌更平衡的内容来源形成鲜明对比；不同AI搜索服务在域名多样性、新鲜度、跨语言稳定性和措辞敏感性方面存在显著差异。

Conclusion: 基于实证结果提出了GEO战略议程，为从业者提供可操作指导，包括优化机器可扫描性、主导权威媒体内容、采用引擎特定策略等，为生成式搜索环境中的可见性提供了基础分析框架。

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [126] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: db3团队在KDD Cup'25 Meta CRAG-MM挑战赛中获胜的解决方案，通过多模态检索管道和LLM幻觉控制技术，在三个任务中分别获得第2、第2和第1名。


<details>
  <summary>Details</summary>
Motivation: 解决CRAG-MM挑战赛中的多模态、多轮问答基准问题，特别是在第一人称视角查询方面的挑战。

Method: 开发了包含图像索引知识图谱、网络资源和多轮对话的领域特定检索管道，以及使用SFT、DPO和RL进行高级拒绝训练的LLM调优方法。

Result: 在Task 1和Task 2中获得第2名，在Task 3中获得第1名，最终赢得总冠军，特别擅长处理以自我为中心的查询。

Conclusion: 该综合框架通过专门的检索管道和先进的幻觉控制技术，在多模态多轮问答任务中表现出色，特别是在第一人称视角处理方面具有优势。

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [127] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: 使用冻结的大型语言模型提取文本用户表示，并通过微调小型语言模型来构建高效的用户行为模拟代理，在推荐系统中平衡可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决用户行为模拟的长期挑战，传统方法需要有效解析大规模表格数据、克服预训练偏差并实现大规模应用，现有方法主要关注复杂提示或微调LLM，但存在效率和扩展性问题。

Method: 采用冻结LLM提取鲁棒的文本用户表示，使用微调的SLMs构建成本效益高的用户代理，并为用户群体训练多个低秩适配器（persona），实现可扩展性与性能的最佳平衡。

Result: 实验提供了有力的实证证据，表明该方法开发出的用户代理能够弥合推荐系统离线指标与实际性能之间的差距。

Conclusion: 该方法通过创新的用户表示提取和高效代理构建，为推荐系统的用户行为模拟提供了有效的解决方案，在保持性能的同时显著提升了资源效率和可扩展性。

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [128] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: 提出基于大语言模型的统一查询理解框架，替代传统多NER模型方案，通过联合建模查询和上下文信号来生成结构化解释，提升推荐准确性和个性化程度


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多个任务特定的命名实体识别模型，存在架构脆弱、维护成本高、难以适应不断变化的分类体系和语言模式等问题

Method: 使用大语言模型构建统一查询理解框架，联合建模用户查询和上下文信号（如个人资料属性），生成结构化解释来驱动更准确和个性化的推荐

Result: 在线A/B测试中提升了相关性质量，同时显著降低了系统复杂性和运营开销

Conclusion: 该解决方案为动态Web应用中的查询理解提供了可扩展和适应性强的技术基础

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [129] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: 提出了一种基于删除诊断的推荐系统可解释性方法，通过比较完整模型与去除特定用户/物品后模型的性能差异，量化单个观测对推荐系统的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐系统虽然性能优越但缺乏可解释性，需要开发模型无关的方法来提高推荐系统的透明度和可解释性。

Method: 使用删除诊断技术，训练完整模型和去除特定用户/物品的对比模型，通过性能差异量化单个观测的影响。在NCF和SVD两种推荐模型上进行验证。

Result: 在MovieLens和Amazon Reviews数据集上的实验表明，该方法能有效揭示模型行为，且适用于不同推荐范式，具有很好的通用性。

Conclusion: 删除诊断是一种有效的模型无关可解释性方法，能够提高推荐系统的透明度和可解释性，适用于深度学习和传统推荐模型。

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [130] [Parameterized Complexity of Vehicle Routing](https://arxiv.org/abs/2509.10361)
*Michelle Döring,Jan Fehse,Tobias Friedrich,Paula Marten,Niklas Mohrin,Kirill Simonov,Farehe Soheil,Jakob Timm,Shaily Verma*

Main category: cs.CC

TL;DR: 本文研究了车辆路径问题(VRP)及其变种在树宽等参数化下的计算复杂性，提出了VRP的FPT算法，证明了CVRP的paraNP-和W[·]-困难性，并给出了树宽和车辆容量双参数化的XP算法。


<details>
  <summary>Details</summary>
Motivation: 车辆路径问题是旅行商问题的推广，在物流和运输领域有重要应用。研究不同参数化下的计算复杂性有助于理解问题的可解性边界，为算法设计提供理论指导。

Method: 采用参数复杂性理论分析，研究VRP和CVRP在树宽等参数下的计算复杂性。使用FPT、paraNP-hardness、W[·]-hardness等复杂性分类工具，并设计了相应的算法。

Result: 1. 对VRP参数化树宽提出了FPT算法；2. 证明了CVRP在树宽等参数化下的paraNP-和W[·]-困难性；3. 为树宽和车辆容量双参数化的CVRP提供了XP算法。

Conclusion: VRP在树宽参数化下是可处理的(FPT)，而CVRP在相同参数化下极不可能存在FPT算法，但在树宽和容量双参数化下存在XP算法，这为实际问题求解提供了理论依据。

Abstract: The Vehicle Routing Problem (VRP) is a popular generalization of the
Traveling Salesperson Problem. Instead of one salesperson traversing the entire
weighted, undirected graph $G$, there are $k$ vehicles available to jointly
cover the set of clients $C \subseteq V(G)$. Every vehicle must start at one of
the depot vertices $D \subseteq V(G)$ and return to its start. Capacitated
Vehicle Routing (CVRP) additionally restricts the route of each vehicle by
limiting the number of clients it can cover, the distance it can travel, or
both.
  In this work, we study the complexity of VRP and the three variants of CVRP
for several parameterizations, in particular focusing on the treewidth of $G$.
We present an FPT algorithm for VRP parameterized by treewidth. For CVRP, we
prove paraNP- and $W[\cdot]$-hardness for various parameterizations, including
treewidth, thereby rendering the existence of FPT algorithms unlikely. In turn,
we provide an XP algorithm for CVRP when parameterized by both treewidth and
the vehicle capacity.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [131] [Reinforcement learning for spin torque oscillator tasks](https://arxiv.org/abs/2509.10057)
*Jakub Mojsiejuk,Sławomir Ziętek,Witold Skowroński*

Main category: physics.app-ph

TL;DR: 使用强化学习自动同步自旋电子振荡器，通过训练两种RL代理在固定步数内与目标频率同步，并展示了收敛性和能量效率的改进


<details>
  <summary>Details</summary>
Motivation: 解决自旋电子振荡器的自动同步问题，传统方法可能效率不高，希望通过强化学习实现更高效的同步控制

Method: 使用宏自旋Landau-Lifschitz-Gilbert-Slonczewski方程数值解模拟STO，训练两种类型的强化学习代理进行频率同步

Result: 在模拟环境中实现了同步收敛性和能量效率的改进，证明了强化学习方法在STO同步中的有效性

Conclusion: 强化学习为自旋电子振荡器的自动同步提供了一种有效且能量效率更高的解决方案，在模拟环境中表现出良好的性能

Abstract: We address the problem of automatic synchronisation of the spintronic
oscillator (STO) by means of reinforcement learning (RL). A numerical solution
of the macrospin Landau-Lifschitz-Gilbert-Slonczewski equation is used to
simulate the STO and we train the two types of RL agents to synchronise with a
target frequency within a fixed number of steps. We explore modifications to
this base task and show an improvement in both convergence and energy
efficiency of the synchronisation that can be easily achieved in the simulated
environment.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [132] [Nearly optimal algorithms to learn sparse quantum Hamiltonians in physically motivated distances](https://arxiv.org/abs/2509.09813)
*Amira Abbas,Nunzia Cerrato,Francisco Escudero Gutiérrez,Dmitry Grinko,Francesco Anna Mele,Pulkit Sinha*

Main category: quant-ph

TL;DR: 本文提出了两种物理动机的距离度量方法，并设计了近乎最优的算法来学习s-稀疏哈密顿量，在实验次数和时间复杂度上都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决现有哈密顿量学习研究中缺乏匹配下界和使用数学方便但物理不透明的误差度量的问题。

Method: 引入时间约束距离和温度约束距离两种物理距离度量，设计基于Valiant-Vazirani定理隔离技术的新算法，通过查询单个泡利系数的时间演化来恢复哈密顿量的泡利支撑。

Result: 实现了O(s log(1/ε))次实验和O(s²/ε)演化时间的学习复杂度，在时间约束距离下建立了Ω((s/n)log(1/ε) + s)实验次数和Ω(√s/ε)演化时间的下界，证明了实验次数的近乎最优性。

Conclusion: 新提出的隔离技术显著改进了稀疏哈密顿量学习效率，为量子系统表征提供了更实用的理论框架和算法工具。

Abstract: We study the problem of learning Hamiltonians $H$ that are $s$-sparse in the
Pauli basis, given access to their time evolution. Although Hamiltonian
learning has been extensively investigated, two issues recur in much of the
existing literature: the absence of matching lower bounds and the use of
mathematically convenient but physically opaque error measures.
  We address both challenges by introducing two physically motivated distances
between Hamiltonians and designing a nearly optimal algorithm with respect to
one of these metrics. The first, time-constrained distance, quantifies
distinguishability through dynamical evolution up to a bounded time. The
second, temperature-constrained distance, captures distinguishability through
thermal states at bounded inverse temperatures.
  We show that $s$-sparse Hamiltonians with bounded operator norm can be
learned in both distances with $O(s \log(1/\epsilon))$ experiments and
$O(s^2/\epsilon)$ evolution time. For the time-constrained distance, we further
establish lower bounds of $\Omega((s/n)\log(1/\epsilon) + s)$ experiments and
$\Omega(\sqrt{s}/\epsilon)$ evolution time, demonstrating near-optimality in
the number of experiments.
  As an intermediate result, we obtain an algorithm that learns every Pauli
coefficient of $s$-sparse Hamiltonians up to error $\epsilon$ in
$O(s\log(1/\epsilon))$ experiments and $O(s/\epsilon)$ evolution time,
improving upon several recent results.
  The source of this improvement is a new isolation technique, inspired by the
Valiant-Vazirani theorem (STOC'85), which shows that NP is as easy as detecting
unique solutions. This isolation technique allows us to query the time
evolution of a single Pauli coefficient of a sparse Hamiltonian--even when the
Pauli support of the Hamiltonian is unknown--ultimately enabling us to recover
the Pauli support itself.

</details>


### [133] [Toward Minimum Graphic Parity Networks](https://arxiv.org/abs/2509.10070)
*Yixin Cao,Yiren Lu,Junhong Nie,Xiaoming Sun,Guojing Tian*

Main category: quant-ph

TL;DR: 本文研究图形奇偶网络合成问题，提出了量子电路中CNOT和Rz门的最小数量下界，并给出了随机算法和特定图类的最小合成方法。


<details>
  <summary>Details</summary>
Motivation: 优化由CNOT和Rz门组成的量子电路合成对量子算法至关重要，特别是解决具有Ising公式的组合优化问题时。

Method: 研究图形奇偶网络合成问题，建立理论下界，提出随机算法，定义新的图类并设计线性时间合成算法。

Result: 证明了连通图需要至少m+n-1个门，当最短环长度≥5时可改进为m+Ω(n^1.5)；提出了期望m+O(n^1.5√log n)的随机算法；证明了识别特定图类是NP完全的。

Conclusion: 为量子电路合成提供了理论下界和实用算法，定义了新的图类并分析了其计算复杂性，为量子算法优化提供了重要理论基础。

Abstract: Quantum circuits composed of CNOT and $R_z$ are fundamental building blocks
of many quantum algorithms, so optimizing the synthesis of such quantum
circuits is crucial. We address this problem from a theoretical perspective by
studying the graphic parity network synthesis problem. A graphic parity network
for a graph $G$ is a quantum circuit composed solely of CNOT gates where each
edge of $G$ is represented in the circuit, and the final state of the wires
matches the original input. We aim to synthesize graphic parity networks with
the minimum number of gates, specifically for quantum algorithms addressing
combinatorial optimization problems with Ising formulations. We demonstrate
that a graphic parity network for a connected graph with $n$ vertices and $m$
edges requires at least $m+n-1$ gates. This lower bound can be improved to
$m+\Omega(m) = m+\Omega(n^{1.5})$ when the shortest cycle in the graph has a
length of at least five. We complement this result with a simple randomized
algorithm that synthesizes a graphic parity network with expected $m +
O(n^{1.5}\sqrt{\log n})$ gates. Additionally, we begin exploring connected
graphs that allow for graphic parity networks with exactly $m+n-1$ gates. We
conjecture that all such graphs belong to a newly defined graph class.
Furthermore, we present a linear-time algorithm for synthesizing minimum
graphic parity networks for graphs within this class. However, this graph class
is not closed under taking induced subgraphs, and we show that recognizing it
is $\textsf{NP}$-complete, which is complemented with a fixed-parameter
tractable algorithm parameterized by the treewidth.

</details>


### [134] [Certifying and learning quantum Ising Hamiltonians](https://arxiv.org/abs/2509.10239)
*Andreas Bluhm,Matthias C. Caro,Francisco Escudero Gutiérrez,Aadil Oufkir,Cambyse Rouzé*

Main category: quant-ph

TL;DR: 本文研究了量子Ising哈密顿量的认证和学习问题，提出了在Frobenius范数下认证Ising哈密顿量的高效算法，以及学习Ising Gibbs态的样本和时间高效方法，并将结果推广到一般k-局域哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 研究量子Ising哈密顿量的认证和学习问题，旨在解决现有方法在样本复杂度和计算效率方面的局限性，特别是针对Gibbs态学习中的指数样本复杂度问题。

Method: 使用Bonami引理进行傅里叶分析，设计了基于时间演化算子的认证算法，以及学习Ising Gibbs态的迹范数方法，算法在样本和时间复杂度上均达到高效。

Result: 实现了近乎最优的哈密顿量认证算法（O~(1/ε)时间演化），解决了Gibbs态学习的指数样本复杂度问题，提供了样本和时间高效的Gibbs态认证算法。

Conclusion: 本文提出了量子Ising哈密顿量认证和Gibbs态学习的高效算法，解决了相关领域的关键问题，并将方法成功推广到更一般的k-局域哈密顿量情形。

Abstract: In this work, we study the problems of certifying and learning quantum Ising
Hamiltonians. Our main contributions are as follows:
  Certification of Ising Hamiltonians. We show that certifying an Ising
Hamiltonian in normalized Frobenius norm via access to its time-evolution
operator requires only $\widetilde O(1/\varepsilon)$ time evolution. This
matches the Heisenberg-scaling lower bound of $\Omega(1/\varepsilon)$ up to
logarithmic factors. To our knowledge, this is the first nearly-optimal
algorithm for testing a Hamiltonian property. A key ingredient in our analysis
is the Bonami Lemma from Fourier analysis.
  Learning Ising Gibbs states. We design an algorithm for learning Ising Gibbs
states in trace norm that is sample-efficient in all parameters. In contrast,
previous approaches learned the underlying Hamiltonian (which implies learning
the Gibbs state) but suffered from exponential sample complexity in the inverse
temperature.
  Certification of Ising Gibbs states. We give an algorithm for certifying
Ising Gibbs states in trace norm that is both sample and time-efficient,
thereby solving a question posed by Anshu (Harvard Data Science Review, 2022).
  Finally, we extend our results on learning and certification of Gibbs states
to general $k$-local Hamiltonians for any constant $k$.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [135] [Testing chatbots on the creation of encoders for audio conditioned image generation](https://arxiv.org/abs/2509.09717)
*Jorge E. León,Miguel Carrasco*

Main category: cs.SD

TL;DR: 研究探索是否能用聊天机器人设计的音频编码器替代Stable Diffusion 1.5中的CLIP文本编码器，实现直接从声音生成图像。虽然多个聊天机器人能生成有效的模型架构，但所有方案都未能达到满意效果，表明音频嵌入与原始文本编码器无法可靠对齐。


<details>
  <summary>Details</summary>
Motivation: 基于聊天机器人在编码任务中的流行表现，以及当前生成图像模型主要依赖文本编码器而忽略音频输入的现状，研究旨在探索是否能用聊天机器人设计的音频编码器实现声音到图像的合成。

Method: 使用五个公开可用的聊天机器人提出神经架构作为音频编码器，每个有效建议的编码器在超过200万个上下文相关的音频-图像-文本观测数据上训练，并在验证集和测试集上使用多种指标进行评估，同时进行生成图像的定性分析。

Result: 几乎所有聊天机器人都生成了有效的模型设计，但没有一个达到满意结果，表明它们的音频嵌入无法与原始文本编码器可靠对齐。其中Gemini音频编码器在定量指标上表现最佳，而Grok音频编码器生成更连贯的图像（特别是与文本编码器配对时）。

Conclusion: 研究揭示了聊天机器人之间存在共同的架构偏见，并强调了这些模型未来版本需要弥补的编码差距。研究还提出了未来需要解决的研究问题，鼓励其他研究人员进行更专注和高度专业化的任务来充分测试聊天机器人的创造力和推理能力。

Abstract: On one hand, recent advances in chatbots has led to a rising popularity in
using these models for coding tasks. On the other hand, modern generative image
models primarily rely on text encoders to translate semantic concepts into
visual representations, even when there is clear evidence that audio can be
employed as input as well. Given the previous, in this work, we explore whether
state-of-the-art conversational agents can design effective audio encoders to
replace the CLIP text encoder from Stable Diffusion 1.5, enabling image
synthesis directly from sound. We prompted five publicly available chatbots to
propose neural architectures to work as these audio encoders, with a set of
well-explained shared conditions. Each valid suggested encoder was trained on
over two million context related audio-image-text observations, and evaluated
on held-out validation and test sets using various metrics, together with a
qualitative analysis of their generated images. Although almost all chatbots
generated valid model designs, none achieved satisfactory results, indicating
that their audio embeddings failed to align reliably with those of the original
text encoder. Among the proposals, the Gemini audio encoder showed the best
quantitative metrics, while the Grok audio encoder produced more coherent
images (particularly, when paired with the text encoder). Our findings reveal a
shared architectural bias across chatbots and underscore the remaining coding
gap that needs to be bridged in future versions of these models. We also
created a public demo so everyone could study and try out these audio encoders.
Finally, we propose research questions that should be tackled in the future,
and encourage other researchers to perform more focused and highly specialized
tasks like this one, so the respective chatbots cannot make use of well-known
solutions and their creativity/reasoning is fully tested.

</details>


### [136] [CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio](https://arxiv.org/abs/2509.09836)
*Marco Pasini,Stefan Lattner,George Fazekas*

Main category: cs.SD

TL;DR: CoDiCodec是一种新颖的音频自编码器，通过总结嵌入和FSQ技术，在同一模型中同时生成连续嵌入和离散标记，提供11Hz的连续嵌入和2.38kbps的离散标记，在相似比特率下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频自编码器需要在连续嵌入和离散标记之间做出选择，且在高压缩比下保持音频保真度仍然是一个挑战。

Method: 使用有限标量量化(FSQ)和新型FSQ-dropout技术，通过单一一致性损失进行端到端训练，支持自回归解码和新型并行解码策略。

Result: 在相似比特率下，CoDiCodec在重建音频质量方面优于现有的连续和离散自编码器，并行解码策略实现了更优的音频质量和更快的解码速度。

Conclusion: CoDiCodec为音频压缩提供了统一方法，弥合了连续和离散生成建模范式之间的差距。

Abstract: Efficiently representing audio signals in a compressed latent space is
critical for latent generative modelling. However, existing autoencoders often
force a choice between continuous embeddings and discrete tokens. Furthermore,
achieving high compression ratios while maintaining audio fidelity remains a
challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes
these limitations by both efficiently encoding global features via summary
embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz
and discrete tokens at a rate of 2.38 kbps from the same trained model,
offering unprecedented flexibility for different downstream generative tasks.
This is achieved through Finite Scalar Quantization (FSQ) and a novel
FSQ-dropout technique, and does not require additional loss terms beyond the
single consistency loss used for end-to-end training. CoDiCodec supports both
autoregressive decoding and a novel parallel decoding strategy, with the latter
achieving superior audio quality and faster decoding. CoDiCodec outperforms
existing continuous and discrete autoencoders at similar bitrates in terms of
reconstruction audio quality. Our work enables a unified approach to audio
compression, bridging the gap between continuous and discrete generative
modelling paradigms.

</details>


### [137] [Prototypical Contrastive Learning For Improved Few-Shot Audio Classification](https://arxiv.org/abs/2509.10074)
*Christos Sgouropoulos,Christos Nikou,Stefanos Vlachos,Vasileios Theiou,Christos Foukanelis,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: 本文研究了在音频分类的少样本学习中集成监督对比损失的效果，提出使用角度损失替代标准对比损失，结合SpecAugment和自注意力机制，在MetaAudio基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然少样本学习在图像领域已有广泛研究，但在音频分类中相对未被充分探索。本文旨在解决音频分类中标注数据有限的问题，探索如何通过改进损失函数来提升少样本学习性能。

Method: 提出将监督对比损失集成到原型少样本训练中，使用角度损失替代标准对比损失。方法采用SpecAugment进行数据增强，然后通过自注意力机制将增强输入版本的多样化信息封装到统一的嵌入表示中。

Result: 在包含五个数据集的MetaAudio基准测试中，该方法在5-way 5-shot设置下取得了最先进的性能，证明了角度损失相比标准对比损失的改进效果。

Conclusion: 集成监督对比损失特别是角度损失，结合SpecAugment和自注意力机制，能够有效提升音频少样本分类的性能，为音频领域的少样本学习提供了新的有效方法。

Abstract: Few-shot learning has emerged as a powerful paradigm for training models with
limited labeled data, addressing challenges in scenarios where large-scale
annotation is impractical. While extensive research has been conducted in the
image domain, few-shot learning in audio classification remains relatively
underexplored. In this work, we investigate the effect of integrating
supervised contrastive loss into prototypical few shot training for audio
classification. In detail, we demonstrate that angular loss further improves
the performance compared to the standard contrastive loss. Our method leverages
SpecAugment followed by a self-attention mechanism to encapsulate diverse
information of augmented input versions into one unified embedding. We evaluate
our approach on MetaAudio, a benchmark including five datasets with predefined
splits, standardized preprocessing, and a comprehensive set of few-shot
learning models for comparison. The proposed approach achieves state-of-the-art
performance in a 5-way, 5-shot setting.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [138] [SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2509.09942)
*Lei Yu,Jingyuan Zhang,Xin Wang,Jiajia Ma,Li Yang,Fengjun Zhang*

Main category: cs.CR

TL;DR: SmartCoder-R1是一个基于Qwen2.5-Coder-7B的新型框架，通过持续预训练、长思维链监督微调和安全感知强化学习，实现了安全且可解释的智能合约生成，在多项指标上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在智能合约生成中的两个关键问题：作为不可审计的"黑盒"缺乏透明推理过程，以及生成的代码存在严重安全漏洞，可能导致灾难性财务损失。

Method: 1) 持续预训练(CPT)进行模型专业化；2) 在7,998个专家验证的推理-代码样本上进行长思维链监督微调(L-CoT SFT)；3) 使用安全感知组相对策略优化(S-GRPO)进行强化学习，优化编译成功、安全合规和格式正确的加权奖励信号。

Result: 在756个真实世界函数的基准测试中，SmartCoder-R1在17个基线模型中表现最佳：ComPass 87.70%、VulRate 8.60%、SafeAval 80.16%、FuncRate 53.84%、FullRate 50.53%（比最强基线DeepSeek-R1相对提升45.79%）。人类评估显示其推理质量优秀：功能性82.7%、安全性85.3%、清晰度90.7%。

Conclusion: SmartCoder-R1通过结合专业化预训练、思维链微调和安全感知强化学习，成功解决了LLM在智能合约生成中的安全性和可解释性问题，建立了新的最先进性能标准。

Abstract: Smart contracts automate the management of high-value assets, where
vulnerabilities can lead to catastrophic financial losses. This challenge is
amplified in Large Language Models (LLMs) by two interconnected failures: they
operate as unauditable "black boxes" lacking a transparent reasoning process,
and consequently, generate code riddled with critical security vulnerabilities.
To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a
novel framework for secure and explainable smart contract generation. It begins
with Continual Pre-training (CPT) to specialize the model. We then apply Long
Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated
reasoning-and-code samples to train the model to emulate human security
analysis. Finally, to directly mitigate vulnerabilities, we employ
Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement
learning phase that refines the generation policy by optimizing a weighted
reward signal for compilation success, security compliance, and format
correctness. Evaluated against 17 baselines on a benchmark of 756 real-world
functions, SmartCoder-R1 establishes a new state of the art, achieving top
performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a
SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This
FullRate marks a 45.79% relative improvement over the strongest baseline,
DeepSeek-R1. Crucially, its generated reasoning also excels in human
evaluations, achieving high-quality ratings for Functionality (82.7%), Security
(85.3%), and Clarity (90.7%).

</details>


### [139] [Automated Testing of Broken Authentication Vulnerabilities in Web APIs with AuthREST](https://arxiv.org/abs/2509.10320)
*Davide Corradini,Mariano Ceccato,Mohammad Ghafari*

Main category: cs.CR

TL;DR: AuthREST是一个开源安全测试工具，专注于检测API身份验证漏洞，包括凭据填充、密码暴力破解和令牌真实性检查等问题


<details>
  <summary>Details</summary>
Motivation: 针对API安全中最普遍的风险之一——身份验证漏洞，开发自动化测试工具以提高Web API的安全性

Method: 开发开源安全测试工具AuthREST，自动测试Web API的三种主要身份验证漏洞：凭据填充、密码暴力破解和未检查令牌真实性

Result: 实证结果显示AuthREST能有效提升Web API安全性，在四个公共API中发现了之前未知的身份验证漏洞

Conclusion: AuthREST是一个有效的自动化安全测试工具，能够成功检测和帮助修复Web API中的身份验证安全漏洞

Abstract: We present AuthREST, an open-source security testing tool targeting broken
authentication, one of the most prevalent API security risks in the wild.
AuthREST automatically tests web APIs for credential stuffing, password brute
forcing, and unchecked token authenticity. Empirical results show that AuthREST
is effective in improving web API security. Notably, it uncovered previously
unknown authentication vulnerabilitiesin in four public APIs.

</details>


### [140] [Bitcoin Cross-Chain Bridge: A Taxonomy and Its Promise in Artificial Intelligence of Things](https://arxiv.org/abs/2509.10413)
*Guojun Tang,Carylyne Chan,Ning Nan,Spencer Yang,Jiayu Zhou,Henry Leung,Mohammad Mamun,Steve Drew*

Main category: cs.CR

TL;DR: 本文提出了比特币跨链桥协议的全面分类法，系统分析了其在AIoT场景中的信任假设、性能特征和适用性，为AIoT系统设计安全高效的跨链基础设施提供了框架。


<details>
  <summary>Details</summary>
Motivation: 比特币有限的脚本功能和缺乏原生互操作性机制限制了其与更广泛区块链生态系统的集成，特别是在DeFi和多链应用方面。

Method: 将桥接设计分为三类：简单代币交换、锚定资产桥和任意消息桥，并从信任模型、延迟、资本效率和DeFi可组合性等关键指标对每类进行评估。

Result: 提出了系统的分类框架，突出了BitVM和递归侧链等新兴创新技术在实现安全、可扩展和可编程的比特币互操作性方面的潜力。

Conclusion: 该分类法为研究人员和从业者在AIoT系统中设计安全高效的跨链基础设施提供了基础框架，并探索了跨链桥在去中心化能源交易、医疗数据集成和供应链自动化等AIoT应用中的实际用例。

Abstract: Bitcoin's limited scripting capabilities and lack of native interoperability
mechanisms have constrained its integration into the broader blockchain
ecosystem, especially decentralized finance (DeFi) and multi-chain
applications. This paper presents a comprehensive taxonomy of Bitcoin
cross-chain bridge protocols, systematically analyzing their trust assumptions,
performance characteristics, and applicability to the Artificial Intelligence
of Things (AIoT) scenarios. We categorize bridge designs into three main types:
naive token swapping, pegged-asset bridges, and arbitrary-message bridges. Each
category is evaluated across key metrics such as trust model, latency, capital
efficiency, and DeFi composability. Emerging innovations like BitVM and
recursive sidechains are highlighted for their potential to enable secure,
scalable, and programmable Bitcoin interoperability. Furthermore, we explore
practical use cases of cross-chain bridges in AIoT applications, including
decentralized energy trading, healthcare data integration, and supply chain
automation. This taxonomy provides a foundational framework for researchers and
practitioners seeking to design secure and efficient cross-chain
infrastructures in AIoT systems.

</details>


### [141] [Investigating Feature Attribution for 5G Network Intrusion Detection](https://arxiv.org/abs/2509.10206)
*Federica Uccello,Simin Nadjm-Tehrani*

Main category: cs.CR

TL;DR: 本文比较了SHAP和VoTE-XAI两种可解释AI方法在5G网络安全中的表现，发现VoTE-XAI在稀疏性、稳定性和效率方面优于SHAP，能提供更简洁且一致的攻击解释。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络在关键应用中的普及，需要从恶意活动检测转向能够提供可靠判决的系统。理解机器学习模型的安全警报对于实现可操作的事件响应编排至关重要，可解释AI技术通过提供警报原因来增强信任。

Method: 研究比较了两种XAI方法：SHAP（基于统计特征关联）和VoTE-XAI（基于逻辑解释）。在三个不同用例中分析它们对XGBoost模型生成的5G通信攻击警报的解释，使用稀疏性、稳定性和效率三个指标进行评估。

Result: 在92个特征的5G网络中，VoTE-XAI对ICMPFlood DoS攻击仅识别6个重要特征，而SHAP识别超过20个。两种方法选择的特征存在显著差异，但VoTE-XAI没有遗漏SHAP排名靠前的特征。VoTE-XAI在高效性方面明显更优，在478维高维设置下能在0.002秒内提供单个解释。

Conclusion: 基于逻辑解释的VoTE-XAI方法在提供简洁、稳定和高效的网络安全警报解释方面优于传统的统计关联方法SHAP，更适合下一代通信系统的安全需求。

Abstract: With the rise of fifth-generation (5G) networks in critical applications, it
is urgent to move from detection of malicious activity to systems capable of
providing a reliable verdict suitable for mitigation. In this regard,
understanding and interpreting machine learning (ML) models' security alerts is
crucial for enabling actionable incident response orchestration. Explainable
Artificial Intelligence (XAI) techniques are expected to enhance trust by
providing insights into why alerts are raised. A dominant approach
statistically associates feature sets that can be correlated to a given alert.
This paper starts by questioning whether such attribution is relevant for
future generation communication systems, and investigates its merits in
comparison with an approach based on logical explanations. We extensively study
two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts
generated by an XGBoost model in three different use cases with several 5G
communication attacks. We identify three metrics for assessing explanations:
sparsity, how concise they are; stability, how consistent they are across
samples from the same attack type; and efficiency, how fast an explanation is
generated. As an example, in a 5G network with 92 features, 6 were deemed
important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while
SHAP identified over 20. More importantly, we found a significant divergence
between features selected by SHAP and VoTE-XAI. However, none of the top-ranked
features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency
of providing interpretations, we found that VoTE-XAI is significantly more
responsive, e.g. it provides a single explanation in under 0.002 seconds, in a
high-dimensional setting (478 features).

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [142] [DCHO: A Decomposition-Composition Framework for Predicting Higher-Order Brain Connectivity to Enhance Diverse Downstream Applications](https://arxiv.org/abs/2509.09696)
*Weibin Li,Wendu Li,Quanying Liu*

Main category: q-bio.NC

TL;DR: DCHO是一个用于建模和预测高阶脑连接动态演化的统一框架，通过分解-组合策略将预测任务分为HOBC推断和潜在轨迹预测两个子问题，在多个神经影像数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的高阶脑连接分析主要关注静态分析，限制了其在动态预测任务中的应用。需要开发能够建模和预测HOBC时间演化的方法。

Method: 采用分解-组合框架，包含双视图编码器提取多尺度拓扑特征，潜在组合学习器捕获高级HOBC信息，以及潜在空间预测损失增强时间轨迹建模。

Result: 在多个神经影像数据集上的实验表明，DCHO在非预测性任务（状态分类）和预测性任务（脑动力学预测）中都取得了优越性能，显著优于现有方法。

Conclusion: DCHO提供了一个有效的统一框架来建模和预测高阶脑连接的动态演化，为脑连接分析提供了新的工具和方法。

Abstract: Higher-order brain connectivity (HOBC), which captures interactions among
three or more brain regions, provides richer organizational information than
traditional pairwise functional connectivity (FC). Recent studies have begun to
infer latent HOBC from noninvasive imaging data, but they mainly focus on
static analyses, limiting their applicability in dynamic prediction tasks. To
address this gap, we propose DCHO, a unified approach for modeling and
forecasting the temporal evolution of HOBC based on a Decomposition-Composition
framework, which is applicable to both non-predictive tasks (state
classification) and predictive tasks (brain dynamics forecasting). DCHO adopts
a decomposition-composition strategy that reformulates the prediction task into
two manageable subproblems: HOBC inference and latent trajectory prediction. In
the inference stage, we propose a dual-view encoder to extract multiscale
topological features and a latent combinatorial learner to capture high-level
HOBC information. In the forecasting stage, we introduce a latent-space
prediction loss to enhance the modeling of temporal trajectories. Extensive
experiments on multiple neuroimaging datasets demonstrate that DCHO achieves
superior performance in both non-predictive tasks (state classification) and
predictive tasks (brain dynamics forecasting), significantly outperforming
existing methods.

</details>
