<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 64]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 9]
- [quant-ph](#quant-ph) [Total: 3]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.AI](#cs.AI) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.SD](#cs.SD) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [TableCopilot: A Table Assistant Empowered by Natural Language Conditional Table Discovery](https://arxiv.org/abs/2507.08283)
*Lingxi Cui,Guanyu Jiang,Huan Li,Ke Chen,Lidan Shou,Gang Chen*

Main category: cs.DB

TL;DR: TableCopilot是一个基于LLM的交互式表格助手，专注于解决大规模表格池中的表格发现问题，通过自然语言条件和查询表格实现灵活发现。


<details>
  <summary>Details</summary>
Motivation: 现有系统假设用户已有完整表格，忽视了大规模表格池中的发现挑战，TableCopilot旨在填补这一空白。

Method: 提出Crofuma方法，通过学习单模态和跨模态匹配分数的交叉融合，实现精确的表格发现。

Result: Crofuma在NDCG@5上比现有单输入方法至少提升12%。

Conclusion: TableCopilot为交互式表格助手设定了新标准，使高级表格发现更易用和集成。

Abstract: The rise of LLM has enabled natural language-based table assistants, but
existing systems assume users already have a well-formed table, neglecting the
challenge of table discovery in large-scale table pools. To address this, we
introduce TableCopilot, an LLM-powered assistant for interactive, precise, and
personalized table discovery and analysis. We define a novel scenario, nlcTD,
where users provide both a natural language condition and a query table,
enabling intuitive and flexible table discovery for users of all expertise
levels. To handle this, we propose Crofuma, a cross-fusion-based approach that
learns and aggregates single-modal and cross-modal matching scores.
Experimental results show Crofuma outperforms SOTA single-input methods by at
least 12% on NDCG@5. We also release an instructional video, codebase,
datasets, and other resources on GitHub to encourage community contributions.
TableCopilot sets a new standard for interactive table assistants, making
advanced table discovery accessible and integrated.

</details>


### [2] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: xpSHACL是一个可解释的SHACL验证系统，通过结合规则驱动的解释树、RAG和LLM，生成多语言、易读的约束违规解释。


<details>
  <summary>Details</summary>
Motivation: 传统SHACL验证引擎的报告难以被非技术用户理解，需要更友好的解释方式。

Method: 结合规则驱动的解释树、检索增强生成（RAG）和大型语言模型（LLM），并使用违规知识图谱（Violation KG）缓存和复用解释。

Result: 生成详细、多语言、易读的约束违规解释，提高效率和一致性。

Conclusion: xpSHACL解决了传统SHACL验证报告难以理解的问题，为非技术用户提供了更好的支持。

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>


### [3] [ONION: A Multi-Layered Framework for Participatory ER Design](https://arxiv.org/abs/2507.08702)
*Viktoriia Makovska,George Fletcher,Julia Stoyanovich*

Main category: cs.DB

TL;DR: ONION是一个多层次的参与式实体关系建模框架，结合了设计正义、参与式AI和概念建模的见解，通过五阶段方法从非结构化输入生成结构化ER图。


<details>
  <summary>Details</summary>
Motivation: 减少设计者偏见，促进包容性参与，并提高建模过程的透明度。

Method: 五阶段方法：观察、培养、整合、优化、规范化。

Result: 在乌克兰的实际工作坊中验证，展示了多样化的利益相关者参与能生成更丰富的数据模型和更深的理解。

Conclusion: ONION在早期数据建模中具有多样性潜力，但需解决扩展和优化的挑战。

Abstract: We present ONION, a multi-layered framework for participatory
Entity-Relationship (ER) modeling that integrates insights from design justice,
participatory AI, and conceptual modeling. ONION introduces a five-stage
methodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports
progressive abstraction from unstructured stakeholder input to structured ER
diagrams.
  Our approach aims to reduce designer bias, promote inclusive participation,
and increase transparency through the modeling process. We evaluate ONION
through real-world workshops focused on sociotechnical systems in Ukraine,
highlighting how diverse stakeholder engagement leads to richer data models and
deeper mutual understanding. Early results demonstrate ONION's potential to
host diversity in early-stage data modeling. We conclude with lessons learned,
limitations and challenges involved in scaling and refining the framework for
broader adoption.

</details>


### [4] [Hashing for Fast Pattern Set Selection](https://arxiv.org/abs/2507.08745)
*Maiju Karjalainen,Pauli Miettinen*

Main category: cs.DB

TL;DR: 本文提出了一种基于bottom-k哈希的高效模式集挖掘方法，以重构误差为衡量标准，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 模式集挖掘是数据挖掘中的基础问题，但现有方法效率不高，需要一种更高效的解决方案。

Method: 采用bottom-k哈希方法高效选择模式集，并扩展至近似模式场景。

Result: 在合成和真实数据集上，哈希方法比标准贪婪算法更快，且结果质量接近。

Conclusion: 该方法在多种应用中具有潜力，如数据库平铺和布尔矩阵分解。

Abstract: Pattern set mining, which is the task of finding a good set of patterns
instead of all patterns, is a fundamental problem in data mining. Many
different definitions of what constitutes a good set have been proposed in
recent years. In this paper, we consider the reconstruction error as a proxy
measure for the goodness of the set, and concentrate on the adjacent problem of
how to find a good set efficiently. We propose a method based on bottom-k
hashing for efficiently selecting the set and extend the method for the common
case where the patterns might only appear in approximate form in the data. Our
approach has applications in tiling databases, Boolean matrix factorization,
and redescription mining, among others. We show that our hashing-based approach
is significantly faster than the standard greedy algorithm while obtaining
almost equally good results in both synthetic and real-world data sets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Supporting Intel(r) SGX on Multi-Package Platforms](https://arxiv.org/abs/2507.08190)
*Simon Johnson,Raghunandan Makaram,Amy Santoni,Vinnie Scarlata*

Main category: cs.DC

TL;DR: 本文探讨了Intel SGX技术在云环境中的扩展需求，以支持可编程的、高性能且安全的可信执行环境（TEE）。


<details>
  <summary>Details</summary>
Motivation: 随着开发者对Intel SGX技术的熟悉，其在云环境中的应用潜力被探索，但需要进一步平台增强以满足云规模需求。

Method: 提出对Intel SGX技术的扩展和改进，以支持多包平台上的高性能和安全性。

Result: 展示了SGX在云环境中的价值，并指出了未来平台增强的方向。

Conclusion: 通过平台增强，SGX技术可以实现可扩展、高性能且安全的云可信执行环境。

Abstract: Intel(r) Software Guard Extensions (SGX) was originally released on client
platforms and later extended to single socket server platforms. As developers
have become familiar with the capabilities of the technology, the applicability
of this capability in the cloud has been tested. Various Cloud Service
Providers (CSPs) are demonstrating the value of using SGX based Trusted
Execution Environments (TEE) to create a new paradigm of Confidential Cloud
Computing. This paper describes the additional platform enhancements we believe
are necessary to deliver a user programmable Trusted Execution Environment that
scales to cloud usages, performs and is secure on multi-package platforms.

</details>


### [6] [Fast and Interactive Byzantine Fault-tolerant Web Services via Session-Based Consensus Decoupling](https://arxiv.org/abs/2507.08281)
*Ahmad Zaki Akmal,Azkario Rizky Pratama,Guntur Dharma Putra*

Main category: cs.DC

TL;DR: 提出了一种新型的两层架构，通过分离交互操作与共识确认，解决了BFT系统中安全性与响应性之间的矛盾，实现了低于200ms的响应时间。


<details>
  <summary>Details</summary>
Motivation: 解决BFT系统在分布式应用中因高延迟而影响用户体验的问题。

Method: 采用两层架构：Layer 2提供即时反馈，Layer 1确保BFT安全性。

Result: Layer 2操作速度是Layer 1的四倍，同时保持交易完整性。

Conclusion: 该架构使BFT应用在需要高响应性和状态一致性的领域（如元宇宙）变得可行。

Abstract: Byzantine fault-tolerant (BFT) web services provide critical integrity
guarantees for distributed applications but face significant latency challenges
that hinder interactive user experiences. We propose a novel two-layer
architecture that addresses this fundamental tension between security and
responsiveness in BFT systems. Our approach introduces a session-aware
transaction buffer layer (Layer 2) that delivers immediate feedback to users
through consensus simulation, while periodically committing batched operations
to a fully Byzantine fault-tolerant consensus layer (Layer 1). By separating
interactive operations from consensus finalization, our system achieves
responsive user experiences of under 200ms, while maintaining strong BFT
security guarantees. We demonstrate the efficacy of our architecture through a
supply chain management implementation, where operators require both immediate
feedback during multi-step workflows and tamper-proof record keeping. Our
evaluation shows that our Layer 2 operations perform four times faster than the
Layer 1 counterpart, while substantially preserving the end-to-end transaction
integrity. Our approach enables BFT applications in domains previously
considered impractical due to latency constraints, such as metaverse
environments, where users require both responsive interaction and guaranteed
state consistency.

</details>


### [7] [Content-Oblivious Leader Election in 2-Edge-Connected Networks](https://arxiv.org/abs/2507.08348)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 论文研究了完全缺陷异步网络中的领导者选举问题，提出了一种新算法，无需预设领导者即可在2边连通网络中实现终止，从而完全反驳了之前的猜想。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决完全缺陷异步网络中领导者选举的必要性问题，特别是针对之前认为预设领导者是必要条件的猜想。

Method: 提出了一种异步内容无关的领导者选举算法，适用于任何2边连通网络，消息复杂度为O(m⋅N⋅ID_min)。

Result: 算法能够在完全缺陷网络中实现终止的领导者选举，结合之前的模拟结果，证明无需预设领导者即可模拟无噪声环境中的算法。

Conclusion: 完全反驳了预设领导者是必要条件的猜想，为完全缺陷网络中的计算提供了更通用的解决方案。

Abstract: Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 \& Distributed Computing
2023) studied fully-defective asynchronous networks, where communication
channels may suffer an extreme form of alteration errors, rendering messages
completely corrupted. The model is equivalent to content-oblivious computation,
where nodes communicate solely via pulses. They showed that if the network is
2-edge-connected, then any algorithm for a noiseless setting can be simulated
in the fully-defective setting; otherwise, no non-trivial computation is
possible in the fully-defective setting. However, their simulation requires a
predesignated leader, which they conjectured to be necessary for any
non-trivial content-oblivious task.
  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture
for the special case of oriented ring topology. They designed two asynchronous
content-oblivious leader election algorithms with message complexity $O(n \cdot
\mathsf{ID}_{\max})$, where $n$ is the number of nodes and $\mathsf{ID}_{\max}$
is the maximum $\mathsf{ID}$. The first algorithm stabilizes in unoriented
rings without termination detection. The second algorithm quiescently
terminates in oriented rings, thus enabling the execution of the simulation
algorithm after leader election.
  In this work, we present an asynchronous content-oblivious leader election
algorithm that quiescently terminates in any 2-edge connected network with
message complexity $O(m \cdot N \cdot \mathsf{ID}_{\min})$, where $m$ is the
number of edges, $N$ is a known upper bound on the number of nodes, and
$\mathsf{ID}_{\min}$ is the smallest $\mathsf{ID}$. Combined with the previous
simulation result, our finding implies that any algorithm from the noiseless
setting can be simulated in the fully-defective setting without assuming a
preselected leader, entirely refuting the original conjecture.

</details>


### [8] [Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline Constraint](https://arxiv.org/abs/2507.08725)
*Dominik Schweisgut,Anne Benoit,Yves Robert,Henning Meyerhenke*

Main category: cs.DC

TL;DR: 论文提出了一种调度方法CaWoSched，用于减少数据中心工作流任务执行时的碳排放，通过优化任务调度以利用绿色能源。


<details>
  <summary>Details</summary>
Motivation: 大型数据中心的能源消耗巨大，其中工作流任务（DAGs）的碳排放问题突出，需要优化调度以减少对非可再生能源的依赖。

Method: 将问题形式化为调度问题，提出多项式时间解（单处理器）和NP难解（多处理器），并设计启发式框架CaWoSched结合贪婪算法和局部搜索。

Result: 实验表明，CaWoSched相比基线算法显著减少了碳排放。

Conclusion: CaWoSched是一种有效的启发式方法，能够在多处理器环境下优化任务调度以减少碳排放。

Abstract: Large data and computing centers consume a significant share of the world's
energy consumption. A prominent subset of the workloads in such centers are
workflows with interdependent tasks, usually represented as directed acyclic
graphs (DAGs). To reduce the carbon emissions resulting from executing such
workflows in centers with a mixed (renewable and non-renewable) energy supply,
it is advisable to move task executions to time intervals with sufficient green
energy when possible. To this end, we formalize the above problem as a
scheduling problem with a given mapping and ordering of the tasks. We show that
this problem can be solved in polynomial time in the uniprocessor case. For at
least two processors, however, the problem becomes NP-hard. Hence, we propose a
heuristic framework called CaWoSched that combines several greedy approaches
with local search. To assess the 16 heuristics resulting from different
combinations, we also devise a simple baseline algorithm and an exact ILP-based
solution. Our experimental results show that our heuristics provide significant
savings in carbon emissions compared to the baseline.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [9] [On the Parallel Complexity of Finding a Matroid Basis](https://arxiv.org/abs/2507.08194)
*Sanjeev Khanna,Aaron Putterman,Junkai Song*

Main category: cs.DS

TL;DR: 论文提出了一种新的并行算法，将寻找任意拟阵基的轮数复杂度从长期以来的$O(\sqrt{n})$降低到$\tilde{O}(n^{7/15})$，并针对分区拟阵提出了$\tilde{O}(n^{1/3})$轮算法。


<details>
  <summary>Details</summary>
Motivation: 解决Karp等人提出的关于在拟阵中寻找基的并行计算问题，填补了$O(\sqrt{n})$和$\widetilde{\Omega}(n^{1/3})$之间的长期未解决的复杂度差距。

Method: 引入了一种新的拟阵分解技术和其他结构洞察，不仅适用于一般拟阵，还针对分区拟阵设计了更高效的算法。

Result: 实现了$\tilde{O}(n^{7/15})$轮的并行算法，显著改进了之前的$O(\sqrt{n})$上界，并为分区拟阵提供了$\tilde{O}(n^{1/3})$轮算法。

Conclusion: 论文首次缩小了拟阵基问题的复杂度差距，为并行计算中的拟阵问题提供了新的理论突破。

Abstract: A fundamental question in parallel computation, posed by Karp, Upfal, and
Wigderson (FOCS 1985, JCSS 1988), asks: \emph{given only independence-oracle
access to a matroid on $n$ elements, how many rounds are required to find a
basis using only polynomially many queries?} This question generalizes, among
others, the complexity of finding bases of linear spaces, partition matroids,
and spanning forests in graphs. In their work, they established an upper bound
of $O(\sqrt{n})$ rounds and a lower bound of $\widetilde{\Omega}(n^{1/3})$
rounds for this problem, and these bounds have remained unimproved since then.
  In this work, we make the first progress in narrowing this gap by designing a
parallel algorithm that finds a basis of an arbitrary matroid in
$\tilde{O}(n^{7/15})$ rounds (using polynomially many independence queries per
round) with high probability, surpassing the long-standing $O(\sqrt{n})$
barrier. Our approach introduces a novel matroid decomposition technique and
other structural insights that not only yield this general result but also lead
to a much improved new algorithm for the class of \emph{partition matroids}
(which underlies the $\widetilde\Omega(n^{1/3})$ lower bound of Karp, Upfal,
and Wigderson). Specifically, we develop an $\tilde{O}(n^{1/3})$-round
algorithm, thereby settling the round complexity of finding a basis in
partition matroids.

</details>


### [10] [Approximation Algorithms for the Cumulative Vehicle Routing Problem with Stochastic Demands](https://arxiv.org/abs/2507.08316)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文提出了针对Cu-VRPSD、VRPSD和Cu-VRP的随机近似算法，显著改进了已知的近似比率。


<details>
  <summary>Details</summary>
Motivation: 解决累积车辆路径问题（Cu-VRP）及其随机需求变体（Cu-VRPSD）的优化问题，提高近似算法的性能。

Method: 提出随机化近似算法，基于车辆负载的累积成本进行优化，并允许客户需求通过多次访问满足。

Result: Cu-VRPSD的近似比率从6降至3.456，VRPSD从3.5降至3.25，Cu-VRP从4降至3.194。

Conclusion: 算法显著提升了近似比率，为相关路径优化问题提供了更高效的解决方案。

Abstract: In the Cumulative Vehicle Routing Problem (Cu-VRP), we need to find a
feasible itinerary for a capacitated vehicle located at the depot to satisfy
customers' demand, as in the well-known Vehicle Routing Problem (VRP), but the
goal is to minimize the cumulative cost of the vehicle, which is based on the
vehicle's load throughout the itinerary. If the demand of each customer is
unknown until the vehicle visits it, the problem is called Cu-VRP with
Stochastic Demands (Cu-VRPSD). Assume that the approximation ratio of metric
TSP is $1.5$. In this paper, we propose a randomized $3.456$-approximation
algorithm for Cu-VRPSD, improving the best-known approximation ratio of $6$
(Discret. Appl. Math. 2020). Since VRP with Stochastic Demands (VRPSD) is a
special case of Cu-VRPSD, as a corollary, we also obtain a randomized
$3.25$-approximation algorithm for VRPSD, improving the best-known
approximation ratio of $3.5$ (Oper. Res. 2012). For Cu-VRP, we give a
randomized $3.194$-approximation algorithm, improving the best-known
approximation ratio of $4$ (Oper. Res. Lett. 2013). Moreover, if each customer
is allowed to be satisfied by using multiple tours, we obtain further
improvements for Cu-VRPSD and Cu-VRP.

</details>


### [11] [H-Planarity and Parametric Extensions: when Modulators Act Globally](https://arxiv.org/abs/2507.08541)
*Fedor V. Fomin,Petr A. Golovach,Laure Morelle,Dimitrios M. Thilikos*

Main category: cs.DS

TL;DR: 论文提出了一种基于图分解的H-平面调制器算法，扩展了平面图的算法潜力，并应用于多种图问题。


<details>
  <summary>Details</summary>
Motivation: 通过H-平面调制器扩展平面图的算法能力，解决更多图问题。

Method: 提出多项式时间算法计算H-平面调制器，并定义H-平面树深和H-平面树宽概念。

Result: 证明了在特定条件下H-平面性可在多项式时间内解决，并设计了多种FPT算法和近似算法。

Conclusion: 该方法为图着色、完美匹配等问题提供了高效算法，并设计了EPTAS方案。

Abstract: We introduce a series of graph decompositions based on the modulator/target
scheme of modification problems that enable several algorithmic applications
that parametrically extend the algorithmic potential of planarity. In the core
of our approach is a polynomial time algorithm for computing planar
H-modulators. Given a graph class H, a planar H-modulator of a graph G is a set
X \subseteq V(G) such that the ``torso'' of X is planar and all connected
components of G - X belong to H. Here, the torso of X is obtained from G[X] if,
for every connected component of G-X, we form a clique out of its neighborhood
on G[X]. We introduce H-Planarity as the problem of deciding whether a graph G
has a planar H-modulator. We prove that, if H is hereditary, CMSO-definable,
and decidable in polynomial time, then H-Planarity is solvable in polynomial
time. Further, we introduce two parametric extensions of H-Planarity by
defining the notions of H-planar treedepth and H-planar treewidth, which
generalize the concepts of elimination distance and tree decompositions to the
class H. Combining this result with existing FPT algorithms for various
H-modulator problems, we thereby obtain FPT algorithms parameterized by
H-planar treedepth and H-planar treewidth for numerous graph classes H. By
combining the well-known algorithmic properties of planar graphs and graphs of
bounded treewidth, our methods for computing H-planar treedepth and H-planar
treewidth lead to a variety of algorithmic applications. For instance, once we
know that a given graph has bounded H-planar treedepth or bounded H-planar
treewidth, we can derive additive approximation algorithms for graph coloring
and polynomial-time algorithms for counting (weighted) perfect matchings.
Furthermore, we design Efficient Polynomial-Time Approximation Schemes
(EPTAS-es) for several problems, including Maximum Independent Set.

</details>


### [12] [Beer Path Problems in Temporal Graphs](https://arxiv.org/abs/2507.08685)
*Andrea D'Ascenzo,Giuseppe F. Italiano,Sotiris Kanellopoulos,Anna Mpanti,Aris Pagourtzis,Christos Pergaminelis*

Main category: cs.DS

TL;DR: 本文提出了在时间图中计算啤酒路径的概念，解决了现有静态图方法无法处理时间信息的问题，并提出了高效算法和预处理技术。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，如交通网络或商店营业时间，时间信息至关重要，但现有啤酒路径研究未考虑时间依赖性。

Method: 在时间图中定义啤酒路径问题，提出最早到达、最晚出发、最快和最短路径算法，并支持动态更新。

Result: 算法的时间复杂度与现有时间路径算法一致，保持了高效性，并支持动态条件下的查询。

Conclusion: 本文扩展了啤酒路径问题到时间图，提供了高效的解决方案，适用于动态场景。

Abstract: Computing paths in graph structures is a fundamental operation in a wide
range of applications, from transportation networks to data analysis. The beer
path problem, which captures the option of visiting points of interest, such as
gas stations or convenience stops, prior to reaching the final destination, has
been recently introduced and extensively studied in static graphs. However,
existing approaches do not account for temporal information, which is often
crucial in real-world scenarios. For instance, transit services may follow
fixed schedules, and shops may only be accessible during certain hours.
  In this work, we introduce the notion of beer paths in temporal graphs, where
edges are time-dependent and certain vertices (beer vertices) are active only
at specific time instances. We formally define the problems of computing
earliest-arrival, latest-departure, fastest, and shortest temporal beer paths
and propose efficient algorithms for these problems under both edge stream and
adjacency list representations. The time complexity of each of our algorithms
is aligned with that of corresponding temporal pathfinding algorithms, thus
preserving efficiency.
  Additionally, we present preprocessing techniques that enable efficient query
answering under dynamic conditions, for example new openings or closings of
shops. We achieve this through appropriate precomputation of selected paths or
by transforming a temporal graph into an equivalent static graph.

</details>


### [13] [On the Constant-Factor Approximability of Minimum Cost Constraint Satisfaction Problems](https://arxiv.org/abs/2507.08693)
*Ian DeHaan,Neng Huang,Euiwoong Lee*

Main category: cs.DS

TL;DR: 研究了最小成本约束满足问题（MinCostCSP）的近似算法，发现具有双判别器多态性的约束语言存在|D|-近似算法，而无近一致性多态性的语言则难以近似。


<details>
  <summary>Details</summary>
Motivation: 探索约束语言的多态性对MinCostCSP近似算法的影响，填补现有研究的空白。

Method: 通过代数方法分析约束语言的多态性，提出近似算法并证明其有效性。

Result: 发现具有双判别器多态性的语言可|D|-近似，而无近一致性多态性的语言则难以近似。

Conclusion: 约束语言的多态性决定了MinCostCSP的近似性，形成了一种二分法。

Abstract: We study minimum cost constraint satisfaction problems (MinCostCSP) through
the algebraic lens. We show that for any constraint language $\Gamma$ which has
the dual discriminator operation as a polymorphism, there exists a
$|D|$-approximation algorithm for MinCostCSP$(\Gamma)$ where $D$ is the domain.
Complementing our algorithmic result, we show that any constraint language
$\Gamma$ where MinCostCSP$(\Gamma)$ admits a constant-factor approximation must
have a \emph{near-unanimity} (NU) polymorphism unless P = NP, extending a
similar result by Dalmau et al. on MinCSPs. These results imply a dichotomy of
constant-factor approximability for constraint languages that contain all
permutation relations (a natural generalization for Boolean CSPs that allow
variable negation): either MinCostCSP$(\Gamma)$ has an NU polymorphism and is
$|D|$-approximable, or it does not have any NU polymorphism and is NP-hard to
approximate within any constant factor. Finally, we present a constraint
language which has a majority polymorphism, but is nonetheless NP-hard to
approximate within any constant factor assuming the Unique Games Conjecture,
showing that the condition of having an NU polymorphism is in general not
sufficient unless UGC fails.

</details>


### [14] [To buy or not to buy: deterministic rent-or-buy problems on node-weighted graphs](https://arxiv.org/abs/2507.08698)
*Sander Borst,Moritz Venzin*

Main category: cs.DS

TL;DR: 论文研究了节点和边加权图中在线Steiner森林问题的租或买变体，提出了一种确定性算法，改进了之前的竞争比，并扩展到随机化算法。


<details>
  <summary>Details</summary>
Motivation: 改进现有算法在节点和边加权图中的竞争比，特别是针对租或买变体的在线Steiner森林问题。

Method: 提出了一种新的确定性算法，基于在线奖赏收集集覆盖实例的充电方案，扩展了Umboh (2015)的见证技术。

Result: 获得了O(log n log n̄)-竞争的确定性算法和O(n̄ log k̃)-竞争的确定性算法，以及O(log k̃ log n̄)-竞争的随机化算法。

Conclusion: 新算法在竞争比上优于现有方法，技术核心是通过充电方案扩展见证技术，适用于更广泛的场景。

Abstract: We study the rent-or-buy variant of the online Steiner forest problem on
node- and edge-weighted graphs. For $n$-node graphs with at most $\bar{n}$
non-zero node-weights, and at most $\tilde{k}$ different arriving terminal
pairs, we obtain a deterministic, $O(\log n \log \bar{n})$-competitive
algorithm. This improves on the previous best, $O(\log^4 n)$-competitive
algorithm obtained by the black-box reduction from (Bartal et al. 2021)
combined with the previously best deterministic algorithms for the simpler
'buy-only' setting. We also obtain a deterministic, $O(\bar{n}\log
\tilde{k})$-competitive algorithm. This generalizes the $O(\log
\tilde{k})$-competitive algorithm for the purely edge-weighted setting from
(Umboh 2015). We also obtain a randomized, $O(\log \tilde{k} \log
\bar{n})$-competitive algorithm. All previous approaches were based on the
randomized, black-box reduction from~\cite{AwerbuchAzarBartal96} that achieves
a $O(\log \tilde{k} \log n)$-competitive ratio when combined with an algorithm
for the 'buy-only' setting. Our key technical ingredient is a novel charging
scheme to an instance of \emph{online prize-collecting set cover}. This allows
us to extend the witness-technique of (Umboh 2015) to the node-weighted setting
and obtain refined guarantees with respect to $\bar{n}$, already in the much
simpler 'buy-only' setting.

</details>


### [15] [On Fair Epsilon Net and Geometric Hitting Set](https://arxiv.org/abs/2507.08758)
*Mohsen Dehghankar,Stavros Sintos,Abolfazl Asudeh*

Main category: cs.DS

TL;DR: 论文探讨了在几何近似问题中引入公平性，提出了两种公平性算法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 填补几何近似工具（如ε-net）在公平性视角下的研究空白。

Method: 提出基于采样和基于差异理论的两种算法，分别用于实现公平性。

Result: 采样算法速度快但输出稍大，差异算法速度稍慢但输出更小；实验验证了算法的实用性。

Conclusion: 在几何近似问题中实现公平性是可行的，且算法在实际中表现良好。

Abstract: Fairness has emerged as a formidable challenge in data-driven decisions. Many
of the data problems, such as creating compact data summaries for approximate
query processing, can be effectively tackled using concepts from computational
geometry, such as $\varepsilon$-nets. However, these powerful tools have yet to
be examined from the perspective of fairness. To fill this research gap, we add
fairness to classical geometric approximation problems of $\varepsilon$-net,
$\varepsilon$-sample, and geometric hitting set. We introduce and address two
notions of group fairness: demographic parity, which requires preserving group
proportions from the input distribution, and custom-ratios fairness, which
demands satisfying arbitrary target ratios. We develop two algorithms to
enforce fairness: one based on sampling and another on discrepancy theory. The
sampling-based algorithm is faster and computes a fair $\varepsilon$-net of
size which is only larger by a $\log(k)$ factor compared to the standard
(unfair) $\varepsilon$-net, where $k$ is the number of demographic groups. The
discrepancy-based algorithm is slightly slower (for bounded VC dimension), but
it computes a smaller fair $\varepsilon$-net. Notably, we reduce the fair
geometric hitting set problem to finding fair $\varepsilon$-nets. This results
in a $O(\log \mathsf{OPT} \times \log k)$ approximation of a fair geometric
hitting set. Additionally, we show that under certain input distributions,
constructing fair $\varepsilon$-samples can be infeasible, highlighting
limitations in fair sampling. Beyond the theoretical guarantees, our
experimental results validate the practical effectiveness of the proposed
algorithms. In particular, we achieve zero unfairness with only a modest
increase in output size compared to the unfair setting.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: 调查显示，核工程领域的计算科学家更倾向于使用现代编程语言、开源代码和模块化软件，预示着未来5到10年的行业趋势。


<details>
  <summary>Details</summary>
Motivation: 研究核工程中软件工具的使用情况，以了解当前趋势及未来发展方向。

Method: 对103位从事核聚变和核裂变能源代码开发的科学家进行问卷调查，涵盖问题、工具和开发体验。

Result: 发现现代编程语言（如Python、C++）取代FORTRAN的趋势，多物理场代码的兴起，以及代码开发预算的增加（高达5000万美元）。

Conclusion: 核工程代码未来将更加模块化、计算量更小，并受到组织的高度重视。

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [17] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 研究探讨了开发者与编码代理的互动，发现其潜力超越现有Copilot工具，但也面临用户理解不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索更自主的AI工具（编码代理）如何影响开发者生产力和体验，并与现有Copilot工具对比。

Method: 通过实验评估GitHub Copilot和OpenHands两种工具，招募常用Copilot的开发者参与。

Result: 编码代理能完成Copilot无法完成的任务并减少用户努力，但需解决用户理解代理行为的挑战。

Conclusion: 编码代理有潜力改变开发者工作流，但需进一步研究以促进其广泛采用。

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [18] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: 研究探讨了GenAI工具对源代码生成的影响，发现其可能降低开发者对代码的理解，进而影响知识模型和Truck Factor算法的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI工具在软件开发中的广泛应用，开发者可能过度依赖这些工具，导致对生成代码的理解不足，从而影响专业知识模型的准确性。

Method: 通过收集ChatGPT生成代码在GitHub项目中的统计数据，并模拟不同GenAI贡献程度的情景，分析其对知识模型和Truck Factor算法的影响。

Result: 大多数情景下，GenAI的使用对专业知识模型产生了可测量的影响，表明当前指标对GenAI的敏感性。

Conclusion: 随着GenAI在开发流程中的深入应用，现有专业知识指标的可靠性可能下降。

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [19] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 论文评估了四种先进LLM（GPT-3.5-Turbo、GPT-4、Flan-T5和Llama3-70b）在用户反馈分类中的能力，旨在解决标注数据不足的问题。通过实验发现，LLM能有效分类粗粒度反馈，且通过LLM标注的数据增强可显著提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，但标注成本高且耗时。研究探索LLM在用户反馈分类中的潜力，以缓解数据不足的挑战。

Method: 在八个标注数据集上实验四种LLM，分析其在细粒度和粗粒度分类中的表现，并利用LLM作为标注工具增强训练数据。

Result: LLM在粗粒度分类中表现良好，且通过LLM标注的数据增强显著提升了BERT分类器的性能。

Conclusion: LLM可作为高效标注工具，增强用户反馈分类任务，尤其在数据有限的情况下表现突出。

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [20] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: PI-detector是一种新方法，用于高效计算浮点程序中的错误，通过注入小扰动并比较结果来检测错误。


<details>
  <summary>Details</summary>
Motivation: 浮点错误可能导致严重后果，现有工具如ATOMU和FPCC存在假阳性或速度慢的问题，需要更优解决方案。

Method: 基于原子操作的条件数观察，PI-detector通过扰动操作数并比较结果来计算浮点错误。

Result: 实验证明PI-detector能高效且准确地计算浮点错误。

Conclusion: PI-detector解决了现有工具的局限性，提供了一种有效的浮点错误检测方法。

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [21] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: InferLog是一种针对在线日志解析的LLM推理优化方法，通过改进上下文学习示例和配置调优，显著提升推理效率而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统生成大量运行时日志，需要高效准确的日志解析以支持关键任务。现有基于LLM的解析器存在隐私风险和高延迟问题。

Method: InferLog设计了前缀感知的上下文学习优化策略和基于元学习的配置调优管道，以加速推理。

Result: 实验表明，InferLog显著优于现有优化方法，并大幅提升LLM日志解析器的性能。

Conclusion: InferLog解决了LLM在日志解析中的推理效率瓶颈，为实际部署提供了可行方案。

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [22] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 论文提出了一种基于提示工程的生成式AI方法，用于自动生成原型人物角色，以提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 手动创建原型人物角色耗时、费力且易偏颇，因此需要一种更高效的方法。

Method: 采用生成式AI和提示工程方法，通过案例研究（19名参与者）进行定性和定量评估。

Result: 方法显著减少了时间和精力，提高了人物角色的质量和可重用性，但在泛化和领域特异性方面存在局限。

Conclusion: 生成式AI可有效整合到产品发现实践中，但需解决泛化和情感共鸣的挑战。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [23] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: 研究发现，大型语言模型（LLM）在代码翻译中容易出错。通过自然语言（NL）和抽象语法树（AST）等中间表示可以提升翻译准确性。实验表明，使用链式思维（CoT）提示结合NL摘要效果最佳，Open Gpt4 8X7B模型在CodeNet和AVATAR基准测试中分别提升了13.8%和6.7%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译中表现不佳，需要探索如何通过中间表示（如NL和AST）提升其准确性。

Method: 研究了多种提示方法（从单次提示到链式思维提示）结合中间表示（NL和AST）对代码翻译的影响，并在Open Gpt4 8X7B、StarCoder和CodeGen模型上进行了测试。

Result: 链式思维提示结合NL摘要效果最好，Open Gpt4 8X7B模型在CodeNet和AVATAR基准测试中分别提升了13.8%和6.7%。

Conclusion: 中间表示（尤其是NL摘要）结合链式思维提示能显著提升LLM的代码翻译准确性。

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [24] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: 论文提出了一种基于大型语言模型（LLM）的注释更新框架LLMCup，通过多提示策略生成候选注释，并结合排名模型CupRank选择最佳注释，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发者常更新代码但忽略注释，导致注释过时或不一致，影响代码维护。现有自动注释更新方法（如CUP和HebCup）存在信息遗漏或误解问题，尤其是在复杂场景下。

Method: LLMCup利用LLM生成多样候选注释，并通过CupRank模型选择最佳注释。

Result: 实验显示LLMCup在准确性、BLEU-4、METEOR、F1和SentenceBert相似度上显著优于CUP和HebCup，部分情况下甚至优于人工更新。

Conclusion: LLMCup展示了LLM在注释更新任务中的潜力，并强调了人工评估在注释质量中的重要性。

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [25] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA是一个在线配置性能学习框架，通过双重层次适应机制处理动态环境中的全局和局部概念漂移。


<details>
  <summary>Details</summary>
Motivation: 现代可配置软件系统在动态环境中运行时，工作负载变化、硬件更新等会导致概念漂移，现有离线学习方法难以实时适应。

Method: DHDA采用双重层次适应：上层重新划分数据以处理全局漂移，下层局部模型异步检测并适应局部漂移，结合增量更新和定期重训练。

Result: 在八个软件系统上的评估显示，DHDA显著提高了准确性，适应漂移能力提升2倍，同时保持合理开销。

Conclusion: DHDA能有效处理动态环境中的概念漂移，优于现有方法。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [26] [Photonic Rails in ML Datacenters](https://arxiv.org/abs/2507.08119)
*Eric Ding,Chuhan Ouyang,Rachee Singh*

Main category: cs.NI

TL;DR: 论文提出了一种基于光电路交换的铁路优化网络架构，以解决传统高基数电交换机在ML训练中的功耗、成本和复杂性高的问题。通过并行驱动铁路重构和Opus控制平面，实现了光交换机的时分复用模拟。


<details>
  <summary>Details</summary>
Motivation: 传统铁路优化网络架构使用高基数电交换机实现全连接，但带来了巨大的功耗、成本和复杂性开销。

Method: 提出并行驱动铁路重构，利用光电路交换机的时分复用特性，设计控制平面Opus模拟电交换机的功能。

Result: 实现了光交换机对电交换机的时分复用模拟，支持ML工作负载中的混合并行通信。

Conclusion: 论文提出了一种与模型并行维度协同演化的数据中心网络架构新研究方向。

Abstract: Rail-optimized network fabrics have become the de facto datacenter scale-out
fabric for large-scale ML training. However, the use of high-radix electrical
switches to provide all-to-all connectivity in rails imposes massive power,
cost, and complexity overheads. We propose a rethinking of the rail abstraction
by retaining its communication semantics, but realizing it using optical
circuit switches. The key challenge is that optical switches support only
one-to-one connectivity at a time, limiting the fan-out of traffic in ML
workloads using hybrid parallelisms. We introduce parallelism-driven rail
reconfiguration as a solution that leverages the sequential ordering between
traffic from different parallelisms. We design a control plane, Opus, to enable
time-multiplexed emulation of electrical rail switches using optical switches.
More broadly, our work discusses a new research agenda: datacenter fabrics that
co-evolve with the model parallelism dimensions within each job, as opposed to
the prevailing mindset of reconfiguring networks before a job begins.

</details>


### [27] [Rattan: An Extensible and Scalable Modular Internet Path Emulator](https://arxiv.org/abs/2507.08134)
*Minhu Wang,Yixin Shen,Bo Wang,Haixuan Tong,Yutong Xie,Yixuan Gao,Yan Liu,Li Chen,Mingwei Xu,Jianping Wu*

Main category: cs.NI

TL;DR: Rattan是一种可扩展且可扩展的软件网络路径模拟器，用于现代互联网条件，通过模块化设计解决现有模拟器的不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有网络路径模拟器在灵活性、可扩展性和可用性方面的不足。

Method: 采用基于单元的架构，将模拟功能拆分为模块化单元，支持用户通过层次化链接组合单元或构建新单元。

Result: 支持单机上数百个千兆级路径的并发模拟，以及多机集群级实验，同时具备模拟新网络条件的能力。

Conclusion: Rattan为开发者和研究人员提供了一种高效、可靠的工具，用于评估和验证网络传输创新。

Abstract: The rapid growth of Internet paths in heterogeneity, scale, and dynamics has
made existing emulators increasingly insufficient in flexibility, scalability,
and usability. To address these limitations, we present Rattan, an extensible
and scalable software network path emulator for modern Internet conditions.
Rattan's core innovation lies in its cell-based architecture: by splitting
emulation functions into modular "cells" with well-documented asynchronous
interfaces, users are allowed to easily compose different cells by
hierarchically linking them and easily construct new cells by using standard
cell interfaces. This design enables: (1) scalability, supporting hundreds of
concurrent gigabit-level paths on a single machine and cluster-level
experiments composed of multiple machines; (2) extensibility, simulating new
network conditions by constructing new cells. Rattan empowers developers and
researchers to efficiently and confidently evaluate, validate, and diagnose
diverse network transport innovations for online services.

</details>


### [28] [KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence](https://arxiv.org/abs/2507.08164)
*Yun Tang,Mengbang Zou,Zeinab Nezami,Syed Ali Raza Zaidi,Weisi Guo*

Main category: cs.NI

TL;DR: KP-A是一个统一的网络知识平面，旨在简化6G网络中智能代理的开发与维护，提升互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前6G网络中智能任务的实现需要独立的知识检索流程，导致数据冗余和解释不一致。

Method: 提出KP-A，将网络知识获取与管理与智能逻辑解耦，提供一致的知识接口。

Result: 在实时网络知识问答和边缘AI服务编排任务中验证了KP-A的有效性。

Conclusion: KP-A为智能代理提供了高效、一致的知识管理方案，支持未来标准化。

Abstract: The emergence of large language models (LLMs) and agentic systems is enabling
autonomous 6G networks with advanced intelligence, including
self-configuration, self-optimization, and self-healing. However, the current
implementation of individual intelligence tasks necessitates isolated knowledge
retrieval pipelines, resulting in redundant data flows and inconsistent
interpretations. Inspired by the service model unification effort in Open-RAN
(to support interoperability and vendor diversity), we propose KP-A: a unified
Network Knowledge Plane specifically designed for Agentic network intelligence.
By decoupling network knowledge acquisition and management from intelligence
logic, KP-A streamlines development and reduces maintenance complexity for
intelligence engineers. By offering an intuitive and consistent knowledge
interface, KP-A also enhances interoperability for the network intelligence
agents. We demonstrate KP-A in two representative intelligence tasks: live
network knowledge Q&A and edge AI service orchestration. All implementation
artifacts have been open-sourced to support reproducibility and future
standardization efforts.

</details>


### [29] [Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization](https://arxiv.org/abs/2507.08403)
*Nan Li,Qi Sun,Lehan Wang,Xiaofei Xu,Jinri Huang,Chunhui Liu,Jing Gao,Yuhong Huang,Chih-Lin I*

Main category: cs.NI

TL;DR: 本文探讨了6G移动网络中AI原生无线接入网络（RAN）的设计与标准化原则，重点研究了其Day 1架构、功能和能力，并提出了标准化方向。


<details>
  <summary>Details</summary>
Motivation: 5G中AI/ML未原生集成，而6G需要从设计之初就整合AI以应对复杂性和支持广泛AI应用。

Method: 基于2G至5G的网络运营和标准化经验，提出了AI-Native RAN框架及其三大核心能力：AI驱动的RAN处理/优化/自动化、可靠的AI生命周期管理（LCM）和AI即服务（AIaaS）。

Result: 通过大规模现场试验验证，提出的架构和AI功能显著改善了平均空口延迟、根因识别和网络能耗。

Conclusion: 本文为6G AI-Native RAN的标准化设计提供了Day 1框架，平衡了技术创新与实际部署。

Abstract: Artificial Intelligence/Machine Learning (AI/ML) has become the most certain
and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not
natively integrated but rather an add-on feature over existing architecture, 6G
shall incorporate AI from the onset to address its complexity and support
ubiquitous AI applications. Based on our extensive mobile network operation and
standardization experience from 2G to 5G, this paper explores the design and
standardization principles of AI-Native radio access networks (RAN) for 6G,
with a particular focus on its critical Day 1 architecture, functionalities and
capabilities. We investigate the framework of AI-Native RAN and present its
three essential capabilities to shed some light on the standardization
direction; namely, AI-driven RAN processing/optimization/automation, reliable
AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The
standardization of AI-Native RAN, in particular the Day 1 features, including
an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale
field trial with over 5000 5G-A base stations have been built and delivered
significant improvements in average air interface latency, root cause
identification, and network energy consumption with the proposed architecture
and the supporting AI functions. This paper aims to provide a Day 1 framework
for 6G AI-Native RAN standardization design, balancing technical innovation
with practical deployment.

</details>


### [30] [Age of Information Optimization in Laser-charged UAV-assisted IoT Networks: A Multi-agent Deep Reinforcement Learning Method](https://arxiv.org/abs/2507.08429)
*Geng Sun,Likun Zhang,Jiahui Li,Jing Wu,Jiacheng Wang,Zemin Sun,Changyuan Zhao,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 论文研究了激光束充电无人机（UAV）辅助物联网（IoT）网络中的信息时效性（AoI）优化问题，提出了一种多智能体近端策略优化框架（MAPPO-TM），显著降低了峰值AoI并提高了能源效率。


<details>
  <summary>Details</summary>
Motivation: 无人机在物联网数据收集中面临能量限制的挑战，激光束充电技术为解决这一问题提供了可能，但需要优化信息时效性和能源效率。

Method: 提出了一种结合时间记忆和多智能体协调的MAPPO-TM框架，通过联合优化无人机轨迹和激光充电策略，解决非凸性问题。

Result: 仿真结果显示，MAPPO-TM算法在峰值AoI最小化和能源效率方面优于传统方法，峰值AoI降低了15.1%。

Conclusion: MAPPO-TM框架为激光充电无人机辅助物联网网络提供了一种高效的解决方案，显著提升了信息时效性和能源效率。

Abstract: The integration of unmanned aerial vehicles (UAVs) with Internet of Things
(IoT) networks offers promising solutions for efficient data collection.
However, the limited energy capacity of UAVs remains a significant challenge.
In this case, laser beam directors (LBDs) have emerged as an effective
technology for wireless charging of UAVs during operation, thereby enabling
sustained data collection without frequent returns to charging stations (CSs).
In this work, we investigate the age of information (AoI) optimization in
LBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from
distributed IoTs while being recharged by laser beams. We formulate a joint
optimization problem that aims to minimize the peak AoI while determining
optimal UAV trajectories and laser charging strategies. This problem is
particularly challenging due to its non-convex nature, complex temporal
dependencies, and the need to balance data collection efficiency with energy
consumption constraints. To address these challenges, we propose a novel
multi-agent proximal policy optimization with temporal memory and multi-agent
coordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal
memory mechanisms to capture the dynamic nature of UAV operations and
facilitates effective coordination among multiple UAVs through decentralized
learning while considering global system objectives. Simulation results
demonstrate that the proposed MAPPO-TM algorithm outperforms conventional
approaches in terms of peak AoI minimization and energy efficiency. Ideally,
the proposed algorithm achieves up to 15.1% reduction in peak AoI compared to
conventional multi-agent deep reinforcement learning (MADRL) methods.

</details>


### [31] [Recovery of UAV Swarm-enabled Collaborative Beamforming in Low-altitude Wireless Networks under Wind Field Disturbances](https://arxiv.org/abs/2507.08507)
*Geng Sun,Chenbang Liu,Jiahui Li,Guannan Qu,Shuang Liang,Jiacheng Wang,Changyuan Zhao,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文研究了无人机群在低空无线网络中利用协作波束成形时，风场干扰对性能的影响，并提出了一种基于PPO-LA算法的实时优化框架以提升性能。


<details>
  <summary>Details</summary>
Motivation: 风场干扰会导致无人机位置误差，破坏波束成形性能，影响通信可靠性，因此需要一种实时优化方法来解决这一问题。

Method: 提出了一种结合LSTM结构和自适应学习率的近端策略优化算法（PPO-LA），用于实时调整激励电流权重以优化波束成形性能。

Result: 仿真结果表明，PPO-LA算法能有效恢复风场干扰下的波束成形性能，显著优于基准算法。

Conclusion: PPO-LA算法为解决风场干扰下的无人机协作波束成形性能问题提供了一种高效且适应性强的解决方案。

Abstract: Unmanned aerial vehicle (UAV) swarms utilizing collaborative beamforming (CB)
in low-altitude wireless networks (LAWN) demonstrate significant potential for
enhanced communication range, energy efficiency, and signal directivity through
the formation of virtual antenna arrays (VAA). However, environmental
disturbances, particularly wind fields, significantly degrade CB performance by
introducing positional errors that disrupt beam patterns, thereby compromising
transmission reliability. This paper investigates the critical challenge of
maintaining CB performance in UAV-based VAAs operating in LAWN under wind field
disturbances. We propose a comprehensive framework that models the impact of
three distinct wind conditions (constant, shear, and turbulent) on UAV array
performance, and formulate a long-term real-time optimization problem to
maximize directivity while minimizing maximum sidelobe levels through adaptive
excitation current weight adjustments. To address the inherent complexity of
this problem, we propose a novel proximal policy optimization algorithm with
long short-term memory (LSTM) structure and adaptive learning rate (PPO-LA),
which effectively captures temporal patterns in wind field disturbances and
enables real-time adaptation without requiring extensive prior training for
specific wind conditions. Our simulation results demonstrate that the proposed
PPO-LA algorithm successfully recovers degraded CB performance across various
wind scenarios, and thus significantly outperforming benchmark algorithms.

</details>


### [32] [Stabilizing and Optimizing Inter-Shell Routing in LEO Networks with Integrated Routing Cost](https://arxiv.org/abs/2507.08549)
*Yaojia Wang,Qi Zhang,Kun Qiu,Yue Gao*

Main category: cs.NI

TL;DR: 论文提出了一种动态规划算法（DP-IRC），用于优化低地球轨道（LEO）多壳网络中的跨壳路由问题，平衡跳数和链路切换成本。


<details>
  <summary>Details</summary>
Motivation: 现有路由策略（如最小跳数路径和自适应路径路由）在动态网络拓扑中表现不佳，导致链路切换频繁或路径效率低下。

Method: 通过将多壳路径建模为多阶段决策问题，DP-IRC算法结合跳数和链路切换成本，优化跨壳路由。

Result: 实验表明，DP-IRC相比最小跳数路径和自适应路径路由，分别减少链路切换率39.1%和22.0%，同时保持接近最优的端到端距离。

Conclusion: DP-IRC算法显著提升了跨壳路由的稳定性和效率，适用于动态LEO网络。

Abstract: The low Earth orbit (LEO) mega-constellation network (LMCN), which uses
thousands of satellites across multi-shell architectures to deliver different
services, is facing challenges in inter-shell routing stability due to dynamic
network topologies and frequent inter-satellite link (ISL) switching. Existing
strategies, such as the Minimum Hop Path set, prioritize minimizing hop counts
to reduce latency, but ignore ISL switching costs, which leads to high
instability. To overcome this, the Adaptive Path Routing Scheme introduces path
similarity thresholds to reduce the ISL switching frequency between shells.
However, the greedy approach of Adaptive Path Routing Scheme is often trapped
in local optima, sacrificing inter-shell path distance efficiency. To address
these limitations, we propose the Dynamic Programming-based Integrated Routing
Cost (DP-IRC) algorithm, which is designed explicitly for inter-shell routing
optimization. By formulating multi-shell paths as a multistage decision
problem, DP-IRC balances hop counts and ISL stability through an Integrated
Routing Cost (IRC) metric, combining inter-/intra-shell hops and switching
costs. Experiments over 60 time slots with real-world Starlink and OneWeb
configurations show that DP-IRC reduces inter-shell ISL switching rates by
39.1% and 22.0% compared to the Minimum Hop Path set strategy and Adaptive Path
Routing Scheme, respectively, while still maintaining near-optimal end-to-end
distances.

</details>


### [33] [Qualitative Assessment of Low Power Wide Area Network Protocols and their Security Aspect](https://arxiv.org/abs/2507.08677)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphal*

Main category: cs.NI

TL;DR: 论文分析了低功耗广域网（LPWAN）协议的定性特征，探讨了在稀疏网络中基于长寿命电池的受限设备使用的挑战与机遇，重点研究了LoRaWAN、NB-IoT和Sigfox三种协议。


<details>
  <summary>Details</summary>
Motivation: 物联网中通信选项众多，尤其在受限和电池供电设备领域（如LPWAN），理解各选项的差异和特性对专业人士和研究者来说是一项挑战。

Method: 通过文献调查分析了三种LPWAN协议（LoRaWAN、NB-IoT和Sigfox），并详细研究了LoRaWAN。

Result: 研究讨论了所选网络协议及其在稀疏传感器物联网解决方案中的应用。

Conclusion: 论文为LPWAN协议的选择和使用提供了参考，特别适用于稀疏网络和长寿命电池设备场景。

Abstract: There are currently many communication options in the Internet of Things,
even in particular areas such as constrained and battery-powered devices, such
as Low Power Wide Area Networks. Understanding the differences and
characteristics of each option is a challenge, even for professionals and
researchers in the field. To meet this need, this work analyses the qualitative
characteristics of Low Power Wide Area Network protocols and the challenges and
opportunities of using constrained devices for sparse networks based on
long-life batteries. For this study, a bibliographic survey of the literature
was carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and
Sigfox), and a detailing of the first one. As a result, there is a discussion
about the chosen network protocol and its use in IoT solutions with sparse
sensors.

</details>


### [34] [Knowledge Graph-Based approach for Sustainable 6G End-to-End System Design](https://arxiv.org/abs/2507.08717)
*Akshay Jain,Sylvaine Kerboeuf,Sokratis Barmpounakis,Cristóbal Vinagre Z.,Stefan Wendt,Dinh Thai Bui,Pol Alemany,Riccardo Nicolicchia,José María Jorquera Valero,Dani Korpi,Mohammad Hossein Moghaddam,Mikko A. Uusitalo,Patrik Rugeland,Abdelkader Outtagarts,Karthik Upadhya,Panagiotis Demestichas,Raul Muñoz,Manuel Gil Pérez,Daniel Adanza,Ricard Vilalta*

Main category: cs.NI

TL;DR: 本文提出了一种基于知识图谱（KG）的6G端到端系统设计新方法，综合考虑性能与可持续性目标，并通过实际用例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 5G等前代通信技术主要关注性能指标，而6G需同时满足性能与可持续性目标，但现有文献缺乏将技术使能器与可持续性指标联系的方法。

Method: 基于知识图谱的方法，输入包括用例KPI、可持续性需求（KV/KVI）、技术使能器能力、6G设计原则、技术成熟度及使能器间依赖关系。

Result: 方法在Hexa-X-II项目的移动机器人用例中应用，筛选出82个使能器，并通过概念验证证明了其有效性。

Conclusion: KG方法为设计可持续的6G系统提供了有效工具，尤其适用于复杂设计空间和主观可持续性指标的权衡。

Abstract: Previous generations of cellular communication, such as 5G, have been
designed with the objective of improving key performance indicators (KPIs) such
as throughput, latency, etc. However, to meet the evolving KPI demands as well
as the ambitious sustainability targets for the ICT industry, 6G will need to
be designed differently. Concretely, 6G will need to consider both the
performance and sustainability targets for the various use cases it will serve.
Moreover, like previous generations, 6G will have various candidate
technological enablers, making the design space of the system even more
complex. Furthermore, given the subjective nature of the sustainability
indicators, in particular social sustainability, there is a significant gap in
literature on how technical enablers and 6G System design can be linked to
them. Hence, in this article a novel method for 6G end-to-end (E2E) system
design based on Knowledge graphs (KG) has been introduced. It considers as its
input: the use case KPIs, use case sustainability requirements expressed as Key
Values (KV) and KV Indicators (KVIs), the ability of the technological enablers
to satisfy these KPIs and KVIs, the 6G system design principles defined in
Hexa-X-II project, the maturity of a technological enabler and the dependencies
between the various enablers. As part of the KG method, a novel approach for
determining the key values a technological enabler addresses, has also been
introduced. The effectiveness of the KG method was demonstrated by its
application in designing the 6G E2E system for the cooperating mobile robot use
case defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,
results from proof-of-concept demonstrations for a subset of the selected
enablers have also been provided, which reinforce the efficacy of the KG method
for designing a sustainable 6G system.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang,Zhaoyang Duan,Dong Xue,Fangzhou Liu,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种联邦少样本学习框架，结合隐私保护机制，解决呼吸疾病诊断中数据标注不足和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注劳动密集且隐私敏感，导致高质量标注数据稀缺，现有集中式方法又无法兼顾隐私保护。

Method: 采用元随机梯度下降算法防止过拟合，并引入高斯差分隐私噪声保护梯度隐私，最后通过加权平均算法聚合模型。

Result: 实验表明，该方法在保护隐私的同时，能有效诊断不同结构、类别和分布的呼吸疾病数据。

Conclusion: 该框架在数据有限和隐私保护需求下，为呼吸疾病诊断提供了可行解决方案。

Abstract: The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [36] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe,Yunzhuo Wang,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文提出了一种针对树结构Parzen估计器（TPE）的高效组合优化算法，通过推广分类核与数值核，并引入距离结构，改进了TPE在组合搜索空间中的性能。


<details>
  <summary>Details</summary>
Motivation: 组合优化在化学和生物学等领域有广泛应用，但TPE在此领域的研究较少。本文旨在填补这一空白，提升TPE在组合优化中的效率。

Method: 通过推广分类核与数值核，引入距离结构，并针对大规模组合搜索空间优化核计算的时间复杂度。

Result: 实验表明，所提方法在合成问题上能以更少的评估次数找到更好的解。

Conclusion: 该算法已集成到开源HPO框架Optuna中，为组合优化提供了高效解决方案。

Abstract: Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [37] [Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions](https://arxiv.org/abs/2507.08068)
*Simon Matrenok,Skander Moalla,Caglar Gulcehre*

Main category: cs.LG

TL;DR: QRPO是一种新方法，能够利用点式绝对奖励进行离线学习，避免了传统方法对在线或相对信号的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DPO和REBEL只能从偏好对或相对信号中学习，而在线方法如PPO和GRPO需要复杂实现。QRPO旨在填补这一空白。

Method: QRPO使用分位数奖励，通过回归KL正则化RL目标的闭式解，避免了相对信号的需求。

Result: QRPO在聊天和编码评估中表现优异，优于DPO、REBEL和SimPO，且减少了长度偏差。

Conclusion: QRPO为离线学习点式绝对奖励提供了简单高效的解决方案，并在实验中验证了其优越性。

Abstract: Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.

</details>


### [38] [Low-rank Momentum Factorization for Memory Efficient Training](https://arxiv.org/abs/2507.08091)
*Pouria Mahdavinia,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: MoFaSGD是一种内存高效的大模型微调方法，通过动态更新低秩SVD表示动量，减少内存占用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大模型微调中内存占用高的问题，特别是优化器状态的内存需求。

Method: 提出Momentum Factorized SGD (MoFaSGD)，动态更新低秩SVD表示动量，实现高效内存优化。

Result: 在大型语言模型对齐任务中表现优异，内存占用与LoRA相当，性能优于其他低秩优化方法。

Conclusion: MoFaSGD提供了一种内存高效且性能优越的大模型微调解决方案。

Abstract: Fine-tuning large foundation models presents significant memory challenges
due to stateful optimizers like AdamW, often requiring several times more GPU
memory than inference. While memory-efficient methods like parameter-efficient
fine-tuning (e.g., LoRA) and optimizer state compression exist, recent
approaches like GaLore bridge these by using low-rank gradient projections and
subspace moment accumulation. However, such methods may struggle with fixed
subspaces or computationally costly offline resampling (e.g., requiring
full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which
maintains a dynamically updated low-rank SVD representation of the first-order
momentum, closely approximating its full-rank counterpart throughout training.
This factorization enables a memory-efficient fine-tuning method that
adaptively updates the optimization subspace at each iteration. Crucially,
MoFaSGD leverages the computed low-rank momentum factors to perform efficient
spectrally normalized updates, offering an alternative to subspace moment
accumulation. We establish theoretical convergence guarantees for MoFaSGD,
proving it achieves an optimal rate for non-convex stochastic optimization
under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness
on large language model alignment benchmarks, achieving a competitive trade-off
between memory reduction (comparable to LoRA) and performance compared to
state-of-the-art low-rank optimization methods. Our implementation is available
at https://github.com/pmahdavi/MoFaSGD.

</details>


### [39] [PDE-aware Optimizer for Physics-informed Neural Networks](https://arxiv.org/abs/2507.08118)
*Hardik Shukla,Manurag Khullar,Vismay Churiwala*

Main category: cs.LG

TL;DR: 论文提出了一种PDE感知优化器，通过调整参数更新以解决PINNs中梯度不对齐问题，并在多个PDE上验证了其优于Adam和SOAP的性能。


<details>
  <summary>Details</summary>
Motivation: 标准优化器（如Adam）在处理PINNs中的刚性或病态系统时难以平衡损失项，导致训练不稳定。

Method: 提出一种基于PDE残差梯度方差的PDE感知优化器，避免二阶优化器的高计算成本。

Result: 在1D Burgers、Allen-Cahn和KdV方程上，PDE感知优化器实现了更平滑的收敛和更低的绝对误差。

Conclusion: PDE残差感知自适应能有效提升PINNs训练的稳定性，未来需进一步研究在大规模架构和硬件加速器上的扩展。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical
constraints into the loss function. However, standard optimizers such as Adam
often struggle to balance competing loss terms, particularly in stiff or
ill-conditioned systems. In this work, we propose a PDE-aware optimizer that
adapts parameter updates based on the variance of per-sample PDE residual
gradients. This method addresses gradient misalignment without incurring the
heavy computational costs of second-order optimizers such as SOAP. We benchmark
the PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and
Korteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer
achieves smoother convergence and lower absolute errors, particularly in
regions with sharp gradients. Our results demonstrate the effectiveness of PDE
residual-aware adaptivity in enhancing stability in PINNs training. While
promising, further scaling on larger architectures and hardware accelerators
remains an important direction for future research.

</details>


### [40] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu,Ivan Oseledets*

Main category: cs.LG

TL;DR: 论文提出QRPINNs，使用低差异序列采样替代随机采样，提升PINNs在高维PDE中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs对采样点敏感，而拟蒙特卡洛方法在高维问题中表现优异，因此提出QRPINNs。

Method: 采用低差异序列（拟蒙特卡洛方法）进行采样，结合物理约束训练神经网络。

Result: 理论证明QRPINNs收敛速度更快，实验显示其显著优于PINNs和自适应采样方法。

Conclusion: QRPINNs在高维PDE中表现优异，结合自适应采样可进一步提升性能。

Abstract: Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [41] [Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints](https://arxiv.org/abs/2507.08124)
*Ashfaq Iftakher,Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: KKT-Hardnet是一种改进的PINN架构，通过KKT条件严格满足约束条件，提高了模型预测的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN无法严格满足约束条件，这在工程系统中可能导致预测结果不可靠。

Method: 利用KKT条件进行可行区域投影，并通过对数-指数变换重新表述非线性KKT条件，构建可微分的稀疏系统。

Result: 在测试问题和实际化学过程模拟中，KKT-Hardnet比传统PINN和多层感知机具有更高的准确性和严格的约束满足性。

Conclusion: KKT-Hardnet为复杂系统的可靠混合建模提供了一种有效方法。

Abstract: Traditional physics-informed neural networks (PINNs) do not guarantee strict
constraint satisfaction. This is problematic in engineering systems where minor
violations of governing laws can significantly degrade the reliability and
consistency of model predictions. In this work, we develop KKT-Hardnet, a PINN
architecture that enforces both linear and nonlinear equality and inequality
constraints up to machine precision. It leverages a projection onto the
feasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a
distance minimization problem. Furthermore, we reformulate the nonlinear KKT
conditions using log-exponential transformation to construct a general sparse
system with only linear and exponential terms, thereby making the projection
differentiable. We apply KKT-Hardnet on both test problems and a real-world
chemical process simulation. Compared to multilayer perceptrons and PINNs,
KKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This
approach allows the integration of domain knowledge into machine learning
towards reliable hybrid modeling of complex systems.

</details>


### [42] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: ALCo-FM是一种自适应长上下文基础模型，用于交通风险预测，通过动态选择上下文窗口和多模态数据融合，实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 交通事故虽罕见但影响重大，需长上下文多模态推理进行准确风险预测。

Method: 模型结合动态上下文窗口选择、浅层交叉注意力、局部GAT层和全局稀疏Transformer，并采用蒙特卡洛dropout增强置信度。

Result: 在15个美国城市数据上训练，ALCo-FM达到0.94准确率、0.92 F1分数和0.04 ECE，优于20多种基线方法。

Conclusion: ALCo-FM在大型城市风险预测中表现优异，代码和数据集已开源。

Abstract: Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [43] [Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness](https://arxiv.org/abs/2507.08154)
*Arisha Khan,Nathaniel Li,Tori Shen,Anna N. Rafferty*

Main category: cs.LG

TL;DR: Text-LENS通过扩展LENS部分变分自编码器，利用项目文本嵌入，提升了对新项目的预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在教育评估中因依赖历史数据而难以处理新项目的问题。

Method: 扩展LENS部分变分自编码器，引入项目文本嵌入，并在Eedi和LLM-Sim数据集上测试。

Result: Text-LENS在已见项目上表现与LENS相当，在未见项目上表现更优，能有效学习学生能力并预测新项目表现。

Conclusion: Text-LENS通过文本嵌入显著提升了对新项目的泛化能力，适用于教育评估。

Abstract: Machine learning has been proposed as a way to improve educational assessment
by making fine-grained predictions about student performance and learning
relationships between items. One challenge with many machine learning
approaches is incorporating new items, as these approaches rely heavily on
historical data. We develop Text-LENS by extending the LENS partial variational
auto-encoder for educational assessment to leverage item text embeddings, and
explore the impact on predictive performance and generalization to previously
unseen items. We examine performance on two datasets: Eedi, a publicly
available dataset that includes item content, and LLM-Sim, a novel dataset with
test items produced by an LLM. We find that Text-LENS matches LENS' performance
on seen items and improves upon it in a variety of conditions involving unseen
items; it effectively learns student proficiency from and makes predictions
about student performance on new items.

</details>


### [44] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: 论文探讨了通过生理信号推断情绪状态的可行性，提出了一种隐私保护的替代方案，优于传统面部识别技术。量子增强的SVM在分类性能上超越经典方法，F1分数超过80%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提供一种隐私保护的情绪识别方法，避免传统面部识别技术的隐私问题，并探索量子机器学习在情绪识别中的潜力。

Method: 使用经典机器学习算法和混合量子机器学习（QML）方法，特别是基于量子核的模型，对生理信号进行分类。

Result: 量子增强的SVM在所有情绪类别中表现优于经典方法，F1分数超过80%，召回率最高提升36%。

Conclusion: 该方法为临床和辅助生活环境中的被动情绪监测奠定了基础，尤其适用于沟通障碍人群。

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [45] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果学习视角的时空异常检测方法，以解决现有黑盒深度学习方法的可解释性和适应性不足问题。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统的互联性和空间分布性增强，确保其抵御网络攻击的韧性变得至关重要。现有数据驱动方法在可解释性和适应性方面存在局限。

Method: 论文提出了三个关键方向：因果图分析、多视图融合和持续因果图学习，以揭示时空动态因果关系。

Result: 通过实际系统（如供水基础设施）验证，因果模型能提供早期预警和根因分析，优于黑盒检测器。

Conclusion: 论文旨在推动可扩展、自适应、可解释的因果驱动异常检测系统，为网络安全研究开辟新方向。

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [46] [CTRLS: Chain-of-Thought Reasoning via Latent State-Transition](https://arxiv.org/abs/2507.08182)
*Junda Wu,Yuxin Xiong,Xintong Li,Zhengmian Hu,Tong Yu,Rui Wang,Xiang Chen,Jingbo Shang,Julian McAuley*

Main category: cs.LG

TL;DR: 论文提出CTRLS框架，将链式思维推理建模为马尔可夫决策过程，通过分布强化学习提升推理多样性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统链式思维方法依赖启发式采样，缺乏对推理转换的结构化建模，限制了推理轨迹的多样性和有效性。

Method: CTRLS框架将推理视为带隐状态转换的MDP，利用分布强化学习进行探索，并引入熵正则化和ε-贪婪策略优化隐状态转换。

Result: 实验显示CTRLS在推理准确性、多样性和探索效率上均有提升。

Conclusion: CTRLS通过结构化建模和强化学习，显著提升了链式思维推理的性能和透明度。

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to
break down complex problems into interpretable intermediate steps,
significantly enhancing model transparency and performance in reasoning tasks.
However, conventional CoT methods rely on heuristic sampling without structured
modeling of reasoning transitions, constraining their ability to systematically
explore and discover diverse and effective reasoning trajectories. In this
work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov
decision process (MDP) with latent state transitions, enabling principled and
state-aware exploration via distributional reinforcement learning. By modelling
reasoning actions as explicit probability distributions in latent space, our
approach explicitly models epistemic uncertainty, facilitating robust
exploration of the reasoning space. As part of our framework, we introduce an
on-policy reinforcement learning strategy incorporating epsilon-greedy
exploration and entropy-based regularization to iteratively refine latent state
transitions without requiring additional fine-tuning of the underlying LLM.
Theoretical analyses provide evidence lower bounds (ELBO), theoretically
grounding our transition-aware modeling of latent reasoning dynamics. Further
experiments demonstrate improvements in reasoning accuracy, diversity, and
exploration efficiency across benchmark reasoning tasks.

</details>


### [47] [EvA: Evolutionary Attacks on Graphs](https://arxiv.org/abs/2507.08212)
*Mohammad Sadegh Akhondzadeh,Soroush H. Zargarbashi,Jimin Cao,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 本文提出了一种基于进化算法的离散优化攻击方法（EvA），显著提升了图神经网络（GNNs）攻击效果，无需依赖梯度信息。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法依赖梯度信息，限制了攻击的适应性和最优性，尤其是在非可微目标下。

Method: 采用进化算法直接解决离散优化问题，支持黑盒模型和任意目标，无需可微代理损失。

Result: EvA在实验中平均比现有最佳攻击方法多降低11%的准确率，并设计了两种新型攻击。

Conclusion: EvA揭示了攻击设计的未开发潜力，适用于更广泛的场景。

Abstract: Even a slight perturbation in the graph structure can cause a significant
drop in the accuracy of graph neural networks (GNNs). Most existing attacks
leverage gradient information to perturb edges. This relaxes the attack's
optimization problem from a discrete to a continuous space, resulting in
solutions far from optimal. It also restricts the adaptability of the attack to
non-differentiable objectives. Instead, we introduce a few simple yet effective
enhancements of an evolutionary-based algorithm to solve the discrete
optimization problem directly. Our Evolutionary Attack (EvA) works with any
black-box model and objective, eliminating the need for a differentiable proxy
loss. This allows us to design two novel attacks that reduce the effectiveness
of robustness certificates and break conformal sets. The memory complexity of
our attack is linear in the attack budget. Among our experiments, EvA shows
$\sim$11\% additional drop in accuracy on average compared to the best previous
attack, revealing significant untapped potential in designing attacks.

</details>


### [48] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: InsightBuild结合因果分析和微调大语言模型，为建筑能源异常提供可读解释。


<details>
  <summary>Details</summary>
Motivation: 解决设施管理者缺乏对能源异常清晰解释的问题。

Method: 两阶段框架：先通过因果推断模块分析传感器数据，再用微调LLM生成解释。

Result: 在真实数据集上验证，结合因果发现与LLM生成清晰、精确的解释。

Conclusion: InsightBuild能有效帮助管理者诊断和缓解能源效率问题。

Abstract: Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [49] [Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions](https://arxiv.org/abs/2507.08238)
*Abinay Reddy Naini,Zhaobo K. Zheng,Teruhisa Misu,Kumar Akash*

Main category: cs.LG

TL;DR: 提出一种自监督学习方法，利用多模态数据解决亲社会行为预测中的数据稀缺问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏大规模标记的亲社会行为数据集，小规模数据难以有效训练深度学习模型。

Method: 采用自监督学习，利用现有生理和行为数据集进行预训练，再通过小规模标记数据微调模型。

Result: 显著提升了亲社会行为预测的性能，为解决数据稀缺问题提供了有效方法。

Conclusion: 该方法为智能车辆系统和人机交互提供了有价值的见解，是亲社会行为预测的有效基准。

Abstract: Human state detection and behavior prediction have seen significant
advancements with the rise of machine learning and multimodal sensing
technologies. However, predicting prosocial behavior intentions in mobility
scenarios, such as helping others on the road, is an underexplored area.
Current research faces a major limitation. There are no large, labeled datasets
available for prosocial behavior, and small-scale datasets make it difficult to
train deep-learning models effectively. To overcome this, we propose a
self-supervised learning approach that harnesses multi-modal data from existing
physiological and behavioral datasets. By pre-training our model on diverse
tasks and fine-tuning it with a smaller, manually labeled prosocial behavior
dataset, we significantly enhance its performance. This method addresses the
data scarcity issue, providing a more effective benchmark for prosocial
behavior prediction, and offering valuable insights for improving intelligent
vehicle systems and human-machine interaction.

</details>


### [50] [Data Generation without Function Estimation](https://arxiv.org/abs/2507.08239)
*Hadi Daneshmand,Ashkan Soleymani*

Main category: cs.LG

TL;DR: 提出了一种无需函数估计的生成方法，通过确定性梯度下降更新点集位置，将均匀分布转换为任意数据分布。


<details>
  <summary>Details</summary>
Motivation: 避免函数估计的计算和统计挑战，探索无需训练神经网络或噪声注入的生成方法。

Method: 利用确定性梯度下降更新点集位置，基于物理中相互作用粒子的最新进展。

Result: 理论和实验表明，该方法能有效生成数据分布。

Conclusion: 该方法为生成模型提供了新的思路，避免了传统方法的复杂性。

Abstract: Estimating the score function (or other population-density-dependent
functions) is a fundamental component of most generative models. However, such
function estimation is computationally and statistically challenging. Can we
avoid function estimation for data generation? We propose an estimation-free
generative method: A set of points whose locations are deterministically
updated with (inverse) gradient descent can transport a uniform distribution to
arbitrary data distribution, in the mean field regime, without function
estimation, training neural networks, and even noise injection. The proposed
method is built upon recent advances in the physics of interacting particles.
We show, both theoretically and experimentally, that these advances can be
leveraged to develop novel generative methods.

</details>


### [51] [CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry](https://arxiv.org/abs/2507.08243)
*Chandra Sekhar Mukherjee,Joonyoung Bae,Jiapeng Zhang*

Main category: cs.LG

TL;DR: 论文提出了CoreSPECT框架，通过结合数据分布与几何特性提升简单聚类算法（如K-Means和GMM）的性能，实验显示显著提升聚类准确性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法通常仅关注数据密度或几何复杂性，而忽略了二者之间的相互作用。本文旨在利用这种相互作用提升聚类性能。

Method: 提出CoreSPECT框架，通过选择关键区域应用简单聚类算法，并基于邻域图的多层传播扩展分区。

Result: 在15个数据集上测试，K-Means的ARI提升40%，GMM提升14%，性能优于主流密度和流形聚类算法。

Conclusion: CoreSPECT框架通过结合分布与几何特性，显著提升了简单聚类算法的性能，并具有理论保证和鲁棒性。

Abstract: Density and geometry have long served as two of the fundamental guiding
principles in clustering algorithm design, with algorithm usually focusing
either on the density structure of the data (e.g., HDBSCAN and Density Peak
Clustering) or the complexity of underlying geometry (e.g., manifold clustering
algorithms).
  In this paper, we identify and formalize a recurring but often overlooked
interaction between distribution and geometry and leverage this insight to
design our clustering enhancement framework CoreSPECT (Core Space
Projection-based Enhancement of Clustering Techniques). Our framework boosts
the performance of simple algorithms like K-Means and GMM by applying them to
strategically selected regions, then extending the partial partition to a
complete partition for the dataset using a novel neighborhood graph based
multi-layer propagation procedure.
  We apply our framework on 15 datasets from three different domains and obtain
consistent and substantial gain in clustering accuracy for both K-Means and
GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by
14%, often surpassing the performance of both manifold-based and recent
density-based clustering algorithms. We further support our framework with
initial theoretical guarantees, ablation to demonstrate the usefulness of the
individual steps and with evidence of robustness to noise.

</details>


### [52] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

Main category: cs.LG

TL;DR: 论文提出Quantum-UnIMP框架，通过将浅层量子电路集成到基于LLM的插补架构中，利用量子特性提升数据插补性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM在混合类型数据插补中因依赖经典嵌入方法而难以捕捉复杂非线性相关性的问题。

Method: 用IQP量子电路生成量子特征映射替代经典输入嵌入，利用量子叠加和纠缠增强数据表示。

Result: 在混合类型数据集上，数值特征插补误差降低15.2%（RMSE），分类特征准确率提升8.7%（F1-Score）。

Conclusion: 量子增强表示在复杂数据插补任务中具有显著潜力，即使使用近期量子硬件。

Abstract: Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [53] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara,Taiki Yamaguchi,Yuichi Inoue*

Main category: cs.LG

TL;DR: 论文提出了一种结合监督微调（SFT）和在线推理强化学习（GRPO）的训练方法，以提升大型语言模型（LLMs）的数学推理能力，实现了高准确性和高效率。


<details>
  <summary>Details</summary>
Motivation: 提升LLMs的数学推理能力是AI发展的关键挑战，但目前缺乏系统的方法来结合SFT和RL以最大化性能。

Method: 通过延长SFT阶段提升模型准确性，再通过GRPO阶段优化解决方案长度，保持性能的同时提高效率。

Result: 实验表明，延长SFT至10个周期对性能突破至关重要，GRPO则显著优化了解决方案长度。该方法在AIMO等基准测试中表现优异。

Conclusion: 该方法为开发高准确性和高效率的数学推理模型提供了实用方案，并开源了完整框架以支持未来研究。

Abstract: Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [54] [Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization](https://arxiv.org/abs/2507.08269)
*Woon Ryong Kim,Jaeheun Jung,Jeong Un Ha,Donghun Lee,Jae Kyung Shim*

Main category: cs.LG

TL;DR: 提出了一种基于监督学习的数据驱动框架，用于平面四杆机构的尺寸综合，避免了传统的方程求解和优化方法。


<details>
  <summary>Details</summary>
Motivation: 平面四杆机构的尺寸综合是一个复杂的逆运动学问题，传统方法效率低且复杂。

Method: 结合合成数据集、LSTM神经网络和混合专家（MoE）架构，针对不同连杆类型进行训练和预测。

Result: 实验表明，该方法能够生成准确且无缺陷的连杆机构，适用于多种配置。

Conclusion: 该方法为非专家用户提供了直观高效的机构设计工具，并为运动学设计中的可扩展和灵活的综合开辟了新途径。

Abstract: Dimensional synthesis of planar four-bar mechanisms is a challenging inverse
problem in kinematics, requiring the determination of mechanism dimensions from
desired motion specifications. We propose a data-driven framework that bypasses
traditional equation-solving and optimization by leveraging supervised
learning. Our method combines a synthetic dataset, an LSTM-based neural network
for handling sequential precision points, and a Mixture of Experts (MoE)
architecture tailored to different linkage types. Each expert model is trained
on type-specific data and guided by a type-specifying layer, enabling both
single-type and multi-type synthesis. A novel simulation metric evaluates
prediction quality by comparing desired and generated motions. Experiments show
our approach produces accurate, defect-free linkages across various
configurations. This enables intuitive and efficient mechanism design, even for
non-expert users, and opens new possibilities for scalable and flexible
synthesis in kinematic design.

</details>


### [55] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 提出一种轻量级但高效的语言模型安全护栏框架，通过合成数据生成和对抗训练，使小型语言模型在内容审核任务中表现优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在内容审核中的高计算成本和对抗攻击脆弱性问题，提供一种更高效、可扩展的解决方案。

Method: 使用高保真合成数据生成（基于人工种子数据的查询增强和改写）和对抗训练（结合GAN架构和强化学习）来优化小型语言模型的安全分类器。

Result: 小型语言模型在内容审核任务中表现优于大型模型，同时降低计算开销并增强对抗攻击的鲁棒性。

Conclusion: 该框架为AI系统的内容审核提供了一种高效、可扩展且抗攻击的解决方案。

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [56] [CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering](https://arxiv.org/abs/2507.08311)
*Krishnendu Das,Sumit Gupta,Awadhesh Kumar*

Main category: cs.LG

TL;DR: 本文综述了聚类中如何选择最佳k值的策略，提出了一种基于Condensed Silhouette方法及多种统计方法的改进算法，显著提升了计算效率和聚类精度。


<details>
  <summary>Details</summary>
Motivation: 在大数据环境下，聚类精度和计算效率的平衡是关键挑战，尤其在文本和图像数据中。

Method: 结合Condensed Silhouette、Local Structures、Gap Statistics等方法，提出CCR和COI算法以优化k值选择。

Result: 实验表明，该方法在高维数据上执行时间提升99%，同时保持精度和可扩展性。

Conclusion: 该方法适用于实时聚类或资源受限场景，显著提升了效率和实用性。

Abstract: Clustering is a critical component of decision-making in todays data-driven
environments. It has been widely used in a variety of fields such as
bioinformatics, social network analysis, and image processing. However,
clustering accuracy remains a major challenge in large datasets. This paper
presents a comprehensive overview of strategies for selecting the optimal value
of k in clustering, with a focus on achieving a balance between clustering
precision and computational efficiency in complex data environments. In
addition, this paper introduces improvements to clustering techniques for text
and image data to provide insights into better computational performance and
cluster validity. The proposed approach is based on the Condensed Silhouette
method, along with statistical methods such as Local Structures, Gap
Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and
COIbased algorithm to calculate the best value of k for K-Means clustering. The
results of comparative experiments show that the proposed approach achieves up
to 99 percent faster execution times on high-dimensional datasets while
retaining both precision and scalability, making it highly suitable for real
time clustering needs or scenarios demanding efficient clustering with minimal
resource utilization.

</details>


### [57] [A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction](https://arxiv.org/abs/2507.08317)
*Jitendra Kumar,Deepika Saxena,Kishu Gupta,Satyam Kumar,Ashutosh Kumar Singh*

Main category: cs.LG

TL;DR: 提出了一种新型的量子神经网络（CA-QNN），用于高效预测云服务中的动态工作负载，显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理高维动态工作负载时效率低下，需要一种更高效的优化方法。

Method: 结合量子计算和结构优化算法，将工作负载数据转换为量子比特，并通过可控NOT门激活函数进行模式识别。

Result: 在四个基准数据集上，CA-QNN的预测误差比现有深度学习和量子神经网络方法降低了93.40%和91.27%。

Conclusion: CA-QNN在动态云服务管理中表现出卓越的预测准确性和效率。

Abstract: Accurate workload prediction and advanced resource reservation are
indispensably crucial for managing dynamic cloud services. Traditional neural
networks and deep learning models frequently encounter challenges with diverse,
high-dimensional workloads, especially during sudden resource demand changes,
leading to inefficiencies. This issue arises from their limited optimization
during training, relying only on parametric (inter-connection weights)
adjustments using conventional algorithms. To address this issue, this work
proposes a novel Comprehensively Adaptive Architectural Optimization-based
Variable Quantum Neural Network (CA-QNN), which combines the efficiency of
quantum computing with complete structural and qubit vector parametric
learning. The model converts workload data into qubits, processed through qubit
neurons with Controlled NOT-gated activation functions for intuitive pattern
recognition. In addition, a comprehensive architecture optimization algorithm
for networks is introduced to facilitate the learning and propagation of the
structure and parametric values in variable-sized QNNs. This algorithm
incorporates quantum adaptive modulation and size-adaptive recombination during
training process. The performance of CA-QNN model is thoroughly investigated
against seven state-of-the-art methods across four benchmark datasets of
heterogeneous cloud workloads. The proposed model demonstrates superior
prediction accuracy, reducing prediction errors by up to 93.40% and 91.27%
compared to existing deep learning and QNN-based approaches.

</details>


### [58] [scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling](https://arxiv.org/abs/2507.08355)
*Hegang Chen,Yuyin Lu,Zhiming Dai,Fu Lee Wang,Qing Li,Yanghui Rao*

Main category: cs.LG

TL;DR: scE2TM是一种结合外部生物知识的单细胞嵌入主题模型，显著提升了聚类性能和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有单细胞嵌入主题模型在可解释性评估上的定性局限和忽视外部生物知识的问题。

Method: 提出scE2TM模型，整合外部生物知识，并设计10个定量指标评估模型可解释性。

Result: 在20个单细胞RNA-seq数据集上，scE2TM的聚类性能优于7种先进方法，且可解释性表现优异。

Conclusion: scE2TM为单细胞数据分析提供了高质量的嵌入和强解释性，有助于揭示生物学机制。

Abstract: Recent advances in sequencing technologies have enabled researchers to
explore cellular heterogeneity at single-cell resolution. Meanwhile,
interpretability has gained prominence parallel to the rapid increase in the
complexity and performance of deep learning models. In recent years, topic
models have been widely used for interpretable single-cell embedding learning
and clustering analysis, which we refer to as single-cell embedded topic
models. However, previous studies evaluated the interpretability of the models
mainly through qualitative analysis, and these single-cell embedded topic
models suffer from the potential problem of interpretation collapse.
Furthermore, their neglect of external biological knowledge constrains
analytical performance. Here, we present scE2TM, an external knowledge-guided
single-cell embedded topic model that provides a high-quality cell embedding
and strong interpretation, contributing to comprehensive scRNA-seq data
analysis. Our comprehensive evaluation across 20 scRNA-seq datasets
demonstrates that scE2TM achieves significant clustering performance gains
compared to 7 state-of-the-art methods. In addition, we propose a new
interpretability evaluation benchmark that introduces 10 metrics to
quantitatively assess the interpretability of single-cell embedded topic
models. The results show that the interpretation provided by scE2TM performs
encouragingly in terms of diversity and consistency with the underlying
biological signals, contributing to a better revealing of the underlying
biological mechanisms.

</details>


### [59] [Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text](https://arxiv.org/abs/2507.08362)
*Phuong Nam Lê,Charlotte Schneider-Depré,Alexandre Goossens,Alexander Stevens,Aurélie Leribaux,Johannes De Smedt*

Main category: cs.LG

TL;DR: 本文提出了一种自动化流程，利用机器学习和大型语言模型从文本中提取BPMN模型，并通过新标注的数据集提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 将文本流程文档转换为BPMN模型的过程耗时且昂贵，现有方法在识别并行结构方面表现不佳。

Method: 提出了一种结合机器学习和大型语言模型的自动化流程，并引入新标注的数据集（包含并行网关）以增强训练。

Result: 该方法在重建准确性方面表现良好，为加速BPMN模型创建提供了可行方案。

Conclusion: 该研究为组织提供了一种高效的工具，能够显著提升BPMN模型生成的效率和准确性。

Abstract: Efficient planning, resource management, and consistent operations often rely
on converting textual process documents into formal Business Process Model and
Notation (BPMN) models. However, this conversion process remains time-intensive
and costly. Existing approaches, whether rule-based or machine-learning-based,
still struggle with writing styles and often fail to identify parallel
structures in process descriptions.
  This paper introduces an automated pipeline for extracting BPMN models from
text, leveraging the use of machine learning and large language models. A key
contribution of this work is the introduction of a newly annotated dataset,
which significantly enhances the training process. Specifically, we augment the
PET dataset with 15 newly annotated documents containing 32 parallel gateways
for model training, a critical feature often overlooked in existing datasets.
This addition enables models to better capture parallel structures, a common
but complex aspect of process descriptions. The proposed approach demonstrates
adequate performance in terms of reconstruction accuracy, offering a promising
foundation for organizations to accelerate BPMN model creation.

</details>


### [60] [Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer](https://arxiv.org/abs/2507.08365)
*Francesco De Cristofaro,Felix Hofbaur,Aixi Yang,Arno Eichberger*

Main category: cs.LG

TL;DR: 论文研究了如何预测前车变道意图，比较了LSTM、CNN和Transformer三种网络结构，发现Transformer表现最佳，准确率在82.79%到96.73%之间。


<details>
  <summary>Details</summary>
Motivation: 前车变道对自动驾驶车辆的运动规划有重大影响，预测变道意图可提升安全性和效率。现有研究多集中于特定时间点的预测，缺乏对时间区间内变道意图的预测及不同网络架构的比较。

Method: 使用公开数据集highD，设计并实现了LSTM、CNN和Transformer三种网络结构，比较了不同输入配置下的性能。

Result: Transformer表现最优，准确率在82.79%到96.73%之间，且不易过拟合。

Conclusion: Transformer网络在预测变道意图方面优于其他网络，具有较高的准确性和鲁棒性。

Abstract: Lane changes of preceding vehicles have a great impact on the motion planning
of automated vehicles especially in complex traffic situations. Predicting them
would benefit the public in terms of safety and efficiency. While many research
efforts have been made in this direction, few concentrated on predicting
maneuvers within a set time interval compared to predicting at a set prediction
time. In addition, there exist a lack of comparisons between different
architectures to try to determine the best performing one and to assess how to
correctly choose the input for such models. In this paper the structure of an
LSTM, a CNN and a Transformer network are described and implemented to predict
the intention of human drivers to perform a lane change. We show how the data
was prepared starting from a publicly available dataset (highD), which features
were used, how the networks were designed and finally we compare the results of
the three networks with different configurations of input data. We found that
transformer networks performed better than the other networks and was less
affected by overfitting. The accuracy of the method spanned from $82.79\%$ to
$96.73\%$ for different input configurations and showed overall good
performances considering also precision and recall.

</details>


### [61] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap,Rohit K Ramakrishnan,Kumari Jyoti,Apoorva D Patel*

Main category: cs.LG

TL;DR: 量子机器学习（QML）结合量子计算与人工智能，旨在利用量子优势提升数据驱动任务。本文综述了QML的理论基础、应用及挑战，并探讨了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 解决经典机器学习在复杂数据处理中的计算瓶颈，探索量子计算在机器学习中的潜力。

Method: 介绍量子数据编码、量子学习理论和优化技术，分类QML方法，并评估量子主成分分析等关键技术的理论与实际限制。

Result: QML在量子化学和传感等特定领域具有潜力，但实际应用受限于NISQ设备的噪声、可扩展性和数据编码问题。

Conclusion: QML的未来发展需量子原生算法、改进的错误校正和实际基准测试，以弥合理论与实践的差距。

Abstract: Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [62] [Two-cluster test](https://arxiv.org/abs/2507.08382)
*Xinying Liu,Lianyu Hu,Mudi Jiang,Simen Zhang,Jun Lou,Zengyou He*

Main category: cs.LG

TL;DR: 论文提出了一种新的两簇检验方法，解决了传统两样本检验在聚类分析中的偏差问题，显著降低了Type-I错误率。


<details>
  <summary>Details</summary>
Motivation: 传统两样本检验在聚类分析中会导致Type-I错误率膨胀，因此需要一种专门针对两簇检验的新方法。

Method: 基于两个子集之间的边界点，提出了一种新的显著性量化方法，推导出解析p值。

Result: 实验表明，该方法显著降低了Type-I错误率，并在树状可解释聚类和显著性层次聚类中得到验证。

Conclusion: 两簇检验是一个独立于传统两样本检验的显著性检验问题，新方法在实际应用中表现出色。

Abstract: Cluster analysis is a fundamental research issue in statistics and machine
learning. In many modern clustering methods, we need to determine whether two
subsets of samples come from the same cluster. Since these subsets are usually
generated by certain clustering procedures, the deployment of classic
two-sample tests in this context would yield extremely smaller p-values,
leading to inflated Type-I error rate. To overcome this bias, we formally
introduce the two-cluster test issue and argue that it is a totally different
significance testing issue from conventional two-sample test. Meanwhile, we
present a new method based on the boundary points between two subsets to derive
an analytical p-value for the purpose of significance quantification.
Experiments on both synthetic and real data sets show that the proposed test is
able to significantly reduce the Type-I error rate, in comparison with several
classic two-sample testing methods. More importantly, the practical usage of
such two-cluster test is further verified through its applications in
tree-based interpretable clustering and significance-based hierarchical
clustering.

</details>


### [63] [Online Pre-Training for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2507.08387)
*Yongjae Shin,Jeonghye Kim,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngsoo Jang,Geonhyeong Kim,Jongseong Chae,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 论文提出了一种新方法OPT，通过在线预训练解决离线预训练代理在在线微调时价值估计不准确的问题，性能提升了30%。


<details>
  <summary>Details</summary>
Motivation: 离线预训练的代理在在线微调时因分布偏移导致价值估计不准确，甚至不如随机初始化。

Method: 引入在线预训练阶段，训练专门用于在线微调的新价值函数。

Result: 在TD3和SPOT上实现，D4RL环境中平均性能提升30%。

Conclusion: OPT有效解决了离线到在线强化学习中的价值估计问题，显著提升了性能。

Abstract: Offline-to-online reinforcement learning (RL) aims to integrate the
complementary strengths of offline and online RL by pre-training an agent
offline and subsequently fine-tuning it through online interactions. However,
recent studies reveal that offline pre-trained agents often underperform during
online fine-tuning due to inaccurate value estimation caused by distribution
shift, with random initialization proving more effective in certain cases. In
this work, we propose a novel method, Online Pre-Training for Offline-to-Online
RL (OPT), explicitly designed to address the issue of inaccurate value
estimation in offline pre-trained agents. OPT introduces a new learning phase,
Online Pre-Training, which allows the training of a new value function tailored
specifically for effective online fine-tuning. Implementation of OPT on TD3 and
SPOT demonstrates an average 30% improvement in performance across a wide range
of D4RL environments, including MuJoCo, Antmaze, and Adroit.

</details>


### [64] [Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling](https://arxiv.org/abs/2507.08390)
*Meihua Dang,Jiaqi Han,Minkai Xu,Kai Xu,Akash Srivastava,Stefano Ermon*

Main category: cs.LG

TL;DR: 论文提出了一种基于粒子吉布斯采样的离散扩散模型推理时间扩展方法，用于奖励引导的文本生成，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言建模中表现出色，但其推理时间扩展尚未充分探索，尤其在奖励引导的文本生成任务中。

Method: 采用粒子吉布斯采样算法，通过条件序贯蒙特卡洛迭代优化扩散轨迹，逐步接近奖励加权目标分布。

Result: 实验表明，该方法在奖励引导的文本生成任务中优于现有推理时间扩展策略，显著提高了准确性。

Conclusion: 提出的方法在固定计算预算下，通过多轨迹迭代优化，实现了高质量的文本生成。

Abstract: Discrete diffusion models have emerged as a powerful paradigm for language
modeling, rivaling auto-regressive models by training-time scaling. However,
inference-time scaling in discrete diffusion models remains relatively
under-explored. In this work, we study sampling-based approaches for achieving
high-quality text generation from discrete diffusion models in reward-guided
settings. We introduce a novel inference-time scaling approach based on
particle Gibbs sampling for discrete diffusion models. The particle Gibbs
sampling algorithm iteratively refines full diffusion trajectories using
conditional Sequential Monte Carlo as its transition mechanism. This process
ensures that the updated samples progressively improve and move closer to the
reward-weighted target distribution. Unlike existing inference-time scaling
methods, which are often limited to single diffusion trajectories, our approach
leverages iterative refinement across multiple trajectories. Within this
framework, we further analyze the trade-offs between four key axes for
inference-time scaling under fixed compute budgets: particle Gibbs iterations,
particle count, denoising steps, and reward estimation cost. Empirically, our
method consistently outperforms prior inference-time strategies on
reward-guided text generation tasks, achieving significant improvement in
accuracy under varying compute budgets.

</details>


### [65] [RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices](https://arxiv.org/abs/2507.08424)
*Anirudh Varanasi,Robin Degraeve,Philippe Roussel,Clement Merckling*

Main category: cs.LG

TL;DR: RTNinja是一个全自动机器学习框架，用于无监督分析随机电报噪声信号，无需先验知识即可识别隐藏源的特性。


<details>
  <summary>Details</summary>
Motivation: 随机电报噪声影响纳米电子器件的可靠性和性能，传统分析方法受限，需要自动化解决方案。

Method: RTNinja包含LevelsExtractor（贝叶斯推断去噪）和SourcesMapper（概率聚类优化）两个模块。

Result: 在7000个数据集中，RTNinja表现出高保真信号重建和准确提取源特性的能力。

Conclusion: RTNinja为随机电报噪声分析提供了强大、可扩展的工具，适用于下一代纳米电子器件的研究。

Abstract: Random telegraph noise is a prevalent variability phenomenon in
nanoelectronic devices, arising from stochastic carrier exchange at defect
sites and critically impacting device reliability and performance. Conventional
analysis techniques often rely on restrictive assumptions or manual
interventions, limiting their applicability to complex, noisy datasets. Here,
we introduce RTNinja, a generalized, fully automated machine learning framework
for the unsupervised analysis of random telegraph noise signals. RTNinja
deconvolves complex signals to identify the number and characteristics of
hidden individual sources, without requiring prior knowledge of the system. The
framework comprises two modular components: LevelsExtractor, which uses
Bayesian inference and model selection to denoise and discretize the signal;
and SourcesMapper, which infers source configurations through probabilistic
clustering and optimization. To evaluate performance, we developed a Monte
Carlo simulator that generates labeled datasets spanning broad signal-to-noise
ratios and source complexities; across 7000 such datasets, RTNinja consistently
demonstrated high-fidelity signal reconstruction and accurate extraction of
source amplitudes and activity patterns. Our results demonstrate that RTNinja
offers a robust, scalable, and device-agnostic tool for random telegraph noise
characterization, enabling large-scale statistical benchmarking,
reliability-centric technology qualification, predictive failure modeling, and
device physics exploration in next-generation nanoelectronics.

</details>


### [66] [KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations](https://arxiv.org/abs/2507.08443)
*Georgios Balanos,Evangelos Chasanis,Konstantinos Skianis,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: KGRAG-Ex利用知识图谱提升RAG系统的可解释性和事实基础，通过扰动方法分析图谱组件对生成答案的影响。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中基于非结构化文本检索的可解释性问题。

Method: 构建领域特定知识图谱，将图谱子结构转化为伪段落，结合扰动方法评估图谱组件对答案的影响。

Result: 实验分析了扰动方法、图谱组件重要性、语义节点类型及图谱指标对解释过程的影响。

Conclusion: KGRAG-Ex通过结构化知识图谱显著提升了RAG系统的可解释性和事实基础。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by grounding
responses in external information, yet explainability remains a critical
challenge, particularly when retrieval relies on unstructured text. Knowledge
graphs (KGs) offer a solution by introducing structured, semantically rich
representations of entities and their relationships, enabling transparent
retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,
a RAG system that improves both factual grounding and explainability by
leveraging a domain-specific KG constructed via prompt-based information
extraction. Given a user query, KGRAG-Ex identifies relevant entities and
semantic paths in the graph, which are then transformed into pseudo-paragraphs:
natural language representations of graph substructures that guide corpus
retrieval. To improve interpretability and support reasoning transparency, we
incorporate perturbation-based explanation methods that assess the influence of
specific KG-derived components on the generated answers. We conduct a series of
experiments to analyze the sensitivity of the system to different perturbation
methods, the relationship between graph component importance and their
structural positions, the influence of semantic node types, and how graph
metrics correspond to the influence of components within the explanations
process.

</details>


### [67] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin,M. Á. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: 提出了一种基于空间填充曲线的注意力机制，用于处理几何域（如流形）上的序列数据，并以2-球面上的极坐标螺旋为例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在几何域（如流形）上缺乏全局顺序的问题，扩展Transformer模型的应用范围。

Method: 采用空间填充曲线引导的注意力头，提出Spiroformer模型，以极坐标螺旋为例在2-球面上实现。

Result: 实验验证了Spiroformer在几何域上的有效性。

Conclusion: 通过空间填充曲线引导的注意力机制，成功将Transformer模型推广到几何域。

Abstract: Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [68] [Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds](https://arxiv.org/abs/2507.08465)
*Feijiang Li,Liuya Zhang,Jieting Wang,Tao Yan,Yuhua Qian*

Main category: cs.LG

TL;DR: 本文提出了一种基于Rank Set Sampling (RSS)的MLP方法（RSS-MLP），通过减少经验损失的方差来提升模型泛化能力，理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过减少经验损失的方差来提升多层感知机（MLP）的泛化能力。

Method: 提出RSS-MLP方法，利用Rank Set Sampling (RSS)在训练数据中引入有序结构，以减少经验损失的方差。

Result: 理论证明RSS估计的经验指数损失和逻辑损失的方差小于Simple Random Sampling (SRS)的估计值。实验在12个基准数据集上验证了RSS-MLP的有效性。

Conclusion: RSS-MLP通过减少方差显著提升了MLP的性能，理论和实验均支持其合理性。

Abstract: Multilayer perceptron (MLP), one of the most fundamental neural networks, is
extensively utilized for classification and regression tasks. In this paper, we
establish a new generalization error bound, which reveals how the variance of
empirical loss influences the generalization ability of the learning model.
Inspired by this learning bound, we advocate to reduce the variance of
empirical loss to enhance the ability of MLP. As is well-known, bagging is a
popular ensemble method to realize variance reduction. However, bagging
produces the base training data sets by the Simple Random Sampling (SRS)
method, which exhibits a high degree of randomness. To handle this issue, we
introduce an ordered structure in the training data set by Rank Set Sampling
(RSS) to further reduce the variance of loss and develop a RSS-MLP method.
Theoretical results show that the variance of empirical exponential loss and
the logistic loss estimated by RSS are smaller than those estimated by SRS,
respectively. To validate the performance of RSS-MLP, we conduct comparison
experiments on twelve benchmark data sets in terms of the two convex loss
functions under two fusion methods. Extensive experimental results and analysis
illustrate the effectiveness and rationality of the propose method.

</details>


### [69] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer,Christian Kroos,Chris Hinze,Viktor Hangya,Luzian Hahn,Fabian Küch*

Main category: cs.LG

TL;DR: 比较三种优化器（AdamW、Lion、Sophia）在LLM预训练中的表现，发现Sophia训练损失最低，Lion最快，AdamW下游任务表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化器对LLM预训练时间和模型性能至关重要，需比较不同优化器的效果。

Method: 使用两种基础架构和单/多轮次方法，固定token数量，通过Maximal Update Parametrization和小模型调参。

Result: Sophia训练损失最低，Lion最快，AdamW下游任务表现最好。

Conclusion: 不同优化器各有优势，需根据具体需求选择。

Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [70] [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)
*Gonçalo Paulo,Nora Belrose*

Main category: cs.LG

TL;DR: 提出了一种无需生成自然语言解释的稀疏编码器可解释性评估方法，并与人类评估结果对比。


<details>
  <summary>Details</summary>
Motivation: 稀疏自动编码器和转码器的可解释性评估缺乏共识，现有方法依赖自然语言解释，难以直接衡量潜在变量的可解释性。

Method: 改进现有方法，避免生成自然语言解释，直接评估稀疏编码器的可解释性，并与人类评估结果对比。

Result: 新方法提供了更直接且标准化的可解释性评估，并与人类评估结果一致。

Conclusion: 建议社区采用更直接的可解释性评估方法，避免依赖自然语言解释的中间步骤。

Abstract: Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an LLM to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
sparse coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.

</details>


### [71] [SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction](https://arxiv.org/abs/2507.08475)
*Haitao Lin,Junjie Wang,Zhifeng Gao,Xiaohong Ji,Rong Zhu,Linfeng Zhang,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: SynBridge是一种基于双向流的生成模型，用于多任务反应预测，通过图到图变换和离散流桥捕捉化学反应的离散状态变化。


<details>
  <summary>Details</summary>
Motivation: 化学反应的本质是电子的重新分布和重组，通常表现为电子转移或电子对的迁移。这些变化在物理世界中是离散且突然的，如原子电荷状态的改变或化学键的形成与断裂。

Method: 提出SynBridge模型，利用图到图变换网络架构和离散流桥，捕捉反应物和产物之间通过键和原子的离散状态进行的双向化学转化。

Result: 在三个基准数据集（USPTO-50K、USPTO-MIT、Pistachio）上进行了广泛实验，证明了模型在正向和逆向合成任务中的最优性能。

Conclusion: 通过消融研究和噪声调度分析，揭示了离散空间结构化扩散对反应预测的益处。

Abstract: The essence of a chemical reaction lies in the redistribution and
reorganization of electrons, which is often manifested through electron
transfer or the migration of electron pairs. These changes are inherently
discrete and abrupt in the physical world, such as alterations in the charge
states of atoms or the formation and breaking of chemical bonds. To model the
transition of states, we propose SynBridge, a bidirectional flow-based
generative model to achieve multi-task reaction prediction. By leveraging a
graph-to-graph transformer network architecture and discrete flow bridges
between any two discrete distributions, SynBridge captures bidirectional
chemical transformations between graphs of reactants and products through the
bonds' and atoms' discrete states. We further demonstrate the effectiveness of
our method through extensive experiments on three benchmark datasets
(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in
both forward and retrosynthesis tasks. Our ablation studies and noise
scheduling analysis reveal the benefits of structured diffusion over discrete
spaces for reaction prediction.

</details>


### [72] [Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R](https://arxiv.org/abs/2507.08505)
*Pablo Robin Guerrero,Yueyang Pan,Sanidhya Kashyap*

Main category: cs.LG

TL;DR: 本文调查了移动设备上视觉语言模型（VLM）的部署框架，评估了llama.cpp、MLC-Imp和mllm在OnePlus 13R上的性能，发现CPU过度使用而GPU和NPU利用不足。


<details>
  <summary>Details</summary>
Motivation: 移动设备上部署VLM面临计算和能效挑战，需评估现有框架的性能瓶颈。

Method: 在OnePlus 13R上运行LLaVA-1.5 7B、MobileVLM-3B和Imp-v1.5 3B，测量CPU、GPU、NPU利用率、温度、推理时间、功耗和用户体验。

Result: CPU在生成令牌时过度使用，GPU和NPU利用不足或饱和，导致设备响应性下降。

Conclusion: 当前部署框架存在CPU过度使用和GPU/NPU利用不稳定的问题，需优化硬件资源分配。

Abstract: Vision-Language Models (VLMs) offer promising capabilities for mobile
devices, but their deployment faces significant challenges due to computational
limitations and energy inefficiency, especially for real-time applications.
This study provides a comprehensive survey of deployment frameworks for VLMs on
mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of
running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads
on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R
while running VLMs, with measurements covering CPU, GPU, and NPU utilization,
temperature, inference time, power consumption, and user experience.
Benchmarking revealed critical performance bottlenecks across frameworks: CPU
resources were consistently over-utilized during token generation, while GPU
and NPU accelerators were largely unused. When the GPU was used, primarily for
image feature extraction, it was saturated, leading to degraded device
responsiveness. The study contributes framework-level benchmarks, practical
profiling tools, and an in-depth analysis of hardware utilization bottlenecks,
highlighting the consistent overuse of CPUs and the ineffective or unstable use
of GPUs and NPUs in current deployment frameworks.

</details>


### [73] [SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.08508)
*Haotian Xu,Jinrui Zhou,Xichong Zhang,Mingjun Xiao,He Sun,Yin Xu*

Main category: cs.LG

TL;DR: SFedKD通过多教师知识蒸馏解决顺序联邦学习中的灾难性遗忘问题，并通过精细权重分配和教师选择机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 顺序联邦学习（SFL）在数据异构环境下具有强收敛性，但面临灾难性遗忘问题，导致模型遗忘先前学到的知识。

Method: 提出SFedKD框架，采用多教师知识蒸馏，基于类别分布差异分配权重，并通过贪婪策略选择互补教师以减少冗余。

Result: 实验表明SFedKD有效缓解灾难性遗忘，性能优于现有联邦学习方法。

Conclusion: SFedKD通过多教师知识蒸馏和教师选择机制，显著提升了顺序联邦学习的模型性能。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm which
coordinates multiple clients to collaboratively train a global model via a
central server. Sequential Federated Learning (SFL) is a newly-emerging FL
training framework where the global model is trained in a sequential manner
across clients. Since SFL can provide strong convergence guarantees under data
heterogeneity, it has attracted significant research attention in recent years.
However, experiments show that SFL suffers from severe catastrophic forgetting
in heterogeneous environments, meaning that the model tends to forget knowledge
learned from previous clients. To address this issue, we propose an SFL
framework with discrepancy-aware multi-teacher knowledge distillation, called
SFedKD, which selects multiple models from the previous round to guide the
current round of training. In SFedKD, we extend the single-teacher Decoupled
Knowledge Distillation approach to our multi-teacher setting and assign
distinct weights to teachers' target-class and non-target-class knowledge based
on the class distributional discrepancy between teacher and student data.
Through this fine-grained weighting strategy, SFedKD can enhance model training
efficacy while mitigating catastrophic forgetting. Additionally, to prevent
knowledge dilution, we eliminate redundant teachers for the knowledge
distillation and formalize it as a variant of the maximum coverage problem.
Based on the greedy strategy, we design a complementary-based teacher selection
mechanism to ensure that the selected teachers achieve comprehensive knowledge
space coverage while reducing communication and computational costs. Extensive
experiments show that SFedKD effectively overcomes catastrophic forgetting in
SFL and outperforms state-of-the-art FL methods.

</details>


### [74] [Recursive Reward Aggregation](https://arxiv.org/abs/2507.08537)
*Yuting Tang,Yivan Zhang,Johannes Ackermann,Yu-Jie Zhang,Soichiro Nishimori,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出一种无需修改奖励函数的强化学习行为对齐方法，通过选择适当的奖励聚合函数实现灵活目标优化。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，复杂目标的行为对齐通常需要精心设计奖励函数，这具有挑战性。

Method: 引入MDP的代数视角，通过递归生成和聚合奖励，推广标准折扣和到其他递归聚合（如折扣最大值和夏普比率）。

Result: 实验证明该方法能有效优化多样目标，适用于确定性和随机环境，并与值基和演员-评论家算法无缝集成。

Conclusion: 该方法展示了灵活性和实际应用潜力，为复杂目标的行为对齐提供了新思路。

Abstract: In reinforcement learning (RL), aligning agent behavior with specific
objectives typically requires careful design of the reward function, which can
be challenging when the desired objectives are complex. In this work, we
propose an alternative approach for flexible behavior alignment that eliminates
the need to modify the reward function by selecting appropriate reward
aggregation functions. By introducing an algebraic perspective on Markov
decision processes (MDPs), we show that the Bellman equations naturally emerge
from the recursive generation and aggregation of rewards, allowing for the
generalization of the standard discounted sum to other recursive aggregations,
such as discounted max and Sharpe ratio. Our approach applies to both
deterministic and stochastic settings and integrates seamlessly with
value-based and actor-critic algorithms. Experimental results demonstrate that
our approach effectively optimizes diverse objectives, highlighting its
versatility and potential for real-world applications.

</details>


### [75] [CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes](https://arxiv.org/abs/2507.08542)
*Tianyou Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer和混合专家的深度学习框架CircFormerMoE，用于直接从植物基因组DNA预测circRNAs，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有circRNA识别方法依赖RNA-seq数据，计算成本高且无法直接从基因组DNA预测，植物中circRNA剪接位点缺乏典型GT-AG模体，且缺乏高效深度学习模型。

Method: CircFormerMoE框架包含剪接位点检测（SSD）和剪接位点配对（SSP）两个子任务，基于Transformer和混合专家模型，直接从基因组DNA预测circRNAs。

Result: 在10种植物基因数据上验证了模型有效性，能够发现未注释的circRNAs，并进行了可解释性分析。

Conclusion: CircFormerMoE为植物大规模circRNA发现提供了快速准确的计算方法和工具，为植物功能基因组学和非编码RNA注释研究奠定了基础。

Abstract: Circular RNAs (circRNAs) are important components of the non-coding RNA
regulatory network. Previous circRNA identification primarily relies on
high-throughput RNA sequencing (RNA-seq) data combined with alignment-based
algorithms that detect back-splicing signals. However, these methods face
several limitations: they can't predict circRNAs directly from genomic DNA
sequences and relies heavily on RNA experimental data; they involve high
computational costs due to complex alignment and filtering steps; and they are
inefficient for large-scale or genome-wide circRNA prediction. The challenge is
even greater in plants, where plant circRNA splice sites often lack the
canonical GT-AG motif seen in human mRNA splicing, and no efficient deep
learning model with strong generalization capability currently exists.
Furthermore, the number of currently identified plant circRNAs is likely far
lower than their true abundance. In this paper, we propose a deep learning
framework named CircFormerMoE based on transformers and mixture-of experts for
predicting circRNAs directly from plant genomic DNA. Our framework consists of
two subtasks known as splicing site detection (SSD) and splicing site pairing
(SSP). The model's effectiveness has been validated on gene data of 10 plant
species. Trained on known circRNA instances, it is also capable of discovering
previously unannotated circRNAs. In addition, we performed interpretability
analyses on the trained model to investigate the sequence patterns contributing
to its predictions. Our framework provides a fast and accurate computational
method and tool for large-scale circRNA discovery in plants, laying a
foundation for future research in plant functional genomics and non-coding RNA
annotation.

</details>


### [76] [STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2507.08563)
*Xinyi Ning,Zilin Bian,Kaan Ozbay,Semiha Ergan*

Main category: cs.LG

TL;DR: 提出了一种新颖的时空风险感知轨迹预测框架，通过风险势场评估周围车辆行为的不确定性或攻击性，显著提升了高风险场景下的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略周围车辆不确定或攻击性行为带来的潜在风险，而准确的轨迹预测对自动驾驶系统的安全性和效率至关重要。

Method: 结合时空编码器和风险感知特征融合解码器，将风险势场嵌入时空特征表示中，并设计了风险缩放损失函数以优化高风险场景的预测。

Result: 在NGSIM和HighD数据集上，平均预测误差分别降低了4.8%和31.2%，尤其是在高风险场景中表现突出。

Conclusion: 该框架提供了可解释的风险感知预测，有助于增强自动驾驶系统的决策鲁棒性。

Abstract: Accurate vehicle trajectory prediction is essential for ensuring safety and
efficiency in fully autonomous driving systems. While existing methods
primarily focus on modeling observed motion patterns and interactions with
other vehicles, they often neglect the potential risks posed by the uncertain
or aggressive behaviors of surrounding vehicles. In this paper, we propose a
novel spatial-temporal risk-attentive trajectory prediction framework that
incorporates a risk potential field to assess perceived risks arising from
behaviors of nearby vehicles. The framework leverages a spatial-temporal
encoder and a risk-attentive feature fusion decoder to embed the risk potential
field into the extracted spatial-temporal feature representations for
trajectory prediction. A risk-scaled loss function is further designed to
improve the prediction accuracy of high-risk scenarios, such as short relative
spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate
that our method reduces average prediction errors by 4.8% and 31.2%
respectively compared to state-of-the-art approaches, especially in high-risk
scenarios. The proposed framework provides interpretable, risk-aware
predictions, contributing to more robust decision-making for autonomous driving
systems.

</details>


### [77] [AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling](https://arxiv.org/abs/2507.08567)
*Preslav Aleksandrov,Meghdad Kurmanji,Fernando Garcia Redondo,David O'Shea,William Shen,Alex Iacob,Lorenzo Sani,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.LG

TL;DR: AbbIE是一种新型递归Transformer编码器，通过动态调整计算资源，在测试时显著提升性能，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索一种无需增加参数或数据量即可提升Transformer性能的方法。

Method: 采用递归编码器架构，在潜在空间进行迭代，无需特殊数据集或训练协议。

Result: AbbIE在零样本学习和语言困惑度上分别提升12%和5%，并能动态调整计算资源。

Conclusion: AbbIE为Transformer性能扩展提供了新途径。

Abstract: We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a
novel recursive generalization of the encoder-only Transformer architecture,
which achieves better perplexity than a standard Transformer and allows for the
dynamic scaling of compute resources at test time. This simple, recursive
approach is a complement to scaling large language model (LLM) performance
through parameter and token counts. AbbIE performs its iterations in latent
space, but unlike latent reasoning models, does not require a specialized
dataset or training protocol. We show that AbbIE upward generalizes (ability to
generalize to arbitrary iteration lengths) at test time by only using 2
iterations during train time, far outperforming alternative iterative methods.
AbbIE's ability to scale its computational expenditure based on the complexity
of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context
learning tasks versus other iterative and standard methods and up to 5\%
improvement in language perplexity. The results from this study open a new
avenue to Transformer performance scaling. We perform all of our evaluations on
model sizes up to 350M parameters.

</details>


### [78] [ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection](https://arxiv.org/abs/2507.08597)
*Md Tanvirul Alam,Aritran Piplai,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ADAPT的半监督伪标签算法，用于解决恶意软件检测中的概念漂移问题，实验表明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在恶意软件分类中因概念漂移导致性能下降，频繁更新需要昂贵标注，半监督学习未被充分探索。

Method: 提出ADAPT，一种模型无关的半监督伪标签算法，适用于多种机器学习模型。

Result: 在五个恶意软件检测数据集上表现优于基线模型和竞争基准。

Conclusion: ADAPT为恶意软件检测中概念漂移的适应提供了更有效的方法。

Abstract: Machine learning models are commonly used for malware classification;
however, they suffer from performance degradation over time due to concept
drift. Adapting these models to changing data distributions requires frequent
updates, which rely on costly ground truth annotations. While active learning
can reduce the annotation burden, leveraging unlabeled data through
semi-supervised learning remains a relatively underexplored approach in the
context of malware detection. In this research, we introduce \texttt{ADAPT}, a
novel pseudo-labeling semi-supervised algorithm for addressing concept drift.
Our model-agnostic method can be applied to various machine learning models,
including neural networks and tree-based algorithms. We conduct extensive
experiments on five diverse malware detection datasets spanning Android,
Windows, and PDF domains. The results demonstrate that our method consistently
outperforms baseline models and competitive benchmarks. This work paves the way
for more effective adaptation of machine learning models to concept drift in
malware detection.

</details>


### [79] [Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India](https://arxiv.org/abs/2507.08605)
*Ando Shah,Rajveer Singh,Akram Zaytar,Girmaw Abebe Tadesse,Caleb Robinson,Negar Tafti,Stephen A. Wood,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: 该研究开发了一种遥感框架，用于监测印度旁遮普地区的水稻可持续灌溉实践（如DSR和AWD），以解决水资源管理挑战，并支持政策制定。


<details>
  <summary>Details</summary>
Motivation: 水稻种植消耗大量淡水，而可持续灌溉实践（如DSR和AWD）可显著减少用水量，但缺乏数据支持政策制定和资源分配。

Method: 研究结合Sentinel-1卫星影像和地面真实数据（来自PRANA项目），开发了分类系统，以区分不同的灌溉实践。

Result: 该方法在区分DSR与传统水稻种植方面达到78%的F1分数，并在旁遮普地区成功映射了约300万农田的DSR采用情况。

Conclusion: 该研究为政策制定者提供了监测可持续水资源管理实践的工具，有助于干预措施的精准实施和效果评估。

Abstract: Rice cultivation consumes 24-30% of global freshwater, creating critical
water management challenges in major rice-producing regions. Sustainable
irrigation practices like direct seeded rice (DSR) and alternate wetting and
drying (AWD) can reduce water use by 20-40% while maintaining yields, helping
secure long-term agricultural productivity as water scarcity intensifies - a
key component of the Zero Hunger Sustainable Development Goal. However, limited
data on adoption rates of these practices prevents evidence-based policymaking
and targeted resource allocation. We developed a novel remote sensing framework
to monitor sustainable water management practices at scale in Punjab, India - a
region facing severe groundwater depletion of 41.6 cm/year. To collect
essential ground truth data, we partnered with the Nature Conservancy's
Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained
approximately 1,400 farmers on water-saving techniques while documenting their
field-level practices. Using this data, we created a classification system with
Sentinel-1 satellite imagery that separates water management along sowing and
irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing
DSR from traditional puddled transplanted rice without requiring prior
knowledge of planting dates. We demonstrated scalability by mapping DSR
adoption across approximately 3 million agricultural plots in Punjab, with
district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)
with government records. This study provides policymakers with a powerful tool
to track sustainable water management adoption, target interventions, and
measure program impacts at scale.

</details>


### [80] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 论文提出了一种名为LoGIC的多智能体强化学习方法，用于无监督图像描述任务，通过游戏化学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于现有标注数据集已被用于训练大型视觉语言模型（VLMs），提升性能变得困难，因此探索无监督图像描述任务至关重要。

Method: 提出LoGIC方法，包含一个‘说话者’和一个‘听者’两个智能体，通过合作共同奖励设置和GRPO算法训练，利用预训练VLMs和LLMs提升性能。

Result: 使用预训练VLMs和LLMs时，LoGIC在无额外标注下达到46 BLEU分，优于基线方法；轻量级组件版本在无监督设置下达到31 BLEU分，显著优于现有方法。

Conclusion: LoGIC通过游戏化学习有效提升无监督图像描述性能，为资源受限场景提供了轻量级解决方案。

Abstract: Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [81] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu,Jiaqi Wang,Haoyu Wang,Mingquan Lin,Han Liu,Nelson S. Yee,Fenglong Ma*

Main category: cs.LG

TL;DR: FedAKD通过异步知识蒸馏解决联邦学习中的不平衡协变量偏移问题，提升协作公平性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了不平衡协变量偏移这一复杂异质性，FedAKD旨在平衡准确预测与协作公平性。

Method: FedAKD结合客户端和服务器更新，通过异步知识蒸馏策略处理正确和错误预测样本的分布差异。

Result: 在多个数据集上验证，FedAKD显著提升了协作公平性、预测准确性，并促进了客户端参与。

Conclusion: FedAKD是一种简单有效的方法，适用于高度异质数据分布下的联邦学习。

Abstract: Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [82] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: WERSA是一种线性时间复杂度的注意力机制，通过结合随机谱特征和多分辨率Haar小波，显著降低计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列处理中因二次时间复杂度带来的高计算成本问题。

Method: 结合内容自适应的随机谱特征、多分辨率Haar小波和可学习参数，实现线性时间复杂度。

Result: 在多个基准测试中表现最佳，显著减少训练时间和计算资源，同时提升准确性。

Conclusion: WERSA为长序列处理提供了高效且经济的解决方案，尤其适用于资源有限的硬件。

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [83] [Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation](https://arxiv.org/abs/2507.08686)
*Uri Stern,Eli Corn,Daphna Weinshall*

Main category: cs.LG

TL;DR: 论文提出了一种衡量深度学习模型在验证数据上遗忘率的分数，揭示了局部过拟合现象，并提出了一种两阶段方法（集成和蒸馏）来恢复遗忘的知识，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统理论认为模型容量增加会导致过拟合，但实践中很少观察到全局过拟合。论文探讨了局部过拟合的可能性及其与双下降现象的关系。

Method: 提出了一种两阶段方法：1）将训练过程中的检查点集成；2）通过知识蒸馏将集成模型压缩为原始大小的单一模型。

Result: 实验表明，该方法在标签噪声情况下优于原始模型和独立训练的集成模型，同时降低了训练和推理复杂度。

Conclusion: 局部过拟合是存在的，且可以通过知识融合和蒸馏有效缓解，实现性能提升和复杂度降低的双赢。

Abstract: Overfitting in deep neural networks occurs less frequently than expected.
This is a puzzling observation, as theory predicts that greater model capacity
should eventually lead to overfitting -- yet this is rarely seen in practice.
But what if overfitting does occur, not globally, but in specific sub-regions
of the data space? In this work, we introduce a novel score that measures the
forgetting rate of deep models on validation data, capturing what we term local
overfitting: a performance degradation confined to certain regions of the input
space. We demonstrate that local overfitting can arise even without
conventional overfitting, and is closely linked to the double descent
phenomenon.
  Building on these insights, we introduce a two-stage approach that leverages
the training history of a single model to recover and retain forgotten
knowledge: first, by aggregating checkpoints into an ensemble, and then by
distilling it into a single model of the original size, thus enhancing
performance without added inference cost.
  Extensive experiments across multiple datasets, modern architectures, and
training regimes validate the effectiveness of our approach. Notably, in the
presence of label noise, our method -- Knowledge Fusion followed by Knowledge
Distillation -- outperforms both the original model and independently trained
ensembles, achieving a rare win-win scenario: reduced training and inference
complexity.

</details>


### [84] [Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning](https://arxiv.org/abs/2507.08697)
*Waqar Muhammad Ashraf,Amir H. Keshavarzzadeh,Abdulelah S. Alshehri,Abdulrahman bin Jumah,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 论文提出了一种基于马氏距离的优化框架（MAD-OPT），将领域知识融入数据驱动的分析中，以提高热电厂中AI的实用性。


<details>
  <summary>Details</summary>
Motivation: 热电厂中AI的采用率低，主要因为AI算法的黑盒特性及传统数据分析中领域知识的缺失。

Method: 开发了MAD-OPT框架，通过马氏距离约束将领域知识引入数据分析，并应用于395 MW燃气轮机系统。

Result: MAD-OPT能估计不同环境下的最优工艺条件，且结果稳健；还能估计超出设计功率限制的工艺条件，与实际数据相符。

Conclusion: 研究表明，不结合领域知识的优化可能无效；MAD-OPT推动了数据驱动与领域知识的结合，为AI在热电厂的安全应用铺平了道路。

Abstract: The domain-consistent adoption of artificial intelligence (AI) remains low in
thermal power plants due to the black-box nature of AI algorithms and low
representation of domain knowledge in conventional data-centric analytics. In
this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)
framework that incorporates the Mahalanobis distance-based constraint to
introduce domain knowledge into data-centric analytics. The developed MAD-OPT
framework is applied to maximize thermal efficiency and minimize turbine heat
rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT
framework can estimate domain-informed optimal process conditions under
different ambient conditions, and the optimal solutions are found to be robust
as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to
estimate optimal process conditions beyond the design power generation limit of
the gas turbine system, and have found comparable results with the actual data
of the power plant. We demonstrate that implementing data-centric optimization
analytics without incorporating domain-informed constraints may provide
ineffective solutions that may not be implementable in the real operation of
the gas turbine system. This research advances the integration of the
data-driven domain knowledge into machine learning-powered analytics that
enhances the domain-informed operation excellence and paves the way for safe AI
adoption in thermal power systems.

</details>


### [85] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley,Zachary Serlin,Tyler Paine,Makai Mann,Michael Benjamin,Calin Belta*

Main category: cs.LG

TL;DR: SPLASH是一种基于偏好的逆向强化学习方法，用于从次优演示中学习长时程和对抗性任务，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有逆向强化学习方法假设专家演示可用，但实际中往往只有次优演示，且不适用于长时程或对抗性任务。

Method: 提出SPLASH方法，结合偏好学习和分层演示，适用于长时程和对抗性任务。

Result: 在模拟的海上夺旗任务和真实无人艇实验中，SPLASH显著优于现有技术。

Conclusion: SPLASH填补了逆向强化学习在次优演示、长时程和对抗性任务中的不足，具有实际应用潜力。

Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [86] [On the Effect of Regularization in Policy Mirror Descent](https://arxiv.org/abs/2507.08718)
*Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: PMD框架结合了两种正则化技术，通过大规模实验验证了它们的相互作用及对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究PMD中两种正则化技术的实际效果及其相互作用，填补理论研究和实证之间的空白。

Method: 在小型RL环境中运行超过50万次训练种子，分析两种正则化技术的组合效果。

Result: 两种正则化技术可以部分替代，但精确组合对性能至关重要。

Conclusion: 研究为开发更稳健的RL算法提供了方向，特别是针对超参数敏感性问题。

Abstract: Policy Mirror Descent (PMD) has emerged as a unifying framework in
reinforcement learning (RL) by linking policy gradient methods with a
first-order optimization method known as mirror descent. At its core, PMD
incorporates two key regularization components: (i) a distance term that
enforces a trust region for stable policy updates and (ii) an MDP regularizer
that augments the reward function to promote structure and robustness. While
PMD has been extensively studied in theory, empirical investigations remain
scarce. This work provides a large-scale empirical analysis of the interplay
between these two regularization techniques, running over 500k training seeds
on small RL environments. Our results demonstrate that, although the two
regularizers can partially substitute each other, their precise combination is
critical for achieving robust performance. These findings highlight the
potential for advancing research on more robust algorithms in RL, particularly
with respect to hyperparameter sensitivity.

</details>


### [87] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer,Metod Jazbec,Christian A. Naesseth,Eric Nalisnick*

Main category: cs.LG

TL;DR: 论文提出了一种结合测试时适应（TTA）和风险监控框架的方法，用于检测模型性能下降的临界点。


<details>
  <summary>Details</summary>
Motivation: 测试时数据偏移是部署预测模型时的常见挑战，TTA虽能暂时适应，但模型最终会退化，需要离线重训练。

Method: 扩展了基于顺序测试的监控工具，结合置信序列，适应无标签测试数据和模型更新的场景。

Result: 提出的TTA监控框架在多种数据集、分布偏移类型和TTA方法中表现有效。

Conclusion: 通过结合TTA和风险监控，能更早发现模型性能下降，避免模型失效。

Abstract: Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [88] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach,Oren Glickman,Tom Tirer*

Main category: cs.LG

TL;DR: 论文提出了一种新方法，通过跟踪参数在训练末期的活动来缓解深度神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘会导致学习新任务时旧任务性能下降，现有正则化方法试图约束重要参数，但效果有限。

Method: 提出在训练末期的高原阶段跟踪参数活动，选择活动性高的参数作为适应新任务的方向。

Result: 实验表明该方法在缓解灾难性遗忘和保持新任务性能方面表现优越。

Conclusion: 该方法通过关注训练末期的参数活动，有效平衡了新旧任务的学习。

Abstract: Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [89] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon,Susana Lopez-Moreno,Eric Dolores-Cuenca,Sieun Lee,Sangil Kim*

Main category: cs.LG

TL;DR: 提出了一种结合延迟嵌入线性输入和可学习多层感知机（MLP）的自适应NVAR模型，提升了预测精度和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统NVAR和RC方法依赖固定非线性结构，适应性差且在高维场景下计算成本高。

Method: 结合延迟嵌入线性输入和可训练MLP，通过梯度优化联合训练，避免参数网格搜索。

Result: 在混沌系统中，自适应模型在预测精度和噪声鲁棒性上优于标准NVAR。

Conclusion: 自适应NVAR模型通过数据驱动的非线性特征学习，提升了预测性能并简化了调参过程。

Abstract: Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [90] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli,Yuanchun He,Stefano Mariani,Martina Siena,Stefano Markidis*

Main category: cs.LG

TL;DR: PHQFNO是一种量子-经典混合的傅里叶神经算子，通过分区计算实现可调混合，并在科学机器学习中表现出优于经典方法的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 扩展量子傅里叶神经算子（QFNO）以支持更高维度和分布式计算，同时探索量子-经典混合方法的潜力。

Method: 采用分区计算策略，结合经典和量子资源，使用单热编码将数据编码为量子态，并通过变分优化量子电路参数。

Result: 在Burgers方程和Navier-Stokes方程上验证，PHQFNO达到经典FNO的精度，并在不可压缩Navier-Stokes中表现更优，且对输入噪声更稳定。

Conclusion: PHQFNO展示了量子-经典混合方法在科学机器学习中的潜力，尤其在精度和稳定性方面优于经典方法。

Abstract: We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [91] [Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network](https://arxiv.org/abs/2507.08749)
*Chuanqi Chen,Zhongrui Wang,Nan Chen,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种离散时间条件高斯Koopman网络（CGKN），用于高维复杂动力系统的状态预测和数据同化，结合科学机器学习与数据同化。


<details>
  <summary>Details</summary>
Motivation: 针对非线性部分观测系统，开发高效的状态预测和数据同化方法，适用于工程和地球科学中的复杂系统。

Method: 利用Koopman嵌入发现潜在状态表示，构建条件高斯系统，通过解析公式高效评估后验分布。

Result: 在多个非线性PDE问题中表现优异，状态预测与数据同化性能媲美现有最佳方法。

Conclusion: CGKN框架为科学机器学习与数据同化的统一提供了示例，并支持外环应用如优化设计和控制。

Abstract: A discrete-time conditional Gaussian Koopman network (CGKN) is developed in
this work to learn surrogate models that can perform efficient state forecast
and data assimilation (DA) for high-dimensional complex dynamical systems,
e.g., systems governed by nonlinear partial differential equations (PDEs).
Focusing on nonlinear partially observed systems that are common in many
engineering and earth science applications, this work exploits Koopman
embedding to discover a proper latent representation of the unobserved system
states, such that the dynamics of the latent states are conditional linear,
i.e., linear with the given observed system states. The modeled system of the
observed and latent states then becomes a conditional Gaussian system, for
which the posterior distribution of the latent states is Gaussian and can be
efficiently evaluated via analytical formulae. The analytical formulae of DA
facilitate the incorporation of DA performance into the learning process of the
modeled system, which leads to a framework that unifies scientific machine
learning (SciML) and data assimilation. The performance of discrete-time CGKN
is demonstrated on several canonical problems governed by nonlinear PDEs with
intermittency and turbulent features, including the viscous Burgers' equation,
the Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with
which we show that the discrete-time CGKN framework achieves comparable
performance as the state-of-the-art SciML methods in state forecast and
provides efficient and accurate DA results. The discrete-time CGKN framework
also serves as an example to illustrate unifying the development of SciML
models and their other outer-loop applications such as design optimization,
inverse problems, and optimal control.

</details>


### [92] [ML-Based Automata Simplification for Symbolic Accelerators](https://arxiv.org/abs/2507.08751)
*Tiffany Yu,Rye Stahle-Smith,Darssan Eswaramoorthi,Rasha Karakchi*

Main category: cs.LG

TL;DR: AutoSlim是一个基于机器学习的图简化框架，用于降低符号加速器的复杂性，通过随机森林分类剪枝低影响力转换，显著减少自动机图密度。


<details>
  <summary>Details</summary>
Motivation: 符号加速器在基因组学、NLP和网络安全等领域应用广泛，但面临内存使用和路由复杂性的可扩展性问题。

Method: AutoSlim利用随机森林分类，基于边分数和结构特征剪枝低影响力转换，支持加权转换的自动化简化。

Result: 在NAPOLY+上评估，AutoSlim实现了FPGA LUT减少40%，转换剪枝超过30%，并能处理比现有基准大一个数量级的图。

Conclusion: AutoSlim通过剪枝有效缓解硬件资源膨胀，展示了硬件互连对成本的重要影响。

Abstract: Symbolic accelerators are increasingly used for symbolic data processing in
domains such as genomics, NLP, and cybersecurity. However, these accelerators
face scalability issues due to excessive memory use and routing complexity,
especially when targeting a large set. We present AutoSlim, a machine
learning-based graph simplification framework designed to reduce the complexity
of symbolic accelerators built on Non-deterministic Finite Automata (NFA)
deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest
classification to prune low-impact transitions based on edge scores and
structural features, significantly reducing automata graph density while
preserving semantic correctness. Unlike prior tools, AutoSlim targets automated
score-aware simplification with weighted transitions, enabling efficient
ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in
NAPOLY+ and conducted performance measurements including latency, throughput,
and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs
and over 30 percent pruning in transitions, while scaling to graphs an order of
magnitude larger than existing benchmarks. Our results also demonstrate how
hardware interconnection (fanout) heavily influences hardware cost and that
AutoSlim's pruning mitigates resource blowup.

</details>


### [93] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim,Yongjae Shin,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 论文提出PARS算法，通过奖励缩放和惩罚机制解决离线强化学习中的Q值外推问题，并在D4RL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，Q值外推误差是主要问题，尤其是线性外推。

Method: 提出奖励缩放与层归一化（RS-LN）和不可行动作惩罚机制（PA），结合为PARS算法。

Result: 在D4RL基准测试中表现优异，尤其在AntMaze Ultra任务中显著成功。

Conclusion: PARS算法有效解决了Q值外推问题，提升了离线强化学习性能。

Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [94] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 提出了一种新的MoE架构BlockFFN，通过改进路由设计和训练目标，提升稀疏性和加速性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统MoE架构在路由不可微、灵活性差以及稀疏性不足的问题，使其更适合低资源设备和主流加速技术。

Method: 引入结合ReLU激活和RMSNorm的路由器，设计CLS感知的训练目标，并实现高效加速内核。

Result: BlockFFN在稀疏性和性能上优于其他MoE基线，TLS超过80%，8-token CLS达到70%，在终端设备上实现3.67倍加速。

Conclusion: BlockFFN通过改进设计和训练目标，显著提升了MoE架构的稀疏性和加速性能，适用于低资源场景。

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [95] [Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/abs/2507.08784)
*Chuyan Chen,Yutong He,Pengrui Li,Weichen Jia,Kun Yuan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributed optimization is pivotal for large-scale signal processing and
machine learning, yet communication overhead remains a major bottleneck.
Low-rank gradient compression, in which the transmitted gradients are
approximated by low-rank matrices to reduce communication, offers a promising
remedy. Existing methods typically adopt either randomized or greedy
compression strategies: randomized approaches project gradients onto randomly
chosen subspaces, introducing high variance and degrading empirical
performance; greedy methods select the most informative subspaces, achieving
strong empirical results but lacking convergence guarantees. To address this
gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression
algorithm for distributed learning with rigorous convergence guarantees.
GreedyLore incorporates error feedback to correct the bias introduced by greedy
compression and introduces a semi-lazy subspace update that ensures the
compression operator remains contractive throughout all iterations. With these
techniques, we prove that GreedyLore achieves a convergence rate of
$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD
and Adam--marking the first linear speedup convergence rate for low-rank
gradient compression. Extensive experiments are conducted to validate our
theoretical findings.

</details>


### [96] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy,Radu Marinescu,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: ORAC方法通过乐观探索和风险规避的结合，优化了RaCRL中的奖励-成本权衡，避免了保守探索导致的次优策略。


<details>
  <summary>Details</summary>
Motivation: 解决RaCRL中因风险规避导致的保守探索和次优策略问题。

Method: 提出ORAC方法，通过最大化奖励值函数的局部上界和最小化风险规避成本值函数的局部下界来构建探索策略。

Result: 实验证明ORAC在Safety-Gymnasium和CityLearn等任务中显著改善了奖励-成本权衡。

Conclusion: ORAC方法有效避免了次优策略，提升了风险规避强化学习的性能。

Abstract: Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


### [97] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 生成奖励模型（LLMs-as-judges）在复杂任务中易受表面操作影响，导致错误奖励。作者提出数据增强策略，训练出更稳健的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 研究生成奖励模型在评估自由形式输出时的脆弱性，尤其是对非单词符号和推理开头的敏感性。

Method: 通过实验展示问题普遍性，并提出数据增强策略训练新模型以提高稳健性。

Result: 新模型显著提高了对表面操作的鲁棒性，适用于多种任务和提示格式。

Conclusion: 强调需要更可靠的LLM评估方法，并发布了改进的奖励模型和训练数据。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [98] [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
*Denis Sutter,Julian Minder,Thomas Hofmann,Tiago Pimentel*

Main category: cs.LG

TL;DR: 论文探讨了因果抽象的概念，指出无限制的对齐映射会导致因果抽象变得无意义，并提出非线性表示困境。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示因果抽象在无限制对齐映射下的局限性，以及线性表示假设的必要性。

Method: 通过理论和实验证明，使用任意强大的对齐映射会导致因果抽象失去意义。

Result: 结果表明，无限制的对齐映射可以使任何神经网络映射到任何算法，但缺乏信息编码假设时因果抽象变得无意义。

Conclusion: 结论是因果抽象不足以支持机制可解释性，未来研究应关注信息编码假设与因果抽象的关系。

Abstract: The concept of causal abstraction got recently popularised to demystify the
opaque decision-making processes of machine learning models; in short, a neural
network can be abstracted as a higher-level algorithm if there exists a
function which allows us to map between them. Notably, most interpretability
papers implement these maps as linear functions, motivated by the linear
representation hypothesis: the idea that features are encoded linearly in a
model's representations. However, this linearity constraint is not required by
the definition of causal abstraction. In this work, we critically examine the
concept of causal abstraction by considering arbitrarily powerful alignment
maps. In particular, we prove that under reasonable assumptions, any neural
network can be mapped to any algorithm, rendering this unrestricted notion of
causal abstraction trivial and uninformative. We complement these theoretical
findings with empirical evidence, demonstrating that it is possible to
perfectly map models to algorithms even when these models are incapable of
solving the actual task; e.g., on an experiment using randomly initialised
language models, our alignment maps reach 100% interchange-intervention
accuracy on the indirect object identification task. This raises the non-linear
representation dilemma: if we lift the linearity constraint imposed to
alignment maps in causal abstraction analyses, we are left with no principled
way to balance the inherent trade-off between these maps' complexity and
accuracy. Together, these results suggest an answer to our title's question:
causal abstraction is not enough for mechanistic interpretability, as it
becomes vacuous without assumptions about how models encode information.
Studying the connection between this information-encoding assumption and causal
abstraction should lead to exciting future work.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [99] [Unraveling the Potential of Diffusion Models in Small Molecule Generation](https://arxiv.org/abs/2507.08005)
*Peining Zhang,Daniel Baker,Minghu Song,Jinbo Bi*

Main category: q-bio.BM

TL;DR: 综述了扩散模型在分子生成中的最新进展与应用，包括理论、分类、性能评估及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在药物研发中的潜力，促进化学空间的广泛探索。

Method: 介绍扩散模型理论，分类基于数学和化学应用的方法，评估模型性能。

Result: 比较了现有3D方法的生成性能，总结了当前挑战。

Conclusion: 强调扩散模型在药物发现中的潜力，并提出未来研究方向。

Abstract: Generative AI presents chemists with novel ideas for drug design and
facilitates the exploration of vast chemical spaces. Diffusion models (DMs), an
emerging tool, have recently attracted great attention in drug R\&D. This paper
comprehensively reviews the latest advancements and applications of DMs in
molecular generation. It begins by introducing the theoretical principles of
DMs. Subsequently, it categorizes various DM-based molecular generation methods
according to their mathematical and chemical applications. The review further
examines the performance of these models on benchmark datasets, with a
particular focus on comparing the generation performance of existing 3D
methods. Finally, it concludes by emphasizing current challenges and suggesting
future research directions to fully exploit the potential of DMs in drug
discovery.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [100] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出一种无需微调的新方法，利用大语言模型从自然语言问题生成SPARQL查询，通过策略性执行查询搜索相关IRIs和字面量，在多种知识图谱和语言模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言生成SPARQL查询的挑战，避免对语言模型进行微调，同时提升在零样本和小样本场景下的性能。

Method: 利用大语言模型策略性执行SPARQL查询，搜索知识图谱中的相关IRIs和字面量，无需微调。

Result: 在Wikidata上达到零样本下的最优性能，在Freebase上接近小样本方法，其他知识图谱上表现良好。

Conclusion: 该方法在多种知识图谱和语言模型上表现优异，展示了无需微调的高效性。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [101] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: MM-Coder是一个多语言多模态软件开发者，通过整合视觉设计输入（如UML图和流程图）与文本指令，提升代码生成的准确性和架构对齐。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）多为纯文本模型，忽略了软件开发中关键的视觉辅助工具（如图表），限制了代码生成的实用性。

Method: 开发了MM-Coder模型，结合视觉工作流（Visual Workflow）和文本指令，并构建了多模态指令调优数据集MMc-Instruct。

Result: 通过新基准MMEval评估，发现模型在视觉信息捕捉、指令遵循和高级编程知识方面仍面临挑战。

Conclusion: MM-Coder旨在通过结合文本和视觉设计，革新工业编程，提升LLMs对复杂规范的理解与实现能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [102] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: MedicalBERT是一种针对生物医学领域的预训练BERT模型，通过领域特定词汇和优化，显著提升了生物医学NLP任务的性能，优于其他BERT变体。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献的领域特定术语对通用NLP模型（如BERT、GPT）提出了挑战，需要专门优化的模型来提升理解能力。

Method: 提出MedicalBERT，基于BERT架构，使用大规模生物医学数据集预训练，并配备领域特定词汇，进一步优化和微调以适应多种任务。

Result: MedicalBERT在命名实体识别、关系抽取等任务上表现优异，平均性能比通用BERT提升5.67%，优于BioBERT等同类模型。

Conclusion: MedicalBERT展示了预训练BERT模型在生物医学NLP任务中的潜力，验证了迁移学习在领域特定信息捕捉中的有效性。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [103] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: FinGPT在金融NLP任务中表现优异，尤其在分类任务（如情感分析）上接近GPT-4，但在推理和生成任务（如问答和摘要）上表现较差。


<details>
  <summary>Details</summary>
Motivation: 评估FinGPT在金融领域的实际应用能力，为未来金融语言模型的研究提供基准。

Method: 在六个金融NLP任务上测试FinGPT，使用金融专用数据集，并与GPT-4和人类基准对比。

Result: FinGPT在分类任务中表现强，但在推理和生成任务中显著落后，尤其在数值准确性和复杂推理方面。

Conclusion: FinGPT适用于结构化金融任务，但需进一步优化架构和领域适应性，以提升综合能力。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [104] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: R3框架通过Review、Remask、Refine三步优化文本生成，无需额外训练，适用于预训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决迭代文本生成中模型难以高效识别和修正自身错误的问题。

Method: 使用Process Reward Model（PRM）评估生成块，根据分数动态调整掩码比例，针对性优化低分部分。

Result: 通过针对性优化，生成文本质量得到提升。

Conclusion: R3是一种简单有效的框架，可显著改进预训练模型的文本生成效果。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [105] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 提出了一种框架，通过集成外部工具（如API、计算器等）提升LLMs在查询中的表现，特别是在教育领域。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因缺乏上下文信息而导致的回答质量差或幻觉问题。

Method: 开发了一个框架，允许访问外部API以获取额外信息，并提供计算能力。

Result: 在MMLU数据集上测试，数学推理准确率83%，科学推理88%，显著优于其他模型。

Conclusion: 该框架为构建围绕LLMs的复杂计算生态系统铺平了道路。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [106] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: 论文研究了微调后LLMs表现出的超出上下文推理（OOCR）现象，发现其原因是LoRA微调引入了一个恒定的引导向量，从而在相关领域实现泛化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在微调后为何能表现出超出分布的泛化能力，以理解其安全可靠部署的关键问题。

Method: 通过LoRA微调分析OOCR现象，并尝试直接训练引导向量来验证其效果。

Result: 发现恒定的引导向量足以解释OOCR现象，且适用于看似需要条件行为的任务（如模型后门）。

Conclusion: 研究揭示了微调过程中学习到的内容，为LLMs为何能进行超出上下文推理提供了机制解释。

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [107] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 该研究利用NLP和HAM-CNN模型分析疼痛体验中的性别差异，发现女性帖子更情感化，且某些疾病在女性中更常见。


<details>
  <summary>Details</summary>
Motivation: 以往研究常忽略性别在疼痛体验中的作用，本研究旨在填补这一空白。

Method: 使用HAM-CNN模型对帖子进行性别分类，并通过NLP分析语言差异。

Result: F1分数为0.86，女性帖子更情感化，偏头痛和鼻窦炎在女性中更常见。

Conclusion: 性别在疼痛体验中起重要作用，未来研究应考虑性别差异。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [108] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 研究探讨了自动语音识别（ASR）转录错误对说话人归属任务的影响，发现ASR转录的错误对任务表现影响较小，甚至可能优于人工转录。


<details>
  <summary>Details</summary>
Motivation: 说话人归属任务在音频不可用或不可靠时尤为重要，但以往研究多基于人工转录，而现实中更多使用ASR转录。

Method: 首次全面研究ASR转录对说话人归属性能的影响，分析转录错误和ASR系统特性对任务的影响。

Result: 说话人归属对单词级转录错误表现出惊人的鲁棒性，且ASR转录的性能可能优于人工转录。

Conclusion: ASR转录在说话人归属任务中表现优异，其转录错误可能捕捉到说话人特有的身份特征。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [109] [Parametrized Quantum Circuit Learning for Quantum Chemical Applications](https://arxiv.org/abs/2507.08183)
*Grier M. Jones,Viki Kumar Prasad,Ulrich Fekl,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 研究了参数化量子电路（PQCs）在量子化学数据集上的表现，分析了电路结构和训练集大小对性能的影响，并评估了在真实量子硬件上的表现。


<details>
  <summary>Details</summary>
Motivation: 探索PQCs在量子化学相关数据集上的潜力与局限性，填补现有研究的空白。

Method: 构建了168种PQCs，结合14种数据编码策略和12种变分ansätze，并在5和16量子比特电路上评估性能。

Result: 发现PQCs在化学相关问题上表现有限，尤其是在与经典机器学习方法相比时。

Conclusion: PQCs在量子化学问题中仍面临挑战，需进一步优化以提升性能。

Abstract: In the field of quantum machine learning (QML), parametrized quantum circuits
(PQCs) -- constructed using a combination of fixed and tunable quantum gates --
provide a promising hybrid framework for tackling complex machine learning
problems. Despite numerous proposed applications, there remains limited
exploration of datasets relevant to quantum chemistry. In this study, we
investigate the potential benefits and limitations of PQCs on two chemically
meaningful datasets: (1) the BSE49 dataset, containing bond separation energies
for 49 different classes of chemical bonds, and (2) a dataset of water
conformations, where coupled-cluster singles and doubles (CCSD) wavefunctions
are predicted from lower-level electronic structure methods using the
data-driven coupled-cluster (DDCC) approach. We construct a comprehensive set
of 168 PQCs by combining 14 data encoding strategies with 12 variational
ans{\"a}tze, and evaluate their performance on circuits with 5 and 16 qubits.
Our initial analysis examines the impact of circuit structure on model
performance using state-vector simulations. We then explore how circuit depth
and training set size influence model performance. Finally, we assess the
performance of the best-performing PQCs on current quantum hardware, using both
noisy simulations ("fake" backends) and real quantum devices. Our findings
underscore the challenges of applying PQCs to chemically relevant problems that
are straightforward for classical machine learning methods but remain
non-trivial for quantum approaches.

</details>


### [110] [Quantum Algorithms for Projection-Free Sparse Convex Optimization](https://arxiv.org/abs/2507.08543)
*Jianhao He,John C. S. Lui*

Main category: quant-ph

TL;DR: 论文提出了两种量子算法，用于解决向量和矩阵域的无投影稀疏凸优化问题，显著降低了查询和时间复杂度，优于经典算法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习和数据科学中重要的稀疏凸优化问题，利用量子计算优势提升效率。

Method: 针对向量域和矩阵域分别设计量子算法，利用函数值预言机降低复杂度。

Result: 向量域算法复杂度为O(√d/ε)和O(1/ε)，矩阵域算法复杂度为O~(rd/ε²)和O~(√rd/ε³)，均优于经典算法。

Conclusion: 量子算法在稀疏凸优化问题中展现出维度依赖的优势，显著提升了计算效率。

Abstract: This paper considers the projection-free sparse convex optimization problem
for the vector domain and the matrix domain, which covers a large number of
important applications in machine learning and data science. For the vector
domain $\mathcal{D} \subset \mathbb{R}^d$, we propose two quantum algorithms
for sparse constraints that finds a $\varepsilon$-optimal solution with the
query complexity of $O(\sqrt{d}/\varepsilon)$ and $O(1/\varepsilon)$ by using
the function value oracle, reducing a factor of $O(\sqrt{d})$ and $O(d)$ over
the best classical algorithm, respectively, where $d$ is the dimension. For the
matrix domain $\mathcal{D} \subset \mathbb{R}^{d\times d}$, we propose two
quantum algorithms for nuclear norm constraints that improve the time
complexity to $\tilde{O}(rd/\varepsilon^2)$ and
$\tilde{O}(\sqrt{r}d/\varepsilon^3)$ for computing the update step, reducing at
least a factor of $O(\sqrt{d})$ over the best classical algorithm, where $r$ is
the rank of the gradient matrix. Our algorithms show quantum advantages in
projection-free sparse convex optimization problems as they outperform the
optimal classical methods in dependence on the dimension $d$.

</details>


### [111] [Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security](https://arxiv.org/abs/2507.08623)
*Pascal Debus,Maximilian Wendlinger,Kilian Tscharke,Daniel Herr,Cedric Brügmann,Daniel Ohl de Mello,Juris Ulmanis,Alexander Erhard,Arthur Schmidt,Fabian Petsch*

Main category: quant-ph

TL;DR: 该论文提出了一种结构化建模量子机器学习（QML）攻击面的方法，借鉴经典网络安全中的杀伤链模型，以更全面地分析QML中的多阶段攻击路径和威胁。


<details>
  <summary>Details</summary>
Motivation: QML系统既继承了经典机器学习的漏洞，又引入了新的量子计算攻击面。目前研究多孤立分析单一攻击向量，缺乏整体防御策略。

Method: 提出将经典网络安全中的杀伤链模型应用于QML，构建量子感知的杀伤链框架，并详细分类QML攻击向量及其相互关系。

Result: 提出了一个详细的QML攻击向量分类，映射到量子感知杀伤链的各阶段，揭示了物理层威胁、数据算法操纵和隐私攻击之间的相互依赖关系。

Conclusion: 该研究为QML领域的威胁建模和深度安全设计提供了基础，支持更现实的攻击分析和防御策略。

Abstract: Quantum Machine Learning (QML) systems inherit vulnerabilities from classical
machine learning while introducing new attack surfaces rooted in the physical
and algorithmic layers of quantum computing. Despite a growing body of research
on individual attack vectors - ranging from adversarial poisoning and evasion
to circuit-level backdoors, side-channel leakage, and model extraction - these
threats are often analyzed in isolation, with unrealistic assumptions about
attacker capabilities and system environments. This fragmentation hampers the
development of effective, holistic defense strategies. In this work, we argue
that QML security requires more structured modeling of the attack surface,
capturing not only individual techniques but also their relationships,
prerequisites, and potential impact across the QML pipeline. We propose
adapting kill chain models, widely used in classical IT and cybersecurity, to
the quantum machine learning context. Such models allow for structured
reasoning about attacker objectives, capabilities, and possible multi-stage
attack paths - spanning reconnaissance, initial access, manipulation,
persistence, and exfiltration. Based on extensive literature analysis, we
present a detailed taxonomy of QML attack vectors mapped to corresponding
stages in a quantum-aware kill chain framework that is inspired by the MITRE
ATLAS for classical machine learning. We highlight interdependencies between
physical-level threats (like side-channel leakage and crosstalk faults), data
and algorithm manipulation (such as poisoning or circuit backdoors), and
privacy attacks (including model extraction and training data inference). This
work provides a foundation for more realistic threat modeling and proactive
security-in-depth design in the emerging field of quantum machine learning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [112] [Mallows Model with Learned Distance Metrics: Sampling and Maximum Likelihood Estimation](https://arxiv.org/abs/2507.08108)
*Yeganeh Alimohammadi,Kiana Asgari*

Main category: stat.ML

TL;DR: 论文提出了一种广义的Mallows模型，通过学习数据中的距离度量来适应不同上下文中的排名变化，并开发了高效的采样和最大似然估计算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用固定距离度量（如Kendall's τ距离），缺乏从数据中学习距离度量的方法。不同上下文中的排名变化需要更灵活的距离度量。

Method: 提出广义Mallows模型，使用Lα距离度量，开发FPTAS采样算法和MLE算法，联合估计中心排名、分散参数和最优距离度量。

Result: 证明了估计器的强一致性，并通过体育排名数据集验证了方法的有效性。

Conclusion: 广义Mallows模型能够灵活适应不同上下文中的排名变化，并提供高效的算法支持。

Abstract: \textit{Mallows model} is a widely-used probabilistic framework for learning
from ranking data, with applications ranging from recommendation systems and
voting to aligning language models with human
preferences~\cite{chen2024mallows, kleinberg2021algorithmic,
rafailov2024direct}. Under this model, observed rankings are noisy
perturbations of a central ranking $\sigma$, with likelihood decaying
exponentially in distance from $\sigma$, i.e, $P (\pi) \propto \exp\big(-\beta
\cdot d(\pi, \sigma)\big),$ where $\beta > 0$ controls dispersion and $d$ is a
distance function.
  Existing methods mainly focus on fixed distances (such as Kendall's $\tau$
distance), with no principled approach to learning the distance metric directly
from data. In practice, however, rankings naturally vary by context; for
instance, in some sports we regularly see long-range swaps (a low-rank team
beating a high-rank one), while in others such events are rare. Motivated by
this, we propose a generalization of Mallows model that learns the distance
metric directly from data. Specifically, we focus on $L_\alpha$ distances:
$d_\alpha(\pi,\sigma):=\sum_{i=1} |\pi(i)-\sigma(i)|^\alpha$.
  For any $\alpha\geq 1$ and $\beta>0$, we develop a Fully Polynomial-Time
Approximation Scheme (FPTAS) to efficiently generate samples that are
$\epsilon$- close (in total variation distance) to the true distribution. Even
in the special cases of $L_1$ and $L_2$, this generalizes prior results that
required vanishing dispersion ($\beta\to0$). Using this sampling algorithm, we
propose an efficient Maximum Likelihood Estimation (MLE) algorithm that jointly
estimates the central ranking, the dispersion parameter, and the optimal
distance metric. We prove strong consistency results for our estimators (for
any values of $\alpha$ and $\beta$), and we validate our approach empirically
using datasets from sports rankings.

</details>


### [113] [CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk](https://arxiv.org/abs/2507.08150)
*Ilia Azizi,Juraj Bodik,Jakob Heiss,Bin Yu*

Main category: stat.ML

TL;DR: CLEAR是一种校准方法，通过结合两种不确定性（随机和认知）来改进回归任务中的预测建模可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理随机或认知不确定性，缺乏平衡两者的方法。

Method: CLEAR使用两个参数（γ₁和γ₂）结合随机和认知不确定性，兼容多种估计器。

Result: 在17个数据集上，CLEAR平均改善了28.2%和17.4%的区间宽度，同时保持覆盖率。

Conclusion: CLEAR在不确定性量化中表现出色，尤其在随机或认知不确定性主导的场景中。

Abstract: Accurate uncertainty quantification is critical for reliable predictive
modeling, especially in regression tasks. Existing methods typically address
either aleatoric uncertainty from measurement noise or epistemic uncertainty
from limited data, but not necessarily both in a balanced way. We propose
CLEAR, a calibration method with two distinct parameters, $\gamma_1$ and
$\gamma_2$, to combine the two uncertainty components for improved conditional
coverage. CLEAR is compatible with any pair of aleatoric and epistemic
estimators; we show how it can be used with (i) quantile regression for
aleatoric uncertainty and (ii) ensembles drawn from the
Predictability-Computability-Stability (PCS) framework for epistemic
uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average
improvement of 28.2% and 17.4% in the interval width compared to the two
individually calibrated baselines while maintaining nominal coverage. This
improvement can be particularly evident in scenarios dominated by either high
epistemic or high aleatoric uncertainty.

</details>


### [114] [Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks](https://arxiv.org/abs/2507.08261)
*Sofia Ivolgina,P. Thomas Fletcher,Baba C. Vemuri*

Main category: stat.ML

TL;DR: 论文提出在批归一化（BN）中使用Stein收缩估计器替代传统样本均值和方差估计器，以提升对抗攻击下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 批归一化在深度学习中广泛使用，但其样本均值和方差估计在对抗攻击下表现不佳。Stein收缩估计器能提供更优的估计，尤其在对抗攻击场景下。

Method: 采用Stein收缩估计器替代传统BN中的样本均值和方差估计，并在图像分类和分割任务中验证其效果。

Result: 在CIFAR-10、PPMI（神经影像）和Cityscape数据集上，使用Stein修正的BN在对抗攻击下取得了SOTA性能。

Conclusion: Stein收缩估计器在BN中显著提升对抗攻击下的模型鲁棒性，适用于多种任务和数据集。

Abstract: Batch normalization (BN) is a ubiquitous operation in deep neural networks
used primarily to achieve stability and regularization during network training.
BN involves feature map centering and scaling using sample means and variances,
respectively. Since these statistics are being estimated across the feature
maps within a batch, this problem is ideally suited for the application of
Stein's shrinkage estimation, which leads to a better, in the
mean-squared-error sense, estimate of the mean and variance of the batch. In
this paper, we prove that the Stein shrinkage estimator for the mean and
variance dominates over the sample mean and variance estimators in the presence
of adversarial attacks when modeling these attacks using sub-Gaussian
distributions. This facilitates and justifies the application of Stein
shrinkage to estimate the mean and variance parameters in BN and use it in
image classification (segmentation) tasks with and without adversarial attacks.
We present SOTA performance results using this Stein corrected batch norm in a
standard ResNet architecture applied to the task of image classification using
CIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using
HRNet on Cityscape data with and without adversarial attacks.

</details>


### [115] [MIRRAMS: Towards Training Models Robust to Missingness Distribution Shifts](https://arxiv.org/abs/2507.08280)
*Jihye Lee,Minseo Kang,Dongha Kim*

Main category: stat.ML

TL;DR: 提出了一种名为MIRRAMS的深度学习框架，通过互信息条件增强模型对缺失数据分布变化的鲁棒性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析中，训练和测试数据的缺失分布差异常见，影响预测鲁棒性，需解决这一问题。

Method: 引入互信息条件（MI robustness conditions），设计损失函数构建MIRRAMS框架，提升模型对缺失模式的鲁棒性。

Result: MIRRAMS在多个基准数据集上优于现有方法，且在无缺失数据时也表现优异，适用于半监督学习。

Conclusion: MIRRAMS是一个通用且强大的学习框架，能有效应对缺失数据分布变化及半监督任务。

Abstract: In real-world data analysis, missingness distributional shifts between
training and test input datasets frequently occur, posing a significant
challenge to achieving robust prediction performance. In this study, we propose
a novel deep learning framework designed to address such shifts in missingness
distributions. We begin by introducing a set of mutual information-based
conditions, called MI robustness conditions, which guide a prediction model to
extract label-relevant information while remaining invariant to diverse
missingness patterns, thereby enhancing robustness to unseen missingness
scenarios at test-time. To make these conditions practical, we propose simple
yet effective techniques to derive loss terms corresponding to each and
formulate a final objective function, termed MIRRAMS(Mutual Information
Regularization for Robustness Against Missingness Shifts). As a by-product, our
analysis provides a theoretical interpretation of the principles underlying
consistency regularization-based semi-supervised learning methods, such as
FixMatch. Extensive experiments across various benchmark datasets show that
MIRRAMS consistently outperforms existing baselines and maintains stable
performance across diverse missingness scenarios. Moreover, our approach
achieves state-of-the-art performance even without missing data and can be
naturally extended to address semi-supervised learning tasks, highlighting
MIRRAMS as a powerful, off-the-shelf framework for general-purpose learning.

</details>


### [116] [Optimal and Practical Batched Linear Bandit Algorithm](https://arxiv.org/abs/2507.08438)
*Sanghoon Yu,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出了一种名为BLAE的新算法，首次在所有情况下实现极小极大最优遗憾，同时仅需O(log log T)批次，计算开销低且实证表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在理论上能实现接近最优的遗憾，但计算成本高或实际表现不佳，因此需要一种更高效的算法。

Method: 结合臂消除与正则化G最优设计，提出BLAE算法，利用批次优化设计和改进的集中边界。

Result: 在所有情况下首次实现极小极大最优遗憾，计算开销低且实证表现优于现有方法。

Conclusion: BLAE是首个在所有情况下兼具理论最优性和实际优越性的批处理线性赌博算法。

Abstract: We study the linear bandit problem under limited adaptivity, known as the
batched linear bandit. While existing approaches can achieve near-optimal
regret in theory, they are often computationally prohibitive or underperform in
practice. We propose \texttt{BLAE}, a novel batched algorithm that integrates
arm elimination with regularized G-optimal design, achieving the minimax
optimal regret (up to logarithmic factors in $T$) in both large-$K$ and
small-$K$ regimes for the first time, while using only $O(\log\log T)$ batches.
Our analysis introduces new techniques for batch-wise optimal design and
refined concentration bounds. Crucially, \texttt{BLAE} demonstrates low
computational overhead and strong empirical performance, outperforming
state-of-the-art methods in extensive numerical evaluations. Thus,
\texttt{BLAE} is the first algorithm to combine provable minimax-optimality in
all regimes and practical superiority in batched linear bandits.

</details>


### [117] [Data Depth as a Risk](https://arxiv.org/abs/2507.08518)
*Arturo Castellanos,Pavlo Mozharovskyi*

Main category: stat.ML

TL;DR: 论文提出了一种基于分类器损失的新数据深度框架“损失深度”，扩展了半空间深度的概念，适用于高维数据，并展示了其在异常检测中的高效性。


<details>
  <summary>Details</summary>
Motivation: 传统数据深度方法（如半空间深度）在泛化性和计算效率上存在局限，需要一种更灵活且高效的新方法。

Method: 通过将半空间深度重新定义为分类器的最小损失，引入“损失深度”框架，支持多种分类器（如SVM、逻辑回归）。

Result: 新框架继承了机器学习算法的计算效率和统计收敛速度，适用于高维数据，并在异常检测中表现优异。

Conclusion: “损失深度”不仅易于解释，还能有效平衡分类器的复杂度与数据集的匹配，为数据深度领域提供了新视角。

Abstract: Data depths are score functions that quantify in an unsupervised fashion how
central is a point inside a distribution, with numerous applications such as
anomaly detection, multivariate or functional data analysis, arising across
various fields. The halfspace depth was the first depth to aim at generalising
the notion of quantile beyond the univariate case. Among the existing variety
of depth definitions, it remains one of the most used notions of data depth.
Taking a different angle from the quantile point of view, we show that the
halfspace depth can also be regarded as the minimum loss of a set of
classifiers for a specific labelling of the points. By changing the loss or the
set of classifiers considered, this new angle naturally leads to a family of
"loss depths", extending to well-studied classifiers such as, e.g., SVM or
logistic regression, among others. This framework directly inherits
computational efficiency of existing machine learning algorithms as well as
their fast statistical convergence rates, and opens the data depth realm to the
high-dimensional setting. Furthermore, the new loss depths highlight a
connection between the dataset and the right amount of complexity or simplicity
of the classifiers. The simplicity of classifiers as well as the interpretation
as a risk makes our new kind of data depth easy to explain, yet efficient for
anomaly detection, as is shown by experiments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [118] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: M2-Reasoning-7B模型通过创新的数据管道和动态多任务训练策略，解决了MLLMs在动态空间交互中的不足，并在8个基准测试中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在动态空间交互能力上存在不足，限制了其实际应用。

Method: 1. 构建高质量数据管道（294.2K样本）；2. 动态多任务训练策略与任务特定奖励。

Result: M2-Reasoning-7B在8个基准测试中表现最优。

Conclusion: M2-Reasoning-7B通过数据与训练策略的创新，显著提升了空间和通用推理能力。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [119] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 论文定义了与对比解释相关的几个典型问题，探讨了它们在命题逻辑中的基本性质，并分析了计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究对比解释问题，回答“为什么是P而不是Q”这类问题，旨在明确比较P和Q的差异。

Method: 在命题逻辑中定义问题，分析其性质，实现基于CNF公式的答案集编程，并通过示例展示实际应用。

Result: 框架捕捉了文献中对比解释的最小基数版本，并提供了计算复杂性的详细分析。

Conclusion: 论文提出的框架有效解决了对比解释问题，并通过实现和示例验证了其实际可行性。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [120] [Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation](https://arxiv.org/abs/2507.05314)
*Daniel Cieślak,Miriam Reca,Olena Onyshchenko,Jacek Rumiński*

Main category: eess.IV

TL;DR: 提出了一种双注意力U-Net++架构，用于临床图像中伤口和比例标记的精确分割，通过集成通道和空间注意力机制解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 临床图像中伤口和比例标记的精确分割对伤口管理和自动化评估至关重要，但现有方法面临类别不平衡和图像变异性挑战。

Method: 采用双注意力U-Net++架构，结合SCSE和空间注意力机制，使用EfficientNet-B7作为编码器，通过数据增强和贝叶斯超参数调优训练两个类特定模型，并集成测试时间增强。

Result: 在NBC 2025 & PCBBE 2025竞赛数据集上，加权F1得分为0.8640，表现优异。

Conclusion: 该方法在复杂医学分割任务中表现出高效性和可靠性。

Abstract: Accurate segmentation of wounds and scale markers in clinical images remainsa
significant challenge, crucial for effective wound management and
automatedassessment. In this study, we propose a novel dual-attention U-Net++
archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms
toaddress severe class imbalance and variability in medical images
effectively.Initially, extensive benchmarking across diverse architectures and
encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal
encoder backbone.Subsequently, we independently trained two class-specific
models with tailoredpreprocessing, extensive data augmentation, and Bayesian
hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test
Time Augmentationto further enhance prediction reliability. Our approach was
evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition.
Segmentationperformance was quantified using a weighted F1-score (75% wounds,
25% scalemarkers), calculated externally by competition organizers on
undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640,
underscoring itseffectiveness for complex medical segmentation tasks.

</details>


### [121] [Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models](https://arxiv.org/abs/2507.08254)
*Ulzee An,Moonseong Jeong,Simon A. Lee,Aditya Gorla,Yuzhe Yang,Sriram Sankararaman*

Main category: eess.IV

TL;DR: Raptor是一种无需训练的方法，通过随机平面张量压缩生成体积数据的语义丰富嵌入，显著降低计算复杂度，并在多种医学体积任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决体积成像数据（如MRI）在训练高维模型和获取大规模数据集时的计算复杂性问题。

Method: 利用预训练的2D基础模型提取医学体积的视觉标记，并通过随机投影进行空间压缩。

Result: 在十种医学体积任务中，Raptor性能优于现有方法（提升3%-14%），且无需训练。

Conclusion: Raptor是一种高效且通用的方法，为医学体积数据的深度学习提供了新基础。

Abstract: Current challenges in developing foundational models for volumetric imaging
data, such as magnetic resonance imaging (MRI), stem from the computational
complexity of training state-of-the-art architectures in high dimensions and
curating sufficiently large datasets of volumes. To address these challenges,
we introduce Raptor (Random Planar Tensor Reduction), a train-free method for
generating semantically rich embeddings for volumetric data. Raptor leverages a
frozen 2D foundation model, pretrained on natural images, to extract visual
tokens from individual cross-sections of medical volumes. These tokens are then
spatially compressed using random projections, significantly reducing
computational complexity while retaining semantic information. Extensive
experiments on ten diverse medical volume tasks verify the superior performance
of Raptor over state-of-the-art methods, including those pretrained exclusively
on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14%
SLIViT), while entirely bypassing the need for costly training. Our results
highlight the effectiveness and versatility of Raptor as a foundation for
advancing deep learning-based methods for medical volumes.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [122] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: 论文提出了一种新的函数类别——filter equivariant functions，用于研究函数在列表元素被移除时的行为一致性，并探讨了其与map equivariant functions的关系。


<details>
  <summary>Details</summary>
Motivation: 研究函数在已知输入/输出示例之外的扩展行为，提出一种基于规则的一致性标准。

Method: 引入filter equivariant functions，证明其基本定理，并与map equivariant functions关联，同时提供几何解释。

Result: 提出了amalgamation算法，能够完美扩展任何filter-equivariant函数的输出。

Conclusion: filter equivariant functions为函数扩展行为提供了一种新的理论框架，并展示了其实际应用潜力。

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [123] [Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees](https://arxiv.org/abs/2507.08653)
*Berire Gunes Reyhan,Sinem Coleri*

Main category: eess.SP

TL;DR: 提出了一种基于优化理论的安全深度强化学习框架，用于超可靠无线网络控制系统，首次在文献中同时满足约束条件和性能优化。


<details>
  <summary>Details</summary>
Motivation: 无线网络控制系统中控制和通信系统相互依赖，需协同设计。现有方法难以同时满足约束条件和性能优化。

Method: 采用两阶段方法：优化理论阶段推导最优条件，简化问题；安全深度强化学习阶段通过师生框架指导代理满足约束。

Result: 仿真表明，该框架优于基于规则和其他优化理论的深度强化学习方法，收敛更快、奖励更高、稳定性更强。

Conclusion: 该框架为超可靠无线网络控制系统提供了一种有效的协同设计和优化方法。

Abstract: In Wireless Networked Control Systems (WNCSs), control and communication
systems must be co-designed due to their strong interdependence. This paper
presents a novel optimization theory-based safe deep reinforcement learning
(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction
while optimizing performance, for the first time in the literature. The
approach minimizes power consumption under key constraints, including Peak Age
of Information (PAoI) violation probability, transmit power, and schedulability
in the finite blocklength regime. PAoI violation probability is uniquely
derived by combining stochastic maximum allowable transfer interval (MATI) and
maximum allowable packet delay (MAD) constraints in a multi-sensor network. The
framework consists of two stages: optimization theory and safe DRL. The first
stage derives optimality conditions to establish mathematical relationships
among variables, simplifying and decomposing the problem. The second stage
employs a safe DRL model where a teacher-student framework guides the DRL agent
(student). The control mechanism (teacher) evaluates compliance with system
constraints and suggests the nearest feasible action when needed. Extensive
simulations show that the proposed framework outperforms rule-based and other
optimization theory based DRL benchmarks, achieving faster convergence, higher
rewards, and greater stability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [124] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种评估文本到图像模型的新框架，重点关注其对提示的遵循能力，并通过实验发现现有模型在生成简单二进制图像时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM和文本到图像模型的研究相对不足，其可靠性和性能评估缺乏系统性，因此需要建立全面的评估框架。

Method: 创建新数据集评估模型对输入文本提示的鲁棒性，利用GPT-4生成真实图像描述，再通过文本到图像模型生成图像，最后比较描述差异。

Result: 实验显示，模型在生成仅含两个变化因素的简单二进制图像时表现不佳，且无法遵循输入数据集的分布。

Conclusion: 现有文本到图像模型在遵循提示和生成特定分布图像方面存在局限性，需进一步改进。

Abstract: The advancements in the domain of LLMs in recent years have surprised many,
showcasing their remarkable capabilities and diverse applications. Their
potential applications in various real-world scenarios have led to significant
research on their reliability and effectiveness. On the other hand, multimodal
LLMs and Text-to-Image models have only recently gained prominence, especially
when compared to text-only LLMs. Their reliability remains constrained due to
insufficient research on assessing their performance and robustness. This paper
aims to establish a comprehensive evaluation framework for Text-to-Image
models, concentrating particularly on their adherence to prompts. We created a
novel dataset that aimed to assess the robustness of these models in generating
images that conform to the specified factors of variation in the input text
prompts. Our evaluation studies present findings on three variants of Stable
Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and
Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro
1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions
generated by the gpt-4o model for our ground-truth images, which are then used
to generate artificial images by passing these descriptions to the
Text-to-Image models. We then pass these generated images again through gpt-4o
using the same system prompt and compare the variation between the two
descriptions. Our results reveal that these models struggle to create simple
binary images with only two factors of variation: a simple geometric shape and
its location. We also show, using pre-trained VAEs on our dataset, that they
fail to generate images that follow our input dataset distribution.

</details>


### [125] [A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters](https://arxiv.org/abs/2507.08047)
*Rolando A. Hernandez-Hernandez,Adrian Rubio-Solis*

Main category: cs.CV

TL;DR: 提出了一种基于ELM-AE和区间二型模糊逻辑的混合多层极限学习机（HML-ELM），用于无人机图像分类，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多层极限学习机（ML-ELM）在自然信号分类中表现良好，但需要进一步提升效率和准确性。

Method: HML-ELM采用两阶段框架：1）自学习特征提取（ELM-AE堆叠）；2）监督分类（SIT2-FELM）。

Result: 实验表明HML-ELM在图像分类和无人机任务中优于ML-ELM、ML-FELM和ELM。

Conclusion: HML-ELM是一种高效且准确的图像分类方法，适用于无人机应用。

Abstract: Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to
be an effective technique for the classification of different natural signals
such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer
Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder
(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active
image classification and applied to Unmanned Aerial Vehicles (UAVs). The
proposed methodology is a hierarchical ELM learning framework that consists of
two main phases: 1) self-taught feature extraction and 2) supervised feature
classification. First, unsupervised multilayer feature encoding is achieved by
stacking a number of ELM-AEs, in which input data is projected into a number of
high-level representations. At the second phase, the final features are
classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with
a fast output reduction layer based on the SC algorithm; an improved version of
the algorithm Center of Sets Type Reducer without Sorting Requirement
(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments
for the classification of images are suggested. First, the HML-ELM is applied
to solve a number of benchmark problems for image classification. Secondly, a
number of real experiments to the active classification and transport of four
different objects between two predefined locations using a UAV is implemented.
Experiments demonstrate that the proposed HML-ELM delivers a superior
efficiency compared to other similar methodologies such as ML-ELM, Multilayer
Fuzzy Extreme Learning Machine (ML-FELM) and ELM.

</details>


### [126] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 研究评估了多种机器学习方法在云和云影掩模任务中的表现，发现CNN结合特征降维在精度、存储和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 云和云影掩模是卫星高光谱成像的关键预处理步骤，需要高效且准确的模型。

Method: 比较了梯度提升方法（XGBoost、LightGBM）和卷积神经网络（CNN），并测试了不同参数规模的CNN变体。

Result: 所有模型精度超过93%，CNN特征降维版在精度、存储和计算效率上表现最优。

Conclusion: 轻量级AI模型适合实时高光谱图像处理，支持星载AI系统开发。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [127] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

TL;DR: 提出了一种自适应扩散去噪平滑方法，用于验证视觉模型对抗对抗样本的预测能力，并能适应输入。通过将引导去噪扩散模型重新解释为一系列自适应高斯差分隐私机制，实现了端到端的鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 提高视觉模型在对抗样本攻击下的鲁棒性，同时保持或提升标准准确性。

Method: 将引导去噪扩散模型视为自适应高斯差分隐私机制的序列，通过GDP隐私过滤器分析其鲁棒性。

Result: 在ImageNet数据集上，针对ℓ2威胁模型，展示了认证准确性和标准准确性的提升。

Conclusion: 该方法通过自适应机制和GDP隐私过滤器，有效提升了模型的鲁棒性和准确性。

Abstract: We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the
predictions of a vision model against adversarial examples, while adapting to
the input. Our key insight is to reinterpret a guided denoising diffusion model
as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms
refining a pure noise sample into an image. We show that these adaptive
mechanisms can be composed through a GDP privacy filter to analyze the
end-to-end robustness of the guided denoising process, yielding a provable
certification that extends the adaptive randomized smoothing analysis. We
demonstrate that our design, under a specific guiding strategy, can improve
both certified accuracy and standard accuracy on ImageNet for an $\ell_2$
threat model.

</details>


### [128] [Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification](https://arxiv.org/abs/2507.08248)
*Jason Kahei Tam,Murilo Gustineli,Anthony Miyaguchi*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉Transformer的少样本细粒度分类方法，用于真菌物种识别，结合数据增强和文本信息，效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 真菌物种识别在计算机视觉中具有挑战性，因物种间差异细微且物种内差异大。

Method: 采用视觉Transformer模型，结合数据增强、加权采样和文本信息，并尝试生成式AI模型进行零样本分类。

Result: 最终模型表现优于基线，排名35/74，显示领域特定预训练和平衡采样策略的有效性。

Conclusion: 需进一步优化元数据选择和领域适应的多模态学习。

Abstract: Accurate identification of fungi species presents a unique challenge in
computer vision due to fine-grained inter-species variation and high
intra-species variation. This paper presents our approach for the FungiCLEF
2025 competition, which focuses on few-shot fine-grained visual categorization
(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented
with multiple vision transformer models, data augmentation, weighted sampling,
and incorporating textual information. We also explored generative AI models
for zero-shot classification using structured prompting but found them to
significantly underperform relative to vision-based models. Our final model
outperformed both competition baselines and highlighted the effectiveness of
domain specific pretraining and balanced sampling strategies. Our approach
ranked 35/74 on the private test set in post-completion evaluation, this
suggests additional work can be done on metadata selection and domain-adapted
multi-modal learning. Our code is available at
https://github.com/dsgt-arc/fungiclef-2025.

</details>


### [129] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

TL;DR: 提出了一种基于可解释性指导的剪枝框架，减少模型复杂度同时保持预测性能和透明度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中取得显著进展，但模型的大规模和缺乏透明度限制了其临床应用。

Method: 通过选择性保留每层最相关的部分，实现有针对性的压缩，保持临床意义的表示。

Result: 在多个医学图像分类基准测试中，该方法实现了高压缩率且精度损失最小。

Conclusion: 为医疗环境中部署轻量级、可解释的模型铺平了道路。

Abstract: Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [130] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

TL;DR: SAM 2在目标分割任务中表现优异，成为视觉目标跟踪的SOTA。本文提出用强化学习优化其内存更新，替代手工规则，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工规则更新内存，难以应对干扰、遮挡和运动。本文探索强化学习作为替代方案，以优化内存控制。

Method: 将内存控制建模为序列决策问题，使用强化学习优化SAM 2的内存更新，每个视频训练独立智能体。

Result: 在过拟合实验中，性能提升超过现有启发式方法的三倍，显示内存银行的潜力。

Conclusion: 强化学习是优化内存控制的有效替代方案，性能显著优于手工规则。

Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks and has become the state-of-the-art for visual object
tracking. The model stores information from previous frames in a memory bank,
enabling temporal consistency across video sequences. Recent methods augment
SAM 2 with hand-crafted update rules to better handle distractors, occlusions,
and object motion. We propose a fundamentally different approach using
reinforcement learning for optimizing memory updates in SAM 2 by framing memory
control as a sequential decision-making problem. In an overfitting setup with a
separate agent per video, our method achieves a relative improvement over SAM 2
that exceeds by more than three times the gains of existing heuristics. These
results reveal the untapped potential of the memory bank and highlight
reinforcement learning as a powerful alternative to hand-crafted update rules
for memory control in visual object tracking.

</details>


### [131] [A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification](https://arxiv.org/abs/2507.08766)
*Ahmed Farooq*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和多井Hopfield网络的混合模型，用于MNIST手写数字分类，测试准确率达99.2%。


<details>
  <summary>Details</summary>
Motivation: 解决手写数字分类中的类内变异性问题，并提供可解释的基于能量的决策框架。

Method: 使用CNN提取特征，k-means聚类生成类特定原型，Hopfield网络通过能量最小化进行分类。

Result: 模型在10,000张MNIST图像上达到99.2%的测试准确率。

Conclusion: 深度特征提取和足够的原型覆盖对高性能至关重要，模型在模式识别中有广泛应用潜力。

Abstract: This study presents a hybrid model for classifying handwritten digits in the
MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well
Hopfield network. The approach employs a CNN to extract high-dimensional
features from input images, which are then clustered into class-specific
prototypes using k-means clustering. These prototypes serve as attractors in a
multi-well energy landscape, where a Hopfield network performs classification
by minimizing an energy function that balances feature similarity and class
assignment.The model's design enables robust handling of intraclass
variability, such as diverse handwriting styles, while providing an
interpretable framework through its energy-based decision process. Through
systematic optimization of the CNN architecture and the number of wells, the
model achieves a high test accuracy of 99.2% on 10,000 MNIST images,
demonstrating its effectiveness for image classification tasks. The findings
highlight the critical role of deep feature extraction and sufficient prototype
coverage in achieving high performance, with potential for broader applications
in pattern recognition.

</details>


### [132] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS是一个通过神经网络直接预测屏幕帧以模拟操作系统GUI的框架，结合RNN和扩散渲染器，训练于Ubuntu XFCE数据集，能生成逼真的GUI序列。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过神经网络模拟操作系统的图形用户界面，为未来人机交互系统提供自适应、生成式界面。

Method: 结合RNN追踪计算机状态和扩散渲染器生成屏幕图像，训练于包含随机和AI生成交互的Ubuntu XFCE数据集。

Result: 成功渲染逼真GUI序列，准确捕捉鼠标交互和状态转换，但键盘交互建模仍有挑战。

Conclusion: NeuralOS为未来自适应神经界面提供了重要一步，尽管键盘交互仍需改进。

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [133] [Audio Inpanting using Discrete Diffusion Model](https://arxiv.org/abs/2507.08333)
*Tali Dror,Iftach Shoham,Moshe Buchris,Oren Gal,Haim Permuter,Gilad Katz,Eliya Nachmani*

Main category: cs.SD

TL;DR: 提出了一种基于离散扩散模型的音频修复方法，适用于长缺失段（最长500毫秒），性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有音频修复方法在缺失段超过100毫秒时质量下降，需改进。

Method: 使用预训练音频标记器生成的离散潜在空间进行扩散建模，直接生成缺失音频。

Result: 在MusicNet和MTG数据集上，该方法在300毫秒和500毫秒缺失段中表现优异。

Conclusion: 该方法为长缺失音频修复提供了稳定且语义连贯的解决方案。

Abstract: Audio inpainting refers to the task of reconstructing missing segments in
corrupted audio recordings. While prior approaches-including waveform and
spectrogram-based diffusion models-have shown promising results for short gaps,
they often degrade in quality when gaps exceed 100 milliseconds (ms). In this
work, we introduce a novel inpainting method based on discrete diffusion
modeling, which operates over tokenized audio representations produced by a
pre-trained audio tokenizer. Our approach models the generative process
directly in the discrete latent space, enabling stable and semantically
coherent reconstruction of missing audio. We evaluate the method on the
MusicNet dataset using both objective and perceptual metrics across gap
durations up to 300 ms. We further evaluated our approach on the MTG dataset,
extending the gap duration to 500 ms. Experimental results demonstrate that our
method achieves competitive or superior performance compared to existing
baselines, particularly for longer gaps, offering a robust solution for
restoring degraded musical recordings. Audio examples of our proposed method
can be found at https://iftach21.github.io/

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [134] [Robust Semi-Supervised CT Radiomics for Lung Cancer Prognosis: Cost-Effective Learning with Limited Labels and SHAP Interpretation](https://arxiv.org/abs/2507.08189)
*Mohammad R. Salmanpour,Amir Hossein Pouria,Sonia Falahati,Shahram Taeb,Somayeh Sadat Mehrnia,Ali Fathi Jouzdani,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim*

Main category: physics.med-ph

TL;DR: 论文提出了一种半监督学习（SSL）框架，用于基于CT扫描的肺癌生存预测，通过利用未标记数据显著提升了预测性能，并在资源有限的环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 监督学习（SL）模型需要大量标记数据，但在实际应用中标记数据稀缺，限制了其应用。因此，研究旨在开发一种更高效、成本更低的半监督学习方法。

Method: 研究分析了977名患者的CT扫描，提取了1218个放射组学特征，并通过56种特征选择和提取算法以及27种分类器进行降维和比较。SSL框架结合伪标记技术，利用478个未标记和499个标记病例进行训练。

Result: SSL在生存预测中比SL表现更优，准确率最高提升17%，在交叉验证和外部测试中分别达到0.90和0.88的准确率。SHAP分析显示SSL具有更强的特征区分能力。

Conclusion: 提出的SSL框架在成本效益、稳定性和可解释性方面表现优异，为基于CT的肺癌生存预测提供了更实用的解决方案。

Abstract: Background: CT imaging is vital for lung cancer management, offering detailed
visualization for AI-based prognosis. However, supervised learning SL models
require large labeled datasets, limiting their real-world application in
settings with scarce annotations.
  Methods: We analyzed CT scans from 977 patients across 12 datasets extracting
1218 radiomics features using Laplacian of Gaussian and wavelet filters via
PyRadiomics Dimensionality reduction was applied with 56 feature selection and
extraction algorithms and 27 classifiers were benchmarked A semi supervised
learning SSL framework with pseudo labeling utilized 478 unlabeled and 499
labeled cases Model sensitivity was tested in three scenarios varying labeled
data in SL increasing unlabeled data in SSL and scaling both from 10 percent to
100 percent SHAP analysis was used to interpret predictions Cross validation
and external testing in two cohorts were performed.
  Results: SSL outperformed SL, improving overall survival prediction by up to
17 percent. The top SSL model, Random Forest plus XGBoost classifier, achieved
0.90 accuracy in cross-validation and 0.88 externally. SHAP analysis revealed
enhanced feature discriminability in both SSL and SL, especially for Class 1
survival greater than 4 years. SSL showed strong performance with only 10
percent labeled data, with more stable results compared to SL and lower
variance across external testing, highlighting SSL's robustness and cost
effectiveness.
  Conclusion: We introduced a cost-effective, stable, and interpretable SSL
framework for CT-based survival prediction in lung cancer, improving
performance, generalizability, and clinical readiness by integrating SHAP
explainability and leveraging unlabeled data.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [135] [Finding a solution to the Erdős-Ginzburg-Ziv theorem in $O(n\log\log\log n)$ time](https://arxiv.org/abs/2507.08139)
*Yui Hin Arvin Leung*

Main category: math.CO

TL;DR: 论文改进了Erdős-Ginzburg-Ziv定理的算法，提出了一种实用的O(n log log n)算法和理论的O(n log log log n)算法，优于之前的O(n log n)方法。


<details>
  <summary>Details</summary>
Motivation: 改进现有算法，证明布尔卷积的特定变体可以比基于FFT的方法更快实现。

Method: 提出两种新算法：一种实用的O(n log log n)算法和一种理论的O(n log log log n)算法。

Result: 新算法在时间和效率上优于之前的O(n log n)方法。

Conclusion: 证明了布尔卷积的特定变体可以实现比FFT方法更快的计算速度。

Abstract: The Erd\H{o}s-Ginzburg-Ziv theorem states that for any sequence of $2n-1$
integers, there exists a subsequence of $n$ elements whose sum is divisible by
$n$. In this article, we provide a simple, practical $O(n\log\log n)$ algorithm
and a theoretical $O(n\log\log\log n)$ algorithm, both of which improve upon
the best previously known $O(n\log n)$ approach. This shows that a specific
variant of boolean convolution can be implemented in time faster than the usual
$O(n\log n)$ expected from FFT-based methods.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [136] [Entity-Specific Cyber Risk Assessment using InsurTech Empowered Risk Factors](https://arxiv.org/abs/2507.08193)
*Jiayi Guo,Zhiyun Quan,Linfeng Zhang*

Main category: q-fin.RM

TL;DR: 论文提出了一种InsurTech框架，通过增强现有网络事件数据集的实体特定属性和应用机器学习模型，解决了高质量公共网络事件数据缺乏的问题，提升了预测和风险评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于公司不愿披露可能损害声誉或投资者信心的网络事件，高质量公共网络事件数据的缺乏限制了网络风险评估的实证研究和预测建模。

Method: 提出了一种InsurTech框架，结合实体特定属性丰富网络事件数据，并开发了多标签分类模型和多输出回归模型，用于预测网络事件类型及其年频率。

Result: InsurTech增强的特征提升了预测的稳健性，但未观察到网络事件类型间的显著相关性。框架生成了透明的、实体特定的网络风险档案。

Conclusion: 该框架为保险公司和组织提供了数据驱动的决策支持，支持定制化承保和主动网络风险缓解。

Abstract: The lack of high-quality public cyber incident data limits empirical research
and predictive modeling for cyber risk assessment. This challenge persists due
to the reluctance of companies to disclose incidents that could damage their
reputation or investor confidence. Therefore, from an actuarial perspective,
potential resolutions conclude two aspects: the enhancement of existing cyber
incident datasets and the implementation of advanced modeling techniques to
optimize the use of the available data. A review of existing data-driven
methods highlights a significant lack of entity-specific organizational
features in publicly available datasets. To address this gap, we propose a
novel InsurTech framework that enriches cyber incident data with
entity-specific attributes. We develop various machine learning (ML) models: a
multilabel classification model to predict the occurrence of cyber incident
types (e.g., Privacy Violation, Data Breach, Fraud and Extortion, IT Error, and
Others) and a multioutput regression model to estimate their annual
frequencies. While classifier and regressor chains are implemented to explore
dependencies among cyber incident types as well, no significant correlations
are observed in our datasets. Besides, we apply multiple interpretable ML
techniques to identify and cross-validate potential risk factors developed by
InsurTech across ML models. We find that InsurTech empowered features enhance
prediction occurrence and frequency estimation robustness compared to only
using conventional risk factors. The framework generates transparent,
entity-specific cyber risk profiles, supporting customized underwriting and
proactive cyber risk mitigation. It provides insurers and organizations with
data-driven insights to support decision-making and compliance planning.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [137] [Predicting Flow Dynamics using Diffusion Models](https://arxiv.org/abs/2507.08106)
*Yannick Gachnang,Vismay Churiwala*

Main category: physics.flu-dyn

TL;DR: 本文旨在复现并扩展DiffFluid论文的结果，验证其方法的可重复性，并测试其在其他模拟类型（如Lattice Boltzmann方法）中的适用性。


<details>
  <summary>Details</summary>
Motivation: 验证DiffFluid模型的可重复性，并探索其作为通用流体动力学求解器的潜力。

Method: 使用去噪扩散概率模型（DDPM）框架处理Navier-Stokes和Darcy流方程，并扩展到Lattice Boltzmann方法。

Result: 展示了模型的灵活性和潜力，但也揭示了在复杂流体动力学问题中应用扩散模型的挑战。

Conclusion: 未来研究应优化计算效率并扩展模型的应用范围。

Abstract: In this work, we aimed to replicate and extend the results presented in the
DiffFluid paper[1]. The DiffFluid model showed that diffusion models combined
with Transformers are capable of predicting fluid dynamics. It uses a denoising
diffusion probabilistic model (DDPM) framework to tackle Navier-Stokes and
Darcy flow equations. Our goal was to validate the reproducibility of the
methods in the DiffFluid paper while testing its viability for other simulation
types, particularly the Lattice Boltzmann method. Despite our computational
limitations and time constraints, this work provides evidence of the
flexibility and potential of the model as a general-purpose solver for fluid
dynamics. Our results show both the potential and challenges of applying
diffusion models to complex fluid dynamics problems. This work highlights the
opportunities for future research in optimizing the computational efficiency
and scaling such models in broader domains.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [138] [Towards Efficient Quantity Retrieval from Text:an Approach via Description Parsing and Weak Supervision](https://arxiv.org/abs/2507.08322)
*Yixuan Cao,Zhengrong Chen,Chengxuan Xia,Kun Wu,Ping Luo*

Main category: cs.IR

TL;DR: 提出了一种基于描述解析的框架，用于从非结构化文档中检索定量事实，显著提高了检索准确率。


<details>
  <summary>Details</summary>
Motivation: 许多长尾定量事实埋藏在非结构化文档中，难以访问，影响了数据驱动的决策。

Method: 通过描述解析将文本转换为结构化（描述、数量）对，并利用弱监督构建大规模复述数据集以优化学习。

Result: 在金融年报数据集上，检索准确率从30.98%提升至64.66%。

Conclusion: 该方法有效解决了非结构化文档中定量事实的检索问题，显著提升了性能。

Abstract: Quantitative facts are continually generated by companies and governments,
supporting data-driven decision-making. While common facts are structured, many
long-tail quantitative facts remain buried in unstructured documents, making
them difficult to access. We propose the task of Quantity Retrieval: given a
description of a quantitative fact, the system returns the relevant value and
supporting evidence. Understanding quantity semantics in context is essential
for this task. We introduce a framework based on description parsing that
converts text into structured (description, quantity) pairs for effective
retrieval. To improve learning, we construct a large paraphrase dataset using
weak supervision based on quantity co-occurrence. We evaluate our approach on a
large corpus of financial annual reports and a newly annotated quantity
description dataset. Our method significantly improves top-1 retrieval accuracy
from 30.98 percent to 64.66 percent.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [139] [SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent Intracortical Motor Decoding](https://arxiv.org/abs/2507.08402)
*Trung Le,Hao Fang,Jingyuan Li,Tung Nguyen,Lu Mi,Amy Orsborn,Uygar Sümbül,Eli Shlizerman*

Main category: q-bio.NC

TL;DR: SPINT是一种新型的神经解码框架，通过动态推断神经单元身份和动态通道丢弃技术，解决了iBCI中神经记录的非平稳性问题，实现了跨会话的稳健解码。


<details>
  <summary>Details</summary>
Motivation: 解决iBCI中神经记录的非平稳性问题，现有方法依赖固定神经身份或需要测试时标签，限制了跨会话的泛化能力。

Method: 提出SPINT框架，采用上下文相关的位置嵌入方案和动态通道丢弃技术，支持可变大小神经群体和少样本无监督适应。

Result: 在FALCON Benchmark的三个多会话数据集上，SPINT表现优于现有零样本和少样本无监督基线，无需测试时对齐或微调。

Conclusion: SPINT为长期iBCI应用提供了一个稳健且可扩展的神经解码框架。

Abstract: Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from
neural population activity, enabling individuals with motor impairments to
regain motor functions and communication abilities. A key challenge in
long-term iBCI is the nonstationarity of neural recordings, where the
composition and tuning profiles of the recorded populations are unstable across
recording sessions. Existing methods attempt to address this issue by explicit
alignment techniques; however, they rely on fixed neural identities and require
test-time labels or parameter updates, limiting their generalization across
sessions and imposing additional computational burden during deployment. In
this work, we introduce SPINT - a Spatial Permutation-Invariant Neural
Transformer framework for behavioral decoding that operates directly on
unordered sets of neural units. Central to our approach is a novel
context-dependent positional embedding scheme that dynamically infers
unit-specific identities, enabling flexible generalization across recording
sessions. SPINT supports inference on variable-size populations and allows
few-shot, gradient-free adaptation using a small amount of unlabeled data from
the test session. To further promote model robustness to population
variability, we introduce dynamic channel dropout, a regularization method for
iBCI that simulates shifts in population composition during training. We
evaluate SPINT on three multi-session datasets from the FALCON Benchmark,
covering continuous motor decoding tasks in human and non-human primates. SPINT
demonstrates robust cross-session generalization, outperforming existing
zero-shot and few-shot unsupervised baselines while eliminating the need for
test-time alignment and fine-tuning. Our work contributes an initial step
toward a robust and scalable neural decoding framework for long-term iBCI
applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [140] [AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08616)
*Florian Grötschla,Luis Müller,Jan Tönshoff,Mikhail Galkin,Bryan Perozzi*

Main category: cs.MA

TL;DR: AgentsNet是一个新的多智能体推理基准，旨在评估智能体系统在协作、自组织和通信方面的能力，并支持大规模网络测试。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统在复杂网络中的自组织和协作能力，现有基准无法满足大规模网络的需求。

Method: 通过借鉴分布式系统和图论中的经典问题，设计AgentsNet基准，评估智能体在给定网络拓扑下的策略形成和通信能力。

Result: 前沿LLM在小规模网络中表现良好，但随着网络规模扩大性能下降；AgentsNet支持多达100个智能体的测试。

Conclusion: AgentsNet为多智能体系统的性能评估提供了新的标准，并展示了LLM在大规模网络中的潜力与局限。

Abstract: Large-language models (LLMs) have demonstrated powerful problem-solving
capabilities, in particular when organized in multi-agent systems. However, the
advent of such systems also raises several questions on the ability of a
complex network of agents to effectively self-organize and collaborate. While
measuring performance on standard reasoning benchmarks indicates how well
multi-agent systems can solve reasoning tasks, it is unclear whether these
systems are able to leverage their topology effectively. Here, we propose
AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration
from classical problems in distributed systems and graph theory, AgentsNet
measures the ability of multi-agent systems to collaboratively form strategies
for problem-solving, self-organization, and effective communication given a
network topology. We evaluate a variety of baseline methods on AgentsNet
including homogeneous networks of agents which first have to agree on basic
protocols for organization and communication. We find that some frontier LLMs
are already demonstrating strong performance for small networks but begin to
fall off once the size of the network scales. While existing multi-agent
benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size
and can scale with new generations of LLMs. As such, we also probe frontier
models in a setup with up to 100 agents.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [141] [Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors](https://arxiv.org/abs/2507.08167)
*Md. Saif Hassan Onim,Andrew M. Kiselica,Himanshu Thapliyal*

Main category: cs.HC

TL;DR: 研究通过穿戴设备获取的生理信号进行老年人情绪识别，避免使用摄像头，验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 理解老年人的情绪健康，尤其在医院和养老院环境中，同时避免侵入性方法。

Method: 使用Empatica E4和Shimmer3 GSR+腕带获取生理信号，结合机器学习模型预测情绪强度。

Result: 在回归任务中取得最高0.782的R2分数和最低0.0006的MSE。

Conclusion: 该方法为隐私保护和高效的情绪识别系统提供了可能，尤其适用于ADRD和PTSD患者。

Abstract: Emotion detection in older adults is crucial for understanding their
cognitive and emotional well-being, especially in hospital and assisted living
environments. In this work, we investigate an edge-based, non-obtrusive
approach to emotion identification that uses only physiological signals
obtained via wearable sensors. Our dataset includes data from 40 older
individuals. Emotional states were obtained using physiological signals from
the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were
recorded using camera-based emotion recognition with the iMotion's Facial
Expression Analysis (FEA) module. The dataset also contains twelve emotion
categories in terms of relative intensities. We aim to study how well emotion
recognition can be accomplished using simply physiological sensor data, without
the requirement for cameras or intrusive facial analysis. By leveraging
classical machine learning models, we predict the intensity of emotional
responses based on physiological signals. We achieved the highest 0.782 r2
score with the lowest 0.0006 MSE on the regression task. This method has
significant implications for individuals with Alzheimer's Disease and Related
Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder
(PTSD) or other cognitive impairments. Our results across multiple classical
regression models validate the feasibility of this method, paving the way for
privacy-preserving and efficient emotion recognition systems in real-world
settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [142] [CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization](https://arxiv.org/abs/2507.08406)
*Weigang Feng,Yijia Zhang,Zekun Wang,Zhengyang Wang,Yi Wang,Peijun Ma,Ningyi Xu*

Main category: cs.AR

TL;DR: CCSS是一个可扩展的多核RTL仿真平台，通过专用架构和编译策略，显著提升仿真速度，解决了CPU仿真速度受限的问题。


<details>
  <summary>Details</summary>
Motivation: 随着单芯片晶体管数量激增，RTL级仿真的复杂性导致仿真周期延长，CPU仿真速度成为瓶颈。

Method: CCSS采用平衡的DAG分区方法、高效布尔计算核心以及低延迟片上网络设计，加速组合逻辑计算和时序逻辑同步。

Result: 实验结果显示，CCSS比现有最先进的多核仿真器快12.9倍。

Conclusion: CCSS通过优化架构和编译策略，显著提升了RTL仿真的速度和效率。

Abstract: As transistor counts in a single chip exceed tens of billions, the complexity
of RTL-level simulation and verification has grown exponentially, often
extending simulation campaigns to several months. In industry practice, RTL
simulation is divided into two phases: functional debug and system validation.
While system validation demands high simulation speed and is typically
accelerated using FPGAs, functional debug relies on rapid compilation-rendering
multi-core CPUs the primary choice. However, the limited simulation speed of
CPUs has become a major bottleneck. To address this challenge, we propose CCSS,
a scalable multi-core RTL simulation platform that achieves both fast
compilation and high simulation throughput. CCSS accelerates combinational
logic computation and sequential logic synchronization through specialized
architecture and compilation strategies. It employs a balanced DAG partitioning
method and efficient boolean computation cores for combinational logic, and
adopts a low-latency network-on-chip (NoC) design to synchronize sequential
states across cores efficiently. Experimental results show that CCSS delivers
up to 12.9x speedup over state-of-the-art multi-core simulators.

</details>


### [143] [Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters](https://arxiv.org/abs/2507.08658)
*Robert B. Kent,Marios S. Pattichis*

Main category: cs.AR

TL;DR: 介绍了一种新的硬件合并排序设备List Offset Merge Sorters (LOMS)，通过偏移输入列表的顺序，使用最小化的列排序和行排序阶段，高效合并多个有序输入列表为单一有序输出列表。


<details>
  <summary>Details</summary>
Motivation: 提高合并排序的效率，解决传统设备如Bitonic和Odd-Even Merge Sorters在输入列表大小不等时的设计困难。

Method: 利用输入2-D设置数组和偏移排序顺序，通过交替的列排序和行排序阶段实现高效合并。

Result: LOMS 2-way设备比Batcher的设备快2.63倍，LOMS 3-way设备比现有设备快1.36倍，且资源占用更少。

Conclusion: LOMS设备在速度和资源效率上优于现有技术，适用于FPGA实现。

Abstract: A new set of hardware merge sort devices are introduced here, which merge
multiple sorted input lists into a single sorted output list in a fast and
efficient manner. In each merge sorter, the values from the sorted input lists
are arranged in an input 2-D setup array, but with the order of each sorted
input list offset from the order of each of the other sorted input lists. In
these new devices, called List Offset Merge Sorters (LOMS), a minimal set of
column sort stages alternating with row sort stages process the input setup
array into a final output array, now in the defined sorted order. LOMS 2-way
sorters, which merge 2 sorted input lists, require only 2 merge stages and are
significantly faster than Kenneth Batcher's previous state-of-the-art 2-way
merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way
sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS)
in their first stage. Both LOMS and S2MS devices can merge any mixture of input
list sizes, while Batcher's merge sorters are difficult to design unless the 2
input lists are equal, and a power-of-2. By themselves, S2MS devices are the
fastest 2-way merge sorters when implemented in this study's target FPGA
devices, but they tend to use a large number of LUT resources. LOMS 2-way
devices use fewer resources than comparable S2MS devices, enabling some large
LOMS devices to be implemented in a given FPGA when comparable S2MS devices
cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with
32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup
of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging
3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a
speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [144] [EP-GAT: Energy-based Parallel Graph Attention Neural Network for Stock Trend Classification](https://arxiv.org/abs/2507.08184)
*Zhuodong Jiang,Pengju Zhang,Peter Martin*

Main category: cs.CE

TL;DR: 提出了一种基于能量差异和Boltzmann分布的动态股票图生成方法，结合并行图注意力机制，用于预测多只股票的未来走势。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态或手动定义的因子建模股票间动态依赖关系，且难以保留股票的层次特征。

Method: 通过能量差异生成动态股票图，并设计并行图注意力机制捕捉层次化股票内动态。

Result: 在五个真实数据集上验证，EP-GAT在多种指标上优于五种基线方法。

Conclusion: EP-GAT有效解决了动态依赖和层次特征保留问题，实验验证了其优越性。

Abstract: Graph neural networks have shown remarkable performance in forecasting stock
movements, which arises from learning complex inter-dependencies between stocks
and intra-dynamics of stocks. Existing approaches based on graph neural
networks typically rely on static or manually defined factors to model changing
inter-dependencies between stocks. Furthermore, these works often struggle to
preserve hierarchical features within stocks. To bridge these gaps, this work
presents the Energy-based Parallel Graph Attention Neural Network, a novel
approach for predicting future movements for multiple stocks. First, it
generates a dynamic stock graph with the energy difference between stocks and
Boltzmann distribution, capturing evolving inter-dependencies between stocks.
Then, a parallel graph attention mechanism is proposed to preserve the
hierarchical intra-stock dynamics. Extensive experiments on five real-world
datasets are conducted to validate the proposed approach, spanning from the US
stock markets (NASDAQ, NYSE, SP) and UK stock markets (FTSE, LSE). The
experimental results demonstrate that EP-GAT consistently outperforms
competitive five baselines on test periods across various metrics. The ablation
studies and hyperparameter sensitivity analysis further validate the
effectiveness of each module in the proposed method.

</details>
