<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 71]
- [eess.IV](#eess.IV) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.CV](#cs.CV) [Total: 11]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [SIEVE: Effective Filtered Vector Search with Collection of Indexes](https://arxiv.org/abs/2507.11907)
*Zhaoheng Li,Silu Huang,Wei Ding,Yongjoo Park,Jianjun Chen*

Main category: cs.DB

TL;DR: 提出了一种基于多索引构建的过滤向量搜索方法，通过分析模型优化索引构建和查询性能，显著提升了效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有图基过滤相似性搜索方法在硬约束条件下性能下降，且难以兼顾效率和广泛谓词支持。

Method: 构建多个索引以支持不同谓词形式，通过三维分析模型优化索引构建和查询选择。

Result: 在多种数据集上实现了最高8.06倍的加速，构建时间低至1%，内存占用仅为标准HNSW图的2.15倍。

Conclusion: 该方法在效率和适应性上优于现有技术，适用于不同选择性和形式的谓词。

Abstract: Many real-world tasks such as recommending videos with the kids tag can be
reduced to finding most similar vectors associated with hard predicates. This
task, filtered vector search, is challenging as prior state-of-the-art
graph-based (unfiltered) similarity search techniques quickly degenerate when
hard constraints are considered. That is, effective graph-based filtered
similarity search relies on sufficient connectivity for reaching the most
similar items within just a few hops. To consider predicates, recent works
propose modifying graph traversal to visit only the items that may satisfy
predicates. However, they fail to offer the just-a-few-hops property for a wide
range of predicates: they must restrict predicates significantly or lose
efficiency if only a small fraction of items satisfy predicates.
  We propose an opposite approach: instead of constraining traversal, we build
many indexes each serving different predicate forms. For effective
construction, we devise a three-dimensional analytical model capturing
relationships among index size, search time, and recall, with which we follow a
workload-aware approach to pack as many useful indexes as possible into a
collection. At query time, the analytical model is employed yet again to
discern the one that offers the fastest search at a given recall. We show
superior performance and support on datasets with varying selectivities and
forms: our approach achieves up to 8.06x speedup while having as low as 1%
build time versus other indexes, with less than 2.15x memory of a standard HNSW
graph and modest knowledge of past workloads.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [The AI Shadow War: SaaS vs. Edge Computing Architectures](https://arxiv.org/abs/2507.11545)
*Rhea Pritham Marpu,Kevin J McNamara,Preeti Gupta*

Main category: cs.DC

TL;DR: 本文分析了集中式云AI与分散式边缘AI的竞争，重点关注计算能力、能效和数据隐私。边缘AI在性能和效率上挑战云系统，并因其本地处理能力在隐私和环保方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 探讨AI架构的两种路径（集中式云AI与分散式边缘AI）的竞争，分析其在计算能力、能效和数据隐私方面的表现，以揭示未来AI生态系统的趋势。

Method: 通过对比分析边缘AI和云AI在性能、能效、隐私保护等方面的表现，结合市场数据和实际应用案例，评估其优劣势。

Result: 边缘AI在能效（10,000倍优势）、隐私保护和实时性（5-10ms延迟）上显著优于云AI，市场预计快速增长（38.5% CAGR）。

Conclusion: 边缘AI的分布式架构与高效信息处理相契合，未来将形成混合边缘-云生态系统。

Abstract: The very DNA of AI architecture presents conflicting paths: centralized
cloud-based models (Software-as-a-Service) versus decentralized edge AI (local
processing on consumer devices). This paper analyzes the competitive
battleground across computational capability, energy efficiency, and data
privacy. Recent breakthroughs show edge AI challenging cloud systems on
performance, leveraging innovations like test-time training and
mixture-of-experts architectures. Crucially, edge AI boasts a 10,000x
efficiency advantage: modern ARM processors consume merely 100 microwatts
forinference versus 1 watt for equivalent cloud processing. Beyond efficiency,
edge AI secures data sovereignty by keeping processing local, dismantling
single points of failure in centralized architectures. This democratizes access
throughaffordable hardware, enables offline functionality, and reduces
environmental impact by eliminating data transmission costs. The edge AI market
projects explosive growth from $9 billion in 2025 to $49.6 billion by 2030
(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical
applications including personalized education, healthcare monitoring,
autonomous transport, and smart infrastructure rely on edge AI's ultra-low
latency (5-10ms versus 100-500ms for cloud). The convergence of architectural
innovation with fundamental physics confirms edge AI's distributed approach
aligns with efficient information processing, signaling the inevitable
emergence of hybrid edge-cloud ecosystems.

</details>


### [3] [A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing](https://arxiv.org/abs/2507.11560)
*Xin Wang,Xiao Huan Li,Xun Wang*

Main category: cs.DC

TL;DR: 论文提出了一种针对IIoT边缘计算环境的AIGC任务卸载框架，首次考虑了AIGC模型切换带来的延迟和能耗问题，并设计了基于多智能体深度确定性策略梯度的算法（MADDPG-MATO），显著降低了延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 工业物联网（IIoT）与人工智能生成内容（AIGC）的结合为智能制造带来新机遇，但也面临计算密集型任务和低延迟需求的挑战。传统基于云计算的生成模型难以满足实时需求，而边缘计算虽能降低延迟，但动态任务、模型切换延迟和资源限制增加了复杂性。

Method: 提出了一种AIGC任务卸载框架，首次考虑模型切换的延迟和能耗。IIoT设备作为多智能体协作卸载动态AIGC任务至部署不同生成模型的边缘服务器，并设计了基于MADDPG的模型感知任务卸载算法（MADDPG-MATO）。

Result: 实验表明，MADDPG-MATO在延迟、能耗和任务完成率上优于基线算法，平均延迟降低6.98%，能耗降低7.12%，任务完成率提高3.72%。

Conclusion: MADDPG-MATO在动态高负载IIoT环境中表现出鲁棒性和高效性，为AIGC任务卸载提供了有效解决方案。

Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial
Intelligence-Generated Content (AIGC) offers new opportunities for smart
manufacturing, but it also introduces challenges related to
computation-intensive tasks and low-latency demands. Traditional generative
models based on cloud computing are difficult to meet the real-time
requirements of AIGC tasks in IIoT environments, and edge computing can
effectively reduce latency through task offloading. However, the dynamic nature
of AIGC tasks, model switching delays, and resource constraints impose higher
demands on edge computing environments. To address these challenges, this paper
proposes an AIGC task offloading framework tailored for IIoT edge computing
environments, considering the latency and energy consumption caused by AIGC
model switching for the first time. IIoT devices acted as multi-agent
collaboratively offload their dynamic AIGC tasks to the most appropriate edge
servers deployed with different generative models. A model aware AIGC task
offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient
(MADDPG-MATO) is devised to minimize the latency and energy. Experimental
results show that MADDPG-MATO outperforms baseline algorithms, achieving an
average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%
increase in task completion rate across four sets of experiments with model
numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is
robust and efficient in dynamic, high-load IIoT environments.

</details>


### [4] [Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers](https://arxiv.org/abs/2507.11563)
*Giulio Attenni,Novella Bartolini*

Main category: cs.DC

TL;DR: 本文提出了一种理论方法，用于在云环境中实现环保意识的任务部署和迁移，旨在最小化资源供应的环境影响，同时满足可持续性要求。


<details>
  <summary>Details</summary>
Motivation: 随着对可持续云服务需求的增长，客户需要基于可持续性指标选择数据中心运营商，并准确报告其服务的生态足迹。

Method: 分析可持续性报告，定义数据中心的全面环境影响概况，并建立优化模型以平衡多种环境因素和用户偏好。

Result: 模拟案例研究表明，该方法相比仅优化单一可持续性因素的基线策略具有潜力。

Conclusion: 本文的方法为云环境中的可持续资源管理提供了理论支持，展示了多因素优化的优势。

Abstract: This paper presents a theoretical discussion for environmentally-conscious
job deployment and migration in cloud environments, aiming to minimize the
environmental impact of resource provisioning while incorporating
sustainability requirements. As the demand for sustainable cloud services
grows, it is crucial for cloud customers to select data center operators based
on sustainability metrics and to accurately report the ecological footprint of
their services. To this end, we analyze sustainability reports and define
comprehensive environmental impact profiles for data centers, incorporating key
sustainability indicators. We formalize the problem as an optimization model,
balancing multiple environmental factors while respecting user preferences. A
simulative case study demonstrates the {potential} of our approach compared to
baseline strategies that optimize for single sustainability factors.

</details>


### [5] [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)
*Seth Ockerman,Amal Gueroudji,Tanwi Mallick,Yixuan He,Line Pouchard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: PGT-I扩展了PyTorch Geometric Temporal，通过分布式数据并行训练和两种新策略（index-batching和distributed-index-batching）解决了ST-GNNs在大规模数据集上的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: ST-GNNs在建模时空数据依赖方面表现强大，但受限于内存约束，主要应用于小规模数据集。当前分布式训练框架缺乏对时空模型的支持，且未充分利用时空数据的特性。

Method: 提出了PGT-I，结合分布式数据并行训练和两种新策略：index-batching（动态构建运行时快照以减少内存开销）和distributed-index-batching（支持多GPU扩展处理）。

Result: 在PeMS数据集上首次实现了无需图分区的ST-GNN训练，峰值内存使用减少89%，128 GPU下速度提升13.1倍。

Conclusion: PGT-I通过优化内存和计算效率，显著扩展了ST-GNNs在大规模数据集上的应用潜力。

Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for
modeling spatial and temporal data dependencies. However, their applications
have been limited primarily to small-scale datasets because of memory
constraints. While distributed training offers a solution, current frameworks
lack support for spatiotemporal models and overlook the properties of
spatiotemporal data. Informed by a scaling study on a large-scale workload, we
present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch
Geometric Temporal that integrates distributed data parallel training and two
novel strategies: index-batching and distributed-index-batching. Our index
techniques exploit spatiotemporal structure to construct snapshots dynamically
at runtime, significantly reducing memory overhead, while
distributed-index-batching extends this approach by enabling scalable
processing across multiple GPUs. Our techniques enable the first-ever training
of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing
peak memory usage by up to 89\% and achieving up to a 13.1x speedup over
standard DDP with 128 GPUs.

</details>


### [6] [Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI](https://arxiv.org/abs/2507.11830)
*Samyam Rajbhandari,Mert Hidayetoglu,Aurick Qiao,Ye Wang,Juncheng Yang,Jeff Rasley,Michael Wyatt,Yuxiong He*

Main category: cs.DC

TL;DR: Arctic Inference 是一个开源的 vLLM 插件，通过动态并行策略 Shift Parallelism 优化 AI 推理性能，显著提升延迟、吞吐量和成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有系统在延迟、吞吐量和成本之间存在权衡，无法满足实际需求。

Method: 采用 Shift Parallelism 动态并行策略，结合推测解码、SwiftKV 计算减少和优化的嵌入推理。

Result: 实现请求完成速度提升 3.4 倍，生成速度提升 1.75 倍，嵌入推理达到 1.6M tokens/sec 每 GPU。

Conclusion: Arctic Inference 提供了业界领先的高效推理解决方案，已应用于 Snowflake Cortex AI 并开源。

Abstract: Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source vLLM plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, SwiftKV compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.

</details>


### [7] [Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst](https://arxiv.org/abs/2507.11899)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.DC

TL;DR: 论文研究了云计算中的负载均衡策略，比较了不同算法在集中式和分布式环境下的性能，发现分布式资源设置下响应时间显著降低，智能负载均衡对优化成本和性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着云环境中工作负载的动态性和不可预测性增加，传统的静态负载均衡方法已不足以满足需求，需要更智能和自适应的策略。

Method: 使用Cloud Analyst仿真工具评估了不同负载均衡算法（如Round Robin、Equally Spread和Throttled）在集中式和分布式资源设置下的性能。

Result: 在单一数据中心中，Round Robin算法处理时间略优；而在分布式环境中，Equally Spread和Throttled算法表现更佳，显著降低响应时间和运营成本。

Conclusion: 智能动态负载均衡和资源管理策略对优化云性能与成本至关重要，需持续评估和整合新兴技术以维持高效可扩展的云操作。

Abstract: Load balancing plays a pivotal role in cloud computing, ensuring that
resources are optimally allocated to maintain high service quality and
operational efficiency. As workloads in cloud environments become increasingly
dynamic and unpredictable, load balancing strategies are evolving from
traditional static methods to more adaptive and intelligent approaches. In this
study, the Cloud Analyst simulation tool was used to evaluate the performance
of different load balancing algorithms under various scenarios, including both
centralized and distributed resource setups. The results highlight that while
the Round Robin algorithm yields slightly better processing times within a
single data center, Equally Spread and Throttled techniques perform
competitively, especially when network latency is considered. More importantly,
when resources are distributed across multiple data centers, response times are
significantly reduced, emphasizing the value of proximity and efficient load
distribution. In these distributed environments, Equally Spread and Throttled
algorithms not only maintain quick response times but also contribute to lower
operational costs. These findings demonstrate the necessity of strategic
resource placement and proactive infrastructure planning to balance performance
and cost. Adopting intelligent, dynamic load balancing and resource management
practices can help organizations meet evolving cloud demands, optimize costs,
and maintain a competitive advantage. Continuous evaluation and integration of
emerging technologies are crucial for sustaining effective and scalable cloud
operations.

</details>


### [8] [Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics](https://arxiv.org/abs/2507.11929)
*Minchen Yu,Yinghao Ren,Jiamu Zhao,Jiaqi Li*

Main category: cs.DC

TL;DR: 本文提出了一种可扩展的无服务器计算设计原则，通过Proteus平台实现，支持开发者自定义控制行为以优化性能，同时保持共享易用的环境。


<details>
  <summary>Details</summary>
Motivation: 通用无服务器平台对复杂工作负载性能不足，而定制化系统又牺牲了简单性和通用性，因此需要一种平衡方案。

Method: 提出可扩展设计原则，并在Proteus平台中实现，引入决策工作流抽象，支持开发者自定义控制行为。

Result: 初步结果显示，Proteus原型有效优化了分析查询执行，并支持跨应用的细粒度资源共享。

Conclusion: Proteus的设计原则为无服务器计算提供了一种兼顾性能和易用性的解决方案。

Abstract: Serverless computing has attracted a broad range of applications due to its
ease of use and resource elasticity. However, developing serverless
applications often poses a dilemma -- relying on general-purpose serverless
platforms can fall short of delivering satisfactory performance for complex
workloads, whereas building application-specific serverless systems undermines
the simplicity and generality. In this paper, we propose an extensible design
principle for serverless computing. We argue that a platform should enable
developers to extend system behaviors for domain-specialized optimizations
while retaining a shared, easy-to-use serverless environment. We take data
analytics as a representative serverless use case and realize this design
principle in Proteus. Proteus introduces a novel abstraction of decision
workflows, allowing developers to customize control-plane behaviors for
improved application performance. Preliminary results show that Proteus's
prototype effectively optimizes analytical query execution and supports
fine-grained resource sharing across diverse applications.

</details>


### [9] [NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning](https://arxiv.org/abs/2507.11978)
*Jiacheng Huang,Zimin Li,Yinghui Li,Haojie Wang*

Main category: cs.DC

TL;DR: NineToothed是一种面向机器学习的领域特定语言，通过将串行代码自动转换为并行代码，简化了开发过程，同时性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习领域特定语言（如Triton）要求开发者具备并行编程专业知识，增加了开发复杂性。

Method: NineToothed提供串行语义，支持张量导向的元编程（TOM），并包含一个高性能并行代码生成器。

Result: 评估显示，NineToothed在简化开发的同时，性能与Triton相当。

Conclusion: NineToothed为深度学习工作负载提供了一种更简单的编程模型，显著降低了开发难度。

Abstract: The emergence of deep learning domain-specific languages (DSLs) has
substantially reduced the obstacles in developing high-performance,
cross-platform compute kernels. However, current DSLs, such as Triton, still
demand that developers possess expertise in parallel programming and expose
them to many low-level details. This requirement complicates the development
process and adds to the difficulty of maintaining compute kernels.
Consequently, developing a new programming model that supports serial
programming for deep learning workloads is crucial.
  This paper introduces NineToothed, a domain-specific language that offers
serial semantics for machine learning programming. Through the automatic
transformation of serial code into parallel code, NineToothed significantly
streamlines the development process while causing minimal performance
degradation. NineToothed encompasses (1) a language with tensor-oriented
metaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the
expression of tiled computations without the need to manage low-level details
and (2) a code generator for generating high-performance parallel code. Our
evaluation results indicate that NineToothed can greatly simplify compute
kernel development while maintaining performance comparable to that of Triton.

</details>


### [10] [ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum](https://arxiv.org/abs/2507.12032)
*Brian-Frederik Jahnke,René Brinkhege,Jan Peter Meyer,Daniel Tebernum,Falk Howar*

Main category: cs.DC

TL;DR: ARRC是一个基于软件工程原则的推荐系统，通过透明、可审核的代理提供跨层资源优化建议，显著减少操作员工作量并提高计算利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘-云连续体中资源优化的可持续性、可解释性和可维护性挑战，避免过度配置和操作复杂性。

Method: 引入ARRC推荐系统，通过专用代理封装优化逻辑，提供透明、可审核的建议，直接集成到操作员工作流中。

Result: 在工业部署中，ARRC减少操作员工作量50%以上，计算利用率提升7.7倍，错误率低于5%。

Conclusion: ARRC展示了可解释、基于建议的架构在生产规模下实现可持续效率和可维护性改进的潜力。

Abstract: Achieving sustainable, explainable, and maintainable automation for resource
optimization is a core challenge across the edge-cloud continuum. Persistent
overprovisioning and operational complexity often stem from heterogeneous
platforms and layered abstractions, while systems lacking explainability and
maintainability become fragile, impede safe recovery, and accumulate technical
debt. Existing solutions are frequently reactive, limited to single abstraction
layers, or require intrusive platform changes, leaving efficiency and
maintainability gains unrealized.
  This paper addresses safe, transparent, and low-effort resource optimization
in dynamic, multi-tenant edge-cloud systems, without disrupting operator
workflows or increasing technical debt. We introduce ARRC, a recommender system
rooted in software engineering design principles, which delivers explainable,
cross-layer resource recommendations directly into operator workflows (such as
tickets and GitOps pull requests). ARRC encapsulates optimization logic in
specialized, auditable agents coordinated via a shared interface, supporting
maintainability and extensibility through transparency and the ability to
inspect both recommendations and their rationale.
  Empirical evaluation in a multi-region industrial deployment shows that ARRC
reduces operator workload by over 50%, improves compute utilization by up to
7.7x, and maintains error rates below 5%, with most benefits achieved through
incremental, operator-approved changes. This demonstrates that explainable,
recommendation-based architectures can achieve sustainable efficiency and
maintainability improvements at production scale.
  ARRC provides an empirically evaluated framework for integrating explainable,
workflow-driven automation into resource management, intended to advance best
practices for robust, maintainable, and transparent edge-cloud continuum
platforms.

</details>


### [11] [Distributed Algorithms for Potential Problems](https://arxiv.org/abs/2507.12038)
*Alkida Balliu,Thomas Boudier,Francesco d'Amore,Dennis Olivetti,Gustav Schmid,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文提出了一种快速分布式算法，用于解决局部势能问题，如局部最优割问题，填补了确定性LOCAL模型中该问题的复杂度空白。


<details>
  <summary>Details</summary>
Motivation: 局部势能问题（如局部最优割）的分布式算法复杂度长期存在较大差距，本文旨在解决这一问题。

Method: 提出了一种在有限度图中解决所有局部势能问题的分布式算法，适用于确定性和随机性LOCAL模型。

Result: 算法在有限度图中实现了对数多项式轮复杂度，解决了局部最优割问题的确定性轮复杂度。

Conclusion: 本文填补了局部势能问题复杂度研究的空白，为相关领域提供了新的理论支持。

Abstract: In this work we present a fast distributed algorithm for local potential
problems: these are graph problems where the task is to find a locally optimal
solution where no node can unilaterally improve the utility in its local
neighborhood by changing its own label. A simple example of such a problem is
the task of finding a locally optimal cut, i.e., a cut where for each node at
least half of its incident edges are cut edges. The distributed round
complexity of locally optimal cut has been wide open; the problem is known to
require $\Omega(\log n)$ rounds in the deterministic LOCAL model and
$\Omega(\log \log n)$ rounds in the randomized LOCAL model, but the only known
upper bound is the trivial brute-force solution of $O(n)$ rounds. Locally
optimal cut in bounded-degree graphs is perhaps the simplest example of a
locally checkable labeling problem for which there is still such a large gap
between current upper and lower bounds. We show that in bounded-degree graphs,
all local potential problems, including locally optimal cut, can be solved in
$\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.
In particular, the deterministic round complexity of the locally optimal cut
problem is now settled to $\log^{\Theta(1)} n$.

</details>


### [12] [Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso](https://arxiv.org/abs/2507.12106)
*Antonio Salis,Gabriele Troina,Gianluca Boanelli,Marco Ottaviano,Paola Fortini,Soraya Versace*

Main category: cs.DC

TL;DR: 论文探讨了通过物联网和数据分析技术优化城市绿地管理，以提升居民生活质量。


<details>
  <summary>Details</summary>
Motivation: 城市绿地作为“绿色肺”，对居民健康和生态系统服务至关重要，但传统管理方式效率不足。

Method: 采用物联网系统、数据驱动平台和机器学习算法，实时监测树木和绿地状态，优化灌溉和决策。

Result: 开发了基于云的决策支持平台，实现智能控制和预测性管理，提升绿地可持续性。

Conclusion: 数字化和技术创新可支持可持续城市治理，增强环境韧性和居民生活质量。

Abstract: The efficient design and management of public green spaces is a key factor in
promoting the health and well-being of urban population, as emphasized by the
WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban
ecosystem, playing a vital role in enhancing quality of life thanks to the
provision of ecosystem services. In this context, the Smart Green City use case
in Campobasso municipality, funded by the Italian Ministry of Enterprises
(MIMIT), emerges as an innovative model for the sustainable management of green
urban areas through the adoption of an advanced system of emerging technologies
integrated and interoperable. The project integrates IoT systems and
data-driven governance platforms, enabling real-time monitoring of the health
status of trees and green areas via a Decision Support System (DSS). It also
facilitates the collection and analysis of data from diverse sources, including
weather conditions, air quality, soil moisture, pollution levels. The resulting
cloud-based platform supports a holistic real time decision making for green
urban managers, technical experts and operational staff. It enables intelligent
control and management of urban green spaces using Tree Talker sensors,
integrated with soil moisture and water potential monitoring systems. Thanks to
predictive models based on machine learning algorithms and real time data
provided by IoT sensors, irrigation of public parks can be optimized by
providing suggestions on when and how much water to apply. Customized alerts
layers are also activated warning users when monitored parameters, such as soil
temperature, humidity, or water potential, exceed predefined thresholds. This
Use Case demonstrates how digitalization, IoT sensors fusion and technological
innovation can support sustainable urban governance, fostering environmental
resilience and improving citizens quality of life.

</details>


### [13] [Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage](https://arxiv.org/abs/2507.12205)
*Junqing Lin,Jingwei Sun,Mingge Lu,Guangzhong Sun*

Main category: cs.DC

TL;DR: EC-SpMV是一种针对稀疏大语言模型（LLM）推理优化的GPU加速方法，通过分层块提取算法和新型压缩稀疏格式（EC-CSR）显著提升性能并减少存储开销。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵-向量乘法（SpMV）在稀疏LLM的本地部署中成为性能瓶颈，现有方法未能充分利用稀疏LLM的结构模式，导致性能不佳和存储开销过大。

Method: 提出EC-SpMV，包括分层块提取算法和多粒度块结构捕获，以及采用增量索引的压缩稀疏格式（EC-CSR）。

Result: 在LLaMA和OPT模型的稀疏权重矩阵上测试，EC-SpMV比现有SpMV库快6.44倍，存储开销比CSR减少55.4%。

Conclusion: EC-SpMV有效解决了稀疏LLM推理中的SpMV性能问题，显著提升了计算效率和存储利用率。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance
bottleneck in the local deployment of sparse Large Language Models (LLMs),
where inference predominantly operates on workloads during the decoder phase
with a batch size of one. Existing SpMV kernels and sparse matrix formats,
originally designed for scientific computing, fail to exploit the unique
structure patterns inherent in sparse LLMs, resulting in suboptimal performance
and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized
SpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a
hierarchical block extraction algorithm that captures multiple granularities of
block structures within sparse LLMs, and (2) a novel compressed sparse format
(EC-CSR) that employs delta indexing to reduce storage overhead and enhance
memory access efficiency. Evaluated on real sparse weight matrices from LLaMA
and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV
libraries and reduces storage overhead by up to 55.4% compared to CSR.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [14] [Finite Pinwheel Scheduling: the k-Visits Problem](https://arxiv.org/abs/2507.11681)
*Sotiris Kanellopoulos,Christos Pergaminelis,Maria Kokkou,Euripides Markou,Aris Pagourtzis*

Main category: cs.DS

TL;DR: 论文研究了Pinwheel Scheduling问题的有限版本k-Visits，证明了2-Visits是强NP完全的，并展示了其在特定条件下的线性时间解法。


<details>
  <summary>Details</summary>
Motivation: Pinwheel Scheduling问题的复杂性尚未完全解决，研究其有限版本k-Visits有助于理解其计算复杂性。

Method: 通过从Numerical 3-Dimensional Matching (N3DM)的归约，证明了2-Visits的强NP完全性，并提出了针对特定输入的线性时间算法。

Result: 证明了2-Visits是强NP完全的，并展示了其在输入为集合时可在多项式时间内解决。

Conclusion: 研究为Pinwheel Scheduling问题的复杂性提供了新见解，并展示了k-Visits问题的复杂性边界。

Abstract: Pinwheel Scheduling is a fundamental scheduling problem, in which each task
$i$ is associated with a positive integer $d_i$, and the objective is to
schedule one task per time slot, ensuring each task perpetually appears at
least once in every $d_i$ time slots. Although conjectured to be
PSPACE-complete, it remains open whether Pinwheel Scheduling is NP-hard (unless
a compact input encoding is used) or even contained in NP.
  We introduce k-Visits, a finite version of Pinwheel Scheduling, where given n
deadlines, the goal is to schedule each task exactly k times. While we observe
that the 1-Visit problem is trivial, we prove that 2-Visits is strongly
NP-complete through a surprising reduction from Numerical 3-Dimensional
Matching (N3DM). As intermediate steps in the reduction, we define NP-complete
variants of N3DM which may be of independent interest. We further extend our
strong NP-hardness result to a generalization of k-Visits $k\geq 2$ in which
the deadline of each task may vary throughout the schedule, as well as to a
similar generalization of Pinwheel Scheduling, thus making progress towards
settling the complexity of Pinwheel Scheduling.
  Additionally, we prove that 2-Visits can be solved in linear time if all
deadlines are distinct, rendering it one of the rare natural problems which
exhibit the interesting dichotomy of being in P if their input is a set and
NP-complete if the input is a multiset. We achieve this through a Turing
reduction from 2-Visits to a variation of N3DM, which we call Position
Matching. Based on this reduction, we also show an FPT algorithm for 2-Visits
parameterized by a value related to how close the input deadlines are to each
other, as well as a linear-time algorithm for instances with up to two distinct
deadlines.

</details>


### [15] [Approaching Optimality for Solving Dense Linear Systems with Low-Rank Structure](https://arxiv.org/abs/2507.11724)
*Michał Dereziński,Aaron Sidford*

Main category: cs.DS

TL;DR: 论文提出了高精度随机算法，用于解决线性系统和回归问题，适用于条件良好但存在k个大奇异值的情况。算法在时间复杂度和性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在条件良好但存在k个大奇异值的情况下，线性系统和回归问题的计算效率问题，并优化现有方法的性能。

Method: 基于三种递归预条件框架，结合矩阵草图和低秩更新公式，针对问题结构进行优化。

Result: 算法在时间上接近自然复杂度极限，显著优于现有方法，并首次实现了密集矩阵核范数的近似计算。

Conclusion: 新算法在解决特定结构的线性系统和回归问题时表现出色，为相关领域提供了高效的计算工具。

Abstract: We provide new high-accuracy randomized algorithms for solving linear systems
and regression problems that are well-conditioned except for $k$ large singular
values. For solving such $d \times d$ positive definite system our algorithms
succeed whp. and run in time $\tilde O(d^2 + k^\omega)$. For solving such
regression problems in a matrix $\mathbf{A} \in \mathbb{R}^{n \times d}$ our
methods succeed whp. and run in time $\tilde O(\mathrm{nnz}(\mathbf{A}) + d^2 +
k^\omega)$ where $\omega$ is the matrix multiplication exponent and
$\mathrm{nnz}(\mathbf{A})$ is the number of non-zeros in $\mathbf{A}$. Our
methods nearly-match a natural complexity limit under dense inputs for these
problems and improve upon a trade-off in prior approaches that obtain running
times of either $\tilde O(d^{2.065}+k^\omega)$ or $\tilde O(d^2 +
dk^{\omega-1})$ for $d\times d$ systems. Moreover, we show how to obtain these
running times even under the weaker assumption that all but $k$ of the singular
values have a suitably bounded generalized mean. Consequently, we give the
first nearly-linear time algorithm for computing a multiplicative approximation
to the nuclear norm of an arbitrary dense matrix. Our algorithms are built on
three general recursive preconditioning frameworks, where matrix sketching and
low-rank update formulas are carefully tailored to the problems' structure.

</details>


### [16] [Pathfinding in Self-Deleting Graphs](https://arxiv.org/abs/2507.12047)
*Michal Dvořák,Dušan Knop,Michal Opler,Jan Pokorný,Ondřej Suchý,Krisztina Szilágyi*

Main category: cs.DS

TL;DR: 研究了路径查找问题在遍历依赖图（即边随访问顶点变化的图）上的复杂性，证明了其NP难性及参数化复杂性。


<details>
  <summary>Details</summary>
Motivation: 探索在动态变化的图中路径查找问题的复杂性，特别是自删除图（self-deleting graphs）中的路径问题。

Method: 通过理论分析，证明了自删除图中的路径问题在多种约束条件下的NP难性和参数化复杂性。

Result: 证明了问题在多种参数下的NP难性和W[1]-完备性，并展示了某些参数化情况下的固定参数可解性（FPT）。

Conclusion: 自删除图中的路径问题在多种情况下是计算困难的，但在某些参数化条件下可解。

Abstract: In this paper, we study the problem of pathfinding on traversal-dependent
graphs, i.e., graphs whose edges change depending on the previously visited
vertices. In particular, we study \emph{self-deleting graphs}, introduced by
Carmesin et al. (Sarah Carmesin, David Woller, David Parker, Miroslav Kulich,
and Masoumeh Mansouri. The Hamiltonian cycle and travelling salesperson
problems with traversal-dependent edge deletion. J. Comput. Sci.), which
consist of a graph $G=(V, E)$ and a function $f\colon V\rightarrow 2^E$, where
$f(v)$ is the set of edges that will be deleted after visiting the vertex $v$.
In the \textsc{(Shortest) Self-Deleting $s$-$t$-path} problem we are given a
self-deleting graph and its vertices $s$ and $t$, and we are asked to find a
(shortest) path from $s$ to $t$, such that it does not traverse an edge in
$f(v)$ after visiting $v$ for any vertex $v$.
  We prove that \textsc{Self-Deleting $s$-$t$-path} is NP-hard even if the
given graph is outerplanar, bipartite, has maximum degree $3$, bandwidth $2$
and $|f(v)|\leq 1$ for each vertex $v$. We show that \textsc{Shortest
Self-Deleting $s$-$t$-path} is W[1]-complete parameterized by the length of the
sought path and that \textsc{Self-Deleting $s$-$t$-path} is \W{1}-complete
parameterized by the vertex cover number, feedback vertex set number and
treedepth. We also show that the problem becomes FPT when we parameterize by
the maximum size of $f(v)$ and several structural parameters. Lastly, we show
that the problem does not admit a polynomial kernel even for parameterization
by the vertex cover number and the maximum size of $f(v)$ combined already on
2-outerplanar graphs.

</details>


### [17] [Weighted $k$-Server Admits an Exponentially Competitive Algorithm](https://arxiv.org/abs/2507.12130)
*Adithya Bijoy,Ankit Mondal,Ashish Chiplunkar*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The weighted $k$-server is a variant of the $k$-server problem, where the
cost of moving a server is the server's weight times the distance through which
it moves. The problem is famous for its intriguing properties and for evading
standard techniques for designing and analyzing online algorithms. Even on
uniform metric spaces with sufficiently many points, the deterministic
competitive ratio of weighted $k$-server is known to increase doubly
exponentially with respect to $k$, while the behavior of its randomized
competitive ratio is not fully understood. Specifically, no upper bound better
than doubly exponential is known, while the best known lower bound is singly
exponential in $k$. In this paper, we close the exponential gap between these
bounds by giving an $\exp(O(k^2))$-competitive randomized online algorithm for
the weighted $k$-server problem on uniform metrics, thus breaking the doubly
exponential barrier for deterministic algorithms for the first time. This is
achieved by a recursively defined notion of a phase which, on the one hand,
forces a lower bound on the cost of any offline solution, while, on the other
hand, also admits a randomized online algorithm with bounded expected cost. The
algorithm is also recursive; it involves running several algorithms virtually
and in parallel and following the decisions of one of them in a random order.
We also show that our techniques can be lifted to construct an
$\exp(O(k^2))$-competitive randomized online algorithm for the generalized
$k$-server problem on weighted uniform metrics.

</details>


### [18] [A near-complete resolution of the exponential-time complexity of k-opt for the traveling salesman problem](https://arxiv.org/abs/2507.12304)
*Sophia Heimann,Hung P. Hoang,Stefan Hougardy*

Main category: cs.DS

TL;DR: 该论文研究了$k$-opt算法在解决旅行商问题时的迭代次数问题，证明了对于$k=3$和$k=4$，即使使用最优枢轴规则，也可能需要指数级迭代次数。此外，还扩展了结果到2.5-opt算法。


<details>
  <summary>Details</summary>
Motivation: 长期以来，$k$-opt算法在小$k$值下的迭代次数上限是一个未解决的问题。本文旨在填补这一空白，特别是在$k=3$和$k=4$的情况下。

Method: 通过理论分析，证明了$k=3$和$k=4$时，$k$-opt算法可能需要指数级迭代次数。同时，扩展了结果到2.5-opt算法。

Result: 证明了$k=3$和$k=4$时，$k$-opt算法在最坏情况下需要指数级迭代次数。2.5-opt算法也有类似的下界。

Conclusion: 本文为$k$-opt算法的迭代次数问题提供了完整答案，特别是对于$k \geq 3$的情况，并扩展了结果到2.5-opt算法。

Abstract: The $k$-opt algorithm is one of the simplest and most widely used heuristics
for solving the traveling salesman problem. Starting from an arbitrary tour,
the $k$-opt algorithm improves the current tour in each iteration by exchanging
up to $k$ edges. The algorithm continues until no further improvement of this
kind is possible. For a long time, it remained an open question how many
iterations the $k$-opt algorithm might require for small values of $k$,
assuming the use of an optimal pivot rule. In this paper, we resolve this
question for the cases $k = 3$ and $k = 4$ by proving that in both these cases
an exponential number of iterations may be needed even if an optimal pivot rule
is used. Combined with a recent result from Heimann, Hoang, and Hougardy (ICALP
2024), this provides a complete answer for all $k \geq 3$ regarding the number
of iterations the $k$-opt algorithm may require under an optimal pivot rule. In
addition we establish an analogous exponential lower bound for the 2.5-opt
algorithm, a variant that generalizes 2-opt and is a restricted version of
3-opt. All our results hold for both the general and the metric traveling
salesman problem.

</details>


### [19] [Online Block Packing](https://arxiv.org/abs/2507.12357)
*Ariel Ben Eliezer,Noam Nisan*

Main category: cs.DS

TL;DR: 论文研究了区块链在多维区块约束和准耐心投标者情况下的算法挑战，提出了在线近似算法，解决了Babaioff和Nisan在EC 2025中留下的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 区块链在多维区块约束和准耐心投标者场景下面临算法挑战，现有方法未能完全解决这些问题。

Method: 提出了在线近似算法来解决这些挑战。

Result: 算法成功解决了Babaioff和Nisan提出的开放性问题。

Conclusion: 该研究为区块链在多维约束下的算法设计提供了有效解决方案。

Abstract: We consider the algorithmic challenge that is faced by blockchains that have
multidimensional block constraints and serve quasi-patient bidders. We provide
online approximation algorithms for this problem, thus solving open problems
left by [Babaioff and Nisan, EC 2025].

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: 该研究提出了量子软件系统中六个关键设计领域的决策模型，帮助开发者选择和应用合适的架构模式与策略。


<details>
  <summary>Details</summary>
Motivation: 量子软件开发者面临选择和实施架构模式与策略的挑战，缺乏指导。

Method: 通过数据挖掘（GitHub和Stack Exchange）和系统文献综述收集相关模式与策略，构建决策模型，并通过16位从业者访谈评估模型。

Result: 决策模型在熟悉度、可理解性、完整性和实用性方面表现良好，能有效辅助开发者。

Conclusion: 提出的决策模型为量子软件架构设计提供了实用工具，数据集公开供社区使用。

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [21] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint是一个新的指令跟随框架，通过指令调优合成数据改进代码质量分析，适应新代码模式而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中表现优异，但在代码质量分析上受限于静态训练数据，难以适应不断发展的最佳实践。

Method: MetaLint通过指令调优合成数据，将代码质量分析任务转化为检测和修复问题代码片段或习语，支持从易到难的泛化。

Result: MetaLint在未见的PEP习语检测中表现优异，F-score达70.37%，召回率70.43%，定位准确率26.73%。

Conclusion: MetaLint展示了在代码质量分析中的潜力，尤其适用于未来代码实践的适应性分析。

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [22] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: 研究发现，计算机科学本科课程中软件质量常被忽视，导致学生代码质量不佳，尤其是在REST API设计方面存在大量常见错误。


<details>
  <summary>Details</summary>
Motivation: 探讨学生代码质量现状，以改进教育及招聘实践。

Method: 通过自动化静态分析管道评估40个全栈Web应用程序的REST API设计规则遵循情况。

Result: 发现常见错误：端点路径缺少连字符（98%）、复数形式错误（88%）、HTTP方法误用（83%）。

Conclusion: 需加强API设计教学，并采用自动化工具提升学生代码质量。

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [23] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: 利用LLM自动化网络软件的极端测试，通过生成违反约束的测试用例，发现HTTP、BGP和DNS实现中的新bug。


<details>
  <summary>Details</summary>
Motivation: 传统极端测试依赖人工，效率低且易遗漏。利用LLM自动化可以提升测试覆盖率和效率。

Method: 分两步：1. 用LLM生成输入约束；2. 用LLM生成违反约束的测试用例。

Result: 成功发现HTTP、BGP和DNS实现中的新bug，并扩展到集中式网络软件。

Conclusion: LLM生成的极端测试超越了传统边界值分析，未来可通过AI代理进一步自动化。

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [24] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: 本文提出了一种任务分类法，用于系统化一致性检查分析中的任务，以帮助研究人员明确可视化的目的。


<details>
  <summary>Details</summary>
Motivation: 当前一致性检查工具的可视化目的不明确，缺乏系统性理解，难以评估其有效性。

Method: 结合过程挖掘和可视化分析，提出任务分类法，从目标、手段、约束类型、数据特征、数据目标和数据基数等方面定义任务。

Result: 任务分类法为研究人员提供了一种系统化方法，以明确可视化的一致性检查任务。

Conclusion: 该分类法促进过程挖掘与可视化分析领域的合作，为一致性检查的可视化研究提供支持。

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [25] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: LLAMA是一个基于大型语言模型（LLMs）的多反馈智能合约模糊测试框架，通过结合LLMs、进化突变策略和混合测试技术，显著提升了覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具主要关注种子调度和生成，而突变调度被忽视，影响了测试效果。LLAMA旨在填补这一空白。

Method: LLAMA采用分层提示策略生成初始种子，结合多反馈优化机制和进化模糊引擎，动态调整突变操作概率。

Result: 实验显示LLAMA在指令和分支覆盖率上分别达到91%和90%，检测出132/148已知漏洞。

Conclusion: LLAMA在智能合约安全测试中表现出高效性、适应性和实用性。

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [26] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的自动化工具AI4Pricing2Yaml，用于将静态HTML定价转换为智能定价（iPricing），以解决SaaS定价管理的复杂性和效率问题。


<details>
  <summary>Details</summary>
Motivation: SaaS市场的快速扩张导致定价管理复杂化，手动管理效率低下且易出错，缺乏自动化工具限制了定价模型的优化和扩展。

Method: 采用LLM驱动的技术，结合网页抓取和信息提取，开发了AI4Pricing2Yaml工具，用于从SaaS网站提取定价组件并转换为动态、机器可读的iPricing。

Result: 在30个商业SaaS网站上的验证表明，系统能有效提取定价元素，但处理幻觉、复杂结构和动态内容仍存在挑战。

Conclusion: 自动化智能定价转换有潜力提升SaaS定价管理的效率和一致性，未来研究将优化提取能力和系统适应性。

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [27] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: 提出了一种基于用户中心方法（如设计思维和语言决策）的网页可用性评估方法，结合A/B测试和角色扮演场景，应用于实际案例。


<details>
  <summary>Details</summary>
Motivation: 提升用户界面满意度，解决现有工具在评估设计时支持不足的问题。

Method: 结合设计思维和语言决策，进行角色扮演和可用性测试（如系统可用性量表），并整合到A/B测试决策支持系统中。

Result: 方法应用于评估墨西哥瓜达拉哈拉大学的三个Moodle平台，使用真实用户验证。

Conclusion: 提出的方法有效支持网页可用性评估，结合用户参与和决策支持系统。

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [28] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: MERA Code是MERA基准家族的新成员，专注于评估俄语代码生成LLM的性能，填补了现有基准在代码质量和实际应用中的空白。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注自然语言任务，忽视了代码质量和实际性能，导致对模型真实能力和风险的误解。

Method: 提出MERA Code基准，包含11个任务和8种编程语言，采用开源代码库、评分系统和平台（排行榜和提交系统）进行评估。

Result: 评估了开源LLM和前沿API模型，揭示了其在非英语语言实际编码任务中的局限性。

Conclusion: 公开发布MERA Code以指导未来研究，推动模型开发创新，并标准化评估流程。

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [29] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: GitChameleon是一个新的数据集，用于评估AI在特定库版本下生成可执行代码的能力，发现现有模型在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 解决软件库快速更新对代码生成的挑战，现有基准缺乏基于执行的评估。

Method: 引入GitChameleon数据集，包含328个Python代码补全问题，附带单元测试，评估LLM和代码辅助工具的表现。

Result: 现有模型在此任务上成功率仅为48-51%，显示问题的复杂性。

Conclusion: GitChameleon为理解动态库代码生成挑战提供了基准，并推动更可靠的AI代码生成方法发展。

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [30] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: SWE-Perf是一个专门用于评估大型语言模型（LLMs）在真实代码库中优化代码性能能力的首个基准测试，包含140个实例，揭示了现有LLMs与专家级优化之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在代码生成和修复方面表现出色，但在代码性能优化方面的能力尚未充分探索，尤其是在真实代码库环境中。

Method: 引入SWE-Perf基准测试，包含来自GitHub的140个性能优化pull请求实例，提供代码库、目标函数、测试、专家补丁和可执行环境。

Result: 评估显示现有LLMs在代码性能优化任务上与专家级表现存在显著差距。

Conclusion: SWE-Perf为研究LLMs在代码性能优化领域的潜力提供了重要基准，并指出了未来的研究方向。

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [31] [Towards a Non-Binary View of IPv6 Adoption](https://arxiv.org/abs/2507.11678)
*Sulyab Thottungal Valapu,John Heidemann*

Main category: cs.NI

TL;DR: 论文研究了IPv6部署的现状，从客户端、服务器和云提供商的角度分析了IPv6的使用情况，发现部署虽在增长但仍存在滞后现象。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6部署的增加，需要从更细致的非二元视角评估其使用情况，而不仅仅是能否使用IPv6。

Method: 从客户端观察用户流量，分析服务器端IPv6采用情况，并评估云提供商对IPv6的支持。

Result: 用户IPv6流量差异显著，仅12.5%的顶级网站完全支持IPv6，云提供商间租户IPv6采用率差异大。

Conclusion: IPv6部署虽在增长，但许多服务仍滞后，云提供商可通过简化IPv6启用流程提升采用率。

Abstract: Twelve years have passed since World IPv6 Launch Day, but what is the current
state of IPv6 deployment? Prior work has examined IPv6 status as a binary: can
you use IPv6, or not? As deployment increases we must consider a more nuanced,
non-binary perspective on IPv6: how much and often can a user or a service use
IPv6? We consider this question as a client, server, and cloud provider.
Considering the client's perspective, we observe user traffic. We see that the
fraction of IPv6 traffic a user sends varies greatly, both across users and
day-by-day, with a standard deviation of over 15%. We show this variation
occurs for two main reasons. First, IPv6 traffic is primarily human-generated,
thus showing diurnal patterns. Second, some services are IPv6-forward and
others IPv6-laggards, so as users do different things their fraction of IPv6
varies. We look at server-side IPv6 adoption in two ways. First, we expand
analysis of web services to examine how many are only partially IPv6 enabled
due to their reliance on IPv4-only resources. Our findings reveal that only
12.5% of top 100k websites qualify as fully IPv6-ready. Finally, we examine
cloud support for IPv6. Although all clouds and CDNs support IPv6, we find that
tenant deployment rates vary significantly across providers. We find that ease
of enabling IPv6 in the cloud is correlated with tenant IPv6 adoption rates,
and recommend best practices for cloud providers to improve IPv6 adoption. Our
results suggest IPv6 deployment is growing, but many services lag, presenting a
potential for improvement.

</details>


### [32] [On QoE-Aware Traffic Management for Real-time, Interactive Video with Time-variant Spatial Complexity](https://arxiv.org/abs/2507.11798)
*Szilveszter Nádas,Lars Ernström,David Lindero,Jonathan Lynam*

Main category: cs.NI

TL;DR: 分析了视频空间复杂度与QoE的关系，提出基于效用的动态资源分配方法，优于静态分配和均等QoE分配。


<details>
  <summary>Details</summary>
Motivation: 研究实时交互视频中空间复杂度与QoE的动态关系，优化资源管理。

Method: 引入效用概念，动态调整资源分配，比较静态与动态分配效果。

Result: 动态分配显著提升性能，提高平均QoE并控制最差情况。

Conclusion: 效用驱动的动态资源分配优于传统方法，适用于实时视频管理。

Abstract: We analyzed spatial complexity, defined as the relationship between the
required bitrate and a corresponding picture Quality of Experience (QoE)
metric, for realistic, long, real-time, interactive video clips. Apart from
variation across different content types, e.g., game genres, we discovered
time-variability within a clip from second to second, and explored the
ramifications for traffic management. We introduced utility as an elegant way
to manage resource sharing preferences. Our analysis of resource sharing
methods shows that frequent QoE-aware reallocation has significant performance
advantages compared to static rate allocation, even in case the latter is based
on rich information about long-term average spatial complexity. We have also
shown that utility-based resource allocation has clear advantages over methods
targeting equal QoE allocation, it increases the average QoE, while it still
controls the worst case QoE.

</details>


### [33] [Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview](https://arxiv.org/abs/2507.11935)
*Jikang Deng,Fizza Hassan,Hui Zhou,Saad Al-Ahmadi,Mohamed-Slim Alouini,Daniel B. Da Costa*

Main category: cs.NI

TL;DR: 本文探讨了如何将开放无线接入网络（ORAN）与非地面网络（NTN）结合，以解决NTN在开发和运维（DevOps）生命周期中的挑战，并提出了一种基于ORAN的NTN框架。


<details>
  <summary>Details</summary>
Motivation: 6G网络的发展需要高效、可靠和灵活的无线网络架构，NTN和ORAN因其潜力受到关注。然而，NTN的高空和移动性带来了DevOps挑战，缺乏原生AI能力，而ORAN的开放性、虚拟化和智能特性为解决这些问题提供了可能。

Method: 文章首先介绍了ORAN和NTN的背景知识，总结了现有研究，并提出了基于ORAN的NTN框架，详细讨论了其架构和功能，包括灵活的前传分割、增强的RAN智能控制器（RICs）和可扩展部署架构。

Result: 提出的ORAN-based NTN框架能够有效解决NTN的DevOps挑战，特别是在智能化和可扩展性方面。

Conclusion: 未来研究方向包括将ORAN-based NTN框架与其他技术结合，并探索潜在的应用场景。

Abstract: As the path toward 6G networks is being charted, the emerging applications
have motivated evolutions of network architectures to realize the efficient,
reliable, and flexible wireless networks. Among the potential architectures,
the non-terrestrial network (NTN) and open radio access network (ORAN) have
received increasing interest from both academia and industry. Although the
deployment of NTNs ensures coverage, enhances spectral efficiency, and improves
the resilience of wireless networks. The high altitude and mobility of NTN
present new challenges in the development and operations (DevOps) lifecycle,
hindering intelligent and scalable network management due to the lack of native
artificial intelligence (AI) capability. With the advantages of ORAN in
disaggregation, openness, virtualization, and intelligence, several works
propose integrating ORAN principles into the NTN, focusing mainly on ORAN
deployment options based on transparent and regenerative systems. However, a
holistic view of how to effectively combine ORAN and NTN throughout the DevOps
lifecycle is still missing, especially regarding how intelligent ORAN addresses
the scalability challenges in NTN. Motivated by this, in this paper, we first
provide the background knowledge about ORAN and NTN, outline the
state-of-the-art research on ORAN for NTNs, and present the DevOps challenges
that motivate the adoption of ORAN solutions. We then propose the ORAN-based
NTN framework, discussing its features and architectures in detail. These
include the discussion about flexible fronthaul split, RAN intelligent
controllers (RICs) enhancement for distributed learning, scalable deployment
architecture, and multi-domain service management. Finally, the future research
directions, including combinations of the ORAN-based NTN framework and other
enabling technologies and schemes, as well as the candidate use cases, are
highlighted.

</details>


### [34] [FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks](https://arxiv.org/abs/2507.12265)
*Zihan Zhu,Dongchao Wu,Zhanbang Zhang,Jian Yang*

Main category: cs.NI

TL;DR: 提出了一种集中式调度算法，支持动态调度，适用于光交换机数据中心网络，实现理论最大吞吐量且重排次数最少。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心网络中光交换机因无缓冲和长切换时间导致的动态调度难题。

Method: 采用替换链概念和位集优化，设计高效集中式调度算法。

Result: 算法在双向Clos网络中实现最大吞吐量，重排次数少，运行时间显著优于其他算法。

Conclusion: 该算法灵活高效，适用于实际环境，支持动态调度需求。

Abstract: Ever since Clos topologies were used in datacenter networks (DCNs), a
practical centralized scheduling algorithm that supports dynamic scheduling has
been absent. The introduction of optical switches in DCNs as a future-proof
solution exacerbates this problem due to several properties of optical
switches, such as the fact that they are generally bufferless and therefore
rely on centralized scheduling, and that they have long switching times and
therefore require the number of rearrangements to be minimized.
  In this paper, we propose a centralized scheduling algorithm that achieves
theoretical maximum throughput even in one-rate bidirectional Clos networks,
while producing schemes with near-minimal numbers of rearrangements. It is the
only algorithm that directly supports bidirectional Clos networks and has a
time efficiency high enough to support dynamic scheduling to date. For static
minimal rewiring, its running time ranges from a fraction to a few hundredths
of other algorithms, and the number of rearrangements has also been steadily
improved, allowing for more frequent adjustments and less impact on ongoing
communications. In addition, the algorithm is very flexible and can support
various functional requirements in real-world environments. We achieve this
result through the replacement chain concept and bitset optimization.

</details>


### [35] [LLM-Based Config Synthesis requires Disambiguation](https://arxiv.org/abs/2507.12443)
*Rajdeep Mondal,Nikolaj Bjorner,Todd Millstein,Alan Tang,George Varghese*

Main category: cs.NI

TL;DR: 论文探讨了LLM在程序合成中的意图模糊问题，提出原型系统Clarify，通过Disambiguator模块解决模糊性。


<details>
  <summary>Details</summary>
Motivation: 用户意图模糊是LLM程序合成的另一大问题，尤其在网络配置合成中，如路由映射和ACL，其优先级难以推断。

Method: 提出Clarify系统，结合LLM与Disambiguator模块，通过用户交互解决模糊性，并验证合成策略。

Result: 实验显示复杂ACL中存在大量重叠，模糊性真实存在；Clarify在小规模任务中成功合成路由策略。

Conclusion: Clarify的方法不仅适用于网络配置，还可推广到其他LLM合成任务中，解决意图模糊问题。

Abstract: Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.

</details>


### [36] [CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments](https://arxiv.org/abs/2507.12445)
*Soheil Mahdizadeh,Amir Mahdi Rasouli,Mohammad Pourashory,Sadra Galavani,Mohsen Ansari*

Main category: cs.NI

TL;DR: 本文提出了一种基于遗传算法的边缘-雾节点部署策略，旨在降低物联网中的延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 云计算无法满足物联网的实时需求，边缘和雾计算通过将计算节点靠近用户来提供更低延迟和更高处理能力。

Method: 采用遗传算法优化边缘和雾节点的部署策略。

Result: 仿真结果显示，该框架可降低2.77%的延迟和31.15%的成本。

Conclusion: 基于遗传算法的节点部署策略有效优化了延迟和成本。

Abstract: Reducing latency in the Internet of Things (IoT) is a critical concern. While
cloud computing facilitates communication, it falls short of meeting real-time
requirements reliably. Edge and fog computing have emerged as viable solutions
by positioning computing nodes closer to end users, offering lower latency and
increased processing power. An edge-fog framework comprises various components,
including edge and fog nodes, whose strategic placement is crucial as it
directly impacts latency and system cost. This paper presents an effective and
tunable node placement strategy based on a genetic algorithm to address the
optimization problem of deploying edge and fog nodes. The main objective is to
minimize latency and cost through optimal node placement. Simulation results
demonstrate that the proposed framework achieves up to 2.77% latency and 31.15%
cost reduction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Main category: cs.LG

TL;DR: 提出了一种名为RUGNN的图神经网络替代模型，用于预测材料成形过程中的变形场，解决了传统AI模型在捕捉3D空间关系和排列不变性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于标量或图像的AI替代模型难以捕捉复杂的3D空间关系和排列不变性，因此需要开发更高效的图神经网络模型。

Method: 开发了RUGNN模型，结合门控循环单元（GRUs）建模时间动态，并采用U-Net启发的图上下采样机制处理空间长程依赖。提出了一种新的‘节点到表面’接触表示方法。

Result: RUGNN在冷成形和热成形案例中表现出色，预测结果与真实有限元模拟高度吻合，优于其他基线GNN架构。

Conclusion: RUGNN是一种可靠的方法，能够支持板材成形设计，提供准确的制造性预测。

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [38] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: 开发并评估了用于预测脊柱手术住院时间的机器学习模型，重点关注时间建模和模型可解释性。SurgeryLSTM模型表现最佳，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 提高脊柱手术住院时间预测的准确性和可解释性，以支持临床决策。

Method: 比较传统ML模型与SurgeryLSTM（带注意力的BiLSTM），使用电子健康记录数据，评估R2和关键预测因子。

Result: SurgeryLSTM预测精度最高（R2=0.86），注意力机制提高了可解释性，关键预测因子包括骨病、慢性肾病和腰椎融合。

Conclusion: SurgeryLSTM为脊柱手术住院时间预测提供了高效且可解释的AI解决方案，支持临床决策系统的整合。

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [39] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: CMCO框架通过结合蒙特卡洛dropout和分形预测，为神经算子提供无需重新训练或定制损失设计的分布无关预测区间，实现高效可靠的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在实时虚拟传感中不确定性量化（UQ）的挑战，特别是在高风险领域，如稀疏、噪声或非共位传感器数据的环境。

Method: 提出Conformalized Monte Carlo Operator（CMCO），将蒙特卡洛dropout与分形预测结合在DeepONet架构中，实现空间分辨的不确定性估计。

Result: 在湍流、弹塑性变形和全球宇宙辐射剂量估计三个应用中，CMCO均实现了接近名义的覆盖范围，即使在强空间梯度和代理传感环境中。

Conclusion: CMCO为神经算子提供了一种通用、即插即用的UQ解决方案，为可扩展、泛化性强且具有不确定性意识的科学机器学习奠定了基础。

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [40] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Main category: cs.LG

TL;DR: Einstein Fields是一种神经表示方法，用于压缩计算密集型四维数值相对论模拟为紧凑的隐式神经网络权重。通过建模广义相对论的核心张量场（度量），Einstein Fields能够通过自动微分推导物理量。


<details>
  <summary>Details</summary>
Motivation: 传统数值相对论模拟计算成本高，Einstein Fields旨在提供一种更高效、可扩展的解决方案。

Method: 提出Neural Tensor Fields（神经张量场），将时空几何编码为神经网络表示，动态自然涌现。

Result: Einstein Fields在4D时空连续建模、存储效率、导数精度等方面表现出潜力，并开源了JAX库。

Conclusion: Einstein Fields为数值相对论提供了更具表达力和可扩展性的方法。

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [41] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Main category: cs.LG

TL;DR: 本文综述了合成表格数据生成的最新进展，重点介绍了方法、隐私保护和数据实用性，并提出了一种新的分类法和基准框架。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规的严格化和真实数据获取受限，合成数据生成成为关键解决方案，尤其是在金融、医疗和社会科学等领域。

Method: 综述了合成表格数据生成的方法，强调保留复杂特征关系、统计保真和隐私保护，并提出新的分类法和基准框架。

Result: 提出了基于实际生成目标的分类法，并设计了基准框架以将技术创新与实际需求对齐。

Conclusion: 本文为未来研究提供了路线图，并为在隐私关键环境中实施合成表格数据提供了指导。

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [42] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Main category: cs.LG

TL;DR: 论文提出了一种基于张量表示和稀疏自编码器的方法，用于处理不规则事件时间序列，并在X射线天文学数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 事件时间序列在多个领域普遍存在，但其不规则性使得传统方法难以提取有意义模式，因此需要新的解决方案。

Method: 采用二维和三维张量表示事件时间序列，并结合稀疏自编码器学习物理意义明确的潜在表示。

Result: 该方法成功捕获了X射线瞬变的时间与频谱特征，并支持多种下游任务。

Conclusion: 该框架为跨领域的不规则事件时间序列分析提供了灵活、可扩展且通用的解决方案。

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [43] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: 提出了一种基于零知识证明（ZKP）的联邦学习隐私保护评估协议，避免通过性能指标泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的评估阶段可能通过共享的性能指标泄露敏感信息，需要一种隐私保护且可验证的评估方法。

Method: 使用零知识证明（ZKPs）生成简洁证明，断言本地损失低于预定义阈值，避免暴露原始损失值。

Result: 在MNIST和HAR数据集上实现了该方法，评估了计算开销、通信成本和可验证性。

Conclusion: 提出的协议在保护隐私的同时实现了可验证的联邦学习评估。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [44] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 研究了五种深度生成模型在工业轮胎架构生成中的表现，发现扩散模型整体表现最佳，尤其在条件生成任务中。


<details>
  <summary>Details</summary>
Motivation: 解决工业实践中关于哪种深度生成模型最适合复杂制造设计任务的问题。

Method: 评估了五种模型（VAE、GAN、MMVAE、DDPM、MDM）在三种工业场景下的表现，并提出了分类修复方法以支持条件生成。

Result: 扩散模型整体表现最佳，MDM在分布内任务中领先，DDPM在分布外任务中表现更好。

Conclusion: 扩散模型在工业设计任务中具有优势，尤其是通过分类修复方法支持的条件生成。

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [45] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Main category: cs.LG

TL;DR: 本文提出了几种实用指标（如Dropout鲁棒性曲线、稀疏性等）来预测神经网络中的“grokking”行为，并揭示了其起源和动态。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中“grokking”现象（即测试精度延迟提升）的预测和理解。

Method: 通过Dropout鲁棒性曲线（DRC）、神经元稀疏性、嵌入相似性等指标分析模型从记忆到泛化的转变过程。

Result: 发现测试精度在“grokking”期间出现局部最大值，神经元活动增加，嵌入分布趋向双峰且与数据集对称性相关。

Conclusion: 提出的指标能有效预测“grokking”行为，并为其动态和起源提供了新见解。

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [46] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 提出了一种结合ABM与深度学习的STAGED方法，用于建模细胞间通讯及其对细胞内基因调控网络的影响。


<details>
  <summary>Details</summary>
Motivation: 单细胞技术虽能解析细胞状态，但传统方法将细胞视为独立点，缺乏对动态细胞间交互的建模。

Method: 结合ABM与深度学习，使用图ODE网络（GDEs）动态学习基因间交互强度。

Result: 模型能捕捉细胞间和细胞内交互，更准确地表示细胞动态。

Conclusion: STAGED为复杂细胞动态提供了一种自适应且数据驱动的建模方法。

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [47] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: 论文提出了一种多轮强化学习（RL）方法Kevin，用于生成和优化CUDA内核，显著提升了正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 编写高效的GPU内核是AI系统性能的关键，但过程复杂且迭代性强，适合应用强化学习。

Method: 开发了一种多轮RL训练方法，解决了长轨迹学习和跨轮奖励分配等挑战。

Result: Kevin在正确性（56%到82%）和性能（0.53x到1.10x）上显著优于基准模型，并超越前沿模型。

Conclusion: 多轮RL方法在CUDA内核优化中表现优异，尤其是串行细化比并行采样更有效。

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [48] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Main category: cs.LG

TL;DR: 论文研究了线性层中的几何基元（如双向量）如何组合成更复杂的功能模块，并提出了一种基于Clifford代数的可微分算法，将线性层分解为旋转器的乘积，参数效率更高。


<details>
  <summary>Details</summary>
Motivation: 探索大型模型中低层次基元如何组合成功能丰富的模块，尤其是线性层中的几何基元。

Method: 使用Clifford代数将线性层表示为双向量的组合，并开发可微分算法将其分解为旋转器的乘积。

Result: 提出的旋转器层在LLM注意力层中表现与强基线（如块Hadamard和低秩近似）相当，且参数效率更高（O(log²d) vs O(d²)）。

Conclusion: 研究为理解几何基元如何在深度模型中组合成高级功能提供了代数视角。

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [49] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: 本文研究了数据集缩减方法（如核心集选择）对模型偏见的影响，发现某些方法可能无意中加剧偏见，而嵌入表征方法风险较低，但难样本优先策略不一定能保证模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 许多数据集存在偏见，导致模型学习虚假相关性而非因果特征，需了解数据集缩减方法如何影响这些偏见。

Method: 在十个虚假相关性基准上，使用五种评分指标和五种数据选择策略，分析核心集选择对偏见水平和模型鲁棒性的影响。

Result: 嵌入表征方法加剧偏见风险较低，难样本优先策略虽可能降低偏见，但不保证模型鲁棒性。

Conclusion: 核心集选择方法需谨慎设计，以避免加剧偏见并确保模型鲁棒性。

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [50] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Main category: cs.LG

TL;DR: 英国铁路因落叶导致交通中断，每年损失超3亿英镑。本研究利用LSTM网络和卫星数据预测落叶时间，误差较小，为铁路行业提供了优化措施的机会。


<details>
  <summary>Details</summary>
Motivation: 落叶导致铁路交通中断，每年造成巨大经济损失，现有预测方法在可扩展性和可靠性上存在局限，亟需改进。

Method: 结合地面真实落叶数据、多光谱和气象卫星数据，训练LSTM网络预测落叶时间。

Result: 模型预测落叶开始和结束时间的均方根误差分别为6.32天和9.31天，优于先前研究。

Conclusion: 该模型为铁路行业提供了可靠的落叶预测工具，同时有助于理解复杂生态系统。

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [51] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的MDP框架（PbMDPs），专注于Borda分数下的对抗性偏好设置，并提出了相应的遗憾下界和优化算法。


<details>
  <summary>Details</summary>
Motivation: 研究在对抗性偏好环境下，如何通过观察偏好而非直接数值损失来优化决策过程，填补了标准MDP与偏好学习之间的空白。

Method: 通过在线线性优化和策略优化两种方法，分别针对已知和未知转移概率的情况，设计了高效的算法。

Result: 证明了PbMDPs的遗憾下界为Ω((H²SK)^(1/3)T^(2/3))，并提出了两种算法分别达到Õ((H²S²K)^(1/3)T^(2/3))和Õ((H⁶SK⁵)^(1/3)T^(2/3))的遗憾上界。

Conclusion: 论文为偏好学习的MDP问题提供了理论框架和实用算法，为未来研究奠定了基础。

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [52] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Main category: cs.LG

TL;DR: FLEX是一个图生成模型框架，通过结构条件生成和对抗性协同训练，提升图神经网络在分布外场景下的链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在链接预测任务中表现良好，但依赖同分布数据；图生成模型虽能生成新图，但应用受限。FLEX旨在弥合这一差距。

Method: FLEX结合结构条件生成和对抗性协同训练（自动编码器与图神经网络），确保样本分布的结构对齐。

Result: 实验证明FLEX在合成和真实世界分布外场景中能显著提升性能，并分析了图数据增强对链接结构的影响。

Conclusion: FLEX无需专家知识即可适应不同分布外场景，为图生成模型的应用提供了新方向。

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [53] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Main category: cs.LG

TL;DR: 本文研究了电力传输网络中全局负载预测的优势，特别是在数据漂移和异构性背景下，比较了特征转换和目标转换模型的性能，并提出了时间序列聚类方法以优化预测效果。


<details>
  <summary>Details</summary>
Motivation: 传统局部预测模型在泛化性、过拟合、数据漂移和冷启动问题上表现不佳，而全局预测模型通过全局化和跨学习提供了更好的解决方案。

Method: 研究了特征转换和目标转换模型，提出了基于模型和加权实例的时间序列聚类方法，并在真实数据集上进行了实验。

Result: 全局目标转换模型在全局特征和聚类技术的支持下表现优于局部模型，而全局特征转换模型需要时间序列聚类来有效管理数据异构性。

Conclusion: 全局目标转换模型在负载预测中更具优势，尤其是在处理数据异构性和漂移时，而特征转换模型需要额外技术来平衡局部和全局动态。

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [54] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Main category: cs.LG

TL;DR: 论文提出了一种基于统计方法的图编码嵌入（GEE）来生成高质量的初始节点特征，结合GNN形成GG框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: GNN在节点级任务中表现优异，但初始特征表示通常随机或缺乏信息，导致收敛慢和次优解。

Method: 利用GEE生成初始节点特征，结合GNN形成GG框架，并进一步提出GG-C变体用于节点分类。

Result: GG在节点聚类中表现最优，收敛更快；GG-C在节点分类中优于基线方法。

Conclusion: 结构感知的特征初始化对发挥GNN潜力至关重要。

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [55] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Main category: cs.LG

TL;DR: 本文探讨了将Conformal Prediction框架与Ensemble-SINDy（E-SINDy）结合，以量化SINDy模型中的不确定性，并在时间序列预测、模型选择和系数不确定性方面展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化SINDy模型中的不确定性对于评估其可靠性至关重要，尤其是在安全关键应用中。

Method: 通过Conformal Prediction框架与E-SINDy结合，提出了三种应用：时间序列预测的不确定性量化、基于特征重要性的模型选择以及模型系数的不确定性量化。

Result: 实验表明，该方法在时间序列预测中能可靠地达到目标覆盖率，有效量化特征重要性，并在非高斯噪声下生成更稳健的系数不确定性区间。

Conclusion: Conformal Prediction与E-SINDy的结合能够显著提升SINDy模型的不确定性量化能力，适用于复杂非线性动态系统。

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [56] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Main category: cs.LG

TL;DR: 提出了一种名为Graph-in-Graph (GiG)的新框架，结合转导学习和归纳学习，显著提升了药物-靶标相互作用预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合药物、靶标及其相互作用的多样化特征方面存在困难，限制了预测性能。

Method: 引入GiG模型，将药物和靶标分子结构图表示为药物-靶标相互作用图中的元节点，以探索其复杂关系。

Result: GiG模型在所有评估指标上显著优于现有方法。

Conclusion: 结合不同学习范式和相互作用数据可显著提升预测性能。

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [57] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: Torsional-GFN是一种基于GFlowNet的生成模型，用于高效采样分子构象，近似玻尔兹曼分布，并实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 分子构象的稳定生成对药物发现至关重要，传统方法如分子动力学效率较低，需要更高效的生成方法。

Method: 提出Torsional-GFN，一种条件GFlowNet，通过奖励函数训练，采样分子扭转角旋转，生成构象。

Result: Torsional-GFN能近似玻尔兹曼分布采样构象，并实现零样本泛化到未见过的键长和键角。

Conclusion: 该方法为扩展到大分子系统和零样本泛化提供了有前景的途径。

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [58] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Main category: cs.LG

TL;DR: 本文研究了对比激活加法（CAA）在不同规模的Llama 2模型（7B、13B、70B）中的有效性，发现CAA在早期至中期层最有效，但随着模型规模增大效果减弱，且负向调控比正向调控效果更显著。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的复杂性和能力提升，较少广泛部署的对齐技术的有效性尚不确定。本文旨在探索CAA在不同模型规模下的表现。

Method: 通过对比对（如仇恨到爱）在模型的残差流向量空间中寻找理想方向，并在前向传播时将该方向添加到残差流中，直接操控残差流以提取特征并控制模型输出。

Result: 研究发现：1）CAA在早期至中期层最有效；2）CAA的效果随模型规模增大而减弱；3）负向调控在所有模型规模中效果更显著。

Conclusion: CAA的有效性受模型规模和调控方向影响，负向调控更具潜力，但需进一步研究其在更大模型中的应用。

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [59] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Main category: cs.LG

TL;DR: 研究使用XGBoost分类器结合拓扑特征预测荷兰铁路网络延误，发现现有方法性能有限，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 荷兰铁路网络是全球最繁忙的之一，延误问题突出，现有研究多关注短期预测，忽视网络级模式。

Method: 改进现有方法，整合节点中心性度量，比较多种分类器（如RandomForest、DecisionTree等）预测延误轨迹。

Result: 结果显示性能有限，尤其在非同步测试场景中，需更多上下文适配。

Conclusion: 研究为交通网络评估提供见解，建议未来开发更鲁棒的延误预测模型。

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [60] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Main category: cs.LG

TL;DR: FlatVI是一种新的训练框架，通过正则化离散似然变分自编码器的潜在流形，使其更适合单细胞计数数据的建模，提升下游任务的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单细胞RNA测序中常假设线性转移和欧几里得几何，但线性插值可能不符合数据流形上的测地线路径，限制了方法的有效性。

Method: FlatVI通过正则化潜在流形，使其更接近欧几里得几何，从而优化潜在空间中的直线插值。

Result: 在合成数据和实际单细胞RNA测序数据上的实验表明，FlatVI提升了轨迹重建和流形插值的性能。

Conclusion: FlatVI通过优化潜在流形的几何特性，显著提升了单细胞数据建模的准确性和下游任务的适用性。

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [61] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 论文提出了一种无需依赖干净标注数据的元学习方法CLID-MU，用于解决噪声标签学习问题。通过利用数据结构的跨层一致性，该方法在合成和真实噪声场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有元学习方法依赖干净标注的元数据集，但实际中难以获取。本文旨在解决噪声标签场景下的元学习问题，无需干净标注数据。

Method: 提出CLID-MU策略，利用干净样本在隐藏层和输出层的数据结构一致性，通过跨层信息差异评估模型性能并指导训练。

Result: 在合成和真实噪声的基准数据集上，CLID-MU优于现有方法。

Conclusion: CLID-MU为噪声标签学习提供了一种有效的元学习方法，无需依赖干净标注数据。

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [62] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: SynCoGen是一个结合掩码图扩散和流匹配的框架，用于生成可合成的3D分子，并在无条件分子图和构象生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决生成小分子设计中可合成性的挑战，尤其是基于几何的条件生成。

Method: 结合掩码图扩散和流匹配，从分子构建块、化学反应和原子坐标的联合分布中采样。

Result: 在无条件分子图和构象生成中达到最先进水平，并在零射击分子连接设计中表现优异。

Conclusion: SynCoGen为未来非自回归分子生成的应用奠定了基础。

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [63] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen是一个自动化、模块化的框架，用于生成特定领域的MNIST风格数据集，结合CLIP语义理解和强化学习，支持复杂分类结构，显著节省时间和提高准确性。


<details>
  <summary>Details</summary>
Motivation: 标准数据集（如MNIST）对领域特定任务不适用，而自定义数据集创建耗时且复杂。MNIST-Gen旨在解决这一问题。

Method: 结合CLIP语义理解、强化学习和人类反馈，采用分层语义分类和可组合的形态建模。

Result: 生成Tree-MNIST和Food-MNIST数据集，自动分类准确率达85%，节省80%时间。

Conclusion: MNIST-Gen为领域特定任务提供了高效的数据生成解决方案，具有模块化和扩展性优势。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [64] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Main category: cs.LG

TL;DR: 论文提出HyperEvent框架，将动态链接预测重构为超事件识别，通过事件关联向量量化历史事件与查询事件的依赖关系，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能捕捉复合超事件的结构内聚性，限制了动态图建模能力。

Method: 动态构建关联序列，利用事件关联向量量化历史事件与查询事件的依赖关系，预测超事件形成。

Result: 在5个数据集中4个表现最优，大规模Flight数据集上MRR提升6.95%，训练时间仅需10.17%。

Conclusion: HyperEvent框架在准确性和效率上均优于现有方法，适用于大规模动态图建模。

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [65] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Main category: cs.LG

TL;DR: Protenix-Mini 是一个轻量化的蛋白质结构预测模型，通过优化架构和采样策略，显著降低计算开销，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决在生物分子结构预测中平衡模型效率和预测准确性的挑战。

Method: 1) 用少步 ODE 采样器替代多步 AF3 采样器；2) 剪枝冗余的 Transformer 模块；3) 用 ESM 模块替代传统的 MSA 模块。

Result: Protenix-Mini 在基准测试中仅损失 1-5% 的精度，显著降低了计算复杂度。

Conclusion: Protenix-Mini 适用于计算资源有限但需要高精度预测的场景。

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [66] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种联合高效的广义线性老虎机算法，实现近乎最优的遗憾边界，且每轮时间和空间复杂度为O(1)。


<details>
  <summary>Details</summary>
Motivation: 广义线性老虎机（GLB）问题因其非线性特性在计算和统计效率上存在挑战，现有方法通常需在两者间权衡。

Method: 通过在线镜像下降（OMD）估计器构建紧密置信集，利用混合损失的新颖分析实现统计效率。

Result: 算法在每轮O(1)复杂度下达到接近最优的遗憾边界，统计效率与最大似然估计相当。

Conclusion: 提出的方法在计算和统计效率上均表现优异，为GLB问题提供了实用的解决方案。

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [67] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Main category: cs.LG

TL;DR: OrdShap是一种新的特征归因方法，通过量化特征位置对模型预测的影响，解决了现有方法中特征值与位置混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法假设特征顺序固定，无法区分特征值与其位置对模型预测的影响。

Method: 提出OrdShap方法，通过置换特征位置来量化其对预测的影响，并建立与Sanchez-Bergantiños值的博弈论联系。

Result: 在健康、自然语言和合成数据集上的实验表明，OrdShap能有效捕捉特征值和位置的影响，提供更深入的模型行为洞察。

Conclusion: OrdShap为理解序列深度学习模型的预测提供了更全面的特征归因方法。

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [68] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Main category: cs.LG

TL;DR: 研究提出了一种动态管理司机接受折扣快车服务的策略，采用改进的深度确定性策略梯度框架（pi-DDPG），以提高学习效率和减少早期训练损失。


<details>
  <summary>Details</summary>
Motivation: 平台整合虽能缓解市场碎片化，但折扣快车服务可能降低利润。个体平台需动态管理司机参与，但缺乏历史数据和早期探索成本高是挑战。

Method: 提出pi-DDPG框架，包含优化模块、卷积LSTM网络和优先级经验回放机制，以应对高随机性和数据不足。

Result: 基于真实数据集的模拟验证显示，pi-DDPG显著提高学习效率并减少早期训练损失。

Conclusion: pi-DDPG框架有效解决了动态管理司机参与折扣快车服务的挑战，为平台提供了实用解决方案。

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [69] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 提出Meta-IR框架，通过元学习推荐最佳的不平衡回归任务处理流程，优于传统方法和AutoML框架。


<details>
  <summary>Details</summary>
Motivation: 不平衡问题在回归任务中同样存在，但现有方法需要测试多种组合，效率低下。

Method: 提出Meta-IR框架，训练元分类器以零样本方式推荐最佳流程（包括重采样策略和学习模型）。

Result: Chained版本表现更优，优于42种传统配置和AutoML框架。

Conclusion: Meta-IR通过元学习有效解决了不平衡回归任务中的流程选择问题。

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [70] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 本文研究了不平衡回归问题，提出了基于回归模型、学习过程和评估指标的分类法，并通过实验验证了不同平衡策略的效果。


<details>
  <summary>Details</summary>
Motivation: 不平衡问题在分类任务中已有研究，但在回归任务中较少被关注。本文旨在填补这一空白，探讨不平衡回归的解决方案。

Method: 通过实验研究多种平衡和预测模型，并提出了基于回归模型、学习过程和评估指标的分类法。

Result: 研究提供了不平衡回归策略的新见解，展示了不同模型在学习过程中的优势。

Conclusion: 本文为不平衡回归问题提供了分类法和实验支持，并指出了未来研究方向。

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [71] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 论文研究了在强化学习中实现可复现学习的样本效率问题，提出了一种在低水平表格MDP中几乎最优的算法。


<details>
  <summary>Details</summary>
Motivation: 解决可复现学习在控制设置（如强化学习）中的样本效率问题，探索是否可复现探索比批量学习更昂贵。

Method: 提出了一种可复现的强化学习算法，样本复杂度为$	ilde{O}(S^2A)$，填补了生成模型和情景设置之间的差距。

Result: 算法在$	ilde{O}(S^2A)$样本下实现可复现学习，并提供了匹配的下界证明其近最优性。

Conclusion: 探索不是可复现学习的显著障碍，样本高效的复现强化学习是可能的。

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [72] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Main category: cs.LG

TL;DR: 论文提出了一种机器学习加速的优化框架，用于RF功率放大器设计，减少65%的仿真需求，同时保持±0.3至±0.4 dBm的精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要全面仿真所有参数组合以满足P2dB压缩规格，耗时且低效。

Method: 结合MaxMin拉丁超立方采样和CatBoost梯度提升，智能探索多维参数空间，仅选择35%的关键仿真点。

Result: 在15种PA工作模式下验证，平均R²为0.901，仿真时间减少58.24%至77.78%。

Conclusion: 该框架通过自动化GUI工作流实现快速设计迭代，且不牺牲生产RF电路所需的精度标准。

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [73] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Main category: cs.LG

TL;DR: 论文提出了一种在强化学习（RL）中结合训练与剪枝的方法，通过优化网络权重和变分伯努利分布参数，实现高效网络压缩，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习网络的特征提取网络虽能提升性能，但计算和内存复杂度高。现有剪枝方法在监督学习中有效，但在RL中应用不足。

Method: 提出XiNet，通过随机优化问题训练网络权重和变分伯努利分布参数，结合成本感知的稀疏正则化方案，自动选择超参数。

Result: 在MuJoCo连续控制基准测试中，OFENet剪枝后性能损失极小，且训练时剪枝比从头训练小网络更高效。

Conclusion: 该方法有效结合RL目标和网络压缩，显著提升RL代理的效率和性能。

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [74] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Main category: cs.LG

TL;DR: MLED框架利用LLMs从文本信息中提取外部知识，通过多级增强器提升图欺诈检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图欺诈检测方法忽略原始文本信息中的丰富语义线索，且难以将LLMs处理的文本嵌入与图结构进行多模态融合。

Method: 设计多级LLM增强框架（类型级和关系级增强器），分别增强欺诈者与良性实体的差异及欺诈者在不同关系中的重要性。

Result: 在四个真实数据集上，MLED作为通用框架实现了最先进的性能。

Conclusion: MLED通过结合LLMs与图结构信息，显著提升了图欺诈检测的效果。

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [75] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Main category: cs.LG

TL;DR: 论文提出了一种利用智能手表的多模态数据（音频和惯性）检测面对面对话的计算方法，并在实验室和半自然环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交互动对人类行为和社会关系至关重要，但现有方法在复杂声学环境中检测对话存在挑战。

Method: 结合音频和惯性数据，采用机器学习和深度学习模型，并通过三种融合方法分析数据。

Result: 在实验室和半自然环境中，模型的宏F1分数分别达到82.0±3.0%和77.2±1.8%。

Conclusion: 多模态传感在特定情境下具有优势，融合音频和惯性数据能更全面地捕捉对话中的语言和非语言线索。

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [76] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: 论文提出了一种名为DUSE的数据扩展框架，通过不确定性评分函数和主动学习策略解决自动调制识别（AMR）中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决AMR领域中因数据稀缺导致模型训练困难的问题，避免高成本的人工标注和数据增强的局限性。

Method: 使用不确定性评分函数筛选有用样本，并结合主动学习策略动态优化评分器。

Result: DUSE在类别平衡和不平衡设置下均优于8种基线方法，并展现出对未见模型的强泛化能力。

Conclusion: DUSE是一种有效的数据扩展框架，能够显著提升AMR模型的性能，尤其在数据稀缺场景下表现优异。

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [77] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了在有限个体反馈下如何更准确地预测群体反馈分布，发现反馈粒度越高，复杂方法比简单正则化平均更有效。


<details>
  <summary>Details</summary>
Motivation: 研究如何在成本限制下，利用有限个体反馈更准确地预测群体反馈分布，尤其是高粒度反馈的优势。

Method: 比较正则化平均与更复杂的方法在不同反馈粒度（如二值和五点反馈）下的表现。

Result: 五点反馈下，复杂方法仅需正则化平均一半的个体即可达到相同性能；二值反馈下差异不明显。

Conclusion: 反馈粒度决定了方法的有效性，高粒度反馈下复杂方法显著优于简单方法。

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [78] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，用于分析基于回放的持续学习方法，通过信息论界限揭示了内存缓冲区与当前任务的交互如何影响泛化性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）在避免灾难性遗忘的同时从顺序任务中获取知识，但现有方法缺乏理论支持，尤其是基于回放的方法。

Method: 建立了一个统一的理论框架，推导了一系列信息论界限，分析了内存缓冲区与当前任务的交互对泛化的影响。

Result: 研究表明，有限的历史任务样本与当前任务数据结合使用，而非完全回放，能改善泛化并减少遗忘。预测界限通过低维变量提供了更紧且可计算的上界。

Conclusion: 理论分析和实验验证表明，所提出的界限能有效捕捉基于回放的持续学习中的泛化动态。

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [79] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Main category: cs.LG

TL;DR: 提出了一种基于动态区域和土地用途的新型数据驱动方法，用于生成城市模拟场景中的出行流，结合cGANs和自适应参数，无需大量校准数据。


<details>
  <summary>Details</summary>
Motivation: 城市规划和交通优化需要模拟人类出行模式，但现有方法依赖历史数据或假设静态场景，无法适应动态变化。

Method: 利用动态区域大小和土地用途类型，结合cGANs生成出行流，支持可调空间粒度。

Result: 在新加坡手机数据上验证了方法的有效性，性能优于现有方法。

Conclusion: 该方法为动态城市模拟提供了一种高效、灵活的解决方案。

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [80] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Main category: cs.LG

TL;DR: 本文提出了一种基于Spotlight Resonance的新方法，用于确定表征对齐，发现网络原语的代数对称性是任务无关表征结构的强预测因子。通过改变激活函数的研究，揭示了离散表征的形成机制。


<details>
  <summary>Details</summary>
Motivation: 研究功能形式选择如何无意中引入归纳偏差，导致任务无关的表征结构，特别是当代形式如何诱导离散化。

Method: 使用改进的Spotlight Resonance方法，通过改变激活函数进行消融研究，分析离散和连续表征的形成。

Result: 发现离散代数置换等变对称性激活函数导致表征离散化，而连续代数正交等变定义则保持连续性。量化表征与重建误差增加相关。

Conclusion: 功能形式对表征有显著影响，离散化可能对下游可解释性现象（如祖母神经元）有重要意义，但可能增加重建误差。

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [81] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Main category: cs.LG

TL;DR: 论文提出了一种衡量预测模型在决策任务中‘有用性’的新框架，称为‘信息差距’，并给出了其双重表征和自然信息度量。


<details>
  <summary>Details</summary>
Motivation: 决策者常需在多个可能不准确的预测模型中选择，需要一种衡量模型在任务中实际‘有用性’的方法。

Method: 引入‘信息差距’概念，定义了两个预测器在所有决策任务中的最大标准化收益优势，并提出了双重表征和一种类似EMD的信息度量方法。

Result: 该框架统一了现有概念（如U-Calibration和Blackwell信息性），并证明了新度量的完备性、有效性及样本高效估计性。

Conclusion: 提出的信息差距框架为模型选择提供了理论基础，并展示了其在理论和实践中的潜力。

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [82] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦学习的半监督频谱感知方法，解决了标签数据稀缺和安全性问题，并通过实验验证了其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无线设备激增导致频谱稀缺，动态频谱分配（DSA）成为关键解决方案，但传统机器学习方法因隐私和带宽问题受限。联邦学习（FL）提供了分布式替代方案，但在频谱感知中面临标签数据稀缺和安全漏洞的挑战。

Method: 采用半监督联邦学习方法结合能量检测，解决标签数据稀缺问题；提出基于疫苗接种启发的防御机制，对抗数据投毒攻击。

Result: 实验表明，该方法在无标签数据集上接近完美准确率，并对抗恶意参与者的数据投毒攻击表现出鲁棒性。

Conclusion: 半监督联邦学习方法有效解决了频谱感知中的标签数据稀缺和安全问题，为动态频谱分配提供了可行方案。

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [83] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Main category: cs.LG

TL;DR: HyDRA是一种混合双模射频架构，结合优化的VMD和新型CNN、Transformer与Mamba组件，支持闭集和开集分类任务，实现高效无线设备识别。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统中的设备识别对安全至关重要，RFFI提供了一种非加密解决方案。

Method: HyDRA整合优化的VMD和CNN、Transformer、Mamba组件，采用TDSE和MLFE处理信号，适应不同条件。

Result: 在公开数据集上实现SOTA精度，并在开集分类中表现稳健，部署后实现毫秒级推理速度和低功耗。

Conclusion: HyDRA为实时无线认证提供了一种实用解决方案。

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [84] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了一种基于流形优化的LoRA改进方法RiemannLoRA，解决了低秩矩阵分解中的过参数化和初始化问题，提升了收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中广泛应用，但仍面临初始化策略和过参数化的挑战。

Method: 将固定秩LoRA矩阵视为光滑流形，通过流形优化消除过参数化并提供初始化方向。

Result: 在LLM和扩散模型上，RiemannLoRA在收敛速度和最终性能上优于标准LoRA及其改进版本。

Conclusion: RiemannLoRA为LoRA提供了一种高效且稳定的优化框架，显著提升了性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [85] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Main category: cs.LG

TL;DR: FourCastNet 3 通过几何机器学习方法提升全球天气建模，实现高效、准确的概率集合预报，超越传统模型和扩散方法。


<details>
  <summary>Details</summary>
Motivation: 改进全球天气建模，提供更高效、准确的概率集合预报，支持气象预警系统。

Method: 采用纯卷积神经网络架构，结合球面几何特性，利用新型训练范式实现大规模并行训练。

Result: 预报速度提升8至60倍，60天内保持高概率校准和真实光谱，单GPU快速推理。

Conclusion: FourCastNet 3 是改进气象预报和预警系统的有力候选方案。

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [86] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM是一种高效的分布式Transformer推理策略，通过减少通信和计算开销，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 基础模型在边缘设备部署时面临通信和计算挑战，需要高效解决方案。

Method: 利用Segment Means近似中间特征，优化自注意力机制，设计分区感知的因果掩码。

Result: 显著减少通信开销（BERT达99.2%）和计算量（BERT达51.24%），精度损失小。

Conclusion: PRISM为资源受限环境中的基础模型部署提供了可扩展的实用方案。

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [87] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 提出了一种新的生成模型GMRF MCVAE，通过嵌入高斯马尔可夫随机场来显式建模多组件关系，提升了生成的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多组件变分自编码器简化了聚合策略，忽略了关键细节，导致生成组件间结构一致性不足。

Method: 在变分自编码器的先验和后验分布中嵌入高斯马尔可夫随机场，显式建模跨组件关系。

Result: 在合成Copula数据集、PolyMNIST基准和真实BIKED数据集上表现优异，显著提升了结构一致性。

Conclusion: GMRF MCVAE特别适用于需要建模多组件一致性的实际应用。

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [88] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: 论文提出了UrbanRadio3D数据集和RadioDiff-3D方法，用于构建高分辨率3D无线电地图，解决了现有方法在方向、时间和垂直空间上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图构建方法仅关注固定2D平面的路径损耗预测，忽略了方向、时间和垂直空间变化等关键参数，限制了其泛化能力。

Method: 通过射线追踪构建UrbanRadio3D数据集，并提出基于3D卷积的UNet和扩散模型RadioDiff-3D，支持辐射感知和非感知场景。

Result: RadioDiff-3D在UrbanRadio3D数据集上表现出色，能构建高维丰富的无线电地图。

Conclusion: 该工作为3D环境感知通信研究提供了基础数据集和基准。

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [89] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文探讨了基于Dempster-Shafer理论的证据聚类解释问题，提出了一种称为IEMM的算法，用于生成可解释的决策树解释。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常存在不确定性和不精确性，传统方法难以处理。证据聚类能解决这些问题，但其结果的解释问题尚未充分研究。

Method: 通过代表性条件，将决策树作为解释器，并引入效用函数和证据错误概念，提出IEMM算法。

Result: 在合成和真实数据上验证，93%的情况下能提供满意的解释。

Conclusion: IEMM算法为证据聚类提供了可解释且谨慎的决策树解释，适用于高风险领域。

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [90] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: TuneQn是一个工具套件，通过选择性量化和多目标优化，优化ONNX模型的性能，减少精度损失和模型大小。


<details>
  <summary>Details</summary>
Motivation: 全量化模型可能导致性能下降和部署困难，因此需要一种方法选择性量化部分层以平衡性能和资源需求。

Method: TuneQn结合选择性量化、部署、执行、性能分析和多目标优化，生成并评估不同量化配置的模型。

Result: 在四种ONNX模型上测试，TuneQn显著减少精度损失（最高54.14%）和模型大小（最高72.9%）。

Conclusion: TuneQn有效解决了选择性量化和模型优化问题，为低端硬件部署提供了实用工具。

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [91] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Main category: cs.LG

TL;DR: 论文提出了一种基于线性基函数组合的物理信息线性模型（PILM），用于解决偏微分方程（PDEs）的正反问题，并在不确定边界条件下验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法解决PDEs及其系数或边界条件的估计问题复杂且计算量大，物理信息神经网络虽受关注但缺乏解析解。PILM旨在提供一种可解析求解的框架。

Method: PILM利用线性基函数组合表示解，通过最小化PDEs、边界条件和数据的残差和来求解。模型在正反问题及不确定边界条件下进行了验证。

Result: PILM成功应用于地壳应变率估计，物理正则化与数学正则化对比显示后者在贝叶斯视角下表现更优。

Conclusion: PILM为线性正反问题、欠定系统和物理正则化提供了一种可解析求解的框架，具有实际应用潜力。

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [92] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Main category: cs.LG

TL;DR: 论文探讨了深度学习优化器不仅影响收敛速度，还影响学习解的性质，呼吁关注优化器的设计以塑造模型特性。


<details>
  <summary>Details</summary>
Motivation: 早期对DNN可行性的怀疑源于其非线性特性导致无法保证全局收敛，但实际经验表明标准训练协议下DNN表现良好。然而，社区过于关注收敛效率，忽视了优化器对学习解性质的影响。

Method: 通过分析现有优化器的行为，提出优化器应被视为编码归纳偏置和改变模型表达能力的工具。

Result: 优化器不仅影响收敛速度，还能塑造学习解的特性，因此应更关注其设计。

Conclusion: 呼吁社区理解现有优化器的偏置，并设计新的优化器以诱导特定解特性，而不仅仅是追求收敛速度。

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [93] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Main category: cs.LG

TL;DR: 提出了一种基于幂律谱特征的鲁棒因果发现方法，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索随机时间序列中的因果关系具有广泛应用，但现有方法对噪声敏感，容易导致误导性推断。

Method: 利用时间序列的幂律谱分布特征，提取因果信号，构建鲁棒的因果发现方法。

Result: 在合成基准和真实数据集上均优于现有方法，表现出鲁棒性和实用性。

Conclusion: 该方法通过幂律谱特征有效提升了因果发现的准确性和鲁棒性。

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [94] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Main category: cs.LG

TL;DR: 提出了一种使用非平稳核的高斯过程框架，通过神经网络动态调整核参数，提升了模型的表达能力和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程使用平稳核限制了模型的表达能力，难以适应复杂数据集。

Method: 将非平稳核参数建模为神经网络的输出，与高斯过程联合训练，利用链式法则计算导数。

Result: 在多个数据集上表现优于平稳模型和变分推断的层次模型，准确性和对数得分更高。

Conclusion: 该方法灵活、易于扩展，并能有效恢复非平稳参数，适用于复杂数据集。

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [95] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: 本文提出RegCL框架，通过模型合并实现多领域知识的高效整合，解决SAM模型在特定领域性能受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于适配器的一步适应方法在跨领域使用时可能导致性能下降，限制了模型的扩展性。

Method: RegCL将模型合并算法引入持续学习范式，通过优化权重合并不同领域的适配模块（如LoRA模块）。

Result: 实验表明，RegCL在多领域下游任务中表现优异，且模型大小恒定，无需存储历史数据。

Conclusion: RegCL有效整合多领域知识，适用于动态场景。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [96] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出了一种基于提示的在线持续学习方法，通过轻量级提示生成器和可训练的缩放-移位器，解决了数据隐私和参数增长问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中数据隐私约束和参数增长问题限制了现有方法的实用性，需要一种高效且合规的解决方案。

Method: 结合轻量级提示生成器、可训练的缩放-移位器、预训练模型泛化保持和硬软更新机制，提出了一种新型提示方法。

Result: 在CIFAR100、ImageNet-R、ImageNet-A和CUB数据集上表现优于现有方法，参数较少且训练效率高。

Conclusion: 该方法在性能和效率上均优于现有技术，适用于实际应用，代码已开源。

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [97] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Main category: cs.LG

TL;DR: 论文提出了一种名为Thought Purity（TP）的防御范式，以解决大型推理模型（LRMs）在链式思维（CoT）生成过程中对安全威胁的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习训练的大型推理模型（如Deepseek-R1）在推理能力上表现出色，但其对安全威胁（如后门提示攻击）的脆弱性仍是一个关键问题。

Method: TP通过三个协同组件实现防御：(1) 安全优化的数据处理流程，(2) 强化学习增强的规则约束，(3) 自适应监控指标。

Result: 该方法首次为强化学习对齐的推理系统提供了全面的防御机制，显著提升了安全性与功能性的平衡。

Conclusion: TP为下一代AI架构的安全性与功能性提供了重要解决方案。

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [98] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: 论文提出了一种名为LEOPARD的概念擦除方法，通过正交投影从文本表示中移除敏感信息，同时保留其他语义信息，以促进公平性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，确保神经模型无法从文本表示中推断敏感信息（如性别或种族）是公平性问题的关键挑战。

Method: 采用正交投影方法，通过调整投影的秩控制信息移除程度，同时保持嵌入的局部结构。

Result: LEOPARD在经典NLP基准测试中实现了最先进的离散属性非线性擦除性能，并有效减少了深度非线性分类器的偏差。

Conclusion: LEOPARD方法在移除敏感信息和促进公平性方面表现出色。

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [99] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Main category: cs.LG

TL;DR: 提出了一种基于组合复形（CCs）的新型拓扑框架，通过引入拉普拉斯算子高效计算热核，解决了高阶消息传递的计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 拓扑神经网络虽强大，但高阶消息传递计算成本高，需要一种更高效的方法。

Method: 在组合复形上引入拉普拉斯算子，计算热核作为节点描述符，支持多尺度信息和置换等变表示。

Result: 理论证明方法具有最大表达能力，能区分任意非同构组合复形；实验显示计算效率显著优于现有拓扑方法，并在分子数据集上表现优异。

Conclusion: 该方法为拓扑深度学习提供了高效且表达能力强的表示，为分子分类和性质预测开辟了新途径。

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [100] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Main category: cs.LG

TL;DR: 本文提出了比现有文献更尖锐的无限时域马尔可夫决策过程（MDP）中强化学习（RL）的样本复杂度PAC界限。通过近似原始MDP并利用状态子集构建更小的MDP，样本复杂度降低了对数因子。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于优化RL的样本复杂度，通过分析状态间的依赖关系，减少学习最优值所需的样本数量。

Method: 方法包括构建基于原始MDP状态子集的小MDP，并设计PAC-MDP算法以扩展到无限时域、无模型设置。

Result: 结果表明样本复杂度降至O(SA log A)时间步，显著优于现有工作。

Conclusion: 结论显示该方法在实验环境中显著提升了性能，验证了理论分析的优越性。

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [101] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: 提出了一种基于$MoS_2$闪存模拟CAM的软树模型硬件-软件协同设计方法，显著提升了模型的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展引发了对模型可解释性和鲁棒性的担忧，传统树模型在扩展性上存在计算成本高的问题。

Method: 采用$MoS_2$闪存模拟CAM实现软边界，结合软树模型进行高效推理。

Result: 在WDBC数据库上达到96%准确率，MNIST数据集上设备阈值变化10%时仅下降0.6%准确率。

Conclusion: 该方法为提升AI的可信度和效率提供了新的硬件解决方案。

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [102] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Main category: cs.LG

TL;DR: 论文研究了测试时扩展技术（如Best-of-N和拒绝采样）的性能如何受验证器不完美性的影响，并通过ROC曲线的几何特性精确描述了这些方法的实例级准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对验证器不完美性如何影响测试时扩展性能的理论理解，本文旨在填补这一空白。

Method: 通过分析验证器的ROC曲线的几何特性，理论推导了Best-of-N和拒绝采样的实例级准确性。

Result: 拒绝采样在固定计算量下优于Best-of-N，但在无限计算量下两者性能相同，由ROC曲线在原点附近的斜率决定。

Conclusion: 验证器的ROC曲线几何特性决定了测试时扩展技术的性能，为实际应用提供了理论指导。

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [103] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Main category: cs.LG

TL;DR: NOCTA是一种非贪婪目标成本权衡获取方法，用于在资源受限的预测任务中动态获取最具信息量的特征。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的应用中（如医疗领域），需要高效获取信息以降低成本和时间，同时考虑特征的动态变化。

Method: 提出NOCTA方法，包括非参数（NOCTA-NP）和参数（NOCTA-P）两种估计器，动态获取特征。

Result: 在合成和真实医疗数据集上，NOCTA优于现有基线方法。

Conclusion: NOCTA能有效平衡信息获取成本与预测性能，适用于动态预测任务。

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [104] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Main category: cs.LG

TL;DR: 论文提出了一种动态选择专家序列的混合专家架构（Mixture of Raytraced Experts），通过可变计算图提升预测精度，无需负载平衡机制，实验显示训练效率提升10%-40%。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构（MoE）通常对每个样本的计算量固定，限制了模型的灵活性和效率。本文旨在设计一种动态选择专家序列的方法，以提升计算效率和模型表达能力。

Method: 采用堆叠的混合专家架构，动态选择专家序列，生成可变宽度和深度的计算图。通过类似循环神经网络的训练方式迭代采样候选专家序列。

Result: 初步实验显示，该方法在减少10%-40%训练轮次的同时，保持了可比或更高的准确率。

Conclusion: 该方法为混合专家架构的研究提供了新方向，有望设计出更快、表达能力更强的模型。代码已开源。

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [105] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Main category: cs.LG

TL;DR: 论文提出了一种名为TDA的新框架，将TMLE直接嵌入神经网络参数空间，解决了现有方法在因果推断中的偏差和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络在预测方面强大，但在因果参数推断（如治疗效果或生存曲线）上缺乏有效性。现有方法要么无法保证解决高效影响函数方程，要么计算成本高。

Method: TDA通过划分模型参数并冻结大部分参数，仅更新一小部分“目标”参数，利用目标梯度迭代调整，从而消除一阶偏差并生成有效的置信区间。

Result: 在IHDP数据集和模拟生存数据上，TDA相比标准神经网络估计器和现有后处理方法，减少了偏差并提高了覆盖率。

Conclusion: TDA为复杂多参数目标的深度架构提供了一种直接、可扩展的严格因果推断途径。

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [106] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级的贝叶斯激励机制，通过经济手段防止联邦学习中的数据投毒攻击，实验证明其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的开放参与性使其容易受到数据投毒攻击，现有防御方法多为被动且计算成本高，需要一种更高效的主动防御机制。

Method: 设计了一种贝叶斯激励机制，将每轮训练建模为不完全信息的贝叶斯博弈，服务器通过验证数据集评估更新质量并发放奖励。

Result: 在MNIST和FashionMNIST的非独立同分布数据上，即使面对50%的标签翻转攻击，模型仍保持96.7%的准确率，显著优于标准FedAvg。

Conclusion: 该机制计算轻量、预算可控，易于集成到现有联邦学习框架中，为构建经济稳健的联邦学习生态系统提供了实用方案。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [107] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Main category: cs.LG

TL;DR: 提出了一种成本感知的贝叶斯优化停止规则，无需启发式调参，适用于不同评估成本，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯优化中，如何在高成本的黑盒函数评估中适时停止是一个重要问题，现有方法缺乏成本控制的理论保证。

Method: 基于Pandora's Box Gittins Index (PBGI)和log expected improvement per cost两种获取函数，提出了一种自适应停止规则。

Result: 实验证明，该停止规则与PBGI结合时，在成本调整简单遗憾指标上优于其他组合。

Conclusion: 提出的停止规则在理论和实践中均表现出色，适用于多种贝叶斯优化任务。

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [108] [Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation](https://arxiv.org/abs/2507.12427)
*Ashkan Shakarami,Azade Farshad,Yousef Yeganeh,Lorenzo Nicole,Peter Schuffler,Stefano Ghidoni,Nassir Navab*

Main category: eess.IV

TL;DR: 提出了一种基于单元的组织分割框架UTS，通过分类固定大小的32*32图块而非像素，减少标注工作量并提高计算效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 减少组织病理学分割中的标注工作量，提高计算效率，同时保持分割准确性。

Method: 采用基于单元的分割方法，引入多级视觉变换器（L-ViT）以捕捉细粒度形态和全局组织上下文。

Result: 在386,371个图块上评估，UTS优于U-Net变体和基于变换器的基线方法。

Conclusion: UTS框架在减少标注和计算成本的同时，实现了高精度的组织分割，支持临床相关任务。

Abstract: We propose UTS, a unit-based tissue segmentation framework for histopathology
that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the
segmentation unit. This approach reduces annotation effort and improves
computational efficiency without compromising accuracy. To implement this
approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits
the multi-level feature representation to capture both fine-grained morphology
and global tissue context. Trained to segment breast tissue into three
categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports
clinically relevant tasks such as tumor-stroma quantification and surgical
margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it
outperforms U-Net variants and transformer-based baselines. Code and Dataset
will be available at GitHub.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [109] [Recent results on searches with boosted Higgs bosons at CMS](https://arxiv.org/abs/2507.11977)
*Farouk Mokhtar*

Main category: hep-ex

TL;DR: CMS实验在LHC上通过增强的Higgs玻色子研究，展示了创新的重建和标记技术，以提高在高能标下的探测灵敏度。


<details>
  <summary>Details</summary>
Motivation: 研究增强的Higgs玻色子为探测高能标下的Higgs耦合和寻找标准模型之外的物理提供了独特机会。

Method: 采用创新的重建和标记技术，以提升在复杂环境中的探测能力。

Result: 展示了CMS实验在增强Higgs玻色子搜索中的最新成果。

Conclusion: 这些技术显著提高了在高能标下探测Higgs玻色子的灵敏度。

Abstract: The study of boosted Higgs bosons at the LHC provides a unique window to
probe Higgs boson couplings at high energy scales and search for signs of
physics beyond the standard model. In these proceedings, we present recent
results on boosted Higgs boson searches at the CMS experiment, highlighting
innovative reconstruction and tagging techniques that enhance sensitivity in
this challenging regime.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [110] [SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics](https://arxiv.org/abs/2507.11588)
*Suyuan Zhao,Yizhen Luo,Ganbo Yang,Yan Zhong,Hao Zhou,Zaiqing Nie*

Main category: q-bio.GN

TL;DR: SToFM是一个多尺度空间转录组基础模型，通过提取多尺度信息并利用SE(2) Transformer生成高质量细胞表示，显著提升空间转录组数据分析能力。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术（ST）能保留细胞的空间背景，但建模ST数据具有挑战性，需整合宏观组织形态、微观细胞微环境和基因表达谱等多尺度信息。

Method: SToFM通过多尺度信息提取构建ST子切片，并利用SE(2) Transformer生成细胞表示；同时构建了SToCorpus-88M预训练语料库。

Result: SToFM在多种下游任务（如组织区域语义分割和细胞类型注释）中表现优异，展示了其对ST数据的全面理解。

Conclusion: SToFM为空间转录组数据分析提供了强大的基础模型，显著提升了多尺度信息的整合能力。

Abstract: Spatial Transcriptomics (ST) technologies provide biologists with rich
insights into single-cell biology by preserving spatial context of cells.
Building foundational models for ST can significantly enhance the analysis of
vast and complex data sources, unlocking new perspectives on the intricacies of
biological tissues. However, modeling ST data is inherently challenging due to
the need to extract multi-scale information from tissue slices containing vast
numbers of cells. This process requires integrating macro-scale tissue
morphology, micro-scale cellular microenvironment, and gene-scale gene
expression profile. To address this challenge, we propose SToFM, a multi-scale
Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale
information extraction on each ST slice, to construct a set of ST sub-slices
that aggregate macro-, micro- and gene-scale information. Then an SE(2)
Transformer is used to obtain high-quality cell representations from the
sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest
high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves
outstanding performance on a variety of downstream tasks, such as tissue region
semantic segmentation and cell type annotation, demonstrating its comprehensive
understanding of ST data

</details>


### [111] [RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery](https://arxiv.org/abs/2507.11950)
*Lauren Lui,Torben Nielsen*

Main category: q-bio.GN

TL;DR: RNAMunin是一种基于机器学习的模型，仅通过基因组序列即可识别非编码RNA（ncRNA），适用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 微生物基因组的功能注释通常偏向蛋白质编码基因，而ncRNA在调控细菌和古菌生理、应激反应和代谢中起关键作用，但缺乏有效识别方法。

Method: RNAMunin基于Rfam序列训练，利用约60 Gbp的长读长宏基因组数据，无需转录组数据即可检测ncRNA。

Result: RNAMunin是目前唯一能仅凭基因组序列在大规模数据中检测ncRNA的模型，且模型小（约1M参数）、速度快。

Conclusion: RNAMunin填补了ncRNA识别领域的空白，为理解微生物的完整调控潜力提供了高效工具。

Abstract: Functional annotation of microbial genomes is often biased toward
protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs
(ncRNAs) that are critical for regulating bacterial and archaeal physiology,
stress response and metabolism. Identifying ncRNAs directly from genomic
sequence is a paramount challenge in bioinformatics and biology, essential for
understanding the complete regulatory potential of an organism. This paper
presents RNAMunin, a machine learning (ML) model that is capable of finding
ncRNAs using genomic sequence alone. It is also computationally viable for
large sequence datasets such as long read metagenomic assemblies with contigs
totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from
approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary
samples. We know of no other model that can detect ncRNAs based solely on
genomic sequence at this scale. Since RNAMunin only requires genomic sequence
as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do
not need transcriptomics data. We wrote this manuscript in a narrative style in
order to best convey how RNAMunin was developed and how it works in detail.
Unlike almost all current ML models, at approximately 1M parameters, RNAMunin
is very small and very fast.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [112] [JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs](https://arxiv.org/abs/2507.11636)
*Junyi Fan,Donald Williamson*

Main category: eess.AS

TL;DR: JSQA是一个两阶段框架，通过感知引导的对比学习预训练音频编码器，再微调用于MOS预测，显著提升了语音质量评估的性能。


<details>
  <summary>Details</summary>
Motivation: MOS预测因感知和实验设计差异存在高方差，现有方法未充分融入感知因素，导致结果不理想。

Method: 首先生成JND级别的音频对，预训练编码器以利用感知质量相似性信息；随后用NISQA数据集微调进行MOS预测。

Result: 实验表明，感知引导的预训练显著提升了模型性能，优于从头训练的模型。

Conclusion: 将感知因素融入预训练对提升SQA性能至关重要。

Abstract: Speech quality assessment (SQA) is often used to learn a mapping from a
high-dimensional input space to a scalar that represents the mean opinion score
(MOS) of the perceptual speech quality. Learning such a mapping is challenging
for many reasons, but largely because MOS exhibits high levels of inherent
variance due to perceptual and experimental-design differences. Many solutions
have been proposed, but many approaches do not properly incorporate perceptual
factors into their learning algorithms (beyond the MOS label), which could lead
to unsatisfactory results. To this end, we propose JSQA, a two-stage framework
that pretrains an audio encoder using perceptually-guided contrastive learning
on just noticeable difference (JND) pairs, followed by fine-tuning for MOS
prediction. We first generate pairs of audio data within JND levels, which are
then used to pretrain an encoder to leverage perceptual quality similarity
information and map it into an embedding space. The JND pairs come from clean
LibriSpeech utterances that are mixed with background noise from CHiME-3, at
different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with
audio samples from the NISQA dataset for MOS prediction. Experimental results
suggest that perceptually-inspired contrastive pretraining significantly
improves the model performance evaluated by various metrics when compared
against the same network trained from scratch without pretraining. These
findings suggest that incorporating perceptual factors into pretraining greatly
contributes to the improvement in performance for SQA.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 论文提出了一种利用GPU并行计算优化深度优先搜索（DFS）的方法，特别是成本受限深度优先搜索（CB-DFS），并扩展了Batch IDA*和Batch BTS算法。


<details>
  <summary>Details</summary>
Motivation: GPU技术的快速发展为经典搜索算法提供了新的优化机会，但目前很少有算法在搜索过程中充分利用GPU。

Method: 提出了一种批处理GPU计算的CB-DFS方法，结合CPU和GPU的并行能力，扩展了Batch IDA*和Batch BTS算法。

Result: 在3x3魔方和4x4滑块拼图（STP）上验证了方法的有效性，并分析了超参数、启发式神经网络大小和硬件资源对性能的影响。

Conclusion: 该方法成功地将GPU批处理应用于DFS，同时保持了最优性保证，为搜索算法的优化提供了新思路。

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [114] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: MLLMs作为验证器存在‘同意偏见’，提出自接地验证（SGV）方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在无明确成功标准的领域中扩展AI验证器的挑战。

Method: 提出自接地验证（SGV），通过无条件与条件生成利用MLLMs的知识和推理。

Result: SGV提升MLLM验证器准确性达20%，任务完成率提升48%。

Conclusion: SGV有效克服MLLMs的偏见，显著提升验证性能。

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [115] [Kernelization for list $H$-coloring for graphs with small vertex cover](https://arxiv.org/abs/2507.12005)
*Marta Piecyk,Astrid Pieterse,Paweł Rzążewski,Magnus Wahlström*

Main category: math.CO

TL;DR: 研究了List H-Coloring问题在参数化为图的顶点覆盖数时的核化性质，定义了新的图不变量c*(H)和d*(H)，并给出了核大小的上下界。


<details>
  <summary>Details</summary>
Motivation: 探索List H-Coloring问题在参数化为顶点覆盖数时的核化性质，以改进现有结果并填补理论空白。

Method: 定义了两个新的图不变量c*(H)和d*(H)，并通过多项式方法分析了核大小的上下界。

Result: 证明了List H-Coloring问题存在大小为O(k^{c*(H)})的核，且在某些情况下核大小的下界为O(k^{d*(H)-ε})。

Conclusion: 通过新定义的不变量和多项式方法，为List H-Coloring问题的核化提供了更精确的界限，并提出了进一步研究的猜想。

Abstract: For a fixed graph $H$, in the List $H$-Coloring problem, we are given a graph
$G$ along with list $L(v) \subseteq V(H)$ for every $v \in V(G)$, and we have
to determine if there exists a list homomorphism $\varphi$ from $(G,L)$ to $H$,
i.e., an edge preserving mapping $\varphi: V(G)\to V(H)$ that satisfies
$\varphi(v)\in L(v)$ for every $v\in V(G)$. Note that if $H$ is the complete
graph on $q$ vertices, the problem is equivalent to List $q$-Coloring. We
investigate the kernelization properties of List $H$-Coloring parameterized by
the vertex cover number of $G$: given an instance $(G,L)$ and a vertex cover of
$G$ of size $k$, can we reduce $(G,L)$ to an equivalent instance $(G',L')$ of
List $H$-Coloring where the size of $G'$ is bounded by a low-degree polynomial
$p(k)$ in $k$? This question has been investigated previously by Jansen and
Pieterse [Algorithmica 2019], who provided an upper bound, which turns out to
be optimal if $H$ is a complete graph, i.e., for List $q$-Coloring. This result
was one of the first applications of the method of kernelization via
bounded-degree polynomials. We define two new integral graph invariants,
$c^*(H)$ and $d^*(H)$, with $d^*(H) \leq c^*(H) \leq d^*(H)+1$, and show that
for every graph $H$, List $H$-Coloring
  -- has a kernel with $\mathcal{O}(k^{c^*(H)})$ vertices,
  -- admits no kernel of size $\mathcal{O}(k^{d^*(H)-\varepsilon})$ for any
$\varepsilon > 0$, unless the polynomial hierarchy collapses.
  -- Furthermore, if $c^*(H) > d^*(H)$, then there is a kernel with
$\mathcal{O}(k^{c^*(H)-\varepsilon})$ vertices where $\varepsilon \geq
2^{1-c^*(H)}$.
  Additionally, we show that for some classes of graphs, including powers of
cycles and graphs $H$ where $\Delta(H) \leq c^*(H)$ (which in particular
includes cliques), the bound $d^*(H)$ is tight, using the polynomial method. We
conjecture that this holds in general.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [116] [MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling](https://arxiv.org/abs/2507.11806)
*Hendrik Kraß,Ju Huang,Seyed Mohamad Moosavi*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文介绍了MOFSimBench，一个用于评估通用机器学习原子间势（uMLIPs）在纳米多孔材料建模中表现的基准测试。结果表明，表现最佳的uMLIPs在所有任务中均优于经典力场和微调的机器学习势，强调了数据质量对性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 研究uMLIPs在纳米多孔材料（如MOFs）建模中的实际可靠性和有效性，填补现有训练数据集的不足。

Method: 提出MOFSimBench基准测试，评估uMLIPs在结构优化、分子动力学稳定性、体性质预测等任务中的表现，比较了20多种不同架构的模型。

Result: 表现最佳的uMLIPs在所有任务中均优于传统方法，数据质量（训练集的多样性和非平衡构象的包含）对性能影响显著。

Conclusion: uMLIPs已准备好用于纳米多孔材料建模，数据质量比模型架构更关键。论文提供了开源基准框架以促进进一步研究。

Abstract: Universal machine learning interatomic potentials (uMLIPs) have emerged as
powerful tools for accelerating atomistic simulations, offering scalable and
efficient modeling with accuracy close to quantum calculations. However, their
reliability and effectiveness in practical, real-world applications remain an
open question. Metal-organic frameworks (MOFs) and related nanoporous materials
are highly porous crystals with critical relevance in carbon capture, energy
storage, and catalysis applications. Modeling nanoporous materials presents
distinct challenges for uMLIPs due to their diverse chemistry, structural
complexity, including porosity and coordination bonds, and the absence from
existing training datasets. Here, we introduce MOFSimBench, a benchmark to
evaluate uMLIPs on key materials modeling tasks for nanoporous materials,
including structural optimization, molecular dynamics (MD) stability, the
prediction of bulk properties, such as bulk modulus and heat capacity, and
guest-host interactions. Evaluating over 20 models from various architectures
on a chemically and structurally diverse materials set, we find that
top-performing uMLIPs consistently outperform classical force fields and
fine-tuned machine learning potentials across all tasks, demonstrating their
readiness for deployment in nanoporous materials modeling. Our analysis
highlights that data quality, particularly the diversity of training sets and
inclusion of out-of-equilibrium conformations, plays a more critical role than
model architecture in determining performance across all evaluated uMLIPs. We
release our modular and extendable benchmarking framework at
https://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide
the adoption for nanoporous materials modeling and further development of
uMLIPs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [117] [The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models](https://arxiv.org/abs/2507.11544)
*Ann-Kathrin Dombrowski,Dillon Bowen,Adam Gleave,Chris Cundy*

Main category: cs.CY

TL;DR: 开源大型语言模型（LLMs）的可修改性带来了创新和风险，研究团队开发了一个工具包来评估模型的安全差距，发现模型规模越大，安全差距越明显。


<details>
  <summary>Details</summary>
Motivation: 开源LLMs的可修改性虽然有益，但也容易被滥用，导致安全风险。研究旨在量化这种安全差距。

Method: 通过评估不同规模的Llama-3和Qwen-2.5模型，测试其生化、网络能力、拒绝率和生成质量，使用多种移除保护措施的技术。

Result: 模型规模越大，安全差距越显著，移除保护措施后危险能力显著增强。

Conclusion: 研究团队开源了安全差距工具包，希望推动开发更抗篡改的保护措施，并欢迎社区贡献。

Abstract: Open-weight large language models (LLMs) unlock huge benefits in innovation,
personalization, privacy, and democratization. However, their core advantage -
modifiability - opens the door to systemic risks: bad actors can trivially
subvert current safeguards, turning beneficial models into tools for harm. This
leads to a 'safety gap': the difference in dangerous capabilities between a
model with intact safeguards and one that has been stripped of those
safeguards. We open-source a toolkit to estimate the safety gap for
state-of-the-art open-weight models. As a case study, we evaluate biochemical
and cyber capabilities, refusal rates, and generation quality of models from
two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to
405B) using different safeguard removal techniques. Our experiments reveal that
the safety gap widens as model scale increases and effective dangerous
capabilities grow substantially when safeguards are removed. We hope that the
Safety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve
as an evaluation framework for common open-source models and as a motivation
for developing and testing tamper-resistant safeguards. We welcome
contributions to the toolkit from the community.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [118] [A Roadmap for Climate-Relevant Robotics Research](https://arxiv.org/abs/2507.11623)
*Alan Papalia,Charles Dawson,Laurentiu L. Anton,Norhan Magdy Bayomi,Bianca Champenois,Jung-Hoon Cho,Levi Cai,Joseph DelPreto,Kristen Edwards,Bilha-Catherine Githinji,Cameron Hickert,Vindula Jayawardana,Matthew Kramer,Shreyaa Raghavan,David Russell,Shide Salimi,Jingnan Shi,Soumya Sudhakar,Yanwei Wang,Shouyi Wang,Luca Carlone,Vijay Kumar,Daniela Rus,John E. Fernandez,Cathy Wu,George Kantor,Derek Young,Hanumant Singh*

Main category: cs.RO

TL;DR: 本文提出了一个气候相关机器人研究的路线图，旨在通过机器人技术与气候领域的合作解决高影响力问题。


<details>
  <summary>Details</summary>
Motivation: 气候变化是21世纪的关键挑战，机器人社区希望为此贡献力量。

Method: 通过识别机器人技术与气候领域（如能源、建筑、交通等）的合作机会，提出具体应用方向。

Result: 提出了包括能源优化、精准农业、环境监测等在内的多个高影响力应用方向。

Conclusion: 本文旨在激发机器人社区的新研究方向，推动其参与解决气候问题的紧迫任务。

Abstract: Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.

</details>


### [119] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种利用人类视频训练视觉-语言-动作（VLA）模型的方法，通过逆运动学和重定向将人类动作转换为机器人动作，并在仿真基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖机器人硬件收集数据，限制了数据规模和多样性。人类视频提供了更丰富的场景和任务，为解决这一问题提供了可能。

Method: 通过人类视频训练VLA模型预测人类手腕和手部动作，结合逆运动学和重定向技术转换为机器人动作，并使用少量机器人演示进行微调。

Result: 在设计的Isaac Humanoid Manipulation Benchmark中，EgoVLA模型表现优于基线，验证了人类数据的重要性。

Conclusion: 利用人类视频训练VLA模型是一种有效的方法，能够显著提升机器人操纵任务的性能。

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [120] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: 本文探讨了将上下文推理扩展到关系型等式饱和（equality saturation）的方法，总结了现有技术、应用及挑战。


<details>
  <summary>Details</summary>
Motivation: 扩展等式饱和技术以支持上下文相关的重写规则，并探索其在关系模型中的应用。

Method: 结合上下文推理与关系型等式饱和，总结现有方法并分析关键挑战。

Result: 概述了上下文等式饱和的主要应用，并指出了在关系模型中实现它的挑战。

Conclusion: 上下文等式饱和在关系模型中的应用具有潜力，但仍需解决关键挑战。

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [121] [Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts](https://arxiv.org/abs/2507.12404)
*Yeming Xian,Xiaoming Wang,Yanfa Yan*

Main category: physics.data-an

TL;DR: 提出了一种结合神经网络、特征重要性分析和符号回归的两阶段框架，用于发现氧化物钙钛矿OER活性的可解释描述符。


<details>
  <summary>Details</summary>
Motivation: 需要准确且物理可解释的描述符来理解和预测氧化物钙钛矿催化剂的OER活性，但符号回归在高维输入和小数据集下性能下降。

Method: 采用两阶段方法：第一阶段用小数据集和七个结构特征改进已知描述符；第二阶段扩展到164个特征，降维并识别关键电子描述符。

Result: 最终公式结合了μ/t、μ/RA和LUMO能量，训练和验证MAE分别为22.1和20.6 meV，提高了准确性且具有强物理可解释性。

Conclusion: NN引导的符号回归在数据稀缺情况下实现了准确、可解释且物理有意义的描述符发现，表明可解释性不必牺牲准确性。

Abstract: Understanding and predicting the activity of oxide perovskite catalysts for
the oxygen evolution reaction (OER) requires descriptors that are both accurate
and physically interpretable. While symbolic regression (SR) offers a path to
discover such formulas, its performance degrades with high-dimensional inputs
and small datasets. We present a two-phase framework that combines neural
networks (NN), feature importance analysis, and symbolic regression (SR) to
discover interpretable descriptors for OER activity in oxide perovskites. In
Phase I, using a small dataset and seven structural features, we reproduce and
improve the known {\mu}/t descriptor by engineering composite features and
applying symbolic regression, achieving training and validation MAEs of 22.8
and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce
dimensionality, and identify LUMO energy as a key electronic descriptor. A
final formula using {\mu}/t, {\mu}/RA, and LUMO energy achieves improved
accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong
physical interpretability. Our results demonstrate that NN-guided symbolic
regression enables accurate, interpretable, and physically meaningful
descriptor discovery in data-scarce regimes, indicating interpretability need
not sacrifice accuracy for materials informatics.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [122] [Universal Fourier Neural Operators for Micromechanics](https://arxiv.org/abs/2507.12233)
*Binh Huy Nguyen,Matti Schneider*

Main category: cs.CE

TL;DR: 该论文提出使用傅里叶神经算子（FNOs）解决均质化中的单元问题，结合快速傅里叶变换（FFT）方法，展示了FNOs在预测任意刚度分布问题中的高效性和普适性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在解决均质化单元问题时速度和通用性不足，且缺乏明确的方向。论文旨在探索FNOs在微力学中的应用潜力，并建立其与FFT方法的联系。

Method: 通过构建一个模仿FFT基础方案的FNO替代模型，该模型无需训练即可预测任意刚度分布问题的解，且不受材料对称性、相数或界面几何的限制。

Result: FNOs能够高效处理大规模问题（超过1亿体素），运行时间与经典FFT求解器相当，并提供明确的精度保证。

Conclusion: FNOs在微力学问题中具有巨大潜力，其与FFT方法的结合有望促进两种方法的相互借鉴与发展。

Abstract: \noindent Solving cell problems in homogenization is hard, and available
deep-learning frameworks fail to match the speed and generality of traditional
computational frameworks. More to the point, it is generally unclear what to
expect of machine-learning approaches, let alone single out which approaches
are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)
for micromechanics, empowering them by insights from computational
micromechanics methods based on the fast Fourier transform (FFT). We construct
an FNO surrogate mimicking the basic scheme foundational for FFT-based methods
and show that the resulting operator predicts solutions to cell problems with
\emph{arbitrary} stiffness distribution only subject to a material-contrast
constraint up to a desired accuracy. In particular, there are no restrictions
on the material symmetry like isotropy, on the number of phases and on the
geometry of the interfaces between materials. Also, the provided fidelity is
sharp and uniform, providing explicit guarantees leveraging our physical
empowerment of FNOs. To show the desired universal approximation property, we
construct an FNO explicitly that requires no training to begin with. Still, the
obtained neural operator complies with the same memory requirements as the
basic scheme and comes with runtimes proportional to classical FFT solvers. In
particular, large-scale problems with more than 100 million voxels are readily
handled. The goal of this work is to underline the potential of FNOs for
solving micromechanical problems, linking FFT-based methods to FNOs. This
connection is expected to provide a fruitful exchange between both worlds.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [123] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)
*Xiaoqing Chen,Siyang Li,Dongrui Wu*

Main category: cs.HC

TL;DR: 提出了一种无需校准的跨数据集EEG解码框架AFPM，通过空间对齐和帧-块编码提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG解码模型在跨数据集学习和泛化中的问题，如通道布局不一致、信号分布非平稳和神经生理学先验整合不足。

Method: AFPM框架包括空间对齐（选择任务相关通道、对齐EEG分布、统一通道布局）和帧-块编码（将信号建模为统一时空块）。

Result: 在运动想象和事件相关电位任务中，性能分别提升4.40%和3.58%，优于17种现有方法。

Conclusion: AFPM是首个无需校准的跨数据集EEG解码框架，显著提升了BCI在实际应用中的实用性。

Abstract: Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

</details>


### [124] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)
*Hyein Hong,Sangbong Yoo,SeokHwan Choi,Jisue Kim,Seongbum Seo,Haneol Cho,Chansoo Kim,Yun Jang*

Main category: cs.HC

TL;DR: 论文提出了一种名为d-DQIVAR的可视化分析系统，结合数据驱动和流程驱动方法，旨在提升数据质量（DQ）并优化机器学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注数据预处理而非真正的数据质量改进（DQI），且传统批处理数据预处理方法常导致数据特性失真，影响模型性能。

Method: 系统整合了数据驱动（如缺失值填充、异常检测）和流程驱动（如DQ评估、Kolmogorov-Smirnov检验）技术，通过可视化分析支持DQI策略。

Result: 通过案例研究、评估和用户研究，系统展示了如何有效利用专家和领域知识提升数据质量和模型性能。

Conclusion: d-DQIVAR系统为数据质量改进提供了实用工具，结合了数据与流程驱动的优势，优化了机器学习模型的表现。

Abstract: Approaches to enhancing data quality (DQ) are classified into two main
categories: data- and process-driven. However, prior research has predominantly
utilized batch data preprocessing within the data-driven framework, which often
proves insufficient for optimizing machine learning (ML) model performance and
frequently leads to distortions in data characteristics. Existing studies have
primarily focused on data preprocessing rather than genuine data quality
improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual
analytics system designed to facilitate DQI strategies aimed at improving ML
model performance. Our system integrates visual analytics techniques that
leverage both data-driven and process-driven approaches. Data-driven techniques
tackle DQ issues such as imputation, outlier detection, deletion, format
standardization, removal of duplicate records, and feature selection.
Process-driven strategies encompass evaluating DQ and DQI procedures by
considering DQ dimensions and ML model performance and applying the
Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness
expert and domain knowledge effectively within a practical workflow through
case studies, evaluations, and user studies.

</details>


### [125] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)
*Hyeon Jeon,Jeongin Park,Soohyun Lee,Dae Hyun Kim,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 提出了一种基于结构复杂性度量的数据集自适应降维优化方法，显著提高了降维效率且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 传统降维技术选择和超参数优化依赖试错，计算开销大。

Method: 利用结构复杂性度量量化数据集内在复杂性，预测降维技术的最大可实现精度。

Result: 验证了度量能有效近似数据集真实复杂性，并显著提升降维优化效率。

Conclusion: 数据集自适应方法为降维优化提供了高效且准确的解决方案。

Abstract: Selecting the appropriate dimensionality reduction (DR) technique and
determining its optimal hyperparameter settings that maximize the accuracy of
the output projections typically involves extensive trial and error, often
resulting in unnecessary computational overhead. To address this challenge, we
propose a dataset-adaptive approach to DR optimization guided by structural
complexity metrics. These metrics quantify the intrinsic complexity of a
dataset, predicting whether higher-dimensional spaces are necessary to
represent it accurately. Since complex datasets are often inaccurately
represented in two-dimensional projections, leveraging these metrics enables us
to predict the maximum achievable accuracy of DR techniques for a given
dataset, eliminating redundant trials in optimizing DR. We introduce the design
and theoretical foundations of these structural complexity metrics. We
quantitatively verify that our metrics effectively approximate the ground truth
complexity of datasets and confirm their suitability for guiding
dataset-adaptive DR workflow. Finally, we empirically show that our
dataset-adaptive workflow significantly enhances the efficiency of DR
optimization without compromising accuracy.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [126] [MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments](https://arxiv.org/abs/2507.12028)
*Soheil Mahdizadeh,Elyas Oustad,Mohsen Ansari*

Main category: cs.AR

TL;DR: MOFCO算法通过结合启发式辅助的进化博弈论，有效解决了三层雾计算环境中的任务卸载问题，显著降低了系统成本。


<details>
  <summary>Details</summary>
Motivation: 用户设备（UE）移动性导致频繁的服务迁移和高成本，影响系统性能，需要一种高效的解决方案。

Method: 将任务卸载和资源分配建模为MINLP问题，并采用启发式辅助的进化博弈论方法求解。

Result: 实验表明，MOFCO平均降低系统成本19%，某些场景下可达43%。

Conclusion: MOFCO在移动性和迁移感知的任务卸载中表现出色，优于现有方法。

Abstract: Task offloading in three-layer fog computing environments presents a critical
challenge due to user equipment (UE) mobility, which frequently triggers costly
service migrations and degrades overall system performance. This paper
addresses this problem by proposing MOFCO, a novel Mobility- and
Migration-aware Task Offloading algorithm for Fog Computing environments. The
proposed method formulates task offloading and resource allocation as a
Mixed-Integer Nonlinear Programming (MINLP) problem and employs a
heuristic-aided evolutionary game theory approach to solve it efficiently. To
evaluate MOFCO, we simulate mobile users using SUMO, providing realistic
mobility patterns. Experimental results show that MOFCO reduces system cost,
defined as a combination of latency and energy consumption, by an average of
19% and up to 43% in certain scenarios compared to state-of-the-art methods.

</details>


### [127] [Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length](https://arxiv.org/abs/2507.12442)
*Saptarshi Mitra,Rachid Karami,Haocheng Xu,Sitao Huang,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 论文比较了Transformer、SSM和混合模型在消费级GPU上的长上下文推理性能，发现SSM在处理超长序列时表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在处理长上下文输入时存在二次复杂度和高内存需求的问题，需要更高效的架构如SSM。

Method: 通过系统性的性能对比测试，分析了Transformer、SSM和混合模型在消费级GPU上的表现。

Result: SSM在超长序列（220K tokens）上表现优于Transformer，速度可达4倍，且硬件优化的SSM内核占主导。

Conclusion: SSM是长上下文推理的理想选择，未来硬件优化应重点关注SSM内核。

Abstract: The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [128] [Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control](https://arxiv.org/abs/2507.12202)
*Anton Klenitskiy,Konstantin Polev,Daria Denisova,Alexey Vasilev,Dmitry Simakov,Gleb Gusev*

Main category: cs.IR

TL;DR: 论文探讨了稀疏自编码器（SAE）在序列推荐任务中的应用，展示了其如何从Transformer模型中提取可解释特征，并灵活控制推荐行为。


<details>
  <summary>Details</summary>
Motivation: 理解并解释基于Transformer的序列推荐模型的内部机制，以更好地控制和调整其行为，满足实际应用需求。

Method: 使用稀疏自编码器（SAE）从Transformer的隐藏状态中提取稀疏线性组合的可解释特征。

Result: SAE提取的特征比原始隐藏状态更具可解释性和单义性，并能灵活控制推荐行为。

Conclusion: SAE为序列推荐任务提供了一种有效的可解释性和控制方法，适用于多样化的实际场景。

Abstract: Many current state-of-the-art models for sequential recommendations are based
on transformer architectures. Interpretation and explanation of such black box
models is an important research question, as a better understanding of their
internals can help understand, influence, and control their behavior, which is
very important in a variety of real-world applications. Recently sparse
autoencoders (SAE) have been shown to be a promising unsupervised approach for
extracting interpretable features from language models. These autoencoders
learn to reconstruct hidden states of the transformer's internal layers from
sparse linear combinations of directions in their activation space.
  This paper is focused on the application of SAE to the sequential
recommendation domain. We show that this approach can be successfully applied
to the transformer trained on a sequential recommendation task: learned
directions turn out to be more interpretable and monosemantic than the original
hidden state dimensions. Moreover, we demonstrate that the features learned by
SAE can be used to effectively and flexibly control the model's behavior,
providing end-users with a straightforward method to adjust their
recommendations to different custom scenarios and contexts.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [129] [Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing](https://arxiv.org/abs/2507.11780)
*Justin Whitehouse,Morgane Austern,Vasilis Syrgkanis*

Main category: econ.EM

TL;DR: 本文提出了一种基于软最大平滑的估计器，用于处理非可微函数的最优值问题，避免了现有方法的限制，如参数假设或计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 构建最优治疗策略值的置信区间在因果推断中很重要，但现有方法因非可微性问题存在局限性，如参数假设或计算复杂性。

Method: 通过控制一阶偏差和二阶余项，提出了一种基于软最大平滑的估计器，用于处理非可微函数的最优值问题。

Result: 该估计器实现了√n收敛速率，避免了参数限制和不现实的边界假设，且通常具有统计效率。

Conclusion: 软最大平滑方法为处理非可微函数的最优值问题提供了一种有效且实用的解决方案。

Abstract: Constructing confidence intervals for the value of an optimal treatment
policy is an important problem in causal inference. Insight into the optimal
policy value can guide the development of reward-maximizing, individualized
treatment regimes. However, because the functional that defines the optimal
value is non-differentiable, standard semi-parametric approaches for performing
inference fail to be directly applicable. Existing approaches for handling this
non-differentiability fall roughly into two camps. In one camp are estimators
based on constructing smooth approximations of the optimal value. These
approaches are computationally lightweight, but typically place unrealistic
parametric assumptions on outcome regressions. In another camp are approaches
that directly de-bias the non-smooth objective. These approaches don't place
parametric assumptions on nuisance functions, but they either require the
computation of intractably-many nuisance estimates, assume unrealistic
$L^\infty$ nuisance convergence rates, or make strong margin assumptions that
prohibit non-response to a treatment. In this paper, we revisit the problem of
constructing smooth approximations of non-differentiable functionals. By
carefully controlling first-order bias and second-order remainders, we show
that a softmax smoothing-based estimator can be used to estimate parameters
that are specified as a maximum of scores involving nuisance components. In
particular, this includes the value of the optimal treatment policy as a
special case. Our estimator obtains $\sqrt{n}$ convergence rates, avoids
parametric restrictions/unrealistic margin assumptions, and is often
statistically efficient.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [130] [Neural Polar Decoders for Deletion Channels](https://arxiv.org/abs/2507.12329)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: 论文提出了一种用于删除信道的神经极化解码器（NPD），通过降低计算复杂度（从O(N^4)降至O(AN log N)），扩展了极化码在删除信道中的应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有极化解码器在删除信道中计算复杂度高（O(N^4)），限制了其在中短块长中的应用。本文旨在通过NPD降低复杂度。

Method: 扩展NPD架构以支持删除信道，仅修改四个神经网络中的一个，并引入用户可调的计算预算参数A。

Result: 在删除率为0.01和0.1的信道中验证了NPD性能，并展示了通过列表解码进一步提升性能的可能性。

Conclusion: NPD为删除信道提供了一种高效解码方案，未来可能应用于DNA存储等领域。

Abstract: This paper introduces a neural polar decoder (NPD) for deletion channels with
a constant deletion rate. Existing polar decoders for deletion channels exhibit
high computational complexity of $O(N^4)$, where $N$ is the block length. This
limits the application of polar codes for deletion channels to
short-to-moderate block lengths. In this work, we demonstrate that employing
NPDs for deletion channels can reduce the computational complexity. First, we
extend the architecture of the NPD to support deletion channels. Specifically,
the NPD architecture consists of four neural networks (NNs), each replicating
fundamental successive cancellation (SC) decoder operations. To support
deletion channels, we change the architecture of only one. The computational
complexity of the NPD is $O(AN\log N)$, where the parameter $A$ represents a
computational budget determined by the user and is independent of the channel.
We evaluate the new extended NPD for deletion channels with deletion rates
$\delta\in\{0.01, 0.1\}$ and we verify the NPD with the ground truth given by
the trellis decoder by Tal et al. We further show that due to the reduced
complexity of the NPD, we are able to incorporate list decoding and further
improve performance. We believe that the extended NPD presented here could have
applications in future technologies like DNA storage.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [131] [Multimodal Coordinated Online Behavior: Trade-offs and Strategies](https://arxiv.org/abs/2507.12108)
*Lorenzo Mannocci,Stefano Cresci,Matteo Magnani,Anna Monreale,Maurizio Tesconi*

Main category: cs.SI

TL;DR: 该研究比较了多模态协调行为检测的不同方法，探讨了弱整合与强整合多模态模型的权衡，发现多模态方法能更全面地理解协调动态。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常依赖单模态或独立处理多模态数据，可能忽略多模态协调的复杂动态，因此需要更全面的检测方法。

Method: 比较单模态和多模态方法，评估不同数据模态的独特贡献，探讨多模态实现方式对检测结果的影响。

Result: 研究发现并非所有模态都能提供独特见解，但多模态方法能更全面地理解协调动态。

Conclusion: 多模态方法提升了检测和分析在线协调行为的能力，为维护数字平台完整性提供了新视角。

Abstract: Coordinated online behavior, which spans from beneficial collective actions
to harmful manipulation such as disinformation campaigns, has become a key
focus in digital ecosystem analysis. Traditional methods often rely on
monomodal approaches, focusing on single types of interactions like co-retweets
or co-hashtags, or consider multiple modalities independently of each other.
However, these approaches may overlook the complex dynamics inherent in
multimodal coordination. This study compares different ways of operationalizing
the detection of multimodal coordinated behavior. It examines the trade-off
between weakly and strongly integrated multimodal models, highlighting the
balance between capturing broader coordination patterns and identifying tightly
coordinated behavior. By comparing monomodal and multimodal approaches, we
assess the unique contributions of different data modalities and explore how
varying implementations of multimodality impact detection outcomes. Our
findings reveal that not all the modalities provide distinct insights, but that
with a multimodal approach we can get a more comprehensive understanding of
coordination dynamics. This work enhances the ability to detect and analyze
coordinated online behavior, offering new perspectives for safeguarding the
integrity of digital platforms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [132] [BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search](https://arxiv.org/abs/2507.12189)
*Azhar Ikhtiarudin,Aditi Das,Param Thakkar,Akash Kundu*

Main category: quant-ph

TL;DR: BenchRL-QAS是一个用于评估强化学习算法在量子架构搜索中的统一框架，涵盖多种任务和系统规模。研究发现RL算法性能高度依赖任务背景，支持“无免费午餐”原则。


<details>
  <summary>Details</summary>
Motivation: 系统评估RL算法在量子架构搜索中的表现，为量子电路设计提供指导。

Method: 提出BenchRL-QAS框架，测试九种RL算法在多种量子任务和噪声环境下的表现，使用加权排名指标进行公平比较。

Result: RL量子分类器优于基线变分分类器，但无单一RL算法在所有任务中表现最优。

Conclusion: 量子电路设计中需根据任务背景选择算法，BenchRL-QAS框架支持未来研究和可重复性。

Abstract: We introduce BenchRL-QAS, a unified benchmarking framework for systematically
evaluating reinforcement learning (RL) algorithms in quantum architecture
search (QAS) across diverse variational quantum algorithm tasks and system
sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including
both value-based and policy-gradient methods on representative quantum problems
such as variational quantum eigensolver, variational quantum state
diagonalization, quantum classification, and state preparation, spanning both
noiseless and realistic noisy regimes. We propose a weighted ranking metric
that balances accuracy, circuit depth, gate count, and computational
efficiency, enabling fair and comprehensive comparison. Our results first
reveal that RL-based quantum classifier outperforms baseline variational
classifiers. Then we conclude that no single RL algorithm is universally
optimal when considering a set of QAS tasks; algorithmic performance is highly
context-dependent, varying with task structure, qubit count, and noise. This
empirical finding provides strong evidence for the "no free lunch" principle in
RL-based quantum circuit design and highlights the necessity of tailored
algorithm selection and systematic benchmarking for advancing quantum circuit
synthesis. This work represents the most comprehensive RL-QAS benchmarking
effort to date, and BenchRL-QAS along with all experimental data are made
publicly available to support reproducibility and future research
https://github.com/azhar-ikhtiarudin/bench-rlqas.

</details>


### [133] [Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator](https://arxiv.org/abs/2507.12256)
*Monica Lăcătuş,Matthias Möller*

Main category: quant-ph

TL;DR: 论文提出了一种学习替代量子电路（SQC）的框架，用于近似D2Q9晶格上的BGK碰撞算子，解决了量子CFD中非线性碰撞步骤的实现难题。


<details>
  <summary>Details</summary>
Motivation: 高雷诺数湍流模拟在传统CFD工具中具有挑战性，量子计算有望提供速度优势，但非线性碰撞步骤的低深度量子电路实现仍是一大难题。

Method: 通过训练一个四量子位电路，近似BGK碰撞算子，并满足物理性质（如质量和动量守恒）。电路设计避免了辅助量子位和后选择。

Result: 在IBM Heron处理器上，15块SQC仅需2,430个原生门，且深度与网格分辨率无关。验证表明其能准确捕捉涡流耗散和流动再循环。

Conclusion: SQC框架为量子CFD中的碰撞步骤提供了一种高效实现，展示了量子并行性的潜力。

Abstract: Direct numerical simulation of turbulent flows at high Reynolds numbers
remains a major challenge for traditional computational fluid dynamics (CFD)
tools running on classical computer hardware. This has motivated growing
interest in quantum algorithms for CFD to enable flow simulations on quantum
computers. The reason being that these computers are expected to deliver
potential speed-ups for certain problems. One promising quantum CFD approach is
a fully quantum implementation of the lattice Boltzmann method called QLBM.
Although efficient quantum routines are now available for the streaming step,
implementing the nonlinear, irreversible collision step with a low depth
circuit that avoids additional ancilla qubits, probabilistic post-selection and
repeated executions remains a significant challenge. In this study, we address
this challenge by introducing a framework for learning a surrogate quantum
circuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision
operator for the D2Q9 lattice. The four qubit circuit is trained to respect the
physical properties of the BGK collision operator, including mass and momentum
conservation, D8 equivariance and scale equivariance. When compiled to the gate
set used by IBM Heron processor under the assumption of full qubit
connectivity, the 15 block SQC requires only 2,430 native gates and uses
neither ancilla qubits nor post-selection or repeated executions. Moreover, its
depth is independent of the grid resolution, as collision is a local operation
that can exploit quantum parallelism to its full extent. We validate the SQC on
two benchmark flows, the Taylor Green vortex decay and the lid driven cavity,
demonstrating that it accurately captures vortex dissipation and flow
recirculation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [134] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE是一种并行GPU实现的BPE算法，针对高吞吐量批量推理优化，比现有CPU实现快2-2.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有BPE实现（如HuggingFace Tokenizers和tiktoken）在CPU上运行且效率低，无法充分利用GPU并行能力。

Method: BlockBPE通过消除Regex预分词步骤，实现高度并行化的token合并，复杂度降至O(nd)。

Result: 在高批量推理任务中，BlockBPE的吞吐量比tiktoken高2倍，比HuggingFace Tokenizers高2.5倍。

Conclusion: BlockBPE为GPU上的高效分词提供了可行方案，适用于大规模语言模型推理。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [135] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: MapIQ是一个新基准数据集，用于评估多模态大语言模型（MLLMs）在地图视觉问答（Map-VQA）中的表现，涵盖多种地图类型和主题。


<details>
  <summary>Details</summary>
Motivation: 现有Map-VQA研究主要关注等值区域图，限制了研究范围。MapIQ旨在填补这一空白，提供更全面的评估。

Method: 构建包含14,706个问答对的MapIQ数据集，涵盖三种地图类型和六个主题，评估多种MLLMs的性能。

Result: 实验比较了MLLMs与人类基线的表现，并探讨了地图设计变化对模型鲁棒性和敏感性的影响。

Conclusion: MapIQ为改进Map-VQA性能提供了潜在方向，并揭示了MLLMs对内部地理知识的依赖。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [136] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）如何处理竞争性的事实与反事实信息，重点关注注意力头的作用。通过复现和整合三项近期研究，探讨了注意力头强度与事实输出比例的关系，评估了关于注意力头抑制机制的竞争性假设，并研究了这些注意力模式的领域特异性。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs如何在事实与反事实信息之间进行权衡，以及注意力头在这一过程中的具体作用，有助于揭示模型内部工作机制。

Method: 复现并整合三项相关研究，使用机制解释工具分析注意力头强度与事实输出比例的关系，评估抑制机制假设，并考察注意力模式的领域特异性。

Result: 研究发现，促进事实输出的注意力头通过通用的复制抑制而非选择性反事实抑制发挥作用，且其行为具有领域依赖性，较大模型表现出更专业和类别敏感的模式。

Conclusion: 注意力头通过通用机制而非选择性机制影响事实输出，且其行为受领域和模型规模影响，为理解LLMs的内部机制提供了新视角。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [137] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 提出了一种新框架，利用自回归语言模型的固有知识，实现多令牌同时预测，显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型的顺序生成特性限制了推理速度和并行性，尤其在生成后期方向明确时。

Method: 结合掩码输入、门控LoRA、轻量采样器、辅助损失和推测生成策略，实现多令牌预测。

Result: 代码和数学生成速度提升近5倍，通用聊天和知识任务提升近2.5倍，且质量无损。

Conclusion: 新框架有效解决了自回归模型的生成速度瓶颈，具有实际应用潜力。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [138] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文综述了深度学习在几何问题求解中的应用，包括任务总结、方法回顾、评估指标分析及未来挑战与方向的讨论。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解是数学推理的关键领域，涉及教育、人工智能数学能力评估和多模态能力评估等多个重要领域。深度学习技术的快速发展，尤其是多模态大语言模型的兴起，推动了该领域的研究热潮。

Method: 本文通过全面总结几何问题求解的相关任务、深入回顾相关深度学习方法、详细分析评估指标和方法，以及批判性讨论当前挑战和未来方向，提供了一个系统性的综述。

Result: 本文为几何问题求解中的深度学习应用提供了全面且实用的参考，旨在推动该领域的进一步发展。

Conclusion: 几何问题求解的深度学习研究具有广阔前景，但仍需解决当前挑战并探索未来方向。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [139] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: 论文提出了IAM框架，通过利用不同规模LLMs之间注意力矩阵的高相似性，优化计算和减少KV缓存使用，实验显示预填充速度提升15%，KV缓存减少22.1%，且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在长上下文场景下资源消耗大，现有方法主要利用模型内部稀疏性，未充分利用外部信息优化。

Method: 通过分析注意力矩阵相似性、选择映射层及验证一致性，提出IAM框架，实现小规模与大规模LLMs间的注意力映射。

Result: IAM框架加速预填充15%，减少KV缓存22.1%，且在不同模型系列中具有通用性。

Conclusion: IAM框架是一种高效且通用的LLM优化工具，可与现有KV缓存优化方法互补。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [140] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 论文提出了一种基于查询的多阶段知识图谱问答框架，用于提升多跳和时序问题的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多跳推理和时序问题上表现不佳，而基于知识图谱的问答提供了一种模块化替代方案。

Method: 采用多阶段查询生成框架，结合实体链接和谓词匹配方法，利用CoT推理。

Result: 实验表明该框架在多跳和时序问答数据集上表现优异，且适用于小型语言模型。

Conclusion: 基于查询的多阶段知识图谱问答框架在多跳和时序问答任务中具有潜力。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [141] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: 提出了一种新型的PoT量化框架，用于LLM权重，在极低精度下优于现有方法，并实现更高效的反量化。


<details>
  <summary>Details</summary>
Motivation: LLM部署因计算资源需求高而困难，现有PoT量化在GPU上效果不佳。

Method: 采用两步后训练算法：初始化量化尺度并校准。

Result: 在2-和3-bit格式下超越现有整数量化方法，反量化速度显著提升。

Conclusion: 新型PoT量化框架在低精度下高效且准确，适合LLM部署。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [142] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: 基于模块化风格测量管道的二元AI检测方法，使用spaCy模型预处理文本并提取特征，采用轻量梯度提升机分类器，训练集包含50万+机器生成文本。


<details>
  <summary>Details</summary>
Motivation: 探索一种非神经网络的、计算成本低且可解释的方法，用于检测机器生成文本。

Method: 使用spaCy模型进行文本预处理和特征提取，结合轻量梯度提升机分类器，并优化参数以利用大规模训练集。

Result: 训练了包含50万+机器生成文本的分类器，通过参数优化提升分类能力。

Conclusion: 该方法延续了先前有效的非神经网络、低计算成本且可解释的检测策略。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [143] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: 论文提出了一种评估大语言模型（LLM）文本增强的框架，包括可扩展性分析和迭代增强与摘要细化（IASR），验证了GPT-3.5 Turbo在语义保真度、多样性和生成效率上的最佳平衡，并在实际任务中显著提升了主题建模效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本增强技术在语义保留方面的不足，特别是在大规模或迭代生成时导致的冗余和不稳定性问题。

Method: 提出了两个评估组件：可扩展性分析（衡量语义一致性）和迭代增强与摘要细化（IASR）（评估语义漂移）。

Result: GPT-3.5 Turbo在语义保真度、多样性和生成效率上表现最佳；在实际任务中，主题建模的粒度提升了400%，并完全消除了主题重叠。

Conclusion: 提出的框架有效评估了LLM增强技术在实际NLP任务中的实用性，验证了其提升语义建模能力的潜力。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [144] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: 该论文提出了一个文本异常检测的基准测试，通过多种预训练语言模型的嵌入和多领域数据集，系统评估了嵌入质量对异常检测效果的影响，并开源了工具包。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化的基准测试，现有文本异常检测方法难以进行严格比较和创新开发。

Method: 利用多种预训练语言模型（如GloVe、BERT、LLaMa系列等）的嵌入和多领域文本数据集，结合多种评估指标（如AUROC、AUPRC）进行系统评估。

Result: 实验发现嵌入质量对异常检测效果至关重要，且深度学习模型在LLM嵌入下并未优于传统浅层算法（如KNN、Isolation Forest）。

Conclusion: 该工作为未来研究提供了基础，并开源了工具包以促进文本异常检测系统的稳健和可扩展发展。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [145] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: 论文研究了如何利用思维链（CoTs）预测语言模型最终输出的对齐风险，发现基于CoT激活的线性探针比文本方法更有效。


<details>
  <summary>Details</summary>
Motivation: 探索思维链（CoTs）是否可用于预测语言模型最终输出的对齐风险，以减少有害内容。

Method: 评估多种监测方法（人类、大语言模型、文本分类器），比较基于CoT文本和激活的预测效果。

Result: 基于CoT激活的线性探针显著优于文本方法，且能早期预测对齐风险。

Conclusion: 轻量级探针可实现实时安全监测和早期干预，适用于不同模型和任务。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [146] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: S2WTM提出了一种新的主题建模方法，通过球形切片Wasserstein距离解决VAE-NTMs中的后验坍塌问题，生成更一致和多样化的主题。


<details>
  <summary>Details</summary>
Motivation: 传统VAE-NTMs在建模高维文本数据的超球面潜在表示时容易发生后验坍塌，导致潜在表示无效。

Method: S2WTM采用单位超球面上的先验分布，并利用球形切片Wasserstein距离对齐后验分布与先验。

Result: 实验表明S2WTM优于现有主题模型，生成的主题更一致和多样化，且在下游任务中表现更好。

Conclusion: S2WTM有效解决了后验坍塌问题，提升了主题建模的性能和效果。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [147] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: 论文提出BETR方法，通过显式优化数据选择目标，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究数据选择方法对模型性能的影响，探索显式优化目标的效果。

Method: 提出BETR方法，基于与基准训练样本的相似性选择预训练文档，训练轻量分类器预测分数。

Result: BETR在计算效率上优于基线方法，提升9/10任务性能，且适应不同规模模型。

Conclusion: 直接匹配预训练数据与目标任务能精确塑造模型能力，数据选择策略需适应模型规模。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [148] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: SketchDNN提出了一种生成CAD草图的模型，通过统一的连续-离散扩散过程联合建模连续参数和离散类别标签。


<details>
  <summary>Details</summary>
Motivation: 解决CAD草图中原始参数化的异质性和原始元素的排列不变性问题。

Method: 采用Gaussian-Softmax扩散，通过高斯噪声扰动logits并通过softmax变换投影到概率单纯形上。

Result: 显著提升生成质量，FID从16.04降至7.80，NLL从84.8降至81.33。

Conclusion: 在SketchGraphs数据集上实现了CAD草图生成的最新最优性能。

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [149] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: 使用变分自编码器（VAE）替代传统CNN进行直肠癌淋巴结转移（LNM）分期，VAE-MLP模型在MRI数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于淋巴结大小、形状和纹理的放射学标准诊断准确性有限，VAE能直接编码视觉特征，生成更可解释的潜在空间。

Method: 采用VAE作为特征编码器，结合MLP模型，在168名未接受新辅助治疗的患者的MRI数据集上进行训练和验证。

Result: VAE-MLP模型在交叉验证中表现优异，AUC为0.86，敏感性0.79，特异性0.85。

Conclusion: VAE-MLP模型在直肠癌LNM分期中具有潜在应用价值，代码已开源。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [150] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: 论文提出一种基于姿势的心理状态推断方法，通过板球运动验证其有效性，F1分数超过75%，AUC-ROC超过80%。


<details>
  <summary>Details</summary>
Motivation: 姿势分析在诊断疲劳、预防伤害和提升表现方面潜力巨大，但面临数据敏感性问题。体育场景为获取多样化情绪数据提供了可行方案。

Method: 通过运动分析从活动视频中识别人类意图，利用现有数据统计作为弱监督验证结果。

Result: 方法在区分攻击性和防御性击球意图上表现优异，表明姿势泄露了强信号。

Conclusion: 研究为体育分析提供了通用技术，并拓展了人类行为分析的应用领域。

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [151] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: 提出了一种多变量自回归空气污染物预测模型（MVAR），解决了现有研究忽视污染物间相互作用和空间响应的问题，并构建了标准化数据集。


<details>
  <summary>Details</summary>
Motivation: 空气污染物对环境和健康构成威胁，现有研究多关注单一污染物预测，忽视了多污染物间的相互作用和空间响应。

Method: 设计了多变量自回归训练范式，结合气象耦合空间变换模块，利用AI气象预报数据，提升预测效率和长期序列预测能力。

Result: 实验表明MVAR优于现有方法，验证了其架构的有效性。

Conclusion: MVAR模型在多污染物预测中表现优异，为污染预警和政策制定提供了有力工具。

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [152] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: 提出了一种基于归一化流的数据驱动方法，用于建模人体姿态的神经先验，利用RealNVP学习6D旋转姿态的灵活分布，并通过Gram-Schmidt过程实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为启发式或表达能力有限，本文旨在提供一种灵活且稳定的概率建模方法，以支持人体运动捕捉和重建任务。

Method: 使用RealNVP学习6D旋转姿态的密度分布，通过Gram-Schmidt过程处理流形上的分布问题，确保训练稳定性和下游兼容性。

Result: 通过定性和定量评估验证了先验的有效性，并通过消融实验分析了其影响。

Conclusion: 为人体姿态先验在运动捕捉和重建中的应用提供了坚实的概率基础。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [153] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 比较Keras、PyTorch和JAX在医学图像分类任务中的性能表现，使用PathMNIST数据集评估训练效率、分类准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习框架在医学图像分类中广泛应用，但不同框架（如Keras、PyTorch和JAX）的性能比较尚未充分研究。

Method: 通过PathMNIST数据集，对CNN在不同框架中的实现进行全面分析，评估训练效率、分类准确性和推理速度。

Result: 研究揭示了计算速度与模型准确性之间的权衡关系。

Conclusion: 为医学图像分析的研究者和从业者提供了有价值的参考，帮助选择适合的深度学习框架。

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [154] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的早期预测支气管肺发育不良（BPD）的方法，利用出生24小时内的胸部X光片，通过领域特定的预训练和优化技术，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: BPD是一种常见的慢性肺病，对极低出生体重婴儿影响深远。现有预防措施风险高，因此需要一种非侵入性的早期预测工具，以减少不必要的治疗风险。

Method: 使用163名极低出生体重婴儿的胸部X光片，微调ResNet-50模型，采用渐进层冻结、CutMix增强和线性探测技术，优化预测性能。

Result: 最佳模型在预测中重度BPD时，AUROC为0.78，平衡准确率为0.69，F1分数为0.67，显著优于ImageNet初始化和传统IRDS分级方法。

Conclusion: 领域特定的预训练结合优化技术，能够从常规X光片中准确预测BPD，为临床提供了一种可行的非侵入性工具。

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [155] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: 论文提出离散潜在码（DLC）作为扩散模型的输入表示，提升生成质量和组合性，并在ImageNet上实现无条件图像生成的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型成功的关键在于输入表示，理想表示应提升生成质量、易于生成且具有组合性。

Method: 引入基于自监督学习的离散潜在码（DLC），作为扩散模型的输入表示，并展示其组合性和生成能力。

Result: DLC提升生成质量，实现ImageNet无条件图像生成的新SOTA，并能生成超出训练分布的样本。

Conclusion: DLC为扩散模型提供高效、组合性强的表示，支持文本到图像生成等应用。

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [156] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: AutoVDC框架利用视觉语言模型自动检测视觉数据集中的错误标注，提升数据质量，验证表明其高效性。


<details>
  <summary>Details</summary>
Motivation: 人工标注存在缺陷且成本高，需自动化方法提升数据集质量。

Method: 利用Vision-Language Models (VLMs)自动识别错误标注，并在KITTI和nuImages数据集上验证。

Result: AutoVDC在错误检测和数据清理实验中表现优异。

Conclusion: AutoVDC能显著提升自动驾驶大规模数据集的可靠性和准确性。

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [157] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DAM-QA利用区域感知的视觉语言模型（DAM）提升文本密集图像中的视觉问答（VQA）性能，通过多区域视图聚合答案，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 针对文本密集图像中的VQA任务，需要细粒度提取文本信息，DAM的区域描述能力可能对此有益。

Method: 提出DAM-QA框架，通过多区域视图聚合答案，优化文本相关元素的证据识别。

Result: 在六个VQA基准测试中表现优异，DocVQA上提升7+分，参数更少且性能优于其他区域感知模型。

Conclusion: DAM-QA展示了区域感知模型在文本密集VQA任务中的潜力，结合高效使用策略可缩小与通用VLMs的差距。

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [158] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: CytoSAE是一种稀疏自编码器，用于医学图像分析，能够识别形态学相关概念并支持疾病分类任务。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域缺乏解释性工具，稀疏自编码器（SAEs）在视觉领域的成功应用启发了其在医学图像中的探索。

Method: 提出CytoSAE，基于40,000多个外周血单细胞图像训练，并在骨髓细胞学等域外数据上验证其泛化能力。

Result: CytoSAE能够识别形态学相关概念，并在AML亚型分类任务中达到与最先进方法相当的性能。

Conclusion: CytoSAE为医学图像分析提供了可解释的工具，支持疾病检测和分类。

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [159] [CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching](https://arxiv.org/abs/2507.11842)
*Sidharth Kannan,Tian Qiu,Carolina Cuesta-Lazaro,Haewon Jeong*

Main category: astro-ph.CO

TL;DR: CosmoFlow模型通过无监督学习，从冷暗物质模拟数据中提取紧凑且语义丰富的潜在表示。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型是否能学习低维表示以保留下游任务所需信息。

Method: 使用基于流匹配的生成模型（CosmoFlow）学习冷暗物质模拟数据的潜在表示。

Result: 模型学习到的表示比原始数据小32倍，可用于重建、合成数据生成和参数推断，且具有可解释性。

Conclusion: CosmoFlow能高效学习冷暗物质数据的潜在表示，适用于多种宇宙学任务。

Abstract: Generative machine learning models have been demonstrated to be able to learn
low dimensional representations of data that preserve information required for
downstream tasks. In this work, we demonstrate that flow matching based
generative models can learn compact, semantically rich latent representations
of field level cold dark matter (CDM) simulation data without supervision. Our
model, CosmoFlow, learns representations 32x smaller than the raw field data,
usable for field level reconstruction, synthetic data generation, and parameter
inference. Our model also learns interpretable representations, in which
different latent channels correspond to features at different cosmological
scales.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [160] [Improved Analysis for Sign-based Methods with Momentum Updates](https://arxiv.org/abs/2507.12091)
*Wei Jiang,Dingzhi Yu,Sifan Yang,Wenhao Yang,Lijun Zhang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we present enhanced analysis for sign-based optimization
algorithms with momentum updates. Traditional sign-based methods, under the
separable smoothness assumption, guarantee a convergence rate of
$\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume
unimodal symmetric stochastic noise. To address these limitations, we
demonstrate that signSGD with momentum can achieve the same convergence rate
using constant batch sizes without additional assumptions. Our analysis, under
the standard $l_2$-smoothness condition, improves upon the result of the prior
momentum-based signSGD method by a factor of $\mathcal{O}(d^{1/2})$, where $d$
is the problem dimension. Furthermore, we explore sign-based methods with
majority vote in distributed settings and show that the proposed momentum-based
method yields convergence rates of $\mathcal{O}\left( d^{1/2}T^{-1/2} +
dn^{-1/2} \right)$ and $\mathcal{O}\left( \max \{ d^{1/4}T^{-1/4},
d^{1/10}T^{-1/5} \} \right)$, which outperform the previous results of
$\mathcal{O}\left( dT^{-1/4} + dn^{-1/2} \right)$ and $\mathcal{O}\left(
d^{3/8}T^{-1/8} \right)$, respectively. Numerical experiments further validate
the effectiveness of the proposed methods.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [161] [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
*Sungkyun Chang,Simon Dixon,Emmanouil Benetos*

Main category: cs.SD

TL;DR: RUMAA是一个基于Transformer的框架，用于音乐表演分析，整合了乐谱到表演的对齐、乐谱转录和错误检测任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法将这些任务分开处理，而RUMAA通过统一框架和预训练编码器解决了这一问题，尤其针对重复结构的乐谱。

Method: 使用预训练的乐谱和音频编码器，结合三流解码器，通过代理任务捕捉任务间的依赖关系。

Result: 在公开钢琴音乐数据集上，RUMAA在非重复乐谱上达到最优对齐效果，在重复乐谱上表现更优，同时提供良好的转录和错误检测结果。

Conclusion: RUMAA通过统一框架解决了音乐表演分析的多个任务，尤其在处理重复结构乐谱时表现突出。

Abstract: This study introduces RUMAA, a transformer-based framework for music
performance analysis that unifies score-to-performance alignment,
score-informed transcription, and mistake detection in a near end-to-end
manner. Unlike prior methods addressing these tasks separately, RUMAA
integrates them using pre-trained score and audio encoders and a novel
tri-stream decoder capturing task interdependencies through proxy tasks. It
aligns human-readable MusicXML scores with repeat symbols to full-length
performance audio, overcoming traditional MIDI-based methods that rely on
manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA
matches state-of-the-art alignment methods on non-repeated scores and
outperforms them on scores with repeats in a public piano music dataset, while
also delivering promising transcription and mistake detection results.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [162] [Galaxy image simplification using Generative AI](https://arxiv.org/abs/2507.11692)
*Sai Teja Erukude,Lior Shamir*

Main category: astro-ph.GA

TL;DR: 论文提出了一种基于生成式AI的新方法，用于简化星系图像并自动转换为“骨架化”形式，以实现更准确的形状测量和分析。


<details>
  <summary>Details</summary>
Motivation: 现代数字天空调查获取了数十亿星系图像，但高精度分析需要自动化解决方案。现有方法依赖预定义类别的机器学习标注，限制了灵活性。

Method: 采用生成式AI技术，将星系图像简化为“骨架化”形式，避免依赖预定义类别，提高分析准确性。

Result: 成功应用于12.5万张DESI Legacy Survey图像，生成的简化图像目录已公开。

Conclusion: 该方法为星系图像分析提供了更灵活、准确的解决方案，代码和数据已公开。

Abstract: Modern digital sky surveys have been acquiring images of billions of
galaxies. While these images often provide sufficient details to analyze the
shape of the galaxies, accurate analysis of such high volumes of images
requires effective automation. Current solutions often rely on machine learning
annotation of the galaxy images based on a set of pre-defined classes. Here we
introduce a new approach to galaxy image analysis that is based on generative
AI. The method simplifies the galaxy images and automatically converts them
into a ``skeletonized" form. The simplified images allow accurate measurements
of the galaxy shapes and analysis that is not limited to a certain pre-defined
set of classes. We demonstrate the method by applying it to galaxy images
acquired by the DESI Legacy Survey. The code and data are publicly available.
The method was applied to 125,000 DESI Legacy Survey images, and the catalog of
the simplified images is publicly available.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [163] [Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions](https://arxiv.org/abs/2507.11783)
*Gayal Kuruppu,Neeraj Wagh,Yogatheesan Varatharajah*

Main category: eess.SP

TL;DR: 本文综述了10种早期自监督EEG基础模型（EEG-FMs），分析了其方法、实证结果及研究空白，指出当前模型评估的局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统监督EEG编码器依赖昂贵信号标注且鲁棒性不足，促使转向自监督EEG基础模型（EEG-FMs）以实现更稳健和可扩展的特征提取。

Method: 研究回顾了10种EEG-FMs，重点关注其基于序列建模的方法（如Transformer架构）和掩码序列重建的自监督学习策略。

Result: 发现当前EEG-FMs评估方法不一致且有限，难以评估其实际应用价值。

Conclusion: 未来需标准化评估、扩大模型规模，并与领域专家合作开发工具和方法，以推动EEG-FMs的实际应用。

Abstract: Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [164] [LLMs are Bayesian, in Expectation, not in Realization](https://arxiv.org/abs/2507.11768)
*Leon Chlon,Sarah Rashidi,Zein Khamis,MarcAntonio M. Awada*

Main category: stat.ML

TL;DR: 论文探讨了大语言模型在上下文学习中的表现，发现其违反贝叶斯更新的基本性质，并提出理论分析和实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在上下文学习中的行为，尤其是其违反贝叶斯更新性质的现象，以改进不确定性量化和计算效率。

Method: 通过理论分析建立四个关键结果，包括位置编码的影响、信息论最优性、隐式后验表示收敛性以及最优思维链长度推导，并在GPT-3上进行实证验证。

Result: 理论预测得到实证支持，模型在20个示例内达到理论熵限的99%，并提供了优化计算效率的方法。

Conclusion: 研究为从位置感知架构中提取校准的不确定性估计和优化部署效率提供了实用方法。

Abstract: Large language models demonstrate remarkable in-context learning
capabilities, adapting to new tasks without parameter updates. While this
phenomenon has been successfully modeled as implicit Bayesian inference, recent
empirical findings reveal a fundamental contradiction: transformers
systematically violate the martingale property, a cornerstone requirement of
Bayesian updating on exchangeable data. This violation challenges the
theoretical foundations underlying uncertainty quantification in critical
applications.
  Our theoretical analysis establishes four key results: (1) positional
encodings induce martingale violations of order $\Theta(\log n / n)$; (2)
transformers achieve information-theoretic optimality with excess risk
$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior
representation converges to the true Bayesian posterior in the space of
sufficient statistics; and (4) we derive the optimal chain-of-thought length as
$k^* = \Theta(\sqrt{n}\log(1/\varepsilon))$ with explicit constants, providing
a principled approach to reduce inference costs while maintaining performance.
Empirical validation on GPT-3 confirms predictions (1)-(3), with transformers
reaching 99\% of theoretical entropy limits within 20 examples. Our framework
provides practical methods for extracting calibrated uncertainty estimates from
position-aware architectures and optimizing computational efficiency in
deployment.

</details>


### [165] [Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?](https://arxiv.org/abs/2507.11891)
*Shuangning Li,Chonghuan Wang,Jingyan Wang*

Main category: stat.ML

TL;DR: 论文研究了A/B实验中因数据共享导致的共生偏差问题，重点探讨了全局处理效应（GTE）符号的重要性，并在多臂老虎机框架下分析了偏差对算法选择的影响。


<details>
  <summary>Details</summary>
Motivation: 标准差异均值估计器在估计全局处理效应时因实验单元间的干扰而存在偏差，这种干扰源于数据共享。研究旨在理解共生偏差对算法选择的影响，尤其是GTE符号的重要性。

Method: 通过多臂老虎机框架，理论分析了数据共享下GTE估计的符号与真实GTE符号的关系，并探讨了探索与利用的平衡对偏差的影响。

Result: 研究发现探索与利用的平衡是共生偏差影响算法选择的关键因素。

Conclusion: 研究强调了GTE符号在算法选择中的重要性，并提供了共生偏差对决策影响的理论基础。

Abstract: We study A/B experiments that are designed to compare the performance of two
recommendation algorithms. Prior work has shown that the standard
difference-in-means estimator is biased in estimating the global treatment
effect (GTE) due to a particular form of interference between experimental
units. Specifically, units under the treatment and control algorithms
contribute to a shared pool of data that subsequently train both algorithms,
resulting in interference between the two groups. The bias arising from this
type of data sharing is known as "symbiosis bias". In this paper, we highlight
that, for decision-making purposes, the sign of the GTE often matters more than
its precise magnitude when selecting the better algorithm. We formalize this
insight under a multi-armed bandit framework and theoretically characterize
when the sign of the expected GTE estimate under data sharing aligns with or
contradicts the sign of the true GTE. Our analysis identifies the level of
exploration versus exploitation as a key determinant of how symbiosis bias
impacts algorithm selection.

</details>


### [166] [Newfluence: Boosting Model interpretability and Understanding in High Dimensions](https://arxiv.org/abs/2507.11895)
*Haolin Zou,Arnab Auddy,Yongchan Kwon,Kamiar Rahnama Rad,Arian Maleki*

Main category: stat.ML

TL;DR: 论文探讨了高维环境下影响函数的准确性，并提出了一种新的替代方法Newfluence，以提高解释复杂AI模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能模型的复杂性增加，需要更有效的工具来解释和优化模型决策。传统影响函数在低维假设下有效，但在高维环境中表现不佳。

Method: 通过理论和实证分析评估影响函数在高维环境中的准确性，并提出新的近似方法Newfluence。

Result: 研究发现影响函数在高维环境中不可靠，而Newfluence在保持计算效率的同时显著提高了准确性。

Conclusion: Newfluence为解释复杂AI模型提供了更准确的工具，其高维框架还可用于分析其他流行技术。

Abstract: The increasing complexity of machine learning (ML) and artificial
intelligence (AI) models has created a pressing need for tools that help
scientists, engineers, and policymakers interpret and refine model decisions
and predictions. Influence functions, originating from robust statistics, have
emerged as a popular approach for this purpose.
  However, the heuristic foundations of influence functions rely on
low-dimensional assumptions where the number of parameters $p$ is much smaller
than the number of observations $n$. In contrast, modern AI models often
operate in high-dimensional regimes with large $p$, challenging these
assumptions.
  In this paper, we examine the accuracy of influence functions in
high-dimensional settings. Our theoretical and empirical analyses reveal that
influence functions cannot reliably fulfill their intended purpose. We then
introduce an alternative approximation, called Newfluence, that maintains
similar computational efficiency while offering significantly improved
accuracy.
  Newfluence is expected to provide more accurate insights than many existing
methods for interpreting complex AI models and diagnosing their issues.
Moreover, the high-dimensional framework we develop in this paper can also be
applied to analyze other popular techniques, such as Shapley values.

</details>


### [167] [Incorporating Fairness Constraints into Archetypal Analysis](https://arxiv.org/abs/2507.12021)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.ML

TL;DR: Fair Archetypal Analysis (FairAA) and its nonlinear extension FairKernelAA address fairness in archetypal analysis by reducing sensitive group influence while maintaining interpretability and utility.


<details>
  <summary>Details</summary>
Motivation: Archetypal Analysis (AA) may encode sensitive attributes, raising fairness concerns. This work aims to mitigate such biases.

Method: Proposes FairAA with fairness regularization and FairKernelAA for nonlinear data, evaluated on synthetic and real-world datasets.

Result: FairAA reduces group separability without significantly compromising explained variance, achieving a balance between utility and fairness.

Conclusion: FairAA is a promising tool for responsible representation learning in sensitive applications.

Abstract: Archetypal Analysis (AA) is an unsupervised learning method that represents
data as convex combinations of extreme patterns called archetypes. While AA
provides interpretable and low-dimensional representations, it can
inadvertently encode sensitive attributes, leading to fairness concerns. In
this work, we propose Fair Archetypal Analysis (FairAA), a modified formulation
that explicitly reduces the influence of sensitive group information in the
learned projections. We also introduce FairKernelAA, a nonlinear extension that
addresses fairness in more complex data distributions. Our approach
incorporates a fairness regularization term while preserving the structure and
interpretability of the archetypes. We evaluate FairAA and FairKernelAA on
synthetic datasets, including linear, nonlinear, and multi-group scenarios,
demonstrating their ability to reduce group separability -- as measured by mean
maximum discrepancy and linear separability -- without substantially
compromising explained variance. We further validate our methods on the
real-world ANSUR I dataset, confirming their robustness and practical utility.
The results show that FairAA achieves a favorable trade-off between utility and
fairness, making it a promising tool for responsible representation learning in
sensitive applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [168] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

Main category: cs.CR

TL;DR: 本文综述了当前ML安全及文档化的现状，指出实践中安全意识和文档标准化的不足，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: ML实践中安全意识和文档质量普遍较低，现有标准未得到充分采用，亟需改进。

Method: 基于现有文献综述，提出扩展文档标准以包含安全部分，并推荐在ML文档中采用。

Result: 发现ML文档中安全相关内容缺失，建议扩展Model Cards和Datasheets标准。

Conclusion: 改进ML文档中的安全内容是提升ML安全的关键一步。

Abstract: This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


### [169] [A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy](https://arxiv.org/abs/2507.12098)
*Xiang Li,Yifan Lin,Yuanzhe Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种结合联邦学习和差分隐私的框架，用于解决个性化广告中的隐私泄露和性能问题。


<details>
  <summary>Details</summary>
Motivation: 解决个性化广告中的隐私泄露和性能问题。

Method: 结合分布式特征提取、动态隐私预算分配和鲁棒模型聚合，同时引入多方安全计算和异常检测机制。

Result: 实验表明，该框架在保证隐私的同时，实现了推荐准确性和系统效率的双重优化。

Conclusion: 该框架为广告推荐中的隐私保护技术提供了实用解决方案和理论基础。

Abstract: To mitigate privacy leakage and performance issues in personalized
advertising, this paper proposes a framework that integrates federated learning
and differential privacy. The system combines distributed feature extraction,
dynamic privacy budget allocation, and robust model aggregation to balance
model accuracy, communication overhead, and privacy protection. Multi-party
secure computing and anomaly detection mechanisms further enhance system
resilience against malicious attacks. Experimental results demonstrate that the
framework achieves dual optimization of recommendation accuracy and system
efficiency while ensuring privacy, providing both a practical solution and a
theoretical foundation for applying privacy protection technologies in
advertisement recommendation.

</details>
