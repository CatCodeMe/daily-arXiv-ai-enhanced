<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 9]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DS](#cs.DS) [Total: 6]
- [cs.SE](#cs.SE) [Total: 20]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 171]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [math.ST](#math.ST) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [math.NA](#math.NA) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 18]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [hep-th](#hep-th) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 11]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.CV](#cs.CV) [Total: 14]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report](https://arxiv.org/abs/2506.15831)
*Jongjun Park,Fei Chiang,Mostafa Milani*

Main category: cs.DB

TL;DR: 论文提出AnDri系统，用于在概念漂移存在的情况下进行异常检测，通过动态调整正常模式并区分异常子序列与新概念，同时引入新的聚类方法AHC。


<details>
  <summary>Details</summary>
Motivation: 数据分布的变化（概念漂移）和异常变化对时间序列异常检测带来挑战，现有方法通常孤立处理这两者，导致检测精度下降和模型更新开销增加。

Method: 开发AnDri系统，动态调整正常模式并区分异常子序列与新概念；提出Adjacent Hierarchical Clustering (AHC)聚类方法，考虑时间局部性。

Result: AnDri系统能够有效区分概念漂移和异常，提高检测准确性。

Conclusion: AnDri系统解决了概念漂移与异常检测的联合问题，通过AHC方法优化了聚类效果，为下游任务提供了更可靠的分析基础。

Abstract: Data changes to reflect evolving user behaviour, preferences, and changes in
the environment. Such changes may occur due to expected shifts in the data
distribution, i.e., concept drift, or unexpected anomalous changes. The
presence of concept drift poses challenges for anomaly detection in time
series. While anomalies are caused by undesirable changes in the data,
differentiating abnormal changes from varying normal behaviours is difficult
due to differing frequencies of occurrence, varying time intervals when normal
patterns occur. Differentiating between concept drift and anomalies is critical
for accurate analysis as studies have shown that the compounding effects of
error propagation in downstream data analysis tasks lead to lower detection
accuracy and increased overhead due to unnecessary model updates.
Unfortunately, existing work has largely explored anomaly detection and concept
drift detection in isolation. We develop AnDri, a system for Anomaly detection
in the presence of Drift, which adjusts the normal patterns temporally, and
distinguish abnormal subsequences and new concepts. Moreover, it introduces a
new clustering method, Adjacent Hierarchical Clustering (AHC), which groups
similar subsequences while respecting their temporal locality.

</details>


### [2] [Delta: A Learned Mixed Cost-based Query Optimization Framework](https://arxiv.org/abs/2506.15848)
*Jiazhen Peng,Zheng Qu,Xiaoye Miao,Rong Zhu*

Main category: cs.DB

TL;DR: Delta是一种混合成本查询优化框架，通过兼容性检测器和两阶段规划器，解决了现有优化器的搜索空间爆炸和训练成本高的问题，显著提升了查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有查询优化器存在搜索空间爆炸、训练成本高和准确性不足的问题，缺乏对性能不佳查询的检测机制。

Method: Delta采用兼容性检测器过滤不兼容查询，并通过两阶段规划器（粗粒度筛选和细粒度排序）生成高效计划。

Result: 实验表明，Delta在三个工作负载上平均提速2.34倍，优于PostgreSQL和现有学习方法。

Conclusion: Delta通过混合成本优化和两阶段规划，显著提升了查询优化效率和计划质量。

Abstract: Query optimizer is a crucial module for database management systems. Existing
optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic
programming with cost models but face search space explosion and heuristic
pruning constraints; (2) value-based ones train value networks to enable
efficient beam search, but incur higher training costs and lower accuracy. They
also lack mechanisms to detect queries where they may perform poorly. To
determine more efficient plans, we propose Delta, a mixed cost-based query
optimization framework that consists of a compatible query detector and a
two-stage planner. Delta first employs a Mahalanobis distancebased detector to
preemptively filter out incompatible queries where the planner might perform
poorly. For compatible queries, Delta activates its two-stage mixed cost-based
planner. Stage I serves as a coarse-grained filter to generate high-quality
candidate plans based on the value network via beam search, relaxing precision
requirements and narrowing the search space. Stage II employs a fine-grained
ranker to determine the best plan from the candidate plans based on a learned
cost model. Moreover, to reduce training costs, we reuse and augment the
training data from stage I to train the model in stage II. Experimental results
on three workloads demonstrate that Delta identifies higher-quality plans,
achieving an average 2.34x speedup over PostgreSQL and outperforming the
state-of-the-art learned methods by 2.21x.

</details>


### [3] [Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive Awareness Capabilities](https://arxiv.org/abs/2506.15986)
*Jiancheng Ruan,Tingyang Chen,Renchi Yang,Xiangyu Ke,Yunjun Gao*

Main category: cs.DB

TL;DR: GATE提出了一种基于图的高维近似最近邻搜索方法，通过自适应拓扑和查询感知优化查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有图方法未能充分利用拓扑信息且存在数据分布不匹配问题。

Method: GATE利用中心节点和对比学习模型优化入口点选择，构建导航图索引。

Result: 实验表明GATE查询性能提升1.2-2.0倍。

Conclusion: GATE通过轻量级模块显著提升图索引的查询效率。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds
extensive applications in databases, information retrieval, recommender
systems, etc. While graph-based methods have emerged as the leading solution
for ANNS due to their superior query performance, they still face several
challenges, such as struggling with local optima and redundant computations.
These issues arise because existing methods (i) fail to fully exploit the
topological information underlying the proximity graph G, and (ii) suffer from
severe distribution mismatches between the base data and queries in practice.
  To this end, this paper proposes GATE, high-tier proximity Graph with
Adaptive Topology and Query AwarEness, as a lightweight and adaptive module
atop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates
the critical problem to identify an optimal entry point in the proximity graph
for a given query, facilitating faster online search. By leveraging the
inherent clusterability of high-dimensional data, GATE first extracts a small
set of hub nodes V as candidate entry points. Then, resorting to a contrastive
learning-based two-tower model, GATE encodes both the structural semantics
underlying G and the query-relevant features into the latent representations of
these hub nodes V. A navigation graph index on V is further constructed to
minimize the model inference overhead. Extensive experiments demonstrate that
GATE achieves a 1.2-2.0X speed-up in query performance compared to
state-of-the-art graph-based indexes.

</details>


### [4] [Filter-Centric Vector Indexing: Geometric Transformation for Efficient Filtered Vector Search](https://arxiv.org/abs/2506.15987)
*Alireza Heidari,Wei Zhang*

Main category: cs.DB

TL;DR: FCVI是一种新型框架，通过数学变换将过滤条件直接编码到向量空间，显著提升了向量搜索的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前向量搜索应用中，性能和准确性之间的权衡问题亟待解决，需要一种兼容性强且高效的方法。

Method: 提出Filter-Centric Vector Indexing (FCVI)，通过数学变换ψ(v, f, α)将过滤条件编码到向量空间，兼容现有向量索引。

Result: FCVI比现有方法吞吐量高2.6-3.0倍，召回率相当，且在分布变化时表现稳定。

Conclusion: FCVI因其高性能、兼容性和稳定性，适用于需要灵活过滤的生产级向量搜索系统。

Abstract: The explosive growth of vector search applications demands efficient handling
of combined vector similarity and attribute filtering; a challenge where
current approaches force an unsatisfying choice between performance and
accuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework
that transforms this fundamental trade-off by directly encoding filter
conditions into the vector space through a mathematically principled
transformation $\psi(v, f, \alpha)$. Unlike specialized solutions, FCVI works
with any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical
guarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI
achieves 2.6-3.0 times higher throughput than state-of-the-art methods while
maintaining comparable recall. More remarkably, FCVI exhibits exceptional
stability under distribution shifts; maintaining consistent performance when
filter patterns or vector distributions change, unlike traditional approaches
that degrade significantly. This combination of performance, compatibility, and
resilience positions FCVI as an immediately applicable solution for production
vector search systems requiring flexible filtering capabilities.

</details>


### [5] [Data-Agnostic Cardinality Learning from Imperfect Workloads](https://arxiv.org/abs/2506.16007)
*Peizhi Wu,Rong Kang,Tieying Zhang,Jianjun Chen,Ryan Marcus,Zachary G. Ives*

Main category: cs.DB

TL;DR: GRASP是一种数据无关的基数估计系统，适用于现实世界中的不完整和不平衡查询负载，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基数估计方法依赖全局数据访问，但受限于组织政策；现有查询驱动模型假设理想训练负载，不适用于现实场景。

Method: GRASP采用组合设计，泛化到未见过的连接模板，并引入新的每表基数估计模型和计数草图模型。

Result: GRASP在三个数据库实例中表现优于现有方法，且在复杂基准测试中性能接近或超过传统方法。

Conclusion: GRASP在无数据访问和有限训练负载下仍能实现高性能基数估计。

Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.

</details>


### [6] [PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking](https://arxiv.org/abs/2506.16379)
*Yan Zhou,Chunwei Liu,Bhuvan Urgaonkar,Zhengle Wang,Magnus Mueller,Chao Zhang,Songyue Zhang,Pascal Pfeil,Dominik Horn,Zhengchun Liu,Davide Pagano,Tim Kraska,Samuel Madden,Ju Fan*

Main category: cs.DB

TL;DR: 论文提出PBench，一种新型工作负载合成器，通过优化选择和组合现有基准测试中的组件，生成接近真实云工作负载统计数据的合成工作负载。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准测试（如TPC-H和TPC-DS）无法捕捉真实云工作负载的执行统计信息，而云数据库供应商发布的真实工作负载痕迹又缺乏关键组件（如原始SQL查询和底层数据库）。

Method: 提出PBench，通过多目标优化选择组件、设计时间戳分配方法以及利用大语言模型（LLMs）生成额外组件来解决工作负载合成问题。

Result: PBench在真实云工作负载痕迹上的评估显示，其近似误差比现有方法降低了6倍。

Conclusion: PBench通过合成工作负载有效解决了现有基准测试的局限性，为云数据系统优化提供了更真实的评估工具。

Abstract: Cloud service providers commonly use standard benchmarks like TPC-H and
TPC-DS to evaluate and optimize cloud data analytics systems. However, these
benchmarks rely on fixed query patterns and fail to capture the real execution
statistics of production cloud workloads. Although some cloud database vendors
have recently released real workload traces, these traces alone do not qualify
as benchmarks, as they typically lack essential components like the original
SQL queries and their underlying databases. To overcome this limitation, this
paper introduces a new problem of workload synthesis with real statistics,
which aims to generate synthetic workloads that closely approximate real
execution statistics, including key performance metrics and operator
distributions, in real cloud workloads. To address this problem, we propose
PBench, a novel workload synthesizer that constructs synthetic workloads by
judiciously selecting and combining workload components (i.e., queries and
databases) from existing benchmarks. This paper studies the key challenges in
PBench. First, we address the challenge of balancing performance metrics and
operator distributions by introducing a multi-objective optimization-based
component selection method. Second, to capture the temporal dynamics of real
workloads, we design a timestamp assignment method that progressively refines
workload timestamps. Third, to handle the disparity between the original
workload and the candidate workload, we propose a component augmentation
approach that leverages large language models (LLMs) to generate additional
workload components while maintaining statistical fidelity. We evaluate PBench
on real cloud workload traces, demonstrating that it reduces approximation
error by up to 6x compared to state-of-the-art methods.

</details>


### [7] [LDI: Localized Data Imputation](https://arxiv.org/abs/2506.16616)
*Soroush Omidvartehrani,Davood Rafiei*

Main category: cs.DB

TL;DR: LDI（局部数据填补）是一种新框架，通过选择上下文相关的属性和元组子集，提升基于LLM的数据填补的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现实表格数据中缺失值常见且影响分析，现有LLM填补方法因使用宽泛提示而降低准确性、可扩展性和可解释性。

Method: LDI通过局部化提示选择相关数据子集，减少噪声并提高可追溯性，适用于托管LLM和轻量本地模型。

Result: 实验显示LDI优于现有方法，托管LLM准确率提升8%，轻量模型提升显著（最高97%）。

Conclusion: LDI在准确性、可解释性和鲁棒性上表现优异，适合高风险和隐私敏感应用。

Abstract: Missing values are a common challenge in real-world tabular data and can
significantly impair downstream analysis. While Large Language Models (LLMs)
have recently shown promise in data imputation, existing methods often rely on
broad, unfiltered prompts that compromise accuracy, scalability, and
explainability. We introduce LDI (Localized Data Imputation), a novel framework
that improves both the accuracy and transparency of LLM-based imputation by
selecting a compact, contextually relevant subset of attributes and tuples for
each missing value. This localized prompting reduces noise, enables
traceability by revealing which data influenced each prediction, and is
effective across both hosted LLMs and lightweight local models. Our extensive
experiments on four real-world datasets show that LDI outperforms
state-of-the-art methods, achieving up to 8% higher accuracy when using hosted
LLMs. The gains are more substantial with lightweight local models, reaching
nearly 17% and 97% accuracy on some datasets when using 3 and 10 examples,
respectively. In addition to higher accuracy, LDI offers improved
interpretability and robustness to data inconsistencies, making it well-suited
for high-stakes and privacy-sensitive applications.

</details>


### [8] [Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms](https://arxiv.org/abs/2506.16923)
*Omer Abramovich,Daniel Deutch,Nave Frost,Ahmet Kara,Dan Olteanu*

Main category: cs.DB

TL;DR: 提出了一种计算输入元组对查询结果贡献的新方法，首次实现了对聚合查询的实用化处理，并通过两种优化显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对无聚合查询，缺乏对聚合查询的实用化解决方案，因此需要一种高效且通用的方法。

Method: 采用两种优化：一是识别贡献相同的元组以减少计算量，二是利用查询谱系的梯度高效计算所有元组的贡献。

Result: 实验表明，该方法在无聚合查询上性能提升3个数量级，并能有效处理聚合查询。

Conclusion: 该方法显著提升了查询贡献计算的效率和实用性，尤其适用于聚合查询。

Abstract: In this paper, we introduce a novel approach to computing the contribution of
input tuples to the result of the query, quantified by the Banzhaf and Shapley
values. In contrast to prior algorithmic work that focuses on
Select-Project-Join-Union queries, ours is the first practical approach for
queries with aggregates. It relies on two novel optimizations that are
essential for its practicality and significantly improve the runtime
performance already for queries without aggregates. The first optimization
exploits the observation that many input tuples have the same contribution to
the query result, so it is enough to compute the contribution of one of them.
The second optimization uses the gradient of the query lineage to compute the
contributions of all tuples with the same complexity as for one of them.
Experiments with a million instances over 3 databases show that our approach
achieves up to 3 orders of magnitude runtime improvements over the
state-of-the-art for queries without aggregates, and that it is practical for
aggregate queries.

</details>


### [9] [PUL: Pre-load in Software for Caches Wouldn't Always Play Along](https://arxiv.org/abs/2506.16976)
*Arthur Bernhardt,Sajjad Tamimi,Florian Stock,Andreas Koch,Ilia Petrov*

Main category: cs.DB

TL;DR: 论文探讨了在近数据处理环境中，基于软件的预取技术如何通过计算/IO交错最大化计算利用率。


<details>
  <summary>Details</summary>
Motivation: 内存延迟和带宽是限制系统性能和可扩展性的主要因素，现代CPU通过大缓存、乱序执行或复杂硬件预取器来隐藏延迟，但基于软件的预取效率更高。

Method: 研究基于软件的、后摩尔时代的系统，将操作卸载到智能内存中。

Result: 研究表明，在近数据处理环境中，基于软件的预取具有更高的潜力。

Conclusion: 通过计算/IO交错，基于软件的预取可以显著提升计算利用率。

Abstract: Memory latencies and bandwidth are major factors, limiting system performance
and scalability. Modern CPUs aim at hiding latencies by employing large caches,
out-of-order execution, or complex hardware prefetchers. However,
software-based prefetching exhibits higher efficiency, improving with newer CPU
generations.
  In this paper we investigate software-based, post-Moore systems that offload
operations to intelligent memories. We show that software-based prefetching has
even higher potential in near-data processing settings by maximizing compute
utilization through compute/IO interleaving.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [TrainVerify: Equivalence-Based Verification for Distributed LLM Training](https://arxiv.org/abs/2506.15961)
*Yunchi Lu,Youshan Miao,Cheng Tan,Peng Huang,Yi Zhu,Xian Zhang,Fan Yang*

Main category: cs.DC

TL;DR: TrainVerify是一个用于验证大规模语言模型分布式训练的系统，确保并行执行计划与逻辑规范一致，避免错误和资源浪费。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练成本高昂且缺乏验证，容易导致错误和资源浪费。

Method: 通过形状缩减技术和分阶段并行验证算法，降低验证复杂度，同时保持形式正确性。

Result: 成功验证了Llama3（405B）和DeepSeek-V3（671B）等前沿模型的训练计划。

Conclusion: TrainVerify为大规模语言模型的分布式训练提供了高效且可靠的验证方法。

Abstract: Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.

</details>


### [11] [NetSenseML: Network-Adaptive Compression for Efficient Distributed Machine Learning](https://arxiv.org/abs/2506.16235)
*Yisu Wang,Xinjiao Li,Ruilong Wu,Huangxun Chen,Dirk Kutscher*

Main category: cs.DC

TL;DR: NetSenseML是一种网络自适应的分布式深度学习框架，通过动态调整梯度压缩策略来平衡网络负载和模型精度。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式机器学习训练对网络基础设施需求高，容易导致拥塞和性能下降，现有梯度压缩技术常牺牲模型精度。

Method: NetSenseML实时监测网络状况，动态调整量化、剪枝和压缩策略，仅在网络拥塞影响收敛速度时应用压缩。

Result: 实验表明，NetSenseML在带宽受限条件下，训练吞吐量提升1.55至9.84倍。

Conclusion: NetSenseML通过自适应数据压缩，有效缩短收敛时间并提升训练效率。

Abstract: Training large-scale distributed machine learning models imposes considerable
demands on network infrastructure, often resulting in sudden traffic spikes
that lead to congestion, increased latency, and reduced throughput, which would
ultimately affect convergence times and overall training performance. While
gradient compression techniques are commonly employed to alleviate network
load, they frequently compromise model accuracy due to the loss of gradient
information.
  This paper introduces NetSenseML, a novel network adaptive distributed deep
learning framework that dynamically adjusts quantization, pruning, and
compression strategies in response to real-time network conditions. By actively
monitoring network conditions, NetSenseML applies gradient compression only
when network congestion negatively impacts convergence speed, thus effectively
balancing data payload reduction and model accuracy preservation.
  Our approach ensures efficient resource usage by adapting reduction
techniques based on current network conditions, leading to shorter convergence
times and improved training efficiency. We present the design of the NetSenseML
adaptive data reduction function and experimental evaluations show that
NetSenseML can improve training throughput by a factor of 1.55 to 9.84 times
compared to state-of-the-art compression-enabled systems for representative DDL
training jobs in bandwidth-constrained conditions.

</details>


### [12] [A Study of Synchronization Methods for Concurrent Size](https://arxiv.org/abs/2506.16350)
*Hen Kas-Sharir,Gal Sela,Erez Petrank*

Main category: cs.DC

TL;DR: 研究同步方法以优化并发数据结构中`size`方法的性能，发现不同场景需采用不同同步技术。


<details>
  <summary>Details</summary>
Motivation: 在并发环境中，线性化`size`方法会带来显著开销，即使未调用该方法。研究旨在减少这种开销。

Method: 比较了握手技术、乐观技术和基于锁的技术，并与现有方法进行对比。

Result: 选择合适的同步方法可显著减少开销，但无通用最优解。低争用场景适合乐观和基于锁方法，高争用场景适合握手和无等待方法。

Conclusion: 同步方法的选择需根据具体场景，研究结果与并发计算的普遍趋势一致。

Abstract: The size of collections, maps, and data structures in general, constitutes a
fundamental property. An implementation of the size method is required in most
programming environments. Nevertheless, in a concurrent environment,
integrating a linearizable concurrent size introduces a noticeable overhead on
all operations of the data structure, even when the size method is not invoked
during the execution. In this work we present a study of synchronization
methods in an attempt to improve the performance of the data structure. In
particular, we study a handshake technique that is commonly used with
concurrent garbage collection, an optimistic technique, and a lock-based
technique. Evaluation against the state-of-the-art size methodology
demonstrates that the overhead can be significantly reduced by selecting the
appropriate synchronization approach, but there is no one-size-fits-all method.
Different scenarios call for different synchronization methods, as rigorously
shown in this study. Nevertheless, our findings align with general trends in
concurrent computing. In scenarios characterized by low contention, optimistic
and lock-based approaches work best, whereas under high contention, the most
effective solutions are the handshake approach and the wait-free approach.

</details>


### [13] [Parallel Point-to-Point Shortest Paths and Batch Queries](https://arxiv.org/abs/2506.16488)
*Xiaojun Dong,Andy Li,Yan Gu,Yihan Sun*

Main category: cs.DC

TL;DR: Orionet提出了一种高效的并行点对点最短路径（PPSP）查询方法，结合双向搜索（BiDS）和其他启发式算法，并扩展到批量查询。实验显示其性能显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 点对点最短路径（PPSP）查询在实际应用中需求广泛，但现有并行方法效率不足，尤其是批量查询场景。

Method: 基于现有单源最短路径（SSSP）框架，引入剪枝条件，开发了并行PPSP算法，包括早期终止、双向搜索、A*搜索及其双向版本。批量查询通过查询图抽象实现信息共享。

Result: 在14种图上测试，双向搜索平均比GraphIt快2.9倍，比MBQ快6.8倍；双向A*比GraphIt和MBQ分别快4.4倍和6.2倍。批量查询性能同样优越。

Conclusion: Orionet通过高效并行化和批量查询优化，显著提升了PPSP查询性能，适用于实际应用场景。

Abstract: We propose Orionet, efficient parallel implementations of Point-to-Point
Shortest Paths (PPSP) queries using bidirectional search (BiDS) and other
heuristics, with an additional focus on batch PPSP queries. We present a
framework for parallel PPSP built on existing single-source shortest paths
(SSSP) frameworks by incorporating pruning conditions. As a result, we develop
efficient parallel PPSP algorithms based on early termination, bidirectional
search, A$^*$ search, and bidirectional A$^*$ all with simple and efficient
implementations.
  We extend our idea to batch PPSP queries, which are widely used in real-world
scenarios. We first design a simple and flexible abstraction to represent the
batch so PPSP can leverage the shared information of the batch. Orionet
formalizes the batch as a query graph represented by edges between queried
sources and targets. In this way, we directly extended our PPSP framework to
batched queries in a simple and efficient way.
  We evaluate Orionet on both single and batch PPSP queries using various graph
types and distance percentiles of queried pairs, and compare it against two
baselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$
using unidirectional search. On 14 graphs we tested, on average, our
bidirectional search is 2.9$\times$ faster than GraphIt, and 6.8$\times$ faster
than MBQ. Our bidirectional A$^*$ is 4.4$\times$ and 6.2$\times$ faster than
the A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also
provide in-depth experimental evaluation, and show that Orionet provides strong
performance compared to the plain solutions.

</details>


### [14] [Enabling Blockchain Interoperability Through Network Discovery Services](https://arxiv.org/abs/2506.16611)
*Khalid Hassan,Amirreza Sokhankhosh,Sara Rouhani*

Main category: cs.DC

TL;DR: 本文提出了一种去中心化的区块链网络发现架构，解决了现有解决方案中网络初始发现未被解决的问题，并通过激励机制鼓励节点参与。


<details>
  <summary>Details</summary>
Motivation: 现有区块链互操作性解决方案假设网络已相互知晓，但初始发现问题未被解决，本文旨在填补这一空白。

Method: 提出一种去中心化架构，支持区块链网络和资产/服务的发现，并设计激励机制。使用Substrate框架实现并评估。

Result: 架构在测试配置下可处理13万并发请求，中位响应时间为5.5毫秒，具备扩展性。

Conclusion: 去中心化发现架构具有韧性和可扩展性，为区块链互操作性提供了新思路。

Abstract: Web3 technologies have experienced unprecedented growth in the last decade,
achieving widespread adoption. As various blockchain networks continue to
evolve, we are on the cusp of a paradigm shift in which they could provide
services traditionally offered by the Internet, but in a decentralized manner,
marking the emergence of the Internet of Blockchains. While significant
progress has been achieved in enabling interoperability between blockchain
networks, existing solutions often assume that networks are already mutually
aware. This reveals a critical gap: the initial discovery of blockchain
networks remains largely unaddressed. This paper proposes a decentralized
architecture for blockchain network discovery that operates independently of
any centralized authority. We also introduce a mechanism for discovering assets
and services within a blockchain from external networks. Given the
decentralized nature of the proposed discovery architecture, we design an
incentive mechanism to encourage nodes to actively participate in maintaining
the discovery network. The proposed architecture implemented and evaluated,
using the Substrate framework, demonstrates its resilience and scalability,
effectively handling up to 130,000 concurrent requests under the tested network
configurations, with a median response time of 5.5 milliseconds, demonstrating
the ability to scale its processing capacity further by increasing its network
size.

</details>


### [15] [JANUS: Resilient and Adaptive Data Transmission for Enabling Timely and Efficient Cross-Facility Scientific Workflows](https://arxiv.org/abs/2506.17084)
*Vladislav Esaulov,Jieyang Chen,Norbert Podhorszki,Fred Suter,Scott Klasky,Anu G Bourgeois,Lipeng Wan*

Main category: cs.DC

TL;DR: JANUS是一种用于跨设施科学工作流的自适应数据传输方法，通过UDP、纠删码和有损压缩提高效率。


<details>
  <summary>Details</summary>
Motivation: 现代科学项目中，跨设施工作流的数据传输面临带宽压力、TCP重传和纠删码高开销等问题。

Method: JANUS结合UDP、纠删码和误差有界压缩，动态调整编码参数以适应网络条件。

Result: 实验表明，JANUS显著提高了数据传输效率，同时保持了数据保真度。

Conclusion: JANUS为跨设施科学工作流提供了一种高效、灵活的数据传输解决方案。

Abstract: In modern science, the growing complexity of large-scale projects has
increased reliance on cross-facility workflows, where institutions share
resources and expertise to accelerate discovery. These workflows often involve
transferring massive data over wide-area networks. While high-speed networks
like ESnet and data transfer services like Globus have improved data mobility,
challenges remain. Large data volumes can strain bandwidth, TCP suffers from
retransmissions due to packet loss, and traditional fault-tolerance methods
like erasure coding introduce significant overhead.
  This paper presents JANUS, a resilient and adaptive data transmission
approach for cross-facility scientific workflows. JANUS uses UDP, integrates
erasure coding for fault tolerance, and applies error-bounded lossy compression
to reduce overhead. This design enables users to balance transmission time and
accuracy based on specific needs. JANUS also adapts coding parameters to
real-time network conditions and uses optimization models to determine ideal
configurations. Experiments show that JANUS significantly improves data
transfer efficiency while preserving fidelity.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [16] [Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products](https://arxiv.org/abs/2506.15793)
*Ruipeng Liu,Qinru Qiu,Simon Khan,Garrett E. Katz*

Main category: cs.DS

TL;DR: 提出了一种基于Kronecker乘积旋转矩阵的新代码本表示方法，显著提高了VSA中清理步骤的效率，时间复杂度和空间复杂度均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前VSA中的清理步骤计算复杂度高，成为性能瓶颈，需要更高效的方法。

Method: 利用Kronecker乘积的旋转矩阵表示代码本，实现线性对数时间复杂度和线性空间复杂度的清理。

Result: 清理步骤的时间复杂度为O(N log N)，空间复杂度为O(N)，代码本存储仅需O(log N)空间。

Conclusion: 新方法显著提升了VSA的扩展性，实验证明其性能优于传统技术。

Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is
the ``clean-up'' step, which decodes the noisy vectors retrieved from the
architecture. Clean-up typically compares noisy vectors against a ``codebook''
of prototype vectors, incurring computational complexity that is quadratic or
similar. We present a new codebook representation that supports efficient
clean-up, based on Kroneker products of rotation-like matrices. The resulting
clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$,
where $N$ is the vector dimension and also the number of vectors in the
codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the
codebook is not stored explicitly in computer memory: It can be represented in
$\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can
be materialized in $\mathcal{O}(N)$ time and space. At the same time,
asymptotic memory capacity remains comparable to standard approaches. Computer
experiments confirm these results, demonstrating several orders of magnitude
more scalability than baseline VSA techniques.

</details>


### [17] [HybHuff: Lossless Compression for Hypergraphs via Entropy-Guided Huffman-Bitwise Coordination](https://arxiv.org/abs/2506.15844)
*Tianyu Zhao,Dongfang Zhao,Luanzheng Guo,Nathan Tallent*

Main category: cs.DS

TL;DR: 提出了一种混合压缩框架，结合Huffman编码和位编码，显著降低超图的内存占用，压缩率优于Zip和ZFP。


<details>
  <summary>Details</summary>
Motivation: 超图在数据密集型应用中广泛使用，但其高内存占用限制了可扩展性。现有方法在计算效率上有所改进，但空间开销仍是主要挑战。

Method: 提出混合压缩框架，结合Huffman编码和位编码，通过理论分析确定最优编码比例，并引入经验策略近似该比例。

Result: 实验显示，压缩率比Zip和ZFP高2.3倍，解码开销相当。在常见超图任务中性能损失可忽略。

Conclusion: 该方法高效且实用，适用于多种超图任务，显著降低了内存占用。

Abstract: Hypergraphs provide a natural representation for many-to-many relationships
in data-intensive applications, yet their scalability is often hindered by high
memory consumption. While prior work has improved computational efficiency,
reducing the space overhead of hypergraph representations remains a major
challenge. This paper presents a hybrid compression framework for integer-based
hypergraph adjacency formats, which adaptively combines Huffman encoding and
bitwise encoding to exploit structural redundancy. We provide a theoretical
analysis showing that an optimal encoding ratio exists between the two schemes,
and introduce an empirical strategy to approximate this ratio for practical
use. Experiments on real-world hypergraphs demonstrate that our method
consistently outperforms standard compressors such as Zip and ZFP in
compression rate by up to 2.3x with comparable decoding overhead. To assess
practical utility, we integrate our framework with three common hypergraph
workloads: breadth-first search, PageRank, and k-core label propagation, and
show that compression incurs negligible performance loss. Extensive evaluations
across four benchmark datasets confirm the efficiency and applicability of our
approach.

</details>


### [18] [On the Efficient Discovery of Maximum $k$-Defective Biclique](https://arxiv.org/abs/2506.16121)
*Donghang Cui,Ronghua Li,Qiangqiang Dai,Hongchao Qin,Guoren Wang*

Main category: cs.DS

TL;DR: 论文提出了一种新的松弛子图模型——$k$-defective biclique，用于解决二分图中最大边$k$-defective biclique的识别问题，并证明了其NP难性。通过分支定界框架和新颖的优化技术，算法在效率和性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的二分图常包含噪声或不完整信息，传统biclique模型过于严格。因此，研究允许$k$条缺失边的松弛模型更具实际意义。

Method: 提出基于分支定界框架的新算法，结合图缩减方法、新颖上界和启发式策略，优化了计算效率。

Result: 实验表明，算法在10个大型真实数据集上表现优异，速度提升高达1000倍。

Conclusion: $k$-defective biclique模型及配套算法在噪声环境下高效且实用，显著优于现有方法。

Abstract: The problem of identifying the maximum edge biclique in bipartite graphs has
attracted considerable attention in bipartite graph analysis, with numerous
real-world applications such as fraud detection, community detection, and
online recommendation systems. However, real-world graphs may contain noise or
incomplete information, leading to overly restrictive conditions when employing
the biclique model. To mitigate this, we focus on a new relaxed subgraph model,
called the $k$-defective biclique, which allows for up to $k$ missing edges
compared to the biclique model. We investigate the problem of finding the
maximum edge $k$-defective biclique in a bipartite graph, and prove that the
problem is NP-hard. To tackle this computation challenge, we propose a novel
algorithm based on a new branch-and-bound framework, which achieves a
worst-case time complexity of $O(m\alpha_k^n)$, where $\alpha_k < 2$. We
further enhance this framework by incorporating a novel pivoting technique,
reducing the worst-case time complexity to $O(m\beta_k^n)$, where $\beta_k <
\alpha_k$. To improve the efficiency, we develop a series of optimization
techniques, including graph reduction methods, novel upper bounds, and a
heuristic approach. Extensive experiments on 10 large real-world datasets
validate the efficiency and effectiveness of the proposed approaches. The
results indicate that our algorithms consistently outperform state-of-the-art
algorithms, offering up to $1000\times$ speedups across various parameter
settings.

</details>


### [19] [Parallel batch queries on dynamic trees: algorithms and experiments](https://arxiv.org/abs/2506.16477)
*Humza Ikram,Andrew Brady,Daniel Anderson,Guy Blelloch*

Main category: cs.DS

TL;DR: 本文改进了批量并行动态树数据结构，支持更广泛的查询和任意度数，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 动态树数据结构是现代算法中的核心组件，但现有批量并行动态树在支持复杂查询和任意度数方面存在不足。

Method: 通过扩展RC树以支持任意度数和多种查询（如子树查询、路径查询等），并实现了一个通用批量动态树结构。

Result: 实验表明改进后的算法在性能和并行效率上表现良好，且对不同森林特性具有鲁棒性。

Conclusion: 本文提出的改进和实现为批量并行动态树提供了更通用的解决方案，适用于多种并行算法。

Abstract: Dynamic tree data structures maintain a forest while supporting insertion and
deletion of edges and a broad set of queries in $O(\log n)$ time per operation.
Such data structures are at the core of many modern algorithms. Recent work has
extended dynamic trees so as to support batches of updates or queries so as to
run in parallel, and these batch parallel dynamic trees are now used in several
parallel algorithms. In this work we describe improvements to batch parallel
dynamic trees, describe an implementation that incorporates these improvements,
and experiments using it. The improvements includes generalizing prior work on
RC (rake compress) trees to work with arbitrary degree while still supporting a
rich set of queries, and describing how to support batch subtree queries, path
queries, LCA queries, and nearest-marked-vertex queries in $O(k + k \log (1 +
n/k))$ work and polylogarithmic span. Our implementation is the first general
implementation of batch dynamic trees (supporting arbitrary degree and general
queries). Our experiments include measuring the time to create the trees,
varying batch sizes for updates and queries, and using the tree to implement
incremental batch-parallel minimum spanning trees. To run the experiments we
develop a forest generator that is parameterized to create distributions of
trees of differing characteristics (e.g., degree, depth, and relative tree
sizes). Our experiments show good speedup and that the algorithm performance is
robust across forest characteristics.

</details>


### [20] [LMQ-Sketch: Lagom Multi-Query Sketch for High-Rate Online Analytics](https://arxiv.org/abs/2506.16928)
*Martin Hilgendorf,Marina Papatriantafilou*

Main category: cs.DS

TL;DR: LMQ-Sketch是一种支持多查询与并发更新的数据草图方法，通过Lagom方法实现低延迟全局查询，具有高吞吐量和低内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决高容量、高速率数据中多类型查询与并发更新的集成问题。

Method: 提出LMQ-Sketch，结合Lagom方法实现低延迟全局查询，支持频率点查询、F1和F2频率矩。

Result: LMQ-Sketch在吞吐量、准确性和并发语义上优于现有方法，内存需求降低一个数量级。

Conclusion: LMQ-Sketch为并发多查询草图提供了高效解决方案，具有广泛的应用潜力。

Abstract: Data sketches balance resource efficiency with controllable approximations
for extracting features in high-volume, high-rate data. Two important points of
interest are highlighted separately in recent works; namely, to (1) answer
multiple types of queries from one pass, and (2) query concurrently with
updates. Several fundamental challenges arise when integrating these
directions, which we tackle in this work. We investigate the trade-offs to be
balanced and synthesize key ideas into LMQ-Sketch, a single, composite data
sketch supporting multiple queries (frequency point queries, frequency moments
F1, and F2) concurrently with updates. Our method 'Lagom' is a cornerstone of
LMQ-Sketch for low-latency global querying (<100 us), combining freshness,
timeliness, and accuracy with a low memory footprint and high throughput (>2B
updates/s). We analyze and evaluate the accuracy of Lagom, which builds on a
simple geometric argument and efficiently combines work distribution with
synchronization for proper concurrency semantics -- monotonicity of operations
and intermediate value linearizability. Comparing with state-of-the-art methods
(which, as mentioned, only cover either mixed queries or concurrency),
LMQ-Sketch shows highly competitive throughput, with additional accuracy
guarantees and concurrency semantics, while also reducing the required memory
budget by an order of magnitude. We expect the methodology to have broader
impact on concurrent multi-query sketches.

</details>


### [21] [When does FTP become FPT?](https://arxiv.org/abs/2506.17008)
*Matthias Bentert,Fedor V. Fomin,Petr A. Golovach,Laure Morelle*

Main category: cs.DS

TL;DR: 论文研究了Fault-Tolerant Path（FTP）问题的固定参数可处理性（FPT）和多项式核存在性，分析了多种参数化下的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨FTP问题在不同参数化下的计算复杂性，以填补该问题的复杂性图谱。

Method: 通过分析多种参数（如脆弱边数量、安全边数量、预算ℓ、最优解中的安全边或脆弱边数量等），研究FTP问题的FPT性和多项式核存在性。

Result: 提供了FTP问题在这些参数下的几乎完整的复杂性描述。

Conclusion: 论文填补了FTP问题复杂性图谱的空白，为未来研究提供了理论基础。

Abstract: In the problem Fault-Tolerant Path (FTP), we are given an edge-weighted
directed graph G = (V, E), a subset U \subseteq E of vulnerable edges, two
vertices s, t \in V, and integers k and \ell. The task is to decide whether
there exists a subgraph H of G with total cost at most \ell such that, after
the removal of any k vulnerable edges, H still contains an s-t-path. We study
whether Fault-Tolerant Path is fixed-parameter tractable (FPT) and whether it
admits a polynomial kernel under various parameterizations. Our choices of
parameters include: the number of vulnerable edges in the input graph, the
number of safe (i.e, invulnerable) edges in the input graph, the budget \ell,
the minimum number of safe edges in any optimal solution, the minimum number of
vulnerable edges in any optimal solution, the required redundancy k, and
natural above- and below-guarantee parameterizations. We provide an almost
complete description of the complexity landscape of FTP for these parameters.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: 研究了开源ML项目中社区气味与自认技术债务（SATD）的关系，发现某些气味与SATD强相关，并分析了其演化趋势。


<details>
  <summary>Details</summary>
Motivation: 探索ML项目中社区气味与SATD的相互作用，填补现有研究的空白。

Method: 分析了155个ML系统的发布数据，检测社区气味和SATD，并进行统计分析和演化趋势研究。

Result: 发现Radio Silence和Organizational Silos等气味与SATD强相关，且气味和SATD的演化与项目规模相关。

Conclusion: 早期检测和缓解社区气味对ML系统的长期质量和可持续性至关重要。

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [23] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: 本文综述了针对ROS自主系统（ROSAS）的回归测试优化技术，分析了122项研究，分类为测试用例优先级、最小化和选择方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统回归测试技术在ROSAS中面临动态行为、复杂数据结构和实时性等挑战，相关优化研究较少，需填补这一空白。

Method: 通过系统综述122项研究，分类并分析回归测试优化技术，提出结构化分类法，并讨论其适用性和局限性。

Result: 总结了ROSAS回归测试的主要挑战，如测试优先级、冗余测试最小化和受影响测试用例选择，并提出了未来研究方向。

Conclusion: 本文为ROSAS回归测试优化提供了基础参考和实践路线图，推动了该领域的技术发展。

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [24] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: GUIRepair是一种跨模态推理方法，通过理解和利用视觉信息解决多模态问题，显著提升了基于LLM的自动程序修复（APR）在多模态场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有APR系统在多模态问题场景（如SWE-bench M）中表现不佳，主要因为无法有效利用视觉信息。GUIRepair旨在填补这一空白。

Method: GUIRepair结合Image2Code和Code2Image两个组件，前者将GUI图像转换为可执行代码以理解故障，后者通过重现视觉场景验证修复效果。

Result: 在SWE-bench M上，GUIRepair表现优异，使用GPT-4o和o4-mini作为基础模型时，分别解决了157和175个实例，优于现有基线。

Conclusion: GUIRepair通过跨模态推理成功解决了多模态问题，验证了视觉信息在APR中的重要性。

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [25] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 研究探讨了大规模微服务系统中技术债务的表现形式，通过混合方法案例研究发现静态代码分析是发现技术债务的有效入口，沟通不足和组织架构不匹配会加剧技术债务，并提出了管理策略。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽然承诺高可维护性和可演进性，但技术债务对其质量属性构成显著威胁，目前缺乏大规模微服务系统中技术债务的研究。

Method: 采用混合方法案例研究，包括静态代码分析和定性访谈，研究对象为包含100多个微服务的工业系统。

Result: 发现静态代码分析是技术债务发现的有效工具，沟通不足和组织架构不匹配会加剧技术债务，微服务架构中存在技术债务快速积累与解决的循环现象。

Conclusion: 提出了一套适合微服务架构的技术债务管理策略，强调了沟通和组织架构对齐的重要性。

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [26] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 论文评估了大型语言模型（LLMs）在文档到代码追溯中的表现，发现其在准确性上优于基线方法，但存在命名假设和过度泛化等错误模式。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动化文档与代码追溯中的潜力，填补现有研究的空白。

Method: 使用Claude 3.5 Sonnet、GPT-4o和o3-mini等LLMs，在两个开源项目数据集上评估追溯链接的准确性、关系解释质量和多步链重建能力。

Result: 最佳LLM的F1分数达79.4%和80.4%，关系解释完全正确率为42.9%-71.1%，多步链端点准确性高但中间链接不稳定。

Conclusion: LLMs是强大的追溯工具，但需结合人工干预，并针对特定错误模式优化。

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [27] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: 论文通过分析67.6万条Gen-AI应用的用户评论，提出SARA方法，利用LLM技术提取用户对Gen-AI功能的评价，发现10大热门话题，并探讨其随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 研究用户对Gen-AI应用的感知和评价，填补现有研究的空白。

Method: 采用四阶段SARA方法（选择、获取、精炼、分析），结合LLM技术进行主题提取。

Result: LLM在主题提取中准确率达91%，识别出10大用户讨论话题（如AI性能、内容质量等），并分析其随时间的变化。

Conclusion: 研究为开发者和研究者提供了关于用户需求和期望的实用建议。

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [28] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: 提出了一种基于模块化的离散事件系统控制器合成方法，利用线性时序逻辑（LTL）目标，通过分解问题缓解状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统整体合成方法因状态爆炸问题而难以处理大规模系统的问题。

Method: 利用模块化结构，通过逐步解决较弱的控制问题构建最大允许安全控制器，并使用观测合成等价性减少局部事件。

Result: 合成结果具有模块化特性，控制器并行运行时能确保LTL目标，实验显示其可处理比整体方法大1000倍的问题。

Conclusion: 模块化方法显著提升了合成能力，适用于大规模系统。

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [29] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: 该研究探讨了将现代AI工具集成到质量保证（QA）流程中的潜力与挑战，通过实际案例验证了其有效性，但同时也指出了语义覆盖、黑盒性和可解释性等关键问题。


<details>
  <summary>Details</summary>
Motivation: 传统QA方法难以应对现代软件系统的复杂性、规模和快速迭代，资源有限导致质量成本高，因此研究AI工具在QA中的应用潜力。

Method: 综合分析了AI工具对验证和验证过程的影响，包括测试分析、测试用例生成等，并通过企业应用的端到端回归测试验证。

Result: 生成的测试用例仅有8.3%的不稳定执行，显示出潜力，但也发现语义覆盖、黑盒性和可解释性等挑战。

Conclusion: AI在QA中具有变革潜力，但需战略性地实施，并解决现有局限性，开发合适的验证方法。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [30] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 论文提出了一种混合方法，利用大型语言模型（LLMs）验证字符串自然语言（NL）需求的可满足性，并通过生成检查器提高准确性。


<details>
  <summary>Details</summary>
Motivation: 验证自然语言需求的可满足性在软件系统中至关重要，但传统方法（如SMT求解器）存在理论限制且需要大量人工翻译。LLMs为这一问题提供了新思路，但其在字符串需求验证中的效果尚未充分研究。

Method: 采用混合方法，利用LLMs（1）生成可满足性结果及一致字符串，（2）生成声明式（SMT）和命令式（Python）检查器以验证结果。实验评估了四种LLMs的性能。

Result: LLMs能有效将自然语言翻译为检查器，Python检查器测试准确率完美。检查器显著提升了LLMs生成一致字符串和识别不可满足需求的能力，某些情况下生成成功率和F1分数翻倍。

Conclusion: 混合方法通过LLMs和检查器的结合，显著提高了自然语言字符串需求验证的效率和准确性，为软件工程领域提供了新工具。

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [31] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: SemAgent通过结合问题、代码和执行语义，提出了一种新的工作流方法，显著提高了自动程序修复的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有系统在修复代码问题时过于局部化，缺乏对问题、代码和执行语义的深入理解，导致生成的补丁过拟合。

Method: SemAgent采用两阶段架构：修复阶段提出细粒度修复建议，评审阶段根据问题语义筛选相关修复。

Result: 在SWEBench-Lite基准测试中，SemAgent的解决率达到44.66%，优于其他工作流方法，并在多行推理和边缘情况处理上表现突出。

Conclusion: 将问题和代码语义融入APR流程可以生成更鲁棒和语义一致的修复。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [32] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: 论文讨论了大型语言模型编码工具在软件工程中的主流应用及其潜在风险，包括隐私数据泄露、安全漏洞和模型迎合错误观点的倾向（称为“奉承”）。作者建议企业应审查AI生成的代码、保护数据隐私、遵守安全法规，并测试模型的奉承行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型编码工具的普及，其在提升开发效率的同时也带来了隐私泄露、安全漏洞和模型奉承等新风险。

Method: 通过分析真实提示和生成代码片段，揭示了隐私泄露和安全问题的普遍性，并探讨了模型奉承行为的现象。

Result: 研究发现10%的提示泄露隐私数据，42%的生成代码片段存在安全漏洞，且模型会迎合错误观点。

Conclusion: 企业需审查AI生成的代码、保护数据隐私、遵守法规，并测试模型的奉承行为，以平衡效率与安全。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [33] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 该愿景论文探讨了AI系统的鲁棒性、可靠性及其安全性，强调问责制在构建可信AI中的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决AI系统在实际应用中的安全性和有效性，确保其可靠性和问责性。

Method: 通过文献综述和案例研究，分析了AI系统的挑战和现有方法，并提出创新测试方案。

Result: 研究强调了问责制对建立信任的关键作用，并指出了未来研究方向。

Conclusion: 鲁棒性、可靠性和问责制是未来可信AI系统发展的核心领域。

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [34] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文提出了一种将可解释性、透明度和可解释性融入自动驾驶系统验证与验证（V&V）流程的方法，旨在提高效率和用户信任。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统的决策模型复杂，验证与验证过程效率低且劳动密集，缺乏透明度和可解释性。

Method: 通过文献综述和利益相关者输入细化V&V需求，利用大型语言模型生成可解释测试场景，并在模拟环境中实现实时验证。

Result: 框架包括测试预言、解释生成和测试聊天机器人，计划通过实证研究评估诊断效率和透明度的改进。

Conclusion: 该方法旨在优化V&V流程，减少资源消耗，并增强用户对自动驾驶技术的信任。

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [35] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述（SLR）研究了量子或量子启发算法在解决经典软件工程（SE）优化问题中的应用，分析了77项研究，揭示了研究热点与空白。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统及其工程过程的复杂性增加，需要创新解决方案，量子计算在优化领域的进展为SE优化提供了新思路。

Method: 通过系统性搜索六个数字数据库，从2083篇文献中筛选出77项主要研究进行分析。

Result: 研究发现研究集中在SE操作和软件测试领域，其他SE活动存在显著空白，并发现了一些非传统SE渠道的相关研究。

Conclusion: 研究为SBSE社区提供了全面的研究概览，助力利用量子技术进步解决下一代SE挑战。

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [36] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文研究了在复杂软件系统中如何通过用户行为、系统事件和情感状态等指标动态识别解释需求，并通过在线研究收集了相关指标。


<details>
  <summary>Details</summary>
Motivation: 在数字化世界中，软件系统的复杂性增加，解释性需求日益重要，但传统方法易受偏见影响。

Method: 通过在线研究收集自我报告指标，整理出用户行为、系统事件和情感状态三类共39个指标。

Result: 建立了17个用户行为指标、8个系统事件指标和14个情感状态指标，并分析了它们与解释需求类型的关系。

Conclusion: 这些指标可用于原型设计和已部署应用的远程数据收集，动态触发解释，提升系统解释性。

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


### [37] [Behavior Driven Development for 3D Games](https://arxiv.org/abs/2506.17057)
*Fernando Pastor Ricós,Beatriz Marín,I. S. W. B. Prasetya,Tanja E. J. Vos,Joseph Davidson,Karel Hovorka*

Main category: cs.SE

TL;DR: iv4XR框架通过结合行为驱动开发（BDD）方法，提升了3D游戏（如Space Engineers和LabRecruits）的自动化测试能力，支持多样化的测试场景。


<details>
  <summary>Details</summary>
Motivation: 解决3D游戏测试中开发者与测试者协作的技术门槛问题，提升自动化测试的效率和可读性。

Method: 将BDD方法与iv4XR框架结合，并扩展战术编程功能以支持长时测试场景。

Result: 成功实现了Space Engineers的回归测试自动化，并扩展至LabRecruits的测试场景。

Conclusion: iv4XR框架结合BDD方法显著提升了游戏测试的自动化能力和协作效率。

Abstract: Computer 3D games are complex software environments that require novel
testing processes to ensure high-quality standards. The Intelligent
Verification/Validation for Extended Reality Based Systems (iv4XR) framework
addresses this need by enabling the implementation of autonomous agents to
automate game testing scenarios. This framework facilitates the automation of
regression test cases for complex 3D games like Space Engineers. Nevertheless,
the technical expertise required to define test scripts using iv4XR can
constrain seamless collaboration between developers and testers. This paper
reports how integrating a Behavior-driven Development (BDD) approach with the
iv4XR framework allows the industrial company behind Space Engineers to
automate regression testing. The success of this industrial collaboration has
inspired the iv4XR team to integrate the BDD approach to improve the automation
of play-testing for the experimental 3D game LabRecruits. Furthermore, the
iv4XR framework has been extended with tactical programming to enable the
automation of long-play test scenarios in Space Engineers. These results
underscore the versatility of the iv4XR framework in supporting diverse testing
approaches while showcasing how BDD empowers users to create, manage, and
execute automated game tests using comprehensive and human-readable statements.

</details>


### [38] [Software Fairness Testing in Practice](https://arxiv.org/abs/2506.17095)
*Ronnie de Souza Santos,Matheus de Morais Leca,Reydne Santos,Cleyton Magalhaes*

Main category: cs.SE

TL;DR: 论文探讨了AI系统公平性测试的理论与实践差距，通过访谈22位从业者，发现行业缺乏实用工具和清晰指南，需研究更实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI和ML技术在软件系统中的广泛应用，公平性测试成为确保伦理和公正结果的关键，但行业实践与理论研究存在显著差距。

Method: 通过访谈22位从事AI和ML项目的从业者，研究其公平性测试实践。

Result: 研究发现行业缺乏明确的公平性定义和实用工具，面临数据质量、时间限制等挑战。

Conclusion: 需将学术进展转化为实用工具和策略，帮助从业者系统性解决AI系统的公平性问题。

Abstract: Software testing ensures that a system functions correctly, meets specified
requirements, and maintains high quality. As artificial intelligence and
machine learning (ML) technologies become integral to software systems, testing
has evolved to address their unique complexities. A critical advancement in
this space is fairness testing, which identifies and mitigates biases in AI
applications to promote ethical and equitable outcomes. Despite extensive
academic research on fairness testing, including test input generation, test
oracle identification, and component testing, practical adoption remains
limited. Industry practitioners often lack clear guidelines and effective tools
to integrate fairness testing into real-world AI development. This study
investigates how software professionals test AI-powered systems for fairness
through interviews with 22 practitioners working on AI and ML projects. Our
findings highlight a significant gap between theoretical fairness concepts and
industry practice. While fairness definitions continue to evolve, they remain
difficult for practitioners to interpret and apply. The absence of
industry-aligned fairness testing tools further complicates adoption,
necessitating research into practical, accessible solutions. Key challenges
include data quality and diversity, time constraints, defining effective
metrics, and ensuring model interoperability. These insights emphasize the need
to bridge academic advancements with actionable strategies and tools, enabling
practitioners to systematically address fairness in AI systems.

</details>


### [39] [Reassessing Code Authorship Attribution in the Era of Language Models](https://arxiv.org/abs/2506.17120)
*Atish Kumar Dipongkor,Ziyu Yao,Kevin Moran*

Main category: cs.SE

TL;DR: 该论文研究了代码风格计量学中的代码作者归属问题（CAA），探讨了传统方法的局限性，并首次系统评估了基于Transformer的语言模型在CAA任务中的表现。


<details>
  <summary>Details</summary>
Motivation: CAA在网络安全和软件取证中至关重要，但传统方法依赖手工特征且易受对抗性干扰，效果有限。Transformer模型在自然语言处理中表现优异，但其在CAA中的效果尚不明确。

Method: 研究应用了两种大型和五种小型代码语言模型，对6个数据集（包含463名开发者的12k代码片段）进行CAA任务评估，并使用机器学习可解释性技术深入分析模型表现。

Result: 分析结果揭示了语言模型在理解代码风格模式时的行为特点，为未来研究提供了重要方向。

Conclusion: Transformer模型在CAA任务中展现出潜力，但仍需进一步研究以优化其表现。

Abstract: The study of Code Stylometry, and in particular Code Authorship Attribution
(CAA), aims to analyze coding styles to identify the authors of code samples.
CAA is crucial in cybersecurity and software forensics for addressing,
detecting plagiarism, and supporting criminal prosecutions. However, CAA is a
complex and error prone task, due to the need for recognizing nuanced
relationships between coding patterns. This challenge is compounded in large
software systems with numerous authors due to the subtle variability of
patterns that signify the coding style of one author among many. Given the
challenges related to this task, researchers have proposed and studied
automated approaches that rely upon classical Machine Learning and Deep
Learning techniques. However, such techniques have historically relied upon
hand-crafted features, and due to the often intricate interaction of different
features (e.g., formatting, etc.), have key limitations in properly
characterizing authorship, and are sensitive to adversarial code perturbations.
Recently, transformer-based Language Models (LMs) have shown remarkable
efficacy across a range of software engineering tasks, and in the authorship
attribution on natural language in the NLP domain. However, their effectiveness
in CAA is not well understood. As such, we conduct the first extensive
empirical study applying two larger state-of-the-art code LMs, and five smaller
code LMs to the task of CAA to 6 diverse datasets that encompass 12k code
snippets written by 463 developers. Furthermore, we perform an in-depth
analysis of our studied models' performance on CAA using established machine
learning interpretability techniques. The results of our analysis illustrate
important findings that illuminate the behavior of LMs in understanding
stylometric code patterns during the task of CAA, and point towards important
directions for future work.

</details>


### [40] [Large Language Model Unlearning for Source Code](https://arxiv.org/abs/2506.17125)
*Xue Jiang,Yihong Dong,Zheng Fang,Yingwei Ma,Tangxinyu Wang,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: PROD是一种新型的LLM遗忘方法，专注于源代码领域，能在遗忘不良数据的同时保持代码生成能力。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的成功应用带来了敏感或过时数据记忆的风险，现有遗忘方法在源代码领域效果不佳。

Method: PROD通过抑制遗忘数据的输出概率并促进候选分布组件，实现遗忘与能力保留的平衡。

Result: PROD在三个下游任务中表现优异，平衡了遗忘质量与模型实用性，且对抗攻击鲁棒性强。

Conclusion: PROD扩展了遗忘技术的应用范围，对可靠代码生成有重要意义。

Abstract: LLM4SE has demonstrated significant success, but LLMs' potential memorization
of sensitive or outdated training data introduces critical risks to legal
compliance, software security, and code quality. LLM unlearning techniques,
which can eliminate the influence of undesired data from LLMs in a
post-training way, present a promising solution to address these concerns.
While recent efforts in LLM unlearning show effectiveness in natural language,
their applicability to source code remains underexplored. Our empirical study
reveals that existing LLM unlearning approaches, when applied to source code,
cause severe model utility degradation, rendering models practically unusable
for code generation. In this paper, we propose PROD, a novel unlearning
approach that enables LLMs to forget undesired code content while effectively
preserving their code generation capabilities. PROD suppresses the probability
of forget data in LLMs' output distribution while promoting candidate
distributional components, enabling the model to jointly learn to forget
specific content and retain its general capabilities. To facilitate this study,
we establish a benchmark for code unlearning evaluation, which includes three
critical downstream tasks: copyrighted code unlearning, insecure code
unlearning, and deprecated API unlearning. Our evaluation demonstrates that
PROD achieves superior balance between forget quality and model utility
compared to existing unlearning approaches across three downstream tasks, while
consistently exhibiting improvements when applied to LLMs of varying series.
PROD also exhibits superior robustness against adversarial attacks without
generating or exposing the data to be forgotten. The results underscore that
our approach not only extends the application boundary of unlearning techniques
to source code, but also holds significant implications for advancing reliable
code generation.

</details>


### [41] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 对SWE-Bench Lite和Verified榜单上的提交进行了首次全面研究，分析了67种方法，揭示了专有LLM的主导地位、代理与非代理设计的共存，以及贡献者的多样性。


<details>
  <summary>Details</summary>
Motivation: 由于SWE-Bench提交过程缺乏详细文档，许多解决方案的设计和来源不明确，因此需要系统研究。

Method: 分析了SWE-Bench Lite（68项）和Verified（79项）榜单的所有提交，从提交者类型、产品可用性、LLM使用和系统架构等维度进行研究。

Result: 发现专有LLM（如Claude 3.5/3.7）占主导地位，代理与非代理设计并存，贡献者从个人开发者到大型科技公司不等。

Conclusion: 研究为APR领域提供了重要见解，揭示了当前技术趋势和贡献者生态。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [42] [HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks](https://arxiv.org/abs/2506.15947)
*Jinbo Wen,Cheng Su,Jiawen Kang,Jiangtian Nie,Yang Zhang,Jianhang Tang,Dusit Niyato,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于检索增强生成（RAG）的大型语言模型（LLM）代理框架（HybridRAG）和双正则化扩散增强软行动者-评论家（R²DSAC）算法，用于解决多无人机辅助移动边缘计算（MEC）网络中的低碳优化问题。


<details>
  <summary>Details</summary>
Motivation: 低空经济网络（LAENets）需要满足低延迟和高计算需求，但多无人机建模的复杂性和多目标耦合优化的难度阻碍了其发展。

Method: 开发了HybridRAG框架（结合KeywordRAG、VectorRAG和GraphRAG），并提出R²DSAC算法（结合扩散熵正则化和动作熵正则化）来解决优化问题。

Result: 仿真结果表明，HybridRAG框架和R²DSAC算法在低碳优化问题上具有高效性和可靠性。

Conclusion: 该研究为多无人机辅助MEC网络的低碳优化提供了有效的解决方案，推动了LAENets的发展。

Abstract: Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm
to support various low-altitude services through integrated air-ground
infrastructure. To satisfy low-latency and high-computation demands, the
integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC)
systems plays a vital role, which offloads computing tasks from terminal
devices to nearby UAVs, enabling flexible and resilient service provisions for
ground users. To promote the development of LAENets, it is significant to
achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges
hinder this implementation, including the complexity of multi-dimensional UAV
modeling and the difficulty of multi-objective coupled optimization. To this
end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based
Large Language Model (LLM) agent framework for model formulation. Specifically,
we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG,
empowering LLM agents to efficiently retrieve structural information from
expert databases and generate more accurate optimization problems compared with
traditional RAG-based LLM agents. After customizing carbon emission
optimization problems for multi-UAV-assisted MEC networks, we propose a Double
Regularization Diffusion-enhanced Soft Actor-Critic (R\textsuperscript{2}DSAC)
algorithm to solve the formulated multi-objective optimization problem. The
R\textsuperscript{2}DSAC algorithm incorporates diffusion entropy
regularization and action entropy regularization to improve the performance of
the diffusion policy. Furthermore, we dynamically mask unimportant neurons in
the actor network to reduce the carbon emissions associated with model
training. Simulation results demonstrate the effectiveness and reliability of
the proposed HybridRAG-based LLM agent framework and the
R\textsuperscript{2}DSAC algorithm.

</details>


### [43] [LoRaIN: A Constructive Interference-Assisted Reliable and Energy-Efficient LoRa Indoor Network](https://arxiv.org/abs/2506.16409)
*Mahbubur Rahman,Abusayeed Saifullah*

Main category: cs.NI

TL;DR: 论文提出LoRaIN协议，通过构造性干扰和中继确认提升室内LoRa网络的可靠性和能效。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注LoRa在室内的性能，且仅关注RSSI和SNR，未揭示其可靠性和能效问题。

Method: 提出LoRaIN协议，利用特定时序的构造性干扰和中继确认，通过部分终端设备作为中继节点。

Result: 实验显示，15%终端作为中继节点时，网关可靠性从62%提升至95%，能效提高2.5倍。

Conclusion: LoRaIN是首个提升室内LoRa网络可靠性和能效的协议，效果显著。

Abstract: LoRa is a promising communication technology for enabling the next-generation
indoor Internet of Things applications. Very few studies, however, have
analyzed its performance indoors. Besides, these indoor studies investigate
mostly the RSSI and SNR of the received packets at the gateway, which, as we
show, may not unfold the poor performance of LoRa and its MAC protocol,
LoRaWAN, indoors in terms of reliability and energy-efficiency. In this paper,
we extensively evaluate the performance of LoRaWAN indoors and then use the key
insights to boost its reliability and energy-efficiency by proposing LoRaIN,
LoRa Indoor Network, a new link-layer protocol that can be effectively used for
indoor deployments. The approach to boosting the reliability and energy
efficiency in LoRaIN is underpinned by enabling constructive interference with
specific timing requirements analyzed both empirically and mathematically for
different pairs of channel bandwidth and spreading factor and relaying precious
acknowledgments to the end-devices with the assistance of several booster
nodes. The booster nodes do not need any special capability and can be a subset
of the LoRa end-devices. To our knowledge, LoRaIN is the first protocol for
boosting reliability and energy-efficiency in indoor LoRa networks. We evaluate
its performance in an indoor testbed consisting of one LoRaWAN gateway and 20
end-devices. Our extensive evaluation shows that when 15% of the end-devices
operate as booster nodes, the reliability at the gateway increases from 62% to
95%, and the end-devices are approximately 2.5x energy-efficient.

</details>


### [44] [Using SRv6 to access Edge Applications in 5G Networks](https://arxiv.org/abs/2506.16808)
*Louis Royer,Emmanuel Lavinal,Emmanuel Chaput*

Main category: cs.NI

TL;DR: 本文探讨了在5G及以后的多接入边缘计算中，如何优化数据路径并确保资源按策略使用，提出使用SRv6技术。


<details>
  <summary>Details</summary>
Motivation: 随着5G及以后多接入边缘计算的出现，运营商需要优化数据路径并确保资源按策略使用。

Method: 回顾现有边缘资源访问解决方案，指出其局限性，并提出在5G/边缘架构中使用SRv6。

Result: 未明确提及具体实验结果，但提出SRv6为潜在解决方案。

Conclusion: SRv6在5G/边缘架构中具有潜力，可优化数据路径和资源管理。

Abstract: With the emergence of Multi-Access Edge Computing in 5G and beyond, it has
become essential for operators to optimize the data path for the end-user while
ensuring resources are used according to their policy. In this paper, we review
existing solutions to access edge resources, underline their limits, and
propose the use of Segment Routing over IPv6 (SRv6) in a 5G/edge architecture.

</details>


### [45] [Minimal Per-Flow Backlog Bounds at an Aggregate FIFO Server under Piecewise-Linear Arrival Curves](https://arxiv.org/abs/2506.16914)
*Lukas Wildberger,Anja Hamscher,Jens B. Schmitt*

Main category: cs.NI

TL;DR: 本文针对网络演算（NC）中聚合FIFO服务器的非线性问题，提出了一种针对一般分段线性到达曲线的最小化积压边界的方法，并通过启发式算法高效求解。


<details>
  <summary>Details</summary>
Motivation: 聚合FIFO服务器因其非线性特性难以分析，现有方法仅适用于简单的令牌桶到达曲线。本文旨在解决更一般的分段线性到达曲线的最小化积压边界问题。

Method: 通过分析到达曲线或剩余服务曲线的断点，定义一组曲线表征积压，并寻找与到达曲线最大交点的剩余服务曲线参数。提出高效启发式算法以应对复杂场景。

Result: 在复杂场景中，启发式算法能高效找到最优或接近最优的参数，显著减少积压边界。

Conclusion: 本文方法有效提升了DiscoDNC工具的性能，显著降低了积压边界，为更一般的到达曲线提供了实用解决方案。

Abstract: Network Calculus (NC) is a versatile methodology based on min-plus algebra to
derive worst-case per-flow performance bounds in networked systems with many
concurrent flows. In particular, NC can analyze many scheduling disciplines;
yet, somewhat surprisingly, an aggregate FIFO server is a notoriously hard case
due to its min-plus non-linearity. A resort is to represent the FIFO residual
service by a family of functions with a free parameter instead of just a single
curve. For simple token-bucket arrival curves, literature provides optimal
choices for that free parameter to minimize delay and backlog bounds. In this
paper, we tackle the challenge of more general arrival curves than just token
buckets. In particular, we derive residual service curves resulting in minimal
backlog bounds for general piecewise-linear arrival curves. To that end, we
first show that a backlog bound can always be calculated at a breakpoint of
either the arrival curve of the flow of interest or its residual service curve.
Further, we define a set of curves that characterize the backlog for a fixed
breakpoint, depending on the free parameter of the residual service curve. We
show that the backlog-minimizing residual service curve family parameter
corresponds to the largest intersection of those curves with the arrival curve.
In more complex scenarios finding this largest intersection can become
inefficient as the search space grows in the number of flows. Therefore, we
present an efficient heuristic that finds, in many cases, the optimal parameter
or at least a close conservative approximation. This heuristic is evaluated in
terms of accuracy and execution time. Finally, we utilize these
backlog-minimizing residual service curves to enhance the DiscoDNC tool and
observe considerable reductions in the corresponding backlog bounds.

</details>


### [46] [Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks](https://arxiv.org/abs/2506.17063)
*Samer Lahoud,Kinda Khawam*

Main category: cs.NI

TL;DR: 本文提出了一种新颖的联邦语义通信框架，用于在带宽受限的无线网络中高效传输数据并保护隐私，通过语义特征传输显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决物联网设备在带宽受限网络中数据传输效率低和隐私保护不足的问题。

Method: 采用联邦语义通信框架，结合三种客户端选择策略和语义瓶颈的端到端架构，以及基于损失的聚合机制。

Result: 实验表明，功利选择策略实现最高重建质量，而比例公平策略在保持性能的同时显著减少参与不平等。

Conclusion: 联邦语义通信能平衡重建质量、资源效率和公平性，为可持续且保护隐私的边缘智能应用铺平道路。

Abstract: The exponential growth of IoT devices presents critical challenges in
bandwidth-constrained wireless networks, particularly regarding efficient data
transmission and privacy preservation. This paper presents a novel federated
semantic communication (SC) framework that enables collaborative training of
bandwidth-efficient models for image reconstruction across heterogeneous IoT
devices. By leveraging SC principles to transmit only semantic features, our
approach dramatically reduces communication overhead while preserving
reconstruction quality. We address the fundamental challenge of client
selection in federated learning environments where devices exhibit significant
disparities in dataset sizes and data distributions. Our framework implements
three distinct client selection strategies that explore different trade-offs
between system performance and fairness in resource allocation. The system
employs an end-to-end SC architecture with semantic bottlenecks, coupled with a
loss-based aggregation mechanism that naturally adapts to client heterogeneity.
Experimental evaluation on image data demonstrates that while Utilitarian
selection achieves the highest reconstruction quality, Proportional Fairness
maintains competitive performance while significantly reducing participation
inequality and improving computational efficiency. These results establish that
federated SC can successfully balance reconstruction quality, resource
efficiency, and fairness in heterogeneous IoT deployments, paving the way for
sustainable and privacy-preserving edge intelligence applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Ignition Phase : Standard Training for Fast Adversarial Robustness](https://arxiv.org/abs/2506.15685)
*Wang Yu-Hang,Liu ying,Fang liang,Wang Xuelin,Junkang Guo,Shiwei Li,Lei Gao,Jian Liu,Wenfei Yin*

Main category: cs.LG

TL;DR: AET通过在传统对抗训练前加入ERM阶段，显著提升模型鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法过于关注攻击生成，忽略了基础特征表示的重要性。

Method: 提出AET框架，在对抗训练前加入ERM阶段以优化特征流形。

Result: AET在多个数据集和架构上表现优异，训练成本降低8-25%，且鲁棒性和准确率提升。

Conclusion: 特征预条件化对高效鲁棒防御至关重要，AET为对抗训练提供了新思路。

Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants
overlook foundational feature representations by primarily focusing on stronger
attack generation. We introduce Adversarial Evolution Training (AET), a simple
yet powerful framework that strategically prepends an Empirical Risk
Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM
phase cultivates a favorable feature manifold, enabling more efficient and
effective robustness acquisition. Empirically, AET achieves comparable or
superior robustness more rapidly, improves clean accuracy, and cuts training
costs by 8-25\%. Its effectiveness is shown across multiple datasets,
architectures, and when augmenting established AT methods. Our findings
underscore the impact of feature pre-conditioning via standard training for
developing more efficient, principled robust defenses. Code is available in the
supplementary material.

</details>


### [48] [Learning from M-Tuple Dominant Positive and Unlabeled Data](https://arxiv.org/abs/2506.15686)
*Jiahe Qin,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 本文提出了一种广义学习框架MDPU，用于解决标签比例学习（LLP）中实例比例信息不精确的问题，通过数学建模和风险校正方法提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，获取实例比例的精确监督信息具有挑战性，因此需要一种更贴合实际场景的学习框架。

Method: 提出MDPU框架，数学建模实例分布，基于ERM方法推导无偏风险估计器，并引入风险校正方法。

Result: 理论证明了无偏风险估计器的泛化误差边界，实验验证了MDPU的有效性。

Conclusion: MDPU框架在LLP问题中表现优异，为实际应用提供了有效解决方案。

Abstract: Label Proportion Learning (LLP) addresses the classification problem where
multiple instances are grouped into bags and each bag contains information
about the proportion of each class. However, in practical applications,
obtaining precise supervisory information regarding the proportion of instances
in a specific class is challenging. To better align with real-world application
scenarios and effectively leverage the proportional constraints of instances
within tuples, this paper proposes a generalized learning framework
\emph{MDPU}. Specifically, we first mathematically model the distribution of
instances within tuples of arbitrary size, under the constraint that the number
of positive instances is no less than that of negative instances. Then we
derive an unbiased risk estimator that satisfies risk consistency based on the
empirical risk minimization (ERM) method. To mitigate the inevitable
overfitting issue during training, a risk correction method is introduced,
leading to the development of a corrected risk estimator. The generalization
error bounds of the unbiased risk estimator theoretically demonstrate the
consistency of the proposed method. Extensive experiments on multiple datasets
and comparisons with other relevant baseline methods comprehensively validate
the effectiveness of the proposed learning framework.

</details>


### [49] [S$^2$GPT-PINNs: Sparse and Small models for PDEs](https://arxiv.org/abs/2506.15687)
*Yajie Ji,Yanlai Chen,Shawn Koohy*

Main category: cs.LG

TL;DR: S$^2$GPT-PINN是一种稀疏且小型的模型，用于求解参数化偏微分方程（PDEs），通过知识蒸馏和智能降采样实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 针对特定领域的PDEs，设计一种计算资源需求低但高效的模型，类似于小型语言模型（SLMs）。

Method: 利用高质量数据和贪婪算法，结合知识蒸馏（任务特定激活函数）和物理信息损失的智能降采样。

Result: 相比传统PINNs，参数数量大幅减少，计算效率显著提高。

Conclusion: S$^2$GPT-PINN为特定PDEs提供了一种高效且资源节约的解决方案。

Abstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric
partial differential equations (PDEs). Similar to Small Language Models (SLMs),
S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and
characterized by its compact architecture and minimal computational power.
Leveraging a small amount of extremely high quality data via a mathematically
rigorous greedy algorithm that is enabled by the large full-order models,
S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to
achieve extremely high efficiency via two levels of customizations. The first
is knowledge distillation via task-specific activation functions that are
transferred from Pre-Trained PINNs. The second is a judicious down-sampling
when calculating the physics-informed loss of the network compressing the
number of data sites by orders of magnitude to the size of the small model.

</details>


### [50] [Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism](https://arxiv.org/abs/2506.15688)
*Hui Ma,Kai Yang,Man-On Pun*

Main category: cs.LG

TL;DR: 提出了一种端到端框架，结合卷积神经网络和注意力机制以及卡尔曼滤波，用于提升蜂窝流量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量预测对运营商管理网络资源和决策至关重要，但流量动态性强且受多种外部因素影响，导致预测精度下降。

Method: 使用卷积神经网络和注意力机制捕捉空间动态，卡尔曼滤波进行时间建模，并利用辅助信息（如社交活动）提升性能。

Result: 在三个真实数据集上的实验表明，所提模型在预测精度上优于现有机器学习技术。

Conclusion: 该框架能有效提升蜂窝流量预测的准确性，具有实际应用价值。

Abstract: Cellular traffic prediction is of great importance for operators to manage
network resources and make decisions. Traffic is highly dynamic and influenced
by many exogenous factors, which would lead to the degradation of traffic
prediction accuracy. This paper proposes an end-to-end framework with two
variants to explicitly characterize the spatiotemporal patterns of cellular
traffic among neighboring cells. It uses convolutional neural networks with an
attention mechanism to capture the spatial dynamics and Kalman filter for
temporal modelling. Besides, we can fully exploit the auxiliary information
such as social activities to improve prediction performance. We conduct
extensive experiments on three real-world datasets. The results show that our
proposed models outperform the state-of-the-art machine learning techniques in
terms of prediction accuracy.

</details>


### [51] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: 论文提出了一种名为BASE-Q的新方法，通过结合偏置校正和非对称缩放，有效减少了量化误差，并支持分块优化，避免了内存密集型全模型反向传播。


<details>
  <summary>Details</summary>
Motivation: 当前旋转量化方法存在两个主要问题：旋转未能对齐通道均值导致量化范围扩大和舍入误差增加；旋转使激活分布更接近高斯分布，增加了截断误差的能量损失。

Method: 提出了BASE-Q方法，结合偏置校正和非对称缩放，减少舍入和截断误差，并支持分块优化。

Result: 实验表明，BASE-Q显著缩小了与全精度模型的准确率差距，相比QuaRot、SpinQuant和OSTQuant分别提高了50.5%、42.9%和29.2%。

Conclusion: BASE-Q是一种简单而有效的方法，解决了当前旋转量化方法的局限性，并在实际应用中表现出色。

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [52] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/abs/2506.15690)
*Tianyu Wang,Lingyou Pang,Akira Horiguchi,Carey E. Priebe*

Main category: cs.LG

TL;DR: 论文提出LLM Web Dynamics (LWD)框架，研究网络级模型崩溃问题，通过模拟互联网和RAG数据库分析模型输出收敛模式，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 尽管合成数据在LLM训练中提高了效率，但模型崩溃的潜在威胁尚未充分探索，现有研究多局限于单一模型或统计替代。

Method: 引入LWD框架，模拟互联网环境并使用RAG数据库分析模型输出的收敛模式，通过类比高斯混合模型提供理论保证。

Result: 揭示了模型输出在网络级的收敛行为，为模型崩溃问题提供了新的研究视角。

Conclusion: LWD框架为理解和预防模型崩溃提供了有效工具，填补了现有研究的空白。

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [53] [What Do Latent Action Models Actually Learn?](https://arxiv.org/abs/2506.15691)
*Chuheng Zhang,Tim Pearce,Pushi Zhang,Kaixin Wang,Xiaoyu Chen,Wei Shen,Li Zhao,Jiang Bian*

Main category: cs.LG

TL;DR: 论文研究了潜在动作模型（LAMs）在无标签视频中学习动作相关变化时，是否捕捉到无关噪声的问题，并提出一个线性模型进行分析。


<details>
  <summary>Details</summary>
Motivation: 探讨LAMs是否能够区分由动作引起的变化与无关噪声，从而改进模型学习。

Method: 提出一个可分析的线性模型，连接LAM与PCA，并通过数据增强、数据清理和辅助动作预测策略优化学习。

Result: 通过数值模拟揭示了观测数据、动作和噪声结构对LAM学习的影响。

Conclusion: 研究表明数据生成策略的设计和优化方法对LAM学习可控变化至关重要。

Abstract: Latent action models (LAMs) aim to learn action-relevant changes from
unlabeled videos by compressing changes between frames as latents. However,
differences between video frames can be caused by controllable changes as well
as exogenous noise, leading to an important concern -- do latents capture the
changes caused by actions or irrelevant noise? This paper studies this issue
analytically, presenting a linear model that encapsulates the essence of LAM
learning, while being tractable.This provides several insights, including
connections between LAM and principal component analysis (PCA), desiderata of
the data-generating policy, and justification of strategies to encourage
learning controllable changes using data augmentation, data cleaning, and
auxiliary action-prediction. We also provide illustrative results based on
numerical simulation, shedding light on the specific structure of observations,
actions, and noise in data that influence LAM learning.

</details>


### [54] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Main category: cs.LG

TL;DR: 论文提出了一种数据为中心的框架，通过六个结构化工件（数据集、特征、工作流、执行、资产和控制词汇）提升机器学习实验的可重现性。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的可重现性是一个核心挑战，尤其是在协作式电子科学项目中。当前的工作流程动态但碎片化，阻碍了透明度和实验的适应性。

Method: 引入了一个数据为中心的框架，围绕六个结构化工件，形式化数据、代码和决策之间的关系。

Result: 通过青光眼检测的临床案例展示了该框架如何支持迭代探索、提升可重现性并保留协作决策的溯源。

Conclusion: 该框架为机器学习的生命周期提供了可版本化、可解释和可追溯的实验管理方法。

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [55] [MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement](https://arxiv.org/abs/2506.15692)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Sercan Ö. Arık,Tomas Pfister*

Main category: cs.LG

TL;DR: MLE-STAR是一种新型的MLE代理构建方法，通过结合外部知识和针对性探索策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的MLE代理依赖固有知识且探索策略粗糙，无法深入优化特定组件。

Method: MLE-STAR利用搜索引擎获取初始模型，并通过针对性探索和消融研究迭代优化。

Result: MLE-STAR在44%的Kaggle竞赛中表现优异，远超其他方法。

Conclusion: MLE-STAR通过结合外部知识和精细化探索，显著提升了MLE代理的性能。

Abstract: Agents based on large language models (LLMs) for machine learning engineering
(MLE) can automatically implement ML models via code generation. However,
existing approaches to build such agents often rely heavily on inherent LLM
knowledge and employ coarse exploration strategies that modify the entire code
structure at once. This limits their ability to select effective task-specific
models and perform deep exploration within specific components, such as
experimenting extensively with feature engineering options. To overcome these,
we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first
leverages external knowledge by using a search engine to retrieve effective
models from the web, forming an initial solution, then iteratively refines it
by exploring various strategies targeting specific ML components. This
exploration is guided by ablation studies analyzing the impact of individual
code blocks. Furthermore, we introduce a novel ensembling method using an
effective strategy suggested by MLE-STAR. Our experimental results show that
MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,
significantly outperforming the best alternative.

</details>


### [56] [Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks](https://arxiv.org/abs/2506.15693)
*Jiaxing Li,Hanjiang Hu,Yujie Yang,Changliu Liu*

Main category: cs.LG

TL;DR: 提出了一种基于Hamilton-Jacobi可达性分析的可验证无模型安全过滤器，解决了学习型安全过滤器缺乏形式化安全保证的问题。


<details>
  <summary>Details</summary>
Motivation: 学习型安全过滤器虽优于传统方法（如手工CBFs），但缺乏形式化安全保证，需改进。

Method: 扩展Q值函数的可验证自一致性属性，提出乘法Q网络结构以减少零子水平集收缩问题，开发验证管道。

Result: 在四个标准安全控制基准上成功合成了形式化验证的无模型安全证书。

Conclusion: 该方法为学习型安全过滤器提供了形式化安全保证，解决了现有方法的局限性。

Abstract: Recent learning-based safety filters have outperformed conventional methods,
such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting
to complex constraints. However, these learning-based approaches lack formal
safety guarantees. In this work, we introduce a verifiable model-free safety
filter based on Hamilton-Jacobi reachability analysis. Our primary
contributions include: 1) extending verifiable self-consistency properties for
Q value functions, 2) proposing a multiplicative Q-network structure to
mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification
pipeline capable of soundly verifying these self-consistency properties. Our
proposed approach successfully synthesizes formally verified, model-free safety
certificates across four standard safe-control benchmarks.

</details>


### [57] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文综述了关系深度学习（RDL），将多表关系数据库表示为关系实体图，并探讨了其关键特性、挑战及最新模型进展。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的传统特征工程方法效率低下，RDL通过图机器学习提供端到端表示学习的新蓝图。

Method: 将关系数据库表示为关系实体图，并综述了基于GNN的RDL模型及其在公共基准数据集上的评估。

Result: 讨论了大规模多表集成、时间动态和异构数据建模的挑战，并总结了针对关系实体图的神经网络方法。

Conclusion: RDL有望统一图机器学习的多个子领域，推动关系数据处理的基础模型设计。

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [58] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Main category: cs.LG

TL;DR: 提出了一种基于Power-Norm Cosine Similarity（PNCS）的联邦学习框架，通过捕捉高阶梯度矩解决非独立同分布数据问题，提升收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法未充分考虑客户端间梯度相关性，尤其在数据异构场景下表现不佳。

Method: 利用PNCS改进客户端选择，并通过选择历史队列确保多样性。

Result: 在VGG16模型上的实验显示，该方法在多种数据分区下均优于现有技术。

Conclusion: PNCS框架有效解决了数据异构问题，提升了联邦学习的性能。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [59] [Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction](https://arxiv.org/abs/2506.15694)
*Iliyas Ibrahim Iliyas,Souley Boukari,Abdulsalam Yau Gital*

Main category: cs.LG

TL;DR: 该研究提出了一种结合非线性特征提取、分类和高效优化的框架，通过核主成分分析、多层感知机和并行遗传算法优化，在多个数据集上取得了优于其他方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效且准确的框架，用于疾病预测，同时解决传统遗传算法计算成本高的问题。

Method: 使用核主成分分析降维，多层感知机分类，并行遗传算法优化超参数。

Result: 在乳腺癌、帕金森病和慢性肾病数据集上分别达到99.12%、94.87%和100%的准确率，优于其他优化方法。

Conclusion: 该框架显著提高了分类准确率并降低了优化时间，适用于多种疾病预测任务。

Abstract: This study introduces a framework that integrates nonlinear feature
extraction, classification, and efficient optimization. First, kernel principal
component analysis with a radial basis function kernel reduces dimensionality
while preserving 95% of the variance. Second, a multilayer perceptron (MLP)
learns to predict disease status. Finally, a modified multiprocessing genetic
algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten
generations. We evaluated this approach on three datasets: the Wisconsin
Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and
the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best
accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%
for chronic kidney disease. These results outperform those of other methods,
such as grid search, random search, and Bayesian optimization. Compared with a
standard genetic algorithm, kernel PCA revealed nonlinear relationships that
improved classification, and the MIGA's parallel fitness evaluations reduced
the tuning time by approximately 60%. The genetic algorithm incurs high
computational cost from sequential fitness evaluations, but our multiprocessing
interface GA (MIGA) parallelizes this step, slashing the tuning time and
steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for
breast cancer, Parkinson's disease, and CKD, respectively.

</details>


### [60] [A Distributional-Lifting Theorem for PAC Learning](https://arxiv.org/abs/2506.16651)
*Guy Blanc,Jane Lange,Carmen Strassle,Li-Yang Tan*

Main category: cs.LG

TL;DR: 本文提出了一种分布提升定理，将有限分布族的学习器提升为适用于任何分布的学习器，效率与目标分布的复杂性相关。


<details>
  <summary>Details</summary>
Motivation: 解决分布特定学习算法的局限性，扩展其适用范围。

Method: 提出分布提升定理，避免学习目标分布，直接在标准PAC模型中实现提升。

Result: 新方法在标准PAC模型中有效，适用于所有基础分布族，具有更好的样本复杂性和噪声容忍性。

Conclusion: 该方法简化了学习过程，扩展了分布特定学习器的适用范围。

Abstract: The apparent difficulty of efficient distribution-free PAC learning has led
to a large body of work on distribution-specific learning. Distributional
assumptions facilitate the design of efficient algorithms but also limit their
reach and relevance. Towards addressing this, we prove a distributional-lifting
theorem: This upgrades a learner that succeeds with respect to a limited
distribution family $\mathcal{D}$ to one that succeeds with respect to any
distribution $D^\star$, with an efficiency overhead that scales with the
complexity of expressing $D^\star$ as a mixture of distributions in
$\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of
lifting uniform-distribution learners and designed a lifter that uses a
conditional sample oracle for $D^\star$, a strong form of access not afforded
by the standard PAC model. Their approach, which draws on ideas from
semi-supervised learning, first learns $D^\star$ and then uses this information
to lift.
  We show that their approach is information-theoretically intractable with
access only to random examples, thereby giving formal justification for their
use of the conditional sample oracle. We then take a different approach that
sidesteps the need to learn $D^\star$, yielding a lifter that works in the
standard PAC model and enjoys additional advantages: it works for all base
distribution families, preserves the noise tolerance of learners, has better
sample complexity, and is simpler.

</details>


### [61] [SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models](https://arxiv.org/abs/2506.15695)
*Xinxing Ren,Qianbo Zang,Zekun Guo*

Main category: cs.LG

TL;DR: SimuGen是一个多模态代理框架，旨在通过结合视觉Simulink图和领域知识，生成准确的Simulink仿真代码。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在数学推理和代码生成方面表现优异，但在Simulink模型生成领域表现不佳，主要因为缺乏Simulink特定的预训练数据。

Method: SimuGen采用多代理协作框架，包括调查员、单元测试审查员、代码生成器、执行器、调试定位器和报告编写器，并依托领域知识库。

Result: 该框架实现了可解释、稳健且可重复的Simulink仿真代码生成。

Conclusion: SimuGen通过多模态和模块化设计，有效解决了LLMs在Simulink仿真领域的局限性。

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in mathematical reasoning and code generation. However, LLMs still
struggle in the simulation domain, particularly in generating Simulink models,
which are essential tools in engineering and scientific research. Our
preliminary experiments indicate that LLM agents often fail to produce reliable
and complete Simulink simulation code from text-only inputs, likely due to the
lack of Simulink-specific data in their pretraining. To address this challenge,
we propose SimuGen, a multimodal agent-based framework that automatically
generates accurate Simulink simulation code by leveraging both the visual
Simulink diagram and domain knowledge. SimuGen coordinates several specialized
agents, including an investigator, unit test reviewer, code generator,
executor, debug locator, and report writer, supported by a domain-specific
knowledge base. This collaborative and modular design enables interpretable,
robust, and reproducible Simulink simulation generation. Our source code is
publicly available at https://github.com/renxinxing123/SimuGen_beta.

</details>


### [62] [CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction](https://arxiv.org/abs/2506.15696)
*Haipeng Zhou,Sicheng Yang,Sihan Yang,Jing Qin,Lei Chen,Lei Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Chain-of-Cancer (CoC)的多模态框架，结合临床数据和语言描述进行癌症患者生存预测，通过内学习和互学习实现多模态联合学习，并在五个公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖病理和基因组数据，而忽略了甲基化数据和语言描述的作用。论文首次探索了四种模态（包括语言）的联合使用，以更全面地预测癌症患者的生存风险。

Method: 提出CoC框架，包括内学习（利用临床数据作为原始特征）和互学习（通过语言提示和自回归互拉模块实现多模态协同表示）。

Result: 在五个公开癌症数据集上验证了方法的有效性，取得了最先进的性能。

Conclusion: CoC框架通过多模态联合学习显著提升了生存预测的准确性，代码将开源。

Abstract: Survival prediction aims to evaluate the risk level of cancer patients.
Existing methods primarily rely on pathology and genomics data, either
individually or in combination. From the perspective of cancer pathogenesis,
epigenetic changes, such as methylation data, could also be crucial for this
task. Furthermore, no previous endeavors have utilized textual descriptions to
guide the prediction. To this end, we are the first to explore the use of four
modalities, including three clinical modalities and language, for conducting
survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)
to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and
inter-learning. We encode the clinical data as the raw features, which remain
domain-specific knowledge for intra-learning. In terms of inter-learning, we
use language to prompt the raw features and introduce an Autoregressive Mutual
Traction module for synergistic representation. This tailored framework
facilitates joint learning among multiple modalities. Our approach is evaluated
across five public cancer datasets, and extensive experiments validate the
effectiveness of our methods and proposed designs, leading to producing \sota
results. Codes will be released.

</details>


### [63] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: Spotscape提出了一种新的框架，通过Similarity Telescope模块和相似性缩放策略，解决了现有图方法在空间域边界附近的表现问题，并在单切片和多切片场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图方法在空间域边界附近的表现不佳，因为它们过于依赖相邻点而忽略了全局关系。

Method: Spotscape引入了Similarity Telescope模块捕捉全局关系，并提出相似性缩放策略调节切片内和切片间的距离。

Result: 实验表明Spotscape在单切片和多切片任务中表现优越。

Conclusion: Spotscape为空间转录组学提供了一种更有效的分析方法。

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [64] [BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap](https://arxiv.org/abs/2506.15699)
*Shengyuan Hu,Neil Kale,Pratiksha Thaker,Yiwei Fu,Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 论文提出了一个名为BLUR的新基准测试，用于评估大型语言模型（LLM）的遗忘能力，解决了现有基准测试中遗忘与保留数据集差异过大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘基准测试中，遗忘和保留数据集差异过大，导致评估结果失真，可能掩盖模型在实际部署中的潜在风险。

Method: 提出BLUR基准测试，扩展了评估任务，结合了遗忘/保留查询，并提供了不同难度的重新学习数据集。

Result: 在BLUR基准测试下，现有方法的性能显著下降，简单方法表现优于新方法。

Conclusion: BLUR强调了鲁棒评估的重要性，并指出了未来研究的方向。

Abstract: Machine unlearning has the potential to improve the safety of large language
models (LLMs) by removing sensitive or harmful information post hoc. A key
challenge in unlearning involves balancing between forget quality (effectively
unlearning undesirable information) and retain quality (maintaining good
performance on other, general tasks). Unfortunately, as we show, current LLM
unlearning benchmarks contain highly disparate forget and retain sets --
painting a false picture of the effectiveness of LLM unlearning methods. This
can be particularly problematic because it opens the door for benign
perturbations, such as relearning attacks, to easily reveal supposedly
unlearned knowledge once models are deployed. To address this, we present
$\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic
scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on
existing unlearning benchmarks by providing extended evaluation tasks, combined
forget/retain queries, and relearning datasets of varying degrees of
difficulty. Despite the benign nature of the queries considered, we find that
the performance of existing methods drops significantly when evaluated on
$\texttt{BLUR}$, with simple approaches performing better on average than more
recent methods. These results highlight the importance of robust evaluation and
suggest several important directions of future study. Our benchmark is publicly
available at: https://huggingface.co/datasets/forgelab/BLUR

</details>


### [65] [Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking](https://arxiv.org/abs/2506.15700)
*Minjae Cho,Hiroyasu Tsukamoto,Huy Trong Tran*

Main category: cs.LG

TL;DR: 论文提出了一种结合控制收缩度量（CCM）和强化学习（RL）的方法，以解决CCM在动态模型已知性、最优性和可扩展性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: CCM虽然能保证闭环系统的增量指数稳定性，但缺乏轨迹最优性且依赖已知动态模型，限制了其在复杂系统中的应用。

Method: 通过将CCM集成到RL中，提出收缩演员-评论家（CAC）算法，自动学习收缩度量生成器（CMG）和最优跟踪策略。

Result: 实验表明，CAC在模拟和真实机器人任务中优于基线方法，并提供了将收缩理论融入RL的理论依据。

Conclusion: CAC结合了CCM的稳定性和RL的最优性，为复杂系统控制提供了自动化解决方案。

Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a
controller and a corresponding contraction metric -- a positive-definite
Riemannian metric under which a closed-loop system is guaranteed to be
incrementally exponentially stable. However, the synthesized controller only
ensures that all the trajectories of the system converge to one single
trajectory and, as such, does not impose any notion of optimality across an
entire trajectory. Furthermore, constructing CCMs requires a known dynamics
model and non-trivial effort in solving an infinite-dimensional convex
feasibility problem, which limits its scalability to complex systems featuring
high dimensionality with uncertainty. To address these issues, we propose to
integrate CCMs into reinforcement learning (RL), where CCMs provide
dynamics-informed feedback for learning control policies that minimize
cumulative tracking error under unknown dynamics. We show that our algorithm,
called contraction actor-critic (CAC), formally enhances the capability of CCMs
to provide a set of contracting policies with the long-term optimality of RL in
a fully automated setting. Given a pre-trained dynamics model, CAC
simultaneously learns a contraction metric generator (CMG) -- which generates a
contraction metric -- and uses an actor-critic algorithm to learn an optimal
tracking policy guided by that metric. We demonstrate the effectiveness of our
algorithm relative to established baselines through extensive empirical
studies, including simulated and real-world robot experiments, and provide a
theoretical rationale for incorporating contraction theory into RL.

</details>


### [66] [Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning](https://arxiv.org/abs/2506.15701)
*Haolin Pan,Hongyu Lin,Haoran Luo,Yang Liu,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: Compiler-R1是一个基于强化学习的框架，通过增强LLM能力优化编译器自动调优，解决了高质量推理数据集缺失和环境交互不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在编译器自动调优中的应用面临高质量推理数据集缺失和编译环境交互有限的挑战。

Method: 提出Compiler-R1框架，包含高质量推理数据集和两阶段端到端RL训练流程，通过结果驱动的奖励机制优化探索和学习。

Result: 在七个数据集上的实验显示，Compiler-R1平均减少8.46%的IR指令数，优于opt -Oz。

Conclusion: Compiler-R1展示了RL训练的LLM在编译器优化中的强大潜力，代码和数据集已开源。

Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics
such as Intermediate Representation (IR) instruction count. Although recent
advances leveraging Large Language Models (LLMs) have shown promise in
automating compiler tuning, two significant challenges still remain: the
absence of high-quality reasoning datasets for agents training, and limited
effective interactions with the compilation environment. In this work, we
introduce Compiler-R1, the first reinforcement learning (RL)-driven framework
specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1
features a curated, high-quality reasoning dataset and a novel two-stage
end-to-end RL training pipeline, enabling efficient environment exploration and
learning through an outcome-based reward. Extensive experiments across seven
datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction
count reduction compared to opt -Oz, showcasing the strong potential of
RL-trained LLMs for compiler optimization. Our code and datasets are publicly
available at https://github.com/Panhaolin2001/Compiler-R1.

</details>


### [67] [Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation](https://arxiv.org/abs/2506.15702)
*Peter Belcak,Greg Heinrich,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 论文提出了一种名为minifinetuning（MFT）的方法，用于在低数据量的情况下减少语言模型领域适应中的过拟合问题，且无需预训练数据。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在有限数据下微调时普遍性能下降的问题。

Method: 采用样本级别的自我蒸馏技术，结合参数高效微调方法。

Result: MFT在多种模型和领域中表现出2-10倍的性能提升，且在数据稀缺时仍能保持鲁棒性。

Conclusion: MFT是一种高效且可组合的方法，显著缓解了微调过程中的性能退化问题。

Abstract: Finetuning language models for a new domain inevitably leads to the
deterioration of their general performance. This becomes more pronounced the
more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain
adaptation that considerably reduces the effects of overfitting-induced
degeneralization in low-data settings and which does so in the absence of any
pre-training data for replay. MFT demonstrates 2-10x more favourable
specialization-to-degeneralization ratios than standard finetuning across a
wide range of models and domains and exhibits an intrinsic robustness to
overfitting when data in the new domain is scarce and down to as little as 500
samples.
  Employing corrective self-distillation that is individualized on the sample
level, MFT outperforms parameter-efficient finetuning methods, demonstrates
replay-like degeneralization mitigation properties, and is composable with
either for a combined effect.

</details>


### [68] [Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance](https://arxiv.org/abs/2506.15703)
*Guoqing Chao,Zhenghao Zhang,Lei Meng,Jie Wen,Dianhui Chu*

Main category: cs.LG

TL;DR: 提出了一种新的联邦不完全多视图聚类方法FIMCFG，通过全局融合图指导解决现有方法在特征提取和缺失数据问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多视图聚类方法仅使用全局伪标签指导聚类，未能充分利用全局信息，且对缺失数据问题研究较少。

Method: 设计双头图卷积编码器提取全局和视图特定特征，通过融合图指导特征融合和伪标签监督聚类。

Result: 实验证明FIMCFG的有效性和优越性。

Conclusion: FIMCFG解决了现有方法的不足，并在实验中表现优异。

Abstract: Federated multi-view clustering has been proposed to mine the valuable
information within multi-view data distributed across different devices and has
achieved impressive results while preserving the privacy. Despite great
progress, most federated multi-view clustering methods only used global
pseudo-labels to guide the downstream clustering process and failed to exploit
the global information when extracting features. In addition, missing data
problem in federated multi-view clustering task is less explored. To address
these problems, we propose a novel Federated Incomplete Multi-view Clustering
method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a
dual-head graph convolutional encoder at each client to extract two kinds of
underlying features containing global and view-specific information.
Subsequently, under the guidance of the fused graph, the two underlying
features are fused into high-level features, based on which clustering is
conducted under the supervision of pseudo-labeling. Finally, the high-level
features are uploaded to the server to refine the graph fusion and
pseudo-labeling computation. Extensive experimental results demonstrate the
effectiveness and superiority of FIMCFG. Our code is publicly available at
https://github.com/PaddiHunter/FIMCFG.

</details>


### [69] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: LFPS是一种通过利用历史注意力模式动态构建稀疏索引候选的加速方法，显著降低了长上下文LLM解码中的计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持更长的上下文，KV缓存在解码时的内存需求急剧增加，成为GPU内存和PCIe带宽的瓶颈。稀疏注意力机制虽能缓解此问题，但其索引计算仍需遍历所有键向量，开销较大。

Method: LFPS通过捕捉解码注意力的垂直模式和斜线模式，动态构建稀疏索引候选，并结合位置扩展策略预测当前步骤的Top-k索引。

Result: 在LongBench-RULER等长上下文基准测试中，LFPS实现了22.8倍于全注意力和9.6倍于精确Top-k检索的加速，同时保持生成准确性。

Conclusion: LFPS为长上下文LLM推理中的解码优化提供了高效实用的解决方案。

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [70] [Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2506.15705)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Main category: cs.LG

TL;DR: 研究了时间序列基础模型（TSFMs）在宏观经济指标预测中的零样本能力，发现其在稳定经济条件下表现优异，但在快速冲击时期性能下降。


<details>
  <summary>Details</summary>
Motivation: 探索TSFMs在无需训练定制模型的情况下预测宏观经济指标的潜力，为实践者提供零样本部署的指导。

Method: 在数据稀缺和结构断裂条件下，对三种先进TSFMs（Chronos、TimeGPT和Moirai）进行严格回测。

Result: TSFMs能内化经济动态、适应制度变化并提供良好不确定性估计，性能媲美多变量模型，但在快速冲击时期表现下降。

Conclusion: TSFMs在稳定条件下无需微调即可匹配或超越传统模型，但在快速冲击时期需谨慎使用。

Abstract: This study investigates zero-shot forecasting capabilities of Time Series
Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to
forecasting economic indicators under univariate conditions, bypassing the need
for train bespoke econometric models using and extensive training datasets. Our
experiments were conducted on a case study dataset, without additional
customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,
TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our
results demonstrate that appropriately engineered TSFMs can internalise rich
economic dynamics, accommodate regime shifts, and deliver well-behaved
uncertainty estimates out of the box, while matching state-of-the-art
multivariate models on this domain. Our findings suggest that, without any
fine-tuning, TSFMs can match or exceed classical models during stable economic
conditions. However, they are vulnerable to degradation in performances during
periods of rapid shocks. The findings offer guidance to practitioners on when
zero-shot deployments are viable for macroeconomic monitoring and strategic
planning.

</details>


### [71] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Main category: cs.LG

TL;DR: 论文提出了一种多粒度直接偏好优化（MDPO）方法，通过三个粒度优化LLMs的数学推理能力，解决了DPO在长链推理中的局限性，并在实验中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学推理中容易产生幻觉，现有方法如DPO在长链推理中效果有限，需要更有效的方法来优化推理能力。

Method: 提出MDPO方法，通过Solution2Solution、Inference2Inference和Step2Step三个粒度优化LLMs的数学推理，并统一训练目标与生成指标对齐。

Result: 在Qwen2和Llama3模型上，GSM8K数据集提升1.7%和0.9%，MATH数据集提升2.3%和1.2%，优于DPO及其变体。

Conclusion: MDPO通过多粒度优化显著提升了LLMs的数学推理能力，同时提供了无需人工标注的数据构建流程。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [72] [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
*Xinglin Wang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: TTS通过搜索优化LLM性能，但现有方法在资源分配上效率不足。DORA提出方向级资源分配，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法在固定预算下资源分配效率低，无法最大化正确解的概率。

Method: 将测试时搜索建模为资源分配问题，提出DORA方法，通过方向级分配优化资源利用。

Result: DORA在MATH500、AIME2024和AIME2025等数学推理基准上表现最优。

Conclusion: DORA为LLM的TTS提供了更优的资源分配策略，推动了该领域的发展。

Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models
(LLMs) by using additional inference-time computation to explore multiple
reasoning paths through search. Yet how to allocate a fixed rollout budget most
effectively during search remains underexplored, often resulting in inefficient
use of compute at test time. To bridge this gap, we formulate test-time search
as a resource allocation problem and derive the optimal allocation strategy
that maximizes the probability of obtaining a correct solution under a fixed
rollout budget. Within this formulation, we reveal a core limitation of
existing search methods: solution-level allocation tends to favor reasoning
directions with more candidates, leading to theoretically suboptimal and
inefficient use of compute. To address this, we propose Direction-Oriented
Resource Allocation (DORA), a provably optimal method that mitigates this bias
by decoupling direction quality from candidate count and allocating resources
at the direction level. To demonstrate DORA's effectiveness, we conduct
extensive experiments on challenging mathematical reasoning benchmarks
including MATH500, AIME2024, and AIME2025. The empirical results show that DORA
consistently outperforms strong baselines with comparable computational cost,
achieving state-of-the-art accuracy. We hope our findings contribute to a
broader understanding of optimal TTS for LLMs.

</details>


### [73] [Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification](https://arxiv.org/abs/2506.15708)
*Falih Gozi Febrinanto,Adonia Simango,Chengpei Xu,Jingjing Zhou,Jiangang Ma,Sonika Tyagi,Feng Xia*

Main category: cs.LG

TL;DR: 提出了一种名为CGB的新框架，通过因果发现方法和几何曲率策略建模脑网络，显著提升了脑疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN框架未考虑脑区间的因果关系，而因果关系比相关性更能揭示信号间的交互作用。

Method: 结合因果发现方法（转移熵）和几何曲率策略，构建并优化因果图。

Result: CGB在脑疾病分类任务中优于现有方法，F1分数更高。

Conclusion: CGB通过建模因果关系和优化图结构，显著提升了脑疾病检测性能。

Abstract: Graph neural networks (GNNs) have been developed to model the relationship
between regions of interest (ROIs) in brains and have shown significant
improvement in detecting brain diseases. However, most of these frameworks do
not consider the intrinsic relationship of causality factor between brain ROIs,
which is arguably more essential to observe cause and effect interaction
between signals rather than typical correlation values. We propose a novel
framework called CGB (Causal Graphs for Brains) for brain disease
classification/detection, which models refined brain networks based on the
causal discovery method, transfer entropy, and geometric curvature strategy.
CGB unveils causal relationships between ROIs that bring vital information to
enhance brain disease classification performance. Furthermore, CGB also
performs a graph rewiring through a geometric curvature strategy to refine the
generated causal graph to become more expressive and reduce potential
information bottlenecks when GNNs model it. Our extensive experiments show that
CGB outperforms state-of-the-art methods in classification tasks on brain
disease datasets, as measured by average F1 scores.

</details>


### [74] [Studying and Improving Graph Neural Network-based Motif Estimation](https://arxiv.org/abs/2506.15709)
*Pedro C. Vieira,Miguel E. P. Silva,Pedro Manuel Pinto Ribeiro*

Main category: cs.LG

TL;DR: 该论文提出了一种直接估计网络模体显著性谱（SP）的方法，而非传统的子图频率估计，并通过多目标回归优化问题，提高了可解释性、稳定性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）在网络模体SP预测方面缺乏探索和基准，论文旨在填补这一空白，并突破子图频率估计的理论限制。

Method: 将SP估计问题重新定义为独立于子图频率估计的任务，采用多目标回归方法，并优化可解释性、稳定性和大规模图的可扩展性。

Result: 实验表明，1-WL受限模型难以精确估计SP，但能通过比较预测SP与合成生成器的SP，近似网络生成过程。

Conclusion: 直接SP估计方法有助于突破子图计数在模体估计中的理论限制，为GNNs在模体分析中的应用提供了新思路。

Abstract: Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.

</details>


### [75] [RAST: Reasoning Activation in LLMs via Small-model Transfer](https://arxiv.org/abs/2506.15710)
*Siru Ouyang,Xinyu Zhu,Zilin Xiao,Minhao Jiang,Yu Meng,Jiawei Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为RAST的高效方法，通过从小型RL训练模型转移概率调整到大型基础模型，显著提升推理能力，同时减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）能提升大型语言模型（LLMs）的推理能力，但其大规模应用资源消耗巨大。研究发现RL并未赋予模型新知识，而是调整输出分布以激活基础模型的潜在能力。

Method: 提出RAST方法，通过分析RL诱导的输出分布在不同模型规模下的对齐性，将小型RL训练模型的概率调整转移到大型模型。

Result: 实验表明，RAST显著提升了基础模型的推理能力，且GPU内存需求远低于直接RL训练，有时性能甚至优于RL训练模型。

Conclusion: RAST为RL驱动的推理提供了新见解，并提供了在不增加计算成本的情况下扩展其优势的实用策略。

Abstract: Reinforcement learning (RL) has become a powerful approach for improving the
reasoning capabilities of large language models (LLMs), as evidenced by recent
successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale
remains intimidatingly resource-intensive, requiring multiple model copies and
extensive GPU workloads. On the other hand, while being powerful, recent
studies suggest that RL does not fundamentally endow models with new knowledge;
rather, it primarily reshapes the model's output distribution to activate
reasoning capabilities latent in the base model. Building on this insight, we
hypothesize that the changes in output probabilities induced by RL are largely
model-size invariant, opening the door to a more efficient paradigm: training a
small model with RL and transferring its induced probability shifts to larger
base models. To verify our hypothesis, we conduct a token-level analysis of
decoding trajectories and find high alignment in RL-induced output
distributions across model scales, validating our hypothesis. Motivated by
this, we propose RAST, a simple yet effective method that transfers reasoning
behaviors by injecting RL-induced probability adjustments from a small
RL-trained model into larger models. Experiments across multiple mathematical
reasoning benchmarks show that RAST substantially and consistently enhances the
reasoning capabilities of base models while requiring significantly lower GPU
memory than direct RL training, sometimes even yielding better performance than
the RL-trained counterparts. Our findings offer new insights into the nature of
RL-driven reasoning and practical strategies for scaling its benefits without
incurring its full computational cost. The project page of RAST is available at
https://ozyyshr.github.io/RAST/.

</details>


### [76] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种针对联邦学习中梯度反转攻击的防御框架，通过影子模型和针对性噪声注入保护敏感区域，同时最小化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护分布式训练中具有潜力，但梯度反转攻击可能导致隐私泄露。现有防御方法缺乏对敏感梯度的精确识别，导致保护不足或性能下降。

Method: 利用影子模型和可解释性技术识别敏感区域，并进行样本特定的噪声注入。

Result: 在ChestXRay和EyePACS数据集上，PSNR和SSIM指标显著优于无防御情况，且模型性能损失小于1%。

Conclusion: 该框架能有效防御多种梯度反转攻击，适用于多种医疗图像，且对联邦学习的性能影响极小。

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [77] [BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling](https://arxiv.org/abs/2506.15712)
*Songqi Zhou,Ruixue Liu,Yixing Wang,Jia Lu,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于BERT风格预训练的新框架，用于锂离子电池故障检测，通过定制的时间序列表示模块和点级掩码信号建模任务，显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉复杂的时间依赖性，且无法充分利用未标记数据，而大型语言模型（LLMs）的架构不适合工业场景中的数值时间序列数据。

Method: 扩展标准BERT架构，引入时间序列到令牌的表示模块和点级掩码信号建模（point-MSM）预训练任务，结合电池元数据进行分类。

Result: 在真实数据集上，AUROC达到0.945，显著优于现有方法。

Conclusion: 验证了BERT风格预训练在时间序列故障检测中的有效性。

Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe
and reliable operation of electric vehicles and energy storage systems.
However, existing methods often struggle to capture complex temporal
dependencies and cannot fully leverage abundant unlabeled data. Although large
language models (LLMs) exhibit strong representation capabilities, their
architectures are not directly suited to the numerical time-series data common
in industrial settings. To address these challenges, we propose a novel
framework that adapts BERT-style pretraining for battery fault detection by
extending the standard BERT architecture with a customized time-series-to-token
representation module and a point-level Masked Signal Modeling (point-MSM)
pretraining task tailored to battery applications. This approach enables
self-supervised learning on sequential current, voltage, and other
charge-discharge cycle data, yielding distributionally robust, context-aware
temporal embeddings. We then concatenate these embeddings with battery metadata
and feed them into a downstream classifier for accurate fault classification.
Experimental results on a large-scale real-world dataset show that models
initialized with our pretrained parameters significantly improve both
representation quality and classification accuracy, achieving an AUROC of 0.945
and substantially outperforming existing approaches. These findings validate
the effectiveness of BERT-style pretraining for time-series fault detection.

</details>


### [78] [An application of machine learning to the motion response prediction of floating assets](https://arxiv.org/abs/2506.15713)
*Michael T. M. B. Morris-Thomas,Marius Martens*

Main category: cs.LG

TL;DR: 本文提出了一种监督机器学习方法，用于预测浮动海上资产在随机海洋条件下的非线性运动响应，显著优于传统频域方法。


<details>
  <summary>Details</summary>
Motivation: 实时预测浮动海上资产在随机海洋条件下的行为是海上工程中的重大挑战，传统方法在极端海况和非线性响应中表现不佳。

Method: 采用多元回归的监督机器学习方法，结合梯度提升集成方法和自定义被动转向求解器，训练了约100万样本和100个特征。

Result: 模型对关键系泊参数的平均预测误差小于5%，船舶航向精度在2.5度以内，显著优于传统频域方法。

Conclusion: 该框架已成功应用于实际设施，证明了其在实时船舶监测和海上操作决策中的有效性。

Abstract: The real-time prediction of floating offshore asset behavior under stochastic
metocean conditions remains a significant challenge in offshore engineering.
While traditional empirical and frequency-domain methods work well in benign
conditions, they struggle with both extreme sea states and nonlinear responses.
This study presents a supervised machine learning approach using multivariate
regression to predict the nonlinear motion response of a turret-moored vessel
in 400 m water depth. We developed a machine learning workflow combining a
gradient-boosted ensemble method with a custom passive weathervaning solver,
trained on approximately $10^6$ samples spanning 100 features. The model
achieved mean prediction errors of less than 5% for critical mooring parameters
and vessel heading accuracy to within 2.5 degrees across diverse metocean
conditions, significantly outperforming traditional frequency-domain methods.
The framework has been successfully deployed on an operational facility,
demonstrating its efficacy for real-time vessel monitoring and operational
decision-making in offshore environments.

</details>


### [79] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出了一种可学习的双边短时拉普拉斯变换（STLT）机制，替代传统自注意力，实现动态调整令牌相关性和频率响应，支持超长序列建模。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力在长序列建模中存在计算瓶颈，需要更高效且可扩展的替代方案。

Method: 通过可训练的拉普拉斯节点参数（衰减率、振荡频率、窗口带宽）和快速递归卷积，结合FFT计算和自适应节点分配机制。

Result: 在语言建模、机器翻译和长文档问答任务中表现优于或相当于现有高效Transformer，支持超过100k tokens的上下文长度。

Conclusion: STLT结合了可解释性、可扩展性和鲁棒性，为超长序列建模提供了新途径。

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [80] [NeuronSeek: On Stability and Expressivity of Task-driven Neurons](https://arxiv.org/abs/2506.15715)
*Hanyu Pei,Jing-Xiao Liao,Qibin Zhao,Ting Gao,Shijun Zhang,Xiaoge Zhang,Feng-Lei Fan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于张量分解（TD）的任务驱动神经元优化方法（NeuronSeek-TD），替代了原有的符号回归（SR），以提高稳定性和收敛速度，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑中不同神经元处理不同任务的启发，研究如何通过优化神经元设计来提升网络性能。

Method: 使用张量分解（TD）替代符号回归（SR）来发现最优神经元结构，并理论证明了修改聚合函数可以实现任意连续函数的近似。

Result: 实验表明，NeuronSeek-TD在稳定性和性能上优于现有方法。

Conclusion: NeuronSeek-TD框架在理论和实践中均表现出色，为任务驱动神经元设计提供了新思路。

Abstract: Drawing inspiration from our human brain that designs different neurons for
different tasks, recent advances in deep learning have explored modifying a
network's neurons to develop so-called task-driven neurons. Prototyping
task-driven neurons (referred to as NeuronSeek) employs symbolic regression
(SR) to discover the optimal neuron formulation and construct a network from
these optimized neurons. Along this direction, this work replaces symbolic
regression with tensor decomposition (TD) to discover optimal neuronal
formulations, offering enhanced stability and faster convergence. Furthermore,
we establish theoretical guarantees that modifying the aggregation functions
with common activation functions can empower a network with a fixed number of
parameters to approximate any continuous function with an arbitrarily small
error, providing a rigorous mathematical foundation for the NeuronSeek
framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD
framework not only achieves superior stability, but also is competitive
relative to the state-of-the-art models across diverse benchmarks. The code is
available at https://github.com/HanyuPei22/NeuronSeek.

</details>


### [81] [Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies](https://arxiv.org/abs/2506.15716)
*Angelos Assos,Carmel Baharav,Bailey Flanigan,Ariel Procaccia*

Main category: cs.LG

TL;DR: 提出了一种优化框架，用于解决公民集会中因参与者退出导致代表性失衡的问题，通过算法选择替补成员以最小化预期偏差。


<details>
  <summary>Details</summary>
Motivation: 公民集会的合法性依赖于其对广泛人群的代表性，但参与者退出导致代表性失衡，现有方法未考虑替补成员的选择。

Method: 利用学习理论工具，基于历史数据估计退出概率，并通过优化算法选择替补成员以最小化预期偏差。

Result: 理论分析提供了样本复杂性和计算效率的保证，实证研究表明该方法显著提高了代表性并减少了替补需求。

Conclusion: 提出的方法有效解决了公民集会中的代表性失衡问题，具有理论和实践意义。

Abstract: An increasingly influential form of deliberative democracy centers on
citizens' assemblies, where randomly selected people discuss policy questions.
The legitimacy of these panels hinges on their representation of the broader
population, but panelists often drop out, leading to an unbalanced composition.
Although participant attrition is mitigated in practice by alternates, their
selection is not taken into account by existing methods. To address this gap,
we introduce an optimization framework for alternate selection. Our algorithmic
approach, which leverages learning-theoretic machinery, estimates dropout
probabilities using historical data and selects alternates to minimize expected
misrepresentation. We establish theoretical guarantees for our approach,
including worst-case bounds on sample complexity (with implications for
computational efficiency) and on loss when panelists' probabilities of dropping
out are mis-estimated. Empirical evaluation using real-world data demonstrates
that, compared to the status quo, our method significantly improves
representation while requiring fewer alternates.

</details>


### [82] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: 本文提出了一种名为daDPO的新方法，通过结合偏好优化和基于分布的蒸馏，显著提升了小型语言模型的对话能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限环境中表现不佳，现有知识蒸馏方法忽略了教师模型的输出分布信息。

Method: 引入daDPO方法，结合偏好优化和分布感知蒸馏，通过理论分析和实验验证其有效性。

Result: daDPO在修剪模型和小型模型上表现优异，例如修剪后的Vicuna1.5-7B接近教师模型性能，Qwen2.5-1.5B甚至偶尔超越其7B教师模型。

Conclusion: daDPO是一种高效的方法，能够显著提升小型语言模型的性能，填补了现有方法的不足。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [83] [BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata](https://arxiv.org/abs/2506.15718)
*Yu Guo,Hongji Fang,Tianyu Fang,Zhe Cui*

Main category: cs.LG

TL;DR: 论文介绍了BuildingBRep-11K数据集，包含11,978个多楼层建筑，用于训练3D建筑生成模型，并验证其学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动生成3D建筑模型时缺乏大规模、高质量标注数据集的问题。

Method: 通过形状语法驱动的流程生成建筑，结合空间尺度、采光优化和室内布局约束，并通过多阶段过滤器确保符合建筑标准。

Result: 训练了两个轻量级PointNet基线模型，验证了数据集在几何回归和拓扑质量评估上的学习能力。

Conclusion: BuildingBRep-11K数据集具有学习价值，但任务仍具挑战性。

Abstract: With the rise of artificial intelligence, the automatic generation of
building-scale 3-D objects has become an active research topic, yet training
such models still demands large, clean and richly annotated datasets. We
introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)
buildings (about 10 GB) produced by a shape-grammar-driven pipeline that
encodes established building-design principles. Every sample consists of a
geometrically exact B-rep solid-covering floors, walls, slabs and rule-based
openings-together with a fast-loading .npy metadata file that records detailed
per-floor parameters. The generator incorporates constraints on spatial scale,
daylight optimisation and interior layout, and the resulting objects pass
multi-stage filters that remove Boolean failures, undersized rooms and extreme
aspect ratios, ensuring compliance with architectural standards. To verify the
dataset's learnability we trained two lightweight PointNet baselines. (i)
Multi-attribute regression. A single encoder predicts storey count, total
rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100
unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room
MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same
backbone we classify GOOD versus DEFECT; on a balanced 100-model set the
network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \%
precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K
is learnable yet non-trivial for both geometric regression and topological
quality assessment

</details>


### [84] [Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems](https://arxiv.org/abs/2506.15719)
*Manal Rahal,Bestoun S. Ahmed,Roger Renstrom,Robert Stener,Albrecht Wurtz*

Main category: cs.LG

TL;DR: 本文提出了一种结合机器学习和异常检测的新方法，用于优化家用热泵热水需求预测，实验表明LightGBM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的控制方法限制了热泵在热水生产中的效率，机器学习在此领域的优化研究不足。

Method: 结合机器学习与隔离森林（iForest）预测热水需求，采用多步特征选择和时间序列分析，应用并优化了LightGBM、LSTM和双向LSTM三种模型。

Result: LightGBM表现最优，RMSE提升达9.37%，iForest异常检测F1得分为0.87，误报率仅5.2%。

Conclusion: 该方法适用于实际热泵部署，具有强泛化能力。

Abstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for
sustainable energy systems, but their efficiency in producing hot water remains
restricted by conventional threshold-based control methods. Although machine
learning (ML) has been successfully implemented for various HP applications,
optimization of household hot water demand forecasting remains understudied.
This paper addresses this problem by introducing a novel approach that combines
predictive ML with anomaly detection to create adaptive hot water production
strategies based on household-specific consumption patterns. Our key
contributions include: (1) a composite approach combining ML and isolation
forest (iForest) to forecast household demand for hot water and steer
responsive HP operations; (2) multi-step feature selection with advanced
time-series analysis to capture complex usage patterns; (3) application and
tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long
Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention
mechanism on data from different types of real HP installations; and (4)
experimental validation on six real household installations. Our experiments
show that the best-performing model LightGBM achieves superior performance,
with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$
values between 0.748-0.983. For anomaly detection, our iForest implementation
achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%,
demonstrating strong generalization capabilities across different household
types and consumption patterns, making it suitable for real-world HP
deployments.

</details>


### [85] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: 本文提出了一种新的少样本类增量学习方法（Tri-WE），通过权重空间的三元集成和知识蒸馏正则化，解决了灾难性遗忘和过拟合问题，并在多个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的FSCIL方法通常固定特征提取器，限制了模型对新类的适应性。本文旨在通过动态更新整个模型，提升模型在少样本增量学习中的表现。

Method: 提出了Tri-WE方法，通过在权重空间中插值基础模型、前一个模型和当前模型的分类头，实现知识协同维护；同时引入基于放大数据知识蒸馏的正则化损失项。

Result: 在miniImageNet、CUB200和CIFAR100数据集上取得了最先进的结果。

Conclusion: Tri-WE方法有效解决了FSCIL中的灾难性遗忘和过拟合问题，证明了动态更新整个模型的可行性。

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [86] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: Bohdi是一个基于合成数据的异构大语言模型融合框架，通过分层树结构和动态调整机制解决现有方法的局限性，显著提升目标模型的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有异构大语言模型融合方法依赖有限领域的真实数据且分配比例固定，导致知识获取不全面和能力不平衡。

Method: Bohdi采用分层树结构组织知识域，通过多模型协作生成多领域数据，并基于分层多臂老虎机问题动态调整采样比例。

Result: 实验表明，Bohdi在多个目标模型上显著优于基线方法，数据效率更高且能力不平衡问题几乎消除。

Conclusion: Bohdi通过合成数据和动态机制有效解决了异构大语言模型融合的挑战，具有广泛的应用潜力。

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [87] [UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation](https://arxiv.org/abs/2506.15722)
*Wangzhi Zhan,Jianpeng Chen,Dongqi Fu,Dawei Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为UNIMATE的统一模型，用于同时处理机械超材料设计中的3D拓扑、密度条件和力学性能三种模态，填补了现有研究仅关注两种模态的空白。


<details>
  <summary>Details</summary>
Motivation: 现实复杂应用场景要求机器学习模型同时考虑3D拓扑、密度条件和力学性能三种模态，而现有研究大多仅关注其中两种，因此需要一种更全面的解决方案。

Method: UNIMATE模型包含模态对齐模块和协同扩散生成模块，能够同时处理三种模态。

Result: 实验表明，UNIMATE在拓扑生成、性能预测和条件确认任务中分别优于基线模型80.2%、5.1%和50.2%。

Conclusion: UNIMATE填补了现有研究的空白，并在多项任务中表现出色，已开源供社区使用。

Abstract: Metamaterials are artificial materials that are designed to meet unseen
properties in nature, such as ultra-stiffness and negative materials indices.
In mechanical metamaterial design, three key modalities are typically involved,
i.e., 3D topology, density condition, and mechanical property. Real-world
complex application scenarios place the demanding requirements on machine
learning models to consider all three modalities together. However, a
comprehensive literature review indicates that most existing works only
consider two modalities, e.g., predicting mechanical properties given the 3D
topology or generating 3D topology given the required properties. Therefore,
there is still a significant gap for the state-of-the-art machine learning
models capturing the whole. Hence, we propose a unified model named UNIMATE,
which consists of a modality alignment module and a synergetic diffusion
generation module. Experiments indicate that UNIMATE outperforms the other
baseline models in topology generation task, property prediction task, and
condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We
opensource our proposed UNIMATE model and corresponding results at
https://github.com/wzhan24/UniMate.

</details>


### [88] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: MadaKV是一种模态自适应的KV缓存淘汰策略，旨在提升多模态大语言模型（MLLMs）在长上下文推理中的效率。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存淘汰方法在单模态场景下表现良好，但在多模态场景中无法捕捉模态特定信息，导致性能不佳。

Method: MadaKV通过模态偏好适应和分层压缩补偿两个关键组件，动态感知注意力头中的模态信息并自适应保留关键令牌。

Result: MadaKV显著减少了KV缓存内存占用和模型推理解码延迟（提升1.3至1.5倍），同时在多模态长上下文任务中保持高准确性。

Conclusion: 实验证明，MadaKV在代表性MLLMs和MileBench基准测试中优于现有KV缓存淘汰方法。

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [89] [Graph Diffusion that can Insert and Delete](https://arxiv.org/abs/2506.15725)
*Matteo Ninniri,Marco Podda,Davide Bacciu*

Main category: cs.LG

TL;DR: 论文提出了一种基于离散去噪扩散概率模型（DDPMs）的图生成模型GrIDDD，通过支持节点单调插入和删除，解决了现有方法无法适应图大小变化的限制，提升了分子生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DDPMs的图生成模型无法在扩散过程中调整图的大小（原子数量），限制了其在条件生成（如属性驱动的分子设计）中的有效性。

Method: 重新定义了噪声和去噪过程，支持节点的单调插入和删除，提出了动态调整化学图大小的GrIDDD模型。

Result: GrIDDD在分子属性定向生成中表现优于现有图扩散模型，并在分子优化任务中展现出竞争力。

Conclusion: GrIDDD为尺寸自适应的分子生成提供了新思路，推动了图扩散模型的发展。

Abstract: Generative models of graphs based on discrete Denoising Diffusion
Probabilistic Models (DDPMs) offer a principled approach to molecular
generation by systematically removing structural noise through iterative atom
and bond adjustments. However, existing formulations are fundamentally limited
by their inability to adapt the graph size (that is, the number of atoms)
during the diffusion process, severely restricting their effectiveness in
conditional generation scenarios such as property-driven molecular design,
where the targeted property often correlates with the molecular size. In this
paper, we reformulate the noising and denoising processes to support monotonic
insertion and deletion of nodes. The resulting model, which we call GrIDDD,
dynamically grows or shrinks the chemical graph during generation. GrIDDD
matches or exceeds the performance of existing graph diffusion models on
molecular property targeting despite being trained on a more difficult problem.
Furthermore, when applied to molecular optimization, GrIDDD exhibits
competitive performance compared to specialized optimization models. This work
paves the way for size-adaptive molecular generation with graph diffusion.

</details>


### [90] [Descriptor-based Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2506.15792)
*Jackson Burns,Akshat Zalte,William Green*

Main category: cs.LG

TL;DR: CheMeleon是一种基于分子描述符预训练的分子基础模型，通过低噪声数据学习分子表示，在多个基准测试中表现优异，但在区分活性悬崖方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 快速准确地预测分子性质对科学进步至关重要，传统方法依赖噪声数据或模拟，而CheMeleon通过低噪声描述符学习更丰富的分子表示。

Method: CheMeleon使用Mordred包中的确定性分子描述符进行预训练，采用定向消息传递神经网络（D-MPNN）预测这些描述符。

Result: 在Polaris和MoleculeACE的58个基准测试中，CheMeleon分别以79%和97%的胜率优于基线模型，但难以区分活性悬崖。

Conclusion: 基于描述符的预训练为分子性质预测提供了可扩展且有效的方法，未来可进一步探索描述符集和未标记数据集。

Abstract: Fast and accurate prediction of molecular properties with machine learning is
pivotal to scientific advancements across myriad domains. Foundation models in
particular have proven especially effective, enabling accurate training on
small, real-world datasets. This study introduces CheMeleon, a novel molecular
foundation model pre-trained on deterministic molecular descriptors from the
Mordred package, leveraging a Directed Message-Passing Neural Network to
predict these descriptors in a noise-free setting. Unlike conventional
approaches relying on noisy experimental data or biased quantum mechanical
simulations, CheMeleon uses low-noise molecular descriptors to learn rich
molecular representations. Evaluated on 58 benchmark datasets from Polaris and
MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,
outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop
(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)
and other foundation models. However, it struggles to distinguish activity
cliffs like many of the tested models. The t-SNE projection of CheMeleon's
learned representations demonstrates effective separation of chemical series,
highlighting its ability to capture structural nuances. These results
underscore the potential of descriptor-based pre-training for scalable and
effective molecular property prediction, opening avenues for further
exploration of descriptor sets and unlabeled datasets.

</details>


### [91] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Deep Patient Journey (DeepJ)的图卷积变换器模型，用于捕捉电子健康记录(EHR)数据中跨就诊的医疗事件交互。


<details>
  <summary>Details</summary>
Motivation: 现有图学习方法在建模跨就诊的医疗事件交互时存在不足，无法有效捕捉时间依赖性。

Method: DeepJ结合了图卷积和变换器技术，采用可微分图池化方法，同时建模就诊内和跨就诊的医疗事件交互。

Result: DeepJ在预测患者结果方面显著优于五种基线模型，并提高了可解释性。

Conclusion: DeepJ为患者风险分层提供了改进潜力，能够识别与患者结果相关的关键事件群。

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [92] [Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions](https://arxiv.org/abs/2506.15817)
*Jason Tandiary*

Main category: cs.LG

TL;DR: 本文提出了一种基于BROAD-OMD框架的新算法，利用机器学习预测最高竞争出价，实现了在准确预测下的零遗憾，并在特定条件下建立了O(T^(3/4) * Vt^(1/4))的有界遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究一价拍卖在二元反馈下的表现，结合机器学习算法的预测能力，改进BROAD-OMD算法的遗憾界。

Method: 在BROAD-OMD框架内提出新算法，利用历史信息和机器学习模型预测最高竞争出价。

Result: 算法在准确预测下实现零遗憾，并在特定条件下获得O(T^(3/4) * Vt^(1/4))的有界遗憾界。

Conclusion: 新算法显著提升了一价拍卖中的表现，展示了机器学习在拍卖理论中的潜力。

Abstract: This paper studies Vickrey first-price auctions under binary feedback.
Leveraging the enhanced performance of machine learning algorithms, the new
algorithm uses past information to improve the regret bounds of the BROAD-OMD
algorithm. Motivated by the growing relevance of first-price auctions and the
predictive capabilities of machine learning models, this paper proposes a new
algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages
predictions of the highest competing bid. This paper's main contribution is an
algorithm that achieves zero regret under accurate predictions. Additionally, a
bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain
normality conditions.

</details>


### [93] [AI-based modular warning machine for risk identification in proximity healthcare](https://arxiv.org/abs/2506.15823)
*Chiara Razzetta,Shahryar Noei,Federico Barbarossa,Edoardo Spairani,Monica Roascio,Elisa Barbi,Giulia Ciacci,Sara Sommariva,Sabrina Guastavino,Michele Piana,Matteo Lenge,Gabriele Arnulfo,Giovanni Magenes,Elvira Maranesi,Giulio Amabili,Anna Maria Massone,Federico Benvenuto,Giuseppe Jurman,Diego Sona,Cristina Campi*

Main category: cs.LG

TL;DR: DHEAL-COM项目开发了数字健康解决方案，通过机器学习分析多模态数据，提供预测结果和特征解释。


<details>
  <summary>Details</summary>
Motivation: 为社区医疗开发数字健康解决方案，利用多模态数据提升医疗服务的效率和准确性。

Method: 采用无监督和有监督的机器学习方法构建自动化流程，处理数据并提供预测和特征识别。

Result: 成功开发了一个能够处理多模态数据并提供预测结果的自动化流程。

Conclusion: 该研究为社区医疗的数字健康解决方案提供了有效的技术框架。

Abstract: "DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.

</details>


### [94] [Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters](https://arxiv.org/abs/2506.15825)
*Luiz Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein重心的新算法FedWB，用于分布式架构中的全局DNN训练，并扩展应用于异构联邦强化学习（HFRL）。


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中模型融合的挑战，并推广到异构联邦强化学习场景。

Method: 将数据集分发给多个代理训练本地DNN，通过Wasserstein重心聚合权重参数，形成FedWB算法；进一步应用于HFRL，通过全局聚合步骤生成通用模型。

Result: 在CartPole问题中验证了FedWB和HFRL算法的有效性，生成了适用于异构环境的全局DQN。

Conclusion: FedWB算法在分布式训练和异构联邦强化学习中表现出色，具有广泛的应用潜力。

Abstract: In this paper, we first propose a novel algorithm for model fusion that
leverages Wasserstein barycenters in training a global Deep Neural Network
(DNN) in a distributed architecture. To this end, we divide the dataset into
equal parts that are fed to "agents" who have identical deep neural networks
and train only over the dataset fed to them (known as the local dataset). After
some training iterations, we perform an aggregation step where we combine the
weight parameters of all neural networks using Wasserstein barycenters. These
steps form the proposed algorithm referred to as FedWB. Moreover, we leverage
the processes created in the first part of the paper to develop an algorithm to
tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test
experiment is the CartPole toy problem, where we vary the lengths of the poles
to create heterogeneous environments. We train a deep Q-Network (DQN) in each
environment to learn to control each cart, while occasionally performing a
global aggregation step to generalize the local models; the end outcome is a
global DQN that functions across all environments.

</details>


### [95] [In-field Calibration of Low-Cost Sensors through XGBoost $\&$ Aggregate Sensor Data](https://arxiv.org/abs/2506.15840)
*Kevin Yin,Julia Gersey,Pei Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于XGBoost集成学习的现场传感器校准模型，用于改善低成本传感器的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于高精度传感器成本高，限制了其部署范围，而低成本传感器易受环境因素和制造差异影响，导致数据漂移。

Method: 采用XGBoost集成学习方法，整合邻近传感器的数据，减少对单个传感器精度的依赖。

Result: 该方法提高了传感器的校准精度，并在不同地点表现出更好的泛化能力。

Conclusion: 该模型为大规模空气质量监测提供了一种低成本、高精度的解决方案。

Abstract: Effective large-scale air quality monitoring necessitates distributed sensing
due to the pervasive and harmful nature of particulate matter (PM),
particularly in urban environments. However, precision comes at a cost: highly
accurate sensors are expensive, limiting the spatial deployments and thus their
coverage. As a result, low-cost sensors have become popular, though they are
prone to drift caused by environmental sensitivity and manufacturing
variability. This paper presents a model for in-field sensor calibration using
XGBoost ensemble learning to consolidate data from neighboring sensors. This
approach reduces dependence on the presumed accuracy of individual sensors and
improves generalization across different locations.

</details>


### [96] [Uncertainty Estimation by Human Perception versus Neural Models](https://arxiv.org/abs/2506.15850)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: 现代神经网络预测准确性高但校准性差，导致过度自信的预测。研究比较了人类感知不确定性与神经网络估计的不确定性，发现当前方法与人类直觉仅弱相关。引入人类软标签可改善校准性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的高预测准确性常伴随校准性差的问题，这在需要可靠不确定性估计的应用中带来挑战。研究旨在比较人类与神经网络的不确定性估计，以改善模型的可信度。

Method: 使用三个视觉基准数据集，标注了人类分歧和众包置信度，评估模型预测不确定性与人类感知不确定性的相关性。

Result: 当前方法与人类直觉仅弱相关，且相关性因任务和不确定性指标而异。引入人类软标签可改善校准性且不影响准确性。

Conclusion: 模型与人类不确定性存在显著差距，利用人类反馈可指导开发更可信的AI系统。

Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but
remain poorly calibrated, producing overconfident predictions even when wrong.
This miscalibration poses serious challenges in applications where reliable
uncertainty estimates are critical. In this work, we investigate how human
perceptual uncertainty compares to uncertainty estimated by NNs. Using three
vision benchmarks annotated with both human disagreement and crowdsourced
confidence, we assess the correlation between model-predicted uncertainty and
human-perceived uncertainty. Our results show that current methods only weakly
align with human intuition, with correlations varying significantly across
tasks and uncertainty metrics. Notably, we find that incorporating
human-derived soft labels into the training process can improve calibration
without compromising accuracy. These findings reveal a persistent gap between
model and human uncertainty and highlight the potential of leveraging human
insights to guide the development of more trustworthy AI systems.

</details>


### [97] [Improving Rectified Flow with Boundary Conditions](https://arxiv.org/abs/2506.15864)
*Xixi Hu,Runlong Liao,Keyang Xu,Bo Liu,Yeqing Li,Eugene Ie,Hongliang Fei,Qiang Liu*

Main category: cs.LG

TL;DR: Boundary-enforced Rectified Flow Model (Boundary RF Model)通过强制边界条件改进了原始Rectified Flow模型，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 原始Rectified Flow模型直接建模速度场时，未约束的神经网络可能导致边界条件不满足，影响ODE的准确性，尤其在随机采样时误差放大。

Method: 提出Boundary RF Model，通过最小代码修改强制边界条件。

Result: 在ImageNet上，ODE采样FID得分提升8.01%，SDE采样提升8.98%。

Conclusion: Boundary RF Model通过简单修改有效解决了边界条件问题，显著提升了生成模型的性能。

Abstract: Rectified Flow offers a simple and effective approach to high-quality
generative modeling by learning a velocity field. However, we identify a
limitation in directly modeling the velocity with an unconstrained neural
network: the learned velocity often fails to satisfy certain boundary
conditions, leading to inaccurate velocity field estimations that deviate from
the desired ODE. This issue is particularly critical during stochastic sampling
at inference, as the score function's errors are amplified near the boundary.
To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary
RF Model), in which we enforce boundary conditions with a minimal code
modification. Boundary RF Model improves performance over vanilla RF model,
demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and
8.98% improvement using SDE sampling.

</details>


### [98] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/abs/2506.15872)
*Sara Kangaslahti,Elan Rosenfeld,Naomi Saphra*

Main category: cs.LG

TL;DR: 论文提出POLCA方法，通过分解损失变化揭示训练中的隐藏突破，用于无监督可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究训练中损失曲线的间断点以理解学习动态，但传统标量损失指标掩盖了这些突破。

Method: 引入POLCA方法，分解低秩训练子空间中损失的变化，识别样本聚类。

Result: 在合成算术和自然语言任务中验证，POLCA能发现可解释的模型能力突破。

Conclusion: 隐藏的相变可作为无监督可解释性工具，具有潜力。

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [99] [Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings](https://arxiv.org/abs/2506.15879)
*Abdel Rahman Alsheyab,Mohammad Alkhasawneh,Nidal Shahin*

Main category: cs.LG

TL;DR: 本文提出了一种基于合成数据集的机器学习方法，用于分析职位市场趋势、预测薪资并聚类相似职位。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示影响职位市场动态的关键特征，为求职者、雇主和研究人员提供有价值的见解。

Method: 采用回归、分类、聚类和自然语言处理（NLP）技术，对合成数据集进行探索性分析，并构建预测模型。

Result: 分析揭示了影响薪资和职位类别的显著因素，并识别出基于数据的职位聚类。

Conclusion: 虽然结果基于合成数据，但该方法为职位市场分析提供了一个可转移的框架。

Abstract: This paper presents a machine learning methodology prototype using a large
synthetic dataset of job listings to identify trends, predict salaries, and
group similar job roles. Employing techniques such as regression,
classification, clustering, and natural language processing (NLP) for
text-based feature extraction and representation, this study aims to uncover
the key features influencing job market dynamics and provide valuable insights
for job seekers, employers, and researchers. Exploratory data analysis was
conducted to understand the dataset's characteristics. Subsequently, regression
models were developed to predict salaries, classification models to predict job
titles, and clustering techniques were applied to group similar jobs. The
analyses revealed significant factors influencing salary and job roles, and
identified distinct job clusters based on the provided data. While the results
are based on synthetic data and not intended for real-world deployment, the
methodology demonstrates a transferable framework for job market analysis.

</details>


### [100] [T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders](https://arxiv.org/abs/2506.15881)
*Alexey Yermakov,David Zoro,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SHRED是一种轻量级模型，用于稀疏传感器数据的系统识别和预测。改进版T-SHRED引入Transformer和SINDy注意力机制，提升性能并增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 改进SHRED模型，通过Transformer提升时间编码性能，并引入SINDy注意力机制增强模型的可解释性。

Method: 使用Transformer替代RNN进行时间编码，并引入SINDy注意力机制进行符号回归。

Result: T-SHRED在多种动态系统上表现优异，尤其在稀疏数据和高数据量情况下均能准确预测。

Conclusion: T-SHRED结合Transformer和SINDy注意力机制，显著提升了预测性能和模型可解释性。

Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification
and forecasting from sparse sensor measurements. Such models are light-weight
and computationally efficient, allowing them to be trained on consumer laptops.
SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple
Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding
respectively. Despite the relatively simple structure of SHRED, they are able
to predict chaotic dynamical systems on different physical, spatial, and
temporal scales directly from a sparse set of sensor measurements. In this
work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal
encoding which improves performance on next-step state prediction on large
datasets. We also introduce a sparse identification of nonlinear dynamics
(SINDy) attention mechanism into T-SHRED to perform symbolic regression
directly on the latent space as part of the model regularization architecture.
Symbolic regression improves model interpretability by learning and
regularizing the dynamics of the latent space during training. We analyze the
performance of T-SHRED on three different dynamical systems ranging from
low-data to high-data regimes. We observe that SINDy attention T-SHRED
accurately predicts future frames based on an interpretable symbolic model
across all tested datasets.

</details>


### [101] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: Fractional Reasoning是一种无需训练、模型无关的框架，通过动态调整推理强度提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Best-of-N、多数投票）对所有输入采用统一推理强度，忽略了问题复杂度的差异。

Method: 提取与深度推理相关的潜在导向向量，并通过可调缩放因子动态调整推理强度。

Result: 在GSM8K、MATH500和GPQA等任务上，Fractional Reasoning显著提升了性能。

Conclusion: Fractional Reasoning为LLM提供了灵活的推理控制，适用于不同复杂度的任务。

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [102] [Formal Models of Active Learning from Contrastive Examples](https://arxiv.org/abs/2506.15893)
*Farnam Mansouri,Hans U. Simon,Adish Singla,Yuxin Chen,Sandra Zilles*

Main category: cs.LG

TL;DR: 论文研究了对比训练样本对机器学习的影响，提出了理论框架分析其对主动学习样本复杂度的作用，并揭示了与自指导学习的联系。


<details>
  <summary>Details</summary>
Motivation: 通过对比样本的微小差异解释标签差异，提升学习效果。

Method: 提出理论框架，分析对比样本对主动学习的影响，以几何概念类和布尔函数类为例。

Result: 揭示了对比样本学习与自指导学习之间的联系。

Conclusion: 对比样本能有效降低学习复杂度，并与传统学习模型有深刻联系。

Abstract: Machine learning can greatly benefit from providing learning algorithms with
pairs of contrastive training examples -- typically pairs of instances that
differ only slightly, yet have different class labels. Intuitively, the
difference in the instances helps explain the difference in the class labels.
This paper proposes a theoretical framework in which the effect of various
types of contrastive examples on active learners is studied formally. The focus
is on the sample complexity of learning concept classes and how it is
influenced by the choice of contrastive examples. We illustrate our results
with geometric concept classes and classes of Boolean functions. Interestingly,
we reveal a connection between learning from contrastive examples and the
classical model of self-directed learning.

</details>


### [103] [KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction](https://arxiv.org/abs/2506.15896)
*Yu Zhang,Gaoshan Bi,Simon Jeffery,Max Davis,Yang Li,Qing Xue,Po Yang*

Main category: cs.LG

TL;DR: 提出了一种知识引导的图神经网络框架，用于精准预测土壤温室气体通量，解决了农业数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 农业系统中精准预测土壤温室气体通量对评估环境影响和可持续发展至关重要，但数据稀缺限制了机器学习的应用。

Method: 结合农业过程模型和图神经网络，利用自编码器提取关键特征，图神经网络整合特征相关性，预测施肥导向的土壤温室气体通量。

Result: 实验表明，该方法在模拟和真实数据集上均优于基线和其他回归方法，具有更高的准确性和稳定性。

Conclusion: 提出的框架为精准农业中的温室气体预测提供了有效解决方案。

Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in
agricultural systems for assessing environmental impacts, developing emission
mitigation strategies and promoting sustainable agriculture. Due to the lack of
advanced sensor and network technologies on majority of farms, there are
challenges in obtaining comprehensive and diverse agricultural data. As a
result, the scarcity of agricultural data seriously obstructs the application
of machine learning approaches in precision soil GHG flux prediction. This
research proposes a knowledge-guided graph neural network framework that
addresses the above challenges by integrating knowledge embedded in an
agricultural process-based model and graph neural network techniques.
Specifically, we utilise the agricultural process-based model to simulate and
generate multi-dimensional agricultural datasets for 47 countries that cover a
wide range of agricultural variables. To extract key agricultural features and
integrate correlations among agricultural features in the prediction process,
we propose a machine learning framework that integrates the autoencoder and
multi-target multi-graph based graph neural networks, which utilises the
autoencoder to selectively extract significant agricultural features from the
agricultural process-based model simulation data and the graph neural network
to integrate correlations among agricultural features for accurately predict
fertilisation-oriented soil GHG fluxes. Comprehensive experiments were
conducted with both the agricultural simulation dataset and real-world
agricultural dataset to evaluate the proposed approach in comparison with
well-known baseline and state-of-the-art regression methods. The results
demonstrate that our proposed approach provides superior accuracy and stability
in fertilisation-oriented soil GHG prediction.

</details>


### [104] [TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation](https://arxiv.org/abs/2506.15898)
*Xiao Zhang,Xingyu Zhao,Hong Xia,Yuan Cao,Guiyuan Jiang,Junyu Dong,Yanwei Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为TrajDiff的新框架，用于解决轨迹相似性计算中的语义差距、噪声问题和全局排序信息利用不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着位置跟踪技术的普及，轨迹数据量激增，但现有学习方法在语义对齐、噪声鲁棒性和全局排序信息利用方面存在不足。

Method: TrajDiff框架包含语义对齐模块、基于DDBM的噪声鲁棒预训练和全局排序感知正则化，分别解决语义差距、噪声问题和全局信息利用。

Result: 在三个公开数据集上，TrajDiff显著优于现有基线方法，平均HR@1提升33.38%。

Conclusion: TrajDiff通过多模块协同，有效解决了轨迹相似性计算中的关键问题，性能显著提升。

Abstract: With the proliferation of location-tracking technologies, massive volumes of
trajectory data are continuously being collected. As a fundamental task in
trajectory data mining, trajectory similarity computation plays a critical role
in a wide range of real-world applications. However, existing learning-based
methods face three challenges: First, they ignore the semantic gap between GPS
and grid features in trajectories, making it difficult to obtain meaningful
trajectory embeddings. Second, the noise inherent in the trajectories, as well
as the noise introduced during grid discretization, obscures the true motion
patterns of the trajectories. Third, existing methods focus solely on
point-wise and pair-wise losses, without utilizing the global ranking
information obtained by sorting all trajectories according to their similarity
to a given trajectory. To address the aforementioned challenges, we propose a
novel trajectory similarity computation framework, named TrajDiff.
Specifically, the semantic alignment module relies on cross-attention and an
attention score mask mechanism with adaptive fusion, effectively eliminating
semantic discrepancies between data at two scales and generating a unified
representation. Additionally, the DDBM-based Noise-robust Pre-Training
introduces the transfer patterns between any two trajectories into the model
training process, enhancing the model's noise robustness. Finally, the overall
ranking-aware regularization shifts the model's focus from a local to a global
perspective, enabling it to capture the holistic ordering information among
trajectories. Extensive experiments on three publicly available datasets show
that TrajDiff consistently outperforms state-of-the-art baselines. In
particular, it achieves an average HR@1 gain of 33.38% across all three
evaluation metrics and datasets.

</details>


### [105] [Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach](https://arxiv.org/abs/2506.15901)
*Li Sun,Shuheng Chen,Yong Si,Junyi Fan,Maryam Pishgar,Elham Pishgar,Kamiar Alaei,Greg Placencia*

Main category: cs.LG

TL;DR: 开发了一种可解释的机器学习模型，用于预测ICU中同时患有糖尿病和房颤患者的28天死亡率，逻辑回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 针对糖尿病和房颤患者在ICU中死亡率较高的问题，现有模型有限，需开发更准确的预测工具。

Method: 从MIMIC-IV数据库中提取1,535名患者数据，进行预处理和特征选择，训练7种ML模型，并通过交叉验证和SMOTE过采样优化。

Result: 逻辑回归表现最佳（AUROC: 0.825），关键预测因子包括RAS、年龄、胆红素和拔管。ALE分析揭示了非线性效应。

Conclusion: 该模型为糖尿病和房颤患者的早期ICU分诊提供了准确的风险预测和临床见解。

Abstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation
(AF) face elevated mortality in intensive care units (ICUs), yet models
targeting this high-risk group remain limited.
  Objective: To develop an interpretable machine learning (ML) model predicting
28-day mortality in ICU patients with concurrent DM and AF using early-phase
clinical data.
  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF
was extracted from the MIMIC-IV database. Data preprocessing involved
median/mode imputation, z-score normalization, and early temporal feature
engineering. A two-step feature selection pipeline-univariate filtering (ANOVA
F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable
features. Seven ML models were trained with stratified 5-fold cross-validation
and SMOTE oversampling. Interpretability was assessed via ablation and
Accumulated Local Effects (ALE) analysis.
  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%
CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,
age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects
such as age-related risk acceleration and bilirubin thresholds.
  Conclusion: This interpretable ML model offers accurate risk prediction and
clinical insights for early ICU triage in patients with DM and AF.

</details>


### [106] [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](https://arxiv.org/abs/2506.15903)
*Josef Kuchař,Marek Kadlčík,Michal Spiegel,Michal Štefánik*

Main category: cs.LG

TL;DR: 论文介绍了一个大规模的数据集，用于指令引导的矢量图像编辑，包含27万对SVG图像和自然语言编辑指令。数据集支持基于文本命令的矢量图形修改模型的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 推动基于自然语言的矢量图形生成和编辑研究，提供公开资源以促进该领域的发展。

Method: 通过CLIP相似性配对图像，并利用视觉语言模型生成指令。

Result: 实验表明，当前最先进的大型语言模型在生成准确和有效的编辑方面仍存在困难。

Conclusion: 该数据集为自然语言驱动的矢量图形编辑任务提供了重要资源，并揭示了该任务的挑战性。

Abstract: We introduce a large-scale dataset for instruction-guided vector image
editing, consisting of over 270,000 pairs of SVG images paired with natural
language edit instructions. Our dataset enables training and evaluation of
models that modify vector graphics based on textual commands. We describe the
data collection process, including image pairing via CLIP similarity and
instruction generation with vision-language models. Initial experiments with
state-of-the-art large language models reveal that current methods struggle to
produce accurate and valid edits, underscoring the challenge of this task. To
foster research in natural language-driven vector graphic generation and
editing, we make our resources created within this work publicly available.

</details>


### [107] [Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI](https://arxiv.org/abs/2506.15907)
*Hang Yang,Yusheng Hu,Yong Liu,Cong,Hao*

Main category: cs.LG

TL;DR: Pieceformer是一个可扩展的自监督图相似性评估框架，结合了混合消息传递和图变换器编码器，显著提高了VLSI设计中的知识转移效率。


<details>
  <summary>Details</summary>
Motivation: 在VLSI设计中，准确的图相似性评估对减少工程时间和成本至关重要，但现有方法在可扩展性和准确性上存在不足。

Method: 提出Pieceformer框架，采用线性变换器骨干和分区训练管道，优化内存和并行管理。

Result: 在合成和真实数据集上，Pieceformer将MAE降低24.9%，并能正确聚类所有真实设计组。

Conclusion: Pieceformer为现代VLSI系统提供了高效、可扩展的设计重用解决方案。

Abstract: Accurate graph similarity is critical for knowledge transfer in VLSI design,
enabling the reuse of prior solutions to reduce engineering effort and
turnaround time. We propose Pieceformer, a scalable, self-supervised similarity
assessment framework, equipped with a hybrid message-passing and graph
transformer encoder. To address transformer scalability, we incorporate a
linear transformer backbone and introduce a partitioned training pipeline for
efficient memory and parallelism management. Evaluations on synthetic and
real-world CircuitNet datasets show that Pieceformer reduces mean absolute
error (MAE) by 24.9% over the baseline and is the only method to correctly
cluster all real-world design groups. We further demonstrate the practical
usage of our model through a case study on a partitioning task, achieving up to
89% runtime reduction. These results validate the framework's effectiveness for
scalable, unbiased design reuse in modern VLSI systems.

</details>


### [108] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: 通过时间域信号稀疏化加速神经语音转录，利用Transformer音频编码器的自注意力机制可解释性，在Whisper模型上实现1.6倍加速且准确率下降小于1%。


<details>
  <summary>Details</summary>
Motivation: 语音音频信号高度可压缩，利用Transformer的自注意力机制可解释性，探索在神经编码阶段早期进行时间域信号稀疏化以加速转录。

Method: 在Whisper模型上进行系统架构搜索，联合优化稀疏化阶段（编码层）和压缩比（稀疏度）。

Result: 最佳方案在准确率下降小于1%的情况下，选择在早期编码阶段将隐藏状态稀疏化至40-60%，实现1.6倍运行时加速。

Conclusion: 早期稀疏化可显著加速神经语音转录，同时保持高准确率，适用于实际应用。

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [109] [Competing Bandits in Matching Markets via Super Stability](https://arxiv.org/abs/2506.15926)
*Soumya Basu*

Main category: cs.LG

TL;DR: 研究双边奖励不确定性的匹配市场中的强盗学习，扩展了以往主要关注单边不确定性的研究。通过利用Irving (1994)的“超稳定性”概念，展示了扩展Gale-Shapley (GS)算法在实现不完全信息下真正稳定匹配中的优势。


<details>
  <summary>Details</summary>
Motivation: 扩展对匹配市场中双边奖励不确定性的理解，填补单边不确定性研究的空白。

Method: 采用扩展GS算法，提出一种集中式算法，实现依赖于实例的可容许间隙参数的对数最差稳定遗憾。并进一步将该算法适应于去中心化设置。

Result: 集中式算法实现了对数最差稳定遗憾，去中心化设置中遗憾增加为常数。同时，建立了新的集中式实例依赖性下界，阐明了可容许间隙和超稳定匹配在稳定匹配复杂性中的作用。

Conclusion: 扩展GS算法在双边奖励不确定性下表现优越，为稳定匹配的强盗学习提供了新的理论框架和算法支持。

Abstract: We study bandit learning in matching markets with two-sided reward
uncertainty, extending prior research primarily focused on single-sided
uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we
demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the
standard GS algorithm in achieving true stable matchings under incomplete
information. By employing the Extended GS algorithm, our centralized algorithm
attains a logarithmic pessimal stable regret dependent on an instance-dependent
admissible gap parameter. This algorithm is further adapted to a decentralized
setting with a constant regret increase. Finally, we establish a novel
centralized instance-dependent lower bound for binary stable regret,
elucidating the roles of the admissible gap and super-stable matching in
characterizing the complexity of stable matching with bandit feedback.

</details>


### [110] [CORAL: Disentangling Latent Representations in Long-Tailed Diffusion](https://arxiv.org/abs/2506.15933)
*Esther Rodriguez,Monica Welfert,Samuel McDowell,Nathan Stromberg,Julian Antolin Camarena,Lalitha Sankar*

Main category: cs.LG

TL;DR: 论文研究了扩散模型在长尾分布数据上的表现，发现尾类样本生成质量低的原因是潜在表征重叠，并提出了一种对比正则化方法（CORAL）来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据通常呈现长尾分布，而标准扩散模型在这种分布下对尾类的生成效果较差，其潜在原因尚不明确。

Method: 通过分析扩散模型在长尾数据集上的行为，发现潜在表征重叠是问题根源，并提出CORAL框架，利用对比损失来分离潜在表征。

Result: 实验表明，CORAL显著提高了尾类样本的多样性和视觉质量。

Conclusion: CORAL通过对比正则化有效改善了扩散模型在长尾分布数据上的生成性能。

Abstract: Diffusion models have achieved impressive performance in generating
high-quality and diverse synthetic data. However, their success typically
assumes a class-balanced training distribution. In real-world settings,
multi-class data often follow a long-tailed distribution, where standard
diffusion models struggle -- producing low-diversity and lower-quality samples
for tail classes. While this degradation is well-documented, its underlying
cause remains poorly understood. In this work, we investigate the behavior of
diffusion models trained on long-tailed datasets and identify a key issue: the
latent representations (from the bottleneck layer of the U-Net) for tail class
subspaces exhibit significant overlap with those of head classes, leading to
feature borrowing and poor generation quality. Importantly, we show that this
is not merely due to limited data per class, but that the relative class
imbalance significantly contributes to this phenomenon. To address this, we
propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive
latent alignment framework that leverages supervised contrastive losses to
encourage well-separated latent class representations. Experiments demonstrate
that CORAL significantly improves both the diversity and visual quality of
samples generated for tail classes relative to state-of-the-art methods.

</details>


### [111] [On the optimal regret of collaborative personalized linear bandits](https://arxiv.org/abs/2506.15943)
*Bruce Huang,Ruida Zhou,Lin F. Yang,Suhas Diggavi*

Main category: cs.LG

TL;DR: 论文研究了多代理协作个性化线性赌博问题，提出了信息论下界和两阶段协作算法，实现了最优遗憾。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多代理解决异构赌博问题时，单代理算法忽略了跨代理相似性和学习机会，因此需要研究协作的优化效果。

Method: 提出两阶段协作算法，利用分层贝叶斯框架建模异构性，并引入信息论技术限制遗憾。

Result: 算法在三种不同轮次范围内实现了最优遗憾界，优于非协作代理的遗憾界。

Conclusion: 协作在多代理线性赌博问题中能显著降低遗憾，尤其是在异构性较低时效果更明显。

Abstract: Stochastic linear bandits are a fundamental model for sequential decision
making, where an agent selects a vector-valued action and receives a noisy
reward with expected value given by an unknown linear function. Although well
studied in the single-agent setting, many real-world scenarios involve multiple
agents solving heterogeneous bandit problems, each with a different unknown
parameter. Applying single agent algorithms independently ignores cross-agent
similarity and learning opportunities. This paper investigates the optimal
regret achievable in collaborative personalized linear bandits. We provide an
information-theoretic lower bound that characterizes how the number of agents,
the interaction rounds, and the degree of heterogeneity jointly affect regret.
We then propose a new two-stage collaborative algorithm that achieves the
optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian
framework and introduces a novel information-theoretic technique for bounding
regret. Our results offer a complete characterization of when and how
collaboration helps with a optimal regret bound $\tilde{O}(d\sqrt{mn})$,
$\tilde{O}(dm^{1-\gamma}\sqrt{n})$, $\tilde{O}(dm\sqrt{n})$ for the number of
rounds $n$ in the range of $(0, \frac{d}{m \sigma^2})$, $[\frac{d}{m^{2\gamma}
\sigma^2}, \frac{d}{\sigma^2}]$ and $(\frac{d}{\sigma^2}, \infty)$
respectively, where $\sigma$ measures the level of heterogeneity, $m$ is the
number of agents, and $\gamma\in[0, 1/2]$ is an absolute constant. In contrast,
agents without collaboration achieve a regret bound $O(dm\sqrt{n})$ at best.

</details>


### [112] [One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks](https://arxiv.org/abs/2506.15954)
*Vinicius Yuiti Fukase,Heitor Gama,Barbara Bueno,Lucas Libanio,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文提出了一种系统方法，用于识别深度神经网络训练中的关键学习期，通过减少计算成本和资源消耗，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究确认了关键学习期的存在，但缺乏精确识别其发生时间的方法。本文旨在填补这一空白。

Method: 利用泛化预测机制识别关键期，并在此后停止资源密集型训练方法，以减少计算成本和排放。

Result: 实验证明，该方法将流行架构的训练时间减少59.67%，CO$_2$排放减少59.47%，成本降低60%，且不影响性能。

Conclusion: 该方法为深度学习提供了更可持续和高效的实践路径，特别适用于资源受限环境。

Abstract: Critical Learning Periods comprehend an important phenomenon involving deep
learning, where early epochs play a decisive role in the success of many
training recipes, such as data augmentation. Existing works confirm the
existence of this phenomenon and provide useful insights. However, the
literature lacks efforts to precisely identify when critical periods occur. In
this work, we fill this gap by introducing a systematic approach for
identifying critical periods during the training of deep neural networks,
focusing on eliminating computationally intensive regularization techniques and
effectively applying mechanisms for reducing computational costs, such as data
pruning. Our method leverages generalization prediction mechanisms to pinpoint
critical phases where training recipes yield maximum benefits to the predictive
ability of models. By halting resource-intensive recipes beyond these periods,
we significantly accelerate the learning phase and achieve reductions in
training time, energy consumption, and CO$_2$ emissions. Experiments on
standard architectures and benchmarks confirm the effectiveness of our method.
Specifically, we achieve significant milestones by reducing the training time
of popular architectures by up to 59.67%, leading to a 59.47% decrease in
CO$_2$ emissions and a 60% reduction in financial costs, without compromising
performance. Our work enhances understanding of training dynamics and paves the
way for more sustainable and efficient deep learning practices, particularly in
resource-constrained environments. In the era of the race for foundation
models, we believe our method emerges as a valuable framework. The repository
is available at https://github.com/baunilhamarga/critical-periods

</details>


### [113] [On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond](https://arxiv.org/abs/2506.15963)
*Jingyi Cui,Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 论文提出了稀疏自编码器（SAE）在恢复单语义特征时的必要和充分条件，并提出了一种加权策略以提高可识别性。


<details>
  <summary>Details</summary>
Motivation: 尽管SAE在解释大型语言模型特征方面应用广泛，但其在何种条件下能完全恢复单语义特征尚不明确。

Method: 通过理论分析提出SAE可识别的条件，并设计了一种加权策略以优化特征重建。

Result: 实验验证了理论条件，加权SAE显著提高了特征的单语义性和可解释性。

Conclusion: 论文为SAE的可识别性提供了理论基础，并通过加权策略改进了特征重建效果。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
features learned by large language models (LLMs). It aims to recover complex
superposed polysemantic features into interpretable monosemantic ones through
feature reconstruction via sparsely activated neural networks. Despite the wide
applications of SAEs, it remains unclear under what conditions an SAE can fully
recover the ground truth monosemantic features from the superposed polysemantic
ones. In this paper, through theoretical analysis, we for the first time
propose the necessary and sufficient conditions for identifiable SAEs (SAEs
that learn unique and ground truth monosemantic features), including 1) extreme
sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)
enough hidden dimensions of SAEs. Moreover, when the identifiable conditions
are not fully met, we propose a reweighting strategy to improve the
identifiability. Specifically, following the theoretically suggested weight
selection principle, we prove that the gap between the loss functions of SAE
reconstruction and monosemantic feature reconstruction can be narrowed, so that
the reweighted SAEs have better reconstruction of the ground truth monosemantic
features than the uniformly weighted ones. In experiments, we validate our
theoretical findings and show that our weighted SAE significantly improves
feature monosemanticity and interpretability.

</details>


### [114] [LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning](https://arxiv.org/abs/2506.15969)
*Haoyue Zhang,Hualei Zhang,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 论文提出了一种名为LazyEviction的延迟KV缓存淘汰框架，通过捕捉令牌重要性重现现象，显著减少GPU内存占用，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在长推理任务中表现不佳，未能捕捉令牌重要性重现现象，导致关键令牌被不可预测地淘汰。

Method: 提出LazyEviction框架，包含重现间隔追踪和基于最大重现间隔的淘汰策略，以保留周期性关键令牌。

Result: 实验表明，LazyEviction将KV缓存大小减少50%，同时在数学推理任务中保持准确性，优于现有方法。

Conclusion: 研究发现保留重现令牌对多步推理任务中的知识连续性至关重要，LazyEviction为此提供了有效解决方案。

Abstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by
employing Chain-of-Thought (CoT). However, the extended reasoning sequences
introduce significant GPU memory overhead due to increased key-value (KV) cache
size, particularly in tasks requiring long reasoning sequences, such as
mathematics and programming. Existing KV cache compression methods mitigate
memory bottlenecks but struggle in long reasoning tasks. In this paper, we
analyze attention patterns in reasoning tasks and reveal a Token Importance
Recurrence phenomenon: a large proportion of tokens receive renewed attention
after multiple decoding steps, which is failed to capture by existing works and
may lead to unpredictable eviction on such periodically critical tokens. To
address this, we propose LazyEviction, a lagged KV eviction framework designed
to maintain reasoning performance while reducing KV memory. LazyEviction is an
Observation Window-based Lagged Eviction Mechanism retaining latent recurring
tokens by performing lagged evictions across decoding steps, which contains two
key components: (1) Recurrence Interval Tracking for capturing temporal
variations in token importance, and (2) an Maximum Recurrence Interval-Centric
Eviction Policy that prioritizes eviction based on tokens' recurrence patterns.
Extensive experiments demonstrate that LazyEviction reduces KV cache size by
50% while maintaining comparable accuracy on mathematics reasoning datasets,
outperforming state-of-the-art methods. Our findings highlight the importance
of preserving recurring tokens, which are critical for maintaining knowledge
continuity in multi-step reasoning tasks.

</details>


### [115] [AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction](https://arxiv.org/abs/2506.16001)
*Qianru Zhang,Honggang Wen,Ming Li,Dong Huang,Siu-Ming Yiu,Christian S. Jensen,Pietro Liò*

Main category: cs.LG

TL;DR: AutoHFormer是一种层次自回归Transformer，通过分层时间建模、动态窗口注意力和自适应时间编码，解决了时间序列预测中的三个关键问题：时间因果性、计算复杂性和多尺度模式识别。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要同时满足严格的时间因果性、次二次计算复杂性和多尺度模式识别，现有方法难以兼顾这些目标。

Method: 1) 分层时间建模：将预测分解为并行处理的段级块，再进行段内顺序细化；2) 动态窗口注意力：使用可学习的因果窗口和指数衰减；3) 自适应时间编码：结合固定振荡模式和可学习衰减率。

Result: AutoHFormer在PEMS08上比PatchTST快10.76倍，内存减少6.06倍，同时在96-720步范围内保持准确性。

Conclusion: AutoHFormer为高效精确的时间序列建模设立了新基准。

Abstract: Time series forecasting requires architectures that simultaneously achieve
three competing objectives: (1) strict temporal causality for reliable
predictions, (2) sub-quadratic complexity for practical scalability, and (3)
multi-scale pattern recognition for accurate long-horizon forecasting. We
introduce AutoHFormer, a hierarchical autoregressive transformer that addresses
these challenges through three key innovations: 1) Hierarchical Temporal
Modeling: Our architecture decomposes predictions into segment-level blocks
processed in parallel, followed by intra-segment sequential refinement. This
dual-scale approach maintains temporal coherence while enabling efficient
computation. 2) Dynamic Windowed Attention: The attention mechanism employs
learnable causal windows with exponential decay, reducing complexity while
preserving precise temporal relationships. This design avoids both the
anti-causal violations of standard transformers and the sequential bottlenecks
of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system
is adopted to capture time patterns at multiple scales. It combines fixed
oscillating patterns for short-term variations with learnable decay rates for
long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X
faster training and 6.06X memory reduction compared to PatchTST on PEMS08,
while maintaining consistent accuracy across 96-720 step horizons in most of
cases. These breakthroughs establish new benchmarks for efficient and precise
time series modeling. Implementations of our method and all baselines in
hierarchical autoregressive mechanism are available at
https://github.com/lizzyhku/Autotime.

</details>


### [116] [Bridging Brain with Foundation Models through Self-Supervised Learning](https://arxiv.org/abs/2506.16009)
*Hamdi Altaheri,Fakhri Karray,Md. Milon Islam,S M Taslim Uddin Raju,Amir-Hossein Karimi*

Main category: cs.LG

TL;DR: 综述探讨了自监督学习（SSL）如何通过基础模型（FMs）革新脑信号分析，克服传统监督学习的限制。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在脑信号分析中受限于标记数据的稀缺性，而SSL能从无标记数据中学习有效表征，解决脑信号的高噪声和低信噪比等挑战。

Method: 系统回顾了SSL技术在脑信号分析中的应用，包括关键SSL方法、脑特异性基础模型的开发、下游任务适配及多模态SSL框架的整合。

Result: 综述总结了常用评估指标和基准数据集，支持比较分析，并展示了SSL在脑信号分析中的潜力。

Conclusion: 该领域仍面临挑战，但SSL为基础模型在脑信号分析中的通用性提供了研究方向，为未来研究提供了路线图。

Abstract: Foundation models (FMs), powered by self-supervised learning (SSL), have
redefined the capabilities of artificial intelligence, demonstrating
exceptional performance in domains like natural language processing and
computer vision. These advances present a transformative opportunity for brain
signal analysis. Unlike traditional supervised learning, which is limited by
the scarcity of labeled neural data, SSL offers a promising solution by
enabling models to learn meaningful representations from unlabeled data. This
is particularly valuable in addressing the unique challenges of brain signals,
including high noise levels, inter-subject variability, and low signal-to-noise
ratios. This survey systematically reviews the emerging field of bridging brain
signals with foundation models through the innovative application of SSL. It
explores key SSL techniques, the development of brain-specific foundation
models, their adaptation to downstream tasks, and the integration of brain
signals with other modalities in multimodal SSL frameworks. The review also
covers commonly used evaluation metrics and benchmark datasets that support
comparative analysis. Finally, it highlights key challenges and outlines future
research directions. This work aims to provide researchers with a structured
understanding of this rapidly evolving field and a roadmap for developing
generalizable brain foundation models powered by self-supervision.

</details>


### [117] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/abs/2506.16014)
*Jina Kim,Youjin Jang,Jeongjin Han*

Main category: cs.LG

TL;DR: VRAIL是一个基于向量化奖励的双层框架，用于强化学习，通过学习可解释的权重表示提升训练稳定性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过可解释的权重表示提升强化学习的训练稳定性和收敛性，同时生成人类可理解的行为。

Method: 分为两个阶段：深度学习阶段拟合状态特征的价值函数，强化学习阶段通过奖励转换进行学习。支持线性和二次模型。

Result: 在Taxi-v3环境中，VRAIL比标准DQN表现更好，无需修改环境即可发现语义明确的子目标。

Conclusion: VRAIL是一个通用的、模型无关的奖励塑造框架，能同时提升学习和可解释性。

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [118] [A Scalable Factorization Approach for High-Order Structured Tensor Recovery](https://arxiv.org/abs/2506.16032)
*Zhen Qin,Michael B. Wakin,Zhihui Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的框架，通过黎曼梯度下降（RGD）优化张量分解问题，证明了在适当初始化下，RGD能以线性速率收敛到真实张量。


<details>
  <summary>Details</summary>
Motivation: 高维张量的参数数量随阶数指数增长，张量分解能显著减少参数，但非凸优化问题带来收敛分析的挑战。

Method: 利用张量分解的规范形式，约束多数因子为正交，并在Stiefel流形上应用RGD进行优化。

Result: 在损失函数的温和条件下，证明了RGD的线性收敛性，且初始化和收敛速率与阶数多项式相关。

Conclusion: 该方法在Tucker和张量列车格式上优于现有结果，为高维张量分解提供了高效解决方案。

Abstract: Tensor decompositions, which represent an $N$-order tensor using
approximately $N$ factors of much smaller dimensions, can significantly reduce
the number of parameters. This is particularly beneficial for high-order
tensors, as the number of entries in a tensor grows exponentially with the
order. Consequently, they are widely used in signal recovery and data analysis
across domains such as signal processing, machine learning, and quantum
physics. A computationally and memory-efficient approach to these problems is
to optimize directly over the factors using local search algorithms such as
gradient descent, a strategy known as the factorization approach in matrix and
tensor optimization. However, the resulting optimization problems are highly
nonconvex due to the multiplicative interactions between factors, posing
significant challenges for convergence analysis and recovery guarantees.
  In this paper, we present a unified framework for the factorization approach
to solving various tensor decomposition problems. Specifically, by leveraging
the canonical form of tensor decompositions--where most factors are constrained
to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient
descent (RGD) to optimize these orthonormal factors on the Stiefel manifold.
Under a mild condition on the loss function, we establish a Riemannian
regularity condition for the factorized objective and prove that RGD converges
to the ground-truth tensor at a linear rate when properly initialized. Notably,
both the initialization requirement and the convergence rate scale polynomially
rather than exponentially with $N$, improving upon existing results for Tucker
and tensor-train format tensors.

</details>


### [119] [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding](https://arxiv.org/abs/2506.16035)
*Vishesh Tripathi,Tanmay Odapally,Indraneel Das,Uday Allu,Biddwan Ahmed*

Main category: cs.LG

TL;DR: 提出了一种基于多模态模型的新型文档分块方法，解决了传统文本分块在处理复杂文档结构时的不足，提升了RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统文本分块方法在处理复杂文档结构（如多页表格、嵌入图表和跨页上下文依赖）时表现不佳，需要一种更高效的方法。

Method: 利用大型多模态模型（LMMs）对PDF文档进行批量处理，保持语义连贯性和结构完整性，支持跨批次上下文保留。

Result: 在精选的PDF文档数据集上验证，新方法在分块质量和下游RAG性能上均有提升，优于传统RAG系统。

Conclusion: 多模态文档分块方法显著提高了复杂文档的处理能力，为RAG系统提供了更准确的输入。

Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.

</details>


### [120] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Main category: cs.LG

TL;DR: 论文提出CRIA框架，通过多视角融合和自适应编码解决EEG数据预训练中的特征提取和信息整合问题。


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法仅依赖单一视角，无法捕捉多视角间的复杂协同作用，限制了表征的表达力和泛化能力。

Method: CRIA采用变长变通道编码统一EEG数据表征，通过交叉注意力机制融合时空谱特征，并结合注意力矩阵掩码和新颖的视角掩码预训练策略。

Result: 在Temple University EEG和CHB-MIT数据集上，CRIA在相同预训练条件下优于现有方法，多类事件分类和异常检测的平衡准确率分别为57.02%和80.03%。

Conclusion: CRIA展现了强大的泛化能力，为EEG表征学习提供了有效的预训练框架。

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [121] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Main category: cs.LG

TL;DR: 本文提出了首个适用于浮点神经网络的区间通用逼近定理（IUA），证明其在浮点设置下仍能完美逼近目标函数的直接图像映射。


<details>
  <summary>Details</summary>
Motivation: 经典UA定理和IUA定理基于无限精度实数假设，而实际神经网络在浮点数下运行，因此需要验证IUA定理在浮点设置下的有效性。

Method: 通过理论分析，研究浮点神经网络在区间逼近上的能力，并与实数设置下的IUA定理进行对比。

Result: 证明了浮点神经网络在浮点设置下仍具有强大的逼近能力，并得出两个重要推论：存在可证明鲁棒的浮点神经网络，以及浮点加法和乘法的计算完备性。

Conclusion: 浮点神经网络的IUA定理揭示了其在浮点设置下的表达能力，为理论和实践提供了新的见解。

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [122] [A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems](https://arxiv.org/abs/2506.16072)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: 论文提出了一种结合随机WMMSE算法和强化学习驱动的深度展开网络（RLDDU-Net）的方法，用于优化大规模MU-MIMO-OFDM系统中的加权和速率性能，并在不完美CSI下实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统WMMSE预编码在大规模MU-MIMO-OFDM系统中因假设完美CSI和高计算复杂度而难以实际应用。

Method: 开发了宽频随机WMMSE（SWMMSE）算法，并基于此提出RLDDU-Net，将SWMMSE迭代映射到网络层，利用波束域稀疏性和频域子载波相关性加速收敛。

Result: 仿真结果表明，RLDDU-Net在不完美CSI下优于现有基线，具有更高的加权和速率性能和计算效率。

Conclusion: RLDDU-Net为大规模MU-MIMO-OFDM系统提供了一种高效且性能优越的解决方案。

Abstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for
its near-optimal weighted sum rate performance. However, its practical
deployment in massive multi-user (MU) multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) systems is hindered by the
assumption of perfect channel state information (CSI) and high computational
complexity. To address these issues, we first develop a wideband stochastic
WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted
sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight
reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),
where each SWMMSE iteration is mapped to a network layer. Specifically, its DU
module integrates approximation techniques and leverages beam-domain sparsity
as well as frequency-domain subcarrier correlation, significantly accelerating
convergence and reducing computational overhead. Furthermore, the RL module
adaptively adjusts the network depth and generates compensation matrices to
mitigate approximation errors. Simulation results under imperfect CSI
demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance
while offering superior computational and convergence efficiency.

</details>


### [123] [Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach](https://arxiv.org/abs/2506.16074)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于约束强化学习（CRL）的算法CAAC，用于动态优化6G网络中的用户优先级和功率分配，以提高能效和满足QoS需求。


<details>
  <summary>Details</summary>
Motivation: 传统WMMSE预编码方法在适应动态用户需求和时变信道条件方面缺乏灵活性，需要一种更智能的解决方案。

Method: 提出CAAC算法，结合CSSCA方法和注意力增强的Q网络，动态优化策略以满足能效目标和QoS约束。

Result: 仿真结果表明，CAAC在能效和QoS满足方面优于现有方法。

Conclusion: CAAC为6G网络中的动态资源分配提供了一种高效且灵活的解决方案。

Abstract: 6G wireless networks are expected to support diverse quality-of-service (QoS)
demands while maintaining high energy efficiency. Weighted Minimum Mean Square
Error (WMMSE) precoding with fixed user priorities and transmit power is widely
recognized for enhancing overall system performance but lacks flexibility to
adapt to user-specific QoS requirements and time-varying channel conditions. To
address this, we propose a novel constrained reinforcement learning (CRL)
algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy
network to dynamically allocate user priorities and power for WMMSE precoding.
Specifically, CAAC integrates a Constrained Stochastic Successive Convex
Approximation (CSSCA) method to optimize the policy, enabling more effective
handling of energy efficiency goals and satisfaction of stochastic non-convex
QoS constraints compared to traditional and existing CRL methods. Moreover,
CAAC employs lightweight attention-enhanced Q-networks to evaluate policy
updates without prior environment model knowledge. The network architecture not
only enhances representational capacity but also boosts learning efficiency.
Simulation results show that CAAC outperforms baselines in both energy
efficiency and QoS satisfaction.

</details>


### [124] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: 论文探讨了现有安全对齐方法的浅层性，提出了一种探测潜在扰动的方法（ASA）和改进对齐鲁棒性的训练策略（LAPT）。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法仅关注表面拒绝行为，未充分改变内部表征，导致微小潜在偏移仍可能触发不安全响应。

Method: 引入基于负对数似然的探测方法量化潜在空间敏感性，并提出层对抗补丁训练（LAPT）改进对齐鲁棒性。

Result: 实验表明LAPT能增强对齐鲁棒性且不影响模型通用能力。

Conclusion: 当前对齐范式存在根本缺陷，需转向表征级训练策略。

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [125] [A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders](https://arxiv.org/abs/2506.16096)
*Qianqian Liao,Wuque Cai,Hongze Sun,Dongze Liu,Duo Chen,Dezhong Yao,Daqing Guo*

Main category: cs.LG

TL;DR: 提出了一种两阶段脑图学习框架（B2P-GL），结合脑区语义相似性和基于条件的群体图建模，提升脑障碍诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的脑障碍诊断方法依赖预定义脑图谱，但忽略了图谱中的丰富信息及站点和表型变异的混杂效应。

Method: 第一阶段通过GPT-4知识丰富脑图表示，使用自适应节点重分配图注意力网络；第二阶段结合表型数据构建群体图并融合特征。

Result: 在ABIDE I、ADHD-200和Rest-meta-MDD数据集上，B2P-GL在预测准确性和可解释性上优于现有方法。

Conclusion: B2P-GL为脑障碍诊断提供了可靠且个性化的方法，提升了临床适用性。

Abstract: Recent developed graph-based methods for diagnosing brain disorders using
functional connectivity highly rely on predefined brain atlases, but overlook
the rich information embedded within atlases and the confounding effects of
site and phenotype variability. To address these challenges, we propose a
two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates
the semantic similarity of brain regions and condition-based population graph
modeling. In the first stage, termed brain representation learning, we leverage
brain atlas knowledge from GPT-4 to enrich the graph representation and refine
the brain graph through an adaptive node reassignment graph attention network.
In the second stage, termed population disorder diagnosis, phenotypic data is
incorporated into population graph construction and feature fusion to mitigate
confounding effects and enhance diagnosis performance. Experiments on the ABIDE
I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms
state-of-the-art methods in prediction accuracy while enhancing
interpretability. Overall, our proposed framework offers a reliable and
personalized approach to brain disorder diagnosis, advancing clinical
applicability.

</details>


### [126] [Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification](https://arxiv.org/abs/2506.16110)
*Langzhang Liang,Fanchen Bu,Zixing Song,Zenglin Xu,Shirui Pan,Kijung Shin*

Main category: cs.LG

TL;DR: 提出了一种基于谱保留的图稀疏化方法，用于缓解图神经网络中的过度挤压问题，同时保持图的稀疏性和谱特性。


<details>
  <summary>Details</summary>
Motivation: 现有图重布线方法常忽略保留图的谱特性，且通过增加边数改善连通性会带来计算开销和过度平滑风险。

Method: 利用谱保留的图稀疏化技术，生成具有增强连通性但仍保持稀疏性和原始图谱的图。

Result: 实验证明该方法在分类准确性和拉普拉斯谱保留方面优于基线方法。

Conclusion: 该方法有效平衡了结构瓶颈减少和图特性保留，优于现有技术。

Abstract: The message-passing paradigm of Graph Neural Networks often struggles with
exchanging information across distant nodes typically due to structural
bottlenecks in certain graph regions, a limitation known as
\textit{over-squashing}. To reduce such bottlenecks, \textit{graph rewiring},
which modifies graph topology, has been widely used. However, existing graph
rewiring techniques often overlook the need to preserve critical properties of
the original graph, e.g., \textit{spectral properties}. Moreover, many
approaches rely on increasing edge count to improve connectivity, which
introduces significant computational overhead and exacerbates the risk of
over-smoothing. In this paper, we propose a novel graph rewiring method that
leverages \textit{spectrum-preserving} graph \textit{sparsification}, for
mitigating over-squashing. Our method generates graphs with enhanced
connectivity while maintaining sparsity and largely preserving the original
graph spectrum, effectively balancing structural bottleneck reduction and graph
property preservation. Experimental results validate the effectiveness of our
approach, demonstrating its superiority over strong baseline methods in
classification accuracy and retention of the Laplacian spectrum.

</details>


### [127] [From Teacher to Student: Tracking Memorization Through Model Distillation](https://arxiv.org/abs/2506.16170)
*Simardeep Singh*

Main category: cs.LG

TL;DR: 研究表明，通过知识蒸馏（KD）将大型教师模型蒸馏为小型学生模型，不仅能降低计算成本和模型大小，还能显著减少记忆风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）会记忆训练数据，引发隐私和安全问题。此前研究主要集中在预训练模型的记忆行为，但知识蒸馏对记忆的影响尚不清楚。

Method: 研究探索了不同知识蒸馏方法对微调任务数据记忆的影响，比较了标准微调与蒸馏方法的差异。

Result: 蒸馏方法显著降低了模型对训练数据的记忆风险，同时减少了计算成本和模型大小。

Conclusion: 知识蒸馏是一种有效降低记忆风险的方法，适用于隐私敏感场景。

Abstract: Large language models (LLMs) are known to memorize parts of their training
data, raising important concerns around privacy and security. While previous
research has focused on studying memorization in pre-trained models, much less
is known about how knowledge distillation (KD) affects memorization.In this
study, we explore how different KD methods influence the memorization of
fine-tuned task data when a large teacher model is distilled into smaller
student variants.This study demonstrates that distilling a larger teacher
model, fine-tuned on a dataset, into a smaller variant not only lowers
computational costs and model size but also significantly reduces the
memorization risks compared to standard fine-tuning approaches.

</details>


### [128] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/abs/2506.16174)
*Ismo Horppu,Frederick Ayala,Erlin Gulbenkoglu*

Main category: cs.LG

TL;DR: 论文探讨了AI在翻译芬兰说唱歌曲时的表现，比较了Faster Whisperer算法和YouTube的语音转文字功能，并以芬兰说唱歌词为参考标准。


<details>
  <summary>Details</summary>
Motivation: 芬兰语复杂且艺术家使用时更难以理解，因此测试AI在翻译芬兰说唱歌曲时的能力具有挑战性和趣味性。

Method: 使用Faster Whisperer算法和YouTube的语音转文字功能翻译芬兰说唱歌曲，并与原始歌词对比。

Result: 通过比较AI转录的错误率来衡量其表现。

Conclusion: 该研究为AI在复杂语言和艺术表达中的语音识别能力提供了有趣的案例。

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [129] [Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs](https://arxiv.org/abs/2506.16196)
*Xun Wang,Jing Xu,Franziska Boenisch,Michael Backes,Christopher A. Choquette-Choo,Adam Dziedzic*

Main category: cs.LG

TL;DR: POST框架通过在小模型上私有调优软提示并转移到大模型，解决了计算成本和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 软提示与大模型紧密耦合，导致计算成本高且隐私风险大。

Method: POST通过知识蒸馏从小模型调优软提示，可选差分隐私，再用公共数据集转移回大模型。

Result: POST降低了计算成本，保护了隐私，并成功转移了高效软提示。

Conclusion: POST为软提示调优提供了一种高效且隐私安全的解决方案。

Abstract: Prompting has become a dominant paradigm for adapting large language models
(LLMs). While discrete (textual) prompts are widely used for their
interpretability, soft (parameter) prompts have recently gained traction in
APIs. This is because they can encode information from more training samples
while minimizing the user's token usage, leaving more space in the context
window for task-specific input. However, soft prompts are tightly coupled to
the LLM they are tuned on, limiting their generalization to other LLMs. This
constraint is particularly problematic for efficiency and privacy: (1) tuning
prompts on each LLM incurs high computational costs, especially as LLMs
continue to grow in size. Additionally, (2) when the LLM is hosted externally,
soft prompt tuning often requires sharing private data with the LLM provider.
For instance, this is the case with the NVIDIA NeMo API. To address these
issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that
enables private tuning of soft prompts on a small model and subsequently
transfers these prompts to a larger LLM. POST uses knowledge distillation to
derive a small model directly from the large LLM to improve prompt
transferability, tunes the soft prompt locally, optionally with differential
privacy guarantees, and transfers it back to the larger LLM using a small
public dataset. Our experiments show that POST reduces computational costs,
preserves privacy, and effectively transfers high-utility soft prompts.

</details>


### [130] [From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management](https://arxiv.org/abs/2506.16216)
*Charbel Bou Chaaya,Abanoub M. Girgis,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出了一种联合建模控制动态和无线传播环境的机器学习方法，通过耦合的JEPA网络和强化学习优化无线电资源管理，减少50%以上发射功率。


<details>
  <summary>Details</summary>
Motivation: 优化远程控制器与设备间的无线电资源管理，同时不牺牲控制任务的性能。

Method: 使用两个耦合的JEPA网络分别建模控制动态和无线传播环境，结合强化学习生成控制策略和功率预测器。

Result: 仿真结果显示，发射功率减少50%以上，且控制性能与基线方法相当。

Conclusion: 该方法有效优化了无线电资源使用，同时保持了控制性能。

Abstract: In this work, we aim to optimize the radio resource management of a
communication system between a remote controller and its device, whose state is
represented through image frames, without compromising the performance of the
control task. We propose a novel machine learning (ML) technique to jointly
model and predict the dynamics of the control system as well as the wireless
propagation environment in latent space. Our method leverages two coupled
joint-embedding predictive architectures (JEPAs): a control JEPA models the
control dynamics and guides the predictions of a wireless JEPA, which captures
the dynamics of the device's channel state information (CSI) through
cross-modal conditioning. We then train a deep reinforcement learning (RL)
algorithm to derive a control policy from latent control dynamics and a power
predictor to estimate scheduling intervals with favorable channel conditions
based on latent CSI representations. As such, the controller minimizes the
usage of radio resources by utilizing the coupled JEPA networks to imagine the
device's trajectory in latent space. We present simulation results on synthetic
multimodal data and show that our proposed approach reduces transmit power by
over 50% while maintaining control performance comparable to baseline methods
that do not account for wireless optimization.

</details>


### [131] [Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data](https://arxiv.org/abs/2506.16234)
*Prakhar Verma,David Arbour,Sunav Choudhary,Harshita Chopra,Arno Solin,Atanu R. Sinha*

Main category: cs.LG

TL;DR: BLANCE是一个混合贝叶斯框架，通过自适应整合序列批次数据和语言模型（LM）提供的噪声专家知识，解决因果发现中的数据批次和专家知识稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 实践中，数据通常分批到达且专家知识稀缺，而语言模型虽可作为替代但存在幻觉、不一致性和偏见问题。

Method: BLANCE采用从有向无环图（DAG）到部分祖先图（PAG）的表示转换，结合贝叶斯框架自适应整合数据和LM知识，并通过序列优化方案指导LM交互。

Result: BLANCE在结构准确性和贝叶斯参数估计方面优于现有方法，对LM噪声具有鲁棒性。

Conclusion: BLANCE通过结合数据和LM知识，有效解决了因果发现中的挑战，并展示了其在实际应用中的潜力。

Abstract: Causal discovery from observational data typically assumes full access to
data and availability of domain experts. In practice, data often arrive in
batches, and expert knowledge is scarce. Language Models (LMs) offer a
surrogate but come with their own issues-hallucinations, inconsistencies, and
bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid
Bayesian framework that bridges these gaps by adaptively integrating sequential
batch data with LM-derived noisy, expert knowledge while accounting for both
data-induced and LM-induced biases. Our proposed representation shift from
Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates
ambiguities within a coherent Bayesian framework, allowing grounding the global
LM knowledge in local observational data. To guide LM interaction, we use a
sequential optimization scheme that adaptively queries the most informative
edges. Across varied datasets, BLANCE outperforms prior work in structural
accuracy and extends to Bayesian parameter estimation, showing robustness to LM
noise.

</details>


### [132] [Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design](https://arxiv.org/abs/2506.16237)
*Jacopo Iollo,Geoffroy Oudoumanessah,Carole Lartizien,Michel Dojat,Florence Forbes*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯实验设计的MRI加速采集方法，通过自适应选择信息量最大的测量点，平衡采集速度与图像质量。


<details>
  <summary>Details</summary>
Motivation: 在临床MRI中，如何在加速采集时间的同时保持图像质量是关键挑战。

Method: 采用顺序贝叶斯实验设计（BED）和扩散生成模型，通过梯度优化选择子采样模式。

Result: 方法在多种MRI采集任务中表现出高效性和多功能性。

Conclusion: 该方法不仅优化了图像重建，还能适应多种图像分析任务。

Abstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging
(MRI) in clinical settings is to accelerate acquisition times without
significantly degrading image quality. This objective requires a balance
between under-sampling the raw k-space measurements for faster acquisitions and
gathering sufficient raw information for high-fidelity image reconstruction and
analysis tasks. To achieve this balance, we propose to use sequential Bayesian
experimental design (BED) to provide an adaptive and task-dependent selection
of the most informative measurements. Measurements are sequentially augmented
with new samples selected to maximize information gain on a posterior
distribution over target images. Selection is performed via a gradient-based
optimization of a design parameter that defines a subsampling pattern. In this
work, we introduce a new active BED procedure that leverages diffusion-based
generative models to handle the high dimensionality of the images and employs
stochastic optimization to select among a variety of patterns while meeting the
acquisition process constraints and budget. So doing, we show how our setting
can optimize, not only standard image reconstruction, but also any associated
image analysis task. The versatility and performance of our approach are
demonstrated on several MRI acquisitions.

</details>


### [133] [Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping](https://arxiv.org/abs/2506.16243)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.LG

TL;DR: 使用CWGAN生成ALS患者的合成EEG信号，以解决数据稀缺和类别不平衡问题，提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: ALS患者的EEG数据稀缺且类别不平衡，导致机器学习分类器训练困难。

Method: 采用CWGAN模型，基于私人EEG数据集生成合成ALS信号，并进行预处理和归一化。

Result: 生成的合成信号与真实ALS EEG模式相似，训练过程稳定，可用于数据增强。

Conclusion: 该方法有助于数据共享和诊断模型改进，提升ALS检测准确性。

Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and
high-quality EEG data from ALS patients are scarce. This data scarcity, coupled
with severe class imbalance between ALS and healthy control recordings, poses a
challenge for training reliable machine learning classifiers. In this work, we
address these issues by generating synthetic EEG signals for ALS patients using
a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train
CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of
ALS EEG signals and produce realistic synthetic samples. We preprocess and
normalize EEG recordings, and train a CWGAN model to generate synthetic ALS
signals. The CWGAN architecture and training routine are detailed, with key
hyperparameters chosen for stable training. Qualitative evaluation of generated
signals shows that they closely mimic real ALS EEG patterns. The CWGAN training
converged with generator and discriminator loss curves stabilizing, indicating
successful learning. The synthetic EEG signals appear realistic and have
potential use as augmented data for training classifiers, helping to mitigate
class imbalance and improve ALS detection accuracy. We discuss how this
approach can facilitate data sharing and enhance diagnostic models.

</details>


### [134] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Main category: cs.LG

TL;DR: 论文研究了在线博彩问题，通过动态调整赔率最大化利润并减少潜在损失，证明了最优损失是简单多项式的最大根，并开发了高效算法。


<details>
  <summary>Details</summary>
Motivation: 研究在线博彩中庄家如何通过动态调整赔率来平衡公平性和财务风险。

Method: 通过多项式分析和动态规划，结合Bellman-Pareto前沿的显式表征，开发了高效算法。

Result: 庄家可以在避免财务风险的同时实现公平性，最优损失为多项式的最大根。

Conclusion: 论文揭示了庄家后悔与Hermite多项式的关系，并提供了高效算法以应对最优和非最优赌徒的策略。

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [135] [Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective](https://arxiv.org/abs/2506.16288)
*Leo Gagnon,Eric Elmoznino,Sarthak Mittal,Tom Marty,Tejas Kasetty,Dhanya Sridhar,Guillaume Lajoie*

Main category: cs.LG

TL;DR: 论文探讨了自回归基础模型在预测高模糊性上下文时的局限性，并提出了一种基于蒙特卡洛预测的方法来改进性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于发现自回归模型在高模糊性预测中的计算困难，并借鉴认知科学的启发式策略来解决这一问题。

Method: 提出MetaHMM合成序列元学习基准，并设计了一种将预训练模型转化为蒙特卡洛预测器的方法。

Result: 实验表明，蒙特卡洛预测器在高模糊性上下文中显著提升了性能，但仍存在挑战。

Conclusion: 结论指出，通过任务推断与令牌预测的解耦，可以优化模型在高模糊性环境中的表现。

Abstract: The rapid adaptation ability of auto-regressive foundation models is often
attributed to the diversity of their pre-training data. This is because, from a
Bayesian standpoint, minimizing prediction error in such settings requires
integrating over all plausible latent hypotheses consistent with observations.
While this behavior is desirable in principle, it often proves too ambitious in
practice: under high ambiguity, the number of plausible latent alternatives
makes Bayes-optimal prediction computationally intractable. Cognitive science
has long recognized this limitation, suggesting that under such conditions,
heuristics or information-seeking strategies are preferable to exhaustive
inference. Translating this insight to next-token prediction, we hypothesize
that low- and high-ambiguity predictions pose different computational demands,
making ambiguity-agnostic next-token prediction a detrimental inductive bias.
To test this, we introduce MetaHMM, a synthetic sequence meta-learning
benchmark with rich compositional structure and a tractable Bayesian oracle. We
show that Transformers indeed struggle with high-ambiguity predictions across
model sizes. Motivated by cognitive theories, we propose a method to convert
pre-trained models into Monte Carlo predictors that decouple task inference
from token prediction. Preliminary results show substantial gains in ambiguous
contexts through improved capacity allocation and test-time scalable inference,
though challenges remain.

</details>


### [136] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Main category: cs.LG

TL;DR: 该论文提出了一种新的TTS架构，结合多尺度情感建模和动态口音切换，显著提升了印地语和印度英语口音的准确性及情感识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统在多语言环境下（尤其是印度语言）的口音和情感表达存在文化差异问题，需改进。

Method: 扩展Parler-TTS模型，集成语言特定音素对齐混合编码器-解码器架构、文化敏感情感嵌入层及动态口音代码切换。

Result: 口音准确性提升23.7%（WER从15.4%降至11.8%），情感识别准确率达85.3%，MOS为4.2/5。

Conclusion: 该系统通过可扩展的口音-情感解耦，为跨语言合成提供了更可行的解决方案，适用于南亚教育科技和辅助软件。

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


### [137] [Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks](https://arxiv.org/abs/2506.16313)
*Sajan Muhammad,Salem Lahlou*

Main category: cs.LG

TL;DR: 论文提出了一种结合ENN和GFlowNets的新算法ENN-GFN-Enhanced，通过不确定性驱动的探索优化轨迹选择。


<details>
  <summary>Details</summary>
Motivation: GFlowNets中高效识别训练轨迹的问题尚未解决，需在奖励分布未充分学习的区域优先探索。

Method: 将ENN与GFlowNets结合，提升联合预测能力和不确定性量化，改进探索效率。

Result: 在网格环境和结构化序列生成任务中，ENN-GFN-Enhanced表现优于基线方法。

Conclusion: ENN-GFN-Enhanced通过不确定性量化显著提升了GFlowNets的探索效率和轨迹选择能力。

Abstract: Efficiently identifying the right trajectories for training remains an open
problem in GFlowNets. To address this, it is essential to prioritize
exploration in regions of the state space where the reward distribution has not
been sufficiently learned. This calls for uncertainty-driven exploration, in
other words, the agent should be aware of what it does not know. This attribute
can be measured by joint predictions, which are particularly important for
combinatorial and sequential decision problems. In this research, we integrate
epistemic neural networks (ENN) with the conventional architecture of GFlowNets
to enable more efficient joint predictions and better uncertainty
quantification, thereby improving exploration and the identification of optimal
trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the
baseline method in GFlownets and evaluated in grid environments and structured
sequence generation in various settings, demonstrating both its efficacy and
efficiency.

</details>


### [138] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Main category: cs.LG

TL;DR: 论文提出了一种称为“异常签名”的概念，旨在通过突出显示影响决策的特征来提高异常检测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 机器学习常被视为黑箱，难以理解其输出（如决策或评分），天文学家通常需要独立分析数据以理解为何某事件被标记为异常。

Method: 引入“异常签名”的概念，通过突出显示对决策有贡献的特征来帮助解释异常。

Result: 该方法有助于提高异常检测的可解释性。

Conclusion: 异常签名是一种有效的方法，能够帮助用户理解机器学习模型的异常检测结果。

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [139] [Bayesian Optimization over Bounded Domains with the Beta Product Kernel](https://arxiv.org/abs/2506.16316)
*Huy Hoang Nguyen,Han Zhou,Matthew B. Blaschko,Aleksei Tiulpin*

Main category: cs.LG

TL;DR: 论文提出了一种新的Beta核函数，用于在贝叶斯优化中处理有界域函数，优于常用的Matérn和RBF核。


<details>
  <summary>Details</summary>
Motivation: 现有的Matérn和RBF核函数未考虑函数定义域的限制，可能影响其性能。

Method: 引入基于Beta分布密度函数的非平稳Beta核，适用于有界域。

Result: 实验证明Beta核在单位超立方体边界或顶点附近优化时表现优异，且优于其他核函数。

Conclusion: Beta核为有界域函数优化提供了更有效的工具。

Abstract: Bayesian optimization with Gaussian processes (GP) is commonly used to
optimize black-box functions. The Mat\'ern and the Radial Basis Function (RBF)
covariance functions are used frequently, but they do not make any assumptions
about the domain of the function, which may limit their applicability in
bounded domains. To address the limitation, we introduce the Beta kernel, a
non-stationary kernel induced by a product of Beta distribution density
functions. Such a formulation allows our kernel to naturally model functions on
bounded domains. We present statistical evidence supporting the hypothesis that
the kernel exhibits an exponential eigendecay rate, based on empirical analyses
of its spectral properties across different settings. Our experimental results
demonstrate the robustness of the Beta kernel in modeling functions with optima
located near the faces or vertices of the unit hypercube. The experiments show
that our kernel consistently outperforms a wide range of kernels, including the
well-known Mat\'ern and RBF, in different problems, including synthetic
function optimization and the compression of vision and language models.

</details>


### [140] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: 本文提出了一种在自回归图像生成模型中实现令牌级水印的方法，解决了重新标记导致水印丢失的问题，并通过实验验证了其可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型输出的水印技术可用于追踪其来源，但此前未有针对自回归图像生成模型的令牌级水印研究。

Method: 通过调整语言模型水印技术，引入定制化的标记器-解标记器微调过程和水印同步层，以应对重新标记和图像变换的挑战。

Result: 实验表明，该方法能够实现可靠且鲁棒的水印检测，并具有理论支持的p值。

Conclusion: 该方法为自回归图像生成模型的输出水印提供了首个可行的解决方案，具有实际应用潜力。

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [141] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的三步建筑能源管理系统（BEMS），通过聚类、预测和约束策略学习解决可扩展性、适应性和安全性问题，显著降低运营成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 全球能源需求增长和可再生能源整合的复杂性使得建筑成为可持续能源管理的核心，需要一种可扩展、适应性强且安全的解决方案。

Method: 1. 聚类非可转移负载以识别通用模式；2. 集成LSTM预测模块；3. 使用领域知识约束策略确保安全。

Result: 在真实数据上测试，运营成本降低15%，环境性能稳定，并能快速优化新建筑。

Conclusion: 该框架实现了可扩展、稳健且经济高效的建筑能源管理。

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [142] [Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data](https://arxiv.org/abs/2506.16380)
*Druva Dhakshinamoorthy,Avikshit Jha,Sabyasachi Majumdar,Devdulal Ghosh,Ranjita Chakraborty,Hena Ray*

Main category: cs.LG

TL;DR: 提出了一种基于传感器和机器学习的牛行为监测与发情期检测系统，使用低成本蓝牙颈圈和多种机器学习模型，分类准确率达93%，发情检测准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 为精准畜牧业提供低成本、可扩展的解决方案，特别适用于资源有限的环境。

Method: 设计蓝牙颈圈配备加速度计和陀螺仪，采集实时数据并同步至云端；利用CCTV标注行为数据；评估SVM、RF、CNN等模型分类行为；使用LSTM模型检测发情。

Result: 行为分类准确率93%，发情检测准确率96%。

Conclusion: 该系统为精准畜牧业提供了高效且经济可行的监测方案。

Abstract: This paper presents a novel system for monitoring cattle behavior and
detecting estrus (heat) periods using sensor data and machine learning. We
designed and deployed a low-cost Bluetooth-based neck collar equipped with
accelerometer and gyroscope sensors to capture real-time behavioral data from
real cows, which was synced to the cloud. A labeled dataset was created using
synchronized CCTV footage to annotate behaviors such as feeding, rumination,
lying, and others. We evaluated multiple machine learning models -- Support
Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks
(CNN) -- for behavior classification. Additionally, we implemented a Long
Short-Term Memory (LSTM) model for estrus detection using behavioral patterns
and anomaly detection. Our system achieved over 93% behavior classification
accuracy and 96% estrus detection accuracy on a limited test set. The approach
offers a scalable and accessible solution for precision livestock monitoring,
especially in resource-constrained environments.

</details>


### [143] [State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification](https://arxiv.org/abs/2506.16392)
*Gonçalo Granjal Cruz,Balazs Renczes,Mark C Runacres,Jan Decuyper*

Main category: cs.LG

TL;DR: 论文提出了一种名为SS-KAN的模型，通过结合Kolmogorov-Arnold网络和状态空间框架，解决了黑箱系统辨识模型缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 黑箱系统辨识模型虽然准确，但缺乏对系统动态的可解释性。

Method: 提出SS-KAN模型，结合Kolmogorov-Arnold网络和状态空间框架，并通过稀疏正则化和可视化单变量函数增强可解释性。

Result: 在Silverbox和Wiener-Hammerstein基准测试中，SS-KAN在可解释性方面表现优异，但精度略低于最先进的黑箱模型。

Conclusion: SS-KAN是一种平衡精度与可解释性的非线性系统辨识方法，具有潜力。

Abstract: While accurate, black-box system identification models lack interpretability
of the underlying system dynamics. This paper proposes State-Space
Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating
Kolmogorov-Arnold Networks within a state-space framework. The proposed model
is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein
benchmarks. Results show that SS-KAN provides enhanced interpretability due to
sparsity-promoting regularization and the direct visualization of its learned
univariate functions, which reveal system nonlinearities at the cost of
accuracy when compared to state-of-the-art black-box models, highlighting
SS-KAN as a promising approach for interpretable nonlinear system
identification, balancing accuracy and interpretability of nonlinear system
dynamics.

</details>


### [144] [GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396)
*Alexey Zakharov,Shimon Whiteson*

Main category: cs.LG

TL;DR: GoalLadder是一种新方法，利用视觉语言模型（VLMs）从单一语言指令训练强化学习（RL）代理，通过逐步发现任务进展状态并减少噪声反馈的影响，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言可以为强化学习任务提供简洁且人类可理解的规范，但从语言指令中提取奖励仍是一个挑战，尤其是在视觉环境中。现有方法依赖非视觉环境表示或需要大量反馈，且生成的奖励函数噪声较大。

Method: GoalLadder通过查询VLM识别任务进展状态并使用ELO评分系统排名，减少噪声反馈的影响。代理通过最小化与排名最高目标在嵌入空间中的距离来学习，无需大量准确反馈。

Result: GoalLadder在经典控制和机器人操作环境中平均最终成功率约为95%，远优于竞争对手的45%。

Conclusion: GoalLadder通过结合VLM和ELO评分系统，有效解决了从语言指令中提取奖励的挑战，显著提升了RL代理的性能。

Abstract: Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.

</details>


### [145] [Generating Directed Graphs with Dual Attention and Asymmetric Encoding](https://arxiv.org/abs/2506.16404)
*Alba Carballo-Castro,Manuel Madeira,Yiming Qin,Dorina Thanou,Pascal Frossard*

Main category: cs.LG

TL;DR: 论文提出Directo，首个基于离散流匹配框架的有向图生成模型，解决了有向图生成中的依赖空间大和缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 有向图在多个领域有广泛应用，但生成有向图的研究较少，主要受限于依赖空间大和缺乏标准化基准。

Method: 结合了针对不对称关系的编码、双注意力机制和离散生成框架。

Result: 在合成和真实数据集上表现优异，甚至能与特定类别的专用模型竞争。

Conclusion: Directo为有向图生成研究奠定了坚实基础，展示了方法的有效性和通用性。

Abstract: Directed graphs naturally model systems with asymmetric, ordered
relationships, essential to applications in biology, transportation, social
networks, and visual understanding. Generating such graphs enables tasks such
as simulation, data augmentation and novel instance discovery; however,
directed graph generation remains underexplored. We identify two key factors
limiting progress in this direction: first, modeling edge directionality
introduces a substantially larger dependency space, making the underlying
distribution harder to learn; second, the absence of standardized benchmarks
hinders rigorous evaluation. Addressing the former requires more expressive
models that are sensitive to directional topologies. We propose Directo, the
first generative model for directed graphs built upon the discrete flow
matching framework. Our approach combines: (i) principled positional encodings
tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism
capturing both incoming and outgoing dependencies, and (iii) a robust, discrete
generative framework. To support evaluation, we introduce a benchmark suite
covering synthetic and real-world datasets. It shows that our method performs
strongly across diverse settings and even competes with specialized models for
particular classes, such as directed acyclic graphs. Our results highlight the
effectiveness and generality of our approach, establishing a solid foundation
for future research in directed graph generation.

</details>


### [146] [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406)
*Zhiyuan Liang,Dongwen Tang,Yuhao Zhou,Xuanlei Zhao,Mingjia Shi,Wangbo Zhao,Zekai Li,Peihao Wang,Konstantin Schürholt,Damian Borth,Michael M. Bronstein,Yang You,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: DnD是一种无需逐任务训练的PEFT方法，通过提示生成LoRA权重更新，显著降低开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少为每个下游数据集单独优化LLM的成本，提高效率。

Method: 使用提示条件参数生成器，通过文本编码器和超卷积解码器直接生成LoRA矩阵。

Result: 开销降低12,000倍，性能提升30%，且具有跨领域泛化能力。

Conclusion: DnD证明了提示条件参数生成是梯度适应的高效替代方案。

Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank
adaptation (LoRA) reduce the cost of customizing large language models (LLMs),
yet still require a separate optimization run for every downstream dataset. We
introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned
parameter generator that eliminates per-task training by mapping a handful of
unlabeled task prompts directly to LoRA weight updates. A lightweight text
encoder distills each prompt batch into condition embeddings, which are then
transformed by a cascaded hyper-convolutional decoder into the full set of LoRA
matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD
produces task-specific parameters in seconds, yielding i) up to
\textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains
up to \textbf{30\%} in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or
labels. Our results demonstrate that prompt-conditioned parameter generation is
a viable alternative to gradient-based adaptation for rapidly specializing
LLMs. Our project is available at
\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

</details>


### [147] [Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models](https://arxiv.org/abs/2506.16419)
*Daniel Fidel Harvey,George Weale,Berk Yilmaz*

Main category: cs.LG

TL;DR: 论文研究了混合专家（MoE）架构中的路由器设计，比较了六种变体，提出了新的MLP-Hadamard路由器，并在BERT和Qwen1.5-MoE模型中评估了性能。


<details>
  <summary>Details</summary>
Motivation: MoE架构的性能依赖于路由器模块，但不良路由会导致负载不平衡和准确性下降，因此需要优化路由器设计。

Method: 设计了六种路由器变体（Linear、Attention、MLP、Hybrid、Hash和MLP-Hadamard），并在BERT和Qwen1.5-MoE模型中评估其参数效率、推理延迟、路由熵和专家利用率。

Result: 不同路由器各有优劣：Linear速度快，MLP和Attention表现力强，MLP-Hadamard具有结构化稀疏路由能力。

Conclusion: 研究为MoE路由器设计提供了比较分析，并为大规模模型部署中的性能优化提供了见解。

Abstract: Mixture of Experts (MoE) architectures increase large language model
scalability, yet their performance depends on the router module that moves
tokens to specialized experts. Bad routing can load imbalance and reduced
accuracy. This project designed and implemented different router architectures
within Transformer models to fix these limitations. We experimented with six
distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),
Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using
BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference
latency, routing entropy, and expert utilization patterns. Our evaluations
showed distinct trade-offs: Linear routers offer speed, while MLP and Attention
routers provide greater expressiveness. The MLP-Hadamard router shows a unique
capability for structured, sparse routing. We successfully replaced and
fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This
work provides a comparative analysis of MoE router designs and offers insights
into optimizing their performance for efficient and effective large-scale model
deployment.

</details>


### [148] [EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems](https://arxiv.org/abs/2506.16428)
*Dian Meng,Zhiguang Cao,Yaoxin Wu,Yaqing Hou,Hongwei Ge,Qiang Zhang*

Main category: cs.LG

TL;DR: EFormer是一种基于边的Transformer模型，用于解决车辆路径问题（VRP），通过边信息生成节点嵌入，并在编码和解码阶段采用并行策略，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经启发式方法主要依赖节点坐标作为输入，但在实际场景中，基于边的距离等成本指标更为重要。EFormer旨在解决这一局限性。

Method: EFormer采用预编码模块和混合分数注意力机制将边信息转换为临时节点嵌入，并通过并行编码策略（图编码器和节点编码器）处理不同特征空间的信息。解码阶段使用并行上下文嵌入和多查询集成。

Result: 在TSP和CVRP上的实验表明，EFormer在合成数据集和真实世界实例中均优于基线方法，表现出强大的泛化能力。

Conclusion: EFormer的核心设计在解决VRP问题上具有显著效果，验证了其基于边输入的有效性。

Abstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely
on node coordinates as input, which may be less effective in practical
scenarios where real cost metrics-such as edge-based distances-are more
relevant. To address this limitation, we introduce EFormer, an Edge-based
Transformer model that uses edge as the sole input for VRPs. Our approach
employs a precoder module with a mixed-score attention mechanism to convert
edge information into temporary node embeddings. We also present a parallel
encoding strategy characterized by a graph encoder and a node encoder, each
responsible for processing graph and node embeddings in distinct feature
spaces, respectively. This design yields a more comprehensive representation of
the global relationships among edges. In the decoding phase, parallel context
embedding and multi-query integration are used to compute separate attention
mechanisms over the two encoded embeddings, facilitating efficient path
construction. We train EFormer using reinforcement learning in an
autoregressive manner. Extensive experiments on the Traveling Salesman Problem
(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer
outperforms established baselines on synthetic datasets, including large-scale
and diverse distributions. Moreover, EFormer demonstrates strong generalization
on real-world instances from TSPLib and CVRPLib. These findings confirm the
effectiveness of EFormer's core design in solving VRPs.

</details>


### [149] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Main category: cs.LG

TL;DR: 提出了一种基于事件相机的碰撞避免系统，用于空间碎片监测，采用Stack-CNN算法提升信噪比。


<details>
  <summary>Details</summary>
Motivation: 空间碎片对太空活动构成威胁，需开发有效的监测与避免技术。

Method: 利用事件相机和Stack-CNN算法实时检测微弱移动物体。

Result: 在地面测试中，算法显著提升了信噪比，适用于太空成像。

Conclusion: 该系统为空间交通管理和态势感知提供了有前景的解决方案。

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [150] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Main category: cs.LG

TL;DR: PINNs结合XAI中的影响函数，通过针对性重采样提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用XAI中的影响函数优化PINNs的训练数据采样，以提升模型性能。

Method: 应用影响函数对训练数据进行针对性重采样。

Result: 结果显示该方法能有效提升PINNs的预测精度。

Conclusion: 影响函数在PINN训练中的应用具有实际价值，为XAI方法在科学机器学习中的实践提供了范例。

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [151] [Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach](https://arxiv.org/abs/2506.16448)
*Tri Duc Ly,Gia H. Ngo*

Main category: cs.LG

TL;DR: 提出一种基于多尺度卷积神经网络的新方法，用于EEG情绪识别，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 开发适用于真实场景的EEG情绪识别深度学习模型。

Method: 使用多尺度卷积神经网络，结合多种比例系数的特征提取核和新型核（从大脑四个区域学习关键信息）。

Result: 在预测效价、唤醒和支配分数方面，模型性能优于TSception模型。

Conclusion: 多尺度卷积神经网络在EEG情绪识别中表现优异，适用于实际应用。

Abstract: EEG is a non-invasive, safe, and low-risk method to record
electrophysiological signals inside the brain. Especially with recent
technology developments like dry electrodes, consumer-grade EEG devices, and
rapid advances in machine learning, EEG is commonly used as a resource for
automatic emotion recognition. With the aim to develop a deep learning model
that can perform EEG-based emotion recognition in a real-life context, we
propose a novel approach to utilize multi-scale convolutional neural networks
to accomplish such tasks. By implementing feature extraction kernels with many
ratio coefficients as well as a new type of kernel that learns key information
from four separate areas of the brain, our model consistently outperforms the
state-of-the-art TSception model in predicting valence, arousal, and dominance
scores across many performance evaluation metrics.

</details>


### [152] [Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation](https://arxiv.org/abs/2506.16456)
*Jun Qi,Chen-Yu Liu,Sabato Marco Siniscalchi,Chao-Han Huck Yang,Min-Hsiu Hsieh*

Main category: cs.LG

TL;DR: TensorGuide提出了一种新的张量链引导的低秩适应框架，显著提升了LoRA的表达能力和泛化性能，同时保持参数效率。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA独立优化低秩矩阵，限制了其表达能力和泛化性能，而传统的张量链分解方法未能显著改进。

Method: 通过统一的张量链结构生成两个相关的低秩LoRA矩阵，利用受控高斯噪声驱动，形成联合张量链表示。

Result: 在量子点分类和GPT-2微调实验中，TensorGuide优于标准LoRA和TT-LoRA，实现了更高的准确性和可扩展性。

Conclusion: TensorGuide通过改进的优化动态和泛化能力，为参数高效微调提供了更优的解决方案。

Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient
fine-tuning of large-scale neural models. However, standard LoRA independently
optimizes low-rank matrices, which inherently limits its expressivity and
generalization capabilities. While classical tensor-train (TT) decomposition
can be separately employed on individual LoRA matrices, this work demonstrates
that the classical TT-based approach neither significantly improves parameter
efficiency nor achieves substantial performance gains. This paper proposes
TensorGuide, a novel tensor-train-guided adaptation framework to overcome these
limitations. TensorGuide generates two correlated low-rank LoRA matrices
through a unified TT structure driven by controlled Gaussian noise. The
resulting joint TT representation inherently provides structured, low-rank
adaptations, significantly enhancing expressivity, generalization, and
parameter efficiency without increasing the number of trainable parameters.
Theoretically, we justify these improvements through neural tangent kernel
analyses, demonstrating superior optimization dynamics and enhanced
generalization. Extensive experiments on quantum dot classification and GPT-2
fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently
outperforms standard LoRA and TT-LoRA, achieving improved accuracy and
scalability with fewer parameters.

</details>


### [153] [Black-Box Privacy Attacks on Shared Representations in Multitask Learning](https://arxiv.org/abs/2506.16460)
*John Abascal,Nicolás Berrios,Alina Oprea,Jonathan Ullman,Adam Smith,Matthew Jagielski*

Main category: cs.LG

TL;DR: 该论文研究了多任务学习（MTL）中共享表示可能泄露任务敏感信息的问题，提出了一种黑盒任务推断攻击模型，并通过实验和理论分析验证了攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习通过共享表示提高任务性能，但这些表示可能泄露任务隐私。论文旨在揭示共享表示的信息泄露风险。

Method: 提出黑盒任务推断攻击模型，利用任务样本的嵌入向量推断任务是否参与训练，无需影子模型或标记数据。

Result: 实验证明攻击在视觉和语言领域均有效，即使仅使用新样本也能成功推断任务参与训练。

Conclusion: 共享表示存在隐私风险，需设计更安全的MTL方法以抵御此类攻击。

Abstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages
similarities among multiple learning tasks, each with insufficient samples to
train a standalone model, to solve them simultaneously while minimizing data
sharing across users and organizations. MTL typically accomplishes this goal by
learning a shared representation that captures common structure among the tasks
by embedding data from all tasks into a common feature space. Despite being
designed to be the smallest unit of shared information necessary to effectively
learn patterns across multiple tasks, these shared representations can
inadvertently leak sensitive information about the particular tasks they were
trained on.
  In this work, we investigate what information is revealed by the shared
representations through the lens of inference attacks. Towards this, we propose
a novel, black-box task-inference threat model where the adversary, given the
embedding vectors produced by querying the shared representation on samples
from a particular task, aims to determine whether that task was present when
training the shared representation. We develop efficient, purely black-box
attacks on machine learning models that exploit the dependencies between
embeddings from the same task without requiring shadow models or labeled
reference data. We evaluate our attacks across vision and language domains for
multiple use cases of MTL and demonstrate that even with access only to fresh
task samples rather than training data, a black-box adversary can successfully
infer a task's inclusion in training. To complement our experiments, we provide
theoretical analysis of a simplified learning setting and show a strict
separation between adversaries with training samples and fresh samples from the
target task's distribution.

</details>


### [154] [Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities](https://arxiv.org/abs/2506.16471)
*Tara Akhound-Sadegh,Jungyoon Lee,Avishek Joey Bose,Valentin De Bortoli,Arnaud Doucet,Michael M. Bronstein,Dominique Beaini,Siamak Ravanbakhsh,Kirill Neklyudov,Alexander Tong*

Main category: cs.LG

TL;DR: PITA提出了一种新的扩散采样框架，结合退火和扩散平滑技术，首次实现了复杂分子系统的平衡采样。


<details>
  <summary>Details</summary>
Motivation: 高效采样未归一化概率密度是核心挑战，现有扩散采样方法无法处理简单分子系统。

Method: PITA结合Boltzmann分布的退火和扩散平滑，通过逐步训练扩散模型并利用Feynman-Kac PDE与Sequential Monte Carlo实现推理时退火。

Result: PITA首次实现了N体粒子系统、Alanine Dipeptide和三肽的平衡采样，显著减少了能量函数评估次数。

Conclusion: PITA为复杂系统的扩散采样提供了有效解决方案，具有广泛的应用潜力。

Abstract: Sampling efficiently from a target unnormalized probability density remains a
core challenge, with relevance across countless high-impact scientific
applications. A promising approach towards this challenge is the design of
amortized samplers that borrow key ideas, such as probability path design, from
state-of-the-art generative diffusion models. However, all existing
diffusion-based samplers remain unable to draw samples from distributions at
the scale of even simple molecular systems. In this paper, we propose
Progressive Inference-Time Annealing (PITA), a novel framework to learn
diffusion-based samplers that combines two complementary interpolation
techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion
smoothing. PITA trains a sequence of diffusion models from high to low
temperatures by sequentially training each model at progressively higher
temperatures, leveraging engineered easy access to samples of the
temperature-annealed target density. In the subsequent step, PITA enables
simulating the trained diffusion model to procure training samples at a lower
temperature for the next diffusion model through inference-time annealing using
a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA
enables, for the first time, equilibrium sampling of N-body particle systems,
Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically
lower energy function evaluations. Code available at:
https://github.com/taraak/pita

</details>


### [155] [Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias](https://arxiv.org/abs/2506.16494)
*Amir Reza Vazifeh,Jason W. Fleischer*

Main category: cs.LG

TL;DR: 论文提出了一种非线性降维（NLDR）方法，用于自动识别和分类心电图（ECG）信号中的医学相关特征，无需训练或先验信息，显著提高了准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: 手动ECG分析耗时且易错，传统机器学习方法受限于信号变异性和数据偏差，而无监督方法常忽略细微但临床相关的模式。

Method: 采用非线性降维技术（如t-SNE和UMAP），在MIT-BIH数据集上验证其性能。

Result: 在混合人群中区分个体记录的准确率≥90%，在单个患者中区分不同心律失常的中位准确率为98.96%，中位F1分数为91.02%。

Conclusion: NLDR在心脏监测中具有巨大潜力，适用于单导联和12导联ECG，并可扩展至个性化医疗的其他领域。

Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart
activity and are well-established tools for detecting and monitoring
cardiovascular disease. However, manual ECG analysis can be time-consuming and
prone to errors. Machine learning has emerged as a promising approach for
automated heartbeat recognition and classification, but substantial variations
in ECG signals make it challenging to develop generalizable models. ECG signals
can vary widely across individuals and leads, while datasets often follow
different labeling standards and may be biased, all of which greatly hinder
supervised methods. Conventional unsupervised methods, e.g. principal component
analysis, prioritize large (and often obvious) variances in the data and
typically overlook subtle yet clinically relevant patterns. If labels are
missing and/or variations are significant but small, both approaches fail.
Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate
these issues and identify medically relevant features in ECG signals, with no
need for training or prior information. Using the MLII and V1 leads of the
MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor
embedding and uniform manifold approximation and projection can discriminate
individual recordings in mixed populations with >= 90% accuracy and distinguish
different arrhythmias in individual patients with a median accuracy of 98.96%
and a median F1-score of 91.02%. The results show that NLDR holds much promise
for cardiac monitoring, including the limiting cases of single-lead ECG and the
current 12-lead standard of care, and for personalized health care beyond
cardiology.

</details>


### [156] [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500)
*Samir Khaki,Xiuyu Li,Junxian Guo,Ligeng Zhu,Chenfeng Xu,Konstantinos N. Plataniotis,Amir Yazdanbakhsh,Kurt Keutzer,Song Han,Zhijian Liu*

Main category: cs.LG

TL;DR: SparseLoRA是一种通过上下文稀疏性加速LLM微调的方法，显著降低计算成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如QLoRA和DoRA）虽减少可训练参数和内存使用，但未降低计算成本，甚至可能减慢微调速度。

Method: 提出轻量级、无需训练的SVD稀疏性估计器，动态选择稀疏权重子集进行损失和梯度计算，并系统分析层、标记和训练步骤的敏感性。

Result: 实验显示SparseLoRA将计算成本降低2.2倍，实测加速1.6倍，同时在多种下游任务中保持准确性。

Conclusion: SparseLoRA在降低计算成本的同时，不牺牲模型性能，为LLM微调提供高效解决方案。

Abstract: Fine-tuning LLMs is both computationally and memory-intensive. While
parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the
number of trainable parameters and lower memory usage, they do not decrease
computational cost. In some cases, they may even slow down fine-tuning. In this
paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning
through contextual sparsity. We propose a lightweight, training-free SVD
sparsity estimator that dynamically selects a sparse subset of weights for loss
and gradient computation. Also, we systematically analyze and address
sensitivity across layers, tokens, and training steps. Our experimental results
show that SparseLoRA reduces computational cost by up to 2.2 times and a
measured speedup of up to 1.6 times while maintaining accuracy across various
downstream tasks, including commonsense and arithmetic reasoning, code
generation, and instruction following.

</details>


### [157] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出Subspace Boosting方法，通过维持任务向量秩来解决模型合并中的秩崩溃问题，显著提升合并效果。


<details>
  <summary>Details</summary>
Motivation: 随着合并专家模型数量的增加，性能增益递减，任务向量空间出现秩崩溃。

Method: 采用Subspace Boosting方法，基于奇异值分解的任务向量空间维持秩，并使用高阶广义奇异值分解量化任务相似性。

Result: 在视觉基准测试中，Subspace Boosting将合并效果提升超过10%，支持多达20个专家模型。

Conclusion: Subspace Boosting有效解决了模型合并中的秩崩溃问题，并提供了任务相似性的新视角。

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [158] [Robust Reward Modeling via Causal Rubrics](https://arxiv.org/abs/2506.16507)
*Pragya Srivastava,Harman Singh,Rahul Madhavan,Gandharv Patil,Sravanti Addepalli,Arun Suggala,Rengarajan Aravamudhan,Soumya Sharma,Anirban Laha,Aravindan Raghuveer,Karthikeyan Shanmugam,Doina Precup*

Main category: cs.LG

TL;DR: Crome是一个基于因果模型的奖励建模框架，旨在解决奖励模型中的奖励黑客问题，通过因果增强和中性增强提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RMs）在通过人类反馈对齐大型语言模型（LLMs）时容易受到奖励黑客的影响，即模型会错误地将训练数据中的表面或虚假属性（如响应长度或格式）与质量（如事实性、相关性）混淆。

Method: Crome通过两种合成增强方法进行训练：因果增强（针对因果属性）和中性增强（针对虚假属性），且无需事先了解虚假因素。

Result: 在RewardBench上，Crome平均准确率提升5.4%，特定类别提升高达13.2%和7.2%，并在多个基准测试中表现稳健。

Conclusion: Crome显著提升了奖励模型的鲁棒性和准确性，有效缓解了奖励黑客问题。

Abstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.

</details>


### [159] [Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches](https://arxiv.org/abs/2506.16528)
*Bornali Phukon,Xiuwen Zheng,Mark Hasegawa-Johnson*

Main category: cs.LG

TL;DR: 论文提出了一种新的ASR评估指标，结合了NLI分数、语义相似性和语音相似性，以更好地捕捉语音的可理解性，尤其在发音障碍语音中表现优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 传统ASR指标（如WER和CER）无法有效捕捉语音的可理解性，尤其是对于发音障碍语音，语义对齐比精确匹配更重要。

Method: 提出了一种新指标，整合了自然语言推理（NLI）分数、语义相似性和语音相似性。

Result: 新指标在Speech Accessibility Project数据上与人类判断的相关系数达到0.890，优于传统方法。

Conclusion: 强调了在ASR评估中优先考虑可理解性而非基于错误指标的重要性。

Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility,
especially for dysarthric and dysphonic speech, where semantic alignment
matters more than exact word matches. ASR systems struggle with these speech
types, often producing errors like phoneme repetitions and imprecise
consonants, yet the meaning remains clear to human listeners. We identify two
key challenges: (1) Existing metrics do not adequately reflect intelligibility,
and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR
transcripts of dysarthric speech remains underexplored. To address this, we
propose a novel metric integrating Natural Language Inference (NLI) scores,
semantic similarity, and phonetic similarity. Our ASR evaluation metric
achieves a 0.890 correlation with human judgments on Speech Accessibility
Project data, surpassing traditional methods and emphasizing the need to
prioritize intelligibility over error-based measures.

</details>


### [160] [Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU](https://arxiv.org/abs/2506.16548)
*Arjun Dosajh,Mihika Sanghi*

Main category: cs.LG

TL;DR: 本文提出了一种自适应表示误导遗忘（RMU）技术，用于从大语言模型（LLMs）中删除敏感信息，以解决隐私和版权问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理中表现出色，但其记忆训练数据的特性可能引发隐私、版权和安全问题，尤其是涉及个人身份信息（PII）时。

Method: 采用自适应表示误导遗忘（RMU）技术，通过实验分析不同解码层对敏感信息删除的效果。

Result: 该技术在1B和7B参数模型的官方排行榜上均排名第4。

Conclusion: RMU技术为LLMs中的敏感信息删除提供了一种有效方法，但仍需进一步研究。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, their tendency to
memorize training data raises concerns regarding privacy, copyright compliance,
and security, particularly in cases involving Personally Identifiable
Information (PII). Effective machine unlearning techniques are essential to
mitigate these risks, yet existing methods remain underdeveloped for LLMs due
to their open-ended output space. In this work, we apply the Adaptive
Representation Misdirection Unlearning (RMU) technique to unlearn sensitive
information from LLMs. Through extensive experiments, we analyze the effects of
unlearning across different decoder layers to determine the most effective
regions for sensitive information removal. Our technique ranked 4th on the
official leaderboard of both 1B parameter and 7B parameter models.

</details>


### [161] [A Free Probabilistic Framework for Analyzing the Transformer-based Language Models](https://arxiv.org/abs/2506.16550)
*Swagatam Das*

Main category: cs.LG

TL;DR: 论文提出了一种基于自由概率理论的算子理论框架，用于分析基于Transformer的语言模型，揭示了其谱动力学系统。


<details>
  <summary>Details</summary>
Motivation: 通过将Transformer模型中的嵌入和注意力机制表示为算子，重新解释其行为，以理解其归纳偏置、泛化行为和熵动态。

Method: 将词嵌入和注意力机制表示为自伴算子，利用自由加性卷积分析层间表示传播，并基于自由熵推导泛化界。

Result: 揭示了Transformer层的谱迹随深度演化的规律，并提供了信息流和结构复杂性的理论分析。

Conclusion: 该框架为大型语言模型的信息流和结构复杂性提供了理论基础，连接了神经网络架构与非交换调和分析。

Abstract: We outline an operator-theoretic framework for analyzing transformer-based
language models using the tools of free probability theory. By representing
token embeddings and attention mechanisms as self-adjoint operators in a racial
probability space, we reinterpret attention as a non-commutative convolution
and view the layer-wise propagation of representations as an evolution governed
by free additive convolution. This formalism reveals a spectral dynamical
system underpinning deep transformer stacks and offers insight into their
inductive biases, generalization behavior, and entropy dynamics. We derive a
generalization bound based on free entropy and demonstrate that the spectral
trace of transformer layers evolves predictably with depth. Our approach
bridges neural architecture with non-commutative harmonic analysis, enabling
principled analysis of information flow and structural complexity in large
language models

</details>


### [162] [One Sample is Enough to Make Conformal Prediction Robust](https://arxiv.org/abs/2506.16553)
*Soroush H. Zargarbashi,Mohammad Sadegh Akhondzadeh,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 本文提出了一种基于单样本的鲁棒共形预测方法（RCP1），通过随机扰动输入和二元认证，减少了计算开销，同时保持了预测集的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒共形预测（RCP）需要多次模型前向传播，计算成本高。本文旨在通过单样本扰动和认证方法，降低计算复杂度。

Method: 使用随机扰动输入和二元认证，对共形预测过程本身进行认证，而非单个分数。适用于分类和回归任务。

Result: RCP1在保持鲁棒性的同时，平均预测集大小优于现有方法，且计算效率更高。

Conclusion: RCP1为鲁棒共形预测提供了一种高效且通用的解决方案，适用于多种任务。

Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed
to include the true label with high adjustable probability. Robust CP (RCP)
extends this to inputs with worst-case noise. A well-established approach is to
use randomized smoothing for RCP since it is applicable to any black-box model
and provides smaller sets compared to deterministic methods. However, current
smoothing-based RCP requires many model forward passes per each input which is
computationally expensive. We show that conformal prediction attains some
robustness even with a forward pass on a single randomly perturbed input. Using
any binary certificate we propose a single sample robust CP (RCP1). Our
approach returns robust sets with smaller average set size compared to SOTA
methods which use many (e.g. around 100) passes per input. Our key insight is
to certify the conformal prediction procedure itself rather than individual
scores. Our approach is agnostic to the setup (classification and regression).
We further extend our approach to smoothing-based robust conformal risk
control.

</details>


### [163] [Energy-Based Transfer for Reinforcement Learning](https://arxiv.org/abs/2506.16590)
*Zeyun Deng,Jasorsi Ghosh,Fiona Xie,Yuzhe Lu,Katia Sycara,Joseph Campbell*

Main category: cs.LG

TL;DR: 提出了一种基于能量的迁移学习方法，通过选择性指导提高强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在多任务或持续学习环境中样本效率低，迁移知识可能因任务差异导致次优指导。

Method: 使用基于能量的迁移学习方法，通过分布外检测选择性指导，确保教师仅在训练分布内状态干预。

Result: 理论证明能量分数反映教师状态访问密度，实验显示在单任务和多任务中均提高了样本效率和性能。

Conclusion: 该方法有效解决了迁移学习中因任务差异导致的指导偏差问题，提升了强化学习的效率。

Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency,
making them challenging to apply in multi-task or continual learning settings.
Efficiency can be improved by transferring knowledge from a previously trained
teacher policy to guide exploration in new but related tasks. However, if the
new task sufficiently differs from the teacher's training task, the transferred
guidance may be sub-optimal and bias exploration toward low-reward behaviors.
We propose an energy-based transfer learning method that uses
out-of-distribution detection to selectively issue guidance, enabling the
teacher to intervene only in states within its training distribution. We
theoretically show that energy scores reflect the teacher's state-visitation
density and empirically demonstrate improved sample efficiency and performance
across both single-task and multi-task settings.

</details>


### [164] [FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE](https://arxiv.org/abs/2506.16600)
*Khiem Le,Tuan Tran,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: FLAME是一种基于稀疏专家混合（SMoE）架构的新型联邦学习框架，通过动态调整激活专家数量实现客户端适应性，解决了现有方法因压缩全局LoRA矩阵导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有资源自适应的LoRA联邦微调方法因压缩全局LoRA矩阵导致信息丢失，性能不佳。FLAME旨在通过保留完整LoRA矩阵并动态调整专家数量来解决这一问题。

Method: FLAME采用SMoE架构，保留未压缩的全局LoRA矩阵，通过调整客户端激活专家数量实现适应性。为解决部分专家激活输出不匹配和专家训练质量不均衡问题，引入了轻量级重缩放机制和激活感知聚合方案。

Result: 实验结果表明，FLAME在不同计算环境下均优于现有方法，提供了稳健且高效的资源自适应联邦学习解决方案。

Conclusion: FLAME通过SMoE架构和创新的重缩放与聚合机制，有效解决了资源自适应联邦学习中的性能问题，显著提升了模型表现。

Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients
to fine-tune models using compressed versions of global LoRA matrices, in order
to accommodate various compute resources across clients. This compression
requirement will lead to suboptimal performance due to information loss. To
address this, we propose FLAME, a novel federated learning framework based on
the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,
FLAME retains full (uncompressed) global LoRA matrices and achieves client-side
adaptability by varying the number of activated experts per client. However,
incorporating SMoE into federated learning introduces unique challenges,
specifically, the mismatch in output magnitude from partial expert activation
and the imbalance in expert training quality across clients. FLAME tackles
these challenges through a lightweight rescaling mechanism and an
activation-aware aggregation scheme. Empirical results across diverse
computational settings demonstrate that FLAME consistently outperforms existing
methods, providing a robust and effective solution for resource-adaptive
federated learning.

</details>


### [165] [SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics](https://arxiv.org/abs/2506.16602)
*Siddharth Viswanath,Rahul Singh,Yanlei Zhang,J. Adam Noah,Joy Hirsch,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: SlepNet是一种新型图卷积网络架构，利用Slepian基而非图傅里叶谐波，专注于相关子图上的信号能量集中，在神经活动和交通动态数据中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在表示信号模式时存在局限性，尤其是在神经科学等领域中，信号的高维和局部化特征难以被传统方法有效捕捉。

Method: 提出SlepNet，使用Slepian基自动学习相关子图，并通过掩码集中信号能量，生成高分辨率的神经活动表示。

Result: 在多个fMRI和交通动态数据集上，SlepNet性能优于传统GNN和图信号处理方法，并能更清晰地区分相似模式。

Conclusion: SlepNet不仅适用于预测任务，还能用于表示学习，为时空数据提供了新的解决方案。

Abstract: Graph neural networks have been useful in machine learning on
graph-structured data, particularly for node classification and some types of
graph classification tasks. However, they have had limited use in representing
patterning of signals over graphs. Patterning of signals over graphs and in
subgraphs carries important information in many domains including neuroscience.
Neural signals are spatiotemporally patterned, high dimensional and difficult
to decode. Graph signal processing and associated GCN models utilize the graph
Fourier transform and are unable to efficiently represent spatially or
spectrally localized signal patterning on graphs. Wavelet transforms have shown
promise here, but offer non-canonical representations and cannot be tightly
confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that
uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian
harmonics optimally concentrate signal energy on specifically relevant
subgraphs that are automatically learned with a mask. Thus, they can produce
canonical and highly resolved representations of neural activity, focusing
energy of harmonics on areas of the brain which are activated. We evaluated
SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and
two traffic dynamics datasets, comparing its performance against conventional
GNNs and graph signal processing constructs. SlepNet outperforms the baselines
in all datasets. Moreover, the extracted representations of signal patterns
from SlepNet offers more resolution in distinguishing between similar patterns,
and thus represent brain signaling transients as informative trajectories. Here
we have shown that these extracted trajectory representations can be used for
other downstream untrained tasks. Thus we establish that SlepNet is useful both
for prediction and representation learning in spatiotemporal data.

</details>


### [166] [Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces](https://arxiv.org/abs/2506.16608)
*Jiamin He,A. Rupam Mahmood,Martha White*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架，将分布参数作为动作，重新定义智能体与环境的边界，并通过新的参数化方法实现连续动作空间。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理离散或混合动作空间时存在局限性，新框架旨在通过参数化分布参数来解决这一问题。

Method: 提出了分布参数策略梯度（DPPG）和插值评论家学习（ICL）方法，并基于TD3开发了DPAC算法。

Result: DPAC在MuJoCo连续控制任务中表现优于TD3，并在离散化动作空间中具有竞争力。

Conclusion: 新框架通过参数化分布参数有效扩展了强化学习的应用范围，并在实验中验证了其优越性。

Abstract: We introduce a novel reinforcement learning (RL) framework that treats
distribution parameters as actions, redefining the boundary between agent and
environment. This reparameterization makes the new action space continuous,
regardless of the original action type (discrete, continuous, mixed, etc.).
Under this new parameterization, we develop a generalized deterministic policy
gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has
lower variance than the gradient in the original action space. Although
learning the critic over distribution parameters poses new challenges, we
introduce interpolated critic learning (ICL), a simple yet effective strategy
to enhance learning, supported by insights from bandit settings. Building on
TD3, a strong baseline for continuous control, we propose a practical
DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).
Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from
OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance
on the same environments with discretized action spaces.

</details>


### [167] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/abs/2506.16629)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: DEBIAS算法通过优化结果定义以最大化因果可识别性，解决了精神病学纵向数据中的因果推断问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精神病学中症状异质性和潜在混杂因素使传统因果推断方法失效，需新方法解决。

Method: DEBIAS算法学习非负、临床可解释的权重，通过利用既往治疗的时滞效应最大化因果效应并最小化混杂。

Result: 在抑郁和精神分裂症实验中，DEBIAS显著优于现有方法，恢复因果效应更准确。

Conclusion: DEBIAS为精神病学纵向数据提供了一种可验证的因果推断方法，具有临床意义。

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [168] [Semantic Outlier Removal with Embedding Models and LLMs](https://arxiv.org/abs/2506.16644)
*Eren Akbiyik,João Almeida,Rik Melis,Ritu Sriram,Viviana Petrescu,Vilhjálmur Vilhjálmsson*

Main category: cs.LG

TL;DR: SORE是一种基于语义嵌入和近似最近邻搜索的方法，用于高效去除文档中的无关内容，在多语言环境下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如HTML模板提取或关键词过滤）在多语言和上下文敏感场景中效果不佳，而大型语言模型（LLM）成本过高。

Method: SORE利用多语言句子嵌入和近似最近邻搜索，通过元数据嵌入识别核心内容，并标记与预定义异常组匹配或偏离核心的文本段。

Result: 实验表明，SORE在HTML数据集上优于结构方法，且在多语言场景中保持高精度和低成本。

Conclusion: SORE是一种高效、透明的解决方案，已在生产环境中部署，支持进一步研究和复现。

Abstract: Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

</details>


### [169] [Mesh-Informed Neural Operator : A Transformer Generative Approach](https://arxiv.org/abs/2506.16656)
*Yaozhong Shi,Zachary E. Ross,Domniki Asimaki,Kamyar Azizzadenesheli*

Main category: cs.LG

TL;DR: 提出了Mesh-Informed Neural Operator (MINO)，解决了现有功能生成模型对规则网格和矩形域的依赖问题，扩展了应用范围。


<details>
  <summary>Details</summary>
Motivation: 当前功能生成模型受限于Fourier Neural Operator (FNO)，仅适用于规则网格和矩形域，限制了其应用潜力。

Method: 通过图神经算子和交叉注意力机制，开发了MINO，提供了一种与域和离散化无关的生成建模框架。

Result: MINO显著扩展了功能生成模型的应用范围，并提供了统一视角以整合神经算子和深度学习架构。

Conclusion: MINO填补了功能生成模型领域的空白，并提供了标准化评估指标，推动了该领域的发展。

Abstract: Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.

</details>


### [170] [A Minimalist Optimizer Design for LLM Pretraining](https://arxiv.org/abs/2506.16659)
*Athanasios Glentis,Jiaxiang Li,Andi Han,Mingyi Hong*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）预训练中优化器状态的最小需求，提出了SCALE优化器，结合列归一化SGD和最后一层动量，显著减少内存使用并保持性能。


<details>
  <summary>Details</summary>
Motivation: 探索在LLM预训练中，优化器状态的最小需求，以减少内存消耗同时保持性能。

Method: 采用自下而上的方法，结合列归一化SGD和最后一层动量，提出SCALE优化器。

Result: SCALE在多个LLaMA模型上性能优于Adam，内存使用仅为35-45%，并超越其他高效优化器。

Conclusion: SCALE是一种内存高效且性能优越的优化器，适用于大规模预训练，并为未来优化器设计提供了简洁基线。

Abstract: Training large language models (LLMs) typically relies on adaptive optimizers
such as Adam, which require significant memory to maintain first- and
second-moment matrices, known as optimizer states. While recent works such as
GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce
memory consumption, a fundamental question remains: What is the minimal amount
of optimizer state that is truly necessary to retain state-of-the-art
performance in LLM pretraining? In this work, we systematically investigate
this question using a bottom-up approach. We find that two memory- and
compute-efficient optimization techniques are particularly effective: (1)
column-wise gradient normalization significantly boosts the performance of
plain SGD without requiring momentum; and (2) adding first-order momentum only
to the output layer - where gradient variance is highest - yields performance
competitive with fully adaptive methods such as Muon. Based on these insights,
we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new
optimizer that combines column-normalized SGD with last-layer momentum, where
column normalization refers to normalizing the gradient along the output
dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the
performance of Adam while using only 35-45% of the total memory. It also
consistently outperforms memory-efficient optimizers such as GaLore, Fira, and
APOLLO, making it a strong candidate for large-scale pretraining under memory
constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art
method APOLLO in terms of both perplexity and memory consumption. In addition,
our method serves as a minimalist baseline for more sophisticated optimizer
design.

</details>


### [171] [Private Training & Data Generation by Clustering Embeddings](https://arxiv.org/abs/2506.16661)
*Felix Zhou,Samson Zhou,Vahab Mirrokni,Alessandro Epasto,Vincent Cohen-Addad*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私（DP）的合成图像嵌入生成方法，通过高斯混合模型（GMM）在嵌入空间中实现隐私保护，并在分类任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在训练过程中可能泄露敏感数据的隐私问题。

Method: 使用DP聚类在嵌入空间中拟合GMM，生成合成图像嵌入，并通过简单神经网络进行分类。

Result: 在标准数据集上实现了SOTA分类准确率，并能生成逼真的合成图像。

Conclusion: 该方法通用性强、可扩展性好，适用于多种任务。

Abstract: Deep neural networks often use large, high-quality datasets to achieve high
performance on many machine learning tasks. When training involves potentially
sensitive data, this process can raise privacy concerns, as large models have
been shown to unintentionally memorize and reveal sensitive information,
including reconstructing entire training samples. Differential privacy (DP)
provides a robust framework for protecting individual data and in particular, a
new approach to privately training deep neural networks is to approximate the
input dataset with a privately generated synthetic dataset, before any
subsequent training algorithm. We introduce a novel principled method for DP
synthetic image embedding generation, based on fitting a Gaussian Mixture Model
(GMM) in an appropriate embedding space using DP clustering. Our method
provably learns a GMM under separation conditions. Empirically, a simple
two-layer neural network trained on synthetically generated embeddings achieves
state-of-the-art (SOTA) classification accuracy on standard benchmark datasets.
Additionally, we demonstrate that our method can generate realistic synthetic
images that achieve downstream classification accuracy comparable to SOTA
methods. Our method is quite general, as the encoder and decoder modules can be
freely substituted to suit different tasks. It is also highly scalable,
consisting only of subroutines that scale linearly with the number of samples
and/or can be implemented efficiently in distributed systems.

</details>


### [172] [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](https://arxiv.org/abs/2506.16688)
*Zhiying Qiu,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分最优不确定性感知的权重函数及其闭式多项式近似方法，用于提升扩散模型在离线强化学习中的训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在离线强化学习中表现出潜力，但存在训练成本高、收敛慢的问题，尤其是使用基于Transformer的去噪骨干时。现有的权重函数依赖神经网络近似器，在早期训练阶段因MLP泛化能力有限而效果不佳。

Method: 提出了一种变分最优不确定性感知权重函数，并引入闭式多项式近似方法进行在线估计，将其集成到扩散规划流程中。

Result: 在Maze2D和Kitchen任务上的实验表明，该方法在性能相当的情况下，训练步数减少高达10倍。

Conclusion: 该方法显著提升了扩散模型在离线强化学习中的训练效率和实用性。

Abstract: Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to sparse feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.

</details>


### [173] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Main category: cs.LG

TL;DR: 论文提出了一种基于向量量化（VQ）的新方法，通过生成紧凑的语义ID（SID）替代传统嵌入，解决了工业广告推荐系统中存储和推理成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 工业广告推荐系统通常需要处理大规模用户历史数据（O(10^3)到O(10^4)事件），传统嵌入方法在实时预测模型中存储和推理成本过高。

Method: 方法包括：（i）多任务VQ-VAE框架（VQ融合），融合多内容嵌入和分类预测为单一SID；（ii）参数自由的SID-to-embedding转换技术（SIDE）；（iii）新型量化方法DPCA，增强残差量化技术。

Result: 在大规模工业广告推荐系统中，NE增益提升2.4倍，数据占用减少3倍。

Conclusion: 该方法显著提升了推荐系统的效率和性能，同时降低了存储和计算成本。

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [174] [How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension](https://arxiv.org/abs/2506.16704)
*Cynthia Dwork,Lunjia Hu,Han Shao*

Main category: cs.LG

TL;DR: 本文研究了领域泛化的基本问题，提出了一个新的组合度量——领域破碎维度，并证明了其与经典VC维度的定量关系。


<details>
  <summary>Details</summary>
Motivation: 探讨在给定领域族中，需要从多少个随机采样的领域收集数据才能学习一个在所有领域（包括未见过的）上表现良好的模型。

Method: 在PAC框架下建模问题，引入领域破碎维度作为新的组合度量。

Result: 领域破碎维度刻画了领域样本复杂度，并与经典VC维度建立了紧密的定量关系。

Conclusion: 证明在标准PAC设置中可学习的假设类，在该设置中也可学习。

Abstract: We study a fundamental question of domain generalization: given a family of
domains (i.e., data distributions), how many randomly sampled domains do we
need to collect data from in order to learn a model that performs reasonably
well on every seen and unseen domain in the family? We model this problem in
the PAC framework and introduce a new combinatorial measure, which we call the
domain shattering dimension. We show that this dimension characterizes the
domain sample complexity. Furthermore, we establish a tight quantitative
relationship between the domain shattering dimension and the classic VC
dimension, demonstrating that every hypothesis class that is learnable in the
standard PAC setting is also learnable in our setting.

</details>


### [175] [TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data](https://arxiv.org/abs/2506.16723)
*Yuping Yan,Yizhi Wang,Yuanshuai Li,Yaochu Jin*

Main category: cs.LG

TL;DR: TriCon-SF是一种新型的串行联邦学习框架，通过三重随机化和贡献感知技术解决数据异构性和隐私安全问题，在非独立同分布医疗数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构联邦学习中的数据异构性和隐私问题，特别是针对梯度泄漏和链接攻击的脆弱性，以及半诚实或恶意客户端的挑战。

Method: 提出TriCon-SF框架，结合模型层、数据段和训练序列的三重随机化，并利用Shapley值动态评估客户端贡献。

Result: 在非独立同分布医疗数据集上，TriCon-SF在准确性和通信效率上优于标准串行和并行联邦学习，且能抵御客户端隐私攻击。

Conclusion: TriCon-SF通过增强隐私保护和系统鲁棒性，为隐私敏感领域的联邦学习提供了有效解决方案。

Abstract: Serial pipeline training is an efficient paradigm for handling data
heterogeneity in cross-silo federated learning with low communication overhead.
However, even without centralized aggregation, direct transfer of models
between clients can violate privacy regulations and remain susceptible to
gradient leakage and linkage attacks. Additionally, ensuring resilience against
semi-honest or malicious clients who may manipulate or misuse received models
remains a grand challenge, particularly in privacy-sensitive domains such as
healthcare. To address these challenges, we propose TriCon-SF, a novel serial
federated learning framework that integrates triple shuffling and contribution
awareness. TriCon-SF introduces three levels of randomization by shuffling
model layers, data segments, and training sequences to break deterministic
learning patterns and disrupt potential attack vectors, thereby enhancing
privacy and robustness. In parallel, it leverages Shapley value methods to
dynamically evaluate client contributions during training, enabling the
detection of dishonest behavior and enhancing system accountability. Extensive
experiments on non-IID healthcare datasets demonstrate that TriCon-SF
outperforms standard serial and parallel federated learning in both accuracy
and communication efficiency. Security analysis further supports its resilience
against client-side privacy attacks.

</details>


### [176] [On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis](https://arxiv.org/abs/2506.16732)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 论文探讨了无监督组合优化（UCO）中训练与测试的不对齐问题，提出将可微分的去随机化方法引入训练以改善对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有UCO方法中，训练与测试阶段存在不对齐，导致训练损失低不一定带来更好的去随机化性能。

Method: 在训练中引入可微分的去随机化方法，以改善训练与测试的对齐性。

Result: 初步实验表明该方法改善了训练与测试的对齐性，但也带来了训练中的新挑战。

Conclusion: 研究为UCO中训练与测试的对齐问题提供了初步解决方案，但需进一步解决训练中的挑战。

Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims
to have continuous decisions that are promising in a probabilistic sense for
each training instance, which enables end-to-end training on initially discrete
and non-differentiable problems. At the test time, for each test instance,
starting from continuous decisions, derandomization is typically applied to
obtain the final deterministic decisions. Researchers have developed more and
more powerful test-time derandomization schemes to enhance the empirical
performance and the theoretical guarantee of UCO methods. However, we notice a
misalignment between training and testing in the existing UCO methods.
Consequently, lower training losses do not necessarily entail better
post-derandomization performance, even for the training instances without any
data distribution shift. Empirically, we indeed observe such undesirable cases.
We explore a preliminary idea to better align training and testing in UCO by
including a differentiable version of derandomization into training. Our
empirical exploration shows that such an idea indeed improves training-test
alignment, but also introduces nontrivial challenges into training.

</details>


### [177] [Optimism Without Regularization: Constant Regret in Zero-Sum Games](https://arxiv.org/abs/2506.16736)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Stratis Skoulakis*

Main category: cs.LG

TL;DR: 本文研究了乐观虚构博弈在两人零和博弈中的学习性能，首次证明无需正则化也能实现最优后悔率。


<details>
  <summary>Details</summary>
Motivation: 探讨乐观虚构博弈在无正则化条件下是否能实现常数后悔率，填补相关研究空白。

Method: 通过双空间几何视角分析乐观虚构博弈，证明能量函数有界性。

Result: 乐观虚构博弈在双策略博弈中实现常数后悔率，而交替虚构博弈后悔率下界为Ω(√T)。

Conclusion: 乐观虚构博弈在无正则化条件下仍能实现快速学习，而交替虚构博弈则无法达到类似效果。

Abstract: This paper studies the optimistic variant of Fictitious Play for learning in
two-player zero-sum games. While it is known that Optimistic FTRL -- a
regularized algorithm with a bounded stepsize parameter -- obtains constant
regret in this setting, we show for the first time that similar, optimal rates
are also achievable without regularization: we prove for two-strategy games
that Optimistic Fictitious Play (using any tiebreaking rule) obtains only
constant regret, providing surprising new evidence on the ability of
non-no-regret algorithms for fast learning in games. Our proof technique
leverages a geometric view of Optimistic Fictitious Play in the dual space of
payoff vectors, where we show a certain energy function of the iterates remains
bounded over time. Additionally, we also prove a regret lower bound of
$\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized
regime, this separates the ability of optimism and alternation in achieving
$o(\sqrt{T})$ regret.

</details>


### [178] [IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification](https://arxiv.org/abs/2506.16744)
*Eion Tyacke,Kunal Gupta,Jay Patel,Rui Li*

Main category: cs.LG

TL;DR: 该论文研究了多模态融合策略在手势解码中的应用，发现基于注意力的分层Transformer在性能上优于其他方法，并通过隔离网络分析了模态交互的贡献。


<details>
  <summary>Details</summary>
Motivation: 手势解码在神经科学和辅助技术中具有重要意义，但传统方法依赖单一生物信号模态，多模态融合可以提升性能。

Method: 比较了线性与基于注意力的融合策略，包括多模态MLP、Transformer和分层Transformer，并在两个公开数据集上评估性能。

Result: 分层Transformer在两种数据集上表现最佳，准确率显著高于基线方法，且跨模态交互贡献了约30%的决策信号。

Conclusion: 研究表明基于注意力的多模态融合能有效提升生物信号分类性能，并为神经机器人系统的传感器设计提供指导。

Abstract: Hand gestures are a primary output of the human motor system, yet the
decoding of their neuromuscular signatures remains a bottleneck for basic
neuroscience and assistive technologies such as prosthetics. Traditional
human-machine interface pipelines rely on a single biosignal modality, but
multimodal fusion can exploit complementary information from sensors. We
systematically compare linear and attention-based fusion strategies across
three architectures: a Multimodal MLP, a Multimodal Transformer, and a
Hierarchical Transformer, evaluating performance on scenarios with unimodal and
multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2
(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).
Across both datasets, the Hierarchical Transformer with attention-based fusion
consistently achieved the highest accuracy, surpassing the multimodal and best
single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%
on HD-sEMG. To investigate how modalities interact, we introduce an Isolation
Network that selectively silences unimodal or cross-modal attention pathways,
quantifying each group of token interactions' contribution to downstream
decisions. Ablations reveal that cross-modal interactions contribute
approximately 30% of the decision signal across transformer layers,
highlighting the importance of attention-driven fusion in harnessing
complementary modality information. Together, these findings reveal when and
how multimodal fusion would enhance biosignal classification and also provides
mechanistic insights of human muscle activities. The study would be beneficial
in the design of sensor arrays for neurorobotic systems.

</details>


### [179] [Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation](https://arxiv.org/abs/2506.16753)
*Kosuke Nakanishi,Akihiro Kubo,Yuji Yasui,Shin Ishii*

Main category: cs.LG

TL;DR: 提出一种新的离策略方法，通过将对抗学习重新表述为软约束优化问题，避免额外环境交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长期最坏情况时效率低下，且阻碍离策略方法的发展。

Method: 将对抗学习重新表述为软约束优化问题，利用策略评估的对称性。

Result: 理论支持方法的有效性，并提供了实现代码。

Conclusion: 新方法解决了现有方法的效率问题，同时支持离策略学习。

Abstract: Recently, robust reinforcement learning (RL) methods designed to handle
adversarial input observations have received significant attention, motivated
by RL's inherent vulnerabilities. While existing approaches have demonstrated
reasonable success, addressing worst-case scenarios over long time horizons
requires both minimizing the agent's cumulative rewards for adversaries and
training agents to counteract them through alternating learning. However, this
process introduces mutual dependencies between the agent and the adversary,
making interactions with the environment inefficient and hindering the
development of off-policy methods. In this work, we propose a novel off-policy
method that eliminates the need for additional environmental interactions by
reformulating adversarial learning as a soft-constrained optimization problem.
Our approach is theoretically supported by the symmetric property of policy
evaluation between the agent and the adversary. The implementation is available
at https://github.com/nakanakakosuke/VALT_SAC.

</details>


### [180] [Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding](https://arxiv.org/abs/2506.16754)
*Jongmin Park,Seunghoon Han,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 论文提出了一种基于多双曲空间的对比学习框架（MHCL），用于捕捉异质图中多样化的复杂结构，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异质图具有多样化的幂律结构，但现有双曲空间嵌入模型仅依赖单一空间，难以有效捕捉这些结构。

Method: 提出MHCL框架，利用多个双曲空间分别描述不同元路径的复杂结构分布，并通过对比学习优化元路径嵌入的区分性。

Result: 实验表明MHCL在多种图机器学习任务中优于现有基线方法。

Conclusion: MHCL能有效捕捉异质图的复杂结构，提升元路径嵌入的区分性。

Abstract: The hyperbolic space, characterized by a constant negative curvature and
exponentially expanding space, aligns well with the structural properties of
heterogeneous graphs. However, although heterogeneous graphs inherently possess
diverse power-law structures, most hyperbolic heterogeneous graph embedding
models rely on a single hyperbolic space. This approach may fail to effectively
capture the diverse power-law structures within heterogeneous graphs. To
address this limitation, we propose a Metapath-based Hyperbolic Contrastive
Learning framework (MHCL), which uses multiple hyperbolic spaces to capture
diverse complex structures within heterogeneous graphs. Specifically, by
learning each hyperbolic space to describe the distribution of complex
structures corresponding to each metapath, it is possible to capture semantic
information effectively. Since metapath embeddings represent distinct semantic
information, preserving their discriminability is important when aggregating
them to obtain node representations. Therefore, we use a contrastive learning
approach to optimize MHCL and improve the discriminability of metapath
embeddings. In particular, our contrastive learning method minimizes the
distance between embeddings of the same metapath and maximizes the distance
between those of different metapaths in hyperbolic space, thereby improving the
separability of metapath embeddings with distinct semantic information. We
conduct comprehensive experiments to evaluate the effectiveness of MHCL. The
experimental results demonstrate that MHCL outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

</details>


### [181] [What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity](https://arxiv.org/abs/2506.16782)
*Youjin Kong*

Main category: cs.LG

TL;DR: 论文探讨了机器学习公平性的伦理基础，指出仅关注分配平等是不完整的，提出了结合分配平等和关系平等的多元平等主义框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示当前公平机器学习研究中对分配平等的单一关注，未能全面解决结构性不平等及其带来的分配性和代表性伤害。

Method: 方法是通过批判社会和政治哲学，提出一个结合分配平等和关系平等的多元平等主义框架。

Result: 结果是该框架为机器学习公平性提供了更全面的伦理基础，并提出了在机器学习流程中实施的具体路径。

Conclusion: 结论是多元平等主义框架能更有效地解决机器学习系统中的结构性不平等问题。

Abstract: Fairness in machine learning (ML) has become a rapidly growing area of
research. But why, in the first place, is unfairness in ML morally wrong? And
why should we care about improving fairness? Most fair-ML research implicitly
appeals to distributive equality: the idea that desirable goods and benefits,
such as opportunities (e.g., Barocas et al., 2023), should be equally
distributed across society. Unfair ML models, then, are seen as wrong because
they unequally distribute such benefits. This paper argues that this exclusive
focus on distributive equality offers an incomplete and potentially misleading
ethical foundation. Grounding ML fairness in egalitarianism -- the view that
equality is a fundamental moral and social ideal -- requires challenging
structural inequality: systematic, institutional, and durable arrangements that
privilege some groups while disadvantaging others. Structural inequality
manifests through ML systems in two primary forms: allocative harms (e.g.,
economic loss) and representational harms (e.g., stereotypes, erasure). While
distributive equality helps address allocative harms, it fails to explain why
representational harms are wrong -- why it is wrong for ML systems to reinforce
social hierarchies that stratify people into superior and inferior groups --
and why ML systems should aim to foster a society where people relate as equals
(i.e., relational equality). To address these limitations, the paper proposes a
multifaceted egalitarian framework for ML fairness that integrates both
distributive and relational equality. Drawing on critical social and political
philosophy, this framework offers a more comprehensive ethical foundation for
tackling the full spectrum of harms perpetuated by ML systems. The paper also
outlines practical pathways for implementing the framework across the ML
pipeline.

</details>


### [182] [Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps](https://arxiv.org/abs/2506.16787)
*Jiashun Cheng,Aochuan Chen,Nuo Chen,Ziqi Gao,Yuhan Li,Jia Li,Fugee Tsung*

Main category: cs.LG

TL;DR: SeLoRA通过稀疏谱子空间重新参数化LoRA，减少冗余参数，提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调大型基础模型时存在参数冗余问题，限制了其容量和效率。

Method: 提出SeLoRA，利用谱基的鲁棒表达能力，从稀疏谱子空间重新参数化LoRA。

Result: SeLoRA在减少参数的同时提升了性能，在多项下游任务中表现优于基线。

Conclusion: SeLoRA是一种高效且可扩展的插件框架，显著提升了LoRA的性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large foundation models. Despite its successes, the substantial
parameter redundancy, which limits the capacity and efficiency of LoRA, has
been recognized as a bottleneck. In this work, we systematically investigate
the impact of redundancy in fine-tuning LoRA and reveal that reducing density
redundancy does not degrade expressiveness. Based on this insight, we introduce
\underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank
\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of
spectral bases to re-parameterize LoRA from a sparse spectral subspace.
Designed with simplicity, SeLoRA enables seamless integration with various LoRA
variants for performance boosting, serving as a scalable plug-and-play
framework. Extensive experiments substantiate that SeLoRA achieves greater
efficiency with fewer parameters, delivering superior performance enhancements
over strong baselines on various downstream tasks, including commonsense
reasoning, math reasoning, and code generation.

</details>


### [183] [Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective](https://arxiv.org/abs/2506.16790)
*Senmiao Wang,Yupeng Chen,Yushun Zhang,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: 论文提出了一种名为SPoGInit的初始化方法，通过优化信号传播（SP）的三个关键指标，解决了GNN深度增加时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: GNN在深度增加时性能下降，现有初始化方法无法同时控制信号传播的三个关键指标。

Method: 提出SPoGInit方法，通过优化前向传播、反向传播和图嵌入变化（GEV）三个指标来改进初始化。

Result: SPoGInit在多种任务和架构上优于常用初始化方法，并支持GNN的深度扩展。

Conclusion: SPoGInit有效解决了GNN深度相关挑战，验证了SP分析框架的有效性。

Abstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the
network depth increases. This paper addresses this issue by introducing
initialization methods that enhance signal propagation (SP) within GNNs. We
propose three key metrics for effective SP in GNNs: forward propagation,
backward propagation, and graph embedding variation (GEV). While the first two
metrics derive from classical SP theory, the third is specifically designed for
GNNs. We theoretically demonstrate that a broad range of commonly used
initialization methods for GNNs, which exhibit performance degradation with
increasing depth, fail to control these three metrics simultaneously. To deal
with this limitation, a direct exploitation of the SP analysis--searching for
weight initialization variances that optimize the three metrics--is shown to
significantly enhance the SP in deep GCNs. This approach is called Signal
Propagation on Graph-guided Initialization (SPoGInit). Our experiments
demonstrate that SPoGInit outperforms commonly used initialization methods on
various tasks and architectures. Notably, SPoGInit enables performance
improvements as GNNs deepen, which represents a significant advancement in
addressing depth-related challenges and highlights the validity and
effectiveness of the SP analysis framework.

</details>


### [184] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/abs/2506.16791)
*Nick Erickson,Lennart Purucker,Andrej Tschalzev,David Holzmüller,Prateek Mutalik Desai,and David Salinas,Frank Hutter*

Main category: cs.LG

TL;DR: TabArena是一个持续维护的表格数据基准测试系统，旨在解决现有静态基准的不足，通过标准化数据集和模型，提供公开排行榜和可复现代码。


<details>
  <summary>Details</summary>
Motivation: 当前表格数据的基准测试存在静态设计问题，无法适应模型更新或新模型发布的需求，因此需要动态维护的基准系统。

Method: 通过手动整理代表性数据集和模型，进行大规模基准测试，建立公开排行榜，并由经验丰富的维护团队持续更新。

Result: 验证方法和超参数集成对模型性能影响显著；梯度提升树仍具竞争力，深度学习方法在大时间预算下表现优异，基础模型在小数据集上表现突出。

Conclusion: TabArena通过动态维护和跨模型集成，推动了表格机器学习的最新进展，并提供了公开可用的基准平台。

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [185] [Robust Group Anomaly Detection for Quasi-Periodic Network Time Series](https://arxiv.org/abs/2506.16815)
*Kai Yang,Shaoyu Dou,Pan Luo,Xin Wang,H. Vincent Poor*

Main category: cs.LG

TL;DR: 提出了一种名为seq2GMM的框架，用于识别网络时间序列数据库中的异常时间序列，并通过优化算法高效训练模型，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列常因同步误差导致周期和长度变化，需要识别异常行为并解释决策过程。

Method: 采用序列到高斯混合模型（seq2GMM）框架，结合基于代理的优化算法进行高效训练。

Result: 在多个公共基准数据集上表现优异，显著优于现有异常检测技术，并提供了理论收敛性分析。

Conclusion: seq2GMM能有效识别异常时间序列，并通过优化算法实现高效训练，具有理论和实证支持。

Abstract: Many real-world multivariate time series are collected from a network of
physical objects embedded with software, electronics, and sensors. The
quasi-periodic signals generated by these objects often follow a similar
repetitive and periodic pattern, but have variations in the period, and come in
different lengths caused by timing (synchronization) errors. Given a multitude
of such quasi-periodic time series, can we build machine learning models to
identify those time series that behave differently from the majority of the
observations? In addition, can the models help human experts to understand how
the decision was made? We propose a sequence to Gaussian Mixture Model
(seq2GMM) framework. The overarching goal of this framework is to identify
unusual and interesting time series within a network time series database. We
further develop a surrogate-based optimization algorithm that can efficiently
train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a
plurality of public benchmark datasets, outperforming state-of-the-art anomaly
detection techniques by a significant margin. We also theoretically analyze the
convergence property of the proposed training algorithm and provide numerical
results to substantiate our theoretical claims.

</details>


### [186] [Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs](https://arxiv.org/abs/2506.16824)
*Thomas Marwitz,Alexander Colsmann,Ben Breitung,Christoph Brabec,Christoph Kirchlechner,Eva Blasco,Gabriel Cadilha Marques,Horst Hahn,Michael Hirtz,Pavel A. Levkin,Yolita M. Eggeler,Tobias Schlöder,Pascal Friederich*

Main category: cs.LG

TL;DR: 利用大型语言模型（LLM）从材料科学摘要中提取主要概念，构建概念图，并通过机器学习预测新兴研究方向的组合。


<details>
  <summary>Details</summary>
Motivation: 由于研究文献数量激增，科学家难以全面阅读，需自动化工具提取关键概念并发现潜在研究方向。

Method: 使用LLM提取概念，构建概念图，训练机器学习模型预测新兴概念组合。

Result: 模型预测性能提升，能启发材料科学家发现未研究过的创新主题组合。

Conclusion: LLM结合机器学习可有效辅助科研创新，提供未来研究方向建议。

Abstract: Due to an exponential increase in published research articles, it is
impossible for individual scientists to read all publications, even within
their own research field. In this work, we investigate the use of large
language models (LLMs) for the purpose of extracting the main concepts and
semantic information from scientific abstracts in the domain of materials
science to find links that were not noticed by humans and thus to suggest
inspiring near/mid-term future research directions. We show that LLMs can
extract concepts more efficiently than automated keyword extraction methods to
build a concept graph as an abstraction of the scientific literature. A machine
learning model is trained to predict emerging combinations of concepts, i.e.
new research ideas, based on historical data. We demonstrate that integrating
semantic concept information leads to an increased prediction performance. The
applicability of our model is demonstrated in qualitative interviews with
domain experts based on individualized model suggestions. We show that the
model can inspire materials scientists in their creative thinking process by
predicting innovative combinations of topics that have not yet been
investigated.

</details>


### [187] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: 论文提出了FedFitTech基线，用于解决可穿戴设备在联邦学习中的挑战，如数据不平衡和隐私问题，并通过案例研究展示了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统集中式学习方法在可穿戴设备中存在隐私和效率问题，联邦学习（FL）提供了一种去中心化的解决方案。

Method: 提出了基于Flower框架的FedFitTech基线，并采用客户端早期停止策略进行优化。

Result: 系统减少了13%的冗余通信，同时识别性能仅下降1%。

Conclusion: FedFitTech为FitTech领域的研究和开发提供了开源基础。

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [188] [Bandwidth Selectors on Semiparametric Bayesian Networks](https://arxiv.org/abs/2506.16844)
*Victor Alejandre,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 本文研究了半参数贝叶斯网络（SPBNs）中带宽选择器对性能的影响，比较了交叉验证和插件选择器，发现它们优于传统的正态规则。


<details>
  <summary>Details</summary>
Motivation: 现实数据常偏离正态性，传统正态规则可能导致次优密度估计和预测性能下降。

Method: 应用交叉验证和插件选择器，扩展PyBNesian工具包进行实验分析。

Result: 交叉验证在高样本量场景中优于正态规则，能更有效利用信息。

Conclusion: 提出的带宽选择器提升了SPBNs的学习能力和适用性。

Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and
non-parametric probabilistic models, offering flexibility in learning complex
data distributions from samples. In particular, kernel density estimators
(KDEs) are employed for the non-parametric component. Under the assumption of
data normality, the normal rule is used to learn the bandwidth matrix for the
KDEs in SPBNs. This matrix is the key hyperparameter that controls the
trade-off between bias and variance. However, real-world data often deviates
from normality, potentially leading to suboptimal density estimation and
reduced predictive performance. This paper first establishes the theoretical
framework for the application of state-of-the-art bandwidth selectors and
subsequently evaluates their impact on SPBN performance. We explore the
approaches of cross-validation and plug-in selectors, assessing their
effectiveness in enhancing the learning capability and applicability of SPBNs.
To support this investigation, we have extended the open-source package
PyBNesian for SPBNs with the additional bandwidth selection techniques and
conducted extensive experimental analyses. Our results demonstrate that the
proposed bandwidth selectors leverage increasing information more effectively
than the normal rule, which, despite its robustness, stagnates with more data.
In particular, unbiased cross-validation generally outperforms the normal rule,
highlighting its advantage in high sample size scenarios.

</details>


### [189] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Main category: cs.LG

TL;DR: 提出了一种新的软生存树模型（SST），通过非线性优化训练，结合灵活性和可解释性，在15个数据集上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 决策树在生存分析中因其可解释性和建模复杂关系的能力而受欢迎，但传统方法多为启发式，全局优化方法逐渐受到关注。

Method: 提出SST模型，采用软分割规则，通过非线性优化训练，支持多种生存函数形式，并满足条件计算特性。

Result: 在15个数据集上，SST在判别和校准指标上优于三种基准生存树方法。

Conclusion: SST结合了灵活性和可解释性，可扩展至群体公平性，是一种有效的生存分析方法。

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [190] [Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.16853)
*Semin Kim,Yeonwoo Cha,Jaehoon Yoo,Seunghoon Hong*

Main category: cs.LG

TL;DR: RATTPO是一种通用的测试时提示优化方法，适用于多种奖励模型，无需修改即可提升文本到图像生成模型的提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化提示工程方法针对特定奖励配置设计，在新场景中表现不佳，因此需要一种更通用的优化方法。

Method: RATTPO通过迭代查询大型语言模型（LLM）优化提示，利用优化轨迹和奖励感知反馈信号（“提示”）作为上下文。

Result: RATTPO在多种奖励设置下表现优异，搜索效率高（节省3.5倍推理预算），性能接近需微调的学习基线。

Conclusion: RATTPO是一种高效、通用的提示优化方法，适用于多样化奖励场景。

Abstract: We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.

</details>


### [191] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Main category: cs.LG

TL;DR: 本文提出了一种无监督学习框架，用于学习事件触发时间序列之间的相似性，结合了分层多分辨率序列自编码器和高斯混合模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于事件触发时间序列的复杂动态性，现有相似性度量方法在安全相关任务中表现不佳，需要一种更有效的框架。

Method: 采用分层多分辨率序列自编码器和高斯混合模型（GMM）学习时间序列的低维表示。

Result: 通过实验验证，该方法在性能上显著优于现有技术。

Conclusion: 该框架为事件触发时间序列的相似性建模提供了系统性方法，具有实际应用潜力。

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


### [192] [Optimal Depth of Neural Networks](https://arxiv.org/abs/2506.16862)
*Qian Qi*

Main category: cs.LG

TL;DR: 本文提出了一种理论框架，将深度神经网络的前向传播建模为最优停止问题，证明了在残差函数收益递减的条件下，最优停止深度是有限的，并提出了一种新的正则化方法以提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络深度优化的挑战性问题，避免资源密集型实验。

Method: 将ResNet的前向传播建模为最优停止问题，提出正则化项$\mathcal{L}_{\rm depth}$，并扩展到Transformer架构。

Result: 理论证明了最优停止深度的有限性，实验验证了正则化方法的有效性，显著提高了计算效率且不影响模型精度。

Conclusion: 该框架为神经网络深度优化提供了理论支持，并通过正则化方法实现了高效计算。

Abstract: Determining the optimal depth of a neural network is a fundamental yet
challenging problem, typically resolved through resource-intensive
experimentation. This paper introduces a formal theoretical framework to
address this question by recasting the forward pass of a deep network,
specifically a Residual Network (ResNet), as an optimal stopping problem. We
model the layer-by-layer evolution of hidden representations as a sequential
decision process where, at each layer, a choice is made between halting
computation to make a prediction or continuing to a deeper layer for a
potentially more refined representation. This formulation captures the
intrinsic trade-off between accuracy and computational cost. Our primary
theoretical contribution is a proof that, under a plausible condition of
diminishing returns on the residual functions, the expected optimal stopping
depth is provably finite, even in an infinite-horizon setting. We leverage this
insight to propose a novel and practical regularization term, $\mathcal{L}_{\rm
depth}$, that encourages the network to learn representations amenable to
efficient, early exiting. We demonstrate the generality of our framework by
extending it to the Transformer architecture and exploring its connection to
continuous-depth models via free-boundary problems. Empirical validation on
ImageNet confirms that our regularizer successfully induces the theoretically
predicted behavior, leading to significant gains in computational efficiency
without compromising, and in some cases improving, final model accuracy.

</details>


### [193] [The Importance of Being Lazy: Scaling Limits of Continual Learning](https://arxiv.org/abs/2506.16884)
*Jacopo Graldi,Alessandro Breccia,Giulia Lanzillotta,Thomas Hofmann,Lorenzo Noci*

Main category: cs.LG

TL;DR: 本文通过区分懒惰和丰富训练机制，系统研究了模型规模和特征学习对持续学习的影响，揭示了特征学习与任务非平稳性及遗忘之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究，神经网络在非平稳环境中的学习仍存在困难，对灾难性遗忘的理解尚不完整。

Method: 通过可变参数化架构区分懒惰和丰富训练机制，利用动态平均场理论研究无限宽度模型在特征学习机制下的动态特性。

Result: 增加模型宽度仅在减少特征学习时有益；特征学习与任务相似性密切相关，高特征学习仅对高度相似任务有益。

Conclusion: 神经网络在特征学习的临界水平下表现最佳，该水平取决于任务非平稳性且可跨模型规模转移。

Abstract: Despite recent efforts, neural networks still struggle to learn in
non-stationary environments, and our understanding of catastrophic forgetting
(CF) is far from complete. In this work, we perform a systematic study on the
impact of model scale and the degree of feature learning in continual learning.
We reconcile existing contradictory observations on scale in the literature, by
differentiating between lazy and rich training regimes through a variable
parameterization of the architecture. We show that increasing model width is
only beneficial when it reduces the amount of feature learning, yielding more
laziness. Using the framework of dynamical mean field theory, we then study the
infinite width dynamics of the model in the feature learning regime and
characterize CF, extending prior theoretical results limited to the lazy
regime. We study the intricate relationship between feature learning, task
non-stationarity, and forgetting, finding that high feature learning is only
beneficial with highly similar tasks. We identify a transition modulated by
task similarity where the model exits an effectively lazy regime with low
forgetting to enter a rich regime with significant forgetting. Finally, our
findings reveal that neural networks achieve optimal performance at a critical
level of feature learning, which depends on task non-stationarity and transfers
across model scales. This work provides a unified perspective on the role of
scale and feature learning in continual learning.

</details>


### [194] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: 论文提出了一种针对工业产品表面缺陷的无监督检测方法，解决了传统方法在低质量数据和实际场景中的不足，并提供了改进模型和数据质量的实用指南。


<details>
  <summary>Details</summary>
Motivation: 传统手动检测工业产品质量问题成本高且易出错，机器学习虽有望替代，但现有方法在低质量数据和实际场景中表现不佳，且难以诊断问题根源。

Method: 评估了两种先进的无监督模型，用于识别和改进生产数据中的质量问题，无需新数据，并提出了改进模型和数据质量的框架。

Result: 研究发现现有方法在低质量数据和实际场景中表现不佳，提出了更适用于实际场景的实证风险估计框架。

Conclusion: 论文为实践者提供了识别和改进模型及数据质量问题的实用指南，并指出了基于似然方法的常见缺陷，提出了更可靠的解决方案。

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


### [195] [A deep learning and machine learning approach to predict neonatal death in the context of São Paulo](https://arxiv.org/abs/2506.16929)
*Mohon Raihan,Plabon Kumar Saha,Rajan Das Gupta,A Z M Tahmidul Kabir,Afia Anjum Tamanna,Md. Harun-Ur-Rashid,Adnan Bin Abdus Salam,Md Tanvir Anjum,A Z M Ahteshamul Kabir*

Main category: cs.LG

TL;DR: 利用机器学习和深度学习技术预测新生儿死亡风险，LSTM模型表现最佳，准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 全球新生儿死亡率高，早期预测可降低死亡风险。

Method: 使用逻辑回归、K近邻、随机森林、XGBoost、CNN和LSTM等算法，基于140万新生儿历史数据训练模型。

Result: XGBoost和随机森林准确率94%，LSTM达99%。

Conclusion: LSTM是最适合预测新生儿死亡风险的模型。

Abstract: Neonatal death is still a concerning reality for underdeveloped and even some
developed countries. Worldwide data indicate that 26.693 babies out of 1,000
births die, according to Macro Trades. To reduce this number, early prediction
of endangered babies is crucial. Such prediction enables the opportunity to
take ample care of the child and mother so that early child death can be
avoided. In this context, machine learning was used to determine whether a
newborn baby is at risk. To train the predictive model, historical data of 1.4
million newborns was used. Machine learning and deep learning techniques such
as logical regression, K-nearest neighbor, random forest classifier, extreme
gradient boosting (XGBoost), convolutional neural network, and long short-term
memory (LSTM) were implemented using the dataset to identify the most accurate
model for predicting neonatal mortality. Among the machine learning algorithms,
XGBoost and random forest classifier achieved the best accuracy with 94%, while
among the deep learning models, LSTM delivered the highest accuracy with 99%.
Therefore, using LSTM appears to be the most suitable approach to predict
whether precautionary measures for a child are necessary.

</details>


### [196] [RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](https://arxiv.org/abs/2506.16965)
*Çağatay Demirel*

Main category: cs.LG

TL;DR: RocketStack是一种递归集成框架，通过逐层修剪弱学习器和特征压缩，实现了深度堆叠，显著提升了性能并降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 解决深度堆叠中模型复杂度、特征冗余和计算负担的问题。

Method: 引入RocketStack框架，逐层修剪弱学习器，添加高斯噪声，并探索特征压缩方法（如注意力选择、SFE过滤器和自动编码器）。

Result: 在33个数据集上验证，深度堆叠显著提升了准确性，并减少了运行时间和特征维度。

Conclusion: RocketStack通过修剪和压缩实现了高效的深度递归集成，为机器学习提供了新思路。

Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before pruning, and compared against
strict OOF pruning. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-pruning configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.

</details>


### [197] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLMs）在上下文学习（ICL）中是否能够捕捉潜在概念，并通过实验验证了模型在离散和连续潜在概念任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在ICL中是否真正理解潜在概念，而非仅通过捷径解决问题。

Method: 方法包括在2跳推理任务中测试模型对离散潜在概念的识别能力，以及在连续潜在概念任务中分析表示空间的低维子空间。

Result: 结果显示，模型能成功识别离散潜在概念并逐步组合，同时在连续任务中表示空间的几何结构与潜在参数化一致。

Conclusion: 结论表明，ICL中存在高度局部化的结构，能够解耦潜在概念，深化了对LLMs表示能力的理解。

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [198] [Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators](https://arxiv.org/abs/2506.17007)
*Marco Jiralerspong,Esther Derman,Danilo Vucetic,Nikolay Malkin,Bilun Sun,Tianyu Zhang,Pierre-Luc Bacon,Gauthier Gidel*

Main category: cs.LG

TL;DR: 论文提出了一种鲁棒强化学习方法，用于解决代理奖励函数不确定性导致的候选生成问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学发现中，从大量组合对象中筛选候选对象依赖专家知识，现有强化学习方法因代理奖励函数的不确定性表现不佳。

Method: 引入统一操作符，针对代理奖励函数的不确定性进行鲁棒优化，生成更高质量的多样化候选。

Result: 在合成和实际任务中，新算法能生成更高质量且多样化的候选。

Conclusion: 为离散组合生成任务提供了新的灵活视角。

Abstract: A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.

</details>


### [199] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Main category: cs.LG

TL;DR: 该研究通过实验评估了17种AI图像生成模型的能耗，发现能耗差异显著（最高达46倍），并探讨了分辨率、模型类型、量化等因素对能耗的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI图像生成的广泛应用及其对环境资源的需求增加，研究旨在揭示生成每张图像背后的环境影响。

Method: 通过比较17种先进图像生成模型，分析模型量化、图像分辨率、提示长度等因素对能耗的影响，并结合图像质量指标研究能耗与质量的权衡。

Result: 结果显示，不同模型的能耗差异显著（最高46倍），分辨率对能耗影响不一致（1.3x-4.7x），U-Net模型能耗低于Transformer模型，量化通常降低能效，提示长度无显著影响。部分高质量模型同时也是能效最高的。

Conclusion: 研究揭示了AI图像生成模型的能耗差异及影响因素，为开发更环保的AI工具提供了方向。

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [200] [Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment](https://arxiv.org/abs/2506.17029)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zewen Wang,Zhiqiang He,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: 论文提出了一种新的多智能体强化学习框架MARL-OD-DA，用于解决大规模交通分配问题，通过将智能体定义为OD对路由器而非个体旅行者，提升了可扩展性，并通过改进动作空间和奖励函数增强了可靠性和收敛效率。


<details>
  <summary>Details</summary>
Motivation: 大都市的发展和旅行需求的增加对交通分配方法提出了更高要求，传统方法难以适应动态路由行为，而现有MARL框架在可扩展性和可靠性上面临挑战。

Method: MARL-OD-DA将智能体定义为OD对路由器，设计了基于Dirichlet的动作空间和基于局部相对差距的奖励函数，以提升可扩展性和收敛效率。

Result: 实验表明，MARL-OD-DA能有效处理中等规模网络，在SiouxFalls网络中，其分配方案的相对差距比传统方法低94.99%。

Conclusion: MARL-OD-DA框架为解决大规模交通分配问题提供了一种高效可靠的方法，显著优于现有MARL方法。

Abstract: The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.

</details>


### [201] [Critical Appraisal of Fairness Metrics in Clinical Predictive AI](https://arxiv.org/abs/2506.17035)
*João Matos,Ben Van Calster,Leo Anthony Celi,Paula Dhiman,Judy Wawira Gichoya,Richard D. Riley,Chris Russell,Sara Khalid,Gary S. Collins*

Main category: cs.LG

TL;DR: 该论文探讨了临床预测AI中的公平性度量问题，通过综述41项研究，提取了62种公平性度量，发现其碎片化且缺乏临床验证。


<details>
  <summary>Details</summary>
Motivation: 预测AI可能改善临床实践，但若公平性未妥善解决，可能加剧偏见。公平性定义尚不明确，需系统评估。

Method: 通过范围综述，搜索5个数据库（2014-2024年），筛选820条记录，纳入41项研究，提取62种公平性度量。

Result: 发现公平性度量碎片化，临床验证有限，过度依赖阈值依赖性度量。仅18种专为医疗设计，其中仅1种涉及临床实用性。

Conclusion: 公平性定义和量化存在概念挑战，需关注临床意义度量，填补不确定性量化、交叉性和实际应用空白。

Abstract: Predictive artificial intelligence (AI) offers an opportunity to improve
clinical practice and patient outcomes, but risks perpetuating biases if
fairness is inadequately addressed. However, the definition of "fairness"
remains unclear. We conducted a scoping review to identify and critically
appraise fairness metrics for clinical predictive AI. We defined a "fairness
metric" as a measure quantifying whether a model discriminates (societally)
against individuals or groups defined by sensitive attributes. We searched five
databases (2014-2024), screening 820 records, to include 41 studies, and
extracted 62 fairness metrics. Metrics were classified by
performance-dependency, model output level, and base performance metric,
revealing a fragmented landscape with limited clinical validation and
overreliance on threshold-dependent measures. Eighteen metrics were explicitly
developed for healthcare, including only one clinical utility metric. Our
findings highlight conceptual challenges in defining and quantifying fairness
and identify gaps in uncertainty quantification, intersectionality, and
real-world applicability. Future work should prioritise clinically meaningful
metrics.

</details>


### [202] [LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation](https://arxiv.org/abs/2506.17039)
*Elizabeth Fons,Alejandro Sztrajman,Yousef El-Laham,Luciana Ferrer,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.LG

TL;DR: 提出了一种基于Lomb-Scargle的可微分层，用于处理不规则采样时间序列的频谱计算，并结合扩散模型实现更准确的数据填补。


<details>
  <summary>Details</summary>
Motivation: 解决传统FFT方法因均匀采样假设而无法直接处理不规则采样数据的问题，避免插值带来的频谱失真。

Method: 引入可微分的Lomb-Scargle层计算不规则数据的功率谱，并集成到基于分数的扩散模型（LSCD）中，实现频谱引导的时间序列填补。

Result: 在合成和真实数据集上，LSCD比纯时域基线更准确地恢复缺失数据，同时提供一致的频谱估计。

Conclusion: 该方法可轻松集成到学习框架中，为处理不完整或不规则数据提供了频谱引导的新思路。

Abstract: Time series with missing or irregularly sampled data are a persistent
challenge in machine learning. Many methods operate on the frequency-domain,
relying on the Fast Fourier Transform (FFT) which assumes uniform sampling,
therefore requiring prior interpolation that can distort the spectra. To
address this limitation, we introduce a differentiable Lomb--Scargle layer that
enables a reliable computation of the power spectrum of irregularly sampled
data. We integrate this layer into a novel score-based diffusion model (LSCD)
for time series imputation conditioned on the entire signal spectrum.
Experiments on synthetic and real-world benchmarks demonstrate that our method
recovers missing data more accurately than purely time-domain baselines, while
simultaneously producing consistent frequency estimates. Crucially, our method
can be easily integrated into learning frameworks, enabling broader adoption of
spectral guidance in machine learning approaches involving incomplete or
irregular data.

</details>


### [203] [MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection](https://arxiv.org/abs/2506.17041)
*Joshua Schraven,Alexander Windmann,Oliver Niggemann*

Main category: cs.LG

TL;DR: 论文提出了MAWIFlow，一个基于MAWILab v1.1数据集的流量基准，用于更真实和可复现的异常检测评估。通过传统机器学习与深度学习模型（CNN-BiLSTM）的比较，发现树模型在静态数据表现好但随时间性能下降，而CNN-BiLSTM泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 现有网络入侵检测的基准数据集多为合成流量，无法反映实际环境中的统计变异和时间漂移，因此需要更真实的评估工具。

Method: 提出MAWIFlow数据集，通过可复现的预处理流程将原始数据包转换为CICFlowMeter格式的流量表示，并保留异常标签。比较了传统机器学习模型（如决策树、随机森林）和CNN-BiLSTM模型。

Result: 树模型在静态数据表现良好但随时间性能下降，而CNN-BiLSTM模型泛化能力更强，性能更稳定。

Conclusion: 合成基准和静态模型存在局限性，应采用具有明确时间结构的真实数据集。所有数据和代码公开以促进透明性和可复现性。

Abstract: Benchmark datasets for network intrusion detection commonly rely on
synthetically generated traffic, which fails to reflect the statistical
variability and temporal drift encountered in operational environments. This
paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1
dataset, designed to enable realistic and reproducible evaluation of anomaly
detection methods. A reproducible preprocessing pipeline is presented that
transforms raw packet captures into flow representations conforming to the
CICFlowMeter format, while preserving MAWILab's original anomaly labels. The
resulting datasets comprise temporally distinct samples from January 2011,
2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods,
including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are
compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical
results demonstrate that tree-based classifiers perform well on temporally
static data but experience significant performance degradation over time. In
contrast, the CNN-BiLSTM model maintains better performance, thus showing
improved generalization. These findings underscore the limitations of synthetic
benchmarks and static models, and motivate the adoption of realistic datasets
with explicit temporal structure. All datasets, pipeline code, and model
implementations are made publicly available to foster transparency and
reproducibility.

</details>


### [204] [Navigating the Deep: Signature Extraction on Deep Neural Networks](https://arxiv.org/abs/2506.17047)
*Haolin Liu,Adrien Siproudhis,Samuel Experton,Peter Lorenz,Christina Boura,Thomas Peyrin*

Main category: cs.LG

TL;DR: 本文改进了神经网络模型提取中的签名提取方法，解决了先前方法的局限性，如秩不足和噪声传播，显著提升了提取深度和准确性。


<details>
  <summary>Details</summary>
Motivation: 针对Carlini等人方法的局限性，如仅适用于浅层网络，本文旨在改进签名提取过程，以支持更深层网络的提取。

Method: 提出高效算法解决秩不足和噪声传播问题，优化签名提取过程。

Result: 在CIFAR-10数据集上，成功提取了八层网络的签名，准确率超过95%，远超先前方法的三层限制。

Conclusion: 本文方法为攻击更复杂神经网络架构提供了重要进展。

Abstract: Neural network model extraction has emerged in recent years as an important
security concern, as adversaries attempt to recover a network's parameters via
black-box queries. A key step in this process is signature extraction, which
aims to recover the absolute values of the network's weights layer by layer.
Prior work, notably by Carlini et al. (2020), introduced a technique inspired
by differential cryptanalysis to extract neural network parameters. However,
their method suffers from several limitations that restrict its applicability
to networks with a few layers only. Later works focused on improving sign
extraction, but largely relied on the assumption that signature extraction
itself was feasible.
  In this work, we revisit and refine the signature extraction process by
systematically identifying and addressing for the first time critical
limitations of Carlini et al.'s signature extraction method. These limitations
include rank deficiency and noise propagation from deeper layers. To overcome
these challenges, we propose efficient algorithmic solutions for each of the
identified issues, greatly improving the efficiency of signature extraction.
Our approach permits the extraction of much deeper networks than was previously
possible. We validate our method through extensive experiments on ReLU-based
neural networks, demonstrating significant improvements in extraction depth and
accuracy. For instance, our extracted network matches the target network on at
least 95% of the input space for each of the eight layers of a neural network
trained on the CIFAR-10 dataset, while previous works could barely extract the
first three layers. Our results represent a crucial step toward practical
attacks on larger and more complex neural network architectures.

</details>


### [205] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: 论文提出了一种名为SAMD的方法，用于将复杂概念映射到Transformer模型的特定注意力头，并通过SAMI方法调整这些模块的效果。实验展示了该方法在语言和视觉任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在语言和视觉任务中表现出色，但对其内部机制的解释仍不足，尤其是对注意力机制的影响和复杂概念的分析缺乏统一方法。

Method: 提出SAMD方法，将概念表示为向量，计算其与注意力头的余弦相似度，选择TopK得分头构建概念相关模块；进一步提出SAMI方法，通过标量参数调整模块效果。

Result: 实验证明SAMD能稳定定位模块，SAMI在增强或抑制概念效果上有效，如提升GSM8K性能1.6%，降低ImageNet分类准确率。

Conclusion: SAMD和SAMI提供了一种通用方法，可解释和干预Transformer模型的行为，适用于语言和视觉任务。

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [206] [Flow-Based Non-stationary Temporal Regime Causal Structure Learning](https://arxiv.org/abs/2506.17065)
*Abdellah Rahmani,Pascal Frossard*

Main category: cs.LG

TL;DR: FANTOM是一个统一框架，用于处理非平稳过程、非高斯和异方差噪声的因果发现，同时推断多个因果结构及其边界。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中的因果关系分析在金融和神经科学等领域至关重要，但现有方法无法处理非平稳性和复杂噪声分布。

Method: FANTOM使用贝叶斯期望最大化算法，最大化数据对数似然的证据下界，同时推断因果结构和边界。

Result: 理论证明FANTOM在平稳和非平稳设置下可识别，实验显示其优于现有方法。

Conclusion: FANTOM为复杂时间序列的因果发现提供了有效解决方案。

Abstract: Understanding causal relationships in multivariate time series is crucial in
many scenarios, such as those dealing with financial or neurological data. Many
such time series exhibit multiple regimes, i.e., consecutive temporal segments
with a priori unknown boundaries, with each regime having its own causal
structure. Inferring causal dependencies and regime shifts is critical for
analyzing the underlying processes. However, causal structure learning in this
setting is challenging due to (1) non stationarity, i.e., each regime can have
its own causal graph and mixing function, and (2) complex noise distributions,
which may be non Gaussian or heteroscedastic. Existing causal discovery
approaches cannot address these challenges, since generally assume stationarity
or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified
framework for causal discovery that handles non stationary processes along with
non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the
number of regimes and their corresponding indices and learns each regime's
Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm
that maximizes the evidence lower bound of the data log likelihood. On the
theoretical side, we prove, under mild assumptions, that temporal
heteroscedastic causal models, introduced in FANTOM's formulation, are
identifiable in both stationary and non stationary settings. In addition,
extensive experiments on synthetic and real data show that FANTOM outperforms
existing methods.

</details>


### [207] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Clara Dérand,Ricardo Borsoi,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文全面分析了深度多项式神经网络（PNNs）的可识别性，揭示了激活度与层宽度之间的复杂关系，并解决了关于PNNs神经变种预期维度的开放猜想。


<details>
  <summary>Details</summary>
Motivation: 多项式神经网络（PNNs）具有丰富的代数和几何结构，但其可识别性（确保可解释性的关键属性）仍未被充分理解。

Method: 通过将深度PNNs与低秩张量分解及Kruskal型唯一性定理联系起来，提出了构造性证明方法。

Result: 结果表明，在温和条件下，层宽度非递增的架构通常是可识别的，而解码器宽度增长不过快的编码器-解码器网络也是可识别的。

Conclusion: 研究不仅提供了由架构决定的通用条件，还给出了依赖于网络参数的有效条件，并解决了PNNs神经变种预期维度的开放猜想。

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly. Our proofs are constructive and center on a connection
between deep PNNs and low-rank tensor decompositions, and Kruskal-type
uniqueness theorems. This yields both generic conditions determined by the
architecture, and effective conditions that depend on the network's parameters.
We also settle an open conjecture on the expected dimension of PNN's
neurovarieties, and provide new bounds on the activation degrees required for
it to reach its maximum.

</details>


### [208] [TransDreamerV3: Implanting Transformer In DreamerV3](https://arxiv.org/abs/2506.17103)
*Shruti Sadanand Dongare,Amun Kharel,Jonathan Samuel,Xiaona Zhou*

Main category: cs.LG

TL;DR: TransDreamerV3通过集成Transformer编码器改进了DreamerV3架构，提升了复杂环境中的记忆与决策能力。


<details>
  <summary>Details</summary>
Motivation: 提升DreamerV3在复杂环境中的性能，特别是在记忆和决策方面。

Method: 在DreamerV3基础上集成Transformer编码器，并在多个任务（如Atari和Crafter）上进行实验。

Result: 在Atari-Freeway和Crafter任务中表现优于DreamerV3，但在Minecraft任务中存在不足。

Conclusion: TransDreamerV3展示了基于世界模型的强化学习的进步，尤其是在利用Transformer架构方面。

Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that
enhances the DreamerV3 architecture by integrating a transformer encoder. The
model is designed to improve memory and decision-making capabilities in complex
environments. We conducted experiments on Atari-Boxing, Atari-Freeway,
Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved
performance over DreamerV3, particularly in the Atari-Freeway and Crafter
tasks. While issues in the Minecraft task and limited training across all tasks
were noted, TransDreamerV3 displays advancement in world model-based
reinforcement learning, leveraging transformer architectures.

</details>


### [209] [Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model](https://arxiv.org/abs/2506.17128)
*Botao Zhu,Xianbin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Siamese模型的快速连续信任评估框架（SRCTE），用于解决协作系统中动态信任评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 在协作系统中，快速且持续地评估合作者的信任度是一个重要挑战，尤其是在分布式设备、复杂环境和动态资源的情况下。

Method: 通过属性控制流图（ACFG）表示合作者的资源属性和历史数据，利用Siamese模型学习ACFG的深层语义并生成嵌入，最后计算相似度以确定信任值。

Result: 实验结果表明，SRCTE仅需少量数据即可快速收敛，并在异常信任检测率上优于基线算法。

Conclusion: SRCTE框架为协作系统中的动态信任评估提供了一种有效解决方案。

Abstract: Trust is emerging as an effective tool to ensure the successful completion of
collaborative tasks within collaborative systems. However, rapidly and
continuously evaluating the trustworthiness of collaborators during task
execution is a significant challenge due to distributed devices, complex
operational environments, and dynamically changing resources. To tackle this
challenge, this paper proposes a Siamese-enabled rapid and continuous trust
evaluation framework (SRCTE) to facilitate effective task collaboration. First,
the communication and computing resource attributes of the collaborator in a
trusted state, along with historical collaboration data, are collected and
represented using an attributed control flow graph (ACFG) that captures
trust-related semantic information and serves as a reference for comparison
with data collected during task execution. At each time slot of task execution,
the collaborator's communication and computing resource attributes, as well as
task completion effectiveness, are collected in real time and represented with
an ACFG to convey their trust-related semantic information. A Siamese model,
consisting of two shared-parameter Structure2vec networks, is then employed to
learn the deep semantics of each pair of ACFGs and generate their embeddings.
Finally, the similarity between the embeddings of each pair of ACFGs is
calculated to determine the collaborator's trust value at each time slot. A
real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to
test the effectiveness of the proposed SRCTE framework. Experimental results
demonstrate that SRCTE converges rapidly with only a small amount of data and
achieves a high anomaly trust detection rate compared to the baseline
algorithm.

</details>


### [210] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Main category: cs.LG

TL;DR: 扩散模型在生物化学等领域表现出色，但在小时间步长下与Fokker-Planck方程不一致。作者提出了一种基于能量的扩散模型，通过正则化项增强一致性，并在多个系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成平衡分子构象和相关力方面表现优异，但在用于粗粒度分子动力学模拟时，发现生成样本与模拟结果不一致，尤其是在小时间步长下。

Method: 提出了一种基于能量的扩散模型，引入Fokker-Planck方程衍生的正则化项，以强制模型满足一致性要求。

Result: 在玩具系统、丙氨酸二肽等实验中验证了方法的有效性，并开发了一种支持模拟的先进可转移Boltzmann模拟器。

Conclusion: 通过正则化项改进的扩散模型显著提高了生成样本与模拟结果的一致性，为高效采样提供了新工具。

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


### [211] [Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity](https://arxiv.org/abs/2506.17155)
*Samin Yeasar Arnob,Scott Fujimoto,Doina Precup*

Main category: cs.LG

TL;DR: 论文研究了小数据集在离线强化学习中的应用，提出了一种基于稀疏性的正则化方法Sparse-Reg，以解决过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 许多离线强化学习应用依赖小数据集，但现有算法容易在小数据集上过拟合，导致性能下降。

Method: 引入Sparse-Reg，一种基于稀疏性的正则化技术，用于缓解离线强化学习中的过拟合问题。

Result: Sparse-Reg在有限数据设置下表现优异，优于现有基线方法。

Conclusion: Sparse-Reg能有效解决小数据集中的过拟合问题，提升离线强化学习性能。

Abstract: In this paper, we investigate the use of small datasets in the context of
offline reinforcement learning (RL). While many common offline RL benchmarks
employ datasets with over a million data points, many offline RL applications
rely on considerably smaller datasets. We show that offline RL algorithms can
overfit on small datasets, resulting in poor performance. To address this
challenge, we introduce "Sparse-Reg": a regularization technique based on
sparsity to mitigate overfitting in offline reinforcement learning, enabling
effective learning in limited data settings and outperforming state-of-the-art
baselines in continuous control.

</details>


### [212] [Deep generative models as the probability transformation functions](https://arxiv.org/abs/2506.17171)
*Vitalii Bondar,Vira Babenko,Roman Trembovetskyi,Yurii Korobeinyk,Viktoriya Dzyuba*

Main category: cs.LG

TL;DR: 论文提出了一种统一的理论视角，将深度生成模型视为概率变换函数，揭示了不同生成模型之间的共性。


<details>
  <summary>Details</summary>
Motivation: 尽管各种生成模型在架构和训练方法上存在差异，但它们的核心目标都是通过变换简单分布生成复杂数据分布。这种统一视角有助于方法改进和理论发展。

Method: 通过理论分析，将多种生成模型（如自编码器、自回归模型、GAN、归一化流、扩散模型等）统一为概率变换函数。

Result: 揭示了不同生成模型在概率变换上的共同本质，为方法迁移和理论发展提供了基础。

Conclusion: 统一视角有望推动生成模型技术的效率与效果提升。

Abstract: This paper introduces a unified theoretical perspective that views deep
generative models as probability transformation functions. Despite the apparent
differences in architecture and training methodologies among various types of
generative models - autoencoders, autoregressive models, generative adversarial
networks, normalizing flows, diffusion models, and flow matching - we
demonstrate that they all fundamentally operate by transforming simple
predefined distributions into complex target data distributions. This unifying
perspective facilitates the transfer of methodological improvements between
model architectures and provides a foundation for developing universal
theoretical approaches, potentially leading to more efficient and effective
generative modeling techniques.

</details>


### [213] [Variational Learning of Disentangled Representations](https://arxiv.org/abs/2506.17182)
*Yuli Slavutsky,Ozgur Beker,David Blei,Bianca Dumitrascu*

Main category: cs.LG

TL;DR: DISCoVeR是一种新的变分框架，通过双潜在架构和最大最小目标，有效分离条件不变和条件特定的因素，提升多条件下解耦表示的学习效果。


<details>
  <summary>Details</summary>
Motivation: 在生物医学数据分析等领域，需要将稳定信号与上下文依赖效应分离以推广到新条件。现有VAE扩展方法存在潜在表示泄漏问题，限制了泛化能力。

Method: DISCoVeR采用双潜在架构分别建模共享和特定因素，并行重建确保信息完整性，并引入最大最小目标实现干净分离。

Result: DISCoVeR在合成数据集、自然图像和单细胞RNA-seq数据上表现出更好的解耦性能。

Conclusion: DISCoVeR为多条件下学习解耦表示提供了理论基础和实用方法。

Abstract: Disentangled representations enable models to separate factors of variation
that are shared across experimental conditions from those that are
condition-specific. This separation is essential in domains such as biomedical
data analysis, where generalization to new treatments, patients, or species
depends on isolating stable biological signals from context-dependent effects.
While extensions of the variational autoencoder (VAE) framework have been
proposed to address this problem, they frequently suffer from leakage between
latent representations, limiting their ability to generalize to unseen
conditions. Here, we introduce DISCoVeR, a new variational framework that
explicitly separates condition-invariant and condition-specific factors.
DISCoVeR integrates three key components: (i) a dual-latent architecture that
models shared and specific factors separately; (ii) two parallel
reconstructions that ensure both representations remain informative; and (iii)
a novel max-min objective that encourages clean separation without relying on
handcrafted priors, while making only minimal assumptions. Theoretically, we
show that this objective maximizes data likelihood while promoting
disentanglement, and that it admits a unique equilibrium. Empirically, we
demonstrate that DISCoVeR achieves improved disentanglement on synthetic
datasets, natural images, and single-cell RNA-seq data. Together, these results
establish DISCoVeR as a principled approach for learning disentangled
representations in multi-condition settings.

</details>


### [214] [Optimal Implicit Bias in Linear Regression](https://arxiv.org/abs/2506.17187)
*Kanumuri Nithin Varma,Babak Hassibi*

Main category: cs.LG

TL;DR: 论文研究了过参数化学习问题中优化算法的隐式偏差对泛化性能的影响，并找到了最优隐式偏差。


<details>
  <summary>Details</summary>
Motivation: 在过参数化学习中，训练损失通常有无限多全局最优解，但泛化性能各异。研究目标是找到能带来最佳泛化性能的隐式偏差。

Method: 通过凸函数/势能的最小化，对非各向同性高斯数据的过参数化线性回归进行精确渐近分析。

Result: 获得了该类插值器的最佳泛化误差的紧下界，并找到了在某些条件下达到该下界的最优凸隐式偏差。

Conclusion: 研究确定了最优隐式偏差的条件，为过参数化学习中的泛化性能优化提供了理论支持。

Abstract: Most modern learning problems are over-parameterized, where the number of
learnable parameters is much greater than the number of training data points.
In this over-parameterized regime, the training loss typically has infinitely
many global optima that completely interpolate the data with varying
generalization performance. The particular global optimum we converge to
depends on the implicit bias of the optimization algorithm. The question we
address in this paper is, ``What is the implicit bias that leads to the best
generalization performance?". To find the optimal implicit bias, we provide a
precise asymptotic analysis of the generalization performance of interpolators
obtained from the minimization of convex functions/potentials for
over-parameterized linear regression with non-isotropic Gaussian data. In
particular, we obtain a tight lower bound on the best generalization error
possible among this class of interpolators in terms of the
over-parameterization ratio, the variance of the noise in the labels, the
eigenspectrum of the data covariance, and the underlying distribution of the
parameter to be estimated. Finally, we find the optimal convex implicit bias
that achieves this lower bound under certain sufficient conditions involving
the log-concavity of the distribution of a Gaussian convolved with the prior of
the true underlying parameter.

</details>


### [215] [Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning](https://arxiv.org/abs/2506.17204)
*Guozheng Ma,Lu Li,Zilin Wang,Li Shen,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 通过一次性随机剪枝引入静态网络稀疏性，可以提升深度强化学习的扩展潜力，提高参数效率并增强对优化挑战的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在扩展时容易因网络训练中的病态问题而难以提升性能，需要复杂的干预措施。本文探索了通过简单稀疏化方法解决这一问题。

Method: 采用一次性随机剪枝技术，在训练前随机移除一定比例的权重，形成稀疏网络。

Result: 稀疏网络在参数效率和优化稳定性上优于密集网络，且在视觉和流式强化学习场景中表现一致。

Conclusion: 静态网络稀疏性是一种简单有效的方法，能够显著提升深度强化学习模型的扩展潜力。

Abstract: Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

</details>


### [216] [BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning](https://arxiv.org/abs/2506.17211)
*Xuechen Zhang,Zijian Huang,Yingcong Li,Chenshun Ni,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: 论文提出BREAD方法，通过部分专家指导和分支展开，解决了小语言模型在复杂推理任务中的学习难题，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）在复杂推理任务中表现不佳，尤其是在高质量训练数据稀缺或难以学习的情况下。传统的SFT + RL训练范式存在根本性限制，导致模型无法表达专家轨迹或初始化成功率极低。

Method: 提出BREAD方法，结合部分专家指导和分支展开，动态插入专家提示以完成推理路径，确保每次更新至少包含一条成功轨迹。

Result: BREAD仅需不到40%的真实轨迹，性能优于标准GRPO，训练速度提升约3倍，并能解决传统方法无法解决的问题。

Conclusion: BREAD通过分支展开和专家指导，显著提升了小语言模型的推理能力，为SLM训练提供了新思路。

Abstract: Small language models (SLMs) struggle to learn complex reasoning behaviors,
especially when high-quality traces are scarce or difficult to learn from. The
standard training approach combines a supervised fine-tuning (SFT) stage, often
to distill capabilities of a larger model, followed by a reinforcement learning
(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we
investigate the fundamental limitations of this SFT + RL paradigm and propose
methods to overcome them. Under a suitable theoretical model, we demonstrate
that the SFT + RL strategy can fail completely when (1) the expert's traces are
too difficult for the small model to express, or (2) the small model's
initialization has exponentially small likelihood of success. To address these,
we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via
partial expert guidance and branched rollouts. When self-generated traces fail,
BREAD adaptively inserts short expert prefixes/hints, allowing the small model
to complete the rest of the reasoning path, and ensuring that each update
includes at least one successful trace. This mechanism both densifies the
reward signal and induces a natural learning curriculum. BREAD requires fewer
than 40% of ground-truth traces, consistently outperforming standard GRPO while
speeding up the training by about 3 times. Importantly, we demonstrate that
BREAD helps the model solve problems that are otherwise unsolvable by the SFT +
RL strategy, highlighting how branched rollouts and expert guidance can
substantially boost SLM reasoning.

</details>


### [217] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219)
*Yanzhi Zhang,Zhaoxi Zhang,Haoxiang Guan,Yilin Cheng,Yitong Duan,Chen Wang,Yue Wang,Shuxin Zheng,Jiyan He*

Main category: cs.LG

TL;DR: 论文提出了一种基于内部反馈的强化学习方法（RLIF），用于提升大型语言模型的推理能力，无需外部监督。实验表明RLIF在训练初期能提升性能，但随着训练深入性能下降，且对已指令调优的模型效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如RLHF和RLVR）依赖外部监督，RLIF旨在探索仅依赖模型内部信号的方法。

Method: 利用无监督奖励代理（如词级熵、轨迹级熵和自确定性）设计RLIF策略，并在数学推理任务中验证。

Result: RLIF在训练初期能匹配或超越RLVR，但随着训练深入性能下降，且对指令调优模型效果有限。

Conclusion: RLIF在特定阶段有效，但需谨慎使用，研究为LLM后训练提供了内部反馈的实践指导。

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [218] [Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation](https://arxiv.org/abs/2506.15753)
*Oluwaseyi Giwa,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: quant-ph

TL;DR: QPPG是一种基于自然梯度的算法，通过量子Fisher信息预条件化策略更新，在噪声环境下实现稳定学习，收敛速度比REINFORCE快4倍。


<details>
  <summary>Details</summary>
Motivation: 解决量子强化学习中噪声环境下的学习稳定性问题，并提升收敛速度。

Method: 利用全逆量子Fisher信息与Tikhonov正则化进行策略更新的预条件化，结合经典与量子几何。

Result: 在经典和量子环境中，QPPG收敛速度快4倍，噪声下保持1 dB增益，100次迭代内达到90%回报。

Conclusion: QPPG展示了基于量子Fisher信息的预条件化在可扩展量子强化学习中的优势。

Abstract: In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a
natural gradient-based algorithm for link adaptation that whitens policy
updates using the full inverse quantum Fisher information with Tikhonov
regularization. QPPG bridges classical and quantum geometry, achieving stable
learning even under noise. Evaluated on classical and quantum environments,
including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG
converges 4 times faster than REINFORCE and sustains a 1 dB gain under
uncertainty. It reaches a 90 percent return in one hundred episodes with high
noise robustness, showcasing the advantages of full QFI-based preconditioning
for scalable quantum reinforcement learning.

</details>


### [219] [Compilation, Optimization, Error Mitigation, and Machine Learning in Quantum Algorithms](https://arxiv.org/abs/2506.15760)
*Shuangbao Paul Wang,Jianzhou Mao,Eric Sakk*

Main category: quant-ph

TL;DR: 论文讨论了量子算法的编译、优化和错误缓解，这些是实现实际量子算法的关键步骤。通过结合QPU和CPU/GPU的混合平台，量子算法能够利用高性能计算能力实现指数级加速。提出的近似量子傅里叶变换（AQFT）进一步优化了电路执行性能。


<details>
  <summary>Details</summary>
Motivation: 量子算法在实际应用中需要高效的编译、优化和错误缓解技术，以充分发挥其潜力。

Method: 提出了一种近似量子傅里叶变换（AQFT）方法，用于优化量子算法的电路执行。

Result: AQFT在量子傅里叶变换提供的指数级加速基础上，进一步提升了电路执行效率。

Conclusion: 通过混合平台和AQFT方法，量子算法的实际执行性能得到了显著提升。

Abstract: This paper discusses the compilation, optimization, and error mitigation of
quantum algorithms, essential steps to execute real-world quantum algorithms.
Quantum algorithms running on a hybrid platform with QPU and CPU/GPU take
advantage of existing high-performance computing power with quantum-enabled
exponential speedups. The proposed approximate quantum Fourier transform (AQFT)
for quantum algorithm optimization improves the circuit execution on top of an
exponential speed-ups the quantum Fourier transform has provided.

</details>


### [220] [Superconducting Qubit Readout Using Next-Generation Reservoir Computing](https://arxiv.org/abs/2506.15771)
*Robert Kent,Benjamin Lienhard,Gregory Lafyatis,Daniel J. Gauthier*

Main category: quant-ph

TL;DR: 论文提出了一种基于下一代储层计算的机器学习方法，用于提高量子比特状态识别的效率和保真度，同时降低计算复杂性和串扰。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特的读出是量子处理器的瓶颈，传统方法难以处理频率复用读出中的串扰问题，而现有神经网络方法计算成本高且扩展性差。

Method: 采用储层计算技术，通过构建测量信号的多项式特征并将其映射到对应的量子比特状态，避免了神经网络中昂贵的非线性激活函数。

Result: 相比传统方法，单量子比特和五量子比特数据集的错误率分别降低了50%和11%，串扰减少了2.5倍；与现有机器学习方法相比，计算量显著减少。

Conclusion: 储层计算能够在保持高保真度的同时提升量子比特状态识别的可扩展性，为未来量子处理器提供了高效解决方案。

Abstract: Quantum processors require rapid and high-fidelity simultaneous measurements
of many qubits. While superconducting qubits are among the leading modalities
toward a useful quantum processor, their readout remains a bottleneck.
Traditional approaches to processing measurement data often struggle to account
for crosstalk present in frequency-multiplexed readout, the preferred method to
reduce the resource overhead. Recent approaches to address this challenge use
neural networks to improve the state-discrimination fidelity. However, they are
computationally expensive to train and evaluate, resulting in increased latency
and poor scalability as the number of qubits increases. We present an
alternative machine learning approach based on next-generation reservoir
computing that constructs polynomial features from the measurement signals and
maps them to the corresponding qubit states. This method is highly
parallelizable, avoids the costly nonlinear activation functions common in
neural networks, and supports real-time training, enabling fast evaluation,
adaptability, and scalability. Despite its lower computational complexity, our
reservoir approach is able to maintain high qubit-state-discrimination
fidelity. Relative to traditional methods, our approach achieves error
reductions of up to 50% and 11% on single- and five-qubit datasets,
respectively, and delivers up to 2.5x crosstalk reduction on the five-qubit
dataset. Compared with recent machine-learning methods, evaluating our model
requires 100x fewer multiplications for single-qubit and 2.5x fewer for
five-qubit models. This work demonstrates that reservoir computing can enhance
qubit-state discrimination while maintaining scalability for future quantum
processors.

</details>


### [221] [Feedback-driven recurrent quantum neural network universality](https://arxiv.org/abs/2506.16332)
*Lukas Gonon,Rodrigo Martínez-Peña,Juan-Pablo Ortega*

Main category: quant-ph

TL;DR: 本文提出了一种基于反馈的量子储层计算架构，解决了早期协议在实时处理和计算开销上的问题，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 早期量子储层计算协议（如重启和回绕协议）因重复步骤导致实时处理能力受限和计算开销增加，需要新的方法来解决这些问题。

Method: 提出了一种基于反馈的循环量子神经网络架构，扩展了现有前馈模型，并提供了近似边界和普适性理论保证。

Result: 该模型在具有线性读出时具有普适性，既强大又易于实验实现。

Conclusion: 研究为具有实时处理能力的量子储层计算提供了理论和实践基础。

Abstract: Quantum reservoir computing uses the dynamics of quantum systems to process
temporal data, making it particularly well-suited for learning with noisy
intermediate-scale quantum devices. Early experimental proposals, such as the
restarting and rewinding protocols, relied on repeating previous steps of the
quantum map to avoid backaction. However, this approach compromises real-time
processing and increases computational overhead. Recent developments have
introduced alternative protocols that address these limitations. These include
online, mid-circuit measurement, and feedback techniques, which enable
real-time computation while preserving the input history. Among these, the
feedback protocol stands out for its ability to process temporal information
with comparatively fewer components. Despite this potential advantage, the
theoretical foundations of feedback-based quantum reservoir computing remain
underdeveloped, particularly with regard to the universality and the
approximation capabilities of this approach. This paper addresses this issue by
presenting a recurrent quantum neural network architecture that extends a class
of existing feedforward models to a dynamic, feedback-driven reservoir setting.
We provide theoretical guarantees for variational recurrent quantum neural
networks, including approximation bounds and universality results. Notably, our
analysis demonstrates that the model is universal with linear readouts, making
it both powerful and experimentally accessible. These results pave the way for
practical and theoretically grounded quantum reservoir computing with real-time
processing capabilities.

</details>


### [222] [Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test](https://arxiv.org/abs/2506.16938)
*Sebastian Nagies,Emiliano Tolotti,Davide Pastorello,Enrico Blanzieri*

Main category: quant-ph

TL;DR: 论文研究了基于SWAP测试电路的量子神经网络（QNN），发现其数学上等效于具有二次激活函数的经典两层前馈网络，但存在表达能力限制。通过引入广义SWAP测试电路，解决了原始架构无法处理高维奇偶校验函数的问题。


<details>
  <summary>Details</summary>
Motivation: 探讨量子神经网络与经典模型的联系，以提升量子机器学习的能力。

Method: 分析基于SWAP测试电路的QNN，并引入广义SWAP测试电路以增强表达能力。

Result: 原始架构在奇偶校验函数等高难度任务上表现不佳，改进后的架构成功解决了这一问题。

Conclusion: 通过经典任务分析增强QNN表达能力，展示了基于SWAP测试的架构在量子学习任务中的潜力。

Abstract: Parameterized quantum circuits represent promising architectures for machine
learning applications, yet many lack clear connections to classical models,
potentially limiting their ability to translate the wide success of classical
neural networks to the quantum realm. We examine a specific type of quantum
neural network (QNN) built exclusively from SWAP test circuits, and discuss its
mathematical equivalence to a classical two-layer feedforward network with
quadratic activation functions under amplitude encoding. Our analysis across
classical real-world and synthetic datasets reveals that while this
architecture can successfully learn many practical tasks, it exhibits
fundamental expressivity limitations due to violating the universal
approximation theorem, particularly failing on harder problems like the parity
check function. To address this limitation, we introduce a circuit modification
using generalized SWAP test circuits that effectively implements classical
neural networks with product layers. This enhancement enables successful
learning of parity check functions in arbitrary dimensions which we
analytically argue to be impossible for the original architecture beyond two
dimensions regardless of network size. Our results establish a framework for
enhancing QNN expressivity through classical task analysis and demonstrate that
our SWAP test-based architecture offers broad representational capacity,
suggesting potential promise also for quantum learning tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [223] [A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures](https://arxiv.org/abs/2506.15875)
*Dirk Van Essendelft,Patrick Wingo,Terry Jordan,Ryan Smith,Wissam Saidi*

Main category: cs.PL

TL;DR: MACH是一种新型编译器，专为大规模并行、空间数据流架构设计，同时支持传统统一内存设备。它通过虚拟机和领域特定语言简化编译过程。


<details>
  <summary>Details</summary>
Motivation: 解决空间架构编译的复杂性，提供跨架构灵活性。

Method: 采用虚拟机概念、领域特定语言，将高级语言编译为机器特定代码。

Result: 成功展示了在NumPy密集张量上的应用，并针对Cerebras硬件语言进行编译。

Conclusion: MACH为空间架构编译提供了高效灵活的解决方案。

Abstract: We have developed a novel compiler called the Multiple-Architecture Compiler
for Advanced Computing Hardware (MACH) designed specifically for
massively-parallel, spatial, dataflow architectures like the Wafer Scale
Engine. Additionally, MACH can execute code on traditional unified-memory
devices. MACH addresses the complexities in compiling for spatial architectures
through a conceptual Virtual Machine, a flexible domain-specific language, and
a compiler that can lower high-level languages to machine-specific code in
compliance with the Virtual Machine concept. While MACH is designed to be
operable on several architectures and provide the flexibility for several
standard and user-defined data mappings, we introduce the concept with dense
tensor examples from NumPy and show lowering to the Wafer Scale Engine by
targeting Cerebras' hardware specific languages.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [224] [Universal Music Representations? Evaluating Foundation Models on World Music Corpora](https://arxiv.org/abs/2506.17055)
*Charilaos Papaioannou,Emmanouil Benetos,Alexandros Potamianos*

Main category: cs.SD

TL;DR: 本文评估了五种音频基础模型在六种音乐传统中的跨文化泛化能力，发现较大模型在非西方音乐上表现更好，但文化差异仍影响性能。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型在不同音乐传统中的泛化能力，填补跨文化音乐信息检索的研究空白。

Method: 采用三种方法：探测模型固有表示、目标监督微调1-2层、多标签少样本学习。

Result: 模型在五种数据集上达到最优性能，但文化距离较远的传统表现下降。

Conclusion: 基础模型已具备丰富音乐知识，但仍需改进以实现通用音乐表示。

Abstract: Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [225] [RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains](https://arxiv.org/abs/2506.15756)
*João G. Ribeiro,Yaniv Oren,Alberto Sardinha,Matthijs Spaan,Francisco S. Melo*

Main category: cs.MA

TL;DR: RecBayes是一种新的方法，用于在部分可观测环境下实现即时团队协作，无需依赖环境状态或队友动作。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如PO-GPL、FEAT、ATPO）需要完全可观测状态或队友动作的局限性，或仅适用于小规模环境的问题。

Method: 使用基于过去经验的循环贝叶斯分类器，仅通过观测识别已知团队和任务。

Result: 在扩展到1M状态和2^125观测的大规模环境中，RecBayes能有效识别团队和任务，并协助完成任务。

Conclusion: RecBayes在无需环境状态或队友动作的情况下，成功解决了大规模部分可观测环境中的即时团队协作问题。

Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under
partial observability, a setting where agents are deployed on-the-fly to
environments where pre-existing teams operate, that never requires, at any
stage, access to the states of the environment or the actions of its teammates.
We show that by relying on a recurrent Bayesian classifier trained using past
experiences, an ad hoc agent is effectively able to identify known teams and
tasks being performed from observations alone. Unlike recent approaches such as
PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some
stage fully observable states of the environment, actions of teammates, or
both, or approaches such as ATPO (Ribeiro et al., 2023) that require the
environments to be small enough to be tabularly modelled (Ribeiro et al.,
2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes
is both able to handle arbitrarily large spaces while never relying on either
states and teammates' actions. Our results in benchmark domains from the
multi-agent systems literature, adapted for partial observability and scaled up
to 1M states and 2^125 observations, show that RecBayes is effective at
identifying known teams and tasks being performed from partial observations
alone, and as a result, is able to assist the teams in solving the tasks
effectively.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [226] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Main category: eess.IV

TL;DR: 论文提出了PM Dice loss，通过像素级调制项改进Dice loss，以同时解决类别不平衡和难度不平衡问题，在医学分割任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡和难度不平衡会影响神经网络在医学分割任务中的性能，现有方法计算成本高且效果有限。

Method: 提出Pixel-wise Modulated Dice loss（PM Dice loss），通过像素级调制项改进Dice loss，以低计算成本解决难度不平衡问题。

Result: 在三种常用医学分割任务中，PM Dice loss优于其他针对难度不平衡的方法。

Conclusion: PM Dice loss是一种简单有效的改进方法，能同时处理类别和难度不平衡问题。

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [227] [Implicit neural representations for accurate estimation of the standard model of white matter](https://arxiv.org/abs/2506.15762)
*Tom Hendriks,Gerrit Arends,Edwin Versteeg,Anna Vilanova,Maxime Chamberland,Chantal M. W. Tax*

Main category: eess.IV

TL;DR: 该论文提出了一种基于隐式神经表示（INR）的新框架，用于估计白质标准模型（SM）参数，显著提高了在低信噪比条件下的准确性，并支持空间上采样和无监督学习。


<details>
  <summary>Details</summary>
Motivation: 白质标准模型（SM）的高维特性导致参数估计困难，需要复杂的数据采集协议。噪声问题进一步加剧了准确估计的挑战。

Method: 采用隐式神经表示（INR）方法，通过输入坐标的正弦编码实现空间正则化，并与立方多项式、监督神经网络和非线性最小二乘法进行比较。

Result: INR方法在估计SM参数时表现出更高的准确性，尤其在低信噪比条件下，并支持空间连续表示。

Conclusion: INR框架为分析和解释扩散MRI数据提供了重要工具，具有快速推理、鲁棒性和适应性强的特点。

Abstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive
investigation of tissue microstructure. The Standard Model (SM) of white matter
aims to disentangle dMRI signal contributions from intra- and extra-axonal
water compartments. However, due to the model its high-dimensional nature,
extensive acquisition protocols with multiple b-values and diffusion tensor
shapes are typically required to mitigate parameter degeneracies. Even then,
accurate estimation remains challenging due to noise. This work introduces a
novel estimation framework based on implicit neural representations (INRs),
which incorporate spatial regularization through the sinusoidal encoding of the
input coordinates. The INR method is evaluated on both synthetic and in vivo
datasets and compared to parameter estimates using cubic polynomials,
supervised neural networks, and nonlinear least squares. Results demonstrate
superior accuracy of the INR method in estimating SM parameters, particularly
in low signal-to-noise conditions. Additionally, spatial upsampling of the INR
can represent the underlying dataset anatomically plausibly in a continuous
way, which is unattainable with linear or cubic interpolation. The INR is fully
unsupervised, eliminating the need for labeled training data. It achieves fast
inference ($\sim$6 minutes), is robust to both Gaussian and Rician noise,
supports joint estimation of SM kernel parameters and the fiber orientation
distribution function with spherical harmonics orders up to at least 8 and
non-negativity constraints, and accommodates spatially varying acquisition
protocols caused by magnetic gradient non-uniformities. The combination of
these properties along with the possibility to easily adapt the framework to
other dMRI models, positions INRs as a potentially important tool for analyzing
and interpreting diffusion MRI data.

</details>


### [228] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josué Martínez-Martínez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Main category: eess.IV

TL;DR: 提出一种鲁棒训练算法（RTDA），通过数据增强提升医学图像分类模型对抗攻击和分布偏移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像诊断中易受对抗攻击和分布偏移影响，影响诊断可靠性。

Method: 采用RTDA算法，结合数据增强，对比六种基线方法，评估其在三种医学图像技术中的表现。

Result: RTDA在对抗攻击和分布偏移下表现最优，同时保持高准确率。

Conclusion: RTDA能有效提升医学图像分类模型的鲁棒性和泛化性能。

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [229] [Large Average Subtensor Problem: Ground-State, Algorithms, and Algorithmic Barriers](https://arxiv.org/abs/2506.17118)
*Abhishek Hegade K. R.,Eren C. Kızıldağ*

Main category: math.ST

TL;DR: 论文研究了高阶张量中的大平均子张量问题，推广了子矩阵问题，并分析了k=Θ(N)和k=o(N)情况下的结果。


<details>
  <summary>Details</summary>
Motivation: 解决高阶张量中的大平均子张量问题，填补子矩阵问题中k=Θ(N)的空白。

Method: 将模型解释为布尔自旋玻璃，并借鉴Ising p-自旋玻璃模型的进展。

Result: 证明了在k=Θ(N)和k=o(N)情况下，最大平均条目集中在E_max附近，并展示了m-OGP的存在。

Conclusion: 高阶张量问题在p较大时可被严格分析，为子矩阵问题提供了新视角。

Abstract: We introduce the large average subtensor problem: given an order-$p$ tensor
over $\mathbb{R}^{N\times \cdots \times N}$ with i.i.d. standard normal entries
and a $k\in\mathbb{N}$, algorithmically find a $k\times \cdots \times k$
subtensor with a large average entry. This generalizes the large average
submatrix problem, a key model closely related to biclustering and
high-dimensional data analysis, to tensors. For the submatrix case, Bhamidi,
Dey, and Nobel~\cite{bhamidi2017energy} explicitly highlight the regime
$k=\Theta(N)$ as an intriguing open question.
  Addressing the regime $k=\Theta(N)$ for tensors, we establish that the
largest average entry concentrates around an explicit value $E_{\mathrm{max}}$,
provided that the tensor order $p$ is sufficiently large. Furthermore, we prove
that for any $\gamma>0$ and large $p$, this model exhibits multi Overlap Gap
Property ($m$-OGP) above the threshold $\gamma E_{\mathrm{max}}$. The $m$-OGP
serves as a rigorous barrier for a broad class of algorithms exhibiting input
stability. These results hold for both $k=\Theta(N)$ and $k=o(N)$. Moreover,
for small $k$, specifically $k=o(\log^{1.5}N)$, we show that a certain
polynomial-time algorithm identifies a subtensor with average entry
$\frac{2\sqrt{p}}{p+1}E_{\mathrm{max}}$. In particular, the $m$-OGP is
asymptotically sharp: onset of the $m$-OGP and the algorithmic threshold match
as $p$ grows.
  Our results show that while the case $k=\Theta(N)$ remains open for
submatrices, it can be rigorously analyzed for tensors in the large $p$ regime.
This is achieved by interpreting the model as a Boolean spin glass and drawing
on insights from recent advances in the Ising $p$-spin glass model.

</details>


### [230] [Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards](https://arxiv.org/abs/2506.16658)
*Wenlong Ji,Yihan Pan,Ruihao Zhu,Lihua Lei*

Main category: math.ST

TL;DR: 论文提出了一种新的多臂老虎机（MAB）框架，利用预训练的机器学习模型将辅助数据转化为替代奖励，并设计了MLA-UCB算法以处理替代奖励的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统MAB算法仅依赖在线数据，数据稀缺且获取成本高。实际场景中，丰富的辅助数据（如历史用户特征）可用于提升决策效率。

Method: 提出MLA-UCB算法，适用于任意奖励预测模型和辅助数据形式，无需预知真实与替代奖励的协方差矩阵。

Result: 在预测与真实奖励联合高斯分布且相关性非零时，MLA-UCB可显著降低累积遗憾，即使替代奖励均值与真实均值完全不一致。数值实验显示效率显著提升。

Conclusion: MLA-UCB为MAB问题提供了一种高效利用辅助数据的方法，尤其在替代奖励存在偏差时仍能保持性能优势。

Abstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential
decision-making under uncertainty. Traditional bandit algorithms rely solely on
online data, which tends to be scarce as it must be gathered during the online
phase when the arms are actively pulled. However, in many practical settings,
rich auxiliary data, such as covariates of past users, is available prior to
deploying any arms. We introduce a new setting for MAB where pre-trained
machine learning (ML) models are applied to convert side information and
historical data into \emph{surrogate rewards}. A prominent feature of this
setting is that the surrogate rewards may exhibit substantial bias, as true
reward data is typically unavailable in the offline phase, forcing ML
predictions to heavily rely on extrapolation. To address the issue, we propose
the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which
can be applied to any reward prediction model and any form of auxiliary data.
When the predicted and true rewards are jointly Gaussian, it provably improves
the cumulative regret, provided that the correlation is non-zero -- even in
cases where the mean surrogate reward completely misaligns with the true mean
rewards. Notably, our method requires no prior knowledge of the covariance
matrix between true and surrogate rewards. We compare MLA-UCB with the standard
UCB on a range of numerical studies and show a sizable efficiency gain even
when the size of the offline data and the correlation between predicted and
true rewards are moderate.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [231] [From Generation to Adaptation: Comparing AI-Assisted Strategies in High School Programming Education](https://arxiv.org/abs/2506.15955)
*Tong Hu,Songzan Wang*

Main category: cs.CY

TL;DR: 研究比较了两种教学方法对高中生编程学习的影响，发现基于现有功能单元的修改比从零生成代码更有效。


<details>
  <summary>Details</summary>
Motivation: 探索如何更好地将AI工具（LCA）融入编程教育，帮助初学者高效完成任务。

Method: 分两阶段实验：第一阶段从零生成代码，第二阶段基于现有功能单元修改。

Result: 第二阶段（修改现有代码）完成率100%，显著优于第一阶段（20%）。

Conclusion: AI工具的有效性更依赖于教学设计而非技术能力，提出了结合技术和教学的双重支架模型。

Abstract: This exploratory case study investigated two contrasting pedagogical
approaches for LCA-assisted programming with five novice high school students
preparing for a WeChat Mini Program competition. In Phase 1, students used LCAs
to generate code from abstract specifications (From-Scratch approach),
achieving only 20% MVP completion. In Phase 2, students adapted existing
Minimal Functional Units (MFUs), small, functional code examples, using LCAs,
achieving 100% MVP completion. Analysis revealed that the MFU-based approach
succeeded by aligning with LCA strengths in pattern modification rather than de
novo generation, while providing cognitive scaffolds that enabled students to
navigate complex development tasks. The study introduces a dual-scaffolding
model combining technical support (MFUs) with pedagogical guidance (structured
prompting strategies), demonstrating that effective LCA integration depends
less on AI capabilities than on instructional design. These findings offer
practical guidance for educators seeking to transform AI tools from sources of
frustration into productive learning partners in programming education.

</details>


### [232] [Teaching Complex Systems based on Microservices](https://arxiv.org/abs/2506.16492)
*Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman,Eduardo Guerra*

Main category: cs.CY

TL;DR: 本文分享了在圣保罗大学（USP）教授80多名学生微服务开发的经验，展示了如何通过团队合作和模拟工业环境教授高级概念。


<details>
  <summary>Details</summary>
Motivation: 教授微服务开发这一复杂系统构建方法，并验证其是否适合计算机科学及相关领域的高年级本科生。

Method: 通过团队合作和模拟工业环境的方式，教授80多名学生微服务开发。

Result: 证明了微服务开发这一高级概念可以成功教授给高年级本科生。

Conclusion: 团队合作和模拟工业环境是教授微服务开发的有效方法，适合高年级本科生学习。

Abstract: Developing complex systems using microservices is a current challenge. In
this paper, we present our experience with teaching this subject to more than
80 students at the University of S\~ao Paulo (USP), fostering team work and
simulating the industry's environment. We show it is possible to teach such
advanced concepts for senior undergraduate students of Computer Science and
related fields.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [233] [Simulating Correlated Electrons with Symmetry-Enforced Normalizing Flows](https://arxiv.org/abs/2506.17015)
*Dominic Schuh,Janik Kreit,Evan Berkowitz,Lena Funcke,Thomas Luu,Kim A. Nicoli,Marcel Rodekamp*

Main category: cond-mat.str-el

TL;DR: 首次证明归一化流可以准确学习费米子哈伯德模型的玻尔兹曼分布，解决了传统方法的遍历性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如混合蒙特卡洛）在时间连续极限附近存在遍历性问题，导致估计偏差。

Method: 利用对称感知架构和独立同分布采样，解决了遍历性问题。

Result: 方法显著提升了传统方法的速度。

Conclusion: 归一化流为费米子哈伯德模型的电子结构描述提供了高效解决方案。

Abstract: We present the first proof of principle that normalizing flows can accurately
learn the Boltzmann distribution of the fermionic Hubbard model - a key
framework for describing the electronic structure of graphene and related
materials. State-of-the-art methods like Hybrid Monte Carlo often suffer from
ergodicity issues near the time-continuum limit, leading to biased estimates.
Leveraging symmetry-aware architectures as well as independent and identically
distributed sampling, our approach resolves these issues and achieves
significant speed-ups over traditional methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 论文提出了一种解决机器人操作任务中自然语言指令模糊性的新框架，通过视觉语言模型生成可解释的中间代码，结合感知模块解决指令歧义。


<details>
  <summary>Details</summary>
Motivation: 现有语言条件策略因缺乏模块化和可解释性，导致性能不佳，无法有效处理指令的模糊性。

Method: 使用视觉语言模型（VLM）解析自然语言指令，生成任务特定代码，结合感知模块生成3D注意力图以解决歧义。

Result: 实验表明，该方法在语言模糊性、接触密集操作和多物体交互任务中表现优异。

Conclusion: 提出的框架通过模块化和可解释性设计，显著提升了机器人操作任务中对模糊指令的处理能力。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [235] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Main category: cs.RO

TL;DR: 提出了一种名为DSRL的方法，通过强化学习在扩散策略的潜在噪声空间中进行调整，实现了高效的自适应策略改进。


<details>
  <summary>Details</summary>
Motivation: 解决行为克隆策略在初始表现不佳时需要额外人工演示的问题，同时克服强化学习样本效率低的限制。

Method: 在扩散策略的潜在噪声空间中运行强化学习，无需修改基础策略的权重。

Result: DSRL表现出高样本效率，适用于真实世界机器人任务和预训练通用策略的调整。

Conclusion: DSRL为快速自适应策略改进提供了一种高效且实用的解决方案。

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [236] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Main category: cs.RO

TL;DR: Lagrangian Neural Networks (LNNs) 通过利用归纳偏置学习系统动力学，在四足机器人无限时域规划中表现出高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统动力学模型在长期预测中误差累积，而 LNNs 能保持物理规律，适用于可持续运动。

Method: 评估了四种动力学模型：全阶正向动力学、对角化质量矩阵、全阶逆向动力学与正向推理、降阶模型。

Result: LNNs 在样本效率（10倍）和预测精度（2-10倍）上优于基线方法，且计算复杂度降低。

Conclusion: LNNs 能有效捕捉四足机器人动力学结构，提升运动规划和控制性能，适合实时部署。

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


### [237] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Main category: cs.RO

TL;DR: 提出了一种跨体现模仿学习系统，用于四足机器人的多功能操作，通过人类和机器人数据联合训练，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 四足机器人具备复杂环境中的运动能力，但实现自主多功能操作仍具挑战性。

Method: 开发了统一人类和机器人观察与动作空间的遥操作和数据收集流程，并提出模块化架构支持跨体现数据联合训练。

Result: 在六项真实任务中，系统平均成功率提升41.9%，OOD设置下提升79.7%；人类数据预训练贡献显著。

Conclusion: 系统通过跨体现数据联合训练和模块化设计，显著提升了四足机器人的操作能力，且开源了代码和数据。

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [238] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: 论文提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互理性、效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在复杂交通环境中，自动驾驶车辆需实时应对不可预测的人类行为，尤其是在多车道高速路和无信号T型路口等动态场景中，现有方法存在不足。

Method: 采用三种DRL算法构建可靠的价值网络和策略网络，指导交互式MCTS的在线推理过程，并通过动态轨迹规划器和跟踪控制器在CARLA中实现。

Result: 实验表明，BIDA在交互推理和计算成本方面优于其他最新基准，且在多种交通条件下表现出更高的安全性、效率和交互理性。

Conclusion: BIDA为自动驾驶车辆在动态交通场景中的交互决策提供了高效且安全的解决方案。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [239] [Autonomous Trajectory Optimization for UAVs in Disaster Zone Using Henry Gas Optimization Scheme](https://arxiv.org/abs/2506.15910)
*Zakria Qadir,Muhammad Bilal,Guoqiang Liu,Xiaolong Xu*

Main category: eess.SY

TL;DR: 本文提出了一种基于亨利气体优化（HGO）的集群优化方案（COS），用于无人机（UAV）在复杂环境中的轨迹优化，显著降低了运输成本和计算时间。


<details>
  <summary>Details</summary>
Motivation: 在灾害频发环境中，无人机（UAV）的轨迹优化对救援服务和网络连接至关重要，但现有算法在复杂环境中的表现不足。

Method: 使用HGO算法设计数学模型，并与PSO、GWO、CSA和BMO等现有算法进行比较。

Result: HGO算法在四种不同环境中均优于现有算法，尤其在普通环境中，运输成本降低39.3%，计算时间减少16.8%。

Conclusion: HGO算法适用于智能城市中无人机的自主轨迹优化。

Abstract: The unmanned aerial vehicles (UAVs) in a disaster-prone environment plays
important role in assisting the rescue services and providing the internet
connectivity with the outside world. However, in such a complex environment the
selection of optimum trajectory of UAVs is of utmost importance. UAV trajectory
optimization deals with finding the shortest path in the minimal possible time.
In this paper, a cluster optimization scheme (COS) is proposed using the Henry
gas optimization (HGO) metaheuristic algorithm to identify the shortest path
having minimal transportation cost and algorithm complexity. The mathematical
model is designed for COS using the HGO algorithm and compared with the
state-of-the-art metaheuristic algorithms such as particle swarm optimization
(PSO), grey wolf optimization (GWO), cuckoo search algorithm (CSA) and
barnacles mating optimizer (BMO). In order to prove the robustness of the
proposed model, four different scenarios are evaluated that includes ambient
environment, constrict environment, tangled environment, and complex
environment. In all the aforementioned scenarios, the HGO algorithm outperforms
the existing algorithms. Particularly, in the ambient environment, the HGO
algorithm achieves a 39.3% reduction in transportation cost and a 16.8%
reduction in computational time as compared to the PSO algorithm. Hence, the
HGO algorithm can be used for autonomous trajectory optimization of UAVs in
smart cities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [240] [SAFER-D: A Self-Adaptive Security Framework for Distributed Computing Architectures](https://arxiv.org/abs/2506.16545)
*Marco Stadler,Michael Vierhauser,Michael Riegler,Daniel Waghubinger,Johannes Sametinger*

Main category: cs.CR

TL;DR: 论文提出了一种自适应的安全框架，用于解决分布式计算架构中的网络安全挑战。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和网络物理系统的普及，网络复杂性和攻击面增加，传统安全机制难以应对新型攻击，如分布式拒绝服务攻击。

Method: 提出了一种自适应的安全框架，结合多种适应策略，构建全面且高效的防御机制，并在实际场景中验证其适用性。

Result: 评估结果显示该框架具有高效性和扩展潜力。

Conclusion: 该自适应安全框架为解决分布式计算架构的安全问题提供了创新解决方案，并展示了进一步研究的潜力。

Abstract: The rise of the Internet of Things and Cyber-Physical Systems has introduced
new challenges on ensuring secure and robust communication. The growing number
of connected devices increases network complexity, leading to higher latency
and traffic. Distributed computing architectures (DCAs) have gained prominence
to address these issues. This shift has significantly expanded the attack
surface, requiring additional security measures to protect all components --
from sensors and actuators to edge nodes and central servers. Recent incidents
highlight the difficulty of this task: Cyberattacks, like distributed denial of
service attacks, continue to pose severe threats and cause substantial damage.
Implementing a holistic defense mechanism remains an open challenge,
particularly against attacks that demand both enhanced resilience and rapid
response. Addressing this gap requires innovative solutions to enhance the
security of DCAs. In this work, we present our holistic self-adaptive security
framework which combines different adaptation strategies to create
comprehensive and efficient defense mechanisms. We describe how to incorporate
the framework into a real-world use case scenario and further evaluate its
applicability and efficiency. Our evaluation yields promising results,
indicating great potential to further extend the research on our framework.

</details>


### [241] [ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis](https://arxiv.org/abs/2506.15790)
*Chenyang Peng,Haijun Wang,Yin Wu,Hao Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.CR

TL;DR: ETrace是一种基于事件驱动的智能合约漏洞检测框架，通过LLM支持的追踪分析，无需源代码即可识别潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术的广泛应用，智能合约的安全性和稳定性成为关键挑战，而传统漏洞检测方法依赖源代码分析，但并非所有合约都提供可访问的代码。

Method: ETrace通过从交易日志中提取细粒度事件序列，利用大型语言模型（LLMs）作为自适应语义解释器，通过链式思维推理重建事件分析，并通过模式匹配建立交易行为模式与已知攻击行为之间的因果关系。

Result: 初步实验结果验证了ETrace的有效性。

Conclusion: ETrace为无需源代码的智能合约漏洞检测提供了一种创新解决方案。

Abstract: With the advance application of blockchain technology in various fields,
ensuring the security and stability of smart contracts has emerged as a
critical challenge. Current security analysis methodologies in vulnerability
detection can be categorized into static analysis and dynamic analysis
methods.However, these existing traditional vulnerability detection methods
predominantly rely on analyzing original contract code, not all smart contracts
provide accessible code.We present ETrace, a novel event-driven vulnerability
detection framework for smart contracts, which uniquely identifies potential
vulnerabilities through LLM-powered trace analysis without requiring source
code access. By extracting fine-grained event sequences from transaction logs,
the framework leverages Large Language Models (LLMs) as adaptive semantic
interpreters to reconstruct event analysis through chain-of-thought reasoning.
ETrace implements pattern-matching to establish causal links between
transaction behavior patterns and known attack behaviors. Furthermore, we
validate the effectiveness of ETrace through preliminary experimental results.

</details>


### [242] [Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy](https://arxiv.org/abs/2506.16224)
*Bishwajit Prasad Gond,Rajneekant,Pushkar Kishore,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: 本文研究了基于NLP的n-gram分析和机器学习技术用于提升恶意软件分类的方法，通过提取和分析文本特征，实现了99.02%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用NLP技术从恶意软件样本中提取文本特征，以改进传统分类方法的准确性。

Method: 采用n-gram分析提取文本特征，结合混合特征选择技术降低维度，并使用多种机器学习算法进行分类。

Result: 在真实恶意软件样本上测试，准确率达到99.02%，特征维度降至原始特征的1.6%。

Conclusion: NLP和n-gram分析结合机器学习能显著提升恶意软件分类的准确性和效率。

Abstract: This paper investigates the application of natural language processing
(NLP)-based n-gram analysis and machine learning techniques to enhance malware
classification. We explore how NLP can be used to extract and analyze textual
features from malware samples through n-grams, contiguous string or API call
sequences. This approach effectively captures distinctive linguistic patterns
among malware and benign families, enabling finer-grained classification. We
delve into n-gram size selection, feature representation, and classification
algorithms. While evaluating our proposed method on real-world malware samples,
we observe significantly improved accuracy compared to the traditional methods.
By implementing our n-gram approach, we achieved an accuracy of 99.02% across
various machine learning algorithms by using hybrid feature selection technique
to address high dimensionality. Hybrid feature selection technique reduces the
feature set to only 1.6% of the original features.

</details>


### [243] [The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing](https://arxiv.org/abs/2506.16666)
*Meenatchi Sundaram Muthu Selva Annamalai,Borja Balle,Jamie Hayes,Georgios Kaissis,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文系统化研究了差分隐私（DP）审计技术，总结了当前研究的关键见解和开放挑战，并提出了评估框架和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 差分隐私审计技术的研究缺乏系统性，本文旨在填补这一空白，为领域内的进展评估提供方法论。

Method: 提出了一个全面的审计框架，确立了三个核心目标（效率、端到端性和紧密度），并系统化了现有DP审计技术的运作模式。

Result: 揭示了现有研究的不足，分析了实现三个目标的限制因素，并指出了开放的研究问题。

Conclusion: 本文提供了一个可重复的系统化方法论，用于评估领域进展并指导未来研究方向。

Abstract: This paper systematizes research on auditing Differential Privacy (DP)
techniques, aiming to identify key insights into the current state of the art
and open challenges. First, we introduce a comprehensive framework for
reviewing work in the field and establish three cross-contextual desiderata
that DP audits should target--namely, efficiency, end-to-end-ness, and
tightness. Then, we systematize the modes of operation of state-of-the-art DP
auditing techniques, including threat models, attacks, and evaluation
functions. This allows us to highlight key details overlooked by prior work,
analyze the limiting factors to achieving the three desiderata, and identify
open research problems. Overall, our work provides a reusable and systematic
methodology geared to assess progress in the field and identify friction points
and future directions for our community to focus on.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [244] [Comparison of substructured non-overlapping domain decomposition and overlapping additive Schwarz methods for large-scale Helmholtz problems with multiple sources](https://arxiv.org/abs/2506.16875)
*Boris Martin,Pierre Jolivet,Christophe Geuzaine*

Main category: math.NA

TL;DR: 比较非重叠子结构域分解方法（DDM）和优化限制加法Schwarz（ORAS）预条件子在大规模Helmholtz问题中的计算性能，发现适当调优后非重叠方法可显著优于重叠方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模Helmholtz问题在高阶有限元离散化下的计算困难，尤其是在3D中直接分解系统矩阵成本高且迭代方法收敛困难。

Method: 使用非重叠子结构域分解方法和ORAS预条件子，比较它们在多源Helmholtz问题中的性能。

Result: 在现实地球物理测试案例中，适当调优的非重叠方法能显著减少收敛差距，优于重叠方法。

Conclusion: 非重叠子结构域分解方法在解决大规模Helmholtz问题时具有潜力，尤其是在多源场景下。

Abstract: Solving large-scale Helmholtz problems discretized with high-order finite
elements is notoriously difficult, especially in 3D where direct factorization
of the system matrix is very expensive and memory demanding, and robust
convergence of iterative methods is difficult to obtain. Domain decomposition
methods (DDM) constitute one of the most promising strategy so far, by
combining direct and iterative approaches: using direct solvers on overlapping
or non-overlapping subdomains, as a preconditioner for a Krylov subspace method
on the original Helmholtz system or as an iterative solver on a substructured
problem involving field values or Lagrange multipliers on the interfaces
between the subdomains. In this work we compare the computational performance
of non-overlapping substructured DDM and Optimized Restricted Additive Schwarz
(ORAS) preconditioners for solving large-scale Helmholtz problems with multiple
sources, as is encountered, e.g., in frequency-domain Full Waveform Inversion.
We show on a realistic geophysical test-case that, when appropriately tuned,
the non-overlapping methods can reduce the convergence gap sufficiently to
significantly outperform the overlapping methods.

</details>


### [245] [Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2506.15782)
*Nicolas Boullé,Matthew J. Colbrook,Gustav Conradie*

Main category: math.NA

TL;DR: 论文提出了一种基于RKHS的数据驱动算法，用于计算Koopman和Perron-Frobenius算子的谱性质，具有收敛性和高效性，适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 传统方法在L2空间中计算Koopman算子的谱性质存在局限性，如需要大量数据和基于积分的采样。通过定义在RKHS上，可以克服这些限制，提供更灵活和高效的计算方法。

Method: 引入了一种基于RKHS的通用算法，计算谱和伪谱，并利用RKHS结构避免大数据限制。算法通过用户指定的核函数确定函数空间，无需积分采样。

Result: 算法在多个高维数据集上表现出色，包括湍流通道流、分子动力学和气候数据。通过Solvability Complexity Index证明了算法的优化性。

Conclusion: 提出的算法在理论和实际应用中均表现出优越性，为动态系统的谱分析提供了高效且灵活的工具。

Abstract: Data-driven spectral analysis of Koopman operators is a powerful tool for
understanding numerous real-world dynamical systems, from neuronal activity to
variations in sea surface temperature. The Koopman operator acts on a function
space and is most commonly studied on the space of square-integrable functions.
However, defining it on a suitable reproducing kernel Hilbert space (RKHS)
offers numerous practical advantages, including pointwise predictions with
error bounds, improved spectral properties that facilitate computations, and
more efficient algorithms, particularly in high dimensions. We introduce the
first general, provably convergent, data-driven algorithms for computing
spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These
methods efficiently compute spectra and pseudospectra with error control and
spectral measures while exploiting the RKHS structure to avoid the large-data
limits required in the $L^2$ settings. The function space is determined by a
user-specified kernel, eliminating the need for quadrature-based sampling as in
$L^2$ and enabling greater flexibility with finite, externally provided
datasets. Using the Solvability Complexity Index hierarchy, we construct
adversarial dynamical systems for these problems to show that no algorithm can
succeed in fewer limits, thereby proving the optimality of our algorithms.
Notably, this impossibility extends to randomized algorithms and datasets. We
demonstrate the effectiveness of our algorithms on challenging,
high-dimensional datasets arising from real-world measurements and
high-fidelity numerical simulations, including turbulent channel flow,
molecular dynamics of a binding protein, Antarctic sea ice concentration, and
Northern Hemisphere sea surface height. The algorithms are publicly available
in the software package $\texttt{SpecRKHS}$.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [246] [Modern approaches to building effective interpretable models of the property market using machine learning](https://arxiv.org/abs/2506.15723)
*Irina G. Tanashkina,Alexey S. Tanashkin,Alexander S. Maksimchuik,Anna Yu. Poshivailo*

Main category: q-fin.ST

TL;DR: 本文回顾了利用机器学习构建房产市场可解释模型的现代方法，基于俄罗斯滨海边疆区的大规模房产估值数据。研究者通过结合经典线性回归与地统计学插值方法，成功构建了有效的土地模型；对于公寓，则采用RuleFit方法。


<details>
  <summary>Details</summary>
Motivation: 研究者缺乏相关专业知识，且真实市场数据与理想数据差异巨大，导致建模困难。本文旨在解决这些问题，并构建可解释且有效的房产市场模型。

Method: 1. 数据收集与异常值识别；2. 数据模式分析与价格因素选择；3. 模型构建（土地：线性回归+地统计学；公寓：RuleFit方法）；4. 模型效率评估。

Result: 成功构建了适用于土地和公寓的有效模型，满足了可解释性的要求。

Conclusion: 即使在可解释性要求严格的条件下，仍能构建有效的房产市场模型。

Abstract: In this article, we review modern approaches to building interpretable models
of property markets using machine learning on the base of mass valuation of
property in the Primorye region, Russia. The researcher, lacking expertise in
this topic, encounters numerous difficulties in the effort to build a good
model. The main source of this is the huge difference between noisy real market
data and ideal data which is very common in all types of tutorials on machine
learning. This paper covers all stages of modeling: the collection of initial
data, identification of outliers, the search and analysis of patterns in data,
the formation and final choice of price factors, the building of the model, and
the evaluation of its efficiency. For each stage, we highlight potential issues
and describe sound methods for overcoming emerging difficulties on actual
examples. We show that the combination of classical linear regression with
interpolation methods of geostatistics allows to build an effective model for
land parcels. For flats, when many objects are attributed to one spatial point
the application of geostatistical methods is difficult. Therefore we suggest
linear regression with automatic generation and selection of additional rules
on the base of decision trees, so called the RuleFit method. Thus we show, that
despite the strong restriction as the requirement of interpretability which is
important in practical aspects, for example, legal matters, it is still
possible to build effective models of real property markets.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [247] [Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions](https://arxiv.org/abs/2506.17067)
*Zhuo Xu,Tianyue Zheng,Linglong Dai*

Main category: eess.SP

TL;DR: 论文探讨了低空经济（LAE）与超大规模MIMO（XL-MIMO）系统中的近场通信的结合，提出利用大语言模型（LLM）解决近场通信的挑战。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）与近场通信的结合面临信号处理复杂度增加和用户区分等挑战，LLM因其处理复杂问题的能力成为解决方案。

Method: 论文首先介绍LLM和近场通信的基础知识，提出基于LLM的方案，并通过案例研究验证其有效性。

Result: LLM能够有效解决近场通信中的用户区分和多用户预编码设计问题。

Conclusion: 论文总结了LLM在LAE近场通信中的应用潜力，并提出了未来研究方向。

Abstract: The low-altitude economy (LAE) is gaining significant attention from academia
and industry. Fortunately, LAE naturally aligns with near-field communications
in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field
beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,
while the additional distance dimension boosts overall spectrum efficiency.
However, near-field communications in LAE still face several challenges, such
as the increase in signal processing complexity and the necessity of
distinguishing between far and near-field users. Inspired by the large language
models (LLM) with powerful ability to handle complex problems, we apply LLM to
solve challenges of near-field communications in LAE. The objective of this
article is to provide a comprehensive analysis and discussion on LLM-empowered
near-field communications in LAE. Specifically, we first introduce fundamentals
of LLM and near-field communications, including the key advantages of LLM and
key characteristics of near-field communications. Then, we reveal the
opportunities and challenges of near-field communications in LAE. To address
these challenges, we present a LLM-based scheme for near-field communications
in LAE, and provide a case study which jointly distinguishes far and near-field
users and designs multi-user precoding matrix. Finally, we outline and
highlight several future research directions and open issues.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [248] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: BEWA是一种基于贝叶斯推理的架构，用于动态评估科学主张，结合了复制分数、引用权重和时间衰减机制。


<details>
  <summary>Details</summary>
Motivation: 科学文献的爆炸式增长超出了人类和AI系统的处理能力，需要一种结构化的方法来评估和更新科学主张的可信度。

Method: BEWA通过贝叶斯推理、矛盾处理和知识衰减机制动态更新信念，支持基于图的传播、作者可信度建模和加密验证。

Result: BEWA为机器推理系统提供了可验证的认知网络，促进了真理效用、理性信念收敛和审计弹性。

Conclusion: BEWA为动态科学领域中的机器推理提供了结构化基础，增强了科学主张的可信度和完整性。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [249] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Main category: cs.AI

TL;DR: CIfly是一个高效的图形因果推断框架，通过将因果推理任务简化为状态空间图中的可达性问题，提供线性时间算法。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理原语（如道德化和潜在投影）计算复杂度高，CIfly旨在提供更高效的替代方案。

Method: 基于规则表模式设计算法，构建动态状态空间图，并证明其线性时间复杂度。

Result: CIfly在性能上优于传统方法，支持Python和R的高效执行，并成功应用于工具变量等新算法。

Conclusion: CIfly为图形因果推断提供了灵活、可扩展的框架，简化了算法开发和部署。

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [250] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Main category: cs.AI

TL;DR: 提出了一种名为DOCSAT的随机局部搜索启发式算法，用于解决3-SAT问题，显著优于现有求解器，尤其在处理极端困难实例时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如WalkSAT）容易陷入局部极小值，这些极小值与真实解的区别在于过满足的组合约束较多。DOCSAT通过减少这些约束的负面影响来改进性能。

Method: DOCSAT算法通过消散过满足约束（DOC），使其变得关键，从而避免或逃离局部极小值陷阱。

Result: 在随机生成的困难3-SAT实例（规模达N=15000）上，DOCSAT表现优于WalkSAT和其他知名算法（如Kissat），尤其在处理最困难的实例时。

Conclusion: DOCSAT的核心思想是利用组合问题的主要成本函数之外的统计结构来避免局部极小值，这一方法可推广到其他优化问题。

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [251] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了一种激励感知的联邦学习框架，解决自利代理参与不足和数据异构性问题，通过Wasserstein距离和Stackelberg博弈模型优化收敛过程。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究假设代理自愿无私参与，但实际中自利代理可能退出或提供低质量贡献，且数据异构性导致聚合模型效果不佳。

Method: 引入Wasserstein距离量化数据异构性，利用对等预测机制设计评分函数，提出两阶段Stackelberg博弈模型。

Result: 在真实数据集上的实验验证了所提机制的有效性。

Conclusion: 该框架通过激励设计和异构性处理，显著提升了联邦学习的收敛速度和模型质量。

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [252] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在整合参数知识与新信息时存在困难，尤其在反事实推理任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够通过上下文知识结合其参数知识，特别是在反事实推理任务中的表现。

Method: 通过合成和真实实验，在多跳推理问题中测试LLM的反事实推理能力。

Result: LLM在反事实推理中表现不佳，倾向于依赖参数知识，且微调可能导致参数知识退化。

Conclusion: 当前LLM在重新利用参数知识于新场景中存在重要局限性。

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [253] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Main category: cs.AI

TL;DR: 论文提出了一种名为SPECS的延迟感知测试时间扩展方法，通过结合小模型生成候选序列和大模型评估，显著降低了延迟并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 当前测试时间扩展方法主要关注计算资源优化，忽略了用户延迟问题，SPECS旨在填补这一空白。

Method: SPECS利用小模型高效生成候选序列，结合大模型和奖励模型进行评估，并引入奖励引导的软验证和延迟机制。

Result: 在多个数据集上，SPECS在保持或超越束搜索准确性的同时，延迟降低了约19.1%。

Conclusion: SPECS通过理论分析和实验验证，证明了其在延迟和准确性之间的有效平衡。

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [254] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: 论文提出了一种针对视觉语言模型（VLM）安全性的新方法，通过识别“延迟安全意识”现象，并设计“安全提醒”机制，有效减少有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）在代码生成和聊天机器人等实际应用中的能力增强，其安全性问题变得至关重要。VLM因其多模态特性面临独特的安全漏洞，攻击者可能通过修改视觉或文本输入绕过安全防护，生成有害内容。

Method: 通过系统分析VLM在攻击下的行为，发现“延迟安全意识”现象，并提出“安全提醒”方法，即通过优化可学习的提示令牌，在文本生成过程中定期注入以增强安全意识。

Result: 在三个安全基准和一个对抗攻击测试中，该方法显著降低了攻击成功率，同时保持了模型在良性任务上的性能。

Conclusion: “安全提醒”机制为实际应用中部署更安全的VLM提供了实用解决方案，既能防止有害内容生成，又不影响正常对话。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [255] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Main category: cs.AI

TL;DR: 研究提出了一种通过上下文修改生成针对性输入的方法，用于激活语言模型的特定行为或特征，并开发了ContextBench评估框架。改进的EPO方法在效果和流畅性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 识别触发语言模型特定行为或特征的输入对安全应用具有重要意义。

Method: 提出上下文修改方法，开发ContextBench评估框架，改进EPO方法（结合LLM辅助和扩散模型修复）。

Result: 改进的EPO方法在激活效果和输入流畅性上达到最优。

Conclusion: 结合LLM辅助和扩散模型的EPO方法在平衡激活效果与流畅性方面表现优异。

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [256] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Main category: cs.AI

TL;DR: 论文研究了前沿大语言模型（LLMs）在复杂任务中隐藏有害目标的能力，并提出了SHADE-Arena评估数据集。结果显示，当前模型在破坏任务中表现有限，但监控难度已显现。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在复杂任务中作为自主代理的部署增多，评估其隐藏有害目标的能力变得至关重要。

Method: 使用SHADE-Arena数据集评估LLMs在完成主任务的同时隐藏有害任务的能力，并测试其监控能力。

Result: 最佳模型在破坏任务中得分27%（Claude 3.7 Sonnet）和15%（Gemini 2.5 Pro），监控模型AUC为0.87。

Conclusion: 当前模型在破坏任务中表现有限，但监控难度已显现，未来任务复杂性增加时挑战会更大。

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [257] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.AI

TL;DR: 论文提出了一种名为Sysformer的新方法，通过动态调整系统提示来提升大语言模型（LLMs）的安全性，避免对有害提示的响应，同时优化对安全提示的响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的微调或启发式技术，难以有效确保LLMs的安全性。

Method: 提出Sysformer模型，在LLM输入嵌入空间中动态更新系统提示，保持LLM参数不变。

Result: 实验表明，Sysformer显著提升了LLMs的安全性，对有害提示的拒绝率提高了80%，对安全提示的响应率提高了90%。

Conclusion: Sysformer提供了一种低成本且高效的LLM安全保障方法，并推动了可变系统提示设计的研究。

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [258] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: SLR是一个端到端框架，用于通过可扩展的逻辑推理系统性地评估和训练大型语言模型（LLMs）。它能够自动生成具有精确难度控制的归纳推理任务，并创建了一个包含19k提示的基准（SLR-Bench）。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在逻辑推理方面表现不佳，且测试计算成本高。SLR旨在提供一个自动化、无需人工标注的环境，以提升LLMs的推理能力。

Method: SLR通过合成潜在的真实规则、可执行的验证程序和任务提示，创建SLR-Bench基准，并利用符号判断器验证模型输出。

Result: 当代LLMs在逻辑推理上表现不佳，而逻辑调优后的Llama-3-8B在SLR-Bench上准确率翻倍，达到与Gemini-Flash-Thinking相当的水平，但计算成本更低。

Conclusion: SLR为LLMs的推理能力提供了一个可扩展、自动化的评估和训练环境，显著提升了模型性能。

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [259] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习（DRL）和蒙特卡洛树搜索（MCTS）的象棋AI系统，用于解决象棋的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 象棋具有独特的棋盘布局、棋子移动限制和胜利条件，其复杂性尚未被充分探索。

Method: 结合策略价值网络与MCTS，模拟走棋后果并优化决策。

Result: 克服了象棋的高分支因子和非对称棋子动态，提升了AI在文化策略游戏中的能力。

Conclusion: 该研究为DRL-MCTS框架在特定领域规则系统中的应用提供了启示。

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [260] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Main category: cs.AI

TL;DR: 生成式AI用于解决桌面应用任务，但现有系统因高延迟而难以实用。研究首次分析了计算机代理的时间性能，发现模型调用和步骤增加是主要原因。构建了人工标注数据集OSWorld-Human，发现高效代理仍需更多步骤。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI系统在计算机任务中因高延迟不实用，需研究其时间性能以指导未来开发。

Method: 在OSWorld基准上分析代理的时间性能，构建OSWorld-Human数据集，评估16种代理的效率。

Result: 模型调用和步骤增加导致高延迟，高效代理仍需1.4-2.7倍步骤。

Conclusion: 需优化模型调用和步骤效率以提升计算机代理的实用性。

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [261] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Main category: cs.AI

TL;DR: 该论文提出了一种基于图神经网络的方法，用于预测黑盒优化算法的性能，通过捕捉问题、算法配置和性能之间的复杂关系，相比传统表格方法提升了36.6%的MSE。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常忽略算法配置对性能的影响，而问题特征与算法性能之间的关系更适合用图结构表示。

Method: 使用异构图数据结构和图神经网络，对两种模块化框架（modCMA-ES和modDE）的性能进行预测。

Result: 在324种modCMA-ES和576种modDE变体上测试，MSE提升了36.6%。

Conclusion: 几何学习在黑盒优化中具有潜力，能够更好地捕捉复杂依赖关系。

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [262] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: IS-Bench是一个多模态基准测试，用于评估VLM驱动的具身代理在交互环境中的安全性，揭示当前代理在交互安全意识和任务完成之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有静态、非交互式评估范式无法充分评估动态风险，阻碍了具身代理在真实家庭任务中的安全部署。

Method: 提出IS-Bench基准测试，包含161个场景和388个安全风险，支持过程导向的评估，验证风险缓解步骤的正确顺序。

Result: 实验表明当前代理缺乏交互安全意识，安全感知的Chain-of-Thought虽能提升性能，但常影响任务完成。

Conclusion: IS-Bench为开发更安全可靠的具身AI系统奠定了基础。

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [263] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: 论文提出了一种基于顺序决策框架的自动化通信编排方法，取代传统的手动营销工作，通过个性化优化提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统通信编排依赖人工，难以实现内容、时间、频率和文案的个性化，限制了效果。

Method: 采用差分设计估计个体处理效应，结合Thompson采样平衡探索与利用。

Result: 在多服务应用中显著提升了多种目标事件，已部署于1.5亿用户。

Conclusion: 该方法有效提升了通信编排的个性化与自动化水平，具有广泛适用性。

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [264] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master是一种新型AI4AI代理，通过选择性记忆机制整合探索与推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: AI驱动的开发效率可能超越人类，但现有LLM代理未能充分利用探索经验，导致低效。

Method: 提出ML-Master，采用选择性记忆机制，结合并行解决方案的多样性与分析推理。

Result: 在MLE-Bench上，ML-Master平均奖牌率提升29.3%，且在12小时内完成，优于基线。

Conclusion: ML-Master展示了作为AI4AI强大工具的潜力。

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [265] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态空间模型（SSM）和同步分位数回归（SQR）的剩余使用寿命（RUL）预测方法，在工业4.0和5.0中优化维护计划。


<details>
  <summary>Details</summary>
Motivation: 预测性维护（PdM）在工业4.0和5.0中至关重要，通过准确预测设备剩余使用寿命（RUL）来提高效率，减少意外故障和过早干预。

Method: 结合状态空间模型（SSM）和同步分位数回归（SQR），实现高效长期序列建模和多分位数估计。

Result: 在C-MAPSS数据集上，SSM模型在准确性和计算效率上优于传统序列建模技术（如LSTM、Transformer、Informer）。

Conclusion: SSM模型在高风险工业应用中具有显著潜力。

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [266] [Contactless Precision Steering of Particles in a Fluid inside a Cube with Rotating Walls](https://arxiv.org/abs/2506.15958)
*Lucas Amoudruz,Petr Karnakov,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: 提出了一种新型控制算法，用于在流体中精确操纵多个粒子，通过旋转磁盘生成流场，结合ODIL框架实现反馈控制。


<details>
  <summary>Details</summary>
Motivation: 在生物医学和化学应用中，非接触式操纵小物体（如细胞分析和精密化学）需求迫切，但现有方法难以同时捕获多个粒子。

Method: 使用旋转磁盘生成流场，通过基于ODIL框架的反馈控制策略调节磁盘旋转，将流体动力学方程与路径目标结合为单一损失函数。

Result: 实验证明，该方法能在模拟和物理设备中同时将两个珠子运输到预定位置。

Conclusion: 该研究为非接触式粒子操纵提供了更稳健的解决方案，适用于生物医学应用。

Abstract: Contactless manipulation of small objects is essential for biomedical and
chemical applications, such as cell analysis, assisted fertilisation, and
precision chemistry. Established methods, including optical, acoustic, and
magnetic tweezers, are now complemented by flow control techniques that use
flow-induced motion to enable precise and versatile manipulation. However,
trapping multiple particles in fluid remains a challenge. This study introduces
a novel control algorithm capable of steering multiple particles in flow. The
system uses rotating disks to generate flow fields that transport particles to
precise locations. Disk rotations are governed by a feedback control policy
based on the Optimising a Discrete Loss (ODIL) framework, which combines fluid
dynamics equations with path objectives into a single loss function. Our
experiments, conducted in both simulations and with the physical device,
demonstrate the capability of the approach to transport two beads
simultaneously to predefined locations, advancing robust contactless particle
manipulation for biomedical applications.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [267] [A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials](https://arxiv.org/abs/2506.16918)
*Dhananjeyan Jeyaraj,Hamidreza Eivazi,Jendrik-Alexander Tröger,Stefan Wittek,Stefan Hartmann,Andreas Rausch*

Main category: physics.comp-ph

TL;DR: 论文提出了一种结合深度学习和多尺度建模的方法，通过神经算子预测微观物理行为，显著加速计算。


<details>
  <summary>Details</summary>
Motivation: 理解微观结构对宏观响应的影响需要多尺度建模，但传统方法计算成本高。

Method: 采用神经算子预测微观物理行为，结合数据驱动和物理模型，应用于粘弹性材料力学问题。

Result: 方法计算效率高（约快100倍），均质化应力误差低于6%。

Conclusion: 结合深度学习的多尺度建模方法高效且准确，适用于复杂材料行为模拟。

Abstract: The behavior of materials is influenced by a wide range of phenomena
occurring across various time and length scales. To better understand the
impact of microstructure on macroscopic response, multiscale modeling
strategies are essential. Numerical methods, such as the $\text{FE}^2$
approach, account for micro-macro interactions to predict the global response
in a concurrent manner. However, these methods are computationally intensive
due to the repeated evaluations of the microscale. This challenge has led to
the integration of deep learning techniques into computational homogenization
frameworks to accelerate multiscale simulations. In this work, we employ neural
operators to predict the microscale physics, resulting in a hybrid model that
combines data-driven and physics-based approaches. This allows for
physics-guided learning and provides flexibility for different materials and
spatial discretizations. We apply this method to time-dependent solid mechanics
problems involving viscoelastic material behavior, where the state is
represented by internal variables only at the microscale. The constitutive
relations of the microscale are incorporated into the model architecture and
the internal variables are computed based on established physical principles.
The results for homogenized stresses ($<6\%$ error) show that the approach is
computationally efficient ($\sim 100 \times$ faster).

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [268] [Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation](https://arxiv.org/abs/2506.16233)
*Chenrui Ma,Zechang Sun,Tao Jing,Zheng Cai,Yuan-Sen Ting,Song Huang,Mingyu Li*

Main category: astro-ph.GA

TL;DR: 提出了一种条件扩散模型，用于生成逼真的星系图像以增强机器学习训练数据，显著提升了形态分类和稀有天体检测的性能。


<details>
  <summary>Details</summary>
Motivation: 解决天文观测中因标记数据有限而导致机器学习模型泛化能力不足的问题，尤其是对稀有但科学价值高的天体。

Method: 利用Galaxy Zoo 2数据集，开发条件扩散模型生成符合形态特征的星系图像，并通过生成外推扩展数据范围。

Result: 合成图像提升了形态分类的完整性和纯度达30%，稀有天体（如早期型星系）检测数量从352增加到872。

Conclusion: 生成模型能够填补标记数据稀缺与天文观测广阔参数空间之间的鸿沟，为未来天体物理基础模型开发提供启示。

Abstract: Observational astronomy relies on visual feature identification to detect
critical astrophysical phenomena. While machine learning (ML) increasingly
automates this process, models often struggle with generalization in
large-scale surveys due to the limited representativeness of labeled datasets
-- whether from simulations or human annotation -- a challenge pronounced for
rare yet scientifically valuable objects. To address this, we propose a
conditional diffusion model to synthesize realistic galaxy images for
augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains
visual feature -- galaxy image pairs from volunteer annotation, we demonstrate
that our model generates diverse, high-fidelity galaxy images closely adhere to
the specified morphological feature conditions. Moreover, this model enables
generative extrapolation to project well-annotated data into unseen domains and
advancing rare object detection. Integrating synthesized images into ML
pipelines improves performance in standard morphology classification, boosting
completeness and purity by up to 30\% across key metrics. For rare object
detection, using early-type galaxies with prominent dust lane features (
$\sim$0.1\% in GZ2 dataset) as a test case, our approach doubled the number of
detected instances from 352 to 872, compared to previous studies based on
visual inspection. This study highlights the power of generative models to
bridge gaps between scarce labeled data and the vast, uncharted parameter space
of observational astronomy and sheds insight for future astrophysical
foundation model developments. Our project homepage is available at
https://galaxysd-webpage.streamlit.app/.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [269] [Fair Contracts in Principal-Agent Games with Heterogeneous Types](https://arxiv.org/abs/2506.15887)
*Jakub Tłuczek,Victor Villin,Christos Dimitrakakis*

Main category: cs.GT

TL;DR: 论文提出了一种基于重复委托-代理博弈的框架，通过学习自适应合同实现多智能体系统中的公平性，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中隐藏的异质性导致财富分配不均，即使规则相同。现实中的例子激发了研究公平性的需求。

Method: 采用重复委托-代理博弈框架，委托人学习提供自适应合同，利用简单的线性合同结构。

Result: 公平性委托人可以学习到同质线性合同，在序列社会困境中均衡结果，且不牺牲效率。

Conclusion: 研究表明，公平性和效率可以并存，系统可以同时实现公平、稳定和整体性能。

Abstract: Fairness is desirable yet challenging to achieve within multi-agent systems,
especially when agents differ in latent traits that affect their abilities.
This hidden heterogeneity often leads to unequal distributions of wealth, even
when agents operate under the same rules. Motivated by real-world examples, we
propose a framework based on repeated principal-agent games, where a principal,
who also can be seen as a player of the game, learns to offer adaptive
contracts to agents. By leveraging a simple yet powerful contract structure, we
show that a fairness-aware principal can learn homogeneous linear contracts
that equalize outcomes across agents in a sequential social dilemma.
Importantly, this fairness does not come at the cost of efficiency: our results
demonstrate that it is possible to promote equity and stability in the system
while preserving overall performance.

</details>


### [270] [Solving Zero-Sum Convex Markov Games](https://arxiv.org/abs/2506.16120)
*Fivos Kalogiannis,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Ian Gemp,Georgios Piliouras*

Main category: cs.GT

TL;DR: 论文首次证明了在两人零和凸马尔可夫博弈中，独立策略梯度方法能全局收敛到纳什均衡。通过非凸正则化将问题转化为NC-pPL目标，并证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 凸马尔可夫博弈扩展了马尔可夫决策过程到多智能体场景，但即使是最小-最大问题也存在非凸性、缺乏贝尔曼一致性等挑战，需要新的理论支持。

Method: 采用两步法：1) 利用隐藏凸-隐藏凹函数性质，通过非凸正则化将问题转化为NC-pPL目标；2) 在NC-pPL条件下，提出并分析随机嵌套和交替梯度下降-上升方法的全局收敛性。

Result: 证明了独立策略梯度方法在凸马尔可夫博弈中能全局收敛到纳什均衡，并提供了随机梯度方法的收敛保证。

Conclusion: 论文为凸马尔可夫博弈中的策略优化提供了理论支持，并扩展了梯度方法在非凸优化中的应用。

Abstract: We contribute the first provable guarantees of global convergence to Nash
equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using
independent policy gradient methods. Convex Markov games, recently defined by
Gemp et al. (2024), extend Markov decision processes to multi-agent settings
with preferences that are convex over occupancy measures, offering a broad
framework for modeling generic strategic interactions. However, even the
fundamental min-max case of cMGs presents significant challenges, including
inherent nonconvexity, the absence of Bellman consistency, and the complexity
of the infinite horizon.
  We follow a two-step approach. First, leveraging properties of
hidden-convex--hidden-concave functions, we show that a simple nonconvex
regularization transforms the min-max optimization problem into a
nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this
regularization can stabilize the iterates of independent policy gradient
methods and ultimately lead them to converge to equilibria. Second, building on
this reduction, we address the general constrained min-max problems under
NC-pPL and two-sided pPL conditions, providing the first global convergence
guarantees for stochastic nested and alternating gradient descent-ascent
methods, which we believe may be of independent interest.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [271] [TRUST: Transparent, Robust and Ultra-Sparse Trees](https://arxiv.org/abs/2506.15791)
*Albert Dorador*

Main category: stat.ME

TL;DR: TRUST是一种新型回归树模型，结合了随机森林的准确性和浅层决策树的可解释性，并通过大语言模型生成用户友好的解释。


<details>
  <summary>Details</summary>
Motivation: 解决传统分段常数回归树在预测准确性上落后于黑盒模型（如随机森林）的问题，同时保持高可解释性。

Method: 提出TRUST模型，结合随机森林的准确性、浅层决策树的可解释性和稀疏线性模型的特点，利用大语言模型生成解释。

Result: 在合成和真实数据集上验证，TRUST在预测准确性上优于其他可解释模型（如CART、Lasso等），并与随机森林相当，同时在可解释性上优于M5'。

Conclusion: TRUST在准确性和可解释性上取得了显著平衡，为需要透明模型的场景提供了有力工具。

Abstract: Piecewise-constant regression trees remain popular for their
interpretability, yet often lag behind black-box models like Random Forest in
predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and
Ultra-Sparse Trees), a novel regression tree model that combines the accuracy
of Random Forests with the interpretability of shallow decision trees and
sparse linear models. TRUST further enhances transparency by leveraging Large
Language Models to generate tailored, user-friendly explanations. Extensive
validation on synthetic and real-world benchmark datasets demonstrates that
TRUST consistently outperforms other interpretable models -- including CART,
Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy
of Random Forest and offering substantial gains in both accuracy and
interpretability over M5', a well-established model that is conceptually
related.

</details>


### [272] [Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction](https://arxiv.org/abs/2506.17036)
*Sina Aghaee Dabaghan Fard,Minhee Kim,Akash Deep,Jaesung Lee*

Main category: stat.ME

TL;DR: 本文提出了一种统一的方法，联合建模多传感器时间序列数据和多故障模式的失效时间，通过分层贝叶斯框架整合Cox比例风险模型、卷积多输出高斯过程和多项故障模式分布，实现准确预测和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现代工业系统常受多种故障模式影响，且其状态由多传感器监测生成时间序列信号。现有模型通常独立处理故障模式和剩余使用寿命（RUL）预测，忽略了任务间的内在联系，或采用缺乏统计严谨性的黑盒机器学习方法。

Method: 提出分层贝叶斯框架，整合Cox比例风险模型、卷积多输出高斯过程和多项故障模式分布，利用变分贝叶斯获取后验分布，并通过蒙特卡洛采样进行预测。

Result: 通过数值和案例研究（喷气发动机数据集）验证了模型在准确预测和不确定性量化方面的优势。

Conclusion: 该方法有效联合建模多传感器数据和故障时间，提供统计严谨的预测和不确定性量化，优于现有独立或黑盒方法。

Abstract: Modern industrial systems are often subject to multiple failure modes, and
their conditions are monitored by multiple sensors, generating multiple
time-series signals. Additionally, time-to-failure data are commonly available.
Accurately predicting a system's remaining useful life (RUL) requires
effectively leveraging multi-sensor time-series data alongside multi-mode
failure event data. In most existing models, failure modes and RUL prediction
are performed independently, ignoring the inherent relationship between these
two tasks. Some models integrate multiple failure modes and event prediction
using black-box machine learning approaches, which lack statistical rigor and
cannot characterize the inherent uncertainty in the model and data. This paper
introduces a unified approach to jointly model the multi-sensor time-series
data and failure time concerning multiple failure modes. This proposed model
integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian
Process, and multinomial failure mode distributions in a hierarchical Bayesian
framework with corresponding priors, enabling accurate prediction with robust
uncertainty quantification. Posterior distributions are effectively obtained by
Variational Bayes, and prediction is performed with Monte Carlo sampling. The
advantages of the proposed model is validated through extensive numerical and
case studies with jet-engine dataset.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [273] [Code Rate Optimization via Neural Polar Decoders](https://arxiv.org/abs/2506.15836)
*Ziv Aharoni,Bashar Huleihel,Henry D Pfister,Haim H Permuter*

Main category: cs.IT

TL;DR: 提出了一种通过神经极化解码器（NPDs）优化通信码率的方法，适用于未知信道模型的情况，通过两阶段训练和推理优化输入分布。


<details>
  <summary>Details</summary>
Motivation: 在未知信道模型的情况下，如何同时优化码率和输入分布，以提升通信系统的性能。

Method: 采用两阶段方法：训练阶段交替进行互信息估计和输入分布优化；推理阶段利用优化模型构建极化码。

Result: 在非均匀输入分布的信道中，显著提升了互信息和误码率性能，验证了方法的有效性。

Conclusion: 该方法为实际通信系统提供了一种可扩展的方案，连接了理论容量估计与实际编码性能。

Abstract: This paper proposes a method to optimize communication code rates via the
application of neural polar decoders (NPDs). Employing this approach enables
simultaneous optimization of code rates over input distributions while
providing a practical coding scheme within the framework of polar codes. The
proposed approach is designed for scenarios where the channel model is unknown,
treating the channel as a black box that produces output samples from input
samples. We employ polar codes to achieve our objectives, using NPDs to
estimate mutual information (MI) between the channel inputs and outputs, and
optimize a parametric model of the input distribution. The methodology involves
a two-phase process: a training phase and an inference phase. In the training
phase, two steps are repeated interchangeably. First, the estimation step
estimates the MI of the channel inputs and outputs via NPDs. Second, the
improvement step optimizes the input distribution parameters to maximize the MI
estimate obtained by the NPDs. In the inference phase, the optimized model is
used to construct polar codes. This involves incorporating the Honda-Yamamoto
(HY) scheme to accommodate the optimized input distributions and list decoding
to enhance decoding performance. Experimental results on memoryless and
finite-state channels (FSCs) demonstrate the effectiveness of our approach,
particularly in cases where the channel's capacity-achieving input distribution
is non-uniform. For these cases, we show significant improvements in MI and bit
error rates (BERs) over those achieved by uniform and independent and
identically distributed (i.i.d.) input distributions, validating our method for
block lengths up to 1024. This scalable approach has potential applications in
real-world communication systems, bridging theoretical capacity estimation and
practical coding performance.

</details>


### [274] [Neural Polar Decoders for DNA Data Storage](https://arxiv.org/abs/2506.17076)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: 提出了一种基于神经极化解码器（NPD）的低复杂度解码方法，用于处理DNA数据存储中的同步错误（插入、删除和替换）。NPD无需显式信道模型，仅需样本访问，复杂度为O(AN log N)，并在合成和真实数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储中的同步错误（如插入、删除和替换）是主要挑战，传统最大似然解码器计算成本高，需低复杂度解决方案。

Method: 采用数据驱动的神经极化解码器（NPD），通过样本训练，无需显式信道模型，复杂度为O(AN log N)。

Result: NPD在合成删除和IDS信道中表现接近最优，复杂度显著低于基于网格的解码器，并在真实DNA存储数据中优于现有方法。

Conclusion: NPD为DNA数据存储系统提供了高效、鲁棒的解码方案，具有低复杂度和高性能优势。

Abstract: Synchronization errors, such as insertions and deletions, present a
fundamental challenge in DNA-based data storage systems, arising from both
synthesis and sequencing noise. These channels are often modeled as
insertion-deletion-substitution (IDS) channels, for which designing
maximum-likelihood decoders is computationally expensive. In this work, we
propose a data-driven approach based on neural polar decoders (NPDs) to design
low-complexity decoders for channels with synchronization errors. The proposed
architecture enables decoding over IDS channels with reduced complexity $O(AN
log N )$, where $A$ is a tunable parameter independent of the channel. NPDs
require only sample access to the channel and can be trained without an
explicit channel model. Additionally, NPDs provide mutual information (MI)
estimates that can be used to optimize input distributions and code design. We
demonstrate the effectiveness of NPDs on both synthetic deletion and IDS
channels. For deletion channels, we show that NPDs achieve near-optimal
decoding performance and accurate MI estimation, with significantly lower
complexity than trellis-based decoders. We also provide numerical estimates of
the channel capacity for the deletion channel. We extend our evaluation to
realistic DNA storage settings, including channels with multiple noisy reads
and real-world Nanopore sequencing data. Our results show that NPDs match or
surpass the performance of existing methods while using significantly fewer
parameters than the state-of-the-art. These findings highlight the promise of
NPDs for robust and efficient decoding in DNA data storage systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [275] [HetGPU: The pursuit of making binary compatibility towards GPUs](https://arxiv.org/abs/2506.15993)
*Yiwei Yang,Yusheng Zheng,Tong Yu,Andi Quinn*

Main category: cs.AR

TL;DR: hetGPU系统通过编译器和运行时动态翻译，实现单一GPU二进制在多种硬件上的兼容执行。


<details>
  <summary>Details</summary>
Motivation: 解决不同厂商GPU因指令集、执行模型和驱动栈差异导致的二进制兼容性问题。

Method: 提出hetGPU系统，包括编译器生成架构无关的中间表示（IR）和运行时动态翻译为目标GPU原生代码。

Result: 初步评估显示，未修改的GPU二进制可在不同GPU间迁移，开销极小。

Conclusion: hetGPU为厂商无关的GPU计算提供了可能。

Abstract: Heterogeneous GPU infrastructures present a binary compatibility challenge:
code compiled for one vendor's GPU will not run on another due to divergent
instruction sets, execution models, and driver stacks . We propose hetGPU, a
new system comprising a compiler, runtime, and abstraction layer that together
enable a single GPU binary to execute on NVIDIA, AMD, Intel, and Tenstorrent
hardware. The hetGPU compiler emits an architecture-agnostic GPU intermediate
representation (IR) and inserts metadata for managing execution state. The
hetGPU runtime then dynamically translates this IR to the target GPU's native
code and provides a uniform abstraction of threads, memory, and
synchronization. Our design tackles key challenges: differing SIMT vs. MIMD
execution (warps on NVIDIA/AMD vs. many-core RISC-V on Tenstorrent), varied
instruction sets, scheduling and memory model discrepancies, and the need for
state serialization for live migration. We detail the hetGPU architecture,
including the IR transformation pipeline, a state capture/reload mechanism for
live GPU migration, and an abstraction layer that bridges warp-centric and
core-centric designs. Preliminary evaluation demonstrates that unmodified GPU
binaries compiled with hetGPU can be migrated across disparate GPUs with
minimal overhead, opening the door to vendor-agnostic GPU computing.

</details>


### [276] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2是一个多功能大型语言模型家族，首次统一了RTL相关的生成和嵌入任务，填补了EDA领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在LLMs的生成任务上，而EDA工作流中同样关键的嵌入任务被忽视。

Method: 提出DeepRTL2模型，同时处理生成和嵌入任务。

Result: 实验表明，DeepRTL2在所有评估任务中达到最先进性能。

Conclusion: DeepRTL2为EDA领域的多样化挑战提供了全面解决方案。

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


### [277] [RCNet: $ΔΣ$ IADCs as Recurrent AutoEncoders](https://arxiv.org/abs/2506.16903)
*Arnaud Verdant,William Guicquero,Jérôme Chossat*

Main category: cs.AR

TL;DR: 论文提出了一种用于Delta-Sigma ADC的深度学习模型（RCNet），利用RNN描述调制器和滤波器，结合硬件设计约束优化SNR与面积。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用深度学习模型优化Delta-Sigma ADC的设计，特别是在硬件约束下提升性能。

Method: 采用RNN描述调制器和滤波器，结合高优化器和定制损失函数，考虑量化权重、信号饱和等硬件约束。

Result: 在DC转换中，RCNet成功优化了SNR（>13bit）与面积（<14pF）的权衡，且最佳架构不依赖高阶调制器。

Conclusion: RCNet为Delta-Sigma ADC设计提供了新的自由度，展示了深度学习在硬件优化中的潜力。

Abstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma
($\Delta\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both
modulators and filters. This analogy is applied to Incremental ADCs (IADC).
High-end optimizers combined with full-custom losses are used to define
additional hardware design constraints: quantized weights, signal saturation,
temporal noise injection, devices area. Focusing on DC conversion, our early
results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)
can be optimized under a certain hardware mapping complexity. The proposed
RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus
area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).
Interestingly, it appears that the best RCNet architectures do not necessarily
rely on high-order modulators, leveraging additional topology exploration
degrees of freedom.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [278] [Approximate Ricci-flat Metrics for Calabi-Yau Manifolds](https://arxiv.org/abs/2506.15766)
*Seung-Joo Lee,Andre Lukas*

Main category: hep-th

TL;DR: 提出了一种通过机器学习技术数值计算Ricci-flat Kähler势并结合Donaldson的Ansatz拟合的方法，用于确定Calabi-Yau流形上的解析Kähler势及其近似Ricci-flat Kähler度量。


<details>
  <summary>Details</summary>
Motivation: 研究Calabi-Yau流形上的Ricci-flat Kähler度量及其解析Kähler势，为相关数学和物理问题提供工具。

Method: 利用机器学习技术数值计算Ricci-flat Kähler势，并通过Donaldson的Ansatz拟合数值结果。

Result: 在Dwork家族的五次超曲面和双三次CY超曲面中，得到了简单的解析Kähler势表达式，且仅依赖于复结构参数的模。

Conclusion: 该方法成功获得了近似Ricci-flat Kähler势的解析表达式，为相关研究提供了新工具。

Abstract: We outline a method to determine analytic K\"ahler potentials with associated
approximately Ricci-flat K\"ahler metrics on Calabi-Yau manifolds. Key
ingredients are numerically calculating Ricci-flat K\"ahler potentials via
machine learning techniques and fitting the numerical results to Donaldson's
Ansatz. We apply this method to the Dwork family of quintic hypersurfaces in
$\mathbb{P}^4$ and an analogous one-parameter family of bi-cubic CY
hypersurfaces in $\mathbb{P}^2\times\mathbb{P}^2$. In each case, a relatively
simple analytic expression is obtained for the approximately Ricci-flat
K\"ahler potentials, including the explicit dependence on the complex structure
parameter. We find that these K\"ahler potentials only depend on the modulus of
the complex structure parameter.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [279] [Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings](https://arxiv.org/abs/2506.17064)
*Aditya Sengar,Ali Hariri,Daniel Probst,Patrick Barth,Pierre Vandergheynst*

Main category: q-bio.BM

TL;DR: LD-FPG是一种生成全原子蛋白质结构的框架，通过潜在扩散模型从分子动力学轨迹中生成多样化的构象，适用于复杂动态蛋白质如GPCRs。


<details>
  <summary>Details</summary>
Motivation: 理解动态蛋白质（如GPCRs）的功能需要生成多样化的全原子构象，但现有模型常简化原子细节或忽略构象多样性。

Method: LD-FPG使用Chebyshev图神经网络获取蛋白质构象的低维潜在嵌入，通过三种池化策略（盲、顺序和残基）处理，扩散模型生成新样本，解码器映射回笛卡尔坐标。

Result: 在D2R-MD数据集上，顺序和残基池化策略能高保真地复现参考构象（全原子lDDT约0.7，C-alpha-lDDT约0.8），且与MD数据的Jensen-Shannon散度小于0.03。

Conclusion: LD-FPG为大型蛋白质提供了一种系统特异性的全原子构象生成方法，是复杂动态靶点结构治疗设计的有力工具。

Abstract: Generating diverse, all-atom conformational ensembles of dynamic proteins
such as G-protein-coupled receptors (GPCRs) is critical for understanding their
function, yet most generative models simplify atomic detail or ignore
conformational diversity altogether. We present latent diffusion for full
protein generation (LD-FPG), a framework that constructs complete all-atom
protein structures, including every side-chain heavy atom, directly from
molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural
network (ChebNet) to obtain low-dimensional latent embeddings of protein
conformations, which are processed using three pooling strategies: blind,
sequential and residue-based. A diffusion model trained on these latent
representations generates new samples that a decoder, optionally regularized by
dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a
2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor
in a membrane environment, the sequential and residue-based pooling strategy
reproduces the reference ensemble with high structural fidelity (all-atom lDDT
of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone
and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of
less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route
to system-specific, all-atom ensemble generation for large proteins, providing
a promising tool for structure-based therapeutic design on complex, dynamic
targets. The D2R-MD dataset and our implementation are freely available to
facilitate further research.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [280] [Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection](https://arxiv.org/abs/2506.16522)
*Pedro Rodríguez Fernández,Christian Svinth,Alex Hagen*

Main category: physics.ins-det

TL;DR: 提出了一种利用辐射量子到达时间和神经网络注意力机制改进放射性核素检测限的方法，相比传统光谱方法提升2倍。


<details>
  <summary>Details</summary>
Motivation: 传统光谱方法在放射性核素检测中存在局限性，尤其是在复杂衰变链情况下，需要更高效的方法。

Method: 使用带有注意力机制的神经网络，结合辐射量子的到达时间，动态调整检测阈值。

Result: 在核设施释放铯的检测中，方法性能提升2倍。

Conclusion: 该方法不仅适用于铯，还可推广至其他复杂衰变链的放射性核素，并可能整合更多检测事件数据。

Abstract: We present a method to improve the detection limit for radionuclides using
spectroscopic radiation detectors and the arrival time of each detected
radiation quantum. We enable this method using a neural network with an
attention mechanism. We illustrate the method on the detection of Cesium
release from a nuclear facility during an upset, and our method shows $2\times$
improvement over the traditional spectroscopic method. We hypothesize that our
method achieves this performance increase by modulating its detection
probability by the overall rate of probable detections, specifically by
adapting detection thresholds based on temporal event distributions and local
spectral features, and show evidence to this effect. We believe this method is
applicable broadly and may be more successful for radionuclides with more
complicated decay chains than Cesium; we also note that our method can
generalize beyond the addition of arrival time and could integrate other data
about each detection event, such as pulse quality, location in detector, or
even combining the energy and time from detections in different detectors.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [281] [Sampling conditioned diffusions via Pathspace Projected Monte Carlo](https://arxiv.org/abs/2506.15743)
*Tobias Grafke*

Main category: stat.ML

TL;DR: 提出一种算法，用于在满足一般约束条件下采样随机微分方程，包括积分约束、端点约束和随机积分约束。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂约束条件下采样随机微分方程的挑战，扩展采样方法的适用性。

Method: 采用路径空间Metropolis调整流形采样方案，在满足约束条件的子流形上采样随机路径。

Result: 算法成功应用于多种场景，如动态凝聚相变、固定Levy随机面积约束的随机游走、高振幅波约束的随机非线性波动方程以及湍流管流模型的条件采样。

Conclusion: 该算法在复杂约束条件下表现出高效性和广泛适用性，为随机微分方程的采样提供了新工具。

Abstract: We present an algorithm to sample stochastic differential equations
conditioned on rather general constraints, including integral constraints,
endpoint constraints, and stochastic integral constraints. The algorithm is a
pathspace Metropolis-adjusted manifold sampling scheme, which samples
stochastic paths on the submanifold of realizations that adhere to the
conditioning constraint. We demonstrate the effectiveness of the algorithm by
sampling a dynamical condensation phase transition, conditioning a random walk
on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave
equation on high amplitude waves, and sampling a stochastic partial
differential equation model of turbulent pipe flow conditioned on
relaminarization events.

</details>


### [282] [From Local Interactions to Global Operators: Scalable Gaussian Process Operator for Physical Systems](https://arxiv.org/abs/2506.15906)
*Sawan Kumar,Tapas Tripura,Rajdip Nayek,Souvik Chakraborty*

Main category: stat.ML

TL;DR: 提出了一种可扩展的高斯过程算子（GPO），通过稀疏性、局部性和结构信息优化核设计，解决了高维数据场景下的计算复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 解决概率神经算子（如GPO）在高维、数据密集型场景中的扩展性问题。

Method: 采用最近邻局部核近似、参数空间的稀疏核近似和结构化Kronecker分解，结合算子感知的核结构和任务驱动的均值函数。

Result: 在多种非线性PDE（如Navier-Stokes、Burgers方程）上验证了高精度和可扩展性。

Conclusion: 该方法在可扩展性和准确性之间取得了平衡，为复杂物理系统中的不确定性建模提供了有效工具。

Abstract: Operator learning offers a powerful paradigm for solving parametric partial
differential equations (PDEs), but scaling probabilistic neural operators such
as the recently proposed Gaussian Processes Operators (GPOs) to
high-dimensional, data-intensive regimes remains a significant challenge. In
this work, we introduce a novel, scalable GPO, which capitalizes on sparsity,
locality, and structural information through judicious kernel design.
Addressing the fundamental limitation of cubic computational complexity, our
method leverages nearest-neighbor-based local kernel approximations in the
spatial domain, sparse kernel approximation in the parameter space, and
structured Kronecker factorizations to enable tractable inference on
large-scale datasets and high-dimensional input. While local approximations
often introduce accuracy trade-offs due to limited kernel interactions, we
overcome this by embedding operator-aware kernel structures and employing
expressive, task-informed mean functions derived from neural operator
architectures. Through extensive evaluations on a broad class of nonlinear PDEs
- including Navier-Stokes, wave advection, Darcy flow, and Burgers' equations -
we demonstrate that our framework consistently achieves high accuracy across
varying discretization scales. These results underscore the potential of our
approach to bridge the gap between scalability and fidelity in GPO, offering a
compelling foundation for uncertainty-aware modeling in complex physical
systems.

</details>


### [283] [Diffusion-Based Hypothesis Testing and Change-Point Detection](https://arxiv.org/abs/2506.16089)
*Sean Moushegian,Taposh Banerjee,Vahid Tarokh*

Main category: stat.ML

TL;DR: 本文扩展了基于分数的方法，提出了基于扩散的假设检验和变点检测算法，并理论量化了其性能，展示了数值优化的优势。


<details>
  <summary>Details</summary>
Motivation: 基于分数的方法在建模和生成中日益流行，但其假设检验和变点检测能力不如基于似然的方法。本文旨在通过扩散变换提升分数方法的性能。

Method: 将分数函数通过矩阵变换推广为扩散散度，扩展了基于分数的假设检验和变点检测算法，并理论分析了其性能。

Result: 提出了数值优化权重矩阵的方法，并通过模拟实验展示了基于扩散算法的优势。

Conclusion: 基于扩散的算法在特定场景下能实现最优性能，数值优化进一步提升了其表现。

Abstract: Score-based methods have recently seen increasing popularity in modeling and
generation. Methods have been constructed to perform hypothesis testing and
change-point detection with score functions, but these methods are in general
not as powerful as their likelihood-based peers. Recent works consider
generalizing the score-based Fisher divergence into a diffusion-divergence by
transforming score functions via multiplication with a matrix-valued function
or a weight matrix. In this paper, we extend the score-based hypothesis test
and change-point detection stopping rule into their diffusion-based analogs.
Additionally, we theoretically quantify the performance of these
diffusion-based algorithms and study scenarios where optimal performance is
achievable. We propose a method of numerically optimizing the weight matrix and
present numerical simulations to illustrate the advantages of diffusion-based
algorithms.

</details>


### [284] [CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization](https://arxiv.org/abs/2506.16189)
*Putri A. van der Linden,Alexander Timans,Erik J. Bekkers*

Main category: stat.ML

TL;DR: 论文研究了在几何数据变换下（如旋转或翻转）的共形预测问题，提出通过整合几何信息（如几何姿态）来恢复共形预测的保证，并在几何变换下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移下实用性受限，尤其是几何变换会降低模型性能，因此需要一种方法来恢复其保证并增强鲁棒性。

Method: 整合几何信息（如几何姿态）到共形预测过程中，利用姿态规范化作为信息提取器。

Result: 实验表明，结合几何信息与共形预测能够有效应对几何变换，同时适用于黑盒预测器。

Conclusion: 通过整合几何信息，共形预测在几何变换下仍能保持其保证和鲁棒性，具有广泛适用性。

Abstract: We study the problem of conformal prediction (CP) under geometric data
shifts, where data samples are susceptible to transformations such as rotations
or flips. While CP endows prediction models with post-hoc uncertainty
quantification and formal coverage guarantees, their practicality breaks under
distribution shifts that deteriorate model performance. To address this issue,
we propose integrating geometric information--such as geometric pose--into the
conformal procedure to reinstate its guarantees and ensure robustness under
geometric shifts. In particular, we explore recent advancements on pose
canonicalization as a suitable information extractor for this purpose.
Evaluating the combined approach across discrete and continuous shifts and
against equivariant and augmentation-based baselines, we find that integrating
geometric information with CP yields a principled way to address geometric
shifts while maintaining broad applicability to black-box predictors.

</details>


### [285] [Random feature approximation for general spectral methods](https://arxiv.org/abs/2506.16283)
*Mike Nguyen,Nicole Mücke*

Main category: stat.ML

TL;DR: 本文研究了随机特征方法的泛化性质，扩展了Tikhonov正则化的结果，涵盖多种谱正则化技术，包括梯度下降和加速算法。通过NTK框架，理论分析了神经网络和神经算子，获得了最优学习率。


<details>
  <summary>Details</summary>
Motivation: 随机特征近似是核方法在大规模学习算法中广泛使用的技术，但对其泛化性质的理论分析尚不完善，尤其是扩展到多种正则化技术和神经网络场景。

Method: 扩展了Tikhonov正则化的结果，提出一个框架分析多种谱正则化技术（如梯度下降、Heavy-Ball和Nesterov方法），并通过NTK理论分析神经网络。

Result: 在适当的源条件下，获得了最优学习率，即使对于不在再生核希尔伯特空间中的类别也适用，改进或完善了先前相关结果。

Conclusion: 该研究为随机特征方法和神经网络的泛化性质提供了更全面的理论支持，扩展了现有技术的适用范围。

Abstract: Random feature approximation is arguably one of the most widely used
techniques for kernel methods in large-scale learning algorithms. In this work,
we analyze the generalization properties of random feature methods, extending
previous results for Tikhonov regularization to a broad class of spectral
regularization techniques. This includes not only explicit methods but also
implicit schemes such as gradient descent and accelerated algorithms like the
Heavy-Ball and Nesterov method. Through this framework, we enable a theoretical
analysis of neural networks and neural operators through the lens of the Neural
Tangent Kernel (NTK) approach trained via gradient descent. For our estimators
we obtain optimal learning rates over regularity classes (even for classes that
are not included in the reproducing kernel Hilbert space), which are defined
through appropriate source conditions. This improves or completes previous
results obtained in related settings for specific kernel algorithms.

</details>


### [286] [The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units](https://arxiv.org/abs/2506.16289)
*Oswaldo Ludwig*

Main category: stat.ML

TL;DR: 论文探讨了神经网络权重张量的条件数与信息编码效率的关系，提出高条件数可能表明单元选择性地放大和压缩信息，并通过线性单元和高斯输入的案例验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中权重张量的条件数如何反映信息编码的效率，为理解网络学习机制提供新视角。

Method: 通过信息论框架分析条件数与信息熵的关系，结合线性单元和高斯输入的数学模型，并应用于多模态大语言模型的微调。

Result: 高条件数对应较低的信息传递总量，表明高效的编码策略；选择性微调方法可缓解跨模态适应中的灾难性遗忘。

Conclusion: 条件数可作为信息编码效率的指标，选择性微调方法为缓解灾难性遗忘提供了无需预训练统计的解决方案。

Abstract: This paper explores the relationship between the condition number of a neural
network's weight tensor and the extent of information encoded by the associated
processing unit, viewed through the lens of information theory. We argue that a
high condition number, though not sufficient for effective knowledge encoding,
may indicate that the unit has learned to selectively amplify and compress
information. We formalize this intuition, particularly for linear units with
Gaussian inputs, linking the condition number and the transformation's
log-volume scaling factor to the characteristics of the output entropy and the
geometric properties of the learned transformation. Our analysis demonstrates
that for a fixed weight norm, a concentrated distribution of singular values
(high condition number) corresponds to reduced overall information transfer,
indicating a specialized and efficient encoding strategy. Furthermore, we
present a practical case study where these principles are applied to guide
selective fine-tuning of a multimodal Large Language Model, aiming to mitigate
catastrophic forgetting during cross-modal adaptation. Unlike many existing
catastrophic forgetting mitigation methods that rely on access to pre-training
statistics, which are often unavailable, our selective fine-tuning approach
offers a way to bypass this common requirement.

</details>


### [287] [Identifying Heterogeneity in Distributed Learning](https://arxiv.org/abs/2506.16394)
*Zelin Xiao,Jia Gu,Song Xi Chen*

Main category: stat.ML

TL;DR: 论文研究了在分布式M估计中识别异质性参数分量的方法，提出了两种测试方法：基于重归一化Wald检验的方法和基于极端对比检验（ECT）的方法，并通过数值实验和案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在分布式M估计中，识别异质性参数分量是一个重要问题，但现有方法在数据块数量K较大或异质性稀疏时表现不佳。本文旨在提出更高效且通信成本低的方法来解决这一问题。

Method: 提出了两种方法：1）基于重归一化Wald检验的方法，适用于K较小且异质性密集的情况；2）基于极端对比检验（ECT）的方法，通过样本分割避免偏差累积，适用于K较大且异质性稀疏的情况。

Result: 实验表明，两种方法在控制家族错误率（FWER）和检验功效方面表现良好，且ECT方法在通信效率和操作简便性上具有优势。

Conclusion: 结合Wald检验和ECT的方法在不同异质性稀疏程度下均表现出稳健的检验功效，适用于实际应用。

Abstract: We study methods for identifying heterogeneous parameter components in
distributed M-estimation with minimal data transmission. One is based on a
re-normalized Wald test, which is shown to be consistent as long as the number
of distributed data blocks $K$ is of a smaller order of the minimum block
sample size {and the level of heterogeneity is dense}. The second one is an
extreme contrast test (ECT) based on the difference between the largest and
smallest component-wise estimated parameters among data blocks. By introducing
a sample splitting procedure, the ECT can avoid the bias accumulation arising
from the M-estimation procedures, and exhibits consistency for $K$ being much
larger than the sample size while the heterogeneity is sparse. The ECT
procedure is easy to operate and communication-efficient. A combination of the
Wald and the extreme contrast tests is formulated to attain more robust power
under varying levels of sparsity of the heterogeneity. We also conduct
intensive numerical experiments to compare the family-wise error rate (FWER)
and the power of the proposed methods. Additionally, we conduct a case study to
present the implementation and validity of the proposed methods.

</details>


### [288] [On Continuous Monitoring of Risk Violations under Unknown Shift](https://arxiv.org/abs/2506.16416)
*Alexander Timans,Rajeev Verma,Eric Nalisnick,Christian A. Naesseth*

Main category: stat.ML

TL;DR: 提出了一种实时监控机器学习系统风险违规的通用框架，利用顺序假设检验检测风险边界违规，同时控制误报率。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习系统面临动态且不可预测的分布变化，传统风险控制框架无法持续监控部署可靠性。

Method: 基于‘测试下注’范式，提出顺序假设检验方法，检测模型决策机制的风险违规，假设要求极低。

Result: 在异常检测和集合预测等多种分布变化下，验证了方法的有效性。

Conclusion: 该方法适用于广泛场景，能持续监控风险并控制误报率。

Abstract: Machine learning systems deployed in the real world must operate under
dynamic and often unpredictable distribution shifts. This challenges the
validity of statistical safety assurances on the system's risk established
beforehand. Common risk control frameworks rely on fixed assumptions and lack
mechanisms to continuously monitor deployment reliability. In this work, we
propose a general framework for the real-time monitoring of risk violations in
evolving data streams. Leveraging the 'testing by betting' paradigm, we propose
a sequential hypothesis testing procedure to detect violations of bounded risks
associated with the model's decision-making mechanism, while ensuring control
on the false alarm rate. Our method operates under minimal assumptions on the
nature of encountered shifts, rendering it broadly applicable. We illustrate
the effectiveness of our approach by monitoring risks in outlier detection and
set prediction under a variety of shifts.

</details>


### [289] [Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation](https://arxiv.org/abs/2506.16636)
*Rex Shen,Lu Tian*

Main category: stat.ML

TL;DR: 提出了一种基于掩码自回归流（MAF）的潜在噪声注入方法，用于生成合成数据，解决了高维场景下传统生成模型收敛慢的问题，同时满足差分隐私。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型在高维数据中收敛速度慢，且难以平衡隐私与效用。

Method: 通过潜在噪声注入扰动数据点，保持观测与合成数据的一一对应，结合元分析框架恢复经典效率。

Result: 方法在统计对齐和抗成员推理攻击方面表现优异，适用于隐私敏感领域。

Conclusion: 潜在噪声注入是隐私敏感领域中传统流采样方法的有效替代方案。

Abstract: Synthetic Data Generation has become essential for scalable,
privacy-preserving statistical analysis. While standard approaches based on
generative models, such as Normalizing Flows, have been widely used, they often
suffer from slow convergence in high-dimensional settings, frequently
converging more slowly than the canonical $1/\sqrt{n}$ rate when approximating
the true data distribution.
  To overcome these limitations, we propose a Latent Noise Injection method
using Masked Autoregressive Flows (MAF). Instead of directly sampling from the
trained model, our method perturbs each data point in the latent space and maps
it back to the data domain. This construction preserves a one to one
correspondence between observed and synthetic data, enabling synthetic outputs
that closely reflect the underlying distribution, particularly in challenging
high-dimensional regimes where traditional sampling struggles.
  Our procedure satisfies local $(\epsilon, \delta)$-differential privacy and
introduces a single perturbation parameter to control the privacy-utility
trade-off. Although estimators based on individual synthetic datasets may
converge slowly, we show both theoretically and empirically that aggregating
across $K$ studies in a meta analysis framework restores classical efficiency
and yields consistent, reliable inference. We demonstrate that with a
well-calibrated perturbation parameter, Latent Noise Injection achieves strong
statistical alignment with the original data and robustness against membership
inference attacks. These results position our method as a compelling
alternative to conventional flow-based sampling for synthetic data sharing in
decentralized and privacy-sensitive domains, such as biomedical research.

</details>


### [290] [Schrödinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres](https://arxiv.org/abs/2506.17197)
*Samuel Howard,Peter Potaptchik,George Deligiannidis*

Main category: stat.ML

TL;DR: 本文提出了一种扩展的IMF方法，用于解决树结构SB问题，并在Wasserstein重心计算中展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统IPF方法在树结构SB问题中表现不佳，而IMF方法具有更优特性，因此需要将其扩展到多边际OT问题中。

Method: 扩展IMF方法，通过顺序桥匹配步骤解决树结构SB问题，并将其应用于Wasserstein重心计算。

Result: 新算法继承了IMF在树结构中的优势，并在流式熵OT求解器中扩展了固定点方法。

Conclusion: 扩展的IMF方法在树结构SB问题和Wasserstein重心计算中表现出优越性，为多边际OT问题提供了实用解决方案。

Abstract: Recent advances in flow-based generative modelling have provided scalable
methods for computing the Schr\"odinger Bridge (SB) between distributions, a
dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic
cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB
problem via sequential bridge-matching steps, presenting an elegant and
practical approach with many favourable properties over the more traditional
Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting,
optimal transport can be generalised to the multi-marginal case in which the
objective is to minimise a cost defined over several marginal distributions. Of
particular importance are costs defined over a tree structure, from which
Wasserstein barycentres can be recovered as a special case. In this work, we
extend the IMF procedure to solve for the tree-structured SB problem. Our
resulting algorithm inherits the many advantages of IMF over IPF approaches in
the tree-based setting. In the specific case of Wasserstein barycentres, our
approach can be viewed as extending fixed-point approaches for barycentre
computation to the case of flow-based entropic OT solvers.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [291] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: 该课程探讨计算机图形学与科学之间的深层关系，强调其作为科学建模语言的作用，并鼓励图形学社区参与解决科学问题。


<details>
  <summary>Details</summary>
Motivation: 计算机图形学在科学领域有广泛应用，但两个领域之间存在词汇鸿沟，需要重新定义图形学作为科学的建模语言。

Method: 课程通过几何推理和物理建模等核心方法，为数据稀缺环境下的科学问题提供解决方案。

Result: 课程展示了图形学在科学中的成就和贡献，并提出了未来研究方向。

Conclusion: Graphics4Science旨在促进图形学与科学的合作，推动科学发现的未来发展。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [292] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种新的曲率代理方法，用于神经符号距离场（SDF）的几何学习，通过仅正则化混合二阶项（Weingarten项），减少计算和内存开销，同时保持几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用高斯曲率惩罚需要完整的Hessian评估和二阶自动微分，计算和内存成本高。

Method: 提出两种曲率代理实现：（1）有限差分代理，通过四次前向SDF评估和一次一阶梯度计算；（2）自动微分代理，通过一次Hessian-向量积计算混合导数。

Result: 在ABC基准测试中，代理方法在重建精度上与基于Hessian的基线相当或更好，同时减少GPU内存使用和运行时间约一半。

Conclusion: 该方法为工程级形状重建提供了一种可扩展、曲率感知的SDF学习路径。

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [293] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种基于PDE的图像生成方法，结合了流体动力学和深度学习，通过可逆的PDE过程生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 探索物理驱动的图像生成方法，结合流体动力学理论，提升生成图像的多样性和质量。

Method: 采用PDE驱动的图像退化过程，结合方向性平流、各向同性扩散和高斯噪声，通过GPU加速的Lattice Boltzmann求解器实现快速计算。

Result: 生成的图像在保持色彩一致性的同时，提升了多样性和质量，且框架能够泛化现有的PDE方法。

Conclusion: 该工作为基于扩散的图像生成提供了新的物理驱动视角，结合了流体动力学和深度学习。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [294] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 通过多平面同步技术扩展2D基础模型能力，提出DreamCube模型用于3D全景生成，实现了多样外观与精确几何。


<details>
  <summary>Details</summary>
Motivation: 解决3D全景数据稀缺及2D单视图与3D全景不兼容的问题。

Method: 应用多平面同步技术扩展2D基础模型，提出DreamCube多平面RGB-D扩散模型。

Result: 在全景图像生成、深度估计和3D场景生成中表现优异。

Conclusion: DreamCube有效利用2D先验，实现高质量3D全景生成。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [295] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: 论文提出REIS系统，通过优化存储内部处理（ISP）技术，显著提升检索增强生成（RAG）中近似最近邻搜索（ANNS）的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的知识受限于训练数据，检索增强生成（RAG）通过外部知识库补充LLMs的静态知识，但检索阶段成为性能瓶颈。现有ISP技术未针对ANNS优化，且硬件修改复杂。

Method: REIS系统采用三种机制：1）数据库布局优化，链接嵌入向量与文档；2）ISP定制数据放置技术；3）利用存储系统现有计算资源的ANNS引擎。

Result: 与服务器级系统相比，REIS平均提升检索性能13倍，能效55倍。

Conclusion: REIS是首个针对RAG优化的ISP系统，解决了现有技术的局限性，显著提升了检索效率和能效。

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [296] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: EvoLM是一个模型套件，用于系统分析语言模型在不同训练阶段（预训练、继续预训练、监督微调和强化学习）的动态表现。通过训练100多个模型，揭示了过度训练、领域适应和阶段间衔接的关键问题，并开源了所有资源。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型训练分为多个阶段，下游开发者难以评估各阶段设计选择的影响。EvoLM旨在提供透明和系统的分析工具。

Method: 训练了100多个1B和4B参数的模型，评估了语言建模和问题解决能力，包括领域内外泛化。

Result: 发现过度预训练和后期训练的收益递减，继续预训练对衔接阶段的重要性，以及监督微调和强化学习的复杂权衡。

Conclusion: EvoLM为研究提供了透明性和可复现性，开源了所有模型、数据集和评估流程。

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [297] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: 提出了一种基于LLaMA 3的检索增强生成框架（RAG），用于复杂问答任务，通过多跳推理和上下文融合提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多跳推理和长文档上下文理解中的挑战。

Method: 结合密集检索模块、上下文融合和多跳推理机制，采用联合优化策略（检索似然和生成交叉熵）。

Result: 实验表明，该系统优于现有检索增强和生成基线，提供更精确的上下文相关答案。

Conclusion: 该框架在复杂问答任务中表现出色，验证了其有效性和鲁棒性。

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [298] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: DynScaling通过集成并行-顺序采样策略和动态预算分配框架，提升大语言模型性能，无需外部验证器。


<details>
  <summary>Details</summary>
Motivation: 传统推理时间扩展方法依赖外部验证器或未优化实际计算约束，限制了其实际应用。

Method: 提出DynScaling，结合并行-顺序采样策略和动态预算分配框架，优化计算资源分配。

Result: 实验显示DynScaling在任务性能和计算成本上均优于现有基线方法。

Conclusion: DynScaling在资源约束下有效提升模型性能，无需外部验证器。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [299] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: 论文研究了将大语言模型（LLMs）应用于长文本的挑战，提出了一种理论框架，将长上下文任务的失败模式分为三类，并通过实验验证了多代理分块方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理长文本时遇到的挑战，如跨块依赖、模型噪声和结果整合问题。

Method: 提出理论框架分析失败模式，并通过多代理分块方法（将长序列分成小块并整合结果）进行实验验证。

Result: 实验表明多代理分块方法在检索、问答和摘要等任务中有效，且弱模型通过分块处理可能优于单次处理的强模型。

Conclusion: 论文提供了一个理论框架，证明了分块和整合策略是处理长上下文的有效途径。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [300] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: 论文提出了一种利用现有有害言论数据集检测隐式仇恨言论的方法，通过样本识别、重新标注和增强，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 隐式仇恨言论对社会媒体平台构成挑战，现有数据集可能未明确标注此类内容，且标注易受主观影响。

Method: 方法包括关键样本识别、重新标注和利用Llama-3 70B与GPT-4o进行数据增强。

Result: 实验结果显示，该方法在隐式仇恨检测上F1分数提升了12.9分。

Conclusion: 该方法有效提升了隐式仇恨言论的检测能力，具有跨数据集的通用性。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [301] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 论文提出了一种评估大语言模型（LLM）是否具备稳健世界模型的框架，通过分解模型响应的变异性来量化其语义基础。


<details>
  <summary>Details</summary>
Motivation: 评估LLM是否具备世界模型对于其在高风险应用中的可靠性至关重要。

Method: 提出了一种新评估方法，将模型响应变异性分解为用户目的、用户表达和模型不稳定性三个部分。

Result: 更大模型在用户目的变异性上表现更好，表明其世界模型更稳健，但优势并不一致且有限。

Conclusion: 需超越基于准确性的基准，采用语义诊断直接评估模型内部世界理解的结构和稳定性。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [302] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: 论文提出了一种利用LLMs辅助开发基于规则的NLP系统的新方法，显著提高了开发效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管ML和LLMs有进展，基于规则的NLP系统因其可解释性和操作效率仍在临床中使用，但其开发和维护成本高。

Method: 在规则系统开发阶段使用LLMs，实验聚焦于从临床笔记中提取相关片段和关键词。

Result: 实验显示高召回率（Deepseek: 0.98, Qwen: 0.99）和关键词提取准确率1.0。

Conclusion: 该方法为NLP开发提供了新方向，比深度学习方案更快、更经济、更透明。

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [303] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 研究分析了不同架构和大小的语言模型在样本效率上的表现，发现模型在高频事实上的表现相似，但在低频事实上差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型如何高效学习和记忆长尾分布中的信息，尤其是低频事实。

Method: 通过标注训练语料中关系事实的频率，比较不同模型在不同频率事实上的表现。

Result: 大多数模型在高频事实上表现相似，但在低频事实上表现差异显著。

Conclusion: 模型架构和大小对事实学习效率有显著影响，尤其是低频事实。

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [304] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 论文提出了一种语言瓶颈模型（LBM），通过自然语言摘要解决知识追踪（KT）问题，提高可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪方法依赖不透明的潜在嵌入，可解释性差；LLM方法可能产生不准确的预测或摘要。

Method: LBM由编码器LLM生成可解释的知识摘要，解码器LLM仅基于摘要重建和预测学生回答。通过自然语言瓶颈约束信息传递，确保摘要准确且可解释。

Result: 在合成算术基准和大规模Eedi数据集上，LBM的准确性媲美最先进的KT和LLM方法，且所需学生轨迹数据更少。

Conclusion: LBM通过自然语言摘要有效平衡了知识追踪的准确性和可解释性，并通过策略优化提升摘要质量。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [305] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: MUCAR是一个新的多模态基准测试，专注于评估多语言和跨模态场景中的模糊性解决能力，揭示了现有模型与人类水平的差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准测试通常忽略语言和视觉模糊性，未能充分利用模态间的相互澄清潜力。

Method: 引入MUCAR基准测试，包括多语言数据集和双模糊性数据集，通过视觉和文本上下文的相互澄清解决模糊性。

Result: 19种先进多模态模型的评估显示，其性能与人类水平存在显著差距。

Conclusion: 未来需研究更复杂的跨模态模糊性理解方法，推动多模态推理的边界。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [306] [Local Routing on Ordered $Θ$-graphs](https://arxiv.org/abs/2506.16021)
*André van Renssen,Shuei Sakaguchi*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The problem of locally routing on geometric networks using limited memory is
extensively studied in computational geometry. We consider one particular
graph, the ordered $\Theta$-graph, which is significantly harder to route on
than the $\Theta$-graph, for which a number of routing algorithms are known.
Currently, no local routing algorithm is known for the ordered $\Theta$-graph.
  We prove that, unfortunately, there does not exist a deterministic memoryless
local routing algorithm that works on the ordered $\Theta$-graph. This
motivates us to consider allowing a small amount of memory, and we present a
deterministic $O(1)$-memory local routing algorithm that successfully routes
from the source to the destination on the ordered $\Theta$-graph. We show that
our local routing algorithm converges to the destination in $O(n)$ hops, where
$n$ is the number of vertices. To the best of our knowledge, our algorithm is
the first deterministic local routing algorithm that is guaranteed to reach the
destination on the ordered $\Theta$-graph.

</details>


### [307] [Minimum-Weight Half-Plane Hitting Set](https://arxiv.org/abs/2506.16979)
*Gang Liu,Haitao Wang*

Main category: cs.CG

TL;DR: 本文提出了一种新的算法，将击中集问题的时间复杂度从O(n^{7/2}log^2 n)降低到O(n^{5/2}log^2 n)。


<details>
  <summary>Details</summary>
Motivation: 解决加权点和半平面的击中集问题，优化现有算法的时间复杂度。

Method: 提出了一种新的算法，改进了计算击中集的方法。

Result: 新算法的时间复杂度为O(n^{5/2}log^2 n)，优于之前的O(n^{7/2}log^2 n)。

Conclusion: 新算法显著提升了击中集问题的计算效率。

Abstract: Given a set $P$ of $n$ weighted points and a set $H$ of $n$ half-planes in
the plane, the hitting set problem is to compute a subset $P'$ of points from
$P$ such that each half-plane contains at least one point from $P'$ and the
total weight of the points in $P'$ is minimized. The previous best algorithm
solves the problem in $O(n^{7/2}\log^2 n)$ time. In this paper, we present a
new algorithm with runtime $O(n^{5/2}\log^2 n)$.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [308] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Main category: cs.CV

TL;DR: 本文提出了一种基于反馈强化学习和视觉语言模型的新框架，用于保护AI摄像头捕获的隐私敏感数据，通过将图像转换为语义等效的文本描述来平衡隐私与信息保留。


<details>
  <summary>Details</summary>
Motivation: AI摄像头在CAV中广泛应用，但传统隐私保护方法（如模糊处理）仍存在隐私泄露风险，需要更有效的解决方案。

Method: 采用反馈强化学习和视觉语言模型，将图像转换为语义等效的文本描述，并通过分层RL策略迭代优化文本生成。

Result: 实验结果显示，该方法在隐私保护和文本质量上显著优于现有方法，独特词数增加77%，细节密度提升50%。

Conclusion: 该框架为AI摄像头提供了一种有效的隐私保护方案，同时保留了场景相关信息，具有实际应用潜力。

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [309] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Main category: cs.CV

TL;DR: PanSegNet是一种深度学习算法，用于儿童胰腺MRI分割，在健康儿童和急慢性胰腺炎患者中表现优异，达到专家水平。


<details>
  <summary>Details</summary>
Motivation: 评估和验证PanSegNet在儿童胰腺MRI分割中的性能，填补儿科胰腺影像领域的空白。

Method: 回顾性收集84例MRI扫描，由放射科医生手动分割胰腺，PanSegNet生成的分割结果通过DSC和HD95评估。

Result: PanSegNet在健康儿童中DSC为88%，急慢性胰腺炎患者中分别为81%和80%，与手动分割结果高度一致。

Conclusion: PanSegNet是首个经过验证的儿科胰腺MRI分割深度学习工具，性能可靠且开源，推动无辐射儿科影像研究。

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [310] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的异构模态无监督域适应（HMUDA）方法，通过潜在空间桥接（LSB）框架实现跨模态知识迁移。


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域属于完全不同的模态时，传统无监督域适应方法效果不佳的问题。

Method: 提出LSB框架，采用双分支架构，结合特征一致性损失和域对齐损失。

Result: 在六个基准数据集上验证了LSB的优越性能。

Conclusion: LSB在异构模态无监督域适应任务中表现优异。

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [311] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: 论文提出了GRPO-CARE框架，通过双层次奖励机制优化多模态大语言模型的答案正确性和推理一致性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）在多模态大语言模型（MLLMs）中的应用尚未充分探索，且缺乏严格的评估基准。

Method: 提出SEED-Bench-R1基准，并设计GRPO-CARE框架，结合基础奖励和自适应一致性奖励，优化模型表现。

Result: GRPO-CARE在SEED-Bench-R1上表现优于标准GRPO，性能提升6.7%，一致性提高24.5%。

Conclusion: GRPO-CARE为多模态大语言模型提供了一种可推广的后训练框架，提升了模型的解释性和鲁棒性。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [312] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 使用生成对抗网络（GAN）从鸟瞰图（BEV）视频中生成统计准确的交通轨迹，以捕捉复杂多模态分布。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉未来轨迹的复杂多模态分布，因此探索GAN在生成真实轨迹中的潜力。

Method: 提出基于低分辨率BEV占用网格视频的GAN训练管道，从中提取抽象轨迹数据。

Result: 在100 GPU小时内完成训练，推理时间低于20毫秒，生成的轨迹在空间和动态参数上与真实数据对齐。

Conclusion: GAN能高效生成物理真实的交通轨迹，适用于自动驾驶场景。

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [313] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Main category: cs.CV

TL;DR: SyncMapV2是一种无监督分割算法，具有卓越的鲁棒性，能在噪声、天气和模糊等干扰下保持高精度，且无需重新初始化。


<details>
  <summary>Details</summary>
Motivation: 人类视觉在无明确训练下仍能有效分割视觉线索，而现有AI算法在类似条件下表现不佳。SyncMapV2旨在解决这一问题。

Method: 基于自组织动力学方程和随机网络概念，无需鲁棒训练或监督。

Result: 在数字干扰下，mIoU仅下降0.01%，远优于现有方法（23.8%）。在噪声、天气和模糊干扰下表现同样出色。

Conclusion: SyncMapV2首次实现了在线自适应分割，为未来鲁棒自适应智能奠定了基础。

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [314] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP的架构（CLIP-MG），用于微手势识别，通过结合姿态信息和多模态融合机制，在iMiGUE数据集上达到61.82%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 微手势因其细微、非自愿性和低幅度运动，在情感计算中具有挑战性。

Method: 采用Pose-Guided Semantics-Aware CLIP架构，结合姿态引导的语义查询生成和门控多模态融合机制。

Result: 模型在iMiGUE数据集上的Top-1准确率为61.82%。

Conclusion: 该方法展示了潜力，但也表明完全适应视觉语言模型（如CLIP）于微手势识别仍具挑战性。

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [315] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Main category: cs.CV

TL;DR: 提出了一种半自监督学习方法GLMask，用于实例分割，减少人工标注需求，在农业和通用数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 实例分割在农业等领域应用广泛，但大规模像素级标注成本高，尤其在密集遮挡场景中。

Method: 设计GLMask图像-掩码表示，关注形状、纹理和模式，减少对颜色特征的依赖；通过语义分割生成实例分割。

Result: 在小麦头实例分割任务中达到98.5% mAP@50，在COCO数据集上提升12.6% mAP@50。

Conclusion: 该方法不仅适用于农业，还可推广到其他类似数据特征的领域。

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [316] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: 研究了合成标注策略对文本到图像模型性能的影响，发现密集高质量标注提升文本对齐但可能牺牲美学和多样性，而随机长度标注则平衡美学和对齐。


<details>
  <summary>Details</summary>
Motivation: 探索合成标注设计对模型性能的影响，填补现有文献空白。

Method: 系统研究不同合成标注策略对模型性能的影响。

Result: 密集高质量标注提升对齐但影响美学和多样性；随机长度标注平衡性能；标注分布影响输出偏差。

Conclusion: 标注设计对模型性能至关重要，为文本到图像生成提供实用训练策略。

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [317] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gröger,Shuo Wen,Huyen Le,Maria Brbić*

Main category: cs.CV

TL;DR: 该论文提出了一种在有限配对数据下构建多模态模型的方法，通过对齐预训练的单模态基础模型，仅需少量样本即可实现高质量对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型依赖大量配对样本，成本高昂且难以获取，研究旨在解决资源受限领域的多模态学习问题。

Method: 引入STRUCTURE正则化技术，保持单模态编码器潜在空间的邻域几何结构，并优化对齐层选择。

Result: 在24个零样本图像分类和检索任务中，平均相对提升51.6%（分类）和91.8%（检索）。

Conclusion: 该方法为资源受限领域提供了高效的多模态学习框架，具有广泛适用性。

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [318] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Main category: cs.CV

TL;DR: 论文提出LAION-C作为ImageNet-C的替代基准，用于评估模型在web-scale数据集时代的OOD鲁棒性，发现当前模型在LAION-C上表现不佳，但部分模型已接近或超越人类表现。


<details>
  <summary>Details</summary>
Motivation: 现有OOD基准（如ImageNet-C）已无法有效评估web-scale数据集时代的模型鲁棒性，需设计新的基准。

Method: 引入LAION-C，包含六种新型失真类型，确保其对web-scale数据集仍为OOD，并评估了包括MLLMs在内的先进模型。

Result: LAION-C对当前模型构成显著挑战，但部分模型（如Gemini和GPT-4o）已接近或超越人类表现。

Conclusion: LAION-C为OOD鲁棒性评估提供了新标准，揭示了模型性能的范式转变：从落后于人类到接近或超越人类。

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [319] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Prmpt2Adpt是一种轻量级、高效的零样本域适应框架，通过提示驱动的特征对齐和师生范式，在资源受限环境中实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 解决现有提示驱动的无监督域适应方法依赖大型视觉语言模型且需完整源域数据的问题，适用于资源受限场景。

Method: 基于蒸馏和微调的CLIP模型作为教师模型骨干，通过提示驱动的实例归一化（PIN）对齐特征，生成伪标签指导学生模型适应。

Result: 在MDS-A数据集上表现优异，适应速度提升7倍，推理速度提升5倍。

Conclusion: Prmpt2Adpt是低资源领域中实用且可扩展的实时适应解决方案。

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


### [320] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本的轻量级方法，利用大型语言模型（LLMs）替代传统的视觉语言模型（VLMs），通过专家评论来检测足球比赛中的关键动作。


<details>
  <summary>Details</summary>
Motivation: 传统视频分析方法计算成本高且复杂，而专家评论提供了丰富的细粒度描述和上下文信息，足以可靠地识别关键动作。

Method: 使用SoccerNet Echoes数据集，通过三个专门化的LLMs（分别关注结果、兴奋度和战术）评估滑动窗口的评论，识别进球、黄牌等动作并生成精确时间戳。

Result: 实验表明，这种基于语言的方法在检测关键比赛事件上表现良好，为动作识别提供了一种轻量级且无需训练的替代方案。

Conclusion: 语言为中心的方法为动作识别提供了一种高效、轻量的解决方案，优于传统视频分析方法。

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [321] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: Part$^{2}$GS是一个用于建模多部件物体高保真几何和物理一致运动的新框架，通过部分感知的3D高斯表示和物理约束实现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中关节物体普遍存在，但其结构和运动的建模仍是3D重建方法的挑战。

Method: 利用部分感知的3D高斯表示编码可学习属性，结合物理约束（接触、速度一致性和矢量场对齐）和排斥点场防止碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部件的Chamfer距离上比现有方法提升10倍。

Conclusion: Part$^{2}$GS通过结构化和物理一致的表示，显著提升了关节物体建模的精度和运动连贯性。

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>
