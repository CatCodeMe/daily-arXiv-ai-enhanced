<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 54]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.PF](#cs.PF) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.SP](#eess.SP) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [The NIAID Discovery Portal: A Unified Search Engine for Infectious and Immune-Mediated Disease Datasets](https://arxiv.org/abs/2509.13524)
*Ginger Tsueng,Emily Bullen,Candice Czech,Dylan Welzel,Leandro Collares,Jason Lin,Everaldo Rodolpho,Zubair Qazi,Nichollette Acosta,Lisa M. Mayer,Sudha Venkatachari,Zorana Mitrović Vučičević,Poromendro N. Burman,Deepti Jain,Jack DiGiovanna,Maria Giovanni,Asiyah Lin,Wilbert Van Panhuis,Laura D. Hughes,Andrew I. Su,Chunlei Wu*

Main category: cs.DB

TL;DR: NIAID数据生态系统发现门户是一个统一的搜索平台，整合了400多万个传染病和免疫介导疾病研究相关数据集，提供用户友好的搜索过滤和API访问功能。


<details>
  <summary>Details</summary>
Motivation: 解决有价值生物医学数据集难以发现和访问的问题，降低数据重用门槛，支持传染病和免疫介导疾病研究社区的数据共享和二次利用。

Method: 集成领域特定和通用存储库的元数据，通过标准化关键元数据字段和统一异构格式，提供过滤功能、预构建查询、数据集集合以及程序化访问API。

Result: 建立了集中式可搜索接口，支持不同技术水平的用户发现和重用数据，提高了数据的可发现性、可访问性和可重用性。

Conclusion: 该门户通过降低重要生物医学数据集的访问障碍，作为研究入口点支持科学进步，最大化公共研究数据的投资回报。

Abstract: The NIAID Data Ecosystem Discovery Portal (https://data.niaid.nih.gov)
provides a unified search interface for over 4 million datasets relevant to
infectious and immune-mediated disease (IID) research. Integrating metadata
from domain-specific and generalist repositories, the Portal enables
researchers to identify and access datasets using user-friendly filters or
advanced queries, without requiring technical expertise. The Portal supports
discovery of a wide range of resources, including epidemiological, clinical,
and multi-omic datasets, and is designed to accommodate exploratory browsing
and precise searches. The Portal provides filters, prebuilt queries, and
dataset collections to simplify the discovery process for users. The Portal
additionally provides documentation and an API for programmatic access to
harmonized metadata. By easing access barriers to important biomedical
datasets, the NIAID Data Ecosystem Discovery Portal serves as an entry point
for researchers working to understand, diagnose, or treat IID.
  Valuable datasets are often overlooked because they are difficult to locate.
The NIAID Data Ecosystem Discovery Portal fills this gap by providing a
centralized, searchable interface that empowers users with varying levels of
technical expertise to find and reuse data. By standardizing key metadata
fields and harmonizing heterogeneous formats, the Portal improves data
findability, accessibility, and reusability. This resource supports hypothesis
generation, comparative analysis, and secondary use of public data by the IID
research community, including those funded by NIAID. The Portal supports data
sharing by standardizing metadata and linking to source repositories, and
maximizes the impact of public investment in research data by supporting
scientific advancement via secondary use.

</details>


### [2] [Tractability Frontiers of the Shapley Value for Aggregate Conjunctive Queries](https://arxiv.org/abs/2509.13565)
*Christoph Standke,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 本文研究了在聚合连接查询中计算元组Shapley值的复杂性，针对不同聚合函数（如min、max、count-distinct、average、quantile）确定了可计算Shapley值的层次化查询类，并证明这些类对于相应聚合函数是最大化的。


<details>
  <summary>Details</summary>
Motivation: Shapley值作为衡量元组对数据库查询结果贡献的重要指标，其计算复杂度问题尚未完全解决。之前的研究已经证明了对于非层次化查询的#P-hard性，但对于常见聚合函数（如min、max、count-distinct等）的复杂度问题仍为开放问题。

Method: 通过识别每个聚合函数对应的层次化查询类，分析在这些类中Shapley值的可计算性，并证明这些类对于相应聚合函数是最大化的（即超出这些类的查询都存在使Shapley值计算为#P-hard的局部值函数）。

Result: 发现不同聚合函数对应不同的层次化查询类：max、min和count-distinct对应all-hierarchical类，average和quantile对应更窄的q-hierarchical类。每个这样的类都是最大化的，确保了在这些类内Shapley值可高效计算，而类外则存在#P-hard情况。

Conclusion: 研究为不同聚合函数的Shapley值计算提供了完整的复杂度刻画，揭示了每个聚合函数对应不同的层次化查询类扩展，解决了之前提出的开放问题，并为数据库查询贡献度分析提供了理论基础。

Abstract: In recent years, the Shapley value has emerged as a general game-theoretic
measure for assessing the contribution of a tuple to the result of a database
query. We study the complexity of calculating the Shapley value of a tuple for
an aggregate conjunctive query, which applies an aggregation function to the
result of a conjunctive query (CQ) based on a value function that assigns a
number to each query answer. Prior work by Livshits, Bertossi, Kimelfeld, and
Sebag (2020) established that this task is #P-hard for every nontrivial
aggregation function when the query is non-hierarchical with respect to its
existential variables, assuming the absence of self-joins. They further showed
that this condition precisely characterizes the class of intractable CQs when
the aggregate function is sum or count. In addition, they posed as open
problems the complexity of other common aggregate functions such as min, max,
count-distinct, average, and quantile (including median). Towards the
resolution of these problems, we identify for each aggregate function a class
of hierarchical CQs where the Shapley value is tractable with every value
function, as long as it is local (i.e., determined by the tuples of one
relation). We further show that each such class is maximal: for every CQ
outside of this class, there is a local (easy-to-compute) value function that
makes the Shapley value #P-hard. Interestingly, our results reveal that each
aggregate function corresponds to a different generalization of the class of
hierarchical CQs from Boolean to non-Boolean queries. In particular, max, min,
and count-distinct match the class of CQs that are all-hierarchical (i.e.,
hierarchical with respect to all variables), and average and quantile match the
narrower class of q-hierarchical CQs introduced by Berkholz, Keppeler, and
Schweikardt (2017) in the context of the fine-grained complexity of query
answering.

</details>


### [3] [XASDB -- Design and Implementation of an Open-Access Spectral Database](https://arxiv.org/abs/2509.13566)
*Denis Spasyuk*

Main category: cs.DB

TL;DR: 本文介绍了XASDB，一个基于网络的X射线吸收光谱数据库平台，包含1000多个参考光谱，支持浏览器端数据处理和分析，促进FAIR数据原则和协作研究。


<details>
  <summary>Details</summary>
Motivation: 随着同步辐射设施产生的XAS数据量和复杂性不断增加，需要强大的数据管理、共享和分析基础设施来处理来自多个光束线和设施的多样化数据格式。

Method: 采用Node.js/MongoDB架构开发网络平台，集成XASproc JavaScript库进行浏览器端数据处理（归一化、背景扣除、EXAFS提取等），并通过XASVue光谱查看器提供跨设备可视化分析。

Result: 建立了包含40种元素、324种化学化合物的1000多个参考光谱的数据库，实现了标准化数据输出、完整元数据和集成分析能力，支持线性组合拟合、机器学习和教育应用。

Conclusion: XASDB展示了以网络为中心的方法在XAS数据分析中的潜力，通过促进FAIR数据原则和协作研究，加速材料科学、环境研究、化学和生物学领域的进展。

Abstract: The increasing volume and complexity of X-ray absorption spectroscopy (XAS)
data generated at synchrotron facilities worldwide require robust
infrastructure for data management, sharing, and analysis. This paper
introduces the XAS Database (XASDB), a comprehensive web-based platform
developed and hosted by the Canadian Light Source (CLS). The database houses
more than 1000 reference spectra spanning 40 elements and 324 chemical
compounds. The platform employs a Node.js/MongoDB architecture designed to
handle diverse data formats from multiple beamlines and synchrotron facilities.
A key innovation is the XASproc JavaScript library, which enables browser-based
XAS data processing including normalization, background sub- traction, extended
X-ray absorption fine structure (EXAFS) extraction, and preliminary analysis
traditionally limited to desktop applications. The integrated XASVue spectral
viewer provides installation-free data visualization and analysis with broad
accessibility across devices and operating systems. By offering standardized
data output, comprehensive metadata, and integrated analytical ca- pabilities,
XASDB facilitates collaborative research and promotes FAIR (Findable,
Accessible, In- teroperable, and Reusable) data principles. The platform serves
as a valuable resource for linear combination fitting (LCF) analysis, machine
learning applications, and educational purposes. This initiative demonstrates
the potential for web-centric approaches in XAS data analysis, accelerating
advances in materials science, environmental research, chemistry, and biology.

</details>


### [4] [Algorithms for Optimizing Acyclic Queries](https://arxiv.org/abs/2509.14144)
*Zheng Luo,Wim Van den Broeck,Guy Van den Broeck,Yisu Remy Wang*

Main category: cs.DB

TL;DR: 本文提出了三种构建无环查询连接树的方法：枚举所有连接树的算法、构建最浅连接树的算法，以及将左深线性计划转换为连接树的算法，为无环连接查询提供了新的优化技术。


<details>
  <summary>Details</summary>
Motivation: 传统查询优化主要关注二元连接算法（如哈希连接和排序合并连接），但近年来理论最优算法（如Yannakakis算法）受到关注。这些算法依赖于连接树，与二元连接的运算符树不同，需要新的优化技术。

Method: 1. 提出枚举α-无环查询所有连接树的算法，具有摊销常数延迟；2. 证明最大基数搜索算法可为Berge-无环查询构建唯一最浅连接树；3. 证明任何γ-无环查询的连通左深线性计划可通过简单算法转换为连接树。

Result: 为无环连接查询提供了三种有效的连接树构建方法，支持基于成本的优化、并行执行大型连接查询，以及重用二元连接的优化基础设施。

Conclusion: 这些方法为无环连接查询的优化提供了新的技术基础，能够更好地支持理论最优算法的应用，提高查询执行效率。

Abstract: Most research on query optimization has centered on binary join algorithms
like hash join and sort-merge join. However, recent years have seen growing
interest in theoretically optimal algorithms, notably Yannakakis' algorithm.
These algorithms rely on join trees, which differ from the operator trees for
binary joins and require new optimization techniques. We propose three
approaches to constructing join trees for acyclic queries. First, we give an
algorithm to enumerate all join trees of an alpha-acyclic query by edits with
amortized constant delay, which forms the basis of a cost-based optimizer for
acyclic joins. Second, we show that the Maximum Cardinality Search algorithm by
Tarjan and Yannakakis constructs a unique shallowest join tree, rooted at any
relation, for a Berge-acyclic query; this tree enables parallel execution of
large join queries. Finally, we prove that any connected left-deep linear plan
for a gamma-acyclic query can be converted into a join tree by a simple
algorithm, allowing reuse of optimization infrastructure developed for binary
joins.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [A User-centric Kubernetes-based Architecture for Green Cloud Computing](https://arxiv.org/abs/2509.13325)
*Matteo Zanotto,Leonardo Vicentini,Redi Vreto,Francesco Lumpp,Diego Braga,Sandro Fiore*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的用户中心绿色云计算架构，通过碳强度预测和绿色能源调度，在严格资源限制下实现13%的碳排放减少


<details>
  <summary>Details</summary>
Motivation: 数据中心规模增长导致电力消耗和CO2排放增加，云提供商虽接近最优能效但缺乏精确可持续性报告，需要在用户侧进一步改进

Method: 实现碳强度预测器，利用区域和时间变化调度工作负载，基于绿色能源可用性进行排放最小化调度

Result: 使用真实云工作负载执行轨迹评估，与轮询调度基线相比，在严格资源限制场景下可实现高达13%的排放减少

Conclusion: 用户中心的Kubernetes架构结合碳强度预测和绿色能源调度，能有效减少云计算碳排放

Abstract: To meet the increasing demand for cloud computing services, the scale and
number of data centers keeps increasing worldwide. This growth comes at the
cost of increased electricity consumption, which directly correlates to CO2
emissions, the main driver of climate change. As such, researching ways to
reduce cloud computing emissions is more relevant than ever. However, although
cloud providers are reportedly already working near optimal power efficiency,
they fail in providing precise sustainability reporting. This calls for further
improvements on the cloud computing consumer's side. To this end, in this paper
we propose a user-centric, Kubernetes-based architecture for green cloud
computing. We implement a carbon intensity forecaster and we use it to schedule
workloads based on the availability of green energy, exploiting both regional
and temporal variations to minimize emissions. We evaluate our system using
real-world traces of cloud workloads execution comparing the achieved carbon
emission savings against a baseline round-robin scheduler. Our findings
indicate that our system can achieve up to a 13% reduction in emissions in a
strict scenario with heavy limitations on the available resources.

</details>


### [6] [Testing and benchmarking emerging supercomputers via the MFC flow solver](https://arxiv.org/abs/2509.13575)
*Benjamin Wilfong,Anand Radhakrishnan,Henry A. Le Berre,Tanush Prathi,Stephen Abbott,Spencer H. Bryngelson*

Main category: cs.DC

TL;DR: MFC是一个计算流体动力学代码，配备了自动化工具链，用于测试和评估超级计算机性能，支持多种GPU和CPU架构，已发现编译器错误并在多个旗舰超级计算机上进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 部署新超级计算机需要通过应用程序代码进行测试和评估，需要便携、用户友好的工具来简化这一过程。

Method: 使用Multicomponent Flow Code (MFC)及其自动化工具链，包括输入生成、编译、批处理作业提交、回归测试和基准测试功能，评估不同编译器-硬件组合的正确性和性能。

Result: 测试了五代NVIDIA GPU、三代AMD GPU和各种CPU架构，使用Intel、Cray、NVIDIA、AMD和GNU编译器，发现了Frontier和El Capitan等新机器上的编译器错误和回归问题，已对约50个计算设备和5个旗舰超级计算机进行了基准测试。

Conclusion: MFC及其工具链为超级计算机评估提供了有效的解决方案，能够帮助用户在有限软件工程经验的情况下进行硬件-编译器组合的性能和正确性评估。

Abstract: Deploying new supercomputers requires testing and evaluation via application
codes. Portable, user-friendly tools enable evaluation, and the Multicomponent
Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this
need. MFC is adorned with a toolchain that automates input generation,
compilation, batch job submission, regression testing, and benchmarking. The
toolchain design enables users to evaluate compiler-hardware combinations for
correctness and performance with limited software engineering experience. As
with other PDE solvers, wall time per spatially discretized grid point serves
as a figure of merit. We present MFC benchmarking results for five generations
of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,
utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have
revealed compiler bugs and regressions on recent machines such as Frontier and
El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship
supercomputers.

</details>


### [7] [Modeling the Carbon Footprint of HPC: The Top 500 and EasyC](https://arxiv.org/abs/2509.13583)
*Varsha Rao,Andrew A. Chien*

Main category: cs.DC

TL;DR: 本文评估了Top 500超级计算机系统的碳足迹，使用EasyC工具在数据有限的情况下建模了391个系统的运营碳排放和283个系统的隐含碳排放，并首次提供了整个Top 500系统的碳足迹估算。


<details>
  <summary>Details</summary>
Motivation: 气候变化对HPC系统至关重要，但GHG协议的碳排放核算方法对单个系统困难，对系统集合几乎不可行，导致没有HPC范围的碳报告，甚至最大的HPC站点也不进行GHG协议报告。

Method: 利用Top500.org公开数据和EasyC工具，在有限数据可用性的情况下建模碳足迹，通过利用额外公共信息增强覆盖范围，然后使用插值法生成估算。

Result: 成功建模了391个HPC系统的运营碳排放（13.937亿吨CO2e/年）和283个系统的隐含碳排放（18.818亿吨CO2e），覆盖范围可提升至运营排放98%和隐含排放80.8%的系统。

Conclusion: EasyC工具能够用少量数据指标有效建模碳足迹，为HPC系统提供了可行的碳足迹评估方法，并预测了到2030年Top 500系统碳足迹的增长趋势。

Abstract: Climate change is a critical concern for HPC systems, but GHG protocol
carbon-emission accounting methodologies are difficult for a single system, and
effectively infeasible for a collection of systems. As a result, there is no
HPC-wide carbon reporting, and even the largest HPC sites do not do GHG
protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The
key challenge lies in modeling the carbon footprint with limited data
availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able
to model the operational carbon of 391 HPC systems and the embodied carbon of
283 HPC systems. We further show how this coverage can be enhanced by
exploiting additional public information. With improved coverage, then
interpolation is used to produce the first carbon footprint estimates of the
Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1
Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top
500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few
data metrics. We explore availability of data and enhancement, showing that
coverage can be increased to 98% of Top 500 systems for operational and 80.8%
of the systems for embodied emissions.

</details>


### [8] [GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach](https://arxiv.org/abs/2509.13703)
*Sriram Srinivasan,Hamdan Alabsi,Rand Obeidat,Nithisha Ponnala,Azene Zenebe*

Main category: cs.DC

TL;DR: 开发了一门GPU架构和编程的专门课程，涵盖硬件基础、并行计算到AI代理开发，通过AWS云平台进行实践教学，评估显示该课程有效提升了学生的技术能力和问题解决能力


<details>
  <summary>Details</summary>
Motivation: 为了满足现代计算密集型领域对并行计算技能的需求，设计一门将GPU硬件、编程和AI开发相结合的实践性课程，为STEM学生提供必要的技术准备

Method: 课程从GPU/CPU硬件基础和并行计算概念开始，逐步发展到RAG开发和GPU优化。学生通过配置云GPU实例、实现并行算法和部署可扩展AI解决方案获得实践经验

Result: 评估结果显示：(1)AWS是经济有效的GPU编程平台；(2)体验式学习显著提升了技术熟练度和参与度；(3)课程通过TensorBoard和HPC分析器等工具增强了学生的问题解决和批判性思维能力

Conclusion: 研究强调了将并行计算整合到STEM教育中的教学价值，建议在STEM课程中更广泛地采用类似选修课，以培养学生应对现代计算密集型领域需求的能力

Abstract: We present the design, implementation, and comprehensive evaluation of a
specialized course on GPU architecture, GPU programming, and how these are used
for developing AI agents. This course is offered to undergraduate and graduate
students during Fall 2024 and Spring 2025. The course began with foundational
concepts in GPU/CPU hardware and parallel computing and progressed to develop
RAG and optimizing them using GPUs. Students gained experience provisioning and
configuring cloud-based GPU instances, implementing parallel algorithms, and
deploying scalable AI solutions. We evaluated learning outcomes through
assessments, course evaluations, and anonymous surveys. The results reveal that
(1) AWS served as an effective and economical platform for practical GPU
programming, (2) experiential learning significantly enhanced technical
proficiency and engagement, and (3) the course strengthened students'
problem-solving and critical thinking skills through tools such as TensorBoard
and HPC profilers, which exposed performance bottlenecks and scaling issues.
Our findings underscore the pedagogical value of integrating parallel computing
into STEM education. We advocate for broader adoption of similar electives
across STEM curricula to prepare students for the demands of modern,
compute-intensive fields.

</details>


### [9] [LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology](https://arxiv.org/abs/2509.13978)
*Renan Souza,Timothy Poteet,Brian Etz,Daniel Rosendo,Amal Gueroudji,Woong Shin,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 本文提出了一种利用交互式大语言模型（LLM）代理进行运行时数据分析的方法论、参考架构和开源实现，通过轻量级元数据驱动设计将自然语言转换为结构化溯源查询。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现越来越依赖在边缘、云和高性能计算（HPC）连续体上处理数据的工作流。虽然工作流溯源技术支持数据分析，但在大规模情况下，溯源数据变得复杂且难以分析，现有系统依赖自定义脚本、结构化查询或静态仪表板，限制了数据交互。

Method: 采用轻量级元数据驱动设计，将自然语言翻译为结构化溯源查询，利用模块化设计、提示调优和检索增强生成（RAG）技术，在LLaMA、GPT、Gemini和Claude等多种LLM上进行评估。

Result: 评估结果显示，该方法能够超越记录的溯源数据，生成准确且有洞察力的LLM代理响应，覆盖多样化查询类别和真实世界的化学工作流。

Conclusion: 交互式LLM代理结合RAG技术可以有效解决大规模工作流溯源数据分析的复杂性，提供比传统方法更灵活和深入的数据交互分析能力。

Abstract: Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [GTA -- An ATSP Method: Shifting the Bottleneck from Algorithm to RAM](https://arxiv.org/abs/2509.13327)
*Wissam Nakhle*

Main category: cs.DS

TL;DR: 提出了一种可扩展的高性能算法GTA，能够在商用计算硬件上确定性地求解大规模非对称旅行商问题(ATSP)，最高可处理5000个节点(约2500万个二元变量)，在8逻辑处理器计算机上实现创纪录的求解速度。


<details>
  <summary>Details</summary>
Motivation: 传统TSP求解器依赖超级计算硬件，算法复杂度高。需要开发一种资源高效、确定性的替代方案，将求解瓶颈从算法复杂度转移到硬件资源(RAM和系统内存)，使其能够在广泛可用的计算机上解决大规模问题。

Method: 结合高效启发式热启动(能在秒级内达到接近最优解)和无需传统MTZ约束的子回路消除策略，采用Gurobi Tabu算法(GTA)，提供实时迭代跟踪和自适应接口。

Result: 在广泛使用的公共数据集上基准测试显示，能够一致解决高达5000个节点的大规模实例，收敛速度与高性能计算框架相当，实现了可重复的结果。

Conclusion: GTA算法代表了TSP求解的根本性转变，将瓶颈从算法复杂性转移到硬件资源，为物流、生物信息学和天文学等领域的调度工作流程提供了确定性、资源高效的解决方案。

Abstract: We present a scalable, high-performance algorithm that deterministically
solves large-scale instances of the Traveling Salesman problem (in its
asymmetric version, ATSP) to optimality using commercially available computing
hardware. By combining an efficient heuristic warm start, capable of achieving
near-optimality within seconds in some cases, with a subtour elimination
strategy that removes the need for traditional MTZ constraints, our approach
consistently resolves instances up to 5,000 nodes (approximately 25 million
binary variables) in record time on widely accessible computers, with eight
logical processors. We demonstrate reproducible results with convergence rates
comparable to those of high-performance computing frameworks. Real-time
iteration tracking and an adaptable interface allow seamless integration into
scheduling workflows in logistics, bioinformatics, and astronomy. Designed to
streamline solutions to large-scale TSP problems across disciplines, our
approach is benchmarked against widely used public datasets, offering a
deterministic, resource-efficient alternative to conventional solvers that rely
on supercomputing hardware. Our GTA (Gurobi Tabu Algorithm) algorithm is a
fundamental shift of TSP solution bottleneck from algorithmic complexity to the
underlying hardware (RAM and system memory), which is a highly desirable
characteristic.

</details>


### [11] [Hardness of Dynamic Core and Truss Decompositions](https://arxiv.org/abs/2509.13584)
*Yan S. Couto,Cristina G. Fernandes*

Main category: cs.DS

TL;DR: 本文证明了在动态图中计算k-core和近似核心值不存在高效算法，除非能改进矩阵乘法等基础算法。基于OMv和SETH猜想，表明不存在比朴素算法更快的动态算法，也不存在有界算法。但提出了2-core的多对数动态算法。


<details>
  <summary>Details</summary>
Motivation: k-core作为重要的凝聚子图模型，在动态图分析中受到广泛关注。Hanauer等人的调查提出了关于k-core动态算法效率的问题，本文旨在回答这些问题并解释为什么该领域研究主要关注有界算法。

Method: 基于OMv（在线矩阵向量乘法）和SETH（强指数时间假说）猜想，通过归约证明技术，建立了k-core动态计算的下界。同时为2-core设计了多对数时间复杂度的动态算法。

Result: 证明了：1）不存在高效的动态k-core算法；2）不存在(2-ε)-近似核心值的高效动态算法；3）不存在有界动态算法；4）这些下界同样适用于有向版本和k-truss问题；5）提出了2-core的多对数动态算法。

Conclusion: 本文解释了为什么k-core动态算法研究主要关注有界算法，但证明了这种算法也不存在。基于复杂性理论的下界结果揭示了该问题的内在难度，同时为2-core提供了可行的解决方案。

Abstract: The k-core of a graph is its maximal subgraph with minimum degree at least k,
and the core value of a vertex u is the largest k for which u is contained in
the k-core of the graph. Among cohesive subgraphs, k-core and its variants have
received a lot of attention recently, particularly on dynamic graphs, as
reported by Hanauer, Henzinger, and Schulz in their recent survey on dynamic
graph algorithms. We answer questions on k-core stated in the survey, proving
that there is no efficient dynamic algorithm for k-core or to find (2 -
{\epsilon})-approximations for the core values, unless we can improve
decade-long state-of-the-art algorithms in many areas including matrix
multiplication and satisfiability, based on the established OMv and SETH
conjectures. Some of our results show that there is no dynamic algorithm for
k-core asymptotically faster than the trivial ones. This explains why most
recent research papers in this area focus not on a generic efficient dynamic
algorithm, but on finding a bounded algorithm, which is fast when few core
values change per update. However, we also prove that such bounded algorithms
do not exist, based on the OMv conjecture. We present lower bounds also for a
directed version of the problem, and for the edge variant of the problem, known
as k-truss. On the positive side, we present a polylogarithmic dynamic
algorithm for 2-core.

</details>


### [12] [On Solving Asymmetric Diagonally Dominant Linear Systems in Sublinear Time](https://arxiv.org/abs/2509.13891)
*Tsz Chiu Kwok,Zhewei Wei,Mingji Yang*

Main category: cs.DS

TL;DR: 本文研究了在次线性时间内求解行/列对角占优线性系统的新方法，通过引入最大p-范数间隙概念，推广了对称对角占优系统的次线性求解技术到非对称情况。


<details>
  <summary>Details</summary>
Motivation: 将对称对角占优系统的次线性求解技术推广到非对称的行/列对角占优系统，为估计特定解与给定向量的内积提供高效的次线性算法。

Method: 使用Neumann级数表达解，提出最大p-范数间隙概念来刻画截断误差，并采用随机游走采样、局部推送及其双向组合等算法技术。

Result: 建立了针对有界最大p-范数间隙系统的多种算法结果，统一了对Forward Push和Backward Push两种基本方法的理解，并改进了PageRank和有效电阻估计的复杂度界限。

Conclusion: 该框架为次线性求解器、局部图算法和有向谱图理论的进一步研究奠定了基础，同时继承了现有硬度结果的下界证明。

Abstract: We initiate a study of solving a row/column diagonally dominant (RDD/CDD)
linear system $Mx=b$ in sublinear time, with the goal of estimating
$t^{\top}x^*$ for a given vector $t\in R^n$ and a specific solution $x^*$. This
setting naturally generalizes the study of sublinear-time solvers for symmetric
diagonally dominant (SDD) systems [AKP19] to the asymmetric case.
  Our first contributions are characterizations of the problem's mathematical
structure. We express a solution $x^*$ via a Neumann series, prove its
convergence, and upper bound the truncation error on this series through a
novel quantity of $M$, termed the maximum $p$-norm gap. This quantity
generalizes the spectral gap of symmetric matrices and captures how the
structure of $M$ governs the problem's computational difficulty.
  For systems with bounded maximum $p$-norm gap, we develop a collection of
algorithmic results for locally approximating $t^{\top}x^*$ under various
scenarios and error measures. We derive these results by adapting the
techniques of random-walk sampling, local push, and their bidirectional
combination, which have proved powerful for special cases of solving RDD/CDD
systems, particularly estimating PageRank and effective resistance on graphs.
Our general framework yields deeper insights, extended results, and improved
complexity bounds for these problems. Notably, our perspective provides a
unified understanding of Forward Push and Backward Push, two fundamental
approaches for estimating random-walk probabilities on graphs.
  Our framework also inherits the hardness results for sublinear-time SDD
solvers and local PageRank computation, establishing lower bounds on the
maximum $p$-norm gap or the accuracy parameter. We hope that our work opens the
door for further study into sublinear solvers, local graph algorithms, and
directed spectral graph theory.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: 本文探讨研究软件科学(RSS)是否应被视为元科学的一种形式，分析了分类对认可度、资金和研究整合的影响，认为RSS最好被理解为一个与元科学对齐的独特跨学科领域。


<details>
  <summary>Details</summary>
Motivation: 随着研究日益依赖计算方法，科学结果的可靠性取决于研究软件的质量、可重现性和透明度。确保这些品质对科学诚信和发现至关重要，需要明确RSS在科学体系中的定位。

Method: 通过定义元科学和RSS，比较两者的原则和目标，检查它们的重叠部分，分析支持和不支持将RSS分类为元科学的论点。

Result: 分析发现RSS推进了元科学的核心目标，特别是在计算可重现性方面，并连接了研究的技术、社会和认知方面。分类取决于采用广义还是狭义的元科学定义。

Conclusion: RSS最好被理解为一个独特的跨学科领域，与元科学对齐并在某些定义中属于元科学。无论分类如何，将科学严谨性应用于研究软件可确保发现工具符合发现本身的标准。

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [14] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 使用代理式LLM方法将法律条文转化为可执行代码，通过高阶蜕变测试和角色框架自动化测试生成，在复杂税法任务中表现优于前沿模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律关键场景中因模糊性和幻觉问题可靠性不足，需要开发可靠的方法将自然语言法规转化为可执行逻辑

Method: 提出基于代理的方法，使用高阶蜕变关系比较相似个体间的系统输出差异，采用LLM驱动的角色框架自动化测试生成和代码合成，实现多代理系统

Result: 使用较小模型(GPT-4o-mini)在最坏情况下通过率达到45%，显著优于前沿模型(GPT-4o和Claude 3.5的9-15%)

Conclusion: 代理式LLM方法为从自然语言规范开发鲁棒、可信赖的法律关键软件提供了可行路径

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [15] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG是一个将自然语言描述转换为可执行Apache Airflow DAG的方法论，通过混合方法实现了78.5%的成功率，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的数据丰富管道需要大量工程专业知识，需要一种方法来简化数据管道开发过程，使其更加民主化。

Method: 评估了四种生成方法（直接法、纯LLM法、混合法和基于模板法），使用13个LLM和5个案例研究进行260次实验，采用惩罚评分框架衡量可靠性、代码质量、结构完整性和可执行性。

Result: 混合方法表现最佳，成功率达78.5%，质量评分稳健（SAT: 6.79, DST: 7.67, PCT: 7.76），显著优于纯LLM法（66.2%）和直接法（29.2%）。混合方法的成本效益是直接提示法的两倍以上。

Conclusion: 结构化混合方法对于平衡自动化工作流生成的灵活性和可靠性至关重要，为数据管道开发的民主化提供了可行路径，可靠性而非内在代码质量是主要区分因素。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [16] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 使用大语言模型增强崩溃报告，通过添加故障位置、根因解释和修复建议，显著提升调试效率


<details>
  <summary>Details</summary>
Motivation: 崩溃报告通常缺乏足够的诊断细节，开发者调试效率低下，需要探索LLM是否能增强崩溃报告的有用性

Method: 研究两种增强策略：Direct-LLM（单次使用堆栈跟踪上下文）和Agentic-LLM（迭代探索代码库获取额外证据）

Result: 在492个真实崩溃报告数据集上，LLM增强报告将Top-1问题定位准确率从10.6%提升至40.2-43.1%，修复建议与开发者补丁相似度达56-57%

Conclusion: 为LLM提供堆栈跟踪和代码库信息可以生成显著更有用的增强崩溃报告，特别是Agentic-LLM在根因解释和修复指导方面表现更佳

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [17] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: GitHub Copilot代码审查功能在检测安全漏洞方面效果不佳，主要关注低严重性问题而非关键安全漏洞


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在软件开发中的普及，评估AI辅助代码审查工具在安全编码支持方面的实际效果变得至关重要

Method: 使用来自多个编程语言和应用领域的开源项目中精选的标记漏洞代码样本，系统评估Copilot检测常见安全漏洞的能力

Result: Copilot代码审查经常无法检测SQL注入、XSS和不安全反序列化等关键漏洞，主要反馈集中在编码风格和排版错误等低严重性问题

Conclusion: AI辅助代码审查的实际效果与预期能力存在显著差距，仍需专用安全工具和人工代码审计来确保软件安全

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [18] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest是首个针对Jupyter笔记本的回归测试框架，支持单元级断言和自动化生成，提高机器学习笔记本的可靠性


<details>
  <summary>Details</summary>
Motivation: 解决笔记本开发中测试支持有限的问题，防止因缺乏测试导致的性能回归和静默错误

Method: 开发NBTest框架，提供断言API库和JupyterLab插件，支持自动化生成数据预处理、模型构建和评估等关键组件的单元级断言

Result: 在592个Kaggle笔记本上生成21163个断言（平均每个35.75个），变异得分为0.57，能捕获回归错误，用户评分4.3/5（直观性）和4.24/5（实用性）

Conclusion: NBTest有效提高了机器学习笔记本的可靠性和可维护性，已被主流ML库采用，并通过统计技术最小化非确定性计算带来的不稳定性

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [19] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: PromptSE框架用于评估代码生成模型对提示词情感和风格变化的敏感性，发现性能与稳定性是解耦的优化目标，提出了AUC-E指标进行跨模型比较。


<details>
  <summary>Details</summary>
Motivation: 代码生成模型对提示词 phrasing 的敏感性未被充分研究，相同需求用不同情感或沟通风格表达会产生不同输出，而现有基准主要关注峰值性能。

Method: 创建语义相同但带有不同情感和个性模板的提示词变体，使用概率感知连续评分或二进制通过率评估稳定性，提出AUC-E曲线下面积指标进行跨模型比较。

Result: 在14个模型（Llama、Qwen、DeepSeek）上的研究表明，性能和稳定性是基本解耦的优化目标，揭示了挑战模型鲁棒性常见假设的架构和规模相关模式。

Conclusion: PromptSE能够量化性能与稳定性的权衡，将提示稳定性定位为与性能和公平性并列的补充评估维度，有助于构建更可信的AI辅助软件开发工具。

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [20] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 本文提出CodeEraser方法，通过机器遗忘技术有效删除代码语言模型中敏感信息的记忆，无需完全重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型存在隐私漏洞，会记忆敏感训练数据并可能泄露机密信息。现有方法需要完全重新训练，计算成本高昂。

Method: 使用梯度上升的机器遗忘方法，开发CodeEraser选择性遗忘代码中的敏感记忆片段，同时保持代码结构完整性和功能正确性。

Result: 在三个代码语言模型家族上的实验验证了CodeEraser在消除目标敏感记忆的同时保持模型效用的有效性和效率。

Conclusion: 机器遗忘是解决代码语言模型隐私问题的有效后处理方法，CodeEraser方法在保护隐私的同时维持了模型性能。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [21] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 本文系统分析了大推理模型(LRMs)在代码生成中的推理行为模式，通过人工标注构建了包含15种推理行为的分类法，揭示了不同模型的推理特点及其与代码正确性的关系，并提出了基于推理的提示策略改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大推理模型(LRMs)在代码生成任务中展现出多步推理能力，但缺乏对其推理模式的系统性分析，以及这些模式如何影响生成代码质量的研究。

Method: 使用代码生成任务提示多个先进LRMs模型，通过开放式编码手动标注推理轨迹，构建包含15种推理行为的分类法，涵盖四个推理阶段。

Result: 发现LRMs遵循类人编码工作流，复杂任务触发额外推理行为；不同模型推理模式差异显著(Qwen3迭代式，DeepSeek-R1-7B线性式)；单元测试和脚手架生成等行为与代码正确性强相关；基于推理的提示策略能有效改进代码质量。

Conclusion: 研究揭示了LRMs的推理行为模式及其对代码生成的影响，为推进自动代码生成提供了实践指导，展示了基于推理的提示策略的改进潜力。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [22] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS是首个基于频谱的多智能体系统故障溯源方法，通过轨迹重放和抽象分析来识别导致故障的具体智能体行为


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务中应用广泛，但故障溯源困难且人工成本高，阻碍了系统调试和改进

Method: 采用系统轨迹重放和抽象，结合针对MAS定制的新型可疑度公式，整合智能体行为组和动作行为组两个关键因素组

Result: 在Who and When基准测试中对比12个基线方法，FAMAS表现出优越性能，优于所有对比方法

Conclusion: FAMAS为多智能体系统提供了一种有效的自动化故障溯源解决方案，能够准确识别导致故障的智能体行为

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [23] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Trace Sampling 2.0通过span级别采样在保持trace结构完整性的同时大幅减少存储开销，Autoscope实现该方法，减少81.2% trace大小并保持98.1%故障span覆盖率


<details>
  <summary>Details</summary>
Motivation: 分布式追踪在微服务系统中是重要的诊断工具，但海量trace数据给后端存储带来巨大负担。传统trace采样方法通常会丢弃有价值的信息，包括用于对比分析的正样本trace

Method: 提出Trace Sampling 2.0，在span级别进行采样同时保持trace结构一致性。设计实现Autoscope，利用静态分析提取执行逻辑，确保关键span被保留而不损害结构完整性

Result: 在两个开源微服务上评估，减少81.2% trace大小，保持98.1%故障span覆盖率，优于现有trace级别采样方法。在根因分析中平均提升8.3%效果

Conclusion: Autoscope能显著提升微服务中的可观测性和存储效率，为性能监控提供强大解决方案

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [24] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: 本研究测试基于提示的大语言模型能否减少需求分类任务的数据需求，发现few-shot提示的LLM可以匹配或超越传统微调模型，添加persona和思维链能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据，成本高、领域依赖性强且泛化能力差。本研究探索使用提示工程的大语言模型来减少数据依赖并提高跨任务泛化能力。

Method: 在PROMISE和SecReq两个英文数据集上测试多种大语言模型和提示风格（zero-shot、few-shot、persona、chain of thought），并与强基线微调transformer模型进行比较。

Result: 基于提示的LLM，特别是few-shot提示，能够匹配或超越基线模型。添加persona或persona加思维链可以带来进一步的性能提升。

Conclusion: 基于提示的大语言模型是实用且可扩展的选择，能够减少对大标注数据的依赖，并提高跨任务的泛化能力。

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [25] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: 本文通过系统文献综述发现，生成式AI在软件建模教育中的伦理问题研究严重不足，在1386篇相关论文中仅有3篇明确讨论伦理考量，揭示了该领域亟需建立结构化伦理框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件建模教育中快速普及，但缺乏明确的伦理监督和教学指导，其伦理影响尚未得到充分探索。研究旨在系统识别和分析该领域的伦理问题研究现状。

Method: 对计算机科学六大数字图书馆（ACM、IEEE、Scopus、ScienceDirect、SpringerLink、Web of Science）进行系统文献综述，筛选讨论GenAI在软件建模教育中伦理方面的研究。

Result: 从1386篇独特论文中仅发现3篇明确处理伦理考量，包括责任、公平性、透明度、多样性和包容性等问题，表明该领域伦理讨论严重缺失。

Conclusion: 研究揭示了生成式AI在建模教育中伦理框架的严重缺失，迫切需要建立结构化伦理指导方针，以确保AI在建模课程中的负责任整合。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [26] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本文分析了自动化问题解决中LLM代理工具的失败模式，提出了一个包含3个主要阶段、9个类别和25个子类别的失败模式分类法，并设计了专家-执行者协作框架来解决推理缺陷和认知僵局问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化问题解决评估主要报告聚合问题解决率，掩盖了成功和失败的根本原因，难以诊断模型弱点或指导针对性改进。需要从高层次性能指标转向根本原因分析。

Method: 首先分析三种最先进工具在SWE-Bench-Verified中的性能和效率，然后对150个失败实例进行系统手动分析，建立失败模式分类法，最后提出专家-执行者协作框架来纠正推理缺陷和打破认知僵局。

Result: 开发了全面的失败模式分类法，揭示了两种架构范式的不同失败特征，代理失败主要源于推理缺陷和认知僵局。提出的协作框架解决了领先单代理22.2%之前无法解决的问题。

Conclusion: 通过诊断性评估和协作设计，为构建更强大的代理铺平了道路，专家-执行者框架能有效解决自动化问题解决中的关键失败模式。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [27] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 本研究探讨了如何将传统软件开发流程（瀑布模型、V模型、敏捷）作为协调框架应用于基于LLM的多智能体系统，分析了不同流程对代码质量、成本和效率的影响。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的多智能体系统在软件开发中的广泛应用，需要探索如何利用传统软件开发流程的结构化协调模式来指导这些智能体之间的协作，以提高系统性能和代码质量。

Method: 在11个不同的软件项目中，使用三种流程模型（瀑布、V模型、敏捷）和四种GPT变体进行了132次运行实验，通过标准化指标评估输出结果，包括代码规模、执行成本（时间和token使用）以及代码质量（代码异味和bug检测）。

Result: 流程模型和LLM选择都对系统性能有显著影响：瀑布模型效率最高，V模型产生最冗长的代码，敏捷模型获得最高代码质量但计算成本更高。

Conclusion: 传统软件流程可以有效地应用于基于LLM的多智能体系统，但每种流程在质量、成本和适应性方面都存在权衡。流程选择应根据项目目标来决定，无论是优先考虑效率、健壮性还是结构化验证。

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [28] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Chain-of-Thought推理虽然提升LLM准确性但带来高计算成本，研究发现过长推理反而有害。提出SEER自适应框架压缩推理长度，在保持准确性的同时减少42.1%推理长度。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽然能提升大语言模型在算术、逻辑和常识任务中的准确性和鲁棒性，但带来了高昂的计算成本（延迟、内存使用和KV缓存需求），特别是在需要简洁确定性输出的软件工程任务中。研究发现过长推理反而会导致截断、准确率下降和高达5倍的延迟。

Method: 提出SEER（Self-Enhancing Efficient Reasoning）自适应框架，结合Best-of-N采样和任务感知自适应过滤，通过预推理输出动态调整阈值来压缩推理长度并减少计算开销。

Result: 在三个软件工程任务和一个数学任务上评估，SEER平均缩短推理长度42.1%，通过减少截断提高准确性，并消除了大多数无限循环问题。

Conclusion: SEER是一种实用方法，即使在资源受限条件下也能使CoT增强的LLM更加高效和鲁棒，挑战了"推理越长越好"的假设，强调了自适应CoT控制的必要性。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [29] [GRU-Based Learning for the Identification of Congestion Protocols in TCP Traffic](https://arxiv.org/abs/2509.13490)
*Paul Bergeron,Sandhya Aneja*

Main category: cs.NI

TL;DR: 使用GRU神经网络模型在Marist大学校园网络中识别TCP Reno、TCP Cubic、TCP Vegas和BBR拥塞控制协议，准确率达到97.04%


<details>
  <summary>Details</summary>
Motivation: 在更复杂和竞争性的网络环境中，开发更快的神经网络架构来准确识别不同的TCP拥塞控制协议

Method: 采用基于GRU（门控循环单元）的深度学习模型，在Marist大学校园网络环境中进行协议识别

Result: 实现了97.04%的高准确率，在复杂网络环境中表现优于现有工作

Conclusion: GRU-based学习模型在复杂网络环境中能够有效识别多种TCP拥塞控制协议，准确率高且速度快

Abstract: This paper presents the identification of congestion control protocols TCP
Reno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an
accuracy of 97.04% using a GRU-based learning model. We used a faster neural
network architecture on a more complex and competitive network in comparison to
existing work and achieved comparably high accuracy.

</details>


### [30] [Odin: Effective End-to-End SLA Decomposition for 5G/6G Network Slicing via Online Learning](https://arxiv.org/abs/2509.13511)
*Duo Cheng,Ramanujan K Sheshadri,Ahan Kak,Nakjung Choi,Xingyu Zhou,Bo Ji*

Main category: cs.NI

TL;DR: Odin是一个基于贝叶斯优化的网络切片SLA分解解决方案，通过利用各域的在线反馈实现高效分解，在SLA满足率方面比基线方法提升45%，同时降低总体资源成本。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络切片需要跨多个域部署端到端SLA，但由于域间异构性、动态网络条件以及SLA编排器对域资源优化的不可知性，SLA分解具有高度挑战性。

Method: 提出基于贝叶斯优化的Odin解决方案，利用各域的在线反馈进行可证明高效的SLA分解。

Result: 理论分析和严格评估表明，Odin的端到端编排器在SLA满足率方面比基线解决方案提升高达45%，同时降低总体资源成本，即使在存在域噪声反馈的情况下也能保持性能。

Conclusion: Odin通过贝叶斯优化方法有效解决了跨域SLA分解的挑战，在提升SLA满足率的同时优化资源利用率，为5G/6G网络切片管理提供了有效的解决方案。

Abstract: Network slicing plays a crucial role in realizing 5G/6G advances, enabling
diverse Service Level Agreement (SLA) requirements related to latency,
throughput, and reliability. Since network slices are deployed end-to-end
(E2E), across multiple domains including access, transport, and core networks,
it is essential to efficiently decompose an E2E SLA into domain-level targets,
so that each domain can provision adequate resources for the slice. However,
decomposing SLAs is highly challenging due to the heterogeneity of domains,
dynamic network conditions, and the fact that the SLA orchestrator is oblivious
to the domain's resource optimization. In this work, we propose Odin, a
Bayesian Optimization-based solution that leverages each domain's online
feedback for provably-efficient SLA decomposition. Through theoretical analyses
and rigorous evaluations, we demonstrate that Odin's E2E orchestrator can
achieve up to 45% performance improvement in SLA satisfaction when compared
with baseline solutions whilst reducing overall resource costs even in the
presence of noisy feedback from the individual domains.

</details>


### [31] [A Framework for Multi-source Prefetching Through Adaptive Weight](https://arxiv.org/abs/2509.13604)
*Yoseph Berhanu Alebachew,Mulugeta Libsie*

Main category: cs.NI

TL;DR: 提出了一种新的预取框架，能够整合基于历史访问和语义信息的预取方案，通过自适应权重管理技术调整各算法的影响，相比现有方案更加节省资源。


<details>
  <summary>Details</summary>
Motivation: 解决Web用户感知延迟问题，现有预取方法主要基于历史访问数据，难以利用应用层面的文档语义关系，且缺乏可扩展的整合方案。

Method: 设计了一个可扩展框架，允许集成不同类型的预取算法，每个算法生成候选对象列表，通过自适应权重管理技术根据性能动态调整各算法的影响力。

Result: 该框架比现有方案更加节省资源（less aggressive），特别适合资源受限的移动设备使用。

Conclusion: 提出的框架能够有效整合不同预取方案，具有良好的扩展性和适应性，特别适用于现代移动Web访问环境。

Abstract: The World Wide Web has come to be a great part of our daily life, yet user
observed latency is still a problem that needs a proper means of handling. Even
though earlier attempts focused on caching as the chief solution to tackling
this issue, its success was extremely limited. Prefetching has come to be the
primary technique in supplementing caching towards soothing the latency problem
associated with the contemporary Internet. However, existing approaches in
prefetching are extremely limited in their ability to employ application level
web document relationship which is often visible only to the content developer.
This is because most approaches are access history based schemes that make
future users' access prediction only based on past user access. Attempts to
incorporate prefetching schemes that utilize semantic information with those
that use users past access history are extremely limited in their
extensibility. In this work we present a novel framework that enables
integration of schemes from both worlds of prefetching without the need for a
major modification to the algorithms. When there is a need/possibility to
capture new application level context, a new algorithm could be developed to do
so and then it can be integrated into the framework. Since each participating
scheme is merely viewed as an algorithm that produces a list of candidate
objects that are likely to be accessed in the near future, the framework can
entertain any one of the existing prefetching schemes. With its adaptive weight
management technique the framework adjusts the effect of each algorithm in the
overall prediction to parallel with its observed performance so far. We have
found this formwork to be less aggressive than its contemporary counterparts
which is extremely important for resource constrained mobile devices that have
come to be the major means of access by users of the current web.

</details>


### [32] [LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones](https://arxiv.org/abs/2509.13714)
*Benoit Pit-Claudel,Muriel Médard,Manya Ghobadi*

Main category: cs.NI

TL;DR: LINC是一种新型网络编码系统，通过在网络内部进行逐链路编码和解码来缓解环境因素导致的丢包，无需终端主机配合，可减少18%的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 混合骨干网络（光纤、卫星、微波）虽然提供低延迟，但会因恶劣天气、施工等环境因素导致偶发包丢失。传统传输协议将丢包误判为网络拥塞，而现有网络编码方案需要终端主机完全配合，限制了实际应用。

Method: LINC采用系统性分组编码方法，在网络内部逐链路进行编码和解码操作，无需终端主机参与。通过建模重传与冗余数据包之间的吞吐量权衡，提出优化公式来确定最佳编码参数。

Result: 在真实骨干网络拓扑上的模拟显示，LINC通过消除不必要的重传，将端到端延迟降低了高达18%。

Conclusion: LINC系统有效解决了混合网络中环境丢包问题，通过网内编码避免了终端主机改造的需求，显著提升了网络性能。

Abstract: The emergence of ultra-low latency applications, such as financial
transactions, has driven the development of hybrid backbone networks that rely
on fiber, satellite, and microwave links. Despite providing low latencies,
these hybrid networks suffer from occasional environmental packet loss caused
by poor weather, construction, and line of sight blockage. Paradoxically,
today's hybrid backbones rely on conventional transport protocols that take
packet loss to signal network congestion, as opposed to transient environmental
obstacles. A common approach to address this challenge is to use network coding
(NC) between the end hosts to recover from these occasional packet loss events.
However, current NC proposals assume full access to the end-hosts' stack to
perform end-to-end encoding/decoding operations. In this paper, we introduce
LINC, a novel system that provides in-network NC capabilities to mitigate
environmental packet loss events without requiring cooperation from the end
hosts. LINC uses a systematic block coding approach on a link-by-link basis,
encoding and decoding packets inside the network. We model the tradeoff in
goodput between end-to-end retransmissions and redundant packets introduced by
LINC, and propose an optimization formulation to determine the optimal choice
of coding parameters. Our simulations on real-world backbone topologies
demonstrate that LINC reduces the end-to-end latency by up to 18% by
eliminating unnecessary retransmissions.

</details>


### [33] [Conducting Mission-Critical Voice Experiments with Automated Speech Recognition and Crowdsourcing](https://arxiv.org/abs/2509.13724)
*Jan Janak,Kahlil Dozier,Lauren Berny,Liang Hu,Dan Rubenstein,Charles Jennings,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 本文开发了关键任务语音通信系统在模拟真实环境中的测试方法和工具，包括ASR机器人和基于编辑距离的QoE评估指标，通过人机对比实验发现人类在准确性任务上优于ASR，且编解码器对QoE和ASR性能有显著影响


<details>
  <summary>Details</summary>
Motivation: 公共安全关键语音通信系统需要可靠运行，但现有研究受限于真实环境模拟困难，且缺乏准确反映用户体验质量(QoE)的评估指标

Method: 开发了模拟真实环境的测试平台和ASR机器人，使用基于编辑距离的指标评估QoE，通过Amazon MTurk志愿者进行人机对比实验

Result: 人类在准确性相关任务上普遍优于ASR，编解码器对终端用户QoE和ASR性能有显著影响

Conclusion: 建立了有效的MCV系统测试方法论，证明了基于编辑距离的指标适合评估理解能力和QoE，为人机性能对比提供了基准

Abstract: Mission-critical voice (MCV) communications systems have been a critical tool
for the public safety community for over eight decades. Public safety users
expect MCV systems to operate reliably and consistently, particularly in
challenging conditions. Because of these expectations, the Public Safety
Communications Research (PSCR) Division of the National Institute of Standards
and Technology (NIST) has been interested in correlating impairments in MCV
communication systems and public safety user quality of experience (QoE).
Previous research has studied MCV voice quality and intelligibility in a
controlled environment. However, such research has been limited by the
challenges inherent in emulating real-world environmental conditions.
Additionally, there is the question of the best metric to use to reflect QoE
accurately.
  This paper describes our efforts to develop the methodology and tools for
human-subject experiments with MCV. We illustrate their use in human-subject
experiments in emulated real-world environments. The tools include a testbed
for emulating real-world MCV systems and an automated speech recognition (ASR)
robot approximating human subjects in transcription tasks. We evaluate QoE
through a Levenshtein Distance-based metric, arguing it is a suitable proxy for
measuring comprehension and the QoE. We conducted human-subject studies with
Amazon MTurk volunteers to understand the influence of selected system
parameters and impairments on human subject performance and end-user QoE. We
also compare the performance of several ASR system configurations with
human-subject performance. We find that humans generally perform better than
ASR in accuracy-related MCV tasks and that the codec significantly influences
the end-user QoE and ASR performance.

</details>


### [34] [Performance Evaluation of Intent-Based Networking Scenarios: A GitOps and Nephio Approach](https://arxiv.org/abs/2509.13901)
*Saptarshi Ghosh,Ioannis Mavromatis,Konstantinos Antonakoglou,Konstantinos Katsaros*

Main category: cs.NI

TL;DR: 本文对三种主流GitOps工具(Argo CD、Flux CD、ConfigSync)在意图驱动网络场景下的性能进行了可复现的基准测试，评估了延迟和资源开销，并分析了工具间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: GitOps已成为云原生基础设施管理的基础范式，但在意图驱动网络(IBN)场景中，GitOps工具的性能和可扩展性缺乏充分评估。

Method: 采用可复现的度量驱动基准测试方法，在单意图和多意图场景下进行受控实验，测量延迟和资源消耗等关键性能指标，并使用Nephio作为编排器研究实际的编排场景。

Result: 研究结果揭示了不同工具在确定性、资源效率和响应性方面的权衡关系，并量化了声明式端到端部署流水线中的处理延迟和开销。

Conclusion: 研究结果为未来自主网络编排系统中的工具选择和优化提供了有价值的见解。

Abstract: GitOps has emerged as a foundational paradigm for managing cloud-native
infrastructures by enabling declarative configuration, version-controlled
state, and automated reconciliation between intents and runtime deployments.
Despite its widespread adoption, the performance and scalability of GitOps
tools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.
This paper presents a reproducible, metric-driven benchmarking, assessing the
latency and resource overheads of three widely used GitOps operators: Argo CD,
Flux CD, and ConfigSync. We conduct controlled experiments under both single-
and multi-intent scenarios, capturing key performance indicators such as
latency and resource consumption. Our results highlight trade-offs between the
tools in terms of determinism, resource efficiency, and responsiveness. We
further investigate a realistic orchestration scenario, using Nephio as our
orchestrator, to quantify the processing latency and overhead in declarative
end-to-end deployment pipelines. Our findings can offer valuable insights for
tool selection and optimisation in future autonomous network orchestration
systems.

</details>


### [35] [Low-cost Highly-interoperable Multiplatform Campus Network: Experience of YARSI University](https://arxiv.org/abs/2509.13954)
*Surya Agustian,Sandra Permana,Salman Teguh Pratista,Syarifu Adam,Iswandi*

Main category: cs.NI

TL;DR: YARSI大学通过结合开源系统和本地组装的PC作为网关路由器，设计了一个低成本的校园网络方案，显著降低了网络基础设施和互联网接入的成本。


<details>
  <summary>Details</summary>
Motivation: 许多组织认为建设校园网络成本高昂，特别是缺乏IT知识的组织如果外包给第三方而没有监督，会导致更大的意外开支。

Method: 使用开源操作系统运行在本地组装的个人电脑作为网关和路由器，结合思科的交换技术，设计基于UTP的低成本校园网络，并通过多个宽带连接和专用无线网络共享互联网接入。

Result: 成功为100多个同时用户提供互联网接入，显著降低了网络基础设施的采购、维护和运营成本。

Conclusion: 这种低成本校园网络和互联网连接的设计模式可以被农村社区或预算有限的组织采用来实现互联网接入。

Abstract: To some organizations, building campus network is sometimes considered to be
very expensive; and this has made the project uneasy to perform. Moreover, if
the organization without sufficient IT knowledge does not have capable IT
engineers, leaving this project to third parties without supervision would lead
to unexpected larger expenses. For this reason, in the year of 2003, YARSI
University formed CMIS (Center for Management Infor-mation System) to perform
tasks in designing, operations and maintenance of campus network and its
services. By combining Open Source operating system run on a local assembled
personal computer as gateway and router, and switching technology from Cisco,
we designed a low-cost UTP-based campus network which covering rooms and
buildings in YARSI environment. Meanwhile the internet access through several
broadband connections and dedicated wireless was shared to more than 100
simultaneous users by a captive portal system. With this strategy, we can
significantly reduce cost for purchasing, maintenance and operations of network
infrastructure and internet access. Our model in designing low-cost campus
network and internet connections could be adopted by rural community or
organizations that have limited budget to have internet access.

</details>


### [36] [Path-Oblivious Entanglement Swapping for the Quantum Internet](https://arxiv.org/abs/2509.13993)
*Vincent Mutolo,Rhea Parekh,Dan Rubenstein*

Main category: cs.NI

TL;DR: 本文提出了一种路径无关的Bell对交换协议，相比传统的预定路径方法，在量子网络资源充足的情况下可能表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统量子互联网中的Bell对交换协议采用预定路径方式，但经典网络经验表明灵活的无预留方法在资源充足的网络中往往表现更优。随着量子态变得更加廉价和稳定，路径无关方法更具优势。

Method: 将交换过程建模为线性规划问题，提出并评估了一个相对简单的基线交换协议，该协议尝试在网络中平衡Bell对分布。

Result: 初步结果显示，虽然简单的平衡方法还有改进空间，但研究路径无关的交换是一个有前景的方向。

Conclusion: 路径无关的Bell对交换方法在量子网络发展中具有重要价值，值得进一步深入研究。

Abstract: Proposed Bell pair swapping protocols, an essential component of the Quantum
Internet, are planned-path: specific, structured, routing paths are reserved
prior to the execution of the swapping process. This makes sense when one
assumes the state used in the swapping process is expensive, fragile, and
unstable. However, lessons from classical networking have shown that while
reservations seem promising in concept, flexible, reservation-light or free
approaches often outperform their more restrictive counterparts in
well-provisioned networks. In this paper, we propose that a path-oblivious
approach is more amenable to supporting swapping as quantum state evolves into
a cheaper, more robust form. We formulate the swapping process as a linear
program and present and evaluate a fairly naive baseline swapping protocol that
tries to balance Bell pairs throughout the network. Preliminary results show
that while naive balancing leaves room for improvement, investigating
path-oblivious swapping is a promising direction.

</details>


### [37] [RepCaM++: Exploring Transparent Visual Prompt With Inference-Time Re-Parameterization for Neural Video Delivery](https://arxiv.org/abs/2509.14002)
*Rongyu Zhang,Xize Duan,Jiaming Liu,Li Du,Yuan Du,Dan Wang,Shanghang Zhang,Fangxin Wang*

Main category: cs.NI

TL;DR: RepCaM++是一个基于重参数化内容感知调制模块的创新框架，通过并行级联参数训练和推理时重参数化技术，解决了视频传输中参数累积问题，同时结合透明视觉提示提升细节恢复质量，在VSD4K数据集上取得了最先进的视频恢复质量和带宽压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有的内容感知方法为每个视频块训练单独的SR模型，导致参数累积问题，随着视频长度增加会带来传输成本上升和性能下降。需要一种能够统一调制视频块同时保持高效参数使用的方法。

Method: 提出RepCaM++框架，包含重参数化内容感知调制(RepCaM)模块，在训练时集成并行级联参数适应多个视频块，在推理时通过重参数化消除额外参数。还提出透明视觉提示(TVP)，使用极少量零初始化图像级参数捕获视频块细节。

Result: 在VSD4K数据集（包含6种不同视频场景）上进行广泛实验，在视频恢复质量和传输带宽压缩方面达到了最先进的性能。

Conclusion: RepCaM++框架通过创新的重参数化技术和透明视觉提示，有效解决了内容感知视频传输中的参数累积问题，实现了高质量的带宽压缩和视频恢复。

Abstract: Recently, content-aware methods have been employed to reduce bandwidth and
enhance the quality of Internet video delivery. These methods involve training
distinct content-aware super-resolution (SR) models for each video chunk on the
server, subsequently streaming the low-resolution (LR) video chunks with the SR
models to the client. Prior research has incorporated additional partial
parameters to customize the models for individual video chunks. However, this
leads to parameter accumulation and can fail to adapt appropriately as video
lengths increase, resulting in increased delivery costs and reduced
performance. In this paper, we introduce RepCaM++, an innovative framework
based on a novel Re-parameterization Content-aware Modulation (RepCaM) module
that uniformly modulates video chunks. The RepCaM framework integrates extra
parallel-cascade parameters during training to accommodate multiple chunks,
subsequently eliminating these additional parameters through
re-parameterization during inference. Furthermore, to enhance RepCaM's
performance, we propose the Transparent Visual Prompt (TVP), which includes a
minimal set of zero-initialized image-level parameters (e.g., less than 0.1%)
to capture fine details within video chunks. We conduct extensive experiments
on the VSD4K dataset, encompassing six different video scenes, and achieve
state-of-the-art results in video restoration quality and delivery bandwidth
compression.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: USPIL框架通过物理信息神经网络和守恒定律统一建模捕食者-猎物系统的时空动力学，在保持物理一致性的同时实现高效计算和机制解释


<details>
  <summary>Details</summary>
Motivation: 生态系统的复杂多尺度动力学挑战传统建模方法，需要新方法既能捕捉时空振荡和涌现模式，又能遵守守恒原理

Method: 提出统一时空物理信息学习(USPIL)框架，整合物理信息神经网络(PINNs)和守恒定律，使用自动微分强制执行物理约束和自适应损失加权

Result: 在Lotka-Volterra系统中，1D时间动力学达到98.9%相关性，2D系统捕捉复杂螺旋波，验证守恒定律遵守度在0.5%内，推理速度比数值求解器快10-50倍

Conclusion: USPIL为多尺度生态建模开辟了新途径，建立了物理信息深度学习作为强大且科学严谨的范式，是生态预测和保护规划的革命性工具

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [39] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 本文通过360次实验对比8种优化器的能耗效率，发现AdamW和NAdam在性能和环保方面表现均衡，SGD在复杂数据集上性能优异但碳排放较高


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型日益复杂和计算需求增长，理解训练决策对环境的影响对于可持续AI发展变得至关重要

Method: 在三个基准数据集(MNIST, CIFAR-10, CIFAR-100)上使用8种流行优化器进行360次受控实验，每个优化器使用15个随机种子，通过CodeCarbon在Apple M1 Pro硬件上精确追踪能耗

Result: 发现训练速度、准确性和环境影响之间存在显著权衡，这些权衡因数据集和模型复杂度而异。AdamW和NAdam表现出一致的效率，而SGD在复杂数据集上表现出色但排放更高

Conclusion: 研究结果为从业者在机器学习工作流中平衡性能和可持续性提供了可行的见解

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [40] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet-Transolver框架，用于解决PET瓶屈曲分析问题，能够同时预测节点位移场和时间相关的反作用力，在几何参数化设计上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理非参数化几何域变化的PDE问题时泛化能力有限，而传统的有限元分析计算成本高昂，需要开发计算高效且可扩展的替代模型。

Method: 采用混合DeepONet-Transolver框架，结合深度算子网络和变换器求解器，在Abaqus非线性有限元模拟生成的254个独特设计数据上进行训练。

Result: 在四参数瓶族上实现位移场平均相对L2误差2.5-13%，时间相关反作用力误差约2.4%，点位移绝对误差10^-4-10^-3量级，能准确捕捉屈曲等关键物理现象。

Conclusion: 该框架作为可扩展的计算高效替代模型，在计算力学和多任务预测应用中具有重要潜力，特别适用于快速设计评估场景。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [41] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS是一个10-800亿参数的像素级Swin扩散变换器，用于改进天气预测的集合校准和稳定性，在Aurora超级计算机上实现了10.21 ExaFLOPS的性能，在90天季节尺度上表现优于IFS ENS系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散方法在高分辨率天气预测中难以稳定扩展的问题，利用生成式机器学习更好地理解复杂地球系统动力学。

Method: 提出AERIS（像素级Swin扩散变换器）和SWiPe技术（结合窗口并行、序列并行和流水线并行），在0.25度ERA5数据集上进行训练和评估。

Result: 在Aurora超级计算机上达到10.21 ExaFLOPS混合精度性能，弱缩放效率95.5%，强缩放效率81.6%，在季节尺度预测上优于IFS ENS系统并保持稳定。

Conclusion: 十亿参数级别的扩散模型在天气和气候预测方面具有巨大潜力，AERIS展示了在高分辨率下稳定扩展的能力。

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [42] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL是一种线性元学习算法，在保持可解释性的同时提高多个化学性质预测的准确性，通过元学习框架识别相关任务间的共享参数，性能比标准岭回归提升1.1-25倍。


<details>
  <summary>Details</summary>
Motivation: 化学研究中高质量数据集有限，机器学习方法对数据需求增加，需要平衡预测准确性和人类可理解性，因此开发既准确又可解释的AI方法。

Method: 提出LAMeL线性元学习算法，利用元学习框架识别相关任务间的共享模型参数，学习共同函数流形作为新任务的更优起点，即使任务间不共享数据。

Result: 性能比标准岭回归提升1.1-25倍，具体提升程度因数据集领域而异，但始终优于或匹配传统线性方法。

Conclusion: LAMeL是化学性质预测中可靠的工具，在准确性和可解释性都至关重要的场景中表现优异。

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [43] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: 研究发现GPT-4o mini存在"单模态瓶颈"安全架构缺陷，多模态推理被上下文无关的安全过滤器系统性地阻断，导致良性内容被错误拦截


<details>
  <summary>Details</summary>
Motivation: 随着大型多模态模型(LMMs)在日常数字生活中的普及，理解其安全架构对AI对齐至关重要，需要系统分析全球部署模型在困难的多模态仇恨言论检测任务中的表现

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段调查，分析模型的推理过程和失败模式，并对144个内容策略拒绝进行定量验证

Result: 实验识别出"单模态瓶颈"架构缺陷，50%的拒绝由视觉内容触发，50%由文本内容触发，安全系统脆弱，不仅阻止高风险图像，还错误拦截良性常见meme格式

Conclusion: 这些发现揭示了最先进LMMs中能力与安全之间的根本张力，突显了需要更集成、上下文感知的对齐策略，以确保AI系统既能安全又能有效地部署

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [44] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 基于自然语言处理和序列感知神经网络的自动化故障分析框架，用于处理ALS控制系统的实时事件日志，通过语义嵌入和异常评分来识别系统故障前的关键事件序列


<details>
  <summary>Details</summary>
Motivation: 为了解决先进光源（ALS）控制系统中复杂故障的快速识别问题，传统方法难以处理实时事件日志中的大量数据并准确识别故障前兆

Method: 将事件日志条目视为自然语言，使用语义嵌入技术转换为上下文向量表示，然后通过基于正常操作数据训练的序列感知神经网络为每个事件分配实时异常分数

Result: 该方法能够标记与基线行为的偏差，使操作人员能够快速识别复杂系统故障前的关键事件序列

Conclusion: 该框架为大型科学设施的控制系统提供了一种有效的自动化故障分析方法，通过自然语言处理和深度学习技术实现了对系统异常的实时监测和预警

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [45] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis是一个并行保护框架，通过模型分割策略在联邦学习中实现隐私-效用-效率的灵活平衡控制


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中现有保护机制（如差分隐私和同态加密）在模型效用和计算效率之间强制刚性权衡的问题，缺乏灵活性阻碍了实际应用

Method: 采用战略性模型分割方案：对模型不太关键的低范数部分应用轻量级差分隐私，其余部分使用同态加密保护，并通过分布式投票机制确保分割共识

Result: 理论分析确认了在相同隐私保护下效率与效用之间的可调节性，实验结果表明通过调整超参数可以灵活优先考虑模型准确性或训练时间

Conclusion: ParaAegis框架为联邦学习实践者提供了对隐私-效用-效率平衡的灵活控制，解决了现有保护机制的刚性权衡问题

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [46] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私的文本生成框架，通过聚合私有记录的推理结果来生成高质量合成文本，在保护隐私的同时保持实用性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在隐私泄露风险，攻击者可能从提示中提取敏感信息，需要一种既能生成高质量文本又能提供强隐私保证的方法

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token的输出分布，生成更长的连贯合成文本，同时提出混合操作结合私有和公共推理以提升效用

Result: 经验评估表明该方法在上下文学习任务上优于先前最先进方法，实现了隐私保护文本生成同时保持高实用性

Conclusion: 该方法为隐私保护文本生成提供了一个有前景的方向，能够在提供强隐私保证的同时生成高质量的合成文本

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [47] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 分布式图正则化高斯混合模型学习，通过相似性图指导参数共享，在异构小样本场景下优于集中式和本地训练方法


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中数据异构且样本有限的高斯混合模型学习问题，避免原始数据传输，利用节点间相似性进行参数共享

Method: 基于图正则化的高斯混合模型学习方法，利用提供的相似性图指导节点间参数共享，实现灵活的邻居参数聚合

Result: 在异构低样本情况下，该方法性能优于集中式训练和本地训练的GMM模型

Conclusion: 图正则化方法为分布式异构数据的高斯混合模型学习提供了有效解决方案，通过参数共享而非数据转移实现更好的模型性能

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [48] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: 提出DeepLogit模型，通过序列约束方法结合深度学习与离散选择模型，在保持参数可解释性的同时提高预测精度


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在规划和政策领域的应用受限于其黑盒性质，需要开发既能保持可解释性又能提高预测准确性的方法

Method: 两阶段方法：先估计线性参数的CNN模型（等同于多项logit模型），然后约束需要解释的参数值，加入高阶项或Transformer等先进架构

Result: 在真实世界的新加坡公交智能卡数据上进行验证，显示该方法在保持参数可解释性的同时显著提高了模型准确性

Conclusion: 该方法展示了理论驱动的离散选择模型与数据驱动的AI模型相互融合的潜力，可在保持规划政策应用适用性的同时实现更准确的建模

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [49] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: 提出WILF-Q方法，使用Q学习自适应学习客户端索引，解决无线联邦学习中的客户端选择问题，显著提升学习效率


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中客户端动态状态变化影响计算和通信效率，需要高效选择客户端以减少达到特定学习精度所需的总时间

Method: 将客户端选择建模为多臂老虎机问题，使用Q学习自适应学习和更新每个客户端的近似Whittle索引，选择索引最高的客户端

Result: WILF-Q在实验结果显示显著优于现有基线策略，提供了一种鲁棒高效的客户端选择方法

Conclusion: WILF-Q方法无需客户端状态转移或数据分布的显式知识，适用于实际联邦学习部署，能有效提升无线联邦学习的学习效率

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [50] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种结合数字孪生和零知识联邦学习的新型框架，用于解决无人机辅助联邦学习系统中的能耗、通信效率和安全性问题，相比传统方法可降低29.6%的能耗。


<details>
  <summary>Details</summary>
Motivation: 无人机辅助联邦学习系统面临能耗过高、通信效率低下和安全漏洞等挑战，需要一种综合解决方案来确保系统的可靠运行。

Method: 集成数字孪生技术进行实时系统监控和预测性维护，采用零知识证明增强安全性，并引入基于块坐标下降和凸优化技术的动态资源分配策略来优化无人机飞行路径、传输功率和处理速率。

Result: 仿真结果显示，该方法相比传统联邦学习方法显著降低系统能耗达29.6%，同时提高了学习性能、安全性和可扩展性。

Conclusion: 该框架为下一代无人机智能网络提供了一个有前景的解决方案，在保证隐私安全的同时显著提升了系统效率和性能。

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [51] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 将多模态生理信号（PPG、GSR、ACC）转换为2D图像矩阵，使用CNN进行压力检测的新方法，通过信号融合和图像化表示提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理多模态生理信号或依赖固定编码，无法有效捕捉信号间的时间依赖性和交叉关系，需要一种能更好利用CNN优势的信号表示方法。

Method: 将PPG、GSR、ACC信号融合成结构化2D图像表示，系统性地重新组织融合信号为多种格式，采用多阶段训练流程来增强泛化能力和模型鲁棒性。

Result: 该方法显著提升了分类性能，不仅改善了可解释性，还作为一种强大的数据增强形式，适用于任何涉及多模态生理信号的领域。

Conclusion: 提出的图像化转换方法为通过可穿戴技术实现更准确、个性化和实时的健康监测铺平了道路，具有广泛的适用性。

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [52] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个将交错图像-文本生成重构为工具使用问题的动态框架，通过强化学习训练LLM智能协调多种视觉工具，在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前统一模型存在"单一工具"瓶颈，仅限于合成图像生成，难以处理需要事实基础或程序化精度的任务，需要更灵活的多工具协作框架。

Method: 设计中央LLM/MLLM代理智能协调专业视觉工具集（在线图像搜索、扩散生成、代码执行、图像编辑），通过结合规则逻辑和LLM评估的混合奖励系统进行强化学习训练。

Result: 在四个基准测试中实现最先进性能，大幅超越现有方法，并引入新的测试时扩展策略获得额外性能提升。

Conclusion: LLM-I框架成功解决了多模态生成中的工具协调问题，为动态、事实基础的多工具协作提供了有效解决方案。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [53] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了GenPAS框架，系统性地分析了生成式推荐中数据增强策略对模型性能的影响，通过三个偏差控制步骤统一了现有方法，并在实验中展现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统中的数据增强策略往往被简化处理或应用不一致，缺乏系统性和原则性的理解，而不同增强策略会导致显著的性能差异。

Method: 提出GenPAS框架，将数据增强建模为包含三个偏差控制步骤的随机采样过程：序列采样、目标采样和输入采样，统一了广泛使用的策略并实现了对训练分布的灵活控制。

Result: 在基准和工业数据集上的广泛实验表明，GenPAS相比现有策略在准确性、数据效率和参数效率方面都表现出优越性能。

Conclusion: GenPAS为生成式推荐中的原则性训练数据构建提供了实用指导，系统性地解决了数据增强策略的设计问题。

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [54] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 提出CPT方法解决NLP中公平性与准确性的权衡控制问题，通过多目标优化实现用户偏好导向的精确控制


<details>
  <summary>Details</summary>
Motivation: 现有工作只寻找单一"最优"解决方案，忽略了帕累托前沿上的多样性，需要根据用户偏好提供可控的权衡方案

Method: 采用多目标优化(MOO)，通过稳定公平性更新的移动平均梯度和剪枝关键参数梯度来实现可控帕累托权衡(CPT)

Result: 在仇恨言论检测和职业分类任务上，CPT比基线方法获得更高质量的帕累托前沿解集，并展现出更好的可控性

Conclusion: CPT方法能够有效实现用户偏好导向的公平性-准确性权衡控制，为NLP任务提供了更灵活的解决方案

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [55] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM是一个基于辐射场的多域无线信道建模框架，通过物理感知的频率相关衰减模型和点云辅助环境增强方法，显著提升了多小区多频段信道建模的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 传统局部统计信道建模方法局限于单小区、单网格和单频段分析，无法捕捉复杂的跨域交互，需要新的框架来克服这些限制

Method: 采用辐射场联合建模大尺度信号衰减和多径分量，引入物理感知的频率相关衰减模型(FDAM)实现跨频段泛化，使用点云辅助环境增强方法支持多小区多网格建模，并利用低秩张量表示和分层张量角度建模算法(HiTAM)提高计算效率

Result: 在真实多小区数据集上的实验表明，RF-LSCM显著优于现有方法，覆盖预测的平均绝对误差(MAE)降低30%，通过有效融合多频数据实现22%的MAE提升

Conclusion: RF-LSCM通过创新的辐射场建模方法和高效的计算架构，成功解决了传统信道建模的局限性，为蜂窝网络优化提供了更准确和高效的信道建模解决方案

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [56] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文提出了一个基于共形预测的分布无关不确定性量化框架，为物理信息神经网络(PINNs)提供严格的统计保证和空间自适应不确定性区间。


<details>
  <summary>Details</summary>
Motivation: 现有的PINNs不确定性量化方法缺乏严格的统计保证，需要一种能够提供有限样本覆盖保证的分布无关方法。

Method: 采用分布无关的共形预测框架，通过在校准集上构建非共形性分数来校准预测区间，并引入局部共形分位数估计来处理空间异方差性。

Result: 在典型PDE系统（阻尼谐振子、泊松、Allen-Cahn和亥姆霍兹方程）上的系统评估表明，该方法实现了可靠的校准和局部自适应不确定性区间，一致优于启发式UQ方法。

Conclusion: 该工作通过将PINNs与分布无关UQ相结合，不仅提高了校准性和可靠性，还为复杂PDE系统的不确定性感知建模开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [57] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 通过智能手表系统和心率数据预训练模型，经过转移学习和细调，开发了一种能够预测社交焦虑情绪波动的方法，在社交焦虑检测上达到60.4%的平衡准确率，并在外部数据集上验证了其普遍性。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑是一种常见的心理健康问题，但之前少有研究测量或预测社交情境中瞬态焦虑情绪的波动。捐描这些日内动态对于设计实时个性化干预措施至关重要。

Method: 采用自定义智能手表系统收集数据，每天进行7次生态瞬态评估(EMA)。基于外部心率数据训练基础模型，进行表征转移和细调，生成概率预测。结合特质水平测量构建元学习器。

Result: 在社交焦虑数据集上达到60.4%的平衡检测准确率，在TILES-18数据集上达到59.1%的平衡准确率，比之前研究提高至7%以上。

Conclusion: 该研究开发了一种能够有效预测社交焦虑情绪波动的方法，为实时个性化干预措施的设计提供了重要技术支撑，并验证了方法的普遍性。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [58] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了DirGraphSSM，首个将状态空间模型系统扩展到有向图学习的框架，通过k-hop ego图序列化和消息传递机制，在保持高效训练的同时实现了SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络和图Transformer在处理有向图时面临两个主要挑战：有效捕捉长距离因果依赖关系，以及在处理大规模图数据时平衡准确性和训练效率

Method: 提出DirEgo2Token方法通过k-hop ego图将有向图序列化，并在此基础上开发DirGraphSSM架构，通过消息传递机制在有向图上实现状态空间模型

Result: 在三个代表性有向图学习任务上达到SOTA性能，在另外两个任务上获得竞争性性能，训练速度比现有SOTA模型快1.5-2倍

Conclusion: DirGraphSSM成功将状态空间模型扩展到有向图学习领域，有效解决了长距离因果依赖捕捉和训练效率的平衡问题

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [59] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK是一个增强大语言模型捕捉时空依赖性的新框架，通过空间增强注意力和记忆检索前馈网络解决LLM在交通预测中的空间建模限制


<details>
  <summary>Details</summary>
Motivation: 大语言模型在交通预测中显示出潜力，但其主要为序列标记处理设计，难以有效捕捉空间依赖关系，特别是在图结构空间数据建模方面存在架构不兼容问题

Method: 提出ST-LINK框架，包含两个关键组件：空间增强注意力（SE-Attention）通过旋转位置嵌入整合空间相关性；记忆检索前馈网络（MRFFN）动态检索历史模式捕捉复杂时间依赖性

Result: 在基准数据集上的综合实验表明，ST-LINK超越了传统深度学习和LLM方法，能有效捕捉常规交通模式和突变

Conclusion: ST-LINK成功解决了LLM在空间建模方面的局限性，为交通预测提供了更有效的时空依赖性捕捉方案

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [60] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 本文从因果视角分析多视图无监督特征选择(MUFS)，提出CAUSA方法，通过因果正则化模块分离混杂因子并平衡分布，有效缓解虚假相关性，提升特征选择性能。


<details>
  <summary>Details</summary>
Motivation: 现有MUFS方法通过捕捉特征与聚类标签之间的相关性来选择判别性特征，但这些相关性可能因混杂因子而产生虚假关联，导致选择不相关特征。本文旨在从因果角度解决这一问题。

Method: 提出CAUSA方法：1) 使用广义无监督谱回归模型识别信息特征；2) 引入因果正则化模块自适应分离多视图数据中的混杂因子，并学习视图共享样本权重来平衡混杂因子分布；3) 将两者整合到统一学习框架中。

Result: 综合实验表明，CAUSA在多个基准数据集上优于现有最先进方法，能够选择因果信息特征。

Conclusion: 这是首个在无监督设置下对因果多视图特征选择的深入研究，CAUSA方法通过因果视角有效解决了传统方法中因忽略混杂因子而产生的虚假相关性问题。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [61] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: FHNN是一种物理结构化的流体-结构相互作用神经网络框架，通过预测可解释的水动力参数并耦合解析运动方程，显著提升预测精度和稳定性


<details>
  <summary>Details</summary>
Motivation: 传统黑盒神经网络模型在浮体流体动力学建模中存在可解释性差和长期预测不稳定的问题，需要结合物理约束来改进

Method: 提出Floating-Body Hydrodynamic Neural Networks (FHNN)，预测方向性附加质量、阻力系数和基于流函数的流动等可解释水动力参数，并与解析运动方程耦合

Result: 在合成涡流数据集上，FHNN比Neural ODEs误差低一个数量级，能恢复物理一致的流场，相比哈密顿和拉格朗日神经网络更有效处理耗散动力学

Conclusion: FHNN成功弥合了黑盒学习与透明系统识别之间的差距，在保持可解释性的同时有效处理耗散动力学问题

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [62] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: 提出了通用物理变换器(GPhyT)，这是一个基于Transformer的物理基础模型，能够在多个物理领域实现零样本泛化，无需重新训练即可模拟各种物理系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的机器学习方法局限于单一狭窄领域，需要为每个新系统重新训练。物理基础模型(PFM)可以民主化高保真模拟的访问，加速科学发现，消除专门求解器开发的需求。

Method: 使用Transformer架构，在1.8TB多样化模拟数据上训练，通过上下文学习从数据中推断控制动力学，无需告知底层方程。

Result: GPhyT在多个物理领域表现优异，比专门架构性能提升高达29倍；通过上下文学习实现零样本泛化到未见过的物理系统；通过50时间步滚动实现稳定的长期预测。

Conclusion: 这项工作证明了单个模型可以从数据中学习可泛化的物理原理，为通向可能改变计算科学与工程的通用物理基础模型开辟了道路。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [63] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子-经典工作流，用于解决普惠金融中数据稀缺的少样本信用风险评估问题，通过量子神经网络在真实硬件上实现了优于经典方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决普惠金融中由于数据稀缺和不平衡导致的信用风险评估难题，传统方法在此类少样本场景下效果有限，需要探索量子机器学习的新范式。

Method: 设计混合量子-经典工作流：首先使用经典机器学习模型（逻辑回归、随机森林、XGBoost）进行特征工程和降维，然后使用参数偏移规则训练的量子神经网络作为核心分类器。

Result: 在279个样本的真实信用数据集上，量子神经网络在模拟中平均AUC达到0.852±0.027，在Quafu量子云平台的ScQ-P21超导处理器上硬件实验获得0.88的AUC，性能超越多个经典基准模型。

Conclusion: 该研究为NISQ时代数据受限的金融场景应用量子计算提供了实用蓝图，证明了量子机器学习在高风险普惠金融应用中的潜力。

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [64] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出了一种端到端可微分的混合框架，将图神经网络嵌入孔隙网络模型中，用于多孔介质渗透率预测，避免了传统方法的理想化几何假设，同时保持了物理基础的流动计算。


<details>
  <summary>Details</summary>
Motivation: 传统纯数据驱动模型缺乏跨尺度泛化能力且不包含显式物理约束，而孔隙网络模型虽然基于物理但依赖于理想化几何假设来估计孔尺度水力传导率，在复杂结构中精度有限。

Method: 开发了一个端到端可微分混合框架，用基于图神经网络的预测替代传统孔隙网络模型中的解析传导率计算公式，通过反向传播梯度和离散伴随方法实现无标签传导率数据的训练。

Result: 该模型实现了高精度和良好的跨尺度泛化能力，优于纯数据驱动和传统孔隙网络模型方法，梯度敏感性分析显示了物理一致的特征影响。

Conclusion: 该方法为复杂多孔介质中的渗透率预测提供了一个可扩展且物理信息丰富的框架，减少了模型不确定性并提高了准确性。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [65] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，将掩码扩散模型解释为离散最优传输中的能量最小化问题，证明了三种能量公式的数学等价性，并通过Beta分布参数化调度实现了高效的采样优化。


<details>
  <summary>Details</summary>
Motivation: 统一掩码扩散模型的理论基础，澄清其在最优传输中的数学本质，并为实际采样改进提供理论指导。

Method: 证明三种能量公式（动能、条件动能、测地能量）在MDMs结构下的数学等价性；使用Beta分布参数化插值调度，将调度设计空间简化为2D搜索。

Result: 理论证明了能量公式的等价性；实验表明基于能量的调度在合成和真实基准测试中优于手工设计的基线，特别是在低步采样设置中表现突出。

Conclusion: 该工作为掩码扩散模型提供了统一的理论框架，提出的能量启发式调度方法能够有效提升采样性能，且无需修改模型即可实现训练后调优。

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [66] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG是一种基于随机采样的历史感知漂移对齐方法，通过统计门控机制解决联邦学习中非IID数据和部分参与导致的客户端漂移问题，显著提升收敛速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中非IID数据和部分客户端参与导致的客户端漂移和局部最优不一致问题，这些问题会造成收敛不稳定和准确率损失。

Method: 维护每个客户端的漂移记忆，积累本地模型差异作为历史梯度的轻量级草图；使用基于观察/预期参与比率的平滑门控函数来控制记忆更新和本地对齐项，该门控在采样噪声主导时保持弱和平滑，在参与统计稳定后加强。

Result: 在CIFAR-10/100数据集上，100/500个客户端，2-15%参与率的情况下，FedSSG始终优于强漂移感知基线，测试准确率平均提升约0.9点(CIFAR-10)和2.7点(CIFAR-100)，目标准确率收敛速度平均加快约4.5倍。

Conclusion: FedSSG证明采样统计可以转化为原则性的历史感知相位控制，有效稳定和加速联邦训练，仅需O(d)客户端内存和常数时间门控，在近IID或均匀采样情况下优雅退化为温和正则化器。

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [67] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter是一个轻量级适配器，无需微调即可为时间序列基础模型添加协变量信息，通过两阶段方法在保持计算效率的同时显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型无法有效利用协变量信息，而这些外生变量在许多应用中对准确预测至关重要。需要一种轻量级方法来增强基础模型而不需要重新训练。

Method: 采用两阶段非参数级联方法：1) 使用简单回归模型生成伪预测 2) 训练高斯过程回归器，结合伪预测、基础模型预测和协变量来优化预测结果

Result: 在真实数据集上，TFMAdapter始终优于基础模型和监督基线，相比基础基础模型实现了24-27%的性能提升，且数据计算开销最小

Conclusion: 轻量级适配器有潜力弥合通用基础模型与领域特定预测需求之间的差距，为时间序列预测提供了高效实用的解决方案

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [68] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx是首个针对交叉公平性的联合优化框架，通过自适应多目标优化器、可微交叉公平指标和理论收敛保证，有效解决多个敏感属性组合的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法仅处理单一敏感属性，无法捕捉交叉子群体面临的复杂多重偏见，需要专门解决交叉公平性的框架。

Method: 提出APFEx框架，包含：1）自适应多目标优化器动态切换帕累托锥投影、梯度加权和探索策略；2）可微交叉公平指标支持梯度优化；3）理论收敛保证。

Result: 在四个真实数据集上实验表明，APFEx显著减少公平性违规，同时保持竞争力准确率。

Conclusion: APFEx填补了公平机器学习的关键空白，为交叉公平性提供了可扩展、模型无关的解决方案。

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [69] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: 通过简单的置信度加权平均方法，无需重新训练即可将多个先进轨迹预测模型集成，在NuScenes和Argoverse数据集上实现10%的性能提升，特别是在长尾指标上表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决如何在不进行昂贵重新训练的情况下，结合多个大型自动驾驶轨迹预测模型的优势，提升整体预测性能的问题。

Method: 使用置信度加权平均方法，将多个最先进的深度学习轨迹预测模型进行集成，无需重新训练或微调。

Result: 在NuScenes和Argoverse数据集上，集成方法比最佳单一模型性能提升10%，特别是在长尾指标上表现显著改善，且改进在整个数据分布范围内都有效。

Conclusion: 简单的置信度加权平均集成方法能够有效提升轨迹预测性能，证明了无需复杂重新训练即可结合多个先进模型优势的可行性。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [70] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN是一种基于物理信息的神经网络扩展框架，用于解决含多裂纹的断裂力学问题，通过能量损失函数、定制积分方案和域分解方法，结合XFEM思想在神经网络解空间中引入特殊函数来捕捉裂纹不连续性和尖端奇异性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多裂纹断裂力学问题时面临挑战，需要开发一种能够有效捕捉裂纹不连续性和尖端奇异性的稳健数值方法。

Method: 提出扩展物理信息神经网络(X-PINN)，采用能量基损失函数、定制积分方案和域分解程序，借鉴XFEM思想在神经网络解空间中引入特殊函数来显式处理裂纹不连续性和奇异性，使用独立神经网络分别建模标准和增强解分量。

Result: 数值实验验证了该方法在1D和2D域中处理复杂多裂纹问题的有效性和稳健性，并具有良好的扩展到3D问题的能力。

Conclusion: X-PINN框架为断裂力学中的多裂纹问题提供了一种灵活有效的解决方案，通过神经网络与物理信息的结合，成功处理了裂纹相关的不连续性和奇异性问题。

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [71] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART是一个用于癫痫发作检测的持续学习框架，通过选择性保留高熵和预测为发作的样本，在有限内存和计算资源下实现个性化适应，在CHB-MIT数据集上F1分数提升21%。


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断依赖专家分析脑电图，过程耗时且需要专业知识。现有深度学习模型存在灾难性遗忘问题，无法适应患者脑电图信号的动态变化，需要开发能够持续学习并个性化适应的自动化检测方法。

Method: 提出EpiSMART持续学习框架，使用大小受限的回放缓冲区和信息样本选择策略，选择性保留高熵和预测为癫痫发作的样本，逐步适应患者特定的脑电图信号特征。

Result: 在CHB-MIT数据集验证中，EpiSMART相比不更新的基线模型F1分数提升21%，平均每天仅需6.46分钟标记数据和6.28次更新，适合可穿戴系统实时部署。

Conclusion: EpiSMART能够在资源受限的现实条件下，有效整合新数据而不损害已有知识，实现鲁棒的个性化癫痫发作检测，推动可穿戴医疗系统的实际应用。

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [72] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 提出基于动态图回归的GNSS干扰抑制方法，使用异构图卷积LSTM网络实时预测并校正接收机水平偏差，在多种干扰场景下显著优于传统时间序列基线模型


<details>
  <summary>Details</summary>
Motivation: GNSS系统日益受到有意干扰的影响，在需要精确定位和授时的时候降低系统可用性，需要开发有效的干扰抑制技术

Method: 将卫星接收环境建模为异构星形图（接收机为中心，卫星为叶节点），使用单层异构图卷积LSTM（HeteroGCLSTM）聚合空间上下文和时间动态信息，实时预测2D偏差向量进行校正

Result: 在两种接收机和三种干扰模式下（连续波、三音调、宽带FM），模型在-45dBm时达到3.64-7.74cm MAE，在-60至-70dBm时提升至1.65-2.08cm。混合模式下MAE为3.78-4.25cm，数据效率优异（仅10%训练数据仍优于基线）

Conclusion: 该方法通过图神经网络有效处理GNSS干扰问题，在强干扰环境下仍能保持厘米级定位精度，具有优越的性能和数据效率

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [73] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习和差分隐私的隐私保护流行病预测方法，在德国县级层面实现本地化预测，在保护数据隐私的同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在流行病爆发时需要快速反应，但本地化机器学习模型训练面临数据不足的问题，而集中数据又存在隐私敏感性挑战。需要找到既能保护隐私又能提供详细情境数据的解决方案。

Method: 使用联邦学习框架，以县/社区为客户端，训练共享的多层感知机模型。客户端只交换经过范数裁剪的更新，服务器使用差分隐私噪声聚合更新，实现客户端级别的差分隐私保护。

Result: 在适度隐私保护水平下，差分隐私模型接近非隐私模型性能：2020年11月R²=0.94（vs 0.95），MAPE=26%；2022年3月R²=0.88（vs 0.93），MAPE=21%。严格隐私保护会导致预测不稳定。

Conclusion: 客户端级别的差分隐私联邦学习能够在提供强隐私保证的同时提供有用的县级预测，可行的隐私预算取决于流行病阶段，支持卫生当局进行隐私合规的本地预测协作。

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [74] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: 本研究通过小波变换将纳米孔电流信号转换为尺度图图像，利用机器学习算法对42种肽进行实时分类，准确率达到81%，创下该领域新纪录，为临床实时疾病诊断提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 开发能够在临床环境中实时分类蛋白质的设备，实现廉价快速的疾病诊断。纳米孔技术虽然具有潜力，但当前信号复杂性限制了其准确性。

Method: 将纳米孔电流信号通过小波变换转换为尺度图图像，捕捉振幅、频率和时间信息，使用机器学习算法进行分类。还展示了模型迁移技术以适应实际硬件部署。

Result: 在42种肽的分类测试中达到约81%的准确率，创下了该领域的新纪录。

Conclusion: 该方法为实时肽/蛋白质诊断在护理点的实际应用迈出了重要一步，为实时疾病诊断开辟了新途径。

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [75] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 提出基于环境传感器融合（温湿度、压力差）的轻量级蜂王检测系统，使用量化决策树在STM32微控制器上实现实时低功耗边缘计算，准确率超99%


<details>
  <summary>Details</summary>
Motivation: 传统蜂王检测依赖人工检查，劳动强度大且干扰蜂群；现有音频方法功耗高、预处理复杂且易受环境噪声影响

Method: 融合蜂巢内外温湿度、压力差等环境传感器数据，采用量化决策树推理算法在STM32微控制器上实现边缘计算

Result: 仅使用环境输入即可实现超过99%的蜂王检测准确率，音频特征未带来显著性能提升

Conclusion: 提供了一种可扩展、可持续的非侵入式蜂巢监测解决方案，为使用现成节能硬件的自主精准养蜂铺平道路

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [76] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究了强化学习中的贝叶斯风险规避方法，通过BRMDP处理模型参数不确定性，证明了贝叶斯风险值函数相对于真实值函数的渐近正态性和悲观低估特性，并提出了基于后验采样的在线RL和CMAB算法，获得了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中由于数据不足导致的认知不确定性问题，通过贝叶斯风险规避方法来处理模型参数的不确定性。

Method: 采用贝叶斯风险马尔可夫决策过程(BRMDP)，推导贝叶斯风险值函数与原始值函数之间的渐近正态性关系，提出基于后验采样的在线RL和CMAB算法。

Result: 理论分析表明贝叶斯风险规避方法会悲观低估原始值函数，这种差异随风险厌恶程度增强而增大，随数据量增加而减小。算法在在线RL和CMAB设置下都获得了次线性遗憾界。

Conclusion: 贝叶斯风险规避方法能有效处理强化学习中的认知不确定性，数值实验验证了理论性质的有效性，为风险敏感的强化学习提供了理论保证和实用算法。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [77] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 该研究比较了不同优化器和神经网络架构在EEG频段分类中的性能，发现Adagrad和RMSprop优化器表现最佳，CNN在空间特征提取方面表现优异，SHAP分析揭示了EEG频段对分类准确性的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同优化器和神经网络架构在EEG频段分类任务中的性能差异，以及如何有效预测大脑左右半球的分类，以提高神经影像分类任务的准确性和可解释性。

Method: 使用三种神经网络架构（深度密集网络、浅层三层网络和卷积神经网络CNN），在TensorFlow和PyTorch框架下实现，采用多种优化器（Adagrad、RMSprop、Adadelta、SGD、FTRL）进行比较，并使用SHAP进行特征重要性分析。

Result: Adagrad和RMSprop优化器在不同频段表现稳定，Adagrad在beta频段表现最佳，RMSprop在gamma频段表现最优。CNN获得第二高准确率，擅长捕捉EEG空间特征。深度密集网络在学习复杂模式方面有竞争力，浅层网络计算效率高但准确率较低。

Conclusion: 优化器选择、模型架构和EEG频段分析对提高分类器性能至关重要，SHAP分析有助于理解EEG频段对模型准确性的贡献，为神经影像分类任务提供了重要见解。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [78] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 提出了Quantile Neural Basis Model，将分位数广义可加模型的解释性原理融入神经网络框架，在保持预测性能的同时提供模型行为的可解释性洞察


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络在多水平概率预测中取得了高精度，但理解特征条件输出的底层机制仍然是一个重大挑战，需要解决模型可解释性问题

Method: 利用共享基分解和权重分解，将Quantile Generalized Additive Models的可解释性原则整合到端到端神经网络训练框架中，避免参数分布假设

Result: 在日前电价预测任务中验证，取得了与分布回归和分位数回归神经网络相当的预测性能，同时通过学习到的从输入特征到输出预测的非线性映射提供有价值的模型行为洞察

Conclusion: Quantile Neural Basis Model在保持神经网络预测性能的同时，显著提升了模型的可解释性，为理解特征条件输出机制提供了有效途径

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [79] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 该研究通过预测建模和实地试验，发现对高风险群体进行针对性心理健康外展服务能有效降低再监禁率，特别是在最高风险人群中效果最显著。


<details>
  <summary>Details</summary>
Motivation: 监狱系统难以应对被监禁者的复杂需求（如精神疾病、药物依赖等），导致再犯罪和监禁循环，特别是对有色人种社区造成不成比例的影响。需要创新方法来打破这一循环。

Method: 采用预测建模方法识别高风险个体，并通过实地试验评估针对性心理健康外展服务的效果，分析不同风险水平下的干预有效性。

Result: 模型对新的监狱收监具有高度预测性，最高风险群体中超过一半在一年内重返监狱。外展服务对最高风险个体最有效，显著改善了心理健康服务使用、EMS调度和刑事司法参与情况。

Conclusion: 针对性心理健康外展服务，特别是针对最高风险人群的干预，是打破监禁循环、减少再监禁率的有效策略，有助于解决刑事司法系统中的种族差异问题。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [80] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: 该论文研究了一种组合核岭回归方法，通过坐标重加权进行特征选择，证明了在某些条件下能够有效识别相关变量并消除噪声变量。


<details>
  <summary>Details</summary>
Motivation: 研究组合架构中的特征学习问题，为变量选择提供理论框架，探索不同核函数在非线性特征恢复中的表现差异。

Method: 采用变分问题形式的组合核岭回归模型，分析全局最小值和驻点对噪声变量的处理能力，比较拉普拉斯核和高斯核的特征恢复效果。

Result: 证明当噪声变量服从高斯分布时，全局最小值和驻点都能有效消除噪声坐标；拉普拉斯核能够恢复非线性效应的特征，而高斯核只能恢复线性特征。

Conclusion: 组合核岭回归为特征学习提供了有效的测试平台，ℓ₁型核（如拉普拉斯核）在非线性特征恢复方面优于高斯核，为变量选择提供了理论保证。

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [81] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 提出了一种端到端框架，通过稀疏常规数据非侵入性估计不可测量的青光眼组织渗透性参数，结合多阶段AI架构、新型数据生成策略和贝叶斯引擎，解决了缺乏真实数据和计算成本高的逆问题挑战。


<details>
  <summary>Details</summary>
Motivation: 青光眼等医疗决策面临关键参数无法测量的挑战，特别是小梁网渗透性这一决定眼压的主要组织特性无法在体内测量，临床依赖间接替代指标，同时缺乏真实数据和计算成本高阻碍了预测模型的发展。

Method: 采用多阶段人工智能架构功能分离问题；提出名为PCDS的新型数据生成策略，避免数十万次昂贵模拟，将有效计算时间从数年缩短至数小时；使用贝叶斯引擎量化预测不确定性。

Result: 非侵入性估计的流出设施与最先进的眼压测量技术表现出优异一致性，精度与直接物理仪器相当；新推导的渗透性生物标志物在按疾病风险分层临床队列方面表现出高准确性。

Conclusion: 该框架为其他数据稀缺、计算密集型领域中类似逆问题的解决提供了可推广的蓝图，展示了在医疗诊断中的潜在应用价值。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [82] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing是一个端到端框架，通过图算法和LLM代理实现电路层次化理解，并将领域知识融入贝叶斯优化，提高模拟电路设计效率。


<details>
  <summary>Details</summary>
Motivation: 解决模拟混合信号电路设计中高质量数据短缺、领域知识难以融入自动化流程的问题，避免传统黑盒优化在低价值区域浪费评估资源。

Method: 1) 使用图算法将电路组织为层次化设备-模块-阶段表示；2) LLM代理执行假设-验证-精炼循环进行一致性检查；3) 将验证后的洞察通过LLM引导的初始采样和停滞触发的信任区域更新整合到贝叶斯优化中。

Result: 实现了稳健的电路理解，提高了优化效率，同时保持了可行性。

Conclusion: TopoSizing框架成功地将电路结构知识转化为优化收益，为模拟电路设计提供了更高效和透明的自动化解决方案。

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [83] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO是一个离线强化学习框架，通过树形轨迹表示和过程奖励模型解决Web Agent训练中的信用分配、标注成本和奖励稀疏问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，使用大模型作为Web Agent进行自动化网页交互变得重要，但强化学习训练面临信用分配不当、标注成本过高和奖励稀疏等关键挑战。

Method: 提出Tree-Guided Preference Optimization (TGPO)框架，采用树形轨迹表示合并语义相同的状态消除标签冲突，包含过程奖励模型自动生成细粒度奖励（通过子目标进度、冗余检测和动作验证），以及动态权重机制优先处理高影响力决策点。

Result: 在Online-Mind2Web和自建C-WebShop数据集上的实验表明，TGPO显著优于现有方法，以更少的冗余步骤实现更高的成功率。

Conclusion: TGPO框架有效解决了Web Agent训练中的关键问题，通过创新的树形结构和过程奖励机制提升了性能表现。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [84] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign是一个轻量级的即插即用框架，通过表示对齐技术解决时间序列预测中输入历史与未来目标之间的分布差异问题，显著提升各种基础预测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法很少采用对比学习等表示学习技术，因为其性能优势不明显。作者认为显式的表示对齐可以提供关键信息来弥合输入历史与未来目标之间的分布差异。

Method: TimeAlign通过简单的重构任务学习辅助特征，并将这些特征反馈给任何基础预测器。该方法架构无关且计算开销极小。

Result: 在8个基准测试上的广泛实验验证了其优越性能，研究发现性能提升主要来自纠正历史输入与未来输出之间的频率不匹配问题。

Conclusion: TimeAlign可以作为现代深度学习时间序列预测系统的通用对齐模块，理论分析表明其能有效增加学习表示与预测目标之间的互信息。

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [85] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出了一个变分框架来形式化基于残差的自适应策略，通过积分残差的凸变换来系统化设计自适应方案，减少离散化误差并改善学习动态。


<details>
  <summary>Details</summary>
Motivation: 现有的基于残差的自适应策略在科学机器学习中广泛使用但缺乏理论依据，需要建立一个统一的理论框架来形式化这些方法。

Method: 引入变分框架，通过积分残差的凸变换来定义不同的目标函数：指数权重对应最小化均匀误差，线性权重对应最小化二次误差。自适应加权等价于选择优化原始目标的采样分布。

Result: 该框架能够系统设计跨范数的自适应方案，通过减少损失估计器的方差来降低离散化误差，并通过改善梯度信噪比来增强学习动态。在算子学习中展示了显著的性能提升。

Conclusion: 为基于残差的自适应性提供了理论依据，为有原则的离散化和训练策略奠定了基础。

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [86] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一个开创性的Banach-Bregman框架，将随机优化从传统的希尔伯特空间扩展到更一般的Banach空间，为下一代优化算法提供了统一的理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有的随机优化理论主要局限于希尔伯特空间，无法有效处理非欧几里得设置，如镜像下降、Bregman近端方法、自然梯度下降等。需要建立一个更通用的理论框架来统一这些方法。

Method: 提出了基于Bregman几何的Banach-Bregman框架，通过Bregman投影和Bregman-Fejer单调性提供统一模板，支持超松弛（λ > 2）在非希尔伯特设置中的应用。

Result: 在机器学习（UCI基准）、深度学习（Transformer训练）、强化学习（actor-critic）和大语言模型（WikiText-2与distilGPT-2）的实证研究中，相比经典基线方法实现了高达20%的更快收敛速度、更低的方差和更高的准确性。

Conclusion: Banach-Bregman几何成为统一优化理论和实践的核心基石，为跨AI核心范式的下一代优化算法提供了理论基础。

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [87] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: RKTV-INR：一种结合龙格-库塔积分和全变分的隐式神经表示去噪框架，用于非线性动力系统的噪声抑制和方程识别


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的数据驱动建模常受测量噪声影响，需要有效的去噪方法来准确恢复系统动力学方程

Method: 使用隐式神经表示(INR)直接拟合噪声观测数据，通过龙格-库塔积分和全变分约束确保重建状态是动力系统轨迹，然后使用SINDy方法识别控制方程

Result: 实验证明该方法能有效抑制噪声、精确估计导数，并可靠地识别系统

Conclusion: RKTV-INR框架为噪声环境下的非线性动力系统建模提供了有效的解决方案，能够同时实现去噪和系统识别

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [88] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 语言模型的激活值线性编码了训练期间信息学习的时间顺序，模型能够区分信息获取的时间点


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否以及如何编码信息在训练过程中被学习的时间顺序，这对于理解模型如何处理冲突数据和知识修改具有重要意义

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的命名实体数据集上，分析激活值的时间编码特性，使用线性探测和2D投影技术

Result: 发现测试样本的平均激活值精确编码了训练顺序（2D投影中呈直线排列），线性探测能准确区分早期和晚期实体（约90%准确率），微调后模型能报告未见实体的训练阶段（约80%准确率）

Conclusion: 语言模型确实能够编码信息获取的时间信息，这种时间信号不是简单的激活幅度、损失或置信度差异，对模型处理冲突数据和知识修改具有重要启示

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [89] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 本文提出使用临界阻尼高阶Langevin动力学来防御扩散模型中的成员推理攻击，通过引入辅助变量和联合扩散过程来保护训练数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI应用的快速发展，数据安全问题日益突出。扩散模型虽然比其他生成模型对成员推理攻击更具抵抗力，但仍然存在风险，需要有效的防御机制来保护训练数据的隐私。

Method: 采用临界阻尼高阶Langevin动力学，引入多个辅助变量和联合扩散过程。通过在扩散过程中早期引入外部随机性来破坏敏感输入数据，从而增强对成员推理攻击的防御能力。

Result: 在玩具数据集和语音数据集上进行了理论分析和实验验证，使用AUROC曲线和FID指标评估防御效果，证明了该方法的有效性。

Conclusion: 提出的基于高阶Langevin动力学的防御机制能够有效提升扩散模型对成员推理攻击的抵抗能力，为生成模型的数据安全提供了新的解决方案。

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [90] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA是一种新颖的结构化剪枝方法，通过神经正切核理论指导的显著性准则、自适应稀疏度分配机制和KL散度校准数据选择策略，在保持零样本准确性的同时实现高效LLM压缩。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的结构化剪枝方法存在显著的性能下降问题，特别是在零样本设置下，且需要昂贵的恢复技术如监督微调或适配器插入。

Method: 基于Adam优化动态下的神经正切核理论推导一阶显著性准则；采用跨层和模块的自适应稀疏度分配机制；使用KL散度进行校准数据选择以确保剪枝决策的可靠性。

Result: 在Llama3、Qwen和T5模型上的综合实验表明，NIRVANA在同等稀疏度约束下优于现有结构化剪枝方法。

Conclusion: NIRVANA提供了一个理论上有依据且实用的LLM压缩方法，平衡了零样本准确性保持和微调能力。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [91] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: CaT（Compute as Teacher）通过将模型在推理时的探索转化为无参考监督，利用并行rollout合成参考答案，将额外计算资源作为教师信号来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决在没有真实标签的后训练阶段，如何获得学习信号的问题，探索如何将推理时的计算资源转化为有效的监督信号。

Method: 使用当前策略生成一组并行rollout，通过冻结的初始策略（anchor）协调冲突和遗漏来合成参考答案，在可验证任务中使用程序等价性，在不可验证任务中使用自提规则和独立LLM评分。

Result: 在Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B上显著提升性能（MATH-500上最高+27%，HealthBench上+12%），结合强化学习（CaT-RL）后进一步提升（最高+33%和+30%）。

Conclusion: CaT成功将推理时计算转化为有效的监督信号，性能随rollout数量扩展，训练后的策略甚至能超越初始教师信号，为无监督学习提供了新思路。

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [92] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 论文提出了一个分析框架来研究LLM跨任务迁移学习中的潜在能力和副作用，发现性能提升主要受隐藏统计因素而非表面数据集相似性影响


<details>
  <summary>Details</summary>
Motivation: 由于无法为所有任务获取高质量训练数据，需要依赖不同特征的迁移学习数据集来处理分布外请求，因此需要分析跨任务交互的复杂动态

Method: 构建迁移学习矩阵和降维分析框架，训练10个模型来识别潜在能力（推理、情感分类、自然语言理解、算术等）并发现迁移学习的副作用

Result: 性能改进往往无法用表面数据集相似性或源数据质量来解释，而是受源数据集的隐藏统计因素（如类别分布、生成长度倾向）和特定语言特征影响更大

Conclusion: 这项工作揭示了迁移学习的复杂动态，为更可预测和有效的LLM适应铺平了道路

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [93] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 基于COMET框架构建的翻译质量评估系统，通过长上下文数据增强训练，预测错误跨度标注分数，整合多种人工标注数据集，实验显示长上下文信息能提升与人工评估的相关性


<details>
  <summary>Details</summary>
Motivation: 解决传统翻译质量评估模型仅基于短片段训练的局限性，通过引入长上下文信息来更好地捕捉翻译质量的整体连贯性和一致性

Method: 使用COMET框架，通过拼接领域内人工标注句子构建长上下文训练数据，计算加权平均分数，整合MQM、SQM、DA等多种人工评估数据集，训练多语言回归模型

Result: 实验结果表明，与仅使用短片段训练的模型相比，引入长上下文信息能够显著提高模型预测与人工评估之间的相关性

Conclusion: 长上下文信息对于翻译质量评估具有重要意义，能够有效提升评估模型的性能和对人工判断的吻合度

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [94] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: Slim-SC是一种通过思想层面的链间相似性识别和移除冗余链的逐步剪枝策略，可在保持或提高准确性的同时显著减少自一致性方法的计算开销。


<details>
  <summary>Details</summary>
Motivation: 自一致性(SC)方法虽然能提升LLM推理性能，但其数量级的计算开销限制了广泛应用。现有加速方法主要依赖模型置信度分数或缺乏实证支持的启发式方法。

Method: 通过理论和实证分析SC的低效性，提出Slim-SC方法：在思想层面使用链间相似性识别冗余链，采用逐步剪枝策略移除这些链。

Result: 在三个STEM推理数据集和两种LLM架构上的实验表明，Slim-SC在保持R1-Distill准确性的同时，推理延迟降低45%，KVC使用减少26%。

Conclusion: Slim-SC为自一致性方法提供了一个简单而高效的测试时缩放替代方案，有效解决了计算开销问题。

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [95] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala是一个阿拉伯语为中心的指令和翻译模型家族，通过翻译调优流程构建，在阿拉伯语基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决阿拉伯语NLP资源不足的问题，开发专门针对阿拉伯语的高质量指令和翻译模型。

Method: 使用FP8压缩的AR-EN教师模型生成高质量双语监督数据，然后微调轻量级语言模型来翻译英语指令集到阿拉伯语，最后通过slerp合并平衡阿拉伯语专业化和基础模型优势。

Result: Hala模型在阿拉伯语基准测试中在"nano"(≤2B)和"small"(7-9B)类别中都达到了最先进水平，超越了其基础模型。

Conclusion: Hala系列模型为阿拉伯语NLP研究提供了有效的解决方案，并发布了模型、数据、评估和训练方法以加速该领域研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [96] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 论文验证了训练数据中上下文丰富样本的稀疏性是机器翻译模型难以有效利用上下文的主要原因，并提出了两种训练策略来改善上下文利用效果。


<details>
  <summary>Details</summary>
Motivation: 标准训练数据中上下文丰富样本的稀疏性被认为是机器翻译难以利用上下文确保连贯性和处理复杂现象（如代词消歧）的原因，需要系统验证这一假设。

Method: 通过构建具有受控比例上下文相关示例的训练数据集，在单语和多语设置下验证数据稀疏性与模型性能的关系，并提出两种训练策略来更好地利用可用数据。

Result: 研究证实数据稀疏性是关键瓶颈，上下文改进无法跨现象泛化，跨语言迁移有限。提出的训练策略使上下文利用得到改善，在ctxPro评估中单语和多语设置分别获得最高6%和8%的准确率提升。

Conclusion: 数据稀疏性是上下文利用的主要障碍，需要针对性的训练策略来改善机器翻译的上下文处理能力，且改进效果具有现象特异性。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [97] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本研究提出了一个整合金融背景和行为金融学的新框架，通过精心策划的19k样本数据集微调Qwen-3-8B模型，在保持较低成本的同时实现了与更大模型相当的性能表现。


<details>
  <summary>Details</summary>
Motivation: 个性化财务建议需要考虑用户目标、约束条件、风险承受能力和司法管辖区。现有的LLM工作主要关注投资者和财务规划师的支持系统，而代理管道方法维护成本高且财务回报率低。

Method: 开发了一个新颖且可复现的框架，整合相关金融背景和行为金融学研究来构建监督数据，创建了19k样本的推理数据集，并对Qwen-3-8B模型进行了全面微调。

Result: 通过保留测试集和盲法LLM评审研究，证明8B模型在事实准确性、流畅性和个性化指标上与更大的基线模型（14-32B参数）表现相当，同时成本降低了80%。

Conclusion: 通过精心数据策划和行为整合，较小的模型可以在个性化财务建议任务中实现与更大模型相当的性能，同时显著降低成本。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [98] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus是一个完全开源的大型语言模型套件，专注于数据合规性和多语言表示，使用公开可用数据训练，尊重robots.txt排除规则，并采用Goldfish目标来抑制记忆风险，在8B和70B规模上达到接近最先进的多语言基准性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前开源模型生态系统中数据合规性和多语言表示不足的系统性问题，许多现有模型发布权重时缺乏可复现的数据管道或对内容所有者权利的尊重。

Method: 使用完全公开可用数据进行预训练，尊重robots.txt排除规则，过滤非许可、有毒和个人身份信息内容，采用Goldfish目标抑制数据逐字记忆，同时在1800多种语言的15T token上训练，其中约40%为非英语内容。

Result: Apertus模型在8B和70B规模上接近完全开源模型在多语言基准测试中的最先进结果，与开源权重对应模型相当或超越，同时提供了完整的科学成果发布。

Conclusion: Apertus提供了一个完全透明、合规的开源LLM解决方案，通过严格的数据处理和Goldfish技术平衡了性能与合规性，为开源社区提供了可审计和扩展的模型生态系统。

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [99] [Outperforming Dijkstra on Sparse Graphs: The Lightning Network Use Case](https://arxiv.org/abs/2509.13448)
*Danila Valko,Rohan Paranjpe,Jorge Marx Gómez*

Main category: cs.PF

TL;DR: BMSSP算法理论上比Dijkstra更快，但在实际闪电网络路由中实现时性能提升有限，速度优势小于理论预期。


<details>
  <summary>Details</summary>
Motivation: 支付通道网络（如闪电网络）需要高效路由算法，虽然Dijkstra算法在稀疏图上被认为是最优的，但新的BMSSP算法理论上具有更好的渐近时间复杂度。

Method: 在Rust中实现BMSSP算法，使用真实的闪电网络拓扑数据，通过多次随机试验和统计测试与Dijkstra算法进行性能比较。

Result: 当前BMSSP实现并未显著优于Dijkstra算法，速度提升比理论预测的要小，可能由于实现开销和常数因子影响。

Conclusion: 研究首次提供了BMSSP加速闪电网络路由的实证证据，为未来PCN路径查找算法的优化提供了参考。

Abstract: Efficient routing is critical for payment channel networks (PCNs) such as the
Lightning Network (LN), where most clients currently rely on Dijkstra-based
algorithms for payment pathfinding. While Dijkstra's algorithm has long been
regarded as optimal on sparse graphs, recent theoretical work challenges this
view. The new Bounded Multi-Source Shortest Path (BMSSP) algorithm by Duan et
al. theoretically achieves $O(m~log^{2/3}~n)$ runtime, which is asymptotically
faster than Dijkstra's $O(m + n~log~n)$ on sparse directed graphs. In this
paper, we implement BMSSP on Rust and compare its performance against
Dijkstra's using real LN topology data. Our evaluation, based on multiple
randomized trials and statistical tests, shows that current implementations of
BMSSP do not significantly outperform Dijkstra's in practice, and speedups are
smaller than what theory predicts, possibly due to implementation and constant
factor overheads. These results provide the first empirical evidence of BMSSP's
potential to accelerate LN routing and inform future optimizations of PCN
pathfinding algorithms.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [100] [Artificial neural networks ensemble methodology to predict significant wave height](https://arxiv.org/abs/2509.14020)
*Felipe Crivellaro Minuzzi,Leandro Farina*

Main category: physics.ao-ph

TL;DR: 提出了一种基于多种神经网络架构（MLP、RNN、LSTM、CNN和CNN-LSTM混合模型）的集成学习方法，用于预测巴西海岸六个不同位置的显著波高，相比NOAA数值模型误差降低5%，计算成本显著减少。


<details>
  <summary>Details</summary>
Motivation: 波浪变量的预测对海洋状态描述至关重要。由于建模微分方程的混沌特性，传统方法采用集成模拟策略。近年来，随着数据量和计算能力的增长，机器学习算法作为数值模型的替代方案显示出可比或更好的结果。

Method: 使用多种神经网络架构（MLP、RNN、LSTM、CNN和CNN-LSTM混合）创建集成模型，利用NOAA数值再预报数据训练，目标是预测观测数据与数值模型输出之间的残差。提出新的训练和目标数据集构建策略。

Result: 框架能够产生高效预测，平均准确率达到80%，最佳情况下可达88%。相比NOAA数值模型，误差指标降低5%，计算成本显著减少。

Conclusion: 该方法成功实现了对显著波高的高精度预测，证明了机器学习集成方法在海洋预报中的有效性，既提高了预测精度又降低了计算成本。

Abstract: The forecast of wave variables are important for several applications that
depend on a better description of the ocean state. Due to the chaotic behaviour
of the differential equations which model this problem, a well know strategy to
overcome the difficulties is basically to run several simulations, by for
instance, varying the initial condition, and averaging the result of each of
these, creating an ensemble. Moreover, in the last few years, considering the
amount of available data and the computational power increase, machine learning
algorithms have been applied as surrogate to traditional numerical models,
yielding comparative or better results. In this work, we present a methodology
to create an ensemble of different artificial neural networks architectures,
namely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict
significant wave height on six different locations in the Brazilian coast. The
networks are trained using NOAA's numerical reforecast data and target the
residual between observational data and the numerical model output. A new
strategy to create the training and target datasets is demonstrated. Results
show that our framework is capable of producing high efficient forecast, with
an average accuracy of $80\%$, that can achieve up to $88\%$ in the best case
scenario, which means $5\%$ reduction in error metrics if compared to NOAA's
numerical model, and a increasingly reduction of computational cost.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [101] [Julia GraphBLAS with Nonblocking Execution](https://arxiv.org/abs/2509.14211)
*Pascal Costanza,Timothy G. Mattson,Raye Kimmerer,Benjamin Brock*

Main category: cs.MS

TL;DR: 本文介绍了在Julia编程语言中实现GraphBLAS非阻塞执行的工作进展，展示了Julia特性如何简化非阻塞执行实现，目前支持PageRank所需的GraphBLAS方法。


<details>
  <summary>Details</summary>
Motivation: GraphBLAS最初设计就支持非阻塞执行，但现有实现范围有限。本文旨在实现更激进的非阻塞执行，利用Julia语言特性简化实现过程。

Method: 利用Julia编程语言的特性和优势，实现GraphBLAS的非阻塞执行机制，构建操作的有向无环图(DAG)，支持函数融合、对象省略和并行化等优化。

Result: 成功实现了支持非阻塞执行的GraphBLAS框架，能够进行DAG保持的转换优化，但目前仅实现了支持PageRank算法所需的基本方法。

Conclusion: Julia语言特性极大简化了GraphBLAS非阻塞执行的实现，展示了非阻塞执行的巨大潜力，但当前仍处于工作进展阶段，功能有限。

Abstract: From the beginning, the GraphBLAS were designed for ``nonblocking
execution''; i.e., calls to GraphBLAS methods return as soon as the arguments
to the methods are validated and define a directed acyclic graph (DAG) of
GraphBLAS operations. This lets GraphBLAS implementations fuse functions, elide
unneeded objects, exploit parallelism, plus any additional DAG-preserving
transformations. GraphBLAS implementations exist that utilize nonblocking
execution but with limited scope. In this paper, we describe our work to
implement GraphBLAS with support for aggressive nonblocking execution. We show
how features of the Julia programming language greatly simplify implementation
of nonblocking execution. This is \emph{work-in-progress} sufficient to show
the potential for nonblocking execution and is limited to GraphBLAS methods
required to support PageRank.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [102] [Valuation of Exotic Options and Counterparty Games Based on Conditional Diffusion](https://arxiv.org/abs/2509.13374)
*Helin Zhao,Junchi Shen*

Main category: q-fin.PR

TL;DR: 本文提出扩散条件概率模型(DDPM)来生成更真实的价格路径，用于定价奇异期权和结构化产品，相比传统模型能更好捕捉厚尾分布和波动率聚集等市场现象。


<details>
  <summary>Details</summary>
Motivation: 传统定价模型无法有效处理真实市场中的厚尾分布和波动率聚集现象，导致奇异期权和结构化产品定价不准确。

Method: 采用扩散条件概率模型(DDPM)，结合复合损失函数和金融特定特征，并提出了P-Q动态博弈框架进行对抗性回测评估。

Result: 静态验证显示模型能有效匹配市场均值和波动率；动态博弈中在欧洲和亚式期权上比传统蒙特卡洛模型获利显著更高，但对极端事件敏感的产品(如雪球和累购期权)存在尾部风险低估问题。

Conclusion: 扩散模型在提升定价准确性方面具有巨大潜力，但需要进一步研究改进对极端市场风险的建模能力。

Abstract: This paper addresses the challenges of pricing exotic options and structured
products, which traditional models often fail to handle due to their inability
to capture real-world market phenomena like fat-tailed distributions and
volatility clustering. We introduce a Diffusion-Conditional Probability Model
(DDPM) to generate more realistic price paths. Our method incorporates a
composite loss function with financial-specific features, and we propose a P-Q
dynamic game framework for evaluating the model's economic value through
adversarial backtesting. Static validation shows our P-model effectively
matches market mean and volatility. In dynamic games, it demonstrates
significantly higher profitability than a traditional Monte Carlo-based model
for European and Asian options. However, the model shows limitations in pricing
products highly sensitive to extreme events, such as snowballs and
accumulators, because it tends to underestimate tail risks. The study concludes
that diffusion models hold significant potential for enhancing pricing
accuracy, though further research is needed to improve their ability to model
extreme market risks.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [103] [Accelerated Gradient Methods with Biased Gradient Estimates: Risk Sensitivity, High-Probability Guarantees, and Large Deviation Bounds](https://arxiv.org/abs/2509.13628)
*Mert Gürbüzbalaban,Yasa Syed,Necdet Serhat Aybat*

Main category: math.OC

TL;DR: 本文研究了广义动量方法在收敛速度和梯度误差鲁棒性之间的权衡关系，提出了基于风险敏感指数的分析框架，为带有偏差梯度的一阶优化方法提供了首个非渐近保证


<details>
  <summary>Details</summary>
Motivation: 研究一阶优化方法在存在对抗性和有偏梯度误差时的收敛性能与鲁棒性之间的权衡关系，填补广义动量方法在有偏梯度条件下的风险敏感分析空白

Method: 使用2x2 Riccati方程分析二次目标函数下的风险敏感指数，建立大偏差原理，连接H∞范数与尾部概率衰减，并在非二次情况下推导有限时间风险敏感指数的非渐近边界

Result: 揭示了收敛速度与鲁棒性之间的帕累托前沿，证明了更强的鲁棒性导致尾部概率更快的衰减，为带有偏差梯度的广义动量方法提供了有限时间高概率保证

Conclusion: 广义动量方法存在收敛速度与鲁棒性的基本权衡，风险敏感指数为分析优化算法在噪声环境下的性能提供了有效框架，实验结果验证了理论发现

Abstract: We study trade-offs between convergence rate and robustness to gradient
errors in first-order methods. Our focus is on generalized momentum methods
(GMMs), a class that includes Nesterov's accelerated gradient, heavy-ball, and
gradient descent. We allow stochastic gradient errors that may be adversarial
and biased, and quantify robustness via the risk-sensitive index (RSI) from
robust control theory. For quadratic objectives with i.i.d. Gaussian noise, we
give closed-form expressions for RSI using 2x2 Riccati equations, revealing a
Pareto frontier between RSI and convergence rate over stepsize and momentum
choices. We prove a large-deviation principle for time-averaged suboptimality
and show that the rate function is, up to scaling, the convex conjugate of the
RSI. We further connect RSI to the $H_{\infty}$-norm, showing that stronger
worst-case robustness (smaller $H_{\infty}$ norm) yields sharper decay of tail
probabilities. Beyond quadratics, under biased sub-Gaussian gradient errors, we
derive non-asymptotic bounds on a finite-time analogue of the RSI, giving
finite-time high-probability guarantees and large-deviation bounds. We also
observe an analogous trade-off between RSI and convergence-rate bounds for
smooth strongly convex functions. To our knowledge, these are the first
non-asymptotic guarantees and risk-sensitive analysis of GMMs with biased
gradients. Numerical experiments on robust regression illustrate the results.

</details>


### [104] [Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain](https://arxiv.org/abs/2509.14203)
*Shengbo Wang,Nian Si*

Main category: math.OC

TL;DR: 本文针对平均奖励鲁棒马尔可夫决策过程(MDPs)提出了一个理论框架，重点分析了常数增益设置下的鲁棒Bellman方程，解决了存在性问题和最优策略表征问题。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒MDP研究主要集中在有限时域或折扣模型，而平均奖励形式在运营研究和管理场景中更自然但研究不足，因为其动态规划基础技术挑战大且存在未解决的基本问题。

Method: 研究平均奖励鲁棒控制问题，分析控制器与S-矩形对手之间可能存在信息不对称的情况，重点研究常数增益鲁棒Bellman方程的解存在性及其与最优平均奖励的关系。

Result: 确定了鲁棒Bellman方程解何时能够表征最优平均奖励和稳态策略，并提供了确保解存在的充分条件。

Conclusion: 这些发现扩展了平均奖励鲁棒MDP的动态规划理论，为运营环境中长期平均准则下的鲁棒动态决策制定奠定了基础。

Abstract: Learning and optimal control under robust Markov decision processes (MDPs)
have received increasing attention, yet most existing theory, algorithms, and
applications focus on finite-horizon or discounted models. The average-reward
formulation, while natural in many operations research and management contexts,
remains underexplored. This is primarily because the dynamic programming
foundations are technically challenging and only partially understood, with
several fundamental questions remaining open. This paper steps toward a general
framework for average-reward robust MDPs by analyzing the constant-gain
setting. We study the average-reward robust control problem with possible
information asymmetries between the controller and an S-rectangular adversary.
Our analysis centers on the constant-gain robust Bellman equation, examining
both the existence of solutions and their relationship to the optimal average
reward. Specifically, we identify when solutions to the robust Bellman equation
characterize the optimal average reward and stationary policies, and we provide
sufficient conditions ensuring solutions' existence. These findings expand the
dynamic programming theory for average-reward robust MDPs and lay a foundation
for robust dynamic decision making under long-run average criteria in
operational environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [105] [A novel approach of day-ahead cooling load prediction and optimal control for ice-based thermal energy storage (TES) system in commercial buildings](https://arxiv.org/abs/2509.13371)
*Xuyuan Kang,Xiao Wang,Jingjing An,Da Yan*

Main category: eess.SY

TL;DR: 提出了一种集成负荷预测和优化控制的冰蓄冷系统新方法，通过中期预测修正和基于分时电价的规则控制策略，实现了9.9%的能源成本节约


<details>
  <summary>Details</summary>
Motivation: 现有TES系统大多采用固定运行计划，无法充分利用负荷转移能力，需要大量调查和优化工作

Method: 开发冷却负荷预测模型并引入中期修正机制，基于预测结果制定基于分时电价的规则控制策略，并引入中期控制调整机制

Result: 在北京某商业综合体应用中，MAE为389kW，变异系数12.5%，能源成本节约率达9.9%，成功部署到实际楼宇自动化系统

Conclusion: 所提出的集成预测控制方法显著提高了冷却系统的效率和自动化水平，为商业建筑冰蓄冷系统提供了有效的优化控制解决方案

Abstract: Thermal energy storage (TES) is an effective method for load shifting and
demand response in buildings. Optimal TES control and management are essential
to improve the performance of the cooling system. Most existing TES systems
operate on a fixed schedule, which cannot take full advantage of its load
shifting capability, and requires extensive investigation and optimization.
This study proposed a novel integrated load prediction and optimized control
approach for ice-based TES in commercial buildings. A cooling load prediction
model was developed and a mid-day modification mechanism was introduced into
the prediction model to improve the accuracy. Based on the predictions, a
rule-based control strategy was proposed according to the time-of-use tariff;
the mid-day control adjustment mechanism was introduced in accordance with the
mid-day prediction modifications. The proposed approach was applied in the
ice-based TES system of a commercial complex in Beijing, and achieved a mean
absolute error (MAE) of 389 kW and coefficient of variance of MAE of 12.5%. The
integrated prediction-based control strategy achieved an energy cost saving
rate of 9.9%. The proposed model was deployed in the realistic building
automation system of the case building and significantly improved the
efficiency and automation of the cooling system.

</details>


### [106] [Circuit realization and hardware linearization of monotone operator equilibrium networks](https://arxiv.org/abs/2509.13793)
*Thomas Chaffey*

Main category: eess.SY

TL;DR: 电阻-二极管网络的端口行为对应于ReLU单调算子平衡网络的解，为模拟硬件中的神经网络提供了简洁构造。该电路的梯度可直接在硬件中计算，实现硬件训练。结果可扩展到级联网络实现前馈等不对称网络，不同非线性元件产生不同激活函数。


<details>
  <summary>Details</summary>
Motivation: 探索在模拟硬件中简洁实现神经网络的方法，利用电阻-二极管网络的物理特性来构建神经网络等效电路，并实现在硬件中直接计算梯度和训练。

Method: 通过分析电阻-二极管网络的端口行为，证明其对应于ReLU单调算子平衡网络的解。提出硬件线性化方法直接在硬件中计算梯度，并通过器件级电路仿真验证。扩展到级联网络结构，研究不同非线性元件产生的激活函数。

Result: 成功构建了模拟硬件中的神经网络等效电路，实现了硬件中的梯度计算和训练。提出了新型二极管ReLU激活函数，并展示了不同非线性元件对应不同激活函数的特性。

Conclusion: 电阻-二极管网络为模拟硬件实现神经网络提供了有效的物理实现方式，硬件线性化方法使得在硬件中直接训练成为可能，为神经网络的硬件实现开辟了新途径。

Abstract: It is shown that the port behavior of a resistor-diode network corresponds to
the solution of a ReLU monotone operator equilibrium network (a neural network
in the limit of infinite depth), giving a parsimonious construction of a neural
network in analog hardware. We furthermore show that the gradient of such a
circuit can be computed directly in hardware, using a procedure we call
hardware linearization. This allows the network to be trained in hardware,
which we demonstrate with a device-level circuit simulation. We extend the
results to cascades of resistor-diode networks, which can be used to implement
feedforward and other asymmetric networks. We finally show that different
nonlinear elements give rise to different activation functions, and introduce
the novel diode ReLU which is induced by a non-ideal diode model.

</details>


### [107] [Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection](https://arxiv.org/abs/2509.13934)
*Zhixion Chen,Jiangzhou Wang,and Hyundong Shin,Arumugam Nallanathan*

Main category: eess.SY

TL;DR: 提出LLM-CRDT框架，结合大语言模型和决策变换器，用于无人机轨迹规划和资源分配，提高数据收集能效


<details>
  <summary>Details</summary>
Motivation: 解决无人机在物联网数据收集中续航和通信范围有限的问题，传统强化学习方法成本高风险大，离线RL训练不稳定且依赖专家数据

Method: 将资源分配问题转化为线性规划求解，提出LLM-CRDT框架：使用评论家网络正则化决策变换器训练，采用预训练LLM作为变换器骨干，使用LoRA参数高效微调策略

Result: 在仿真中优于基准在线和离线RL方法，比当前最先进的DT方法能效提高36.7%

Conclusion: LLM-CRDT框架能够从小规模数据集中学习有效策略，实现高效节能的无人机数据收集

Abstract: The deployment of unmanned aerial vehicles (UAVs) for reliable and
energy-efficient data collection from spatially distributed devices holds great
promise in supporting diverse Internet of Things (IoT) applications.
Nevertheless, the limited endurance and communication range of UAVs necessitate
intelligent trajectory planning. While reinforcement learning (RL) has been
extensively explored for UAV trajectory optimization, its interactive nature
entails high costs and risks in real-world environments. Offline RL mitigates
these issues but remains susceptible to unstable training and heavily rely on
expert-quality datasets. To address these challenges, we formulate a joint UAV
trajectory planning and resource allocation problem to maximize energy
efficiency of data collection. The resource allocation subproblem is first
transformed into an equivalent linear programming formulation and solved
optimally with polynomial-time complexity. Then, we propose a large language
model (LLM)-empowered critic-regularized decision transformer (DT) framework,
termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we
incorporate critic networks to regularize the DT model training, thereby
integrating the sequence modeling capabilities of DT with critic-based value
guidance to enable learning effective policies from suboptimal datasets.
Furthermore, to mitigate the data-hungry nature of transformer models, we
employ a pre-trained LLM as the transformer backbone of the DT model and adopt
a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid
adaptation to UAV control tasks with small-scale dataset and low computational
overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark
online and offline RL methods, achieving up to 36.7\% higher energy efficiency
than the current state-of-the-art DT approaches.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [108] [Why all roads don't lead to Rome: Representation geometry varies across the human visual cortical hierarchy](https://arxiv.org/abs/2509.13459)
*Arna Ghosh,Zahraa Chorghay,Shahab Bakhtiari,Blake A. Richards*

Main category: q-bio.NC

TL;DR: 本文研究了生物和人工智能系统中的效率-鲁棒性权衡问题，发现人类视觉皮层和自监督学习的人工神经网络都表现出尺度无关的表征几何特性，但这种特性并非普遍存在，而是取决于计算目标。


<details>
  <summary>Details</summary>
Motivation: 研究生物和人工智能系统如何在效率-鲁棒性权衡中进行最优编码，特别关注分层处理系统如人类大脑如何处理这一挑战。

Method: 采用群体几何框架分析人类视觉皮层和人工神经网络的表征，比较不同区域和训练方式下的表征几何特性。

Result: 在腹侧视觉通路中发现大多数区域具有尺度无关表征（幂律衰减特征谱），但某些高阶视觉区域没有；自监督学习的ANN也表现出尺度无关几何，但经过特定任务微调后消失。

Conclusion: 系统的表征几何不是普遍属性，而是取决于具体的计算目标，这为理解智能系统的编码策略提供了重要见解。

Abstract: Biological and artificial intelligence systems navigate the fundamental
efficiency-robustness tradeoff for optimal encoding, i.e., they must
efficiently encode numerous attributes of the input space while also being
robust to noise. This challenge is particularly evident in hierarchical
processing systems like the human brain. With a view towards understanding how
systems navigate the efficiency-robustness tradeoff, we turned to a population
geometry framework for analyzing representations in the human visual cortex
alongside artificial neural networks (ANNs). In the ventral visual stream, we
found general-purpose, scale-free representations characterized by a power
law-decaying eigenspectrum in most areas. However, in certain higher-order
visual areas did not have scale-free representations, indicating that
scale-free geometry is not a universal property of the brain. In parallel, ANNs
trained with a self-supervised learning objective also exhibited free-free
geometry, but not after fine-tune on a specific task. Based on these empirical
results and our analytical insights, we posit that a system's representation
geometry is not a universal property and instead depends upon the computational
objective.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [109] [On the Rate of Gaussian Approximation for Linear Regression Problems](https://arxiv.org/abs/2509.14039)
*Marat Khusainov,Marina Sheshukova,Alain Durmus,Sergey Samsonov*

Main category: stat.ML

TL;DR: 该论文研究了在线线性回归任务中的高斯近似问题，分析了恒定学习率设置下的收敛速率及其对问题维度d和设计矩阵相关量的显式依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在线线性回归是机器学习中的重要问题，需要理解其统计性质和高斯近似的行为，特别是在恒定学习率设置下收敛速率如何随问题维度变化。

Method: 推导了在线线性回归的高斯近似理论，分析了恒定学习率设置下的收敛行为，研究了收敛速率对问题维度d和设计矩阵相关量的显式依赖关系。

Result: 当迭代次数n已知时，在样本量n足够大的条件下，获得了阶数为√(log n/n)的正态近似速率。

Conclusion: 该研究为在线线性回归的高斯近似提供了理论保证，明确了收敛速率与问题维度和样本量的定量关系，对理解在线学习算法的统计性质具有重要意义。

Abstract: In this paper, we consider the problem of Gaussian approximation for the
online linear regression task. We derive the corresponding rates for the
setting of a constant learning rate and study the explicit dependence of the
convergence rate upon the problem dimension $d$ and quantities related to the
design matrix. When the number of iterations $n$ is known in advance, our
results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$,
provided that the sample size $n$ is large enough.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [110] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: 基于强化学习的无人机超视距路径规划方法，通过最大化蜂窝链路质量同时最小化飞行距离，确保安全可靠的超视距飞行操作


<details>
  <summary>Details</summary>
Motivation: 解决超视距无人机在蜂窝通信中的连接质量挑战，需要研究考虑实际空中覆盖约束和信道模型的路径规划方法

Method: 采用强化学习技术训练智能体，使用无人机与基站之间的通信链路质量作为奖励函数，结合经验空中信道模型

Result: 仿真结果表明该方法能有效训练智能体并生成可行的无人机路径规划，RL算法能高效识别最优路径确保最大连接性

Conclusion: 该方法可作为离线路径规划模块集成到未来地面控制系统中，在复杂长距离无人机应用中具有潜力，推动了蜂窝连接无人机路径规划技术的发展

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [111] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: Point-JEPA自监督预训练方法在低标签数据情况下显著提升抓取关节角度预测性能，在DLR-Hand II数据集上RMSE降低26%，达到全监督性能水平


<details>
  <summary>Details</summary>
Motivation: 研究3D自监督预训练是否能够实现标签高效的抓取关节角度预测，解决数据标注成本高的问题

Method: 使用从网格标记化的点云数据，采用ShapeNet预训练的Point-JEPA编码器，训练轻量级多假设头部网络，结合winner-takes-all策略和top-logit选择评估

Result: 在DLR-Hand II数据集的对象级分割上，Point-JEPA在低标签情况下将RMSE降低高达26%，并达到与全监督方法相当的性能

Conclusion: JEPA风格的预训练是数据高效抓取学习的一种实用方法，证明了自监督预训练在3D抓取任务中的有效性

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [112] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: ASTREA是首个在飞行级硬件上部署的自主航天器操作智能体系统，结合了资源受限的LLM智能体和强化学习控制器，在地面实验中改善了热稳定性，但在太空站轨道验证中因推理延迟与快速热循环不匹配而出现性能下降。


<details>
  <summary>Details</summary>
Motivation: 开发首个在飞行级硬件上部署的自主航天器操作智能体系统，探索将语义推理与自适应控制结合在空间合格平台上的可行性。

Method: 采用异步架构，集成资源受限的大型语言模型智能体与强化学习控制器，以热控制为代表性用例进行验证。

Result: 地面实验显示LLM引导的监督改善了热稳定性并减少了违规，但在国际空间站的轨道验证中，由于推理延迟与低地球轨道卫星的快速热循环不匹配，导致性能下降。

Conclusion: 研究揭示了基于LLM的智能体系统在真实飞行环境中的机遇和当前局限性，为未来空间自主性提供了实用的设计指导原则。

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [113] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: 提出分层多智能体近端策略优化框架，解决水下自主航行器协同检测中的隐蔽通信问题，通过双时间尺度控制实现高效协作与隐蔽操作


<details>
  <summary>Details</summary>
Motivation: 水下自主航行器协同检测面临通信暴露风险，在对抗环境中需要平衡高效协作与隐蔽操作的需求

Method: 采用分层多智能体近端策略优化框架，高层由中央AUV决定任务参与者，低层通过功率和轨迹控制降低暴露概率

Result: 仿真结果显示框架快速收敛，性能优于基准算法，在确保隐蔽操作的同时最大化长期协作效率

Conclusion: 该分层框架有效解决了水下协同任务中隐蔽性与效率的平衡问题，为对抗环境下的AUV协同操作提供了可行方案

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [114] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: VEGA是一个基于强化学习的电动汽车充电感知导航系统，使用物理信息神经网络和PPO算法优化路径规划和充电策略，无需额外传感器即可实现接近特斯拉导航的性能。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆需求增长和电动汽车计算能力提升，需要开发能够根据车辆实时状态和环境条件进行充电感知路径优化的AI系统，以降低电动汽车成本并提高能效。

Method: 采用两模块架构：1)物理信息神经网络算子(PINO)从车速日志学习车辆定制化动力学参数；2)强化学习代理使用PPO算法在充电站标注的道路图上进行预算A*师生指导下的SOC约束路径优化。

Result: 在长距离路线(如旧金山到纽约)上，VEGA的停车点、停留时间、SOC管理和总旅行时间与特斯拉行程规划器高度吻合，稍显保守但更符合实际车辆状况。系统在美国训练后能在法国和日本成功计算最优路径，展示了良好的泛化能力。

Conclusion: VEGA成功实现了物理信息学习与强化学习在电动汽车生态路由中的实用集成，可作为虚拟传感器用于功率和效率估计，有望降低电动汽车成本。

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [115] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: TreeIRL结合蒙特卡洛树搜索和逆强化学习，在自动驾驶规划中实现安全性与人类驾驶相似性的平衡，在仿真和真实道路测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶规划中安全性与人类驾驶行为相似性的平衡问题，传统方法往往难以同时兼顾安全性、效率和自然性

Method: 使用蒙特卡洛树搜索(MCTS)生成安全候选轨迹，然后通过深度逆强化学习(IRL)评分函数选择最像人类驾驶的轨迹

Result: 在大规模仿真和拉斯维加斯500+英里真实道路测试中，TreeIRL在密集城市交通、自适应巡航、切入场景和交通灯等场景中表现最佳，综合平衡了安全性、进度、舒适度和人类相似性

Conclusion: 这是首个在公共道路上展示的基于MCTS的规划方法，强调了在多样化指标和真实环境中评估规划器的重要性，为探索经典方法和学习方法的组合提供了可扩展框架

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [116] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: 提出了一种分布式移动传感框架，使用机器学习有限元模型指导多机器人团队在复杂流动环境中进行信息驱动的源定位，相比基线方法实现了更快的误差减少和更准确的定位。


<details>
  <summary>Details</summary>
Motivation: 复杂流动环境中的源定位（如化学泄漏或石油泄漏）面临挑战，包括时变混沌流动、间歇性传感器读数、复杂环境几何形状以及机载计算资源有限难以运行计算密集型数值模型的问题。

Method: 每个机器人携带机器学习有限元环境模型，使用近似互信息准则来指导信息驱动采样，通过infotaxis控制策略选择预期能最大化源定位信息量的传感区域。

Result: 相比基线传感策略实现了更快的误差减少，相比基线机器学习方法获得了更准确的源定位结果。

Conclusion: 分布式移动传感框架结合机器学习有限元模型和信息驱动控制策略，能够有效解决复杂流动环境中的源定位问题，在计算资源有限的情况下仍能实现高性能的定位效果。

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [117] [A Geometric Graph-Based Deep Learning Model for Drug-Target Affinity Prediction](https://arxiv.org/abs/2509.13476)
*Md Masud Rana,Farjana Tasnim Mukta,Duc D. Nguyen*

Main category: q-bio.BM

TL;DR: DeepGGL是一个深度卷积神经网络，通过整合残差连接和注意力机制，在几何图学习框架中有效捕获蛋白质-配体复合物的多尺度原子级相互作用，在结合亲和力预测方面达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 在基于结构的药物设计中，准确估计候选配体与其蛋白质受体之间的结合亲和力是一个核心挑战。传统经验方法和物理方法存在局限性，而深度学习方法在该任务上展现出优越性能。

Method: 开发了DeepGGL深度卷积神经网络，整合残差连接和注意力机制，利用多尺度加权彩色二分图子图，在几何图学习框架中捕获蛋白质-配体复合物的多尺度原子级相互作用。

Result: 在CASF-2013和CASF-2016基准测试中达到最先进性能，各项评估指标均有显著提升。在CSAR-NRC-HiQ数据集和PDBbind v2019保留集上保持高预测准确性，显示出良好的鲁棒性和泛化能力。

Conclusion: DeepGGL在结合亲和力预测方面表现出优异的适应性和可靠性，为基于结构的药物发现提供了有效的计算工具。

Abstract: In structure-based drug design, accurately estimating the binding affinity
between a candidate ligand and its protein receptor is a central challenge.
Recent advances in artificial intelligence, particularly deep learning, have
demonstrated superior performance over traditional empirical and physics-based
methods for this task, enabled by the growing availability of structural and
experimental affinity data. In this work, we introduce DeepGGL, a deep
convolutional neural network that integrates residual connections and an
attention mechanism within a geometric graph learning framework. By leveraging
multiscale weighted colored bipartite subgraphs, DeepGGL effectively captures
fine-grained atom-level interactions in protein-ligand complexes across
multiple scales. We benchmarked DeepGGL against established models on CASF-2013
and CASF-2016, where it achieved state-of-the-art performance with significant
improvements across diverse evaluation metrics. To further assess robustness
and generalization, we tested the model on the CSAR-NRC-HiQ dataset and the
PDBbind v2019 holdout set. DeepGGL consistently maintained high predictive
accuracy, highlighting its adaptability and reliability for binding affinity
prediction in structure-based drug discovery.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [118] [Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement](https://arxiv.org/abs/2509.13699)
*Max Barth,Marie-Christine Jakobs*

Main category: cs.LO

TL;DR: 本文提出了一种并行化trace abstraction的方法，通过并行分析可能违反安全属性的不同程序路径来加速软件模型检测，在Ultimate Automizer工具中实现并取得了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 自动软件验证（特别是软件模型检测）耗时较长，限制了其在持续集成等实际应用中的使用。为了利用多核CPU减少响应时间，需要并行化验证过程。

Method: 并行化trace abstraction中的抽象精化过程，通过并行分析不同的程序路径（trace）来验证安全属性，在Ultimate Automizer工具中实现该并行化方法。

Result: 评估显示并行化方法比顺序trace abstraction更有效，在许多耗时任务上能显著更快地提供结果，并且比最近的并行抽象方法DSS更有效。

Conclusion: 并行化trace abstraction是提高软件模型检测效率的有效方法，能够充分利用多核CPU资源，显著减少验证时间，提升实际应用价值。

Abstract: Automatic software verification is a valuable means for software quality
assurance. However, automatic verification and in particular software model
checking can be time-consuming, which hinders their practical applicability
e.g., the use in continuous integration. One solution to address the issue is
to reduce the response time of the verification procedure by leveraging today's
multi-core CPUs.
  In this paper, we propose a solution to parallelize trace abstraction, an
abstraction-based approach to software model checking. The underlying idea of
our approach is to parallelize the abstraction refinement. More concretely, our
approach analyzes different traces (syntactic program paths) that could violate
the safety property in parallel. We realize our parallelized version of trace
abstraction in the verification tool Ulti mate Automizer and perform a thorough
evaluation. Our evaluation shows that our parallelization is more effective
than sequential trace abstraction and can provide results significantly faster
on many time-consuming tasks. Also, our approach is more effective than DSS, a
recent parallel approach to abstraction-based software model checking.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [119] [A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds](https://arxiv.org/abs/2509.13390)
*Deepti Kunte,Bram Cornelis,Claudio Colangeli,Karl Janssens,Brecht Van Baelen,Konstantinos Gryllias*

Main category: cs.SD

TL;DR: 提出了一种基于领域知识的模型选择方法，通过工程化代理异常来改进汽车舱内声音异常检测的无监督学习模型选择


<details>
  <summary>Details</summary>
Motivation: 汽车舱内声音异常检测通常是无监督学习问题，但由于缺乏标记故障数据，传统验证方法如重构误差不可靠，模型选择成为重大挑战

Method: 使用结构化扰动健康频谱图生成代理异常，在验证集中支持模型选择，基于领域知识设计代理异常

Result: 在包含五种典型故障类型的高保真电动汽车数据集上实验表明，使用代理异常选择的模型显著优于传统模型选择策略

Conclusion: 提出的领域知识驱动的代理异常方法为无监督异常检测提供了有效的模型选择解决方案，解决了缺乏真实故障样本的验证难题

Abstract: The detection of anomalies in automotive cabin sounds is critical for
ensuring vehicle quality and maintaining passenger comfort. In many real-world
settings, this task is more appropriately framed as an unsupervised learning
problem rather than the supervised case due to the scarcity or complete absence
of labeled faulty data. In such an unsupervised setting, the model is trained
exclusively on healthy samples and detects anomalies as deviations from normal
behavior. However, in the absence of labeled faulty samples for validation and
the limited reliability of commonly used metrics, such as validation
reconstruction error, effective model selection remains a significant
challenge. To overcome these limitations, a domain-knowledge-informed approach
for model selection is proposed, in which proxy-anomalies engineered through
structured perturbations of healthy spectrograms are used in the validation set
to support model selection. The proposed methodology is evaluated on a
high-fidelity electric vehicle dataset comprising healthy and faulty cabin
sounds across five representative fault types viz., Imbalance, Modulation,
Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced
sound synthesis techniques, and validated via expert jury assessments, has been
made publicly available to facilitate further research. Experimental
evaluations on the five fault cases demonstrate the selection of optimal models
using proxy-anomalies, significantly outperform conventional model selection
strategies.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [120] [Benchmarking Dimensionality Reduction Techniques for Spatial Transcriptomics](https://arxiv.org/abs/2509.13344)
*Md Ishtyaq Mahmud,Veena Kochat,Suresh Satpati,Jagan Mohan Reddy Dwarampudi,Kunal Rai,Tania Banerjee*

Main category: q-bio.GN

TL;DR: 提出了一个统一框架来评估空间转录组学中的降维技术，系统比较了PCA、NMF、自编码器、VAE等六种方法在不同潜在维度和聚类分辨率下的性能，并引入了两个新的生物学指标CMC和MER。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组学分析主要依赖标准PCA方法，缺乏对多种降维技术的系统性评估框架，需要开发更全面的评估指标来指导方法选择。

Method: 在胆管癌Xenium数据集上测试六种降维方法，系统变化潜在维度(k=5-40)和聚类分辨率(ρ=0.1-1.2)，使用重建误差、解释方差、聚类内聚性以及新提出的CMC和MER指标进行评估。

Result: 不同方法表现各异：PCA提供快速基线，NMF最大化标记富集，VAE平衡重建和可解释性，自编码器处于中间位置。MER引导的重分配使CMC分数平均提高12%。

Conclusion: 该框架为空间转录组学分析提供了原则性的降维方法选择指南，通过Pareto最优分析和生物学指标指导实现更好的生物学保真度。

Abstract: We introduce a unified framework for evaluating dimensionality reduction
techniques in spatial transcriptomics beyond standard PCA approaches. We
benchmark six methods PCA, NMF, autoencoder, VAE, and two hybrid embeddings on
a cholangiocarcinoma Xenium dataset, systematically varying latent dimensions
($k$=5-40) and clustering resolutions ($\rho$=0.1-1.2). Each configuration is
evaluated using complementary metrics including reconstruction error, explained
variance, cluster cohesion, and two novel biologically-motivated measures:
Cluster Marker Coherence (CMC) and Marker Exclusion Rate (MER). Our results
demonstrate distinct performance profiles: PCA provides a fast baseline, NMF
maximizes marker enrichment, VAE balances reconstruction and interpretability,
while autoencoders occupy a middle ground. We provide systematic hyperparameter
selection using Pareto optimal analysis and demonstrate how MER-guided
reassignment improves biological fidelity across all methods, with CMC scores
improving by up to 12\% on average. This framework enables principled selection
of dimensionality reduction methods tailored to specific spatial
transcriptomics analyses.

</details>


### [121] [PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction](https://arxiv.org/abs/2509.14037)
*Ranga Baminiwatte,Kazi Jewel Rana,Aaron J. Masino*

Main category: q-bio.GN

TL;DR: PhenoGnet是一个基于图对比学习的疾病相似性预测框架，整合基因功能互作网络和人类表型本体，通过GCN/GAT编码和跨视图对比学习，在疾病相似性预测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 理解疾病相似性对于诊断、药物发现和个性化治疗策略至关重要，需要开发能够整合基因和表型信息的新方法来准确预测疾病间的关系。

Method: 提出PhenoGnet框架，包含两个组件：1）使用GCN和GAT分别编码基因和表型图的内部视图模型；2）使用共享权重的MLP实现跨视图对比学习，对齐基因和表型嵌入。使用已知基因-表型关联作为正样本，随机采样不相关对作为负样本进行训练。

Result: 在1,100个相似和866个不相似疾病对的标准测试集上，基因嵌入方法达到AUCPR 0.9012和AUROC 0.8764，优于现有最先进方法，能够捕捉超越直接重叠的潜在生物学关系。

Conclusion: PhenoGnet提供了一个可扩展且可解释的疾病相似性预测解决方案，在罕见病研究和精准医学中具有重要应用潜力。

Abstract: Understanding disease similarity is critical for advancing diagnostics, drug
discovery, and personalized treatment strategies. We present PhenoGnet, a novel
graph-based contrastive learning framework designed to predict disease
similarity by integrating gene functional interaction networks with the Human
Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view
model that separately encodes gene and phenotype graphs using Graph
Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross
view model implemented as a shared weight multilayer perceptron (MLP) that
aligns gene and phenotype embeddings through contrastive learning. The model is
trained using known gene phenotype associations as positive pairs and randomly
sampled unrelated pairs as negatives. Diseases are represented by the mean
embeddings of their associated genes and/or phenotypes, and pairwise similarity
is computed via cosine similarity. Evaluation on a curated benchmark of 1,100
similar and 866 dissimilar disease pairs demonstrates strong performance, with
gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,
outperforming existing state of the art methods. Notably, PhenoGnet captures
latent biological relationships beyond direct overlap, offering a scalable and
interpretable solution for disease similarity prediction. These results
underscore its potential for enabling downstream applications in rare disease
research and precision medicine.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [122] [Smaller Circuits for Bit Addition](https://arxiv.org/abs/2509.13966)
*Mikhail Goncharov,Alexander S. Kulikov,Georgie Levtsov*

Main category: cs.CC

TL;DR: 本文研究二进制加法电路的面积优化，通过改进电路结构将上界从5n-3m降低到4.5n-2m，在输出位数m远小于输入位数n时实现10%的面积改进。


<details>
  <summary>Details</summary>
Motivation: 位加法在数字电路中无处不在，虽然已有大量研究关注电路深度优化，但在全二进制基下的电路面积优化研究相对不足。传统基于半加器和全加器的电路在许多场景下面积并非最优。

Method: 提出新的电路构造方法，替代传统的半加器和全加器结构，通过理论分析证明新方法在面积上的优势。同时开发开源实现生成器，可快速生成和比较不同加法电路设计。

Result: 成功将加法电路的面积上界从5n-3m改进到4.5n-2m，在m远小于n的场景下（如n位求和或n位整数乘法）实现10%的面积减少。

Conclusion: 新方法在加法电路面积优化方面显著优于传统设计，特别是在输出位数较少的应用场景中。开源实现便于实际应用和进一步研究。

Abstract: Bit addition arises virtually everywhere in digital circuits: arithmetic
operations, increment/decrement operators, computing addresses and table
indices, and so on. Since bit addition is such a basic task in Boolean circuit
synthesis, a lot of research has been done on constructing efficient circuits
for various special cases of it. A vast majority of these results are devoted
to optimizing the circuit depth (also known as delay).
  In this paper, we investigate the circuit size (also known as area) over the
full binary basis of bit addition. Though most of the known circuits are built
from Half Adders and Full Adders, we show that, in many interesting scenarios,
these circuits have suboptimal size. Namely, we improve an upper bound $5n-3m$
to $4.5n-2m$, where $n$ is the number of input bits and $m$ is the number of
output bits. In the regimes where $m$ is small compared to $n$ (for example,
for computing the sum of $n$ bits or multiplying two $n$-bit integers), this
leads to $10\%$ improvement.
  We complement our theoretical result by an open-source implementation of
generators producing circuits for bit addition and multiplication. The
generators allow one to produce the corresponding circuits in two lines of code
and to compare them to existing designs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [123] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: 提出了TICL方法，通过文本嵌入KNN选择语义相关的上下文示例，显著提升多模态模型的语音识别性能，无需微调即可在多个挑战性任务上实现高达84.7%的相对WER降低。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型已展现出上下文学习能力，但有效的上下文示例选择方法尚未得到充分探索，这限制了语音上下文学习的性能。

Method: 提出Text-Embedding KNN for SICL (TICL)管道，利用语义上下文选择相关示例，增强现有多模态模型的语音识别能力，无需进行模型微调。

Result: 在重口音英语、多语言语音和儿童语音等挑战性ASR任务中，该方法使模型超越零样本性能，相对WER降低最高达84.7%。消融研究证明了方法的鲁棒性和效率。

Conclusion: TICL是一种简单有效的上下文示例选择方法，能够显著提升语音基础模型的上下文学习性能，在多种实际应用场景中展现出强大潜力。

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


### [124] [Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection](https://arxiv.org/abs/2509.13878)
*Janne Laakkonen,Ivan Kukanov,Ville Hautamäki*

Main category: eess.AS

TL;DR: 提出了一种基于LoRA专家混合的方法来提升音频深度伪造检测的泛化能力，通过多个低秩适配器和路由机制来适应新型伪造方法，在域内和域外场景下均优于标准微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型如Wav2Vec2在音频深度伪造检测中表现良好，但在面对训练时未见过的新型伪造方法时泛化能力不足，需要一种能够适应不断演变的深度伪造攻击的方法。

Method: 采用混合LoRA专家方法，在模型的注意力层中集成多个低秩适配器(LoRA)，通过路由机制选择性地激活专门化的专家，增强对演化中深度伪造攻击的适应性。

Result: 实验结果表明该方法在域内和域外场景下均优于标准微调，最佳MoE-LoRA模型将平均域外等错误率从8.55%降低到6.08%。

Conclusion: 该方法有效实现了可泛化的音频深度伪造检测，为解决新型伪造方法的检测挑战提供了有效解决方案。

Abstract: Foundation models such as Wav2Vec2 excel at representation learning in speech
tasks, including audio deepfake detection. However, after being fine-tuned on a
fixed set of bonafide and spoofed audio clips, they often fail to generalize to
novel deepfake methods not represented in training. To address this, we propose
a mixture-of-LoRA-experts approach that integrates multiple low-rank adapters
(LoRA) into the model's attention layers. A routing mechanism selectively
activates specialized experts, enhancing adaptability to evolving deepfake
attacks. Experimental results show that our method outperforms standard
fine-tuning in both in-domain and out-of-domain scenarios, reducing equal error
rates relative to baseline models. Notably, our best MoE-LoRA model lowers the
average out-of-domain EER from 8.55\% to 6.08\%, demonstrating its
effectiveness in achieving generalizable audio deepfake detection.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [125] [Improving cosmological reach of a gravitational wave observatory using Deep Loop Shaping](https://arxiv.org/abs/2509.14016)
*Jonas Buchli,Brendan Tracey,Tomislav Andric,Christopher Wipf,Yu Him Justin Chiu,Matthias Lochbrunner,Craig Donner,Rana X. Adhikari,Jan Harms,Iain Barr,Roland Hafner,Andrea Huber,Abbas Abdolmaleki,Charlie Beattie,Joseph Betzwieser,Serkan Cabi,Jonas Degrave,Yuzhu Dong,Leslie Fritz,Anchal Gupta,Oliver Groth,Sandy Huang,Tamara Norman,Hannah Openshaw,Jameson Rollins,Greg Thornton,George Van Den Driessche,Markus Wulfmeier,Pushmeet Kohli,Martin Riedmiller,LIGO Instrument Team*

Main category: astro-ph.IM

TL;DR: 使用强化学习方法Deep Loop Shaping消除引力波观测站控制噪声，在LIGO Livingston天文台实现10-30Hz频段控制噪声降低30倍以上


<details>
  <summary>Details</summary>
Motivation: 提高引力波观测站的低频灵敏度可以研究中质量黑洞合并、双黑洞偏心率，并为双中子星合并的多信使观测提供早期预警，但当前镜面稳定控制注入有害噪声阻碍了灵敏度提升

Method: Deep Loop Shaping方法，使用频域奖励的强化学习技术来消除控制噪声

Result: 在LIGO Livingston天文台，控制器在10-30Hz频段将控制噪声降低了30倍以上，在子频段最高降低100倍，超过了量子极限的设计目标

Conclusion: Deep Loop Shaping有潜力改进当前和未来的引力波观测站，以及更广泛的仪器和控制系统

Abstract: Improved low-frequency sensitivity of gravitational wave observatories would
unlock study of intermediate-mass black hole mergers, binary black hole
eccentricity, and provide early warnings for multi-messenger observations of
binary neutron star mergers. Today's mirror stabilization control injects
harmful noise, constituting a major obstacle to sensitivity improvements. We
eliminated this noise through Deep Loop Shaping, a reinforcement learning
method using frequency domain rewards. We proved our methodology on the LIGO
Livingston Observatory (LLO). Our controller reduced control noise in the
10--30Hz band by over 30x, and up to 100x in sub-bands surpassing the design
goal motivated by the quantum limit. These results highlight the potential of
Deep Loop Shaping to improve current and future GW observatories, and more
broadly instrumentation and control systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [126] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 本文提出了一种基于联邦学习的分布式方法，用于从卫星图像中识别和定位森林砍伐，在保护数据隐私的同时实现多客户端协作训练。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练方法需要合并数据，会损害客户端数据安全。联邦学习能够在分布式网络客户端协作训练模型的同时，保持活跃用户的数据隐私和安全。

Method: 使用FLOWER框架与RAY框架执行分布式学习工作负载，采用YOLOS-small（Vision Transformer变体）、基于ResNet50的Faster R-CNN和基于MobileNetV3的Faster R-CNN模型，在公开数据集上进行训练和测试。

Result: 该方法为卫星图像的图像分割任务提供了新的视角，能够有效识别和定位森林砍伐区域。

Conclusion: 联邦学习框架为卫星图像分析提供了一种安全高效的分布式解决方案，在保护数据隐私的同时实现了准确的森林砍伐识别。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [127] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 提出基于证据检索的不确定性感知决策机制，用实例自适应的证据条件阈值替代全局固定阈值，通过Dempster-Shafer理论融合近邻样本预测分布，实现更可靠、可解释的决策


<details>
  <summary>Details</summary>
Motivation: 传统基于预测熵的全局阈值方法在不确定性感知决策中存在置信错误较多、缺乏可解释性等问题，需要一种更透明、可审计的替代方案

Method: 为每个测试实例在嵌入空间中检索近邻样本，使用Dempster-Shafer理论融合这些证据样本的预测分布，生成实例自适应的阈值标准

Result: 在CIFAR-10/100数据集上，相比预测熵阈值方法，取得了相当或更好的不确定性感知性能，显著减少了置信错误，且只需少量证据即可实现这些改进

Conclusion: 证据条件标记为操作不确定性感知决策提供了比固定预测熵阈值更可靠和可解释的替代方案，支持透明和可审计的决策过程

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [128] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 混合量子-经典神经网络在准确性、训练效率和参数可扩展性方面优于纯经典模型，特别是在复杂视觉任务中表现突出


<details>
  <summary>Details</summary>
Motivation: 系统比较混合量子-经典神经网络与纯经典模型在性能、效率和鲁棒性方面的差异，评估量子计算在深度学习中的实际价值

Method: 在三个基准数据集（MNIST、CIFAR100、STL10）上对比混合模型（参数化量子电路+经典深度学习架构）与经典CNN模型，进行50个训练周期的实验，评估验证准确率、测试准确率、训练时间、计算资源使用和对抗鲁棒性

Result: 混合模型在所有数据集上准确率更高（MNIST: 99.38% vs 98.21%, CIFAR100: 41.69% vs 32.25%, STL10: 74.05% vs 63.76%），训练速度快5-12倍，参数减少6-32%，内存和CPU使用更低，在简单数据集上对抗鲁棒性显著更好

Conclusion: 混合量子-经典架构在准确性、训练效率和参数可扩展性方面具有明显优势，特别适用于复杂视觉任务，为量子计算在深度学习中的应用提供了有力证据

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [129] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 本文提出了一种基于截面曲率的几何分析方法，用于构建离散度量空间的曲率轮廓，并以此评估数据表示效果和估计数据集的内在维度。


<details>
  <summary>Details</summary>
Motivation: 利用新发展的截面曲率抽象概念，构建离散度量空间的几何特征描述，为评估数据表示效果和维度约简技术提供量化工具。

Method: 基于捕获三点与其他点之间度量关系的曲率概念，构建曲率轮廓分析方法，用于分析经验网络的大规模几何特征。

Result: 实验表明该方法能有效估计数据集的内在维度，并评估维度约简技术的效果。

Conclusion: 曲率分析方法为离散度量空间的几何特征描述和数据表示效果评估提供了有效的量化手段。

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [130] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: EdiVal-Agent是一个自动化的多轮指令图像编辑评估框架，通过对象中心视角和专家工具套件提供细粒度评估，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前指令图像编辑评估要么依赖配对参考图像（覆盖有限且存在偏见），要么仅使用零样本视觉语言模型（评估不精确），需要更可靠和可解释的评估方法。

Method: 首先将图像分解为语义对象，合成多样化的上下文感知编辑指令，然后结合VLM和开放词汇对象检测器评估指令遵循，使用语义级特征提取器评估内容一致性，利用人类偏好模型判断视觉质量。

Result: 结合VLM和对象检测器在指令遵循评估中比单独使用VLM和CLIP指标与人类判断更一致，模块化设计支持未来工具集成以持续提升评估准确性。

Conclusion: EdiVal-Agent框架能够识别现有编辑模型的失败模式，为下一代编辑模型的发展提供信息，并构建了覆盖9种指令类型和11种最先进编辑模型的EdiVal-Bench基准。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [131] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything是一个统一的基于transformer的前馈模型，能够处理单张或多张图像及可选几何输入，直接回归度量3D场景几何和相机参数，在多种3D视觉任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统3D重建方法需要专门模型处理不同任务的问题，研究者希望开发一个统一的模型来处理多种3D视觉任务，包括未标定结构运动、多视图立体视觉、单目深度估计等。

Method: 采用基于transformer的前馈架构，输入图像和可选几何信息，使用分解的多视图场景几何表示（深度图、局部射线图、相机位姿和度量尺度因子），通过标准化监督和灵活输入增强进行训练。

Result: 实验表明MapAnything在多个3D视觉任务上优于或匹配专门的专家模型，同时提供更高效的联合训练性能。

Conclusion: MapAnything为通用3D重建骨干网络的发展铺平了道路，展示了统一模型在多样化3D视觉任务中的强大潜力。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [132] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap是一个模型无关的框架，用于发现稳定扩散模型中的潜在概念级表征偏见，通过交叉注意力归因图揭示人口统计特征与语义概念的结构性纠缠，并提出基于能量引导扩散采样的偏见缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏见发现方法主要关注输出层面的人口统计分布，无法保证偏见缓解后概念表征的解耦。需要更深入地探索生成过程中的表征偏见。

Method: 利用交叉注意力归因图分析人口统计特征与语义概念的空间纠缠，通过IoU量化概念耦合程度，并采用能量引导扩散采样在去噪过程中最小化SoftIoU来缓解偏见。

Result: 研究发现现有公平性干预措施可能减少输出分布差距，但往往无法解耦概念级耦合，而BiasMap方法能够在图像生成中缓解概念纠缠并补充分布偏见缓解。

Conclusion: BiasMap提供了一个新的视角来发现和缓解生成模型中的概念级表征偏见，相比现有方法能够更深入地揭示和处理偏见问题。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [133] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter是一个基于扩散模型的深度估计方法，能够从单目结肠镜视频生成时间一致的深度图，在C3VD数据集上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜中的3D场景理解面临重大挑战，现有深度估计模型在视频序列中缺乏时间一致性，限制了其在3D重建中的应用。

Method: 使用基于扩散模型的深度估计方法，从合成结肠镜序列学习鲁棒的几何先验来生成时间一致的深度图，并引入风格迁移技术将真实临床视频适配到合成训练域。

Result: 在C3VD数据集上实现了最先进的零样本性能，超越了通用和结肠镜专用方法，能够生成3D点云和进行表面覆盖评估。

Conclusion: 虽然完整的轨迹3D重建仍然具有挑战性，但ColonCrafter展示了在临床相关应用中的潜力，包括3D点云生成和表面覆盖评估。

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [134] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 本文提出了一种新的轨迹级OOD检测框架，通过建模预测误差的模式依赖性来提升自动驾驶车辆在复杂驾驶环境中的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在部署时面临训练数据与现实条件之间的分布偏移问题，现有研究主要关注计算机视觉任务的OOD检测，而轨迹级别的OOD检测研究相对不足。

Method: 基于快速变化检测(QCD)任务，引入自适应机制，显式建模预测误差的模式依赖性分布，这些分布随时间演变并具有数据集特定的动态特性。

Result: 在多个真实世界数据集上的实验表明，该方法在检测延迟和误报率方面都有显著提升，在准确性和计算效率上均优于现有的UQ和基于视觉的OOD方法。

Conclusion: 该框架为实现可靠、驾驶感知的自主性提供了一条实用路径，能够有效处理复杂驾驶环境中的分布偏移问题。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [135] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 本文挑战了表示学习中无关联视图足以学习有意义表示的假设，提出显式对齐不同视图表示的方法来提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法隐含假设数据点的无关联视图足以学习有意义的表示，但作者发现潜在空间的有意义结构不会自然出现，需要显式诱导。

Method: 提出一致视图对齐方法，将数据不同视图的表示对齐以整合互补信息，同时避免产生假阳性。

Result: 在MICCAI 2025 SSL3D挑战赛中获得第一名和第二名，使用Primus vision transformer和ResEnc卷积神经网络分别取得优异表现。

Conclusion: 结构化视图对齐在学习有效表示中起着关键作用，提出的自监督学习方法能显著提升下游任务性能。

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [136] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecDiff是一种训练自由的多级特征缓存策略，通过自推测引入未来信息，结合历史信息实现动态特征选择和分类，在保持质量的同时显著加速扩散模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅依赖历史信息，导致精度和速度性能受限。需要引入未来信息来突破速度-精度权衡瓶颈。

Method: 提出自推测范式，基于不同迭代次数下相同时步的信息相似性引入未来信息。包括基于自推测信息的缓存特征选择算法和基于特征重要性分数的多级特征分类算法。

Result: 在Stable Diffusion 3、3.5和FLUX上分别实现平均2.80×、2.74×和3.17×的加速，质量损失可忽略不计。

Conclusion: 通过融合推测信息和历史信息，SpecDiff突破了速度-精度权衡瓶颈，推动了高效扩散模型推理的帕累托前沿。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [137] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss是一种基于高斯溅射辐射光栅化的X射线层析成像重建算法，通过专门的检测器到世界坐标变换模型和初始化策略，在稀疏视图条件下实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 传统CT在板状结构检测中存在几何约束问题，而层析成像在稀疏视图条件下的高质量重建仍然具有挑战性。

Method: 结合高斯溅射辐射光栅化和包含层析倾斜角的专用检测器到世界坐标变换模型，采用初始化策略过滤层析伪影，防止高斯粒子分配到错误结构。

Result: 在合成和真实数据集上验证了方法的有效性，仅使用3%的全视图就能达到优于在全数据集上优化的迭代方法的性能。

Conclusion: LamiGauss能够直接从稀疏投影中有效优化，在有限数据条件下实现准确高效的重建，优于现有技术。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [138] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA是一种知识蒸馏方法，将大型视觉-语言教师模型的多模态语义知识转移到轻量级视觉目标检测学生模型中，通过对象级别的对齐实现高效语义迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注密集或全局对齐，而MOCHA专注于对象级别的多模态语义知识蒸馏，旨在在不修改教师模型且无需推理时文本输入的情况下，实现高效的语义知识转移。

Method: 使用翻译模块将学生特征映射到联合空间，通过双目标损失函数（局部对齐和全局关系一致性）指导学生和翻译模块的训练，实现对象级别的跨架构对齐。

Result: 在四个个性化检测基准测试中，平均得分提升+10.1，尽管架构紧凑，但性能可与更大的多模态模型相媲美。

Conclusion: MOCHA证明了在少样本情况下，通过对象级别的知识蒸馏可以有效提升轻量级检测器的性能，适合实际部署应用。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [139] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出了Dense Video Understanding (DVU)框架和Gated Residual Tokenization (GRT)方法，通过运动补偿和语义场景融合技术，实现高效的高帧率视频理解，解决了传统方法在密集时间信息处理上的计算冗余问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型和基准测试主要依赖低帧率采样，丢弃了密集的时间信息，无法处理需要精确时间对齐的任务（如讲座理解），且存在计算冗余和token线性增长的问题。

Method: 提出了GRT两阶段框架：1) 运动补偿门控tokenization，利用像素级运动估计跳过静态区域；2) 语义场景内tokenization合并，融合静态区域内的token，在保留动态语义的同时减少冗余。

Result: 在DIVE基准测试上，GRT超越了更大的VLLM基线模型，并且随着帧率的增加表现出积极的扩展性。

Conclusion: 密集时间信息对视频理解至关重要，GRT方法能够实现高效、可扩展的高帧率视频理解，为密集时间推理任务提供了有效的解决方案。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [140] [Secure, Scalable and Privacy Aware Data Strategy in Cloud](https://arxiv.org/abs/2509.13627)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.CR

TL;DR: 本文提出了一个有效的云上企业数据战略，解决大数据处理、存储的安全性和可扩展性挑战，支持快速数据驱动决策


<details>
  <summary>Details</summary>
Motivation: 企业面临处理海量数据的安全、可扩展存储挑战，需要支持决策者快速做出数据驱动的明智决策

Method: 讨论有效数据战略的各个组件，提供解决安全性、可扩展性和隐私问题的架构方案

Result: 开发了一套完整的云上企业数据战略框架，包含安全架构、可扩展性解决方案和隐私保护机制

Conclusion: 提出的云数据战略能够有效帮助企业应对大数据时代的挑战，实现安全、可扩展的数据管理和快速决策支持

Abstract: The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.

</details>


### [141] [A Survey and Evaluation Framework for Secure DNS Resolution](https://arxiv.org/abs/2509.13797)
*Ali Sadeghi Jahromi,AbdelRahman Abdou,Paul C. van Oorschot*

Main category: cs.CR

TL;DR: 本文对DNS安全方案进行系统分析，提出了14个安全属性评估框架，发现单一方案无法提供完整保护，建议组合使用不同阶段的互补方案来实现全面DNS安全。


<details>
  <summary>Details</summary>
Motivation: 由于DNS最初设计时未考虑安全性，现有各种安全增强方案要么试图完全替换DNS基础设施（未成功），要么在保持DNS两阶段结构基础上进行改进。需要系统评估这些方案的有效性和互补性。

Method: 建立全面的DNS威胁模型和攻击分类法，制定14个安全、隐私和可用性属性作为评估标准，对12种安全DNS方案进行客观比较分析。

Result: 评估显示没有任何单一方案能在整个解析路径上提供理想保护，不同方案倾向于针对特定阶段的属性子集。针对不同阶段的方案具有互补性，可以协同工作。

Conclusion: 组合使用兼容的DNS安全方案是实现DNS解析过程全面安全的实用有效方法，不同阶段的方案可以相互补充形成整体保护。

Abstract: Since security was not among the original design goals of the Domain Name
System (herein called Vanilla DNS), many secure DNS schemes have been proposed
to enhance the security and privacy of the DNS resolution process. Some
proposed schemes aim to replace the existing DNS infrastructure entirely, but
none have succeeded in doing so. In parallel, numerous schemes focus on
improving DNS security without modifying its fundamental two-stage structure.
These efforts highlight the feasibility of addressing DNS security as two
distinct but compatible stages. We survey DNS resolution process attacks and
threats and develop a comprehensive threat model and attack taxonomy for their
systematic categorization. This analysis results in the formulation of 14
desirable security, privacy, and availability properties to mitigate the
identified threats. Using these properties, we develop an objective evaluation
framework and apply it to comparatively analyze 12 secure DNS schemes surveyed
in this work that aim to augment the properties of the DNS resolution process.
Our evaluation reveals that no single scheme provides ideal protection across
the entire resolution path. Instead, the schemes tend to address a subset of
properties specific to individual stages. Since these schemes targeting
different stages of DNS resolution are complementary and can operate together,
combining compatible schemes offers a practical and effective approach to
achieving comprehensive security in the DNS resolution process.

</details>


### [142] [Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.13772)
*Baolei Zhang,Haoran Xin,Yuxi Chen,Zhuqing Liu,Biao Yi,Tong Li,Lihai Nie,Zheli Liu,Minghong Fang*

Main category: cs.CR

TL;DR: RAGOrigin是一个黑盒责任溯源框架，用于识别RAG系统中导致错误生成的污染文本，通过检索排名、语义相关性和生成影响来分配责任分数，并在多种攻击场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: RAG系统容易受到投毒攻击，现有防御措施容易被更复杂的攻击规避，需要一种有效的方法来溯源污染知识。

Method: 构建针对每个错误生成事件的专注溯源范围，通过评估检索排名、语义相关性和对生成响应的影响来分配责任分数，使用无监督聚类方法隔离污染文本。

Result: 在7个数据集和15种投毒攻击（包括新开发的适应性投毒策略和多攻击者场景）上评估，RAGOrigin在识别污染内容方面优于现有基线，并在动态和嘈杂条件下保持鲁棒性。

Conclusion: RAGOrigin为追踪RAG系统中污染知识的来源提供了实用有效的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge into large
language models to improve response quality. However, recent work has shown
that RAG systems are highly vulnerable to poisoning attacks, where malicious
texts are inserted into the knowledge database to influence model outputs.
While several defenses have been proposed, they are often circumvented by more
adaptive or sophisticated attacks.
  This paper presents RAGOrigin, a black-box responsibility attribution
framework designed to identify which texts in the knowledge database are
responsible for misleading or incorrect generations. Our method constructs a
focused attribution scope tailored to each misgeneration event and assigns a
responsibility score to each candidate text by evaluating its retrieval
ranking, semantic relevance, and influence on the generated response. The
system then isolates poisoned texts using an unsupervised clustering method. We
evaluate RAGOrigin across seven datasets and fifteen poisoning attacks,
including newly developed adaptive poisoning strategies and multi-attacker
scenarios. Our approach outperforms existing baselines in identifying poisoned
content and remains robust under dynamic and noisy conditions. These results
suggest that RAGOrigin provides a practical and effective solution for tracing
the origins of corrupted knowledge in RAG systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [143] [Dual Actor DDPG for Airborne STAR-RIS Assisted Communications](https://arxiv.org/abs/2509.13328)
*Danish Rizvi,David Boyle*

Main category: eess.SP

TL;DR: 本研究提出了一种基于无人机搭载的STAR-RIS（Aerial-STAR）系统，采用耦合TRC相位偏移模型，通过联合优化无人机轨迹、基站波束成形和RIS参数来提升通信效率，并设计了新型DA-DDPG算法处理高维混合动作空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设TRC独立，但实际中传输和反射系数存在耦合关系。本研究旨在探索这种耦合特性，并利用无人机机动性来提升多用户下行通信系统的性能。

Method: 提出Aerial-STAR系统，采用耦合TRC相位偏移模型；设计TRC为离散和连续动作的组合；提出DA-DDPG算法（双actor网络处理高维混合动作空间）；设计基于调和平均指数（HFI）的奖励函数保证用户公平性。

Result: DA-DDPG算法比传统DDPG和DQN分别提升24%和97%的累积奖励；三维轨迹优化比二维和高度优化提升28%通信效率；HFI奖励函数降低41%的QoS拒绝率；移动Aerial-STAR系统优于固定部署方案。

Conclusion: Aerial-STAR系统具有巨大潜力，提出的DA-DDPG方法能有效优化其性能，耦合相位STAR-RIS优于传统RIS配置，为未来空中智能反射面系统提供了有效解决方案。

Abstract: This study departs from the prevailing assumption of independent Transmission
and Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect
Reconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a
novel multi-user downlink communication system that leverages a UAV-mounted
STAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key
contributions include the joint optimization of UAV trajectory, active
beamforming vectors at the base station, and passive RIS TRCs to enhance
communication efficiency, while considering UAV energy constraints. We design
the TRC as a combination of discrete and continuous actions, and propose a
novel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The
algorithm relies on two separate actor networks for high-dimensional hybrid
action space. We also propose a novel harmonic mean index (HFI)-based reward
function to ensure communication fairness amongst users. For comprehensive
analysis, we study the impact of RIS size on UAV aerodynamics showing that it
increases drag and energy demand. Simulation results demonstrate that the
proposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based
solutions by 24% and 97%, respectively, in accumulated reward.
Three-dimensional UAV trajectory optimization achieves 28% higher communication
efficiency compared to two-dimensional and altitude optimization. The HFI based
reward function provides 41% lower QoS denial rates as compared to other
benchmarks. The mobile Aerial-STAR system shows superior performance over fixed
deployed counterparts, with the coupled phase STAR-RIS outperforming dual
Transmit/Reflect RIS and conventional RIS setups. These findings highlight the
potential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG
approach in optimizing their performance.

</details>


### [144] [Domino: Dominant Path-based Compensation for Hardware Impairments in Modern WiFi Sensing](https://arxiv.org/abs/2509.13807)
*Ruiqi Kong,He Chen*

Main category: eess.SP

TL;DR: Domino是一个新的WiFi感知框架，通过将CSI转换为CIR并利用延迟域处理来精确补偿硬件引起的RF失真，在呼吸监测实验中比现有方法准确度提高至少2倍


<details>
  <summary>Details</summary>
Motivation: 现代WiFi卡（支持802.11ac/ax协议）的自动增益控制和独立RF链引入了复杂动态的RF失真，使现有补偿方法失效，严重影响WiFi感知的可靠性

Method: 将信道状态信息(CSI)转换为信道脉冲响应(CIR)，利用硬件失真对所有信号路径影响均匀的特性，以主导静态路径作为参考，通过延迟域处理进行有效补偿

Result: 真实呼吸监测实验显示，Domino比现有方法准确度至少提高2倍，中位误差低于0.24 bpm，即使在单天线、直视和非直视场景下也能保持稳健性能

Conclusion: Domino框架通过延迟域处理和主导静态路径参考，有效解决了现代WiFi卡硬件引起的RF失真问题，显著提升了WiFi感知的可靠性和准确性

Abstract: WiFi sensing faces a critical reliability challenge due to hardware-induced
RF distortions, especially with modern, market-dominant WiFi cards supporting
802.11ac/ax protocols. These cards employ sensitive automatic gain control and
separate RF chains, introducing complex and dynamic distortions that render
existing compensation methods ineffective. In this paper, we introduce Domino,
a new framework that transforms channel state information (CSI) into channel
impulse response (CIR) and leverages it for precise distortion compensation.
Domino is built on the key insight that hardware-induced distortions impact all
signal paths uniformly, allowing the dominant static path to serve as a
reliable reference for effective compensation through delay-domain processing.
Real-world respiration monitoring experiments show that Domino achieves at
least 2x higher mean accuracy over existing methods, maintaining robust
performance with a median error below 0.24 bpm, even using a single antenna in
both direct line-of-sight and obstructed scenarios.

</details>


### [145] [Active Inference Framework for Closed-Loop Sensing, Communication, and Control in UAV Systems](https://arxiv.org/abs/2509.14201)
*Guangjin Pan,Liping Bai,Zhuojun Tian,Hui Chen,Mehdi Bennis,Henk Wymeersch*

Main category: eess.SP

TL;DR: 将主动推理框架(AIF)引入支持SCC的无人机系统，实现联合状态估计、控制和感知资源分配，相比基线方法降低了控制和感知成本


<details>
  <summary>Details</summary>
Motivation: 现有SCC解决方案通常将感知和控制分开处理，导致性能次优和资源使用效率低下，需要统一的框架来优化集成感知通信系统

Method: 通过构建统一的生成模型，将问题转化为最小化变分自由能进行推理和最小化期望自由能进行行动规划

Result: 仿真结果显示，相对于基线方法，控制成本和感知成本均有所降低

Conclusion: 主动推理框架为SCC-enabled无人机系统提供了一种有效的联合优化方法，能够同时改善状态估计、控制和资源分配性能

Abstract: Integrated sensing and communication (ISAC) is a core technology for 6G, and
its application to closed-loop sensing, communication, and control (SCC)
enables various services. Existing SCC solutions often treat sensing and
control separately, leading to suboptimal performance and resource usage. In
this work, we introduce the active inference framework (AIF) into SCC-enabled
unmanned aerial vehicle (UAV) systems for joint state estimation, control, and
sensing resource allocation. By formulating a unified generative model, the
problem reduces to minimizing variational free energy for inference and
expected free energy for action planning. Simulation results show that both
control cost and sensing cost are reduced relative to baselines.

</details>


### [146] [Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device](https://arxiv.org/abs/2509.12510)
*Wei Shao,Ruoyu Zhang,Zequan Liang,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出了首个完全无监督的腕部PPG信号质量评估管道，结合自监督学习和拓扑数据分析，实现跨设备的信号质量分类


<details>
  <summary>Details</summary>
Motivation: 可穿戴光电容积脉搏波(PPG)信号易受运动、灌注损失和环境光干扰，现有信号质量评估方法要么依赖脆弱的启发式规则，要么需要大量标注数据的监督模型

Method: 两阶段方法：第一阶段使用对比学习1-D ResNet-18在276小时未标注数据上训练，获得光学发射器和运动不变的嵌入表示；第二阶段通过持久同调将512维嵌入转换为4维拓扑特征，用HDBSCAN聚类，最密集簇代表可接受信号

Result: 无需重新调优，在10,000个窗口的分层样本上获得Silhouette得分0.72、Davies-Bouldin得分0.34、Calinski-Harabasz得分6173

Conclusion: 提出的SSL-TDA混合框架为PPG信号提供了一个即插即用、可扩展、跨设备的质量门控解决方案

Abstract: Wearable photoplethysmography (PPG) is embedded in billions of devices, yet
its optical waveform is easily corrupted by motion, perfusion loss, and ambient
light, jeopardizing downstream cardiometric analytics. Existing signal-quality
assessment (SQA) methods rely either on brittle heuristics or on data-hungry
supervised models. We introduce the first fully unsupervised SQA pipeline for
wrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,
unlabeled data from heterogeneous sources (varying in device and sampling
frequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,
the learned representation is stable across differences in LED wavelength,
drive intensity, and device optics, as well as wrist motion). Stage 2 converts
each 512-D encoder embedding into a 4-D topological signature via persistent
homology (PH) and clusters these signatures with HDBSCAN. To produce a binary
signal-quality index (SQI), the acceptable PPG signals are represented by the
densest cluster while the remaining clusters are assumed to mainly contain
poor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,
Davies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,
respectively, on a stratified sample of 10,000 windows. In this study, we
propose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)
framework that offers a drop-in, scalable, cross-device quality gate for PPG
signals.

</details>


### [147] [Classification Filtering](https://arxiv.org/abs/2509.13975)
*Ilker Bayram*

Main category: eess.SP

TL;DR: 提出一种用于流式信号分类的状态空间模型和滤波器，通过融合多个分类器的输出并利用时序信息来提高分类精度


<details>
  <summary>Details</summary>
Motivation: 在流式信号分类中，多个分类器以固定策略提供不同准确度的类别概率，需要融合这些输出并利用时序信息来提升分类准确性

Method: 设计状态空间模型并开发专门用于实时执行的自适应滤波器，融合多个分类器的概率输出

Result: 在基于可穿戴设备IMU数据的活动分类应用中验证了所提出滤波器的有效性

Conclusion: 提出的状态空间模型和滤波器能够有效利用时序信息融合多分类器输出，显著提升流式信号的分类精度

Abstract: We consider a streaming signal in which each sample is linked to a latent
class. We assume that multiple classifiers are available, each providing class
probabilities with varying degrees of accuracy. These classifiers are employed
following a straightforward and fixed policy. In this setting, we consider the
problem of fusing the output of the classifiers while incorporating the
temporal aspect to improve classification accuracy. We propose a state-space
model and develop a filter tailored for realtime execution. We demonstrate the
effectiveness of the proposed filter in an activity classification application
based on inertial measurement unit (IMU) data from a wearable device.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [148] [Unleashing the power of computational insights in revealing the complexity of biological systems in the new era of spatial multi-omics](https://arxiv.org/abs/2509.13376)
*Zhiwei Fan,Tiangang Wang,Kexin Huang,Binwu Ying,Xiaobo Zhou*

Main category: q-bio.QM

TL;DR: 空间组学技术的最新进展使生物系统研究达到前所未有的分辨率，通过保留分子测量的空间背景，能够全面绘制细胞异质性、组织结构和动态生物过程。


<details>
  <summary>Details</summary>
Motivation: 系统概述空间组学技术和计算算法的持续进展，推动对哺乳动物组织和器官结构与机制的更深入系统理解。

Method: 采用先进的机器学习算法和多组学整合建模方法，解码复杂生物过程，包括器官发育过程中的细胞空间组织和拓扑关系，以及肿瘤发生和转移的关键分子特征和调控网络。

Result: 空间组学技术为发育生物学、神经科学、肿瘤学和进化研究提供了革命性的研究工具，能够全面解析生物系统的空间组织结构。

Conclusion: 空间组学在精准医学领域具有广阔的应用前景，未来需要在技术创新和建模方法上继续发展。

Abstract: Recent advances in spatial omics technologies have revolutionized our ability
to study biological systems with unprecedented resolution. By preserving the
spatial context of molecular measurements, these methods enable comprehensive
mapping of cellular heterogeneity, tissue architecture, and dynamic biological
processes in developmental biology, neuroscience, oncology, and evolutionary
studies. This review highlights a systematic overview of the continuous
advancements in both technology and computational algorithms that are paving
the way for a deeper, more systematic comprehension of the structure and
mechanisms of mammalian tissues and organs by using spatial multi-omics. Our
viewpoint demonstrates how advanced machine learning algorithms and multi-omics
integrative modeling can decode complex biological processes, including the
spatial organization and topological relationships of cells during organ
development, as well as key molecular signatures and regulatory networks
underlying tumorigenesis and metastasis. Finally, we outline future directions
for technological innovation and modeling insights of spatial omics in
precision medicine.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [149] [Efficient Last-Iterate Convergence in Regret Minimization via Adaptive Reward Transformation](https://arxiv.org/abs/2509.13653)
*Hang Ren,Yulin Wu,Shuhan Qi,Jiajia Zhang,Xiaozhen Sun,Tianzi Ma,Xuan Wang*

Main category: cs.GT

TL;DR: 提出自适应奖励变换方法，解决传统RT框架参数敏感问题，在NFGs和EFGs中实现更好的最后迭代收敛和线性收敛速度


<details>
  <summary>Details</summary>
Motivation: 传统遗憾最小化方法只能保证平均策略收敛，计算成本高；奖励变换(RT)框架虽然能实现最后迭代收敛，但对人工调参敏感，实际性能与理论保证不一致

Method: 提出自适应技术，动态调整RT框架参数，平衡探索与利用，改进遗憾积累机制，应用于RTRM和RTCFR等算法

Result: 实验结果表明自适应方法显著加速收敛速度，优于现有最优算法，实现更好的渐近最后迭代收敛和线性收敛

Conclusion: 自适应奖励变换方法有效解决了RT框架的实践挑战，在理论和实际性能间取得更好一致性，为NFGs和EFGs求解提供了更有效的工具

Abstract: Regret minimization is a powerful method for finding Nash equilibria in
Normal-Form Games (NFGs) and Extensive-Form Games (EFGs), but it typically
guarantees convergence only for the average strategy. However, computing the
average strategy requires significant computational resources or introduces
additional errors, limiting its practical applicability. The Reward
Transformation (RT) framework was introduced to regret minimization to achieve
last-iterate convergence through reward function regularization. However, it
faces practical challenges: its performance is highly sensitive to manually
tuned parameters, which often deviate from theoretical convergence conditions,
leading to slow convergence, oscillations, or stagnation in local optima.
  Inspired by previous work, we propose an adaptive technique to address these
issues, ensuring better consistency between theoretical guarantees and
practical performance for RT Regret Matching (RTRM), RT Counterfactual Regret
Minimization (RTCFR), and their variants in solving NFGs and EFGs more
effectively. Our adaptive methods dynamically adjust parameters, balancing
exploration and exploitation while improving regret accumulation, ultimately
enhancing asymptotic last-iterate convergence and achieving linear convergence.
Experimental results demonstrate that our methods significantly accelerate
convergence, outperforming state-of-the-art algorithms.

</details>


### [150] [Nash Equilibria in Games with Playerwise Concave Coupling Constraints: Existence and Computation](https://arxiv.org/abs/2509.14032)
*Philip Jordan,Maryam Kamgarpour*

Main category: cs.GT

TL;DR: 该论文研究具有共享耦合约束的连续静态博弈中纳什均衡的存在性和计算问题，提出了在玩家级凹约束条件下的存在性证明和基于势函数的梯度计算方法。


<details>
  <summary>Details</summary>
Motivation: 传统纳什均衡存在性理论依赖于联合凸性等强假设，无法处理玩家级凹约束的博弈场景，需要开发更弱条件下的存在性理论和有效计算方法。

Method: 利用拓扑不动点理论和可行集可收缩性结构分析证明存在性；对于计算，采用对数障碍正则化梯度上升法配合自适应步长，在势函数假设下求解近似纳什均衡。

Result: 在玩家级凹效用和约束条件下证明了纳什均衡的存在性；提出的计算方法从可行初始点出发，在精确梯度反馈下以O(ε⁻³)迭代次数收敛到ε-近似约束纳什均衡。

Conclusion: 该工作扩展了纳什均衡存在性理论到更一般的约束条件，并提供了有效的计算框架，为具有复杂约束的博弈问题提供了理论基础和算法解决方案。

Abstract: We study the existence and computation of Nash equilibria in continuous
static games where the players' admissible strategies are subject to shared
coupling constraints, i.e., constraints that depend on their \emph{joint}
strategies. Specifically, we focus on a class of games characterized by
playerwise concave utilities and playerwise concave constraints. Prior results
on the existence of Nash equilibria are not applicable to this class, as they
rely on strong assumptions such as joint convexity of the feasible set. By
leveraging topological fixed point theory and novel structural insights into
the contractibility of feasible sets under playerwise concave constraints, we
give an existence proof for Nash equilibria under weaker conditions. Having
established existence, we then focus on the computation of Nash equilibria via
independent gradient methods under the additional assumption that the utilities
admit a potential function. To account for the possibly nonconvex feasible
region, we employ a log barrier regularized gradient ascent with adaptive
stepsizes. Starting from an initial feasible strategy profile and under exact
gradient feedback, the proposed method converges to an $\epsilon$-approximate
constrained Nash equilibrium within $\mathcal{O}(\epsilon^{-3})$ iterations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [151] [A Closeness Centrality-based Circuit Partitioner for Quantum Simulations](https://arxiv.org/abs/2509.14098)
*Doru Thom Popovici,Harlin Lee,Mauro Del Ben,Naoki Yoshioka,Nobuyasu Ito,Katherine Klymko,Daan Camps,Anastasiia Butko*

Main category: quant-ph

TL;DR: 提出了一个端到端框架，用于高效分区大规模量子电路模拟，通过图论方法最小化节点间数据移动，生成可移植的优化代码。


<details>
  <summary>Details</summary>
Motivation: 量子电路模拟需要大量计算资源，现有方法在大型集群上存在数据移动效率低下的问题，需要更高效的分布式计算方案。

Method: 将量子态和电路分布建模为图问题，应用接近中心性评估门重要性，设计快速可扩展的分区方法，并生成高度优化的可移植代码。

Result: 框架能够在各种超级计算机上无缝运行，为量子算法模拟提供性能和可扩展性的关键洞察。

Conclusion: 该工作提供了一个高效的端到端解决方案，显著改善了大规模量子电路模拟的资源利用和数据移动效率。

Abstract: Simulating quantum circuits (QC) on high-performance computing (HPC) systems
has become an essential method to benchmark algorithms and probe the potential
of large-scale quantum computation despite the limitations of current quantum
hardware. However, these simulations often require large amounts of resources,
necessitating the use of large clusters with thousands of compute nodes and
large memory footprints. In this work, we introduce an end-to-end framework
that provides an efficient partitioning scheme for large-scale QCs alongside a
flexible code generator to offer a portable solution that minimizes data
movement between compute nodes. By formulating the distribution of quantum
states and circuits as a graph problem, we apply closeness centrality to assess
gate importance and design a fast, scalable partitioning method. The resulting
partitions are compiled into highly optimized codes that run seamlessly on a
wide range of supercomputers, providing critical insights into the performance
and scalability of quantum algorithm simulations.

</details>


### [152] [Learning quantum many-body data locally: A provably scalable framework](https://arxiv.org/abs/2509.13705)
*Koki Chinzei,Quoc Hoan Tran,Norifumi Matsumoto,Yasuhiro Endo,Hirotaka Oshima*

Main category: quant-ph

TL;DR: 提出了一个名为GLQK的可扩展机器学习框架，利用量子多体系统中普遍存在的关联衰减现象，显著提高了学习量子期望值多项式的样本效率，特别是在平移对称数据下实现了与量子比特数无关的常数样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 机器学习在从量子多体实验数据中提取洞察方面具有巨大潜力，但处理大规模问题需要大量数据，超出了近期量子设备的有限计算资源。需要开发更高效的框架来利用量子数据。

Method: 提出了几何局部量子核（GLQK）框架，通过在关联长度尺度上从局部量子信息构建特征空间，利用非临界系统中普遍存在的关联指数衰减现象来高效学习量子多体实验数据。

Result: 严格证明了GLQK在量子期望值多项式学习任务中，相比现有影子核方法显著改善了多项式样本复杂度，特别是在目标多项式的每个项涉及较少局部子系统时。对于平移对称数据，实现了与量子比特数无关的常数样本复杂度。数值实验在两个量子多体现象学习任务中展示了其高可扩展性。

Conclusion: GLQK框架为利用实验数据推进量子多体物理理解开辟了新途径，通过利用量子系统的局部性特征实现了高效学习，为量子机器学习在实验环境中的应用提供了重要进展。

Abstract: Machine learning (ML) holds great promise for extracting insights from
complex quantum many-body data obtained in quantum experiments. This approach
can efficiently solve certain quantum problems that are classically
intractable, suggesting potential advantages of harnessing quantum data.
However, addressing large-scale problems still requires significant amounts of
data beyond the limited computational resources of near-term quantum devices.
We propose a scalable ML framework called Geometrically Local Quantum Kernel
(GLQK), designed to efficiently learn quantum many-body experimental data by
leveraging the exponential decay of correlations, a phenomenon prevalent in
noncritical systems. In the task of learning an unknown polynomial of quantum
expectation values, we rigorously prove that GLQK substantially improves
polynomial sample complexity in the number of qubits $n$, compared to the
existing shadow kernel, by constructing a feature space from local quantum
information at the correlation length scale. This improvement is particularly
notable when each term of the target polynomial involves few local subsystems.
Remarkably, for translationally symmetric data, GLQK achieves constant sample
complexity, independent of $n$. We numerically demonstrate its high scalability
in two learning tasks on quantum many-body phenomena. These results establish
new avenues for utilizing experimental data to advance the understanding of
quantum many-body physics.

</details>


### [153] [Learning Minimal Representations of Many-Body Physics from Snapshots of a Quantum Simulator](https://arxiv.org/abs/2509.13821)
*Frederik Møller,Gabriel Fernández-Fernández,Thomas Schweigler,Paulin de Schoulepnikoff,Jörg Schmiedmayer,Gorka Muñoz-Gil*

Main category: quant-ph

TL;DR: 使用变分自编码器分析量子模拟器实验数据，从噪声测量中提取物理可解释变量，揭示平衡和非平衡动力学特征


<details>
  <summary>Details</summary>
Motivation: 量子模拟器实验数据常受测量噪声、有限可观测量和微观模型不完整性的影响，阻碍物理洞察的提取

Method: 基于变分自编码器的机器学习方法，无监督学习隧道耦合一维玻色气体的干涉测量数据

Result: VAE学习到与系统平衡控制参数强相关的最小潜在表示，发现快速冷却后冻结孤子特征和传统相关方法未捕捉的异常淬火后动力学

Conclusion: 生成模型可直接从噪声稀疏实验数据提取物理可解释变量，为量子多体系统提供可扩展的数据驱动发现途径

Abstract: Analog quantum simulators provide access to many-body dynamics beyond the
reach of classical computation. However, extracting physical insights from
experimental data is often hindered by measurement noise, limited observables,
and incomplete knowledge of the underlying microscopic model. Here, we develop
a machine learning approach based on a variational autoencoder (VAE) to analyze
interference measurements of tunnel-coupled one-dimensional Bose gases, which
realize the sine-Gordon quantum field theory. Trained in an unsupervised
manner, the VAE learns a minimal latent representation that strongly correlates
with the equilibrium control parameter of the system. Applied to
non-equilibrium protocols, the latent space uncovers signatures of frozen-in
solitons following rapid cooling, and reveals anomalous post-quench dynamics
not captured by conventional correlation-based methods. These results
demonstrate that generative models can extract physically interpretable
variables directly from noisy and sparse experimental data, providing
complementary probes of equilibrium and non-equilibrium physics in quantum
simulators. More broadly, our work highlights how machine learning can
supplement established field-theoretical techniques, paving the way for
scalable, data-driven discovery in quantum many-body systems.

</details>


### [154] [Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.14026)
*Jiun-Cheng Jiang,Morris Yu-Chao Huang,Tianlong Chen,Hsi-Sheng Goan*

Main category: quant-ph

TL;DR: 提出了量子变分激活函数(QVAFs)和DARUANs技术，将量子电路与KANs结合，创建了量子启发的KANs(QKANs)，在保持可解释性的同时提高了参数效率和泛化能力


<details>
  <summary>Details</summary>
Motivation: 结合变分量子电路(VQCs)和Kolmogorov-Arnold网络(KANs)的优势，利用可学习激活函数的力量，开发更高效的量子机器学习方法

Method: 通过单量子比特数据重上传电路(DARUANs)实现量子变分激活函数，嵌入到KANs中形成QKANs，并引入层扩展和混合QKANs(HQKANs)技术来增强可扩展性

Result: DARUANs具有指数增长的频率谱，相比基于傅里叶的激活函数实现了参数数量的指数减少而不损失表达能力。QKANs在函数回归、图像分类和自回归生成语言建模等任务中表现出高效性和可扩展性

Conclusion: DARUANs和QKANs为在NISQ硬件和经典量子模拟器上推进量子机器学习提供了一个有前景的方向，结合了量子计算和经典神经网络的优势

Abstract: Variational quantum circuits (VQCs) are central to quantum machine learning,
while recent progress in Kolmogorov-Arnold networks (KANs) highlights the power
of learnable activation functions. We unify these directions by introducing
quantum variational activation functions (QVAFs), realized through single-qubit
data re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We
show that DARUAN with trainable weights in data pre-processing possesses an
exponentially growing frequency spectrum with data repetitions, enabling an
exponential reduction in parameter size compared with Fourier-based activations
without loss of expressivity. Embedding DARUAN into KANs yields
quantum-inspired KANs (QKANs), which retain the interpretability of KANs while
improving their parameter efficiency, expressivity, and generalization. We
further introduce two novel techniques to enhance scalability, feasibility and
computational efficiency, such as layer extension and hybrid QKANs (HQKANs) as
drop-in replacements of multi-layer perceptrons (MLPs) for feed-forward
networks in large-scale models. We provide theoretical analysis and extensive
experiments on function regression, image classification, and autoregressive
generative language modeling, demonstrating the efficiency and scalability of
QKANs. DARUANs and QKANs offer a promising direction for advancing quantum
machine learning on both noisy intermediate-scale quantum (NISQ) hardware and
classical quantum simulators.

</details>


### [155] [Quantum Reinforcement Learning-Guided Diffusion Model for Image Synthesis via Hybrid Quantum-Classical Generative Model Architectures](https://arxiv.org/abs/2509.14163)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 使用量子强化学习控制器动态调整扩散模型中的分类器自由引导(CFG)参数，相比静态调度在保持感知质量的同时减少参数数量


<details>
  <summary>Details</summary>
Motivation: 传统的静态或启发式CFG调度无法适应不同时间步和噪声条件，需要更智能的动态调整方法

Method: 采用混合量子-经典actor-critic架构：变分量子电路生成策略特征，多层感知机映射为高斯动作，使用PPO和GAE优化策略

Result: 在CIFAR-10上实验显示，QRL策略在LPIPS、PSNR、SSIM等指标上提升感知质量，同时减少参数数量

Conclusion: 量子强化学习控制器能有效动态调整CFG参数，在准确性和效率间取得平衡，并在长扩散调度下保持鲁棒生成

Abstract: Diffusion models typically employ static or heuristic classifier-free
guidance (CFG) schedules, which often fail to adapt across timesteps and noise
conditions. In this work, we introduce a quantum reinforcement learning (QRL)
controller that dynamically adjusts CFG at each denoising step. The controller
adopts a hybrid quantum--classical actor--critic architecture: a shallow
variational quantum circuit (VQC) with ring entanglement generates policy
features, which are mapped by a compact multilayer perceptron (MLP) into
Gaussian actions over $\Delta$CFG, while a classical critic estimates value
functions. The policy is optimized using Proximal Policy Optimization (PPO)
with Generalized Advantage Estimation (GAE), guided by a reward that balances
classification confidence, perceptual improvement, and action regularization.
Experiments on CIFAR-10 demonstrate that our QRL policy improves perceptual
quality (LPIPS, PSNR, SSIM) while reducing parameter count compared to
classical RL actors and fixed schedules. Ablation studies on qubit number and
circuit depth reveal trade-offs between accuracy and efficiency, and extended
evaluations confirm robust generation under long diffusion schedules.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [156] [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](https://arxiv.org/abs/2509.14054)
*Weihao Yan,Christoph Brune,Mengwu Guo*

Main category: cs.CE

TL;DR: 提出了一种新颖的两阶段贝叶斯框架，结合物理驱动的深度核学习和哈密顿蒙特卡洛方法，用于从稀疏观测数据中稳健推断高维偏微分方程参数并量化不确定性


<details>
  <summary>Details</summary>
Motivation: 高维偏微分方程参数推断面临维度灾难和传统数值方法局限性的挑战，需要开发能够处理数据稀疏性和模型复杂性的稳健方法

Method: 两阶段方法：第一阶段使用物理驱动的深度核学习训练代理模型，获得优化的神经网络特征提取器和参数初始估计；第二阶段在固定神经网络权重的情况下，使用哈密顿蒙特卡洛采样核超参数和PDE参数的联合后验分布

Result: 在典型和高维逆PDE问题上的数值实验表明，该框架能够准确估计参数、提供可靠的不确定性估计，有效解决数据稀疏性和模型复杂性的挑战

Conclusion: 该框架为各种科学和工程应用提供了一个稳健且可扩展的工具，能够有效处理高维PDE参数推断问题

Abstract: Inferring parameters of high-dimensional partial differential equations
(PDEs) poses significant computational and inferential challenges, primarily
due to the curse of dimensionality and the inherent limitations of traditional
numerical methods. This paper introduces a novel two-stage Bayesian framework
that synergistically integrates training, physics-based deep kernel learning
(DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE
parameters and quantify their uncertainties from sparse, exact observations.
The first stage leverages physics-based DKL to train a surrogate model, which
jointly yields an optimized neural network feature extractor and robust initial
estimates for the PDE parameters. In the second stage, with the neural network
weights fixed, HMC is employed within a full Bayesian framework to efficiently
sample the joint posterior distribution of the kernel hyperparameters and the
PDE parameters. Numerical experiments on canonical and high-dimensional inverse
PDE problems demonstrate that our framework accurately estimates parameters,
provides reliable uncertainty estimates, and effectively addresses challenges
of data sparsity and model complexity, offering a robust and scalable tool for
diverse scientific and engineering applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [157] [PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma](https://arxiv.org/abs/2509.13360)
*L. Zimmer,J. Weidner,M. Balcerak,F. Kofler,I. Ezhov,B. Menze,B. Wiestler*

Main category: eess.IV

TL;DR: PREDICT-GBM是一个用于胶质母细胞瘤生长建模和评估的综合平台，包含255例患者的临床数据集，通过个性化放疗计划相比传统均匀边界方法能更好地覆盖肿瘤复发区域。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤具有高度侵袭性和高复发率，传统放疗采用均匀治疗边界无法考虑患者特异性因素。现有计算模型虽能生成肿瘤细胞分布图，但临床采用有限，需要搭建平台加速模型开发和临床验证。

Method: 开发PREDICT-GBM集成管道和数据集，包含专家整理的255例患者完整肿瘤分割和组织特征图，用于系统评估最先进的肿瘤生长模型。

Result: 分析显示，基于肿瘤生长预测的个性化放疗计划相比传统均匀边界方法，在两个评估模型中实现了更好的复发覆盖。

Conclusion: 该工作建立了强大的平台，用于推进和系统评估前沿肿瘤生长建模方法，最终目标是促进临床转化和改善患者预后。

Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by
its highly invasive behavior and exceptionally high rates of recurrence.
Conventional radiation therapy, which employs uniform treatment margins, fails
to account for patient-specific anatomical and biological factors that
critically influence tumor cell migration. To address this limitation, numerous
computational models of glioblastoma growth have been developed, enabling
generation of tumor cell distribution maps extending beyond radiographically
visible regions and thus informing more precise treatment strategies. However,
despite encouraging preliminary findings, the clinical adoption of these growth
models remains limited. To bridge this translational gap and accelerate both
model development and clinical validation, we introduce PREDICT-GBM, a
comprehensive integrated pipeline and dataset for modeling and evaluation. This
platform enables systematic benchmarking of state-of-the-art tumor growth
models using an expert-curated clinical dataset comprising 255 subjects with
complete tumor segmentations and tissue characterization maps. Our analysis
demonstrates that personalized radiation treatment plans derived from tumor
growth predictions achieved superior recurrence coverage compared to
conventional uniform margin approaches for two of the evaluated models. This
work establishes a robust platform for advancing and systematically evaluating
cutting-edge tumor growth modeling approaches, with the ultimate goal of
facilitating clinical translation and improving patient outcomes.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [158] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: 本文批评了当前AI监管过度依赖准确性作为主要基准，提出了"准确性悖论"概念，认为这种单一指标会掩盖LLM幻觉的更深层社会危害，并呼吁转向更全面的可信AI治理方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常决策中的普及，其产生的幻觉（虚假、误导性输出）带来了严重的认识论和社会风险。作者发现当前监管和学术讨论过度依赖准确性作为主要评估标准，这种单一指标可能产生反效果。

Method: 通过跨学科文献分析，构建了幻觉类型的分类法，并从三个维度（输出、个体、社会）分析准确性悖论，同时考察了欧盟AI法案、GDPR和DSA等现行法规的结构性局限。

Result: 研究发现准确性作为单一指标存在三个主要问题：1）只是可靠性的表面代理，鼓励修辞流畅性而非认识论可信度；2）无法检测非事实错误但具有误导性的危害；3）掩盖了幻觉的社会后果，如社会分类、隐私侵犯、公平性损害等。

Conclusion: 当前法规在结构上无法充分应对这些认识论、关系和系统性危害。需要从根本上转向多元化、情境感知和抗操纵的AI可信治理方法，超越单一的准确性指标。

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [159] [Synthetic Data and the Shifting Ground of Truth](https://arxiv.org/abs/2509.13355)
*Dietmar Offenhuber*

Main category: cs.CY

TL;DR: 本文探讨合成数据如何挑战传统的地面真值概念，分析在缺乏真实世界参照的情况下，机器学习研究者如何建立和使用合成数据作为训练数据和地面真值库。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在隐私保护、训练数据生成等方面的广泛应用，传统基于表示准确性的数据保真度假设受到挑战。作者旨在研究在这种缺乏真实世界参照的悖论情况下，研究者如何构建地面真值。

Method: 通过理论分析，探讨合成数据的特点及其对机器学习实践的影响，分析从表示性数据概念向模仿性/图标性数据概念的转变。

Result: 研究发现合成数据虽然缺乏真实世界参照，但通过补偿已知偏差、防止过拟合、支持泛化等方式，反而可能带来更好的模型性能。地面真值成为自我参照的事务。

Conclusion: 合成数据的兴起促使我们需要重新思考数据保真度和地面真值的概念，从传统的表示性范式转向模仿性范式，这对机器学习理论和实践都具有深远影响。

Abstract: The emergence of synthetic data for privacy protection, training data
generation, or simply convenient access to quasi-realistic data in any shape or
volume complicates the concept of ground truth. Synthetic data mimic real-world
observations, but do not refer to external features. This lack of a
representational relationship, however, not prevent researchers from using
synthetic data as training data for AI models and ground truth repositories. It
is claimed that the lack of data realism is not merely an acceptable tradeoff,
but often leads to better model performance than realistic data: compensate for
known biases, prevent overfitting and support generalization, and make the
models more robust in dealing with unexpected outliers. Indeed, injecting noisy
and outright implausible data into training sets can be beneficial for the
model. This greatly complicates usual assumptions based on which
representational accuracy determines data fidelity (garbage in - garbage out).
Furthermore, ground truth becomes a self-referential affair, in which the
labels used as a ground truth repository are themselves synthetic products of a
generative model and as such not connected to real-world observations. My paper
examines how ML researchers and practitioners bootstrap ground truth under such
paradoxical circumstances without relying on the stable ground of
representation and real-world reference. It will also reflect on the broader
implications of a shift from a representational to what could be described as a
mimetic or iconic concept of data.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [160] [Catalpa: GC for a Low-Variance Software Stack](https://arxiv.org/abs/2509.13429)
*Anthony Arnold,Mark Marron*

Main category: cs.PL

TL;DR: 本文提出了Catalpa垃圾收集器，专为Bosque语言设计，通过利用语言特性实现有界收集暂停、固定内存开销和无屏障同步，以优化尾延迟性能。


<details>
  <summary>Details</summary>
Motivation: 实际应用中性能是二元的（快/慢），工业界更关注95th和99th百分位尾延迟而非平均响应时间，需要编程语言和运行时系统主动支持这种需求。

Method: 利用Bosque语言的不变性和无引用环特性，设计Catalpa垃圾收集器，实现有界收集暂停、固定常数内存开销，且不需要与应用程序代码进行屏障或同步。

Result: 收集器能够最小化延迟和变异性，同时保持高吞吐量和小内存开销。

Conclusion: Catalpa收集器通过语言特性实现了对尾延迟性能的优化，为构建响应迅速的软件栈提供了有效解决方案。

Abstract: The performance of an application/runtime is usually conceptualized as a
continuous function where, the lower the amount of memory/time used on a given
workload, then the better the compiler/runtime is. However, in practice, good
performance of an application is viewed as more of a binary function - either
the application responds in under, say 100 ms, and is fast enough for a user to
barely notice, or it takes a noticeable amount of time, leaving the user
waiting and potentially abandoning the task. Thus, performance really means how
often the application is fast enough to be usable, leading industrial
developers to focus on the 95th and 99th percentile tail-latencies as heavily,
or moreso, than average response time. Our vision is to create a software stack
that actively supports these needs via programming language and runtime system
design. In this paper we present a novel garbage-collector design, the Catalpa
collector, for the Bosque programming language and runtime. This allocator is
designed to minimize latency and variability while maintaining high-throughput
and incurring small memory overheads. To achieve these goals we leverage
various features of the Bosque language, including immutability and
reference-cycle freedom, to construct a collector that has bounded collection
pauses, incurs fixed-constant memory overheads, and does not require any
barriers or synchronization with application code.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [161] [A reduced-order derivative-informed neural operator for subsurface fluid-flow](https://arxiv.org/abs/2509.13620)
*Jeongjin,Park,Grant Bruer,Huseyin Tuna Erdinc,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: physics.comp-ph

TL;DR: DeFINO是一种基于导数信息的神经算子训练框架，通过Fisher信息矩阵识别主导特征方向来投影雅可比矩阵，显著降低计算成本，在保持流体动力学预测精度的同时提高梯度准确性。


<details>
  <summary>Details</summary>
Motivation: 神经算子作为流体模拟器的替代模型在渗透率反演等任务中很有效，但传统方法中显式雅可比矩阵的计算成本随参数数量呈二次增长，限制了其可扩展性。

Method: 提出DeFINO框架，将傅里叶神经算子与基于Fisher信息矩阵的导数训练策略结合，通过将雅可比矩阵投影到FIM识别的主导特征方向来捕获关键灵敏度信息。

Result: 在地下多相流体流动的合成实验中验证，DeFINO在保持流体动力学稳健预测的同时提高了梯度准确性，显著降低了计算成本。

Conclusion: DeFINO为复杂现实场景中的反演问题提供了实用、可扩展的解决方案，在显著降低计算成本的同时保持了性能。

Abstract: Neural operators have emerged as cost-effective surrogates for expensive
fluid-flow simulators, particularly in computationally intensive tasks such as
permeability inversion from time-lapse seismic data, and uncertainty
quantification. In these applications, the fidelity of the surrogate's
gradients with respect to system parameters is crucial, as the accuracy of
downstream tasks, such as optimization and Bayesian inference, relies directly
on the quality of the derivative information. Recent advances in
physics-informed methods have leveraged derivative information to improve
surrogate accuracy. However, incorporating explicit Jacobians can become
computationally prohibitive, as the complexity typically scales quadratically
with the number of input parameters. To address this limitation, we propose
DeFINO (Derivative-based Fisher-score Informed Neural Operator), a
reduced-order, derivative-informed training framework. DeFINO integrates
Fourier neural operators (FNOs) with a novel derivative-based training strategy
guided by the Fisher Information Matrix (FIM). By projecting Jacobians onto
dominant eigen-directions identified by the FIM, DeFINO captures critical
sensitivity information directly informed by observational data, significantly
reducing computational expense. We validate DeFINO through synthetic
experiments in the context of subsurface multi-phase fluid-flow, demonstrating
improvements in gradient accuracy while maintaining robust forward predictions
of underlying fluid dynamics. These results highlight DeFINO's potential to
offer practical, scalable solutions for inversion problems in complex
real-world scenarios, all at substantially reduced computational cost.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [162] [Spacing Test for Fused Lasso](https://arxiv.org/abs/2509.14229)
*Rieko Tasaka,Tatsuya Kimura,Joe Suzuki*

Main category: math.ST

TL;DR: 本研究将Spacing Test框架扩展到融合lasso，为后选择推断提供理论基础，通过分析解路径推导精确条件p值，在数值实验中展示了良好的I类错误控制和检测能力。


<details>
  <summary>Details</summary>
Motivation: 解决融合lasso中正则化参数选择的问题，扩展Spacing Test框架到融合惩罚结构，为结构化信号估计问题提供理论可靠的计算实用解决方案。

Method: 使用LARS类型算法分析融合lasso的解路径，将选择事件表征为多面体约束，推导所选变化点的精确条件p值。

Result: 数值实验表明，与AIC、BIC序列版本和交叉验证相比，该方法能正确控制I类错误同时保持高检测能力。

Conclusion: 该研究为融合lasso提供了理论严谨的后选择推断方法，扩展了Spacing Test的应用范围，在结构化信号估计问题中具有重要价值。

Abstract: This study addresses the unresolved problem of selecting the regularization
parameter in the fused lasso. In particular, we extend the framework of the
Spacing Test proposed by Tibshirani et al. to the fused lasso, providing a
theoretical foundation for post-selection inference by characterizing the
selection event as a polyhedral constraint. Based on the analysis of the
solution path of the fused lasso using a LARS-type algorithm, we derive exact
conditional $p$-values for the selected change-points. Our method broadens the
applicability of the Spacing Test from the standard lasso to fused penalty
structures. Furthermore, through numerical experiments comparing the proposed
method with sequential versions of AIC and BIC as well as cross-validation, we
demonstrate that the proposed approach properly controls the type I error while
achieving high detection power. This work offers a theoretically sound and
computationally practical solution for parameter selection and post-selection
inference in structured signal estimation problems. Keywords: Fused Lasso,
Regularization parameter selection, Spacing Test for Lasso, Selective
inference, Change-point detection

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [163] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind是一个专门为工业管理系统设计的GUI代理框架，通过系统探索、记忆规划、状态识别、知识蒸馏和多层安全机制，解决了LLM-based GUI代理在工业管理中的五大挑战。


<details>
  <summary>Details</summary>
Motivation: 工业基础设施管理面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战，现有RPA方案灵活性有限且维护成本高，通用LLM-based GUI代理在工业管理中存在元素理解、精度效率、状态定位、部署约束和安全需求等五大问题。

Method: 提出InfraMind框架，包含五个创新模块：基于系统搜索的探索（使用虚拟机快照）、记忆驱动的规划、高级状态识别、结构化知识蒸馏和多层安全机制。

Result: 在开源和商业DCIM平台上的广泛实验表明，该方法在任务成功率和操作效率方面持续优于现有框架。

Conclusion: InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了LLM-based GUI代理在工业环境中的关键挑战。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [164] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: FRIT是一种通过干预训练提升思维链推理因果一致性的对齐方法，通过生成忠实/不忠实推理对来训练模型偏好因果一致的推理路径，在多个任务上显著提升了推理的忠实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理方法存在推理步骤与最终答案缺乏因果关联的问题，导致输出脆弱且不可信。虽然已有工作主要关注测量忠实性，但系统性改进方法仍然有限。

Method: 提出FRIT方法：1）通过在模型生成的思维链中对单个推理步骤进行干预，生成合成训练数据（忠实/不忠实推理对）；2）应用直接偏好优化训练模型偏好因果一致的推理路径。

Result: 在Qwen3-8B和Mistral-7B-v0.1模型上测试，FRIT在GSM8K任务上将Mistral的忠实推理提升了3.4个百分点，准确率提升了7.6个百分点。

Conclusion: FRIT提供了第一个可扩展、无监督的方法来训练语言模型产生更可靠和可解释的推理，解决了推理性能与可信度之间的关键差距。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [165] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 利用世界模型生成想象环境来训练鲁棒智能体，通过无监督环境设计自动生成课程，在狭窄数据集上训练的世界模型中实现对新环境的强泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决在具身环境中训练智能体需要大量训练数据或精确模拟的问题，利用离线被动收集数据的世界模型作为替代方案

Method: 提出IMAC方法，结合无监督环境设计(UED)在世界模型生成的想象环境中自动生成课程，训练智能体适应多样化任务变化

Result: 在具有挑战性的程序生成环境中，仅使用狭窄数据集学习的世界模型进行训练，就能在保留环境中实现强大的迁移性能

Conclusion: 该方法为利用更大规模的基础世界模型训练通用能力智能体开辟了道路

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [166] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2是一个完全自动化的RL智能体生成框架，通过LLM驱动将自然语言任务描述转换为高性能强化学习解决方案，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统RL智能体开发需要大量专业知识和迭代过程，失败率高且可访问性有限，需要实现完全自动化的智能体设计。

Method: 采用双智能体架构：生成器智能体作为自主AI设计器分析任务并生成可执行RL智能体，目标智能体是自动生成的RL智能体。框架将RL开发分解为MDP建模和算法优化两个阶段，基于模型上下文协议提供统一框架。

Result: 在MuJoCo、MetaDrive、MPE和SMAC等多个基准测试中，Agent^2始终优于人工设计的解决方案，性能提升高达55%，平均表现也有显著提升。

Conclusion: 这项工作通过实现真正端到端的闭环自动化，建立了智能体设计和优化其他智能体的新范式，是自动化AI系统的根本性突破。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [167] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl是一个评估表示引导方法的基准，重点关注偏见、有害生成和幻觉等核心对齐目标，以及这些方法对次要行为（如奉承和常识道德）的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐工作往往只关注真实性或推理能力来展示表示引导的副作用，但许多权衡关系尚未被系统性地理解。

Method: 构建了一个包含安全相关主要和次要行为的数据集，基于五个流行引导方法创建模块化引导框架，使用独特组件作为现有方法的构建块。在Qwen-2.5-7B和Llama-3.1-8B模型上进行评估。

Result: 发现强引导性能取决于引导方法、模型和目标行为的特定组合，不良组合会导致严重的概念纠缠。

Conclusion: 表示引导的效果具有高度情境依赖性，需要仔细考虑方法、模型和目标的组合，以避免意外的负面副作用。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [168] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 该研究使用人工神经网络模型探讨网络信息流结构变化是否能带来认知性能的过渡性转变，发现循环网络相比前馈网络在处理复杂语法时表现出质的性能提升，并观察到训练难度形成的过渡障碍。


<details>
  <summary>Details</summary>
Motivation: 探索认知进化是否通过一系列重大转变实现，这些转变通过操纵生物神经网络结构来根本改变信息流，从而产生过渡性的认知性能变化。

Method: 使用理想化的信息流模型和人工神经网络，比较前馈、循环和分层拓扑结构的网络在学习和种复杂度人工语法时的性能表现，控制网络大小和资源。

Result: 循环网络相比前馈网络能够处理更广泛的输入类型，在最复杂语法学习上表现出质的性能提升；循环网络的训练难度形成了过渡障碍和偶然不可逆性；分层网络在语法学习任务中并未表现出优势。

Conclusion: 某些信息流结构的变化确实能够产生认知性能的过渡性转变，这为理解认知进化通过重大转变实现的假设提供了计算模型支持。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [169] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过层次架构（GCN作为一阶学习器，MLP作为二阶学习器）实证验证了二阶学习能促进环境-认知同构的心理表征形成，在迷宫导航任务中表现出显著性能提升和强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究心理表征（结构化内部模型反映外部环境）对高级认知的重要性，但实证研究存在挑战。现有理论假设二阶学习（适应一阶学习的学习机制）能促进环境-认知同构性的出现。

Method: 提出分层架构：使用图卷积网络（GCN）作为一阶学习器直接映射节点特征到最优导航路径预测，使用MLP控制器作为二阶学习器在遇到结构新颖的迷宫环境时动态调整GCN参数。

Result: 当认知系统发展出与环境结构同构的内部心理地图时，二阶学习特别有效。定量和定性结果显示了在未见迷宫任务上的显著性能改进和强大泛化能力。

Conclusion: 研究为结构化心理表征在最大化二阶学习有效性中的关键作用提供了实证支持，验证了环境-认知同构性假说。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [170] [LLM Chatbot-Creation Approaches](https://arxiv.org/abs/2509.13326)
*Hemil Mehta,Tanvi Raut,Kohav Yadav,Edward F. Gehringer*

Main category: cs.HC

TL;DR: 本研究比较了教育场景中低代码平台与定制编码两种课程聊天机器人开发方法，发现低代码平台适合快速原型但定制性有限，定制编码提供更好控制但需要技术专长。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4、LLaMA等大语言模型的兴起，教育机构需要选择最优的聊天机器人开发策略，在易用性、定制化、数据隐私和可扩展性之间取得平衡。

Method: 研究比较了AnythingLLM、Botpress等低代码平台与使用LangChain、FAISS、FastAPI的定制编码方案，通过提示工程、检索增强生成(RAG)和个性化技术评估技术性能、可扩展性和用户体验。

Result: 低代码平台支持快速原型开发但面临定制和扩展限制，定制编码系统提供更多控制但需要专业技术知识。两种方法都成功实现了自适应反馈循环和对话连续性等关键研究原则。

Conclusion: 研究提供了基于机构目标和资源选择适当开发策略的框架，未来工作将专注于结合低代码易用性与模块化定制的混合解决方案，并为智能辅导系统整合多模态输入。

Abstract: This full research-to-practice paper explores approaches for developing
course chatbots by comparing low-code platforms and custom-coded solutions in
educational contexts. With the rise of Large Language Models (LLMs) like GPT-4
and LLaMA, LLM-based chatbots are being integrated into teaching workflows to
automate tasks, provide assistance, and offer scalable support. However,
selecting the optimal development strategy requires balancing ease of use,
customization, data privacy, and scalability. This study compares two
development approaches: low-code platforms like AnythingLLM and Botpress, with
custom-coded solutions using LangChain, FAISS, and FastAPI. The research uses
Prompt engineering, Retrieval-augmented generation (RAG), and personalization
to evaluate chatbot prototypes across technical performance, scalability, and
user experience. Findings indicate that while low-code platforms enable rapid
prototyping, they face limitations in customization and scaling, while
custom-coded systems offer more control but require significant technical
expertise. Both approaches successfully implement key research principles such
as adaptive feedback loops and conversational continuity. The study provides a
framework for selecting the appropriate development strategy based on
institutional goals and resources. Future work will focus on hybrid solutions
that combine low-code accessibility with modular customization and incorporate
multimodal input for intelligent tutoring systems.

</details>
