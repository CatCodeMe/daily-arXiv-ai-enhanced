<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.AI](#cs.AI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.CR](#cs.CR) [Total: 8]
- [eess.IV](#eess.IV) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [ORCA: ORchestrating Causal Agent](https://arxiv.org/abs/2508.21304)
*Joanie Hayoun Chung,Chaemyung Lim,Sumin Lee,Sungbin Lim*

Main category: cs.DB

TL;DR: ORCA是一个基于LLM的智能代理系统，旨在自动化关系数据库中的因果推断工作流，通过人机交互保持专家监督，显著提升因果效应估计效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模增长，数据分析工作流（从数据整理到因果分析）复杂性急剧增加，非专家在关系数据库中执行工作流会导致重复性瓶颈，阻碍及时的业务洞察。

Method: ORCA是一个LLM代理系统，能够自动化RDBMS中的常规工作流，包括：解释自然语言查询、导航数据库表、生成SQL代码、数据预处理以及使用因果推断库配置建模过程。

Result: 在基准测试和合成电商数据集上的实证评估显示，ORCA在表理解、查询生成和因果效应估计方面具有竞争优势，在估计平均处理效应方面相比GPT-4o mini实现了7倍以上的改进。

Conclusion: ORCA系统能够通过人机交互实现全数据分析管道的自动化，使领域专家能够以较少的技术专业知识进行稳健的数据驱动决策，有效解决了因果推断工作流中的自动化挑战。

Abstract: Causal inference is essential for decision-making science while the
complexity of the data analysis workflow, ranging from data wrangling to causal
analysis, increases substantially as the scale of data grows in complicated
business environments. Especially, the execution of the workflow in relational
databases by non-experts can result in repetitive bottlenecks which impede
timely and responsible business insights. To address this challenge, we propose
ORCA (Orchestrating Causal Agent), an LLM agentic system that can automate
routine workflows in RDBMS while preserving expert oversight via human-AI
interactions. ORCA orchestrates the full data analysis pipeline: interpreting
natural language queries, navigating tables from DB servers, generating proper
SQL codes, preprocessing data, and configuring modeling processes using causal
inference libraries. Domain experts still can control the automation through
iterative interactions with ORCA, enabling robust data-driven decision making
with less technical expertise in statistical computing. Empirical evaluations
on benchmark and synthetic e-commerce datasets demonstrate competitive
performance of ORCA in table understanding, query generation, and cause-effect
estimation -- achieving over $7\times$ improvement in estimating average
treatment compared to GPT-4o mini.

</details>


### [2] [Hilbert Forest in the SISAP 2025 Indexing Challenge](https://arxiv.org/abs/2508.21682)
*Yasunobu Imamura,Takeshi Shinohara,Naoya Higuchi,Kouichi Hirata,Tetsuji Kuboyama*

Main category: cs.DB

TL;DR: 提出基于Hilbert空间填充曲线的Hilbert森林索引方法，在SISAP 2025挑战赛中表现出色，特别是在严格内存限制下实现了快速构建和竞争性搜索性能


<details>
  <summary>Details</summary>
Motivation: 解决高维数据在严格内存约束（16GB RAM，8 CPU核心）下的近似最近邻搜索和k近邻图构建问题

Method: 使用快速Hilbert排序算法对高维点沿Hilbert曲线排序，构建多个Hilbert树来支持近似最近邻搜索

Result: 在Task 1（PUBMED23数据集近似搜索）中表现竞争性，在Task 2（GOOAQ数据集k近邻图构建）中实现了最快的构建时间，同时满足召回率要求

Conclusion: Hilbert顺序索引方法在严格内存限制下具有实际有效性，特别适合资源受限环境中的高维数据索引

Abstract: We report our participation in the SISAP 2025 Indexing Challenge using a
novel indexing technique called the Hilbert forest. The method is based on the
fast Hilbert sort algorithm, which efficiently orders high-dimensional points
along a Hilbert space-filling curve, and constructs multiple Hilbert trees to
support approximate nearest neighbor search. We submitted implementations to
both Task 1 (approximate search on the PUBMED23 dataset) and Task 2 (k-nearest
neighbor graph construction on the GOOAQ dataset) under the official resource
constraints of 16 GB RAM and 8 CPU cores. The Hilbert forest demonstrated
competitive performance in Task 1 and achieved the fastest construction time in
Task 2 while satisfying the required recall levels. These results highlight the
practical effectiveness of Hilbert order-based indexing under strict memory
limitations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores](https://arxiv.org/abs/2508.21230)
*Brian Curless,Michael Gowanlock*

Main category: cs.DC

TL;DR: 这篇论文提出了一种基于混合精度(FP16-32)张量核的欧几里得距离计算算法FaSTED，在高维数据集上实现了显著的性能提升，速度比现有最佳算法提升2.5-51倍，而准确性损失仅小于0.06%。


<details>
  <summary>Details</summary>
Motivation: 张量核(TCs)虽然主要用于AI负载中的矩阵乘法，但其高计算速度也可用于其他算法以获得性能提升。欧几里得距离计算在数据分析中应用广泛，但现有研究仅考虑FP64数据，而TCs支持更高速的FP16-32混合精度运算。

Method: 提出FaSTED算法，采用FP16-32混合精度张量核计算欧几里得距离。设计重点包括：层次化数据重用、汇聚存储器和寄存器的最大化利用、以及在相似性搜索应用中集成索引数据结构。

Result: 在4个真实高维数据集(128-960维)上，混合精度暴力计算方法比现有最佳算法实现2.5-51倍速度提升。与FP64基准相比，算法准确性损失仅小于0.06%。

Conclusion: FaSTED算法成功利用张量核的高速FP16-32计算能力，通过优化数据重用和存储器利用，在保持高准确性的同时实现了显著的性能提升，为欧几里得距离计算提供了高效解决方案。

Abstract: Modern GPUs are equipped with tensor cores (TCs) that are commonly used for
matrix multiplication in artificial intelligence workloads. However, because
they have high computational throughput, they can lead to significant
performance gains in other algorithms if they can be successfully exploited. We
examine using TCs to compute Euclidean distance calculations, which are used in
many data analytics applications. Prior work has only investigated using 64 bit
floating point (FP64) data for computation; however, TCs can operate on lower
precision floating point data (i.e., 16 bit matrix multiplication and 32 bit
accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so
high that TCs are easily starved of data. We propose a Fast and Scalable Tensor
core Euclidean Distance (FaSTED) algorithm. To achieve high computational
throughput, we design FaSTED for significant hierarchical reuse of data and
maximize memory utilization at every level (global memory, shared memory, and
registers). We apply FaSTED to the application of similarity searches, which
typically employ an indexing data structure to eliminate superfluous Euclidean
distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean
distance algorithm in the literature that employs FP64, as well as to two
single precision (FP32) CUDA core algorithms that both employ an index. We find
that across four real-world high-dimensional datasets spanning 128-960
dimensions, the mixed-precision brute force approach achieves a speedup over
the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed
precision algorithm to be less than <0.06% when compared to the FP64 baseline.

</details>


### [4] [Decentralized Federated Averaging via Random Walk](https://arxiv.org/abs/2508.21286)
*Changheng Wang,Zhiqing Wei,Lizhe Liu,Qiao Deng,Yingda Wu,Yangyang Niu,Yashan Pang,Zhiyong Feng*

Main category: cs.DC

TL;DR: 提出基于随机游走的去中心化联邦平均算法DFedRW，通过随机游走更新替代多步本地更新，解决异构数据下的收敛问题，并引入量化版本提升通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在异构和不平衡数据下收敛慢且可能陷入次优解，中心化架构存在单点故障和隐私风险，去中心化联邦平均算法通常忽略掉队者导致训练数据减少和采样偏差。

Method: 提出DFedRW算法，用随机游走更新替代多步本地更新，允许聚合部分随机游走更新确保每次计算都贡献模型更新，并开发量化版本提升通信效率。

Result: 在凸条件下达到O(1/k^(1-q))的收敛上界，量化版本在通信和收敛间取得平衡，数值分析显示在高异构性下测试准确率比传统FedAvg提升38.3%和37.5%。

Conclusion: DFedRW算法有效解决了去中心化联邦学习在异构数据下的收敛问题，通过随机游走和量化技术实现了更好的收敛性能和通信效率。

Abstract: Federated Learning (FL) is a communication-efficient distributed machine
learning method that allows multiple devices to collaboratively train models
without sharing raw data. FL can be categorized into centralized and
decentralized paradigms. The centralized paradigm relies on a central server to
aggregate local models, potentially resulting in single points of failure,
communication bottlenecks, and exposure of model parameters. In contrast, the
decentralized paradigm, which does not require a central server, provides
improved robustness and privacy. The essence of federated learning lies in
leveraging multiple local updates for efficient communication. However, this
approach may result in slower convergence or even convergence to suboptimal
models in the presence of heterogeneous and imbalanced data. To address this
challenge, we study decentralized federated averaging via random walk (DFedRW),
which replaces multiple local update steps on a single device with random walk
updates. Traditional Federated Averaging (FedAvg) and its decentralized
versions commonly ignore stragglers, which reduces the amount of training data
and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial
random walk updates, ensuring that each computation contributes to the model
update. To further improve communication efficiency, we also propose a
quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves
convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex
conditions. Furthermore, we propose a sufficient condition that reveals when
quantization balances communication and convergence. Numerical analysis
indicates that our proposed algorithms outperform (decentralized) FedAvg in
both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in
test accuracy under high levels of heterogeneities.

</details>


### [5] [Addressing Reproducibility Challenges in HPC with Continuous Integration](https://arxiv.org/abs/2508.21289)
*Valérie Hayot-Sasson,Nathaniel Hudson,André Bauer,Maxime Gonthier,Ian Foster,Kyle Chard*

Main category: cs.DC

TL;DR: 本文提出CORRECT GitHub Action工具，通过在远程HPC资源上安全执行测试来解决HPC领域可重复性研究的挑战，并展示了其在三种不同类型HPC应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: HPC社区虽然采用激励结构促进可重复研究，但由于HPC基础设施和软件的特殊性以及严格的访问要求，许多论文仍无法满足可重复性要求。在缺乏资源访问的情况下，需要通过持续集成和完整的溯源信息来替代。

Method: 开发了CORRECT GitHub Action工具，支持在远程HPC资源上安全执行测试，通过自动化测试和文档化来评估可重复性。

Result: 在三种不同类型的HPC应用中评估了CORRECT的可用性，证明了使用CORRECT进行自动化可重复性评估的有效性。

Conclusion: 更好的HPC兼容持续集成解决方案将提高应用程序的可重复性，CORRECT工具为解决HPC可重复性限制提供了有效方法。

Abstract: The high-performance computing (HPC) community has adopted incentive
structures to motivate reproducible research, with major conferences awarding
badges to papers that meet reproducibility requirements. Yet, many papers do
not meet such requirements. The uniqueness of HPC infrastructure and software,
coupled with strict access requirements, may limit opportunities for
reproducibility. In the absence of resource access, we believe that regular
documented testing, through continuous integration (CI), coupled with complete
provenance information, can be used as a substitute. Here, we argue that better
HPC-compliant CI solutions will improve reproducibility of applications. We
present a survey of reproducibility initiatives and describe the barriers to
reproducibility in HPC. To address existing limitations, we present a GitHub
Action, CORRECT, that enables secure execution of tests on remote HPC
resources. We evaluate CORRECT's usability across three different types of HPC
applications, demonstrating the effectiveness of using CORRECT for automating
and documenting reproducibility evaluations.

</details>


### [6] [A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling](https://arxiv.org/abs/2508.21328)
*Zhiyu Wang,Mohammad Goudarzi,Mingming Gong,Rajkumar Buyya*

Main category: cs.DC

TL;DR: KD-AFRL框架通过知识蒸馏增强的自适应联邦强化学习，解决了云边端IoT环境中多域调度优化的三大挑战：计算异构性、非IID数据分布和跨域协作不足。


<details>
  <summary>Details</summary>
Motivation: IoT应用在异构云边端环境中的快速发展带来了分布式调度优化的重大挑战，包括固定神经网络架构与计算异构性不兼容、跨域数据分布非IID以及跨域协作机制不足等问题。

Method: 提出KD-AFRL框架，包含三个核心创新：1）资源感知的混合架构生成机制；2）隐私保护的基于环境聚类的联邦学习方法；3）面向环境的跨架构知识蒸馏机制。

Result: 实验显示相比最佳基线，KD-AFRL实现了21%的更快收敛速度，在完成时间、能耗和加权成本方面分别获得15.7%、10.8%和13.9%的性能提升，且可扩展性表现优异。

Conclusion: KD-AFRL框架有效解决了多域IoT应用调度中的关键挑战，在性能、效率和可扩展性方面均显著优于现有解决方案。

Abstract: The rapid proliferation of Internet of Things (IoT) applications across
heterogeneous Cloud-Edge-IoT environments presents significant challenges in
distributed scheduling optimization. Existing approaches face issues, including
fixed neural network architectures that are incompatible with computational
heterogeneity, non-Independent and Identically Distributed (non-IID) data
distributions across IoT scheduling domains, and insufficient cross-domain
collaboration mechanisms. This paper proposes KD-AFRL, a Knowledge
Distillation-empowered Adaptive Federated Reinforcement Learning framework that
addresses multi-domain IoT application scheduling through three core
innovations. First, we develop a resource-aware hybrid architecture generation
mechanism that creates dual-zone neural networks enabling heterogeneous devices
to participate in collaborative learning while maintaining optimal resource
utilization. Second, we propose a privacy-preserving environment-clustered
federated learning approach that utilizes differential privacy and K-means
clustering to address non-IID challenges and facilitate effective collaboration
among compatible domains. Third, we introduce an environment-oriented
cross-architecture knowledge distillation mechanism that enables efficient
knowledge transfer between heterogeneous models through temperature-regulated
soft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure
demonstrate KD-AFRL's effectiveness using diverse IoT applications. Results
show significant improvements over the best baseline, with 21% faster
convergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,
energy consumption, and weighted cost, respectively. Scalability experiments
reveal that KD-AFRL achieves 3-5 times better performance retention compared to
existing solutions as the number of domains increases.

</details>


### [7] [Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage](https://arxiv.org/abs/2508.21473)
*Daniil Vostrikov,Yash Madhwal,Andrey Seoev,Anastasiia Smirnova,Yury Yanovich,Alexey Smirnov,Vladimir Gorgadze*

Main category: cs.DC

TL;DR: 本文分析Polygon区块链上的最大可提取价值(MEV)，特别关注原子套利交易，比较垃圾邮件式和拍卖式回跑策略的流行度和盈利性。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术从加密货币扩展到DeFi等更广泛领域，MEV成为重要挑战，需要深入理解其在Polygon等网络中的动态和影响。

Method: 建立原子套利交易识别标准，分析22个月2300万个区块的数据集，研究搜索者行为、竞价动态和代币使用等关键因素。

Result: 发现垃圾邮件式交易更普遍，但拍卖式交易盈利能力更强，揭示了网络架构、交易排序和MEV提取之间的相互作用。

Conclusion: 研究结果强调了需要强大的交易排序机制，并指出了新兴MEV策略对区块链网络的重要影响。

Abstract: The evolution of blockchain technology, from its origins as a decentralized
ledger for cryptocurrencies to its broader applications in areas like
decentralized finance (DeFi), has significantly transformed financial
ecosystems while introducing new challenges such as Maximum Extractable Value
(MEV). This paper explores MEV on the Polygon blockchain, with a particular
focus on Atomic Arbitrage (AA) transactions. We establish criteria for
identifying AA transactions and analyze key factors such as searcher behavior,
bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and
covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based
and Auction-based backrunning strategies. Our findings reveal that while
Spam-based transactions are more prevalent, Auction-based transactions
demonstrate greater profitability. Through detailed examples and analysis, we
investigate the interactions between network architecture, transaction
sequencing, and MEV extraction, offering comprehensive insights into the
evolution and challenges of MEV in decentralized ecosystems. These results
emphasize the need for robust transaction ordering mechanisms and highlight the
implications of emerging MEV strategies for blockchain networks.

</details>


### [8] [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)
*Yuhang Zhou,Zhibin Wang,Peng Jiang,Haoran Xia,Junhe Lu,Qianyu Jiang,Rong Gu,Hengxi Xu,Xinjing Huang,Guanghuan Fang,Zhiheng Hu,Jingyi Zhang,Yongjin Cai,Jian He,Chen Tian*

Main category: cs.DC

TL;DR: Odyssey是一个自适应容错系统，通过智能选择最优恢复策略来解决大语言模型训练中的中断问题，相比现有方法性能提升显著


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练经常因各种故障而中断，需要强大的容错能力。现有无备份方法都存在性能损失问题，包括持续开销、长重新配置时间或恢复后效率低下

Method: Odyssey采用统一性能模型、快速执行计划搜索、准确性能估计和高效通信优化，在故障发生时智能选择最优恢复策略

Result: 在32卡集群上的实验显示，Odyssey保持恢复后与无故障训练的性能差距在11.00%以内，同时保持模型收敛和高效内存使用。相比最先进方法，平均吞吐量分别比Oobleck和Recycle高1.229倍和1.355倍

Conclusion: Odyssey系统通过自适应容错策略有效解决了大语言模型训练中的故障恢复问题，在性能、收敛性和内存效率方面都表现出色

Abstract: Training large language models faces frequent interruptions due to various
faults, demanding robust fault-tolerance. Existing backup-free methods, such as
redundant computation, dynamic parallelism, and data rerouting, each incur
performance penalties, whether from ongoing overhead, lengthy reconfigurations,
or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant
system that intelligently selects optimal recovery strategies when a failure
occurs. Odyssey achieves this through a unified performance model, expedient
execution plan search, accurate performance estimation, and efficient
communication optimizations. Experiments on a 32-card cluster show that Odyssey
maintains a performance gap of within 11.00% between post-recovery and
failure-free training, while preserving model convergence and efficient memory
usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and
1.355x higher average throughput than Oobleck and Recycle, respectively.

</details>


### [9] [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)
*Zhibin Wang,Zhonghui Zhang,Yuhang Zhou,Zibo Wang,Mo Zhou,Peng Jiang,Weilin Cai,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: SpecMoEOff是一种基于推测解码的MoE模型卸载技术，通过增大专家工作负载来提高硬件利用率，在GPU和CPU协同计算下实现最高2.5倍的解码吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型卸载技术由于I/O瓶颈和稀疏计算问题，导致硬件利用率低下，需要新的方法来充分利用硬件资源

Method: 采用推测解码技术扩大每个专家的工作负载，通过理论和经验屋顶线分析协调GPU和CPU，开发专用CPU分块注意力验证内核，并集成自动超参数调优器

Result: 实验结果显示，SpecMoEOff相比最先进的MoE卸载技术实现了最高2.5倍的解码吞吐量提升

Conclusion: SpecMoEOff通过推测解码和硬件协同优化，有效解决了MoE模型卸载中的硬件利用率问题，显著提升了推理性能

Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly
increased their parameter scale as well as model performance. Extensive
offloading techniques have been proposed to address the GPU memory limitations
of MoE inference. However, due to the I/O bottleneck and sparse computation of
MoE models, existing offloading techniques still suffer from low hardware
utilization. To fully utilize the hardware resources, we propose SpecMoEOff,
which employs the speculative decoding technique to enlarge the workload of
each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and
empirical roofline analysis. In addition, we develop a dedicated CPU chunked
attention verification kernel to fit the speculative decoding in offloading
scenarios as well as minimizing the additional overhead led by draft models.
SpecMoEOff further integrates an optimizer to automatically tune the
hyperparameters of speculative decoding under given hardware and workload.
Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput
improvement over the state-of-the-art MoE offloading techniques.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [10] [Faster Linear Algebra Algorithms with Structured Random Matrices](https://arxiv.org/abs/2508.21189)
*Chris Camaño,Ethan N. Epperly,Raphael A. Meyer,Joel A. Tropp*

Main category: cs.DS

TL;DR: 本文提出了基于遗忘子空间注入(OSI)属性的结构化降维新视角，将随机线性代数算法分析分解为两部分：证明算法在OSI下有效，以及证明特定随机矩阵具有OSI属性。


<details>
  <summary>Details</summary>
Motivation: 尽管结构化随机矩阵在低秩近似和最小二乘回归中广泛应用，但其设计和分析的基本问题仍未解决，需要更统一的理论框架。

Method: 提出OSI属性作为相对较弱的假设条件，分析标准随机算法在OSI假设下的性能，并识别多种具有OSI属性的随机矩阵类型（稀疏矩阵、随机三角变换、张量积结构等）。

Result: 理论结果证明了多个基本线性代数任务的更快、接近最优运行时间，并通过实证证据显示结构化随机矩阵在合成问题和科学应用中表现优异。

Conclusion: OSI框架为结构化降维提供了统一的分析方法，能够处理多种随机矩阵类型，并为实际实现提供指导，证明了结构化随机矩阵在实际应用中的卓越性能。

Abstract: To achieve the greatest possible speed, practitioners regularly implement
randomized algorithms for low-rank approximation and least-squares regression
with structured dimension reduction maps. Despite significant research effort,
basic questions remain about the design and analysis of randomized linear
algebra algorithms that employ structured random matrices.
  This paper develops a new perspective on structured dimension reduction,
based on the oblivious subspace injection (OSI) property. The OSI property is a
relatively weak assumption on a random matrix that holds when the matrix
preserves the length of vectors on average and, with high probability, does not
annihilate any vector in a low-dimensional subspace. With the OSI abstraction,
the analysis of a randomized linear algebra algorithm factors into two parts:
(i) proving that the algorithm works when implemented with an OSI; and (ii)
proving that a given random matrix model has the OSI property.
  This paper develops both parts of the program. First, it analyzes standard
randomized algorithms for low-rank approximation and least-squares regression
under the OSI assumption. Second, it identifies many examples of OSIs,
including random sparse matrices, randomized trigonometric transforms, and
random matrices with tensor product structure. These theoretical results imply
faster, near-optimal runtimes for several fundamental linear algebra tasks. The
paper also provides guidance on implementation, along with empirical evidence
that structured random matrices offer exemplary performance for a range of
synthetic problems and contemporary scientific applications.

</details>


### [11] [$Δ$-Motif: Subgraph Isomorphism at Scale via Data-Centric](https://arxiv.org/abs/2508.21287)
*Yulun Wang,Esteban Ginez,Jamie Friel,Yuval Baum,Jin-Sung Kim,Alex Shih,Oded Green*

Main category: cs.DS

TL;DR: Δ-Motif是一个GPU加速的子图同构算法，通过数据库操作重构问题，利用关系操作实现大规模并行，相比VF2算法获得最高595倍加速


<details>
  <summary>Details</summary>
Motivation: 传统回溯算法如VF2存在顺序瓶颈，无法充分利用现代并行硬件，限制了子图同构问题在大规模图分析中的性能

Method: 将数据和模式图表示为表格形式，将子图同构转化为数据库基本操作（连接、排序、合并、过滤），通过分解图为motif构建块并使用关系操作组合

Result: 在GPU上实现了最高595倍的性能提升，显著优于VF2等传统算法，并在量子电路编译中解决了关键瓶颈

Conclusion: 通过熟悉的数据库抽象实现了高性能图处理，消除了底层编程需求，同时提供了卓越的计算效率，为量子计算等应用提供了可扩展解决方案

Abstract: Subgraph isomorphism is a fundamental problem in graph analysis that seeks to
find all instances of a pattern graph within a larger data graph while
preserving structural relationships. This NP-complete problem is central to
domains such as biological network analysis, social network mining, and quantum
circuit optimization. Traditional approaches rely on backtracking algorithms
like VF2, which suffer from sequential bottlenecks that limit their ability to
exploit modern parallel hardware. In this work, we introduce $\Delta$-Motif, a
GPU-accelerated subgraph isomorphism algorithm that reformulates the task
through the lens of database operations. Our key insight is to represent both
data and pattern graphs in tabular form, turning subgraph isomorphism into
database primitives including joins, sorts, merges, and filters. $\Delta$-Motif
decomposes graphs into small building blocks called motifs and systematically
combines them using scalable relational operations. By leveraging mature,
optimized libraries from the NVIDIA RAPIDS ecosystem and Pandas framework, our
solution achieves massive parallelism while remaining portable across systems
supporting standard relational primitives. Benchmarks show that $\Delta$-Motif
outperforms established algorithms like VF2, achieving speedups of up to
$595\times$ on GPUs. We further demonstrate its impact by applying it to
quantum circuit compilation, addressing a critical bottleneck in quantum
computing and enabling scaling to near- and medium-term devices. Our approach
democratizes high-performance graph processing by exposing it through familiar
database abstractions, eliminating the need for low-level programming while
delivering exceptional computational efficiency.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本文提出利用检索增强生成(RAG)的大语言模型(LLM)进行模型到代码转换的新研究方向，特别针对量子软件系统，通过UML模型实例生成Qiskit量子代码，实验显示精心设计的提示可将CodeBLEU分数提高四倍。


<details>
  <summary>Details</summary>
Motivation: 量子混合软件系统存在平台异构和开发人员技能缺乏的问题，模型驱动方法可以降低成本和风险，需要探索LLM+RAG在模型到代码转换中的应用。

Method: 部署包含GitHub公开Qiskit代码样本的RAG管道，通过精心设计的提示工程，从UML模型实例生成基于Qiskit库的量子代码。

Result: 实验结果表明，良好设计的提示可以将CodeBLEU分数提升高达四倍，产生更准确和一致的量子代码。

Conclusion: 该方法为量子代码生成提供了有效解决方案，未来可进一步研究将软件系统模型实例作为RAG信息源，以及代码到代码转换等应用场景。

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [13] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: UTR是一种通过对抗性强化学习训练LLM生成高质量单元测试的新框架，通过测试生成器和代码生成器的对抗训练来提升测试质量。


<details>
  <summary>Details</summary>
Motivation: 目前使用LLM自动生成单元测试的方法尚不成熟，需要探索更有效的训练方法来生成高质量的测试框架。

Method: 设计了对抗性强化学习框架UTR，同时训练两个LLM：测试生成器通过最大化辨别奖励（曝露代码漏洞），代码生成器通过最大化代码奖励（通过测试）。

Result: UTR训练的Qwen3-4B在生成高质量单元测试方面超过了相同模型的监督学习方法，甚至超过GPT-4.1等前沿模型。

Conclusion: UTR框架通过对抗性训练方式能够有效提升LLM生成单元测试的质量，为自动化测试生成提供了新的解决方案。

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [14] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: 这篇论文提出了一种轻量级框架，利用指令微调的大语言模型通过LoRA适配器和候选约束解码来实现故障分配，在大型项目中展现出良好的短列质量和实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大型项目中的故障分配工作通常速度慢、不一致，需要一种更高效、一致的解决方案来提高故障处理效率。

Method: 使用指令微调的大语言模型(LLM)，结合LoRA适配器进行调优，并采用候选约束解码技术确保分配结果的有效性。

Result: 在EclipseJDT和Mozilla数据集上进行测试，模型在Hit@10指标上达到0.753，虽然Top-1准确率不高，但短列质量强劲。在最新数据集上准确率显著提升，显示了该框架在实际应用中的潜力。

Conclusion: 指令微调的大语言模型可以作为一种实用的替代方案，避免了成本较高的特征工程和图基方法，适合实际生产环境中的人在循环故障分配。

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [15] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 简单的观测掩码策略在SWE任务中与LLM摘要方法效果相当但成本更低，成本减半且解决率略有提升


<details>
  <summary>Details</summary>
Motivation: 识别LLM基代理在处理复杂任务时产生长上下文历史的成本问题，并对比传统摘要方法与简单省略旧观测的效果差异

Method: 在SWE-agent框架下使用SWE-bench Verified数据集，对比五种模型配置中的原始代理、LLM摘要和观测掩码策略的性能

Result: 观测掩码策略将成本降低50%，解决率与LLM摘要相当或略有提升（如Qwen3-Coder 480B从53.8%提升到54.8%）

Conclusion: 在SWE-agent框架下，最简单的上下文管理策略可能是最高效和最经济的选择

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [16] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: LMPA是一个将大语言模型(LLM)集成到指针分析中的新框架，通过LLM识别类似系统API的用户定义函数并改进摘要分析，以提高指针分析的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析框架由于对代码语义理解不足，在处理用户定义函数时过于保守，导致错误事实传播。LLM的发展为解决这一问题提供了新机会。

Method: 提出LMPA框架：1) 使用LLM识别类似系统API的用户定义函数并进行相应建模；2) 通过LLM推断初始点集；3) 引入基于自然语言的增强摘要策略

Result: 该方法能够减少错误的跨调用上下文传播，提高指针分析的精度，同时保持可扩展性。

Conclusion: LMPA展示了LLM在指针分析中的巨大潜力，通过语义理解能力的增强来突破传统分析方法的局限性，但仍面临一些关键挑战需要解决。

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [17] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: 提出MPTCS方法，从任何策略测试框架生成的多策略测试用例中选择可重用、多样化的测试套件，基于可解性、多样性和通用难度来评估RL策略行为。


<details>
  <summary>Details</summary>
Motivation: 现有RL策略测试方法生成的测试套件通常针对特定策略定制，对其他策略的适用性不明确，需要一种策略无关的通用测试用例选择方法。

Method: 使用一组策略基于难度分数从候选测试用例池中选择测试用例，采用离散化通用测试用例描述符表面来促进测试套件多样性，覆盖状态空间并触发策略产生错误行为。

Result: 评估了难度分数的有效性，分析了方法效果和成本与策略数量的关系，验证了多样性促进方法对状态空间覆盖和错误行为触发的作用。

Conclusion: MPTCS能够选择出多样化、可重用的策略无关测试用例，有效揭示智能体行为的典型缺陷，为RL策略验证提供了通用测试框架。

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [18] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: 大规模比较人类与AI（ChatGPT、DeepSeek-Coder、Qwen-Coder）编写的代码质量，发现AI代码更简单但包含更多安全漏洞，人类代码结构更复杂但可维护性问题更多


<details>
  <summary>Details</summary>
Motivation: 随着AI代码助手在软件开发中的广泛应用，需要了解AI生成代码与人类编写代码在质量上的差异，以确保软件的可靠性、可维护性和安全性

Method: 评估超过50万个Python和Java代码样本，使用正交缺陷分类法分析代码缺陷，使用通用弱点枚举分析安全漏洞，比较三个先进LLM与人类开发者的代码质量

Result: AI生成的代码通常更简单、重复性更高，但更容易出现未使用的结构和硬编码调试；人类编写的代码结构更复杂，可维护性问题更集中；AI生成的代码包含更多高风险安全漏洞

Conclusion: AI和人类编写的代码具有不同的缺陷特征，需要在AI辅助编程中采用专门的质量保证实践

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [19] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本研究通过11次半结构化访谈和主题分析，探讨了敏捷方法在DevOps实践中的可行性和适用性，提出了敏捷方法与DevOps实践相互关系的新理解。


<details>
  <summary>Details</summary>
Motivation: IT行业对快速软件交付的需求日益增长，需要评估敏捷方法在DevOps实践中的整合可行性，以满足客户对高质量软件产品和服务的期望。

Method: 采用定性研究方法，对IT行业不同领域的敏捷和DevOps从业者进行11次半结构化访谈，通过主题分析提取51个独特代码并综合为19个主题。

Result: 研究详细分析了DevOps生命周期的每个阶段，特别是敏捷方法在DevOps实践中的整合和实施情况，提出了敏捷方法与DevOps相互关系的新理解框架。

Conclusion: 研究成功达成了研究目标，为敏捷方法在DevOps实践中的整合提供了理论支持和实践指导，有助于提升软件交付效率和质量。

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting](https://arxiv.org/abs/2508.21516)
*Federico Chiariotti,Fabio Saggese,Andrea Munari,Leonardo Badia,Petar Popovski*

Main category: cs.NI

TL;DR: 这篇论文提出了一种推-拉更新调度器(PPS)中经访问框架，用于动态分配数字双胞中拉取更新和推送更新的通信资源，在保持异常检测保证的同时优化了数字双胞的对齐精度和资源使用。


<details>
  <summary>Details</summary>
Motivation: 数字双胞的准确性依赖于与物理系统的及时同步更新。拉取更新和推送更新有不同的特性和优先级，需要一种动态资源分配机制来平衡正常条件下的对齐精度和异常报告的需求。

Method: 设计了推-拉更新调度器(PPS)中经访问框架，动态分配通信资源给两种类型的更新：拉取更新(降低偏移)和推送更新(紧急异常信息)。

Result: 该方案在保持相同异常检测保证的前提下，将错误信息的偏移年龄(AoII)优化了20%以上，并将最坏情况下的异常检测AoII从70ms降低到20ms(在1ms平均偏移AoII约束下)。

Conclusion: PPS框架能够有效地平衡数字双胞的正常对齐需求和异常检测需求，显著提升了系统性能和资源利用效率，为数字双胞的实时同步更新提供了有效的解决方案。

Abstract: A digital twin (DT) contains a set of virtual models of real systems and
processes that are synchronized to their physical counterparts. This enables
experimentation and examination of counterfactuals, simulating the consequences
of decisions in real time. However, the DT accuracy relies on timely updates
that maintain alignment with the real system. We can distinguish between: (i)
pull-updates, which follow a request from the DT to the sensors, to decrease
its drift from the physical state; (ii) push-updates, which are sent directly
by the sensors since they represent urgent information, such as anomalies. In
this work, we devise a push-pull scheduler (PPS) medium access framework, which
dynamically allocates the communication resources used for these two types of
updates. Our scheme strikes a balance in the trade-off between DT alignment in
normal conditions and anomaly reporting, optimizing resource usage and reducing
the drift age of incorrect information (AoII) by over 20% with respect to
state-of-the-art solutions, while maintaining the same anomaly detection
guarantees, as well as reducing the worst-case anomaly detection AoII from 70
ms to 20 ms when considering a 1 ms average drift AoII constraint.

</details>


### [21] [QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective](https://arxiv.org/abs/2508.21783)
*Mohamed Seliem,Utz Roedig,Cormac Sreenan,Dirk Pesch*

Main category: cs.NI

TL;DR: 本文扩展了Simu5G仿真框架，支持QFI级别的多流建模，并提出了QoS-PF调度器来优化5G智能工厂中的异构流量资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架缺乏在QoS Flow Identifier级别对多流行为的高保真建模能力，无法满足智能工厂中设备处理多种并发流量流的QoS需求。

Method: 扩展Simu5G支持每QFI建模，引入新颖的QoS感知比例公平调度器(QoS-PF)，动态平衡延迟、保证比特率和优先级指标。

Result: 在包含边缘机器视觉、实时控制回路和批量数据传输的智能工厂场景中，QoS-PF提高了截止时间遵守率和公平性，同时不牺牲吞吐量。

Conclusion: 该工作为工业5G部署中高级QoS策略的仿真和分析提供了方法论和架构基础，所有扩展都以模块化和开源方式实现以支持未来研究。

Abstract: Private 5G networks are emerging as key enablers for smart factories, where a
single device often handles multiple concurrent traffic flows with distinct
Quality of Service (QoS) requirements. Existing simulation frameworks, however,
lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier
(QFI) level. This paper addresses this gap by extending Simu5G to support
per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness
(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit
Rate (GBR), and priority metrics to optimize resource allocation across
heterogeneous flows. We evaluate the proposed approach in a realistic smart
factory scenario featuring edge-hosted machine vision, real-time control loops,
and bulk data transfer. Results show that QoS-PF improves deadline adherence
and fairness without compromising throughput. All extensions are implemented in
a modular and open-source manner to support future research. Our work provides
both a methodological and architectural foundation for simulating and analyzing
advanced QoS policies in industrial 5G deployments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 提出了一种混合字符串相似度、主题建模、层次聚类和基于规则的流水线方法，用于聚类银行支付系统中的交易对手方，解决了自然语言模型不适用于短文本聚类的问题。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言处理模型不适用于银行支付消息系统（如SWIFT）中的交易对手方聚类，因为这些文本缺乏句子结构但包含大量人工输入带来的变异和噪声，现有模糊匹配工具效果有限。

Method: 采用混合方法：结合字符串相似度计算、主题建模、层次聚类和基于规则的流水线，支持未知聚类数量的场景，并设计了基于精确率和召回率的评估指标。

Result: 在真实标注数据集上的测试显示，该方法相比基于规则的基线方法性能显著提升，同时保持了规则系统的可解释性，减少了人工审核需求。

Conclusion: 该方法有效填补了交易对手方聚类的技术空白，特别适用于制裁调查等场景，能更好地控制遗漏实体变体的风险，提高支付流分析的效率和准确性。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [23] [Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI](https://arxiv.org/abs/2508.21101)
*Dilruk Perera,Gousia Habib,Qianyi Xu,Daniel J. Tan,Kai He,Erik Cambria,Mengling Feng*

Main category: cs.LG

TL;DR: 这篇论文综述了强化学习在医疗保健领域的应用，探讨了RL从预测工具向主动决策智能的转变，分析了技术方法、应用场景以及伦理部署挑战。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI主要关注预测结果，而强化学习能够主动制定干预决策并优化长期目标，为医疗保健带来革命性可能性和新风险。论文旨在全面分析RL在医疗领域的应用现状、技术挑战和发展前景。

Method: 采用系统性综述方法，首先构建RL技术框架（包括基于模型和无模型方法、离线学习等），然后全面分析在重症监护、慢性病、心理健康等领域的应用，并批判性分析伦理和部署挑战。

Result: 论文系统梳理了RL在医疗保健的多源信息融合机制、不同架构部署方式，识别了各应用领域的趋势、差距和转化瓶颈，提出了安全、人本对齐的策略学习经验。

Conclusion: 强化学习代表了医疗AI从预测机器向主动临床智能的根本转变，需要重点关注伦理、奖励设计和安全部署等挑战，为医疗保健领域的智能化决策提供了技术路线和批判性思考。

Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

</details>


### [24] [Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning](https://arxiv.org/abs/2508.21103)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: 提出基于GAMEEMO数据集的统一多粒度EEG情绪分类框架，使用LSTM-GRU模型在二元效价分类中达到0.932 F1分数，在多类和多标签分类中分别达到94.5%和90.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有EEG情绪识别研究大多局限于二元效价预测或特定被试分类，限制了在真实情感计算系统中的通用性和部署能力。

Method: 采用结构化预处理策略（时间窗分割、混合统计和频域特征提取、z-score归一化），构建三种情绪标签编码方式（二元效价、多类分类、多标签表示），评估多种机器学习模型和深度学习架构。

Result: LSTM-GRU模型表现最佳，在二元效价任务中F1分数达0.932，在多类和多标签情绪分类中分别达到94.5%和90.6%的准确率。

Conclusion: 该多粒度框架显著提升了EEG情绪分类性能，为真实情感计算应用提供了有效的解决方案。

Abstract: Recent advancements in EEG-based emotion recognition have shown promising
outcomes using both deep learning and classical machine learning approaches;
however, most existing studies focus narrowly on binary valence prediction or
subject-specific classification, which limits generalizability and deployment
in real-world affective computing systems. To address this gap, this paper
presents a unified, multigranularity EEG emotion classification framework built
on the GAMEEMO dataset, which consists of 14-channel EEG recordings and
continuous self-reported emotion ratings (boring, horrible, calm, and funny)
from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline
employs a structured preprocessing strategy that comprises temporal window
segmentation, hybrid statistical and frequency-domain feature extraction, and
z-score normalization to convert raw EEG signals into robust, discriminative
input vectors. Emotion labels are derived and encoded across three
complementary axes: (i) binary valence classification based on the averaged
polarity of positive and negative emotion ratings, and (ii) Multi-class emotion
classification, where the presence of the most affective state is predicted.
(iii) Fine-grained multi-label representation via binning each emotion into 10
ordinal classes. We evaluate a broad spectrum of models, including Random
Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,
LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently
outperforms the others, achieving an F1-score of 0.932 in the binary valence
task and 94.5% and 90.6% in both multi-class and Multi-Label emotion
classification.

</details>


### [25] [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)
*Wenfeng Feng,Penghong Zhao,Guochao Jiang,Chuzhan Hao,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: PVPO是一种通过优势参考锚点和数据预采样增强的高效强化学习方法，解决了无评论家方法中的局部最优和高计算成本问题


<details>
  <summary>Details</summary>
Motivation: 传统无评论家强化学习方法依赖策略内多次采样和比较来估计优势值，容易陷入局部最优且计算成本高昂

Method: 使用参考模型提前进行rollout计算奖励分数作为参考锚点，通过数据预采样评估样本难度并选择高增益数据

Result: 在9个数据集的两个领域中实现了SOTA性能，展现出强大的跨任务泛化能力和不同规模模型的可扩展性能

Conclusion: PVPO方法有效纠正了组内比较引入的累积偏差，显著减少了对rollout次数的依赖，提高了训练效率

Abstract: Critic-free reinforcement learning methods, particularly group policies, have
attracted considerable attention for their efficiency in complex tasks.
However, these methods rely heavily on multiple sampling and comparisons within
the policy to estimate advantage, which may cause the policy to fall into local
optimum and increase computational cost. To address these issues, we propose
PVPO, an efficient reinforcement learning method enhanced by an advantage
reference anchor and data pre-sampling. Specifically, we use the reference
model to rollout in advance and employ the calculated reward score as a
reference anchor. Our approach effectively corrects the cumulative bias
introduced by intra-group comparisons and significantly reduces reliance on the
number of rollouts. Meanwhile, the reference model can assess sample difficulty
during data pre-sampling, enabling effective selection of high-gain data to
improve training efficiency. Experiments conducted on nine datasets across two
domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our
approach not only demonstrates robust generalization across multiple tasks, but
also exhibits scalable performance across models of varying scales.

</details>


### [26] [Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models](https://arxiv.org/abs/2508.21106)
*Tatyana Matveeva,Aleksandr Katrutsa,Evgeny Frolov*

Main category: cs.LG

TL;DR: AdaGram是一种高效的全矩阵自适应梯度优化器，通过快速对称分解和低秩近似来降低计算和内存开销，在保持性能的同时实现比对角自适应方法更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应梯度方法（如Adagrad）使用对角预处理矩阵，无法捕捉参数相关性。全矩阵自适应方法虽然能建模这些相关性，但计算和内存成本过高，难以应用于大规模模型。

Method: 提出AdaGram优化器，采用快速对称分解计算预处理更新方向，并使用矩阵积分器方法保持预处理器的低秩结构，从而降低内存和计算开销。

Result: 在标准机器学习任务上的数值实验表明，AdaGram在使用秩5及更低秩近似时，收敛速度更快或与对角自适应优化器性能相当。

Conclusion: AdaGram为大规模模型中的自适应优化提供了一个可扩展的解决方案，能够在保持效率的同时实现更好的收敛性能。

Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in
large-scale optimization. However, their use of diagonal preconditioning
matrices limits the ability to capture parameter correlations. Full-matrix
adaptive methods, approximating the exact Hessian, can model these correlations
and may enable faster convergence. At the same time, their computational and
memory costs are often prohibitive for large-scale models. To address this
limitation, we propose AdaGram, an optimizer that enables efficient full-matrix
adaptive gradient updates. To reduce memory and computational overhead, we
utilize fast symmetric factorization for computing the preconditioned update
direction at each iteration. Additionally, we maintain the low-rank structure
of a preconditioner along the optimization trajectory using matrix integrator
methods. Numerical experiments on standard machine learning tasks show that
AdaGram converges faster or matches the performance of diagonal adaptive
optimizers when using rank five and smaller rank approximations. This
demonstrates AdaGram's potential as a scalable solution for adaptive
optimization in large models.

</details>


### [27] [An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity](https://arxiv.org/abs/2508.21109)
*Georgios Vamvouras,Konstantinos Braimakis,Christos Tzivanidis*

Main category: cs.LG

TL;DR: 本文提出了一个基于深度学习的48小时温度、太阳辐照度和相对湿度预测框架，用于智能HVAC系统的模型预测控制，采用带注意力机制的堆叠双向LSTM网络，在多个气象变量预测上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为智能HVAC系统的模型预测控制提供准确可靠的短期气象预测，支持建筑节能控制，需要解决传统数值天气预报和机器学习方法在精度和可解释性方面的不足。

Method: 使用堆叠双向长短期记忆网络（BiLSTM）结合注意力机制，联合预测温度、太阳辐照度和相对湿度三个变量，利用2019-2022年历史气象数据训练，编码周期性时间特征，使用2023年数据评估泛化能力。

Result: 模型在温度预测上达到1.3°C的平均绝对误差，太阳辐照度31 W/m²，相对湿度6.7个百分点，优于最先进的数值天气预报和机器学习基准方法。通过积分梯度和注意力权重分析增强了模型的可解释性。

Conclusion: 该框架通过结合多变量预测、基于注意力的深度学习和可解释性分析，推进了数据驱动的天气预测技术，其准确性和透明度展示了在节能建筑控制中的应用潜力。

Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of
temperature, solar irradiance, and relative humidity to support Model
Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked
Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing
temporal and cross-feature dependencies by jointly predicting all three
variables. Historical meteorological data (2019-2022) with encoded cyclical
time features were used for training, while 2023 data evaluated generalization.
The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),
31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming
state-of-the-art numerical weather prediction and machine learning benchmarks.
Integrated Gradients quantified feature contributions, and attention weights
revealed temporal patterns, enhancing interpretability. By combining
multivariate forecasting, attention-based DL, and explainability, this work
advances data-driven weather prediction. The demonstrated accuracy and
transparency highlight the framework's potential for energy-efficient building
control through reliable short-term meteorological forecasting.

</details>


### [28] [Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI](https://arxiv.org/abs/2508.21111)
*Evan J. Chou,Lisa S. Locke,Harvey M. Soldan*

Main category: cs.LG

TL;DR: 该研究开发了一个基于机器学习和强化学习的异常检测系统，用于监测NASA深空网络(DSN)设备的退化问题，通过预测分析和统计计算来识别实时数据中的异常，并结合大语言模型提供解释。


<details>
  <summary>Details</summary>
Motivation: NASA深空网络的设备会随时间退化，可能导致数据流中断和影响数十个航天器的地球连接，需要开发方法来帮助工程师直接识别异常和设备退化问题。

Method: 使用机器学习技术进行数据重建和预测分析，结合强化学习子系统对异常进行分类，集成大语言模型为异常提供解释，并构建完整的数据管道系统连接数据提取、解析和处理流程。

Result: 成功实现了完整的DSN异常检测数据工作流，包括训练好的机器学习模型、强化学习分类系统和AI代理系统进行复杂推理。

Conclusion: 该系统能够有效监测DSN设备的退化问题，通过自动化异常检测和解释功能，为未来太空任务的维护和操作提供支持，并且可以通过人工反馈持续改进。

Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities
that generate a large volume of multivariate time-series data. These facilities
contain DSN antennas and transmitters that undergo degradation over long
periods of time, which may cause costly disruptions to the data flow and
threaten the earth-connection of dozens of spacecraft that rely on the Deep
Space Network for their lifeline. The purpose of this study was to experiment
with different methods that would be able to assist JPL engineers with directly
pinpointing anomalies and equipment degradation through collected data, and
continue conducting maintenance and operations of the DSN for future space
missions around our universe. As such, we have researched various machine
learning techniques that can fully reconstruct data through predictive
analysis, and determine anomalous data entries within real-time datasets
through statistical computations and thresholds. On top of the fully trained
and tested machine learning models, we have also integrated the use of a
reinforcement learning subsystem that classifies identified anomalies based on
severity level and a Large Language Model that labels an explanation for each
anomalous data entry, all of which can be improved and fine-tuned over time
through human feedback/input. Specifically, for the DSN transmitters, we have
also implemented a full data pipeline system that connects the data extraction,
parsing, and processing workflow all together as there was no coherent program
or script for performing these tasks before. Using this data pipeline system,
we were able to then also connect the models trained from DSN antenna data,
completing the data workflow for DSN anomaly detection. This was all wrapped
around and further connected by an agentic AI system, where complex reasoning
was utilized to determine the classifications and predictions of anomalous
data.

</details>


### [29] [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)
*Pranoy Panda,Raghav Magazine,Chaitanya Devaguptapu,Sho Takemori,Vishal Sharma*

Main category: cs.LG

TL;DR: 将LLM路由问题建模为上下文多臂老虎机问题，提出PILOT方法通过共享嵌入空间和在线学习实现自适应路由选择


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法需要完整的查询-LLM最优配对信息，但现实场景缺乏这种完整映射且查询不断变化，需要更自适应的路由方案

Method: 构建查询和LLM的共享嵌入空间反映亲和性，基于离线人类偏好数据初始化，通过在线老虎机反馈优化。提出PILOT算法（基于LinUCB的扩展）和在线成本策略处理预算约束

Result: 提出了能够自适应选择最适合LLM的路由框架，无需对所有查询在所有LLM上进行完整推理

Conclusion: 将LLM路由建模为上下文老虎机问题是一种有前景的方法，能够实现资源高效的自适应路由决策

Abstract: Large Language Models (LLMs) have revolutionized natural language processing,
but their varying capabilities and costs pose challenges in practical
applications. LLM routing addresses this by dynamically selecting the most
suitable LLM for each query/task. Previous approaches treat this as a
supervised learning problem, assuming complete knowledge of optimal query-LLM
pairings. However, real-world scenarios lack such comprehensive mappings and
face evolving user queries. We thus propose to study LLM routing as a
contextual bandit problem, enabling adaptive decision-making using bandit
feedback without requiring exhaustive inference across all LLMs for all queries
(in contrast to supervised routing). To address this problem, we develop a
shared embedding space for queries and LLMs, where query and LLM embeddings are
aligned to reflect their affinity. This space is initially learned from offline
human preference data and refined through online bandit feedback. We
instantiate this idea through Preference-prior Informed Linucb fOr adaptive
rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets
for model routing, we introduce an online cost policy modeled as a multi-choice
knapsack problem, ensuring resource-efficient routing.

</details>


### [30] [Privacy Auditing Synthetic Data Release through Local Likelihood Attacks](https://arxiv.org/abs/2508.21146)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文提出Gen-LRA，一种无需模型知识或访问权限的成员推理攻击方法，专门针对表格生成模型过度拟合训练数据特定区域的问题，通过评估测试观测对替代模型局部似然比估计的影响来进行隐私审计。


<details>
  <summary>Details</summary>
Motivation: 现有的合成数据隐私审计框架大多依赖启发式方法和不合理假设，对生成模型失败模式的攻击能力有限，难以有效检测通过合成数据发布导致的训练数据隐私泄露问题。

Method: 提出Generative Likelihood Ratio Attack (Gen-LRA)，这是一种计算高效的No-Box成员推理攻击方法，无需模型知识或访问权限，通过评估测试观测在替代模型对合成数据局部似然比估计中的影响来构建攻击。

Result: 在涵盖多样化数据集、模型架构和攻击参数的综合基准测试中，Gen-LRA在多个性能指标上始终优于其他生成模型成员推理攻击方法。

Conclusion: Gen-LRA作为合成数据发布的隐私审计工具具有显著效果，突显了生成模型过度拟合在现实应用中带来的重大隐私风险。

Abstract: Auditing the privacy leakage of synthetic data is an important but unresolved
problem. Most existing privacy auditing frameworks for synthetic data rely on
heuristics and unreasonable assumptions to attack the failure modes of
generative models, exhibiting limited capability to describe and detect the
privacy exposure of training data through synthetic data release. In this
paper, we study designing Membership Inference Attacks (MIAs) that specifically
exploit the observation that tabular generative models tend to significantly
overfit to certain regions of the training distribution. Here, we propose
Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally
efficient No-Box MIA that, with no assumption of model knowledge or access,
formulates its attack by evaluating the influence a test observation has in a
surrogate model's estimation of a local likelihood ratio over the synthetic
data. Assessed over a comprehensive benchmark spanning diverse datasets, model
architectures, and attack parameters, we find that Gen-LRA consistently
dominates other MIAs for generative models across multiple performance metrics.
These results underscore Gen-LRA's effectiveness as a privacy auditing tool for
the release of synthetic data, highlighting the significant privacy risks posed
by generative model overfitting in real-world applications.

</details>


### [31] [Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks](https://arxiv.org/abs/2508.21172)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种基于时间残差连接的深度未训练循环神经网络DeepResESN，通过层次化的残差循环层显著提升了记忆能力和长期时间建模性能。


<details>
  <summary>Details</summary>
Motivation: 传统的回声状态网络(ESN)在长期信息处理方面存在困难，需要改进以提升其记忆容量和时间建模能力。

Method: 采用时间残差连接构建深度未训练循环神经网络，研究了随机生成和固定结构的正交配置，并进行了数学稳定性分析。

Result: 在多种时间序列任务上的实验表明，该方法优于传统的浅层和深度储备池计算模型。

Conclusion: DeepResESN通过残差连接有效解决了传统ESN的长期信息处理限制，提供了更强大的时间序列建模能力。

Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent
Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular
for their fast and efficient learning. However, traditional ESNs often struggle
with long-term information processing. In this paper, we introduce a novel
class of deep untrained RNNs based on temporal residual connections, called
Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a
hierarchy of untrained residual recurrent layers significantly boosts memory
capacity and long-term temporal modeling. For the temporal residual
connections, we consider different orthogonal configurations, including
randomly generated and fixed-structure configurations, and we study their
effect on network dynamics. A thorough mathematical analysis outlines necessary
and sufficient conditions to ensure stable dynamics within DeepResESN. Our
experiments on a variety of time series tasks showcase the advantages of the
proposed approach over traditional shallow and deep RC.

</details>


### [32] [FUTURE: Flexible Unlearning for Tree Ensemble](https://arxiv.org/abs/2508.21181)
*Ziheng Chen,Jin Huang,Jiali Cheng,Yuchan Guo,Mengjie Wang,Lalitesh Morishetti,Kaushiki Nag,Hadi Amiri*

Main category: cs.LG

TL;DR: FUTURE是一种新的树集成遗忘算法，通过梯度优化框架解决现有方法难以泛化和效率低的问题


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和"被遗忘权"日益重要，现有树集成遗忘方法存在模型特定依赖和离散结构限制，难以处理复杂集成和大规模数据集

Method: 将遗忘样本问题转化为基于梯度的优化任务，采用概率模型近似来处理树集成的不可微分性，实现端到端的高效遗忘

Result: 在真实数据集上的广泛实验表明，FUTURE能够实现显著且成功的遗忘性能

Conclusion: FUTURE为树集成提供了一种有效且高效的遗忘算法，解决了现有方法的局限性

Abstract: Tree ensembles are widely recognized for their effectiveness in
classification tasks, achieving state-of-the-art performance across diverse
domains, including bioinformatics, finance, and medical diagnosis. With
increasing emphasis on data privacy and the \textit{right to be forgotten},
several unlearning algorithms have been proposed to enable tree ensembles to
forget sensitive information. However, existing methods are often tailored to a
particular model or rely on the discrete tree structure, making them difficult
to generalize to complex ensembles and inefficient for large-scale datasets. To
address these limitations, we propose FUTURE, a novel unlearning algorithm for
tree ensembles. Specifically, we formulate the problem of forgetting samples as
a gradient-based optimization task. In order to accommodate
non-differentiability of tree ensembles, we adopt the probabilistic model
approximations within the optimization framework. This enables end-to-end
unlearning in an effective and efficient manner. Extensive experiments on
real-world datasets show that FUTURE yields significant and successful
unlearning performance.

</details>


### [33] [Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium](https://arxiv.org/abs/2508.21186)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 该论文将大语言模型中的解码过程形式化为概率单纯形上的约束变分原理，证明了下一个token分布沿着平滑轨迹收敛到softmax均衡，并分析了温度、top-k和nucleus采样等实践参数的理论性质。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型解码过程通常被描述为token评分和softmax归一化，但缺乏严格的数学形式化。作者希望从变分原理的角度为这一过程提供最小化、自包含的理论框架。

Method: 将解码过程建模为概率单纯形上的约束变分原理，使用乘法权重（熵镜像）更新和复制器流分析离散和连续时间动态，证明收敛性并分析实践参数的理论性质。

Result: 证明了在固定上下文和温度下，下一个token分布沿着平滑轨迹收敛到softmax均衡；温度作为时间重缩放参数；top-k和nucleus采样将流限制在具有相同保证的面内。

Conclusion: 该工作为LLM解码提供了严格的数学框架，形式化了"流形遍历"直觉，并得出了面向实践的精确理论结果，为理解路径依赖的分数调整和幻觉行为奠定了基础。

Abstract: Decoding in large language models is often described as scoring tokens and
normalizing with softmax. We give a minimal, self-contained account of this
step as a constrained variational principle on the probability simplex. The
discrete, normalization-respecting ascent is the classical
multiplicative-weights (entropic mirror) update; its continuous-time limit is
the replicator flow. From these ingredients we prove that, for a fixed context
and temperature, the next-token distribution follows a smooth trajectory inside
the simplex and converges to the softmax equilibrium. This formalizes the
common ``manifold traversal'' intuition at the output-distribution level. The
analysis yields precise, practice-facing consequences: temperature acts as an
exact rescaling of time along the same trajectory, while top-k and nucleus
sampling restrict the flow to a face with identical guarantees. We also outline
a controlled account of path-dependent score adjustments and their connection
to loop-like, hallucination-style behavior. We make no claims about training
dynamics or internal representations; those are deferred to future work.

</details>


### [34] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 研究发现，大语言模型强化学习中的反直觉现象（如单样本训练效果、不精确奖励信号、负样本训练等）仅在模型与任务高度对齐时有效，而在更具挑战性的场景中，传统RL方法仍更稳健有效。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现LLM在RL训练中出现了一系列反直觉现象，但这些现象的有效条件和失败场景尚不明确，需要系统研究其背后的关键因素。

Method: 通过系统性的实验验证，在不同模型架构和任务领域检验各种反直觉主张，重点关注模型-任务对齐程度（用pass@k准确率衡量）对结果的影响。

Result: 研究表明，许多反直觉的RL结果只在模型与任务已经高度对齐时出现；在更具挑战性的场景中，这些技术无法驱动实质性学习，而标准RL方法仍然有效。

Conclusion: 模型-任务对齐程度是区分RL观察结果的关键因素，标准RL训练在各种设置下都保持稳健，而反直觉技术仅在特定对齐条件下有效。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [35] [Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay](https://arxiv.org/abs/2508.21240)
*Pujan Thapa,Alexander Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 基于自组织地图(SOM)和变分自动编码器(VAE)的新题生成式持续学习框架，无需存储原始数据或任务标签，通过SOM单元的统计信息生成合成样本进行回放学习。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的恒永学习问题，避免存储大量原始数据样本或依赖任务标签信息，提供内存效率高的解决方案。

Method: 采用SOM和VAE结合的方法：高维数据通过VAE学习潜在空间后用SOM处理，低维数据直接用SOM处理。对每个SOM单元存储运行均值、方差和协方差，从这些统计信息生成合成样本用于后续回放学习。

Result: 在CIFAR-10和CIFAR-100标准类增量测试中，方法表现竞争力强，超过最佳无内存方法，并在CIFAR-10和CIFAR-100上将单类增量性能分别提高近10%和7%。方法还支持学习过程可视化和训练后作为生成模型使用。

Conclusion: 该方法提供了一种可扩展、无需任务标签、内存效率高的持续学习解决方案，在保持竞争性能的同时大大减少了内存占用。

Abstract: This work introduces a novel generative continual learning framework based on
self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable
memory-efficient replay, eliminating the need to store raw data samples or task
labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100,
we design a scheme where the SOM operates over the latent space learned by a
VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and
FashionMNIST, the SOM operates in a standalone fashion. Our method stores a
running mean, variance, and covariance for each SOM unit, from which synthetic
samples are then generated during future learning iterations. For the VAE-based
method, generated samples are then fed through the decoder to then be used in
subsequent replay. Experimental results on standard class-incremental
benchmarks show that our approach performs competitively with state-of-the-art
memory-based methods and outperforms memory-free methods, notably improving
over best state-of-the-art single class incremental performance on CIFAR-10 and
CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further
facilitates easy visualization of the learning process and can also be utilized
as a generative model post-training. Results show our method's capability as a
scalable, task-label-free, and memory-efficient solution for continual
learning.

</details>


### [36] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出一种基于混合专家(MoE)的元学习框架，动态整合三种先进CFD代理模型的预测，在汽车空气动力学模拟中显著降低预测误差


<details>
  <summary>Details</summary>
Motivation: 高保真CFD模拟计算成本高，现有ML代理模型架构多样但无单一最优方案，需要利用架构多样性提升预测精度

Method: 使用门控网络动态组合DoMINO、X-MeshGraphNet和FigConvNet三种异构代理模型，通过熵正则化防止模型坍塌，在DrivAerML数据集上训练验证

Result: MoE模型在所有评估物理量上均显著降低L-2预测误差，优于集成平均和最准确的单个专家模型

Conclusion: MoE框架通过协同整合专业架构的互补优势，为创建更鲁棒准确的复合代理模型提供了有效策略

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [37] [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
*Farnoush Rezaei Jafari,Oliver Eberle,Ashkan Khakzar,Neel Nanda*

Main category: cs.LG

TL;DR: Relevance Patching (RelP) 是一种新的机制解释性方法，使用层间相关性传播(LRP)系数替代梯度，在保持计算效率的同时比标准归因修补更准确地近似激活修补。


<details>
  <summary>Details</summary>
Motivation: 激活修补是机制解释性的标准方法但计算昂贵，归因修补虽然更快但存在噪声和可靠性问题，特别是在深度非线性网络中。

Method: 使用Layer-wise Relevance Propagation (LRP)的传播系数替代归因修补中的局部梯度，保持两次前向传播和一次反向传播的计算效率。

Result: 在多个模型和任务上验证显示，RelP比标准归因修补更准确地近似激活修补，特别是在GPT-2 Large的IOI任务中，相关系数从0.006提升到0.956。

Conclusion: RelP在保持计算效率的同时显著提升了修补的准确性，为大规模机制解释性分析提供了更可靠的替代方案。

Abstract: Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.

</details>


### [38] [Owen Sampling Accelerates Contribution Estimation in Federated Learning](https://arxiv.org/abs/2508.21261)
*Hossein KhademSohi,Hadi Hemmati,Jiayu Zhou,Steve Drew*

Main category: cs.LG

TL;DR: FedOwen是一个高效的联邦学习框架，使用Owen采样近似Shapley值来评估客户端贡献，在相同计算预算下比现有方法获得更高精度


<details>
  <summary>Details</summary>
Motivation: 联邦学习中准确评估客户端贡献对于公平奖励和选择有用客户端至关重要，但精确计算Shapley值在客户端数量大时计算复杂度呈指数增长

Method: 提出FedOwen框架，使用Owen采样近似Shapley值，采用自适应客户端选择策略平衡利用高价值客户端和探索未充分采样客户端

Result: 在相同计算预算下，FedOwen在非IID基准测试中比最先进基线方法获得高达23%的最终精度提升

Conclusion: FedOwen通过高效的Shapley值近似和自适应客户端选择策略，显著提升了联邦学习的性能和收敛速度

Abstract: Federated Learning (FL) aggregates information from multiple clients to train
a shared global model without exposing raw data. Accurately estimating each
client's contribution is essential not just for fair rewards, but for selecting
the most useful clients so the global model converges faster. The Shapley value
is a principled choice, yet exact computation scales exponentially with the
number of clients, making it infeasible for large federations. We propose
FedOwen, an efficient framework that uses Owen sampling to approximate Shapley
values under the same total evaluation budget as existing methods while keeping
the approximation error small. In addition, FedOwen uses an adaptive client
selection strategy that balances exploiting high-value clients with exploring
under-sampled ones, reducing bias and uncovering rare but informative data.
Under a fixed valuation cost, FedOwen achieves up to 23 percent higher final
accuracy within the same number of communication rounds compared to
state-of-the-art baselines on non-IID benchmarks.

</details>


### [39] [Guess-and-Learn (G&L): Measuring the Cumulative Error Cost of Cold-Start Adaptation](https://arxiv.org/abs/2508.21270)
*Roland Arnold*

Main category: cs.LG

TL;DR: G&L v1.0提出了一个评估机器学习模型冷启动适应性的新框架，通过测量模型在顺序标注未标记数据集时累积的错误数量，来评估模型的早期学习效率，而不仅仅是最终准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型评估过于强调最终准确率，而忽视了模型从零开始学习过程中的适应成本（累积错误）。G&L旨在填补这一空白，量化模型在早期学习阶段的错误成本。

Method: 设计了四个评估轨道（从零开始/预训练 × 在线/批量更新），通过序列化标注过程：模型选择实例、预测标签、接收真实标签并更新参数。使用MNIST和AG News数据集测试了感知机、k-NN、CNN、ResNet-50、ViT-B/16、BERT-base等多种模型。

Result: 实验发现小模型在早期阶段能以更少的初始错误适应，预训练的好处因领域而异。所有当前模型的表现都远高于oracle参考带，显示出明显的适应性差距。

Conclusion: G&L框架通过量化早期学习的错误成本，补充了传统基准测试，为开发既能在极限情况下准确又能从第一个样本开始就可靠的学习者提供了可复现的评估框架。

Abstract: Evaluation of machine learning models typically emphasizes final accuracy,
overlooking the cost of adaptation: the cumulative errors incurred while
learning from scratch. Guess-and- Learn (G&L) v1.0 addresses this gap by
measuring cold-start adaptability - the total mistakes a model makes while
sequentially labeling an unlabeled dataset. At each step, the learner selects
an instance, predicts its label, receives the ground truth, and updates
parameters under either online (per-sample) or batch (delayed) mode. The
resulting error trajectory exposes adaptation speed, selection quality, and
bias - dynamics invisible to endpoint metrics.
  G&L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) to
disentangle the effects of initialization and update frequency. We formalize
the protocol, relate it to classical mistake-bound theory, and estimate a
heuristic "oracle reference band" for MNIST as a plausibility reference.
Baseline experiments on MNIST and AG News, spanning classical methods
(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), and
pretrained transformers (ViT-B/16, BERT-base), reveal systematic differences in
early-phase efficiency: smaller models can adapt with fewer initial errors,
while pretraining benefits vary by domain. Across settings, current models
remain well above the oracle band, highlighting an adaptability gap.
  By quantifying the mistake cost of early learning, G&L complements
conventional benchmarks and provides a reproducible framework for developing
learners that are not only accurate in the limit but also reliable from the
first examples.

</details>


### [40] [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)
*Ashok Devireddy,Shunping Huang*

Main category: cs.LG

TL;DR: CALM框架：基于Apache Beam和TimesFm的实时异常检测系统，通过持续微调和LLM判断机制应对概念漂移问题


<details>
  <summary>Details</summary>
Motivation: 传统离线训练的异常检测模型在面对数据统计特性随时间变化（概念漂移）时性能显著下降，需要实时自适应解决方案

Method: 构建基于Apache Beam分布式处理框架的端到端系统，使用TimesFm基础模型进行预测式异常检测，包含持续微调机制和LLM-as-a-Judge组件进行语义判断

Result: 在TSB-UAD基准测试中，持续微调模型相比静态预训练基础模型在大多数数据集上提升了ROC AUC分数

Conclusion: CALM框架通过自适应和LLM引导的方法，在动态流式环境中有效维持高性能异常检测

Abstract: The detection of anomalies in non-stationary time-series streams is a
critical but challenging task across numerous industrial and scientific
domains. Traditional models, trained offline, suffer significant performance
degradation when faced with concept drift, where the underlying statistical
properties of the data change over time. This paper introduces CALM
(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for
real-time anomaly detection designed to address this challenge. CALM is built
on the Apache Beam distributed processing framework and leverages the TimesFm
foundation model for forecasting-based anomaly detection. The framework's
novelty lies in two core contributions. First, it implements a closed-loop,
continuous fine-tuning mechanism that allows the anomaly detection model to
adapt to evolving data patterns in near real-time. Second, it introduces an
LLM-as-a-Judge component, a Large Language Model that provides semantic,
context-aware judgments on detected anomalies to curate a high-quality training
dataset, deciding whether an anomaly represents transient noise or a meaningful
pattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Our
results demonstrate that the continuously fine-tuned model improves the ROC AUC
score in most datasets compared to the static, pre-trained base model,
validating the efficacy of our adaptive, LLM-guided approach to maintaining
high-performance anomaly detection in dynamic streaming environments.

</details>


### [41] [Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning](https://arxiv.org/abs/2508.21278)
*Yibin Sun,Nick Lim,Guilherme Weigert Cassales,Heitor Murilo Gomes,Bernhard Pfahringer,Albert Bifet,Anany Dwivedi*

Main category: cs.LG

TL;DR: 本文探索使用数据流学习技术检测肌电信号中的域偏移，通过KPCA预处理和多种漂移检测方法评估，发现现有技术在实时检测方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 肌电信号具有固有的非平稳性，检测域偏移对于维持稳定的肌电解码模型至关重要，特别是在不同受试者和记录会话之间的域变化。

Method: 使用KPCA（余弦核）预处理数据，定义基于不同受试者和记录会话的时间序列段作为域，评估CUSUM、Page-Hinckley和ADWIN等多种漂移检测方法。

Result: 研究揭示了当前技术在实现肌电信号实时域偏移检测方面的高性能存在局限性，现有方法表现不佳。

Conclusion: 流式处理方法在维持稳定肌电解码模型方面具有潜力，但需要进一步研究来提高在真实场景中的鲁棒性和准确性。

Abstract: Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.

</details>


### [42] [MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems](https://arxiv.org/abs/2508.21296)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: MyGO是一个受生物睡眠-觉醒周期启发的终身学习框架，通过生成式记忆模型避免存储原始数据，在睡眠阶段使用伪数据进行知识蒸馏来缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有持续学习方法面临的数据隐私、存储限制以及任务不相似时性能下降等挑战，避免存储原始样本的需求。

Method: 采用觉醒-睡眠双阶段框架：觉醒阶段快速学习新任务并训练紧凑生成模型；睡眠阶段使用所有生成模型生成伪数据，通过知识蒸馏将新旧知识整合到核心特征提取器中。

Result: 在计算机视觉和自然语言处理基准测试中，MyGO显著减轻了灾难性遗忘，保持了跨任务的高平均准确率。

Conclusion: MyGO框架有效解决了持续学习中的关键问题，具有隐私保护、存储高效和领域通用性等优势。

Abstract: Continual or Lifelong Learning aims to develop models capable of acquiring
new knowledge from a sequence of tasks without catastrophically forgetting what
has been learned before. Existing approaches often rely on storing samples from
previous tasks (experience replay) or employing complex regularization terms to
protect learned weights. However, these methods face challenges related to data
privacy, storage limitations, and performance degradation when tasks are
dissimilar. To address these challenges, we introduce MyGO (Memory Yielding
Generative Offline-consolidation), a novel lifelong learning framework inspired
by the biological wake-sleep cycle. During the "wake" phase, the system rapidly
learns a new task and trains a compact generative model (Generative Memory,
G-mem) to capture its data distribution. During the "sleep" phase, the system
enters an offline state, using all learned G-mem models to generate pseudo-data
("dreams") and consolidate new and old knowledge into a core feature extractor
via knowledge distillation. This approach obviates the need to store any raw
data, retaining only compact generative models, which offers significant
advantages in privacy and storage efficiency. We evaluate MyGO on computer
vision (Split-MNIST) and natural language processing (Split-AG News)
benchmarks, comparing it against a sequential fine-tuning baseline. The results
demonstrate that MyGO significantly mitigates catastrophic forgetting and
maintains high average accuracy across tasks, proving the framework's
effectiveness and domain-generality.

</details>


### [43] [Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning](https://arxiv.org/abs/2508.21300)
*Yejin Kim,Eunwon Kim,Buru Chang,Junsuk Choe*

Main category: cs.LG

TL;DR: VILA是一个新的机器遗忘框架，通过改进参数识别方法，比现有方法FILA提高100倍参数效率和40倍训练速度，有效移除敏感信息而无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法FILA需要访问所有模型参数且未充分考虑Fisher信息的基本假设，导致参数重要性估计不准确和计算成本高。

Method: 提出VILA框架，明确考虑FILA忽略的假设，提高遗忘集参数识别准确性，同时无需访问整个模型即可进行参数识别。

Result: 在TOFU、WMDP和MUSE基准测试中达到最先进性能，相比FILA实现100倍参数效率和40倍训练速度提升。

Conclusion: VILA通过改进参数识别方法，显著降低了机器遗忘的计算成本，同时保持了高效的敏感信息移除能力。

Abstract: LLMs have demonstrated remarkable performance across various tasks but face
challenges related to unintentionally generating outputs containing sensitive
information. A straightforward approach to address this issue is to retrain the
model after excluding the problematic data. However, this approach incurs
prohibitively high computational costs. To overcome this limitation, machine
unlearning has emerged as a promising solution that can effectively remove
sensitive information without the need to retrain the model from scratch.
Recently, FILA has been proposed as a parameter-efficient unlearning method by
integrating LoRA adapters. Specifically, it calculates the Fisher information
to identify parameters associated with the forget set and assigns them to LoRA
adapters for updates. Despite its innovative approach, FILA still requires
access to all model parameters and does not adequately account for fundamental
assumptions underlying Fisher information, leading to inaccuracies in
importance estimation. To address these limitations, we propose VILA, a novel
unlearning framework that explicitly considers the assumptions overlooked in
FILA, thereby enhancing the accuracy of parameter identification for the forget
set. Moreover, VILA significantly reduces computational costs by enabling
parameter identification without accessing the entire model. Our method
achieves up to 100x higher parameter efficiency and 40x faster training speed
compared to FILA, and sets new state-of-the-art performance on benchmarks
including TOFU, WMDP, and MUSE. Our code is available at
https://github.com/kyj93790/VILA.

</details>


### [44] [Convergence of regularized agent-state-based Q-learning in POMDPs](https://arxiv.org/abs/2508.21314)
*Amit Sinha,Matthieu Geist,Aditya Mahajan*

Main category: cs.LG

TL;DR: 提出了一个理解Q学习算法收敛性的理论框架，分析了基于代理状态和策略正则化的Q学习算法(RASQL)，证明了其在温和技术条件下收敛到正则化MDP的固定点


<details>
  <summary>Details</summary>
Motivation: 实际应用中常用的Q学习算法具有两个显著特征：(i)使用非信念状态的代理状态递归更新Q表，(ii)使用策略正则化来鼓励探索和稳定学习算法。需要对这些算法的收敛性提供理论保证

Method: 提出了正则化代理状态Q学习(RASQL)框架，分析了最简单的Q学习算法形式。通过理论分析证明算法收敛性，并扩展到学习周期性策略的变体

Result: 证明了RASQL在温和技术条件下收敛到适当定义的正则化MDP的固定点，该固定点取决于行为策略诱导的平稳分布。数值实验验证了理论收敛行为

Conclusion: 为实践中常用的基于代理状态和正则化的Q学习算法提供了收敛性理论保证，证明了它们收敛到正则化MDP的固定点，为实际应用提供了理论基础

Abstract: In this paper, we present a framework to understand the convergence of
commonly used Q-learning reinforcement learning algorithms in practice. Two
salient features of such algorithms are: (i)~the Q-table is recursively updated
using an agent state (such as the state of a recurrent neural network) which is
not a belief state or an information state and (ii)~policy regularization is
often used to encourage exploration and stabilize the learning algorithm. We
investigate the simplest form of such Q-learning algorithms which we call
regularized agent-state-based Q-learning (RASQL) and show that it converges
under mild technical conditions to the fixed point of an appropriately defined
regularized MDP, which depends on the stationary distribution induced by the
behavioral policy. We also show that a similar analysis continues to work for a
variant of RASQL that learns periodic policies. We present numerical examples
to illustrate that the empirical convergence behavior matches with the proposed
theoretical limit.

</details>


### [45] [Distribution-Aware Feature Selection for SAEs](https://arxiv.org/abs/2508.21324)
*Narmeen Oozeer,Nirmalendu Prakash,Michael Lan,Alice Rigg,Amirali Abdullah*

Main category: cs.LG

TL;DR: Sampled-SAE是一种改进的稀疏自编码器方法，通过在批次级别进行特征选择来解决传统TopK SAE的效率问题，并通过可调节的参数l在全局一致性和细粒度重建之间找到平衡。


<details>
  <summary>Details</summary>
Motivation: 传统TopK SAE方法在处理不同信息量的token时效率低下，BatchTopK方法虽然提高了平均重建质量，但存在"激活彩票"问题，即罕见的高幅值特征会挤掉信息量更大但幅值较低的特征。

Method: 提出Sampled-SAE方法：通过对批次激活矩阵的列（特征）进行评分（使用L2范数或熵），形成大小为Kl的候选池，然后在受限的特征池中对批次中的token应用Top-K选择。参数l控制从批次级别到token特定选择的频谱。

Result: 在Pythia-160M模型上的实验表明，没有单一的l值能在所有指标上达到最优，最佳选择取决于共享结构、重建保真度和下游性能之间的权衡。

Conclusion: Sampled-SAE将BatchTopK重新构建为一个可调节的、分布感知的方法家族，提供了在全局一致性和细粒度重建之间的灵活权衡。

Abstract: Sparse autoencoders (SAEs) decompose neural activations into interpretable
features. A widely adopted variant, the TopK SAE, reconstructs each token from
its K most active latents. However, this approach is inefficient, as some
tokens carry more information than others. BatchTopK addresses this limitation
by selecting top activations across a batch of tokens. This improves average
reconstruction but risks an "activation lottery," where rare high-magnitude
features crowd out more informative but lower-magnitude ones. To address this
issue, we introduce Sampled-SAE: we score the columns (representing features)
of the batch activation matrix (via $L_2$ norm or entropy), forming a candidate
pool of size $Kl$, and then apply Top-$K$ to select tokens across the batch
from the restricted pool of features. Varying $l$ traces a spectrum between
batch-level and token-specific selection. At $l=1$, tokens draw only from $K$
globally influential features, while larger $l$ expands the pool toward
standard BatchTopK and more token-specific features across the batch. Small $l$
thus enforces global consistency; large $l$ favors fine-grained reconstruction.
On Pythia-160M, no single value optimizes $l$ across all metrics: the best
choice depends on the trade-off between shared structure, reconstruction
fidelity, and downstream performance. Sampled-SAE thus reframes BatchTopK as a
tunable, distribution-aware family.

</details>


### [46] [Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models](https://arxiv.org/abs/2508.21330)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: Stage-Diff是一个基于扩散模型的分阶段生成模型，专门针对具有长期时间依赖性和数据分布漂移的长时序数据生成问题，通过分阶段生成和渐进式序列分解来平衡长期依赖和分布变化。


<details>
  <summary>Details</summary>
Motivation: 长时序数据具有复杂的长程时间依赖性和随时间逐渐变化的数据分布，同时包含复杂的特征序列间相互关系。现有生成模型在处理这些挑战时存在困难，需要平衡长期依赖和分布漂移，并有效捕捉序列内和序列间的依赖关系。

Method: 提出Stage-Diff模型：1）通过分阶段序列生成和阶段间信息传递来保持长期序列依赖并建模数据分布漂移；2）在每个阶段内应用渐进式序列分解，在不同时间尺度上进行通道独立建模；3）阶段间信息传递采用多通道融合建模，结合通道独立建模的鲁棒性和多通道建模的信息融合优势。

Result: 在多个真实世界数据集上的广泛实验验证了Stage-Diff在长时序生成任务中的有效性。

Conclusion: Stage-Diff通过分阶段生成和渐进式分解的方法，成功解决了长时序数据生成中的长期依赖、分布漂移和序列间依赖关系等关键挑战，为长时序生成提供了有效的解决方案。

Abstract: Generative models have been successfully used in the field of time series
generation. However, when dealing with long-term time series, which span over
extended periods and exhibit more complex long-term temporal patterns, the task
of generation becomes significantly more challenging. Long-term time series
exhibit long-range temporal dependencies, but their data distribution also
undergoes gradual changes over time. Finding a balance between these long-term
dependencies and the drift in data distribution is a key challenge. On the
other hand, long-term time series contain more complex interrelationships
between different feature sequences, making the task of effectively capturing
both intra-sequence and inter-sequence dependencies another important
challenge. To address these issues, we propose Stage-Diff, a staged generative
model for long-term time series based on diffusion models. First, through
stage-wise sequence generation and inter-stage information transfer, the model
preserves long-term sequence dependencies while enabling the modeling of data
distribution shifts. Second, within each stage, progressive sequence
decomposition is applied to perform channel-independent modeling at different
time scales, while inter-stage information transfer utilizes multi-channel
fusion modeling. This approach combines the robustness of channel-independent
modeling with the information fusion advantages of multi-channel modeling,
effectively balancing the intra-sequence and inter-sequence dependencies of
long-term time series. Extensive experiments on multiple real-world datasets
validate the effectiveness of Stage-Diff in long-term time series generation
tasks.

</details>


### [47] [DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks](https://arxiv.org/abs/2508.21340)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: 提出DLGAN双阶段生成对抗网络，通过序列特征提取和重建两阶段处理，解决现有时间序列合成方法在保持时序依赖性和准确捕捉特征信息方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有时间序列合成方法基于随机序列进行时序建模，难以确保生成时间序列的时序依赖性，且直接从随机序列建模难以准确捕捉原始时间序列的特征信息

Method: DLGAN将时间序列生成过程分解为序列特征提取和序列重建两个阶段：1）构建完整的时间序列自编码器进行监督学习；2）使用GAN生成与真实序列特征向量对齐的合成特征向量

Result: 在四个公共数据集上的大量实验表明，该模型在各种评估指标上均表现出优越性

Conclusion: DLGAN是一个简单但有效的生成模型，能够有效解决时间序列合成中的时序依赖性和特征捕捉问题

Abstract: Time series synthesis is an effective approach to ensuring the secure
circulation of time series data. Existing time series synthesis methods
typically perform temporal modeling based on random sequences to generate
target sequences, which often struggle to ensure the temporal dependencies in
the generated time series. Additionally, directly modeling temporal features on
random sequences makes it challenging to accurately capture the feature
information of the original time series. To address the above issues, we
propose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer
\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named
\textbf{DLGAN}. The model decomposes the time series generation process into
two stages: sequence feature extraction and sequence reconstruction. First,
these two stages form a complete time series autoencoder, enabling supervised
learning on the original time series to ensure that the reconstruction process
can restore the temporal dependencies of the sequence. Second, a Generative
Adversarial Network (GAN) is used to generate synthetic feature vectors that
align with the real-time sequence feature vectors, ensuring that the generator
can capture the temporal features from real time series. Extensive experiments
on four public datasets demonstrate the superiority of this model across
various evaluation metrics.

</details>


### [48] [Adaptive Heavy-Tailed Stochastic Gradient Descent](https://arxiv.org/abs/2508.21353)
*Bodu Gong,Gustavo Enrique Batista,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: 提出了AHTSGD算法，通过根据损失景观的锐度动态调整注入噪声的尾部特性，促进收敛到宽盆地，提高泛化性能


<details>
  <summary>Details</summary>
Motivation: 针对大规模神经网络优化中过度依赖训练损失导致泛化能力差的问题，基于梯度噪声的重尾分布特性和Edge of Stability现象，旨在找到更宽的局部最小值盆地来提升模型稳定性

Method: AHTSGD算法在训练早期注入重尾噪声增强探索，随着锐度稳定逐渐过渡到轻尾噪声，动态适应损失景观的锐度变化

Result: 在MNIST、CIFAR-10等基准测试中 consistently优于SGD和其他基于噪声的方法，在SVHN等噪声数据集上表现尤为突出，加速了从不良初始化的早期训练

Conclusion: AHTSGD通过基于Edge of Stability现象动态调整注入噪声特性，有效促进了向宽盆地的收敛，提高了在干净和噪声设置下的泛化能力，且对学习率选择具有鲁棒性

Abstract: In the era of large-scale neural network models, optimization algorithms
often struggle with generalization due to an overreliance on training loss. One
key insight widely accepted in the machine learning community is the idea that
wide basins (regions around a local minimum where the loss increases gradually)
promote better generalization by offering greater stability to small changes in
input data or model parameters. In contrast, sharp minima are typically more
sensitive and less stable. Motivated by two key empirical observations - the
inherent heavy-tailed distribution of gradient noise in stochastic gradient
descent and the Edge of Stability phenomenon during neural network training, in
which curvature grows before settling at a plateau, we introduce Adaptive Heavy
Tailed Stochastic Gradient Descent (AHTSGD). The algorithm injects
heavier-tailed noise into the optimizer during the early stages of training to
enhance exploration and gradually transitions to lighter-tailed noise as
sharpness stabilizes. By dynamically adapting to the sharpness of the loss
landscape throughout training, AHTSGD promotes accelerated convergence to wide
basins. AHTSGD is the first algorithm to adjust the nature of injected noise
into an optimizer based on the Edge of Stability phenomenon. AHTSGD
consistently outperforms SGD and other noise-based methods on benchmarks like
MNIST and CIFAR-10, with marked gains on noisy datasets such as SVHN. It
ultimately accelerates early training from poor initializations and improves
generalization across clean and noisy settings, remaining robust to learning
rate choices.

</details>


### [49] [Iterative Inference in a Chess-Playing Neural Network](https://arxiv.org/abs/2508.21380)
*Elias Sandmann,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 论文通过分析Leela Chess Zero的策略网络，发现神经网络表示构建并非总是平滑渐进，而是存在复杂的非平滑轨迹，包括早期发现但随后丢弃的正确解、与最终输出相关性低的移动排名，以及直到网络后期仍存在的高策略分歧。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否通过平滑渐进的方式构建表示，还是通过更复杂的计算过程，特别是在与语言模型不同的棋类AI中验证这一现象。

Method: 扩展logit lens技术来分析Leela Chess Zero（一个超人类象棋引擎）的策略网络，通过观察不同层的玩棋强度和解题能力的变化趋势。

Result: 发现虽然玩棋强度和解题能力在各层呈现强单调趋势，但策略分布经常遵循非平滑轨迹，包括正确解早期发现后被丢弃、移动排名与最终输出相关性低、直到网络后期仍存在高策略分歧等现象。

Conclusion: 神经网络表示构建过程可能比语言模型中观察到的平滑分布收敛更加复杂，存在显著的非平滑计算特征，这挑战了关于神经网络表示形成方式的传统认知。

Abstract: Do neural networks build their representations through smooth, gradual
refinement, or via more complex computational processes? We investigate this by
extending the logit lens to analyze the policy network of Leela Chess Zero, a
superhuman chess engine. We find strong monotonic trends in playing strength
and puzzle-solving ability across layers, yet policy distributions frequently
follow non-smooth trajectories. Evidence for this includes correct puzzle
solutions that are discovered early but subsequently discarded, move rankings
that remain poorly correlated with final outputs, and high policy divergence
until late in the network. These findings contrast with the smooth
distributional convergence typically observed in language models.

</details>


### [50] [PMODE: Theoretically Grounded and Modular Mixture Modeling](https://arxiv.org/abs/2508.21396)
*Robert A. Vandermeulen*

Main category: cs.LG

TL;DR: PMODE是一个模块化的混合密度估计框架，通过数据分区和分别拟合估计器来构建混合模型，支持参数和非参数组件，并在高维异常检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统混合模型在处理不同分布族组件时的局限性，以及将理论方法扩展到高维实际应用的挑战。

Method: 采用数据分区策略，将数据划分为多个子集，对每个子集分别拟合密度估计器（可以是参数或非参数方法），然后组合成混合模型。

Result: PMODE达到了该类估计器的近最优速率，MV-PMODE版本成功扩展到数千维的高维密度估计，在CIFAR-10异常检测任务中与深度基线方法竞争性相当。

Conclusion: PMODE提供了一个通用且模块化的混合建模框架，能够有效处理不同分布族的混合组件，并在高维应用中展现出实用价值。

Abstract: We introduce PMODE (Partitioned Mixture Of Density Estimators), a general and
modular framework for mixture modeling with both parametric and nonparametric
components. PMODE builds mixtures by partitioning the data and fitting separate
estimators to each subset. It attains near-optimal rates for this estimator
class and remains valid even when the mixture components come from different
distribution families. As an application, we develop MV-PMODE, which scales a
previously theoretical approach to high-dimensional density estimation to
settings with thousands of dimensions. Despite its simplicity, it performs
competitively against deep baselines on CIFAR-10 anomaly detection.

</details>


### [51] [Benchmarking the State of Networks with a Low-Cost Method Based on Reservoir Computing](https://arxiv.org/abs/2508.21420)
*Felix Simon Reimers,Carl-Hendrik Peters,Stefano Nichele*

Main category: cs.LG

TL;DR: 使用移动网络数据通过池子计算框架监控网络状态，提出了一种非侵入、低成本的网络监测方法


<details>
  <summary>Details</summary>
Motivation: 需要一种使用易获得数据集的低成本方法来监控通信和移动网络的状态

Method: 将移动网络利用数据转换为池子计算框架下的模型，通过测量模型在代理任务上的性能来监测网络状态

Result: 实验证明代理任务的性能与网络状态相关，并显示出在网络受到干扰时性能明显下降

Conclusion: 该方法作为概念验证，有潜力用于近实时监控和识别移动通信网络及交通网络的弱点

Abstract: Using data from mobile network utilization in Norway, we showcase the
possibility of monitoring the state of communication and mobility networks with
a non-invasive, low-cost method. This method transforms the network data into a
model within the framework of reservoir computing and then measures the model's
performance on proxy tasks. Experimentally, we show how the performance on
these proxies relates to the state of the network. A key advantage of this
approach is that it uses readily available data sets and leverages the
reservoir computing framework for an inexpensive and largely agnostic method.
Data from mobile network utilization is available in an anonymous, aggregated
form with multiple snapshots per day. This data can be treated like a weighted
network. Reservoir computing allows the use of weighted, but untrained networks
as a machine learning tool. The network, initialized as a so-called echo state
network (ESN), projects incoming signals into a higher dimensional space, on
which a single trained layer operates. This consumes less energy than deep
neural networks in which every weight of the network is trained. We use
neuroscience inspired tasks and trained our ESN model to solve them. We then
show how the performance depends on certain network configurations and also how
it visibly decreases when perturbing the network. While this work serves as
proof of concept, we believe it can be elevated to be used for near-real-time
monitoring as well as the identification of possible weak spots of both mobile
communication networks as well as transportation networks.

</details>


### [52] [Rethinking Layer-wise Model Merging through Chain of Merges](https://arxiv.org/abs/2508.21421)
*Pietro Buzzega,Riccardo Salami,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 提出Chain of Merges (CoM)方法，通过自回归方式更新激活统计量来解决模型合并中的层间依赖问题，有效缓解协变量偏移导致的性能下降


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并技术往往独立处理每一层，忽略了深度网络中固有的层间依赖关系，导致分布不匹配问题，特别是在激活基方法中早期层的变化无法正确反映到下游层

Method: CoM是一种分层合并过程，以自回归方式更新激活统计量，显式考虑跨层交互，通过一系列条件最优更新产生连贯的合并模型

Result: 在标准基准测试中，CoM实现了最先进的性能表现

Conclusion: CoM方法通过解决模型合并中的内部协变量偏移问题，为多任务模型融合提供了有效的解决方案

Abstract: Fine-tuning pretrained models has become a standard pathway to achieve
state-of-the-art performance across a wide range of domains, leading to a
proliferation of task-specific model variants. As the number of such
specialized modules in-creases, merging them into a unified model without
retraining has become a critical challenge. Existing merging techniques often
rely on interference heuristics,importance weighting, or activation matching
while treating each layer independently, thereby failing to account for the
inter-layer dependencies inherent in deep networks. This simplification leads
to distributional mismatches, especially inactivation-based methods, when
changes in early layers are not properly reflected in downstream ones. We
identify these mismatches as a form of internal covariate shift, comparable to
the phenomenon encountered in the initial phases of neural networks training.
To address it, we propose Chain of Merges (CoM), a layer-wise merging procedure
that updates activation statistics in an auto-regressive fashion, explicitly
accounting for cross-layer interactions. CoM produces a coherent merged model
through a series of conditionally optimal updates, effectively mitigating
degradation caused by covariate shift. Experiments on standard bench-marks
demonstrate that CoM achieves state-of-the-art performance.

</details>


### [53] [Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing](https://arxiv.org/abs/2508.21438)
*Rajiv Kailasanathan,William R. Clements,Mohammad Reza Boskabadi,Shawn M. Gibford,Emmanouil Papadakis,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.LG

TL;DR: 提出基于生成对抗网络(GAN)集成的新型无监督异常检测框架，用于连续生物制造过程，并探索混合量子/经典GAN方法在异常检测性能上的提升


<details>
  <summary>Details</summary>
Motivation: 连续生物制造过程需要早期异常检测，因为即使微小偏差也会影响产量和稳定性，导致生产中断和经济损失。这些过程具有复杂的非线性动态和变量间复杂关系

Method: 建立基准数据集模拟正常和异常操作状态，开发基于GAN集成的无监督异常检测框架，评估混合量子/经典GAN方法（包括模拟量子电路和真实光子量子处理器）

Result: GAN框架能有效检测由突然原料变化引起的异常，混合量子/经典方法显示出改进的异常检测率

Conclusion: 混合量子/经典方法在解决复杂连续生物制造过程中的实际问题方面具有潜力

Abstract: The development of continuous biomanufacturing processes requires robust and
early anomaly detection, since even minor deviations can compromise yield and
stability, leading to disruptions in scheduling, reduced weekly production, and
diminished economic performance. These processes are inherently complex and
exhibit non-linear dynamics with intricate relationships between process
variables, thus making advanced methods for anomaly detection essential for
efficient operation. In this work, we present a novel framework for
unsupervised anomaly detection in continuous biomanufacturing based on an
ensemble of generative adversarial networks (GANs). We first establish a
benchmark dataset simulating both normal and anomalous operation regimes in a
continuous process for the production of a small molecule. We then demonstrate
the effectiveness of our GAN-based framework in detecting anomalies caused by
sudden feedstock variability. Finally, we evaluate the impact of using a hybrid
quantum/classical GAN approach with both a simulated quantum circuit and a real
photonic quantum processor on anomaly detection performance. We find that the
hybrid approach yields improved anomaly detection rates. Our work shows the
potential of hybrid quantum/classical approaches for solving real-world
problems in complex continuous biomanufacturing processes.

</details>


### [54] [Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning](https://arxiv.org/abs/2508.21443)
*Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 提出了一种结合期望累积奖励和时间平均增长率的新型强化学习算法，通过几何平均和滑动窗口估计器来优化个体轨迹的长期性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL算法优化期望累积奖励（无限轨迹的平均性能），但在实际部署中，个体轨迹的性能更重要。需要同时考虑整体平均和个体轨迹的长期表现。

Method: 定义了时间平均增长率的Bellman算子，在乘法奖励动态下使用几何平均，对于更一般的奖励动态提出带N滑动窗口的修正几何平均估计器，并将其作为正则化项嵌入目标函数。

Result: 在具有挑战性的仿真环境中，该算法优于传统RL方法。

Conclusion: 该方法成功地将整体平均和时间平均相结合，能够同时从两者中受益，提升了个体轨迹的长期性能表现。

Abstract: Reinforcement learning (RL) algorithms typically optimize the expected
cumulative reward, i.e., the expected value of the sum of scalar rewards an
agent receives over the course of a trajectory. The expected value averages the
performance over an infinite number of trajectories. However, when deploying
the agent in the real world, this ensemble average may be uninformative for the
performance of individual trajectories. Thus, in many applications, optimizing
the long-term performance of individual trajectories might be more desirable.
In this work, we propose a novel RL algorithm that combines the standard
ensemble average with the time-average growth rate, a measure for the long-term
performance of individual trajectories. We first define the Bellman operator
for the time-average growth rate. We then show that, under multiplicative
reward dynamics, the geometric mean aligns with the time-average growth rate.
To address more general and unknown reward dynamics, we propose a modified
geometric mean with $N$-sliding window that captures the path-dependency as an
estimator for the time-average growth rate. This estimator is embedded as a
regularizer into the objective, forming a practical algorithm and enabling the
policy to benefit from ensemble average and time-average simultaneously. We
evaluate our algorithm in challenging simulations, where it outperforms
conventional RL methods.

</details>


### [55] [Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces](https://arxiv.org/abs/2508.21466)
*Kota Fukuzawa,Atsushi Suzuki,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 提出了黎曼流形归一化最大似然(Rm-NML)方法，解决了传统NML在黎曼流形上的扩展问题，具有坐标变换不变性，并在双曲空间等对称空间上实现了计算简化。


<details>
  <summary>Details</summary>
Motivation: 随着图数据规模扩大，双曲空间等黎曼流形数据空间受到关注，但传统NML方法主要针对欧氏空间设计，存在坐标系统依赖性，难以扩展到黎曼流形。

Method: 定义了反映黎曼流形几何结构的新NML方法(Rm-NML)，扩展现有计算技术到黎曼流形，并在黎曼对称空间上推导了计算简化方法。

Result: Rm-NML具有坐标变换不变性，在欧氏空间自然参数化下与传统NML一致，并在双曲空间正态分布上进行了具体计算验证。

Conclusion: 成功解决了NML在黎曼流形上的扩展问题，为双曲空间等非欧几何数据空间的模型选择和遗憾最小化提供了理论基础和实用方法。

Abstract: In recent years, with the large-scale expansion of graph data, there has been
an increased focus on Riemannian manifold data spaces other than Euclidean
space. In particular, the development of hyperbolic spaces has been remarkable,
and they have high expressive power for graph data with hierarchical
structures. Normalized Maximum Likelihood (NML) is employed in regret
minimization and model selection. However, existing formulations of NML have
been developed primarily in Euclidean spaces and are inherently dependent on
the choice of coordinate systems, making it non-trivial to extend NML to
Riemannian manifolds. In this study, we define a new NML that reflects the
geometric structure of Riemannian manifolds, called the Riemannian manifold NML
(Rm-NML). This Rm-NML is invariant under coordinate transformations and
coincides with the conventional NML under the natural parameterization in
Euclidean space. We extend existing computational techniques for NML to the
setting of Riemannian manifolds. Furthermore, we derive a method to simplify
the computation of Rm-NML on Riemannian symmetric spaces, which encompass data
spaces of growing interest such as hyperbolic spaces. To illustrate the
practical application of our proposed method, we explicitly computed the Rm-NML
for normal distributions on hyperbolic spaces.

</details>


### [56] [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](https://arxiv.org/abs/2508.21468)
*Seungyeon Choi,Hwanhee Kim,Chihyun Park,Dahyeon Lee,Seungyong Lee,Yoonju Kim,Hyoungjoon Park,Sein Kwon,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: CByG框架将贝叶斯流网络扩展为基于梯度的条件生成模型，有效整合多药理学属性指导，在结合亲和力、合成可行性和选择性等关键指标上显著优于基线模型


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计主要关注结合亲和力，但实际药物发现还需要合成可行性和选择性等关键属性，这些在以往评估中被忽视

Method: 提出CByG框架，将贝叶斯流网络扩展为基于梯度的条件生成模型，能够稳健地整合属性特定的指导信号

Result: 在多个关键评估标准上显著优于基线模型，证明了其在真实药物发现应用中的有效性

Conclusion: CByG框架通过全面评估方案和有效的属性指导机制，为实际药物发现提供了更实用的解决方案

Abstract: Recent advances in Structure-based Drug Design (SBDD) have leveraged
generative models for 3D molecular generation, predominantly evaluating model
performance by binding affinity to target proteins. However, practical drug
discovery necessitates high binding affinity along with synthetic feasibility
and selectivity, critical properties that were largely neglected in previous
evaluations. To address this gap, we identify fundamental limitations of
conventional diffusion-based generative models in effectively guiding molecule
generation toward these diverse pharmacological properties. We propose CByG, a
novel framework extending Bayesian Flow Network into a gradient-based
conditional generative model that robustly integrates property-specific
guidance. Additionally, we introduce a comprehensive evaluation scheme
incorporating practical benchmarks for binding affinity, synthetic feasibility,
and selectivity, overcoming the limitations of conventional evaluation methods.
Extensive experiments demonstrate that our proposed CByG framework
significantly outperforms baseline models across multiple essential evaluation
criteria, highlighting its effectiveness and practicality for real-world drug
discovery applications.

</details>


### [57] [Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning](https://arxiv.org/abs/2508.21488)
*Pascal R. van der Vaart,Neil Yorke-Smith,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 本文发现贝叶斯深度Q学习中存在"冷后验效果"，即降低后验温度能提高性能，这与理论相背离。研究指出常规高斯如果假设常被违反，建议改进先验分布以提升贝叶斯强化学习算法的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的不确定性量化可以提高探索效率和稳健性。虽然近似贝叶斯方法在模型免算法中得到广泛关注，但研究重点多集中在后验近似的准确性，而忽视了先验和如果假设的准确性。

Method: 通过实验研究贝叶斯深度Q学习中的后验温度效应，并使用统计测试检验常规高斯如果假设的合理性。研究了不同先验分布的影响，并提出简单可实现的改进方案。

Result: 实验结果显示存在冷后验效果，降低温度能提高算法性能。统计测试证实高斯如果假设经常被违反。提出的改进先验分布能够产生更高性能的贝叶斯算法。

Conclusion: 贝叶斯强化学习研究应该重点关注如果和先验假设的合理性。发展更适合的如果分布和先验分布是未来研究的关键方向，这些改进能够导致更高性能的贝叶斯算法。

Abstract: Uncertainty quantification in reinforcement learning can greatly improve
exploration and robustness. Approximate Bayesian approaches have recently been
popularized to quantify uncertainty in model-free algorithms. However, so far
the focus has been on improving the accuracy of the posterior approximation,
instead of studying the accuracy of the prior and likelihood assumptions
underlying the posterior. In this work, we demonstrate that there is a cold
posterior effect in Bayesian deep Q-learning, where contrary to theory,
performance increases when reducing the temperature of the posterior. To
identify and overcome likely causes, we challenge common assumptions made on
the likelihood and priors in Bayesian model-free algorithms. We empirically
study prior distributions and show through statistical tests that the common
Gaussian likelihood assumption is frequently violated. We argue that developing
more suitable likelihoods and priors should be a key focus in future Bayesian
reinforcement learning research and we offer simple, implementable solutions
for better priors in deep Q-learning that lead to more performant Bayesian
algorithms.

</details>


### [58] [Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration](https://arxiv.org/abs/2508.21495)
*Piotr Kubaty,Filip Szatkowski,Metod Jazbec,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 本文指出传统基于置信度的校准方法在多出口模型中可能误导性能评估，提出使用失败预测作为更可靠的早期退出模型性能代理指标


<details>
  <summary>Details</summary>
Motivation: 现有早期退出模型主要依赖基于置信度的退出策略，但研究发现校准度量可能误导多出口模型的性能评估，需要寻找更可靠的评估方法

Method: 分析校准方法的局限性，提出使用失败预测作为替代方案，通过实验验证失败预测与效率改进之间的强相关性

Result: 实验证明误校准网络有时优于校准网络，失败预测能更好地反映样本排序变化，与效率改进呈现强相关性

Conclusion: 失败预测比校准方法更适合作为设计和评估早期退出模型的基础，能够更可靠地指导模型优化

Abstract: Early-exit models speed up inference by attaching internal classifiers to
intermediate layers of the model and allowing computation to stop once a
prediction satisfies an exit criterion. Most early-exit methods rely on
confidence-based exit strategies, which motivated some works to calibrate
intermediate classifiers to improve the performance of the entire model. In
this paper, we show that calibration measures can be misleading indicators of
the performance of multi-exit models: a well-calibrated classifier may still
waste computation, and common calibration methods do not preserve the sample
ranking within a classifier. We demonstrate empirical cases where miscalibrated
networks outperform calibrated ones. As an alternative, we propose to use
failure prediction as a more useful proxy for early-exit model performance.
Unlike calibration, failure prediction accounts for changes in the ranking of
samples and shows a strong correlation with efficiency improvements, making it
a more dependable basis for designing and evaluating early-exit models.

</details>


### [59] [Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control](https://arxiv.org/abs/2508.21505)
*Vishal Pandey,Debasmita Biswas*

Main category: cs.LG

TL;DR: SNN-DT将脉冲神经网络与决策变换器结合，在保持性能的同时实现超低功耗，每决策仅发射不到10个脉冲，能耗降低4个数量级


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的强化学习代理依赖密集矩阵运算，不适合能源受限的边缘平台。脉冲神经网络具有超低功耗优势，但尚未与回报条件序列建模无缝融合

Method: 在自注意力块中嵌入泄漏积分发放神经元，通过替代梯度端到端训练，结合生物启发的三因子可塑性、相移脉冲位置编码和轻量级树突路由模块

Result: 在经典控制基准测试中匹配或超越标准决策变换器性能，每决策发射少于10个脉冲，推理能耗降低超过4个数量级

Conclusion: SNN-DT通过将序列建模与神经形态效率结合，为嵌入式可穿戴设备开辟了实时低功耗控制的新途径

Abstract: Reinforcement learning agents based on Transformer architectures have
achieved impressive performance on sequential decision-making tasks, but their
reliance on dense matrix operations makes them ill-suited for
energy-constrained, edge-oriented platforms. Spiking neural networks promise
ultra-low-power, event-driven inference, yet no prior work has seamlessly
merged spiking dynamics with return-conditioned sequence modeling. We present
the Spiking Decision Transformer (SNN-DT), which embeds Leaky
Integrate-and-Fire neurons into each self-attention block, trains end-to-end
via surrogate gradients, and incorporates biologically inspired three-factor
plasticity, phase-shifted spike-based positional encodings, and a lightweight
dendritic routing module. Our implementation matches or exceeds standard
Decision Transformer performance on classic control benchmarks (CartPole-v1,
MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes
per decision, an energy proxy suggesting over four orders-of-magnitude
reduction in per inference energy. By marrying sequence modeling with
neuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power
control on embedded and wearable devices.

</details>


### [60] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 评估LLMs在贷款审批表格数据上的性能和公平性，发现序列化格式选择对结果有显著影响，ICL提升性能但对公平性影响不一


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在贷款审批等高风险决策任务中的应用增加，需要评估其在处理表格数据时的性能和公平性问题

Method: 使用来自加纳、德国和美国的序列化贷款审批数据集，评估LLMs的零样本学习和上下文学习能力，比较不同序列化格式的效果

Result: 序列化格式选择显著影响LLMs的性能和公平性，某些格式如GReat和LIFT能提高F1分数但加剧公平性差距；ICL相对零样本基线提升性能4.9-59.6%，但对公平性的影响因数据集而异

Conclusion: 需要开发有效的表格数据表示方法和具备公平性意识的模型，以提高LLMs在金融决策中的可靠性

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [61] [On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature](https://arxiv.org/abs/2508.21513)
*Geri Skenderi*

Main category: cs.LG

TL;DR: 本文通过图里奇曲率分析发现，随机k-SAT公式的二部图具有负曲率特性，且曲率随问题难度降低，这导致GNN求解器出现过度挤压现象，限制了长距离依赖关系的处理能力。


<details>
  <summary>Details</summary>
Motivation: GNN在求解布尔可满足性问题时，面对困难实例性能急剧下降，需要探究这是否反映了根本性的架构限制。

Method: 使用图里奇曲率量化局部连接瓶颈，证明随机k-SAT公式的二部图具有固有负曲率特性，并通过实验验证曲率与问题复杂度的关系。

Result: 发现曲率是问题复杂度的强指标，可用于预测GNN性能，确认GNN求解器受到过度挤压现象的影响。

Conclusion: 研究结果为现有求解器的设计原则提供了理论解释，并指出了未来工作的有前景方向。

Abstract: Graph Neural Networks (GNNs) have recently shown promise as solvers for
Boolean Satisfiability Problems (SATs) by operating on graph representations of
logical formulas. However, their performance degrades sharply on harder
instances, raising the question of whether this reflects fundamental
architectural limitations. In this work, we provide a geometric explanation
through the lens of graph Ricci Curvature (RC), which quantifies local
connectivity bottlenecks. We prove that bipartite graphs derived from random
k-SAT formulas are inherently negatively curved, and that this curvature
decreases with instance difficulty. Building on this, we show that GNN-based
SAT solvers are affected by oversquashing, a phenomenon where long-range
dependencies become impossible to compress into fixed-length representations.
We validate our claims empirically across different SAT benchmarks and confirm
that curvature is both a strong indicator of problem complexity and can be used
to predict performance. Finally, we connect our findings to design principles
of existing solvers and outline promising directions for future work.

</details>


### [62] [What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems](https://arxiv.org/abs/2508.21547)
*Jens Leysen,Marco Favier,Bart Goethals*

Main category: cs.LG

TL;DR: 推荐系统隐式反馈数据最小化技术可行性研究：在不重大性能损失下可实现数据大幅减少，但实际应用依赖于技术设置和用户特征等因素


<details>
  <summary>Details</summary>
Motivation: 数据最小化是个人数据处理的法律原则，但在需要大量个人数据的推荐系统中运作化这一原则仍面临重大挑战

Method: 提出新题问形式，分析各种最小化技术，研究影响其有效性的关键因素

Result: 证明在不产生重大性能损失的情况下，大幅减少推荐推理数据在技术上是可行的

Conclusion: 虽然技术上可行，但数据最小化仍面临实践挑战，其实际效果依赖于技术设置和用户特征，使得统一的数据"必要性"标准难以实施

Abstract: Data minimization is a legal principle requiring personal data processing to
be limited to what is necessary for a specified purpose. Operationalizing this
principle for recommender systems, which rely on extensive personal data,
remains a significant challenge. This paper conducts a feasibility study on
minimizing implicit feedback inference data for such systems. We propose a
novel problem formulation, analyze various minimization techniques, and
investigate key factors influencing their effectiveness. We demonstrate that
substantial inference data reduction is technically feasible without
significant performance loss. However, its practicality is critically
determined by two factors: the technical setting (e.g., performance targets,
choice of model) and user characteristics (e.g., history size, preference
complexity). Thus, while we establish its technical feasibility, we conclude
that data minimization remains practically challenging and its dependence on
the technical and user context makes a universal standard for data `necessity'
difficult to implement.

</details>


### [63] [Comprehensive Signal Quality Evaluation of a Wearable Textile ECG Garment: A Sex-Balanced Study](https://arxiv.org/abs/2508.21554)
*Maximilian P. Oppelt,Tobias S. Zech,Sarah H. Lorenz,Laurenz Ottmann,Jan Steffan,Bjoern M. Eskofier,Nadine R. Lang-Richter,Norman Pfeiffer*

Main category: cs.LG

TL;DR: 开发了一种新型纺织服装ECG设备，通过创新的电极布局减少噪声和运动伪影，在男女各15名参与者的综合评估中显示出与参考设备相当的信号质量，并揭示了性别特异性设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统ECG设备存在噪声和运动伪影问题，且缺乏对不同性别解剖生理差异的考虑，需要开发更可靠、适用于真实场景的纺织服装式ECG监测解决方案。

Method: 采用创新的电极布局设计，通过定量信号质量指标、生理参数分析、机器学习分类、ECG形态学分析和电极投影角度研究等多种方法，在男女各15名参与者中进行全面评估。

Result: 纺织系统在节律和形态学分析中与参考设备高度一致，表现出强大的分类性能，能够识别影响信号采集的关键性别特异性决定因素。

Conclusion: 纺织基ECG服装在生理监测和心理生理状态检测方面具有实际可行性，强调了在可穿戴健康技术中纳入性别特异性设计考虑以确保公平可靠的心脏诊断的重要性。

Abstract: We introduce a novel wearable textile-garment featuring an innovative
electrode placement aimed at minimizing noise and motion artifacts, thereby
enhancing signal fidelity in Electrocardiography (ECG) recordings. We present a
comprehensive, sex-balanced evaluation involving 15 healthy males and 15
healthy female participants to ensure the device's suitability across
anatomical and physiological variations. The assessment framework encompasses
distinct evaluation approaches: quantitative signal quality indices to
objectively benchmark device performance; rhythm-based analyzes of
physiological parameters such as heart rate and heart rate variability; machine
learning classification tasks to assess application-relevant predictive
utility; morphological analysis of ECG features including amplitude and
interval parameters; and investigations of the effects of electrode projection
angle given by the textile / body shape, with all analyzes stratified by sex to
elucidate sex-specific influences. Evaluations were conducted across various
activity phases representing real-world conditions. The results demonstrate
that the textile system achieves signal quality highly concordant with
reference devices in both rhythm and morphological analyses, exhibits robust
classification performance, and enables identification of key sex-specific
determinants affecting signal acquisition. These findings underscore the
practical viability of textile-based ECG garments for physiological monitoring
as well as psychophysiological state detection. Moreover, we identify the
importance of incorporating sex-specific design considerations to ensure
equitable and reliable cardiac diagnostics in wearable health technologies.

</details>


### [64] [Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation](https://arxiv.org/abs/2508.21559)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: PINNs在智能电网建模中通过物理约束损失函数实现优于传统数据驱动模型的泛化性能，在插值、交叉验证和轨迹预测任务中表现更稳定可靠


<details>
  <summary>Details</summary>
Motivation: 解决传统数据驱动方法在智能电网建模中的数据稀缺和物理一致性不足的问题，将物理定律直接整合到学习框架中

Method: 使用物理信息神经网络(PINNs)，通过基于物理的损失函数（功率平衡、运行约束、电网稳定性）进行训练，并与XGBoost、随机森林、线性回归进行对比实验

Result: PINNs在动态电网操作中保持较低的MAE，可靠地捕捉状态转换，在随机和专家驱动控制场景中表现稳定，而传统模型表现不稳定

Conclusion: PINNs作为智能电网代理模型的范式转变工具，将数据驱动的灵活性与第一性原理的严谨性相结合，对安全关键应用至关重要

Abstract: Physics-Informed Neural Networks (PINNs) present a transformative approach
for smart grid modeling by integrating physical laws directly into learning
frameworks, addressing critical challenges of data scarcity and physical
consistency in conventional data-driven methods. This paper evaluates PINNs'
capabilities as surrogate models for smart grid dynamics, comparing their
performance against XGBoost, Random Forest, and Linear Regression across three
key experiments: interpolation, cross-validation, and episodic trajectory
prediction. By training PINNs exclusively through physics-based loss functions
(enforcing power balance, operational constraints, and grid stability) we
demonstrate their superior generalization, outperforming data-driven models in
error reduction. Notably, PINNs maintain comparatively lower MAE in dynamic
grid operations, reliably capturing state transitions in both random and
expert-driven control scenarios, while traditional models exhibit erratic
performance. Despite slight degradation in extreme operational regimes, PINNs
consistently enforce physical feasibility, proving vital for safety-critical
applications. Our results contribute to establishing PINNs as a
paradigm-shifting tool for smart grid surrogation, bridging data-driven
flexibility with first-principles rigor. This work advances real-time grid
control and scalable digital twins, emphasizing the necessity of physics-aware
architectures in mission-critical energy systems.

</details>


### [65] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab是一个基于洞察蒸馏的框架，通过规则总结、策略性示例和反思学习，帮助大语言模型更好地处理表格分类任务，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型在少样本表格分类中具有潜力，但结构化数据的变异性带来了挑战，需要将数据蒸馏为可操作的洞察来提升分类效果。

Method: 提出InsightTab框架，受人类学习过程启发，采用分而治之、先易后难和反思学习原则，结合大语言模型和数据建模技术进行规则总结、策略性示例和洞察反思。

Result: 在9个数据集上的广泛评估显示，该方法相比最先进方法取得了持续改进，消融研究验证了原则指导的蒸馏过程的有效性。

Conclusion: InsightTab通过洞察蒸馏使大语言模型能够更好地将其通用知识和能力与特定表格任务的要求对齐，有效利用标注数据并管理偏差。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


### [66] [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](https://arxiv.org/abs/2508.21570)
*Bo Li,Yingqi Feng,Ming Jin,Xin Zheng,Yufei Tang,Laurent Cherubin,Alan Wee-Chung Liew,Can Wang,Qinghua Lu,Jingwei Yao,Shirui Pan,Hong Zhang,Xingquan Zhu*

Main category: cs.LG

TL;DR: 提出了OASIS扩散对抗框架，用于解决海洋盐度测量数据稀疏、不规则和噪声的问题


<details>
  <summary>Details</summary>
Motivation: 海洋盐度对环流、气候和海洋生态系统至关重要，但现有测量方法存在稀疏、不规则、噪声大等问题，传统方法受限于线性假设和卫星观测限制，机器学习方法在严重稀疏情况下效果不佳

Method: 开发了OceAn Salinity Imputation System (OASIS)，一种新颖的扩散对抗框架

Result: 该方法旨在克服传统方法的局限性，能够更好地处理稀疏数据和整合物理协变量

Conclusion: OASIS框架为解决海洋盐度数据插补问题提供了新的解决方案

Abstract: Ocean salinity plays a vital role in circulation, climate, and marine
ecosystems, yet its measurement is often sparse, irregular, and noisy,
especially in drifter-based datasets. Traditional approaches, such as remote
sensing and optimal interpolation, rely on linearity and stationarity, and are
limited by cloud cover, sensor drift, and low satellite revisit rates. While
machine learning models offer flexibility, they often fail under severe
sparsity and lack principled ways to incorporate physical covariates without
specialized sensors. In this paper, we introduce the OceAn Salinity Imputation
System (OASIS), a novel diffusion adversarial framework designed to address
these challenges.

</details>


### [67] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: 本文证明了过参数化两层物理信息神经网络(PINNs)中随机梯度下降/流的线性收敛性，扩展了之前梯度下降的分析结果


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络(PINNs)是求解偏微分方程的流行方法，通常使用随机梯度下降算法训练，因此需要对其收敛性提供理论保证

Method: 分析随机优化方法引入的动态随机性，关键在于确保训练过程中合适Gram矩阵的正定性，建立高概率下的线性收敛理论

Result: 证明了对于一类通用激活函数，过参数化两层PINNs中随机梯度下降/流能够实现线性收敛

Conclusion: 该分析揭示了优化过程的动态特性，为随机算法训练的神经网络提供了收敛性保证，扩展了现有理论结果

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


### [68] [Physics-Informed Spectral Modeling for Hyperspectral Imaging](https://arxiv.org/abs/2508.21618)
*Zuzanna Gawrysiak,Krzysztof Krawiec*

Main category: cs.LG

TL;DR: PhISM是一种无需监督学习的物理信息深度学习架构，能够显式解耦高光谱观测数据并用连续基函数建模，在多个分类和回归基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 为了解决高光谱数据分析中需要大量标注数据、模型可解释性差的问题，开发一种能够无监督学习并显式解耦高光谱观测的物理信息深度学习架构

Method: 采用物理信息深度学习架构，通过连续基函数对高光谱观测进行建模，实现无监督学习下的显式特征解耦

Result: 在多个分类和回归基准测试中性能优于现有方法，仅需有限标注数据，并提供了可解释的潜在表示

Conclusion: PhISM架构成功实现了高光谱数据的无监督显式解耦建模，在性能和可解释性方面都有显著提升，为高光谱分析提供了新的有效解决方案

Abstract: We present PhISM, a physics-informed deep learning architecture that learns
without supervision to explicitly disentangle hyperspectral observations and
model them with continuous basis functions. \mname outperforms prior methods on
several classification and regression benchmarks, requires limited labeled
data, and provides additional insights thanks to interpretable latent
representation.

</details>


### [69] [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](https://arxiv.org/abs/2508.21620)
*Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 这篇专论为非专家提供了概率决策算法理论分析的易读介绍，涵盖多臂老虎机、贝叶斯优化和树搜索算法，旨在降低理论分析的门槛。


<details>
  <summary>Details</summary>
Motivation: 决策理论算法在材料发现、药物研发等实际应用中表现出色，但现有理论分析对非专家难以理解，阻碍了算法的进一步发展和应用。

Method: 提供自包含的理论分析介绍，仅需概率论、统计学基础知识和高斯过程的基本了解，使非专家也能理解常用概率决策算法的理论分析。

Result: 专论系统介绍了多臂老虎机、贝叶斯优化和树搜索算法的理论分析框架，使这些复杂理论对更广泛的受众变得可访问。

Conclusion: 通过降低理论分析的门槛，该专论有助于促进决策理论算法在科学发现等成本敏感领域的更广泛应用和发展。

Abstract: Decision theories offer principled methods for making choices under various
types of uncertainty. Algorithms that implement these theories have been
successfully applied to a wide range of real-world problems, including
materials and drug discovery. Indeed, they are desirable since they can
adaptively gather information to make better decisions in the future, resulting
in data-efficient workflows. In scientific discovery, where experiments are
costly, these algorithms can thus significantly reduce the cost of
experimentation. Theoretical analyses of these algorithms are crucial for
understanding their behavior and providing valuable insights for developing
next-generation algorithms. However, theoretical analyses in the literature are
often inaccessible to non-experts. This monograph aims to provide an
accessible, self-contained introduction to the theoretical analysis of commonly
used probabilistic decision-making algorithms, including bandit algorithms,
Bayesian optimization, and tree search algorithms. Only basic knowledge of
probability theory and statistics, along with some elementary knowledge about
Gaussian processes, is assumed.

</details>


### [70] [Predicting Social Media Engagement from Emotional and Temporal Features](https://arxiv.org/abs/2508.21650)
*Yunwoo Kim,Junhyuk Hwang*

Main category: cs.LG

TL;DR: 使用情感和时间特征预港社交媒体互动，点赞预测效果极佳（R²=0.98）但评论预测效果较差（R²=0.41）


<details>
  <summary>Details</summary>
Motivation: 研究情感和时间特征如何预港社交媒体互动（点赞和评论），以了解不同类型互动的驱动因素

Method: 使用HistGradientBoostingRegressor多目标回归模型，基于600首歌曲的价值、激活度和情感指标进行训练，对偏斜的互动数据进行对数变换

Result: 模型在点赞预测上表现优异（R²=0.98），但评论预测效果较差（R²=0.41），显示点赞主要受情感和暴露度影响，而评论需要其他未包含因素

Conclusion: 情感和时间元数据统解视频流量可有效预港未来互动，但不同互动类型有不同驱动机制，评论预测需要更多特征

Abstract: We present a machine learning approach for predicting social media engagement
(comments and likes) from emotional and temporal features. The dataset contains
600 songs with annotations for valence, arousal, and related sentiment metrics.
A multi target regression model based on HistGradientBoostingRegressor is
trained on log transformed engagement ratios to address skewed targets.
Performance is evaluated with both a custom order of magnitude accuracy and
standard regression metrics, including the coefficient of determination (R^2).
Results show that emotional and temporal metadata, together with existing view
counts, predict future engagement effectively. The model attains R^2 = 0.98 for
likes but only R^2 = 0.41 for comments. This gap indicates that likes are
largely driven by readily captured affective and exposure signals, whereas
comments depend on additional factors not represented in the current feature
set.

</details>


### [71] [Activation Subspaces for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21695)
*Barış Zöngür,Robin Hesse,Stefan Roth*

Main category: cs.LG

TL;DR: 提出ActSub方法，通过奇异值分解分类头权重矩阵，将激活分解为决定性分量和无关分量，分别处理远分布偏移和近分布偏移的OOD检测问题，在多个基准测试中达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 确保深度学习模型在真实应用中的可靠性，需要有效区分训练分布内(ID)和分布外(OOD)样本。现有方法在处理不同分布偏移程度时存在局限性

Method: 使用SVD分解分类头权重矩阵，将模型激活分解为决定性分量（对分类输出贡献最大）和无关分量（贡献最小）。远OOD时利用无关分量，近OOD时利用决定性分量

Result: 在多个标准OOD基准测试中取得了最先进的结果，证明了该方法在不同分布偏移场景下的有效性

Conclusion: ActSub方法通过智能地利用模型激活的不同分量，为OOD检测提供了新的有效解决方案，特别在处理不同分布偏移程度时表现出色

Abstract: To ensure the reliability of deep models in real-world applications,
out-of-distribution (OOD) detection methods aim to distinguish samples close to
the training distribution (in-distribution, ID) from those farther away (OOD).
In this work, we propose a novel OOD detection method that utilizes singular
value decomposition of the weight matrix of the classification head to
decompose the model's activations into decisive and insignificant components,
which contribute maximally, respectively minimally, to the final classifier
output. We find that the subspace of insignificant components more effectively
distinguishes ID from OOD data than raw activations in regimes of large
distribution shifts (Far-OOD). This occurs because the classification objective
leaves the insignificant subspace largely unaffected, yielding features that
are ''untainted'' by the target classification task. Conversely, in regimes of
smaller distribution shifts (Near-OOD), we find that activation shaping methods
profit from only considering the decisive subspace, as the insignificant
component can cause interference in the activation space. By combining two
findings into a single approach, termed ActSub, we achieve state-of-the-art
results in various standard OOD benchmarks.

</details>


### [72] [Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety](https://arxiv.org/abs/2508.21722)
*Siddharth Mangalik,Ojas Deshpande,Adithya V. Ganesan,Sean A. P. Clouston,H. Andrew Schwartz*

Main category: cs.LG

TL;DR: 通过统计学习框架展布纵向回归断点设计(LRDD)，预测时间特定事件对社区精神健康的不连续性和斜率变化效果


<details>
  <summary>Details</summary>
Motivation: 传统的精神健康预测方法对事件影响的洞察力有限，需要更好地估计社区特定事件对心理健康的因果效应

Method: 将LRDD进行扩展，统合社区历史数据、动态协变量和外生变量，建立统计学习框架来预测未来的不连续性和斜率变化

Result: 在COVID-19事件对美国县级焦虑的预测中，集成外生和动态协变量的模型表现最佳，不连续性预测相关系数r=+0.46，斜率预测r=+0.65，显著超过传统静态表征方法

Conclusion: 不连续性预测为估计未来或假想事件对特定社区的特异效应提供了新可能，尽管任务很具挑战性但通过提升模型复杂度可以实现

Abstract: Estimating community-specific mental health effects of local events is vital
for public health policy. While forecasting mental health scores alone offers
limited insights into the impact of events on community well-being,
quasi-experimental designs like the Longitudinal Regression Discontinuity
Design (LRDD) from econometrics help researchers derive more effects that are
more likely to be causal from observational data. LRDDs aim to extrapolate the
size of changes in an outcome (e.g. a discontinuity in running scores for
anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond
traditional forecasting into a statistical learning framework whereby future
discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear
trajectories) are estimated given a location's history of the score, dynamic
covariates (other running assessments), and exogenous variables (static
representations). Applying our framework to predict discontinuities in the
anxiety of US counties from COVID-19 events, we found the task was difficult
but more achievable as the sophistication of models was increased, with the
best results coming from integrating exogenous and dynamic covariates. Our
approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$
for slope) over traditional static community representations. Discontinuity
forecasting raises new possibilities for estimating the idiosyncratic effects
of potential future or hypothetical events on specific communities.

</details>


### [73] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SLAC开发了SNL框架和Auto-SNL工具，用于在FPGA上部署实时机器学习推理模型，解决了高能物理实验中TB/s级数据流的实时处理挑战，相比现有工具hls4ml在延迟和资源利用方面表现更优。


<details>
  <summary>Details</summary>
Motivation: LCLS-II自由电子激光器产生高达1MHz的X射线脉冲，探测器数据吞吐量超过1TB/s，传统数据传输和存储成本过高，而常规机器学习实现引入过多延迟，无法满足高速实验环境的实时数据处理需求。

Method: 开发SLAC神经网络库(SNL)框架，支持在FPGA上部署实时ML推理模型，并能动态更新模型权重而无需FPGA重新合成。同时开发Auto-SNL Python扩展工具，简化Python神经网络模型到SNL兼容的高级合成代码的转换过程。

Result: 与当前最先进工具hls4ml的基准测试比较显示，SNL在大多数测试的神经网络架构中实现了竞争性或更优的延迟性能，在某些情况下还能节省FPGA资源。

Conclusion: SNL框架展示了其多功能性，为高能物理、医学成像、机器人等领域的研究人员和学者开辟了新的机会，解决了高速实验环境中的实时数据处理挑战。

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


### [74] [UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking](https://arxiv.org/abs/2508.21772)
*V. Bugra Yesilkaynak,Emine Dari,Alican Mertan,Gozde Unal*

Main category: cs.LG

TL;DR: UniMLR是一种新的多标签排序框架，利用正标签间的排序信息来建模类别相关性，而不是将所有正标签视为同等重要，从而统一了排序和分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有的多标签排序方法只利用标签的二分类信息（正/负），没有充分利用正标签之间的排序信息，无法捕捉类别的重要性和相关性差异。

Method: 提出UniMLR框架，将隐式类别相关性建模为概率分布，利用正标签的排序信息；同时创建了8个合成数据集（Ranked MNISTs）来解决数据稀缺和标注偏差问题。

Result: 统计证明该方法能准确学习正标签的排序表示，与真实排序一致且与底层重要性值成比例；在真实和合成数据集上的实验验证了框架的有效性。

Conclusion: UniMLR通过利用正标签排序信息，为多标签排序提供了新的范式，能够更好地建模类别相关性，统一了排序和分类任务，并通过合成数据集解决了数据稀缺问题。

Abstract: Existing multi-label ranking (MLR) frameworks only exploit information
deduced from the bipartition of labels into positive and negative sets.
Therefore, they do not benefit from ranking among positive labels, which is the
novel MLR approach we introduce in this paper. We propose UniMLR, a new MLR
paradigm that models implicit class relevance/significance values as
probability distributions using the ranking among positive labels, rather than
treating them as equally important. This approach unifies ranking and
classification tasks associated with MLR. Additionally, we address the
challenges of scarcity and annotation bias in MLR datasets by introducing eight
synthetic datasets (Ranked MNISTs) generated with varying
significance-determining factors, providing an enriched and controllable
experimental environment. We statistically demonstrate that our method
accurately learns a representation of the positive rank order, which is
consistent with the ground truth and proportional to the underlying
significance values. Finally, we conduct comprehensive empirical experiments on
both real-world and synthetic datasets, demonstrating the value of our proposed
framework.

</details>


### [75] [Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling](https://arxiv.org/abs/2508.21785)
*Peng Yang,Zhengdong Huang,Zicheng Xie,Wentao Tian,Jingyu Liu,Lunhong Dong*

Main category: cs.LG

TL;DR: 提出一个处理心率预测中数据异质性（设备源和用户差异）的框架，通过随机特征丢弃和时间感知注意力模块，在新建的ParroTao数据集和FitRec数据集上分别提升17%和15%的性能。


<details>
  <summary>Details</summary>
Motivation: 心率预测在现实部署中面临数据异质性挑战，包括设备源异质性（不同设备特征集不同）和用户异质性（不同用户生理模式差异）。现有方法要么丢弃设备特定信息，要么无法建模用户差异，限制了实际性能。

Method: 提出一个学习对两种异质性都不敏感的潜在表示框架：使用随机特征丢弃策略处理源异质性，使模型对各种特征集具有鲁棒性；使用时序感知注意力模块捕获长期生理特征，并结合对比学习目标构建判别性表示空间。

Result: 在新建的ParroTao数据集和公开的FitRec数据集上，模型分别显著超越现有基线17%和15%。学习到的表示展现出强判别能力，下游应用任务验证了模型的实用价值。

Conclusion: 该框架有效解决了心率预测中的数据异质性问题，通过创新的特征处理和表示学习方法，在多个数据集上取得了显著性能提升，具有实际应用价值。

Abstract: Heart rate prediction is vital for personalized health monitoring and
fitness, while it frequently faces a critical challenge when deploying in
real-world: data heterogeneity. We classify it in two key dimensions: source
heterogeneity from fragmented device markets with varying feature sets, and
user heterogeneity reflecting distinct physiological patterns across
individuals and activities. Existing methods either discard device-specific
information, or fail to model user-specific differences, limiting their
real-world performance. To address this, we propose a framework that learns
latent representations agnostic to both heterogeneity, enabling downstream
predictors to work consistently under heterogeneous data patterns.
Specifically, we introduce a random feature dropout strategy to handle source
heterogeneity, making the model robust to various feature sets. To manage user
heterogeneity, we employ a time-aware attention module to capture long-term
physiological traits and use a contrastive learning objective to build a
discriminative representation space. To reflect the heterogeneous nature of
real-world data, we created and publicly released a new benchmark dataset,
ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that
our model significantly outperforms existing baselines by 17% and 15%,
respectively. Furthermore, analysis of the learned representations demonstrates
their strong discriminative power, and one downstream application task confirm
the practical value of our model.

</details>


### [76] [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](https://arxiv.org/abs/2508.21793)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: MoE-Health是一个新颖的混合专家框架，用于医疗健康预测中的多模态融合，能够处理不同模态可用性的样本，在MIMIC-IV数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实医疗系统中多模态数据往往不完整或存在差异，现有方法需要完整模态数据或依赖人工选择策略，限制了在真实临床环境中的应用。

Method: 采用混合专家框架，包含专门化的专家网络和动态门控机制，根据可用数据模态动态选择和组合相关专家，实现灵活适应不同数据可用性场景。

Result: 在MIMIC-IV数据集的三个关键临床预测任务（院内死亡率预测、长期住院预测和再入院预测）上，MoE-Health相比现有多模态融合方法表现出更优越的性能，并在不同模态可用性模式下保持鲁棒性。

Conclusion: 该框架有效整合多模态信息，在处理异构和不完整医疗数据方面提供改进的预测性能和鲁棒性，特别适合部署在数据可用性各异的不同医疗环境中。

Abstract: Healthcare systems generate diverse multimodal data, including Electronic
Health Records (EHR), clinical notes, and medical images. Effectively
leveraging this data for clinical prediction is challenging, particularly as
real-world samples often present with varied or incomplete modalities. Existing
approaches typically require complete modality data or rely on manual selection
strategies, limiting their applicability in real-world clinical settings where
data availability varies across patients and institutions. To address these
limitations, we propose MoE-Health, a novel Mixture of Experts framework
designed for robust multimodal fusion in healthcare prediction. MoE-Health
architecture is specifically developed to handle samples with differing
modalities and improve performance on critical clinical tasks. By leveraging
specialized expert networks and a dynamic gating mechanism, our approach
dynamically selects and combines relevant experts based on available data
modalities, enabling flexible adaptation to varying data availability
scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical
clinical prediction tasks: in-hospital mortality prediction, long length of
stay, and hospital readmission prediction. Experimental results demonstrate
that MoE-Health achieves superior performance compared to existing multimodal
fusion methods while maintaining robustness across different modality
availability patterns. The framework effectively integrates multimodal
information, offering improved predictive performance and robustness in
handling heterogeneous and incomplete healthcare data, making it particularly
suitable for deployment in diverse healthcare environments with heterogeneous
data availability.

</details>


### [77] [QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.21810)
*Jessica Liang,Anirudh Bharadwaj*

Main category: cs.LG

TL;DR: QR-LoRA是一种参数高效微调方法，通过QR分解提取预训练权重的正交基，仅训练标量系数来大幅减少可训练参数数量，在保持性能的同时实现77倍参数压缩。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法需要训练两个低秩矩阵，而基于SVD初始化的变体计算成本高且奇异向量难以解释。需要一种更高效、结构更清晰的参数高效微调方法。

Method: 使用带列旋转的QR分解从预训练权重矩阵中提取正交基，将LoRA更新表示为这些基向量的线性组合，仅训练标量系数。

Result: 在GLUE任务上，QR-LoRA仅用601个参数就能达到或超过全微调、标准LoRA和SVD-LoRA的性能，参数减少超过1000倍（相比全微调）和77倍（相比典型LoRA）。

Conclusion: QR-LoRA提供了一种结构清晰、参数效率极高的微调方法，在保持性能的同时大幅降低了计算和存储需求。

Abstract: The growing scale of Large Language Models (LLMs) has necessitated the
development of parameter-efficient fine-tuning techniques. Low-Rank Adaptation
(LoRA) has emerged as a promising approach, reducing the number of trainable
parameters by applying low-rank updates to pretrained weights. While standard
LoRA learns both update factors directly, several recent variants first
initialize those matrices via an SVD of the pretrained weights -- an operation
that can be expensive on large models and yields singular vectors that are not
always easy to interpret. In this work, we extract an orthonormal basis from
the pretrained weight matrix using QR decomposition with column pivoting, and
then express the LoRA update as a linear combination of these basis vectors --
training only the scalar coefficients, which imposes clear structure on
adaptation and drastically reduces parameter count. Experiments across GLUE
tasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,
standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular
value decomposition) with as few as 601 parameters -- a reduction of over 1000x
compared to full fine-tuning and 77x fewer than typical LoRA setups.

</details>


### [78] [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](https://arxiv.org/abs/2508.21815)
*Tobias Hyrup,Emmanouil Panagiotou,Arjun Roy,Arthur Zimek,Eirini Ntoutsi,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FLIP是一个基于transformer的变分自编码器，结合潜在扩散模型生成异构表格数据，在隐私保护下同时解决公平性问题


<details>
  <summary>Details</summary>
Motivation: 随着GDPR、HIPAA等隐私法规和AI责任框架的实施，真实数据使用面临更多限制，需要开发既能保护隐私又能确保公平性的合成数据生成方法

Method: 使用transformer-based VAE和潜在扩散模型，通过Rényi差分隐私约束训练，在输入空间采用RDP兼容的平衡采样，在潜在空间使用CKA对齐不同保护组的神经元激活模式

Result: 实证结果表明FLIP在差分隐私约束下能有效提升任务无关公平性，并在多种下游任务中表现良好

Conclusion: FLIP提供了一个有效的解决方案，能够在隐私保护前提下生成公平的合成表格数据，具有广泛的适用性

Abstract: As privacy regulations such as the GDPR and HIPAA and responsibility
frameworks for artificial intelligence such as the AI Act gain traction, the
ethical and responsible use of real-world data faces increasing constraints.
Synthetic data generation has emerged as a promising solution to risk-aware
data sharing and model development, particularly for tabular datasets that are
foundational to sensitive domains such as healthcare. To address both privacy
and fairness concerns in this setting, we propose FLIP (Fair Latent
Intervention under Privacy guarantees), a transformer-based variational
autoencoder augmented with latent diffusion to generate heterogeneous tabular
data. Unlike the typical setup in fairness-aware data generation, we assume a
task-agnostic setup, not reliant on a fixed, defined downstream task, thus
offering broader applicability. To ensure privacy, FLIP employs R\'enyi
differential privacy (RDP) constraints during training and addresses fairness
in the input space with RDP-compatible balanced sampling that accounts for
group-specific noise levels across multiple sampling rates. In the latent
space, we promote fairness by aligning neuron activation patterns across
protected groups using Centered Kernel Alignment (CKA), a similarity measure
extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment
encourages statistical independence between latent representations and the
protected feature. Empirical results demonstrate that FLIP effectively provides
significant fairness improvements for task-agnostic fairness and across diverse
downstream tasks under differential privacy constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation](https://arxiv.org/abs/2508.21320)
*Mohsen Nayebi Kerdabadi,Arya Hadizadeh Moghaddam,Dongjie Wang,Zijun Yao*

Main category: cs.AI

TL;DR: LINKO是一个基于大语言模型的医疗本体集成学习框架，通过同时利用多个本体图谱，在异构本体系统内和跨系统进行双轴知识传播，以增强医疗概念表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一本体系统或多个孤立本体系统的知识整合，缺乏统一的学习结构，导致概念表示学习局限于本体内部关系，忽略了跨本体连接。

Method: 1) 使用LLM提供图检索增强的概念嵌入初始化；2) 通过双轴知识传播联合学习：本体内部垂直传播和本体间水平传播；3) 作为插件编码器与现有EHR预测模型兼容。

Result: 在两个公共数据集上的实验验证了LINKO优于最先进基线方法的性能，在数据有限和罕见疾病预测场景中表现出更强的鲁棒性。

Conclusion: LINKO框架成功整合了多个异构本体系统的知识，通过双轴传播机制显著提升了医疗概念表示学习效果，为EHR预测模型提供了更好的编码器解决方案。

Abstract: Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [80] [ImmunoAI: Accelerated Antibody Discovery Using Gradient-Boosted Machine Learning with Thermodynamic-Hydrodynamic Descriptors and 3D Geometric Interface Topology](https://arxiv.org/abs/2508.21082)
*Shawnak Shivakumar,Matthew Sandora*

Main category: q-bio.QM

TL;DR: ImmunoAI是一个机器学习框架，通过预测高亲和力抗体候选物，将抗体发现时间从10-12个月大幅缩短，搜索空间减少89%，并成功识别出针对hMPV病毒的高效抗体。


<details>
  <summary>Details</summary>
Motivation: 传统抗体发现流程需要10-12个月，无法满足快速应对病毒爆发的需求，特别是针对对人类健康构成严重威胁的人类偏肺病毒(hMPV)。

Method: 使用梯度提升模型(LightGBM)，基于热力学、流体力学和3D拓扑界面描述符训练模型，预测抗体-抗原结合亲和力。使用AlphaFold2预测hMPV A2.2变体的3D结构。

Result: 模型将抗体候选搜索空间减少89%，在SARS-CoV-2数据上RMSE从1.70降至0.92。成功识别出两个针对关键突变位点(G42V和E96K)的皮摩尔级亲和力抗体。

Conclusion: ImmunoAI显著缩短了设计周期，能够实现更快、基于结构信息的病毒爆发响应，为快速抗体发现提供了有效解决方案。

Abstract: Human metapneumovirus (hMPV) poses serious risks to pediatric, elderly, and
immunocompromised populations. Traditional antibody discovery pipelines require
10-12 months, limiting their applicability for rapid outbreak response. This
project introduces ImmunoAI, a machine learning framework that accelerates
antibody discovery by predicting high-affinity candidates using
gradient-boosted models trained on thermodynamic, hydrodynamic, and 3D
topological interface descriptors. A dataset of 213 antibody-antigen complexes
was curated to extract geometric and physicochemical features, and a LightGBM
regressor was trained to predict binding affinity with high precision. The
model reduced the antibody candidate search space by 89%, and fine-tuning on
117 SARS-CoV-2 binding pairs further reduced Root Mean Square Error (RMSE) from
1.70 to 0.92. In the absence of an experimental structure for the hMPV A2.2
variant, AlphaFold2 was used to predict its 3D structure. The fine-tuned model
identified two optimal antibodies with predicted picomolar affinities targeting
key mutation sites (G42V and E96K), making them excellent candidates for
experimental testing. In summary, ImmunoAI shortens design cycles and enables
faster, structure-informed responses to viral outbreaks.

</details>


### [81] [Data-driven Discovery of Digital Twins in Biomedical Research](https://arxiv.org/abs/2508.21484)
*Clémence Métayer,Annabelle Ballesta,Julien Martinelli*

Main category: q-bio.QM

TL;DR: 这篇论文综述了从生物时间序列数据自动推断数字双胎模型的方法，评估了简约回归和符号回归等算法的表现，并提出混合框架和标准化评测方法。


<details>
  <summary>Details</summary>
Motivation: 高速通量生物数据的丰富为数字双胎模型提供了基础，但人工数据整合效率低下，需要自动化推断方法来推动药物发现和个性化治疗。

Method: 综述了符号回归和简约回归等自动推断方法，并按八大生物学和方法论挑战进行评估，包括噪声/缺失数据、多条件、先验知识集成等。

Result: 简约回归通常表现更优，尤其是采用贝叶斯框架时。深度学习和大语言模型在先验知识集成方面显示潜力，但可靠性需提升。

Conclusion: 治疗数字双胎的发展需要混合框架，结合化学反应网络机理、贝叶斯不确定性量化和深度学习生成能力，并通过标准化评测推动方法进步。

Abstract: Recent technological advances have expanded the availability of
high-throughput biological datasets, enabling the reliable design of digital
twins of biomedical systems or patients. Such computational tools represent key
reaction networks driving perturbation or drug response and can guide drug
discovery and personalized therapeutics. Yet, their development still relies on
laborious data integration by the human modeler, so that automated approaches
are critically needed. The success of data-driven system discovery in Physics,
rooted in clean datasets and well-defined governing laws, has fueled interest
in applying similar techniques in Biology, which presents unique challenges.
Here, we reviewed methodologies for automatically inferring digital twins from
biological time series, which mostly involve symbolic or sparse regression. We
evaluate algorithms according to eight biological and methodological
challenges, associated to noisy/incomplete data, multiple conditions, prior
knowledge integration, latent variables, high dimensionality, unobserved
variable derivatives, candidate library design, and uncertainty quantification.
Upon these criteria, sparse regression generally outperformed symbolic
regression, particularly when using Bayesian frameworks. We further highlight
the emerging role of deep learning and large language models, which enable
innovative prior knowledge integration, though the reliability and consistency
of such approaches must be improved. While no single method addresses all
challenges, we argue that progress in learning digital twins will come from
hybrid and modular frameworks combining chemical reaction network-based
mechanistic grounding, Bayesian uncertainty quantification, and the generative
and knowledge integration capacities of deep learning. To support their
development, we further propose a benchmarking framework to evaluate methods
across all challenges.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [82] [Constructive l2-Discrepancy Minimization with Additive Deviations](https://arxiv.org/abs/2508.21423)
*Kunal Dutta*

Main category: cs.DM

TL;DR: 本文提出了一个多项式时间随机算法，用于解决ℓ₂范数下的有符号序列问题，达到了O(√d + log²n)的构造性界，改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 解决ℓ₂范数下有符号序列问题的构造性界改进需求，现有最佳构造性界为O(√d log n)，而存在性界为O(√d + log n)，需要缩小这一差距。

Method: 基于Bansal和Garg的框架，引入(i)额外的线性和谱正交约束来构建随机游走步骤的协方差矩阵，(ii)使用类似Freedman版本的Hanson-Wright集中不等式来处理滤波依赖的子高斯混沌和。

Result: 提出了多项式时间随机算法，能够找到符号使得最大部分和范数为O(√d + log²n)，当d ≥ log²n时解决了两个猜想。

Conclusion: 该算法显著改进了有符号序列问题的构造性界，同时通过构造性约简也为Steinitz问题提供了O(√d + log n)的构造性界。

Abstract: The \emph{signed series} problem in the $\ell_2$ norm asks, given set of
vectors $v_1,\ldots,v_n\in \mathbf{R}^d$ having at most unit $\ell_2$ norm,
does there always exist a series $(\varepsilon_i)_{i\in [n]}$ of $\pm 1$ signs
such that for all $i\in [n]$, $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i
v_i\|_2 = O(\sqrt{d})$. A result of Banaszczyk [2012, \emph{Rand. Struct.
Alg.}] states that there exist signs $\varepsilon_i\in \{-1,1\},\; i\in [n]$
such that $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i v_i\|_2 =
O(\sqrt{d+\log n})$. The best constructive bound known so far is of
$O(\sqrt{d\log n})$, by Bansal and Garg [2017, \emph{STOC.}, 2019, \emph{SIAM
J. Comput.}]. We give a polynomial-time randomized algorithm to find signs
$x(i) \in \{-1,1\},\; i\in [n]$ such that \[ \max_{i\in [n]} \|\sum_{j=1}^i
x(i)v_i\|_2 = O(\sqrt{d + \log^2 n}) = O(\sqrt{d}+\log n).\] By the
constructive reduction of Harvey and Samadi [\emph{COLT}, 2014], this also
yields a constructive bound of $O(\sqrt{d}+\log n)$ for the Steinitz problem in
the $\ell_2$-norm. Thus, our result settles both conjectures when $d \geq
\log^2n$. Our algorithm is based on the framework on Bansal and Garg, together
with a new analysis involving $(i)$ additional linear and spectral
orthogonality constraints during the construction of the covariance matrix of
the random walk steps, which allow us to control the quadratic variation in the
linear as well as the quadratic components of the discrepancy increment vector,
alongwith $(ii)$ a ``Freedman-like" version of the Hanson-Wright concentration
inequality, for filtration-dependent sums of subgaussian chaoses.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [83] [An Optimistic Gradient Tracking Method for Distributed Minimax Optimization](https://arxiv.org/abs/2508.21431)
*Yan Huang,Jinming Xu,Jiming Chen,Karl Henrik Johansson*

Main category: math.OC

TL;DR: 提出分布式乐观梯度跟踪方法DOGT和加速版本ADOGT，解决网络分布式极小极大优化问题，实现线性收敛和最优收敛速率


<details>
  <summary>Details</summary>
Motivation: 解决网络分布式极小极大优化问题，提升收敛性能，同时保持对目标函数异质性的鲁棒性

Method: 提出DOGT方法，通过构建代理函数捕捉局部目标函数相似性来近似集中式乐观方法；进一步集成加速共识协议得到ADOGT算法

Result: DOGT实现强凸-强凹目标函数的线性收敛；ADOGT达到最优收敛速率O(κlog(ε⁻¹))和通信复杂度O(κlog(ε⁻¹)/√(1-√ρ_W))

Conclusion: 所提算法在理论和实验上均表现出色，有效解决了分布式极小极大优化问题，具有最优收敛性能和鲁棒性

Abstract: This paper studies the distributed minimax optimization problem over
networks. To enhance convergence performance, we propose a distributed
optimistic gradient tracking method, termed DOGT, which solves a surrogate
function that captures the similarity between local objective functions to
approximate a centralized optimistic approach locally. Leveraging a
Lyapunov-based analysis, we prove that DOGT achieves linear convergence to the
optimal solution for strongly convex-strongly concave objective functions while
remaining robust to the heterogeneity among them. Moreover, by integrating an
accelerated consensus protocol, the accelerated DOGT (ADOGT) algorithm achieves
an optimal convergence rate of $\mathcal{O} \left( \kappa \log \left( \epsilon
^{-1} \right) \right)$ and communication complexity of $\mathcal{O} \left(
\kappa \log \left( \epsilon ^{-1} \right) /\sqrt{1-\sqrt{\rho _W}} \right)$ for
a suboptimality level of $\epsilon>0$, where $\kappa$ is the condition number
of the objective function and $\rho_W$ is the spectrum gap of the network.
Numerical experiments illustrate the effectiveness of the proposed algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

TL;DR: 本研究评估了三种深度学习方法来分类全景X射线图像中的牙齿状况，发现CNN与随机森林的混合模型表现最佳，准确率达到85.4%


<details>
  <summary>Details</summary>
Motivation: 开发自动化牙齿状况分类系统，为牙科诊断提供计算机辅助支持，提高诊断效率和准确性

Method: 使用1,512张全景X射线图像数据集，评估三种方法：自定义CNN、CNN特征提取与传统分类器混合模型、预训练架构微调，采用5折交叉验证

Result: 混合CNN随机森林模型准确率最高（85.4%），优于自定义CNN基线（74.3%）；预训练模型中VGG16表现最好（82.3%）

Conclusion: CNN特征提取与集成分类器结合是自动化牙科诊断支持的有效途径，但需要更大数据集和进一步临床验证

Abstract: This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [85] [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113)
*Jie Jiang,Qi Yang,Bolin Ni,Shiming Xiang,Han Hu,Houwen Peng*

Main category: cs.CV

TL;DR: R-4B是一个自适应思考的多模态大语言模型，能够根据问题复杂度自动决定是否启用思考过程，在保持高性能的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在处理简单问题时也会进行冗余的逐步思考过程，导致计算效率低下。需要一种能够自适应决定是否思考的模型来提高效率。

Method: 采用双模式退火训练方法，让模型同时具备思考和非思考能力，并通过双模式策略优化(BPO)改进模型决定是否激活思考过程的准确性。训练分为两个阶段：首先在精心策划的数据集上训练，然后在改进的GRPO框架下进行第二阶段训练。

Result: R-4B在25个具有挑战性的基准测试中实现了最先进的性能，在大多数任务中优于Qwen2.5-VL-7B，在推理密集型基准测试中与更大的模型(如Kimi-VL-A3B-Thinking-2506)性能相当，但计算成本更低。

Conclusion: R-4B通过自适应思考机制成功解决了多模态大语言模型在处理简单问题时的效率问题，在保持高性能的同时显著降低了计算开销，为高效的多模态推理提供了新的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking
capabilities have demonstrated remarkable performance on complex reasoning
problems. However, this thinking process is redundant for simple problems
solvable without complex reasoning. To address this inefficiency, we propose
R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on
problem complexity. The central idea of R-4B is to empower the model with both
thinking and non-thinking capabilities using bi-mode annealing, and apply
Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in
determining whether to activate the thinking process. Specifically, we first
train the model on a carefully curated dataset spanning various topics, which
contains samples from both thinking and non-thinking modes. Then it undergoes a
second phase of training under an improved GRPO framework, where the policy
model is forced to generate responses from both modes for each input query.
Experimental results show that R-4B achieves state-of-the-art performance
across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks
and achieves performance comparable to larger models such as
Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower
computational cost.

</details>


### [86] [SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing](https://arxiv.org/abs/2508.21402)
*Jakub Straka,Ivan Gruber*

Main category: cs.CV

TL;DR: SatDINO是一个基于DINO对比自监督学习的卫星图像表示学习模型，在多个数据集和测试设置中优于基于掩码自编码器的方法，并提出了新的地面采样距离编码和自适应视图采样增强技术。


<details>
  <summary>Details</summary>
Motivation: 遥感领域存在大量未标记数据，需要有效的自监督学习方法。本研究旨在探索DINO对比自监督方法在遥感图像预训练中的应用，开发专门针对卫星图像的表示学习模型。

Method: 提出SatDINO模型，基于DINO对比自监督学习方法，专门为卫星图像设计。引入了新的地面采样距离(GSD)编码方法和自适应视图采样技术，并进行了全面的消融实验评估各个组件。

Result: 在多个数据集和测试设置中，SatDINO显著优于基于掩码自编码器(MAE)的最先进方法，在多个基准测试中取得了有竞争力的结果。

Conclusion: SatDINO证明了对比自监督学习在遥感图像预训练中的有效性，提出的GSD编码和自适应视图采样增强技术可以独立应用于模型，为卫星图像表示学习提供了新的解决方案。

Abstract: Self-supervised learning has emerged as a powerful tool for remote sensing,
where large amounts of unlabeled data are available. In this work, we
investigate the use of DINO, a contrastive self-supervised method, for
pretraining on remote sensing imagery. We introduce SatDINO, a model tailored
for representation learning in satellite imagery. Through extensive experiments
on multiple datasets in multiple testing setups, we demonstrate that SatDINO
outperforms other state-of-the-art methods based on much more common masked
autoencoders (MAE) and achieves competitive results in multiple benchmarks.
  We also provide a rigorous ablation study evaluating SatDINO's individual
components. Finally, we propose a few novel enhancements, such as a new way to
incorporate ground sample distance (GSD) encoding and adaptive view sampling.
These enhancements can be used independently on our SatDINO model. Our code and
trained models are available at: https://github.com/strakaj/SatDINO.

</details>


### [87] [Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives](https://arxiv.org/abs/2508.21418)
*Gernot Fiala,Markus Plass,Robert Harb,Peter Regitnig,Kristijan Skok,Wael Al Zoughbi,Carmen Zerner,Paul Torke,Michaela Kargl,Heimo Müller,Tomas Brazdil,Matej Gallo,Jaroslav Kubín,Roman Stoklasa,Rudolf Nenutil,Norman Zerbe,Andreas Holzinger,Petr Holub*

Main category: cs.CV

TL;DR: 提出了一种为全屏截图图像(WSI)生成二维索引地图的标准框架，通过三层组织结构提供细粒度的组织地图信息


<details>
  <summary>Details</summary>
Motivation: 解决WSI图像缺乏标准元数据标准的问题，避免大规模数据集中手动检查的需求

Method: 使用通用语法和语义学构建三层组织地图：来源层、组织类型层和病理改变层

Result: 实现了不同目录之间的互操性，并在WSI目录、机器学习和图象表示中证明了其优势

Conclusion: 该标准框架能够有效地管理大规模WSI数据集，为AI算法开发提供了重要的元数据支撑

Abstract: A Whole Slide Image (WSI) is a high-resolution digital image created by
scanning an entire glass slide containing a biological specimen, such as tissue
sections or cell samples, at multiple magnifications. These images can be
viewed, analyzed, shared digitally, and are used today for Artificial
Intelligence (AI) algorithm development. WSIs are used in a variety of fields,
including pathology for diagnosing diseases and oncology for cancer research.
They are also utilized in neurology, veterinary medicine, hematology,
microbiology, dermatology, pharmacology, toxicology, immunology, and forensic
science.
  When assembling cohorts for the training or validation of an AI algorithm, it
is essential to know what is present on such a WSI. However, there is currently
no standard for this metadata, so such selection has mainly been done through
manual inspection, which is not suitable for large collections with several
million objects.
  We propose a general framework to generate a 2D index map for WSI and a
profiling mechanism for specific application domains. We demonstrate this
approach in the field of clinical pathology, using common syntax and semantics
to achieve interoperability between different catalogs.
  Our approach augments each WSI collection with a detailed tissue map that
provides fine-grained information about the WSI content. The tissue map is
organized into three layers: source, tissue type, and pathological alterations,
with each layer assigning segments of the WSI to specific classes.
  We illustrate the advantages and applicability of the proposed standard
through specific examples in WSI catalogs, Machine Learning (ML), and
graph-based WSI representations.

</details>


### [88] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 该论文提出从单词级OCR向行级OCR的转变，通过绕过错误单词检测并提供更大句子上下文来提升准确性和效率


<details>
  <summary>Details</summary>
Motivation: 传统OCR技术存在字符分割错误问题，现代方法转向单词级OCR但仍受限于单词分割瓶颈。作者观察到需要进一步发展到行级OCR来突破这一限制

Method: 提出行级OCR方法，直接处理整行文本而不是单个单词，从而避免单词检测错误并利用更大的语言模型上下文

Result: 实验显示端到端准确率提升5.4%，效率比基于单词的流程提高4倍，并贡献了包含251个英文页面图像的行级标注数据集

Conclusion: 行级OCR是自然且逻辑的进展，能显著提升OCR准确性和效率，特别适合文档图像处理，且能与大型语言模型的持续改进相兼容

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


### [89] [Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations](https://arxiv.org/abs/2508.21769)
*Ha Min Son,Zhe Zhao,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: 本文提出了CLIP-DCA方法，通过增强域感知表示和域不变分类的解耦，来提升CLIP模型在域泛化任务中的性能，特别是在更具挑战性的分布外数据集上。


<details>
  <summary>Details</summary>
Motivation: 当前对CLIP等基础模型的域泛化评估存在局限性，因为网络规模的预训练数据可能已经覆盖了许多基准测试。需要更有效地评估CLIP在真实未见域数据上的性能。

Method: 提出CLIP-DCA方法：1）使用单独的域头识别和增强CLIP编码器中的域感知表示；2）通过合成生成多样化域数据；3）鼓励域特征与分类特征解耦以实现域不变分类。

Result: CLIP-DCA在更具挑战性的评估中相比现有方法显示出显著改进，特别是在更分布外的数据集上表现优异。

Conclusion: 增强域感知表示是基础模型实现有效域不变分类的前提，CLIP-DCA通过解耦方法成功提升了CLIP在域泛化任务中的性能。

Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.

</details>


### [90] [Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering](https://arxiv.org/abs/2508.21773)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出了无监督视频持续学习(uVCL)的新场景，使用非参数化的核密度估计方法处理无标签、无任务边界的视频序列学习问题，在多个标准数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 视频数据具有丰富的时空信息，但在无监督持续学习领域尚未充分探索。现有研究主要依赖标签和任务边界的监督学习，而获取标注数据成本高且不实用。

Method: 使用无监督视频transformer网络提取深度嵌入特征，采用核密度估计(KDE)作为数据的非参数概率表示，引入新颖性检测标准动态扩展内存簇以捕获新知识，并利用先前任务的迁移学习作为初始状态。

Result: 在UCF101、HMDB51和Something-to-Something V2三个标准视频动作识别数据集上进行了深入评估，不使用任何标签或类别边界，结果表明所提方法在连续学习多个任务时显著提升了模型性能。

Conclusion: 提出的无监督视频持续学习方法有效解决了视频数据处理的计算和内存挑战，为非参数化视频持续学习提供了可行的解决方案，在无标签条件下实现了良好的知识积累和迁移。

Abstract: We propose a realistic scenario for the unsupervised video learning where
neither task boundaries nor labels are provided when learning a succession of
tasks. We also provide a non-parametric learning solution for the
under-explored problem of unsupervised video continual learning. Videos
represent a complex and rich spatio-temporal media information, widely used in
many applications, but which have not been sufficiently explored in
unsupervised continual learning. Prior studies have only focused on supervised
continual learning, relying on the knowledge of labels and task boundaries,
while having labeled data is costly and not practical. To address this gap, we
study the unsupervised video continual learning (uVCL). uVCL raises more
challenges due to the additional computational and memory requirements of
processing videos when compared to images. We introduce a general benchmark
experimental protocol for uVCL by considering the learning of unstructured
video data categories during each task. We propose to use the Kernel Density
Estimation (KDE) of deep embedded video features extracted by unsupervised
video transformer networks as a non-parametric probabilistic representation of
the data. We introduce a novelty detection criterion for the incoming new task
data, dynamically enabling the expansion of memory clusters, aiming to capture
new knowledge when learning a succession of tasks. We leverage the use of
transfer learning from the previous tasks as an initial state for the knowledge
transfer to the current learning task. We found that the proposed methodology
substantially enhances the performance of the model when successively learning
many tasks. We perform in-depth evaluations on three standard video action
recognition datasets, including UCF101, HMDB51, and Something-to-Something V2,
without using any labels or class boundaries.

</details>


### [91] [Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight](https://arxiv.org/abs/2508.21777)
*Ugur Dinc,Jibak Sarkar,Philipp Schubert,Sabine Semrau,Thomas Weissmann,Andre Karius,Johann Brand,Bernd-Niklas Axer,Ahmed Gomaa,Pluvio Stephan,Ishita Sheth,Sogand Beirami,Annette Schwarz,Udo Gaipl,Benjamin Frey,Christoph Bert,Stefanie Corradini,Rainer Fietkau,Florian Putz*

Main category: cs.CV

TL;DR: GPT-5在放射肿瘤学领域表现出色，在多项选择题测试中准确率达92.8%，显著优于GPT-4和GPT-3.5。在真实病例治疗建议生成中，正确性和全面性评分较高，幻觉现象罕见，但仍需专家监督。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-5在临床决策支持，特别是放射肿瘤学领域的应用潜力，验证其在该专业领域的实际表现。

Method: 使用两个基准测试：ACR放射肿瘤学培训考试（300道选择题）和60个真实放射肿瘤病例。四位认证放射肿瘤学家评估治疗建议的正确性、全面性和幻觉现象。

Result: GPT-5在选择题测试中准确率92.8%，显著优于GPT-4（78.8%）和GPT-3.5（62.1%）。病例评估中正确性评分3.24/4，全面性评分3.59/4，幻觉罕见。

Conclusion: GPT-5在放射肿瘤学领域表现优异，但仍存在改进空间，生成的建议需要专家监督才能临床使用。

Abstract: Introduction: Large language models (LLM) have shown great potential in
clinical decision support. GPT-5 is a novel LLM system that has been
specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benchmarks: (i) the
ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300
multiple-choice items, and (ii) a curated set of 60 authentic radiation
oncologic vignettes representing diverse disease sites and treatment
indications. For the vignette evaluation, GPT-5 was instructed to generate
concise therapeutic plans. Four board-certified radiation oncologists rated
correctness, comprehensiveness, and hallucinations. Inter-rater reliability was
quantified using Fleiss' \k{appa}.
  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,
outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were
most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's
treatment recommendations were rated highly for correctness (mean 3.24/4, 95%
CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).
Hallucinations were rare with no case reaching majority consensus for their
presence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 for
correctness), reflecting inherent variability in clinical judgment. Errors
clustered in complex scenarios requiring precise trial knowledge or detailed
clinical adaptation.
  Discussion: GPT-5 clearly outperformed prior model variants on the radiation
oncology multiple-choice benchmark. Although GPT-5 exhibited favorable
performance in generating real-world radiation oncology treatment
recommendations, correctness ratings indicate room for further improvement.
While hallucinations were infrequent, the presence of substantive errors
underscores that GPT-5-generated recommendations require rigorous expert
oversight before clinical implementation.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [92] [Some Applications and Limitations of Convex Optimization Hierarchies for Discrete and Continuous Optimization Problems](https://arxiv.org/abs/2508.21327)
*Mrinalkanti Ghosh*

Main category: cs.CC

TL;DR: 本文研究了凸松弛层次在离散和连续优化问题中的算法应用与局限性，包括CSP问题的线性规划松弛二分性、多项式优化的SoS SDP松弛性能分析，以及矩阵范数近似计算的NP-hardness结果


<details>
  <summary>Details</summary>
Motivation: 探索凸松弛层次（如Sherali-Adams层次和SoS层次）在各种优化问题中的近似能力和计算效率，理解这些方法的理论极限和实际应用价值

Method: 采用几何分析工具，包括线性规划松弛、半定规划松弛（SoS层次）、嵌入定理和凸体直径估计等技术，结合理论证明和算法分析

Result: 证明了CSP问题的基本LP松弛在近似能力上不弱于超常数层次；为多项式优化提供了运行时间与近似比的权衡；首次证明了超压缩情况下矩阵范数近似的NP-hardness

Conclusion: 几何方法在优化问题分析中具有核心作用，凸松弛层次在不同类型优化问题中表现出不同的近似能力和计算特性，为算法设计提供了理论指导

Abstract: This thesis explores algorithmic applications and limitations of convex
relaxation hierarchies for approximating some discrete and continuous
optimization problems.
  - We show a dichotomy of approximability of constraint satisfaction problems
(CSPs) by linear programming (LP) relaxations: for every CSP, the approximation
obtained by a basic LP relaxation, is no weaker than the approximation obtained
using relaxations given by super-constant levels of the Sherali-Adams hierarchy
on instances of size $n$.
  - For the problem of approximating the absolute maximum of an n-variate
degree-d homogeneous polynomial f with real coefficients over the unit sphere,
we analyze the optimum value of the level-t sum-of-squares (SoS) SDP relaxation
of the problem. Our results offer a trade-off between the approximation ratio
and running time, which can take advantage of additional structure in the
polynomial, such as non-negativity or sparsity of the coefficients.
  - We study the problem of approximating the $p \to q$-norm of a matrix $A$,
and prove the first NP-hardness result for approximating norms in the
hypercontractive case $1< p < q < \infty$. We also prove almost tight
algorithmic results for the case when $p \geq q$ (with $2 \in [q,p]$) where
constant factor approximations for the matrix norms are possible.
  A common theme for these results is their connection to geometry. For the
discrete optimization problem of CSP, geometry appears as a crucial tool for
our lower bound proof. For the problem of polynomial optimization, we show that
SDPs capture and extend earlier algorithms based on diameter estimation for
convex bodies. For the matrix (operator) norm problem, the definition itself is
geometric in nature and embedding theorems play a crucial role in our proofs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [93] [Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators](https://arxiv.org/abs/2508.21524)
*Wenyong Zhou,Zhengwu Liu,Yuan Ren,Ngai Wong*

Main category: cs.AR

TL;DR: 提出了一种新颖的二进制权重多比特激活(BWMA)方法，用于CIM加速器上的CNN部署，在保持硬件效率的同时显著提升模型精度


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用二进制权重和激活量化但精度较低，要么使用多比特权重和激活但效率有限，需要找到精度和效率的最佳平衡点

Method: 推导每层权重量化的闭式解以提升二值化权重表示能力；开发可微分激活量化函数来逼近理想多比特函数，避免繁琐的参数搜索

Result: 在CIFAR-10和ImageNet数据集上分别获得1.44%-5.46%和0.35%-5.37%的精度提升；硬件仿真显示4比特激活量化在硬件成本和模型性能间达到最优平衡

Conclusion: BWMA方法成功解决了CIM加速器上CNN部署时精度与效率的权衡问题，为实际应用提供了有效的量化解决方案

Abstract: Compute-in-memory (CIM) accelerators have emerged as a promising way for
enhancing the energy efficiency of convolutional neural networks (CNNs).
Deploying CNNs on CIM platforms generally requires quantization of network
weights and activations to meet hardware constraints. However, existing
approaches either prioritize hardware efficiency with binary weight and
activation quantization at the cost of accuracy, or utilize multi-bit weights
and activations for greater accuracy but limited efficiency. In this paper, we
introduce a novel binary weight multi-bit activation (BWMA) method for CNNs on
CIM-based accelerators. Our contributions include: deriving closed-form
solutions for weight quantization in each layer, significantly improving the
representational capabilities of binarized weights; and developing a
differentiable function for activation quantization, approximating the ideal
multi-bit function while bypassing the extensive search for optimal settings.
Through comprehensive experiments on CIFAR-10 and ImageNet datasets, we show
that BWMA achieves notable accuracy improvements over existing methods,
registering gains of 1.44\%-5.46\% and 0.35\%-5.37\% on respective datasets.
Moreover, hardware simulation results indicate that 4-bit activation
quantization strikes the optimal balance between hardware cost and model
performance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [94] [Population-Scale Network Embeddings Expose Educational Divides in Network Structure Related to Right-Wing Populist Voting](https://arxiv.org/abs/2508.21236)
*Malte Lüken,Javier Garcia-Bernardo,Sreeparna Deb,Flavio Hafner,Megha Khosla*

Main category: cs.SI

TL;DR: 使用人口级网络嵌入技术分析荷兰全国网络数据，通过学校关联网络结构预测右羽民沃投票行为，并实现了网络嵌入的可解释性。


<details>
  <summary>Details</summary>
Motivation: 利用行政登记数据构建人口级网络，通过机器学习嵌入技术自动提取个体网络位置特征，识别与右羽民沃投票相关的结构性网络差异。

Method: 从荷兰全国人口网络（包括邻里、工作、家庭、家庭成员和学校共享环境）生成嵌入表征，进行右羽民沃投票预测。通过嵌入转换使维度更稀疏和正交，实现可解释性分析。

Result: 嵌入单独预测效果超过随机水平但差于个人特征。组合嵌入和个人特征仅轻微提升预测效果。经过转换后发现一个嵌入维度与右羽民沃投票强相关，对应不同学校联系和教育水平的网络结构差异。

Conclusion: 研究方法上展示了人口级网络嵌入的可解释性方法，实质上发现教育网络结构差异与右羽民沃投票存在联系。

Abstract: Administrative registry data can be used to construct population-scale
networks whose ties reflect shared social contexts between persons. With
machine learning, such networks can be encoded into numerical representations
-- embeddings -- that automatically capture individuals' position within the
network. We created embeddings for all persons in the Dutch population from a
population-scale network that represents five shared contexts: neighborhood,
work, family, household, and school. To assess the informativeness of these
embeddings, we used them to predict right-wing populist voting. Embeddings
alone predicted right-wing populist voting above chance-level but performed
worse than individual characteristics. Combining the best subset of embeddings
with individual characteristics only slightly improved predictions. However,
after transforming the embeddings to make their dimensions more sparse and
orthogonal, we found that one embedding dimension was strongly associated with
the outcome. Mapping this dimension back to the population network revealed
differences in network structure related to right-wing populist voting between
different school ties and achieved education levels. Our study contributes
methodologically by demonstrating how population-scale network embeddings can
be made interpretable, and substantively by linking structural network
differences in education to right-wing populist voting.

</details>


### [95] [Faster Inference of Cell Complexes from Flows via Matrix Factorization](https://arxiv.org/abs/2508.21372)
*Til Spreuer,Josef Hoppe,Michael T. Schaub*

Main category: cs.SI

TL;DR: 提出了一种基于矩阵分解的启发式方法，用于在图数据中通过添加2-cells来构建cell complex，从而稀疏表示边流信号


<details>
  <summary>Details</summary>
Motivation: 解决在图上观察到的边流信号如何通过提升到cell complex来获得稀疏的梯度流和旋度流表示的问题，该问题已被证明是NP难的

Method: 开发了一种新颖的基于矩阵分解的启发式算法，通过添加2-cells（由闭合非相交路径环绕的多边形）来扩充图结构

Result: 计算实验表明，新方法计算成本显著低于先前启发式方法，在大多数设置下性能仅略差，在噪声设置下在解质量和计算速度方面均优于现有技术

Conclusion: 该方法为NP难的图提升问题提供了高效的近似解决方案，特别是在噪声环境下表现出优越性能

Abstract: We consider the following inference problem: Given a set of edge-flow signals
observed on a graph, lift the graph to a cell complex, such that the observed
edge-flow signals can be represented as a sparse combination of gradient and
curl flows on the cell complex. Specifically, we aim to augment the observed
graph by a set of 2-cells (polygons encircled by closed, non-intersecting
paths), such that the eigenvectors of the Hodge Laplacian of the associated
cell complex provide a sparse, interpretable representation of the observed
edge flows on the graph. As it has been shown that the general problem is
NP-hard in prior work, we here develop a novel matrix-factorization-based
heuristic to solve the problem. Using computational experiments, we demonstrate
that our new approach is significantly less computationally expensive than
prior heuristics, while achieving only marginally worse performance in most
settings. In fact, we find that for specifically noisy settings, our new
approach outperforms the previous state of the art in both solution quality and
computational speed.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [96] [Adapting to Change: A Comparison of Continual and Transfer Learning for Modeling Building Thermal Dynamics under Concept Drifts](https://arxiv.org/abs/2508.21615)
*Fabian Raisch,Max Langtry,Felix Koch,Ruchi Choudhary,Christoph Goebel,Benjamin Tischler*

Main category: eess.SY

TL;DR: 本研究比较了多种持续学习(CL)和迁移学习(TL)策略，以及从头训练的模型，用于建筑热动力学建模。提出了季节性记忆学习(SML)策略，在包含概念流移的场景下显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前迁移学习是在有限数据下建模建筑热动力学的最有效方法，但随着时间推移收集到更多运营数据时，如何更新模型以提高预测准确性并应对概念流移(如改造或占用情况变化)的挑战仍不明确。

Method: 研究比较了多种CL和TL策略，以及从头训练的模型。使用5-7年的模拟数据，包含改造和占用情况变化导致的概念流移场景。提出了季节性记忆学习(SML)策略。

Result: SML策略在无概念流移情况下比初始精细调整预测准确性提高28.1%，在有概念流移情况下提高34.9%，同时保持低计算开销。

Conclusion: 季节性记忆学习(SML)是一种高效的CL策略，能够在建筑运营过程中持续改善热动力学模型的预测性能，特别是在存在概念流移的情况下。

Abstract: Transfer Learning (TL) is currently the most effective approach for modeling
building thermal dynamics when only limited data are available. TL uses a
pretrained model that is fine-tuned to a specific target building. However, it
remains unclear how to proceed after initial fine-tuning, as more operational
measurement data are collected over time. This challenge becomes even more
complex when the dynamics of the building change, for example, after a retrofit
or a change in occupancy. In Machine Learning literature, Continual Learning
(CL) methods are used to update models of changing systems. TL approaches can
also address this challenge by reusing the pretrained model at each update step
and fine-tuning it with new measurement data. A comprehensive study on how to
incorporate new measurement data over time to improve prediction accuracy and
address the challenges of concept drifts (changes in dynamics) for building
thermal dynamics is still missing.
  Therefore, this study compares several CL and TL strategies, as well as a
model trained from scratch, for thermal dynamics modeling during building
operation. The methods are evaluated using 5--7 years of simulated data
representative of single-family houses in Central Europe, including scenarios
with concept drifts from retrofits and changes in occupancy. We propose a CL
strategy (Seasonal Memory Learning) that provides greater accuracy improvements
than existing CL and TL methods, while maintaining low computational effort.
SML outperformed the benchmark of initial fine-tuning by 28.1\% without concept
drifts and 34.9\% with concept drifts.

</details>


### [97] [DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers](https://arxiv.org/abs/2508.21797)
*Navid Aftabi,Abhishek Hanchate,Satish Bukkapatnam,Dan Li*

Main category: eess.SY

TL;DR: DynaMark是一个基于强化学习的自适应动态水印框架，通过马尔可夫决策过程在线学习最优水印策略，有效防御工业4.0机床控制器的重放攻击，显著降低水印能量消耗并保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中高度网络化的机床控制器容易受到重放攻击，现有动态水印方案假设线性高斯动态和使用恒定统计量，无法适应机床控制器时变和部分专有的行为特性。

Method: 提出DynaMark强化学习框架，将动态水印建模为马尔可夫决策过程，在线学习自适应策略，动态调整零均值高斯水印的协方差，无需系统知识。开发贝叶斯置信度更新机制实现实时检测置信度。

Result: 在西门子Sinumerik 828D控制器数字孪生上，相比恒定方差基线，水印能量降低70%同时保持标称轨迹，平均检测延迟相当于一个采样间隔。物理步进电机测试平台验证了这些发现。

Conclusion: DynaMark框架有效解决了现有动态水印方案的局限性，在保持控制性能的同时显著降低能耗，并提供了快速攻击检测能力，超越了现有基准方法。

Abstract: Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime
targets for replay attacks that use outdated sensor data to manipulate
actuators. Dynamic watermarking can reveal such tampering, but current schemes
assume linear-Gaussian dynamics and use constant watermark statistics, making
them vulnerable to the time-varying, partly proprietary behavior of MTCs. We
close this gap with DynaMark, a reinforcement learning framework that models
dynamic watermarking as a Markov decision process (MDP). It learns an adaptive
policy online that dynamically adapts the covariance of a zero-mean Gaussian
watermark using available measurements and detector feedback, without needing
system knowledge. DynaMark maximizes a unique reward function balancing control
performance, energy consumption, and detection confidence dynamically. We
develop a Bayesian belief updating mechanism for real-time detection confidence
in linear systems. This approach, independent of specific system assumptions,
underpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D
controller digital twin, DynaMark achieves a reduction in watermark energy by
70% while preserving the nominal trajectory, compared to constant variance
baselines. It also maintains an average detection delay equivalent to one
sampling interval. A physical stepper-motor testbed validates these findings,
rapidly triggering alarms with less control performance decline and exceeding
existing benchmarks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [98] [Synthetic CVs To Build and Test Fairness-Aware Hiring Tools](https://arxiv.org/abs/2508.21179)
*Jorge Saldivar,Anna Gatzioura,Carlos Castillo*

Main category: cs.CY

TL;DR: 本文提出了一种构建合成简历数据集的方法，用于研究算法招聘中的偏见问题，包含1730份基于真实材料建模的简历


<details>
  <summary>Details</summary>
Motivation: 算法招聘系统可能无意中引入基于年龄、性别、国籍等因素的偏见，但缺乏反映多样化背景特征的简历数据集来进行偏见测量和缓解研究

Method: 通过数据捐赠活动收集真实材料，构建合成简历数据集，特征基于真实材料建模

Result: 成功创建了包含1730份简历的数据集，可作为算法招聘歧视研究的基准标准

Conclusion: 该合成数据集为解决算法招聘中的偏见问题提供了重要的研究资源，有望成为该领域研究的基准工具

Abstract: Algorithmic hiring has become increasingly necessary in some sectors as it
promises to deal with hundreds or even thousands of applicants. At the heart of
these systems are algorithms designed to retrieve and rank candidate profiles,
which are usually represented by Curricula Vitae (CVs). Research has shown,
however, that such technologies can inadvertently introduce bias, leading to
discrimination based on factors such as candidates' age, gender, or national
origin. Developing methods to measure, mitigate, and explain bias in
algorithmic hiring, as well as to evaluate and compare fairness techniques
before deployment, requires sets of CVs that reflect the characteristics of
people from diverse backgrounds.
  However, datasets of these characteristics that can be used to conduct this
research do not exist. To address this limitation, this paper introduces an
approach for building a synthetic dataset of CVs with features modeled on real
materials collected through a data donation campaign. Additionally, the
resulting dataset of 1,730 CVs is presented, which we envision as a potential
benchmarking standard for research on algorithmic hiring discrimination.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [99] [Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)](https://arxiv.org/abs/2508.21205)
*Usman A. Khan,Mouhacine Benosman,Wenliang Liu,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: 基于最优运输理论和模型预测控制的多机器人路径规划与调度方法，解决路径重叠和死锁问题


<details>
  <summary>Details</summary>
Motivation: 多机器人在共享空间中导航时，先映射机器人到目标再规划路径的方法容易导致路径重叠和死锁问题

Method: 将空间离散化为K个单元，构建K×K转移成本矩阵，利用最优运输理论求解最优且不重叠的单元转移路径，并集成重新规划和模型预测控制处理动态问题

Result: 算法在最坏情况下需要O(K³logK)计算量，在良好问题中需要O(K²logK)计算量，能够提供最优且不重叠的路径规划

Conclusion: 该方法通过最优运输理论有效解决了多机器人路径规划中的路径重叠和死锁问题，具有较高的计算效率和实用性

Abstract: In this paper, we propose a novel methodology for path planning and
scheduling for multi-robot navigation that is based on optimal transport theory
and model predictive control. We consider a setup where $N$ robots are tasked
to navigate to $M$ targets in a common space with obstacles. Mapping robots to
targets first and then planning paths can result in overlapping paths that lead
to deadlocks. We derive a strategy based on optimal transport that not only
provides minimum cost paths from robots to targets but also guarantees
non-overlapping trajectories. We achieve this by discretizing the space of
interest into $K$ cells and by imposing a ${K\times K}$ cost structure that
describes the cost of transitioning from one cell to another. Optimal transport
then provides \textit{optimal and non-overlapping} cell transitions for the
robots to reach the targets that can be readily deployed without any scheduling
considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$
computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for
well-behaved problems. To further accommodate potentially overlapping
trajectories (unavoidable in certain situations) as well as robot dynamics, we
show that a temporal structure can be integrated into optimal transport with
the help of \textit{replans} and \textit{model predictive control}.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [100] [A Soft Inducement Framework for Incentive-Aided Steering of No-Regret Players](https://arxiv.org/abs/2508.21672)
*Asrin Efe Yorulmaz,Raj Kiriti Velicheti,Melih Bastopcu,Tamer Başar*

Main category: cs.GT

TL;DR: 本文研究中介增强的两人正规形式博弈中的引导问题，通过信息和激励设计指导玩家达到特定行动配置。分析了信息设计的局限性，提出了结合一次性信息设计和重复博弈的新方法，提高了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过中介的信息和激励设计来引导博弈玩家达到期望的行动配置，解决单纯信息设计或次线性支付方案的局限性。

Method: 首先分析成功引导的可能性条件，然后提出结合一次性信息设计和重复博弈的增强方法，将先验互动转化为Stackelberg博弈。

Result: 证明了该方法能够以高概率将玩家行动配置的收敛速度提高一个常数因子，并通过实证结果支持理论发现。

Conclusion: 提出的增强方法有效克服了信息设计的限制，为博弈引导提供了更高效的解决方案，在理论和实践上都显示出优势。

Abstract: In this work, we investigate a steering problem in a mediator-augmented
two-player normal-form game, where the mediator aims to guide players toward a
specific action profile through information and incentive design. We first
characterize the games for which successful steering is possible. Moreover, we
establish that steering players to any desired action profile is not always
achievable with information design alone, nor when accompanied with sublinear
payment schemes. Consequently, we derive a lower bound on the constant payments
required per round to achieve this goal. To address these limitations incurred
with information design, we introduce an augmented approach that involves a
one-shot information design phase before the start of the repeated game,
transforming the prior interaction into a Stackelberg game. Finally, we
theoretically demonstrate that this approach improves the convergence rate of
players' action profiles to the target point by a constant factor with high
probability, and support it with empirical results.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [101] [Can Layer-wise SSL Features Improve Zero-Shot ASR Performance for Children's Speech?](https://arxiv.org/abs/2508.21225)
*Abhijit Sinha,Hemant Kumar Kathania,Sudarsana Reddy Kadiri,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 本文研究通过提取自监督学习模型的层级特征，在零检查场景下显著提升了儿童语音识别性能，Wav2Vec2模型第22层获得最低词错率为5.15%。


<details>
  <summary>Details</summary>
Motivation: 因为儿童语音具有独特的声学和语言特征，传统ASR系统在处理儿童语音时表现异常困难，而自监督学习模型在成人语音上取得了重大进步，因此需要研究如何利用这些模型来改善儿童语音识别性能。

Method: 研究了Wav2Vec2、HuBERT、Data2Vec和WavLM等领先自监督学习模型的各层特征提取效果，并将这些特征集成到基于Kaldi的简化DNN语音识别系统中。在零检查场景下，使用WSJCAM0成人语音训练，使用PFSTAR儿童语音测试。

Result: 实验结果显示Wav2Vec2模型第22层获得最低词错率5.15%，相比于直接使用Wav2Vec2的零检查解码（WER 10.65%）相对提升51.64%。随着年龄增长，性能持续改善，广泛年龄段均有显著收益。在CMU Kids数据集上的进一步实验确认了方法的普遍性。

Conclusion: 通过提取自监督学习模型的适当层特征，可以在零检查场景下显著提升儿童语音识别性能，为解决儿童语音识别挑战提供了有效方法。

Abstract: Automatic Speech Recognition (ASR) systems often struggle to accurately
process children's speech due to its distinct and highly variable acoustic and
linguistic characteristics. While recent advancements in self-supervised
learning (SSL) models have greatly enhanced the transcription of adult speech,
accurately transcribing children's speech remains a significant challenge. This
study investigates the effectiveness of layer-wise features extracted from
state-of-the-art SSL pre-trained models - specifically, Wav2Vec2, HuBERT,
Data2Vec, and WavLM in improving the performance of ASR for children's speech
in zero-shot scenarios. A detailed analysis of features extracted from these
models was conducted, integrating them into a simplified DNN-based ASR system
using the Kaldi toolkit. The analysis identified the most effective layers for
enhancing ASR performance on children's speech in a zero-shot scenario, where
WSJCAM0 adult speech was used for training and PFSTAR children speech for
testing. Experimental results indicated that Layer 22 of the Wav2Vec2 model
achieved the lowest Word Error Rate (WER) of 5.15%, representing a 51.64%
relative improvement over the direct zero-shot decoding using Wav2Vec2 (WER of
10.65%). Additionally, age group-wise analysis demonstrated consistent
performance improvements with increasing age, along with significant gains
observed even in younger age groups using the SSL features. Further experiments
on the CMU Kids dataset confirmed similar trends, highlighting the
generalizability of the proposed approach.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [102] [Surface Stability Modeling with Universal Machine Learning Interatomic Potentials: A Comprehensive Cleavage Energy Benchmarking Study](https://arxiv.org/abs/2508.21663)
*Ardavan Mehdizadeh,Peter Schindler*

Main category: cond-mat.mtrl-sci

TL;DR: 对19种最先进的通用机器学习原子间势能(uMLIPs)在解理能预测方面的系统性评估，发现训练数据组成比架构复杂性更重要，适当数据训练的简单模型能达到与复杂Transformer相当的精度且计算速度提升10-100倍


<details>
  <summary>Details</summary>
Motivation: 尽管MLIPs在预测体材料性质方面取得了显著成功，但尚未有系统评估这些通用MLIPs在预测解理能方面的表现，而解理能是控制断裂、催化、表面稳定性和界面现象的关键性质

Method: 使用包含36,718个板状结构的DFT数据库，对19种最先进的uMLIPs进行综合基准测试，评估不同架构范式在化学成分、晶体系统、厚度和表面取向方面的性能

Result: OMat24数据集训练的模型平均绝对百分比误差低于6%，在87%的情况下正确识别热力学最稳定表面终止，而仅使用平衡数据集训练的相同架构模型误差高出5倍，表面吸附数据训练的模型性能下降17倍

Conclusion: 研究结果表明社区应专注于生成能够捕捉相关物理现象的战略性训练数据，而非过度追求架构复杂性，简单架构配合适当数据可获得与复杂模型相当的精度和显著的计算加速

Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized
computational materials science by bridging the gap between quantum mechanical
accuracy and classical simulation efficiency, enabling unprecedented
exploration of materials properties across the periodic table. Despite their
remarkable success in predicting bulk properties, no systematic evaluation has
assessed how well these universal MLIPs (uMLIPs) can predict cleavage energies,
a critical property governing fracture, catalysis, surface stability, and
interfacial phenomena. Here, we present a comprehensive benchmark of 19
state-of-the-art uMLIPs for cleavage energy prediction using our previously
established density functional theory (DFT) database of 36,718 slab structures
spanning elemental, binary, and ternary metallic compounds. We evaluate diverse
architectural paradigms, analyzing their performance across chemical
compositions, crystal systems, thickness, and surface orientations. Our results
reveal that training data composition dominates architectural sophistication:
models trained on the Open Materials 2024 (OMat24) dataset, which emphasizes
non-equilibrium configurations, achieve mean absolute percentage errors below
6% and correctly identify the thermodynamically most stable surface
terminations in 87% of cases, without any explicit surface energy training. In
contrast, architecturally identical models trained on equilibrium-only datasets
show five-fold higher errors, while models trained on surface-adsorbate data
fail catastrophically with a 17-fold degradation. Remarkably, simpler
architectures trained on appropriate data achieve comparable accuracy to
complex transformers while offering 10-100x computational speedup. These
findings show that the community should focus on strategic training data
generation that captures the relevant physical phenomena.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [103] [Block Encoding of Sparse Matrices via Coherent Permutation](https://arxiv.org/abs/2508.21667)
*Abhishek Setty*

Main category: quant-ph

TL;DR: 提出了一个统一的框架，用于高效实现任意稀疏矩阵的块编码，解决了多控制X门开销、振幅重排序和硬件连接性等关键问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵的块编码是量子奇异值变换、哈密顿模拟和量子线性求解器等强大量子算法的基础，但任意稀疏矩阵的高效门级实现仍然是一个主要挑战。

Method: 采用组合优化方法系统分配控制量子比特实现最近邻连接，并使用相干置换算子在保持叠加态的同时实现振幅重排序。

Result: 在结构化稀疏矩阵上展示了方法有效性，显著降低了电路深度和控制开销。

Conclusion: 该框架弥合了量子算法理论公式与实际电路实现之间的差距，为任意稀疏矩阵的高效块编码提供了实用的门级构造方案。

Abstract: Block encoding of sparse matrices underpins powerful quantum algorithms such
as quantum singular value transformation, Hamiltonian simulation, and quantum
linear solvers, but its efficient gate-level implementation for arbitrary
sparse matrices remains a major challenge. We introduce a unified framework
that overcomes the key obstacles of multi-controlled X gates overhead,
amplitude reordering, and hardware connectivity, enabling efficient block
encoding for arbitrary sparse matrices with explicit gate-level constructions.
Central to our approach are a novel connection with combinatorial optimization,
which enables systematic assignment of control qubits to achieve
nearest-neighbor connectivity, and coherent permutation operators that preserve
superposition while enabling amplitude reordering. We demonstrate our methods
on structured sparse matrices, showing significant reductions in circuit depth
and control overhead, thereby bridging the gap between theoretical formulations
and practical circuit implementations for quantum algorithms.

</details>


### [104] [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 这篇论文对量子文本生成模型与传统Transformer/MLP架构进行了系统性评估，发现Transformer在总体性能上仍最优，但量子受启发模型在特定场景中表现竞争力


<details>
  <summary>Details</summary>
Motivation: 因对量子计算在自然语言处理应用中日益增长的兴趣，需要系统性评估量子文本生成模型的性能

Method: 使用5种不同模型（Transformer、QKSAN、QRWKV、QASA）在5个多样化数据集上进行实验，采用迭代度、BLEU分数、词汇多样性、重复率和流畅度等多种指标评估

Result: Transformer模型总体最优（平均迭代度1.21，BLEU-1 0.2895），但量子模型在特定方面表现竞争力：QKSAN实现了零重复率和竞争性BLEU-1分数（0.2800），QRWKV在某些任务中实现了完美的词汇多样性

Conclusion: 传统Transformer模型在文本生成任务中仍然占据优势，但量子受启发模型在特定性能指标上显示出竞争力，为量子计算在NLP领域的应用提供了有价值的参考

Abstract: This paper presents a comprehensive evaluation of quantum text generation
models against traditional Transformer/MLP architectures, addressing the
growing interest in quantum computing applications for natural language
processing. We conduct systematic experiments comparing five distinct models:
Transformer (baseline), Quantum Kernel Self-Attention Network (QKSAN), Quantum
RWKV (QRWKV), and Quantum Attention Sequence Architecture (QASA) across five
diverse datasets including simple sentences, short stories, quantum phrases,
haiku poetry, and proverbs. Our evaluation employs multiple metrics including
perplexity, BLEU scores, vocabulary diversity, repetition rates, and fluency
measures to assess different aspects of text generation quality. The
experimental results reveal that while traditional Transformer models maintain
overall superiority with the lowest average perplexity (1.21) and highest
BLEU-1 score (0.2895), quantum-inspired models demonstrate competitive
performance in specific scenarios. Notably, QKSAN achieves a competitive BLEU-1
score of 0.2800 while maintaining zero repetition rates, and QRWKV demonstrates
perfect vocabulary diversity (Distinct-1 = 1.000) in certain tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [105] [Quantum-inspired probability metrics define a complete, universal space for statistical learning](https://arxiv.org/abs/2508.21086)
*Logan S. McCarty*

Main category: stat.ML

TL;DR: 提出了量子概率度量(QPMs)，通过将概率测度嵌入量子态空间来克服高维和非紧致域中传统方法(如MMD)的局限性，提供了更好的分布差异敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如最大均值差异(MMD)在高维和非紧致域中表现不佳，需要新的概率分布比较方法来处理这些挑战。

Method: 将概率测度嵌入量子态空间(希尔伯特空间上的正定单位迹算子)，构建量子概率度量，扩展基于核的方法并克服MMD在非紧致空间的不完备性。

Result: QPMs能够均匀逼近所有有界一致连续函数，对高维分布中的细微差异更敏感，在生成建模任务中显著提升性能，但计算复杂度较高(O(n^3))。

Conclusion: 通过结合量子力学和经典概率论的数学框架，QPMs为分析和操作概率测度提供了强大的工具基础，可作为MMD的直接替代方案。

Abstract: Comparing probability distributions is a core challenge across the natural,
social, and computational sciences. Existing methods, such as Maximum Mean
Discrepancy (MMD), struggle in high-dimensional and non-compact domains. Here
we introduce quantum probability metrics (QPMs), derived by embedding
probability measures in the space of quantum states: positive, unit-trace
operators on a Hilbert space. This construction extends kernel-based methods
and overcomes the incompleteness of MMD on non-compact spaces. Viewed as an
integral probability metric (IPM), QPMs have dual functions that uniformly
approximate all bounded, uniformly continuous functions on $\mathbb{R}^n$,
offering enhanced sensitivity to subtle distributional differences in high
dimensions. For empirical distributions, QPMs are readily calculated using
eigenvalue methods, with analytic gradients suited for learning and
optimization. Although computationally more intensive for large sample sizes
($O(n^3)$ vs. $O(n^2)$), QPMs can significantly improve performance as a
drop-in replacement for MMD, as demonstrated in a classic generative modeling
task. By combining the rich mathematical framework of quantum mechanics with
classical probability theory, this approach lays the foundation for powerful
tools to analyze and manipulate probability measures.

</details>


### [106] [Weighted Support Points from Random Measures: An Interpretable Alternative for Generative Modeling](https://arxiv.org/abs/2508.21255)
*Peiqi Zhao,Carlos E. Rodríguez,Ramsés H. Mena,Stephen G. Walker*

Main category: stat.ML

TL;DR: 提出基于随机加权支持点的生成建模框架，通过Dirichlet过程和贝叶斯自助法启发的加权方案生成多样化样本，无需概率建模假设或神经网络架构，计算成本远低于GAN和DDPM。


<details>
  <summary>Details</summary>
Motivation: 支持点通过代表性点集来概括大型数据集，但传统方法缺乏生成多样化样本的能力。需要一种既能保持数据底层结构，又能生成解释性样本的轻量级生成方法。

Method: 基于Dirichlet过程和贝叶斯自助法的随机加权方案，使用凸凹过程(CCP)进行高效优化，生成随机加权支持点作为生成模型。

Result: 在MNIST和CelebA-HQ数据集上实验表明，该方法能以较低计算成本产生高质量、多样化的输出，性能优于GAN和DDPM等黑盒替代方案。

Conclusion: 随机加权支持点提供了原则性、可扩展且可解释的生成建模替代方案，能够产生真正保持底层数据结构的插值样本。

Abstract: Support points summarize a large dataset through a smaller set of
representative points that can be used for data operations, such as Monte Carlo
integration, without requiring access to the full dataset. In this sense,
support points offer a compact yet informative representation of the original
data. We build on this idea to introduce a generative modeling framework based
on random weighted support points, where the randomness arises from a weighting
scheme inspired by the Dirichlet process and the Bayesian bootstrap. The
proposed method generates diverse and interpretable sample sets from a fixed
dataset, without relying on probabilistic modeling assumptions or neural
network architectures. We present the theoretical formulation of the method and
develop an efficient optimization algorithm based on the Convex--Concave
Procedure (CCP). Empirical results on the MNIST and CelebA-HQ datasets show
that our approach produces high-quality and diverse outputs at a fraction of
the computational cost of black-box alternatives such as Generative Adversarial
Networks (GANs) or Denoising Diffusion Probabilistic Models (DDPMs). These
results suggest that random weighted support points offer a principled,
scalable, and interpretable alternative for generative modeling. A key feature
is their ability to produce genuinely interpolative samples that preserve
underlying data structure.

</details>


### [107] [Adaptive generative moment matching networks for improved learning of dependence structures](https://arxiv.org/abs/2508.21531)
*Marius Hofert,Gan Yao*

Main category: stat.ML

TL;DR: 提出了一种自适应带宽选择方法用于MMD混合核，改进了生成矩匹配网络(GMMNs)的训练，在copula随机数生成器学习中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统GMMNs在训练过程中带宽选择固定，无法自适应调整，影响了模型性能和收敛效果。需要一种能够根据训练过程动态调整带宽的方法来提升生成模型的质量

Method: 基于训练损失的相对误差动态增加核数量，使用验证损失的相对误差作为早停准则，构建自适应训练的AGMMNs

Result: AGMMNs训练时间与GMMNs相当但性能显著提升，在高维copula采样、金融衍生品定价和风险度量等应用中均优于传统GMMNs和参数化copula模型

Conclusion: 自适应带宽选择的AGMMNs在copula建模中表现出色，为高维随机数生成和金融风险管理提供了有效的非参数建模工具

Abstract: An adaptive bandwidth selection procedure for the mixture kernel in the
maximum mean discrepancy (MMD) for fitting generative moment matching networks
(GMMNs) is introduced, and its ability to improve the learning of copula random
number generators is demonstrated. Based on the relative error of the training
loss, the number of kernels is increased during training; additionally, the
relative error of the validation loss is used as an early stopping criterion.
While training time of such adaptively trained GMMNs (AGMMNs) is similar to
that of GMMNs, training performance is increased significantly in comparison to
GMMNs, which is assessed and shown based on validation MMD trajectories,
samples and validation MMD values. Superiority of AGMMNs over GMMNs, as well as
typical parametric copula models, is demonstrated in terms of three
applications. First, convergence rates of quasi-random versus pseudo-random
samples from high-dimensional copulas are investigated for three functionals of
interest and in dimensions as large as 100 for the first time. Second,
replicated validation MMDs, as well as Monte Carlo and quasi-Monte Carlo
applications based on the expected payoff of a basked call option and the risk
measure expected shortfall as functionals are used to demonstrate the improved
training of AGMMNs over GMMNs for a copula model fitted to the standardized
residuals of the 50 constituents of the S&P 500 index after deGARCHing. Last,
both the latter dataset and 50 constituents of the FTSE~100 are used to
demonstrate that the improved training of AGMMNs over GMMNs and in comparison
to the fitting of classical parametric copula models indeed also translates to
an improved model prediction.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [108] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文提出了一种基于联盟区块链的去中心化智能家庭设备入网框架，解决传统PKI模型的安全风险和用户主权问题。


<details>
  <summary>Details</summary>
Motivation: 智能家庭和IoT安全系统的普及带来了方便性和安全性提升的机遇，但设备入网过程中的凭证传统和云平台信任建立仍面临挑战。传统的中心化PKI模型和制造商控制的密钥导致安全风险和用户数字主权受限，阻碍了IoT解决方案的大规模部署。

Method: 研究提出了一种新的入网框架，在现有网络层入网技术基础上扩展到应用层。通过集成联盟区块链技术，实现了去中心化的入网机制，支持设备注册、密钥撤销、访问控制管理和通过事件驱动报警进行风险检测。使用Tamarin Prover工具在Dolev-Yao攻击模型下对协议进行正式形式化分析。

Result: 原型实现证明了系统在智能家庭环境中的可行性，验证完成时间仅0.34秒，显示了其可扩展性和适合于约束设备。性能评估显示，区块链方案能够有效处理不同工作负荷，保持高吞吐量和低延迟，支持近实时IoT数据处理。

Conclusion: 该研究提出的去中心化入网框架通过区块链技术有效解决了智能家庭设备入网过程中的安全性、透明度和监控问题，为大规模IoT部署提供了可扩展的解决方案。

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [109] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: 提出了一种基于原始PCAP数据的加密流量分类模型，利用跨流信号捕获时间相关性和包量分布，在多个分类任务和数据集上表现优异，准确率最高达99%。


<details>
  <summary>Details</summary>
Motivation: 现有加密流量分类方法通常需要先验假设且缺乏通用性，无法有效处理跨流的时间相关性和包量分布特征。

Method: 开发了一种直接处理原始PCAP数据的通用模型，采用创新的跨流信号表示方法，捕获流量间的时间相关性和包量分布模式。

Result: 模型在几乎所有分类任务和大多数数据集上都优于现有方法，部分情况下准确率达到99%，展现了出色的鲁棒性和适应性。

Conclusion: 该模型通过直接处理原始数据和利用跨流信号，实现了对加密流量的高效准确分类，为网络流量分析提供了新的有效解决方案。

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [110] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus是一个新颖的定向模糊测试框架，通过合成谓词来捕获语义上有意义的中间状态作为里程碑，显著提高了发现真实世界漏洞的效率，平均加速41.6倍


<details>
  <summary>Details</summary>
Motivation: 定向模糊测试面临挑战，因为目标状态通常深藏在程序中，而搜索空间巨大。现有方法依赖分支距离或手动指定约束，但这些方法不够精确且难以泛化到不同的目标状态和程序

Method: Locus采用智能代理框架，通过程序分析工具合成和迭代精化候选谓词，确保谓词严格松弛目标状态以防止误拒绝，同时使用符号执行进行验证

Result: Locus显著提高了8个最先进模糊测试器的效率，平均加速41.6倍，发现了8个之前未修补的漏洞，其中一个已获得确认并准备修补

Conclusion: Locus通过合成语义上有意义的中间状态谓词，为定向模糊测试提供了更精确的进度指导，解决了现有方法的局限性，在漏洞发现方面表现出色

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [111] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: 欧盟网络安全立法采用风险导向方法，涵盖技术、组织和人为安全等多维度风险，但存在可接受风险、非概率风险和残余风险等概念缺失。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟五项核心网络安全立法中风险概念的框架方式，分析这些立法在风险概念上的趋同或分歧，以及描述法律风险概念时使用的限定词和术语。

Method: 基于定性法律解释和分类构建的方法论，对五项欧盟网络安全立法文件进行系统分析。

Result: 五项立法涵盖广泛的网络安全风险类型，包括技术、组织和人为安全风险，以及非人为因素风险。多数立法使用技术方面和资产来构建法律风险概念，其中一项立法采用威胁中心视角。但存在可接受风险、非概率风险和残余风险等显著概念缺失。

Conclusion: 欧盟新网络安全立法显著扩展了基于风险的监管方法，但同时也增加了复杂性和合规负担。论文最后提出了处理合规问题和进行相关研究的实用建议。

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [112] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: 这篇论文对52个开源大语言模型进行了依赖供应链安全实证分析，发现半数漏洞长期未修复，75.8%的LLM包含存在漏洞的依赖项。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型发展快速，但它们依赖外部代码库形成了复杂的依赖供应链，而现有研究主要集中在模型层面的安全威胁，对依赖链漏洞关注不够。

Method: 对52个开源LLM进行实证分析，检查其第三方依赖和相关漏洞，分析仓库活动了解维护者实践，并与Python生态系统进行对比。

Result: 发现LLM生态系统中半数漏洞长期未修复（超过56.2个月），远长于Python生态系统，且75.8%的LLM在配置文件中包含有漏洞的依赖项。

Conclusion: 这项研究提升了对LLM依赖供应链风险的理解，为实践者提供了见解，并指明了改善LLM依赖链安全的潜在方向。

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [113] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: 提出RepoMark数据标记框架，用于审计代码LLM的数据使用情况，通过生成语义等价代码变体并使用排序基于假设检验检测模型记忆，在保持低假检出率的前提下达到90%检测成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在开源代码库训练引发了代码授权和开源许可话的依法问题，需要一种方法让仓库所有者验证其代码是否被用于模型训练。

Method: 通过生成多个语义等价的代码变体，在代码文件中引入数据标记，检测时使用新的排序基于假设检验来检测模型对标记代码的记忆情况。

Result: 在5%假检出率保证下，对小型代码仓库达到超过90%的检测成功率，显著超过现有技术（仅55%准确率）。

Conclusion: RepoMark提供了一种健壁、理论坚固的数据审计方案，能够在代码文件数量少的情况下有效工作，为代码LLM训练的透明性提供保障，维护仓库所有者权益。

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [114] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: 这篇论文研究了无触发器数据毒化攻击对代码生成模型的影响，评估了现有检测方法的有效性，发现它们都难以检测这种隐蔽性攻击。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在代码生成中的应用日益普及，数据毒化攻击成为严重威胁。特别是无触发器的目标攻击，能够在不依赖显式触发器的情况下突然替换安全代码为漏洞代码，极难检测。

Method: 对三个深度学习模型（CodeBERT、CodeT5+、AST-T5）进行目标毒化攻击，并评估三种防御方法：谱签名分析、激活聚类和静态分析。

Result: 所有检测方法都在无触发器毒化攻击下表现屏弱：表征基方法无法隔离毒化样本，静态分析存在假阻性和假阻性问题。

Conclusion: 现有防御方法对无触发器数据毒化攻击效果有限，强调需要发展更加健壮、不依赖触发器的防御方法来保护AI辅助代码生成系统。

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


### [115] [I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks](https://arxiv.org/abs/2508.21654)
*Daryna Oliynyk,Rudolf Mayer,Kathrin Grosse,Andreas Rauber*

Main category: cs.CR

TL;DR: 这篇论文首次提出了模型盗窃攻击的标准化评估方法，通过分析图像分类模型攻击案例，提出了完整的威胁模型和攻击比较框架，并提出了攻击开发的最佳实践和开放性研究问题。


<details>
  <summary>Details</summary>
Motivation: 模型盗窃攻击威胁机器学习模型的保密性，但现有攻击设计和评估缺乏标准化，导致难以比较不同研究成果和评估领域进展。

Method: 研穆大量依赖代替模型训练的图像分类模型攻击案例，提出第一个完整的威胁模型和攻击比较框架，分析相关工作的攻击设置。

Result: 提出了攻击开发在实验前、实验中和实验后的最佳实践，并派生了一个广泛的开放性研究问题清单，为模型盗窃攻击建立了第一个通用性评估方法。

Conclusion: 该研究填补了模型盗窃攻击评估方法的空白，提供的标准化建议和框架不仅适用于图像分类领域，还可以转移到其他问题领域，推动该领域的进一步发展。

Abstract: Model stealing attacks endanger the confidentiality of machine learning
models offered as a service. Although these models are kept secret, a malicious
party can query a model to label data samples and train their own substitute
model, violating intellectual property. While novel attacks in the field are
continually being published, their design and evaluations are not standardised,
making it challenging to compare prior works and assess progress in the field.
This paper is the first to address this gap by providing recommendations for
designing and evaluating model stealing attacks. To this end, we study the
largest group of attacks that rely on training a substitute model -- those
attacking image classification models. We propose the first comprehensive
threat model and develop a framework for attack comparison. Further, we analyse
attack setups from related works to understand which tasks and models have been
studied the most. Based on our findings, we present best practices for attack
development before, during, and beyond experiments and derive an extensive list
of open research questions regarding the evaluation of model stealing attacks.
Our findings and recommendations also transfer to other problem domains, hence
establishing the first generic evaluation methodology for model stealing
attacks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [116] [Deep Active Learning for Lung Disease Severity Classification from Chest X-rays: Learning with Less Data in the Presence of Class Imbalance](https://arxiv.org/abs/2508.21263)
*Roy M. Gabriel,Mohammadreza Zandehshahvar,Marly van Assen,Nattakorn Kittisut,Kyle Peters,Carlo N. De Cecco,Ali Adibi*

Main category: eess.IV

TL;DR: 该研究应用贝叶斯神经网络近似和加权损失函数的深度主动学习方法，在胸部X光片肺病严重程度分类中，仅需15.4%-23.1%的标注数据即可达到93.7%的准确率，有效解决了类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 减少胸部X光片肺病严重程度分类所需的标注数据量，特别是在类别不平衡的情况下，降低标注成本和提高效率。

Method: 使用蒙特卡洛Dropout的深度神经网络，结合主动学习和加权损失函数，通过不同的采集函数迭代选择最有信息量的样本进行训练。

Result: 熵采样在二分类任务中达到93.7%准确率（AU ROC 0.91），仅使用15.4%训练数据；多分类任务中平均标准差采样达到70.3%准确率（AU ROC 0.86），使用23.1%标注数据。

Conclusion: 深度主动学习结合贝叶斯神经网络近似和加权损失函数能显著减少标注需求，同时保持或超越诊断性能，有效解决类别不平衡问题。

Abstract: To reduce the amount of required labeled data for lung disease severity
classification from chest X-rays (CXRs) under class imbalance, this study
applied deep active learning with a Bayesian Neural Network (BNN) approximation
and weighted loss function. This retrospective study collected 2,319 CXRs from
963 patients (mean age, 59.2 $\pm$ 16.6 years; 481 female) at Emory Healthcare
affiliated hospitals between January and November 2020. All patients had
clinically confirmed COVID-19. Each CXR was independently labeled by 3 to 6
board-certified radiologists as normal, moderate, or severe. A deep neural
network with Monte Carlo Dropout was trained using active learning to classify
disease severity. Various acquisition functions were used to iteratively select
the most informative samples from an unlabeled pool. Performance was evaluated
using accuracy, area under the receiver operating characteristic curve (AU
ROC), and area under the precision-recall curve (AU PRC). Training time and
acquisition time were recorded. Statistical analysis included descriptive
metrics and performance comparisons across acquisition strategies. Entropy
Sampling achieved 93.7% accuracy (AU ROC, 0.91) in binary classification
(normal vs. diseased) using 15.4% of the training data. In the multi-class
setting, Mean STD sampling achieved 70.3% accuracy (AU ROC, 0.86) using 23.1%
of the labeled data. These methods outperformed more complex and
computationally expensive acquisition functions and significantly reduced
labeling needs. Deep active learning with BNN approximation and weighted loss
effectively reduces labeled data requirements while addressing class imbalance,
maintaining or exceeding diagnostic performance.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [117] [Trajectory learning for ensemble forecasts via the continuous ranked probability score: a Lorenz '96 case study](https://arxiv.org/abs/2508.21664)
*Sagy Ephrati,James Woodfield*

Main category: math.NA

TL;DR: 使用连续排序概率评分(CRPS)作为损失函数，在Lorenz '96系统中开发随机参数化方法，生成更准确和锐利的集合预报


<details>
  <summary>Details</summary>
Motivation: 探索轨迹学习在集合预报中的可行性，特别是利用CRPS作为损失函数来改进参数化方案

Method: 采用两尺度Lorenz '96系统作为案例研究，开发和训练加性和乘性随机参数化方法，使用CRPS作为损失函数进行轨迹学习

Result: CRPS轨迹学习产生的参数化方案既准确又锐利，易于校准，在短期预报中优于基于导数拟合的参数化方法

Conclusion: 该方法在数据同化应用中特别有前景，因为其在短预报时效内具有高准确性

Abstract: This paper demonstrates the feasibility of trajectory learning for ensemble
forecasts by employing the continuous ranked probability score (CRPS) as a loss
function. Using the two-scale Lorenz '96 system as a case study, we develop and
train both additive and multiplicative stochastic parametrizations to generate
ensemble predictions. Results indicate that CRPS-based trajectory learning
produces parametrizations that are both accurate and sharp. The resulting
parametrizations are straightforward to calibrate and outperform
derivative-fitting-based parametrizations in short-term forecasts. This
approach is particularly promising for data assimilation applications due to
its accuracy over short lead times.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [118] [Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models](https://arxiv.org/abs/2508.21165)
*Natalia L. Rubio,Eric F. Darve,Alison L. Marsden*

Main category: cs.CE

TL;DR: 提出了一种结合机器学习预测分叉系数的零维血流动力学降阶模型，显著提高了计算精度同时保持计算效率，在心血管血流模拟中实现了准确性和实时性的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统三维有限元模拟心血管血流计算成本高，临床实用性有限；而现有降阶模型在血管分叉处精度不足，无法准确捕捉复杂流动物理特性。

Method: 开发了电阻-电阻-电感(RRI)模型，使用神经网络从分叉几何预测压力-流量关系，包含线性和二次阻力以及电感效应，采用无量纲化减少训练数据需求，并通过优化策略集成到零维模型中。

Result: 在所有血管树和雷诺数范围内，RRI方法将入口压力误差从标准0D模型的54 mmHg(45%)降低到25 mmHg(17%)，简化版RI变体达到31 mmHg(26%)误差，在高雷诺数和复杂血管网络中表现尤为有效。

Conclusion: 这种混合数值方法为临床决策支持、不确定性量化和心血管生物医学工程中的数字孪生提供了准确、实时的血流动力学建模能力。

Abstract: Three-dimensional (3D) finite-element simulations of cardiovascular flows
provide high-fidelity predictions to support cardiovascular medicine, but their
high computational cost limits clinical practicality. Reduced-order models
(ROMs) offer computationally efficient alternatives but suffer reduced
accuracy, particularly at vessel bifurcations where complex flow physics are
inadequately captured by standard Poiseuille flow assumptions. We present an
enhanced numerical framework that integrates machine learning-predicted
bifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve
accuracy while maintaining computational efficiency. We develop a
resistor-resistor-inductor (RRI) model that uses neural networks to predict
pressure-flow relationships from bifurcation geometry, incorporating linear and
quadratic resistances along with inductive effects. The method employs
non-dimensionalization to reduce training data requirements and apriori flow
split prediction for improved bifurcation characterization. We incorporate the
RRI model into a 0D model using an optimization-based solution strategy. We
validate the approach in isolated bifurcations and vascular trees, across
Reynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D
finite element simulation. Results demonstrate substantial accuracy
improvements: averaged across all trees and Reynolds numbers, the RRI method
reduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25
mmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg
(26%) error. The enhanced 0D models show particular effectiveness at high
Reynolds numbers and in extensive vascular networks. This hybrid numerical
approach enables accurate, real-time hemodynamic modeling for clinical decision
support, uncertainty quantification, and digital twins in cardiovascular
biomedical engineering.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [119] [Considerations for Estimating Causal Effects of Informatively Timed Treatments](https://arxiv.org/abs/2508.21804)
*Arman Oganisian*

Main category: stat.ME

TL;DR: 本文探讨了流行病学研究中治疗决策时间点信息性对因果效应估计的影响，提出了将治疗间隔时间作为时变混杂因子处理的g-方法解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往忽略治疗决策时间点的变异性及其对后续治疗决策和潜在结果的信息性影响，缺乏对此问题的认识和解决方案。

Method: 使用g-方法分析信息性时间点的序贯治疗，将治疗间隔时间视为时变混杂因子进行调整，并通过合成示例说明调整方法。

Result: 研究表明，不调整治疗间隔时间的g-方法可能存在偏差，而通过调整可以处理患者在治疗间死亡或删失的情况。

Conclusion: 考虑治疗时间点对有效推断至关重要，使用g-方法调整治疗间隔时间作为时变混杂因子可以校正信息性时间点问题。

Abstract: Epidemiological studies are often concerned with estimating causal effects of
a sequence of treatment decisions on survival outcomes. In many settings,
treatment decisions do not occur at fixed, pre-specified followup times.
Rather, timing varies across subjects in ways that may be informative of
subsequent treatment decisions and potential outcomes. Awareness of the issue
and its potential solutions is lacking in the literature, which motivate this
work. Here, we formalize the issue of informative timing, problems associated
with ignoring it, and show how g-methods can be used to analyze sequential
treatments that are informatively timed. As we describe, in such settings, the
waiting times between successive treatment decisions may be properly viewed as
a time-varying confounders. Using synthetic examples, we illustrate how
g-methods that do not adjust for these waiting times may be biased and how
adjustment can be done in scenarios where patients may die or be censored in
between treatments. We draw connections between adjustment and identification
with discrete-time versus continuous-time models. Finally, we provide
implementation guidance and examples using publicly available software. Our
concluding message is that 1) considering timing is important for valid
inference and 2) correcting for informative timing can be done with g-methods
that adjust for waiting times between treatments as time-varying confounders.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [120] [Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education](https://arxiv.org/abs/2508.21666)
*Imran S. A. Khan,Emmanuel G. Blanchard,Sébastien George*

Main category: cs.HC

TL;DR: FACTS是一个结合物联网传感器和生成式AI的气候韧性教育平台，通过实时大气数据和个性化学习挑战提升教育效果


<details>
  <summary>Details</summary>
Motivation: 应对气候变化挑战，通过技术创新提升气候韧性教育的有效性和参与度

Method: 结合物联网传感器收集实时大气数据，利用知识库资源动态生成本地化学习挑战，通过生成式AI服务器分析学习者响应并提供个性化反馈

Result: 用户评估显示系统易于使用且能有效构建气候韧性相关知识

Conclusion: 物联网和生成式AI在大气自适应学习技术中的整合具有显著潜力，能增强教育参与度和培养气候意识

Abstract: This paper introduces the Future Atmospheric Conditions Training System
(FACTS), a novel platform that advances climate resilience education through
place-based, adaptive learning experiences. FACTS combines real-time
atmospheric data collected by IoT sensors with curated resources from a
Knowledge Base to dynamically generate localized learning challenges. Learner
responses are analyzed by a Generative AI powered server, which delivers
personalized feedback and adaptive support. Results from a user evaluation
indicate that participants found the system both easy to use and effective for
building knowledge related to climate resilience. These findings suggest that
integrating IoT and Generative AI into atmospherically adaptive learning
technologies holds significant promise for enhancing educational engagement and
fostering climate awareness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [121] [RARR : Robust Real-World Activity Recognition with Vibration by Scavenging Near-Surface Audio Online](https://arxiv.org/abs/2508.21167)
*Dong Yoon Lee,Alyssa Weakley,Hui Wei,Blake Brown,Keyana Carrion,Shijia Pan*

Main category: cs.SD

TL;DR: 开发了一种基于结构振动传感器的远程监测系统，通过音频合成数据预训练模型，只需少量标注数据即可实现准确的日常活动识别


<details>
  <summary>Details</summary>
Motivation: 四分之一痴呆患者独居，需要远程照护。现有远程监测方案存在隐私保护、活动识别和模型泛化性等限制

Method: 使用结构振动传感器系统，通过近表面声学音频合成数据进行模型预训练，然后用有限数据进行微调

Result: 创建了一个稳健的日常活动跟踪框架，能够在用户家中部署时仅需少量标注数据

Conclusion: 该方法提供了一种可扩展的解决方案，解决了现有振动传感器系统需要大量标注数据的问题，适用于真实家庭环境中的活动监测

Abstract: One in four people dementia live alone, leading family members to take on
caregiving roles from a distance. Many researchers have developed remote
monitoring solutions to lessen caregiving needs; however, limitations remain
including privacy preserving solutions, activity recognition, and model
generalizability to new users and environments. Structural vibration sensor
systems are unobtrusive solutions that have been proven to accurately monitor
human information, such as identification and activity recognition, in
controlled settings by sensing surface vibrations generated by activities.
However, when deploying in an end user's home, current solutions require a
substantial amount of labeled data for accurate activity recognition. Our
scalable solution adapts synthesized data from near-surface acoustic audio to
pretrain a model and allows fine tuning with very limited data in order to
create a robust framework for daily routine tracking.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [122] [Pep2Prob Benchmark: Predicting Fragment Ion Probability for MS$^2$-based Proteomics](https://arxiv.org/abs/2508.21076)
*Hao Xu,Zhichao Wang,Shengqi Sang,Pisit Wajanasara,Nuno Bandeira*

Main category: q-bio.BM

TL;DR: Pep2Prob是首个用于肽特异性碎片离子概率预测的综合数据集和基准，包含60多万个独特前体的碎片离子概率统计，基于1.83亿高质量MS2谱图构建。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质组学中肽段碎片离子概率预测方法依赖于全局统计假设，认为所有肽段的碎片概率相同，这种简化假设从生化原理角度看过于简单，限制了预测准确性。

Method: 构建了包含608,780个独特前体（肽序列+电荷状态对）的碎片离子概率数据集，基于1.83亿个高质量HCD MS2谱图，建立了基于统计规则和机器学习方法的基准性能。

Result: 利用肽特异性信息的模型显著优于仅使用全局碎片统计的方法，且随着模型容量增加，性能提升表明肽-碎片关系存在复杂的非线性特征。

Conclusion: 肽特异性碎片离子概率预测需要复杂的机器学习方法，Pep2Prob数据集为开发更准确的肽段鉴定算法提供了重要资源。

Abstract: Proteins perform nearly all cellular functions and constitute most drug
targets, making their analysis fundamental to understanding human biology in
health and disease. Tandem mass spectrometry (MS$^2$) is the major analytical
technique in proteomics that identifies peptides by ionizing them, fragmenting
them, and using the resulting mass spectra to identify and quantify proteins in
biological samples. In MS$^2$ analysis, peptide fragment ion probability
prediction plays a critical role, enhancing the accuracy of peptide
identification from mass spectra as a complement to the intensity information.
Current approaches rely on global statistics of fragmentation, which assumes
that a fragment's probability is uniform across all peptides. Nevertheless,
this assumption is oversimplified from a biochemical principle point of view
and limits accurate prediction. To address this gap, we present Pep2Prob, the
first comprehensive dataset and benchmark designed for peptide-specific
fragment ion probability prediction. The proposed dataset contains fragment ion
probability statistics for 608,780 unique precursors (each precursor is a pair
of peptide sequence and charge state), summarized from more than 183 million
high-quality, high-resolution, HCD MS$^2$ spectra with validated peptide
assignments and fragmentation annotations. We establish baseline performance
using simple statistical rules and learning-based methods, and find that models
leveraging peptide-specific information significantly outperform previous
methods using only global fragmentation statistics. Furthermore, performance
across benchmark models with increasing capacities suggests that the
peptide-fragmentation relationship exhibits complex nonlinearities requiring
sophisticated machine learning approaches.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [123] [Machine Intelligence on the Edge: Interpretable Cardiac Pattern Localisation Using Reinforcement Learning](https://arxiv.org/abs/2508.21652)
*Haozhe Tian,Qiyu Rao,Nina Moutonnet,Pietro Ferraro,Danilo Mandic*

Main category: eess.SP

TL;DR: 提出序列匹配滤波器(SMF)，用强化学习设计的滤波器序列替代传统单匹配滤波器，解决低信噪比信号模式定位问题，在ECG检测中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统匹配滤波器在低信噪比信号(如边缘设备记录的ear-ECG)中效果下降，因为噪声模式可能与目标信号相似。

Method: 将滤波器设计建模为顺序决策过程，使用强化学习代理设计信号特定的滤波器序列，保持完全可解释性。

Result: 在两个具有挑战性的真实ECG数据集上实现了最先进的R峰检测和生理状态分类性能。

Conclusion: SMF框架具有可靠和可解释的临床决策支持潜力，可扩展到各种需要从噪声污染信号中准确定位模式的应用。

Abstract: Matched filters are widely used to localise signal patterns due to their high
efficiency and interpretability. However, their effectiveness deteriorates for
low signal-to-noise ratio (SNR) signals, such as those recorded on edge
devices, where prominent noise patterns can closely resemble the target within
the limited length of the filter. One example is the ear-electrocardiogram
(ear-ECG), where the cardiac signal is attenuated and heavily corrupted by
artefacts. To address this, we propose the Sequential Matched Filter (SMF), a
paradigm that replaces the conventional single matched filter with a sequence
of filters designed by a Reinforcement Learning agent. By formulating filter
design as a sequential decision-making process, SMF adaptively design
signal-specific filter sequences that remain fully interpretable by revealing
key patterns driving the decision-making. The proposed SMF framework has strong
potential for reliable and interpretable clinical decision support, as
demonstrated by its state-of-the-art R-peak detection and physiological state
classification performance on two challenging real-world ECG datasets. The
proposed formulation can also be extended to a broad range of applications that
require accurate pattern localisation from noise-corrupted signals.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [124] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 这篇论文对比分析了闭源GPT-4o和开源DeepSeek-V3-0324两种大语言模型的挑战应对方式，揭示了闭源模型在安全性和开源模型在效率适配性方面的特点，为不同领域应用提供选型指南。


<details>
  <summary>Details</summary>
Motivation: 大语言模型开发部署复杂性高，需要系统性分析16个关键挑战，通过对比最新闭源和开源模型来探索不同开发路径的优势和适用场景。

Method: 采用调查研究法，综述16个关键挑战领域，对比分析OpenAI GPT-4o(2024年5月)和DeepSeek-V3-0324(2025年3月)两款先进模型的解决方案，涉及聊天机器人、编程工具、医疗健康和教育等多个应用领域。

Result: 证明闭源模型在安全性和稳定性方面更优，而开源模型在效率和适配性方面更具优势，不同应用场景对模型特性有不同需求。

Conclusion: 为AI研究人员、开发者和决策者提供了完整的LLM能力限制和最佳实践指南，帮助根据具体应用需求选择合适的模型类型。

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [125] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 提出了一种基于层次聚类的自动分类器选择方法，通过优先考虑多样性来改进集成学习在假新闻检测中的性能


<details>
  <summary>Details</summary>
Motivation: 现有的集成学习方法在假新闻检测中性能严重依赖分类器的多样性，但选择真正多样化的模型仍然是一个关键挑战，特别是当模型倾向于学习冗余模式时

Method: 首先计算分类器之间的成对多样性，应用层次聚类将它们组织成不同粒度的组，然后通过HierarchySelect探索层次级别来选择每个级别的一个分类器池，每个池代表不同的内部多样性，最后选择最具多样性的池用于集成构建

Result: 在6个数据集中的2个上实现了最高准确率，优于Elbow启发式方法和最先进的基线方法

Conclusion: 提出的层次选择方法能够有效选择多样化的分类器，提高集成学习在假新闻检测中的性能

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [126] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了MahaSTS马拉地语句子相似度数据集和MahaSBERT-STS-v2模型，包含16,860个标注句子对，通过均衡分布标注分数来减少偏差，在低资源环境下有效提升句子相似度任务性能。


<details>
  <summary>Details</summary>
Motivation: 为马拉地语这种低资源语言构建高质量的句子相似度数据集和专用模型，解决现有资源不足的问题。

Method: 创建人工标注的MahaSTS数据集（16,860个句子对，0-5分连续标注），采用分数均衡分桶策略减少标签偏差，基于MahaSBERT模型进行回归式相似度评分微调。

Result: MahaSBERT-STS-v2模型在马拉地语句子相似度任务上表现优于MahaBERT、MuRIL、IndicBERT和IndicSBERT等基线模型。

Conclusion: 人工标注数据、针对性微调和结构化监督在低资源语言环境中对句子相似度任务具有显著提升效果，MahaSTS数据集和模型为马拉地语NLP研究提供了重要资源。

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>
